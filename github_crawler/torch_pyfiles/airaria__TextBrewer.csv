file_path,api_count,code
setup.py,0,"b'""""""\nSimple check list from AllenNLP repo: https://github.com/allenai/allennlp/blob/master/setup.py\nTo create the package for pypi.\n1. Change the version in __init__.py, setup.py as well as docs/source/conf.py.\n2. Commit these changes with the message: ""Release: VERSION""\n3. Add a tag in git to mark the release: ""git tag VERSION -m\'Adds tag VERSION for pypi\' ""\n   Push the tag to git: git push --tags origin master\n4. Build both the sources and the wheel. Do not change anything in setup.py between\n   creating the wheel and the source distribution (obviously).\n   For the wheel, run: ""python setup.py bdist_wheel"" in the top level directory.\n   (this will build a wheel for the python version you use to build it).\n   For the sources, run: ""python setup.py sdist""\n   You should now have a /dist directory with both .whl and .tar.gz source versions.\n5. Check that everything looks correct by uploading the package to the pypi test server:\n   twine upload dist/* -r pypitest\n   (pypi suggest using twine as other methods upload files via plaintext.)\n   Check that you can install it in a virtualenv by running:\n   pip install -i https://testpypi.python.org/pypi transformers\n6. Upload the final version to actual pypi:\n   twine upload dist/* -r pypi\n7. Copy the release notes from RELEASE.md to the tag in github once everything is looking hunky-dory.\n""""""\n\nimport shutil\nfrom pathlib import Path\n\nfrom setuptools import find_packages, setup\n\n\nsetup(\n    name=""textbrewer"",\n    version=""0.1.9"",\n    author=""ziqingyang"",\n    author_email=""zqyang5@iflytek.com"",\n    description=""PyTorch-based knowledge distillation toolkit for natural language processing"",\n    long_description=""PyTorch-based knowledge distillation toolkit for natural language processing."",\n    #long_description=open(""READMEshort.md"", ""r"", encoding=""utf-8"").read(),\n    long_description_content_type=""text/markdown"",\n    keywords=""NLP deep learning knowledge distillation pytorch"",\n    #license="""",\n    url=""http://textbrewer.hfl-rc.com"",\n    #package_dir={"""": ""src""},\n    packages=[\'textbrewer\'],\n    package_dir={\'\':\'src\'},\n    install_requires=[\n        ""numpy"",\n        ""torch >= 1.1"",\n        ""tensorboard"",\n        ""tqdm""\n    ],\n\n    python_requires="">=3.6"",\n    classifiers=[\n        #""Development Status :: 5 - Production/Stable"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Education"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Operating System :: OS Independent"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n)\n'"
docs/source/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../../src\'))\n\nfrom textbrewer import __version__\n\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'TextBrewer\'\nauthor = \'Joint Laboratory of HIT and iFLYTEK Research (HFL)\'\ncopyright = \'2020, \'+author\n\n# The full version, including alpha/beta/rc tags\nversion = __version__\nrelease = __version__\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'recommonmark\',\n              \'sphinx_markdown_tables\',\n              \'sphinx.ext.autodoc\',\n              \'sphinxcontrib.napoleon\',\n              \'sphinx.ext.viewcode\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n#html_theme = \'alabaster\'\nhtml_theme = \'sphinx_rtd_theme\'\n\nhtml_theme_options = {\n        \'logo_only\': True,\n        #\'style_nav_header_background\' :\'#EEEEEE\'\n        }\nhtml_logo = \'../../pics/nav_banner.png\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\ndef setup(app):\n    app.add_stylesheet(\'css/custom.css\')\n'"
examples/cmrc2018_example/cmrc2018_evaluate.py,0,"b'# -*- coding: utf-8 -*-\n\'\'\'\nEvaluation script for CMRC 2018\nversion: v5 - special\nNote: \nv5 - special: Evaluate on SQuAD-style CMRC 2018 Datasets\nv5: formatted output, add usage description\nv4: fixed segmentation issues\n\'\'\'\nfrom __future__ import print_function\nfrom collections import Counter, OrderedDict\nimport string\nimport re\nimport argparse\nimport json\nimport sys\n#sys.setdefaultencoding(\'utf8\')\nimport nltk\nimport pdb\n\n# split Chinese with English\ndef mixed_segmentation(in_str, rm_punc=False):\n\tin_str = (in_str).lower().strip()\n\tsegs_out = []\n\ttemp_str = """"\n\tsp_char = [\'-\',\':\',\'_\',\'*\',\'^\',\'/\',\'\\\\\',\'~\',\'`\',\'+\',\'=\',\n\t\t\t   \'\xef\xbc\x8c\',\'\xe3\x80\x82\',\'\xef\xbc\x9a\',\'\xef\xbc\x9f\',\'\xef\xbc\x81\',\'\xe2\x80\x9c\',\'\xe2\x80\x9d\',\'\xef\xbc\x9b\',\'\xe2\x80\x99\',\'\xe3\x80\x8a\',\'\xe3\x80\x8b\',\'\xe2\x80\xa6\xe2\x80\xa6\',\'\xc2\xb7\',\'\xe3\x80\x81\',\n\t\t\t   \'\xe3\x80\x8c\',\'\xe3\x80\x8d\',\'\xef\xbc\x88\',\'\xef\xbc\x89\',\'\xef\xbc\x8d\',\'\xef\xbd\x9e\',\'\xe3\x80\x8e\',\'\xe3\x80\x8f\']\n\tfor char in in_str:\n\t\tif rm_punc and char in sp_char:\n\t\t\tcontinue\n\t\tif re.search(r\'[\\u4e00-\\u9fa5]\', char) or char in sp_char:\n\t\t\tif temp_str != """":\n\t\t\t\tss = nltk.word_tokenize(temp_str)\n\t\t\t\tsegs_out.extend(ss)\n\t\t\t\ttemp_str = """"\n\t\t\tsegs_out.append(char)\n\t\telse:\n\t\t\ttemp_str += char\n\n\t#handling last part\n\tif temp_str != """":\n\t\tss = nltk.word_tokenize(temp_str)\n\t\tsegs_out.extend(ss)\n\n\treturn segs_out\n\n\n# remove punctuation\ndef remove_punctuation(in_str):\n\tin_str = str(in_str).lower().strip()\n\tsp_char = [\'-\',\':\',\'_\',\'*\',\'^\',\'/\',\'\\\\\',\'~\',\'`\',\'+\',\'=\',\n\t\t\t   \'\xef\xbc\x8c\',\'\xe3\x80\x82\',\'\xef\xbc\x9a\',\'\xef\xbc\x9f\',\'\xef\xbc\x81\',\'\xe2\x80\x9c\',\'\xe2\x80\x9d\',\'\xef\xbc\x9b\',\'\xe2\x80\x99\',\'\xe3\x80\x8a\',\'\xe3\x80\x8b\',\'\xe2\x80\xa6\xe2\x80\xa6\',\'\xc2\xb7\',\'\xe3\x80\x81\',\n\t\t\t   \'\xe3\x80\x8c\',\'\xe3\x80\x8d\',\'\xef\xbc\x88\',\'\xef\xbc\x89\',\'\xef\xbc\x8d\',\'\xef\xbd\x9e\',\'\xe3\x80\x8e\',\'\xe3\x80\x8f\']\n\tout_segs = []\n\tfor char in in_str:\n\t\tif char in sp_char:\n\t\t\tcontinue\n\t\telse:\n\t\t\tout_segs.append(char)\n\treturn \'\'.join(out_segs)\n\n\n# find longest common string\ndef find_lcs(s1, s2):\n\tm = [[0 for i in range(len(s2)+1)] for j in range(len(s1)+1)]\n\tmmax = 0\n\tp = 0\n\tfor i in range(len(s1)):\n\t\tfor j in range(len(s2)):\n\t\t\tif s1[i] == s2[j]:\n\t\t\t\tm[i+1][j+1] = m[i][j]+1\n\t\t\t\tif m[i+1][j+1] > mmax:\n\t\t\t\t\tmmax=m[i+1][j+1]\n\t\t\t\t\tp=i+1\n\treturn s1[p-mmax:p], mmax\n\n#\ndef evaluate(ground_truth_file, prediction_file):\n\tf1 = 0\n\tem = 0\n\ttotal_count = 0\n\tskip_count = 0\n\tfor instance in ground_truth_file[""data""]:\n\t\t#context_id   = instance[\'context_id\'].strip()\n\t\t#context_text = instance[\'context_text\'].strip()\n\t\tfor para in instance[""paragraphs""]:\n\t\t\tfor qas in para[\'qas\']:\n\t\t\t\ttotal_count += 1\n\t\t\t\tquery_id    = qas[\'id\'].strip()\n\t\t\t\tquery_text  = qas[\'question\'].strip()\n\t\t\t\tanswers \t= [x[""text""] for x in qas[\'answers\']]\n\n\t\t\t\tif query_id not in prediction_file:\n\t\t\t\t\tsys.stderr.write(\'Unanswered question: {}\\n\'.format(query_id))\n\t\t\t\t\tskip_count += 1\n\t\t\t\t\tcontinue\n\n\t\t\t\tprediction \t= (prediction_file[query_id])\n\t\t\t\tf1 += calc_f1_score(answers, prediction)\n\t\t\t\tem += calc_em_score(answers, prediction)\n\n\tf1_score = 100.0 * f1 / total_count\n\tem_score = 100.0 * em / total_count\n\treturn f1_score, em_score, total_count, skip_count\n\n\ndef calc_f1_score(answers, prediction):\n\tf1_scores = []\n\tfor ans in answers:\n\t\tans_segs = mixed_segmentation(ans, rm_punc=True)\n\t\tprediction_segs = mixed_segmentation(prediction, rm_punc=True)\n\t\tlcs, lcs_len = find_lcs(ans_segs, prediction_segs)\n\t\tif lcs_len == 0:\n\t\t\tf1_scores.append(0)\n\t\t\tcontinue\n\t\tprecision \t= 1.0*lcs_len/len(prediction_segs)\n\t\trecall \t\t= 1.0*lcs_len/len(ans_segs)\n\t\tf1 \t\t\t= (2*precision*recall)/(precision+recall)\n\t\tf1_scores.append(f1)\n\treturn max(f1_scores)\n\n\ndef calc_em_score(answers, prediction):\n\tem = 0\n\tfor ans in answers:\n\t\tans_ = remove_punctuation(ans)\n\t\tprediction_ = remove_punctuation(prediction)\n\t\tif ans_ == prediction_:\n\t\t\tem = 1\n\t\t\tbreak\n\treturn em\n\nif __name__ == \'__main__\':\n\tparser = argparse.ArgumentParser(description=\'Evaluation Script for CMRC 2018\')\n\tparser.add_argument(\'dataset_file\', help=\'Official dataset file\')\n\tparser.add_argument(\'prediction_file\', help=\'Your prediction File\')\n\targs = parser.parse_args()\n\tground_truth_file   = json.load(open(args.dataset_file, \'rb\'))\n\tprediction_file     = json.load(open(args.prediction_file, \'rb\'))\n\tF1, EM, TOTAL, SKIP = evaluate(ground_truth_file, prediction_file)\n\tAVG = (EM+F1)*0.5\n\toutput_result = OrderedDict()\n\toutput_result[\'AVERAGE\'] = \'%.3f\' % AVG\n\toutput_result[\'F1\'] = \'%.3f\' % F1\n\toutput_result[\'EM\'] = \'%.3f\' % EM\n\toutput_result[\'TOTAL\'] = TOTAL\n\toutput_result[\'SKIP\'] = SKIP\n\toutput_result[\'FILE\'] = args.prediction_file\n\tprint(json.dumps(output_result))\n\n'"
examples/cmrc2018_example/config.py,0,"b'import argparse\n\nargs = None\n\ndef parse(opt=None):\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n\n    parser.add_argument(""--vocab_file"", default=None, type=str, required=True,\n                        help=""The vocabulary file that the BERT model was trained on."")\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True,\n                        help=""The output directory where the model checkpoints will be written."")\n\n    ## Other parameters\n    parser.add_argument(""--train_file"", default=None, type=str, help=""SQuAD json for training. E.g., train-v2.0.json"")\n    parser.add_argument(""--predict_file"", default=None, type=str,\n                        help=""SQuAD json for predictions. E.g., dev-v2.0.json or test-v2.0.json"")\n    parser.add_argument(""--do_lower_case"", action=\'store_true\',\n                        help=""Whether to lower case the input text. Should be True for uncased ""\n                             ""models and False for cased models."")\n    parser.add_argument(""--max_seq_length"", default=416, type=int,\n                        help=""The maximum total input sequence length after WordPiece tokenization. Sequences ""\n                             ""longer than this will be truncated, and sequences shorter than this will be padded."")\n    parser.add_argument(""--doc_stride"", default=128, type=int,\n                        help=""When splitting up a long document into chunks, how much stride to take between chunks."")\n    parser.add_argument(""--max_query_length"", default=64, type=int,\n                        help=""The maximum number of tokens for the question. Questions longer than this will ""\n                             ""be truncated to this length."")\n    parser.add_argument(""--do_train"", default=False, action=\'store_true\', help=""Whether to run training."")\n    parser.add_argument(""--do_predict"", default=False, action=\'store_true\', help=""Whether to run eval on the dev set."")\n    parser.add_argument(""--train_batch_size"", default=32, type=int, help=""Total batch size for training."")\n    parser.add_argument(""--predict_batch_size"", default=8, type=int, help=""Total batch size for predictions."")\n    parser.add_argument(""--learning_rate"", default=3e-5, type=float, help=""The initial learning rate for Adam."")\n    parser.add_argument(""--num_train_epochs"", default=3.0, type=float,\n                        help=""Total number of training epochs to perform."")\n    parser.add_argument(""--warmup_proportion"", default=0.1, type=float,\n                        help=""Proportion of training to perform linear learning rate warmup for. E.g., 0.1 = 10% ""\n                             ""of training."")\n    parser.add_argument(""--n_best_size"", default=20, type=int,\n                        help=""The total number of n-best predictions to generate in the nbest_predictions.json ""\n                             ""output file."")\n    parser.add_argument(""--max_answer_length"", default=30, type=int,\n                        help=""The maximum length of an answer that can be generated. This is needed because the start ""\n                             ""and end predictions are not conditioned on one another."")\n    parser.add_argument(""--verbose_logging"", default=False, action=\'store_true\',\n                        help=""If true, all of the warnings related to data processing will be printed. ""\n                             ""A number of warnings are expected for a normal SQuAD evaluation."")\n    parser.add_argument(""--no_cuda"",\n                        default=False,\n                        action=\'store_true\',\n                        help=""Whether not to use CUDA when available"")\n    parser.add_argument(\'--gradient_accumulation_steps\',\n                        type=int,\n                        default=1,\n                        help=""Number of updates steps to accumualte before performing a backward/update pass."")\n    parser.add_argument(""--local_rank"",\n                        type=int,\n                        default=-1,\n                        help=""local_rank for distributed training on gpus"")\n    parser.add_argument(\'--fp16\',\n                        default=False,\n                        action=\'store_true\',\n                        help=""Whether to use 16-bit float precisoin instead of 32-bit"")\n\n    parser.add_argument(\'--random_seed\',type=int,default=10236797)\n    parser.add_argument(\'--fake_file_1\',type=str,default=None)\n    parser.add_argument(\'--fake_file_2\',type=str,default=None)\n    parser.add_argument(\'--load_model_type\',type=str,default=\'bert\',choices=[\'bert\',\'all\',\'none\'])\n    parser.add_argument(\'--weight_decay_rate\',type=float,default=0.01)\n    parser.add_argument(\'--do_eval\',action=\'store_true\')\n    parser.add_argument(\'--PRINT_EVERY\',type=int,default=200)\n    parser.add_argument(\'--weight\',type=float,default=1.0)\n    parser.add_argument(\'--ckpt_frequency\',type=int,default=2)\n\n    parser.add_argument(\'--tuned_checkpoint_T\',type=str,default=None)\n    parser.add_argument(\'--tuned_checkpoint_S\',type=str,default=None)\n    parser.add_argument(""--init_checkpoint_S"", default=None, type=str)\n    parser.add_argument(""--bert_config_file_T"", default=None, type=str, required=True)\n    parser.add_argument(""--bert_config_file_S"", default=None, type=str, required=True)\n    parser.add_argument(""--temperature"", default=1, type=float, required=False)\n    parser.add_argument(""--teacher_cached"",action=\'store_true\')\n\n    parser.add_argument(\'--s_opt1\',type=float,default=1.0, help=""release_start / step1 / ratio"")\n    parser.add_argument(\'--s_opt2\',type=float,default=0.0, help=""release_level / step2"")\n    parser.add_argument(\'--s_opt3\',type=float,default=1.0, help=""not used / decay rate"")\n    parser.add_argument(\'--schedule\',type=str,default=\'warmup_linear_release\')\n    parser.add_argument(\'--null_score_diff_threshold\',type=float,default=99.0)\n\n    parser.add_argument(\'--tag\',type=str,default=\'RB\')\n    parser.add_argument(\'--no_inputs_mask\',action=\'store_true\')\n    parser.add_argument(\'--no_logits\', action=\'store_true\')\n    parser.add_argument(\'--output_att_score\',default=\'true\',choices=[\'true\',\'false\'])\n    parser.add_argument(\'--output_att_sum\', default=\'false\',choices=[\'true\',\'false\'])\n    parser.add_argument(\'--output_encoded_layers\'  ,default=\'true\',choices=[\'true\',\'false\'])\n    parser.add_argument(\'--output_attention_layers\',default=\'true\',choices=[\'true\',\'false\'])\n    parser.add_argument(\'--matches\',nargs=\'*\',type=str)\n    global args\n    if opt is None:\n        args = parser.parse_args()\n    else:\n        args = parser.parse_args(opt)\n\n\nif __name__ == \'__main__\':\n    print (args)\n    parse([\'--SAVE_DIR\',\'test\'])\n    print(args)\n'"
examples/cmrc2018_example/main.distill.py,18,"b'import logging\nlogger = logging.getLogger(""Main"")\nlogger.setLevel(logging.INFO)\nhandler_stream = logging.StreamHandler()\nhandler_stream.setLevel(logging.INFO)\nformatter = logging.Formatter(fmt=\'%(asctime)s - %(levelname)s - %(name)s -  %(message)s\', datefmt=\'%Y/%m/%d %H:%M:%S\')\nhandler_stream.setFormatter(formatter)\nlogger.addHandler(handler_stream)\n\nimport os,random\nimport numpy as np\nimport torch\nfrom processing import convert_examples_to_features, read_squad_examples\nfrom processing import ChineseFullTokenizer\nfrom pytorch_pretrained_bert.my_modeling import BertConfig\nfrom optimization import BERTAdam\nimport config\nfrom utils import read_and_convert, divide_parameters\nfrom modeling import BertForQASimple, BertForQASimpleAdaptor\nfrom textbrewer import DistillationConfig, TrainingConfig, GeneralDistiller\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nfrom functools import partial\n\nfrom train_eval import predict\n\ndef args_check(args):\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n        logger.warning(""Output directory () already exists and is not empty."")\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                            args.gradient_accumulation_steps))\n\n    if not args.do_train and not args.do_predict:\n        raise ValueError(""At least one of `do_train` or `do_predict` must be True."")\n\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        n_gpu = torch.cuda.device_count() if not args.no_cuda else 0\n    else:\n        device = torch.device(""cuda"", args.local_rank)\n        n_gpu = 1\n        torch.distributed.init_process_group(backend=\'nccl\')\n    logger.info(""device %s n_gpu %d distributed training %r"", device, n_gpu, bool(args.local_rank != -1))\n    args.n_gpu = n_gpu\n    args.device = device\n    return device, n_gpu\n\ndef main():\n    #parse arguments\n    config.parse()\n    args = config.args\n    for k,v in vars(args).items():\n        logger.info(f""{k}:{v}"")\n    #set seeds\n    torch.manual_seed(args.random_seed)\n    torch.cuda.manual_seed_all(args.random_seed)\n    np.random.seed(args.random_seed)\n    random.seed(args.random_seed)\n\n    #arguments check\n    device, n_gpu = args_check(args)\n    os.makedirs(args.output_dir, exist_ok=True)\n    forward_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n    args.forward_batch_size = forward_batch_size\n\n    #load bert config\n    bert_config_T = BertConfig.from_json_file(args.bert_config_file_T)\n    bert_config_S = BertConfig.from_json_file(args.bert_config_file_S)\n    assert args.max_seq_length <= bert_config_T.max_position_embeddings\n    assert args.max_seq_length <= bert_config_S.max_position_embeddings\n\n    #read data\n    train_examples = None\n    train_features = None\n    eval_examples = None\n    eval_features = None\n    num_train_steps = None\n\n    tokenizer = ChineseFullTokenizer(vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n    convert_fn = partial(convert_examples_to_features,\n                         tokenizer=tokenizer,\n                         max_seq_length=args.max_seq_length,\n                         doc_stride=args.doc_stride,\n                         max_query_length=args.max_query_length)\n    if args.do_train:\n        train_examples,train_features = read_and_convert(args.train_file,is_training=True, do_lower_case=args.do_lower_case,\n                                                         read_fn=read_squad_examples,convert_fn=convert_fn)\n        if args.fake_file_1:\n            fake_examples1,fake_features1 = read_and_convert(args.fake_file_1,is_training=True, do_lower_case=args.do_lower_case,\n                                                             read_fn=read_squad_examples,convert_fn=convert_fn)\n            train_examples += fake_examples1\n            train_features += fake_features1\n        if args.fake_file_2:\n            fake_examples2, fake_features2 = read_and_convert(args.fake_file_2,is_training=True, do_lower_case=args.do_lower_case,\n                                                              read_fn=read_squad_examples,convert_fn=convert_fn)\n            train_examples += fake_examples2\n            train_features += fake_features2\n\n        num_train_steps = int(len(train_features)/args.train_batch_size) * args.num_train_epochs\n\n    if args.do_predict:\n        eval_examples,eval_features = read_and_convert(args.predict_file,is_training=False, do_lower_case=args.do_lower_case,\n                                                         read_fn=read_squad_examples,convert_fn=convert_fn)\n\n    #Build Model and load checkpoint\n    model_T = BertForQASimple(bert_config_T,args)\n    model_S = BertForQASimple(bert_config_S,args)\n    #Load teacher\n    if args.tuned_checkpoint_T is not None:\n        state_dict_T = torch.load(args.tuned_checkpoint_T, map_location=\'cpu\')\n        model_T.load_state_dict(state_dict_T)\n        model_T.eval()\n    else:\n        assert args.do_predict is True\n    #Load student\n    if args.load_model_type==\'bert\':\n        assert args.init_checkpoint_S is not None\n        state_dict_S = torch.load(args.init_checkpoint_S, map_location=\'cpu\')\n        state_weight = {k[5:]:v for k,v in state_dict_S.items() if k.startswith(\'bert.\')}\n        missing_keys,_ = model_S.bert.load_state_dict(state_weight,strict=False)\n        assert len(missing_keys)==0\n    elif args.load_model_type==\'all\':\n        assert args.tuned_checkpoint_S is not None\n        state_dict_S = torch.load(args.tuned_checkpoint_S,map_location=\'cpu\')\n        model_S.load_state_dict(state_dict_S)\n    else:\n        logger.info(""Model is randomly initialized."")\n    model_T.to(device)\n    model_S.to(device)\n\n    if args.local_rank != -1 or n_gpu > 1:\n        if args.local_rank != -1:\n            raise NotImplementedError\n        elif n_gpu > 1:\n            model_T = torch.nn.DataParallel(model_T) #,output_device=n_gpu-1)\n            model_S = torch.nn.DataParallel(model_S) #,output_device=n_gpu-1)\n\n    if args.do_train:\n        #parameters\n        params = list(model_S.named_parameters())\n        all_trainable_params = divide_parameters(params, lr=args.learning_rate)\n        logger.info(""Length of all_trainable_params: %d"", len(all_trainable_params))\n\n\n        optimizer = BERTAdam(all_trainable_params,lr=args.learning_rate,\n                             warmup=args.warmup_proportion,t_total=num_train_steps,schedule=args.schedule,\n                             s_opt1=args.s_opt1, s_opt2=args.s_opt2, s_opt3=args.s_opt3)\n\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num orig examples = %d"", len(train_examples))\n        logger.info(""  Num split examples = %d"", len(train_features))\n        logger.info(""  Forward batch size = %d"", forward_batch_size)\n        logger.info(""  Num backward steps = %d"", num_train_steps)\n\n        ########### DISTILLATION ###########\n        train_config = TrainingConfig(\n            gradient_accumulation_steps = args.gradient_accumulation_steps,\n            ckpt_frequency = args.ckpt_frequency,\n            log_dir = args.output_dir,\n            output_dir = args.output_dir,\n            device = args.device)\n\n        from matches import matches\n        intermediate_matches = None\n        if isinstance(args.matches,(list,tuple)):\n            intermediate_matches = []\n            for match in args.matches:\n                intermediate_matches += matches[match]\n        logger.info(f""{intermediate_matches}"")\n        distill_config = DistillationConfig(\n            temperature = args.temperature,\n            intermediate_matches=intermediate_matches)\n\n        adaptor_T = partial(BertForQASimpleAdaptor, no_logits=args.no_logits, no_mask = args.no_inputs_mask)\n        adaptor_S = partial(BertForQASimpleAdaptor, no_logits=args.no_logits, no_mask = args.no_inputs_mask)\n\n        distiller = GeneralDistiller(train_config = train_config,\n                                   distill_config = distill_config,\n                                   model_T = model_T, model_S = model_S,\n                                   adaptor_T = adaptor_T,\n                                   adaptor_S = adaptor_S)\n\n        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n        all_doc_mask = torch.tensor([f.doc_mask for f in train_features], dtype=torch.float)\n        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n        all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=torch.long)\n        all_end_positions = torch.tensor([f.end_position for f in train_features], dtype=torch.long)\n\n        train_dataset = TensorDataset(all_input_ids, all_segment_ids, all_input_mask, all_doc_mask,\n                                   all_start_positions, all_end_positions)\n        if args.local_rank == -1:\n            train_sampler = RandomSampler(train_dataset)\n        else:\n            raise NotImplementedError\n        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.forward_batch_size,drop_last=True)\n        callback_func = partial(predict, \n                eval_examples=eval_examples,\n                eval_features=eval_features,\n                args=args)\n        with distiller:\n            distiller.train(optimizer, scheduler=None, dataloader=train_dataloader,\n                              num_epochs=args.num_train_epochs, callback=callback_func)\n\n    if not args.do_train and args.do_predict:\n        res = predict(model_S,eval_examples,eval_features,step=0,args=args)\n        print (res)\n\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/cmrc2018_example/main.trainer.py,16,"b'import logging\nlogger = logging.getLogger(""Main"")\nlogger.setLevel(logging.INFO)\nhandler_stream = logging.StreamHandler()\nhandler_stream.setLevel(logging.INFO)\nformatter = logging.Formatter(fmt=\'%(asctime)s - %(levelname)s - %(name)s -  %(message)s\', datefmt=\'%Y/%m/%d %H:%M:%S\')\nhandler_stream.setFormatter(formatter)\nlogger.addHandler(handler_stream)\n\nimport os,random\nimport numpy as np\nimport torch\nfrom processing import convert_examples_to_features, read_squad_examples\nfrom processing import ChineseFullTokenizer\nfrom pytorch_pretrained_bert.my_modeling import BertConfig\nfrom optimization import BERTAdam\nimport config\nfrom utils import read_and_convert, divide_parameters\nfrom modeling import BertForQASimple, BertForQASimpleAdaptorTraining\nfrom textbrewer import DistillationConfig, TrainingConfig, BasicTrainer\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nfrom functools import partial\n\nfrom train_eval import predict\n\ndef args_check(args):\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n        logger.warning(""Output directory () already exists and is not empty."")\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                            args.gradient_accumulation_steps))\n\n    if not args.do_train and not args.do_predict:\n        raise ValueError(""At least one of `do_train` or `do_predict` must be True."")\n\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        n_gpu = torch.cuda.device_count() if not args.no_cuda else 0\n    else:\n        device = torch.device(""cuda"", args.local_rank)\n        n_gpu = 1\n        torch.distributed.init_process_group(backend=\'nccl\')\n    logger.info(""device %s n_gpu %d distributed training %r"", device, n_gpu, bool(args.local_rank != -1))\n    args.n_gpu = n_gpu\n    args.device = device\n    return device, n_gpu\n\ndef main():\n    #parse arguments\n    config.parse()\n    args = config.args\n    for k,v in vars(args).items():\n        logger.info(f""{k}:{v}"")\n    #set seeds\n    torch.manual_seed(args.random_seed)\n    torch.cuda.manual_seed_all(args.random_seed)\n    np.random.seed(args.random_seed)\n    random.seed(args.random_seed)\n\n    #arguments check\n    device, n_gpu = args_check(args)\n    os.makedirs(args.output_dir, exist_ok=True)\n    forward_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n    args.forward_batch_size = forward_batch_size\n\n    #load bert config\n    bert_config_S = BertConfig.from_json_file(args.bert_config_file_S)\n    assert args.max_seq_length <= bert_config_S.max_position_embeddings\n\n    #read data\n    train_examples = None\n    train_features = None\n    eval_examples = None\n    eval_features = None\n    num_train_steps = None\n\n    tokenizer = ChineseFullTokenizer(vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n    convert_fn = partial(convert_examples_to_features,\n                         tokenizer=tokenizer,\n                         max_seq_length=args.max_seq_length,\n                         doc_stride=args.doc_stride,\n                         max_query_length=args.max_query_length)\n    if args.do_train:\n        train_examples,train_features = read_and_convert(args.train_file,is_training=True, do_lower_case=args.do_lower_case,\n                                                         read_fn=read_squad_examples,convert_fn=convert_fn)\n        if args.fake_file_1:\n            fake_examples1,fake_features1 = read_and_convert(args.fake_file_1,is_training=True, do_lower_case=args.do_lower_case,\n                                                             read_fn=read_squad_examples,convert_fn=convert_fn)\n            train_examples += fake_examples1\n            train_features += fake_features1\n        if args.fake_file_2:\n            fake_examples2, fake_features2 = read_and_convert(args.fake_file_2,is_training=True, do_lower_case=args.do_lower_case,\n                                                              read_fn=read_squad_examples,convert_fn=convert_fn)\n            train_examples += fake_examples2\n            train_features += fake_features2\n\n        num_train_steps = int(len(train_features)/args.train_batch_size) * args.num_train_epochs\n\n    if args.do_predict:\n        eval_examples,eval_features = read_and_convert(args.predict_file,is_training=False, do_lower_case=args.do_lower_case,\n                                                         read_fn=read_squad_examples,convert_fn=convert_fn)\n\n    #Build Model and load checkpoint\n    model_S = BertForQASimple(bert_config_S,args)\n    #Load student\n    if args.load_model_type==\'bert\':\n        assert args.init_checkpoint_S is not None\n        state_dict_S = torch.load(args.init_checkpoint_S, map_location=\'cpu\')\n        state_weight = {k[5:]:v for k,v in state_dict_S.items() if k.startswith(\'bert.\')}\n        missing_keys,_ = model_S.bert.load_state_dict(state_weight,strict=False)\n        assert len(missing_keys)==0\n    elif args.load_model_type==\'all\':\n        assert args.tuned_checkpoint_S is not None\n        state_dict_S = torch.load(args.tuned_checkpoint_S,map_location=\'cpu\')\n        model_S.load_state_dict(state_dict_S)\n    else:\n        logger.info(""Model is randomly initialized."")\n    model_S.to(device)\n\n    if args.local_rank != -1 or n_gpu > 1:\n        if args.local_rank != -1:\n            raise NotImplementedError\n        elif n_gpu > 1:\n            model_S = torch.nn.DataParallel(model_S) #,output_device=n_gpu-1)\n\n    if args.do_train:\n        #parameters\n        params = list(model_S.named_parameters())\n        all_trainable_params = divide_parameters(params, lr=args.learning_rate)\n        logger.info(""Length of all_trainable_params: %d"", len(all_trainable_params))\n\n\n        optimizer = BERTAdam(all_trainable_params,lr=args.learning_rate,\n                             warmup=args.warmup_proportion,t_total=num_train_steps,schedule=args.schedule,\n                             s_opt1=args.s_opt1, s_opt2=args.s_opt2, s_opt3=args.s_opt3)\n\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num orig examples = %d"", len(train_examples))\n        logger.info(""  Num split examples = %d"", len(train_features))\n        logger.info(""  Forward batch size = %d"", forward_batch_size)\n        logger.info(""  Num backward steps = %d"", num_train_steps)\n\n        ########### DISTILLATION ###########\n        train_config = TrainingConfig(\n            gradient_accumulation_steps = args.gradient_accumulation_steps,\n            ckpt_frequency = args.ckpt_frequency,\n            log_dir = args.output_dir,\n            output_dir = args.output_dir,\n            device = args.device)\n\n        distiller = BasicTrainer(train_config = train_config,\n                                   model = model_S,\n                                   adaptor = BertForQASimpleAdaptorTraining)\n\n        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n        all_doc_mask = torch.tensor([f.doc_mask for f in train_features], dtype=torch.float)\n        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n        all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=torch.long)\n        all_end_positions = torch.tensor([f.end_position for f in train_features], dtype=torch.long)\n\n        train_dataset = TensorDataset(all_input_ids, all_segment_ids, all_input_mask, all_doc_mask,\n                                   all_start_positions, all_end_positions)\n        if args.local_rank == -1:\n            train_sampler = RandomSampler(train_dataset)\n        else:\n            raise NotImplementedError\n        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.forward_batch_size,drop_last=True)\n        callback_func = partial(predict, \n                eval_examples=eval_examples,\n                eval_features=eval_features,\n                args=args)\n        with distiller:\n            distiller.train(optimizer, scheduler=None, dataloader=train_dataloader,\n                              num_epochs=args.num_train_epochs, callback=callback_func)\n\n    if not args.do_train and args.do_predict:\n        res = predict(model_S,eval_examples,eval_features,step=0,args=args)\n        print (res)\n\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/cmrc2018_example/matches.py,0,"b'L3_attention_mse=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1}]\n\nL3_attention_ce=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1}]\n\nL3_attention_mse_sum=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1}]\n\nL3_attention_ce_mean=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1}]\n\nL3_hidden_smmd=[{""layer_T"":[0,0],  ""layer_S"":[0,0], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[4,4],  ""layer_S"":[1,1], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[8,8],  ""layer_S"":[2,2], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[12,12],""layer_S"":[3,3], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1}]\n\nL3n_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]},\n                {""layer_T"":4, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]},\n                {""layer_T"":8, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]},\n                {""layer_T"":12,""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]}]\n\nL3_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1},\n               {""layer_T"":4, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1},\n               {""layer_T"":8, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1},\n               {""layer_T"":12,""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}]\n\nL3l_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",1024,768]},\n                {""layer_T"":4, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",1024,768]},\n                {""layer_T"":8, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",1024,768]},\n                {""layer_T"":12,""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",1024,768]}]\n#######################L4################\nL4_attention_mse=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1}]\n\nL4_attention_ce=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1}]\n\nL4_attention_mse_sum=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1}]\n\nL4_attention_ce_mean=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1}]\n\nL4_hidden_smmd=[{""layer_T"":[0,0],  ""layer_S"":[0,0], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[3,3],  ""layer_S"":[1,1], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[6,6],  ""layer_S"":[2,2], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[9,9],  ""layer_S"":[3,3], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[12,12],""layer_S"":[4,4], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1}]\n\nL4t_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":3, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":6, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":9, ""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":12,""layer_S"":4, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]}]\n\nmatches={\'L3_attention_mse\':L3_attention_mse,\'L3_attention_mse_sum\':L3_attention_mse_sum,\n         \'L3_attention_ce\' :L3_attention_ce, \'L3_attention_ce_mean\':L3_attention_ce_mean,\n         \'L3n_hidden_mse\'  :L3n_hidden_mse,  \'L3_hidden_smmd\'      :L3_hidden_smmd,       \'L3_hidden_mse\': L3_hidden_mse,\n         \'L3l_hidden_mse\'  :L3l_hidden_mse,\n         \'L4_attention_mse\':L4_attention_mse,\'L4_attention_mse_sum\':L4_attention_mse_sum,\n         \'L4_attention_ce\' :L4_attention_ce, \'L4_attention_ce_mean\':L4_attention_ce_mean,\n         \'L4t_hidden_mse\'  :L4t_hidden_mse,  \'L4_hidden_smmd\'      :L4_hidden_smmd,\n        }\n'"
examples/cmrc2018_example/modeling.py,2,"b""import logging\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\nimport torch.nn.functional as F\nimport config as Conf\nfrom pytorch_pretrained_bert.my_modeling import BertModel, BertLayerNorm\n\n\ndef initializer_builder(std):\n    _std = std\n    def init_bert_weights(module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=_std)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n    return init_bert_weights\n\n\nclass BertForQA(nn.Module):\n    def __init__(self, config):\n        super(BertForQA, self).__init__()\n        self.bert = BertModel(config, output_score=True, output_sum=1)\n        self.qa_outputs  = nn.Linear(config.hidden_size, 2)\n        self.cls_outputs = nn.Linear(config.hidden_size, 1)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        initializer = initializer_builder(config.initializer_range)\n        self.apply(initializer)\n        self.num_heads = config.num_attention_heads\n\n    def forward(self, input_ids, token_type_ids, attention_mask, doc_mask,\n                start_positions=None, end_positions=None, start_logits_T=None, end_logits_T=None,\n                attention_probs_sum_layer=None, attention_probs_sum_T=None):\n        if attention_probs_sum_layer is not None:\n            sequence_output, pooled_output, all_attention_probs_sum = self.bert(input_ids, token_type_ids, attention_mask,\n                                                                                output_all_encoded_layers=False, output_attention_layer=[attention_probs_sum_layer])\n            attention_probs_sum = all_attention_probs_sum[attention_probs_sum_layer]\n        else:\n            sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,  output_all_encoded_layers=False)\n        output_for_cls = self.dropout(pooled_output)\n\n        span_logits = self.qa_outputs(sequence_output)\n        cls_logits  = self.cls_outputs(output_for_cls).squeeze(-1)  # output size: batch_size\n\n        doc_mask[:,0] = 0\n        span_logits = span_logits + (1.0 - doc_mask.unsqueeze(-1)) * -10000.0\n        start_logits, end_logits = span_logits.split(1, dim=-1)  # use_cls\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        if start_logits_T is not None or start_positions is not None:\n            total_loss = 0\n            att_loss = None\n            if start_logits_T is not None:\n                temp=Conf.args.temperature\n                temp_2 = temp*temp\n                start_logits_T /= temp\n                end_logits_T /= temp\n                start_logits /= temp\n                end_logits /= temp\n                start_prob_T = F.softmax(start_logits_T,dim=-1)\n                end_prob_T = F.softmax(end_logits_T,dim=-1)\n                ce_loss = -(start_prob_T * F.log_softmax(start_logits, dim=-1) + end_prob_T * F.log_softmax(end_logits, dim=-1)).sum(dim=-1)\n                ce_loss = ce_loss.mean() #* temp_2\n                total_loss += ce_loss\n                if attention_probs_sum_T is not None:\n                    attention_probs_sum_T = (attention_probs_sum_T / self.num_heads)\n                    attention_probs_sum   = (attention_probs_sum   / self.num_heads)\n                    attention_probs_sum_T = F.softmax(attention_probs_sum_T, dim=-1)\n                    att_loss = -((attention_probs_sum_T * F.log_softmax(attention_probs_sum, dim=-1)).sum(dim=-1) * attention_mask.to(attention_probs_sum)).sum()/attention_mask.sum() * Conf.args.att_loss_weight\n                    #att_loss = F.mse_loss(attention_probs_sum, attention_probs_sum_T) * Conf.args.att_loss_weight\n                    total_loss += att_loss\n                #mle_loss = (F.mse_loss(start_logits,start_logits_T) + F.mse_loss(end_logits,end_logits_T))/2\n                #total_loss += mle_loss\n            if start_positions is not None:\n                # If we are on multi-GPU, split add a dimension - if not this is a no-op\n                if len(start_positions.size()) > 1:\n                    start_positions = start_positions.squeeze(-1)\n                    end_positions = end_positions.squeeze(-1)\n                # sometimes the start/end positions are outside our model inputs, we ignore these terms\n                ignored_index = start_logits.size(1)\n                start_positions.clamp_(0, ignored_index)\n                end_positions.clamp_(0, ignored_index)\n                is_noans = (start_positions == 0).float()\n                loss_fct_span  = CrossEntropyLoss(ignore_index=ignored_index,reduction='none')\n                #loss_fct_noans = BCEWithLogitsLoss()\n                start_loss = (loss_fct_span(start_logits, start_positions)*(1-is_noans)).mean()\n                end_loss   = (loss_fct_span(end_logits,   end_positions  )*(1-is_noans)).mean()\n                #cls_loss   = loss_fct_noans(cls_logits, is_noans)\n                total_loss += (start_loss + end_loss)/2 #+ cls_loss\n            return total_loss, att_loss\n        else:\n            if attention_probs_sum_layer is not None:\n                return start_logits, end_logits, cls_logits, attention_probs_sum\n            else:\n                return start_logits, end_logits, cls_logits\n\n\nclass BertForQASimple(nn.Module):\n    def __init__(self, config,args):\n        super(BertForQASimple, self).__init__()\n        self.output_encoded_layers   = (args.output_encoded_layers=='true')\n        self.output_attention_layers = (args.output_attention_layers=='true')\n        self.bert = BertModel(config, output_score=(args.output_att_score=='true'), output_sum=(args.output_att_sum=='true'))\n        self.qa_outputs  = nn.Linear(config.hidden_size, 2)\n        self.cls_outputs = nn.Linear(config.hidden_size, 1)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        initializer = initializer_builder(config.initializer_range)\n        self.apply(initializer)\n\n    def forward(self, input_ids, token_type_ids, attention_mask, doc_mask,\n                start_positions=None, end_positions=None):\n        sequence_output, pooled_output, attention_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                                                    output_all_encoded_layers=(self.output_encoded_layers),\n                                                                    output_all_attention_layers=(self.output_attention_layers))\n        #output_for_cls = self.dropout(pooled_output)\n        if self.output_encoded_layers is True:\n            span_logits = self.qa_outputs(sequence_output[-1])\n        else:\n            span_logits = self.qa_outputs(sequence_output)\n        #cls_logits  = self.cls_outputs(output_for_cls).squeeze(-1)  # output size: batch_size\n\n        doc_mask[:,0] = 0\n        span_logits = span_logits + (1.0 - doc_mask.unsqueeze(-1)) * -10000.0\n        start_logits, end_logits = span_logits.split(1, dim=-1)  # use_cls\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        if start_positions is not None:\n            total_loss = 0\n            # If we are on multi-GPU, split add a dimension - if not this is a no-op\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n            is_noans = (start_positions == 0).float()\n            loss_fct_span  = CrossEntropyLoss(ignore_index=ignored_index,reduction='none')\n            #loss_fct_noans = BCEWithLogitsLoss()\n            start_loss = (loss_fct_span(start_logits, start_positions)*(1-is_noans)).mean()\n            end_loss   = (loss_fct_span(end_logits,   end_positions  )*(1-is_noans)).mean()\n            #cls_loss   = loss_fct_noans(cls_logits, is_noans)\n            total_loss += (start_loss + end_loss)/2 #+ cls_loss\n            return start_logits, end_logits, sequence_output, attention_output, total_loss\n        else:\n            return start_logits, end_logits\n\n#def BertForQASimpleAdaptor(batch, model_outputs):\n#    return {'logits':      (model_outputs[0],model_outputs[1]),\n#            'hidden':      model_outputs[2],\n#            'attention':   model_outputs[3],\n#            'inputs_mask': batch[2]\n#           }\n\ndef BertForQASimpleAdaptor(batch, model_outputs, no_mask=False, no_logits=False):\n    dict_obj = {'hidden':      model_outputs[2], 'attention':   model_outputs[3]}\n    if no_mask is False:\n        dict_obj['inputs_mask'] = batch[2]\n    if no_logits is False:\n        dict_obj['logits'] = (model_outputs[0],model_outputs[1])\n    return dict_obj\n\ndef BertForQASimpleAdaptorNoMask(batch, model_outputs):\n    return {'logits':      (model_outputs[0],model_outputs[1]),\n            'hidden':      model_outputs[2],\n            'attention':   model_outputs[3]}\n\ndef BertForQASimpleAdaptorTraining(batch, model_outputs):\n    return {'losses':(model_outputs[4],)}\n"""
examples/cmrc2018_example/optimization.py,7,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch optimization for BERT model.""""""\n\nimport math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.nn.utils import clip_grad_norm_\n\ndef warmup_cosine(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 0.5 * (1.0 + torch.cos(math.pi * x))\n\ndef warmup_constant(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 1.0\n\ndef warmup_linear(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 1.0 - x\n\ndef warmup_linear_release(x, warmup=0.002,opt1=1, opt2=0, opt3=1):  #opt1:release_start  opt2:release_level\n    if x < warmup:\n        return x/warmup\n    if x < opt1:\n        return 1-(1-opt2)/opt1* x\n    return opt2\n\ndef warmup_step(x, warmup=0.002, opt1=1,opt2=0,opt3=1): # opt1:step1 opt2:step2 opt3:decay\n    if x < warmup:\n        return x/warmup\n    if x < opt1:\n        return 1.\n    if x < opt2:\n        return opt3;\n    return opt3*opt3\n\ndef slanted_triangular(x, warmup=0.1, opt1=1,opt2=0,opt3=1): # opt1:ratio\n    if x < warmup:\n        p = x/warmup\n    else:\n        p=1-(x-warmup)/(1-warmup)\n    return (1 + p*(opt1-1))/opt1\n\nSCHEDULES = {\n    \'warmup_cosine\':warmup_cosine,\n    \'warmup_constant\':warmup_constant,\n    \'warmup_linear\':warmup_linear,\n    \'warmup_linear_release\':warmup_linear_release,\n    \'warmup_step\':warmup_step,\n    \'slanted_triangular\':slanted_triangular\n}\n\n\nclass BERTAdam(Optimizer):\n    """"""Implements BERT version of Adam algorithm with weight decay fix (and no ).\n    Params:\n        lr: learning rate\n        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n        t_total: total number of training steps for the learning\n            rate schedule, -1  means constant learning rate. Default: -1\n        schedule: schedule to use for the warmup (see above). Default: \'warmup_linear\'\n        b1: Adams b1. Default: 0.9\n        b2: Adams b2. Default: 0.999\n        e: Adams epsilon. Default: 1e-6\n        weight_decay_rate: Weight decay. Default: 0.01\n        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n    """"""\n    def __init__(self, params, lr, warmup=-1, t_total=-1, schedule=\'warmup_linear_release\',\n                 s_opt1=1, s_opt2=0,s_opt3=1,\n                 b1=0.9, b2=0.999, e=1e-6, weight_decay_rate=0.01,\n                 max_grad_norm=1.0):\n        if not lr >= 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if schedule not in SCHEDULES:\n            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))\n        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n            raise ValueError(""Invalid warmup: {} - should be in [0.0, 1.0[ or -1"".format(warmup))\n        if not 0.0 <= b1 < 1.0:\n            raise ValueError(""Invalid b1 parameter: {} - should be in [0.0, 1.0["".format(b1))\n        if not 0.0 <= b2 < 1.0:\n            raise ValueError(""Invalid b2 parameter: {} - should be in [0.0, 1.0["".format(b2))\n        if not e >= 0.0:\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(e))\n\n        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n                    b1=b1, b2=b2, e=e, weight_decay_rate=weight_decay_rate,\n                    max_grad_norm=max_grad_norm,opt1=s_opt1, opt2=s_opt2,opt3=s_opt3)\n\n        super(BERTAdam, self).__init__(params, defaults)\n\n    def get_lr(self):\n        lr = []\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                if len(state) == 0:\n                    return [0]\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n                lr.append(lr_scheduled)\n        return lr\n    \'\'\'\n    def to(self, device):\n        """""" Move the optimizer state to a specified device""""""\n        for state in self.state.values():\n            state[\'exp_avg\'].to(device)\n            state[\'exp_avg_sq\'].to(device)\n\n    def initialize_step(self, initial_step):\n        """"""Initialize state with a defined step (but we don\'t have stored averaged).\n        Arguments:\n            initial_step (int): Initial step number.\n        """"""\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                # State initialization\n                state[\'step\'] = initial_step\n                # Exponential moving average of gradient values\n                state[\'exp_avg\'] = torch.zeros_like(p.data)\n                # Exponential moving average of squared gradient values\n                state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n    \'\'\'\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'next_m\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'next_v\'] = torch.zeros_like(p.data)\n\n                next_m, next_v = state[\'next_m\'], state[\'next_v\']\n                beta1, beta2 = group[\'b1\'], group[\'b2\']\n\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                next_m.mul_(beta1).add_(1 - beta1, grad)\n                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                update = next_m / (next_v.sqrt() + group[\'e\'])\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want ot decay the weights in a manner that doesn\'t interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                if group[\'weight_decay_rate\'] > 0.0:\n                    update += group[\'weight_decay_rate\'] * p.data\n\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'],group[\'opt1\'],group[\'opt2\'],group[\'opt3\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n\n                update_with_lr = lr_scheduled * update\n                p.data.add_(-update_with_lr)\n\n                state[\'step\'] += 1\n\n                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n                # bias_correction1 = 1 - beta1 ** state[\'step\']\n                # bias_correction2 = 1 - beta2 ** state[\'step\']\n\n        return loss\n'"
examples/cmrc2018_example/processing.py,0,"b'import tokenization\nfrom scipy.misc import logsumexp\nfrom tqdm import tqdm\nimport json\nimport logging\nimport collections\nimport six\nimport numpy as np\nimport math\nimport config\n#from tokenization import spacy_parser\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nhandler_stream = logging.StreamHandler()\nhandler_stream.setLevel(logging.INFO)\nformatter = logging.Formatter(fmt=\'%(asctime)s - %(levelname)s - %(name)s -  %(message)s\', datefmt=\'%Y/%m/%d %H:%M:%S\')\nhandler_stream.setFormatter(formatter)\nlogger.addHandler(handler_stream)\n\nclass SquadExample(object):\n    """"""A single training/test example for simple sequence classification.""""""\n\n    def __init__(self,\n                 qas_id,\n                 question_text,\n                 doc_tokens,\n                 orig_answer_text=None,\n                 start_position=None,\n                 end_position=None,\n                 is_impossible=None):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.doc_tokens = doc_tokens\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n        self.is_impossible = is_impossible\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = """"\n        s += ""qas_id: %s"" % (tokenization.printable_text(self.qas_id))\n        s += "", question_text: %s"" % (\n            tokenization.printable_text(self.question_text))\n        s += "", doc_tokens: [%s]"" % ("" "".join(self.doc_tokens))\n        if self.start_position:\n            s += "", start_position: %d"" % (self.start_position)\n        if self.start_position:\n            s += "", end_position: %d"" % (self.end_position)\n        if self.is_impossible:\n            s += "", is_impossible: %s"" % (self.is_impossible)\n        return s\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self,\n                 unique_id,\n                 example_index,\n                 doc_span_index,\n                 tokens,\n                 token_to_orig_map,\n                 token_is_max_context,\n                 input_ids,\n                 input_mask,\n                 doc_mask,input_span_mask,\n                 segment_ids,\n                 start_position=None,\n                 end_position=None,\n                 is_impossible=None):\n        self.unique_id = unique_id\n        self.example_index = example_index\n        self.doc_span_index = doc_span_index\n        self.tokens = tokens\n        self.token_to_orig_map = token_to_orig_map\n        self.token_is_max_context = token_is_max_context\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.doc_mask = doc_mask\n        self.input_span_mask = input_span_mask\n        self.segment_ids = segment_ids\n        self.start_position = start_position\n        self.end_position = end_position\n        self.is_impossible = is_impossible\n\ndef _is_chinese_char(cp):\n    """"""Checks whether CP is the codepoint of a CJK character.""""""\n    # This defines a ""chinese character"" as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n            (cp >= 0x3400 and cp <= 0x4DBF) or  #\n            (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n            (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n            (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n            (cp >= 0x2B820 and cp <= 0x2CEAF) or\n            (cp >= 0xF900 and cp <= 0xFAFF) or  #\n            (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n        return True\n    return False\n\ndef customize_tokenizer(text, do_lower_case=True):\n    temp_x = """"\n    text = tokenization.convert_to_unicode(text)\n    for c in text:\n        if _is_chinese_char(ord(c)) or tokenization._is_punctuation(c) or tokenization._is_whitespace(c) or tokenization._is_control(c):\n            temp_x += "" "" + c + "" ""\n        else:\n            temp_x += c\n    if do_lower_case:\n        temp_x = temp_x.lower()\n    return temp_x.split()\n\nclass ChineseFullTokenizer(object):\n    """"""Runs end-to-end tokenziation.""""""\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = tokenization.load_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.wordpiece_tokenizer = tokenization.WordpieceTokenizer(vocab=self.vocab)\n        self.do_lower_case = do_lower_case\n    def tokenize(self, text):\n        split_tokens = []\n        for token in customize_tokenizer(text, do_lower_case=self.do_lower_case):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return tokenization.convert_tokens_to_ids(self.vocab, tokens)\n\n\ndef read_squad_examples(input_file, is_training,do_lower_case):\n    """"""Read a SQuAD json file into a list of SquadExample.""""""\n    with open(input_file, ""r"",encoding=\'utf-8\') as reader:\n        input_data = json.load(reader)[""data""]\n\n    examples = []\n    for entry in input_data:\n        for paragraph in entry[""paragraphs""]:\n            paragraph_text = paragraph[""context""]\n            raw_doc_tokens = customize_tokenizer(paragraph_text, do_lower_case=do_lower_case)\n            doc_tokens = []\n            char_to_word_offset = []\n            prev_is_whitespace = True\n\n            k = 0\n            temp_word = """"\n            for c in paragraph_text:\n                if tokenization._is_whitespace(c):\n                    char_to_word_offset.append(k-1)\n                    continue\n                else:\n                    temp_word += c\n                    char_to_word_offset.append(k)\n                if do_lower_case is True:\n                    temp_word = temp_word.lower()\n                if temp_word == raw_doc_tokens[k]:\n                    doc_tokens.append(temp_word)\n                    temp_word = """"\n                    k += 1\n\n            try:\n                assert k==len(raw_doc_tokens)\n            except AssertionError:\n                print (len(raw_doc_tokens),len(doc_tokens))\n                for i in range(min(len(doc_tokens),len(raw_doc_tokens))):\n                    if raw_doc_tokens[i]!=doc_tokens[i]:\n                        print (raw_doc_tokens[i-3:i+3],doc_tokens[i-3:i+3])\n                        break\n                print (\'\'.join(doc_tokens[500:]))\n                print (""----"")\n                print (\'\'.join(raw_doc_tokens[500:]))\n                raise AssertionError\n\n            for qa in paragraph[""qas""]:\n                qas_id = qa[""id""]\n                question_text = qa[""question""]\n                is_impossible = False\n                start_position = None\n                end_position = None\n                orig_answer_text = None\n                if is_training:\n                    is_impossible = len(qa[\'answers\']) == 0\n                    if len(qa[""answers""]) > 1:\n                        pass\n                        #raise ValueError(\n                        #    ""For training, each question should have less than 1 answer."")\n                    if len(qa[\'answers\']) == 0:\n                        orig_answer_text = """"\n                        start_position = end_position = 0 # use_cls\n                    else:\n                        answer = qa[""answers""][0]\n                        orig_answer_text = answer[""text""]\n                        if orig_answer_text not in paragraph_text:\n                            logger.warning(""Could not find answer"")\n                            continue\n                        answer_offset = paragraph_text.index(orig_answer_text)\n                        #answer_offset = answer[""answer_start""]\n                        answer_length = len(orig_answer_text)\n                        start_position = char_to_word_offset[answer_offset]\n                        end_position = char_to_word_offset[answer_offset + answer_length - 1]\n                        # Only add answers where the text can be exactly recovered from the\n                        # document. If this CAN\'T happen it\'s likely due to weird Unicode\n                        # stuff so we will just skip the example.\n                        #\n                        # Note that this means for training mode, every example is NOT\n                        # guaranteed to be preserved.\n                        actual_text = """".join(doc_tokens[start_position:(end_position + 1)])\n                        cleaned_answer_text = """".join(tokenization.whitespace_tokenize(orig_answer_text))\n                        if do_lower_case:\n                            cleaned_answer_text = cleaned_answer_text.lower()\n                        if actual_text.find(cleaned_answer_text) == -1:\n                            logger.warning(""Could not find answer: \'%s\' vs. \'%s\'"",\n                                               actual_text, cleaned_answer_text)\n                            continue\n\n                example = SquadExample(\n                    qas_id=qas_id,\n                    question_text=question_text,\n                    doc_tokens=doc_tokens,\n                    orig_answer_text=orig_answer_text,\n                    start_position=start_position,\n                    end_position=end_position,\n                    is_impossible=is_impossible)\n                examples.append(example)\n    return examples\n\ndef convert_examples_to_features(examples, tokenizer, max_seq_length,\n                                 doc_stride, max_query_length, is_training):\n    """"""Loads a data file into a list of `InputBatch`s.""""""\n\n    unique_id = 1000000000\n\n    num_na = 0\n    num_all = 0\n\n    features = []\n    for (example_index, example) in enumerate(tqdm(examples,disable=None)):\n        query_tokens = tokenizer.tokenize(example.question_text)\n\n        if len(query_tokens) > max_query_length:\n            query_tokens = query_tokens[0:max_query_length]\n\n        tok_to_orig_index = []\n        orig_to_tok_index = []\n        all_doc_tokens = []\n\n        for (i, token) in enumerate(example.doc_tokens):\n            orig_to_tok_index.append(len(all_doc_tokens))\n            sub_tokens = tokenizer.tokenize(token)\n            for sub_token in sub_tokens:\n                tok_to_orig_index.append(i)\n                all_doc_tokens.append(sub_token)\n\n\n        tok_start_position = None\n        tok_end_position = None\n        if is_training:\n            if example.is_impossible:\n                tok_start_position = -1\n                tok_end_position = -1\n            else:\n                tok_start_position = orig_to_tok_index[example.start_position]\n                if example.end_position < len(example.doc_tokens) - 1:\n                    tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n                else:\n                    tok_end_position = len(all_doc_tokens) - 1\n                (tok_start_position, tok_end_position) = _improve_answer_span(\n                    all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n                    example.orig_answer_text)\n\n        # The -3 accounts for [CLS], [SEP] and [SEP]\n        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n        # We can have documents that are longer than the maximum sequence length.\n        # To deal with this we do a sliding window approach, where we take chunks\n        # of the up to our max length with a stride of `doc_stride`.\n        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n            ""DocSpan"", [""start"", ""length""])\n        doc_spans = []\n        start_offset = 0\n        while start_offset < len(all_doc_tokens):\n            length = len(all_doc_tokens) - start_offset\n            if length > max_tokens_for_doc:\n                length = max_tokens_for_doc\n            doc_spans.append(_DocSpan(start=start_offset, length=length))\n            if start_offset + length == len(all_doc_tokens):\n                break\n            start_offset += min(length, doc_stride)\n            #if not is_training:\n            #    break  # only want 1 span when prediction\n\n        #assert len(doc_spans) == 1 or is_training\n        for (doc_span_index, doc_span) in enumerate(doc_spans):\n            tokens = []\n            token_to_orig_map = {}\n            token_is_max_context = {}\n            segment_ids = []\n            doc_mask = []\n            input_span_mask = []\n\n            tokens.append(""[CLS]"")\n            segment_ids.append(0)\n            doc_mask.append(0)\n            input_span_mask.append(1)\n            for token in query_tokens:\n                tokens.append(token)\n                segment_ids.append(0)\n                doc_mask.append(0)\n                input_span_mask.append(0)\n            tokens.append(""[SEP]"")\n            segment_ids.append(0)\n            doc_mask.append(0)\n            input_span_mask.append(0)\n\n            for i in range(doc_span.length):\n                split_token_index = doc_span.start + i\n                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                                       split_token_index)\n                token_is_max_context[len(tokens)] = is_max_context\n\n                doc_token = all_doc_tokens[split_token_index]\n                tokens.append(doc_token)\n                segment_ids.append(0) # Zero for RoBERTa\n                doc_mask.append(1)\n                input_span_mask.append(1)\n            tokens.append(""[SEP]"")\n            segment_ids.append(0) # Zero for RoBERTa\n            doc_mask.append(0)\n            input_span_mask.append(0)\n\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n            # tokens are attended to.\n            input_mask = [1] * len(input_ids)\n\n            # Zero-pad up to the sequence length.\n            while len(input_ids) < max_seq_length:\n                #ent.append(0)\n                #pos.append(0)\n                input_ids.append(0)\n                input_mask.append(0)\n                segment_ids.append(0)\n                doc_mask.append(0)\n                input_span_mask.append(0)\n\n            assert len(input_ids) == max_seq_length\n            assert len(input_mask) == max_seq_length\n            assert len(segment_ids) == max_seq_length\n            assert len(doc_mask) == max_seq_length\n            assert len(input_span_mask) == max_seq_length\n\n            start_position = None\n            end_position = None\n            if is_training:\n                # For training, if our document chunk does not contain an annotation\n                # we throw it out, since there is nothing to predict.\n                doc_start = doc_span.start\n                doc_end = doc_span.start + doc_span.length - 1\n                if example.is_impossible:\n                    start_position = 0\n                    end_position = 0\n                    #continue #TODO if doc_span_index>0: continue\n                else:\n                    if tok_start_position < doc_start or tok_end_position > doc_end:\n                        start_position = 0\n                        end_position = 0\n                        continue #alternative TODO\n                    else:\n                        doc_offset = len(query_tokens) + 2\n                        start_position = tok_start_position - doc_start + doc_offset\n                        end_position = tok_end_position - doc_start + doc_offset\n                        # in case of exception\n                        if start_position >= max_seq_length-1:\n                            start_position = end_position = 0\n                            logger.warning(""exception"")\n                        elif end_position >= max_seq_length-1:\n                            start_position = end_position = 0 #max_seq_length - 2\n                            logger.warning(""exception"")\n\n            if example_index == 300:\n                logger.info(""*** Example ***"")\n                logger.info(""unique_id: %s"" % (unique_id))\n                logger.info(""example_index: %s"" % (example_index))\n                logger.info(""doc_span_index: %s"" % (doc_span_index))\n                logger.info(""tokens: %s"" % "" "".join([tokenization.printable_text(x) for x in tokens]))\n                logger.info(""token_to_orig_map: %s"" % "" "".join([""%d:%d"" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n                logger.info(""token_is_max_context: %s"" % "" "".join([""%d:%s"" % (x, y) for (x, y) in six.iteritems(token_is_max_context)]))\n                logger.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n                logger.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n                logger.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n                if is_training:\n                    answer_text = "" "".join(tokens[start_position:(end_position + 1)]) if not example.is_impossible else """"\n                    logger.info(""start_position: %d"" % (start_position))\n                    logger.info(""is_impossible: %s"" % (example.is_impossible))\n                    logger.info(""end_position: %d"" % (end_position))\n                    logger.info(\n                        ""answer: %s"" % (tokenization.printable_text(answer_text)))\n\n            features.append(\n                InputFeatures(\n                    unique_id=unique_id,\n                    example_index=example_index,\n                    doc_span_index=doc_span_index,\n                    tokens=tokens,\n                    token_to_orig_map=token_to_orig_map,\n                    token_is_max_context=token_is_max_context, #ent = ent, pos = pos,\n                    input_ids=input_ids,\n                    input_mask=input_mask,\n                    doc_mask = doc_mask,\n                    input_span_mask = input_span_mask,\n                    segment_ids=segment_ids,\n                    start_position=start_position,\n                    end_position=end_position,\n                    is_impossible= start_position==0))  # use_cls\n            unique_id += 1\n\n            num_all += 1\n            num_na += int(start_position==0)\n    logger.info(f""Num all: {num_all}"")\n    logger.info(f""Num na : {num_na}"")\n    return features\n\n\ndef _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text):\n    """"""Returns tokenized answer spans that better match the annotated answer.""""""\n\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a ""better match"". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be ""(1895-1943)."". However\n    # after tokenization, our tokens will be ""( 1895 - 1943 ) ."". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose ""Japan"" as a character sub-span of\n    # the word ""Japanese"". Since our WordPiece tokenizer does not split\n    # ""Japanese"", we just use ""Japanese"" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = "" "".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = "" "".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n\ndef _check_is_max_context(doc_spans, cur_span_index, position):\n    """"""Check if this is the \'max context\' doc span for the token.""""""\n\n    # Because of the sliding window approach taken to scoring documents, a single\n    # token can appear in multiple documents. E.g.\n    #  Doc: the man went to the store and bought a gallon of milk\n    #  Span A: the man went to the\n    #  Span B: to the store and bought\n    #  Span C: and bought a gallon of\n    #  ...\n    #\n    # Now the word \'bought\' will have two scores from spans B and C. We only\n    # want to consider the score with ""maximum context"", which we define as\n    # the *minimum* of its left and right context (the *sum* of left and\n    # right context will always be the same, of course).\n    #\n    # In the example the maximum context for \'bought\' would be span C since\n    # it has 1 left context and 3 right context, while span B has 4 left context\n    # and 0 right context.\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\ndef get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n    """"""Project the tokenized prediction back to the original text.""""""\n\n    # When we created the data, we kept track of the alignment between original\n    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n    # now `orig_text` contains the span of our original text corresponding to the\n    # span that we predicted.\n    #\n    # However, `orig_text` may contain extra characters that we don\'t want in\n    # our prediction.\n    #\n    # For example, let\'s say:\n    #   pred_text = steve smith\n    #   orig_text = Steve Smith\'s\n    #\n    # We don\'t want to return `orig_text` because it contains the extra ""\'s"".\n    #\n    # We don\'t want to return `pred_text` because it\'s already been normalized\n    # (the SQuAD eval script also does punctuation stripping/lower casing but\n    # our tokenizer does additional normalization like stripping accent\n    # characters).\n    #\n    # What we really want to return is ""Steve Smith"".\n    #\n    # Therefore, we have to apply a semi-complicated alignment heruistic between\n    # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n    # can fail in certain cases in which case we just return `orig_text`.\n\n    def _strip_spaces(text):\n        ns_chars = []\n        ns_to_s_map = collections.OrderedDict()\n        for (i, c) in enumerate(text):\n            if c == "" "":\n                continue\n            ns_to_s_map[len(ns_chars)] = i\n            ns_chars.append(c)\n        ns_text = """".join(ns_chars)\n        return (ns_text, ns_to_s_map)\n\n    # We first tokenize `orig_text`, strip whitespace from the result\n    # and `pred_text`, and check if they are the same length. If they are\n    # NOT the same length, the heuristic has failed. If they are the same\n    # length, we assume the characters are one-to-one aligned.\n    tokenizer = tokenization.BasicTokenizer(do_lower_case=do_lower_case)\n\n    tok_text = "" "".join(tokenizer.tokenize(orig_text))\n\n    start_position = tok_text.find(pred_text)\n    if start_position == -1:\n        if verbose_logging:\n            logger.info(\n                ""Unable to find text: \'%s\' in \'%s\'"" % (pred_text, orig_text))\n        return orig_text\n    end_position = start_position + len(pred_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n    if len(orig_ns_text) != len(tok_ns_text):\n        if verbose_logging:\n            logger.info(""Length not equal after stripping spaces: \'%s\' vs \'%s\'"",\n                            orig_ns_text, tok_ns_text)\n        return orig_text\n\n    # We then project the characters in `pred_text` back to `orig_text` using\n    # the character-to-character alignment.\n    tok_s_to_ns_map = {}\n    for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n        tok_s_to_ns_map[tok_index] = i\n\n    orig_start_position = None\n    if start_position in tok_s_to_ns_map:\n        ns_start_position = tok_s_to_ns_map[start_position]\n        if ns_start_position in orig_ns_to_s_map:\n            orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n    if orig_start_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map start position"")\n        return orig_text\n\n    orig_end_position = None\n    if end_position in tok_s_to_ns_map:\n        ns_end_position = tok_s_to_ns_map[end_position]\n        if ns_end_position in orig_ns_to_s_map:\n            orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n    if orig_end_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map end position"")\n        return orig_text\n\n    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n    return output_text\n\n\ndef _get_best_indexes(logits, n_best_size, offset=0):\n    """"""Get the n-best logits from a list.""""""\n    sorted_indices = np.argsort(logits)[::-1] + offset\n    return list(sorted_indices[:n_best_size])\n\n\ndef softmax2d(scores):\n    if not scores:\n      return []\n    z = np.array(scores)\n    assert len(z.shape) == 2\n    s = np.max(z, axis=1)\n    s = s[:, np.newaxis]  # necessary step to do broadcasting\n    e_x = np.exp(z - s)\n    div = np.sum(e_x, axis=1)\n    div = div[:, np.newaxis]  # dito\n    return e_x / div\n\ndef softmax1d(scores):\n  if not scores:\n    return []\n  z = np.array(scores)\n  e_x = np.exp(z - np.max(z))\n  div = np.sum(e_x)\n  return e_x / div\n\ndef _compute_softmax(scores):\n    """"""Compute softmax probability over raw logits.""""""\n    if not scores:\n        return []\n\n    max_score = None\n    for score in scores:\n        if max_score is None or score > max_score:\n            max_score = score\n\n    exp_scores = []\n    total_sum = 0.0\n    for score in scores:\n        x = math.exp(score - max_score)\n        exp_scores.append(x)\n        total_sum += x\n\n    probs = []\n    for score in exp_scores:\n        probs.append(score / total_sum)\n    return probs\n\ndef log_softmax1d(scores):\n    if not scores:\n        return []\n    x = np.array(scores)\n    z = logsumexp(x)\n    return x-z\n\ndef log_sigmoid(score):\n    return math.log(1/(1+math.exp(-score)))\n\nRawResult = collections.namedtuple(""RawResult"",\n                                   [""unique_id"", ""start_logits"", ""end_logits"", ""cls_logits""])\n\n\ndef write_predictions_google(all_examples, all_features, all_results, n_best_size,\n                      max_answer_length, do_lower_case, output_prediction_file,\n                      output_nbest_file, output_null_log_odds_file):\n    """"""Write final predictions to the json file.""""""\n    logger.info(""Writing predictions to: %s"" % (output_prediction_file))\n    #logger.info(""Writing nbest to: %s"" % (output_nbest_file))\n\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""PrelimPrediction"",\n        [""feature_index"", ""start_index"", ""end_index"", ""start_logit"", ""end_logit""])\n\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict()\n\n    for (example_index, example) in enumerate(all_examples):\n        features = example_index_to_features[example_index]\n\n        prelim_predictions = []\n        # keep track of the minimum score of null start+end of position 0\n        score_null = 1000000  # large and positive\n        min_null_feature_index = 0\n        null_ls = 0\n        #null_end_logit = 0\n        for (feature_index, feature) in enumerate(features):\n            result = unique_id_to_result[feature.unique_id]\n            result_start_ls = log_softmax1d(result.start_logits)\n            result_end_ls   = log_softmax1d(result.end_logits)\n            start_indexes = _get_best_indexes(result_start_ls, n_best_size)\n            end_indexes   = _get_best_indexes(result_end_ls, n_best_size)\n            # if we could have irrelevant answers, get the min score of irrelevant\n\n            #feature_null_score =   log_sigmoid(result.cls_logits) #result.start_logits[0] + result.end_logits[0]\n            #feature_HasAns_score = log_sigmoid(-result.cls_logits)\n            span_start_ls = result_start_ls #+ feature_HasAns_score\n            span_end_ls   = result_end_ls #+ feature_HasAns_score\n\n            #if feature_null_score < score_null:\n            #    score_null = feature_null_score\n            #    min_null_feature_index = feature_index\n            #    null_ls = feature_null_score #result.start_logits[0]\n            #    #null_end_logit = result.end_logits[0]\n\n\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= len(feature.tokens):\n                        continue\n                    if end_index >= len(feature.tokens):\n                        continue\n                    if start_index not in feature.token_to_orig_map:\n                        continue\n                    if end_index not in feature.token_to_orig_map:\n                        continue\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n                    prelim_predictions.append(\n                        _PrelimPrediction(\n                            feature_index=feature_index,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_logit=span_start_ls[start_index],\n                            end_logit=span_end_ls[end_index]))\n\n\n        #if FLAGS.version_2_with_negative:\n        #prelim_predictions.append(\n        #    _PrelimPrediction(\n        #        feature_index=min_null_feature_index,\n        #        start_index=0,\n        #        end_index=0,\n        #        start_logit=null_ls/2,\n        #        end_logit=null_ls/2))\n\n\n        prelim_predictions = sorted(\n            prelim_predictions,\n            key=lambda x: (x.start_logit + x.end_logit),\n            reverse=True)\n\n        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            ""NbestPrediction"", [""text"", ""start_logit"", ""end_logit""])\n\n        seen_predictions = {}\n        nbest = []\n        for pred in prelim_predictions:\n            if len(nbest) >= n_best_size:\n                break\n            feature = features[pred.feature_index]\n            if pred.start_index > 0:  # this is a non-null prediction\n                tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n                orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n                tok_text = "" "".join(tok_tokens)\n\n                # De-tokenize WordPieces that have been split off.\n                tok_text = tok_text.replace("" ##"", """")\n                tok_text = tok_text.replace(""##"", """")\n\n                # Clean whitespace\n                tok_text = tok_text.strip()\n                tok_text = "" "".join(tok_text.split())\n                orig_text = "" "".join(orig_tokens)\n\n                final_text = get_final_text(tok_text, orig_text, do_lower_case)\n                final_text = final_text.replace(\' \',\'\')\n                if final_text in seen_predictions:\n                    continue\n\n                seen_predictions[final_text] = True\n            else:\n                final_text = """"\n                seen_predictions[final_text] = True\n\n            nbest.append(\n                _NbestPrediction(\n                    text=final_text,\n                    start_logit=pred.start_logit,\n                    end_logit=pred.end_logit))\n\n        # if we didn\'t inlude the empty option in the n-best, inlcude it\n\n        #if FLAGS.version_2_with_negative:\n        #if """" not in seen_predictions:\n        #    nbest.append(\n        #        _NbestPrediction(\n        #            text="""",\n        #            start_logit=null_ls/2,\n        #            end_logit=null_ls/2))\n\n\n        # In very rare edge cases we could have no valid predictions. So we\n        # just create a nonce prediction in this case to avoid failure.\n        if not nbest:\n            nbest.append(\n                _NbestPrediction(text="""", start_logit=0.0, end_logit=0.0))\n\n        assert len(nbest) >= 1\n\n        total_scores = []\n        best_non_null_entry = None\n        #index_best_non_null_entry = None\n        for (i,entry) in enumerate(nbest):\n            total_scores.append(entry.start_logit + entry.end_logit)\n            if not best_non_null_entry:\n                if entry.text:\n                    #index_best_non_null_entry = i\n                    best_non_null_entry = entry\n\n        probs = np.exp(total_scores)\n\n        nbest_json = []\n        for (i, entry) in enumerate(nbest):\n            output = collections.OrderedDict()\n            output[""text""] = entry.text\n            output[""probability""] = probs[i]\n            output[""start_logit""] = entry.start_logit\n            output[""end_logit""] = entry.end_logit\n            nbest_json.append(output)\n\n        assert len(nbest_json) >= 1\n\n\n        #if not FLAGS.version_2_with_negative:\n        #    all_predictions[example.qas_id] = nbest_json[0][""text""]\n        #else:\n            # predict """" iff the null score - the score of best non-null > threshold\n\n        #if best_non_null_entry is None:\n        #    score_diff = 999999\n        #else:\n        #    score_diff = np.exp(score_null) - np.exp((best_non_null_entry.start_logit + best_non_null_entry.end_logit))\n        #scores_diff_json[example.qas_id] = float(score_diff)\n        ##scores_diff_json[example.qas_id] = float(np.exp(score_null))\n        #if score_diff > config.args.null_score_diff_threshold:\n        #    all_predictions[example.qas_id] = """"\n        #else:\n        #    all_predictions[example.qas_id] = best_non_null_entry.text\n\n        all_predictions[example.qas_id] = nbest_json[0][""text""]\n        all_nbest_json[example.qas_id] = nbest_json\n\n    with open(output_prediction_file, ""w"",encoding=\'utf-8\') as writer:\n        writer.write(json.dumps(all_predictions, indent=4,ensure_ascii=False) + ""\\n"")\n\n    #with open(output_nbest_file, ""w"") as writer:\n    #    writer.write(json.dumps(all_nbest_json, indent=4) + ""\\n"")\n\n    #with open(output_null_log_odds_file,""w"") as writer:\n    #    writer.write(json.dumps(scores_diff_json, indent=4) + ""\\n"")\n\n    return all_predictions, scores_diff_json\n'"
examples/cmrc2018_example/tokenization.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport unicodedata\nimport six\n\'\'\'\nimport spacy\nfrom spacy.tokens import Doc\nfrom typing import List\n\nclass SpacyParser(object):\n    def __init__(self,do_lower_case=True):\n        self.nlp =spacy.load(\'en\')\n        ent_labels = [\'\',\'PERSON\', \'NORP\', \'FAC\', \'ORG\', \'GPE\', \'LOC\', \'PRODUCT\', \'EVENT\', \'WORK_OF_ART\',\n                      \'LAW\', \'LANGUAGE\', \'DATE\', \'TIME\', \'PERCENT\', \'MONEY\', \'QUANTITY\', \'ORDINAL\', \'CARDINAL\']\n        self.ent2id =  {v:k for k,v in enumerate(ent_labels)}\n        self.id2ent =  {k:v for k,v in enumerate(ent_labels)}\n        pos_labels = [\'<PAD>\',\'X\',\'ADJ\',\'ADP\',\'ADV\',\'AUX\',\'CONJ\',\'CCONJ\',\'DET\',\'INTJ\',\'NOUN\',\'NUM\',\'PART\',\'PRON\',\'PROPN\',\n                      \'PUNCT\',\'SCONJ\',\'SYM\',\'VERB\',\'SPACE\']\n        self.pos2id = {v:k for k,v in enumerate(pos_labels)}\n        self.id2pos = {k:v for k,v in enumerate(pos_labels)}\n\n    def parse(self,words):\n        ent_tokens = [] # List of int\n        pos_tokens = [] # List of int\n        spaces = [True] * len(words)\n        spaces[-1] =  False\n        doc = Doc(self.nlp.vocab, words=words, spaces=spaces)\n        for name,proc in self.nlp.pipeline:\n            doc = proc(doc)\n        for token in doc:\n            ent_tokens.append(self.ent2id[token.ent_type_])\n            pos_tokens.append(self.pos2id[token.pos_])\n        return ent_tokens, pos_tokens\n\nspacy_parser = SpacyParser()\n\'\'\'\n\n\n\ndef convert_to_unicode(text):\n    """"""Converts `text` to Unicode (if it\'s not already), assuming utf-8 input.""""""\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(""utf-8"", ""ignore"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text.decode(""utf-8"", ""ignore"")\n        elif isinstance(text, unicode):\n            return text\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    else:\n        raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef printable_text(text):\n    """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""\n\n    # These functions want `str` for both Python2 and Python3, but in one case\n    # it\'s a Unicode string and in the other it\'s a byte string.\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(""utf-8"", ""ignore"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, unicode):\n            return text.encode(""utf-8"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    else:\n        raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef load_vocab(vocab_file):\n    """"""Loads a vocabulary file into a dictionary.""""""\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, ""r"",encoding=\'utf-8\') as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n    """"""Converts a sequence of tokens into ids using the vocab.""""""\n    ids = []\n    for token in tokens:\n        ids.append(vocab[token])\n    return ids\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a peice of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass FullTokenizer(object):\n    """"""Runs end-to-end tokenziation.""""""\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = load_vocab(vocab_file)\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text, do_parse=False):\n        split_tokens = []\n        split_ent = []\n        split_pos = []\n        if False: #do_parse:\n            tokens = self.basic_tokenizer.tokenize(text)\n            ent, pos = spacy_parser.parse(tokens)\n            for i,token in enumerate(tokens):\n                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                    split_tokens.append(sub_token)\n                    split_ent.append(ent[i])\n                    split_pos.append(pos[i])\n\n            return split_tokens, split_ent, split_pos\n\n        else:\n            for token in self.basic_tokenizer.tokenize(text):\n                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                    split_tokens.append(sub_token)\n\n            return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_tokens_to_ids(self.vocab, tokens)\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self, do_lower_case=True):\n        """"""Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        """"""\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text.""""""\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text):\n        """"""Splits punctuation on a piece of text.""""""\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
examples/cmrc2018_example/train_eval.py,11,"b'import torch\nfrom collections import OrderedDict\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nimport os, logging\nfrom tqdm import tqdm, trange\nfrom processing import RawResult, write_predictions_google\nfrom cmrc2018_evaluate import evaluate\nimport json\nimport numpy as np\n\nlogger = logging.getLogger(""Train_eval"")\nlogger.setLevel(logging.INFO)\nhandler_stream = logging.StreamHandler()\nhandler_stream.setLevel(logging.INFO)\nformatter = logging.Formatter(fmt=\'%(asctime)s - %(levelname)s - %(name)s -  %(message)s\', datefmt=\'%Y/%m/%d %H:%M:%S\')\nhandler_stream.setFormatter(formatter)\nlogger.addHandler(handler_stream)\n\n\ndef predict(model, eval_examples, eval_features, step, args):\n    device = args.device\n    logger.info(""Predicting..."")\n    logger.info(""***** Running predictions *****"")\n    logger.info(""  Num orig examples = %d"", len(eval_examples))\n    logger.info(""  Num split examples = %d"", len(eval_features))\n    logger.info(""  Batch size = %d"", args.predict_batch_size)\n\n    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n    all_doc_mask = torch.tensor([f.doc_mask for f in eval_features], dtype=torch.float)\n    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n\n    eval_data = TensorDataset(all_input_ids, all_input_mask, all_doc_mask, all_segment_ids, all_example_index)\n    if args.local_rank == -1:\n        eval_sampler = SequentialSampler(eval_data)\n    else:\n        eval_sampler = DistributedSampler(eval_data)\n    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.predict_batch_size)\n\n    model.eval()\n    all_results = []\n    logger.info(""Start evaluating"")\n\n    if os.path.exists(\'all_results.tmp\') and not args.do_train:\n        pass  # all_results = pickle.load(open(\'all_results.tmp\', \'rb\'))\n    else:\n        for input_ids, input_mask, doc_mask, segment_ids, example_indices \\\n                in tqdm(eval_dataloader, desc=""Evaluating"", disable=None):\n            if len(all_results) % 1000 == 0:\n                logger.info(""Processing example: %d"" % (len(all_results)))\n            input_ids = input_ids.to(device)\n            input_mask = input_mask.to(device)\n            doc_mask = doc_mask.to(device)\n            segment_ids = segment_ids.to(device)\n            with torch.no_grad():\n                batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask,\n                                                                               doc_mask)\n            for i, example_index in enumerate(example_indices):\n                start_logits = batch_start_logits[i].detach().cpu().tolist()\n                end_logits = batch_end_logits[i].detach().cpu().tolist()\n                cls_logits = 0 # Not used batch_cls_logits[i].detach().cpu().tolist()\n                eval_feature = eval_features[example_index.item()]\n                unique_id = int(eval_feature.unique_id)\n                all_results.append(RawResult(unique_id=unique_id,\n                                             start_logits=start_logits,\n                                             end_logits=end_logits,\n                                             cls_logits=cls_logits))\n        if not args.do_train:\n            pass\n            # try:\n            #    pickle.dump(all_results, open(\'all_results.tmp\', \'wb\'))\n            # except:\n            #    print(""can\'t save all_results.tmp"")\n\n    logger.info(""Write predictions..."")\n    output_prediction_file = os.path.join(args.output_dir, ""predictions_%d.json"" % step)\n\n    all_predictions, scores_diff_json = \\\n        write_predictions_google(eval_examples, eval_features, all_results,\n                                 args.n_best_size, args.max_answer_length,\n                                 args.do_lower_case, output_prediction_file,\n                                 output_nbest_file=None, output_null_log_odds_file=None)\n    model.train()\n    if args.do_eval:\n        eval_data = json.load(open(args.predict_file, \'r\', encoding=\'utf-8\'))\n        F1, EM, TOTAL, SKIP = evaluate(eval_data, all_predictions)  # ,scores_diff_json, na_prob_thresh=0)\n        AVG = (EM+F1)*0.5\n        output_result = OrderedDict()\n        output_result[\'AVERAGE\'] = \'%.3f\' % AVG\n        output_result[\'F1\'] = \'%.3f\' % F1\n        output_result[\'EM\'] = \'%.3f\' % EM\n        output_result[\'TOTAL\'] = TOTAL\n        output_result[\'SKIP\'] = SKIP\n        logger.info(""***** Eval results {} *****"".format(step))\n        logger.info(json.dumps(output_result)+\'\\n\')\n\n        output_eval_file = os.path.join(args.output_dir, ""eval_results.txt"")\n        with open(output_eval_file, ""a"") as writer:\n            writer.write(f""Step: {step} {json.dumps(output_result)}\\n"")\n\n    #output_nbest_file = os.path.join(args.output_dir, ""nbest_predictions_%d.json"" % step)\n    #output_null_odds_file = os.path.join(args.output_dir, ""null_odds_%d.json"" % (step))\n\n    # torch.save(state_dict, os.path.join(args.output_dir,""EM{:.4f}_F{:.4f}_gs{}.pkl"".format(em,f1,global_step)))\n    # print (""saving at finish"")\n    # coreModel = model.module if \'DataParallel\' in model.__class__.__name__ else model\n    # torch.save(coreModel.state_dict(),os.path.join(args.output_dir,""%d.pkl"" % (global_step)))\n    # predict(global_step)\n'"
examples/cmrc2018_example/utils.py,0,"b'import pickle\nimport os\nimport config\nimport logging\nlogger = logging.getLogger(""utils"")\nlogger.setLevel(logging.INFO)\n\ndef read_and_convert(fn,is_training,read_fn,convert_fn,do_lower_case):\n    data_dirname, data_basename = os.path.split(fn)\n    cased = \'\' if do_lower_case else \'cased\'\n    if config.args.max_seq_length != 416:\n        data_pklname = data_basename + \'%s%d_l%d_cHA.t%s.pkl\' % (cased,config.args.doc_stride,config.args.max_seq_length,config.args.tag)\n    else:\n        data_pklname = data_basename + \'%s%d_cHA.t%s.pkl\' % (cased,config.args.doc_stride,config.args.tag)\n    full_pklname = os.path.join(data_dirname,data_pklname)\n    if os.path.exists(full_pklname):\n        logger.info(""Loading dataset %s "" % data_pklname)\n        with open(full_pklname,\'rb\') as f:\n            examples,features = pickle.load(f)\n    else:\n        logger.info(""Building dataset %s "" % data_pklname)\n        examples = read_fn(input_file=fn,is_training=is_training,do_lower_case=do_lower_case)\n        logger.info(f""Size: {len(examples)}"")\n        features = convert_fn(examples=examples,is_training=is_training)\n        try:\n            with open(full_pklname,\'wb\') as f:\n                pickle.dump((examples,features),f)\n        except:\n            logger.info(""Can\'t save train data file."")\n    return examples,features\n\n\ndef divide_parameters(named_parameters,lr=None):\n    no_decay = [\'bias\', \'LayerNorm.bias\',\'LayerNorm.weight\']\n    decay_parameters_names = list(zip(*[(p,n) for n,p in named_parameters if not any((di in n) for di in no_decay)]))\n    no_decay_parameters_names = list(zip(*[(p,n) for n,p in named_parameters if any((di in n) for di in no_decay)]))\n    param_group = []\n    if len(decay_parameters_names)>0:\n        decay_parameters, decay_names = decay_parameters_names\n        #print (""decay:"",decay_names)\n        if lr is not None:\n            decay_group = {\'params\':decay_parameters,   \'weight_decay_rate\': config.args.weight_decay_rate, \'lr\':lr}\n        else:\n            decay_group = {\'params\': decay_parameters, \'weight_decay_rate\': config.args.weight_decay_rate}\n        param_group.append(decay_group)\n\n    if len(no_decay_parameters_names)>0:\n        no_decay_parameters, no_decay_names = no_decay_parameters_names\n        #print (""no decay:"", no_decay_names)\n        if lr is not None:\n            no_decay_group = {\'params\': no_decay_parameters, \'weight_decay_rate\': 0.0, \'lr\': lr}\n        else:\n            no_decay_group = {\'params\': no_decay_parameters, \'weight_decay_rate\': 0.0}\n        param_group.append(no_decay_group)\n\n    assert len(param_group)>0\n    return param_group\n'"
examples/conll2003_example/run_ner.py,30,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Fine-tuning the library models for named entity recognition on CoNLL-2003 (Bert or Roberta). """"""\n\n# from __future__ import absolute_import, division, print_function\n\nimport argparse\nimport glob\nimport logging\nimport os\nimport random\n\nimport numpy as np\nimport torch\nfrom seqeval.metrics import precision_score, recall_score, f1_score\nfrom tensorboardX import SummaryWriter\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm, trange\nfrom utils_ner import convert_examples_to_features, get_labels, read_examples_from_file\n\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import WEIGHTS_NAME, BertConfig, BertForTokenClassification, BertTokenizer\nfrom transformers import RobertaConfig, RobertaForTokenClassification, RobertaTokenizer\nfrom transformers import DistilBertConfig, DistilBertForTokenClassification, DistilBertTokenizer\nfrom transformers import CamembertConfig, CamembertForTokenClassification, CamembertTokenizer\n\nlogger = logging.getLogger(__name__)\n\nALL_MODELS = sum(\n    (tuple(conf.pretrained_config_archive_map.keys()) for conf in (BertConfig, RobertaConfig, DistilBertConfig)),\n    ())\n\nMODEL_CLASSES = {\n    ""bert"": (BertConfig, BertForTokenClassification, BertTokenizer),\n    ""roberta"": (RobertaConfig, RobertaForTokenClassification, RobertaTokenizer),\n    ""distilbert"": (DistilBertConfig, DistilBertForTokenClassification, DistilBertTokenizer),\n    ""camembert"": (CamembertConfig, CamembertForTokenClassification, CamembertTokenizer),\n}\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n\ndef train(args, train_dataset, model, tokenizer, labels, pad_token_label_id,predict_callback):\n    """""" Train the model """"""\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [""bias"", ""LayerNorm.weight""]\n    optimizer_grouped_parameters = [\n        {""params"": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n         ""weight_decay"": args.weight_decay},\n        {""params"": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], ""weight_decay"": 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n                                                          output_device=args.local_rank,\n                                                          find_unused_parameters=True)\n\n    # Train!\n    logger.info(""***** Running training *****"")\n    logger.info(""  Num examples = %d"", len(train_dataset))\n    logger.info(""  Num Epochs = %d"", args.num_train_epochs)\n    logger.info(""  Instantaneous batch size per GPU = %d"", args.per_gpu_train_batch_size)\n    logger.info(""  Total train batch size (w. parallel, distributed & accumulation) = %d"",\n                args.train_batch_size * args.gradient_accumulation_steps * (\n                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info(""  Gradient Accumulation steps = %d"", args.gradient_accumulation_steps)\n    logger.info(""  Total optimization steps = %d"", t_total)\n\n    global_step = 0\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    train_iterator = trange(int(args.num_train_epochs), desc=""Epoch"", disable=args.local_rank not in [-1, 0])\n    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=""Iteration"", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            inputs = {""input_ids"": batch[0],\n                      ""attention_mask"": batch[1],\n                      ""labels"": batch[3]}\n            if args.model_type != ""distilbert"":\n                inputs[""token_type_ids""] = batch[2] if args.model_type in [""bert"", ""xlnet""] else None  # XLM and RoBERTa don""t use segment_ids\n\n            outputs = model(**inputs)\n            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                scheduler.step()  # Update learning rate schedule\n                optimizer.step()\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n                        results, _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=""dev"")\n                    logging_loss = tr_loss\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, ""checkpoint-{}"".format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, ""module"") else model  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, ""training_args.bin""))\n                    logger.info(""Saving model checkpoint to %s"", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        predict_callback(model,global_step)\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    return global_step, tr_loss / global_step\n\n\ndef evaluate(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=""""):\n    eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode)\n\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Note that DistributedSampler samples randomly\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.info(""***** Running evaluation %s *****"", prefix)\n    logger.info(""  Num examples = %d"", len(eval_dataset))\n    logger.info(""  Batch size = %d"", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    preds = None\n    out_label_ids = None\n    model.eval()\n    for batch in tqdm(eval_dataloader, desc=""Evaluating""):\n        batch = tuple(t.to(args.device) for t in batch)\n\n        with torch.no_grad():\n            inputs = {""input_ids"": batch[0],\n                      ""attention_mask"": batch[1],\n                      ""labels"": batch[3]}\n            if args.model_type != ""distilbert"":\n                inputs[""token_type_ids""] = batch[2] if args.model_type in [""bert"", ""xlnet""] else None  # XLM and RoBERTa don""t use segment_ids\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n\n            if args.n_gpu > 1:\n                tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n\n            eval_loss += tmp_eval_loss.item()\n        nb_eval_steps += 1\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[""labels""].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(out_label_ids, inputs[""labels""].detach().cpu().numpy(), axis=0)\n\n    eval_loss = eval_loss / nb_eval_steps\n    preds = np.argmax(preds, axis=2)\n\n    label_map = {i: label for i, label in enumerate(labels)}\n\n    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n\n    for i in range(out_label_ids.shape[0]):\n        for j in range(out_label_ids.shape[1]):\n            if out_label_ids[i, j] != pad_token_label_id:\n                out_label_list[i].append(label_map[out_label_ids[i][j]])\n                preds_list[i].append(label_map[preds[i][j]])\n\n    results = {\n        ""loss"": eval_loss,\n        ""precision"": precision_score(out_label_list, preds_list),\n        ""recall"": recall_score(out_label_list, preds_list),\n        ""f1"": f1_score(out_label_list, preds_list)\n    }\n\n    logger.info(""***** Eval results %s *****"", prefix)\n    for key in sorted(results.keys()):\n        logger.info(""  %s = %s"", key, str(results[key]))\n\n    return results, preds_list\n\n\ndef load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, ""cached_{}_{}_{}"".format(mode,\n        list(filter(None, args.model_name_or_path.split(""/""))).pop(),\n        str(args.max_seq_length)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(""Loading features from cached file %s"", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(""Creating features from dataset file at %s"", args.data_dir)\n        examples = read_examples_from_file(args.data_dir, mode)\n        features = convert_examples_to_features(examples, labels, args.max_seq_length, tokenizer,\n                                                cls_token_at_end=bool(args.model_type in [""xlnet""]),\n                                                # xlnet has a cls token at the end\n                                                cls_token=tokenizer.cls_token,\n                                                cls_token_segment_id=2 if args.model_type in [""xlnet""] else 0,\n                                                sep_token=tokenizer.sep_token,\n                                                sep_token_extra=bool(args.model_type in [""roberta""]),\n                                                # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n                                                pad_on_left=bool(args.model_type in [""xlnet""]),\n                                                # pad on the left for xlnet\n                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                                                pad_token_segment_id=4 if args.model_type in [""xlnet""] else 0,\n                                                pad_token_label_id=pad_token_label_id\n                                                )\n        if args.local_rank in [-1, 0]:\n            logger.info(""Saving features into cached file %s"", cached_features_file)\n            torch.save(features, cached_features_file)\n\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n\n    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n    return dataset\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(""--data_dir"", default=None, type=str, required=True,\n                        help=""The input data dir. Should contain the training files for the CoNLL-2003 NER task."")\n    parser.add_argument(""--model_type"", default=None, type=str, required=True,\n                        help=""Model type selected in the list: "" + "", "".join(MODEL_CLASSES.keys()))\n    parser.add_argument(""--model_name_or_path"", default=None, type=str, required=True,\n                        help=""Path to pre-trained model or shortcut name selected in the list: "" + "", "".join(ALL_MODELS))\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True,\n                        help=""The output directory where the model predictions and checkpoints will be written."")\n\n    ## Other parameters\n    parser.add_argument(""--labels"", default="""", type=str,\n                        help=""Path to a file containing all labels. If not specified, CoNLL-2003 labels are used."")\n    parser.add_argument(""--config_name"", default="""", type=str,\n                        help=""Pretrained config name or path if not the same as model_name"")\n    parser.add_argument(""--tokenizer_name"", default="""", type=str,\n                        help=""Pretrained tokenizer name or path if not the same as model_name"")\n    parser.add_argument(""--cache_dir"", default="""", type=str,\n                        help=""Where do you want to store the pre-trained models downloaded from s3"")\n    parser.add_argument(""--max_seq_length"", default=128, type=int,\n                        help=""The maximum total input sequence length after tokenization. Sequences longer ""\n                             ""than this will be truncated, sequences shorter will be padded."")\n    parser.add_argument(""--do_train"", action=""store_true"",\n                        help=""Whether to run training."")\n    parser.add_argument(""--do_eval"", action=""store_true"",\n                        help=""Whether to run eval on the dev set."")\n    parser.add_argument(""--do_predict"", action=""store_true"",\n                        help=""Whether to run predictions on the test set."")\n    parser.add_argument(""--evaluate_during_training"", action=""store_true"",\n                        help=""Whether to run evaluation during training at each logging step."")\n    parser.add_argument(""--do_lower_case"", action=""store_true"",\n                        help=""Set this flag if you are using an uncased model."")\n\n    parser.add_argument(""--per_gpu_train_batch_size"", default=8, type=int,\n                        help=""Batch size per GPU/CPU for training."")\n    parser.add_argument(""--per_gpu_eval_batch_size"", default=8, type=int,\n                        help=""Batch size per GPU/CPU for evaluation."")\n    parser.add_argument(""--gradient_accumulation_steps"", type=int, default=1,\n                        help=""Number of updates steps to accumulate before performing a backward/update pass."")\n    parser.add_argument(""--learning_rate"", default=5e-5, type=float,\n                        help=""The initial learning rate for Adam."")\n    parser.add_argument(""--weight_decay"", default=0.0, type=float,\n                        help=""Weight decay if we apply some."")\n    parser.add_argument(""--adam_epsilon"", default=1e-8, type=float,\n                        help=""Epsilon for Adam optimizer."")\n    parser.add_argument(""--max_grad_norm"", default=1.0, type=float,\n                        help=""Max gradient norm."")\n    parser.add_argument(""--num_train_epochs"", default=3.0, type=float,\n                        help=""Total number of training epochs to perform."")\n    parser.add_argument(""--max_steps"", default=-1, type=int,\n                        help=""If > 0: set total number of training steps to perform. Override num_train_epochs."")\n    parser.add_argument(""--warmup_steps"", default=0, type=int,\n                        help=""Linear warmup over warmup_steps."")\n\n    parser.add_argument(""--logging_steps"", type=int, default=50,\n                        help=""Log every X updates steps."")\n    parser.add_argument(""--save_steps"", type=int, default=50,\n                        help=""Save checkpoint every X updates steps."")\n    parser.add_argument(""--eval_all_checkpoints"", action=""store_true"",\n                        help=""Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number"")\n    parser.add_argument(""--no_cuda"", action=""store_true"",\n                        help=""Avoid using CUDA when available"")\n    parser.add_argument(""--overwrite_output_dir"", action=""store_true"",\n                        help=""Overwrite the content of the output directory"")\n    parser.add_argument(""--overwrite_cache"", action=""store_true"",\n                        help=""Overwrite the cached training and evaluation sets"")\n    parser.add_argument(""--seed"", type=int, default=42,\n                        help=""random seed for initialization"")\n\n    parser.add_argument(""--fp16"", action=""store_true"",\n                        help=""Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit"")\n    parser.add_argument(""--fp16_opt_level"", type=str, default=""O1"",\n                        help=""For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].""\n                             ""See details at https://nvidia.github.io/apex/amp.html"")\n    parser.add_argument(""--local_rank"", type=int, default=-1,\n                        help=""For distributed training: local_rank"")\n    parser.add_argument(""--server_ip"", type=str, default="""", help=""For distant debugging."")\n    parser.add_argument(""--server_port"", type=str, default="""", help=""For distant debugging."")\n    args = parser.parse_args()\n\n    if os.path.exists(args.output_dir) and os.listdir(\n            args.output_dir) and args.do_train and not args.overwrite_output_dir:\n        raise ValueError(\n            ""Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome."".format(\n                args.output_dir))\n\n    # Setup distant debugging if needed\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n        print(""Waiting for debugger attach"")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        args.n_gpu = torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(""cuda"", args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"")\n        args.n_gpu = 1\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n                        datefmt=""%m/%d/%Y %H:%M:%S"",\n                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning(""Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s"",\n                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n\n    # Set seed\n    set_seed(args)\n\n    # Prepare CONLL-2003 task\n    labels = get_labels(args.labels)\n    num_labels = len(labels)\n    logging.info(""label num {}"".format(num_labels))\n    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n    pad_token_label_id = CrossEntropyLoss().ignore_index\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    args.model_type = args.model_type.lower()\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n                                          num_labels=num_labels,\n                                          cache_dir=args.cache_dir if args.cache_dir else None)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n                                                do_lower_case=args.do_lower_case,\n                                                cache_dir=args.cache_dir if args.cache_dir else None)\n    model = model_class.from_pretrained(args.model_name_or_path,\n                                        from_tf=bool("".ckpt"" in args.model_name_or_path),\n                                        config=config,\n                                        cache_dir=args.cache_dir if args.cache_dir else None)\n\n    if args.local_rank == 0:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    model.to(args.device)\n\n    logger.info(""Training/evaluation parameters %s"", args)\n    def predict_callback(model,step):\n        labels = get_labels(args.labels)\n        config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n        # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n        pad_token_label_id = CrossEntropyLoss().ignore_index\n        if args.do_predict and args.local_rank in [-1, 0]:\n            tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path, do_lower_case=args.do_lower_case)\n            evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=""test"")\n        model.train()\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=""train"")\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer, labels, pad_token_label_id,predict_callback)\n        logger.info("" global_step = %s, average loss = %s"", global_step, tr_loss)\n\n    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        # Create output directory if needed\n        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(args.output_dir)\n\n        logger.info(""Saving model checkpoint to %s"", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = model.module if hasattr(model, ""module"") else model  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, ""training_args.bin""))\n\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + ""/**/"" + WEIGHTS_NAME, recursive=True)))\n            logging.getLogger(""pytorch_transformers.modeling_utils"").setLevel(logging.WARN)  # Reduce logging\n        logger.info(""Evaluate the following checkpoints: %s"", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(""-"")[-1] if len(checkpoints) > 1 else """"\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result, _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=""dev"", prefix=global_step)\n            if global_step:\n                result = {""{}_{}"".format(global_step, k): v for k, v in result.items()}\n            results.update(result)\n        output_eval_file = os.path.join(args.output_dir, ""eval_results.txt"")\n        with open(output_eval_file, ""w"") as writer:\n            for key in sorted(results.keys()):\n                writer.write(""{} = {}\\n"".format(key, str(results[key])))\n\n    if args.do_predict and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model = model_class.from_pretrained(args.output_dir)\n        model.to(args.device)\n        result, predictions = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=""test"")\n        # Save results\n        output_test_results_file = os.path.join(args.output_dir, ""test_results.txt"")\n        with open(output_test_results_file, ""w"") as writer:\n            for key in sorted(result.keys()):\n                writer.write(""{} = {}\\n"".format(key, str(result[key])))\n        # Save predictions\n        output_test_predictions_file = os.path.join(args.output_dir, ""test_predictions.txt"")\n        with open(output_test_predictions_file, ""w"") as writer:\n            with open(os.path.join(args.data_dir, ""test.txt""), ""r"") as f:\n                example_id = 0\n                for line in f:\n                    if line.startswith(""-DOCSTART-"") or line == """" or line == ""\\n"":\n                        writer.write(line)\n                        if not predictions[example_id]:\n                            example_id += 1\n                    elif predictions[example_id]:\n                        output_line = line.split()[0] + "" "" + predictions[example_id].pop(0) + ""\\n""\n                        writer.write(output_line)\n                    else:\n                        logger.warning(""Maximum sequence length exceeded: No prediction for \'%s\'."", line.split()[0])\n\n    return results\n\n\nif __name__ == ""__main__"":\n    main()\n\n'"
examples/conll2003_example/run_ner_distill.py,27,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Fine-tuning the library models for named entity recognition on CoNLL-2003 (Bert or Roberta). """"""\n\n# from __future__ import absolute_import, division, print_function\n\nimport argparse\nimport glob\nimport logging\nimport os\nimport random\n\nimport numpy as np\nimport torch\nfrom seqeval.metrics import precision_score, recall_score, f1_score\nfrom tensorboardX import SummaryWriter\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, Dataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm, trange\nfrom utils_ner import convert_examples_to_features, get_labels, read_examples_from_file\n\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import WEIGHTS_NAME, BertConfig, BertForTokenClassification, BertTokenizer\n\nclass MyDataset(Dataset):\n    def __init__(self,all_input_ids, all_input_mask, all_segment_ids, all_labels):\n        super(MyDataset, self).__init__()\n        self.all_input_ids = all_input_ids\n        self.all_input_mask = all_input_mask\n        self.all_segment_ids =all_segment_ids\n        self.all_labels = all_labels\n    def __getitem__(self, index):\n        input_ids = self.all_input_ids[index]\n        input_mask = self.all_input_mask[index]\n        segment_ids = self.all_segment_ids[index]\n        labels = self.all_labels[index]\n        return {\'input_ids\':input_ids,\n                \'attention_mask\':input_mask,\n                \'token_type_ids\':segment_ids,\n                \'labels\':labels}\n    def __len__(self):\n        return len(self.all_labels)\n\nlogger = logging.getLogger(__name__)\n\nALL_MODELS = sum(\n    (tuple(conf.pretrained_config_archive_map.keys()) for conf in (BertConfig,)),\n    ())\n\nMODEL_CLASSES = {\n    ""bert"": (BertConfig, BertForTokenClassification, BertTokenizer),\n}\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n\ndef train(args, train_dataset,model_T, model, tokenizer, labels, pad_token_label_id,predict_callback):\n    """""" Train the model """"""\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [""bias"", ""LayerNorm.weight""]\n    optimizer_grouped_parameters = [\n        {""params"": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n         ""weight_decay"": args.weight_decay},\n        {""params"": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], ""weight_decay"": 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler_class = get_linear_schedule_with_warmup\n    scheduler_args = {\'num_warmup_steps\':int(args.warmup_steps*t_total), \'num_training_steps\':t_total}\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n                                                          output_device=args.local_rank,\n                                                          find_unused_parameters=True)\n\n    # Train!\n    logger.info(""***** Running training *****"")\n    logger.info(""  Num examples = %d"", len(train_dataset))\n    logger.info(""  Num Epochs = %d"", args.num_train_epochs)\n    logger.info(""  Instantaneous batch size per GPU = %d"", args.per_gpu_train_batch_size)\n    logger.info(""  Total train batch size (w. parallel, distributed & accumulation) = %d"",\n                args.train_batch_size * args.gradient_accumulation_steps * (\n                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info(""  Gradient Accumulation steps = %d"", args.gradient_accumulation_steps)\n    logger.info(""  Total optimization steps = %d"", t_total)\n    if args.do_train and args.do_distill:\n        from textbrewer import DistillationConfig,TrainingConfig,GeneralDistiller\n        distill_config = DistillationConfig(\n            temperature = 8,\n              # intermediate_matches = [{\'layer_T\':10, \'layer_S\':3, \'feature\':\'hidden\',\'loss\': \'hidden_mse\', \'weight\' : 1}]\n            )\n        train_config = TrainingConfig(device=""cuda"",log_dir=args.output_dir,output_dir=args.output_dir)\n        def adaptor_T(batch,model_output):\n            return {""logits"":(model_output[1],),\n                    \'logits_mask\':(batch[\'attention_mask\'],)}\n        def adaptor_S(batch,model_output):\n            return {""logits"":(model_output[1],),\n                    \'logits_mask\':(batch[\'attention_mask\'],)}\n\n        distiller=GeneralDistiller(train_config,distill_config,model_T,model,adaptor_T,adaptor_S,)\n        distiller.train(optimizer,train_dataloader,,args.num_train_epochs,\n                        scheduler_class=scheduler_class, scheduler_args=scheduler_args,\n                        max_grad_norm=1.0, callback=predict_callback)\n        return\n\n\ndef evaluate(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=""""):\n    eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode)\n\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Note that DistributedSampler samples randomly\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.info(""***** Running evaluation %s *****"", prefix)\n    logger.info(""  Num examples = %d"", len(eval_dataset))\n    logger.info(""  Batch size = %d"", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    preds = None\n    out_label_ids = None\n    model.eval()\n    for batch in tqdm(eval_dataloader, desc=""Evaluating""):\n        batch = tuple(t.to(args.device) for t in batch)\n\n        with torch.no_grad():\n            inputs = {""input_ids"": batch[0],\n                      ""attention_mask"": batch[1],\n                      ""labels"": batch[3]}\n            if args.model_type != ""distilbert"":\n                inputs[""token_type_ids""] = batch[2] if args.model_type in [""bert"", ""xlnet""] else None  # XLM and RoBERTa don""t use segment_ids\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n\n            if args.n_gpu > 1:\n                tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n\n            eval_loss += tmp_eval_loss.item()\n        nb_eval_steps += 1\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[""labels""].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(out_label_ids, inputs[""labels""].detach().cpu().numpy(), axis=0)\n\n    eval_loss = eval_loss / nb_eval_steps\n    preds = np.argmax(preds, axis=2)\n\n    label_map = {i: label for i, label in enumerate(labels)}\n\n    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n\n    for i in range(out_label_ids.shape[0]):\n        for j in range(out_label_ids.shape[1]):\n            if out_label_ids[i, j] != pad_token_label_id:\n                out_label_list[i].append(label_map[out_label_ids[i][j]])\n                preds_list[i].append(label_map[preds[i][j]])\n\n    results = {\n        ""loss"": eval_loss,\n        ""precision"": precision_score(out_label_list, preds_list),\n        ""recall"": recall_score(out_label_list, preds_list),\n        ""f1"": f1_score(out_label_list, preds_list)\n    }\n\n    logger.info(""***** Eval results %s *****"", prefix)\n    for key in sorted(results.keys()):\n        logger.info(""  %s = %s"", key, str(results[key]))\n\n    return results, preds_list\n\n\ndef load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, ""cached_{}_{}_{}"".format(mode,\n        list(filter(None, args.model_name_or_path.split(""/""))).pop(),\n        str(args.max_seq_length)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(""Loading features from cached file %s"", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(""Creating features from dataset file at %s"", args.data_dir)\n        examples = read_examples_from_file(args.data_dir, mode)\n        features = convert_examples_to_features(examples, labels, args.max_seq_length, tokenizer,\n                                                cls_token_at_end=bool(args.model_type in [""xlnet""]),\n                                                # xlnet has a cls token at the end\n                                                cls_token=tokenizer.cls_token,\n                                                cls_token_segment_id=2 if args.model_type in [""xlnet""] else 0,\n                                                sep_token=tokenizer.sep_token,\n                                                sep_token_extra=bool(args.model_type in [""roberta""]),\n                                                # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n                                                pad_on_left=bool(args.model_type in [""xlnet""]),\n                                                # pad on the left for xlnet\n                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                                                pad_token_segment_id=4 if args.model_type in [""xlnet""] else 0,\n                                                pad_token_label_id=pad_token_label_id\n                                                )\n        if args.local_rank in [-1, 0]:\n            logger.info(""Saving features into cached file %s"", cached_features_file)\n            torch.save(features, cached_features_file)\n\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n    if mode==""train"":\n        dataset = MyDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids) #distill must input a dict\n    else:\n        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n    return dataset\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(""--data_dir"", default=None, type=str, required=True,\n                        help=""The input data dir. Should contain the training files for the CoNLL-2003 NER task."")\n    parser.add_argument(""--model_type"", default=None, type=str, required=True,\n                        help=""Model type selected in the list: "" + "", "".join(MODEL_CLASSES.keys()))\n    parser.add_argument(""--model_name_or_path"", default=None, type=str, required=True,\n                        help=""Path to pre-trained model or shortcut name selected in the list: "" + "", "".join(ALL_MODELS))\n    parser.add_argument(""--model_name_or_path_student"", default=None, type=str, required=False,\n                        help=""Path to pre-trained model or shortcut name selected in the list: "" + "", "".join(ALL_MODELS))\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True,\n                        help=""The output directory where the model predictions and checkpoints will be written."")\n    parser.add_argument(""--num_hidden_layers"", default=3, type=int,\n                        help=""number of layers of student model"")\n\n    ## Other parameters\n    parser.add_argument(""--labels"", default="""", type=str,\n                        help=""Path to a file containing all labels. If not specified, CoNLL-2003 labels are used."")\n    parser.add_argument(""--config_name"", default="""", type=str,\n                        help=""Pretrained config name or path if not the same as model_name"")\n    parser.add_argument(""--tokenizer_name"", default="""", type=str,\n                        help=""Pretrained tokenizer name or path if not the same as model_name"")\n    parser.add_argument(""--cache_dir"", default="""", type=str,\n                        help=""Where do you want to store the pre-trained models downloaded from s3"")\n    parser.add_argument(""--max_seq_length"", default=128, type=int,\n                        help=""The maximum total input sequence length after tokenization. Sequences longer ""\n                             ""than this will be truncated, sequences shorter will be padded."")\n    parser.add_argument(""--do_train"", action=""store_true"",\n                        help=""Whether to run training."")\n    parser.add_argument(""--do_distill"", action=""store_true"",\n                        help=""Whether to run training."")\n    parser.add_argument(""--do_eval"", action=""store_true"",\n                        help=""Whether to run eval on the dev set."")\n    parser.add_argument(""--do_predict"", action=""store_true"",\n                        help=""Whether to run predictions on the test set."")\n    parser.add_argument(""--evaluate_during_training"", action=""store_true"",\n                        help=""Whether to run evaluation during training at each logging step."")\n    parser.add_argument(""--do_lower_case"", action=""store_true"",\n                        help=""Set this flag if you are using an uncased model."")\n\n    parser.add_argument(""--per_gpu_train_batch_size"", default=8, type=int,\n                        help=""Batch size per GPU/CPU for training."")\n    parser.add_argument(""--per_gpu_eval_batch_size"", default=8, type=int,\n                        help=""Batch size per GPU/CPU for evaluation."")\n    parser.add_argument(""--gradient_accumulation_steps"", type=int, default=1,\n                        help=""Number of updates steps to accumulate before performing a backward/update pass."")\n    parser.add_argument(""--learning_rate"", default=5e-5, type=float,\n                        help=""The initial learning rate for Adam."")\n    parser.add_argument(""--weight_decay"", default=0.0, type=float,\n                        help=""Weight decay if we apply some."")\n    parser.add_argument(""--adam_epsilon"", default=1e-8, type=float,\n                        help=""Epsilon for Adam optimizer."")\n    parser.add_argument(""--max_grad_norm"", default=1.0, type=float,\n                        help=""Max gradient norm."")\n    parser.add_argument(""--num_train_epochs"", default=3.0, type=float,\n                        help=""Total number of training epochs to perform."")\n    parser.add_argument(""--max_steps"", default=-1, type=int,\n                        help=""If > 0: set total number of training steps to perform. Override num_train_epochs."")\n    parser.add_argument(""--warmup_steps"", default=0, type=float,\n                        help=""Linear warmup over warmup_steps."")\n\n    parser.add_argument(""--logging_steps"", type=int, default=50,\n                        help=""Log every X updates steps."")\n    parser.add_argument(""--save_steps"", type=int, default=50,\n                        help=""Save checkpoint every X updates steps."")\n    parser.add_argument(""--eval_all_checkpoints"", action=""store_true"",\n                        help=""Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number"")\n    parser.add_argument(""--no_cuda"", action=""store_true"",\n                        help=""Avoid using CUDA when available"")\n    parser.add_argument(""--overwrite_output_dir"", action=""store_true"",\n                        help=""Overwrite the content of the output directory"")\n    parser.add_argument(""--overwrite_cache"", action=""store_true"",\n                        help=""Overwrite the cached training and evaluation sets"")\n    parser.add_argument(""--seed"", type=int, default=42,\n                        help=""random seed for initialization"")\n\n    parser.add_argument(""--fp16"", action=""store_true"",\n                        help=""Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit"")\n    parser.add_argument(""--fp16_opt_level"", type=str, default=""O1"",\n                        help=""For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].""\n                             ""See details at https://nvidia.github.io/apex/amp.html"")\n    parser.add_argument(""--local_rank"", type=int, default=-1,\n                        help=""For distributed training: local_rank"")\n    parser.add_argument(""--server_ip"", type=str, default="""", help=""For distant debugging."")\n    parser.add_argument(""--server_port"", type=str, default="""", help=""For distant debugging."")\n    args = parser.parse_args()\n\n    if os.path.exists(args.output_dir) and os.listdir(\n            args.output_dir) and args.do_train and not args.overwrite_output_dir:\n        raise ValueError(\n            ""Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome."".format(\n                args.output_dir))\n\n\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        args.n_gpu = torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(""cuda"", args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"")\n        args.n_gpu = 1\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n                        datefmt=""%m/%d/%Y %H:%M:%S"",\n                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning(""Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s"",\n                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n\n    # Set seed\n    set_seed(args)\n\n    # Prepare CONLL-2003 task\n    labels = get_labels(args.labels)\n    num_labels = len(labels)\n    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n    pad_token_label_id = CrossEntropyLoss().ignore_index\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    args.model_type = args.model_type.lower()\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n                                          num_labels=num_labels,\n                                          cache_dir=args.cache_dir if args.cache_dir else None)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n                                                do_lower_case=args.do_lower_case,\n                                                cache_dir=args.cache_dir if args.cache_dir else None)\n    model_T = model_class.from_pretrained(args.model_name_or_path,\n                                        from_tf=bool("".ckpt"" in args.model_name_or_path),\n                                        config=config,\n                                        cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.model_name_or_path_student != None:\n        config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path_student,\n                                              num_labels=num_labels,\n                                              cache_dir=args.cache_dir if args.cache_dir else None)\n        config.num_hidden_layers=args.num_hidden_layers\n        model = model_class.from_pretrained(args.model_name_or_path_student,\n                                        from_tf=bool("".ckpt"" in args.model_name_or_path),\n                                        config=config,\n                                        cache_dir=args.cache_dir if args.cache_dir else None)\n    else:\n        config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n                                          num_labels=num_labels,\n                                          cache_dir=args.cache_dir if args.cache_dir else None)\n        config.num_hidden_layers=args.num_hidden_layers\n        model = model_class.from_pretrained(args.model_name_or_path,\n                                        from_tf=bool("".ckpt"" in args.model_name_or_path),\n                                        config=config,\n                                        cache_dir=args.cache_dir if args.cache_dir else None)\n\n\n\n\n\n\n    if args.local_rank == 0:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    model.to(args.device)\n    model_T.to(args.device)\n\n    logger.info(""Training/evaluation parameters %s"", args)\n    def predict_callback(model,step):\n        labels = get_labels(args.labels)\n        config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n        # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n        pad_token_label_id = CrossEntropyLoss().ignore_index\n        if args.do_predict and args.local_rank in [-1, 0]:\n            tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path, do_lower_case=args.do_lower_case)\n            evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=""test"")\n        model.train()\n\n    # Training\n    if args.do_train :\n        train_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=""train"")\n        # global_step, tr_loss = \\\n        train(args, train_dataset,model_T, model, tokenizer, labels, pad_token_label_id,predict_callback)\n        # logger.info("" global_step = %s, average loss = %s"", global_step, tr_loss)\n\n    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        # Create output directory if needed\n        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(args.output_dir)\n\n        logger.info(""Saving model checkpoint to %s"", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = model.module if hasattr(model, ""module"") else model  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, ""training_args.bin""))\n\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + ""/**/"" + WEIGHTS_NAME, recursive=True)))\n            logging.getLogger(""pytorch_transformers.modeling_utils"").setLevel(logging.WARN)  # Reduce logging\n        logger.info(""Evaluate the following checkpoints: %s"", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(""-"")[-1] if len(checkpoints) > 1 else """"\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result, _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=""dev"", prefix=global_step)\n            if global_step:\n                result = {""{}_{}"".format(global_step, k): v for k, v in result.items()}\n            results.update(result)\n        output_eval_file = os.path.join(args.output_dir, ""eval_results.txt"")\n        with open(output_eval_file, ""w"") as writer:\n            for key in sorted(results.keys()):\n                writer.write(""{} = {}\\n"".format(key, str(results[key])))\n\n    if args.do_predict and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model = model_class.from_pretrained(args.output_dir)\n        model.to(args.device)\n        result, predictions = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=""test"")\n        # Save results\n        output_test_results_file = os.path.join(args.output_dir, ""test_results.txt"")\n        with open(output_test_results_file, ""w"") as writer:\n            for key in sorted(result.keys()):\n                writer.write(""{} = {}\\n"".format(key, str(result[key])))\n        # Save predictions\n        output_test_predictions_file = os.path.join(args.output_dir, ""test_predictions.txt"")\n        with open(output_test_predictions_file, ""w"") as writer:\n            with open(os.path.join(args.data_dir, ""test.txt""), ""r"") as f:\n                example_id = 0\n                for line in f:\n                    if line.startswith(""-DOCSTART-"") or line == """" or line == ""\\n"":\n                        writer.write(line)\n                        if not predictions[example_id]:\n                            example_id += 1\n                    elif predictions[example_id]:\n                        output_line = line.split()[0] + "" "" + predictions[example_id].pop(0) + ""\\n""\n                        writer.write(output_line)\n                    else:\n                        logger.warning(""Maximum sequence length exceeded: No prediction for \'%s\'."", line.split()[0])\n\n    return results\n\n\nif __name__ == ""__main__"":\n    main()\n\n'"
examples/conll2003_example/utils_ner.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Named entity recognition fine-tuning: utilities to work with CoNLL-2003 task. """"""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport logging\nimport os\nfrom io import open\n\nlogger = logging.getLogger(__name__)\n\n\nclass InputExample(object):\n    """"""A single training/test example for token classification.""""""\n\n    def __init__(self, guid, words, labels):\n        """"""Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            words: list. The words of the sequence.\n            labels: (Optional) list. The labels for each word of the sequence. This should be\n            specified for train and dev examples, but not for test examples.\n        """"""\n        self.guid = guid\n        self.words = words\n        self.labels = labels\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_ids = label_ids\n\n\ndef read_examples_from_file(data_dir, mode):\n    file_path = os.path.join(data_dir, ""{}.txt"".format(mode))\n    guid_index = 1\n    examples = []\n    with open(file_path, encoding=""utf-8"") as f:\n        words = []\n        labels = []\n        for line in f:\n            line=line.replace(""\\u200e"","""")\n            line=line.replace(""\\u200f"","""")\n\n            if line.startswith(""-DOCSTART-"") or line == """" or line == ""\\n"":\n                if words:\n                    examples.append(InputExample(guid=""{}-{}"".format(mode, guid_index),\n                                                 words=words,\n                                                 labels=labels))\n                    guid_index += 1\n                    words = []\n                    labels = []\n            else:\n                splits = line.split("" "")\n                if splits[0] =="""":\n                    continue\n                words.append(splits[0])\n                if len(splits) > 1:\n                    labels.append(splits[-1].replace(""\\n"", """"))\n                else:\n                    # Examples could have no label for mode = ""test""\n                    labels.append(""O"")\n        if words:\n            examples.append(InputExample(guid=""%s-%d"".format(mode, guid_index),\n                                         words=words,\n                                         labels=labels))\n    return examples\n\n\ndef convert_examples_to_features(examples,\n                                 label_list,\n                                 max_seq_length,\n                                 tokenizer,\n                                 cls_token_at_end=False,\n                                 cls_token=""[CLS]"",\n                                 cls_token_segment_id=1,\n                                 sep_token=""[SEP]"",\n                                 sep_token_extra=False,\n                                 pad_on_left=False,\n                                 pad_token=0,\n                                 pad_token_segment_id=0,\n                                 pad_token_label_id=-1,\n                                 sequence_a_segment_id=0,\n                                 mask_padding_with_zero=True):\n    """""" Loads a data file into a list of `InputBatch`s\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n    """"""\n\n    label_map = {label: i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(""Writing example %d of %d"", ex_index, len(examples))\n\n        tokens = []\n        label_ids = []\n        for word, label in zip(example.words, example.labels):\n            word_tokens = tokenizer.tokenize(word)\n            tokens.extend(word_tokens)\n            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n            label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n\n        # Account for [CLS] and [SEP] with ""- 2"" and with ""- 3"" for RoBERTa.\n        special_tokens_count = 3 if sep_token_extra else 2\n        if len(tokens) > max_seq_length - special_tokens_count:\n            tokens = tokens[:(max_seq_length - special_tokens_count)]\n            label_ids = label_ids[:(max_seq_length - special_tokens_count)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids:   0   0   0   0  0     0   0\n        #\n        # Where ""type_ids"" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambiguously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the ""sentence vector"". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens += [sep_token]\n        label_ids += [pad_token_label_id]\n        if sep_token_extra:\n            # roberta uses an extra separator b/w pairs of sentences\n            tokens += [sep_token]\n            label_ids += [pad_token_label_id]\n        segment_ids = [sequence_a_segment_id] * len(tokens)\n\n        if cls_token_at_end:\n            tokens += [cls_token]\n            label_ids += [pad_token_label_id]\n            segment_ids += [cls_token_segment_id]\n        else:\n            tokens = [cls_token] + tokens\n            label_ids = [pad_token_label_id] + label_ids\n            segment_ids = [cls_token_segment_id] + segment_ids\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        padding_length = max_seq_length - len(input_ids)\n        if pad_on_left:\n            input_ids = ([pad_token] * padding_length) + input_ids\n            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n        else:\n            input_ids += ([pad_token] * padding_length)\n            input_mask += ([0 if mask_padding_with_zero else 1] * padding_length)\n            segment_ids += ([pad_token_segment_id] * padding_length)\n            label_ids += ([pad_token_label_id] * padding_length)\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n        # logger.info(""label idx len {}"".format(len(label_ids)))\n        # logger.info(""data :{}"".format("" "".join([str(x) for x in tokens])))\n        # logger.info(""data :{}"".format(label_ids))\n        #logger.info(""words :{}"".format(example.words))\n        assert len(label_ids) == max_seq_length\n\n        if ex_index == 5:\n            logger.info(""*** Example ***"")\n            logger.info(""guid: %s"", example.guid)\n            logger.info(""tokens: %s"", "" "".join([str(x) for x in tokens]))\n            logger.info(""input_ids: %s"", "" "".join([str(x) for x in input_ids]))\n            logger.info(""input_mask: %s"", "" "".join([str(x) for x in input_mask]))\n            logger.info(""segment_ids: %s"", "" "".join([str(x) for x in segment_ids]))\n            logger.info(""label_ids: %s"", "" "".join([str(x) for x in label_ids]))\n\n        features.append(\n                InputFeatures(input_ids=input_ids,\n                              input_mask=input_mask,\n                              segment_ids=segment_ids,\n                              label_ids=label_ids))\n    return features\n\n\ndef get_labels(path):\n    if path:\n        with open(path, ""r"") as f:\n            labels = f.read().splitlines()\n        if ""O"" not in labels:\n            labels = [""O""] + labels\n        return labels\n    else:\n        return [""O"", ""B-MISC"", ""I-MISC"",  ""B-PER"", ""I-PER"", ""B-ORG"", ""I-ORG"", ""B-LOC"", ""I-LOC""]\n'"
examples/matches/matches.py,0,"b'L3_attention_mse=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1}]\n\nL3_attention_ce=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1}]\n\nL3_attention_mse_sum=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1}]\n\nL3_attention_ce_mean=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1}]\n\nL3_hidden_smmd=[{""layer_T"":[0,0],  ""layer_S"":[0,0], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[4,4],  ""layer_S"":[1,1], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[8,8],  ""layer_S"":[2,2], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[12,12],""layer_S"":[3,3], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1}]\n\nL3n_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]},\n                {""layer_T"":4, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]},\n                {""layer_T"":8, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]},\n                {""layer_T"":12,""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]}]\n\nL3_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1},\n               {""layer_T"":4, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1},\n               {""layer_T"":8, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1},\n               {""layer_T"":12,""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}]\n\n#######################L4################\nL4_attention_mse=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1}]\n\nL4_attention_ce=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1}]\n\nL4_attention_mse_sum=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1}]\n\nL4_attention_ce_mean=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1}]\n\nL4_hidden_smmd=[{""layer_T"":[0,0],  ""layer_S"":[0,0], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[3,3],  ""layer_S"":[1,1], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[6,6],  ""layer_S"":[2,2], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[9,9],  ""layer_S"":[3,3], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[12,12],""layer_S"":[4,4], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1}]\n\nL4t_hidden_sgram=[{""layer_T"":[0,0],  ""layer_S"":[0,0], ""feature"":""hidden"", ""loss"":""gram"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                  {""layer_T"":[3,3],  ""layer_S"":[1,1], ""feature"":""hidden"", ""loss"":""gram"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                  {""layer_T"":[6,6],  ""layer_S"":[2,2], ""feature"":""hidden"", ""loss"":""gram"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                  {""layer_T"":[9,9],  ""layer_S"":[3,3], ""feature"":""hidden"", ""loss"":""gram"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                  {""layer_T"":[12,12],""layer_S"":[4,4], ""feature"":""hidden"", ""loss"":""gram"", ""weight"":1, ""proj"":[""linear"",312,768]}]\n\nL4t_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":3, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":6, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":9, ""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":12,""layer_S"":4, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]}]\n\n###########L6#############\nL6_hidden_smmd=[{""layer_T"":[0,0],  ""layer_S"":[0,0], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[2,2],  ""layer_S"":[1,1], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[4,4],  ""layer_S"":[2,2], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[6,6],  ""layer_S"":[3,3], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[8,8],  ""layer_S"":[4,4], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[10,10],""layer_S"":[5,5], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[12,12],""layer_S"":[6,6], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1}]\n\nL6_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":2, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":4, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":6, ""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":8, ""layer_S"":4, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":10,""layer_S"":5, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":12,""layer_S"":6, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}]\n\n#electra-small\nsmall_hidden_smmd=[{""layer_T"":[0,0],  ""layer_S"":[0,0], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                   {""layer_T"":[2,2],  ""layer_S"":[2,2], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                   {""layer_T"":[4,4],  ""layer_S"":[4,4], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                   {""layer_T"":[6,6],  ""layer_S"":[6,6], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                   {""layer_T"":[8,8],  ""layer_S"":[8,8], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                   {""layer_T"":[10,10],""layer_S"":[10,10],""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                   {""layer_T"":[12,12],""layer_S"":[12,12],""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1}]\n\nsmall_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",256,768]}, \n                  {""layer_T"":2, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",256,768]}, \n                  {""layer_T"":4, ""layer_S"":4, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",256,768]}, \n                  {""layer_T"":6, ""layer_S"":6, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",256,768]}, \n                  {""layer_T"":8, ""layer_S"":8, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",256,768]}, \n                  {""layer_T"":10,""layer_S"":10,""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",256,768]}, \n                  {""layer_T"":12,""layer_S"":12,""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",256,768]}]\n\n\n\n\nmatches={\'L3_attention_mse\':L3_attention_mse,\'L3_attention_mse_sum\':L3_attention_mse_sum,\n         \'L3_attention_ce\' :L3_attention_ce, \'L3_attention_ce_mean\':L3_attention_ce_mean,\n         \'L3n_hidden_mse\'  :L3n_hidden_mse,  \'L3_hidden_smmd\'      :L3_hidden_smmd,       \'L3_hidden_mse\': L3_hidden_mse,\n         \'L4_attention_mse\':L4_attention_mse,\'L4_attention_mse_sum\':L4_attention_mse_sum,\n         \'L4_attention_ce\' :L4_attention_ce, \'L4_attention_ce_mean\':L4_attention_ce_mean,\n         \'L4t_hidden_mse\'  :L4t_hidden_mse,  \'L4_hidden_smmd\'      :L4_hidden_smmd,       \'L4t_hidden_sgram\': L4t_hidden_sgram,\n         \'L6_hidden_mse\'   :L6_hidden_mse,   \'L6_hidden_smmd\'      :L6_hidden_smmd,\n         \'small_hidden_mse\':small_hidden_mse,\'small_hidden_smmd\'   :small_hidden_smmd\n        }\n'"
examples/mnli_example/config.py,0,"b'import argparse\nfrom utils_glue import processors\nargs = None\n\ndef parse(opt=None):\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n\n    parser.add_argument(""--vocab_file"", default=None, type=str, required=True,\n                        help=""The vocabulary file that the BERT model was trained on."")\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True,\n                        help=""The output directory where the model checkpoints will be written."")\n\n    ## Other parameters\n    parser.add_argument(""--data_dir"", default=None, type=str)\n    parser.add_argument(""--do_lower_case"", action=\'store_true\',\n                        help=""Whether to lower case the input text. Should be True for uncased ""\n                             ""models and False for cased models."")\n    parser.add_argument(""--max_seq_length"", default=416, type=int,\n                        help=""The maximum total input sequence length after WordPiece tokenization. Sequences ""\n                             ""longer than this will be truncated, and sequences shorter than this will be padded."")\n    parser.add_argument(""--do_train"", default=False, action=\'store_true\', help=""Whether to run training."")\n    parser.add_argument(""--do_predict"", default=False, action=\'store_true\', help=""Whether to run eval on the dev set."")\n    parser.add_argument(""--train_batch_size"", default=32, type=int, help=""Total batch size for training."")\n    parser.add_argument(""--predict_batch_size"", default=8, type=int, help=""Total batch size for predictions."")\n    parser.add_argument(""--learning_rate"", default=3e-5, type=float, help=""The initial learning rate for Adam."")\n    parser.add_argument(""--num_train_epochs"", default=3.0, type=float,\n                        help=""Total number of training epochs to perform."")\n    parser.add_argument(""--warmup_proportion"", default=0.1, type=float,\n                        help=""Proportion of training to perform linear learning rate warmup for. E.g., 0.1 = 10% ""\n                             ""of training."")\n    parser.add_argument(""--verbose_logging"", default=False, action=\'store_true\',\n                        help=""If true, all of the warnings related to data processing will be printed. ""\n                             ""A number of warnings are expected for a normal SQuAD evaluation."")\n    parser.add_argument(""--no_cuda"",\n                        default=False,\n                        action=\'store_true\',\n                        help=""Whether not to use CUDA when available"")\n    parser.add_argument(\'--gradient_accumulation_steps\',\n                        type=int,\n                        default=1,\n                        help=""Number of updates steps to accumualte before performing a backward/update pass."")\n    parser.add_argument(""--local_rank"",\n                        type=int,\n                        default=-1,\n                        help=""local_rank for distributed training on gpus"")\n    parser.add_argument(\'--fp16\',\n                        default=False,\n                        action=\'store_true\',\n                        help=""Whether to use 16-bit float precisoin instead of 32-bit"")\n\n    parser.add_argument(\'--random_seed\',type=int,default=10236797)\n    parser.add_argument(\'--load_model_type\',type=str,default=\'bert\',choices=[\'bert\',\'all\',\'none\'])\n    parser.add_argument(\'--weight_decay_rate\',type=float,default=0.01)\n    parser.add_argument(\'--do_eval\',action=\'store_true\')\n    parser.add_argument(\'--PRINT_EVERY\',type=int,default=200)\n    parser.add_argument(\'--weight\',type=float,default=1.0)\n    parser.add_argument(\'--ckpt_frequency\',type=int,default=2)\n\n    parser.add_argument(\'--tuned_checkpoint_T\',type=str,default=None)\n    parser.add_argument(\'--tuned_checkpoint_Ts\',nargs=\'*\',type=str)\n\n    parser.add_argument(\'--tuned_checkpoint_S\',type=str,default=None)\n    parser.add_argument(""--init_checkpoint_S"", default=None, type=str)\n    parser.add_argument(""--bert_config_file_T"", default=None, type=str, required=True)\n    parser.add_argument(""--bert_config_file_S"", default=None, type=str, required=True)\n    parser.add_argument(""--temperature"", default=1, type=float, required=False)\n    parser.add_argument(""--teacher_cached"",action=\'store_true\')\n\n    parser.add_argument(\'--s_opt1\',type=float,default=1.0, help=""release_start / step1 / ratio"")\n    parser.add_argument(\'--s_opt2\',type=float,default=0.0, help=""release_level / step2"")\n    parser.add_argument(\'--s_opt3\',type=float,default=1.0, help=""not used / decay rate"")\n    parser.add_argument(\'--schedule\',type=str,default=\'warmup_linear_release\')\n\n    parser.add_argument(\'--no_inputs_mask\',action=\'store_true\')\n    parser.add_argument(\'--no_logits\', action=\'store_true\')\n    parser.add_argument(\'--output_att_score\',default=\'true\',choices=[\'true\',\'false\'])\n    parser.add_argument(\'--output_att_sum\', default=\'false\',choices=[\'true\',\'false\'])\n    parser.add_argument(\'--output_encoded_layers\'  ,default=\'true\',choices=[\'true\',\'false\'])\n    parser.add_argument(\'--output_attention_layers\',default=\'true\',choices=[\'true\',\'false\'])\n    parser.add_argument(\'--matches\',nargs=\'*\',type=str)\n    parser.add_argument(\'--task_name\',type=str,choices=list(processors.keys()))\n    parser.add_argument(\'--aux_task_name\',type=str,choices=list(processors.keys()),default=None)\n    parser.add_argument(\'--aux_data_dir\', type=str)\n\n    parser.add_argument(\'--only_load_embedding\',action=\'store_true\')\n    global args\n    if opt is None:\n        args = parser.parse_args()\n    else:\n        args = parser.parse_args(opt)\n\n\nif __name__ == \'__main__\':\n    print (args)\n    parse([\'--SAVE_DIR\',\'test\'])\n    print(args)\n'"
examples/mnli_example/main.distill.py,14,"b'import logging\nlogger = logging.getLogger(""Main"")\nlogger.setLevel(logging.INFO)\nhandler_stream = logging.StreamHandler()\nhandler_stream.setLevel(logging.INFO)\nformatter = logging.Formatter(fmt=\'%(asctime)s - %(levelname)s - %(name)s -  %(message)s\', datefmt=\'%Y/%m/%d %H:%M:%S\')\nhandler_stream.setFormatter(formatter)\nlogger.addHandler(handler_stream)\n\nimport os,random\nimport numpy as np\nimport torch\nfrom utils_glue import output_modes, processors\nfrom pytorch_pretrained_bert.my_modeling import BertConfig\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom optimization import BERTAdam\nimport config\nfrom utils import divide_parameters, load_and_cache_examples\nfrom modeling import BertForGLUESimple,BertForGLUESimpleAdaptor\n\nfrom textbrewer import DistillationConfig, TrainingConfig, GeneralDistiller\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, DistributedSampler\nfrom tqdm import tqdm\nfrom utils_glue import compute_metrics\nfrom functools import partial\n\n\ndef args_check(args):\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n        logger.warning(""Output directory () already exists and is not empty."")\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                            args.gradient_accumulation_steps))\n    if not args.do_train and not args.do_predict:\n        raise ValueError(""At least one of `do_train` or `do_predict` must be True."")\n\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        n_gpu = torch.cuda.device_count() if not args.no_cuda else 0\n    else:\n        device = torch.device(""cuda"", args.local_rank)\n        n_gpu = 1\n        torch.distributed.init_process_group(backend=\'nccl\')\n    logger.info(""device %s n_gpu %d distributed training %r"", device, n_gpu, bool(args.local_rank != -1))\n    args.n_gpu = n_gpu\n    args.device = device\n    return device, n_gpu\n\ndef predict(model,eval_datasets,step,args):\n    eval_task_names = (""mnli"", ""mnli-mm"") if args.task_name == ""mnli"" else (args.task_name,)\n    eval_output_dir = args.output_dir\n    results = {}\n    for eval_task,eval_dataset in zip(eval_task_names, eval_datasets):\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n        logger.info(""Predicting..."")\n        logger.info(""***** Running predictions *****"")\n        logger.info("" task name = %s"", eval_task)\n        logger.info(""  Num  examples = %d"", len(eval_dataset))\n        logger.info(""  Batch size = %d"", args.predict_batch_size)\n        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.predict_batch_size)\n        model.eval()\n\n        pred_logits = []\n        label_ids = []\n        for batch in tqdm(eval_dataloader, desc=""Evaluating"", disable=None):\n            input_ids, input_mask, segment_ids, labels = batch\n            input_ids = input_ids.to(args.device)\n            input_mask = input_mask.to(args.device)\n            segment_ids = segment_ids.to(args.device)\n            with torch.no_grad():\n                logits = model(input_ids, input_mask, segment_ids)\n            cpu_logits = logits.detach().cpu()\n            for i in range(len(cpu_logits)):\n                pred_logits.append(cpu_logits[i].numpy())\n                label_ids.append(labels[i])\n\n        pred_logits = np.array(pred_logits)\n        label_ids   = np.array(label_ids)\n\n        if args.output_mode == ""classification"":\n            preds = np.argmax(pred_logits, axis=1)\n        else: # args.output_mode == ""regression"":\n            preds = np.squeeze(pred_logits)\n        result = compute_metrics(eval_task, preds, label_ids)\n        logger.info(f""task:,{eval_task}"")\n        logger.info(f""result: {result}"")\n        results.update(result)\n\n    output_eval_file = os.path.join(eval_output_dir, ""eval_results-%s.txt"" % eval_task)\n    with open(output_eval_file, ""a"") as writer:\n        logger.info(""***** Eval results {} task {} *****"".format(step, eval_task))\n        writer.write(""step: %d ****\\n "" % step)\n        for key in sorted(results.keys()):\n            logger.info(""%s = %s"", key, str(results[key]))\n            writer.write(""%s = %s\\n"" % (key, str(results[key])))\n    model.train()\n    return results\n\ndef main():\n    #parse arguments\n    config.parse()\n    args = config.args\n    for k,v in vars(args).items():\n        logger.info(f""{k}:{v}"")\n    #set seeds\n    torch.manual_seed(args.random_seed)\n    torch.cuda.manual_seed_all(args.random_seed)\n    np.random.seed(args.random_seed)\n    random.seed(args.random_seed)\n\n    #arguments check\n    device, n_gpu = args_check(args)\n    os.makedirs(args.output_dir, exist_ok=True)\n    forward_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n    args.forward_batch_size = forward_batch_size\n\n    #load bert config\n    bert_config_T = BertConfig.from_json_file(args.bert_config_file_T)\n    bert_config_S = BertConfig.from_json_file(args.bert_config_file_S)\n    assert args.max_seq_length <= bert_config_T.max_position_embeddings\n    assert args.max_seq_length <= bert_config_S.max_position_embeddings\n\n    #Prepare GLUE task\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n\n    #read data\n    train_dataset = None\n    eval_datasets  = None\n    num_train_steps = None\n    tokenizer = BertTokenizer(vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        if args.aux_task_name:\n            aux_train_dataset = load_and_cache_examples(args, args.aux_task_name, tokenizer, evaluate=False, is_aux=True)\n            train_dataset = torch.utils.data.ConcatDataset([train_dataset, aux_train_dataset])\n        num_train_steps = int(len(train_dataset)/args.train_batch_size) * args.num_train_epochs\n    if args.do_predict:\n        eval_datasets = []\n        eval_task_names = (""mnli"", ""mnli-mm"") if args.task_name == ""mnli"" else (args.task_name,)\n        for eval_task in eval_task_names:\n            eval_datasets.append(load_and_cache_examples(args, eval_task, tokenizer, evaluate=True))\n    logger.info(""Data loaded"")\n\n\n    #Build Model and load checkpoint\n    model_T = BertForGLUESimple(bert_config_T, num_labels=num_labels,args=args)\n    model_S = BertForGLUESimple(bert_config_S, num_labels=num_labels,args=args)\n    #Load teacher\n    if args.tuned_checkpoint_T is not None:\n        state_dict_T = torch.load(args.tuned_checkpoint_T, map_location=\'cpu\')\n        model_T.load_state_dict(state_dict_T)\n        model_T.eval()\n    else:\n        assert args.do_predict is True\n    #Load student\n    if args.load_model_type==\'bert\':\n        assert args.init_checkpoint_S is not None\n        state_dict_S = torch.load(args.init_checkpoint_S, map_location=\'cpu\')\n        if args.only_load_embedding:\n            state_weight = {k[5:]:v for k,v in state_dict_S.items() if k.startswith(\'bert.embeddings\')}\n            missing_keys,_ = model_S.bert.load_state_dict(state_weight,strict=False)\n            logger.info(f""Missing keys {list(missing_keys)}"")\n        else:\n            state_weight = {k[5:]:v for k,v in state_dict_S.items() if k.startswith(\'bert.\')}\n            missing_keys,_ = model_S.bert.load_state_dict(state_weight,strict=False)\n            assert len(missing_keys)==0\n        logger.info(""Model loaded"")\n    elif args.load_model_type==\'all\':\n        assert args.tuned_checkpoint_S is not None\n        state_dict_S = torch.load(args.tuned_checkpoint_S,map_location=\'cpu\')\n        model_S.load_state_dict(state_dict_S)\n        logger.info(""Model loaded"")\n    else:\n        logger.info(""Model is randomly initialized."")\n    model_T.to(device)\n    model_S.to(device)\n\n    if args.local_rank != -1 or n_gpu > 1:\n        if args.local_rank != -1:\n            raise NotImplementedError\n        elif n_gpu > 1:\n            model_T = torch.nn.DataParallel(model_T) #,output_device=n_gpu-1)\n            model_S = torch.nn.DataParallel(model_S) #,output_device=n_gpu-1)\n\n    if args.do_train:\n        #parameters\n        params = list(model_S.named_parameters())\n        all_trainable_params = divide_parameters(params, lr=args.learning_rate)\n        logger.info(""Length of all_trainable_params: %d"", len(all_trainable_params))\n\n        optimizer = BERTAdam(all_trainable_params,lr=args.learning_rate,\n                             warmup=args.warmup_proportion,t_total=num_train_steps,schedule=args.schedule,\n                             s_opt1=args.s_opt1, s_opt2=args.s_opt2, s_opt3=args.s_opt3)\n\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num examples = %d"", len(train_dataset))\n        logger.info(""  Forward batch size = %d"", forward_batch_size)\n        logger.info(""  Num backward steps = %d"", num_train_steps)\n\n        ########### DISTILLATION ###########\n        train_config = TrainingConfig(\n            gradient_accumulation_steps = args.gradient_accumulation_steps,\n            ckpt_frequency = args.ckpt_frequency,\n            log_dir = args.output_dir,\n            output_dir = args.output_dir,\n            device = args.device)\n\n        from matches import matches\n        intermediate_matches = None\n        if isinstance(args.matches,(list,tuple)):\n            intermediate_matches = []\n            for match in args.matches:\n                intermediate_matches += matches[match]\n        logger.info(f""{intermediate_matches}"")\n        distill_config = DistillationConfig(\n            temperature = args.temperature,\n            intermediate_matches=intermediate_matches)\n\n        logger.info(f""{train_config}"")\n        logger.info(f""{distill_config}"")\n        adaptor_T = partial(BertForGLUESimpleAdaptor, no_logits=args.no_logits, no_mask = args.no_inputs_mask)\n        adaptor_S = partial(BertForGLUESimpleAdaptor, no_logits=args.no_logits, no_mask = args.no_inputs_mask)\n\n        distiller = GeneralDistiller(train_config = train_config,\n                                   distill_config = distill_config,\n                                   model_T = model_T, model_S = model_S,\n                                   adaptor_T = adaptor_T,\n                                   adaptor_S = adaptor_S)\n\n        if args.local_rank == -1:\n            train_sampler = RandomSampler(train_dataset)\n        else:\n            raise NotImplementedError\n        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.forward_batch_size,drop_last=True)\n        callback_func = partial(predict, eval_datasets=eval_datasets, args=args)\n        with distiller:\n            distiller.train(optimizer, scheduler=None, dataloader=train_dataloader,\n                              num_epochs=args.num_train_epochs, callback=callback_func)\n\n    if not args.do_train and args.do_predict:\n        res = predict(model_S,eval_datasets,step=0,args=args)\n        print (res)\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/mnli_example/main.multiteacher.py,14,"b'import logging\nlogger = logging.getLogger(""Main"")\nlogger.setLevel(logging.INFO)\nhandler_stream = logging.StreamHandler()\nhandler_stream.setLevel(logging.INFO)\nformatter = logging.Formatter(fmt=\'%(asctime)s - %(levelname)s - %(name)s -  %(message)s\', datefmt=\'%Y/%m/%d %H:%M:%S\')\nhandler_stream.setFormatter(formatter)\nlogger.addHandler(handler_stream)\n\nimport os,random\nimport numpy as np\nimport torch\nfrom utils_glue import output_modes, processors\nfrom pytorch_pretrained_bert.my_modeling import BertConfig\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom optimization import BERTAdam\nimport config\nfrom utils import divide_parameters, load_and_cache_examples\nfrom modeling import BertForGLUESimple,BertForGLUESimpleAdaptor\n\nfrom textbrewer import DistillationConfig, TrainingConfig, MultiTeacherDistiller\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, DistributedSampler\nfrom tqdm import tqdm\nfrom utils_glue import compute_metrics\nfrom functools import partial\n\n\ndef args_check(args):\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n        logger.warning(""Output directory () already exists and is not empty."")\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                            args.gradient_accumulation_steps))\n    if not args.do_train and not args.do_predict:\n        raise ValueError(""At least one of `do_train` or `do_predict` must be True."")\n\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        n_gpu = torch.cuda.device_count() if not args.no_cuda else 0\n    else:\n        device = torch.device(""cuda"", args.local_rank)\n        n_gpu = 1\n        torch.distributed.init_process_group(backend=\'nccl\')\n    logger.info(""device %s n_gpu %d distributed training %r"", device, n_gpu, bool(args.local_rank != -1))\n    args.n_gpu = n_gpu\n    args.device = device\n    return device, n_gpu\n\ndef predict(model,eval_datasets,step,args):\n    eval_task_names = (""mnli"", ""mnli-mm"") if args.task_name == ""mnli"" else (args.task_name,)\n    eval_output_dir = args.output_dir\n    results = {}\n    for eval_task,eval_dataset in zip(eval_task_names, eval_datasets):\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n        logger.info(""Predicting..."")\n        logger.info(""***** Running predictions *****"")\n        logger.info("" task name = %s"", eval_task)\n        logger.info(""  Num  examples = %d"", len(eval_dataset))\n        logger.info(""  Batch size = %d"", args.predict_batch_size)\n        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.predict_batch_size)\n        model.eval()\n\n        pred_logits = []\n        label_ids = []\n        for batch in tqdm(eval_dataloader, desc=""Evaluating"", disable=None):\n            input_ids, input_mask, segment_ids, labels = batch\n            input_ids = input_ids.to(args.device)\n            input_mask = input_mask.to(args.device)\n            segment_ids = segment_ids.to(args.device)\n            with torch.no_grad():\n                logits = model(input_ids, input_mask, segment_ids)\n            cpu_logits = logits.detach().cpu()\n            for i in range(len(cpu_logits)):\n                pred_logits.append(cpu_logits[i].numpy())\n                label_ids.append(labels[i])\n\n        pred_logits = np.array(pred_logits)\n        label_ids   = np.array(label_ids)\n\n        if args.output_mode == ""classification"":\n            preds = np.argmax(pred_logits, axis=1)\n        else: # args.output_mode == ""regression"":\n            preds = np.squeeze(pred_logits)\n        result = compute_metrics(eval_task, preds, label_ids)\n        logger.info(f""task:,{eval_task}"")\n        logger.info(f""result: {result}"")\n        results.update(result)\n\n    output_eval_file = os.path.join(eval_output_dir, ""eval_results-%s.txt"" % eval_task)\n    with open(output_eval_file, ""a"") as writer:\n        logger.info(""***** Eval results {} task {} *****"".format(step, eval_task))\n        writer.write(""step: %d ****\\n "" % step)\n        for key in sorted(results.keys()):\n            logger.info(""%s = %s"", key, str(results[key]))\n            writer.write(""%s = %s\\n"" % (key, str(results[key])))\n    model.train()\n    return results\n\ndef main():\n    #parse arguments\n    config.parse()\n    args = config.args\n    for k,v in vars(args).items():\n        logger.info(f""{k}:{v}"")\n    #set seeds\n    torch.manual_seed(args.random_seed)\n    torch.cuda.manual_seed_all(args.random_seed)\n    np.random.seed(args.random_seed)\n    random.seed(args.random_seed)\n\n    #arguments check\n    device, n_gpu = args_check(args)\n    os.makedirs(args.output_dir, exist_ok=True)\n    forward_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n    args.forward_batch_size = forward_batch_size\n\n    #load bert config\n    bert_config_T = BertConfig.from_json_file(args.bert_config_file_T)\n    bert_config_S = BertConfig.from_json_file(args.bert_config_file_S)\n    assert args.max_seq_length <= bert_config_T.max_position_embeddings\n    assert args.max_seq_length <= bert_config_S.max_position_embeddings\n\n    #Prepare GLUE task\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n\n    #read data\n    train_dataset = None\n    eval_datasets  = None\n    num_train_steps = None\n    tokenizer = BertTokenizer(vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        if args.aux_task_name:\n            aux_train_dataset = load_and_cache_examples(args, args.aux_task_name, tokenizer, evaluate=False, is_aux=True)\n            train_dataset = torch.utils.data.ConcatDataset([train_dataset, aux_train_dataset])\n        num_train_steps = int(len(train_dataset)/args.train_batch_size) * args.num_train_epochs\n    if args.do_predict:\n        eval_datasets = []\n        eval_task_names = (""mnli"", ""mnli-mm"") if args.task_name == ""mnli"" else (args.task_name,)\n        for eval_task in eval_task_names:\n            eval_datasets.append(load_and_cache_examples(args, eval_task, tokenizer, evaluate=True))\n    logger.info(""Data loaded"")\n\n\n    #Build Model and load checkpoint\n\n    model_S = BertForGLUESimple(bert_config_S, num_labels=num_labels,args=args)\n    #Load teacher\n    if args.tuned_checkpoint_Ts:\n        model_Ts = [BertForGLUESimple(bert_config_T, num_labels=num_labels,args=args) for i in range(len(args.tuned_checkpoint_Ts))]\n        for model_T, ckpt_T in zip(model_Ts,args.tuned_checkpoint_Ts):\n            logger.info(""Load state dict %s"" % ckpt_T)\n            state_dict_T = torch.load(ckpt_T, map_location=\'cpu\')\n            model_T.load_state_dict(state_dict_T)\n            model_T.eval()\n    else:\n        assert args.do_predict is True\n    #Load student\n    if args.load_model_type==\'bert\':\n        assert args.init_checkpoint_S is not None\n        state_dict_S = torch.load(args.init_checkpoint_S, map_location=\'cpu\')\n        state_weight = {k[5:]:v for k,v in state_dict_S.items() if k.startswith(\'bert.\')}\n        missing_keys,_ = model_S.bert.load_state_dict(state_weight,strict=False)\n        assert len(missing_keys)==0\n    elif args.load_model_type==\'all\':\n        assert args.tuned_checkpoint_S is not None\n        state_dict_S = torch.load(args.tuned_checkpoint_S,map_location=\'cpu\')\n        model_S.load_state_dict(state_dict_S)\n    else:\n        logger.info(""Model is randomly initialized."")\n    if args.do_train:\n        for model_T in model_Ts:\n            model_T.to(device)\n    model_S.to(device)\n\n    if args.local_rank != -1 or n_gpu > 1:\n        if args.local_rank != -1:\n            raise NotImplementedError\n        elif n_gpu > 1:\n            if args.do_train:\n                model_Ts = [torch.nn.DataParallel(model_T) for model_T in model_Ts]\n            model_S = torch.nn.DataParallel(model_S) #,output_device=n_gpu-1)\n\n    if args.do_train:\n        #parameters\n        params = list(model_S.named_parameters())\n        all_trainable_params = divide_parameters(params, lr=args.learning_rate)\n        logger.info(""Length of all_trainable_params: %d"", len(all_trainable_params))\n\n        optimizer = BERTAdam(all_trainable_params,lr=args.learning_rate,\n                             warmup=args.warmup_proportion,t_total=num_train_steps,schedule=args.schedule,\n                             s_opt1=args.s_opt1, s_opt2=args.s_opt2, s_opt3=args.s_opt3)\n\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num examples = %d"", len(train_dataset))\n        logger.info(""  Forward batch size = %d"", forward_batch_size)\n        logger.info(""  Num backward steps = %d"", num_train_steps)\n\n        ########### DISTILLATION ###########\n        train_config = TrainingConfig(\n            gradient_accumulation_steps = args.gradient_accumulation_steps,\n            ckpt_frequency = args.ckpt_frequency,\n            log_dir = args.output_dir,\n            output_dir = args.output_dir,\n            device = args.device)\n\n        distill_config = DistillationConfig(\n            temperature = args.temperature,\n            kd_loss_type = \'ce\')\n\n        logger.info(f""{train_config}"")\n        logger.info(f""{distill_config}"")\n        adaptor = partial(BertForGLUESimpleAdaptor, no_logits=False, no_mask = False)\n\n\n        distiller = MultiTeacherDistiller(train_config = train_config,\n                            distill_config = distill_config,\n                            model_T = model_Ts, model_S = model_S,\n                            adaptor_T=adaptor,\n                            adaptor_S=adaptor)\n        if args.local_rank == -1:\n            train_sampler = RandomSampler(train_dataset)\n        else:\n            raise NotImplementedError\n        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.forward_batch_size,drop_last=True)\n        callback_func = partial(predict, eval_datasets=eval_datasets, args=args)\n        with distiller:\n            distiller.train(optimizer, scheduler=None, dataloader=train_dataloader,\n                              num_epochs=args.num_train_epochs, callback=callback_func)\n\n    if not args.do_train and args.do_predict:\n        res = predict(model_S,eval_datasets,step=0,args=args)\n        print (res)\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/mnli_example/main.trainer.py,12,"b'import logging\nlogger = logging.getLogger(""Main"")\nlogger.setLevel(logging.INFO)\nhandler_stream = logging.StreamHandler()\nhandler_stream.setLevel(logging.INFO)\nformatter = logging.Formatter(fmt=\'%(asctime)s - %(levelname)s - %(name)s -  %(message)s\', datefmt=\'%Y/%m/%d %H:%M:%S\')\nhandler_stream.setFormatter(formatter)\nlogger.addHandler(handler_stream)\n\nimport os,random\nimport numpy as np\nimport torch\nfrom utils_glue import output_modes, processors\nfrom pytorch_pretrained_bert.my_modeling import BertConfig\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom optimization import BERTAdam\nimport config\nfrom utils import divide_parameters, load_and_cache_examples\nfrom modeling import BertForGLUESimple,BertForGLUESimpleAdaptorTraining\n\nfrom textbrewer import DistillationConfig, TrainingConfig, BasicTrainer\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, DistributedSampler\nfrom tqdm import tqdm\nfrom utils_glue import compute_metrics\nfrom functools import partial\n\n\ndef args_check(args):\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n        logger.warning(""Output directory () already exists and is not empty."")\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                            args.gradient_accumulation_steps))\n    if not args.do_train and not args.do_predict:\n        raise ValueError(""At least one of `do_train` or `do_predict` must be True."")\n\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        n_gpu = torch.cuda.device_count() if not args.no_cuda else 0\n    else:\n        device = torch.device(""cuda"", args.local_rank)\n        n_gpu = 1\n        torch.distributed.init_process_group(backend=\'nccl\')\n    logger.info(""device %s n_gpu %d distributed training %r"", device, n_gpu, bool(args.local_rank != -1))\n    args.n_gpu = n_gpu\n    args.device = device\n    return device, n_gpu\n\ndef predict(model,eval_datasets,step,args):\n    eval_task_names = (""mnli"", ""mnli-mm"") if args.task_name == ""mnli"" else (args.task_name,)\n    eval_output_dir = args.output_dir\n    results = {}\n    for eval_task,eval_dataset in zip(eval_task_names, eval_datasets):\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n        logger.info(""Predicting..."")\n        logger.info(""***** Running predictions *****"")\n        logger.info("" task name = %s"", eval_task)\n        logger.info(""  Num  examples = %d"", len(eval_dataset))\n        logger.info(""  Batch size = %d"", args.predict_batch_size)\n        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.predict_batch_size)\n        model.eval()\n\n        pred_logits = []\n        label_ids = []\n        for batch in tqdm(eval_dataloader, desc=""Evaluating"", disable=None):\n            input_ids, input_mask, segment_ids, labels = batch\n            input_ids = input_ids.to(args.device)\n            input_mask = input_mask.to(args.device)\n            segment_ids = segment_ids.to(args.device)\n            with torch.no_grad():\n                logits = model(input_ids, input_mask, segment_ids)\n            cpu_logits = logits.detach().cpu()\n            for i in range(len(cpu_logits)):\n                pred_logits.append(cpu_logits[i].numpy())\n                label_ids.append(labels[i])\n\n        pred_logits = np.array(pred_logits)\n        label_ids   = np.array(label_ids)\n\n        if args.output_mode == ""classification"":\n            preds = np.argmax(pred_logits, axis=1)\n        else: # args.output_mode == ""regression"":\n            preds = np.squeeze(pred_logits)\n        result = compute_metrics(eval_task, preds, label_ids)\n        logger.info(f""task:,{eval_task}"")\n        logger.info(f""result: {result}"")\n        results.update(result)\n\n    output_eval_file = os.path.join(eval_output_dir, ""eval_results-%s.txt"" % eval_task)\n    with open(output_eval_file, ""a"") as writer:\n        logger.info(""***** Eval results {} task {} *****"".format(step, eval_task))\n        writer.write(""step: %d ****\\n "" % step)\n        for key in sorted(results.keys()):\n            logger.info(""%s = %s"", key, str(results[key]))\n            writer.write(""%s = %s\\n"" % (key, str(results[key])))\n    model.train()\n    return results\n\ndef main():\n    #parse arguments\n    config.parse()\n    args = config.args\n    for k,v in vars(args).items():\n        logger.info(f""{k}:{v}"")\n    #set seeds\n    torch.manual_seed(args.random_seed)\n    torch.cuda.manual_seed_all(args.random_seed)\n    np.random.seed(args.random_seed)\n    random.seed(args.random_seed)\n\n    #arguments check\n    device, n_gpu = args_check(args)\n    os.makedirs(args.output_dir, exist_ok=True)\n    forward_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n    args.forward_batch_size = forward_batch_size\n\n    #load bert config\n    bert_config_S = BertConfig.from_json_file(args.bert_config_file_S)\n    assert args.max_seq_length <= bert_config_S.max_position_embeddings\n\n    #Prepare GLUE task\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n\n    #read data\n    train_dataset = None\n    eval_datasets  = None\n    num_train_steps = None\n    tokenizer = BertTokenizer(vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        if args.aux_task_name:\n            aux_train_dataset = load_and_cache_examples(args, args.aux_task_name, tokenizer, evaluate=False, is_aux=True)\n            train_dataset = torch.utils.data.ConcatDataset([train_dataset, aux_train_dataset])\n        num_train_steps = int(len(train_dataset)/args.train_batch_size) * args.num_train_epochs\n    if args.do_predict:\n        eval_datasets = []\n        eval_task_names = (""mnli"", ""mnli-mm"") if args.task_name == ""mnli"" else (args.task_name,)\n        for eval_task in eval_task_names:\n            eval_datasets.append(load_and_cache_examples(args, eval_task, tokenizer, evaluate=True))\n    logger.info(""Data loaded"")\n\n\n    #Build Model and load checkpoint\n    model_S = BertForGLUESimple(bert_config_S, num_labels=num_labels,args=args)\n    #Load student\n    if args.load_model_type==\'bert\':\n        assert args.init_checkpoint_S is not None\n        state_dict_S = torch.load(args.init_checkpoint_S, map_location=\'cpu\')\n        if args.only_load_embedding:\n            state_weight = {k[5:]:v for k,v in state_dict_S.items() if k.startswith(\'bert.embeddings\')}\n            missing_keys,_ = model_S.bert.load_state_dict(state_weight,strict=False)\n            logger.info(f""Missing keys {list(missing_keys)}"")\n        else:\n            state_weight = {k[5:]:v for k,v in state_dict_S.items() if k.startswith(\'bert.\')}\n            missing_keys,_ = model_S.bert.load_state_dict(state_weight,strict=False)\n            assert len(missing_keys)==0\n        logger.info(""Model loaded"")\n    elif args.load_model_type==\'all\':\n        assert args.tuned_checkpoint_S is not None\n        state_dict_S = torch.load(args.tuned_checkpoint_S,map_location=\'cpu\')\n        model_S.load_state_dict(state_dict_S)\n        logger.info(""Model loaded"")\n    else:\n        logger.info(""Model is randomly initialized."")\n    model_S.to(device)\n\n    if args.local_rank != -1 or n_gpu > 1:\n        if args.local_rank != -1:\n            raise NotImplementedError\n        elif n_gpu > 1:\n            model_S = torch.nn.DataParallel(model_S) #,output_device=n_gpu-1)\n\n    if args.do_train:\n        #parameters\n        params = list(model_S.named_parameters())\n        all_trainable_params = divide_parameters(params, lr=args.learning_rate)\n        logger.info(""Length of all_trainable_params: %d"", len(all_trainable_params))\n\n        optimizer = BERTAdam(all_trainable_params,lr=args.learning_rate,\n                             warmup=args.warmup_proportion,t_total=num_train_steps,schedule=args.schedule,\n                             s_opt1=args.s_opt1, s_opt2=args.s_opt2, s_opt3=args.s_opt3)\n\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num examples = %d"", len(train_dataset))\n        logger.info(""  Forward batch size = %d"", forward_batch_size)\n        logger.info(""  Num backward steps = %d"", num_train_steps)\n\n        ########### DISTILLATION ###########\n        train_config = TrainingConfig(\n            gradient_accumulation_steps = args.gradient_accumulation_steps,\n            ckpt_frequency = args.ckpt_frequency,\n            log_dir = args.output_dir,\n            output_dir = args.output_dir,\n            device = args.device)\n\n\n        distiller = BasicTrainer(train_config = train_config,\n                                 model = model_S,\n                                 adaptor = BertForGLUESimpleAdaptorTraining)\n\n        if args.local_rank == -1:\n            train_sampler = RandomSampler(train_dataset)\n        else:\n            raise NotImplementedError\n        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.forward_batch_size,drop_last=True)\n        callback_func = partial(predict, eval_datasets=eval_datasets, args=args)\n        with distiller:\n            distiller.train(optimizer, scheduler=None, dataloader=train_dataloader,\n                              num_epochs=args.num_train_epochs, callback=callback_func)\n\n    if not args.do_train and args.do_predict:\n        res = predict(model_S,eval_datasets,step=0,args=args)\n        print (res)\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/mnli_example/matches.py,0,"b'L3_attention_mse=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1}]\n\nL3_attention_ce=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1}]\n\nL3_attention_mse_sum=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1}]\n\nL3_attention_ce_mean=[{""layer_T"":4,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":8,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1}]\n\nL3_hidden_smmd=[{""layer_T"":[0,0],  ""layer_S"":[0,0], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[4,4],  ""layer_S"":[1,1], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[8,8],  ""layer_S"":[2,2], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[12,12],""layer_S"":[3,3], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1}]\n\nL3n_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]},\n                {""layer_T"":4, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]},\n                {""layer_T"":8, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]},\n                {""layer_T"":12,""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",384,768]}]\n\nL3_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1},\n               {""layer_T"":4, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1},\n               {""layer_T"":8, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1},\n               {""layer_T"":12,""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}]\n\n#######################L4################\nL4_attention_mse=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1},\n                  {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_mse"", ""weight"":1}]\n\nL4_attention_ce=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1},\n                 {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_ce"", ""weight"":1}]\n\nL4_attention_mse_sum=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_mse_sum"", ""weight"":1}]\n\nL4_attention_ce_mean=[{""layer_T"":3,  ""layer_S"":1, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":6,  ""layer_S"":2, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":9,  ""layer_S"":3, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1},\n                      {""layer_T"":12, ""layer_S"":4, ""feature"":""attention"", ""loss"":""attention_ce_mean"", ""weight"":1}]\n\nL4_hidden_smmd=[{""layer_T"":[0,0],  ""layer_S"":[0,0], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[3,3],  ""layer_S"":[1,1], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[6,6],  ""layer_S"":[2,2], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[9,9],  ""layer_S"":[3,3], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[12,12],""layer_S"":[4,4], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1}]\n\nL4t_hidden_sgram=[{""layer_T"":[0,0],  ""layer_S"":[0,0], ""feature"":""hidden"", ""loss"":""gram"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                  {""layer_T"":[3,3],  ""layer_S"":[1,1], ""feature"":""hidden"", ""loss"":""gram"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                  {""layer_T"":[6,6],  ""layer_S"":[2,2], ""feature"":""hidden"", ""loss"":""gram"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                  {""layer_T"":[9,9],  ""layer_S"":[3,3], ""feature"":""hidden"", ""loss"":""gram"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                  {""layer_T"":[12,12],""layer_S"":[4,4], ""feature"":""hidden"", ""loss"":""gram"", ""weight"":1, ""proj"":[""linear"",312,768]}]\n\nL4t_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":3, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":6, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":9, ""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]},\n                {""layer_T"":12,""layer_S"":4, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1, ""proj"":[""linear"",312,768]}]\n\n###########L6#############\nL6_hidden_smmd=[{""layer_T"":[0,0],  ""layer_S"":[0,0], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[2,2],  ""layer_S"":[1,1], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[4,4],  ""layer_S"":[2,2], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[6,6],  ""layer_S"":[3,3], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[8,8],  ""layer_S"":[4,4], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[10,10],""layer_S"":[5,5], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1},\n                {""layer_T"":[12,12],""layer_S"":[6,6], ""feature"":""hidden"", ""loss"":""mmd"", ""weight"":1}]\n\nL6_hidden_mse=[{""layer_T"":0, ""layer_S"":0, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":2, ""layer_S"":1, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":4, ""layer_S"":2, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":6, ""layer_S"":3, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":8, ""layer_S"":4, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":10,""layer_S"":5, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}, \n               {""layer_T"":12,""layer_S"":6, ""feature"":""hidden"", ""loss"":""hidden_mse"", ""weight"":1}]\n\nmatches={\'L3_attention_mse\':L3_attention_mse,\'L3_attention_mse_sum\':L3_attention_mse_sum,\n         \'L3_attention_ce\' :L3_attention_ce, \'L3_attention_ce_mean\':L3_attention_ce_mean,\n         \'L3n_hidden_mse\'  :L3n_hidden_mse,  \'L3_hidden_smmd\'      :L3_hidden_smmd,       \'L3_hidden_mse\': L3_hidden_mse,\n         \'L4_attention_mse\':L4_attention_mse,\'L4_attention_mse_sum\':L4_attention_mse_sum,\n         \'L4_attention_ce\' :L4_attention_ce, \'L4_attention_ce_mean\':L4_attention_ce_mean,\n         \'L4t_hidden_mse\'  :L4t_hidden_mse,  \'L4_hidden_smmd\'      :L4_hidden_smmd,       \'L4t_hidden_sgram\': L4t_hidden_sgram,\n         \'L6_hidden_mse\'   :L6_hidden_mse,   \'L6_hidden_smmd\'      :L6_hidden_smmd\n        }\n'"
examples/mnli_example/modeling.py,3,"b""import logging\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\nimport torch.nn.functional as F\nimport config as Conf\nfrom pytorch_pretrained_bert.my_modeling import BertModel, BertLayerNorm\nlogger = logging.getLogger(__name__)\n\n\ndef initializer_builder(std):\n    _std = std\n    def init_bert_weights(module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=_std)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n    return init_bert_weights\n\n\nclass BertForGLUE(nn.Module):\n    def __init__(self, config, num_labels):\n        super(BertForGLUE, self).__init__()\n        self.num_labels = num_labels\n        output_sum = None if Conf.args.output_sum < 0 else Conf.args.output_sum\n        self.bert = BertModel(config,Conf.args.output_score, output_sum)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        initializer = initializer_builder(config.initializer_range)\n        self.apply(initializer)\n\n    def forward(self, input_ids, token_type_ids, attention_mask, labels=None, logits_T=None,\n                attention_probs_sum_layer=None, attention_probs_sum_T=None, hidden_match_layer=None, hidden_match_T=None):\n        if hidden_match_layer is not None:\n            sequence_output, pooled_output, attention_probs_sum = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=True,  output_attention_layer=attention_probs_sum_layer)\n            hidden_states = [sequence_output[i] for i in hidden_match_layer]\n            sequence_output = sequence_output[-1]\n        else:\n            sequence_output, pooled_output, attention_probs_sum = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False,  output_attention_layer=attention_probs_sum_layer)\n            hidden_states = []\n\n\n        output_for_cls = self.dropout(pooled_output)\n        logits  = self.classifier(output_for_cls)  # output size: batch_size,num_labels\n\n        if logits_T is not None or labels is not None or attention_probs_sum_T is not None:\n            total_loss = 0\n            hidden_losses = None\n            att_losses = None\n            if logits_T is not None and self.num_labels!=1:\n                temp=Conf.args.temperature\n                logits_T /= temp\n                logits /= temp\n                prob_T = F.softmax(logits_T,dim=-1)\n                ce_loss = -(prob_T * F.log_softmax(logits, dim=-1)).sum(dim=-1)\n                ce_loss = ce_loss.mean() #* temp_2\n                total_loss += ce_loss\n            if attention_probs_sum_T:\n                if Conf.args.mask_inter==1:\n                    mask = attention_mask.to(attention_probs_sum[0])\n                    valid_count = torch.pow(mask.sum(dim=1),2).sum()\n                    att_losses = [(F.mse_loss(attention_probs_sum[i], attention_probs_sum_T[i], reduction='none') * mask.unsqueeze(-1) * mask.unsqueeze(1)).sum() / valid_count for i in range(len(attention_probs_sum_T))]\n                else:\n                    att_losses = [F.mse_loss(attention_probs_sum[i], attention_probs_sum_T[i]) for i in range(len(attention_probs_sum_T))]\n                att_loss = sum(att_losses) * Conf.args.att_loss_weight\n                total_loss += att_loss\n                #mle_loss = (F.mse_loss(start_logits,start_logits_T) + F.mse_loss(end_logits,end_logits_T))/2\n                #total_loss += mle_loss\n            if hidden_match_T:\n                if Conf.args.mask_inter==1:\n                    mask = attention_mask.to(hidden_states[0])\n                    valid_count = mask.sum() * hidden_states[0].size(-1)\n                    hidden_losses = [(F.mse_loss(hidden_states[i],hidden_match_T[i], reduction='none')*mask.unsqueeze(-1)).sum() / valid_count for i in range(len(hidden_match_layer))]\n                else:\n                    hidden_losses = [F.mse_loss(hidden_states[i],hidden_match_T[i]) for i in range(len(hidden_match_layer))]\n                hidden_loss = sum(hidden_losses) * Conf.args.hidden_loss_weight\n                total_loss += hidden_loss\n\n            if labels is not None:\n                if self.num_labels == 1:\n                    loss = F.mse_loss(logits.view(-1), labels.view(-1))\n                else:\n                    loss = F.cross_entropy(logits,labels)\n                total_loss += loss\n            return total_loss, att_losses, hidden_losses\n        else:\n            if attention_probs_sum_layer is not None or hidden_match_layer is not None:\n                return logits, attention_probs_sum, hidden_states\n            else:\n                return logits, None\n\nclass BertForGLUESimple(nn.Module):\n    def __init__(self, config, num_labels, args):\n        super(BertForGLUESimple, self).__init__()\n        self.num_labels = num_labels\n        self.output_encoded_layers   = (args.output_encoded_layers=='true')\n        self.output_attention_layers = (args.output_attention_layers=='true')\n        self.bert = BertModel(config, output_score=(args.output_att_score=='true'), output_sum=(args.output_att_sum=='true'))\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        initializer = initializer_builder(config.initializer_range)\n        self.apply(initializer)\n\n    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n        sequence_output, pooled_output, attention_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                                                    output_all_encoded_layers=(self.output_encoded_layers),\n                                                                    output_all_attention_layers=(self.output_attention_layers))\n        output_for_cls = self.dropout(pooled_output)\n        logits  = self.classifier(output_for_cls)  # output size: batch_size,num_labels\n        #assert len(sequence_output)==self.bert.config.num_hidden_layers + 1  # embeddings + 12 hiddens\n        #assert len(attention_output)==self.bert.config.num_hidden_layers + 1 # None + 12 attentions\n        if labels is not None:\n            if self.num_labels == 1:\n                loss = F.mse_loss(logits.view(-1), labels.view(-1))\n            else:\n                loss = F.cross_entropy(logits,labels)\n            return logits, sequence_output, attention_output, loss\n        else:\n            return logits\n\n\n\ndef BertForGLUESimpleAdaptor(batch, model_outputs, no_logits, no_mask):\n    dict_obj = {'hidden': model_outputs[1], 'attention': model_outputs[2]}\n    if no_mask is False:\n        dict_obj['inputs_mask'] = batch[1]\n    if no_logits is False:\n        dict_obj['logits'] = (model_outputs[0],)\n    return dict_obj\n\ndef BertForGLUESimpleAdaptorTraining(batch, model_outputs):\n    return {'losses':(model_outputs[3],)}\n"""
examples/mnli_example/optimization.py,7,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch optimization for BERT model.""""""\n\nimport math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.nn.utils import clip_grad_norm_\n\ndef warmup_cosine(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 0.5 * (1.0 + torch.cos(math.pi * x))\n\ndef warmup_constant(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 1.0\n\ndef warmup_linear(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 1.0 - x\n\ndef warmup_linear_release(x, warmup=0.002,opt1=1, opt2=0, opt3=1):  #opt1:release_start  opt2:release_level\n    if x < warmup:\n        return x/warmup\n    if x < opt1:\n        return 1-(1-opt2)/opt1* x\n    return opt2\n\ndef warmup_step(x, warmup=0.002, opt1=1,opt2=0,opt3=1): # opt1:step1 opt2:step2 opt3:decay\n    if x < warmup:\n        return x/warmup\n    if x < opt1:\n        return 1.\n    if x < opt2:\n        return opt3;\n    return opt3*opt3\n\ndef slanted_triangular(x, warmup=0.1, opt1=1,opt2=0,opt3=1): # opt1:ratio\n    if x < warmup:\n        p = x/warmup\n    else:\n        p=1-(x-warmup)/(1-warmup)\n    return (1 + p*(opt1-1))/opt1\n\nSCHEDULES = {\n    \'warmup_cosine\':warmup_cosine,\n    \'warmup_constant\':warmup_constant,\n    \'warmup_linear\':warmup_linear,\n    \'warmup_linear_release\':warmup_linear_release,\n    \'warmup_step\':warmup_step,\n    \'slanted_triangular\':slanted_triangular\n}\n\n\nclass BERTAdam(Optimizer):\n    """"""Implements BERT version of Adam algorithm with weight decay fix (and no ).\n    Params:\n        lr: learning rate\n        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n        t_total: total number of training steps for the learning\n            rate schedule, -1  means constant learning rate. Default: -1\n        schedule: schedule to use for the warmup (see above). Default: \'warmup_linear\'\n        b1: Adams b1. Default: 0.9\n        b2: Adams b2. Default: 0.999\n        e: Adams epsilon. Default: 1e-6\n        weight_decay_rate: Weight decay. Default: 0.01\n        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n    """"""\n    def __init__(self, params, lr, warmup=-1, t_total=-1, schedule=\'warmup_linear_release\',\n                 s_opt1=1, s_opt2=0,s_opt3=1,\n                 b1=0.9, b2=0.999, e=1e-6, weight_decay_rate=0.01,\n                 max_grad_norm=1.0):\n        if not lr >= 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if schedule not in SCHEDULES:\n            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))\n        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n            raise ValueError(""Invalid warmup: {} - should be in [0.0, 1.0[ or -1"".format(warmup))\n        if not 0.0 <= b1 < 1.0:\n            raise ValueError(""Invalid b1 parameter: {} - should be in [0.0, 1.0["".format(b1))\n        if not 0.0 <= b2 < 1.0:\n            raise ValueError(""Invalid b2 parameter: {} - should be in [0.0, 1.0["".format(b2))\n        if not e >= 0.0:\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(e))\n\n        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n                    b1=b1, b2=b2, e=e, weight_decay_rate=weight_decay_rate,\n                    max_grad_norm=max_grad_norm,opt1=s_opt1, opt2=s_opt2,opt3=s_opt3)\n\n        super(BERTAdam, self).__init__(params, defaults)\n\n    def get_lr(self):\n        lr = []\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                if len(state) == 0:\n                    return [0]\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n                lr.append(lr_scheduled)\n        return lr\n    \'\'\'\n    def to(self, device):\n        """""" Move the optimizer state to a specified device""""""\n        for state in self.state.values():\n            state[\'exp_avg\'].to(device)\n            state[\'exp_avg_sq\'].to(device)\n\n    def initialize_step(self, initial_step):\n        """"""Initialize state with a defined step (but we don\'t have stored averaged).\n        Arguments:\n            initial_step (int): Initial step number.\n        """"""\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                # State initialization\n                state[\'step\'] = initial_step\n                # Exponential moving average of gradient values\n                state[\'exp_avg\'] = torch.zeros_like(p.data)\n                # Exponential moving average of squared gradient values\n                state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n    \'\'\'\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'next_m\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'next_v\'] = torch.zeros_like(p.data)\n\n                next_m, next_v = state[\'next_m\'], state[\'next_v\']\n                beta1, beta2 = group[\'b1\'], group[\'b2\']\n\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                next_m.mul_(beta1).add_(1 - beta1, grad)\n                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                update = next_m / (next_v.sqrt() + group[\'e\'])\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want ot decay the weights in a manner that doesn\'t interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                if group[\'weight_decay_rate\'] > 0.0:\n                    update += group[\'weight_decay_rate\'] * p.data\n\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'],group[\'opt1\'],group[\'opt2\'],group[\'opt3\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n\n                update_with_lr = lr_scheduled * update\n                p.data.add_(-update_with_lr)\n\n                state[\'step\'] += 1\n\n                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n                # bias_correction1 = 1 - beta1 ** state[\'step\']\n                # bias_correction2 = 1 - beta2 ** state[\'step\']\n\n        return loss\n'"
examples/mnli_example/utils.py,8,"b'import pickle\nimport os\nimport config\nimport logging\nlogger = logging.getLogger(__name__)\nimport torch\nfrom torch.utils.data import TensorDataset\nfrom utils_glue import processors, output_modes, convert_examples_to_features\n\ndef read_and_convert(fn,is_training,read_fn,convert_fn):\n    data_dirname, data_basename = os.path.split(fn)\n    cased = \'\' if config.args.do_lower_case else \'cased\'\n    if config.args.max_seq_length != 416:\n        data_pklname = data_basename + \'%s%d_l%d_cHA.pkl\' % (cased,config.args.doc_stride,config.args.max_seq_length)\n    else:\n        data_pklname = data_basename + \'%s%d_cHA.pkl\' % (cased,config.args.doc_stride)\n    full_pklname = os.path.join(data_dirname,data_pklname)\n    if os.path.exists(full_pklname):\n        print(""Loading dataset %s "" % data_pklname)\n        with open(full_pklname,\'rb\') as f:\n            examples,features = pickle.load(f)\n    else:\n        print (""Building dataset %s "" % data_pklname)\n        examples = read_fn(input_file=fn,is_training=is_training)\n        print (""Size: "",len(examples))\n        features = convert_fn(examples=examples,is_training=is_training)\n        try:\n            with open(full_pklname,\'wb\') as f:\n                pickle.dump((examples,features),f)\n        except:\n            print (""Can\'t save train data file."")\n    return examples,features\n\ndef load_and_cache_examples(args, task, tokenizer, evaluate=False, is_aux=False):\n    if is_aux:\n        data_dir = args.aux_data_dir\n    else:\n        data_dir = args.data_dir\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(data_dir, \'cached_{}_{}_{}\'.format(\n        \'dev\' if evaluate else \'train\',\n        str(args.max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file):\n        logger.info(""Loading features from cached file %s"", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(""Creating features from dataset file at %s"", data_dir)\n        label_list = processor.get_labels()\n        examples = processor.get_dev_examples(data_dir) if evaluate else processor.get_train_examples(data_dir)\n        features = convert_examples_to_features(examples, label_list, args.max_seq_length, tokenizer, output_mode,\n                                                cls_token_segment_id=0,pad_token_segment_id=0)\n        if args.local_rank in [-1, 0]:\n            logger.info(""Saving features into cached file %s"", cached_features_file)\n            torch.save(features, cached_features_file)\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    if output_mode == ""classification"":\n        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n    elif output_mode == ""regression"":\n        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n    return dataset\n\n\ndef divide_parameters(named_parameters,lr=None):\n    no_decay = [\'bias\', \'LayerNorm.bias\',\'LayerNorm.weight\']\n    decay_parameters_names = list(zip(*[(p,n) for n,p in named_parameters if not any((di in n) for di in no_decay)]))\n    no_decay_parameters_names = list(zip(*[(p,n) for n,p in named_parameters if any((di in n) for di in no_decay)]))\n    param_group = []\n    if len(decay_parameters_names)>0:\n        decay_parameters, decay_names = decay_parameters_names\n        #print (""decay:"",decay_names)\n        if lr is not None:\n            decay_group = {\'params\':decay_parameters,   \'weight_decay_rate\': config.args.weight_decay_rate, \'lr\':lr}\n        else:\n            decay_group = {\'params\': decay_parameters, \'weight_decay_rate\': config.args.weight_decay_rate}\n        param_group.append(decay_group)\n\n    if len(no_decay_parameters_names)>0:\n        no_decay_parameters, no_decay_names = no_decay_parameters_names\n        #print (""no decay:"", no_decay_names)\n        if lr is not None:\n            no_decay_group = {\'params\': no_decay_parameters, \'weight_decay_rate\': 0.0, \'lr\': lr}\n        else:\n            no_decay_group = {\'params\': no_decay_parameters, \'weight_decay_rate\': 0.0}\n        param_group.append(no_decay_group)\n\n    assert len(param_group)>0\n    return param_group\n'"
examples/mnli_example/utils_glue.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" BERT classification fine-tuning: utilities to work with GLUE tasks """"""\n\nimport csv\nimport logging\nimport os\nimport sys\nfrom io import open\n\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.metrics import matthews_corrcoef, f1_score\n\nlogger = logging.getLogger(__name__)\n\n\nclass InputExample(object):\n    """"""A single training/test example for simple sequence classification.""""""\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        """"""Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        """"""\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\nclass DataProcessor(object):\n    """"""Base class for data converters for sequence classification data sets.""""""\n\n    def get_train_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the train set.""""""\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\n        raise NotImplementedError()\n\n    def get_labels(self):\n        """"""Gets the list of labels for this data set.""""""\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        """"""Reads a tab separated value file.""""""\n        with open(input_file, ""r"", encoding=""utf-8-sig"") as f:\n            reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                lines.append(line)\n            return lines\n\n\nclass MrpcProcessor(DataProcessor):\n    """"""Processor for the MRPC data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        logger.info(""LOOKING AT {}"".format(os.path.join(data_dir, ""train.tsv"")))\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[3]\n            text_b = line[4]\n            label = line[0]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass MnliProcessor(DataProcessor):\n    """"""Processor for the MultiNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev_matched.tsv"")),\n            ""dev_matched"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""contradiction"", ""entailment"", ""neutral""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[8]\n            text_b = line[9]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass MnliMismatchedProcessor(MnliProcessor):\n    """"""Processor for the MultiNLI Mismatched data set (GLUE version).""""""\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev_mismatched.tsv"")),\n            ""dev_matched"")\n\n\nclass ColaProcessor(DataProcessor):\n    """"""Processor for the CoLA data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[3]\n            label = line[1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n\n\nclass Sst2Processor(DataProcessor):\n    """"""Processor for the SST-2 data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[0]\n            label = line[1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n\n\nclass StsbProcessor(DataProcessor):\n    """"""Processor for the STS-B data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [None]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[7]\n            text_b = line[8]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass QqpProcessor(DataProcessor):\n    """"""Processor for the QQP data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            try:\n                text_a = line[3]\n                text_b = line[4]\n                label = line[5]\n            except IndexError:\n                continue\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass QnliProcessor(DataProcessor):\n    """"""Processor for the QNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")),\n            ""dev_matched"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""entailment"", ""not_entailment""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass RteProcessor(DataProcessor):\n    """"""Processor for the RTE data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""entailment"", ""not_entailment""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass WnliProcessor(DataProcessor):\n    """"""Processor for the WNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\ndef convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer, output_mode,\n                                 cls_token_at_end=False, pad_on_left=False,\n                                 cls_token=\'[CLS]\', sep_token=\'[SEP]\', pad_token=0,\n                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n                                 cls_token_segment_id=1, pad_token_segment_id=0,\n                                 mask_padding_with_zero=True):\n    """""" Loads a data file into a list of `InputBatch`s\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n    """"""\n\n    label_map = {label : i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(""Writing example %d of %d"" % (ex_index, len(examples)))\n\n        tokens_a = tokenizer.tokenize(example.text_a)\n\n        tokens_b = None\n        if example.text_b:\n            tokens_b = tokenizer.tokenize(example.text_b)\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with ""- 3""\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP] with ""- 2""\n            if len(tokens_a) > max_seq_length - 2:\n                tokens_a = tokens_a[:(max_seq_length - 2)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids:   0   0   0   0  0     0   0\n        #\n        # Where ""type_ids"" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambiguously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the ""sentence vector"". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens = tokens_a + [sep_token]\n        segment_ids = [sequence_a_segment_id] * len(tokens)\n\n        if tokens_b:\n            tokens += tokens_b + [sep_token]\n            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n\n        if cls_token_at_end:\n            tokens = tokens + [cls_token]\n            segment_ids = segment_ids + [cls_token_segment_id]\n        else:\n            tokens = [cls_token] + tokens\n            segment_ids = [cls_token_segment_id] + segment_ids\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        padding_length = max_seq_length - len(input_ids)\n        if pad_on_left:\n            input_ids = ([pad_token] * padding_length) + input_ids\n            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n        else:\n            input_ids = input_ids + ([pad_token] * padding_length)\n            input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n            segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        if output_mode == ""classification"":\n            label_id = label_map[example.label]\n        elif output_mode == ""regression"":\n            label_id = float(example.label)\n        else:\n            raise KeyError(output_mode)\n\n        if ex_index < 5:\n            logger.info(""*** Example ***"")\n            logger.info(""guid: %s"" % (example.guid))\n            logger.info(""tokens: %s"" % "" "".join(\n                    [str(x) for x in tokens]))\n            logger.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n            logger.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n            logger.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n            logger.info(""label: %s (id = %d)"" % (example.label, label_id))\n\n        features.append(\n                InputFeatures(input_ids=input_ids,\n                              input_mask=input_mask,\n                              segment_ids=segment_ids,\n                              label_id=label_id))\n    return features\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    """"""Truncates a sequence pair in place to the maximum length.""""""\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that\'s truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\ndef simple_accuracy(preds, labels):\n    return (preds == labels).mean()\n\n\ndef acc_and_f1(preds, labels):\n    acc = simple_accuracy(preds, labels)\n    f1 = f1_score(y_true=labels, y_pred=preds)\n    return {\n        ""acc"": acc,\n        ""f1"": f1,\n        ""acc_and_f1"": (acc + f1) / 2,\n    }\n\n\ndef pearson_and_spearman(preds, labels):\n    pearson_corr = pearsonr(preds, labels)[0]\n    spearman_corr = spearmanr(preds, labels)[0]\n    return {\n        ""pearson"": pearson_corr,\n        ""spearmanr"": spearman_corr,\n        ""corr"": (pearson_corr + spearman_corr) / 2,\n    }\n\n\ndef compute_metrics(task_name, preds, labels):\n    assert len(preds) == len(labels)\n    if task_name == ""cola"":\n        return {""mcc"": matthews_corrcoef(labels, preds)}\n    elif task_name == ""sst-2"":\n        return {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""mrpc"":\n        return acc_and_f1(preds, labels)\n    elif task_name == ""sts-b"":\n        return pearson_and_spearman(preds, labels)\n    elif task_name == ""qqp"":\n        return acc_and_f1(preds, labels)\n    elif task_name == ""mnli"":\n        return {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""mnli-mm"":\n        return {""mm-acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""qnli"":\n        return {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""rte"":\n        return {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""wnli"":\n        return {""acc"": simple_accuracy(preds, labels)}\n    else:\n        raise KeyError(task_name)\n\nprocessors = {\n    ""cola"": ColaProcessor,\n    ""mnli"": MnliProcessor,\n    ""mnli-mm"": MnliMismatchedProcessor,\n    ""mrpc"": MrpcProcessor,\n    ""sst-2"": Sst2Processor,\n    ""sts-b"": StsbProcessor,\n    ""qqp"": QqpProcessor,\n    ""qnli"": QnliProcessor,\n    ""rte"": RteProcessor,\n    ""wnli"": WnliProcessor,\n}\n\noutput_modes = {\n    ""cola"": ""classification"",\n    ""mnli"": ""classification"",\n    ""mnli-mm"": ""classification"",\n    ""mrpc"": ""classification"",\n    ""sst-2"": ""classification"",\n    ""sts-b"": ""regression"",\n    ""qqp"": ""classification"",\n    ""qnli"": ""classification"",\n    ""rte"": ""classification"",\n    ""wnli"": ""classification"",\n}\n\nGLUE_TASKS_NUM_LABELS = {\n    ""cola"": 2,\n    ""mnli"": 3,\n    ""mrpc"": 2,\n    ""sst-2"": 2,\n    ""sts-b"": 1,\n    ""qqp"": 2,\n    ""qnli"": 2,\n    ""rte"": 2,\n    ""wnli"": 2,\n}\n'"
examples/random_tokens_example/distill.py,6,"b'import textbrewer\nfrom textbrewer import GeneralDistiller\nfrom textbrewer import TrainingConfig, DistillationConfig\nfrom transformers import BertForSequenceClassification, BertConfig, AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport torch\nfrom torch.utils.data import Dataset,DataLoader\nimport numpy as np\n\n#device\ndevice = torch.device(\'cpu\')\n\n# Define models\nbert_config = BertConfig.from_json_file(\'bert_config/bert_config.json\')\nbert_config_T3 = BertConfig.from_json_file(\'bert_config/bert_config_T3.json\')\n\nbert_config.output_hidden_states = True\nbert_config_T3.output_hidden_states = True\n\n\nteacher_model = BertForSequenceClassification(bert_config) #, num_labels = 2\n# Teacher should be initialized with pre-trained weights and fine-tuned on the downstream task.\n# For the demonstration purpose, we omit these steps here\n\nstudent_model = BertForSequenceClassification(bert_config_T3) #, num_labels = 2\n\nteacher_model.to(device=device)\nstudent_model.to(device=device)\n\n# Define Dict Dataset\nclass DictDataset(Dataset):\n    def __init__(self, all_input_ids, all_attention_mask, all_labels):\n        assert len(all_input_ids)==len(all_attention_mask)==len(all_labels)\n        self.all_input_ids = all_input_ids\n        self.all_attention_mask = all_attention_mask\n        self.all_labels = all_labels\n\n    def __getitem__(self, index):\n        return {\'input_ids\': self.all_input_ids[index],\n                \'attention_mask\': self.all_attention_mask[index],\n                \'labels\': self.all_labels[index]}\n    \n    def __len__(self):\n        return self.all_input_ids.size(0)\n\n# Prepare random data\nall_input_ids = torch.randint(low=0,high=100,size=(100,128))  # 100 examples of length 128\nall_attention_mask = torch.ones_like(all_input_ids)\nall_labels = torch.randint(low=0,high=2,size=(100,))\ndataset = DictDataset(all_input_ids, all_attention_mask, all_labels)\neval_dataset = DictDataset(all_input_ids, all_attention_mask, all_labels)\ndataloader = DataLoader(dataset,batch_size=32)\nnum_epochs = 10\nnum_training_steps = len(dataloader) * num_epochs\n# Optimizer and learning rate scheduler\noptimizer = AdamW(student_model.parameters(), lr=1e-4)\n\nscheduler_class = get_linear_schedule_with_warmup\n# arguments dict except \'optimizer\'\nscheduler_args = {\'num_warmup_steps\':int(0.1*num_training_steps), \'num_training_steps\':num_training_steps}\n\n\n# display model parameters statistics\nprint(""\\nteacher_model\'s parametrers:"")\nresult, _ = textbrewer.utils.display_parameters(teacher_model,max_level=3)\nprint (result)\n\nprint(""student_model\'s parametrers:"")\nresult, _ = textbrewer.utils.display_parameters(student_model,max_level=3)\nprint (result)\n\ndef simple_adaptor(batch, model_outputs):\n    # The second element of model_outputs is the logits before softmax\n    # The third element of model_outputs is hidden states\n    return {\'logits\': model_outputs[1],\n            \'hidden\': model_outputs[2],\n            \'inputs_mask\': batch[\'attention_mask\']}\n\n\n#Define callback function\ndef predict(model, eval_dataset, step, device):\n    \'\'\'\n    eval_dataset: \xe9\xaa\x8c\xe8\xaf\x81\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    \'\'\'\n    model.eval()\n    pred_logits = []\n    label_ids =[]\n    dataloader = DataLoader(eval_dataset,batch_size=32)\n    for batch in dataloader:\n        input_ids = batch[\'input_ids\'].to(device)\n        attention_mask = batch[\'attention_mask\'].to(device)\n        labels = batch[\'labels\']\n        with torch.no_grad():\n            logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n            cpu_logits = logits.detach().cpu()\n        for i in range(len(cpu_logits)):\n            pred_logits.append(cpu_logits[i].numpy())\n            label_ids.append(labels[i])\n    model.train()\n    pred_logits = np.array(pred_logits)\n    label_ids = np.array(label_ids)\n    y_p = pred_logits.argmax(axis=-1)\n    accuracy = (y_p==label_ids).sum()/len(label_ids)\n    print (""Number of examples: "",len(y_p))\n    print (""Acc: "", accuracy)\nfrom functools import partial\ncallback_fun = partial(predict, eval_dataset=eval_dataset, device=device) # fill other arguments\n\n\n\n# Initialize configurations and distiller\ntrain_config = TrainingConfig(device=device)\ndistill_config = DistillationConfig(\n    temperature=8,\n    hard_label_weight=0,\n    kd_loss_type=\'ce\',\n    probability_shift=False,\n    intermediate_matches=[\n        {\'layer_T\':0, \'layer_S\':0, \'feature\':\'hidden\', \'loss\': \'hidden_mse\',\'weight\' : 1},\n        {\'layer_T\':8, \'layer_S\':2, \'feature\':\'hidden\', \'loss\': \'hidden_mse\', \'weight\' : 1},\n        {\'layer_T\':[0,0], \'layer_S\':[0,0], \'feature\':\'hidden\', \'loss\': \'nst\', \'weight\': 1},\n        {\'layer_T\':[8,8], \'layer_S\':[2,2], \'feature\':\'hidden\', \'loss\': \'nst\', \'weight\': 1}]\n)\n\nprint (""train_config:"")\nprint (train_config)\n\nprint (""distill_config:"")\nprint (distill_config)\n\ndistiller = GeneralDistiller(\n    train_config=train_config, distill_config = distill_config,\n    model_T = teacher_model, model_S = student_model, \n    adaptor_T = simple_adaptor, adaptor_S = simple_adaptor)\n\n# Start distilling\nwith distiller:\n    distiller.train(optimizer,dataloader, num_epochs=num_epochs, \n    scheduler_class=scheduler_class, scheduler_args = scheduler_args, callback=callback_fun)\n'"
src/textbrewer/__init__.py,0,"b'__version__ = ""0.1.9""\n\nfrom .distillers import BasicTrainer\nfrom .distillers import BasicDistiller\nfrom .distillers import GeneralDistiller\nfrom .distillers import MultiTaskDistiller\nfrom .distillers import MultiTeacherDistiller\n\n\nfrom .configurations import TrainingConfig, DistillationConfig\n\nfrom .presets import FEATURES\nfrom .presets import ADAPTOR_KEYS\nfrom .presets import KD_LOSS_MAP, MATCH_LOSS_MAP, PROJ_MAP\nfrom .presets import WEIGHT_SCHEDULER, TEMPERATURE_SCHEDULER\nfrom .presets import register_new\n\nDistillers = {\n    \'Basic\': BasicDistiller,\n    \'General\': GeneralDistiller,\n    \'MultiTeacher\': MultiTeacherDistiller,\n    \'MultiTask\': MultiTaskDistiller,\n    \'Train\': BasicTrainer\n}\n'"
src/textbrewer/compatibility.py,3,"b""import torch\n\nif torch.__version__ < '1.2':\n    mask_dtype = torch.uint8\nelse:\n    mask_dtype = torch.bool"""
src/textbrewer/configurations.py,1,"b'import json\nimport os\nfrom typing import Union, List, Optional, Dict\nfrom .presets import *\n\nclass Config:\n    """"""Base class for TrainingConfig and DistillationConfig.""""""\n    def __init__(self,**kwargs):\n        pass\n\n    @classmethod\n    def from_json_file(cls, json_filename):\n        """"""Construct configurations from a json file.""""""\n        with open(json_filename,\'r\') as f:\n            json_data = json.load(f)\n        return cls.from_dict(json_data)\n\n    @classmethod\n    def from_dict(cls, dict_object):\n        """"""Construct configurations from a dict.""""""\n        config = cls(**dict_object)\n        return config\n\n    def __str__(self):\n        str = """"\n        for k,v in self.__dict__.items():\n            str += f""{k} : {v}\\n""\n        return str\n\n    def __repr__(self):\n        classname = self.__class__.__name__\n        return classname +"":\\n""+self.__str__()\n\n\nclass TrainingConfig(Config):\n    """"""\n    Configurations related to general model training.\n\n    Args:\n        gradient_accumulation_steps (int): accumulates gradients before optimization to reduce GPU memory usage. It calls ``optimizer.step()`` every `gradient_accumulation_steps` backward steps.\n        ckpt_frequency (int): stores model weights `ckpt_frequency` times every epoch.\n        ckpt_epoch_frequency (int): stores model weights every `ckpt_epoch_frequency` epochs.\n        ckpt_steps (int):  if *num_steps* is passes to ``distiller.train()``, saves the model every **ckpt_steps**, meanwhile ignore `ckpt_frequency` and `ckpt_epoch_frequency` .\n        log_dir (str): directory to save the tensorboard log file. Set it to ``None`` to disable tensorboard.\n        output_dir (str): directory to save model weights.\n        device (str or torch.device) : training on CPU or GPU.\n\n    Example::\n\n        # Usually just need to set log_dir and output_dir and leave others default\n        train_config = TrainingConfig(log_dir=my_log_dir, output_dir=my_output_dir)\n        \n        # Stores model at the end of each epoch\n        train_config = TrainingConfig(ckpt_frequency=1, ckpt_epoch_frequency=1)\n        # Stores model twice (at the middle and at the end) in each epoch\n        train_config = TrainingConfig(ckpt_frequency=2, ckpt_epoch_frequency=1)\n        # Stores model once every two epochs\n        train_config = TrainingConfig(ckpt_frequency=1, ckpt_epoch_frequency=2)\n\n    """"""\n    def __init__(self,gradient_accumulation_steps = 1,\n                 ckpt_frequency = 1,\n                 ckpt_epoch_frequency = 1,\n                 ckpt_steps = None,\n                 log_dir = None,\n                 output_dir = \'./saved_models\',\n                 device = \'cuda\'\n                 ):\n        super(TrainingConfig, self).__init__()\n\n        self.gradient_accumulation_steps =gradient_accumulation_steps\n        self.ckpt_frequency = ckpt_frequency\n        self.ckpt_epoch_frequency = ckpt_epoch_frequency\n        self.ckpt_steps = ckpt_steps\n        self.log_dir = log_dir\n        self.output_dir = output_dir\n        self.device = device\n\n        if not os.path.exists(self.output_dir):\n            os.makedirs(self.output_dir)\n\n\nclass IntermediateMatch:\n    def __init__(self,layer_T: Union[int,List[int]], layer_S: Union[int,List[int]],\n                 weight: float, loss: str, feature: str, proj: Optional[List] = None):\n        self.layer_T = layer_T\n        self.layer_S = layer_S\n        self.feature = feature\n        self.weight = weight\n        self.loss = loss\n        self.proj = proj\n        assert feature in FEATURES\n        if proj:\n            assert proj[0] in PROJ_MAP.keys()\n            assert type(proj[1]) is int and type(proj[2]) is int\n            if len(proj)==3:\n                self.proj.append(dict())   # [\'linear\', dim_T, dim_S, {...}]\n            else:\n                assert type(proj[3]) is dict\n\n    def __str__(self):\n        str = """"\n        for k,v in self.__dict__.items():\n            str += f""{k} : {v}, ""\n        return str[:-2]\n\n    def __repr__(self):\n        classname = self.__class__.__name__\n        return \'\\n\'+classname +"": ""+self.__str__()\n\n    @classmethod\n    def from_dict(cls,dict_object):\n        if dict_object is None:\n            return None\n        else:\n            return cls(**dict_object)\n\n\nclass DistillationConfig(Config):\n    """"""\n    Configurations related to distillation methods. It defines the total loss to be optimized:\n    \n    .. math::\n\n        \\mathcal{L}_{total}=  \\mathcal{L}_{KD} * w_{KD} + \\mathcal{L}_{hl} * w_{hl} + sum(\\\\textrm{intermediate_losses})\n    \n    where\n\n        * :math:`\\mathcal{L}_{KD}` is the KD loss on logits, :math:`w_{KD}` is its weight;\n        * :math:`\\mathcal{L}_{hl}` is the sum of ``losses`` returned by the adaptor and :math:`w_{hl}` is its weight;\n        * intermediate_losses is defined via `intermediate_matches`.\n    \n    Args:\n        temperature (float) :temperature for the distillation. The teacher and student models\' logits will be divided by the temperature in computing the KD loss. The temperature typicially ranges from 1 to 10. We found that temperature higher than 1 usually leads to better performance.\n        temperature_scheduler: dynamically adjusts temperature. See :data:`~textbrewer.presets.TEMPERATURE_SCHEDULER` for all available options.\n        kd_loss_type (str): KD loss function for the ``logits`` term returned by the adaptor, can be ``\'ce\'`` or ``\'mse\'``. See :data:`~textbrewer.presets.KD_LOSS_MAP`.\n        kd_loss_weight (float): the weight for the KD loss.\n        hard_label_weight (float): the weight for the sum of ``losses`` term returned by the adaptor. ``losses`` may include the losses on the ground-truth labels and other user-defined losses. \n        kd_loss_weight_scheduler: Dynamically adjusts KD loss weight. See :data:`~textbrewer.presets.WEIGHT_SCHEDULER` for all available options.\n        hard_label_weight_scheduler: Dynamically adjusts the weight of the sum of ``losses``. See :data:`~textbrewer.presets.WEIGHT_SCHEDULER` for all available options.\n        probability_shift (bool): if ``True``, switch the ground-truth label\'s logit and the largest logit predicted by the teacher, to make the ground-truth label\'s logit largest. Requires ``labels`` term returned by the adaptor.\n        is_caching_logits (bool): if ``True``, caches the batches and the output logits of the teacher model in memory, so that those logits will only be calcuated once. It will speed up the distillation process. This feature is **only available** for :class:`~textbrewer.BasicDistiller` and :class:`~textbrewer.MultiTeacherDistiller`, and only when distillers\' ``train()`` method is called with ``num_steps=None``. It is suitable for small and medium datasets.\n        intermediate_matches (`List[Dict]`) : Configuration for intermediate feature matching. Each element in the list is a dict, representing a pair of matching config. \n    \n    The dict in `intermediate_matches` contains the following keys:\n\n        * \'**layer_T**\': `layer_T` (*int*): selects the layer_T-th layer of teacher model.\n        * \'**layer_S**\': `layer_S` (*int*): selects the layer_S-th layer of student model.\n\n        .. Note::\n        \n            1. `layer_T` and `layer_S` indicate layers in ``attention`` or ``hidden`` list in the returned dict of the adaptor, rather than the actual layers in the model. \n            2. If the loss is :func:`fst <textbrewer.losses.fsp_loss>` or :func:`nst <textbrewer.losses.mmd_loss>`, two layers have to be chosen from the teacher and the student respectively. In this case, `layer_T` and `layer_S` are lists of two ints. See the example below.\n\n        * \'**feature**\': `feature` (*str*): features of intermediate layers. It can be:\n\n            * \'**attention**\' : attention matrix, of the shape (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)\n            * \'**hidden**\'\xef\xbc\x9ahidden states, of the shape (*batch_size*, *length*, *hidden_dim*).\n\n        * \'**loss**\' : `loss` (*str*) : loss function. See :data:`~textbrewer.presets.MATCH_LOSS_MAP` for available losses. Currently includes: ``\'attention_mse\'``, ``\'attention_ce\'``, ``\'hidden_mse\'``, ``\'nst\'``, etc.\n        * \'**weight**\': `weight` (float) : weight for the loss.\n        * \'**proj**\' : `proj` (*List*, optional) : if the teacher and the student have the same feature dimension, it is optional; otherwise it is required. It is the mapping function to match teacher and student intermediate feature dimension. It is a list, with these elements:\n\n            * **proj[0]** (*str*): mapping function, can be ``\'linear\'``, ``\'relu\'``, ``\'tanh\'``. See :data:`textbrewer.presets.PROJ_MAP`.\n            * **proj[1]** (*int*): feature dimension of student model.\n            * **proj[2]** (*int*): feature dimension of teacher model.\n            * **proj[3]** (*dict*): optional, provides configurations such as learning rate. If not provided, the learning rate and optimizer configurations will follow the default config of the optimizer, otherwise it will use the ones specified here.\n\n    Example::\n\n        from textbrewer import DistillationConfig\n\n        # simple configuration: use default values, or try different temperatures \n        distill_config = DistillationConfig(temperature=8)\n\n        # adding intermediate feature matching\n        # under this setting, the returned dict results_T/S of adaptor_T/S should contain \'hidden\' key.\n        # The mse loss between teacher\'s results_T[\'hidden\'][10] and student\'s results_S[\'hidden\'][3] will be computed\n        distill_config = DistillationConfig(\n            temperature=8,\n            intermediate_matches = [{\'layer_T\':10, \'layer_S\':3, \'feature\':\'hidden\', \'loss\':\'hidden_mse\', \'weight\':1}]\n        )\n\n        # multiple inatermediate feature matching. The teacher and the student have a  hidden_dim of 768 and 384 respectively. \n        distill_config = DistillationConfig(\n            temperature = 8, \n            intermediate_matches = [ \\\\\n            {\'layer_T\':0,  \'layer_S\':0, \'feature\':\'hidden\',\'loss\': \'hidden_mse\', \'weight\' : 1,\'proj\':[\'linear\',384,768]},\n            {\'layer_T\':4,  \'layer_S\':1, \'feature\':\'hidden\',\'loss\': \'hidden_mse\', \'weight\' : 1,\'proj\':[\'linear\',384,768]},\n            {\'layer_T\':8,  \'layer_S\':2, \'feature\':\'hidden\',\'loss\': \'hidden_mse\', \'weight\' : 1,\'proj\':[\'linear\',384,768]},\n            {\'layer_T\':12, \'layer_S\':3, \'feature\':\'hidden\',\'loss\': \'hidden_mse\', \'weight\' : 1,\'proj\':[\'linear\',384,768]}]\n        )\n\n    """"""\n    def __init__(self,temperature=4,\n                      temperature_scheduler = \'none\',\n                      hard_label_weight=0,\n                      hard_label_weight_scheduler = \'none\',\n                      kd_loss_type=\'ce\',\n                      kd_loss_weight=1,\n                      kd_loss_weight_scheduler = \'none\',\n                      probability_shift = False,\n                      intermediate_matches:Optional[List[Dict]]=None,\n                      is_caching_logits = False):\n        super(DistillationConfig, self).__init__()\n\n        self.temperature = temperature\n        self.temperature_scheduler = None\n        if temperature_scheduler is not \'none\':\n            assert temperature_scheduler in TEMPERATURE_SCHEDULER, \\\n                    ""Invalid temperature_scheduler""\n            self.temperature_scheduler = TEMPERATURE_SCHEDULER[temperature_scheduler]\n\n        self.hard_label_weight = hard_label_weight\n        self.hard_label_weight_scheduler = None\n        if hard_label_weight_scheduler is not \'none\':\n            assert hard_label_weight_scheduler in WEIGHT_SCHEDULER, \\\n                    ""Invalid hard_label_weight_scheduler""\n            self.hard_label_weight_scheduler = WEIGHT_SCHEDULER[hard_label_weight_scheduler]\n\n        self.kd_loss_type = kd_loss_type\n        self.kd_loss_weight = kd_loss_weight\n        self.kd_loss_weight_scheduler = None\n        if kd_loss_weight_scheduler is not \'none\':\n            assert kd_loss_weight_scheduler in WEIGHT_SCHEDULER, \\\n                    ""Invalid kd_loss_weight_scheduler""\n            self.kd_loss_weight_scheduler = WEIGHT_SCHEDULER[kd_loss_weight_scheduler]\n\n        self.probability_shift = probability_shift\n\n        self.intermediate_matches:[List[IntermediateMatch]] = []\n        if intermediate_matches:\n            self.intermediate_matches = [IntermediateMatch.from_dict(im) for im in intermediate_matches]\n\n        self.is_caching_logits = is_caching_logits'"
src/textbrewer/data_utils.py,0,"b'import numpy as np\nimport random\n\ndef masking(tokens, p = 0.1, mask=\'[MASK]\'):\n    """"""\n    Returns a new list by replacing elements in `tokens` by `mask` with probability `p`.\n\n    Args:\n        tokens (list): list of tokens or token ids.\n        p (float): probability to mask each element in `tokens`.\n    Returns:\n        A new list by replacing elements in `tokens` by `mask` with probability `p`.\n    """"""\n    outputs = tokens[:]\n    for i in range(len(tokens)):\n        if np.random.rand() < p:\n            outputs[i] = mask \n    return outputs\n\ndef deleting(tokens, p = 0.1):\n    """"""\n    Returns a new list by deleting elements in `tokens` with probability `p`.\n\n    Args:\n        tokens (list): list of tokens or token ids.\n        p (float): probability to delete each element in `tokens`.\n    Retunrns:\n        a new list by deleting elements in :`tokens` with probability `p`.\n    """"""\n    choice = np.random.binomial(1,1-p,len(tokens))\n    outputs = [tokens[i] for i in range(len(tokens)) if choice[i]==1]\n    return outputs\n\n\ndef n_gram_sampling(tokens, \n                    p_ng = [0.2,0.2,0.2,0.2,0.2],\n                    l_ng = [1,2,3,4,5]):\n    """"""\n    Samples a length `l` from `l_ng` with probability distribution `p_ng`, then returns a random span of length `l` from `tokens`.\n\n    Args:\n        tokens (list): list of tokens or token ids.\n        p_ng (list): probability distribution of the n-grams, should sum to 1.\n        l_ng (list): specify the n-grams.\n    Returns:\n        a n-gram random span from `tokens`.\n    """"""\n    span_length = np.random.choice(l_ng,p= p_ng)\n    start_position = max(0,np.random.randint(0,len(tokens)-span_length+1))\n    n_gram_span = tokens[start_position:start_position+span_length]\n    return n_gram_span\n\n\ndef short_disorder(tokens, p = [0.9,0.1,0,0,0]):  # untouched + four cases abc, bac, cba, cab, bca\n    """"""\n    Returns a new list by disordering `tokens` with probability distribution `p` at every possible position. Let `abc` be a 3-gram in `tokens`, \n    there are five ways to disorder, corresponding to five probability values:\n\n        | abc -> abc\n        | abc -> bac\n        | abc -> cba\n        | abc -> cab\n        | abc -> bca\n    \n    Args:\n        tokens (list): list of tokens or token ids.\n        p (list): probability distribution of 5 disorder types, should sum to 1.\n    Returns:\n        a new disordered list\n    """"""\n    i = 0\n    outputs = tokens[:]\n    l = len(tokens)\n    while i < l-1:\n        permutation = np.random.choice([0,1,2,3,4],p=p)\n        if permutation!=0 and i==l-2:\n            outputs[i], outputs[i+1] = outputs[i+1], outputs[i]\n            i += 2\n        elif permutation==1:\n            outputs[i], outputs[i+1] = outputs[i+1], outputs[i]\n            i += 2\n        elif permutation==2:\n            outputs[i], outputs[i+2] = outputs[i+2], outputs[i]\n            i +=3\n        elif permutation==3:\n            outputs[i],outputs[i+1],outputs[i+2] = outputs[i+2],outputs[i],outputs[i+1]\n            i += 3\n        elif permutation==4:\n            outputs[i],outputs[i+1],outputs[i+2] = outputs[i+1],outputs[i+2],outputs[i]\n            i += 3\n        else:\n            i += 1\n    return outputs\n\ndef long_disorder(tokens,p = 0.1, length=20):\n    """"""\n    Performs a long-range disordering. If ``length>1``, then swaps the two halves of each span of length `length` in `tokens`; \n    if ``length<=1``, treats `length` as the relative length. For example::\n    \n        >>>long_disorder([0,1,2,3,4,5,6,7,8,9,10], p=1, length=0.4)\n        [2, 3, 0, 1, 6, 7, 4, 5, 8, 9]\n\n    Args:\n        tokens (list): list of tokens or token ids.\n        p (list): probability to swaps the two halves of a spans at possible positions.\n        length (int or float): length of the disordered span.\n    Returns:\n        a new disordered list\n    """"""\n    outputs = tokens[:]\n    if int(length) <= 1:\n        length = len(tokens)*length\n    length = (int(length)+1) //2 * 2\n    i = 0\n    while i<=len(outputs)-length:\n        if np.random.rand() < p:\n            outputs[i:i+length//2], outputs[i+length//2:i+length] = outputs[i+length//2:i+length], outputs[i:i+length//2]\n            i += length\n        else:\n            i += 1\n    return outputs'"
src/textbrewer/distillation.py,40,"b'import torch\nfrom collections import OrderedDict\nfrom tqdm import tqdm\nfrom torch import nn\ntry:\n    from tensorboardX import SummaryWriter\nexcept ImportError:\n    from torch.utils.tensorboard import SummaryWriter\nimport os, random, json\nimport numpy as np\nimport logging\nfrom typing import Optional, Dict, Union\nfrom .presets import *\nfrom .configurations import TrainingConfig, DistillationConfig\n\nfrom .compatibility import mask_dtype\n\nlogger = logging.getLogger(""Distillation"")\nlogger.setLevel(logging.INFO)\n\nhandler_stream = logging.StreamHandler()\nhandler_stream.setLevel(logging.INFO)\nformatter = logging.Formatter(fmt=\'%(asctime)s - %(levelname)s - %(name)s -  %(message)s\', datefmt=\'%Y/%m/%d %H:%M:%S\')\nhandler_stream.setFormatter(formatter)\nlogger.addHandler(handler_stream)\n\nclass CustomMatch:\n    def __init__(self, module_T, module_S, weight, loss,\n                 proj_func =None, proj_group = None):\n        self.module_T = module_T\n        self.module_S = module_S\n        self.loss     = loss,\n        self.weight   = weight,\n        self.proj_func     = proj_func\n        if proj_group is None:\n            self.proj_group = dict()\n        else:\n            self.proj_group = proj_group\n    def to_dict(self):\n        return {\'module_T\':self.module_T,\n                \'module_S\':self.module_S,\n                \'weight\':self.weight,\n                \'loss\':self.loss,\n                \'proj_func\':self.proj_func,\n                \'proj_group\':self.proj_group}\n    @classmethod\n    def from_dict(cls,dict_object):\n        return cls(**dict_object)\n\n\nclass DistillationContext:\n    def __init__(self):\n        self.model_S = None\n        self.model_T = None\n    def __enter__(self):\n        if isinstance(self.model_T,(list,tuple)):\n            self.model_T_is_training = [model_t.training for model_t in self.model_T]\n            for model_t in self.model_T:\n                model_t.eval()\n        elif isinstance(self.model_T,dict):\n            self.model_T_is_training = {name:model.training for name,model in self.model_T.items()}\n        else:\n            self.model_T_is_training = self.model_T.training\n            self.model_T.eval()\n\n        self.model_S_is_training = self.model_S.training\n        self.model_S.train()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        #Restore model status\n        if isinstance(self.model_T,(list,tuple)):\n            for i in range(len(self.model_T_is_training)):\n                self.model_T[i].train(self.model_T_is_training[i])\n        elif isinstance(self.model_T,dict):\n            for name,is_training  in self.model_T_is_training.items():\n                self.model_T[name].train(is_training)\n        else:\n            self.model_T.train(self.model_T_is_training)\n\n        self.model_S.train(self.model_S_is_training)\n\n\nclass AbstractDistiller(DistillationContext):\n    def __init__(self, train_config: TrainingConfig,\n                       distill_config: DistillationConfig,\n                       model_T, model_S, adaptor_T, adaptor_S):\n        super(AbstractDistiller, self).__init__()\n        self.t_config = train_config\n        self.d_config = distill_config\n\n        self.model_T = model_T\n        self.model_S = model_S\n        self.adaptor_S = adaptor_S\n        self.adaptor_T = adaptor_T\n\n        self.kd_loss = KD_LOSS_MAP[self.d_config.kd_loss_type]\n        if self.t_config.log_dir is not None:\n            self.tb_writer = SummaryWriter(log_dir = self.t_config.log_dir)\n        else:\n            self.tb_writer = no_op\n        \n        self.print_freq = 20\n\nclass BasicDistiller(AbstractDistiller):\n    """"""\n    Performs **single-teacher single-task** distillation, provides basic distillation strategies.\n\n    Args:\n        train_config (:class:`TrainingConfig`): training configuration.\n        distill_config (:class:`DistillationConfig`): distillation configuration.\n        model_T (:class:`torch.nn.Module`): teacher model.\n        model_S (:class:`torch.nn.Module`): student model.\n        adaptor_T (Callable): teacher model\'s adaptor.\n        adaptor_S (Callable): student model\'s adaptor.\n\n    The roles of `adaptor_T` and `adaptor_S` are explained in :py:func:`adaptor`.\n\n    """"""\n    def __init__(self, train_config,\n                       distill_config,\n                 model_T,\n                 model_S,\n                 adaptor_T,\n                 adaptor_S):\n        super(BasicDistiller, self).__init__(train_config, distill_config, model_T, model_S, adaptor_T, adaptor_S)\n\n    def save_and_callback(self,global_step, step, epoch, callback):\n        logger.info(f""Saving at global step {global_step}, epoch step {step + 1} epoch {epoch+1}"")\n        coreModel = self.model_S.module if \\\n            \'DataParallel\' in self.model_S.__class__.__name__ else self.model_S\n        state_dict = coreModel.state_dict()\n        torch.save(state_dict, os.path.join(self.t_config.output_dir, f""gs{global_step}.pkl""))\n        if callback is not None:\n            logger.info(""Running callback function..."")\n            callback(model=self.model_S, step=global_step)\n            self.model_S.train()\n\n    def write_loss(self, total_loss, writer_step):\n\n        cpu_total_loss = total_loss.cpu().item() * self.t_config.gradient_accumulation_steps\n        self.tb_writer.add_scalar(\'scalar/total_loss\', cpu_total_loss, writer_step)\n\n        #for name, loss in losses_dict.items():\n        #    cpu_loss = loss.cpu().item() * self.t_config.gradient_accumulation_steps\n        #    self.tb_writer.add_scalar(f""scalar/{name}"", cpu_loss, writer_step)\n\n\n    def train(self, optimizer, scheduler, dataloader, num_epochs, num_steps=None, callback=None, batch_postprocessor=None, **args):\n        """"""\n        trains the student model.\n\n        Args:\n            optimizer: optimizer.\n            scheduler: used to adjust learning rate, optional, can be None.\n            dataloader: dataset iterator.\n            num_epochs (int): number of training epochs.\n            num_steps (int): number of training steps. If it is not None, distiller will ignore `num_epochs` and trains for `num_steps`, and dataloader can have an unkonwn size, i.e., has no `__len__` attribute. Dataloader will be cycled automatically after iterating over the whole dataset.\n            callback (Callable): function called after each epoch, can be None. It is called as ``callback(model=self.model_S, step = global_step)``. It can be used to evaluate the model at each checkpoint.\n            batch_postprocessor (Callable): a function for post-processing batches. It should take a batch and return a batch. Its output is fed to the models and adaptors.\n            **args: additional arguments fed to the model.\n        Note:\n            * If the batch is a list or tuple, model is called as: ``model(*batch, **args)``. Make sure the order of elements in the batch matches their order in ``model.forward``.\n            * If the batch is a dict, model is called as: ``model(**batch,**args)``. Make sure the keys of the batch match the arguments of the ``model.forward``.\n\n        """"""\n        if num_steps is not None:\n            total_global_steps = num_steps\n            ckpt_steps =self.t_config.ckpt_steps\n            print_every = ckpt_steps // self.print_freq\n            if print_every == 0:\n                print_every = ckpt_steps\n            checkpoints = [ i * ckpt_steps for i in range(1,num_steps//ckpt_steps+1)] + [total_global_steps]\n            logger.info(f""Total training steps: {total_global_steps}"")\n            logger.info(f""Checkpoints(step): {checkpoints}"")\n\n            global_step = 0\n            writer_step = 0\n            for step, batch in tqdm(enumerate(cycle(dataloader)),disable=None):\n                if batch_postprocessor is not None:\n                    batch = batch_postprocessor(batch)\n                total_loss = self.train_on_batch(batch,args)\n                total_loss /= self.t_config.gradient_accumulation_steps\n                total_loss.backward()\n\n                self.write_loss(total_loss, writer_step)\n                writer_step += 1\n\n                if (step+1)%self.t_config.gradient_accumulation_steps == 0:\n                    optimizer.step()\n                    if scheduler is not None:\n                        scheduler.step()\n                    self.model_S.zero_grad()\n                    global_step += 1\n                    if self.d_config.kd_loss_weight_scheduler is not None:\n                        self.d_config.kd_loss_weight = \\\n                            self.d_config.kd_loss_weight_scheduler(global_step/total_global_steps)\n                    if self.d_config.hard_label_weight_scheduler is not None:\n                        self.d_config.hard_label_weight = \\\n                            self.d_config.hard_label_weight_scheduler(global_step/total_global_steps)\n\n                    if (global_step) % print_every == 0:\n                        logger.info(f""Global step: {global_step}, epoch step:{step+1}"")\n                    if (global_step%ckpt_steps==0) or global_step==total_global_steps:\n                        self.save_and_callback(global_step, step, 0, callback)\n            logger.info(""Training finished"")\n            return\n\n\n        train_steps_per_epoch = len(dataloader)//self.t_config.gradient_accumulation_steps\n        total_global_steps = train_steps_per_epoch * num_epochs\n        print_every = train_steps_per_epoch // self.print_freq\n        if print_every == 0:\n            print_every = train_steps_per_epoch\n        checkpoints = [int(train_steps_per_epoch*ci/self.t_config.ckpt_frequency) for ci in range(self.t_config.ckpt_frequency)]\n        logger.info(f""Training steps per epoch: {train_steps_per_epoch}"")\n        logger.info(f""Checkpoints(step): {checkpoints}"")\n\n        global_step = 0\n        writer_step = 0\n        for current_epoch in tqdm(range(int(num_epochs)),disable=None):\n            logger.info(f""Epoch {current_epoch+1}"")\n            self.model_S.zero_grad()\n            logger.info(f""Length of current epoch in forward batch: {len(dataloader)}"")\n            for step, batch in tqdm(enumerate(dataloader),disable=None):\n                if batch_postprocessor is not None:\n                    batch = batch_postprocessor(batch)\n                total_loss = self.train_on_batch(batch,args)\n                total_loss /= self.t_config.gradient_accumulation_steps\n                total_loss.backward()\n\n                self.write_loss(total_loss, writer_step)\n                writer_step += 1\n\n                if (step+1)%self.t_config.gradient_accumulation_steps == 0:\n                    optimizer.step()\n                    if scheduler is not None:\n                        scheduler.step()\n                    self.model_S.zero_grad()\n                    global_step += 1\n                    if self.d_config.kd_loss_weight_scheduler is not None:\n                        self.d_config.kd_loss_weight = \\\n                            self.d_config.kd_loss_weight_scheduler(global_step/total_global_steps)\n                    if self.d_config.hard_label_weight_scheduler is not None:\n                        self.d_config.hard_label_weight = \\\n                            self.d_config.hard_label_weight_scheduler(global_step/total_global_steps)\n\n                    if (global_step) % print_every == 0:\n                        logger.info(f""Global step: {global_step}, epoch step:{step+1}"")\n                    if (global_step%train_steps_per_epoch in checkpoints) \\\n                            and ((current_epoch+1)%self.t_config.ckpt_epoch_frequency==0 or current_epoch+1==num_epochs):\n                        self.save_and_callback(global_step, step, current_epoch, callback)\n\n            logger.info(f""Epoch {current_epoch+1} finished"")\n\n    def train_on_batch(self, batch, args):\n        #TODO implement caching\n        if type(batch) is dict:\n            for k,v in batch.items():\n                if type(v) is torch.Tensor:\n                    batch[k] = v.to(self.t_config.device)\n            with torch.no_grad():\n                results_T = self.model_T(**batch, **args)\n            results_S = self.model_S(**batch, **args)\n        else:\n            moved_batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n            batch = moved_batch\n            with torch.no_grad():\n                results_T = self.model_T(*batch, **args)\n            results_S = self.model_S(*batch, **args)\n\n        results_T = post_adaptor(self.adaptor_T(batch,results_T))\n        results_S = post_adaptor(self.adaptor_S(batch,results_S))\n        logits_list_T = results_T[\'logits\']  # list of tensor\n        logits_list_S = results_S[\'logits\']  # list of tensor\n        total_loss  = 0\n\n        if \'logits_mask\' in results_S:\n            masks_list_S = results_S[\'logits_mask\']\n            logits_list_S = select_logits_with_mask(logits_list_S,masks_list_S)  #(mask_sum, num_of_class)\n        if \'logits_mask\' in results_T:\n            masks_list_T = results_T[\'logits_mask\']\n            logits_list_T = select_logits_with_mask(logits_list_T,masks_list_T)  #(mask_sum, num_of_class)\n\n        if self.d_config.probability_shift is True:\n            labels_list = results_S[\'labels\']\n            for l_T, l_S, labels in zip(logits_list_T, logits_list_S, labels_list):\n                l_T = probability_shift_(l_T, labels)\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                kd_loss = self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n                total_loss += kd_loss\n        else:\n            for l_T,l_S in zip(logits_list_T,logits_list_S):\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                kd_loss = self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n                total_loss += kd_loss\n\n        if \'losses\' in results_S:\n            for loss in results_S[\'losses\']:\n                # in case of multi-GPU\n                total_loss += loss.mean() * self.d_config.hard_label_weight\n\n        return total_loss\n\n\nclass MultiTeacherDistiller(BasicDistiller):\n    """"""\n    Distills multiple teacher models (of the same tasks) into a student model. **It doesn\'t support intermediate feature matching**.\n\n    Args:\n        train_config (:class:`TrainingConfig`): training configuration.\n        distill_config (:class:`DistillationConfig`): distillation configuration.\n        model_T (List[torch.nn.Module]): list of teacher models.\n        model_S (torch.nn.Module): student model.\n        adaptor_T (Callable): teacher model\'s adaptor.\n        adaptor_S (Callable): student model\'s adaptor.\n\n    The roles of `adaptor_T` and `adaptor_S` are explained in :py:func:`adaptor`.\n    """"""\n\n    def __init__(self, train_config,\n                 distill_config,\n                 model_T,\n                 model_S,\n                 adaptor_T,\n                 adaptor_S):\n        super(MultiTeacherDistiller, self).__init__(\n            train_config, distill_config,\n            model_T, model_S,\n            adaptor_T, adaptor_S)\n        if hasattr(self.adaptor_T,\'__iter__\'):\n            assert len(self.adaptor_T)==len(self.model_T)\n        self.avg = True\n\n    def train_on_batch(self, batch, args):\n        # Basic uses no cache\n\n        selected = None\n        num_T = len(self.model_T)\n\n        if type(batch) is dict:\n            for k,v in batch.items():\n                if type(v) is torch.Tensor:\n                    batch[k] = v.to(self.t_config.device)\n            with torch.no_grad():\n                if self.avg:\n                    results_T = [model_t(**batch, **args) for model_t in self.model_T]\n                else:\n                    selected = random.choice(range(num_T))\n                    results_T = [self.model_T[selected](**batch,**args)]\n            results_S = self.model_S(**batch, **args)\n        else:\n            moved_batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n            batch = moved_batch\n            with torch.no_grad():\n                if self.avg:\n                    results_T = [model_T(*batch, **args) for model_T in self.model_T]\n                else:\n                    selected = random.choice(range(num_T))\n                    results_T = [self.model_T[selected](*batch,**args)]\n            results_S = self.model_S(*batch, **args)\n\n        if hasattr(self.adaptor_T,\'__iter__\'):\n            if self.avg:\n                results_T = [post_adaptor(adpt_t(batch,results_t)) for results_t,adpt_t in zip(results_T,self.adaptor_T)]\n            else:\n                results_T = [post_adaptor(self.adaptor_T[selected](batch,results_T[0]))]\n        else:\n            results_T = [post_adaptor(self.adaptor_T(batch,results_t)) for results_t in results_T]\n        results_S = post_adaptor(self.adaptor_S(batch,results_S))\n\n        logits_list_T = [results_t[\'logits\'] for results_t in results_T]  # list of tensor\n        logits_list_S = results_S[\'logits\']  # list of tensor\n        total_loss  = 0\n\n        if \'logits_mask\' in results_S:\n            masks_list_S = results_S[\'logits_mask\']\n            logits_list_S = select_logits_with_mask(logits_list_S,masks_list_S)  #(mask_sum, num_of_class)\n        if \'logits_mask\' in results_T[0]:\n            masks_list_T = results_T[0][\'logits_mask\']\n            logits_list_T = [select_logits_with_mask(logits_list_t,masks_list_T)\n                             for logits_list_t in logits_list_T] #(mask_sum, num_of_class)\n\n        if self.d_config.probability_shift is True:\n            labels_list = results_S[\'labels\']\n            for l_T, l_S, labels in zip(zip(*logits_list_T),logits_list_S,labels_list):\n                mean_l_T = sum(l_T)/len(l_T)\n                mean_l_T = probability_shift_(mean_l_T, labels)\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, mean_l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                total_loss += self.kd_loss(l_S, mean_l_T, temperature) * self.d_config.kd_loss_weight\n        else:\n            for l_T, l_S in zip(zip(*logits_list_T),logits_list_S):\n                mean_l_T = sum(l_T)/len(l_T)\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, mean_l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                total_loss += self.kd_loss(l_S, mean_l_T, temperature) * self.d_config.kd_loss_weight\n\n        if \'losses\' in results_S:\n            for loss in results_S[\'losses\']:\n                # in case of multi-GPU\n                total_loss += loss.mean() * self.d_config.hard_label_weight\n        return total_loss\n\n\nclass GeneralDistiller(BasicDistiller):\n    """"""\n    Supports intermediate features matching. **Recommended for single-teacher single-task distillation**.\n\n    Args:\n        train_config (:class:`TrainingConfig`): training configuration.\n        distill_config (:class:`DistillationConfig`): distillation configuration.\n        model_T (:class:`torch.nn.Module`): teacher model.\n        model_S (:class:`torch.nn.Module`): student model.\n        adaptor_T (Callable): teacher model\'s adaptor.\n        adaptor_S (Callable): student model\'s adaptor.\n        custom_matches (list): supports more flexible user-defined matches (testing).\n\n    The roles of `adaptor_T` and `adaptor_S` are explained in :py:func:`adaptor`.\n\n    """"""\n    def __init__(self, train_config,\n                 distill_config,\n                 model_T,\n                 model_S,\n                 adaptor_T,\n                 adaptor_S,\n                 custom_matches: Optional[List[CustomMatch]] = None):\n        # custom_matches=[{\'module_T\': module_T, \'module_S\':module_S,\n        #                 \'loss\': loss, \'weight\': weight},...]\n        super(GeneralDistiller, self).__init__(train_config, distill_config, model_T, model_S, adaptor_T, adaptor_S)\n\n        self.projs = []\n        self.projs_group = []\n        for im in self.d_config.intermediate_matches:\n            if im.proj is not None:\n                projection = im.proj[0]\n                dim_in = im.proj[1]\n                dim_out = im.proj[2]\n                self.projs_group.append(im.proj[3])\n                self.projs.append(PROJ_MAP[projection](dim_in,dim_out))\n                self.projs[-1].to(self.t_config.device)\n            else:\n                self.projs.append(None)\n                self.projs_group.append(None)\n\n        self.has_custom_matches = False\n        if custom_matches:\n            self.handles_T = []\n            self.handles_S = []\n            self.custom_matches_cache = {\'hook_outputs_T\': [], \'hook_outputs_S\': [], \'match_proj_funcs\': [],\n                                         \'match_weights\':  [], \'match_losses\':   [], \'match_proj_groups\': []}\n            for match in custom_matches:\n                self.add_match(match)\n            self.has_custom_matches = True\n\n    def save_and_callback(self,global_step, step, epoch, callback):\n        if self.has_custom_matches:\n            handles_T = self.model_T._forward_hooks\n            handles_S = self.model_S._forward_hooks\n            self.model_S._forward_hooks = OrderedDict()  # clear hooks\n            self.model_T._forward_hooks = OrderedDict()\n\n        super(GeneralDistiller, self).save_and_callback(global_step, step, epoch, callback)\n\n        if self.has_custom_matches:\n            self.model_S._forward_hooks = handles_S  # restore hooks\n            self.model_T._forward_hooks = handles_T\n\n    def train(self, optimizer, scheduler, dataloader, num_epochs, num_steps=None, callback=None, batch_postprocessor=None, **args):\n        """"""\n        trains the student model. See :meth:`BasicDistiller.train`.\n        """"""\n        # update optimizer for projection layer\n        for proj,proj_group in zip(self.projs, self.projs_group):\n            if proj is not None:\n                assert isinstance(proj,nn.Module)\n                optimizer.add_param_group({**{\'params\':proj.parameters()},**proj_group})\n\n        if self.has_custom_matches:\n            for proj_func,proj_group in zip(self.custom_matches_cache[\'match_proj_funcs\'],\n                                                   self.custom_matches_cache[\'match_proj_groups\']):\n                if isinstance(proj_func,nn.Module):\n                    optimizer.add_param_group({**{\'params\':proj_func.parameters()},**proj_group})\n\n        logger.debug(""Optimizer param group: "")\n        for group in optimizer.param_groups:\n            for k,v in group.items():\n                logger.debug(f""{k}:{v}"")\n\n        super(GeneralDistiller, self).train(optimizer, scheduler, dataloader, num_epochs, num_steps, callback, batch_postprocessor, **args)\n\n    def train_on_batch(self, batch, args):\n        if type(batch) is dict:\n            for k,v in batch.items():\n                if type(v) is torch.Tensor:\n                    batch[k] = v.to(self.t_config.device)\n            with torch.no_grad():\n                results_T = self.model_T(**batch, **args)\n            results_S = self.model_S(**batch, **args)\n        else:\n            moved_batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n            batch = moved_batch\n            with torch.no_grad():\n                results_T = self.model_T(*batch, **args)\n            results_S = self.model_S(*batch, **args)\n\n        results_T = post_adaptor(self.adaptor_T(batch,results_T))\n        results_S = post_adaptor(self.adaptor_S(batch,results_S))\n\n        total_loss  = 0\n        if \'logits\' in results_T and \'logits\' in results_S:\n            logits_list_T = results_T[\'logits\']  # list of tensor\n            logits_list_S = results_S[\'logits\']  # list of tensor\n\n            if \'logits_mask\' in results_S:\n                masks_list_S = results_S[\'logits_mask\']\n                logits_list_S = select_logits_with_mask(logits_list_S,masks_list_S)  #(mask_sum, num_of_class)\n            if \'logits_mask\' in results_T:\n                masks_list_T = results_T[\'logits_mask\']\n                logits_list_T = select_logits_with_mask(logits_list_T,masks_list_T)  #(mask_sum, num_of_class)\n\n            if self.d_config.probability_shift is True:\n                labels_list = results_S[\'labels\']\n                for l_T, l_S, labels in zip(logits_list_T, logits_list_S, labels_list):\n                    l_T = probability_shift_(l_T, labels)\n                    if self.d_config.temperature_scheduler is not None:\n                        temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                    else:\n                        temperature = self.d_config.temperature\n                    kd_loss = self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n                    total_loss += kd_loss\n            else:\n                for l_T,l_S in zip(logits_list_T,logits_list_S):\n                    if self.d_config.temperature_scheduler is not None:\n                        temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                    else:\n                        temperature = self.d_config.temperature\n                    kd_loss = self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n                    total_loss += kd_loss\n\n        inters_T = {feature: results_T.get(feature,[]) for feature in FEATURES}\n        inters_S = {feature: results_S.get(feature,[]) for feature in FEATURES}\n        inputs_mask_T = results_T.get(\'inputs_mask\',None)\n        inputs_mask_S = results_S.get(\'inputs_mask\',None)\n        for ith,inter_match in enumerate(self.d_config.intermediate_matches):\n            layer_T = inter_match.layer_T\n            layer_S = inter_match.layer_S\n            feature = inter_match.feature\n            loss_type = inter_match.loss\n            match_weight = inter_match.weight\n            match_loss = MATCH_LOSS_MAP[loss_type]\n\n            if type(layer_S) is list and type(layer_T) is list:\n                inter_S = [inters_S[feature][s] for s in layer_S]\n                inter_T = [inters_T[feature][t] for t in layer_T]\n                if self.projs[ith]:\n                    #inter_T = [self.projs[ith](t) for t in inter_T]\n                    inter_S = [self.projs[ith](s) for s in inter_S]\n            else:\n                inter_S = inters_S[feature][layer_S]\n                inter_T = inters_T[feature][layer_T]\n                if self.projs[ith]:\n                    #inter_T = self.projs[ith](inter_T)\n                    inter_S = self.projs[ith](inter_S)\n            total_loss += match_loss(inter_S, inter_T, mask=inputs_mask_S) * match_weight\n\n\n        if self.has_custom_matches:\n            for hook_T, hook_S, match_weight, match_loss, proj_func  in \\\n                    zip(self.custom_matches_cache[\'hook_outputs_T\'], self.custom_matches_cache[\'hook_outputs_S\'],\n                        self.custom_matches_cache[\'match_weghts\'], self.custom_matches_cache[\'match_losses\'],\n                        self.custom_matches_cache[\'match_proj_funcs\']):\n                if proj_func is not None:\n                    hook_S = proj_func(hook_S)\n                total_loss += match_weight * match_loss(hook_S,hook_T,inputs_mask_S,inputs_mask_T)\n            self.custom_matches_cache[\'hook_outputs_T\'] = []\n            self.custom_matches_cache[\'hook_outputs_S\'] = []\n\n        if \'losses\' in results_S:\n            for loss in results_S[\'losses\']:\n                # in case of multi-GPU\n                total_loss += loss.mean() * self.d_config.hard_label_weight\n\n        return total_loss\n\n    def add_match(self,match: CustomMatch):\n        if type(match.module_T) is str or type(match.module_S) is str:\n            raise NotImplementedError\n        else:\n            module_T   = match.module_T\n            module_S   = match.module_S\n            weight     = match.weight\n            loss       = match.loss\n            proj_func = match.proj_func\n            proj_group = match.proj_group\n        self.add_match_by_module(module_T,module_S,proj_func,proj_group,weight,loss)\n\n\n    def add_match_by_module(self,module_T : torch.nn.Module,\n                                 module_S : torch.nn.Module,\n                                 proj_func, proj_group,\n                                 match_weight, match_loss):\n\n        self.handles_T = module_T.register_forward_hook(self._hook_T)\n        self.handles_S = module_S.register_forward_hook(self._hook_S)\n        self.custom_matches_cache[\'match_weights\'].append(match_weight)\n        self.custom_matches_cache[\'match_losses\'].append(match_loss)\n        self.custom_matches_cache[\'match_proj_funcs\'].append(proj_func)\n        if isinstance(proj_func,nn.Module):\n            self.custom_matches_cache[\'match_proj_funcs\'][-1].to(self.t_config.device)\n        self.custom_matches_cache[\'match_proj_groups\'].append(proj_group)\n\n    def _hook_T(self,module,input, output):\n        self.custom_matches_cache[\'hook_outputs_T\'].append(output)\n\n    def _hook_S(self, module, input, output):\n        self.custom_matches_cache[\'hook_outputs_S\'].append(output)\n\n\n\nclass MultiTaskDistiller(BasicDistiller):\n    """"""\n    distills multiple teacher models (of different tasks) into a single student. **It doesn\'t support intermediate feature matching**.\n\n    Args:\n        train_config (:class:`TrainingConfig`): training configuration.\n        distill_config (:class:`DistillationConfig`): distillation configuration.\n        model_T (dict): dict of teacher models: {task1:model1, task2:model2, .... }. Keys are tasknames.\n        model_S (torch.nn.Module): student model.\n        adaptor_T (dict): dict of teacher adaptors: {task1:adpt1, task2:adpt2, .... }. Keys are tasknames.\n        adaptor_S (dict): dict of student adaptors: {task1:adpt1, task2:adpt2, .... }. Keys are tasknames.\n\n    """"""\n    \n    def __init__(self, train_config,\n                 distill_config,\n                 model_T,\n                 model_S,\n                 adaptor_T,\n                 adaptor_S):\n\n        super(MultiTaskDistiller, self).__init__(\n            train_config, distill_config,\n            model_T, model_S,\n            adaptor_T, adaptor_S)\n        if hasattr(self.adaptor_T,\'__iter__\'):\n            assert len(self.adaptor_T)==len(self.model_T)==len(self.adaptor_S)\n        assert (self.d_config.kd_loss_weight_scheduler is None) and (self.d_config.hard_label_weight_scheduler is None),\\\n                ""BasicMultiTaskDistiller does not support WEIGHT_SCHEDULER in the current version.""\n\n\n    def train(self, optimizer, scheduler, dataloaders, num_steps, tau=1, callback=None, batch_postprocessors=None, **args):\n        """"""\n        trains the student model.\n\n        Args:\n            optimizer: optimizer.\n            scheduler: used to adjust learning rate, optional, can be None.\n            dataloaders (dict): dict of dataset iterator. Keys are tasknames, values are corresponding dataloaders.\n            num_steps (int): number of training steps.\n            tau (float): the probability of sampling an example from task `d` is proportional to \\|d\\|^{tau}, where \\|d\\| is the size of `d`\'s training set. If the size of any dataset is unknown, ignores tau and samples examples unifromly from each dataset.\n            callback (Callable): function called after each epoch, can be None. It is called as ``callback(model=self.model_S, step = global_step)``. It can be used to do evaluation of the model at each checkpoint.\n            batch_postprocessors (dict): a dict of batch_postprocessors. Keys are tasknames, values are corresponding batch_postprocessors. Each batch_postprocessor should take a batch and return a batch.\n            **args: additional arguments fed to the model.\n        """"""\n        total_global_steps = num_steps\n        ckpt_steps =self.t_config.ckpt_steps\n        print_every = ckpt_steps // self.print_freq\n        if print_every == 0:\n            print_every = ckpt_steps\n        checkpoints = [ i * ckpt_steps for i in range(1,num_steps//ckpt_steps+1)] + [total_global_steps]\n        logger.info(f""Total training steps: {total_global_steps}"")\n        logger.info(f""Checkpoints(step): {checkpoints}"")\n\n        dataiters = {k:cycle(v) for k,v in dataloaders}\n        if all(hasattr(v,\'__len__\') for v in dataloaders.values()):\n            dataloader_sizes = {k:len(v) for k,v in dataloaders.items()}\n            total_size = sum(v for k,v in dataloader_sizes.items())//self.t_config.gradient_accumulation_steps\n            logger.info(f""Total size of all datasets (in number of batch_size):{total_size}"")\n            Z = sum(pow(v,tau) for v in dataloader_sizes.values())\n            tasknames, sampling_weights = zip(*((k,pow(v,tau)/Z) for k,v in dataloader_sizes.items()))\n        else:\n            logger.info(""The size of some datasets are unknown, so tau=1"")\n            tasknames = tuple(dataloaders.keys())\n            sampling_weights = None\n\n            \n        global_step = 0\n        writer_step = 0\n        self.model_S.zero_grad()\n        while global_step < num_steps:\n            global_step += 1\n            for _ in range(self.t_config.gradient_accumulation_steps):\n                #sampling taskname\n                taskname = np.random.choice(tasknames,p=sampling_weights)\n                dataiter = dataiters[taskname]\n                batch = next(dataiter)\n                if batch_postprocessors is not None:\n                    batch = batch_postprocessors[taskname](batch)\n                batch_taskname = (batch, taskname)\n                total_loss = self.train_on_batch(batch_taskname, args)\n                total_loss /= self.t_config.gradient_accumulation_steps\n                total_loss.backward()\n                scalar_total_loss = total_loss.cpu().item() * self.t_config.gradient_accumulation_steps\n                self.tb_writer.add_scalar(\'scalar/total_loss\', scalar_total_loss, writer_step)\n                writer_step += 1\n            optimizer.step()\n            if scheduler is not None:\n                scheduler.step()\n            self.model_S.zero_grad()\n\n            if self.d_config.kd_loss_weight_scheduler is not None:\n                self.d_config.kd_loss_weight = \\\n                    self.d_config.kd_loss_weight_scheduler(global_step/total_global_steps)\n            if self.d_config.hard_label_weight_scheduler is not None:\n                self.d_config.hard_label_weight = \\\n                    self.d_config.hard_label_weight_scheduler(global_step/total_global_steps)\n\n            if (global_step) % print_every == 0:\n                logger.info(f""Global step: {global_step}/{num_steps}"")\n            if (global_step % ckpt_steps == 0) or global_step==total_global_steps:\n                self.save_and_callback(global_step, global_step-1, 0, callback)\n        logger.info(""Training finished"")\n\n    def train_on_batch(self, batch_taskname, args) -> torch.Tensor:\n        # Basic uses no cache\n        batch, taskname = batch_taskname\n        model_T = self.model_T[taskname]\n        adaptor_T = self.adaptor_T[taskname]\n        adaptor_S = self.adaptor_S[taskname]\n\n        if type(batch) is dict:\n            for k,v in batch.items():\n                if type(v) is torch.Tensor:\n                    batch[k] = v.to(self.t_config.device)\n            with torch.no_grad():\n                results_T = model_T(**batch, **args)\n            results_S = self.model_S(**batch, **args)\n        else:\n            moved_batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n            batch = moved_batch\n            with torch.no_grad():\n                results_T = model_T(*batch, **args)\n            results_S = self.model_S(*batch, **args)\n\n        results_T = post_adaptor(adaptor_T(batch,results_T))\n        results_S = post_adaptor(adaptor_S(batch,results_S))\n\n        logits_list_T = results_T[\'logits\']  # list of tensor\n        logits_list_S = results_S[taskname][\'logits\']  # list of tensor\n        total_loss  = 0\n\n        if \'logits_mask\' in results_S[taskname]:\n            masks_list_S = results_S[taskname][\'logits_mask\']\n            logits_list_S = select_logits_with_mask(logits_list_S,masks_list_S)  #(mask_sum, num_of_class)\n        if \'logits_mask\' in results_T: #TODO\n            masks_list_T = results_T[\'logits_mask\']\n            logits_list_T = select_logits_with_mask(logits_list_T,masks_list_T)\n\n        if self.d_config.probability_shift is True:\n            labels_list = results_S[\'labels\']\n            for l_T, l_S, labels in zip(logits_list_T, logits_list_S, labels_list):\n                l_T = probability_shift_(l_T, labels)\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                kd_loss = self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n                total_loss += kd_loss\n        else:\n            for l_T,l_S in zip(logits_list_T,logits_list_S):\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                total_loss += self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n\n        if \'losses\' in results_S:\n            for loss in results_S[taskname][\'losses\']:\n                # in case of multi-GPU\n                total_loss += loss.mean() * self.d_config.hard_label_weight\n\n        return total_loss\n\n\ndef select_logits_with_mask(logits_list, masks_list):\n    output_logits = []\n    if len(masks_list)==len(logits_list):\n        for logits,mask in zip(logits_list,masks_list):\n            if len(logits.shape)==3:\n                mask = mask.unsqueeze(-1).expand_as(logits).to(mask_dtype)\n                logits_select = torch.masked_select(logits,mask).view(-1,logits.size(-1))\n            else:\n                logits_select = logits #Logits_mask has no effect on logits of shape (batch_size, logits_to_be_softmaxed)\n            output_logits.append(logits_select)\n    elif len(masks_list)==1:\n        mask = masks_list[0]\n        for logits in logits_list:\n            if len(logits.shape)==3:\n                mask = mask.unsqueeze(-1).expand_as(logits).to(mask_dtype)\n                logits_select = torch.masked_select(logits,mask).view(-1,logits.size(-1))\n            else:\n                logits_select = logits #Logits_mask has no effect on logits of shape (batch_size, logits_to_be_softmaxed)\n            output_logits.append(logits_select)\n    else:\n        raise AssertionError(""lengths of logits list and masks list mismatch"")\n    return output_logits\n\n\nclass BasicAdaptor:\n    def __init__(self):\n        self.batch = None\n        self.model_outputs = None\n    def __call__(self,batch,model_outputs):\n        self.batch = batch\n        self.model_outputs = model_outputs\n    def __getattr__(self, item):\n        raise NotImplementedError\n\n\ndef post_adaptor(dict_object):\n    if \'logits\' in dict_object:\n        logits = dict_object[\'logits\']\n        if not isinstance(logits,(list,tuple)):\n            dict_object[\'logits\'] = [ logits ]\n    if \'logits_mask\' in dict_object:\n        logits_mask = dict_object[\'logits_mask\']\n        if not isinstance(logits_mask,(list,tuple)):\n            dict_object[\'logits_mask\'] = [ logits_mask ]\n    if \'losses\' in dict_object:\n        losses = dict_object[\'losses\']\n        if not isinstance(losses,(list,tuple)):\n            dict_object[\'losses\'] = [ losses ]\n    if \'labels\' in dict_object:\n        labels = dict_object[\'labels\']\n        if not isinstance(labels,(list,tuple)):\n            dict_object[\'labels\'] = [ labels ]\n    return dict_object\n\n\nclass BasicTrainer:\n    """"""\n    It performs supervised training, not distillation. It can be used for training the teacher model.\n\n    Args:\n        train_config (:class:`TrainingConfig`): training configuration.\n        model (:class:`torch.nn.Module`): model to be trained.\n        adaptor (Callable)\xef\xbc\x9aadaptor of the model.\n    \n    The role of `adaptor` is explained in :py:func:`adaptor`.\n    """"""\n\n    def __enter__(self):\n        self.model_is_training = self.model.training\n        self.model.train()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Restore model status\n        self.model.train(self.model_is_training)\n\n    def __init__(self, train_config: TrainingConfig,\n                 model: torch.nn.Module, adaptor):\n        super(BasicTrainer, self).__init__()\n        self.t_config = train_config\n        self.model = model\n        self.adaptor = adaptor\n        if self.t_config.log_dir is not None:\n            self.tb_writer = SummaryWriter(log_dir = self.t_config.log_dir)\n        else:\n            self.tb_writer = no_op\n        self.print_freq = 20\n\n    def train(self, optimizer, scheduler, dataloader, num_epochs, num_steps=None, callback=None, batch_postprocessor=None, **args):\n        """"""\n        trains the model. See :meth:`BasicDistiller.train`.\n        """"""\n        if num_steps is not None:\n            total_global_steps = num_steps\n            ckpt_steps =self.t_config.ckpt_steps\n            print_every = ckpt_steps // self.print_freq\n            if print_every == 0:\n                print_every = ckpt_steps\n            checkpoints = [ i * ckpt_steps for i in range(1,num_steps//ckpt_steps+1)] + [total_global_steps]\n            logger.info(f""Total training steps: {total_global_steps}"")\n            logger.info(f""Checkpoints: {checkpoints}"")\n\n            global_step = 0\n            writer_step = 0\n            for step, batch in tqdm(enumerate(cycle(dataloader)),disable=None):\n                if batch_postprocessor is not None:\n                    batch = batch_postprocessor(batch)\n                total_loss = self.train_on_batch(batch,args)\n                total_loss /= self.t_config.gradient_accumulation_steps\n                total_loss.backward()\n\n                scalar_total_loss = total_loss.cpu().item() * self.t_config.gradient_accumulation_steps\n                self.tb_writer.add_scalar(\'scalar/total_loss\', scalar_total_loss, writer_step)\n                writer_step += 1\n\n                if (step+1)%self.t_config.gradient_accumulation_steps == 0:\n                    optimizer.step()\n                    if scheduler is not None:\n                        scheduler.step()\n                    self.model.zero_grad()\n                    global_step += 1\n                    if (global_step) % print_every == 0:\n                        logger.info(f""Global step: {global_step}, epoch step:{step+1}"")\n                    if (global_step%ckpt_steps==0) or global_step==total_global_steps:\n                        logger.info(f""Saving at global step {global_step}"")\n                        coreModel = self.model.module if \\\n                            \'DataParallel\' in self.model.__class__.__name__ else self.model\n                        state_dict = coreModel.state_dict()\n                        torch.save(state_dict, os.path.join(self.t_config.output_dir,f""gs{global_step}.pkl""))\n                        if callback is not None:\n                            logger.info(""Running callback function..."")\n                            callback(model=self.model, step=global_step)\n                            self.model.train()\n            logger.info(""Training finished"")\n            return\n\n        train_steps_per_epoch = len(dataloader)//self.t_config.gradient_accumulation_steps\n        print_every = train_steps_per_epoch // self.print_freq\n        if print_every == 0:\n            print_every = train_steps_per_epoch\n        checkpoints = [int(train_steps_per_epoch*ci/self.t_config.ckpt_frequency) for ci in range(self.t_config.ckpt_frequency)]\n        logger.info(f""Training steps per epoch: {train_steps_per_epoch}"")\n        logger.info(f""Checkpoints(step): {checkpoints}"")\n\n        global_step = 0\n        writer_step = 0\n        for current_epoch in tqdm(range(int(num_epochs)),disable=None):\n            logger.info(f""Epoch {current_epoch+1}"")\n            self.model.zero_grad()\n            logger.info(f""Length of current epoch in forward batch: {len(dataloader)}"")\n            for step, batch in tqdm(enumerate(dataloader),disable=None):\n                if batch_postprocessor is not None:\n                    batch = batch_postprocessor(batch)\n                total_loss = self.train_on_batch(batch,args)\n                total_loss /= self.t_config.gradient_accumulation_steps\n                total_loss.backward()\n\n                scalar_total_loss = total_loss.cpu().item() * self.t_config.gradient_accumulation_steps\n                self.tb_writer.add_scalar(\'scalar/total_loss\', scalar_total_loss, writer_step)\n                writer_step += 1\n\n                if (step+1)%self.t_config.gradient_accumulation_steps == 0:\n                    optimizer.step()\n                    if scheduler is not None:\n                        scheduler.step()\n                    self.model.zero_grad()\n                    global_step += 1\n                    if (global_step) % print_every == 0:\n                        logger.info(f""Global step: {global_step}, epoch step:{step+1}"")\n                    if (global_step%train_steps_per_epoch in checkpoints) \\\n                            and ((current_epoch+1)%self.t_config.ckpt_epoch_frequency==0 or current_epoch+1==num_epochs):\n                        logger.info(f""Saving at global step {global_step}, epoch step {step+1} epoch {current_epoch+1}"")\n                        coreModel = self.model.module if \\\n                            \'DataParallel\' in self.model.__class__.__name__ else self.model\n                        state_dict = coreModel.state_dict()\n                        torch.save(state_dict, os.path.join(self.t_config.output_dir,f""gs{global_step}.pkl""))\n                        if callback is not None:\n                            logger.info(""Running callback function..."")\n                            callback(model=self.model, step=global_step)\n                            self.model.train()\n\n            logger.info(f""Epoch {current_epoch+1} finished"")\n\n    def train_on_batch(self, batch, args) -> torch.Tensor:\n        if type(batch) is dict:\n            for k,v in batch.items():\n                if type(v) is torch.Tensor:\n                    batch[k] = v.to(self.t_config.device)\n            results = self.model(**batch, **args)\n        else:\n            moved_batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n            batch = moved_batch\n            results = self.model(*batch, **args)\n\n        results = post_adaptor(self.adaptor(batch,results))\n        total_loss  = 0\n\n        if \'losses\' not in results:\n            raise KeyError(""\'losses\' not in the output of adaptor. Nothing to optimize!"")\n        else:\n            for loss in results[\'losses\']:\n                # in case of multi-GPU\n                total_loss += loss.mean()\n\n        return total_loss\n\n\ndef probability_shift_(tensor, labels):  # In-place operation. shape (batch_size, num_classes), (batch_size,)\n    if len(tensor.shape)==2:\n        max_position = tensor.argmax(dim=-1) # shape (batch_size,)\n        index = torch.arange(tensor.size(0)).to(tensor.device)\n        max_clone = tensor[index,max_position].clone()\n        truth_clone = tensor[index,labels].clone()\n\n        tensor[index,max_position] = truth_clone\n        tensor[index,labels] = max_clone\n        return tensor\n\n    elif len(tensor.shape)==3:   # shape (batch_size, length, num_classes)\n        original_shape = tensor.size()\n\n        tensor = tensor.view(-1,tensor.size(-1))   # (batch_size * length, num_classes)\n\n        max_position = tensor.argmax(dim=-1) # shape (batch_size * length, )\n        labels = labels.view(-1) # (batch_size * length, )\n        nonneg_labels = torch.where(labels<0, max_position, labels)\n\n        index = torch.arange(tensor.size(0)).to(tensor.device)   # (batch_size * length)\n\n        max_clone = tensor[index,max_position].clone()\n        truth_clone = tensor[index,nonneg_labels].clone()\n\n        tensor[index,max_position] = truth_clone\n        tensor[index,nonneg_labels] = max_clone\n        tensor = tensor.view(original_shape)\n        return tensor\n    else:\n        raise TypeError(""Rank of tensor must be 2 or 3"")\n\nclass no_op:\n    @staticmethod\n    def add_scalar(*args, **kwargs):\n        pass\n'"
src/textbrewer/distiller_basic.py,15,"b'from .distiller_utils import *\n\nclass BasicDistiller(AbstractDistiller):\n    """"""\n    Performs **single-teacher single-task** distillation, provides basic distillation strategies.\n\n    Args:\n        train_config (:class:`TrainingConfig`): training configuration.\n        distill_config (:class:`DistillationConfig`): distillation configuration.\n        model_T (:class:`torch.nn.Module`): teacher model.\n        model_S (:class:`torch.nn.Module`): student model.\n        adaptor_T (Callable): teacher model\'s adaptor.\n        adaptor_S (Callable): student model\'s adaptor.\n\n    The roles of `adaptor_T` and `adaptor_S` are explained in :py:func:`adaptor`.\n\n    """"""\n    def __init__(self, train_config,\n                       distill_config,\n                 model_T,\n                 model_S,\n                 adaptor_T,\n                 adaptor_S):\n        super(BasicDistiller, self).__init__(train_config, distill_config, model_T, model_S, adaptor_T, adaptor_S)\n\n    def save_and_callback(self,global_step, step, epoch, callback):\n        logger.info(f""Saving at global step {global_step}, epoch step {step + 1} epoch {epoch+1}"")\n        coreModel = self.model_S.module if \\\n            \'DataParallel\' in self.model_S.__class__.__name__ else self.model_S\n        state_dict = coreModel.state_dict()\n        torch.save(state_dict, os.path.join(self.t_config.output_dir, f""gs{global_step}.pkl""))\n        if callback is not None:\n            logger.info(""Running callback function..."")\n            callback(model=self.model_S, step=global_step)\n            self.model_S.train()\n\n    def write_loss(self, total_loss, writer_step):\n\n        cpu_total_loss = total_loss.cpu().item() * self.t_config.gradient_accumulation_steps\n        self.tb_writer.add_scalar(\'scalar/total_loss\', cpu_total_loss, writer_step)\n\n        #for name, loss in losses_dict.items():\n        #    cpu_loss = loss.cpu().item() * self.t_config.gradient_accumulation_steps\n        #    self.tb_writer.add_scalar(f""scalar/{name}"", cpu_loss, writer_step)\n\n\n    def train(self, optimizer, dataloader, num_epochs, scheduler_class=None, scheduler_args=None, scheduler=None, max_grad_norm = -1.0, num_steps=None, callback=None, batch_postprocessor=None, **args):\n        """"""\n        trains the student model.\n\n        Args:\n            optimizer: optimizer.\n            dataloader: dataset iterator.\n            num_epochs (int): number of training epochs.\n            num_steps (int): number of training steps. If it is not None, distiller will ignore `num_epochs` and trains for `num_steps`, and dataloader can have an unkonwn size, i.e., has no `__len__` attribute. Dataloader will be cycled automatically after iterating over the whole dataset.\n            callback (Callable): function called after each epoch, can be None. It is called as ``callback(model=self.model_S, step = global_step)``. It can be used to evaluate the model at each checkpoint.\n            batch_postprocessor (Callable): a function for post-processing batches. It should take a batch and return a batch. Its output is fed to the models and adaptors.\n            scheduler_class (class): the class of the scheduler to be constructed.\n            scheduler_args (dict): arguments (excluding `optimizer`) passed to the `scheduler_class` to construct the scheduler object. See the example below.\n            scheduler (deprecated): used to adjust learning rate, optional, can be None, is deprecated in favor of `scheduler_class` and `scheduler_args`.\n            max_grad_norm (float): Maximum norm for the gradients (-1 means no clipping). Default: -1.0\n            **args: additional arguments fed to the model.\n        Note:\n            * If the batch is a list or tuple, model is called as: ``model(*batch, **args)``. Make sure the order of elements in the batch matches their order in ``model.forward``.\n            * If the batch is a dict, model is called as: ``model(**batch,**args)``. Make sure the keys of the batch match the arguments of the ``model.forward``.\n        Note:\n            If you want to provide a lr scheduler, DON\'T USE `scheduler` , use `scheduler_class` and `scheduler_args` instead. Example:\n\n            .. code-block::\n\n                from transformers import get_linear_schedule_with_warmup\n                distiller.train(optimizer, scheduler_class = get_linear_schedule_with_warmup, scheduler_args= {\'num_warmup_steps\': 100, \'num_training_steps\': 1000})\n        """"""\n\n        # update scheduler\n        if scheduler_class is not None:\n            # overwrite scheduler\n            scheduler = scheduler_class(**{\'optimizer\':optimizer},**scheduler_args)\n\n        if num_steps is not None:\n            if self.d_config.is_caching_logits is True:\n                logger.warning(""is_caching_logits is True, but num_steps is not None!"")\n            total_global_steps = num_steps\n            ckpt_steps =self.t_config.ckpt_steps\n            print_every = ckpt_steps // self.print_freq\n            if print_every == 0:\n                print_every = ckpt_steps\n            checkpoints = [ i * ckpt_steps for i in range(1,num_steps//ckpt_steps+1)] + [total_global_steps]\n            logger.info(f""Total training steps: {total_global_steps}"")\n            logger.info(f""Checkpoints(step): {checkpoints}"")\n\n            global_step = 0\n            writer_step = 0\n            for step, batch in tqdm(enumerate(cycle(dataloader)),disable=None):\n                if batch_postprocessor is not None:\n                    batch = batch_postprocessor(batch)\n                total_loss = self.train_on_batch(batch,args)\n                total_loss /= self.t_config.gradient_accumulation_steps\n                total_loss.backward()\n\n                self.write_loss(total_loss, writer_step)\n                writer_step += 1\n\n                if (step+1)%self.t_config.gradient_accumulation_steps == 0:\n                    if max_grad_norm > 0:\n                        torch.nn.utils.clip_grad_norm_(self.model_S.parameters(), max_grad_norm)\n                    optimizer.step()\n                    if scheduler is not None:\n                        scheduler.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n                    if self.d_config.kd_loss_weight_scheduler is not None:\n                        self.d_config.kd_loss_weight = \\\n                            self.d_config.kd_loss_weight_scheduler(global_step/total_global_steps)\n                    if self.d_config.hard_label_weight_scheduler is not None:\n                        self.d_config.hard_label_weight = \\\n                            self.d_config.hard_label_weight_scheduler(global_step/total_global_steps)\n\n                    if (global_step) % print_every == 0:\n                        logger.info(f""Global step: {global_step}, epoch step:{step+1}"")\n                    if (global_step%ckpt_steps==0) or global_step==total_global_steps:\n                        self.save_and_callback(global_step, step, 0, callback)\n            logger.info(""Training finished"")\n            return\n\n\n        train_steps_per_epoch = len(dataloader)//self.t_config.gradient_accumulation_steps\n        total_global_steps = train_steps_per_epoch * num_epochs\n        print_every = train_steps_per_epoch // self.print_freq\n        if print_every == 0:\n            print_every = train_steps_per_epoch\n        checkpoints = [int(train_steps_per_epoch*ci/self.t_config.ckpt_frequency) for ci in range(self.t_config.ckpt_frequency)]\n        logger.info(f""Training steps per epoch: {train_steps_per_epoch}"")\n        logger.info(f""Checkpoints(step): {checkpoints}"")\n\n        global_step = 0\n        writer_step = 0\n\n        if self.d_config.is_caching_logits is True:\n            logger.info(f""Caching batches and teacher\'s logits..."")\n            for step, batch in tqdm(enumerate(dataloader),disable=None):\n                self.cache_logits(batch, args, batch_postprocessor)\n\n        for current_epoch in tqdm(range(int(num_epochs)),disable=None):\n            logger.info(f""Epoch {current_epoch+1}"")\n            optimizer.zero_grad()\n            if self.d_config.is_caching_logits:\n                random.shuffle(self.logits_cache)\n                dataloader = self.logits_cache\n            logger.info(f""Length of current epoch in forward batch: {len(dataloader)}"")\n            for step, batch in tqdm(enumerate(dataloader),disable=None):\n                if self.d_config.is_caching_logits is False and batch_postprocessor is not None:\n                        batch = batch_postprocessor(batch)\n                total_loss = self.train_on_batch(batch,args)\n                total_loss /= self.t_config.gradient_accumulation_steps\n                total_loss.backward()\n\n                self.write_loss(total_loss, writer_step)\n                writer_step += 1\n\n                if (step+1)%self.t_config.gradient_accumulation_steps == 0:\n                    if max_grad_norm > 0:\n                        torch.nn.utils.clip_grad_norm_(self.model_S.parameters(), max_grad_norm)\n                    optimizer.step()\n                    if scheduler is not None:\n                        scheduler.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n                    if self.d_config.kd_loss_weight_scheduler is not None:\n                        self.d_config.kd_loss_weight = \\\n                            self.d_config.kd_loss_weight_scheduler(global_step/total_global_steps)\n                    if self.d_config.hard_label_weight_scheduler is not None:\n                        self.d_config.hard_label_weight = \\\n                            self.d_config.hard_label_weight_scheduler(global_step/total_global_steps)\n\n                    if (global_step) % print_every == 0:\n                        logger.info(f""Global step: {global_step}, epoch step:{step+1}"")\n                        #logger.info(f""lrs:{[g[\'lr\'] for g in optimizer.param_groups]}"")\n                    if (global_step%train_steps_per_epoch in checkpoints) \\\n                            and ((current_epoch+1)%self.t_config.ckpt_epoch_frequency==0 or current_epoch+1==num_epochs):\n                        self.save_and_callback(global_step, step, current_epoch, callback)\n\n            logger.info(f""Epoch {current_epoch+1} finished"")\n\n    def train_on_batch(self, batch, args):\n        if self.d_config.is_caching_logits is False:\n            if type(batch) is dict:\n                for k,v in batch.items():\n                    if type(v) is torch.Tensor:\n                        batch[k] = v.to(self.t_config.device)\n                with torch.no_grad():\n                    results_T = self.model_T(**batch, **args)\n                results_S = self.model_S(**batch, **args)\n            else:\n                batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n                with torch.no_grad():\n                    results_T = self.model_T(*batch, **args)\n                results_S = self.model_S(*batch, **args)\n            results_T = post_adaptor(self.adaptor_T(batch,results_T))\n            results_S = post_adaptor(self.adaptor_S(batch,results_S))\n\n        else:\n            batch, cached_logits = batch\n            if type(batch) is dict:\n                new_batch = {}\n                for k,v in batch.items():\n                    if type(v) is torch.Tensor:\n                        new_batch[k] = v.to(self.t_config.device)\n                    else:\n                        new_batch[k] = v\n                batch = new_batch\n                results_S = self.model_S(**batch, **args)\n            else:\n                batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n                results_S = self.model_S(*batch, **args)\n            results_S = post_adaptor(self.adaptor_S(batch,results_S))\n\n            results_T = {\'logits\':[logits.to(self.t_config.device) for logits in cached_logits]}\n            if \'logits_mask\' in results_S:\n                results_T[\'logits_mask\'] = results_S[\'logits_mask\']\n    \n        logits_list_T = results_T[\'logits\']  # list of tensor\n        logits_list_S = results_S[\'logits\']  # list of tensor\n        total_loss  = 0\n\n        if \'logits_mask\' in results_S:\n            masks_list_S = results_S[\'logits_mask\']\n            logits_list_S = select_logits_with_mask(logits_list_S,masks_list_S)  #(mask_sum, num_of_class)\n        if \'logits_mask\' in results_T:\n            masks_list_T = results_T[\'logits_mask\']\n            logits_list_T = select_logits_with_mask(logits_list_T,masks_list_T)  #(mask_sum, num_of_class)\n\n        if self.d_config.probability_shift is True:\n            labels_list = results_S[\'labels\']\n            for l_T, l_S, labels in zip(logits_list_T, logits_list_S, labels_list):\n                l_T = probability_shift_(l_T, labels)\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                kd_loss = self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n                total_loss += kd_loss\n        else:\n            for l_T,l_S in zip(logits_list_T,logits_list_S):\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                kd_loss = self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n                total_loss += kd_loss\n\n        if \'losses\' in results_S:\n            for loss in results_S[\'losses\']:\n                # in case of multi-GPU\n                total_loss += loss.mean() * self.d_config.hard_label_weight\n\n        return total_loss\n\n    def cache_logits(self, batch, args, batch_postprocessor):\n            if batch_postprocessor is not None:\n                batch = batch_postprocessor(batch)\n\n            if type(batch) is dict:\n                new_batch = {}\n                for k,v in batch.items():\n                    if type(v) is torch.Tensor:\n                        new_batch[k] = v.to(self.t_config.device)\n                    else:\n                        new_batch[k] = v\n                with torch.no_grad():\n                    results_T = self.model_T(**new_batch, **args)\n            else:\n                new_batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n                with torch.no_grad():\n                    results_T = self.model_T(*new_batch, **args)\n            results_T = post_adaptor(self.adaptor_T(batch,results_T))\n\n            self.logits_cache.append([batch, [logits.to(\'cpu\') for logits in results_T[\'logits\']]])'"
src/textbrewer/distiller_general.py,8,"b'\nfrom .distiller_utils import *\nfrom .distiller_basic import BasicDistiller\n\nclass GeneralDistiller(BasicDistiller):\n    """"""\n    Supports intermediate features matching. **Recommended for single-teacher single-task distillation**.\n\n    Args:\n        train_config (:class:`TrainingConfig`): training configuration.\n        distill_config (:class:`DistillationConfig`): distillation configuration.\n        model_T (:class:`torch.nn.Module`): teacher model.\n        model_S (:class:`torch.nn.Module`): student model.\n        adaptor_T (Callable): teacher model\'s adaptor.\n        adaptor_S (Callable): student model\'s adaptor.\n        custom_matches (list): supports more flexible user-defined matches (testing).\n\n    The roles of `adaptor_T` and `adaptor_S` are explained in :py:func:`adaptor`.\n\n    """"""\n    def __init__(self, train_config,\n                 distill_config,\n                 model_T,\n                 model_S,\n                 adaptor_T,\n                 adaptor_S,\n                 custom_matches: Optional[List[CustomMatch]] = None):\n        # custom_matches=[{\'module_T\': module_T, \'module_S\':module_S,\n        #                 \'loss\': loss, \'weight\': weight},...]\n        super(GeneralDistiller, self).__init__(train_config, distill_config, model_T, model_S, adaptor_T, adaptor_S)\n\n        self.projs = []\n        self.projs_group = []\n        for im in self.d_config.intermediate_matches:\n            if im.proj is not None:\n                projection = im.proj[0]\n                dim_in = im.proj[1]\n                dim_out = im.proj[2]\n                self.projs_group.append(im.proj[3])\n                self.projs.append(PROJ_MAP[projection](dim_in,dim_out))\n                self.projs[-1].to(self.t_config.device)\n            else:\n                self.projs.append(None)\n                self.projs_group.append(None)\n\n        self.has_custom_matches = False\n        if custom_matches:\n            self.handles_T = []\n            self.handles_S = []\n            self.custom_matches_cache = {\'hook_outputs_T\': [], \'hook_outputs_S\': [], \'match_proj_funcs\': [],\n                                         \'match_weights\':  [], \'match_losses\':   [], \'match_proj_groups\': []}\n            for match in custom_matches:\n                self.add_match(match)\n            self.has_custom_matches = True\n        \n        self.d_config.is_caching_logits = False\n\n    def save_and_callback(self,global_step, step, epoch, callback):\n        if self.has_custom_matches:\n            handles_T = self.model_T._forward_hooks\n            handles_S = self.model_S._forward_hooks\n            self.model_S._forward_hooks = OrderedDict()  # clear hooks\n            self.model_T._forward_hooks = OrderedDict()\n\n        super(GeneralDistiller, self).save_and_callback(global_step, step, epoch, callback)\n\n        if self.has_custom_matches:\n            self.model_S._forward_hooks = handles_S  # restore hooks\n            self.model_T._forward_hooks = handles_T\n\n    def train(self, optimizer, dataloader, num_epochs, scheduler_class=None, scheduler_args=None, scheduler=None, max_grad_norm = -1.0, num_steps=None, callback=None, batch_postprocessor=None, **args):\n        """"""\n        trains the student model. See :meth:`BasicDistiller.train`.\n        """"""\n        # update optimizer for projection layer\n        for proj,proj_group in zip(self.projs, self.projs_group):\n            if proj is not None:\n                assert isinstance(proj,nn.Module)\n                optimizer.add_param_group({**{\'params\':proj.parameters()},**proj_group})\n\n        if self.has_custom_matches:\n            for proj_func,proj_group in zip(self.custom_matches_cache[\'match_proj_funcs\'],\n                                                   self.custom_matches_cache[\'match_proj_groups\']):\n                if isinstance(proj_func,nn.Module):\n                    optimizer.add_param_group({**{\'params\':proj_func.parameters()},**proj_group})\n\n        logger.debug(""Optimizer param group: "")\n        logger.debug(f""{[[s.shape for s in g[\'params\']] for g in optimizer.param_groups]}"")\n\n        super(GeneralDistiller, self).train(optimizer, dataloader, num_epochs,  scheduler_class, scheduler_args, scheduler, max_grad_norm, num_steps, callback, batch_postprocessor, **args)\n\n    def train_on_batch(self, batch, args):\n        if type(batch) is dict:\n            for k,v in batch.items():\n                if type(v) is torch.Tensor:\n                    batch[k] = v.to(self.t_config.device)\n            with torch.no_grad():\n                results_T = self.model_T(**batch, **args)\n            results_S = self.model_S(**batch, **args)\n        else:\n            moved_batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n            batch = moved_batch\n            with torch.no_grad():\n                results_T = self.model_T(*batch, **args)\n            results_S = self.model_S(*batch, **args)\n\n        results_T = post_adaptor(self.adaptor_T(batch,results_T))\n        results_S = post_adaptor(self.adaptor_S(batch,results_S))\n\n        total_loss  = 0\n        if \'logits\' in results_T and \'logits\' in results_S:\n            logits_list_T = results_T[\'logits\']  # list of tensor\n            logits_list_S = results_S[\'logits\']  # list of tensor\n\n            if \'logits_mask\' in results_S:\n                masks_list_S = results_S[\'logits_mask\']\n                logits_list_S = select_logits_with_mask(logits_list_S,masks_list_S)  #(mask_sum, num_of_class)\n            if \'logits_mask\' in results_T:\n                masks_list_T = results_T[\'logits_mask\']\n                logits_list_T = select_logits_with_mask(logits_list_T,masks_list_T)  #(mask_sum, num_of_class)\n\n            if self.d_config.probability_shift is True:\n                labels_list = results_S[\'labels\']\n                for l_T, l_S, labels in zip(logits_list_T, logits_list_S, labels_list):\n                    l_T = probability_shift_(l_T, labels)\n                    if self.d_config.temperature_scheduler is not None:\n                        temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                    else:\n                        temperature = self.d_config.temperature\n                    kd_loss = self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n                    total_loss += kd_loss\n            else:\n                for l_T,l_S in zip(logits_list_T,logits_list_S):\n                    if self.d_config.temperature_scheduler is not None:\n                        temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                    else:\n                        temperature = self.d_config.temperature\n                    kd_loss = self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n                    total_loss += kd_loss\n\n        inters_T = {feature: results_T.get(feature,[]) for feature in FEATURES}\n        inters_S = {feature: results_S.get(feature,[]) for feature in FEATURES}\n        inputs_mask_T = results_T.get(\'inputs_mask\',None)\n        inputs_mask_S = results_S.get(\'inputs_mask\',None)\n        for ith,inter_match in enumerate(self.d_config.intermediate_matches):\n            layer_T = inter_match.layer_T\n            layer_S = inter_match.layer_S\n            feature = inter_match.feature\n            loss_type = inter_match.loss\n            match_weight = inter_match.weight\n            match_loss = MATCH_LOSS_MAP[loss_type]\n\n            if type(layer_S) is list and type(layer_T) is list:\n                inter_S = [inters_S[feature][s] for s in layer_S]\n                inter_T = [inters_T[feature][t] for t in layer_T]\n                if self.projs[ith]:\n                    #inter_T = [self.projs[ith](t) for t in inter_T]\n                    inter_S = [self.projs[ith](s) for s in inter_S]\n            else:\n                inter_S = inters_S[feature][layer_S]\n                inter_T = inters_T[feature][layer_T]\n                if self.projs[ith]:\n                    #inter_T = self.projs[ith](inter_T)\n                    inter_S = self.projs[ith](inter_S)\n            total_loss += match_loss(inter_S, inter_T, mask=inputs_mask_S) * match_weight\n\n\n        if self.has_custom_matches:\n            for hook_T, hook_S, match_weight, match_loss, proj_func  in \\\n                    zip(self.custom_matches_cache[\'hook_outputs_T\'], self.custom_matches_cache[\'hook_outputs_S\'],\n                        self.custom_matches_cache[\'match_weghts\'], self.custom_matches_cache[\'match_losses\'],\n                        self.custom_matches_cache[\'match_proj_funcs\']):\n                if proj_func is not None:\n                    hook_S = proj_func(hook_S)\n                total_loss += match_weight * match_loss(hook_S,hook_T,inputs_mask_S,inputs_mask_T)\n            self.custom_matches_cache[\'hook_outputs_T\'] = []\n            self.custom_matches_cache[\'hook_outputs_S\'] = []\n\n        if \'losses\' in results_S:\n            for loss in results_S[\'losses\']:\n                # in case of multi-GPU\n                total_loss += loss.mean() * self.d_config.hard_label_weight\n\n        return total_loss\n\n    def add_match(self,match: CustomMatch):\n        if type(match.module_T) is str or type(match.module_S) is str:\n            raise NotImplementedError\n        else:\n            module_T   = match.module_T\n            module_S   = match.module_S\n            weight     = match.weight\n            loss       = match.loss\n            proj_func = match.proj_func\n            proj_group = match.proj_group\n        self.add_match_by_module(module_T,module_S,proj_func,proj_group,weight,loss)\n\n\n    def add_match_by_module(self,module_T : torch.nn.Module,\n                                 module_S : torch.nn.Module,\n                                 proj_func, proj_group,\n                                 match_weight, match_loss):\n\n        self.handles_T = module_T.register_forward_hook(self._hook_T)\n        self.handles_S = module_S.register_forward_hook(self._hook_S)\n        self.custom_matches_cache[\'match_weights\'].append(match_weight)\n        self.custom_matches_cache[\'match_losses\'].append(match_loss)\n        self.custom_matches_cache[\'match_proj_funcs\'].append(proj_func)\n        if isinstance(proj_func,nn.Module):\n            self.custom_matches_cache[\'match_proj_funcs\'][-1].to(self.t_config.device)\n        self.custom_matches_cache[\'match_proj_groups\'].append(proj_group)\n\n    def _hook_T(self,module,input, output):\n        self.custom_matches_cache[\'hook_outputs_T\'].append(output)\n\n    def _hook_S(self, module, input, output):\n        self.custom_matches_cache[\'hook_outputs_S\'].append(output)\n\n'"
src/textbrewer/distiller_multitask.py,7,"b'from .distiller_utils import *\nfrom .distiller_basic import BasicDistiller\n\nclass MultiTaskDistiller(BasicDistiller):\n    """"""\n    distills multiple teacher models (of different tasks) into a single student. **It doesn\'t support intermediate feature matching**.\n\n    Args:\n        train_config (:class:`TrainingConfig`): training configuration.\n        distill_config (:class:`DistillationConfig`): distillation configuration.\n        model_T (dict): dict of teacher models: {task1:model1, task2:model2, .... }. Keys are tasknames.\n        model_S (torch.nn.Module): student model.\n        adaptor_T (dict): dict of teacher adaptors: {task1:adpt1, task2:adpt2, .... }. Keys are tasknames.\n        adaptor_S (dict): dict of student adaptors: {task1:adpt1, task2:adpt2, .... }. Keys are tasknames.\n\n    """"""\n    \n    def __init__(self, train_config,\n                 distill_config,\n                 model_T,\n                 model_S,\n                 adaptor_T,\n                 adaptor_S):\n\n        super(MultiTaskDistiller, self).__init__(\n            train_config, distill_config,\n            model_T, model_S,\n            adaptor_T, adaptor_S)\n        if hasattr(self.adaptor_T,\'__iter__\'):\n            assert len(self.adaptor_T)==len(self.model_T)==len(self.adaptor_S)\n        assert (self.d_config.kd_loss_weight_scheduler is None) and (self.d_config.hard_label_weight_scheduler is None),\\\n                ""BasicMultiTaskDistiller does not support WEIGHT_SCHEDULER in the current version.""\n\n        self.d_config.is_caching_logits = False\n\n    def train(self, optimizer, dataloaders, num_steps, scheduler_class=None, scheduler_args=None, scheduler=None, max_grad_norm = -1.0, tau=1, callback=None, batch_postprocessors=None, **args):\n        """"""\n        trains the student model.\n\n        Args:\n            optimizer: optimizer.\n            dataloaders (dict): dict of dataset iterator. Keys are tasknames, values are corresponding dataloaders.\n            num_steps (int): number of training steps.\n            scheduler_class (class): the class of the scheduler to be constructed.\n            scheduler_args (dict): arguments (excluding `optimizer`) passed to the `scheduler_class` to construct the scheduler object.\n            scheduler (deprecated): used to adjust learning rate, optional, can be None, is deprecated in favor of `scheduler_class` and `scheduler_args`.\n            max_grad_norm (float): Maximum norm for the gradients (-1 means no clipping). Default: -1.0\n            tau (float): the probability of sampling an example from task `d` is proportional to \\|d\\|^{tau}, where \\|d\\| is the size of `d`\'s training set. If the size of any dataset is unknown, ignores tau and samples examples unifromly from each dataset.\n            callback (Callable): function called after each epoch, can be None. It is called as ``callback(model=self.model_S, step = global_step)``. It can be used to do evaluation of the model at each checkpoint.\n            batch_postprocessors (dict): a dict of batch_postprocessors. Keys are tasknames, values are corresponding batch_postprocessors. Each batch_postprocessor should take a batch and return a batch.\n            **args: additional arguments fed to the model.\n        """"""\n\n        # update scheduler\n        if scheduler_class is not None:\n            # overwrite scheduler\n            scheduler = scheduler_class(**{\'optimizer\':optimizer},**scheduler_args)\n\n        total_global_steps = num_steps\n        ckpt_steps =self.t_config.ckpt_steps\n        print_every = ckpt_steps // self.print_freq\n        if print_every == 0:\n            print_every = ckpt_steps\n        checkpoints = [ i * ckpt_steps for i in range(1,num_steps//ckpt_steps+1)] + [total_global_steps]\n        logger.info(f""Total training steps: {total_global_steps}"")\n        logger.info(f""Checkpoints(step): {checkpoints}"")\n\n        dataiters = {k:cycle(v) for k,v in dataloaders}\n        if all(hasattr(v,\'__len__\') for v in dataloaders.values()):\n            dataloader_sizes = {k:len(v) for k,v in dataloaders.items()}\n            total_size = sum(v for k,v in dataloader_sizes.items())//self.t_config.gradient_accumulation_steps\n            logger.info(f""Total size of all datasets (in number of batch_size):{total_size}"")\n            Z = sum(pow(v,tau) for v in dataloader_sizes.values())\n            tasknames, sampling_weights = zip(*((k,pow(v,tau)/Z) for k,v in dataloader_sizes.items()))\n        else:\n            logger.info(""The size of some datasets are unknown, so tau=1"")\n            tasknames = tuple(dataloaders.keys())\n            sampling_weights = None\n\n            \n        global_step = 0\n        writer_step = 0\n        optimizer.zero_grad()\n        while global_step < num_steps:\n            global_step += 1\n            for _ in range(self.t_config.gradient_accumulation_steps):\n                #sampling taskname\n                taskname = np.random.choice(tasknames,p=sampling_weights)\n                dataiter = dataiters[taskname]\n                batch = next(dataiter)\n                if batch_postprocessors is not None:\n                    batch = batch_postprocessors[taskname](batch)\n                batch_taskname = (batch, taskname)\n                total_loss = self.train_on_batch(batch_taskname, args)\n                total_loss /= self.t_config.gradient_accumulation_steps\n                total_loss.backward()\n\n                scalar_total_loss = total_loss.cpu().item() * self.t_config.gradient_accumulation_steps\n                self.tb_writer.add_scalar(\'scalar/total_loss\', scalar_total_loss, writer_step)\n                writer_step += 1\n            if max_grad_norm > 0:\n                torch.nn.utils.clip_grad_norm_(self.model_S.parameters(), max_grad_norm) \n            optimizer.step()\n            if scheduler is not None:\n                scheduler.step()\n            optimizer.zero_grad()\n\n            if self.d_config.kd_loss_weight_scheduler is not None:\n                self.d_config.kd_loss_weight = \\\n                    self.d_config.kd_loss_weight_scheduler(global_step/total_global_steps)\n            if self.d_config.hard_label_weight_scheduler is not None:\n                self.d_config.hard_label_weight = \\\n                    self.d_config.hard_label_weight_scheduler(global_step/total_global_steps)\n\n            if (global_step) % print_every == 0:\n                logger.info(f""Global step: {global_step}/{num_steps}"")\n            if (global_step % ckpt_steps == 0) or global_step==total_global_steps:\n                self.save_and_callback(global_step, global_step-1, 0, callback)\n        logger.info(""Training finished"")\n\n    def train_on_batch(self, batch_taskname, args) -> torch.Tensor:\n        # Basic uses no cache\n        batch, taskname = batch_taskname\n        model_T = self.model_T[taskname]\n        adaptor_T = self.adaptor_T[taskname]\n        adaptor_S = self.adaptor_S[taskname]\n\n        if type(batch) is dict:\n            for k,v in batch.items():\n                if type(v) is torch.Tensor:\n                    batch[k] = v.to(self.t_config.device)\n            with torch.no_grad():\n                results_T = model_T(**batch, **args)\n            results_S = self.model_S(**batch, **args)\n        else:\n            moved_batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n            batch = moved_batch\n            with torch.no_grad():\n                results_T = model_T(*batch, **args)\n            results_S = self.model_S(*batch, **args)\n\n        results_T = post_adaptor(adaptor_T(batch,results_T))\n        results_S = post_adaptor(adaptor_S(batch,results_S))\n\n        logits_list_T = results_T[\'logits\']  # list of tensor\n        logits_list_S = results_S[taskname][\'logits\']  # list of tensor\n        total_loss  = 0\n\n        if \'logits_mask\' in results_S[taskname]:\n            masks_list_S = results_S[taskname][\'logits_mask\']\n            logits_list_S = select_logits_with_mask(logits_list_S,masks_list_S)  #(mask_sum, num_of_class)\n        if \'logits_mask\' in results_T: #TODO\n            masks_list_T = results_T[\'logits_mask\']\n            logits_list_T = select_logits_with_mask(logits_list_T,masks_list_T)\n\n        if self.d_config.probability_shift is True:\n            labels_list = results_S[\'labels\']\n            for l_T, l_S, labels in zip(logits_list_T, logits_list_S, labels_list):\n                l_T = probability_shift_(l_T, labels)\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                kd_loss = self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n                total_loss += kd_loss\n        else:\n            for l_T,l_S in zip(logits_list_T,logits_list_S):\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                total_loss += self.kd_loss(l_S, l_T, temperature) * self.d_config.kd_loss_weight\n\n        if \'losses\' in results_S:\n            for loss in results_S[taskname][\'losses\']:\n                # in case of multi-GPU\n                total_loss += loss.mean() * self.d_config.hard_label_weight\n\n        return total_loss'"
src/textbrewer/distiller_multiteacher.py,12,"b'from .distiller_utils import *\nfrom .distiller_basic import BasicDistiller\n\nclass MultiTeacherDistiller(BasicDistiller):\n    """"""\n    Distills multiple teacher models (of the same tasks) into a student model. **It doesn\'t support intermediate feature matching**.\n\n    Args:\n        train_config (:class:`TrainingConfig`): training configuration.\n        distill_config (:class:`DistillationConfig`): distillation configuration.\n        model_T (List[torch.nn.Module]): list of teacher models.\n        model_S (torch.nn.Module): student model.\n        adaptor_T (Callable): teacher model\'s adaptor.\n        adaptor_S (Callable): student model\'s adaptor.\n\n    The roles of `adaptor_T` and `adaptor_S` are explained in :py:func:`adaptor`.\n    """"""\n\n    def __init__(self, train_config,\n                 distill_config,\n                 model_T,\n                 model_S,\n                 adaptor_T,\n                 adaptor_S):\n        super(MultiTeacherDistiller, self).__init__(\n            train_config, distill_config,\n            model_T, model_S,\n            adaptor_T, adaptor_S)\n        if hasattr(self.adaptor_T,\'__iter__\'):\n            assert len(self.adaptor_T)==len(self.model_T)\n        self.avg = True\n\n    def train_on_batch(self, batch, args):\n        if self.d_config.is_caching_logits is False:\n            if type(batch) is dict:\n                for k,v in batch.items():\n                    if type(v) is torch.Tensor:\n                        batch[k] = v.to(self.t_config.device)\n                with torch.no_grad():\n                    results_T = [model_t(**batch, **args) for model_t in self.model_T]\n                results_S = self.model_S(**batch, **args)\n            else:\n                moved_batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n                batch = moved_batch\n                with torch.no_grad():\n                    results_T = [model_T(*batch, **args) for model_T in self.model_T]\n                results_S = self.model_S(*batch, **args)\n\n            if hasattr(self.adaptor_T,\'__iter__\'):\n                results_T = [post_adaptor(adpt_t(batch,results_t)) for results_t,adpt_t in zip(results_T,self.adaptor_T)]\n            else:\n                results_T = [post_adaptor(self.adaptor_T(batch,results_t)) for results_t in results_T]\n            results_S = post_adaptor(self.adaptor_S(batch,results_S))\n        else:\n            batch, cached_logits = batch\n            if type(batch) is dict:\n                new_batch = {}\n                for k,v in batch.items():\n                    if type(v) is torch.Tensor:\n                        new_batch[k] = v.to(self.t_config.device)\n                    else:\n                        new_batch[k] = v\n                batch = new_batch\n                results_S = self.model_S(**batch, **args)\n            else:\n                batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n                results_S = self.model_S(*batch, **args)\n            results_S = post_adaptor(self.adaptor_S(batch,results_S))\n            results_T = [{\'logits\': [lo.to(self.t_config.device) for lo in logits]} for logits in cached_logits]\n            if \'logits_mask\' in results_S:\n                results_T[0][\'logits_mask\'] = results_S[\'logits_mask\']\n    \n\n        logits_list_T = [results_t[\'logits\'] for results_t in results_T]  # list of tensor\n        logits_list_S = results_S[\'logits\']  # list of tensor\n        total_loss  = 0\n\n        if \'logits_mask\' in results_S:\n            masks_list_S = results_S[\'logits_mask\']\n            logits_list_S = select_logits_with_mask(logits_list_S,masks_list_S)  #(mask_sum, num_of_class)\n        if \'logits_mask\' in results_T[0]:\n            masks_list_T = results_T[0][\'logits_mask\']\n            logits_list_T = [select_logits_with_mask(logits_list_t,masks_list_T)\n                             for logits_list_t in logits_list_T] #(mask_sum, num_of_class)\n\n        if self.d_config.probability_shift is True:\n            labels_list = results_S[\'labels\']\n            for l_T, l_S, labels in zip(zip(*logits_list_T),logits_list_S,labels_list):\n                mean_l_T = sum(l_T)/len(l_T)\n                mean_l_T = probability_shift_(mean_l_T, labels)\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, mean_l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                total_loss += self.kd_loss(l_S, mean_l_T, temperature) * self.d_config.kd_loss_weight\n        else:\n            for l_T, l_S in zip(zip(*logits_list_T),logits_list_S):\n                mean_l_T = sum(l_T)/len(l_T)\n                if self.d_config.temperature_scheduler is not None:\n                    temperature = self.d_config.temperature_scheduler(l_S, mean_l_T, self.d_config.temperature)\n                else:\n                    temperature = self.d_config.temperature\n                total_loss += self.kd_loss(l_S, mean_l_T, temperature) * self.d_config.kd_loss_weight\n\n        if \'losses\' in results_S:\n            for loss in results_S[\'losses\']:\n                # in case of multi-GPU\n                total_loss += loss.mean() * self.d_config.hard_label_weight\n        return total_loss\n\n    def cache_logits(self, batch, args, batch_postprocessor):\n        if batch_postprocessor is not None:\n            batch = batch_postprocessor(batch)\n\n        if type(batch) is dict:\n            new_batch = {}\n            for k,v in batch.items():\n                if type(v) is torch.Tensor:\n                    new_batch[k] = v.to(self.t_config.device)\n                else:\n                    new_batch[k] = v\n            with torch.no_grad():\n                results_T = [model_t(**new_batch, **args) for model_t in self.model_T]\n        else:\n            new_batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n            with torch.no_grad():\n                results_T = [model_t(*new_batch, **args) for model_t in self.model_T]\n\n        if hasattr(self.adaptor_T,\'__iter__\'):\n            results_T = [post_adaptor(adpt_t(batch,results_t)) for results_t,adpt_t in zip(results_T,self.adaptor_T)]\n        else:\n            results_T = [post_adaptor(self.adaptor_T(batch,results_t)) for results_t in results_T]\n\n        self.logits_cache.append([batch, [[logits.to(\'cpu\') for logits in results_t[\'logits\']] for results_t in results_T]])'"
src/textbrewer/distiller_train.py,9,"b'from .distiller_utils import *\n\nclass BasicTrainer:\n    """"""\n    It performs supervised training, not distillation. It can be used for training the teacher model.\n\n    Args:\n        train_config (:class:`TrainingConfig`): training configuration.\n        model (:class:`torch.nn.Module`): model to be trained.\n        adaptor (Callable)\xef\xbc\x9aadaptor of the model.\n    \n    The role of `adaptor` is explained in :py:func:`adaptor`.\n    """"""\n\n    def __enter__(self):\n        self.model_is_training = self.model.training\n        self.model.train()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Restore model status\n        self.model.train(self.model_is_training)\n\n    def __init__(self, train_config: TrainingConfig,\n                 model: torch.nn.Module, adaptor):\n        super(BasicTrainer, self).__init__()\n        self.t_config = train_config\n        self.model = model\n        self.adaptor = adaptor\n        if self.t_config.log_dir is not None:\n            self.tb_writer = SummaryWriter(log_dir = self.t_config.log_dir)\n        else:\n            self.tb_writer = no_op\n        self.print_freq = 20\n\n    def train(self, optimizer, dataloader, num_epochs, scheduler_class=None, scheduler_args=None, scheduler=None, max_grad_norm = -1.0, num_steps=None, callback=None, batch_postprocessor=None, **args):\n        """"""\n        trains the model. See :meth:`BasicDistiller.train`.\n        """"""\n        # update scheduler\n        if scheduler_class is not None:\n            # overwrite scheduler\n            scheduler = scheduler_class(**{\'optimizer\':optimizer},**scheduler_args)\n\n        if num_steps is not None:\n            total_global_steps = num_steps\n            ckpt_steps =self.t_config.ckpt_steps\n            print_every = ckpt_steps // self.print_freq\n            if print_every == 0:\n                print_every = ckpt_steps\n            checkpoints = [ i * ckpt_steps for i in range(1,num_steps//ckpt_steps+1)] + [total_global_steps]\n            logger.info(f""Total training steps: {total_global_steps}"")\n            logger.info(f""Checkpoints: {checkpoints}"")\n\n            global_step = 0\n            writer_step = 0\n            for step, batch in tqdm(enumerate(cycle(dataloader)),disable=None):\n                if batch_postprocessor is not None:\n                    batch = batch_postprocessor(batch)\n                total_loss = self.train_on_batch(batch,args)\n                total_loss /= self.t_config.gradient_accumulation_steps\n                total_loss.backward()\n\n                scalar_total_loss = total_loss.cpu().item() * self.t_config.gradient_accumulation_steps\n                self.tb_writer.add_scalar(\'scalar/total_loss\', scalar_total_loss, writer_step)\n                writer_step += 1\n\n                if (step+1)%self.t_config.gradient_accumulation_steps == 0:\n                    if max_grad_norm > 0:\n                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm) \n                    optimizer.step()\n                    if scheduler is not None:\n                        scheduler.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n                    if (global_step) % print_every == 0:\n                        logger.info(f""Global step: {global_step}, epoch step:{step+1}"")\n                    if (global_step%ckpt_steps==0) or global_step==total_global_steps:\n                        logger.info(f""Saving at global step {global_step}"")\n                        coreModel = self.model.module if \\\n                            \'DataParallel\' in self.model.__class__.__name__ else self.model\n                        state_dict = coreModel.state_dict()\n                        torch.save(state_dict, os.path.join(self.t_config.output_dir,f""gs{global_step}.pkl""))\n                        if callback is not None:\n                            logger.info(""Running callback function..."")\n                            callback(model=self.model, step=global_step)\n                            self.model.train()\n            logger.info(""Training finished"")\n            return\n\n        train_steps_per_epoch = len(dataloader)//self.t_config.gradient_accumulation_steps\n        print_every = train_steps_per_epoch // self.print_freq\n        if print_every == 0:\n            print_every = train_steps_per_epoch\n        checkpoints = [int(train_steps_per_epoch*ci/self.t_config.ckpt_frequency) for ci in range(self.t_config.ckpt_frequency)]\n        logger.info(f""Training steps per epoch: {train_steps_per_epoch}"")\n        logger.info(f""Checkpoints(step): {checkpoints}"")\n\n        global_step = 0\n        writer_step = 0\n        for current_epoch in tqdm(range(int(num_epochs)),disable=None):\n            logger.info(f""Epoch {current_epoch+1}"")\n            optimizer.zero_grad()\n            logger.info(f""Length of current epoch in forward batch: {len(dataloader)}"")\n            for step, batch in tqdm(enumerate(dataloader),disable=None):\n                if batch_postprocessor is not None:\n                    batch = batch_postprocessor(batch)\n                total_loss = self.train_on_batch(batch,args)\n                total_loss /= self.t_config.gradient_accumulation_steps\n                total_loss.backward()\n\n                scalar_total_loss = total_loss.cpu().item() * self.t_config.gradient_accumulation_steps\n                self.tb_writer.add_scalar(\'scalar/total_loss\', scalar_total_loss, writer_step)\n                writer_step += 1\n\n                if (step+1)%self.t_config.gradient_accumulation_steps == 0:\n                    if max_grad_norm > 0:\n                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n                    optimizer.step()\n                    if scheduler is not None:\n                        scheduler.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n                    if (global_step) % print_every == 0:\n                        logger.info(f""Global step: {global_step}, epoch step:{step+1}"")\n                    if (global_step%train_steps_per_epoch in checkpoints) \\\n                            and ((current_epoch+1)%self.t_config.ckpt_epoch_frequency==0 or current_epoch+1==num_epochs):\n                        logger.info(f""Saving at global step {global_step}, epoch step {step+1} epoch {current_epoch+1}"")\n                        coreModel = self.model.module if \\\n                            \'DataParallel\' in self.model.__class__.__name__ else self.model\n                        state_dict = coreModel.state_dict()\n                        torch.save(state_dict, os.path.join(self.t_config.output_dir,f""gs{global_step}.pkl""))\n                        if callback is not None:\n                            logger.info(""Running callback function..."")\n                            callback(model=self.model, step=global_step)\n                            self.model.train()\n\n            logger.info(f""Epoch {current_epoch+1} finished"")\n\n    def train_on_batch(self, batch, args) -> torch.Tensor:\n        if type(batch) is dict:\n            for k,v in batch.items():\n                if type(v) is torch.Tensor:\n                    batch[k] = v.to(self.t_config.device)\n            results = self.model(**batch, **args)\n        else:\n            moved_batch = tuple(item.to(self.t_config.device) if type(item) is torch.Tensor else item for item in batch)\n            batch = moved_batch\n            results = self.model(*batch, **args)\n\n        results = post_adaptor(self.adaptor(batch,results))\n        total_loss  = 0\n\n        if \'losses\' not in results:\n            raise KeyError(""\'losses\' not in the output of adaptor. Nothing to optimize!"")\n        else:\n            for loss in results[\'losses\']:\n                # in case of multi-GPU\n                total_loss += loss.mean()\n\n        return total_loss'"
src/textbrewer/distiller_utils.py,6,"b'import torch\nfrom collections import OrderedDict\nfrom tqdm import tqdm\nfrom torch import nn\ntry:\n    from tensorboardX import SummaryWriter\nexcept ImportError:\n    from torch.utils.tensorboard import SummaryWriter\nimport os, random, json\nimport numpy as np\nimport logging\nfrom typing import Optional, Dict, Union\nfrom .presets import *\nfrom .configurations import TrainingConfig, DistillationConfig\nimport random\nfrom .compatibility import mask_dtype\n\nlogger = logging.getLogger(""Distillation"")\nlogger.setLevel(logging.INFO)\n\nhandler_stream = logging.StreamHandler()\nhandler_stream.setLevel(logging.INFO)\nformatter = logging.Formatter(fmt=\'%(asctime)s - %(levelname)s - %(name)s -  %(message)s\', datefmt=\'%Y/%m/%d %H:%M:%S\')\nhandler_stream.setFormatter(formatter)\nlogger.addHandler(handler_stream)\n\nclass CustomMatch:\n    def __init__(self, module_T, module_S, weight, loss,\n                 proj_func =None, proj_group = None):\n        self.module_T = module_T\n        self.module_S = module_S\n        self.loss     = loss,\n        self.weight   = weight,\n        self.proj_func     = proj_func\n        if proj_group is None:\n            self.proj_group = dict()\n        else:\n            self.proj_group = proj_group\n    def to_dict(self):\n        return {\'module_T\':self.module_T,\n                \'module_S\':self.module_S,\n                \'weight\':self.weight,\n                \'loss\':self.loss,\n                \'proj_func\':self.proj_func,\n                \'proj_group\':self.proj_group}\n    @classmethod\n    def from_dict(cls,dict_object):\n        return cls(**dict_object)\n\n\nclass DistillationContext:\n    def __init__(self):\n        self.model_S = None\n        self.model_T = None\n    def __enter__(self):\n        if isinstance(self.model_T,(list,tuple)):\n            self.model_T_is_training = [model_t.training for model_t in self.model_T]\n            for model_t in self.model_T:\n                model_t.eval()\n        elif isinstance(self.model_T,dict):\n            self.model_T_is_training = {name:model.training for name,model in self.model_T.items()}\n            for name in self.model_T:\n                self.model_T[name].eval()\n        else:\n            self.model_T_is_training = self.model_T.training\n            self.model_T.eval()\n\n        self.model_S_is_training = self.model_S.training\n        self.model_S.train()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        #Restore model status\n        if isinstance(self.model_T,(list,tuple)):\n            for i in range(len(self.model_T_is_training)):\n                self.model_T[i].train(self.model_T_is_training[i])\n        elif isinstance(self.model_T,dict):\n            for name,is_training  in self.model_T_is_training.items():\n                self.model_T[name].train(is_training)\n        else:\n            self.model_T.train(self.model_T_is_training)\n\n        self.model_S.train(self.model_S_is_training)\n\n\nclass AbstractDistiller(DistillationContext):\n    def __init__(self, train_config: TrainingConfig,\n                       distill_config: DistillationConfig,\n                       model_T, model_S, adaptor_T, adaptor_S):\n        super(AbstractDistiller, self).__init__()\n        self.t_config = train_config\n        self.d_config = distill_config\n\n        self.model_T = model_T\n        self.model_S = model_S\n        self.adaptor_S = adaptor_S\n        self.adaptor_T = adaptor_T\n\n        self.kd_loss = KD_LOSS_MAP[self.d_config.kd_loss_type]\n        if self.t_config.log_dir is not None:\n            self.tb_writer = SummaryWriter(log_dir = self.t_config.log_dir)\n        else:\n            self.tb_writer = no_op\n        \n        self.print_freq = 20\n\n        self.logits_cache = []\n\n\ndef select_logits_with_mask(logits_list, masks_list):\n    output_logits = []\n    if len(masks_list)==len(logits_list):\n        for logits,mask in zip(logits_list,masks_list):\n            if len(logits.shape)==3:\n                mask = mask.unsqueeze(-1).expand_as(logits).to(mask_dtype)\n                logits_select = torch.masked_select(logits,mask).view(-1,logits.size(-1))\n            else:\n                logits_select = logits #Logits_mask has no effect on logits of shape (batch_size, logits_to_be_softmaxed)\n            output_logits.append(logits_select)\n    elif len(masks_list)==1:\n        mask = masks_list[0]\n        for logits in logits_list:\n            if len(logits.shape)==3:\n                mask = mask.unsqueeze(-1).expand_as(logits).to(mask_dtype)\n                logits_select = torch.masked_select(logits,mask).view(-1,logits.size(-1))\n            else:\n                logits_select = logits #Logits_mask has no effect on logits of shape (batch_size, logits_to_be_softmaxed)\n            output_logits.append(logits_select)\n    else:\n        raise AssertionError(""lengths of logits list and masks list mismatch"")\n    return output_logits\n\n\nclass BasicAdaptor:\n    def __init__(self):\n        self.batch = None\n        self.model_outputs = None\n    def __call__(self,batch,model_outputs):\n        self.batch = batch\n        self.model_outputs = model_outputs\n    def __getattr__(self, item):\n        raise NotImplementedError\n\n\ndef post_adaptor(dict_object):\n    if \'logits\' in dict_object:\n        logits = dict_object[\'logits\']\n        if not isinstance(logits,(list,tuple)):\n            dict_object[\'logits\'] = [ logits ]\n    if \'logits_mask\' in dict_object:\n        logits_mask = dict_object[\'logits_mask\']\n        if not isinstance(logits_mask,(list,tuple)):\n            dict_object[\'logits_mask\'] = [ logits_mask ]\n    if \'losses\' in dict_object:\n        losses = dict_object[\'losses\']\n        if not isinstance(losses,(list,tuple)):\n            dict_object[\'losses\'] = [ losses ]\n    if \'labels\' in dict_object:\n        labels = dict_object[\'labels\']\n        if not isinstance(labels,(list,tuple)):\n            dict_object[\'labels\'] = [ labels ]\n    return dict_object\n\n\ndef probability_shift_(tensor, labels):  # In-place operation. shape (batch_size, num_classes), (batch_size,)\n    if len(tensor.shape)==2:\n        max_position = tensor.argmax(dim=-1) # shape (batch_size,)\n        index = torch.arange(tensor.size(0)).to(tensor.device)\n        max_clone = tensor[index,max_position].clone()\n        truth_clone = tensor[index,labels].clone()\n\n        tensor[index,max_position] = truth_clone\n        tensor[index,labels] = max_clone\n        return tensor\n\n    elif len(tensor.shape)==3:   # shape (batch_size, length, num_classes)\n        original_shape = tensor.size()\n\n        tensor = tensor.view(-1,tensor.size(-1))   # (batch_size * length, num_classes)\n\n        max_position = tensor.argmax(dim=-1) # shape (batch_size * length, )\n        labels = labels.view(-1) # (batch_size * length, )\n        nonneg_labels = torch.where(labels<0, max_position, labels)\n\n        index = torch.arange(tensor.size(0)).to(tensor.device)   # (batch_size * length)\n\n        max_clone = tensor[index,max_position].clone()\n        truth_clone = tensor[index,nonneg_labels].clone()\n\n        tensor[index,max_position] = truth_clone\n        tensor[index,nonneg_labels] = max_clone\n        tensor = tensor.view(original_shape)\n        return tensor\n    else:\n        raise TypeError(""Rank of tensor must be 2 or 3"")\n\nclass no_op:\n    @staticmethod\n    def add_scalar(*args, **kwargs):\n        pass\n'"
src/textbrewer/distillers.py,0,b'from .distiller_train import BasicTrainer\nfrom .distiller_basic import BasicDistiller\nfrom .distiller_general import GeneralDistiller\nfrom .distiller_multitask import MultiTaskDistiller\nfrom .distiller_multiteacher import MultiTeacherDistiller\n'
src/textbrewer/losses.py,50,"b""import torch.nn.functional as F\nimport torch\nfrom typing import List\n\nfrom .compatibility import mask_dtype\n\ndef kd_mse_loss(logits_S, logits_T, temperature=1):\n    '''\n    Calculate the mse loss between logits_S and logits_T\n\n    :param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n    :param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n    :param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,)\n    '''\n    if isinstance(temperature, torch.Tensor) and temperature.dim() > 0:\n        temperature = temperature.unsqueeze(-1)\n    beta_logits_T = logits_T / temperature\n    beta_logits_S = logits_S / temperature\n    loss = F.mse_loss(beta_logits_S, beta_logits_T)\n    return loss\n\n\ndef kd_ce_loss(logits_S, logits_T, temperature=1):\n    '''\n    Calculate the cross entropy between logits_S and logits_T\n\n    :param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n    :param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n    :param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,)\n    '''\n    if isinstance(temperature, torch.Tensor) and temperature.dim() > 0:\n        temperature = temperature.unsqueeze(-1)\n    beta_logits_T = logits_T / temperature\n    beta_logits_S = logits_S / temperature\n    p_T = F.softmax(beta_logits_T, dim=-1)\n    loss = -(p_T * F.log_softmax(beta_logits_S, dim=-1)).sum(dim=-1).mean()\n    return loss\n\n\ndef att_mse_loss(attention_S, attention_T, mask=None):\n    '''\n    * Calculates the mse loss between `attention_S` and `attention_T`.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n\n    :param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)\n    :param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)\n    :param torch.Tensor mask: tensor of shape  (*batch_size*, *length*)\n    '''\n    if mask is None:\n        attention_S_select = torch.where(attention_S <= -1e-3, torch.zeros_like(attention_S), attention_S)\n        attention_T_select = torch.where(attention_T <= -1e-3, torch.zeros_like(attention_T), attention_T)\n        loss = F.mse_loss(attention_S_select, attention_T_select)\n    else:\n        mask = mask.to(attention_S).unsqueeze(1).expand(-1, attention_S.size(1), -1) # (bs, num_of_heads, len)\n        valid_count = torch.pow(mask.sum(dim=2),2).sum()\n        loss = (F.mse_loss(attention_S, attention_T, reduction='none') * mask.unsqueeze(-1) * mask.unsqueeze(2)).sum() / valid_count\n    return loss\n\n\ndef att_mse_sum_loss(attention_S, attention_T, mask=None):\n    '''\n    * Calculates the mse loss between `attention_S` and `attention_T`. \n    * If the the shape is (*batch_size*, *num_heads*, *length*, *length*), sums along the `num_heads` dimension and then calcuates the mse loss between the two matrices.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n\n    :param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)\n    :param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)\n    :param torch.Tensor mask:     tensor of shape  (*batch_size*, *length*)\n    '''\n    if len(attention_S.size())==4:\n        attention_T = attention_T.sum(dim=1)\n        attention_S = attention_S.sum(dim=1)\n    if mask is None:\n        attention_S_select = torch.where(attention_S <= -1e-3, torch.zeros_like(attention_S), attention_S)\n        attention_T_select = torch.where(attention_T <= -1e-3, torch.zeros_like(attention_T), attention_T)\n        loss = F.mse_loss(attention_S_select, attention_T_select)\n    else:\n        mask = mask.to(attention_S)\n        valid_count = torch.pow(mask.sum(dim=1), 2).sum()\n        loss = (F.mse_loss(attention_S, attention_T, reduction='none') * mask.unsqueeze(-1) * mask.unsqueeze(1)).sum() / valid_count\n    return loss\n\n\ndef att_ce_loss(attention_S, attention_T, mask=None):\n    '''\n\n    * Calculates the cross-entropy loss between `attention_S` and `attention_T`, where softmax is to applied on ``dim=-1``.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n    \n    :param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)\n    :param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)\n    :param torch.Tensor mask:     tensor of shape  (*batch_size*, *length*)\n    '''\n    probs_T = F.softmax(attention_T, dim=-1)\n    if mask is None:\n        probs_T_select = torch.where(attention_T <= -1e-3, torch.zeros_like(attention_T), probs_T)\n        loss = -((probs_T_select * F.log_softmax(attention_S, dim=-1)).sum(dim=-1)).mean()\n    else:\n        mask = mask.to(attention_S).unsqueeze(1).expand(-1, attention_S.size(1), -1) # (bs, num_of_heads, len)\n        loss = -((probs_T * F.log_softmax(attention_S, dim=-1) * mask.unsqueeze(2)).sum(dim=-1) * mask).sum() / mask.sum()\n    return loss\n\n\ndef att_ce_mean_loss(attention_S, attention_T, mask=None):\n    '''\n    * Calculates the cross-entropy loss between `attention_S` and `attention_T`, where softmax is to applied on ``dim=-1``.\n    * If the shape is (*batch_size*, *num_heads*, *length*, *length*), averages over dimension `num_heads` and then computes cross-entropy loss between the two matrics.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n    \n    :param torch.tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)\n    :param torch.tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)\n    :param torch.tensor mask:     tensor of shape  (*batch_size*, *length*)\n    '''\n    if len(attention_S.size())==4:\n        attention_S = attention_S.mean(dim=1) # (bs, len, len)\n        attention_T = attention_T.mean(dim=1)\n    probs_T = F.softmax(attention_T, dim=-1)\n    if mask is None:\n        probs_T_select = torch.where(attention_T <= -1e-3, torch.zeros_like(attention_T), probs_T)\n        loss = -((probs_T_select * F.log_softmax(attention_S, dim=-1)).sum(dim=-1)).mean()\n    else:\n        mask = mask.to(attention_S)\n        loss = -((probs_T * F.log_softmax(attention_S, dim=-1) * mask.unsqueeze(1)).sum(dim=-1) * mask).sum() / mask.sum()\n    return loss\n\n\ndef hid_mse_loss(state_S, state_T, mask=None):\n    '''\n    * Calculates the mse loss between `state_S` and `state_T`, which are the hidden state of the models.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.\n\n    :param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.Tensor mask:    tensor of shape  (*batch_size*, *length*)\n    '''\n    if mask is None:\n        loss = F.mse_loss(state_S, state_T)\n    else:\n        mask = mask.to(state_S)\n        valid_count = mask.sum() * state_S.size(-1)\n        loss = (F.mse_loss(state_S, state_T, reduction='none') * mask.unsqueeze(-1)).sum() / valid_count\n    return loss\n\n\ndef cos_loss(state_S, state_T, mask=None):\n    '''\n    * Computes the cosine similarity loss between the inputs. This is the loss used in DistilBERT, see `DistilBERT <https://arxiv.org/abs/1910.01108>`_\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.\n\n    :param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.Tensor mask:    tensor of shape  (*batch_size*, *length*)\n    '''\n    if mask is  None:\n        state_S = state_S.view(-1,state_S.size(-1))\n        state_T = state_T.view(-1,state_T.size(-1))\n    else:\n        mask = mask.to(state_S).unsqueeze(-1).expand_as(state_S).to(mask_dtype) #(bs,len,dim)\n        state_S = torch.masked_select(state_S, mask).view(-1, mask.size(-1))  #(bs * select, dim)\n        state_T = torch.masked_select(state_T, mask).view(-1, mask.size(-1))  # (bs * select, dim)\n\n    target = state_S.new(state_S.size(0)).fill_(1)\n    loss = F.cosine_embedding_loss(state_S, state_T, target, reduction='mean')\n    return loss\n\n\ndef pkd_loss(state_S, state_T, mask=None):\n    '''\n    * Computes normalized vector mse loss at position 0 along `length` dimension. This is the loss used in BERT-PKD, see `Patient Knowledge Distillation for BERT Model Compression <https://arxiv.org/abs/1908.09355>`_.\n    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.\n\n    :param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param mask: not used.\n    '''\n\n    cls_T = state_T[:,0] # (batch_size, hidden_dim)\n    cls_S = state_S[:,0] # (batch_size, hidden_dim)\n    normed_cls_T = cls_T/torch.norm(cls_T,dim=1,keepdim=True)\n    normed_cls_S = cls_S/torch.norm(cls_S,dim=1,keepdim=True)\n    loss = (normed_cls_S - normed_cls_T).pow(2).sum(dim=-1).mean()\n    return loss\n\n\ndef fsp_loss(state_S, state_T, mask=None):\n    '''\n    * Takes in two lists of matrics `state_S` and `state_T`. Each list contains two matrices of the shape (*batch_size*, *length*, *hidden_size*). Computes the similarity matrix between the two matrices in `state_S` ( with the resulting shape (*batch_size*, *hidden_size*, *hidden_size*) ) and the ones in B ( with the resulting shape (*batch_size*, *hidden_size*, *hidden_size*) ), then computes the mse loss between the similarity matrices:\n\n    .. math::\n\n        loss = mean((S_{1}^T \\cdot S_{2} - T_{1}^T \\cdot T_{2})^2)\n\n    * It is a Variant of FSP loss in `A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning <http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf>`_.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.\n\n    :param torch.tensor state_S: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.tensor state_T: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.tensor mask:    tensor of the shape  (*batch_size*, *length*)\n\n    Example in `intermediate_matches`::\n\n        intermediate_matches = [\n        {'layer_T':[0,0], 'layer_S':[0,0], 'feature':'hidden','loss': 'fsp', 'weight' : 1, 'proj':['linear',384,768]},\n        ...]\n    '''\n    if mask is None:\n        state_S_0 = state_S[0] # (batch_size , length, hidden_dim)\n        state_S_1 = state_S[1] # (batch_size,  length, hidden_dim)\n        state_T_0 = state_T[0]\n        state_T_1 = state_T[1]\n        gram_S = torch.bmm(state_S_0.transpose(1, 2), state_S_1) / state_S_1.size(1)  # (batch_size, hidden_dim, hidden_dim)\n        gram_T = torch.bmm(state_T_0.transpose(1, 2), state_T_1) / state_T_1.size(1)\n    else:\n        mask = mask.to(state_S[0]).unsqueeze(-1)\n        lengths = mask.sum(dim=1,keepdim=True)\n        state_S_0 = state_S[0] * mask\n        state_S_1 = state_S[1] * mask\n        state_T_0 = state_T[0] * mask\n        state_T_1 = state_T[1] * mask\n        gram_S = torch.bmm(state_S_0.transpose(1,2), state_S_1)/lengths\n        gram_T = torch.bmm(state_T_0.transpose(1,2), state_T_1)/lengths\n    loss = F.mse_loss(gram_S, gram_T)\n    return loss\n\n\ndef mmd_loss(state_S, state_T, mask=None):\n    '''\n    * Takes in two lists of matrices `state_S` and `state_T`. Each list contains 2 matrices of the shape (*batch_size*, *length*, *hidden_size*). `hidden_size` of matrices in `State_S` doesn't need to be the same as that of `state_T`. Computes the similarity matrix between the two matrices in `state_S` ( with the resulting shape (*batch_size*, *length*, *length*) ) and the ones in B ( with the resulting shape (*batch_size*, *length*, *length*) ), then computes the mse loss between the similarity matrices:\n    \n    .. math::\n\n            loss = mean((S_{1} \\cdot S_{2}^T - T_{1} \\cdot T_{2}^T)^2)\n\n    * It is a Variant of the NST loss in `Like What You Like: Knowledge Distill via Neuron Selectivity Transfer <https://arxiv.org/abs/1707.01219>`_\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n\n    :param torch.tensor state_S: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.tensor state_T: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.tensor mask:    tensor of the shape  (*batch_size*, *length*)\n\n    Example in `intermediate_matches`::\n\n        intermediate_matches = [\n        {'layer_T':[0,0], 'layer_S':[0,0], 'feature':'hidden','loss': 'nst', 'weight' : 1},\n        ...]\n    '''\n    state_S_0 = state_S[0] # (batch_size , length, hidden_dim_S)\n    state_S_1 = state_S[1] # (batch_size , length, hidden_dim_S)\n    state_T_0 = state_T[0] # (batch_size , length, hidden_dim_T)\n    state_T_1 = state_T[1] # (batch_size , length, hidden_dim_T)\n    if mask is None:\n        gram_S = torch.bmm(state_S_0, state_S_1.transpose(1, 2)) / state_S_1.size(2)  # (batch_size, length, length)\n        gram_T = torch.bmm(state_T_0, state_T_1.transpose(1, 2)) / state_T_1.size(2)\n        loss = F.mse_loss(gram_S, gram_T)\n    else:\n        mask = mask.to(state_S[0])\n        valid_count = torch.pow(mask.sum(dim=1), 2).sum()\n        gram_S = torch.bmm(state_S_0, state_S_1.transpose(1, 2)) / state_S_1.size(1)  # (batch_size, length, length)\n        gram_T = torch.bmm(state_T_0, state_T_1.transpose(1, 2)) / state_T_1.size(1)\n        loss = (F.mse_loss(gram_S, gram_T, reduction='none') * mask.unsqueeze(-1) * mask.unsqueeze(1)).sum() / valid_count\n    return loss\n\n\n"""
src/textbrewer/presets.py,0,"b'import collections\n\nfrom .losses import *\nfrom .schedulers import *\nfrom .utils import cycle\nfrom .projections import linear_projection, projection_with_activation\n\nclass DynamicKeyDict:\n    def __init__(self, kv_dict):\n        self.store = kv_dict\n    def __getitem__(self, key):\n        if not isinstance(key,(list,tuple)):\n            return self.store[key]\n        else:\n            name = key[0]\n            args = key[1:]\n            if len(args)==1 and isinstance(args[0],dict):\n                return self.store[name](**(args[0]))\n            else:\n                return self.store[name](*args)\n    def __setitem__(self, key, value):\n        self.store[key] = value\n    def __contains__(self, key):\n        if isinstance(key, (list,tuple)):\n            return key[0] in self.store\n        else:\n            return key in self.store\n\nTEMPERATURE_SCHEDULER=DynamicKeyDict(\n    {\'constant\': constant_temperature_scheduler,\n     \'flsw\': flsw_temperature_scheduler_builder,\n     \'cwsm\':cwsm_temperature_scheduler_builder})\n""""""\n(*custom dict*) used to dynamically adjust distillation temperature.\n\n    * \'**constant**\' : Constant temperature.\n    * \'**flsw**\' :  See `Preparing Lessons: Improve Knowledge Distillation with Better Supervision <https://arxiv.org/abs/1911.07471>`_. Needs parameters ``beta`` and ``gamma``.\n    * \'**cwsm**\': See `Preparing Lessons: Improve Knowledge Distillation with Better Supervision <https://arxiv.org/abs/1911.07471>`_. Needs parameter ``beta``.\n\nDifferent from other options, when using ``\'flsw\'`` and ``\'cwsm\'``, you need to provide extra parameters, for example::\n\n    #flsw\n    distill_config = DistillationConfig(\n        temperature_scheduler = [\'flsw\', 1\xef\xbc\x8c 2]  # beta=1, gamma=2\n    )\n    \n    #cwsm\n    distill_config = DistillationConfig(\n        temperature_scheduler = [\'cwsm\', 1] # beta = 1\n    )\n\n""""""\n\n\n\nFEATURES = [\'hidden\',\'attention\']\n\n\nADAPTOR_KEYS = [\'logits\',\'logits_mask\',\'losses\',\'inputs_mask\',\'labels\'] + FEATURES\n""""""\n(*list*) valid keys of the dict returned by the adaptor, includes:\n\n    * \'**logits**\'\n    * \'**logits_mask**\'\n    * \'**losses**\'\n    * \'**inputs_mask**\'\n    * \'**labels**\'\n    * \'**hidden**\'\n    * \'**attention**\'\n""""""\n\n\nKD_LOSS_MAP = {\'mse\': kd_mse_loss,\n                \'ce\': kd_ce_loss}\n""""""\n(*dict*) available KD losses\n\n  * \'**mse**\' : mean squared error \n  * \'**ce**\': cross-entropy loss\n""""""\n\nMATCH_LOSS_MAP = {\'attention_mse_sum\': att_mse_sum_loss,\n                  \'attention_mse\': att_mse_loss,\n                  \'attention_ce_mean\': att_ce_mean_loss,\n                  \'attention_ce\': att_ce_loss,\n                  \'hidden_mse\'    : hid_mse_loss,\n                  \'cos\'  : cos_loss,\n                  \'pkd\'  : pkd_loss,\n                  \'gram\' : fsp_loss,\n                  \'fsp\'  : fsp_loss,\n                  \'mmd\'  : mmd_loss,\n                  \'nst\'  : mmd_loss}\n""""""\n(*dict*) intermediate feature matching loss functions, includes:\n\n* :func:`attention_mse_sum <textbrewer.losses.att_mse_sum_loss>`\n* :func:`attention_mse <textbrewer.losses.att_mse_loss>`\n* :func:`attention_ce_mean <textbrewer.losses.att_ce_mean_loss>`\n* :func:`attention_ce <textbrewer.losses.att_ce_loss>`\n* :func:`hidden_mse <textbrewer.losses.hid_mseloss>`\n* :func:`cos <textbrewer.losses.cos_loss>`\n* :func:`pkd <textbrewer.losses.pkd_loss>`\n* :func:`fsp <textbrewer.losses.fsp_loss>`, :func:`gram <textbrewer.losses.fsp_loss>`\n* :func:`nst <textbrewer.losses.nst_loss>`, :func:`mmd <textbrewer.losses.nst_loss>`\n\nSee :ref:`intermediate_losses` for details.\n""""""\n\nPROJ_MAP = {\'linear\': linear_projection,\n            \'relu\'  : projection_with_activation(\'ReLU\'),\n            \'tanh\'  : projection_with_activation(\'Tanh\')\n            }\n""""""\n(*dict*) layers used to match the different dimensions of intermediate features\n\n  * \'**linear**\' : linear layer, no activation\n  * \'**relu**\' : ReLU activation\n  * \'**tanh**\': Tanh activation\n""""""\n\nWEIGHT_SCHEDULER = {\'linear_decay\': linear_decay_weight_scheduler,\n                    \'linear_growth\' : linear_growth_weight_scheduler}\n""""""\n(dict) Scheduler used to dynamically adjust KD loss weight and hard_label_loss weight.\n\n  * \xe2\x80\x98**linear_decay**\' : decay from 1 to 0 during the whole training process.\n  * \'**linear_growth**\' : grow from 0 to 1 during the whole training process.\n""""""\n\n#TEMPERATURE_SCHEDULER = {\'constant\': constant_temperature_scheduler,\n#                         \'flsw_scheduler\': flsw_temperature_scheduler_builder(1,1)}\n\n\nMAPS = {\'kd_loss\': KD_LOSS_MAP,\n        \'match_Loss\': MATCH_LOSS_MAP,\n        \'projection\': PROJ_MAP,\n        \'weight_scheduler\': WEIGHT_SCHEDULER,\n        \'temperature_scheduler\': TEMPERATURE_SCHEDULER}\n\n\ndef register_new(map_name, name, func):\n    assert map_name in MAPS\n    assert callable(func), ""Functions to be registered is not callable""\n    MAPS[map_name][name] = func\n\n\n\'\'\'\nAdd new loss:\ndef my_L1_loss(feature_S, feature_T, mask=None):\n    return (feature_S-feature_T).abs().mean()\n\nMATCH_LOSS_MAP[\'my_L1_loss\'] = my_L1_loss\n\'\'\'\n'"
src/textbrewer/projections.py,5,"b'import torch\nfrom .utils import initializer_builder\nfrom typing import List\n\nact_dict = {}\nfor k,v in  torch.nn.modules.activation.__dict__.items():\n    if not k.startswith(\'__\'):\n        act_dict[k] = v\n\ndef linear_projection(dim_in, dim_out):\n    model = torch.nn.Linear(in_features=dim_in, out_features=dim_out, bias=True)\n    initializer = initializer_builder(0.02)\n    model.apply(initializer)\n    return model\n\ndef projection_with_activation(act_fn):\n    if type(act_fn) is str:\n        assert act_fn in act_dict, f""invalid activations, please choice from {list(act_dict.keys())}""\n        act_fn = act_dict[act_fn]()\n    else:\n        assert isinstance(act_fn,torch.nn.Module), ""act_fn must be a string or module""\n        act_fn = act_fn()\n    def projection(dim_in, dim_out):\n        model = torch.nn.Sequential(\n            torch.nn.Linear(in_features=dim_in, out_features=dim_out, bias=True),\n            act_fn)\n        initializer = initializer_builder(0.02)\n        model.apply(initializer)\n        return model\n    return projection'"
src/textbrewer/schedulers.py,7,"b""import torch.nn.functional as F\nimport torch\n\n# x is between 0 and 1\ndef linear_growth_weight_scheduler(x):\n    return x\n\ndef linear_decay_weight_scheduler(x):\n    return 1-x\n\ndef constant_temperature_scheduler(logits_S, logits_T, base_temperature):\n    '''\n    Remember to detach logits_S \n    '''\n    return base_temperature\n\n\ndef flsw_temperature_scheduler_builder(beta,gamma,eps=1e-4, *args):\n    '''\n    adapted from arXiv:1911.07471\n    '''\n    def flsw_temperature_scheduler(logits_S, logits_T, base_temperature):\n        v = logits_S.detach()\n        t = logits_T.detach()\n        with torch.no_grad():\n            v = v/(torch.norm(v,dim=-1,keepdim=True)+eps)\n            t = t/(torch.norm(t,dim=-1,keepdim=True)+eps)\n            w = torch.pow((1 - (v*t).sum(dim=-1)),gamma)\n            tau = base_temperature + (w.mean()-w)*beta\n        return tau\n    return flsw_temperature_scheduler\n\n\ndef cwsm_temperature_scheduler_builder(beta,*args):\n    '''\n    adapted from arXiv:1911.07471\n    '''\n    def cwsm_temperature_scheduler(logits_S, logits_T, base_temperature):\n        v = logits_S.detach()\n        with torch.no_grad():\n            v = torch.softmax(v,dim=-1)\n            v_max = v.max(dim=-1)[0]\n            w = 1 / (v_max + 1e-3)\n            tau = base_temperature + (w.mean()-w)*beta\n        return tau\n    return cwsm_temperature_scheduler\n"""
src/textbrewer/snippet.py,5,"b""import torch\nfrom functools import partial\nfrom textbrewer import GeneralDistiller\nfrom textbrewer import TrainingConfig, DistillationConfig\n\n# We omit the initialization of models, optimizer, and dataloader. \nteacher_model : torch.nn.Module = ...\nstudent_model : torch.nn.Module = ...\ndataloader : torch.utils.data.DataLoader = ...\noptimizer : torch.optim.Optimizer = ...\nscheduler : torch.optim.lr_scheduler = ...\n\ndef simple_adaptor(batch, model_outputs):\n    # We assume that the first element of model_outputs \n    # is the logits before softmax\n    return {'logits': model_outputs[0]}  \n\ntrain_config = TrainingConfig()\ndistill_config = DistillationConfig()\ndistiller = GeneralDistiller(\n    train_config=train_config, distill_config = distill_config,\n    model_T = teacher_model, model_S = student_model, \n    adaptor_T = simple_adaptor, adaptor_S = simple_adaptor)\n\ndistiller.train(optimizer, scheduler, \n    dataloader, num_epochs, callback=None)\n\n\n\n\n\ndef predict(model, eval_dataset, step, args): \n  raise NotImplementedError\n# fill other arguments\nmy_callback = partial(predict, eval_dataset=my_eval_dataset, args=args) \ntrain_config = TrainingConfig()\n\n# \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\x8e\xe8\xaf\x84\xe4\xbc\xb0\xe5\x87\xbd\xe6\x95\xb0\ndef predict(model, eval_dataset, step, args): \n  '''\n  eval_dataset: \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n  args: \xe8\xaf\x84\xe4\xbc\xb0\xe4\xb8\xad\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe5\x85\xb6\xe4\xbb\x96\xe5\x8f\x82\xe6\x95\xb0\n  '''\n  raise NotImplementedError\n\n # \xe5\xa1\xab\xe5\x85\x85\xe5\xa4\x9a\xe4\xbd\x99\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\nmy_callback = partial(predict, eval_dataset=my_eval_dataset, args=args) \ndistillator.train(..., callback = my_callback)\n"""
src/textbrewer/utils.py,5,"b'import torch\nfrom typing import List\n    \ndef cycle(iterable):\n    while True:\n        for x in iterable:\n            yield x\n\ndef initializer_builder(std):\n    _std = std\n    def init_weights(module):\n        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=_std)\n        if isinstance(module, torch.nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n    return init_weights\n\n\nclass LayerNode:\n    def __init__(self,name,parent=None,value=None,fullname=None):\n        self.name = name\n        self.fullname = fullname\n        self.value = None\n        self.children_name = {}\n        self.parent = parent\n    def __contains__(self, key):\n        return key in self.children_name\n    def __getitem__(self,key):\n        return self.children_name[key]\n    def __setitem__(self,key,value):\n        self.children_name[key]=value\n    def update(self,value):\n        if self.parent:\n            if self.parent.value is None:\n                self.parent.value = value\n            else:\n                if isinstance(value,(tuple,list)):\n                    old_value = self.parent.value\n                    new_value = [old_value[i]+value[i] for i in range(len(value))]\n                    self.parent.value = new_value\n                else:\n                    self.parent.value += value\n\n            self.parent.update(value)\n    \n    def format(self, level=0, total=None ,indent=\'--\',max_level=None,max_length=None):\n        string =\'\'\n        if total is None:\n            total = self.value[0]\n        if level ==0:\n            max_length = self._max_name_length(indent,\'  \',max_level=max_level) + 1\n            string += \'\\n\'\n            string +=f""{\'LAYER NAME\':<{max_length}}\\t{\'#PARAMS\':>15}\\t{\'RATIO\':>10}\\t{\'MEM(MB)\':>8}\\n""\n\n        if max_level is not None and level==max_level:\n            string += f""{indent+self.name+\':\':<{max_length}}\\t{self.value[0]:15,d}\\t{self.value[0]/total:>10.2%}\\t{self.value[1]:>8.2f}\\n""\n        else:\n            if len(self.children_name)==1:\n                string += f""{indent+self.name:{max_length}}\\n""\n            else:\n                string += f""{indent+self.name+\':\':<{max_length}}\\t{self.value[0]:15,d}\\t{self.value[0]/total:>10.2%}\\t{self.value[1]:>8.2f}\\n""\n            for child_name, child in self.children_name.items():\n                string += child.format(level+1, total, \n                                       indent=\'  \'+indent, max_level=max_level,max_length=max_length) \n        return string\n\n    def _max_name_length(self,indent1=\'--\', indent2=\'  \',level=0,max_level=None):\n        length = len(self.name) + len(indent1) + level *len(indent2)\n        if max_level is not None and level >= max_level:\n            child_lengths = []\n        else:\n            child_lengths = [child._max_name_length(indent1,indent2,level=level+1,max_level=max_level) \n                            for child in self.children_name.values()]\n        max_length = max(child_lengths+[length])\n        return max_length\n\n\ndef display_parameters(model,max_level=None):\n    """"""\n    Display the numbers and memory usage of module parameters.\n\n    Args:\n        model (torch.nn.Module or dict): the model to be inspected.\n        max_level (int or None): The max level to display. If ``max_level==None``, show all the levels.\n    Returns:\n        A formatted string and a :class:`~textbrewer.utils.LayerNode` object representing the model.\n    """"""\n    if isinstance(model,torch.nn.Module):\n        state_dict = model.state_dict()\n    elif isinstance(model,dict):\n        state_dict = model\n    else:\n        raise TypeError(""model should be either torch.nn.Module or a dict"")\n    hash_set = set()\n    model_node = LayerNode(\'model\',fullname=\'model\')\n    current = model_node\n    for key,value  in state_dict.items():\n        names = key.split(\'.\')\n        for i,name in enumerate(names):\n            if name not in current:\n                current[name] = LayerNode(name,parent=current,fullname=\'.\'.join(names[:i+1]))\n            current = current[name]\n        \n        if (value.data_ptr()) in hash_set:\n            current.value = [0,0]\n            current.name += ""(shared)""\n            current.fullname += ""(shared)""\n            current.update(current.value)\n        else:\n            hash_set.add(value.data_ptr())\n            current.value = [value.numel(),value.numel() * value.element_size() / 1024/1024]\n            current.update(current.value)\n            \n        current = model_node\n\n    result = model_node.format(max_level=max_level)\n    #print (result)\n    return result, model_node\n\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/__init__.py,0,"b'__version__ = ""0.6.2""\nfrom .tokenization import BertTokenizer, BasicTokenizer, WordpieceTokenizer\nfrom .tokenization_openai import OpenAIGPTTokenizer\nfrom .tokenization_transfo_xl import (TransfoXLTokenizer, TransfoXLCorpus)\nfrom .tokenization_gpt2 import GPT2Tokenizer\n\nfrom .modeling import (BertConfig, BertModel, BertForPreTraining,\n                       BertForMaskedLM, BertForNextSentencePrediction,\n                       BertForSequenceClassification, BertForMultipleChoice,\n                       BertForTokenClassification, BertForQuestionAnswering,\n                       load_tf_weights_in_bert)\nfrom .modeling_openai import (OpenAIGPTConfig, OpenAIGPTModel,\n                              OpenAIGPTLMHeadModel, OpenAIGPTDoubleHeadsModel,\n                              load_tf_weights_in_openai_gpt)\nfrom .modeling_transfo_xl import (TransfoXLConfig, TransfoXLModel, TransfoXLLMHeadModel,\n                                  load_tf_weights_in_transfo_xl)\nfrom .modeling_gpt2 import (GPT2Config, GPT2Model,\n                            GPT2LMHeadModel, GPT2DoubleHeadsModel,\n                            load_tf_weights_in_gpt2)\n\nfrom .optimization import BertAdam\nfrom .optimization_openai import OpenAIAdam\n\nfrom .file_utils import PYTORCH_PRETRAINED_BERT_CACHE, cached_path, WEIGHTS_NAME, CONFIG_NAME\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/__main__.py,0,"b'# coding: utf8\ndef main():\n    import sys\n    if (len(sys.argv) != 4 and len(sys.argv) != 5) or sys.argv[1] not in [\n        ""convert_tf_checkpoint_to_pytorch"",\n        ""convert_openai_checkpoint"",\n        ""convert_transfo_xl_checkpoint"",\n        ""convert_gpt2_checkpoint"",\n    ]:\n        print(\n        ""Should be used as one of: \\n""\n        "">> `pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT`, \\n""\n        "">> `pytorch_pretrained_bert convert_openai_checkpoint OPENAI_GPT_CHECKPOINT_FOLDER_PATH PYTORCH_DUMP_OUTPUT [OPENAI_GPT_CONFIG]`, \\n""\n        "">> `pytorch_pretrained_bert convert_transfo_xl_checkpoint TF_CHECKPOINT_OR_DATASET PYTORCH_DUMP_OUTPUT [TF_CONFIG]` or \\n""\n        "">> `pytorch_pretrained_bert convert_gpt2_checkpoint TF_CHECKPOINT PYTORCH_DUMP_OUTPUT [GPT2_CONFIG]`"")\n    else:\n        if sys.argv[1] == ""convert_tf_checkpoint_to_pytorch"":\n            try:\n                from .convert_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch\n            except ImportError:\n                print(""pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, ""\n                    ""In that case, it requires TensorFlow to be installed. Please see ""\n                    ""https://www.tensorflow.org/install/ for installation instructions."")\n                raise\n\n            if len(sys.argv) != 5:\n                # pylint: disable=line-too-long\n                print(""Should be used as `pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT`"")\n            else:\n                PYTORCH_DUMP_OUTPUT = sys.argv.pop()\n                TF_CONFIG = sys.argv.pop()\n                TF_CHECKPOINT = sys.argv.pop()\n                convert_tf_checkpoint_to_pytorch(TF_CHECKPOINT, TF_CONFIG, PYTORCH_DUMP_OUTPUT)\n        elif sys.argv[1] == ""convert_openai_checkpoint"":\n            from .convert_openai_checkpoint_to_pytorch import convert_openai_checkpoint_to_pytorch\n            OPENAI_GPT_CHECKPOINT_FOLDER_PATH = sys.argv[2]\n            PYTORCH_DUMP_OUTPUT = sys.argv[3]\n            if len(sys.argv) == 5:\n                OPENAI_GPT_CONFIG = sys.argv[4]\n            else:\n                OPENAI_GPT_CONFIG = """"\n            convert_openai_checkpoint_to_pytorch(OPENAI_GPT_CHECKPOINT_FOLDER_PATH,\n                                                 OPENAI_GPT_CONFIG,\n                                                 PYTORCH_DUMP_OUTPUT)\n        elif sys.argv[1] == ""convert_transfo_xl_checkpoint"":\n            try:\n                from .convert_transfo_xl_checkpoint_to_pytorch import convert_transfo_xl_checkpoint_to_pytorch\n            except ImportError:\n                print(""pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, ""\n                    ""In that case, it requires TensorFlow to be installed. Please see ""\n                    ""https://www.tensorflow.org/install/ for installation instructions."")\n                raise\n\n            if \'ckpt\' in sys.argv[2].lower():\n                TF_CHECKPOINT = sys.argv[2]\n                TF_DATASET_FILE = """"\n            else:\n                TF_DATASET_FILE = sys.argv[2]\n                TF_CHECKPOINT = """"\n            PYTORCH_DUMP_OUTPUT = sys.argv[3]\n            if len(sys.argv) == 5:\n                TF_CONFIG = sys.argv[4]\n            else:\n                TF_CONFIG = """"\n            convert_transfo_xl_checkpoint_to_pytorch(TF_CHECKPOINT, TF_CONFIG, PYTORCH_DUMP_OUTPUT, TF_DATASET_FILE)\n        else:\n            try:\n                from .convert_gpt2_checkpoint_to_pytorch import convert_gpt2_checkpoint_to_pytorch\n            except ImportError:\n                print(""pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, ""\n                    ""In that case, it requires TensorFlow to be installed. Please see ""\n                    ""https://www.tensorflow.org/install/ for installation instructions."")\n                raise\n\n            TF_CHECKPOINT = sys.argv[2]\n            PYTORCH_DUMP_OUTPUT = sys.argv[3]\n            if len(sys.argv) == 5:\n                TF_CONFIG = sys.argv[4]\n            else:\n                TF_CONFIG = """"\n            convert_gpt2_checkpoint_to_pytorch(TF_CHECKPOINT, TF_CONFIG, PYTORCH_DUMP_OUTPUT)\nif __name__ == \'__main__\':\n    main()\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/convert_gpt2_checkpoint_to_pytorch.py,1,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convert OpenAI GPT checkpoint.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nfrom io import open\n\nimport torch\n\nfrom pytorch_pretrained_bert.modeling_gpt2 import (CONFIG_NAME, WEIGHTS_NAME,\n                                                     GPT2Config,\n                                                     GPT2Model,\n                                                     load_tf_weights_in_gpt2)\n\n\ndef convert_gpt2_checkpoint_to_pytorch(gpt2_checkpoint_path, gpt2_config_file, pytorch_dump_folder_path):\n    # Construct model\n    if gpt2_config_file == """":\n        config = GPT2Config()\n    else:\n        config = GPT2Config(gpt2_config_file)\n    model = GPT2Model(config)\n\n    # Load weights from numpy\n    load_tf_weights_in_gpt2(model, gpt2_checkpoint_path)\n\n    # Save pytorch-model\n    pytorch_weights_dump_path = pytorch_dump_folder_path + \'/\' + WEIGHTS_NAME\n    pytorch_config_dump_path = pytorch_dump_folder_path + \'/\' + CONFIG_NAME\n    print(""Save PyTorch model to {}"".format(pytorch_weights_dump_path))\n    torch.save(model.state_dict(), pytorch_weights_dump_path)\n    print(""Save configuration file to {}"".format(pytorch_config_dump_path))\n    with open(pytorch_config_dump_path, ""w"", encoding=""utf-8"") as f:\n        f.write(config.to_json_string())\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(""--gpt2_checkpoint_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path the TensorFlow checkpoint path."")\n    parser.add_argument(""--pytorch_dump_folder_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the output PyTorch model."")\n    parser.add_argument(""--gpt2_config_file"",\n                        default = """",\n                        type = str,\n                        help = ""An optional config json file corresponding to the pre-trained OpenAI model. \\n""\n                            ""This specifies the model architecture."")\n    args = parser.parse_args()\n    convert_gpt2_checkpoint_to_pytorch(args.gpt2_checkpoint_path,\n                                         args.gpt2_config_file,\n                                         args.pytorch_dump_folder_path)\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/convert_openai_checkpoint_to_pytorch.py,1,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convert OpenAI GPT checkpoint.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nfrom io import open\n\nimport torch\n\nfrom pytorch_pretrained_bert.modeling_openai import (CONFIG_NAME, WEIGHTS_NAME,\n                                                     OpenAIGPTConfig,\n                                                     OpenAIGPTModel,\n                                                     load_tf_weights_in_openai_gpt)\n\n\ndef convert_openai_checkpoint_to_pytorch(openai_checkpoint_folder_path, openai_config_file, pytorch_dump_folder_path):\n    # Construct model\n    if openai_config_file == """":\n        config = OpenAIGPTConfig()\n    else:\n        config = OpenAIGPTConfig(openai_config_file)\n    model = OpenAIGPTModel(config)\n\n    # Load weights from numpy\n    load_tf_weights_in_openai_gpt(model, openai_checkpoint_folder_path)\n\n    # Save pytorch-model\n    pytorch_weights_dump_path = pytorch_dump_folder_path + \'/\' + WEIGHTS_NAME\n    pytorch_config_dump_path = pytorch_dump_folder_path + \'/\' + CONFIG_NAME\n    print(""Save PyTorch model to {}"".format(pytorch_weights_dump_path))\n    torch.save(model.state_dict(), pytorch_weights_dump_path)\n    print(""Save configuration file to {}"".format(pytorch_config_dump_path))\n    with open(pytorch_config_dump_path, ""w"", encoding=""utf-8"") as f:\n        f.write(config.to_json_string())\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(""--openai_checkpoint_folder_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path the TensorFlow checkpoint path."")\n    parser.add_argument(""--pytorch_dump_folder_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the output PyTorch model."")\n    parser.add_argument(""--openai_config_file"",\n                        default = """",\n                        type = str,\n                        help = ""An optional config json file corresponding to the pre-trained OpenAI model. \\n""\n                            ""This specifies the model architecture."")\n    args = parser.parse_args()\n    convert_openai_checkpoint_to_pytorch(args.openai_checkpoint_folder_path,\n                                         args.openai_config_file,\n                                         args.pytorch_dump_folder_path)\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py,1,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convert BERT checkpoint.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport argparse\nimport tensorflow as tf\nimport torch\nimport numpy as np\n\nfrom pytorch_pretrained_bert.modeling import BertConfig, BertForPreTraining, load_tf_weights_in_bert\n\ndef convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n    # Initialise PyTorch model\n    config = BertConfig.from_json_file(bert_config_file)\n    print(""Building PyTorch model from configuration: {}"".format(str(config)))\n    model = BertForPreTraining(config)\n\n    # Load weights from tf checkpoint\n    load_tf_weights_in_bert(model, tf_checkpoint_path)\n\n    # Save pytorch-model\n    print(""Save PyTorch model to {}"".format(pytorch_dump_path))\n    torch.save(model.state_dict(), pytorch_dump_path)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(""--tf_checkpoint_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path the TensorFlow checkpoint path."")\n    parser.add_argument(""--bert_config_file"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""The config json file corresponding to the pre-trained BERT model. \\n""\n                            ""This specifies the model architecture."")\n    parser.add_argument(""--pytorch_dump_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the output PyTorch model."")\n    args = parser.parse_args()\n    convert_tf_checkpoint_to_pytorch(args.tf_checkpoint_path,\n                                     args.bert_config_file,\n                                     args.pytorch_dump_path)\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py,3,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convert Transformer XL checkpoint and datasets.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport os\nimport sys\nfrom io import open\n\nimport torch\n\nimport pytorch_pretrained_bert.tokenization_transfo_xl as data_utils\nfrom pytorch_pretrained_bert.modeling_transfo_xl import (CONFIG_NAME,\n                                                         WEIGHTS_NAME,\n                                                         TransfoXLConfig,\n                                                         TransfoXLLMHeadModel,\n                                                         load_tf_weights_in_transfo_xl)\nfrom pytorch_pretrained_bert.tokenization_transfo_xl import (CORPUS_NAME,\n                                                             VOCAB_NAME)\n\nif sys.version_info[0] == 2:\n    import cPickle as pickle\nelse:\n    import pickle\n\n# We do this to be able to load python 2 datasets pickles\n# See e.g. https://stackoverflow.com/questions/2121874/python-pickling-after-changing-a-modules-directory/2121918#2121918\ndata_utils.Vocab = data_utils.TransfoXLTokenizer\ndata_utils.Corpus = data_utils.TransfoXLCorpus\nsys.modules[\'data_utils\'] = data_utils\nsys.modules[\'vocabulary\'] = data_utils\n\ndef convert_transfo_xl_checkpoint_to_pytorch(tf_checkpoint_path,\n                                             transfo_xl_config_file,\n                                             pytorch_dump_folder_path,\n                                             transfo_xl_dataset_file):\n    if transfo_xl_dataset_file:\n        # Convert a pre-processed corpus (see original TensorFlow repo)\n        with open(transfo_xl_dataset_file, ""rb"") as fp:\n            corpus = pickle.load(fp, encoding=""latin1"")\n        # Save vocabulary and dataset cache as Dictionaries (should be better than pickles for the long-term)\n        pytorch_vocab_dump_path = pytorch_dump_folder_path + \'/\' + VOCAB_NAME\n        print(""Save vocabulary to {}"".format(pytorch_vocab_dump_path))\n        corpus_vocab_dict = corpus.vocab.__dict__\n        torch.save(corpus_vocab_dict, pytorch_vocab_dump_path)\n\n        corpus_dict_no_vocab = corpus.__dict__\n        corpus_dict_no_vocab.pop(\'vocab\', None)\n        pytorch_dataset_dump_path = pytorch_dump_folder_path + \'/\' + CORPUS_NAME\n        print(""Save dataset to {}"".format(pytorch_dataset_dump_path))\n        torch.save(corpus_dict_no_vocab, pytorch_dataset_dump_path)\n\n    if tf_checkpoint_path:\n        # Convert a pre-trained TensorFlow model\n        config_path = os.path.abspath(transfo_xl_config_file)\n        tf_path = os.path.abspath(tf_checkpoint_path)\n\n        print(""Converting Transformer XL checkpoint from {} with config at {}"".format(tf_path, config_path))\n        # Initialise PyTorch model\n        if transfo_xl_config_file == """":\n            config = TransfoXLConfig()\n        else:\n            config = TransfoXLConfig(transfo_xl_config_file)\n        print(""Building PyTorch model from configuration: {}"".format(str(config)))\n        model = TransfoXLLMHeadModel(config)\n\n        model = load_tf_weights_in_transfo_xl(model, config, tf_path)\n        # Save pytorch-model\n        pytorch_weights_dump_path = os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)\n        pytorch_config_dump_path = os.path.join(pytorch_dump_folder_path, CONFIG_NAME)\n        print(""Save PyTorch model to {}"".format(os.path.abspath(pytorch_weights_dump_path)))\n        torch.save(model.state_dict(), pytorch_weights_dump_path)\n        print(""Save configuration file to {}"".format(os.path.abspath(pytorch_config_dump_path)))\n        with open(pytorch_config_dump_path, ""w"", encoding=""utf-8"") as f:\n            f.write(config.to_json_string())\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--pytorch_dump_folder_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the folder to store the PyTorch model or dataset/vocab."")\n    parser.add_argument(""--tf_checkpoint_path"",\n                        default = """",\n                        type = str,\n                        help = ""An optional path to a TensorFlow checkpoint path to be converted."")\n    parser.add_argument(""--transfo_xl_config_file"",\n                        default = """",\n                        type = str,\n                        help = ""An optional config json file corresponding to the pre-trained BERT model. \\n""\n                            ""This specifies the model architecture."")\n    parser.add_argument(""--transfo_xl_dataset_file"",\n                        default = """",\n                        type = str,\n                        help = ""An optional dataset file to be converted in a vocabulary."")\n    args = parser.parse_args()\n    convert_transfo_xl_checkpoint_to_pytorch(args.tf_checkpoint_path,\n                                     args.transfo_xl_config_file,\n                                     args.pytorch_dump_folder_path,\n                                     args.transfo_xl_dataset_file)\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/file_utils.py,0,"b'""""""\nUtilities for working with the local dataset cache.\nThis file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\nCopyright by the AllenNLP authors.\n""""""\nfrom __future__ import (absolute_import, division, print_function, unicode_literals)\n\nimport sys\nimport json\nimport logging\nimport os\nimport shutil\nimport tempfile\nimport fnmatch\nfrom functools import wraps\nfrom hashlib import sha256\nimport sys\nfrom io import open\n\nimport boto3\nimport requests\nfrom botocore.exceptions import ClientError\nfrom tqdm import tqdm\n\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n\ntry:\n    from pathlib import Path\n    PYTORCH_PRETRAINED_BERT_CACHE = Path(os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\',\n                                                   Path.home() / \'.pytorch_pretrained_bert\'))\nexcept (AttributeError, ImportError):\n    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\',\n                                              os.path.join(os.path.expanduser(""~""), \'.pytorch_pretrained_bert\'))\n\nCONFIG_NAME = ""config.json""\nWEIGHTS_NAME = ""pytorch_model.bin""\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef url_to_filename(url, etag=None):\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    """"""\n    url_bytes = url.encode(\'utf-8\')\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\'utf-8\')\n        etag_hash = sha256(etag_bytes)\n        filename += \'.\' + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename, cache_dir=None):\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise EnvironmentError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + \'.json\'\n    if not os.path.exists(meta_path):\n        raise EnvironmentError(""file {} not found"".format(meta_path))\n\n    with open(meta_path, encoding=""utf-8"") as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[\'url\']\n    etag = metadata[\'etag\']\n\n    return url, etag\n\n\ndef cached_path(url_or_filename, cache_dir=None):\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (\'http\', \'https\', \'s3\'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == \'\':\n        # File, but it doesn\'t exist.\n        raise EnvironmentError(""file {} not found"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\n\n\ndef split_s3_path(url):\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(""bad s3 path {}"".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise EnvironmentError(""file {} not found"".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url):\n    """"""Check ETag on S3 object.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file):\n    """"""Pull a file directly from S3.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef http_get(url, temp_file):\n    req = requests.get(url, stream=True)\n    content_length = req.headers.get(\'Content-Length\')\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=""B"", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk: # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(url, cache_dir=None):\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(""s3://""):\n        etag = s3_etag(url)\n    else:\n        try:\n            response = requests.head(url, allow_redirects=True)\n            if response.status_code != 200:\n                etag = None\n            else:\n                etag = response.headers.get(""ETag"")\n        except EnvironmentError:\n            etag = None\n\n    if sys.version_info[0] == 2 and etag is not None:\n        etag = etag.decode(\'utf-8\')\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    # If we don\'t have a connection (etag is None) and can\'t identify the file\n    # try to get the last downloaded one\n    if not os.path.exists(cache_path) and etag is None:\n        matching_files = fnmatch.filter(os.listdir(cache_dir), filename + \'.*\')\n        matching_files = list(filter(lambda s: not s.endswith(\'.json\'), matching_files))\n        if matching_files:\n            cache_path = os.path.join(cache_dir, matching_files[-1])\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(""%s not found in cache, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(""s3://""):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n            with open(cache_path, \'wb\') as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(""creating metadata file for %s"", cache_path)\n            meta = {\'url\': url, \'etag\': etag}\n            meta_path = cache_path + \'.json\'\n            with open(meta_path, \'w\') as meta_file:\n                output_string = json.dumps(meta)\n                if sys.version_info[0] == 2 and isinstance(output_string, str):\n                    output_string = unicode(output_string, \'utf-8\')  # The beauty of python 2\n                meta_file.write(output_string)\n\n            logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path\n\n\ndef read_set_from_file(filename):\n    \'\'\'\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    \'\'\'\n    collection = set()\n    with open(filename, \'r\', encoding=\'utf-8\') as file_:\n        for line in file_:\n            collection.add(line.rstrip())\n    return collection\n\n\ndef get_file_extension(path, dot=True, lower=True):\n    ext = os.path.splitext(path)[1]\n    ext = ext if dot else ext[1:]\n    return ext.lower() if lower else ext\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/modeling.py,76,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch BERT model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom .file_utils import cached_path, WEIGHTS_NAME, CONFIG_NAME\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz"",\n}\nBERT_CONFIG_NAME = \'bert_config.json\'\nTF_WEIGHTS_NAME = \'model.ckpt\'\n\ndef load_tf_weights_in_bert(model, tf_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    print(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\'/\')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [""adam_v"", ""adam_m"", ""global_step""] for n in name):\n            print(""Skipping {}"".format(""/"".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'output_weights\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'squad\':\n                pointer = getattr(pointer, \'classifier\')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    print(""Skipping {}"".format(""/"".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \'_embeddings\':\n            pointer = getattr(pointer, \'weight\')\n        elif m_name == \'kernel\':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    """"""Implementation of the gelu activation function.\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish}\n\n\nclass BertConfig(object):\n    """"""Configuration class to store the configuration of a `BertModel`.\n    """"""\n    def __init__(self,\n                 vocab_size_or_config_json_file,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02):\n        """"""Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             ""or the path to a pretrained model config file (str)"")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n        config = BertConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `BertConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n\ntry:\n    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\nexcept ImportError:\n    logger.info(""Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex ."")\n    class BertLayerNorm(nn.Module):\n        def __init__(self, hidden_size, eps=1e-12):\n            """"""Construct a layernorm module in the TF style (epsilon inside the square root).\n            """"""\n            super(BertLayerNorm, self).__init__()\n            self.weight = nn.Parameter(torch.ones(hidden_size))\n            self.bias = nn.Parameter(torch.zeros(hidden_size))\n            self.variance_epsilon = eps\n\n        def forward(self, x):\n            u = x.mean(-1, keepdim=True)\n            s = (x - u).pow(2).mean(-1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n            return self.weight * x + self.bias\n\nclass BertEmbeddings(nn.Module):\n    """"""Construct the embeddings from word, position and token_type embeddings.\n    """"""\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None):\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads))\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        self.output_score = config.output_score\n        self.output_sum   = config.output_sum\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n        attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        if self.output_score is True:\n            attention_probs_sum = attention_scores # (batch_size, h, l, l)\n        else:\n            attention_probs_sum = attention_probs # (batch_size, h, l, l)\n        if self.output_sum is not None:\n            attention_probs_sum = attention_probs_sum.sum(dim=self.output_sum)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        return context_layer, attention_probs_sum\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n\n    def forward(self, input_tensor, attention_mask):\n        self_output, attention_probs_sum = self.self(input_tensor, attention_mask)\n        attention_output = self.output(self_output, input_tensor)\n        return attention_output, attention_probs_sum\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states, attention_mask):\n        attention_output, attention_probs_sum = self.attention(hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output, attention_probs_sum\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        layer = BertLayer(config)\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, output_attention_layer=[]):\n        all_encoder_layers = []\n        all_attention_probs_sum = []\n        for idx,layer_module in enumerate(self.layer):\n            if idx in output_attention_layer:\n                hidden_states, attention_probs_sum = layer_module(hidden_states, attention_mask)\n                all_attention_probs_sum.append(attention_probs_sum)\n            else:\n                hidden_states, _ = layer_module(hidden_states, attention_mask)\n                all_attention_probs_sum.append(None)\n            if output_all_encoded_layers:\n                all_encoder_layers.append(hidden_states)\n        if not output_all_encoded_layers:\n            all_encoder_layers.append(hidden_states)\n        return all_encoder_layers, all_attention_probs_sum\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(BertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertLMPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n                                 bert_model_embedding_weights.size(0),\n                                 bias=False)\n        self.decoder.weight = bert_model_embedding_weights\n        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertOnlyMLMHead, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertPreTrainingHeads, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\n\nclass BertPreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    def __init__(self, config, *inputs, **kwargs):\n        super(BertPreTrainedModel, self).__init__()\n        if not isinstance(config, BertConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `BertConfig`. ""\n                ""To create a model from a Google pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        self.config = config\n\n    def init_bert_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n        """"""\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        """"""\n        state_dict = kwargs.get(\'state_dict\', None)\n        kwargs.pop(\'state_dict\', None)\n        cache_dir = kwargs.get(\'cache_dir\', None)\n        kwargs.pop(\'cache_dir\', None)\n        from_tf = kwargs.get(\'from_tf\', False)\n        kwargs.pop(\'from_tf\', None)\n\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                ""associated to this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n                    archive_file))\n            return None\n        if resolved_archive_file == archive_file:\n            logger.info(""loading archive file {}"".format(archive_file))\n        else:\n            logger.info(""loading archive file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n        tempdir = None\n        if os.path.isdir(resolved_archive_file) or from_tf:\n            serialization_dir = resolved_archive_file\n        else:\n            # Extract archive to temp dir\n            tempdir = tempfile.mkdtemp()\n            logger.info(""extracting archive file {} to temp dir {}"".format(\n                resolved_archive_file, tempdir))\n            with tarfile.open(resolved_archive_file, \'r:gz\') as archive:\n                archive.extractall(tempdir)\n            serialization_dir = tempdir\n        # Load config\n        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n        if not os.path.exists(config_file):\n            # Backward compatibility with old naming format\n            config_file = os.path.join(serialization_dir, BERT_CONFIG_NAME)\n        config = BertConfig.from_json_file(config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n            state_dict = torch.load(weights_path, map_location=\'cpu\')\n        if tempdir:\n            # Clean up temp dir\n            shutil.rmtree(tempdir)\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint\n            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n            return load_tf_weights_in_bert(model, weights_path)\n        # Load from a PyTorch state_dict\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if \'gamma\' in key:\n                new_key = key.replace(\'gamma\', \'weight\')\n            if \'beta\' in key:\n                new_key = key.replace(\'beta\', \'bias\')\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \'_metadata\', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=\'\'):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + \'.\')\n        start_prefix = \'\'\n        if not hasattr(model, \'bert\') and any(s.startswith(\'bert.\') for s in state_dict.keys()):\n            start_prefix = \'bert.\'\n        load(model, prefix=start_prefix)\n        if len(missing_keys) > 0:\n            logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(""Weights from pretrained model not used in {}: {}"".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) > 0:\n            raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\n                               model.__class__.__name__, ""\\n\\t"".join(error_msgs)))\n        return model\n\n\nclass BertModel(BertPreTrainedModel):\n    """"""BERT model (""Bidirectional Embedding Representations from a Transformer"").\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n\n    Outputs: Tuple of (encoded_layers, pooled_output)\n        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n            classifier pretrained on top of the hidden state associated to the first character of the\n            input (`CLS`) to train on the Next-Sentence task (see BERT\'s paper).\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = modeling.BertModel(config=config)\n    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, output_score=False, output_sum=1):\n        super(BertModel, self).__init__(config)\n        config.output_score = output_score\n        config.output_sum   = output_sum\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, output_attention_layer=[]):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        embedding_output = self.embeddings(input_ids, token_type_ids)\n        encoded_layers,attention_probs_sum = self.encoder(embedding_output,\n                                      extended_attention_mask,\n                                      output_all_encoded_layers=output_all_encoded_layers,\n                                      output_attention_layer=output_attention_layer)\n        sequence_output = encoded_layers[-1]\n        pooled_output = self.pooler(sequence_output)\n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n        if len(output_attention_layer)>0:\n            return encoded_layers, pooled_output, attention_probs_sum\n        else:\n            return encoded_layers, pooled_output\n\n\nclass BertForPreTraining(BertPreTrainedModel):\n    """"""BERT model with pre-training heads.\n    This module comprises the BERT model followed by the two pre-training heads:\n        - the masked language modeling head, and\n        - the next sentence classification head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: optional masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n        `next_sentence_label`: optional next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n\n    Outputs:\n        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `masked_lm_labels` or `next_sentence_label` is `None`:\n            Outputs a tuple comprising\n            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n            - the next sentence classification logits of shape [batch_size, 2].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForPreTraining(config)\n    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForPreTraining, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                                   output_all_encoded_layers=False)\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            return total_loss\n        else:\n            return prediction_scores, seq_relationship_score\n\n\nclass BertForMaskedLM(BertPreTrainedModel):\n    """"""BERT model with the masked language modeling head.\n    This module comprises the BERT model followed by the masked language modeling head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n\n    Outputs:\n        if `masked_lm_labels` is  not `None`:\n            Outputs the masked language modeling loss.\n        if `masked_lm_labels` is `None`:\n            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForMaskedLM(config)\n    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForMaskedLM, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n                                       output_all_encoded_layers=False)\n        prediction_scores = self.cls(sequence_output)\n\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            return masked_lm_loss\n        else:\n            return prediction_scores\n\n\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    """"""BERT model with next sentence prediction head.\n    This module comprises the BERT model followed by the next sentence classification head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n\n    Outputs:\n        if `next_sentence_label` is not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `next_sentence_label` is `None`:\n            Outputs the next sentence classification logits of shape [batch_size, 2].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForNextSentencePrediction(config)\n    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForNextSentencePrediction, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                     output_all_encoded_layers=False)\n        seq_relationship_score = self.cls( pooled_output)\n\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            return next_sentence_loss\n        else:\n            return seq_relationship_score\n\n\nclass BertForSequenceClassification(BertPreTrainedModel):\n    """"""BERT model for classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary. Items in the batch should begin with the special ""CLS"" token. (see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_labels].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_labels = 2\n\n    model = BertForSequenceClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForMultipleChoice(BertPreTrainedModel):\n    """"""BERT model for multiple choice tasks.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_choices`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_choices = 2\n\n    model = BertForMultipleChoice(config, num_choices)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_choices):\n        super(BertForMultipleChoice, self).__init__(config)\n        self.num_choices = num_choices\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, self.num_choices)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            return loss\n        else:\n            return reshaped_logits\n\n\nclass BertForTokenClassification(BertPreTrainedModel):\n    """"""BERT model for token-level classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the full hidden state of the last layer.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [0, ..., num_labels].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_labels = 2\n\n    model = BertForTokenClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForTokenClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForQuestionAnswering(BertPreTrainedModel):\n    """"""BERT model for Question Answering (span extraction).\n    This module is composed of the BERT model with a linear layer on top of\n    the sequence output that computes start_logits and end_logits\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n            into account for computing the loss.\n        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n            into account for computing the loss.\n\n    Outputs:\n        if `start_positions` and `end_positions` are not `None`:\n            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n        if `start_positions` or `end_positions` is `None`:\n            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n            position tokens of shape [batch_size, sequence_length].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForQuestionAnswering(config)\n    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.bert = BertModel(config)\n        # TODO check with Google if it\'s normal there is no dropout on the token classifier of SQuAD in the TF version\n        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            return total_loss\n        else:\n            return start_logits, end_logits\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/modeling_gpt2.py,43,"b'# coding=utf-8\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch OpenAI GPT-2 model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .file_utils import cached_path, CONFIG_NAME, WEIGHTS_NAME\nfrom .modeling import BertLayerNorm as LayerNorm\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {""gpt2"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin""}\nPRETRAINED_CONFIG_ARCHIVE_MAP = {""gpt2"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json""}\n\ndef load_tf_weights_in_gpt2(model, gpt2_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(gpt2_checkpoint_path)\n    print(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array.squeeze())\n\n    for name, array in zip(names, arrays):\n        name = name[6:]  # skip ""model/""\n        name = name.split(\'/\')\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+\\d+\', m_name):\n                l = re.split(r\'(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'w\' or l[0] == \'g\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'b\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'wpe\' or l[0] == \'wte\':\n                pointer = getattr(pointer, l[0])\n                pointer = getattr(pointer, \'weight\')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\n\nclass GPT2Config(object):\n    """"""Configuration class to store the configuration of a `GPT2Model`.\n    """"""\n\n    def __init__(\n        self,\n        vocab_size_or_config_json_file=50257,\n        n_positions=1024,\n        n_ctx=1024,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n    ):\n        """"""Constructs GPT2Config.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `GPT2Model` or a configuration json file.\n            n_positions: Number of positional embeddings.\n            n_ctx: Size of the causal mask (usually same as n_positions).\n            n_embd: Dimensionality of the embeddings and hidden states.\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            layer_norm_epsilon: epsilon to use in the layer norm layers\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=""utf-8"") as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.n_ctx = n_ctx\n            self.n_positions = n_positions\n            self.n_embd = n_embd\n            self.n_layer = n_layer\n            self.n_head = n_head\n            self.layer_norm_epsilon = layer_norm_epsilon\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(\n                ""First argument must be either a vocabulary size (int)""\n                ""or the path to a pretrained model config file (str)""\n            )\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `GPT2Config` from a Python dictionary of parameters.""""""\n        config = GPT2Config(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `GPT2Config` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=""utf-8"") as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n\n\nclass Conv1D(nn.Module):\n    def __init__(self, nf, nx):\n        super(Conv1D, self).__init__()\n        self.nf = nf\n        w = torch.empty(nx, nf)\n        nn.init.normal_(w, std=0.02)\n        self.weight = Parameter(w)\n        self.bias = Parameter(torch.zeros(nf))\n\n    def forward(self, x):\n        size_out = x.size()[:-1] + (self.nf,)\n        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n        x = x.view(*size_out)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, nx, n_ctx, config, scale=False):\n        super(Attention, self).__init__()\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n        assert n_state % config.n_head == 0\n        self.register_buffer(""bias"", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.c_attn = Conv1D(n_state * 3, nx)\n        self.c_proj = Conv1D(n_state, nx)\n\n    def _attn(self, q, k, v):\n        w = torch.matmul(q, k)\n        if self.scale:\n            w = w / math.sqrt(v.size(-1))\n        nd, ns = w.size(-2), w.size(-1)\n        b = self.bias[:, :, ns-nd:ns, :ns]\n        w = w * b - 1e4 * (1 - b)\n\n        w = nn.Softmax(dim=-1)(w)\n        return torch.matmul(w, v)\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n\n    def split_heads(self, x, k=False):\n        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n        if k:\n            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n        else:\n            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n\n    def forward(self, x, layer_past=None):\n        x = self.c_attn(x)\n        query, key, value = x.split(self.split_size, dim=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n        if layer_past is not None:\n            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n            key = torch.cat((past_key, key), dim=-1)\n            value = torch.cat((past_value, value), dim=-2)\n        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n        a = self._attn(query, key, value)\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n        return a, present\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n        super(MLP, self).__init__()\n        nx = config.n_embd\n        self.c_fc = Conv1D(n_state, nx)\n        self.c_proj = Conv1D(nx, n_state)\n        self.act = gelu\n\n    def forward(self, x):\n        h = self.act(self.c_fc(x))\n        h2 = self.c_proj(h)\n        return h2\n\n\nclass Block(nn.Module):\n    def __init__(self, n_ctx, config, scale=False):\n        super(Block, self).__init__()\n        nx = config.n_embd\n        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n        self.attn = Attention(nx, n_ctx, config, scale)\n        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n        self.mlp = MLP(4 * nx, config)\n\n    def forward(self, x, layer_past=None):\n        a, present = self.attn(self.ln_1(x), layer_past=layer_past)\n        x = x + a\n        m = self.mlp(self.ln_2(x))\n        x = x + m\n        return x, present\n\n\nclass GPT2LMHead(nn.Module):\n    """""" Language Model Head for the transformer """"""\n\n    def __init__(self, model_embeddings_weights, config):\n        super(GPT2LMHead, self).__init__()\n        self.n_embd = config.n_embd\n        self.set_embeddings_weights(model_embeddings_weights)\n\n    def set_embeddings_weights(self, model_embeddings_weights):\n        embed_shape = model_embeddings_weights.shape\n        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n        self.decoder.weight = model_embeddings_weights  # Tied weights\n\n    def forward(self, hidden_state):\n        # Truncated Language modeling logits (we remove the last token)\n        # h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)\n        lm_logits = self.decoder(hidden_state)\n        return lm_logits\n\n\nclass GPT2MultipleChoiceHead(nn.Module):\n    """""" Classifier Head for the transformer """"""\n\n    def __init__(self, config):\n        super(GPT2MultipleChoiceHead, self).__init__()\n        self.n_embd = config.n_embd\n        self.linear = nn.Linear(config.n_embd, 1)\n\n        nn.init.normal_(self.linear.weight, std=0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, hidden_states, mc_token_ids):\n        # Classification logits\n        # hidden_state (bsz, num_choices, seq_length, hidden_size)\n        # mc_token_ids (bsz, num_choices)\n        mc_token_ids = mc_token_ids.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, hidden_states.size(-1))\n        # (bsz, num_choices, 1, hidden_size)\n        multiple_choice_h = hidden_states.gather(2, mc_token_ids).squeeze(2)\n        # (bsz, num_choices, hidden_size)\n        multiple_choice_logits = self.linear(multiple_choice_h).squeeze(-1)\n        # (bsz, num_choices)\n        return multiple_choice_logits\n\n\nclass GPT2PreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n\n    def __init__(self, config, *inputs, **kwargs):\n        super(GPT2PreTrainedModel, self).__init__()\n        if not isinstance(config, GPT2Config):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `GPT2Config`. ""\n                ""To create a model from a pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                )\n            )\n        self.config = config\n\n    def set_tied(self):\n        pass\n\n    def init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    @classmethod\n    def from_pretrained(\n        cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None, from_tf=False, *inputs, **kwargs\n    ):\n        """"""\n        Instantiate a GPT2PreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `gpt2`\n                - a path or url to a pretrained model archive containing:\n                    . `gpt2_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a GPT2Model instance\n                - a path or url to a pretrained model archive containing:\n                    . `gpt2_config.json` a configuration file for the model\n                    . a TensorFlow checkpoint with trained weights\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionary (collections.OrderedDict object) to use instead of pre-trained models\n            *inputs, **kwargs: additional input for the specific GPT class\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n            config_file = PRETRAINED_CONFIG_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} and {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path, "", "".join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()), pretrained_model_name_or_path,\n                    archive_file, config_file\n                )\n            )\n            return None\n        if resolved_archive_file == archive_file and resolved_config_file == config_file:\n            logger.info(""loading weights file {}"".format(archive_file))\n            logger.info(""loading configuration file {}"".format(config_file))\n        else:\n            logger.info(""loading weights file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n            logger.info(""loading configuration file {} from cache at {}"".format(\n                config_file, resolved_config_file))\n        # Load config\n        config = GPT2Config.from_json_file(resolved_config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            state_dict = torch.load(resolved_archive_file, map_location=\'cpu\')\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint (stored as NumPy array)\n            return load_tf_weights_in_gpt2(model, resolved_archive_file)\n\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if key.endswith("".g""):\n                new_key = key[:-2] + "".weight""\n            elif key.endswith("".b""):\n                new_key = key[:-2] + "".bias""\n            elif key.endswith("".w""):\n                new_key = key[:-2] + "".weight""\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, ""_metadata"", None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=""""):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\n            )\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + ""."")\n\n        start_model = model\n        if hasattr(model, ""transformer"") and all(not s.startswith(\'transformer.\') for s in state_dict.keys()):\n            start_model = model.transformer\n        load(start_model, prefix="""")\n\n        if len(missing_keys) > 0:\n            logger.info(\n                ""Weights of {} not initialized from pretrained model: {}"".format(model.__class__.__name__, missing_keys)\n            )\n        if len(unexpected_keys) > 0:\n            logger.info(\n                ""Weights from pretrained model not used in {}: {}"".format(model.__class__.__name__, unexpected_keys)\n            )\n        if len(error_msgs) > 0:\n            raise RuntimeError(\n                ""Error(s) in loading state_dict for {}:\\n\\t{}"".format(model.__class__.__name__, ""\\n\\t"".join(error_msgs))\n            )\n\n        # Make sure we are still sharing the output and input embeddings after loading weights\n        model.set_tied()\n        return model\n\n\nclass GPT2Model(GPT2PreTrainedModel):\n    """"""OpenAI GPT-2 model (""Language Models are Unsupervised Multitask Learners"").\n\n    Params:\n        config: a GPT2Config class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] (or more generally [d_1, ..., d_n, sequence_length]\n            were d_1 ... d_n are arbitrary dimensions) with the word BPE token indices selected in the range [0, config.vocab_size[\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n        `past`: an optional list of torch.LongTensor that contains pre-computed hidden-states\n            (key and values in the attention blocks) to speed up sequential decoding\n            (this is the presents output of the model, cf. below).\n\n    Outputs a tuple consisting of:\n        `hidden_states`: the encoded-hidden-states at the top of the model\n            as a torch.FloatTensor of size [batch_size, sequence_length, hidden_size]\n            (or more generally [d_1, ..., d_n, hidden_size] were d_1 ... d_n are the dimension of input_ids)\n        `presents`: a list of pre-computed hidden-states (key and values in each attention blocks) as\n            torch.FloatTensors. They can be reused to speed up sequential decoding.\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n\n    config = modeling_gpt2.GPT2Config()\n\n    model = modeling_gpt2.GPT2Model(config)\n    hidden_states, presents = model(input_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(GPT2Model, self).__init__(config)\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n        block = Block(config.n_ctx, config, scale=True)\n        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n\n        self.apply(self.init_weights)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, past=None):\n        if past is None:\n            past_length = 0\n            past = [None] * len(self.h)\n        else:\n            past_length = past[0][0].size(-2)\n        if position_ids is None:\n            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        position_ids = position_ids.view(-1, position_ids.size(-1))\n\n        inputs_embeds = self.wte(input_ids)\n        position_embeds = self.wpe(position_ids)\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n            token_type_embeds = self.wte(token_type_ids)\n        else:\n            token_type_embeds = 0\n        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n        presents = []\n        for block, layer_past in zip(self.h, past):\n            hidden_states, present = block(hidden_states, layer_past)\n            presents.append(present)\n        hidden_states = self.ln_f(hidden_states)\n        output_shape = input_shape + (hidden_states.size(-1),)\n        return hidden_states.view(*output_shape), presents\n\n\nclass GPT2LMHeadModel(GPT2PreTrainedModel):\n    """"""OpenAI GPT-2 model with a Language Modeling head (""Language Models are Unsupervised Multitask Learners"").\n\n    Params:\n        config: a GPT2Config class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] (or more generally [d_1, ..., d_n, sequence_length]\n            were d_1 ... d_n are arbitrary dimensions) with the word BPE token indices selected in the range [0, config.vocab_size[\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n        `lm_labels`: optional language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n        `past`: an optional list of torch.LongTensor that contains pre-computed hidden-states\n            (key and values in the attention blocks) to speed up sequential decoding\n            (this is the presents output of the model, cf. below).\n\n    Outputs:\n        if `lm_labels` is not `None`:\n            Outputs the language modeling loss.\n        else a tuple:\n            `lm_logits`: the language modeling logits as a torch.FloatTensor of size [batch_size, sequence_length, config.vocab_size]\n                (or more generally [d_1, ..., d_n, config.vocab_size] were d_1 ... d_n are the dimension of input_ids)\n            `presents`: a list of pre-computed hidden-states (key and values in each attention blocks) as\n                torch.FloatTensors. They can be reused to speed up sequential decoding.\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n\n    config = modeling_gpt2.GPT2Config()\n\n    model = modeling_gpt2.GPT2LMHeadModel(config)\n    lm_logits, presents = model(input_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(GPT2LMHeadModel, self).__init__(config)\n        self.transformer = GPT2Model(config)\n        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n        self.apply(self.init_weights)\n\n    def set_tied(self):\n        """""" Make sure we are sharing the embeddings\n        """"""\n        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        lm_logits = self.lm_head(hidden_states)\n        if lm_labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = lm_logits[:, :-1].contiguous()\n            shift_labels = lm_labels[:, 1:].contiguous()\n\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n            return loss\n        return lm_logits, presents\n\n\nclass GPT2DoubleHeadsModel(GPT2PreTrainedModel):\n    """"""OpenAI GPT-2 model with a Language Modeling and a Multiple Choice head (""Language Models are Unsupervised Multitask Learners"").\n\n    Params:\n        config: a GPT2Config class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length] with the BPE token\n            indices selected in the range [0, config.vocab_size[\n        `mc_token_ids`: a torch.LongTensor of shape [batch_size, num_choices] with the index of the token from\n            which we should take the hidden state to feed the multiple choice classifier (usually last token of the sequence)\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n        `lm_labels`: optional language modeling labels: torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with indices selected in [-1, 0, ..., config.vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., config.vocab_size]\n        `multiple_choice_labels`: optional multiple choice labels: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n        `past`: an optional list of torch.LongTensor that contains pre-computed hidden-states\n            (key and values in the attention blocks) to speed up sequential decoding\n            (this is the presents output of the model, cf. below).\n\n    Outputs:\n        if `lm_labels` and `multiple_choice_labels` are not `None`:\n            Outputs a tuple of losses with the language modeling loss and the multiple choice loss.\n        else: a tuple with\n            `lm_logits`: the language modeling logits as a torch.FloatTensor of size [batch_size, num_choices, sequence_length, config.vocab_size]\n            `multiple_choice_logits`: the multiple choice logits as a torch.FloatTensor of size [batch_size, num_choices]\n            `presents`: a list of pre-computed hidden-states (key and values in each attention blocks) as\n                torch.FloatTensors. They can be reused to speed up sequential decoding.\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]]])  # (bsz, number of choice, seq length)\n    mc_token_ids = torch.LongTensor([[2], [1]]) # (bsz, number of choice)\n\n    config = modeling_gpt2.GPT2Config()\n\n    model = modeling_gpt2.GPT2LMHeadModel(config)\n    lm_logits, multiple_choice_logits, presents = model(input_ids, mc_token_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(GPT2DoubleHeadsModel, self).__init__(config)\n        self.transformer = GPT2Model(config)\n        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n        self.multiple_choice_head = GPT2MultipleChoiceHead(config)\n        self.apply(self.init_weights)\n\n    def set_tied(self):\n        """""" Make sure we are sharing the embeddings\n        """"""\n        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n\n    def forward(self, input_ids, mc_token_ids, lm_labels=None, mc_labels=None, token_type_ids=None, position_ids=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        lm_logits = self.lm_head(hidden_states)\n        mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids)\n        losses = []\n        if lm_labels is not None:\n            shift_logits = lm_logits[:, :-1].contiguous()\n            shift_labels = lm_labels[:, 1:].contiguous()\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            losses.append(loss_fct(shift_logits.view(-1,\n                          shift_logits.size(-1)), shift_labels.view(-1)))\n        if mc_labels is not None:\n            loss_fct = CrossEntropyLoss()\n            losses.append(loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1)))\n        if losses:\n            return losses\n        return lm_logits, mc_logits, presents\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/modeling_openai.py,38,"b'# coding=utf-8\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch OpenAI GPT model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .file_utils import cached_path, CONFIG_NAME, WEIGHTS_NAME\nfrom .modeling import BertLayerNorm as LayerNorm\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {""openai-gpt"": ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin""}\nPRETRAINED_CONFIG_ARCHIVE_MAP = {""openai-gpt"": ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json""}\n\n\ndef load_tf_weights_in_openai_gpt(model, openai_checkpoint_folder_path):\n    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)\n    """"""\n    import re\n    import numpy as np\n    print(""Loading weights..."")\n    names = json.load(open(openai_checkpoint_folder_path + \'/parameters_names.json\', ""r"", encoding=\'utf-8\'))\n    shapes = json.load(open(openai_checkpoint_folder_path + \'/params_shapes.json\', ""r"", encoding=\'utf-8\'))\n    offsets = np.cumsum([np.prod(shape) for shape in shapes])\n    init_params = [np.load(openai_checkpoint_folder_path + \'/params_{}.npy\'.format(n)) for n in range(10)]\n    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\n    init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n\n    # This was used when we had a single embedding matrix for positions and tokens\n    # init_params[0] = np.concatenate([init_params[1], init_params[0]], 0)\n    # del init_params[1]\n    init_params = [arr.squeeze() for arr in init_params]\n\n    try:\n        assert model.tokens_embed.weight.shape == init_params[1].shape\n        assert model.positions_embed.weight.shape == init_params[0].shape\n    except AssertionError as e:\n        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)\n        e.args += (model.positions_embed.weight.shape, init_params[0].shape)\n        raise\n\n    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])\n    model.positions_embed.weight.data = torch.from_numpy(init_params[0])\n    names.pop(0)\n    # Pop position and token embedding arrays\n    init_params.pop(0)\n    init_params.pop(0)\n\n    for name, array in zip(names, init_params): # names[1:n_transfer], init_params[1:n_transfer]):\n        name = name[6:]  # skip ""model/""\n        assert name[-2:] == "":0""\n        name = name[:-2]\n        name = name.split(\'/\')\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+\\d+\', m_name):\n                l = re.split(r\'(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'g\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'b\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'w\':\n                pointer = getattr(pointer, \'weight\')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT_FNS = {""relu"": nn.ReLU, ""swish"": swish, ""gelu"": gelu}\n\n\nclass OpenAIGPTConfig(object):\n    """"""Configuration class to store the configuration of a `OpenAIGPTModel`.\n    """"""\n\n    def __init__(\n        self,\n        vocab_size_or_config_json_file=40478,\n        n_special=0,\n        n_positions=512,\n        n_ctx=512,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        afn=""gelu"",\n        resid_pdrop=0.1,\n        embd_pdrop=0.1,\n        attn_pdrop=0.1,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n    ):\n        """"""Constructs OpenAIGPTConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `OpenAIGPTModel` or a configuration json file.\n            n_special: The number of special tokens to learn during fine-tuning (\'[SEP]\', \'[CLF]\', ...)\n            n_positions: Number of positional embeddings.\n            n_ctx: Size of the causal mask (usually same as n_positions).\n            n_embd: Dimensionality of the embeddings and hidden states.\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            afn: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            resid_pdrop: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attn_pdrop: The dropout ratio for the attention\n                probabilities.\n            embd_pdrop: The dropout ratio for the embeddings.\n            layer_norm_epsilon: epsilon to use in the layer norm layers\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=""utf-8"") as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.n_special = n_special\n            self.n_ctx = n_ctx\n            self.n_positions = n_positions\n            self.n_embd = n_embd\n            self.n_layer = n_layer\n            self.n_head = n_head\n            self.afn = afn\n            self.resid_pdrop = resid_pdrop\n            self.embd_pdrop = embd_pdrop\n            self.attn_pdrop = attn_pdrop\n            self.layer_norm_epsilon = layer_norm_epsilon\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(\n                ""First argument must be either a vocabulary size (int)""\n                ""or the path to a pretrained model config file (str)""\n            )\n\n    @property\n    def total_tokens_embeddings(self):\n        return self.vocab_size + self.n_special\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `OpenAIGPTConfig` from a Python dictionary of parameters.""""""\n        config = OpenAIGPTConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `OpenAIGPTConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=""utf-8"") as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n\n\nclass Conv1D(nn.Module):\n    def __init__(self, nf, rf, nx):\n        super(Conv1D, self).__init__()\n        self.rf = rf\n        self.nf = nf\n        if rf == 1:  # faster 1x1 conv\n            w = torch.empty(nx, nf)\n            nn.init.normal_(w, std=0.02)\n            self.weight = Parameter(w)\n            self.bias = Parameter(torch.zeros(nf))\n        else:  # was used to train LM\n            raise NotImplementedError\n\n    def forward(self, x):\n        if self.rf == 1:\n            size_out = x.size()[:-1] + (self.nf,)\n            x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n            x = x.view(*size_out)\n        else:\n            raise NotImplementedError\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, nx, n_ctx, config, scale=False):\n        super(Attention, self).__init__()\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n        assert n_state % config.n_head == 0\n        self.register_buffer(""bias"", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.c_attn = Conv1D(n_state * 3, 1, nx)\n        self.c_proj = Conv1D(n_state, 1, nx)\n        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n\n    def _attn(self, q, k, v):\n        w = torch.matmul(q, k)\n        if self.scale:\n            w = w / math.sqrt(v.size(-1))\n        # w = w * self.bias + -1e9 * (1 - self.bias)  # TF implem method: mask_attn_weights\n        # XD: self.b may be larger than w, so we need to crop it\n        b = self.bias[:, :, : w.size(-2), : w.size(-1)]\n        w = w * b + -1e9 * (1 - b)\n\n        w = nn.Softmax(dim=-1)(w)\n        w = self.attn_dropout(w)\n        return torch.matmul(w, v)\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n\n    def split_heads(self, x, k=False):\n        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n        if k:\n            return x.permute(0, 2, 3, 1)\n        else:\n            return x.permute(0, 2, 1, 3)\n\n    def forward(self, x):\n        x = self.c_attn(x)\n        query, key, value = x.split(self.split_size, dim=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n        a = self._attn(query, key, value)\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n        a = self.resid_dropout(a)\n        return a\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n        super(MLP, self).__init__()\n        nx = config.n_embd\n        self.c_fc = Conv1D(n_state, 1, nx)\n        self.c_proj = Conv1D(nx, 1, n_state)\n        self.act = ACT_FNS[config.afn]\n        self.dropout = nn.Dropout(config.resid_pdrop)\n\n    def forward(self, x):\n        h = self.act(self.c_fc(x))\n        h2 = self.c_proj(h)\n        return self.dropout(h2)\n\n\nclass Block(nn.Module):\n    def __init__(self, n_ctx, config, scale=False):\n        super(Block, self).__init__()\n        nx = config.n_embd\n        self.attn = Attention(nx, n_ctx, config, scale)\n        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n        self.mlp = MLP(4 * nx, config)\n        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n\n    def forward(self, x):\n        a = self.attn(x)\n        n = self.ln_1(x + a)\n        m = self.mlp(n)\n        h = self.ln_2(n + m)\n        return h\n\n\nclass OpenAIGPTLMHead(nn.Module):\n    """""" Language Model Head for the transformer """"""\n\n    def __init__(self, model_embeddings_weights, config):\n        super(OpenAIGPTLMHead, self).__init__()\n        self.n_embd = config.n_embd\n        self.set_embeddings_weights(model_embeddings_weights)\n\n    def set_embeddings_weights(self, model_embeddings_weights):\n        embed_shape = model_embeddings_weights.shape\n        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n        self.decoder.weight = model_embeddings_weights  # Tied weights\n\n    def forward(self, hidden_state):\n        # Truncated Language modeling logits (we remove the last token)\n        # h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)\n        lm_logits = self.decoder(hidden_state)\n        return lm_logits\n\n\nclass OpenAIGPTMultipleChoiceHead(nn.Module):\n    """""" Classifier Head for the transformer """"""\n\n    def __init__(self, config):\n        super(OpenAIGPTMultipleChoiceHead, self).__init__()\n        self.n_embd = config.n_embd\n        # self.multiple_choice_token = multiple_choice_token\n        self.dropout = nn.Dropout2d(config.resid_pdrop)  # To reproduce the noise_shape parameter of TF implementation\n        self.linear = nn.Linear(config.n_embd, 1)\n\n        nn.init.normal_(self.linear.weight, std=0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, hidden_states, mc_token_ids):\n        # Classification logits\n        # hidden_state (bsz, num_choices, seq_length, hidden_size)\n        # mc_token_ids (bsz, num_choices)\n        mc_token_ids = mc_token_ids.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, hidden_states.size(-1))\n        # (bsz, num_choices, 1, hidden_size)\n        multiple_choice_h = hidden_states.gather(2, mc_token_ids).squeeze(2)\n        # (bsz, num_choices, hidden_size)\n        multiple_choice_h = self.dropout(multiple_choice_h.transpose(1, 2)).transpose(1, 2)\n        multiple_choice_logits = self.linear(multiple_choice_h).squeeze(-1)\n        # (bsz, num_choices)\n        return multiple_choice_logits\n\n\nclass OpenAIGPTPreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n\n    def __init__(self, config, *inputs, **kwargs):\n        super(OpenAIGPTPreTrainedModel, self).__init__()\n        if not isinstance(config, OpenAIGPTConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `OpenAIGPTConfig`. ""\n                ""To create a model from a pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                )\n            )\n        self.config = config\n\n    def init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def set_num_special_tokens(self, num_special_tokens):\n        pass\n\n    @classmethod\n    def from_pretrained(\n        cls, pretrained_model_name_or_path, num_special_tokens=None, state_dict=None, cache_dir=None, from_tf=False, *inputs, **kwargs\n    ):\n        """"""\n        Instantiate a OpenAIGPTPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `openai-gpt`\n                - a path or url to a pretrained model archive containing:\n                    . `openai_gpt_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a OpenAIGPTModel instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . a series of NumPy files containing OpenAI TensorFlow trained weights\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n            config_file = PRETRAINED_CONFIG_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} and {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path, "", "".join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()), pretrained_model_name_or_path,\n                    archive_file, config_file\n                )\n            )\n            return None\n        if resolved_archive_file == archive_file and resolved_config_file == config_file:\n            logger.info(""loading weights file {}"".format(archive_file))\n            logger.info(""loading configuration file {}"".format(config_file))\n        else:\n            logger.info(""loading weights file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n            logger.info(""loading configuration file {} from cache at {}"".format(\n                config_file, resolved_config_file))\n        # Load config\n        config = OpenAIGPTConfig.from_json_file(resolved_config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            state_dict = torch.load(resolved_archive_file, map_location=\'cpu\')\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint (stored as NumPy array)\n            return load_tf_weights_in_openai_gpt(model, resolved_archive_file)\n\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if key.endswith("".g""):\n                new_key = key[:-2] + "".weight""\n            elif key.endswith("".b""):\n                new_key = key[:-2] + "".bias""\n            elif key.endswith("".w""):\n                new_key = key[:-2] + "".weight""\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, ""_metadata"", None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=""""):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\n            )\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + ""."")\n\n        start_model = model\n        if hasattr(model, ""transformer"") and all(not s.startswith(\'transformer.\') for s in state_dict.keys()):\n            start_model = model.transformer\n        load(start_model, prefix="""")\n\n        if len(missing_keys) > 0:\n            logger.info(\n                ""Weights of {} not initialized from pretrained model: {}"".format(model.__class__.__name__, missing_keys)\n            )\n        if len(unexpected_keys) > 0:\n            logger.info(\n                ""Weights from pretrained model not used in {}: {}"".format(model.__class__.__name__, unexpected_keys)\n            )\n        if len(error_msgs) > 0:\n            raise RuntimeError(\n                ""Error(s) in loading state_dict for {}:\\n\\t{}"".format(model.__class__.__name__, ""\\n\\t"".join(error_msgs))\n            )\n\n        # Add additional embeddings for special tokens if needed\n        # This step also make sure we are still sharing the output and input embeddings after loading weights\n        model.set_num_special_tokens(num_special_tokens if num_special_tokens is not None else config.n_special)\n        return model\n\n\nclass OpenAIGPTModel(OpenAIGPTPreTrainedModel):\n    """"""OpenAI GPT model (""Improving Language Understanding by Generative Pre-Training"").\n\n    OpenAI GPT use a single embedding matrix to store the word and special embeddings.\n    Special tokens embeddings are additional tokens that are not pre-trained: [SEP], [CLS]...\n    Special tokens need to be trained during the fine-tuning if you use them.\n    The number of special embeddings can be controled using the `set_num_special_tokens(num_special_tokens)` function.\n\n    The embeddings are ordered as follow in the token embeddings matrice:\n        [0,                                                         ----------------------\n         ...                                                        -> word embeddings\n         config.vocab_size - 1,                                     ______________________\n         config.vocab_size,\n         ...                                                        -> special embeddings\n         config.vocab_size + config.n_special - 1]                  ______________________\n\n    where total_tokens_embeddings can be obtained as config.total_tokens_embeddings and is:\n        total_tokens_embeddings = config.vocab_size + config.n_special\n    You should use the associate indices to index the embeddings.\n\n    Params:\n        config: a OpenAIGPTConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] (or more generally [d_1, ..., d_n, sequence_length]\n            were d_1 ... d_n are arbitrary dimensions) with the word BPE token indices selected in the range [0, total_tokens_embeddings[\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n\n    Outputs:\n        `hidden_states`: the encoded-hidden-states at the top of the model\n            as a torch.FloatTensor of size [batch_size, sequence_length, hidden_size]\n            (or more generally [d_1, ..., d_n, hidden_size] were d_1 ... d_n are the dimension of input_ids)\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n\n    config = modeling_openai.OpenAIGPTConfig()\n\n    model = modeling_openai.OpenAIGPTModel(config)\n    hidden_states = model(input_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(OpenAIGPTModel, self).__init__(config)\n        num_tokens = config.vocab_size + config.n_special\n        self.tokens_embed = nn.Embedding(num_tokens, config.n_embd)\n        self.positions_embed = nn.Embedding(config.n_positions, config.n_embd)\n        self.drop = nn.Dropout(config.embd_pdrop)\n        block = Block(config.n_ctx, config, scale=True)\n        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n\n        self.apply(self.init_weights)\n        # nn.init.normal_(self.embed.weight, std=0.02)\n\n    def set_num_special_tokens(self, num_special_tokens):\n        "" Update input embeddings with new embedding matrice if needed ""\n        if self.config.n_special == num_special_tokens:\n            return\n        # Update config\n        self.config.n_special = num_special_tokens\n        # Build new embeddings and initialize all new embeddings (in particular the special tokens)\n        old_embed = self.tokens_embed\n        self.tokens_embed = nn.Embedding(self.config.total_tokens_embeddings, self.config.n_embd)\n        self.tokens_embed.to(old_embed.weight.device)\n        self.init_weights(self.tokens_embed)\n        # Copy word embeddings from the previous weights\n        self.tokens_embed.weight.data[:self.config.vocab_size, :] = old_embed.weight.data[:self.config.vocab_size, :]\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None):\n        if position_ids is None:\n            # This was used when we had a single embedding matrice from position and token embeddings\n            # start = self.config.vocab_size + self.config.n_special\n            # end = start + input_ids.size(-1)\n            # position_ids = torch.arange(start, end, dtype=torch.long, device=input_ids.device)\n            position_ids = torch.arange(input_ids.size(-1), dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        position_ids = position_ids.view(-1, position_ids.size(-1))\n\n        inputs_embeds = self.tokens_embed(input_ids)\n        position_embeds = self.positions_embed(position_ids)\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n            token_type_embeds = self.tokens_embed(token_type_ids)\n        else:\n            token_type_embeds = 0\n        # Add the position information to the input embeddings\n        # h = e.sum(dim=2)\n        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n        for block in self.h:\n            hidden_states = block(hidden_states)\n        output_shape = input_shape + (hidden_states.size(-1),)\n        return hidden_states.view(*output_shape)\n\n\nclass OpenAIGPTLMHeadModel(OpenAIGPTPreTrainedModel):\n    """"""OpenAI GPT model with a Language Modeling head (""Improving Language Understanding by Generative Pre-Training"").\n\n    OpenAI GPT use a single embedding matrix to store the word and special embeddings.\n    Special tokens embeddings are additional tokens that are not pre-trained: [SEP], [CLS]...\n    Special tokens need to be trained during the fine-tuning if you use them.\n    The number of special embeddings can be controled using the `set_num_special_tokens(num_special_tokens)` function.\n\n    The embeddings are ordered as follow in the token embeddings matrice:\n        [0,                                                         ----------------------\n         ...                                                        -> word embeddings\n         config.vocab_size - 1,                                     ______________________\n         config.vocab_size,\n         ...                                                        -> special embeddings\n         config.vocab_size + config.n_special - 1]                  ______________________\n\n    where total_tokens_embeddings can be obtained as config.total_tokens_embeddings and is:\n        total_tokens_embeddings = config.vocab_size + config.n_special\n    You should use the associate indices to index the embeddings.\n\n    Params:\n        config: a OpenAIGPTConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] (or more generally [d_1, ..., d_n, sequence_length]\n            were d_1 ... d_n are arbitrary dimensions) with the word BPE token indices selected in the range [0, total_tokens_embeddings[\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n        `lm_labels`: optional language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n\n    Outputs:\n        if `lm_labels` is not `None`:\n            Outputs the language modeling loss.\n        else:\n            `lm_logits`: the language modeling logits as a torch.FloatTensor of size [batch_size, sequence_length, total_tokens_embeddings]\n                (or more generally [d_1, ..., d_n, total_tokens_embeddings] were d_1 ... d_n are the dimension of input_ids)\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n\n    config = modeling_openai.OpenAIGPTConfig()\n\n    model = modeling_openai.OpenAIGPTLMHeadModel(config)\n    lm_logits = model(input_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(OpenAIGPTLMHeadModel, self).__init__(config)\n        self.transformer = OpenAIGPTModel(config)\n        self.lm_head = OpenAIGPTLMHead(self.transformer.tokens_embed.weight, config)\n        self.apply(self.init_weights)\n\n    def set_num_special_tokens(self, num_special_tokens):\n        """""" Update input and output embeddings with new embedding matrice\n            Make sure we are sharing the embeddings\n        """"""\n        self.transformer.set_num_special_tokens(num_special_tokens)\n        self.lm_head.set_embeddings_weights(self.transformer.tokens_embed.weight)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None):\n        hidden_states = self.transformer(input_ids, position_ids, token_type_ids)\n        lm_logits = self.lm_head(hidden_states)\n        if lm_labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = lm_logits[..., :-1, :].contiguous()\n            shift_labels = lm_labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n            return loss\n        return lm_logits\n\n\nclass OpenAIGPTDoubleHeadsModel(OpenAIGPTPreTrainedModel):\n    """"""OpenAI GPT model with a Language Modeling and a Multiple Choice head (""Improving Language Understanding by Generative Pre-Training"").\n\n    OpenAI GPT use a single embedding matrix to store the word and special embeddings.\n    Special tokens embeddings are additional tokens that are not pre-trained: [SEP], [CLS]...\n    Special tokens need to be trained during the fine-tuning if you use them.\n    The number of special embeddings can be controled using the `set_num_special_tokens(num_special_tokens)` function.\n\n    The embeddings are ordered as follow in the token embeddings matrice:\n        [0,                                                         ----------------------\n         ...                                                        -> word embeddings\n         config.vocab_size - 1,                                     ______________________\n         config.vocab_size,\n         ...                                                        -> special embeddings\n         config.vocab_size + config.n_special - 1]                  ______________________\n\n    where total_tokens_embeddings can be obtained as config.total_tokens_embeddings and is:\n        total_tokens_embeddings = config.vocab_size + config.n_special\n    You should use the associate indices to index the embeddings.\n\n    Params:\n        config: a OpenAIGPTConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length] with the BPE token\n            indices selected in the range [0, total_tokens_embeddings[\n        `mc_token_ids`: a torch.LongTensor of shape [batch_size, num_choices] with the index of the token from\n            which we should take the hidden state to feed the multiple choice classifier (usually last token of the sequence)\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n        `lm_labels`: optional language modeling labels: torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with indices selected in [-1, 0, ..., total_tokens_embeddings]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., total_tokens_embeddings]\n        `multiple_choice_labels`: optional multiple choice labels: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n\n    Outputs:\n        if `lm_labels` and `multiple_choice_labels` are not `None`:\n            Outputs a tuple of losses with the language modeling loss and the multiple choice loss.\n        else: a tuple with\n            `lm_logits`: the language modeling logits as a torch.FloatTensor of size [batch_size, num_choices, sequence_length, total_tokens_embeddings]\n            `multiple_choice_logits`: the multiple choice logits as a torch.FloatTensor of size [batch_size, num_choices]\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]]])  # (bsz, number of choice, seq length)\n    mc_token_ids = torch.LongTensor([[2], [1]]) # (bsz, number of choice)\n\n    config = modeling_openai.OpenAIGPTConfig()\n\n    model = modeling_openai.OpenAIGPTLMHeadModel(config)\n    lm_logits, multiple_choice_logits = model(input_ids, mc_token_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(OpenAIGPTDoubleHeadsModel, self).__init__(config)\n        self.transformer = OpenAIGPTModel(config)\n        self.lm_head = OpenAIGPTLMHead(self.transformer.tokens_embed.weight, config)\n        self.multiple_choice_head = OpenAIGPTMultipleChoiceHead(config)\n        self.apply(self.init_weights)\n\n    def set_num_special_tokens(self, num_special_tokens):\n        """""" Update input and output embeddings with new embedding matrice\n            Make sure we are sharing the embeddings\n        """"""\n        self.transformer.set_num_special_tokens(num_special_tokens)\n        self.lm_head.set_embeddings_weights(self.transformer.tokens_embed.weight)\n\n    def forward(self, input_ids, mc_token_ids, lm_labels=None, mc_labels=None, token_type_ids=None, position_ids=None):\n        hidden_states = self.transformer(input_ids, position_ids, token_type_ids)\n        lm_logits = self.lm_head(hidden_states)\n        mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids)\n        losses = []\n        if lm_labels is not None:\n            shift_logits = lm_logits[..., :-1, :].contiguous()\n            shift_labels = lm_labels[..., 1:].contiguous()\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            losses.append(loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)))\n        if mc_labels is not None:\n            loss_fct = CrossEntropyLoss()\n            losses.append(loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1)))\n        if losses:\n            return losses\n        return lm_logits, mc_logits\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/modeling_transfo_xl.py,70,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" PyTorch Transformer XL model.\n    Adapted from https://github.com/kimiyoung/transformer-xl.\n    In particular https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/mem_transformer.py\n""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport copy\nimport json\nimport math\nimport logging\nimport tarfile\nimport tempfile\nimport shutil\nimport collections\nimport sys\nfrom io import open\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .modeling import BertLayerNorm as LayerNorm\nfrom .modeling_transfo_xl_utilities import ProjectedAdaptiveLogSoftmax, sample_logits\nfrom .file_utils import cached_path, CONFIG_NAME, WEIGHTS_NAME\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin"",\n}\nPRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-config.json"",\n}\n\nTF_WEIGHTS_NAME = \'model.ckpt\'\n\ndef build_tf_to_pytorch_map(model, config):\n    """""" A map of modules from TF to PyTorch.\n        This time I use a map to keep the PyTorch model as identical to the original PyTorch model as possible.\n    """"""\n    tf_to_pt_map = {}\n\n    if hasattr(model, \'transformer\'):\n        # We are loading in a TransfoXLLMHeadModel => we will load also the Adaptive Softmax\n        tf_to_pt_map.update({\n            ""transformer/adaptive_softmax/cutoff_0/cluster_W"": model.crit.cluster_weight,\n            ""transformer/adaptive_softmax/cutoff_0/cluster_b"": model.crit.cluster_bias})\n        for i, (out_l, proj_l, tie_proj) in enumerate(zip(\n                                model.crit.out_layers,\n                                model.crit.out_projs,\n                                config.tie_projs)):\n            layer_str = ""transformer/adaptive_softmax/cutoff_%d/"" % i\n            if config.tie_weight:\n                tf_to_pt_map.update({\n                    layer_str + \'b\': out_l.bias})\n            else:\n                raise NotImplementedError\n                # I don\'t think this is implemented in the TF code\n                tf_to_pt_map.update({\n                    layer_str + \'lookup_table\': out_l.weight,\n                    layer_str + \'b\': out_l.bias})\n            if not tie_proj:\n                tf_to_pt_map.update({\n                    layer_str + \'proj\': proj_l\n                    })\n        # Now load the rest of the transformer\n        model = model.transformer\n\n    # Embeddings\n    for i, (embed_l, proj_l) in enumerate(zip(model.word_emb.emb_layers, model.word_emb.emb_projs)):\n        layer_str = ""transformer/adaptive_embed/cutoff_%d/"" % i\n        tf_to_pt_map.update({\n            layer_str + \'lookup_table\': embed_l.weight,\n            layer_str + \'proj_W\': proj_l\n            })\n\n    # Transformer blocks\n    for i, b in enumerate(model.layers):\n        layer_str = ""transformer/layer_%d/"" % i\n        tf_to_pt_map.update({\n            layer_str + ""rel_attn/LayerNorm/gamma"": b.dec_attn.layer_norm.weight,\n            layer_str + ""rel_attn/LayerNorm/beta"": b.dec_attn.layer_norm.bias,\n            layer_str + ""rel_attn/o/kernel"": b.dec_attn.o_net.weight,\n            layer_str + ""rel_attn/qkv/kernel"": b.dec_attn.qkv_net.weight,\n            layer_str + ""rel_attn/r/kernel"": b.dec_attn.r_net.weight,\n            layer_str + ""ff/LayerNorm/gamma"": b.pos_ff.layer_norm.weight,\n            layer_str + ""ff/LayerNorm/beta"": b.pos_ff.layer_norm.bias,\n            layer_str + ""ff/layer_1/kernel"": b.pos_ff.CoreNet[0].weight,\n            layer_str + ""ff/layer_1/bias"": b.pos_ff.CoreNet[0].bias,\n            layer_str + ""ff/layer_2/kernel"": b.pos_ff.CoreNet[3].weight,\n            layer_str + ""ff/layer_2/bias"": b.pos_ff.CoreNet[3].bias,\n        })\n\n    # Relative positioning biases\n    if config.untie_r:\n        r_r_list = []\n        r_w_list = []\n        for b in model.layers:\n            r_r_list.append(b.dec_attn.r_r_bias)\n            r_w_list.append(b.dec_attn.r_w_bias)\n    else:\n        r_r_list = [model.r_r_bias]\n        r_w_list = [model.r_w_bias]\n    tf_to_pt_map.update({\n        \'transformer/r_r_bias\': r_r_list,\n        \'transformer/r_w_bias\': r_w_list})\n    return tf_to_pt_map\n\ndef load_tf_weights_in_transfo_xl(model, config, tf_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    # Build TF to PyTorch weights loading map\n    tf_to_pt_map = build_tf_to_pytorch_map(model, config)\n\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    tf_weights = {}\n    for name, shape in init_vars:\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        tf_weights[name] = array\n\n    for name, pointer in tf_to_pt_map.items():\n        assert name in tf_weights\n        array = tf_weights[name]\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if \'kernel\' in name or \'proj\' in name:\n            array = np.transpose(array)\n        if (\'r_r_bias\' in name or \'r_w_bias\' in name) and len(pointer) > 1:\n            # Here we will split the TF weigths\n            assert len(pointer) == array.shape[0]\n            for i, p_i in enumerate(pointer):\n                arr_i = array[i, ...]\n                try:\n                    assert p_i.shape == arr_i.shape\n                except AssertionError as e:\n                    e.args += (p_i.shape, arr_i.shape)\n                    raise\n                print(""Initialize PyTorch weight {} for layer {}"".format(name, i))\n                p_i.data = torch.from_numpy(arr_i)\n        else:\n            try:\n                assert pointer.shape == array.shape\n            except AssertionError as e:\n                e.args += (pointer.shape, array.shape)\n                raise\n            print(""Initialize PyTorch weight {}"".format(name))\n            pointer.data = torch.from_numpy(array)\n        tf_weights.pop(name, None)\n        tf_weights.pop(name + \'/Adam\', None)\n        tf_weights.pop(name + \'/Adam_1\', None)\n\n    print(""Weights not copied to PyTorch model: {}"".format(\', \'.join(tf_weights.keys())))\n    return model\n\n\nclass TransfoXLConfig(object):\n    """"""Configuration class to store the configuration of a `TransfoXLModel`.\n    """"""\n    def __init__(self,\n                 vocab_size_or_config_json_file=267735,\n                 cutoffs=[20000, 40000, 200000],\n                 d_model=1024,\n                 d_embed=1024,\n                 n_head=16,\n                 d_head=64,\n                 d_inner=4096,\n                 div_val=4,\n                 pre_lnorm=False,\n                 n_layer=18,\n                 tgt_len=128,\n                 ext_len=0,\n                 mem_len=1600,\n                 clamp_len=1000,\n                 same_length=True,\n                 proj_share_all_but_first=True,\n                 attn_type=0,\n                 sample_softmax=-1,\n                 adaptive=True,\n                 tie_weight=True,\n                 dropout=0.1,\n                 dropatt=0.0,\n                 untie_r=True,\n                 init=""normal"",\n                 init_range=0.01,\n                 proj_init_std=0.01,\n                 init_std=0.02):\n        """"""Constructs TransfoXLConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `TransfoXLModel` or a configuration json file.\n            cutoffs: cutoffs for the adaptive softmax\n            d_model: Dimensionality of the model\'s hidden states.\n            d_embed: Dimensionality of the embeddings\n            d_head: Dimensionality of the model\'s heads.\n            div_val: divident value for adapative input and softmax\n            pre_lnorm: apply LayerNorm to the input instead of the output\n            d_inner: Inner dimension in FF\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            tgt_len: number of tokens to predict\n            ext_len: length of the extended context\n            mem_len: length of the retained previous heads\n            same_length: use the same attn length for all tokens\n            proj_share_all_but_first: True to share all but first projs, False not to share.\n            attn_type: attention type. 0 for Transformer-XL, 1 for Shaw et al, 2 for Vaswani et al, 3 for Al Rfou et al.\n            clamp_len: use the same pos embeddings after clamp_len\n            sample_softmax: number of samples in sampled softmax\n            adaptive: use adaptive softmax\n            tie_weight: tie the word embedding and softmax weights\n            dropout: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            dropatt: The dropout ratio for the attention probabilities.\n            untie_r: untie relative position biases           \n            embd_pdrop: The dropout ratio for the embeddings.\n            init: parameter initializer to use\n            init_range: parameters initialized by U(-init_range, init_range).\n            proj_init_std: parameters initialized by N(0, init_std)\n            init_std: parameters initialized by N(0, init_std)\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.n_token = vocab_size_or_config_json_file\n            self.cutoffs = []\n            self.cutoffs.extend(cutoffs)\n            self.tie_weight = tie_weight\n            if proj_share_all_but_first:\n                self.tie_projs = [False] + [True] * len(self.cutoffs)\n            else:\n                self.tie_projs = [False] + [False] * len(self.cutoffs)\n            self.d_model = d_model\n            self.d_embed = d_embed\n            self.d_head = d_head\n            self.d_inner = d_inner\n            self.div_val = div_val\n            self.pre_lnorm = pre_lnorm\n            self.n_layer = n_layer\n            self.n_head = n_head\n            self.tgt_len = tgt_len\n            self.ext_len = ext_len\n            self.mem_len = mem_len\n            self.same_length = same_length\n            self.attn_type = attn_type\n            self.clamp_len = clamp_len\n            self.sample_softmax = sample_softmax\n            self.adaptive = adaptive\n            self.dropout = dropout\n            self.dropatt = dropatt\n            self.untie_r = untie_r\n            self.init = init\n            self.init_range = init_range\n            self.proj_init_std = proj_init_std\n            self.init_std = init_std\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             ""or the path to a pretrained model config file (str)"")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `TransfoXLConfig` from a Python dictionary of parameters.""""""\n        config = TransfoXLConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `TransfoXLConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, demb):\n        super(PositionalEmbedding, self).__init__()\n\n        self.demb = demb\n\n        inv_freq = 1 / (10000 ** (torch.arange(0.0, demb, 2.0) / demb))\n        self.register_buffer(\'inv_freq\', inv_freq)\n\n    def forward(self, pos_seq, bsz=None):\n        sinusoid_inp = torch.ger(pos_seq, self.inv_freq)\n        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n\n        if bsz is not None:\n            return pos_emb[:,None,:].expand(-1, bsz, -1)\n        else:\n            return pos_emb[:,None,:]\n\n\nclass PositionwiseFF(nn.Module):\n    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False):\n        super(PositionwiseFF, self).__init__()\n\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n\n        self.CoreNet = nn.Sequential(\n            nn.Linear(d_model, d_inner), nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(d_inner, d_model),\n            nn.Dropout(dropout),\n        )\n\n        self.layer_norm = LayerNorm(d_model)\n\n        self.pre_lnorm = pre_lnorm\n\n    def forward(self, inp):\n        if self.pre_lnorm:\n            ##### layer normalization + positionwise feed-forward\n            core_out = self.CoreNet(self.layer_norm(inp))\n\n            ##### residual connection\n            output = core_out + inp\n        else:\n            ##### positionwise feed-forward\n            core_out = self.CoreNet(inp)\n\n            ##### residual connection + layer normalization\n            output = self.layer_norm(inp + core_out)\n\n        return output\n\nclass MultiHeadAttn(nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0, \n                 pre_lnorm=False, r_r_bias=None, r_w_bias=None):\n        super(MultiHeadAttn, self).__init__()\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.q_net = nn.Linear(d_model, n_head * d_head, bias=False)\n        self.kv_net = nn.Linear(d_model, 2 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = LayerNorm(d_model)\n\n        self.scale = 1 / (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n        if r_r_bias is None or r_w_bias is None: # Biases are not shared\n            self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n            self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n        else:\n            self.r_r_bias = r_r_bias\n            self.r_w_bias = r_w_bias\n\n    def forward(self, h, attn_mask=None, mems=None):\n        ##### multihead attention\n        # [hlen x bsz x n_head x d_head]\n\n        if mems is not None:\n            c = torch.cat([mems, h], 0)\n        else:\n            c = h\n\n        if self.pre_lnorm:\n            ##### layer normalization\n            c = self.layer_norm(c)\n\n        head_q = self.q_net(h)\n        head_k, head_v = torch.chunk(self.kv_net(c), 2, -1)\n\n        head_q = head_q.view(h.size(0), h.size(1), self.n_head, self.d_head)\n        head_k = head_k.view(c.size(0), c.size(1), self.n_head, self.d_head)\n        head_v = head_v.view(c.size(0), c.size(1), self.n_head, self.d_head)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = torch.einsum(\'ibnd,jbnd->ijbn\', (head_q, head_k))\n        attn_score.mul_(self.scale)\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score.masked_fill_(attn_mask[None,:,:,None], -float(\'inf\'))\n            elif attn_mask.dim() == 3:\n                attn_score.masked_fill_(attn_mask[:,:,:,None], -float(\'inf\'))\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        # [qlen x klen x bsz x n_head] + [klen x bsz x n_head x d_head] -> [qlen x bsz x n_head x d_head]\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', (attn_prob, head_v))\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = h + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(h + attn_out)\n\n        return output\n\nclass RelMultiHeadAttn(nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n                 tgt_len=None, ext_len=None, mem_len=None, pre_lnorm=False,\n                 r_r_bias=None, r_w_bias=None):\n        super(RelMultiHeadAttn, self).__init__()\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = LayerNorm(d_model)\n\n        self.scale = 1 / (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n        if r_r_bias is None or r_w_bias is None: # Biases are not shared\n            self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n            self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n        else:\n            self.r_r_bias = r_r_bias\n            self.r_w_bias = r_w_bias\n\n    def _parallelogram_mask(self, h, w, left=False):\n        mask = torch.ones((h, w)).byte()\n        m = min(h, w)\n        mask[:m,:m] = torch.triu(mask[:m,:m])\n        mask[-m:,-m:] = torch.tril(mask[-m:,-m:])\n\n        if left:\n            return mask\n        else:\n            return mask.flip(0)\n\n    def _shift(self, x, qlen, klen, mask, left=False):\n        if qlen > 1:\n            zero_pad = torch.zeros((x.size(0), qlen-1, x.size(2), x.size(3)),\n                                    device=x.device, dtype=x.dtype)\n        else:\n            zero_pad = torch.zeros(0, device=x.device, dtype=x.dtype)\n\n        if left:\n            mask = mask.flip(1)\n            x_padded = torch.cat([zero_pad, x], dim=1).expand(qlen, -1, -1, -1)\n        else:\n            x_padded = torch.cat([x, zero_pad], dim=1).expand(qlen, -1, -1, -1)\n\n        x = x_padded.masked_select(mask[:,:,None,None]) \\\n                    .view(qlen, klen, x.size(2), x.size(3))\n\n        return x\n\n    def _rel_shift(self, x, zero_triu=False):\n        zero_pad_shape = (x.size(0), 1) + x.size()[2:]\n        zero_pad = torch.zeros(zero_pad_shape, device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=1)\n\n        x_padded_shape = (x.size(1) + 1, x.size(0)) + x.size()[2:]\n        x_padded = x_padded.view(*x_padded_shape)\n\n        x = x_padded[1:].view_as(x)\n\n        if zero_triu:\n            ones = torch.ones((x.size(0), x.size(1)))\n            x = x * torch.tril(ones, x.size(1) - x.size(0))[:,:,None,None]\n\n        return x\n\n    def forward(self, w, r, attn_mask=None, mems=None):\n        raise NotImplementedError\n\nclass RelPartialLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelPartialLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n        self.r_net = nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)\n\n    def forward(self, w, r, attn_mask=None, mems=None):\n        qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n\n        r_head_k = r_head_k.view(rlen, self.n_head, self.d_head)                # qlen x n_head x d_head\n\n        #### compute attention score\n        rw_head_q = w_head_q + self.r_w_bias                                    # qlen x bsz x n_head x d_head\n        AC = torch.einsum(\'ibnd,jbnd->ijbn\', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n\n        rr_head_q = w_head_q + self.r_r_bias\n        BD = torch.einsum(\'ibnd,jnd->ijbn\', (rr_head_q, r_head_k))              # qlen x klen x bsz x n_head\n        BD = self._rel_shift(BD)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[None,:,:,None], -1e30).type_as(attn_score)\n            elif attn_mask.dim() == 3:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[:,:,:,None], -1e30).type_as(attn_score)\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output\n\nclass RelLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n    def forward(self, w, r_emb, r_w_bias, r_bias, attn_mask=None, mems=None):\n        # r_emb: [klen, n_head, d_head], used for term B\n        # r_w_bias: [n_head, d_head], used for term C\n        # r_bias: [klen, n_head], used for term D\n\n        qlen, bsz = w.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)\n\n        if klen > r_emb.size(0):\n            r_emb_pad = r_emb[0:1].expand(klen-r_emb.size(0), -1, -1)\n            r_emb = torch.cat([r_emb_pad, r_emb], 0)\n            r_bias_pad = r_bias[0:1].expand(klen-r_bias.size(0), -1)\n            r_bias = torch.cat([r_bias_pad, r_bias], 0)\n        else:\n            r_emb = r_emb[-klen:]\n            r_bias = r_bias[-klen:]\n\n        #### compute attention score\n        rw_head_q = w_head_q + r_w_bias[None]                                   # qlen x bsz x n_head x d_head\n\n        AC = torch.einsum(\'ibnd,jbnd->ijbn\', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n        B_ = torch.einsum(\'ibnd,jnd->ijbn\', (w_head_q, r_emb))                  # qlen x klen x bsz x n_head\n        D_ = r_bias[None, :, None]                                              # 1    x klen x 1   x n_head\n        BD = self._rel_shift(B_ + D_)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score.masked_fill_(attn_mask[None,:,:,None], -float(\'inf\'))\n            elif attn_mask.dim() == 3:\n                attn_score.masked_fill_(attn_mask[:,:,:,None], -float(\'inf\'))\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout, **kwargs):\n        super(DecoderLayer, self).__init__()\n\n        self.dec_attn = MultiHeadAttn(n_head, d_model, d_head, dropout, **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get(\'pre_lnorm\'))\n\n    def forward(self, dec_inp, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\nclass RelLearnableDecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelLearnableMultiHeadAttn(n_head, d_model, d_head, dropout,\n                                         **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get(\'pre_lnorm\'))\n\n    def forward(self, dec_inp, r_emb, r_w_bias, r_bias, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r_emb, r_w_bias, r_bias,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\nclass RelPartialLearnableDecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelPartialLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelPartialLearnableMultiHeadAttn(n_head, d_model,\n                            d_head, dropout, **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get(\'pre_lnorm\'))\n\n    def forward(self, dec_inp, r, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\n\nclass AdaptiveEmbedding(nn.Module):\n    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, \n                 sample_softmax=False):\n        super(AdaptiveEmbedding, self).__init__()\n\n        self.n_token = n_token\n        self.d_embed = d_embed\n\n        self.cutoffs = cutoffs + [n_token]\n        self.div_val = div_val\n        self.d_proj = d_proj\n\n        self.emb_scale = d_proj ** 0.5\n\n        self.cutoff_ends = [0] + self.cutoffs\n\n        self.emb_layers = nn.ModuleList()\n        self.emb_projs = nn.ParameterList()\n        if div_val == 1:\n            self.emb_layers.append(\n                nn.Embedding(n_token, d_embed, sparse=sample_softmax>0)\n            )\n            if d_proj != d_embed:\n                self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_embed)))\n        else:\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]\n                d_emb_i = d_embed // (div_val ** i)\n                self.emb_layers.append(nn.Embedding(r_idx-l_idx, d_emb_i))\n                self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_emb_i)))\n\n    def forward(self, inp):\n        if self.div_val == 1:\n            embed = self.emb_layers[0](inp)\n            if self.d_proj != self.d_embed:\n                embed  = F.linear(embed, self.emb_projs[0])\n        else:\n            param = next(self.parameters())\n            inp_flat = inp.view(-1)\n            emb_flat = torch.zeros([inp_flat.size(0), self.d_proj], \n                dtype=param.dtype, device=param.device)\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n\n                mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n\n                if indices_i.numel() == 0:\n                    continue\n\n                inp_i = inp_flat.index_select(0, indices_i) - l_idx\n                emb_i = self.emb_layers[i](inp_i)\n                emb_i = F.linear(emb_i, self.emb_projs[i])\n\n                emb_flat.index_copy_(0, indices_i, emb_i)\n\n            embed_shape = inp.size() + (self.d_proj,)\n            embed = emb_flat.view(embed_shape)\n\n        embed.mul_(self.emb_scale)\n\n        return embed\n\n\nclass TransfoXLPreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    def __init__(self, config, *inputs, **kwargs):\n        super(TransfoXLPreTrainedModel, self).__init__()\n        if not isinstance(config, TransfoXLConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `TransfoXLConfig`. ""\n                ""To create a model from a pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        self.config = config\n\n    def init_weight(self, weight):\n        if self.config.init == \'uniform\':\n            nn.init.uniform_(weight, -self.config.init_range, self.config.init_range)\n        elif self.config.init == \'normal\':\n            nn.init.normal_(weight, 0.0, self.config.init_std)\n\n    def init_bias(self, bias):\n        nn.init.constant_(bias, 0.0)\n\n    def init_weights(self, m):\n        """""" Initialize the weights.\n        """"""\n        classname = m.__class__.__name__\n        if classname.find(\'Linear\') != -1:\n            if hasattr(m, \'weight\') and m.weight is not None:\n                self.init_weight(m.weight)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                self.init_bias(m.bias)\n        elif classname.find(\'AdaptiveEmbedding\') != -1:\n            if hasattr(m, \'emb_projs\'):\n                for i in range(len(m.emb_projs)):\n                    if m.emb_projs[i] is not None:\n                        nn.init.normal_(m.emb_projs[i], 0.0, self.config.proj_init_std)\n        elif classname.find(\'Embedding\') != -1:\n            if hasattr(m, \'weight\'):\n                self.init_weight(m.weight)\n        elif classname.find(\'ProjectedAdaptiveLogSoftmax\') != -1:\n            if hasattr(m, \'cluster_weight\') and m.cluster_weight is not None:\n                self.init_weight(m.cluster_weight)\n            if hasattr(m, \'cluster_bias\') and m.cluster_bias is not None:\n                self.init_bias(m.cluster_bias)\n            if hasattr(m, \'out_projs\'):\n                for i in range(len(m.out_projs)):\n                    if m.out_projs[i] is not None:\n                        nn.init.normal_(m.out_projs[i], 0.0, self.config.proj_init_std)\n        elif classname.find(\'LayerNorm\') != -1:\n            if hasattr(m, \'weight\'):\n                nn.init.normal_(m.weight, 1.0, self.config.init_std)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                self.init_bias(m.bias)\n        elif classname.find(\'TransformerLM\') != -1:\n            if hasattr(m, \'r_emb\'):\n                self.init_weight(m.r_emb)\n            if hasattr(m, \'r_w_bias\'):\n                self.init_weight(m.r_w_bias)\n            if hasattr(m, \'r_r_bias\'):\n                self.init_weight(m.r_r_bias)\n            if hasattr(m, \'r_bias\'):\n                self.init_bias(m.r_bias)\n\n    def set_num_special_tokens(self, num_special_tokens):\n        pass\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None,\n                        from_tf=False, *inputs, **kwargs):\n        """"""\n        Instantiate a TransfoXLPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `transfo-xl`\n                - a path or url to a pretrained model archive containing:\n                    . `transfo_xl_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a TransfoXLModel instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n            config_file = PRETRAINED_CONFIG_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} and {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n                    pretrained_model_name_or_path,\n                    archive_file, config_file))\n            return None\n        if resolved_archive_file == archive_file and resolved_config_file == config_file:\n            logger.info(""loading weights file {}"".format(archive_file))\n            logger.info(""loading configuration file {}"".format(config_file))\n        else:\n            logger.info(""loading weights file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n            logger.info(""loading configuration file {} from cache at {}"".format(\n                config_file, resolved_config_file))\n        # Load config\n        config = TransfoXLConfig.from_json_file(resolved_config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            state_dict = torch.load(resolved_archive_file, map_location=\'cpu\')\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint\n            return load_tf_weights_in_transfo_xl(model, config, pretrained_model_name_or_path)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \'_metadata\', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=\'\'):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + \'.\')\n\n        start_prefix = \'\'\n        if not hasattr(model, \'transformer\') and any(s.startswith(\'transformer.\') for s in state_dict.keys()):\n            start_prefix = \'transformer.\'\n        load(model, prefix=start_prefix)\n\n        if len(missing_keys) > 0:\n            logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(""Weights from pretrained model not used in {}: {}"".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) > 0:\n            raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\n                               model.__class__.__name__, ""\\n\\t"".join(error_msgs)))\n        # Make sure we are still sharing the input and output embeddings\n        if hasattr(model, \'tie_weights\'):\n            model.tie_weights()\n        return model\n\n\nclass TransfoXLModel(TransfoXLPreTrainedModel):\n    """"""Transformer XL model (""Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"").\n\n    Transformer XL use a relative positioning (with sinusiodal patterns) and adaptive softmax inputs which means that:\n    - you don\'t need to specify positioning embeddings indices\n    - the tokens in the vocabulary have to be sorted to decreasing frequency.\n\n    Params:\n        config: a TransfoXLConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the token indices selected in the range [0, self.config.n_token[\n        `mems`: optional memomry of hidden states from previous forward passes\n            as a list (num layers) of hidden states at the entry of each layer\n            each hidden states has shape [self.config.mem_len, bsz, self.config.d_model]\n            Note that the first two dimensions are transposed in `mems` with regards to `input_ids` and `target`\n    Outputs:\n        A tuple of (last_hidden_state, new_mems)\n        `last_hidden_state`: the encoded-hidden-states at the top of the model\n            as a torch.FloatTensor of size [batch_size, sequence_length, self.config.d_model]\n        `new_mems`: list (num layers) of updated mem states at the entry of each layer\n            each mem state is a torch.FloatTensor of size [self.config.mem_len, batch_size, self.config.d_model]\n            Note that the first two dimensions are transposed in `mems` with regards to `input_ids` and `target`\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_ids_next = torch.LongTensor([[53, 21, 1], [64, 23, 100]])\n\n    config = TransfoXLConfig()\n\n    model = TransfoXLModel(config)\n    last_hidden_state, new_mems = model(input_ids)\n\n    # Another time on input_ids_next using the memory:\n    last_hidden_state, new_mems = model(input_ids_next, new_mems)\n    ```\n    """"""\n    def __init__(self, config):\n        super(TransfoXLModel, self).__init__(config)\n        self.n_token = config.n_token\n\n        self.d_embed = config.d_embed\n        self.d_model = config.d_model\n        self.n_head = config.n_head\n        self.d_head = config.d_head\n\n        self.word_emb = AdaptiveEmbedding(config.n_token, config.d_embed, config.d_model, config.cutoffs, \n                                          div_val=config.div_val)\n\n        self.drop = nn.Dropout(config.dropout)\n\n        self.n_layer = config.n_layer\n\n        self.tgt_len = config.tgt_len\n        self.mem_len = config.mem_len\n        self.ext_len = config.ext_len\n        self.max_klen = config.tgt_len + config.ext_len + config.mem_len\n\n        self.attn_type = config.attn_type\n\n        if not config.untie_r:\n            self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n            self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n\n        self.layers = nn.ModuleList()\n        if config.attn_type == 0: # the default attention\n            for i in range(config.n_layer):\n                self.layers.append(\n                    RelPartialLearnableDecoderLayer(\n                        config.n_head, config.d_model, config.d_head, config.d_inner, config.dropout,\n                        tgt_len=config.tgt_len, ext_len=config.ext_len, mem_len=config.mem_len,\n                        dropatt=config.dropatt, pre_lnorm=config.pre_lnorm,\n                        r_w_bias=None if config.untie_r else self.r_w_bias,\n                        r_r_bias=None if config.untie_r else self.r_r_bias)\n                )\n        elif config.attn_type == 1: # learnable embeddings\n            for i in range(config.n_layer):\n                self.layers.append(\n                    RelLearnableDecoderLayer(\n                        config.n_head, config.d_model, config.d_head, config.d_inner, config.dropout,\n                        tgt_len=config.tgt_len, ext_len=config.ext_len, mem_len=config.mem_len,\n                        dropatt=config.dropatt, pre_lnorm=config.pre_lnorm,\n                        r_w_bias=None if config.untie_r else self.r_w_bias,\n                        r_r_bias=None if config.untie_r else self.r_r_bias)\n                )\n        elif config.attn_type in [2, 3]: # absolute embeddings\n            for i in range(config.n_layer):\n                self.layers.append(\n                    DecoderLayer(\n                        config.n_head, config.d_model, config.d_head, config.d_inner, config.dropout,\n                        dropatt=config.dropatt, pre_lnorm=config.pre_lnorm,\n                        r_w_bias=None if config.untie_r else self.r_w_bias,\n                        r_r_bias=None if config.untie_r else self.r_r_bias)\n                )\n\n        self.same_length = config.same_length\n        self.clamp_len = config.clamp_len\n\n        if self.attn_type == 0: # default attention\n            self.pos_emb = PositionalEmbedding(self.d_model)\n        elif self.attn_type == 1: # learnable\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n            self.r_bias = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head))\n        elif self.attn_type == 2: # absolute standard\n            self.pos_emb = PositionalEmbedding(self.d_model)\n        elif self.attn_type == 3: # absolute deeper SA\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n        self.apply(self.init_weights)\n\n    def backward_compatible(self):\n        self.sample_softmax = -1\n\n\n    def reset_length(self, tgt_len, ext_len, mem_len):\n        self.tgt_len = tgt_len\n        self.mem_len = mem_len\n        self.ext_len = ext_len\n\n    def init_mems(self, data):\n        if self.mem_len > 0:\n            mems = []\n            param = next(self.parameters())\n            for i in range(self.n_layer):\n                empty = torch.zeros(self.mem_len, data.size(1), self.config.d_model,\n                                    dtype=param.dtype, device=param.device)\n                mems.append(empty)\n\n            return mems\n        else:\n            return None\n\n    def _update_mems(self, hids, mems, qlen, mlen):\n        # does not deal with None\n        if mems is None: return None\n\n        # mems is not None\n        assert len(hids) == len(mems), \'len(hids) != len(mems)\'\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        # For the next step, the last `ext_len` of the `qlen` tokens\n        # will be used as the extended context. Hence, we only cache\n        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n        # to `mlen + qlen - self.ext_len`.\n        with torch.no_grad():\n            new_mems = []\n            end_idx = mlen + max(0, qlen - 0 - self.ext_len)\n            beg_idx = max(0, end_idx - self.mem_len)\n            for i in range(len(hids)):\n\n                cat = torch.cat([mems[i], hids[i]], dim=0)\n                new_mems.append(cat[beg_idx:end_idx].detach())\n\n        return new_mems\n\n    def _forward(self, dec_inp, mems=None):\n        qlen, bsz = dec_inp.size()\n\n        word_emb = self.word_emb(dec_inp)\n\n        mlen = mems[0].size(0) if mems is not None else 0\n        klen = mlen + qlen\n        if self.same_length:\n            all_ones = word_emb.new_ones(qlen, klen)\n            mask_len = klen - self.mem_len\n            if mask_len > 0:\n                mask_shift_len = qlen - mask_len\n            else:\n                mask_shift_len = qlen\n            dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n                    + torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1\n        else:\n            dec_attn_mask = torch.triu(\n                word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None]\n\n        hids = []\n        if self.attn_type == 0: # default\n            pos_seq = torch.arange(klen-1, -1, -1.0, device=word_emb.device, \n                                   dtype=word_emb.dtype)\n            if self.clamp_len > 0:\n                pos_seq.clamp_(max=self.clamp_len)\n            pos_emb = self.pos_emb(pos_seq)\n\n            core_out = self.drop(word_emb)\n            pos_emb = self.drop(pos_emb)\n\n            for i, layer in enumerate(self.layers):\n                hids.append(core_out)\n                mems_i = None if mems is None else mems[i]\n                core_out = layer(core_out, pos_emb, dec_attn_mask=dec_attn_mask, mems=mems_i)\n        elif self.attn_type == 1: # learnable\n            core_out = self.drop(word_emb)\n            for i, layer in enumerate(self.layers):\n                hids.append(core_out)\n                if self.clamp_len > 0:\n                    r_emb = self.r_emb[i][-self.clamp_len :]\n                    r_bias = self.r_bias[i][-self.clamp_len :]\n                else:\n                    r_emb, r_bias = self.r_emb[i], self.r_bias[i]\n\n                mems_i = None if mems is None else mems[i]\n                core_out = layer(core_out, r_emb, self.r_w_bias[i],\n                        r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i)\n        elif self.attn_type == 2: # absolute\n            pos_seq = torch.arange(klen - 1, -1, -1.0, device=word_emb.device,\n                                   dtype=word_emb.dtype)\n            if self.clamp_len > 0:\n                pos_seq.clamp_(max=self.clamp_len)\n            pos_emb = self.pos_emb(pos_seq)\n\n            core_out = self.drop(word_emb + pos_emb[-qlen:])\n\n            for i, layer in enumerate(self.layers):\n                hids.append(core_out)\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and i == 0:\n                    mems_i += pos_emb[:mlen]\n                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n                                 mems=mems_i)\n        elif self.attn_type == 3:\n            core_out = self.drop(word_emb)\n\n            for i, layer in enumerate(self.layers):\n                hids.append(core_out)\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and mlen > 0:\n                    cur_emb = self.r_emb[i][:-qlen]\n                    cur_size = cur_emb.size(0)\n                    if cur_size < mlen:\n                        cur_emb_pad = cur_emb[0:1].expand(mlen-cur_size, -1, -1)\n                        cur_emb = torch.cat([cur_emb_pad, cur_emb], 0)\n                    else:\n                        cur_emb = cur_emb[-mlen:]\n                    mems_i += cur_emb.view(mlen, 1, -1)\n                core_out += self.r_emb[i][-qlen:].view(qlen, 1, -1)\n\n                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n                                 mems=mems_i)\n\n        core_out = self.drop(core_out)\n\n        new_mems = self._update_mems(hids, mems, mlen, qlen)\n\n        return core_out, new_mems\n\n    def forward(self, input_ids, mems=None):\n        """""" Params:\n                input_ids :: [bsz, len]\n                mems :: optional mems from previous forwar passes (or init_mems)\n                    list (num layers) of mem states at the entry of each layer\n                        shape :: [self.config.mem_len, bsz, self.config.d_model]\n                    Note that the first two dimensions are transposed in `mems` with regards to `input_ids` and `target`\n            Returns:\n                tuple (last_hidden, new_mems) where:\n                    new_mems: list (num layers) of mem states at the entry of each layer\n                        shape :: [self.config.mem_len, bsz, self.config.d_model]\n                    last_hidden: output of the last layer:\n                        shape :: [bsz, len, self.config.d_model]\n        """"""\n        # the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library\n        # so we transpose here from shape [bsz, len] to shape [len, bsz]\n        input_ids = input_ids.transpose(0, 1).contiguous()\n\n        if mems is None:\n            mems = self.init_mems(input_ids)\n        last_hidden, new_mems = self._forward(input_ids, mems=mems)\n\n        # We transpose back here to shape [bsz, len, hidden_dim]\n        last_hidden = last_hidden.transpose(0, 1).contiguous()\n        return (last_hidden, new_mems)\n\n\nclass TransfoXLLMHeadModel(TransfoXLPreTrainedModel):\n    """"""Transformer XL model (""Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"").\n\n    This model add an (adaptive) softmax head on top of the TransfoXLModel\n\n    Transformer XL use a relative positioning (with sinusiodal patterns) and adaptive softmax inputs which means that:\n    - you don\'t need to specify positioning embeddings indices\n    - the tokens in the vocabulary have to be sorted to decreasing frequency.\n\n    Call self.tie_weights() if you update/load the weights of the transformer to keep the weights tied.\n\n    Params:\n        config: a TransfoXLConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the token indices selected in the range [0, self.config.n_token[\n        `target`: an optional torch.LongTensor of shape [batch_size, sequence_length]\n            with the target token indices selected in the range [0, self.config.n_token[\n        `mems`: an optional memory of hidden states from previous forward passes\n            as a list (num layers) of hidden states at the entry of each layer\n            each hidden states has shape [self.config.mem_len, bsz, self.config.d_model]\n            Note that the first two dimensions are transposed in `mems` with regards to `input_ids` and `target`\n\n    Outputs:\n        A tuple of (last_hidden_state, new_mems)\n        `softmax_output`: output of the (adaptive) softmax:\n            if target is None:\n                Negative log likelihood of shape [batch_size, sequence_length] \n            else:\n                log probabilities of tokens, shape [batch_size, sequence_length, n_tokens]\n        `new_mems`: list (num layers) of updated mem states at the entry of each layer\n            each mem state is a torch.FloatTensor of size [self.config.mem_len, batch_size, self.config.d_model]\n            Note that the first two dimensions are transposed in `mems` with regards to `input_ids` and `target`\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_ids_next = torch.LongTensor([[53, 21, 1], [64, 23, 100]])\n\n    config = TransfoXLConfig()\n\n    model = TransfoXLModel(config)\n    last_hidden_state, new_mems = model(input_ids)\n\n    # Another time on input_ids_next using the memory:\n    last_hidden_state, new_mems = model(input_ids_next, mems=new_mems)\n    ```\n    """"""\n    def __init__(self, config):\n        super(TransfoXLLMHeadModel, self).__init__(config)\n        self.transformer = TransfoXLModel(config)\n        self.sample_softmax = config.sample_softmax\n        # use sampled softmax\n        if config.sample_softmax > 0:\n            self.out_layer = nn.Linear(config.d_model, config.n_token)\n            self.sampler = LogUniformSampler(config.n_token, config.sample_softmax)\n        # use adaptive softmax (including standard softmax)\n        else:\n            self.crit = ProjectedAdaptiveLogSoftmax(config.n_token, config.d_embed, config.d_model, \n                                                    config.cutoffs, div_val=config.div_val)\n        self.apply(self.init_weights)\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Run this to be sure output and input (adaptive) softmax weights are tied """"""\n        # sampled softmax\n        if self.sample_softmax > 0:\n            if self.config.tie_weight:\n                self.out_layer.weight = self.transformer.word_emb.weight\n        # adaptive softmax (including standard softmax)\n        else:\n            if self.config.tie_weight:\n                for i in range(len(self.crit.out_layers)):\n                    self.crit.out_layers[i].weight = self.transformer.word_emb.emb_layers[i].weight\n            if self.config.tie_projs:\n                for i, tie_proj in enumerate(self.config.tie_projs):\n                    if tie_proj and self.config.div_val == 1 and self.config.d_model != self.config.d_embed:\n                        self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[0]\n                    elif tie_proj and self.config.div_val != 1:\n                        self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[i]\n\n    def reset_length(self, tgt_len, ext_len, mem_len):\n        self.transformer.reset_length(tgt_len, ext_len, mem_len)\n\n    def init_mems(self, data):\n        return self.transformer.init_mems(data)\n\n    def forward(self, input_ids, target=None, mems=None):\n        """""" Params:\n                input_ids :: [bsz, len]\n                target :: [bsz, len]\n            Returns:\n                tuple(softmax_output, new_mems) where:\n                    new_mems: list (num layers) of hidden states at the entry of each layer\n                        shape :: [mem_len, bsz, self.config.d_model] :: Warning: shapes are transposed here w. regards to input_ids\n                    softmax_output: output of the (adaptive) softmax:\n                        if target is None:\n                            Negative log likelihood of shape :: [bsz, len] \n                        else:\n                            log probabilities of tokens, shape :: [bsz, len, n_tokens]\n        """"""\n        bsz = input_ids.size(0)\n        tgt_len = input_ids.size(1)\n\n        last_hidden, new_mems = self.transformer(input_ids, mems)\n\n        pred_hid = last_hidden[:, -tgt_len:]\n        if self.sample_softmax > 0 and self.training:\n            assert self.config.tie_weight\n            logit = sample_logits(self.transformer.word_emb, self.out_layer.bias, target, pred_hid, self.sampler)\n            softmax_output = -F.log_softmax(logit, -1)[:, :, 0]\n        else:\n            softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), target)\n            if target is None:\n                softmax_output = softmax_output.view(bsz, tgt_len, -1)\n            else:\n                softmax_output = softmax_output.view(bsz, tgt_len)\n\n        # We transpose back\n        return (softmax_output, new_mems)\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/modeling_transfo_xl_utilities.py,32,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Utilities for PyTorch Transformer XL model.\n    Directly adapted from https://github.com/kimiyoung/transformer-xl.\n""""""\n\nfrom collections import defaultdict\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# CUDA_MAJOR = int(torch.version.cuda.split(\'.\')[0])\n# CUDA_MINOR = int(torch.version.cuda.split(\'.\')[1])\n\nclass ProjectedAdaptiveLogSoftmax(nn.Module):\n    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1,\n                 keep_order=False):\n        super(ProjectedAdaptiveLogSoftmax, self).__init__()\n\n        self.n_token = n_token\n        self.d_embed = d_embed\n        self.d_proj = d_proj\n\n        self.cutoffs = cutoffs + [n_token]\n        self.cutoff_ends = [0] + self.cutoffs\n        self.div_val = div_val\n\n        self.shortlist_size = self.cutoffs[0]\n        self.n_clusters = len(self.cutoffs) - 1\n        self.head_size = self.shortlist_size + self.n_clusters\n\n        if self.n_clusters > 0:\n            self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))\n            self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))\n\n        self.out_layers = nn.ModuleList()\n        self.out_projs = nn.ParameterList()\n\n        if div_val == 1:\n            for i in range(len(self.cutoffs)):\n                if d_proj != d_embed:\n                    self.out_projs.append(\n                        nn.Parameter(torch.Tensor(d_proj, d_embed))\n                    )\n                else:\n                    self.out_projs.append(None)\n\n            self.out_layers.append(nn.Linear(d_embed, n_token))\n        else:\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]\n                d_emb_i = d_embed // (div_val ** i)\n\n                self.out_projs.append(\n                    nn.Parameter(torch.Tensor(d_proj, d_emb_i))\n                )\n\n                self.out_layers.append(nn.Linear(d_emb_i, r_idx-l_idx))\n\n        self.keep_order = keep_order\n\n    def _compute_logit(self, hidden, weight, bias, proj):\n        if proj is None:\n            logit = F.linear(hidden, weight, bias=bias)\n        else:\n            # if CUDA_MAJOR <= 9 and CUDA_MINOR <= 1:\n            proj_hid = F.linear(hidden, proj.t().contiguous())\n            logit = F.linear(proj_hid, weight, bias=bias)\n            # else:\n            #     logit = torch.einsum(\'bd,de,ev->bv\', (hidden, proj, weight.t()))\n            #     if bias is not None:\n            #         logit = logit + bias\n\n        return logit\n\n    def forward(self, hidden, target=None, keep_order=False):\n        \'\'\'\n            Params:\n                hidden :: [len*bsz x d_proj]\n                target :: [len*bsz]\n            Return:\n                if target is None:\n                    out :: [len*bsz] Negative log likelihood\n                else:\n                    out :: [len*bsz x n_tokens] log probabilities of tokens over the vocabulary\n            We could replace this implementation by the native PyTorch one\n            if their\'s had an option to set bias on all clusters in the native one.\n            here: https://github.com/pytorch/pytorch/blob/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da/torch/nn/modules/adaptive.py#L138\n        \'\'\'\n\n        if target is not None:\n            target = target.view(-1)\n            if hidden.size(0) != target.size(0):\n                raise RuntimeError(\'Input and target should have the same size \'\n                                \'in the batch dimension.\')\n\n        if self.n_clusters == 0:\n            logit = self._compute_logit(hidden, self.out_layers[0].weight,\n                                        self.out_layers[0].bias, self.out_projs[0])\n            if target is not None:\n                output = -F.log_softmax(logit, dim=-1) \\\n                        .gather(1, target.unsqueeze(1)).squeeze(1)\n            else:\n                output = F.log_softmax(logit, dim=-1)\n        else:\n            # construct weights and biases\n            weights, biases = [], []\n            for i in range(len(self.cutoffs)):\n                if self.div_val == 1:\n                    l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n                    weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                    bias_i = self.out_layers[0].bias[l_idx:r_idx]\n                else:\n                    weight_i = self.out_layers[i].weight\n                    bias_i = self.out_layers[i].bias\n\n                if i == 0:\n                    weight_i = torch.cat(\n                        [weight_i, self.cluster_weight], dim=0)\n                    bias_i = torch.cat(\n                        [bias_i, self.cluster_bias], dim=0)\n\n                weights.append(weight_i)\n                biases.append(bias_i)\n\n            head_weight, head_bias, head_proj = weights[0], biases[0], self.out_projs[0]\n\n            head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n            head_logprob = F.log_softmax(head_logit, dim=1)\n\n            if target is None:\n                out = hidden.new_empty((head_logit.size(0), self.n_token))\n            else:\n                out = torch.zeros_like(target, dtype=hidden.dtype, device=hidden.device)\n\n            offset = 0\n            cutoff_values = [0] + self.cutoffs\n            for i in range(len(cutoff_values) - 1):\n                l_idx, r_idx = cutoff_values[i], cutoff_values[i + 1]\n\n                if target is not None:\n                    mask_i = (target >= l_idx) & (target < r_idx)\n                    indices_i = mask_i.nonzero().squeeze()\n\n                    if indices_i.numel() == 0:\n                        continue\n\n                    target_i = target.index_select(0, indices_i) - l_idx\n                    head_logprob_i = head_logprob.index_select(0, indices_i)\n                    hidden_i = hidden.index_select(0, indices_i)\n                else:\n                    hidden_i = hidden\n\n                if i == 0:\n                    if target is not None:\n                        logprob_i = head_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                    else:\n                        out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n                else:\n                    weight_i, bias_i, proj_i = weights[i], biases[i], self.out_projs[i]\n\n                    tail_logit_i = self._compute_logit(hidden_i, weight_i, bias_i, proj_i)\n                    tail_logprob_i = F.log_softmax(tail_logit_i, dim=1)\n                    cluster_prob_idx = self.cutoffs[0] + i - 1  # No probability for the head cluster\n                    if target is not None:\n                        logprob_i = head_logprob_i[:, cluster_prob_idx] \\\n                                + tail_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                    else:\n                        logprob_i = head_logprob[:, cluster_prob_idx, None] + tail_logprob_i\n                        out[:, l_idx:r_idx] = logprob_i\n\n                if target is not None:\n                    if (hasattr(self, \'keep_order\') and self.keep_order) or keep_order:\n                        out.index_copy_(0, indices_i, -logprob_i)\n                    else:\n                        out[offset:offset+logprob_i.size(0)].copy_(-logprob_i)\n                    offset += logprob_i.size(0)\n\n        return out\n\n\n    def log_prob(self, hidden):\n        r"""""" Computes log probabilities for all :math:`n\\_classes`\n        From: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/adaptive.py\n        Args:\n            hidden (Tensor): a minibatch of examples\n        Returns:\n            log-probabilities of for each class :math:`c`\n            in range :math:`0 <= c <= n\\_classes`, where :math:`n\\_classes` is a\n            parameter passed to ``AdaptiveLogSoftmaxWithLoss`` constructor.\n        Shape:\n            - Input: :math:`(N, in\\_features)`\n            - Output: :math:`(N, n\\_classes)`\n        """"""\n        if self.n_clusters == 0:\n            logit = self._compute_logit(hidden, self.out_layers[0].weight,\n                                        self.out_layers[0].bias, self.out_projs[0])\n            return F.log_softmax(logit, dim=-1)\n        else:\n            # construct weights and biases\n            weights, biases = [], []\n            for i in range(len(self.cutoffs)):\n                if self.div_val == 1:\n                    l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n                    weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                    bias_i = self.out_layers[0].bias[l_idx:r_idx]\n                else:\n                    weight_i = self.out_layers[i].weight\n                    bias_i = self.out_layers[i].bias\n\n                if i == 0:\n                    weight_i = torch.cat(\n                        [weight_i, self.cluster_weight], dim=0)\n                    bias_i = torch.cat(\n                        [bias_i, self.cluster_bias], dim=0)\n\n                weights.append(weight_i)\n                biases.append(bias_i)\n\n            head_weight, head_bias, head_proj = weights[0], biases[0], self.out_projs[0]\n            head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n\n            out = hidden.new_empty((head_logit.size(0), self.n_token))\n            head_logprob = F.log_softmax(head_logit, dim=1)\n\n            cutoff_values = [0] + self.cutoffs\n            for i in range(len(cutoff_values) - 1):\n                start_idx, stop_idx = cutoff_values[i], cutoff_values[i + 1]\n\n                if i == 0:\n                    out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n                else:\n                    weight_i, bias_i, proj_i = weights[i], biases[i], self.out_projs[i]\n\n                    tail_logit_i = self._compute_logit(hidden, weight_i, bias_i, proj_i)\n                    tail_logprob_i = F.log_softmax(tail_logit_i, dim=1)\n\n                    logprob_i = head_logprob[:, -i] + tail_logprob_i\n                    out[:, start_idx, stop_idx] = logprob_i\n\n            return out\n\n\nclass LogUniformSampler(object):\n    def __init__(self, range_max, n_sample):\n        """"""\n        Reference : https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/candidate_sampling_ops.py\n            `P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)`\n\n        expected count can be approximated by 1 - (1 - p)^n\n        and we use a numerically stable version -expm1(num_tries * log1p(-p))\n\n        Our implementation fixes num_tries at 2 * n_sample, and the actual #samples will vary from run to run\n        """"""\n        with torch.no_grad():\n            self.range_max = range_max\n            log_indices = torch.arange(1., range_max+2., 1.).log_()\n            self.dist = (log_indices[1:] - log_indices[:-1]) / log_indices[-1]\n            # print(\'P\', self.dist.numpy().tolist()[-30:])\n\n            self.log_q = (- (-self.dist.double().log1p_() * 2 * n_sample).expm1_()).log_().float()\n\n        self.n_sample = n_sample\n\n    def sample(self, labels):\n        """"""\n            labels: [b1, b2]\n        Return\n            true_log_probs: [b1, b2]\n            samp_log_probs: [n_sample]\n            neg_samples: [n_sample]\n        """"""\n\n        # neg_samples = torch.empty(0).long()\n        n_sample = self.n_sample\n        n_tries = 2 * n_sample\n\n        with torch.no_grad():\n            neg_samples = torch.multinomial(self.dist, n_tries, replacement=True).unique()\n            device = labels.device\n            neg_samples = neg_samples.to(device)\n            true_log_probs = self.log_q[labels].to(device)\n            samp_log_probs = self.log_q[neg_samples].to(device)\n            return true_log_probs, samp_log_probs, neg_samples\n\ndef sample_logits(embedding, bias, labels, inputs, sampler):\n    """"""\n        embedding: an nn.Embedding layer\n        bias: [n_vocab]\n        labels: [b1, b2]\n        inputs: [b1, b2, n_emb]\n        sampler: you may use a LogUniformSampler\n    Return\n        logits: [b1, b2, 1 + n_sample]\n    """"""\n    true_log_probs, samp_log_probs, neg_samples = sampler.sample(labels)\n    n_sample = neg_samples.size(0)\n    b1, b2 = labels.size(0), labels.size(1)\n    all_ids = torch.cat([labels.view(-1), neg_samples])\n    all_w = embedding(all_ids)\n    true_w = all_w[: -n_sample].view(b1, b2, -1)\n    sample_w = all_w[- n_sample:].view(n_sample, -1)\n\n    all_b = bias[all_ids]\n    true_b = all_b[: -n_sample].view(b1, b2)\n    sample_b = all_b[- n_sample:]\n\n    hit = (labels[:, :, None] == neg_samples).detach()\n\n    true_logits = torch.einsum(\'ijk,ijk->ij\',\n        [true_w, inputs]) + true_b - true_log_probs\n    sample_logits = torch.einsum(\'lk,ijk->ijl\',\n        [sample_w, inputs]) + sample_b - samp_log_probs\n    sample_logits.masked_fill_(hit, -1e30)\n    logits = torch.cat([true_logits[:, :, None], sample_logits], -1)\n\n    return logits\n\n\n# class LogUniformSampler(object):\n#     def __init__(self, range_max, unique=False):\n#         """"""\n#         Reference : https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/candidate_sampling_ops.py\n#             `P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)`\n#         """"""\n#         self.range_max = range_max\n#         log_indices = torch.arange(1., range_max+2., 1.).log_()\n#         self.dist = (log_indices[1:] - log_indices[:-1]) / log_indices[-1]\n\n#         self.unique = unique\n\n#         if self.unique:\n#             self.exclude_mask = torch.ByteTensor(range_max).fill_(0)\n\n#     def sample(self, n_sample, labels):\n#         pos_sample, new_labels = labels.unique(return_inverse=True)\n#         n_pos_sample = pos_sample.size(0)\n#         n_neg_sample = n_sample - n_pos_sample\n\n#         if self.unique:\n#             self.exclude_mask.index_fill_(0, pos_sample, 1)\n#             sample_dist = self.dist.clone().masked_fill_(self.exclude_mask, 0)\n#             self.exclude_mask.index_fill_(0, pos_sample, 0)\n#         else:\n#             sample_dist = self.dist\n\n#         neg_sample = torch.multinomial(sample_dist, n_neg_sample)\n\n#         sample = torch.cat([pos_sample, neg_sample])\n#         sample_prob = self.dist[sample]\n\n#         return new_labels, sample, sample_prob\n\n\nif __name__ == \'__main__\':\n    S, B = 3, 4\n    n_vocab = 10000\n    n_sample = 5\n    H = 32\n\n    labels = torch.LongTensor(S, B).random_(0, n_vocab)\n\n    # sampler = LogUniformSampler(n_vocab, unique=False)\n    # new_labels, sample, sample_prob = sampler.sample(n_sample, labels)\n\n    sampler = LogUniformSampler(n_vocab, n_sample)#, unique=True)\n    # true_probs, samp_probs, neg_samples = sampler.sample(n_sample, labels)\n\n    # print(\'true_probs\', true_probs.numpy().tolist())\n    # print(\'samp_probs\', samp_probs.numpy().tolist())\n    # print(\'neg_samples\', neg_samples.numpy().tolist())\n\n    # print(\'sum\', torch.sum(sampler.dist).item())\n\n    # assert torch.all(torch.sort(sample.unique())[0].eq(torch.sort(sample)[0])).item()\n\n    embedding = nn.Embedding(n_vocab, H)\n    bias = torch.zeros(n_vocab)\n    inputs = torch.Tensor(S, B, H).normal_()\n\n    logits, out_labels = sample_logits(embedding, bias, labels, inputs, sampler, n_sample)\n    print(\'logits\', logits.detach().numpy().tolist())\n    print(\'logits shape\', logits.size())\n    print(\'out_labels\', out_labels.detach().numpy().tolist())\n    print(\'out_labels shape\', out_labels.size())\n\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/my_modeling.py,78,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch BERT model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom .file_utils import cached_path, WEIGHTS_NAME, CONFIG_NAME\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz"",\n}\nBERT_CONFIG_NAME = \'bert_config.json\'\nTF_WEIGHTS_NAME = \'model.ckpt\'\n\ndef load_tf_weights_in_bert(model, tf_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    print(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\'/\')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [""adam_v"", ""adam_m"", ""global_step""] for n in name):\n            print(""Skipping {}"".format(""/"".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'output_weights\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'squad\':\n                pointer = getattr(pointer, \'classifier\')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    print(""Skipping {}"".format(""/"".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \'_embeddings\':\n            pointer = getattr(pointer, \'weight\')\n        elif m_name == \'kernel\':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    """"""Implementation of the gelu activation function.\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish}\n\n\nclass BertConfig(object):\n    """"""Configuration class to store the configuration of a `BertModel`.\n    """"""\n    def __init__(self,\n                 vocab_size_or_config_json_file,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02,\n                 layer_norm_eps=1e-12):\n        """"""Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n            self.layer_norm_eps=layer_norm_eps\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             ""or the path to a pretrained model config file (str)"")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n        config = BertConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `BertConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n\ntry:\n    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\nexcept ImportError:\n    logger.info(""Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex ."")\n    class BertLayerNorm(nn.Module):\n        def __init__(self, hidden_size, eps=1e-12):\n            """"""Construct a layernorm module in the TF style (epsilon inside the square root).\n            """"""\n            super(BertLayerNorm, self).__init__()\n            self.weight = nn.Parameter(torch.ones(hidden_size))\n            self.bias = nn.Parameter(torch.zeros(hidden_size))\n            self.variance_epsilon = eps\n\n        def forward(self, x):\n            u = x.mean(-1, keepdim=True)\n            s = (x - u).pow(2).mean(-1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n            return self.weight * x + self.bias\n\nclass BertEmbeddings(nn.Module):\n    """"""Construct the embeddings from word, position and token_type embeddings.\n    """"""\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None):\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads))\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        self.output_score = config.output_score\n        self.output_sum   = config.output_sum\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n        attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        if self.output_score is True:\n            attention_probs_sum = attention_scores # (batch_size, h, l, l)\n        else:\n            attention_probs_sum = attention_probs # (batch_size, h, l, l)\n        if self.output_sum is True:\n            attention_probs_sum = attention_probs_sum.sum(dim=1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        return context_layer, attention_probs_sum\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n\n    def forward(self, input_tensor, attention_mask):\n        self_output, att_layer = self.self(input_tensor, attention_mask)\n        attention_output = self.output(self_output, input_tensor)\n        return attention_output, att_layer\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states, attention_mask):\n        attention_output, att_layer = self.attention(hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output, att_layer\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        layer = BertLayer(config)\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, output_all_attention_layers=False):\n        all_encoder_layers = [hidden_states]\n        all_attention_layers = [None]\n        for idx,layer_module in enumerate(self.layer):\n            hidden_states, att_layer = layer_module(hidden_states, attention_mask)\n            if output_all_encoded_layers:\n                all_encoder_layers.append(hidden_states)\n            if output_all_attention_layers:\n                all_attention_layers.append(att_layer)\n        if not output_all_encoded_layers:\n            all_encoder_layers.append(hidden_states)\n        return all_encoder_layers, all_attention_layers\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(BertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertLMPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n                                 bert_model_embedding_weights.size(0),\n                                 bias=False)\n        self.decoder.weight = bert_model_embedding_weights\n        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertOnlyMLMHead, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertPreTrainingHeads, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\n\nclass BertPreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    def __init__(self, config, *inputs, **kwargs):\n        super(BertPreTrainedModel, self).__init__()\n        if not isinstance(config, BertConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `BertConfig`. ""\n                ""To create a model from a Google pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        self.config = config\n\n    def init_bert_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n        """"""\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        """"""\n        state_dict = kwargs.get(\'state_dict\', None)\n        kwargs.pop(\'state_dict\', None)\n        cache_dir = kwargs.get(\'cache_dir\', None)\n        kwargs.pop(\'cache_dir\', None)\n        from_tf = kwargs.get(\'from_tf\', False)\n        kwargs.pop(\'from_tf\', None)\n\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                ""associated to this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n                    archive_file))\n            return None\n        if resolved_archive_file == archive_file:\n            logger.info(""loading archive file {}"".format(archive_file))\n        else:\n            logger.info(""loading archive file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n        tempdir = None\n        if os.path.isdir(resolved_archive_file) or from_tf:\n            serialization_dir = resolved_archive_file\n        else:\n            # Extract archive to temp dir\n            tempdir = tempfile.mkdtemp()\n            logger.info(""extracting archive file {} to temp dir {}"".format(\n                resolved_archive_file, tempdir))\n            with tarfile.open(resolved_archive_file, \'r:gz\') as archive:\n                archive.extractall(tempdir)\n            serialization_dir = tempdir\n        # Load config\n        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n        if not os.path.exists(config_file):\n            # Backward compatibility with old naming format\n            config_file = os.path.join(serialization_dir, BERT_CONFIG_NAME)\n        config = BertConfig.from_json_file(config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n            state_dict = torch.load(weights_path, map_location=\'cpu\')\n        if tempdir:\n            # Clean up temp dir\n            shutil.rmtree(tempdir)\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint\n            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n            return load_tf_weights_in_bert(model, weights_path)\n        # Load from a PyTorch state_dict\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if \'gamma\' in key:\n                new_key = key.replace(\'gamma\', \'weight\')\n            if \'beta\' in key:\n                new_key = key.replace(\'beta\', \'bias\')\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \'_metadata\', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=\'\'):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + \'.\')\n        start_prefix = \'\'\n        if not hasattr(model, \'bert\') and any(s.startswith(\'bert.\') for s in state_dict.keys()):\n            start_prefix = \'bert.\'\n        load(model, prefix=start_prefix)\n        if len(missing_keys) > 0:\n            logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(""Weights from pretrained model not used in {}: {}"".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) > 0:\n            raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\n                               model.__class__.__name__, ""\\n\\t"".join(error_msgs)))\n        return model\n\n\nclass BertModel(BertPreTrainedModel):\n    """"""BERT model (""Bidirectional Embedding Representations from a Transformer"").\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n\n    Outputs: Tuple of (encoded_layers, pooled_output)\n        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n            classifier pretrained on top of the hidden state associated to the first character of the\n            input (`CLS`) to train on the Next-Sentence task (see BERT\'s paper).\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = modeling.BertModel(config=config)\n    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, output_score=False, output_sum=False):\n        super(BertModel, self).__init__(config)\n        config.output_score = output_score\n        config.output_sum   = output_sum\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, output_all_attention_layers=False):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        embedding_output = self.embeddings(input_ids, token_type_ids)\n        encoded_layers,att_layers = self.encoder(embedding_output,\n                                      extended_attention_mask,\n                                      output_all_encoded_layers=output_all_encoded_layers,\n                                      output_all_attention_layers=output_all_attention_layers)\n        sequence_output = encoded_layers[-1]\n        pooled_output = self.pooler(sequence_output)\n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n        return encoded_layers, pooled_output, att_layers\n\n\nclass BertForPreTraining(BertPreTrainedModel):\n    """"""BERT model with pre-training heads.\n    This module comprises the BERT model followed by the two pre-training heads:\n        - the masked language modeling head, and\n        - the next sentence classification head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: optional masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n        `next_sentence_label`: optional next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n\n    Outputs:\n        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `masked_lm_labels` or `next_sentence_label` is `None`:\n            Outputs a tuple comprising\n            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n            - the next sentence classification logits of shape [batch_size, 2].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForPreTraining(config)\n    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForPreTraining, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                                   output_all_encoded_layers=False)\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            return total_loss\n        else:\n            return prediction_scores, seq_relationship_score\n\n\nclass BertForMaskedLM(BertPreTrainedModel):\n    """"""BERT model with the masked language modeling head.\n    This module comprises the BERT model followed by the masked language modeling head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n\n    Outputs:\n        if `masked_lm_labels` is  not `None`:\n            Outputs the masked language modeling loss.\n        if `masked_lm_labels` is `None`:\n            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForMaskedLM(config)\n    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForMaskedLM, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n                                       output_all_encoded_layers=False)\n        prediction_scores = self.cls(sequence_output)\n\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            return masked_lm_loss\n        else:\n            return prediction_scores\n\n\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    """"""BERT model with next sentence prediction head.\n    This module comprises the BERT model followed by the next sentence classification head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n\n    Outputs:\n        if `next_sentence_label` is not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `next_sentence_label` is `None`:\n            Outputs the next sentence classification logits of shape [batch_size, 2].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForNextSentencePrediction(config)\n    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForNextSentencePrediction, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                     output_all_encoded_layers=False)\n        seq_relationship_score = self.cls( pooled_output)\n\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            return next_sentence_loss\n        else:\n            return seq_relationship_score\n\n\nclass BertForSequenceClassification(BertPreTrainedModel):\n    """"""BERT model for classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary. Items in the batch should begin with the special ""CLS"" token. (see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_labels].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_labels = 2\n\n    model = BertForSequenceClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForMultipleChoice(BertPreTrainedModel):\n    """"""BERT model for multiple choice tasks.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_choices`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_choices = 2\n\n    model = BertForMultipleChoice(config, num_choices)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_choices):\n        super(BertForMultipleChoice, self).__init__(config)\n        self.num_choices = num_choices\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, self.num_choices)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            return loss\n        else:\n            return reshaped_logits\n\n\nclass BertForTokenClassification(BertPreTrainedModel):\n    """"""BERT model for token-level classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the full hidden state of the last layer.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [0, ..., num_labels].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_labels = 2\n\n    model = BertForTokenClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForTokenClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForQuestionAnswering(BertPreTrainedModel):\n    """"""BERT model for Question Answering (span extraction).\n    This module is composed of the BERT model with a linear layer on top of\n    the sequence output that computes start_logits and end_logits\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n            into account for computing the loss.\n        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n            into account for computing the loss.\n\n    Outputs:\n        if `start_positions` and `end_positions` are not `None`:\n            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n        if `start_positions` or `end_positions` is `None`:\n            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n            position tokens of shape [batch_size, sequence_length].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForQuestionAnswering(config)\n    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.bert = BertModel(config)\n        # TODO check with Google if it\'s normal there is no dropout on the token classifier of SQuAD in the TF version\n        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            return total_loss\n        else:\n            return start_logits, end_logits\n\nclass ALBertEncoder(nn.Module):\n    def __init__(self, config):\n        super(ALBertEncoder, self).__init__()\n        layer = BertLayer(config)\n        self.num_hidden_layers = config.num_hidden_layers\n        self.skip_connection = config.skip_connection\n        self.num_repetitions = config.num_repetitions\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(self.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, output_attention_layer=None):\n        all_encoder_layers = []\n        all_attention_probs_sum = []\n\n        for idx_outer, layer_module in enumerate(self.layer):\n            x0 = hidden_states\n            for idx_inner in range(self.num_repetitions):\n                idx = idx_inner + idx_outer * self.num_repetitions\n                if idx_inner==0 or self.skip_connection is False:\n                    hidden_states, attention_probs_sum = layer_module(hidden_states, attention_mask)\n                else:\n                    hidden_states, attention_probs_sum = layer_module(hidden_states + x0, attention_mask)\n                if output_attention_layer is not None and idx in output_attention_layer:\n                    all_attention_probs_sum.append(attention_probs_sum)\n                if output_all_encoded_layers:\n                    all_encoder_layers.append(hidden_states)\n        if not output_all_encoded_layers:\n            all_encoder_layers.append(hidden_states)\n        return  all_encoder_layers, all_attention_probs_sum\n\n\nclass ALBertModel(BertPreTrainedModel):\n\n    def __init__(self, config, output_score=False, output_sum=1, skip_connection=False, num_repetitions=1):\n        super(ALBertModel, self).__init__(config)\n        config.output_score = output_score\n        config.output_sum   = output_sum\n        config.skip_connection = skip_connection\n        config.num_repetitions = num_repetitions\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = ALBertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, output_attention_layer=None):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        embedding_output = self.embeddings(input_ids, token_type_ids)\n        encoded_layers,attention_probs_sum = self.encoder(embedding_output,\n                                      extended_attention_mask,\n                                      output_all_encoded_layers=output_all_encoded_layers,\n                                      output_attention_layer=output_attention_layer)\n        sequence_output = encoded_layers[-1]\n        pooled_output = self.pooler(sequence_output)\n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n        return encoded_layers, pooled_output, attention_probs_sum\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/optimization.py,5,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch optimization for BERT model.""""""\n\nimport math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.optimizer import required\nfrom torch.nn.utils import clip_grad_norm_\nimport logging\nimport abc\nimport sys\n\nlogger = logging.getLogger(__name__)\n\n\nif sys.version_info >= (3, 4):\n    ABC = abc.ABC\nelse:\n    ABC = abc.ABCMeta(\'ABC\', (), {})\n\n\nclass _LRSchedule(ABC):\n    """""" Parent of all LRSchedules here. """"""\n    warn_t_total = False        # is set to True for schedules where progressing beyond t_total steps doesn\'t make sense\n    def __init__(self, warmup=0.002, t_total=-1, **kw):\n        """"""\n        :param warmup:  what fraction of t_total steps will be used for linear warmup\n        :param t_total: how many training steps (updates) are planned\n        :param kw:\n        """"""\n        super(_LRSchedule, self).__init__(**kw)\n        if t_total < 0:\n            logger.warning(""t_total value of {} results in schedule not being applied"".format(t_total))\n        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n            raise ValueError(""Invalid warmup: {} - should be in [0.0, 1.0[ or -1"".format(warmup))\n        warmup = max(warmup, 0.)\n        self.warmup, self.t_total = float(warmup), float(t_total)\n        self.warned_for_t_total_at_progress = -1\n\n    def get_lr(self, step, nowarn=False):\n        """"""\n        :param step:    which of t_total steps we\'re on\n        :param nowarn:  set to True to suppress warning regarding training beyond specified \'t_total\' steps\n        :return:        learning rate multiplier for current update\n        """"""\n        if self.t_total < 0:\n            return 1.\n        progress = float(step) / self.t_total\n        ret = self.get_lr_(progress)\n        # warning for exceeding t_total (only active with warmup_linear\n        if not nowarn and self.warn_t_total and progress > 1. and progress > self.warned_for_t_total_at_progress:\n            logger.warning(\n                ""Training beyond specified \'t_total\'. Learning rate multiplier set to {}. Please set \'t_total\' of {} correctly.""\n                    .format(ret, self.__class__.__name__))\n            self.warned_for_t_total_at_progress = progress\n        # end warning\n        return ret\n\n    @abc.abstractmethod\n    def get_lr_(self, progress):\n        """"""\n        :param progress:    value between 0 and 1 (unless going beyond t_total steps) specifying training progress\n        :return:            learning rate multiplier for current update\n        """"""\n        return 1.\n\n\nclass ConstantLR(_LRSchedule):\n    def get_lr_(self, progress):\n        return 1.\n\n\nclass WarmupCosineSchedule(_LRSchedule):\n    """"""\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    Decreases learning rate from 1. to 0. over remaining `1 - warmup` steps following a cosine curve.\n    If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n    """"""\n    warn_t_total = True\n    def __init__(self, warmup=0.002, t_total=-1, cycles=.5, **kw):\n        """"""\n        :param warmup:      see LRSchedule\n        :param t_total:     see LRSchedule\n        :param cycles:      number of cycles. Default: 0.5, corresponding to cosine decay from 1. at progress==warmup and 0 at progress==1.\n        :param kw:\n        """"""\n        super(WarmupCosineSchedule, self).__init__(warmup=warmup, t_total=t_total, **kw)\n        self.cycles = cycles\n\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        else:\n            progress = (progress - self.warmup) / (1 - self.warmup)   # progress after warmup\n            return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))\n\n\nclass WarmupCosineWithHardRestartsSchedule(WarmupCosineSchedule):\n    """"""\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    If `cycles` (default=1.) is different from default, learning rate follows `cycles` times a cosine decaying\n    learning rate (with hard restarts).\n    """"""\n    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n        super(WarmupCosineWithHardRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n        assert(cycles >= 1.)\n\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        else:\n            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n            ret = 0.5 * (1. + math.cos(math.pi * ((self.cycles * progress) % 1)))\n            return ret\n\n\nclass WarmupCosineWithWarmupRestartsSchedule(WarmupCosineWithHardRestartsSchedule):\n    """"""\n    All training progress is divided in `cycles` (default=1.) parts of equal length.\n    Every part follows a schedule with the first `warmup` fraction of the training steps linearly increasing from 0. to 1.,\n    followed by a learning rate decreasing from 1. to 0. following a cosine curve.\n    """"""\n    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n        assert(warmup * cycles < 1.)\n        warmup = warmup * cycles if warmup >= 0 else warmup\n        super(WarmupCosineWithWarmupRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n\n    def get_lr_(self, progress):\n        progress = progress * self.cycles % 1.\n        if progress < self.warmup:\n            return progress / self.warmup\n        else:\n            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n            ret = 0.5 * (1. + math.cos(math.pi * progress))\n            return ret\n\n\nclass WarmupConstantSchedule(_LRSchedule):\n    """"""\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    Keeps learning rate equal to 1. after warmup.\n    """"""\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        return 1.\n\n\nclass WarmupLinearSchedule(_LRSchedule):\n    """"""\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    Linearly decreases learning rate from 1. to 0. over remaining `1 - warmup` steps.\n    """"""\n    warn_t_total = True\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        return max((progress - 1.) / (self.warmup - 1.), 0.)\n\n\nSCHEDULES = {\n    None:       ConstantLR,\n    ""none"":     ConstantLR,\n    ""warmup_cosine"": WarmupCosineSchedule,\n    ""warmup_constant"": WarmupConstantSchedule,\n    ""warmup_linear"": WarmupLinearSchedule\n}\n\n\nclass BertAdam(Optimizer):\n    """"""Implements BERT version of Adam algorithm with weight decay fix.\n    Params:\n        lr: learning rate\n        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n        t_total: total number of training steps for the learning\n            rate schedule, -1  means constant learning rate of 1. (no warmup regardless of warmup setting). Default: -1\n        schedule: schedule to use for the warmup (see above).\n            Can be `\'warmup_linear\'`, `\'warmup_constant\'`, `\'warmup_cosine\'`, `\'none\'`, `None` or a `_LRSchedule` object (see below).\n            If `None` or `\'none\'`, learning rate is always kept constant.\n            Default : `\'warmup_linear\'`\n        b1: Adams b1. Default: 0.9\n        b2: Adams b2. Default: 0.999\n        e: Adams epsilon. Default: 1e-6\n        weight_decay: Weight decay. Default: 0.01\n        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n    """"""\n    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule=\'warmup_linear\',\n                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01, max_grad_norm=1.0, **kwargs):\n        if lr is not required and lr < 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))\n        if not 0.0 <= b1 < 1.0:\n            raise ValueError(""Invalid b1 parameter: {} - should be in [0.0, 1.0["".format(b1))\n        if not 0.0 <= b2 < 1.0:\n            raise ValueError(""Invalid b2 parameter: {} - should be in [0.0, 1.0["".format(b2))\n        if not e >= 0.0:\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(e))\n        # initialize schedule object\n        if not isinstance(schedule, _LRSchedule):\n            schedule_type = SCHEDULES[schedule]\n            schedule = schedule_type(warmup=warmup, t_total=t_total)\n        else:\n            if warmup != -1 or t_total != -1:\n                logger.warning(""warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. ""\n                               ""Please specify custom warmup and t_total in _LRSchedule object."")\n        defaults = dict(lr=lr, schedule=schedule,\n                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm)\n        super(BertAdam, self).__init__(params, defaults)\n\n    def get_lr(self):\n        lr = []\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                if len(state) == 0:\n                    return [0]\n                lr_scheduled = group[\'lr\']\n                lr_scheduled *= group[\'schedule\'].get_lr(state[\'step\'])\n                lr.append(lr_scheduled)\n        return lr\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'next_m\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'next_v\'] = torch.zeros_like(p.data)\n\n                next_m, next_v = state[\'next_m\'], state[\'next_v\']\n                beta1, beta2 = group[\'b1\'], group[\'b2\']\n\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                next_m.mul_(beta1).add_(1 - beta1, grad)\n                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                update = next_m / (next_v.sqrt() + group[\'e\'])\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want to decay the weights in a manner that doesn\'t interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                if group[\'weight_decay\'] > 0.0:\n                    update += group[\'weight_decay\'] * p.data\n\n                lr_scheduled = group[\'lr\']\n                lr_scheduled *= group[\'schedule\'].get_lr(state[\'step\'])\n\n                update_with_lr = lr_scheduled * update\n                p.data.add_(-update_with_lr)\n\n                state[\'step\'] += 1\n\n                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n                # No bias correction\n                # bias_correction1 = 1 - beta1 ** state[\'step\']\n                # bias_correction2 = 1 - beta2 ** state[\'step\']\n\n        return loss\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/optimization_openai.py,5,"b'# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch optimization for OpenAI GPT model.""""""\n\nimport math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.optimizer import required\nfrom torch.nn.utils import clip_grad_norm_\nimport logging\nfrom .optimization import SCHEDULES, _LRSchedule, WarmupCosineWithWarmupRestartsSchedule, \\\n    WarmupCosineWithHardRestartsSchedule, WarmupCosineSchedule, WarmupLinearSchedule, WarmupConstantSchedule\n\nlogger = logging.getLogger(__name__)\n\n\nclass OpenAIAdam(Optimizer):\n    """"""Implements Open AI version of Adam algorithm with weight decay fix.\n    """"""\n    def __init__(self, params, lr=required, schedule=\'warmup_linear\', warmup=-1, t_total=-1,\n                 b1=0.9, b2=0.999, e=1e-8, weight_decay=0,\n                 vector_l2=False, max_grad_norm=-1, **kwargs):\n        if lr is not required and lr < 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))\n        if not 0.0 <= b1 < 1.0:\n            raise ValueError(""Invalid b1 parameter: {} - should be in [0.0, 1.0["".format(b1))\n        if not 0.0 <= b2 < 1.0:\n            raise ValueError(""Invalid b2 parameter: {} - should be in [0.0, 1.0["".format(b2))\n        if not e >= 0.0:\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(e))\n        # initialize schedule object\n        if not isinstance(schedule, _LRSchedule):\n            schedule_type = SCHEDULES[schedule]\n            schedule = schedule_type(warmup=warmup, t_total=t_total)\n        else:\n            if warmup != -1 or t_total != -1:\n                logger.warning(""warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. ""\n                               ""Please specify custom warmup and t_total in _LRSchedule object."")\n        defaults = dict(lr=lr, schedule=schedule,\n                        b1=b1, b2=b2, e=e, weight_decay=weight_decay, vector_l2=vector_l2,\n                        max_grad_norm=max_grad_norm)\n        super(OpenAIAdam, self).__init__(params, defaults)\n\n    def get_lr(self):\n        lr = []\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                if len(state) == 0:\n                    return [0]\n                lr_scheduled = group[\'lr\']\n                lr_scheduled *= group[\'schedule\'].get_lr(state[\'step\'])\n                lr.append(lr_scheduled)\n        return lr\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'b1\'], group[\'b2\']\n\n                state[\'step\'] += 1\n\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                denom = exp_avg_sq.sqrt().add_(group[\'e\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n\n                lr_scheduled = group[\'lr\']\n                lr_scheduled *= group[\'schedule\'].get_lr(state[\'step\'])\n\n                step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n                # Add weight decay at the end (fixed version)\n                if (len(p.size()) > 1 or group[\'vector_l2\']) and group[\'weight_decay\'] > 0:\n                    p.data.add_(-lr_scheduled * group[\'weight_decay\'], p.data)\n\n        return loss\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/tokenization.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport logging\nimport os\nimport unicodedata\nfrom io import open\n\nfrom .file_utils import cached_path\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt"",\n}\nPRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n    \'bert-base-uncased\': 512,\n    \'bert-large-uncased\': 512,\n    \'bert-base-cased\': 512,\n    \'bert-large-cased\': 512,\n    \'bert-base-multilingual-uncased\': 512,\n    \'bert-base-multilingual-cased\': 512,\n    \'bert-base-chinese\': 512,\n}\nVOCAB_NAME = \'vocab.txt\'\n\n\ndef load_vocab(vocab_file):\n    """"""Loads a vocabulary file into a dictionary.""""""\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, ""r"", encoding=""utf-8"") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass BertTokenizer(object):\n    """"""Runs end-to-end tokenization: punctuation splitting + wordpiece""""""\n\n    def __init__(self, vocab_file, do_lower_case=True, max_len=None, do_basic_tokenize=True,\n                 never_split=(""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]"")):\n        """"""Constructs a BertTokenizer.\n\n        Args:\n          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n          do_lower_case: Whether to lower case the input\n                         Only has an effect when do_wordpiece_only=False\n          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n          max_len: An artificial maximum length to truncate tokenized sequences to;\n                         Effective maximum length is always the minimum of this\n                         value (if specified) and the underlying BERT model\'s\n                         sequence length.\n          never_split: List of tokens which will never be split during tokenization.\n                         Only has an effect when do_wordpiece_only=False\n        """"""\n        if not os.path.isfile(vocab_file):\n            raise ValueError(\n                ""Can\'t find a vocabulary file at path \'{}\'. To load the vocabulary from a Google pretrained ""\n                ""model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(vocab_file))\n        self.vocab = load_vocab(vocab_file)\n        self.ids_to_tokens = collections.OrderedDict(\n            [(ids, tok) for tok, ids in self.vocab.items()])\n        self.do_basic_tokenize = do_basic_tokenize\n        if do_basic_tokenize:\n          self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n                                                never_split=never_split)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n        self.max_len = max_len if max_len is not None else int(1e12)\n\n    def tokenize(self, text):\n        split_tokens = []\n        if self.do_basic_tokenize:\n            for token in self.basic_tokenizer.tokenize(text):\n                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                    split_tokens.append(sub_token)\n        else:\n            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        """"""Converts a sequence of tokens into ids using the vocab.""""""\n        ids = []\n        for token in tokens:\n            ids.append(self.vocab[token])\n        if len(ids) > self.max_len:\n            logger.warning(\n                ""Token indices sequence length is longer than the specified maximum ""\n                "" sequence length for this BERT model ({} > {}). Running this""\n                "" sequence through BERT will result in indexing errors"".format(len(ids), self.max_len)\n            )\n        return ids\n\n    def convert_ids_to_tokens(self, ids):\n        """"""Converts a sequence of ids in wordpiece tokens using the vocab.""""""\n        tokens = []\n        for i in ids:\n            tokens.append(self.ids_to_tokens[i])\n        return tokens\n\n    def save_vocabulary(self, vocab_path):\n        """"""Save the tokenizer vocabulary to a directory or file.""""""\n        index = 0\n        if os.path.isdir(vocab_path):\n            vocab_file = os.path.join(vocab_path, VOCAB_NAME)\n        with open(vocab_file, ""w"", encoding=""utf-8"") as writer:\n            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: vocabulary indices are not consecutive.""\n                                   "" Please check that the vocabulary is not corrupted!"".format(vocab_file))\n                    index = token_index\n                writer.write(token + u\'\\n\')\n                index += 1\n        return vocab_file\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n            if \'-cased\' in pretrained_model_name_or_path and kwargs.get(\'do_lower_case\', True):\n                logger.warning(""The pre-trained model you are loading is a cased model but you have not set ""\n                               ""`do_lower_case` to False. We are setting `do_lower_case=False` for you but ""\n                               ""you may want to check this behavior."")\n                kwargs[\'do_lower_case\'] = False\n            elif \'-cased\' not in pretrained_model_name_or_path and not kwargs.get(\'do_lower_case\', True):\n                logger.warning(""The pre-trained model you are loading is an uncased model but you have set ""\n                               ""`do_lower_case` to False. We are setting `do_lower_case=True` for you ""\n                               ""but you may want to check this behavior."")\n                kwargs[\'do_lower_case\'] = True\n        else:\n            vocab_file = pretrained_model_name_or_path\n        if os.path.isdir(vocab_file):\n            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                ""associated to this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    vocab_file))\n            return None\n        if resolved_vocab_file == vocab_file:\n            logger.info(""loading vocabulary file {}"".format(vocab_file))\n        else:\n            logger.info(""loading vocabulary file {} from cache at {}"".format(\n                vocab_file, resolved_vocab_file))\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n            # if we\'re using a pretrained model, ensure the tokenizer wont index sequences longer\n            # than the number of positional embeddings\n            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name_or_path]\n            kwargs[\'max_len\'] = min(kwargs.get(\'max_len\', int(1e12)), max_len)\n        # Instantiate tokenizer.\n        tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)\n        return tokenizer\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self,\n                 do_lower_case=True,\n                 never_split=(""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]"")):\n        """"""Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        """"""\n        self.do_lower_case = do_lower_case\n        self.never_split = never_split\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text.""""""\n        text = self._clean_text(text)\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case and token not in self.never_split:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text):\n        """"""Splits punctuation on a piece of text.""""""\n        if text in self.never_split:\n            return [text]\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/tokenization_gpt2.py,0,"b'# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for OpenAI GPT.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport sys\nimport json\nimport logging\nimport os\nimport regex as re\nfrom io import open\n\ntry:\n    from functools import lru_cache\nexcept ImportError:\n    # Just a dummy decorator to get the checks to run on python2\n    # because honestly I don\'t want to support a byte-level unicode BPE tokenizer on python 2 right now.\n    def lru_cache():\n        return lambda func: func\n\nfrom .file_utils import cached_path\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\n    \'gpt2\': ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json"",\n}\nPRETRAINED_MERGES_ARCHIVE_MAP = {\n    \'gpt2\': ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"",\n}\nPRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n    \'gpt2\': 1024,\n}\nVOCAB_NAME = \'vocab.json\'\nMERGES_NAME = \'merges.txt\'\nSPECIAL_TOKENS_NAME = \'special_tokens.txt\'\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""\n    _chr = unichr if sys.version_info[0] == 2 else chr\n    bs = list(range(ord(""!""), ord(""~"")+1))+list(range(ord(""\xc2\xa1""), ord(""\xc2\xac"")+1))+list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [_chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass GPT2Tokenizer(object):\n    """"""\n    GPT-2 BPE tokenizer. Peculiarities:\n        - Byte-level BPE\n    """"""\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n            merges_file = PRETRAINED_MERGES_ARCHIVE_MAP[pretrained_model_name_or_path]\n            special_tokens_file = None\n        else:\n            vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)\n            merges_file = os.path.join(pretrained_model_name_or_path, MERGES_NAME)\n            special_tokens_file = os.path.join(pretrained_model_name_or_path, SPECIAL_TOKENS_NAME)\n            if not os.path.exists(special_tokens_file):\n                special_tokens_file = None\n            else:\n                logger.info(""loading special tokens file {}"".format(special_tokens_file))\n        # redirect to the cache, if necessary\n        try:\n            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n            resolved_merges_file = cached_path(merges_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} and {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    pretrained_model_name_or_path,\n                    vocab_file, merges_file))\n            return None\n        if resolved_vocab_file == vocab_file and resolved_merges_file == merges_file:\n            logger.info(""loading vocabulary file {}"".format(vocab_file))\n            logger.info(""loading merges file {}"".format(merges_file))\n        else:\n            logger.info(""loading vocabulary file {} from cache at {}"".format(\n                vocab_file, resolved_vocab_file))\n            logger.info(""loading merges file {} from cache at {}"".format(\n                merges_file, resolved_merges_file))\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n            # if we\'re using a pretrained model, ensure the tokenizer wont index sequences longer\n            # than the number of positional embeddings\n            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name_or_path]\n            kwargs[\'max_len\'] = min(kwargs.get(\'max_len\', int(1e12)), max_len)\n        # Instantiate tokenizer.\n        if special_tokens_file and \'special_tokens\' not in kwargs:\n            special_tokens = open(special_tokens_file, encoding=\'utf-8\').read().split(\'\\n\')[:-1]\n        else:\n            special_tokens = kwargs.pop(\'special_tokens\', [])\n        tokenizer = cls(resolved_vocab_file, resolved_merges_file, special_tokens=special_tokens, *inputs, **kwargs)\n        return tokenizer\n\n    def __init__(self, vocab_file, merges_file, errors=\'replace\', special_tokens=None, max_len=None):\n        self.max_len = max_len if max_len is not None else int(1e12)\n        self.encoder = json.load(open(vocab_file))\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        self.errors = errors # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n        bpe_data = open(merges_file, encoding=\'utf-8\').read().split(\'\\n\')[1:-1]\n        bpe_merges = [tuple(merge.split()) for merge in bpe_data]\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n        self.special_tokens = {}\n        self.special_tokens_decoder = {}\n        self.set_special_tokens(special_tokens)\n\n    def __len__(self):\n        return len(self.encoder) + len(self.special_tokens)\n\n    def set_special_tokens(self, special_tokens):\n        """""" Add a list of additional tokens to the encoder.\n            The additional tokens are indexed starting from the last index of the\n            current vocabulary in the order of the `special_tokens` list.\n        """"""\n        if not special_tokens:\n            self.special_tokens = {}\n            self.special_tokens_decoder = {}\n            return\n        self.special_tokens = dict((tok, len(self.encoder) + i) for i, tok in enumerate(special_tokens))\n        self.special_tokens_decoder = {v:k for k, v in self.special_tokens.items()}\n        logger.info(""Special tokens {}"".format(self.special_tokens))\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        self.cache[token] = word\n        return word\n\n    def tokenize(self, text):\n        """""" Tokenize a string. """"""\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = \'\'.join(self.byte_encoder[ord(b)] for b in token)\n            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\' \'))\n        return bpe_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        """""" Converts a sequence of tokens into ids using the vocab. """"""\n        ids = []\n        if isinstance(tokens, str) or (sys.version_info[0] == 2 and isinstance(tokens, unicode)):\n            if tokens in self.special_tokens:\n                return self.special_tokens[tokens]\n            else:\n                return self.encoder.get(tokens, 0)\n        for token in tokens:\n            if token in self.special_tokens:\n                ids.append(self.special_tokens[token])\n            else:\n                ids.append(self.encoder.get(token, 0))\n        if len(ids) > self.max_len:\n            logger.warning(\n                ""Token indices sequence length is longer than the specified maximum ""\n                "" sequence length for this OpenAI GPT model ({} > {}). Running this""\n                "" sequence through the model will result in indexing errors"".format(len(ids), self.max_len)\n            )\n        return ids\n\n    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n        """"""Converts a sequence of ids in BPE tokens using the vocab.""""""\n        tokens = []\n        for i in ids:\n            if i in self.special_tokens_decoder:\n                if not skip_special_tokens:\n                    tokens.append(self.special_tokens_decoder[i])\n            else:\n                tokens.append(self.decoder[i])\n        return tokens\n\n    def encode(self, text):\n        return self.convert_tokens_to_ids(self.tokenize(text))\n\n    def decode(self, tokens):\n        text = \'\'.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\'utf-8\', errors=self.errors)\n        return text\n\n    def save_vocabulary(self, vocab_path):\n        """"""Save the tokenizer vocabulary and merge files to a directory.""""""\n        if not os.path.isdir(vocab_path):\n            logger.error(""Vocabulary path ({}) should be a directory"".format(vocab_path))\n            return\n        vocab_file = os.path.join(vocab_path, VOCAB_NAME)\n        merge_file = os.path.join(vocab_path, MERGES_NAME)\n        special_tokens_file = os.path.join(vocab_path, SPECIAL_TOKENS_NAME)\n\n        with open(vocab_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.encoder, ensure_ascii=False))\n\n        index = 0\n        with open(merge_file, ""w"", encoding=""utf-8"") as writer:\n            writer.write(u\'#version: 0.2\\n\')\n            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: BPE merge indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(merge_file))\n                    index = token_index\n                writer.write(\' \'.join(bpe_tokens) + u\'\\n\')\n                index += 1\n\n        index = len(self.encoder)\n        with open(special_tokens_file, \'w\', encoding=\'utf-8\') as writer:\n            for token, token_index in sorted(self.special_tokens.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving special tokens vocabulary to {}: BPE indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(special_tokens_file))\n                    index = token_index\n                writer.write(token + u\'\\n\')\n                index += 1\n\n        return vocab_file, merge_file, special_tokens_file\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/tokenization_openai.py,0,"b'# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for OpenAI GPT.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport json\nimport logging\nimport os\nimport re\nimport sys\nfrom io import open\n\nfrom tqdm import tqdm\n\nfrom .file_utils import cached_path\nfrom .tokenization import BasicTokenizer\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\n    \'openai-gpt\': ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json"",\n}\nPRETRAINED_MERGES_ARCHIVE_MAP = {\n    \'openai-gpt\': ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt"",\n}\nPRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n    \'openai-gpt\': 512,\n}\nVOCAB_NAME = \'vocab.json\'\nMERGES_NAME = \'merges.txt\'\nSPECIAL_TOKENS_NAME = \'special_tokens.txt\'\n\ndef get_pairs(word):\n    """"""\n    Return set of symbol pairs in a word.\n    word is represented as tuple of symbols (symbols being variable-length strings)\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\ndef text_standardize(text):\n    """"""\n    fixes some issues the spacy tokenizer had on books corpus\n    also does some whitespace standardization\n    """"""\n    text = text.replace(\'\xe2\x80\x94\', \'-\')\n    text = text.replace(\'\xe2\x80\x93\', \'-\')\n    text = text.replace(\'\xe2\x80\x95\', \'-\')\n    text = text.replace(\'\xe2\x80\xa6\', \'...\')\n    text = text.replace(\'\xc2\xb4\', ""\'"")\n    text = re.sub(r\'\'\'(-+|~+|!+|""+|;+|\\?+|\\++|,+|\\)+|\\(+|\\\\+|\\/+|\\*+|\\[+|\\]+|}+|{+|\\|+|_+)\'\'\', r\' \\1 \', text)\n    text = re.sub(r\'\\s*\\n\\s*\', \' \\n \', text)\n    text = re.sub(r\'[^\\S\\n]+\', \' \', text)\n    return text.strip()\n\nclass OpenAIGPTTokenizer(object):\n    """"""\n    BPE tokenizer. Peculiarities:\n        - lower case all inputs\n        - uses SpaCy tokenizer and ftfy for pre-BPE tokenization if they are installed, fallback to BERT\'s BasicTokenizer if not.\n        - argument special_tokens and function set_special_tokens:\n            can be used to add additional symbols (ex: ""__classify__"") to a vocabulary.\n    """"""\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n            merges_file = PRETRAINED_MERGES_ARCHIVE_MAP[pretrained_model_name_or_path]\n            special_tokens_file = None\n        else:\n            vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)\n            merges_file = os.path.join(pretrained_model_name_or_path, MERGES_NAME)\n            special_tokens_file = os.path.join(pretrained_model_name_or_path, SPECIAL_TOKENS_NAME)\n            if not os.path.exists(special_tokens_file):\n                special_tokens_file = None\n            else:\n                logger.info(""loading special tokens file {}"".format(special_tokens_file))\n        # redirect to the cache, if necessary\n        try:\n            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n            resolved_merges_file = cached_path(merges_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} and {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    pretrained_model_name_or_path,\n                    vocab_file, merges_file))\n            return None\n        if resolved_vocab_file == vocab_file and resolved_merges_file == merges_file:\n            logger.info(""loading vocabulary file {}"".format(vocab_file))\n            logger.info(""loading merges file {}"".format(merges_file))\n        else:\n            logger.info(""loading vocabulary file {} from cache at {}"".format(\n                vocab_file, resolved_vocab_file))\n            logger.info(""loading merges file {} from cache at {}"".format(\n                merges_file, resolved_merges_file))\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n            # if we\'re using a pretrained model, ensure the tokenizer wont index sequences longer\n            # than the number of positional embeddings\n            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name_or_path]\n            kwargs[\'max_len\'] = min(kwargs.get(\'max_len\', int(1e12)), max_len)\n        # Instantiate tokenizer.\n        if special_tokens_file and \'special_tokens\' not in kwargs:\n            special_tokens = open(special_tokens_file, encoding=\'utf-8\').read().split(\'\\n\')[:-1]\n        else:\n            special_tokens = kwargs.pop(\'special_tokens\', [])\n        tokenizer = cls(resolved_vocab_file, resolved_merges_file, special_tokens=special_tokens, *inputs, **kwargs)\n        return tokenizer\n\n    def __init__(self, vocab_file, merges_file, special_tokens=None, max_len=None):\n        try:\n            import ftfy\n            import spacy\n            self.nlp = spacy.load(\'en\', disable=[\'parser\', \'tagger\', \'ner\', \'textcat\'])\n            self.fix_text = ftfy.fix_text\n        except ImportError:\n            logger.warning(""ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy."")\n            self.nlp = BasicTokenizer(do_lower_case=True,\n                                      never_split=special_tokens if special_tokens is not None else [])\n            self.fix_text = None\n\n        self.max_len = max_len if max_len is not None else int(1e12)\n        self.encoder = json.load(open(vocab_file, encoding=""utf-8""))\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        merges = open(merges_file, encoding=\'utf-8\').read().split(\'\\n\')[1:-1]\n        merges = [tuple(merge.split()) for merge in merges]\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {}\n        self.special_tokens = {}\n        self.special_tokens_decoder = {}\n        self.set_special_tokens(special_tokens)\n\n    def __len__(self):\n        return len(self.encoder) + len(self.special_tokens)\n\n    def set_special_tokens(self, special_tokens):\n        """""" Add a list of additional tokens to the encoder.\n            The additional tokens are indexed starting from the last index of the\n            current vocabulary in the order of the `special_tokens` list.\n        """"""\n        if not special_tokens:\n            self.special_tokens = {}\n            self.special_tokens_decoder = {}\n            return\n        self.special_tokens = dict((tok, len(self.encoder) + i) for i, tok in enumerate(special_tokens))\n        self.special_tokens_decoder = {v:k for k, v in self.special_tokens.items()}\n        if self.fix_text is None:\n            # Using BERT\'s BasicTokenizer: we can update the tokenizer\n            self.nlp.never_split = special_tokens\n        logger.info(""Special tokens {}"".format(self.special_tokens))\n\n    def bpe(self, token):\n        word = tuple(token[:-1]) + (token[-1] + \'</w>\',)\n        if token in self.cache:\n            return self.cache[token]\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+\'</w>\'\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        if word == \'\\n  </w>\':\n            word = \'\\n</w>\'\n        self.cache[token] = word\n        return word\n\n    def tokenize(self, text):\n        """""" Tokenize a string. """"""\n        split_tokens = []\n        if self.fix_text is None:\n            # Using BERT\'s BasicTokenizer\n            text = self.nlp.tokenize(text)\n            for token in text:\n                split_tokens.extend([t for t in self.bpe(token).split(\' \')])\n        else:\n            # Using SpaCy & ftfy (original tokenization process of OpenAI GPT)\n            text = self.nlp(text_standardize(self.fix_text(text)))\n            for token in text:\n                split_tokens.extend([t for t in self.bpe(token.text.lower()).split(\' \')])\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        """""" Converts a sequence of tokens into ids using the vocab. """"""\n        ids = []\n        if isinstance(tokens, str) or (sys.version_info[0] == 2 and isinstance(tokens, unicode)):\n            if tokens in self.special_tokens:\n                return self.special_tokens[tokens]\n            else:\n                return self.encoder.get(tokens, 0)\n        for token in tokens:\n            if token in self.special_tokens:\n                ids.append(self.special_tokens[token])\n            else:\n                ids.append(self.encoder.get(token, 0))\n        if len(ids) > self.max_len:\n            logger.warning(\n                ""Token indices sequence length is longer than the specified maximum ""\n                "" sequence length for this OpenAI GPT model ({} > {}). Running this""\n                "" sequence through the model will result in indexing errors"".format(len(ids), self.max_len)\n            )\n        return ids\n\n    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n        """"""Converts a sequence of ids in BPE tokens using the vocab.""""""\n        tokens = []\n        for i in ids:\n            if i in self.special_tokens_decoder:\n                if not skip_special_tokens:\n                    tokens.append(self.special_tokens_decoder[i])\n            else:\n                tokens.append(self.decoder[i])\n        return tokens\n\n    def encode(self, text):\n        return self.convert_tokens_to_ids(self.tokenize(text))\n\n    def decode(self, ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n        """"""Converts a sequence of ids in a string.""""""\n        tokens = self.convert_ids_to_tokens(ids, skip_special_tokens=skip_special_tokens)\n        out_string = \'\'.join(tokens).replace(\'</w>\', \' \').strip()\n        if clean_up_tokenization_spaces:\n            out_string = out_string.replace(\'<unk>\', \'\')\n            out_string = out_string.replace(\' .\', \'.\').replace(\' ?\', \'?\').replace(\' !\', \'!\').replace(\' ,\', \',\').replace(\' ,\', \',\'\n                    ).replace("" \' "", ""\'"").replace("" n\'t"", ""n\'t"").replace("" \'m"", ""\'m"").replace("" do not"", "" don\'t""\n                    ).replace("" \'s"", ""\'s"").replace("" \'ve"", ""\'ve"").replace("" \'re"", ""\'re"")\n        return out_string\n\n    def save_vocabulary(self, vocab_path):\n        """"""Save the tokenizer vocabulary and merge files to a directory.""""""\n        if not os.path.isdir(vocab_path):\n            logger.error(""Vocabulary path ({}) should be a directory"".format(vocab_path))\n            return\n        vocab_file = os.path.join(vocab_path, VOCAB_NAME)\n        merge_file = os.path.join(vocab_path, MERGES_NAME)\n        special_tokens_file = os.path.join(vocab_path, SPECIAL_TOKENS_NAME)\n\n        with open(vocab_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.encoder, ensure_ascii=False))\n\n        index = 0\n        with open(merge_file, ""w"", encoding=""utf-8"") as writer:\n            writer.write(u\'#version: 0.2\\n\')\n            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: BPE merge indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(merge_file))\n                    index = token_index\n                writer.write(\' \'.join(bpe_tokens) + u\'\\n\')\n                index += 1\n\n        index = len(self.encoder)\n        with open(special_tokens_file, \'w\', encoding=\'utf-8\') as writer:\n            for token, token_index in sorted(self.special_tokens.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving special tokens vocabulary to {}: BPE indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(special_tokens_file))\n                    index = token_index\n                writer.write(token + u\'\\n\')\n                index += 1\n\n        return vocab_file, merge_file, special_tokens_file\n'"
examples/cmrc2018_example/pytorch_pretrained_bert/tokenization_transfo_xl.py,13,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Tokenization classes for Transformer XL model.\n    Adapted from https://github.com/kimiyoung/transformer-xl.\n""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport glob\nimport logging\nimport os\nimport sys\nfrom collections import Counter, OrderedDict\nfrom io import open\nimport unicodedata\n\nimport torch\nimport numpy as np\n\nfrom .file_utils import cached_path\n\nif sys.version_info[0] == 2:\n    import cPickle as pickle\nelse:\n    import pickle\n\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\n    \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-vocab.bin"",\n}\nVOCAB_NAME = \'vocab.bin\'\n\nPRETRAINED_CORPUS_ARCHIVE_MAP = {\n    \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-corpus.bin"",\n}\nCORPUS_NAME = \'corpus.bin\'\n\nclass TransfoXLTokenizer(object):\n    """"""\n    Transformer-XL tokenizer adapted from Vocab class in https://github.com/kimiyoung/transformer-xl\n    """"""\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a TransfoXLTokenizer.\n        The TransfoXLTokenizer.\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            if os.path.isdir(pretrained_model_name_or_path):\n                vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)\n            else:\n                vocab_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    pretrained_model_name_or_path,\n                    vocab_file))\n            return None\n        if resolved_vocab_file == vocab_file:\n            logger.info(""loading vocabulary file {}"".format(vocab_file))\n        else:\n            logger.info(""loading vocabulary file {} from cache at {}"".format(\n                vocab_file, resolved_vocab_file))\n\n        # Instantiate tokenizer.\n        tokenizer = cls(*inputs, **kwargs)\n        vocab_dict = torch.load(resolved_vocab_file)\n        for key, value in vocab_dict.items():\n            tokenizer.__dict__[key] = value\n        return tokenizer\n\n    def __init__(self, special=[], min_freq=0, max_size=None, lower_case=False,\n                 delimiter=None, vocab_file=None, never_split=(""<unk>"", ""<eos>"", ""<formula>"")):\n        self.counter = Counter()\n        self.special = special\n        self.min_freq = min_freq\n        self.max_size = max_size\n        self.lower_case = lower_case\n        self.delimiter = delimiter\n        self.vocab_file = vocab_file\n        self.never_split = never_split\n\n    def count_file(self, path, verbose=False, add_eos=False):\n        if verbose: print(\'counting file {} ...\'.format(path))\n        assert os.path.exists(path)\n\n        sents = []\n        with open(path, \'r\', encoding=\'utf-8\') as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                    print(\'    line {}\'.format(idx))\n                symbols = self.tokenize(line, add_eos=add_eos)\n                self.counter.update(symbols)\n                sents.append(symbols)\n\n        return sents\n\n    def count_sents(self, sents, verbose=False):\n        """"""\n            sents : a list of sentences, each a list of tokenized symbols\n        """"""\n        if verbose: print(\'counting {} sents ...\'.format(len(sents)))\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                print(\'    line {}\'.format(idx))\n            self.counter.update(symbols)\n\n    def _build_from_file(self, vocab_file):\n        self.idx2sym = []\n        self.sym2idx = OrderedDict()\n\n        with open(vocab_file, \'r\', encoding=\'utf-8\') as f:\n            for line in f:\n                symb = line.strip().split()[0]\n                self.add_symbol(symb)\n        if \'<UNK>\' in self.sym2idx:\n            self.unk_idx = self.sym2idx[\'<UNK>\']\n        elif \'<unk>\' in self.sym2idx:\n            self.unk_idx = self.sym2idx[\'<unk>\']\n        else:\n            raise ValueError(\'No <unkown> token in vocabulary\')\n\n    def save_vocabulary(self, vocab_path):\n        """"""Save the tokenizer vocabulary to a directory or file.""""""\n        index = 0\n        if os.path.isdir(vocab_path):\n            vocab_file = os.path.join(vocab_path, VOCAB_NAME)\n        torch.save(self.__dict__, vocab_file)\n        return vocab_file\n\n    def build_vocab(self):\n        if self.vocab_file:\n            print(\'building vocab from {}\'.format(self.vocab_file))\n            self._build_from_file(self.vocab_file)\n            print(\'final vocab size {}\'.format(len(self)))\n        else:\n            print(\'building vocab with min_freq={}, max_size={}\'.format(\n                self.min_freq, self.max_size))\n            self.idx2sym = []\n            self.sym2idx = OrderedDict()\n\n            for sym in self.special:\n                self.add_special(sym)\n\n            for sym, cnt in self.counter.most_common(self.max_size):\n                if cnt < self.min_freq: break\n                self.add_symbol(sym)\n\n            print(\'final vocab size {} from {} unique tokens\'.format(\n                len(self), len(self.counter)))\n\n    def encode_file(self, path, ordered=False, verbose=False, add_eos=True,\n            add_double_eos=False):\n        if verbose: print(\'encoding file {} ...\'.format(path))\n        assert os.path.exists(path)\n        encoded = []\n        with open(path, \'r\', encoding=\'utf-8\') as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                    print(\'    line {}\'.format(idx))\n                symbols = self.tokenize(line, add_eos=add_eos,\n                    add_double_eos=add_double_eos)\n                encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    def encode_sents(self, sents, ordered=False, verbose=False):\n        if verbose: print(\'encoding {} sents ...\'.format(len(sents)))\n        encoded = []\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                print(\'    line {}\'.format(idx))\n            encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    def add_special(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n            setattr(self, \'{}_idx\'.format(sym.strip(\'<>\')), self.sym2idx[sym])\n\n    def add_symbol(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n\n    def get_sym(self, idx):\n        assert 0 <= idx < len(self), \'Index {} out of vocabulary range\'.format(idx)\n        return self.idx2sym[idx]\n\n    def get_idx(self, sym):\n        if sym in self.sym2idx:\n            return self.sym2idx[sym]\n        else:\n            # print(\'encounter unk {}\'.format(sym))\n            # assert \'<eos>\' not in sym\n            if hasattr(self, \'unk_idx\'):\n                return self.sym2idx.get(sym, self.unk_idx)\n            # Backward compatibility with pre-trained models\n            elif \'<unk>\' in self.sym2idx:\n                return self.sym2idx[\'<unk>\']\n            elif \'<UNK>\' in self.sym2idx:\n                return self.sym2idx[\'<UNK>\']\n            else:\n                raise ValueError(\'Token not in vocabulary and no <unk> token in vocabulary for replacement\')\n\n    def convert_ids_to_tokens(self, indices):\n        """"""Converts a sequence of indices in symbols using the vocab.""""""\n        return [self.get_sym(idx) for idx in indices]\n\n    def convert_tokens_to_ids(self, symbols):\n        """"""Converts a sequence of symbols into ids using the vocab.""""""\n        return [self.get_idx(sym) for sym in symbols]\n\n    def convert_to_tensor(self, symbols):\n        return torch.LongTensor(self.convert_tokens_to_ids(symbols))\n\n    def decode(self, indices, exclude=None):\n        """"""Converts a sequence of indices in a string.""""""\n        if exclude is None:\n            return \' \'.join([self.get_sym(idx) for idx in indices])\n        else:\n            return \' \'.join([self.get_sym(idx) for idx in indices if idx not in exclude])\n\n    def __len__(self):\n        return len(self.idx2sym)\n\n    def tokenize(self, line, add_eos=False, add_double_eos=False):\n        line = line.strip()\n        # convert to lower case\n        if self.lower_case:\n            line = line.lower()\n\n        # empty delimiter \'\' will evaluate False\n        if self.delimiter == \'\':\n            symbols = line\n        else:\n            symbols = line.split(self.delimiter)\n\n        if add_double_eos: # lm1b\n            return [\'<S>\'] + symbols + [\'<S>\']\n        elif add_eos:\n            return symbols + [\'<eos>\']\n        else:\n            return symbols\n\n\nclass LMOrderedIterator(object):\n    def __init__(self, data, bsz, bptt, device=\'cpu\', ext_len=None):\n        """"""\n            data -- LongTensor -- the LongTensor is strictly ordered\n        """"""\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n\n        # Work out how cleanly we can divide the dataset into bsz parts.\n        self.n_step = data.size(0) // bsz\n\n        # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n        data = data.narrow(0, 0, self.n_step * bsz)\n\n        # Evenly divide the data across the bsz batches.\n        self.data = data.view(bsz, -1).t().contiguous().to(device)\n\n        # Number of mini-batches\n        self.n_batch = (self.n_step + self.bptt - 1) // self.bptt\n\n    def get_batch(self, i, bptt=None):\n        if bptt is None: bptt = self.bptt\n        seq_len = min(bptt, self.data.size(0) - 1 - i)\n\n        end_idx = i + seq_len\n        beg_idx = max(0, i - self.ext_len)\n\n        data = self.data[beg_idx:end_idx]\n        target = self.data[i+1:i+1+seq_len]\n\n        data_out = data.transpose(0, 1).contiguous().to(self.device)\n        target_out = target.transpose(0, 1).contiguous().to(self.device)\n\n        return data_out, target_out, seq_len\n\n    def get_fixlen_iter(self, start=0):\n        for i in range(start, self.data.size(0) - 1, self.bptt):\n            yield self.get_batch(i)\n\n    def get_varlen_iter(self, start=0, std=5, min_len=5, max_deviation=3):\n        max_len = self.bptt + max_deviation * std\n        i = start\n        while True:\n            bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.\n            bptt = min(max_len, max(min_len, int(np.random.normal(bptt, std))))\n            data, target, seq_len = self.get_batch(i, bptt)\n            i += seq_len\n            yield data, target, seq_len\n            if i >= self.data.size(0) - 2:\n                break\n\n    def __iter__(self):\n        return self.get_fixlen_iter()\n\n\nclass LMShuffledIterator(object):\n    def __init__(self, data, bsz, bptt, device=\'cpu\', ext_len=None, shuffle=False):\n        """"""\n            data -- list[LongTensor] -- there is no order among the LongTensors\n        """"""\n        self.data = data\n\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n        self.shuffle = shuffle\n\n    def get_sent_stream(self):\n        # index iterator\n        epoch_indices = np.random.permutation(len(self.data)) if self.shuffle \\\n            else np.array(range(len(self.data)))\n\n        # sentence iterator\n        for idx in epoch_indices:\n            yield self.data[idx]\n\n    def stream_iterator(self, sent_stream):\n        # streams for each data in the batch\n        streams = [None] * self.bsz\n\n        data = torch.LongTensor(self.bptt, self.bsz)\n        target = torch.LongTensor(self.bptt, self.bsz)\n\n        n_retain = 0\n\n        while True:\n            # data   : [n_retain+bptt x bsz]\n            # target : [bptt x bsz]\n            data[n_retain:].fill_(-1)\n            target.fill_(-1)\n\n            valid_batch = True\n\n            for i in range(self.bsz):\n                n_filled = 0\n                try:\n                    while n_filled < self.bptt:\n                        if streams[i] is None or len(streams[i]) <= 1:\n                            streams[i] = next(sent_stream)\n                        # number of new tokens to fill in\n                        n_new = min(len(streams[i]) - 1, self.bptt - n_filled)\n                        # first n_retain tokens are retained from last batch\n                        data[n_retain+n_filled:n_retain+n_filled+n_new, i] = \\\n                            streams[i][:n_new]\n                        target[n_filled:n_filled+n_new, i] = \\\n                            streams[i][1:n_new+1]\n                        streams[i] = streams[i][n_new:]\n                        n_filled += n_new\n                except StopIteration:\n                    valid_batch = False\n                    break\n\n            if not valid_batch:\n                return\n\n            data_out = data.transpose(0, 1).contiguous().to(self.device)\n            target_out = target.transpose(0, 1).contiguous().to(self.device)\n\n            yield data_out, target_out, self.bptt\n\n            n_retain = min(data.size(0), self.ext_len)\n            if n_retain > 0:\n                data[:n_retain] = data[-n_retain:]\n            data.resize_(n_retain + self.bptt, data.size(1))\n\n    def __iter__(self):\n        # sent_stream is an iterator\n        sent_stream = self.get_sent_stream()\n\n        for batch in self.stream_iterator(sent_stream):\n            yield batch\n\n\nclass LMMultiFileIterator(LMShuffledIterator):\n    def __init__(self, paths, vocab, bsz, bptt, device=\'cpu\', ext_len=None,\n        shuffle=False):\n\n        self.paths = paths\n        self.vocab = vocab\n\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n        self.shuffle = shuffle\n\n    def get_sent_stream(self, path):\n        sents = self.vocab.encode_file(path, add_double_eos=True)\n        if self.shuffle:\n            np.random.shuffle(sents)\n        sent_stream = iter(sents)\n\n        return sent_stream\n\n    def __iter__(self):\n        if self.shuffle:\n            np.random.shuffle(self.paths)\n\n        for path in self.paths:\n            # sent_stream is an iterator\n            sent_stream = self.get_sent_stream(path)\n            for batch in self.stream_iterator(sent_stream):\n                yield batch\n\n\nclass TransfoXLCorpus(object):\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a pre-processed corpus.\n        """"""\n        vocab = TransfoXLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        if pretrained_model_name_or_path in PRETRAINED_CORPUS_ARCHIVE_MAP:\n            corpus_file = PRETRAINED_CORPUS_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            corpus_file = os.path.join(pretrained_model_name_or_path, CORPUS_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_corpus_file = cached_path(corpus_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Corpus \'{}\' was not found in corpus list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    pretrained_model_name_or_path,\n                    corpus_file))\n            return None\n        if resolved_corpus_file == corpus_file:\n            logger.info(""loading corpus file {}"".format(corpus_file))\n        else:\n            logger.info(""loading corpus file {} from cache at {}"".format(\n                corpus_file, resolved_corpus_file))\n\n        # Instantiate tokenizer.\n        corpus = cls(*inputs, **kwargs)\n        corpus_dict = torch.load(resolved_corpus_file)\n        for key, value in corpus_dict.items():\n            corpus.__dict__[key] = value\n        corpus.vocab = vocab\n        if corpus.train is not None:\n            corpus.train = torch.tensor(corpus.train, dtype=torch.long)\n        if corpus.valid is not None:\n            corpus.valid = torch.tensor(corpus.valid, dtype=torch.long)\n        if corpus.test is not None:\n            corpus.test = torch.tensor(corpus.test, dtype=torch.long)\n        return corpus\n\n    def __init__(self, *args, **kwargs):\n        self.vocab = TransfoXLTokenizer(*args, **kwargs)\n        self.dataset = None\n        self.train = None\n        self.valid = None\n        self.test = None\n\n    def build_corpus(self, path, dataset):\n        self.dataset = dataset\n\n        if self.dataset in [\'ptb\', \'wt2\', \'enwik8\', \'text8\']:\n            self.vocab.count_file(os.path.join(path, \'train.txt\'))\n            self.vocab.count_file(os.path.join(path, \'valid.txt\'))\n            self.vocab.count_file(os.path.join(path, \'test.txt\'))\n        elif self.dataset == \'wt103\':\n            self.vocab.count_file(os.path.join(path, \'train.txt\'))\n        elif self.dataset == \'lm1b\':\n            train_path_pattern = os.path.join(\n                path, \'1-billion-word-language-modeling-benchmark-r13output\',\n                \'training-monolingual.tokenized.shuffled\', \'news.en-*\')\n            train_paths = glob.glob(train_path_pattern)\n            # the vocab will load from file when build_vocab() is called\n\n        self.vocab.build_vocab()\n\n        if self.dataset in [\'ptb\', \'wt2\', \'wt103\']:\n            self.train = self.vocab.encode_file(\n                os.path.join(path, \'train.txt\'), ordered=True)\n            self.valid = self.vocab.encode_file(\n                os.path.join(path, \'valid.txt\'), ordered=True)\n            self.test = self.vocab.encode_file(\n                os.path.join(path, \'test.txt\'), ordered=True)\n        elif self.dataset in [\'enwik8\', \'text8\']:\n            self.train = self.vocab.encode_file(\n                os.path.join(path, \'train.txt\'), ordered=True, add_eos=False)\n            self.valid = self.vocab.encode_file(\n                os.path.join(path, \'valid.txt\'), ordered=True, add_eos=False)\n            self.test = self.vocab.encode_file(\n                os.path.join(path, \'test.txt\'), ordered=True, add_eos=False)\n        elif self.dataset == \'lm1b\':\n            self.train = train_paths\n            self.valid = self.vocab.encode_file(\n                os.path.join(path, \'valid.txt\'), ordered=False, add_double_eos=True)\n            self.test = self.vocab.encode_file(\n                os.path.join(path, \'test.txt\'), ordered=False, add_double_eos=True)\n\n    def get_iterator(self, split, *args, **kwargs):\n        if split == \'train\':\n            if self.dataset in [\'ptb\', \'wt2\', \'wt103\', \'enwik8\', \'text8\']:\n                data_iter = LMOrderedIterator(self.train, *args, **kwargs)\n            elif self.dataset == \'lm1b\':\n                kwargs[\'shuffle\'] = True\n                data_iter = LMMultiFileIterator(self.train, self.vocab, *args, **kwargs)\n        elif split in [\'valid\', \'test\']:\n            data = self.valid if split == \'valid\' else self.test\n            if self.dataset in [\'ptb\', \'wt2\', \'wt103\', \'enwik8\', \'text8\']:\n                data_iter = LMOrderedIterator(data, *args, **kwargs)\n            elif self.dataset == \'lm1b\':\n                data_iter = LMShuffledIterator(data, *args, **kwargs)\n\n        return data_iter\n\n\ndef get_lm_corpus(datadir, dataset):\n    fn = os.path.join(datadir, \'cache.pt\')\n    fn_pickle = os.path.join(datadir, \'cache.pkl\')\n    if os.path.exists(fn):\n        print(\'Loading cached dataset...\')\n        corpus = torch.load(fn_pickle)\n    elif os.path.exists(fn):\n        print(\'Loading cached dataset from pickle...\')\n        with open(fn, ""rb"") as fp:\n            corpus = pickle.load(fp)\n    else:\n        print(\'Producing dataset {}...\'.format(dataset))\n        kwargs = {}\n        if dataset in [\'wt103\', \'wt2\']:\n            kwargs[\'special\'] = [\'<eos>\']\n            kwargs[\'lower_case\'] = False\n        elif dataset == \'ptb\':\n            kwargs[\'special\'] = [\'<eos>\']\n            kwargs[\'lower_case\'] = True\n        elif dataset == \'lm1b\':\n            kwargs[\'special\'] = []\n            kwargs[\'lower_case\'] = False\n            kwargs[\'vocab_file\'] = os.path.join(datadir, \'1b_word_vocab.txt\')\n        elif dataset in [\'enwik8\', \'text8\']:\n            pass\n\n        corpus = TransfoXLCorpus(datadir, dataset, **kwargs)\n        torch.save(corpus, fn)\n\n    return corpus\n'"
examples/mnli_example/pytorch_pretrained_bert/__init__.py,0,"b'__version__ = ""0.6.2""\nfrom .tokenization import BertTokenizer, BasicTokenizer, WordpieceTokenizer\nfrom .tokenization_openai import OpenAIGPTTokenizer\nfrom .tokenization_transfo_xl import (TransfoXLTokenizer, TransfoXLCorpus)\nfrom .tokenization_gpt2 import GPT2Tokenizer\n\nfrom .modeling import (BertConfig, BertModel, BertForPreTraining,\n                       BertForMaskedLM, BertForNextSentencePrediction,\n                       BertForSequenceClassification, BertForMultipleChoice,\n                       BertForTokenClassification, BertForQuestionAnswering,\n                       load_tf_weights_in_bert)\nfrom .modeling_openai import (OpenAIGPTConfig, OpenAIGPTModel,\n                              OpenAIGPTLMHeadModel, OpenAIGPTDoubleHeadsModel,\n                              load_tf_weights_in_openai_gpt)\nfrom .modeling_transfo_xl import (TransfoXLConfig, TransfoXLModel, TransfoXLLMHeadModel,\n                                  load_tf_weights_in_transfo_xl)\nfrom .modeling_gpt2 import (GPT2Config, GPT2Model,\n                            GPT2LMHeadModel, GPT2DoubleHeadsModel,\n                            load_tf_weights_in_gpt2)\n\nfrom .optimization import BertAdam\nfrom .optimization_openai import OpenAIAdam\n\nfrom .file_utils import PYTORCH_PRETRAINED_BERT_CACHE, cached_path, WEIGHTS_NAME, CONFIG_NAME\n'"
examples/mnli_example/pytorch_pretrained_bert/__main__.py,0,"b'# coding: utf8\ndef main():\n    import sys\n    if (len(sys.argv) != 4 and len(sys.argv) != 5) or sys.argv[1] not in [\n        ""convert_tf_checkpoint_to_pytorch"",\n        ""convert_openai_checkpoint"",\n        ""convert_transfo_xl_checkpoint"",\n        ""convert_gpt2_checkpoint"",\n    ]:\n        print(\n        ""Should be used as one of: \\n""\n        "">> `pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT`, \\n""\n        "">> `pytorch_pretrained_bert convert_openai_checkpoint OPENAI_GPT_CHECKPOINT_FOLDER_PATH PYTORCH_DUMP_OUTPUT [OPENAI_GPT_CONFIG]`, \\n""\n        "">> `pytorch_pretrained_bert convert_transfo_xl_checkpoint TF_CHECKPOINT_OR_DATASET PYTORCH_DUMP_OUTPUT [TF_CONFIG]` or \\n""\n        "">> `pytorch_pretrained_bert convert_gpt2_checkpoint TF_CHECKPOINT PYTORCH_DUMP_OUTPUT [GPT2_CONFIG]`"")\n    else:\n        if sys.argv[1] == ""convert_tf_checkpoint_to_pytorch"":\n            try:\n                from .convert_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch\n            except ImportError:\n                print(""pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, ""\n                    ""In that case, it requires TensorFlow to be installed. Please see ""\n                    ""https://www.tensorflow.org/install/ for installation instructions."")\n                raise\n\n            if len(sys.argv) != 5:\n                # pylint: disable=line-too-long\n                print(""Should be used as `pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT`"")\n            else:\n                PYTORCH_DUMP_OUTPUT = sys.argv.pop()\n                TF_CONFIG = sys.argv.pop()\n                TF_CHECKPOINT = sys.argv.pop()\n                convert_tf_checkpoint_to_pytorch(TF_CHECKPOINT, TF_CONFIG, PYTORCH_DUMP_OUTPUT)\n        elif sys.argv[1] == ""convert_openai_checkpoint"":\n            from .convert_openai_checkpoint_to_pytorch import convert_openai_checkpoint_to_pytorch\n            OPENAI_GPT_CHECKPOINT_FOLDER_PATH = sys.argv[2]\n            PYTORCH_DUMP_OUTPUT = sys.argv[3]\n            if len(sys.argv) == 5:\n                OPENAI_GPT_CONFIG = sys.argv[4]\n            else:\n                OPENAI_GPT_CONFIG = """"\n            convert_openai_checkpoint_to_pytorch(OPENAI_GPT_CHECKPOINT_FOLDER_PATH,\n                                                 OPENAI_GPT_CONFIG,\n                                                 PYTORCH_DUMP_OUTPUT)\n        elif sys.argv[1] == ""convert_transfo_xl_checkpoint"":\n            try:\n                from .convert_transfo_xl_checkpoint_to_pytorch import convert_transfo_xl_checkpoint_to_pytorch\n            except ImportError:\n                print(""pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, ""\n                    ""In that case, it requires TensorFlow to be installed. Please see ""\n                    ""https://www.tensorflow.org/install/ for installation instructions."")\n                raise\n\n            if \'ckpt\' in sys.argv[2].lower():\n                TF_CHECKPOINT = sys.argv[2]\n                TF_DATASET_FILE = """"\n            else:\n                TF_DATASET_FILE = sys.argv[2]\n                TF_CHECKPOINT = """"\n            PYTORCH_DUMP_OUTPUT = sys.argv[3]\n            if len(sys.argv) == 5:\n                TF_CONFIG = sys.argv[4]\n            else:\n                TF_CONFIG = """"\n            convert_transfo_xl_checkpoint_to_pytorch(TF_CHECKPOINT, TF_CONFIG, PYTORCH_DUMP_OUTPUT, TF_DATASET_FILE)\n        else:\n            try:\n                from .convert_gpt2_checkpoint_to_pytorch import convert_gpt2_checkpoint_to_pytorch\n            except ImportError:\n                print(""pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, ""\n                    ""In that case, it requires TensorFlow to be installed. Please see ""\n                    ""https://www.tensorflow.org/install/ for installation instructions."")\n                raise\n\n            TF_CHECKPOINT = sys.argv[2]\n            PYTORCH_DUMP_OUTPUT = sys.argv[3]\n            if len(sys.argv) == 5:\n                TF_CONFIG = sys.argv[4]\n            else:\n                TF_CONFIG = """"\n            convert_gpt2_checkpoint_to_pytorch(TF_CHECKPOINT, TF_CONFIG, PYTORCH_DUMP_OUTPUT)\nif __name__ == \'__main__\':\n    main()\n'"
examples/mnli_example/pytorch_pretrained_bert/convert_gpt2_checkpoint_to_pytorch.py,1,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convert OpenAI GPT checkpoint.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nfrom io import open\n\nimport torch\n\nfrom pytorch_pretrained_bert.modeling_gpt2 import (CONFIG_NAME, WEIGHTS_NAME,\n                                                     GPT2Config,\n                                                     GPT2Model,\n                                                     load_tf_weights_in_gpt2)\n\n\ndef convert_gpt2_checkpoint_to_pytorch(gpt2_checkpoint_path, gpt2_config_file, pytorch_dump_folder_path):\n    # Construct model\n    if gpt2_config_file == """":\n        config = GPT2Config()\n    else:\n        config = GPT2Config(gpt2_config_file)\n    model = GPT2Model(config)\n\n    # Load weights from numpy\n    load_tf_weights_in_gpt2(model, gpt2_checkpoint_path)\n\n    # Save pytorch-model\n    pytorch_weights_dump_path = pytorch_dump_folder_path + \'/\' + WEIGHTS_NAME\n    pytorch_config_dump_path = pytorch_dump_folder_path + \'/\' + CONFIG_NAME\n    print(""Save PyTorch model to {}"".format(pytorch_weights_dump_path))\n    torch.save(model.state_dict(), pytorch_weights_dump_path)\n    print(""Save configuration file to {}"".format(pytorch_config_dump_path))\n    with open(pytorch_config_dump_path, ""w"", encoding=""utf-8"") as f:\n        f.write(config.to_json_string())\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(""--gpt2_checkpoint_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path the TensorFlow checkpoint path."")\n    parser.add_argument(""--pytorch_dump_folder_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the output PyTorch model."")\n    parser.add_argument(""--gpt2_config_file"",\n                        default = """",\n                        type = str,\n                        help = ""An optional config json file corresponding to the pre-trained OpenAI model. \\n""\n                            ""This specifies the model architecture."")\n    args = parser.parse_args()\n    convert_gpt2_checkpoint_to_pytorch(args.gpt2_checkpoint_path,\n                                         args.gpt2_config_file,\n                                         args.pytorch_dump_folder_path)\n'"
examples/mnli_example/pytorch_pretrained_bert/convert_openai_checkpoint_to_pytorch.py,1,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convert OpenAI GPT checkpoint.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nfrom io import open\n\nimport torch\n\nfrom pytorch_pretrained_bert.modeling_openai import (CONFIG_NAME, WEIGHTS_NAME,\n                                                     OpenAIGPTConfig,\n                                                     OpenAIGPTModel,\n                                                     load_tf_weights_in_openai_gpt)\n\n\ndef convert_openai_checkpoint_to_pytorch(openai_checkpoint_folder_path, openai_config_file, pytorch_dump_folder_path):\n    # Construct model\n    if openai_config_file == """":\n        config = OpenAIGPTConfig()\n    else:\n        config = OpenAIGPTConfig(openai_config_file)\n    model = OpenAIGPTModel(config)\n\n    # Load weights from numpy\n    load_tf_weights_in_openai_gpt(model, openai_checkpoint_folder_path)\n\n    # Save pytorch-model\n    pytorch_weights_dump_path = pytorch_dump_folder_path + \'/\' + WEIGHTS_NAME\n    pytorch_config_dump_path = pytorch_dump_folder_path + \'/\' + CONFIG_NAME\n    print(""Save PyTorch model to {}"".format(pytorch_weights_dump_path))\n    torch.save(model.state_dict(), pytorch_weights_dump_path)\n    print(""Save configuration file to {}"".format(pytorch_config_dump_path))\n    with open(pytorch_config_dump_path, ""w"", encoding=""utf-8"") as f:\n        f.write(config.to_json_string())\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(""--openai_checkpoint_folder_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path the TensorFlow checkpoint path."")\n    parser.add_argument(""--pytorch_dump_folder_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the output PyTorch model."")\n    parser.add_argument(""--openai_config_file"",\n                        default = """",\n                        type = str,\n                        help = ""An optional config json file corresponding to the pre-trained OpenAI model. \\n""\n                            ""This specifies the model architecture."")\n    args = parser.parse_args()\n    convert_openai_checkpoint_to_pytorch(args.openai_checkpoint_folder_path,\n                                         args.openai_config_file,\n                                         args.pytorch_dump_folder_path)\n'"
examples/mnli_example/pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py,1,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convert BERT checkpoint.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport argparse\nimport tensorflow as tf\nimport torch\nimport numpy as np\n\nfrom pytorch_pretrained_bert.modeling import BertConfig, BertForPreTraining, load_tf_weights_in_bert\n\ndef convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n    # Initialise PyTorch model\n    config = BertConfig.from_json_file(bert_config_file)\n    print(""Building PyTorch model from configuration: {}"".format(str(config)))\n    model = BertForPreTraining(config)\n\n    # Load weights from tf checkpoint\n    load_tf_weights_in_bert(model, tf_checkpoint_path)\n\n    # Save pytorch-model\n    print(""Save PyTorch model to {}"".format(pytorch_dump_path))\n    torch.save(model.state_dict(), pytorch_dump_path)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(""--tf_checkpoint_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path the TensorFlow checkpoint path."")\n    parser.add_argument(""--bert_config_file"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""The config json file corresponding to the pre-trained BERT model. \\n""\n                            ""This specifies the model architecture."")\n    parser.add_argument(""--pytorch_dump_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the output PyTorch model."")\n    args = parser.parse_args()\n    convert_tf_checkpoint_to_pytorch(args.tf_checkpoint_path,\n                                     args.bert_config_file,\n                                     args.pytorch_dump_path)\n'"
examples/mnli_example/pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py,3,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convert Transformer XL checkpoint and datasets.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport os\nimport sys\nfrom io import open\n\nimport torch\n\nimport pytorch_pretrained_bert.tokenization_transfo_xl as data_utils\nfrom pytorch_pretrained_bert.modeling_transfo_xl import (CONFIG_NAME,\n                                                         WEIGHTS_NAME,\n                                                         TransfoXLConfig,\n                                                         TransfoXLLMHeadModel,\n                                                         load_tf_weights_in_transfo_xl)\nfrom pytorch_pretrained_bert.tokenization_transfo_xl import (CORPUS_NAME,\n                                                             VOCAB_NAME)\n\nif sys.version_info[0] == 2:\n    import cPickle as pickle\nelse:\n    import pickle\n\n# We do this to be able to load python 2 datasets pickles\n# See e.g. https://stackoverflow.com/questions/2121874/python-pickling-after-changing-a-modules-directory/2121918#2121918\ndata_utils.Vocab = data_utils.TransfoXLTokenizer\ndata_utils.Corpus = data_utils.TransfoXLCorpus\nsys.modules[\'data_utils\'] = data_utils\nsys.modules[\'vocabulary\'] = data_utils\n\ndef convert_transfo_xl_checkpoint_to_pytorch(tf_checkpoint_path,\n                                             transfo_xl_config_file,\n                                             pytorch_dump_folder_path,\n                                             transfo_xl_dataset_file):\n    if transfo_xl_dataset_file:\n        # Convert a pre-processed corpus (see original TensorFlow repo)\n        with open(transfo_xl_dataset_file, ""rb"") as fp:\n            corpus = pickle.load(fp, encoding=""latin1"")\n        # Save vocabulary and dataset cache as Dictionaries (should be better than pickles for the long-term)\n        pytorch_vocab_dump_path = pytorch_dump_folder_path + \'/\' + VOCAB_NAME\n        print(""Save vocabulary to {}"".format(pytorch_vocab_dump_path))\n        corpus_vocab_dict = corpus.vocab.__dict__\n        torch.save(corpus_vocab_dict, pytorch_vocab_dump_path)\n\n        corpus_dict_no_vocab = corpus.__dict__\n        corpus_dict_no_vocab.pop(\'vocab\', None)\n        pytorch_dataset_dump_path = pytorch_dump_folder_path + \'/\' + CORPUS_NAME\n        print(""Save dataset to {}"".format(pytorch_dataset_dump_path))\n        torch.save(corpus_dict_no_vocab, pytorch_dataset_dump_path)\n\n    if tf_checkpoint_path:\n        # Convert a pre-trained TensorFlow model\n        config_path = os.path.abspath(transfo_xl_config_file)\n        tf_path = os.path.abspath(tf_checkpoint_path)\n\n        print(""Converting Transformer XL checkpoint from {} with config at {}"".format(tf_path, config_path))\n        # Initialise PyTorch model\n        if transfo_xl_config_file == """":\n            config = TransfoXLConfig()\n        else:\n            config = TransfoXLConfig(transfo_xl_config_file)\n        print(""Building PyTorch model from configuration: {}"".format(str(config)))\n        model = TransfoXLLMHeadModel(config)\n\n        model = load_tf_weights_in_transfo_xl(model, config, tf_path)\n        # Save pytorch-model\n        pytorch_weights_dump_path = os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)\n        pytorch_config_dump_path = os.path.join(pytorch_dump_folder_path, CONFIG_NAME)\n        print(""Save PyTorch model to {}"".format(os.path.abspath(pytorch_weights_dump_path)))\n        torch.save(model.state_dict(), pytorch_weights_dump_path)\n        print(""Save configuration file to {}"".format(os.path.abspath(pytorch_config_dump_path)))\n        with open(pytorch_config_dump_path, ""w"", encoding=""utf-8"") as f:\n            f.write(config.to_json_string())\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--pytorch_dump_folder_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the folder to store the PyTorch model or dataset/vocab."")\n    parser.add_argument(""--tf_checkpoint_path"",\n                        default = """",\n                        type = str,\n                        help = ""An optional path to a TensorFlow checkpoint path to be converted."")\n    parser.add_argument(""--transfo_xl_config_file"",\n                        default = """",\n                        type = str,\n                        help = ""An optional config json file corresponding to the pre-trained BERT model. \\n""\n                            ""This specifies the model architecture."")\n    parser.add_argument(""--transfo_xl_dataset_file"",\n                        default = """",\n                        type = str,\n                        help = ""An optional dataset file to be converted in a vocabulary."")\n    args = parser.parse_args()\n    convert_transfo_xl_checkpoint_to_pytorch(args.tf_checkpoint_path,\n                                     args.transfo_xl_config_file,\n                                     args.pytorch_dump_folder_path,\n                                     args.transfo_xl_dataset_file)\n'"
examples/mnli_example/pytorch_pretrained_bert/file_utils.py,0,"b'""""""\nUtilities for working with the local dataset cache.\nThis file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\nCopyright by the AllenNLP authors.\n""""""\nfrom __future__ import (absolute_import, division, print_function, unicode_literals)\n\nimport sys\nimport json\nimport logging\nimport os\nimport shutil\nimport tempfile\nimport fnmatch\nfrom functools import wraps\nfrom hashlib import sha256\nimport sys\nfrom io import open\n\nimport boto3\nimport requests\nfrom botocore.exceptions import ClientError\nfrom tqdm import tqdm\n\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n\ntry:\n    from pathlib import Path\n    PYTORCH_PRETRAINED_BERT_CACHE = Path(os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\',\n                                                   Path.home() / \'.pytorch_pretrained_bert\'))\nexcept (AttributeError, ImportError):\n    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\',\n                                              os.path.join(os.path.expanduser(""~""), \'.pytorch_pretrained_bert\'))\n\nCONFIG_NAME = ""config.json""\nWEIGHTS_NAME = ""pytorch_model.bin""\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef url_to_filename(url, etag=None):\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    """"""\n    url_bytes = url.encode(\'utf-8\')\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\'utf-8\')\n        etag_hash = sha256(etag_bytes)\n        filename += \'.\' + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename, cache_dir=None):\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise EnvironmentError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + \'.json\'\n    if not os.path.exists(meta_path):\n        raise EnvironmentError(""file {} not found"".format(meta_path))\n\n    with open(meta_path, encoding=""utf-8"") as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[\'url\']\n    etag = metadata[\'etag\']\n\n    return url, etag\n\n\ndef cached_path(url_or_filename, cache_dir=None):\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (\'http\', \'https\', \'s3\'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == \'\':\n        # File, but it doesn\'t exist.\n        raise EnvironmentError(""file {} not found"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\n\n\ndef split_s3_path(url):\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(""bad s3 path {}"".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise EnvironmentError(""file {} not found"".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url):\n    """"""Check ETag on S3 object.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file):\n    """"""Pull a file directly from S3.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef http_get(url, temp_file):\n    req = requests.get(url, stream=True)\n    content_length = req.headers.get(\'Content-Length\')\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=""B"", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk: # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(url, cache_dir=None):\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(""s3://""):\n        etag = s3_etag(url)\n    else:\n        try:\n            response = requests.head(url, allow_redirects=True)\n            if response.status_code != 200:\n                etag = None\n            else:\n                etag = response.headers.get(""ETag"")\n        except EnvironmentError:\n            etag = None\n\n    if sys.version_info[0] == 2 and etag is not None:\n        etag = etag.decode(\'utf-8\')\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    # If we don\'t have a connection (etag is None) and can\'t identify the file\n    # try to get the last downloaded one\n    if not os.path.exists(cache_path) and etag is None:\n        matching_files = fnmatch.filter(os.listdir(cache_dir), filename + \'.*\')\n        matching_files = list(filter(lambda s: not s.endswith(\'.json\'), matching_files))\n        if matching_files:\n            cache_path = os.path.join(cache_dir, matching_files[-1])\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(""%s not found in cache, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(""s3://""):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n            with open(cache_path, \'wb\') as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(""creating metadata file for %s"", cache_path)\n            meta = {\'url\': url, \'etag\': etag}\n            meta_path = cache_path + \'.json\'\n            with open(meta_path, \'w\') as meta_file:\n                output_string = json.dumps(meta)\n                if sys.version_info[0] == 2 and isinstance(output_string, str):\n                    output_string = unicode(output_string, \'utf-8\')  # The beauty of python 2\n                meta_file.write(output_string)\n\n            logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path\n\n\ndef read_set_from_file(filename):\n    \'\'\'\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    \'\'\'\n    collection = set()\n    with open(filename, \'r\', encoding=\'utf-8\') as file_:\n        for line in file_:\n            collection.add(line.rstrip())\n    return collection\n\n\ndef get_file_extension(path, dot=True, lower=True):\n    ext = os.path.splitext(path)[1]\n    ext = ext if dot else ext[1:]\n    return ext.lower() if lower else ext\n'"
examples/mnli_example/pytorch_pretrained_bert/modeling.py,76,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch BERT model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom .file_utils import cached_path, WEIGHTS_NAME, CONFIG_NAME\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz"",\n}\nBERT_CONFIG_NAME = \'bert_config.json\'\nTF_WEIGHTS_NAME = \'model.ckpt\'\n\ndef load_tf_weights_in_bert(model, tf_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    print(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\'/\')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [""adam_v"", ""adam_m"", ""global_step""] for n in name):\n            print(""Skipping {}"".format(""/"".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'output_weights\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'squad\':\n                pointer = getattr(pointer, \'classifier\')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    print(""Skipping {}"".format(""/"".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \'_embeddings\':\n            pointer = getattr(pointer, \'weight\')\n        elif m_name == \'kernel\':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    """"""Implementation of the gelu activation function.\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish}\n\n\nclass BertConfig(object):\n    """"""Configuration class to store the configuration of a `BertModel`.\n    """"""\n    def __init__(self,\n                 vocab_size_or_config_json_file,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02):\n        """"""Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             ""or the path to a pretrained model config file (str)"")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n        config = BertConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `BertConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n\ntry:\n    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\nexcept ImportError:\n    logger.info(""Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex ."")\n    class BertLayerNorm(nn.Module):\n        def __init__(self, hidden_size, eps=1e-12):\n            """"""Construct a layernorm module in the TF style (epsilon inside the square root).\n            """"""\n            super(BertLayerNorm, self).__init__()\n            self.weight = nn.Parameter(torch.ones(hidden_size))\n            self.bias = nn.Parameter(torch.zeros(hidden_size))\n            self.variance_epsilon = eps\n\n        def forward(self, x):\n            u = x.mean(-1, keepdim=True)\n            s = (x - u).pow(2).mean(-1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n            return self.weight * x + self.bias\n\nclass BertEmbeddings(nn.Module):\n    """"""Construct the embeddings from word, position and token_type embeddings.\n    """"""\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None):\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads))\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        self.output_score = config.output_score\n        self.output_sum   = config.output_sum\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n        attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        if self.output_score is True:\n            attention_probs_sum = attention_scores # (batch_size, h, l, l)\n        else:\n            attention_probs_sum = attention_probs # (batch_size, h, l, l)\n        if self.output_sum is not None:\n            attention_probs_sum = attention_probs_sum.sum(dim=self.output_sum)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        return context_layer, attention_probs_sum\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n\n    def forward(self, input_tensor, attention_mask):\n        self_output, attention_probs_sum = self.self(input_tensor, attention_mask)\n        attention_output = self.output(self_output, input_tensor)\n        return attention_output, attention_probs_sum\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states, attention_mask):\n        attention_output, attention_probs_sum = self.attention(hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output, attention_probs_sum\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        layer = BertLayer(config)\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, output_attention_layer=[]):\n        all_encoder_layers = []\n        all_attention_probs_sum = []\n        for idx,layer_module in enumerate(self.layer):\n            if idx in output_attention_layer:\n                hidden_states, attention_probs_sum = layer_module(hidden_states, attention_mask)\n                all_attention_probs_sum.append(attention_probs_sum)\n            else:\n                hidden_states, _ = layer_module(hidden_states, attention_mask)\n                all_attention_probs_sum.append(None)\n            if output_all_encoded_layers:\n                all_encoder_layers.append(hidden_states)\n        if not output_all_encoded_layers:\n            all_encoder_layers.append(hidden_states)\n        return all_encoder_layers, all_attention_probs_sum\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(BertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertLMPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n                                 bert_model_embedding_weights.size(0),\n                                 bias=False)\n        self.decoder.weight = bert_model_embedding_weights\n        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertOnlyMLMHead, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertPreTrainingHeads, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\n\nclass BertPreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    def __init__(self, config, *inputs, **kwargs):\n        super(BertPreTrainedModel, self).__init__()\n        if not isinstance(config, BertConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `BertConfig`. ""\n                ""To create a model from a Google pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        self.config = config\n\n    def init_bert_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n        """"""\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        """"""\n        state_dict = kwargs.get(\'state_dict\', None)\n        kwargs.pop(\'state_dict\', None)\n        cache_dir = kwargs.get(\'cache_dir\', None)\n        kwargs.pop(\'cache_dir\', None)\n        from_tf = kwargs.get(\'from_tf\', False)\n        kwargs.pop(\'from_tf\', None)\n\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                ""associated to this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n                    archive_file))\n            return None\n        if resolved_archive_file == archive_file:\n            logger.info(""loading archive file {}"".format(archive_file))\n        else:\n            logger.info(""loading archive file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n        tempdir = None\n        if os.path.isdir(resolved_archive_file) or from_tf:\n            serialization_dir = resolved_archive_file\n        else:\n            # Extract archive to temp dir\n            tempdir = tempfile.mkdtemp()\n            logger.info(""extracting archive file {} to temp dir {}"".format(\n                resolved_archive_file, tempdir))\n            with tarfile.open(resolved_archive_file, \'r:gz\') as archive:\n                archive.extractall(tempdir)\n            serialization_dir = tempdir\n        # Load config\n        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n        if not os.path.exists(config_file):\n            # Backward compatibility with old naming format\n            config_file = os.path.join(serialization_dir, BERT_CONFIG_NAME)\n        config = BertConfig.from_json_file(config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n            state_dict = torch.load(weights_path, map_location=\'cpu\')\n        if tempdir:\n            # Clean up temp dir\n            shutil.rmtree(tempdir)\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint\n            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n            return load_tf_weights_in_bert(model, weights_path)\n        # Load from a PyTorch state_dict\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if \'gamma\' in key:\n                new_key = key.replace(\'gamma\', \'weight\')\n            if \'beta\' in key:\n                new_key = key.replace(\'beta\', \'bias\')\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \'_metadata\', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=\'\'):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + \'.\')\n        start_prefix = \'\'\n        if not hasattr(model, \'bert\') and any(s.startswith(\'bert.\') for s in state_dict.keys()):\n            start_prefix = \'bert.\'\n        load(model, prefix=start_prefix)\n        if len(missing_keys) > 0:\n            logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(""Weights from pretrained model not used in {}: {}"".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) > 0:\n            raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\n                               model.__class__.__name__, ""\\n\\t"".join(error_msgs)))\n        return model\n\n\nclass BertModel(BertPreTrainedModel):\n    """"""BERT model (""Bidirectional Embedding Representations from a Transformer"").\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n\n    Outputs: Tuple of (encoded_layers, pooled_output)\n        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n            classifier pretrained on top of the hidden state associated to the first character of the\n            input (`CLS`) to train on the Next-Sentence task (see BERT\'s paper).\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = modeling.BertModel(config=config)\n    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, output_score=False, output_sum=1):\n        super(BertModel, self).__init__(config)\n        config.output_score = output_score\n        config.output_sum   = output_sum\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, output_attention_layer=[]):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        embedding_output = self.embeddings(input_ids, token_type_ids)\n        encoded_layers,attention_probs_sum = self.encoder(embedding_output,\n                                      extended_attention_mask,\n                                      output_all_encoded_layers=output_all_encoded_layers,\n                                      output_attention_layer=output_attention_layer)\n        sequence_output = encoded_layers[-1]\n        pooled_output = self.pooler(sequence_output)\n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n        if len(output_attention_layer)>0:\n            return encoded_layers, pooled_output, attention_probs_sum\n        else:\n            return encoded_layers, pooled_output\n\n\nclass BertForPreTraining(BertPreTrainedModel):\n    """"""BERT model with pre-training heads.\n    This module comprises the BERT model followed by the two pre-training heads:\n        - the masked language modeling head, and\n        - the next sentence classification head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: optional masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n        `next_sentence_label`: optional next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n\n    Outputs:\n        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `masked_lm_labels` or `next_sentence_label` is `None`:\n            Outputs a tuple comprising\n            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n            - the next sentence classification logits of shape [batch_size, 2].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForPreTraining(config)\n    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForPreTraining, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                                   output_all_encoded_layers=False)\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            return total_loss\n        else:\n            return prediction_scores, seq_relationship_score\n\n\nclass BertForMaskedLM(BertPreTrainedModel):\n    """"""BERT model with the masked language modeling head.\n    This module comprises the BERT model followed by the masked language modeling head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n\n    Outputs:\n        if `masked_lm_labels` is  not `None`:\n            Outputs the masked language modeling loss.\n        if `masked_lm_labels` is `None`:\n            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForMaskedLM(config)\n    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForMaskedLM, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n                                       output_all_encoded_layers=False)\n        prediction_scores = self.cls(sequence_output)\n\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            return masked_lm_loss\n        else:\n            return prediction_scores\n\n\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    """"""BERT model with next sentence prediction head.\n    This module comprises the BERT model followed by the next sentence classification head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n\n    Outputs:\n        if `next_sentence_label` is not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `next_sentence_label` is `None`:\n            Outputs the next sentence classification logits of shape [batch_size, 2].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForNextSentencePrediction(config)\n    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForNextSentencePrediction, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                     output_all_encoded_layers=False)\n        seq_relationship_score = self.cls( pooled_output)\n\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            return next_sentence_loss\n        else:\n            return seq_relationship_score\n\n\nclass BertForSequenceClassification(BertPreTrainedModel):\n    """"""BERT model for classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary. Items in the batch should begin with the special ""CLS"" token. (see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_labels].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_labels = 2\n\n    model = BertForSequenceClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForMultipleChoice(BertPreTrainedModel):\n    """"""BERT model for multiple choice tasks.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_choices`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_choices = 2\n\n    model = BertForMultipleChoice(config, num_choices)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_choices):\n        super(BertForMultipleChoice, self).__init__(config)\n        self.num_choices = num_choices\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, self.num_choices)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            return loss\n        else:\n            return reshaped_logits\n\n\nclass BertForTokenClassification(BertPreTrainedModel):\n    """"""BERT model for token-level classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the full hidden state of the last layer.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [0, ..., num_labels].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_labels = 2\n\n    model = BertForTokenClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForTokenClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForQuestionAnswering(BertPreTrainedModel):\n    """"""BERT model for Question Answering (span extraction).\n    This module is composed of the BERT model with a linear layer on top of\n    the sequence output that computes start_logits and end_logits\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n            into account for computing the loss.\n        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n            into account for computing the loss.\n\n    Outputs:\n        if `start_positions` and `end_positions` are not `None`:\n            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n        if `start_positions` or `end_positions` is `None`:\n            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n            position tokens of shape [batch_size, sequence_length].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForQuestionAnswering(config)\n    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.bert = BertModel(config)\n        # TODO check with Google if it\'s normal there is no dropout on the token classifier of SQuAD in the TF version\n        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            return total_loss\n        else:\n            return start_logits, end_logits\n'"
examples/mnli_example/pytorch_pretrained_bert/modeling_gpt2.py,43,"b'# coding=utf-8\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch OpenAI GPT-2 model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .file_utils import cached_path, CONFIG_NAME, WEIGHTS_NAME\nfrom .modeling import BertLayerNorm as LayerNorm\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {""gpt2"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin""}\nPRETRAINED_CONFIG_ARCHIVE_MAP = {""gpt2"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json""}\n\ndef load_tf_weights_in_gpt2(model, gpt2_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(gpt2_checkpoint_path)\n    print(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array.squeeze())\n\n    for name, array in zip(names, arrays):\n        name = name[6:]  # skip ""model/""\n        name = name.split(\'/\')\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+\\d+\', m_name):\n                l = re.split(r\'(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'w\' or l[0] == \'g\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'b\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'wpe\' or l[0] == \'wte\':\n                pointer = getattr(pointer, l[0])\n                pointer = getattr(pointer, \'weight\')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\n\nclass GPT2Config(object):\n    """"""Configuration class to store the configuration of a `GPT2Model`.\n    """"""\n\n    def __init__(\n        self,\n        vocab_size_or_config_json_file=50257,\n        n_positions=1024,\n        n_ctx=1024,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n    ):\n        """"""Constructs GPT2Config.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `GPT2Model` or a configuration json file.\n            n_positions: Number of positional embeddings.\n            n_ctx: Size of the causal mask (usually same as n_positions).\n            n_embd: Dimensionality of the embeddings and hidden states.\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            layer_norm_epsilon: epsilon to use in the layer norm layers\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=""utf-8"") as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.n_ctx = n_ctx\n            self.n_positions = n_positions\n            self.n_embd = n_embd\n            self.n_layer = n_layer\n            self.n_head = n_head\n            self.layer_norm_epsilon = layer_norm_epsilon\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(\n                ""First argument must be either a vocabulary size (int)""\n                ""or the path to a pretrained model config file (str)""\n            )\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `GPT2Config` from a Python dictionary of parameters.""""""\n        config = GPT2Config(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `GPT2Config` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=""utf-8"") as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n\n\nclass Conv1D(nn.Module):\n    def __init__(self, nf, nx):\n        super(Conv1D, self).__init__()\n        self.nf = nf\n        w = torch.empty(nx, nf)\n        nn.init.normal_(w, std=0.02)\n        self.weight = Parameter(w)\n        self.bias = Parameter(torch.zeros(nf))\n\n    def forward(self, x):\n        size_out = x.size()[:-1] + (self.nf,)\n        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n        x = x.view(*size_out)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, nx, n_ctx, config, scale=False):\n        super(Attention, self).__init__()\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n        assert n_state % config.n_head == 0\n        self.register_buffer(""bias"", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.c_attn = Conv1D(n_state * 3, nx)\n        self.c_proj = Conv1D(n_state, nx)\n\n    def _attn(self, q, k, v):\n        w = torch.matmul(q, k)\n        if self.scale:\n            w = w / math.sqrt(v.size(-1))\n        nd, ns = w.size(-2), w.size(-1)\n        b = self.bias[:, :, ns-nd:ns, :ns]\n        w = w * b - 1e4 * (1 - b)\n\n        w = nn.Softmax(dim=-1)(w)\n        return torch.matmul(w, v)\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n\n    def split_heads(self, x, k=False):\n        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n        if k:\n            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n        else:\n            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n\n    def forward(self, x, layer_past=None):\n        x = self.c_attn(x)\n        query, key, value = x.split(self.split_size, dim=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n        if layer_past is not None:\n            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n            key = torch.cat((past_key, key), dim=-1)\n            value = torch.cat((past_value, value), dim=-2)\n        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n        a = self._attn(query, key, value)\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n        return a, present\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n        super(MLP, self).__init__()\n        nx = config.n_embd\n        self.c_fc = Conv1D(n_state, nx)\n        self.c_proj = Conv1D(nx, n_state)\n        self.act = gelu\n\n    def forward(self, x):\n        h = self.act(self.c_fc(x))\n        h2 = self.c_proj(h)\n        return h2\n\n\nclass Block(nn.Module):\n    def __init__(self, n_ctx, config, scale=False):\n        super(Block, self).__init__()\n        nx = config.n_embd\n        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n        self.attn = Attention(nx, n_ctx, config, scale)\n        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n        self.mlp = MLP(4 * nx, config)\n\n    def forward(self, x, layer_past=None):\n        a, present = self.attn(self.ln_1(x), layer_past=layer_past)\n        x = x + a\n        m = self.mlp(self.ln_2(x))\n        x = x + m\n        return x, present\n\n\nclass GPT2LMHead(nn.Module):\n    """""" Language Model Head for the transformer """"""\n\n    def __init__(self, model_embeddings_weights, config):\n        super(GPT2LMHead, self).__init__()\n        self.n_embd = config.n_embd\n        self.set_embeddings_weights(model_embeddings_weights)\n\n    def set_embeddings_weights(self, model_embeddings_weights):\n        embed_shape = model_embeddings_weights.shape\n        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n        self.decoder.weight = model_embeddings_weights  # Tied weights\n\n    def forward(self, hidden_state):\n        # Truncated Language modeling logits (we remove the last token)\n        # h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)\n        lm_logits = self.decoder(hidden_state)\n        return lm_logits\n\n\nclass GPT2MultipleChoiceHead(nn.Module):\n    """""" Classifier Head for the transformer """"""\n\n    def __init__(self, config):\n        super(GPT2MultipleChoiceHead, self).__init__()\n        self.n_embd = config.n_embd\n        self.linear = nn.Linear(config.n_embd, 1)\n\n        nn.init.normal_(self.linear.weight, std=0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, hidden_states, mc_token_ids):\n        # Classification logits\n        # hidden_state (bsz, num_choices, seq_length, hidden_size)\n        # mc_token_ids (bsz, num_choices)\n        mc_token_ids = mc_token_ids.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, hidden_states.size(-1))\n        # (bsz, num_choices, 1, hidden_size)\n        multiple_choice_h = hidden_states.gather(2, mc_token_ids).squeeze(2)\n        # (bsz, num_choices, hidden_size)\n        multiple_choice_logits = self.linear(multiple_choice_h).squeeze(-1)\n        # (bsz, num_choices)\n        return multiple_choice_logits\n\n\nclass GPT2PreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n\n    def __init__(self, config, *inputs, **kwargs):\n        super(GPT2PreTrainedModel, self).__init__()\n        if not isinstance(config, GPT2Config):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `GPT2Config`. ""\n                ""To create a model from a pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                )\n            )\n        self.config = config\n\n    def set_tied(self):\n        pass\n\n    def init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    @classmethod\n    def from_pretrained(\n        cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None, from_tf=False, *inputs, **kwargs\n    ):\n        """"""\n        Instantiate a GPT2PreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `gpt2`\n                - a path or url to a pretrained model archive containing:\n                    . `gpt2_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a GPT2Model instance\n                - a path or url to a pretrained model archive containing:\n                    . `gpt2_config.json` a configuration file for the model\n                    . a TensorFlow checkpoint with trained weights\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionary (collections.OrderedDict object) to use instead of pre-trained models\n            *inputs, **kwargs: additional input for the specific GPT class\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n            config_file = PRETRAINED_CONFIG_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} and {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path, "", "".join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()), pretrained_model_name_or_path,\n                    archive_file, config_file\n                )\n            )\n            return None\n        if resolved_archive_file == archive_file and resolved_config_file == config_file:\n            logger.info(""loading weights file {}"".format(archive_file))\n            logger.info(""loading configuration file {}"".format(config_file))\n        else:\n            logger.info(""loading weights file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n            logger.info(""loading configuration file {} from cache at {}"".format(\n                config_file, resolved_config_file))\n        # Load config\n        config = GPT2Config.from_json_file(resolved_config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            state_dict = torch.load(resolved_archive_file, map_location=\'cpu\')\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint (stored as NumPy array)\n            return load_tf_weights_in_gpt2(model, resolved_archive_file)\n\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if key.endswith("".g""):\n                new_key = key[:-2] + "".weight""\n            elif key.endswith("".b""):\n                new_key = key[:-2] + "".bias""\n            elif key.endswith("".w""):\n                new_key = key[:-2] + "".weight""\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, ""_metadata"", None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=""""):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\n            )\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + ""."")\n\n        start_model = model\n        if hasattr(model, ""transformer"") and all(not s.startswith(\'transformer.\') for s in state_dict.keys()):\n            start_model = model.transformer\n        load(start_model, prefix="""")\n\n        if len(missing_keys) > 0:\n            logger.info(\n                ""Weights of {} not initialized from pretrained model: {}"".format(model.__class__.__name__, missing_keys)\n            )\n        if len(unexpected_keys) > 0:\n            logger.info(\n                ""Weights from pretrained model not used in {}: {}"".format(model.__class__.__name__, unexpected_keys)\n            )\n        if len(error_msgs) > 0:\n            raise RuntimeError(\n                ""Error(s) in loading state_dict for {}:\\n\\t{}"".format(model.__class__.__name__, ""\\n\\t"".join(error_msgs))\n            )\n\n        # Make sure we are still sharing the output and input embeddings after loading weights\n        model.set_tied()\n        return model\n\n\nclass GPT2Model(GPT2PreTrainedModel):\n    """"""OpenAI GPT-2 model (""Language Models are Unsupervised Multitask Learners"").\n\n    Params:\n        config: a GPT2Config class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] (or more generally [d_1, ..., d_n, sequence_length]\n            were d_1 ... d_n are arbitrary dimensions) with the word BPE token indices selected in the range [0, config.vocab_size[\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n        `past`: an optional list of torch.LongTensor that contains pre-computed hidden-states\n            (key and values in the attention blocks) to speed up sequential decoding\n            (this is the presents output of the model, cf. below).\n\n    Outputs a tuple consisting of:\n        `hidden_states`: the encoded-hidden-states at the top of the model\n            as a torch.FloatTensor of size [batch_size, sequence_length, hidden_size]\n            (or more generally [d_1, ..., d_n, hidden_size] were d_1 ... d_n are the dimension of input_ids)\n        `presents`: a list of pre-computed hidden-states (key and values in each attention blocks) as\n            torch.FloatTensors. They can be reused to speed up sequential decoding.\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n\n    config = modeling_gpt2.GPT2Config()\n\n    model = modeling_gpt2.GPT2Model(config)\n    hidden_states, presents = model(input_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(GPT2Model, self).__init__(config)\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n        block = Block(config.n_ctx, config, scale=True)\n        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n\n        self.apply(self.init_weights)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, past=None):\n        if past is None:\n            past_length = 0\n            past = [None] * len(self.h)\n        else:\n            past_length = past[0][0].size(-2)\n        if position_ids is None:\n            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        position_ids = position_ids.view(-1, position_ids.size(-1))\n\n        inputs_embeds = self.wte(input_ids)\n        position_embeds = self.wpe(position_ids)\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n            token_type_embeds = self.wte(token_type_ids)\n        else:\n            token_type_embeds = 0\n        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n        presents = []\n        for block, layer_past in zip(self.h, past):\n            hidden_states, present = block(hidden_states, layer_past)\n            presents.append(present)\n        hidden_states = self.ln_f(hidden_states)\n        output_shape = input_shape + (hidden_states.size(-1),)\n        return hidden_states.view(*output_shape), presents\n\n\nclass GPT2LMHeadModel(GPT2PreTrainedModel):\n    """"""OpenAI GPT-2 model with a Language Modeling head (""Language Models are Unsupervised Multitask Learners"").\n\n    Params:\n        config: a GPT2Config class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] (or more generally [d_1, ..., d_n, sequence_length]\n            were d_1 ... d_n are arbitrary dimensions) with the word BPE token indices selected in the range [0, config.vocab_size[\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n        `lm_labels`: optional language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n        `past`: an optional list of torch.LongTensor that contains pre-computed hidden-states\n            (key and values in the attention blocks) to speed up sequential decoding\n            (this is the presents output of the model, cf. below).\n\n    Outputs:\n        if `lm_labels` is not `None`:\n            Outputs the language modeling loss.\n        else a tuple:\n            `lm_logits`: the language modeling logits as a torch.FloatTensor of size [batch_size, sequence_length, config.vocab_size]\n                (or more generally [d_1, ..., d_n, config.vocab_size] were d_1 ... d_n are the dimension of input_ids)\n            `presents`: a list of pre-computed hidden-states (key and values in each attention blocks) as\n                torch.FloatTensors. They can be reused to speed up sequential decoding.\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n\n    config = modeling_gpt2.GPT2Config()\n\n    model = modeling_gpt2.GPT2LMHeadModel(config)\n    lm_logits, presents = model(input_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(GPT2LMHeadModel, self).__init__(config)\n        self.transformer = GPT2Model(config)\n        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n        self.apply(self.init_weights)\n\n    def set_tied(self):\n        """""" Make sure we are sharing the embeddings\n        """"""\n        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        lm_logits = self.lm_head(hidden_states)\n        if lm_labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = lm_logits[:, :-1].contiguous()\n            shift_labels = lm_labels[:, 1:].contiguous()\n\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n            return loss\n        return lm_logits, presents\n\n\nclass GPT2DoubleHeadsModel(GPT2PreTrainedModel):\n    """"""OpenAI GPT-2 model with a Language Modeling and a Multiple Choice head (""Language Models are Unsupervised Multitask Learners"").\n\n    Params:\n        config: a GPT2Config class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length] with the BPE token\n            indices selected in the range [0, config.vocab_size[\n        `mc_token_ids`: a torch.LongTensor of shape [batch_size, num_choices] with the index of the token from\n            which we should take the hidden state to feed the multiple choice classifier (usually last token of the sequence)\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n        `lm_labels`: optional language modeling labels: torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with indices selected in [-1, 0, ..., config.vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., config.vocab_size]\n        `multiple_choice_labels`: optional multiple choice labels: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n        `past`: an optional list of torch.LongTensor that contains pre-computed hidden-states\n            (key and values in the attention blocks) to speed up sequential decoding\n            (this is the presents output of the model, cf. below).\n\n    Outputs:\n        if `lm_labels` and `multiple_choice_labels` are not `None`:\n            Outputs a tuple of losses with the language modeling loss and the multiple choice loss.\n        else: a tuple with\n            `lm_logits`: the language modeling logits as a torch.FloatTensor of size [batch_size, num_choices, sequence_length, config.vocab_size]\n            `multiple_choice_logits`: the multiple choice logits as a torch.FloatTensor of size [batch_size, num_choices]\n            `presents`: a list of pre-computed hidden-states (key and values in each attention blocks) as\n                torch.FloatTensors. They can be reused to speed up sequential decoding.\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]]])  # (bsz, number of choice, seq length)\n    mc_token_ids = torch.LongTensor([[2], [1]]) # (bsz, number of choice)\n\n    config = modeling_gpt2.GPT2Config()\n\n    model = modeling_gpt2.GPT2LMHeadModel(config)\n    lm_logits, multiple_choice_logits, presents = model(input_ids, mc_token_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(GPT2DoubleHeadsModel, self).__init__(config)\n        self.transformer = GPT2Model(config)\n        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n        self.multiple_choice_head = GPT2MultipleChoiceHead(config)\n        self.apply(self.init_weights)\n\n    def set_tied(self):\n        """""" Make sure we are sharing the embeddings\n        """"""\n        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n\n    def forward(self, input_ids, mc_token_ids, lm_labels=None, mc_labels=None, token_type_ids=None, position_ids=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        lm_logits = self.lm_head(hidden_states)\n        mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids)\n        losses = []\n        if lm_labels is not None:\n            shift_logits = lm_logits[:, :-1].contiguous()\n            shift_labels = lm_labels[:, 1:].contiguous()\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            losses.append(loss_fct(shift_logits.view(-1,\n                          shift_logits.size(-1)), shift_labels.view(-1)))\n        if mc_labels is not None:\n            loss_fct = CrossEntropyLoss()\n            losses.append(loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1)))\n        if losses:\n            return losses\n        return lm_logits, mc_logits, presents\n'"
examples/mnli_example/pytorch_pretrained_bert/modeling_openai.py,38,"b'# coding=utf-8\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch OpenAI GPT model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .file_utils import cached_path, CONFIG_NAME, WEIGHTS_NAME\nfrom .modeling import BertLayerNorm as LayerNorm\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {""openai-gpt"": ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin""}\nPRETRAINED_CONFIG_ARCHIVE_MAP = {""openai-gpt"": ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json""}\n\n\ndef load_tf_weights_in_openai_gpt(model, openai_checkpoint_folder_path):\n    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)\n    """"""\n    import re\n    import numpy as np\n    print(""Loading weights..."")\n    names = json.load(open(openai_checkpoint_folder_path + \'/parameters_names.json\', ""r"", encoding=\'utf-8\'))\n    shapes = json.load(open(openai_checkpoint_folder_path + \'/params_shapes.json\', ""r"", encoding=\'utf-8\'))\n    offsets = np.cumsum([np.prod(shape) for shape in shapes])\n    init_params = [np.load(openai_checkpoint_folder_path + \'/params_{}.npy\'.format(n)) for n in range(10)]\n    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\n    init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n\n    # This was used when we had a single embedding matrix for positions and tokens\n    # init_params[0] = np.concatenate([init_params[1], init_params[0]], 0)\n    # del init_params[1]\n    init_params = [arr.squeeze() for arr in init_params]\n\n    try:\n        assert model.tokens_embed.weight.shape == init_params[1].shape\n        assert model.positions_embed.weight.shape == init_params[0].shape\n    except AssertionError as e:\n        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)\n        e.args += (model.positions_embed.weight.shape, init_params[0].shape)\n        raise\n\n    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])\n    model.positions_embed.weight.data = torch.from_numpy(init_params[0])\n    names.pop(0)\n    # Pop position and token embedding arrays\n    init_params.pop(0)\n    init_params.pop(0)\n\n    for name, array in zip(names, init_params): # names[1:n_transfer], init_params[1:n_transfer]):\n        name = name[6:]  # skip ""model/""\n        assert name[-2:] == "":0""\n        name = name[:-2]\n        name = name.split(\'/\')\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+\\d+\', m_name):\n                l = re.split(r\'(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'g\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'b\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'w\':\n                pointer = getattr(pointer, \'weight\')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT_FNS = {""relu"": nn.ReLU, ""swish"": swish, ""gelu"": gelu}\n\n\nclass OpenAIGPTConfig(object):\n    """"""Configuration class to store the configuration of a `OpenAIGPTModel`.\n    """"""\n\n    def __init__(\n        self,\n        vocab_size_or_config_json_file=40478,\n        n_special=0,\n        n_positions=512,\n        n_ctx=512,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        afn=""gelu"",\n        resid_pdrop=0.1,\n        embd_pdrop=0.1,\n        attn_pdrop=0.1,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n    ):\n        """"""Constructs OpenAIGPTConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `OpenAIGPTModel` or a configuration json file.\n            n_special: The number of special tokens to learn during fine-tuning (\'[SEP]\', \'[CLF]\', ...)\n            n_positions: Number of positional embeddings.\n            n_ctx: Size of the causal mask (usually same as n_positions).\n            n_embd: Dimensionality of the embeddings and hidden states.\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            afn: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            resid_pdrop: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attn_pdrop: The dropout ratio for the attention\n                probabilities.\n            embd_pdrop: The dropout ratio for the embeddings.\n            layer_norm_epsilon: epsilon to use in the layer norm layers\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=""utf-8"") as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.n_special = n_special\n            self.n_ctx = n_ctx\n            self.n_positions = n_positions\n            self.n_embd = n_embd\n            self.n_layer = n_layer\n            self.n_head = n_head\n            self.afn = afn\n            self.resid_pdrop = resid_pdrop\n            self.embd_pdrop = embd_pdrop\n            self.attn_pdrop = attn_pdrop\n            self.layer_norm_epsilon = layer_norm_epsilon\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(\n                ""First argument must be either a vocabulary size (int)""\n                ""or the path to a pretrained model config file (str)""\n            )\n\n    @property\n    def total_tokens_embeddings(self):\n        return self.vocab_size + self.n_special\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `OpenAIGPTConfig` from a Python dictionary of parameters.""""""\n        config = OpenAIGPTConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `OpenAIGPTConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=""utf-8"") as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n\n\nclass Conv1D(nn.Module):\n    def __init__(self, nf, rf, nx):\n        super(Conv1D, self).__init__()\n        self.rf = rf\n        self.nf = nf\n        if rf == 1:  # faster 1x1 conv\n            w = torch.empty(nx, nf)\n            nn.init.normal_(w, std=0.02)\n            self.weight = Parameter(w)\n            self.bias = Parameter(torch.zeros(nf))\n        else:  # was used to train LM\n            raise NotImplementedError\n\n    def forward(self, x):\n        if self.rf == 1:\n            size_out = x.size()[:-1] + (self.nf,)\n            x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n            x = x.view(*size_out)\n        else:\n            raise NotImplementedError\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, nx, n_ctx, config, scale=False):\n        super(Attention, self).__init__()\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n        assert n_state % config.n_head == 0\n        self.register_buffer(""bias"", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.c_attn = Conv1D(n_state * 3, 1, nx)\n        self.c_proj = Conv1D(n_state, 1, nx)\n        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n\n    def _attn(self, q, k, v):\n        w = torch.matmul(q, k)\n        if self.scale:\n            w = w / math.sqrt(v.size(-1))\n        # w = w * self.bias + -1e9 * (1 - self.bias)  # TF implem method: mask_attn_weights\n        # XD: self.b may be larger than w, so we need to crop it\n        b = self.bias[:, :, : w.size(-2), : w.size(-1)]\n        w = w * b + -1e9 * (1 - b)\n\n        w = nn.Softmax(dim=-1)(w)\n        w = self.attn_dropout(w)\n        return torch.matmul(w, v)\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n\n    def split_heads(self, x, k=False):\n        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n        if k:\n            return x.permute(0, 2, 3, 1)\n        else:\n            return x.permute(0, 2, 1, 3)\n\n    def forward(self, x):\n        x = self.c_attn(x)\n        query, key, value = x.split(self.split_size, dim=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n        a = self._attn(query, key, value)\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n        a = self.resid_dropout(a)\n        return a\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n        super(MLP, self).__init__()\n        nx = config.n_embd\n        self.c_fc = Conv1D(n_state, 1, nx)\n        self.c_proj = Conv1D(nx, 1, n_state)\n        self.act = ACT_FNS[config.afn]\n        self.dropout = nn.Dropout(config.resid_pdrop)\n\n    def forward(self, x):\n        h = self.act(self.c_fc(x))\n        h2 = self.c_proj(h)\n        return self.dropout(h2)\n\n\nclass Block(nn.Module):\n    def __init__(self, n_ctx, config, scale=False):\n        super(Block, self).__init__()\n        nx = config.n_embd\n        self.attn = Attention(nx, n_ctx, config, scale)\n        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n        self.mlp = MLP(4 * nx, config)\n        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n\n    def forward(self, x):\n        a = self.attn(x)\n        n = self.ln_1(x + a)\n        m = self.mlp(n)\n        h = self.ln_2(n + m)\n        return h\n\n\nclass OpenAIGPTLMHead(nn.Module):\n    """""" Language Model Head for the transformer """"""\n\n    def __init__(self, model_embeddings_weights, config):\n        super(OpenAIGPTLMHead, self).__init__()\n        self.n_embd = config.n_embd\n        self.set_embeddings_weights(model_embeddings_weights)\n\n    def set_embeddings_weights(self, model_embeddings_weights):\n        embed_shape = model_embeddings_weights.shape\n        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n        self.decoder.weight = model_embeddings_weights  # Tied weights\n\n    def forward(self, hidden_state):\n        # Truncated Language modeling logits (we remove the last token)\n        # h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)\n        lm_logits = self.decoder(hidden_state)\n        return lm_logits\n\n\nclass OpenAIGPTMultipleChoiceHead(nn.Module):\n    """""" Classifier Head for the transformer """"""\n\n    def __init__(self, config):\n        super(OpenAIGPTMultipleChoiceHead, self).__init__()\n        self.n_embd = config.n_embd\n        # self.multiple_choice_token = multiple_choice_token\n        self.dropout = nn.Dropout2d(config.resid_pdrop)  # To reproduce the noise_shape parameter of TF implementation\n        self.linear = nn.Linear(config.n_embd, 1)\n\n        nn.init.normal_(self.linear.weight, std=0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, hidden_states, mc_token_ids):\n        # Classification logits\n        # hidden_state (bsz, num_choices, seq_length, hidden_size)\n        # mc_token_ids (bsz, num_choices)\n        mc_token_ids = mc_token_ids.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, hidden_states.size(-1))\n        # (bsz, num_choices, 1, hidden_size)\n        multiple_choice_h = hidden_states.gather(2, mc_token_ids).squeeze(2)\n        # (bsz, num_choices, hidden_size)\n        multiple_choice_h = self.dropout(multiple_choice_h.transpose(1, 2)).transpose(1, 2)\n        multiple_choice_logits = self.linear(multiple_choice_h).squeeze(-1)\n        # (bsz, num_choices)\n        return multiple_choice_logits\n\n\nclass OpenAIGPTPreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n\n    def __init__(self, config, *inputs, **kwargs):\n        super(OpenAIGPTPreTrainedModel, self).__init__()\n        if not isinstance(config, OpenAIGPTConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `OpenAIGPTConfig`. ""\n                ""To create a model from a pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                )\n            )\n        self.config = config\n\n    def init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def set_num_special_tokens(self, num_special_tokens):\n        pass\n\n    @classmethod\n    def from_pretrained(\n        cls, pretrained_model_name_or_path, num_special_tokens=None, state_dict=None, cache_dir=None, from_tf=False, *inputs, **kwargs\n    ):\n        """"""\n        Instantiate a OpenAIGPTPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `openai-gpt`\n                - a path or url to a pretrained model archive containing:\n                    . `openai_gpt_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a OpenAIGPTModel instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . a series of NumPy files containing OpenAI TensorFlow trained weights\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n            config_file = PRETRAINED_CONFIG_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} and {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path, "", "".join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()), pretrained_model_name_or_path,\n                    archive_file, config_file\n                )\n            )\n            return None\n        if resolved_archive_file == archive_file and resolved_config_file == config_file:\n            logger.info(""loading weights file {}"".format(archive_file))\n            logger.info(""loading configuration file {}"".format(config_file))\n        else:\n            logger.info(""loading weights file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n            logger.info(""loading configuration file {} from cache at {}"".format(\n                config_file, resolved_config_file))\n        # Load config\n        config = OpenAIGPTConfig.from_json_file(resolved_config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            state_dict = torch.load(resolved_archive_file, map_location=\'cpu\')\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint (stored as NumPy array)\n            return load_tf_weights_in_openai_gpt(model, resolved_archive_file)\n\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if key.endswith("".g""):\n                new_key = key[:-2] + "".weight""\n            elif key.endswith("".b""):\n                new_key = key[:-2] + "".bias""\n            elif key.endswith("".w""):\n                new_key = key[:-2] + "".weight""\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, ""_metadata"", None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=""""):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\n            )\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + ""."")\n\n        start_model = model\n        if hasattr(model, ""transformer"") and all(not s.startswith(\'transformer.\') for s in state_dict.keys()):\n            start_model = model.transformer\n        load(start_model, prefix="""")\n\n        if len(missing_keys) > 0:\n            logger.info(\n                ""Weights of {} not initialized from pretrained model: {}"".format(model.__class__.__name__, missing_keys)\n            )\n        if len(unexpected_keys) > 0:\n            logger.info(\n                ""Weights from pretrained model not used in {}: {}"".format(model.__class__.__name__, unexpected_keys)\n            )\n        if len(error_msgs) > 0:\n            raise RuntimeError(\n                ""Error(s) in loading state_dict for {}:\\n\\t{}"".format(model.__class__.__name__, ""\\n\\t"".join(error_msgs))\n            )\n\n        # Add additional embeddings for special tokens if needed\n        # This step also make sure we are still sharing the output and input embeddings after loading weights\n        model.set_num_special_tokens(num_special_tokens if num_special_tokens is not None else config.n_special)\n        return model\n\n\nclass OpenAIGPTModel(OpenAIGPTPreTrainedModel):\n    """"""OpenAI GPT model (""Improving Language Understanding by Generative Pre-Training"").\n\n    OpenAI GPT use a single embedding matrix to store the word and special embeddings.\n    Special tokens embeddings are additional tokens that are not pre-trained: [SEP], [CLS]...\n    Special tokens need to be trained during the fine-tuning if you use them.\n    The number of special embeddings can be controled using the `set_num_special_tokens(num_special_tokens)` function.\n\n    The embeddings are ordered as follow in the token embeddings matrice:\n        [0,                                                         ----------------------\n         ...                                                        -> word embeddings\n         config.vocab_size - 1,                                     ______________________\n         config.vocab_size,\n         ...                                                        -> special embeddings\n         config.vocab_size + config.n_special - 1]                  ______________________\n\n    where total_tokens_embeddings can be obtained as config.total_tokens_embeddings and is:\n        total_tokens_embeddings = config.vocab_size + config.n_special\n    You should use the associate indices to index the embeddings.\n\n    Params:\n        config: a OpenAIGPTConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] (or more generally [d_1, ..., d_n, sequence_length]\n            were d_1 ... d_n are arbitrary dimensions) with the word BPE token indices selected in the range [0, total_tokens_embeddings[\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n\n    Outputs:\n        `hidden_states`: the encoded-hidden-states at the top of the model\n            as a torch.FloatTensor of size [batch_size, sequence_length, hidden_size]\n            (or more generally [d_1, ..., d_n, hidden_size] were d_1 ... d_n are the dimension of input_ids)\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n\n    config = modeling_openai.OpenAIGPTConfig()\n\n    model = modeling_openai.OpenAIGPTModel(config)\n    hidden_states = model(input_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(OpenAIGPTModel, self).__init__(config)\n        num_tokens = config.vocab_size + config.n_special\n        self.tokens_embed = nn.Embedding(num_tokens, config.n_embd)\n        self.positions_embed = nn.Embedding(config.n_positions, config.n_embd)\n        self.drop = nn.Dropout(config.embd_pdrop)\n        block = Block(config.n_ctx, config, scale=True)\n        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n\n        self.apply(self.init_weights)\n        # nn.init.normal_(self.embed.weight, std=0.02)\n\n    def set_num_special_tokens(self, num_special_tokens):\n        "" Update input embeddings with new embedding matrice if needed ""\n        if self.config.n_special == num_special_tokens:\n            return\n        # Update config\n        self.config.n_special = num_special_tokens\n        # Build new embeddings and initialize all new embeddings (in particular the special tokens)\n        old_embed = self.tokens_embed\n        self.tokens_embed = nn.Embedding(self.config.total_tokens_embeddings, self.config.n_embd)\n        self.tokens_embed.to(old_embed.weight.device)\n        self.init_weights(self.tokens_embed)\n        # Copy word embeddings from the previous weights\n        self.tokens_embed.weight.data[:self.config.vocab_size, :] = old_embed.weight.data[:self.config.vocab_size, :]\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None):\n        if position_ids is None:\n            # This was used when we had a single embedding matrice from position and token embeddings\n            # start = self.config.vocab_size + self.config.n_special\n            # end = start + input_ids.size(-1)\n            # position_ids = torch.arange(start, end, dtype=torch.long, device=input_ids.device)\n            position_ids = torch.arange(input_ids.size(-1), dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        position_ids = position_ids.view(-1, position_ids.size(-1))\n\n        inputs_embeds = self.tokens_embed(input_ids)\n        position_embeds = self.positions_embed(position_ids)\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n            token_type_embeds = self.tokens_embed(token_type_ids)\n        else:\n            token_type_embeds = 0\n        # Add the position information to the input embeddings\n        # h = e.sum(dim=2)\n        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n        for block in self.h:\n            hidden_states = block(hidden_states)\n        output_shape = input_shape + (hidden_states.size(-1),)\n        return hidden_states.view(*output_shape)\n\n\nclass OpenAIGPTLMHeadModel(OpenAIGPTPreTrainedModel):\n    """"""OpenAI GPT model with a Language Modeling head (""Improving Language Understanding by Generative Pre-Training"").\n\n    OpenAI GPT use a single embedding matrix to store the word and special embeddings.\n    Special tokens embeddings are additional tokens that are not pre-trained: [SEP], [CLS]...\n    Special tokens need to be trained during the fine-tuning if you use them.\n    The number of special embeddings can be controled using the `set_num_special_tokens(num_special_tokens)` function.\n\n    The embeddings are ordered as follow in the token embeddings matrice:\n        [0,                                                         ----------------------\n         ...                                                        -> word embeddings\n         config.vocab_size - 1,                                     ______________________\n         config.vocab_size,\n         ...                                                        -> special embeddings\n         config.vocab_size + config.n_special - 1]                  ______________________\n\n    where total_tokens_embeddings can be obtained as config.total_tokens_embeddings and is:\n        total_tokens_embeddings = config.vocab_size + config.n_special\n    You should use the associate indices to index the embeddings.\n\n    Params:\n        config: a OpenAIGPTConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] (or more generally [d_1, ..., d_n, sequence_length]\n            were d_1 ... d_n are arbitrary dimensions) with the word BPE token indices selected in the range [0, total_tokens_embeddings[\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n        `lm_labels`: optional language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n\n    Outputs:\n        if `lm_labels` is not `None`:\n            Outputs the language modeling loss.\n        else:\n            `lm_logits`: the language modeling logits as a torch.FloatTensor of size [batch_size, sequence_length, total_tokens_embeddings]\n                (or more generally [d_1, ..., d_n, total_tokens_embeddings] were d_1 ... d_n are the dimension of input_ids)\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n\n    config = modeling_openai.OpenAIGPTConfig()\n\n    model = modeling_openai.OpenAIGPTLMHeadModel(config)\n    lm_logits = model(input_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(OpenAIGPTLMHeadModel, self).__init__(config)\n        self.transformer = OpenAIGPTModel(config)\n        self.lm_head = OpenAIGPTLMHead(self.transformer.tokens_embed.weight, config)\n        self.apply(self.init_weights)\n\n    def set_num_special_tokens(self, num_special_tokens):\n        """""" Update input and output embeddings with new embedding matrice\n            Make sure we are sharing the embeddings\n        """"""\n        self.transformer.set_num_special_tokens(num_special_tokens)\n        self.lm_head.set_embeddings_weights(self.transformer.tokens_embed.weight)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None):\n        hidden_states = self.transformer(input_ids, position_ids, token_type_ids)\n        lm_logits = self.lm_head(hidden_states)\n        if lm_labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = lm_logits[..., :-1, :].contiguous()\n            shift_labels = lm_labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n            return loss\n        return lm_logits\n\n\nclass OpenAIGPTDoubleHeadsModel(OpenAIGPTPreTrainedModel):\n    """"""OpenAI GPT model with a Language Modeling and a Multiple Choice head (""Improving Language Understanding by Generative Pre-Training"").\n\n    OpenAI GPT use a single embedding matrix to store the word and special embeddings.\n    Special tokens embeddings are additional tokens that are not pre-trained: [SEP], [CLS]...\n    Special tokens need to be trained during the fine-tuning if you use them.\n    The number of special embeddings can be controled using the `set_num_special_tokens(num_special_tokens)` function.\n\n    The embeddings are ordered as follow in the token embeddings matrice:\n        [0,                                                         ----------------------\n         ...                                                        -> word embeddings\n         config.vocab_size - 1,                                     ______________________\n         config.vocab_size,\n         ...                                                        -> special embeddings\n         config.vocab_size + config.n_special - 1]                  ______________________\n\n    where total_tokens_embeddings can be obtained as config.total_tokens_embeddings and is:\n        total_tokens_embeddings = config.vocab_size + config.n_special\n    You should use the associate indices to index the embeddings.\n\n    Params:\n        config: a OpenAIGPTConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length] with the BPE token\n            indices selected in the range [0, total_tokens_embeddings[\n        `mc_token_ids`: a torch.LongTensor of shape [batch_size, num_choices] with the index of the token from\n            which we should take the hidden state to feed the multiple choice classifier (usually last token of the sequence)\n        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n            with the position indices (selected in the range [0, config.n_positions - 1[.\n        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n            You can use it to add a third type of embedding to each input token in the sequence\n            (the previous two being the word and position embeddings).\n            The input, position and token_type embeddings are summed inside the Transformer before the first\n            self-attention block.\n        `lm_labels`: optional language modeling labels: torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with indices selected in [-1, 0, ..., total_tokens_embeddings]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., total_tokens_embeddings]\n        `multiple_choice_labels`: optional multiple choice labels: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n\n    Outputs:\n        if `lm_labels` and `multiple_choice_labels` are not `None`:\n            Outputs a tuple of losses with the language modeling loss and the multiple choice loss.\n        else: a tuple with\n            `lm_logits`: the language modeling logits as a torch.FloatTensor of size [batch_size, num_choices, sequence_length, total_tokens_embeddings]\n            `multiple_choice_logits`: the multiple choice logits as a torch.FloatTensor of size [batch_size, num_choices]\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]]])  # (bsz, number of choice, seq length)\n    mc_token_ids = torch.LongTensor([[2], [1]]) # (bsz, number of choice)\n\n    config = modeling_openai.OpenAIGPTConfig()\n\n    model = modeling_openai.OpenAIGPTLMHeadModel(config)\n    lm_logits, multiple_choice_logits = model(input_ids, mc_token_ids)\n    ```\n    """"""\n\n    def __init__(self, config):\n        super(OpenAIGPTDoubleHeadsModel, self).__init__(config)\n        self.transformer = OpenAIGPTModel(config)\n        self.lm_head = OpenAIGPTLMHead(self.transformer.tokens_embed.weight, config)\n        self.multiple_choice_head = OpenAIGPTMultipleChoiceHead(config)\n        self.apply(self.init_weights)\n\n    def set_num_special_tokens(self, num_special_tokens):\n        """""" Update input and output embeddings with new embedding matrice\n            Make sure we are sharing the embeddings\n        """"""\n        self.transformer.set_num_special_tokens(num_special_tokens)\n        self.lm_head.set_embeddings_weights(self.transformer.tokens_embed.weight)\n\n    def forward(self, input_ids, mc_token_ids, lm_labels=None, mc_labels=None, token_type_ids=None, position_ids=None):\n        hidden_states = self.transformer(input_ids, position_ids, token_type_ids)\n        lm_logits = self.lm_head(hidden_states)\n        mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids)\n        losses = []\n        if lm_labels is not None:\n            shift_logits = lm_logits[..., :-1, :].contiguous()\n            shift_labels = lm_labels[..., 1:].contiguous()\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            losses.append(loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)))\n        if mc_labels is not None:\n            loss_fct = CrossEntropyLoss()\n            losses.append(loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1)))\n        if losses:\n            return losses\n        return lm_logits, mc_logits\n'"
examples/mnli_example/pytorch_pretrained_bert/modeling_transfo_xl.py,70,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" PyTorch Transformer XL model.\n    Adapted from https://github.com/kimiyoung/transformer-xl.\n    In particular https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/mem_transformer.py\n""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport copy\nimport json\nimport math\nimport logging\nimport tarfile\nimport tempfile\nimport shutil\nimport collections\nimport sys\nfrom io import open\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .modeling import BertLayerNorm as LayerNorm\nfrom .modeling_transfo_xl_utilities import ProjectedAdaptiveLogSoftmax, sample_logits\nfrom .file_utils import cached_path, CONFIG_NAME, WEIGHTS_NAME\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin"",\n}\nPRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-config.json"",\n}\n\nTF_WEIGHTS_NAME = \'model.ckpt\'\n\ndef build_tf_to_pytorch_map(model, config):\n    """""" A map of modules from TF to PyTorch.\n        This time I use a map to keep the PyTorch model as identical to the original PyTorch model as possible.\n    """"""\n    tf_to_pt_map = {}\n\n    if hasattr(model, \'transformer\'):\n        # We are loading in a TransfoXLLMHeadModel => we will load also the Adaptive Softmax\n        tf_to_pt_map.update({\n            ""transformer/adaptive_softmax/cutoff_0/cluster_W"": model.crit.cluster_weight,\n            ""transformer/adaptive_softmax/cutoff_0/cluster_b"": model.crit.cluster_bias})\n        for i, (out_l, proj_l, tie_proj) in enumerate(zip(\n                                model.crit.out_layers,\n                                model.crit.out_projs,\n                                config.tie_projs)):\n            layer_str = ""transformer/adaptive_softmax/cutoff_%d/"" % i\n            if config.tie_weight:\n                tf_to_pt_map.update({\n                    layer_str + \'b\': out_l.bias})\n            else:\n                raise NotImplementedError\n                # I don\'t think this is implemented in the TF code\n                tf_to_pt_map.update({\n                    layer_str + \'lookup_table\': out_l.weight,\n                    layer_str + \'b\': out_l.bias})\n            if not tie_proj:\n                tf_to_pt_map.update({\n                    layer_str + \'proj\': proj_l\n                    })\n        # Now load the rest of the transformer\n        model = model.transformer\n\n    # Embeddings\n    for i, (embed_l, proj_l) in enumerate(zip(model.word_emb.emb_layers, model.word_emb.emb_projs)):\n        layer_str = ""transformer/adaptive_embed/cutoff_%d/"" % i\n        tf_to_pt_map.update({\n            layer_str + \'lookup_table\': embed_l.weight,\n            layer_str + \'proj_W\': proj_l\n            })\n\n    # Transformer blocks\n    for i, b in enumerate(model.layers):\n        layer_str = ""transformer/layer_%d/"" % i\n        tf_to_pt_map.update({\n            layer_str + ""rel_attn/LayerNorm/gamma"": b.dec_attn.layer_norm.weight,\n            layer_str + ""rel_attn/LayerNorm/beta"": b.dec_attn.layer_norm.bias,\n            layer_str + ""rel_attn/o/kernel"": b.dec_attn.o_net.weight,\n            layer_str + ""rel_attn/qkv/kernel"": b.dec_attn.qkv_net.weight,\n            layer_str + ""rel_attn/r/kernel"": b.dec_attn.r_net.weight,\n            layer_str + ""ff/LayerNorm/gamma"": b.pos_ff.layer_norm.weight,\n            layer_str + ""ff/LayerNorm/beta"": b.pos_ff.layer_norm.bias,\n            layer_str + ""ff/layer_1/kernel"": b.pos_ff.CoreNet[0].weight,\n            layer_str + ""ff/layer_1/bias"": b.pos_ff.CoreNet[0].bias,\n            layer_str + ""ff/layer_2/kernel"": b.pos_ff.CoreNet[3].weight,\n            layer_str + ""ff/layer_2/bias"": b.pos_ff.CoreNet[3].bias,\n        })\n\n    # Relative positioning biases\n    if config.untie_r:\n        r_r_list = []\n        r_w_list = []\n        for b in model.layers:\n            r_r_list.append(b.dec_attn.r_r_bias)\n            r_w_list.append(b.dec_attn.r_w_bias)\n    else:\n        r_r_list = [model.r_r_bias]\n        r_w_list = [model.r_w_bias]\n    tf_to_pt_map.update({\n        \'transformer/r_r_bias\': r_r_list,\n        \'transformer/r_w_bias\': r_w_list})\n    return tf_to_pt_map\n\ndef load_tf_weights_in_transfo_xl(model, config, tf_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    # Build TF to PyTorch weights loading map\n    tf_to_pt_map = build_tf_to_pytorch_map(model, config)\n\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    tf_weights = {}\n    for name, shape in init_vars:\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        tf_weights[name] = array\n\n    for name, pointer in tf_to_pt_map.items():\n        assert name in tf_weights\n        array = tf_weights[name]\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if \'kernel\' in name or \'proj\' in name:\n            array = np.transpose(array)\n        if (\'r_r_bias\' in name or \'r_w_bias\' in name) and len(pointer) > 1:\n            # Here we will split the TF weigths\n            assert len(pointer) == array.shape[0]\n            for i, p_i in enumerate(pointer):\n                arr_i = array[i, ...]\n                try:\n                    assert p_i.shape == arr_i.shape\n                except AssertionError as e:\n                    e.args += (p_i.shape, arr_i.shape)\n                    raise\n                print(""Initialize PyTorch weight {} for layer {}"".format(name, i))\n                p_i.data = torch.from_numpy(arr_i)\n        else:\n            try:\n                assert pointer.shape == array.shape\n            except AssertionError as e:\n                e.args += (pointer.shape, array.shape)\n                raise\n            print(""Initialize PyTorch weight {}"".format(name))\n            pointer.data = torch.from_numpy(array)\n        tf_weights.pop(name, None)\n        tf_weights.pop(name + \'/Adam\', None)\n        tf_weights.pop(name + \'/Adam_1\', None)\n\n    print(""Weights not copied to PyTorch model: {}"".format(\', \'.join(tf_weights.keys())))\n    return model\n\n\nclass TransfoXLConfig(object):\n    """"""Configuration class to store the configuration of a `TransfoXLModel`.\n    """"""\n    def __init__(self,\n                 vocab_size_or_config_json_file=267735,\n                 cutoffs=[20000, 40000, 200000],\n                 d_model=1024,\n                 d_embed=1024,\n                 n_head=16,\n                 d_head=64,\n                 d_inner=4096,\n                 div_val=4,\n                 pre_lnorm=False,\n                 n_layer=18,\n                 tgt_len=128,\n                 ext_len=0,\n                 mem_len=1600,\n                 clamp_len=1000,\n                 same_length=True,\n                 proj_share_all_but_first=True,\n                 attn_type=0,\n                 sample_softmax=-1,\n                 adaptive=True,\n                 tie_weight=True,\n                 dropout=0.1,\n                 dropatt=0.0,\n                 untie_r=True,\n                 init=""normal"",\n                 init_range=0.01,\n                 proj_init_std=0.01,\n                 init_std=0.02):\n        """"""Constructs TransfoXLConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `TransfoXLModel` or a configuration json file.\n            cutoffs: cutoffs for the adaptive softmax\n            d_model: Dimensionality of the model\'s hidden states.\n            d_embed: Dimensionality of the embeddings\n            d_head: Dimensionality of the model\'s heads.\n            div_val: divident value for adapative input and softmax\n            pre_lnorm: apply LayerNorm to the input instead of the output\n            d_inner: Inner dimension in FF\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            tgt_len: number of tokens to predict\n            ext_len: length of the extended context\n            mem_len: length of the retained previous heads\n            same_length: use the same attn length for all tokens\n            proj_share_all_but_first: True to share all but first projs, False not to share.\n            attn_type: attention type. 0 for Transformer-XL, 1 for Shaw et al, 2 for Vaswani et al, 3 for Al Rfou et al.\n            clamp_len: use the same pos embeddings after clamp_len\n            sample_softmax: number of samples in sampled softmax\n            adaptive: use adaptive softmax\n            tie_weight: tie the word embedding and softmax weights\n            dropout: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            dropatt: The dropout ratio for the attention probabilities.\n            untie_r: untie relative position biases           \n            embd_pdrop: The dropout ratio for the embeddings.\n            init: parameter initializer to use\n            init_range: parameters initialized by U(-init_range, init_range).\n            proj_init_std: parameters initialized by N(0, init_std)\n            init_std: parameters initialized by N(0, init_std)\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.n_token = vocab_size_or_config_json_file\n            self.cutoffs = []\n            self.cutoffs.extend(cutoffs)\n            self.tie_weight = tie_weight\n            if proj_share_all_but_first:\n                self.tie_projs = [False] + [True] * len(self.cutoffs)\n            else:\n                self.tie_projs = [False] + [False] * len(self.cutoffs)\n            self.d_model = d_model\n            self.d_embed = d_embed\n            self.d_head = d_head\n            self.d_inner = d_inner\n            self.div_val = div_val\n            self.pre_lnorm = pre_lnorm\n            self.n_layer = n_layer\n            self.n_head = n_head\n            self.tgt_len = tgt_len\n            self.ext_len = ext_len\n            self.mem_len = mem_len\n            self.same_length = same_length\n            self.attn_type = attn_type\n            self.clamp_len = clamp_len\n            self.sample_softmax = sample_softmax\n            self.adaptive = adaptive\n            self.dropout = dropout\n            self.dropatt = dropatt\n            self.untie_r = untie_r\n            self.init = init\n            self.init_range = init_range\n            self.proj_init_std = proj_init_std\n            self.init_std = init_std\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             ""or the path to a pretrained model config file (str)"")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `TransfoXLConfig` from a Python dictionary of parameters.""""""\n        config = TransfoXLConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `TransfoXLConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, demb):\n        super(PositionalEmbedding, self).__init__()\n\n        self.demb = demb\n\n        inv_freq = 1 / (10000 ** (torch.arange(0.0, demb, 2.0) / demb))\n        self.register_buffer(\'inv_freq\', inv_freq)\n\n    def forward(self, pos_seq, bsz=None):\n        sinusoid_inp = torch.ger(pos_seq, self.inv_freq)\n        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n\n        if bsz is not None:\n            return pos_emb[:,None,:].expand(-1, bsz, -1)\n        else:\n            return pos_emb[:,None,:]\n\n\nclass PositionwiseFF(nn.Module):\n    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False):\n        super(PositionwiseFF, self).__init__()\n\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n\n        self.CoreNet = nn.Sequential(\n            nn.Linear(d_model, d_inner), nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(d_inner, d_model),\n            nn.Dropout(dropout),\n        )\n\n        self.layer_norm = LayerNorm(d_model)\n\n        self.pre_lnorm = pre_lnorm\n\n    def forward(self, inp):\n        if self.pre_lnorm:\n            ##### layer normalization + positionwise feed-forward\n            core_out = self.CoreNet(self.layer_norm(inp))\n\n            ##### residual connection\n            output = core_out + inp\n        else:\n            ##### positionwise feed-forward\n            core_out = self.CoreNet(inp)\n\n            ##### residual connection + layer normalization\n            output = self.layer_norm(inp + core_out)\n\n        return output\n\nclass MultiHeadAttn(nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0, \n                 pre_lnorm=False, r_r_bias=None, r_w_bias=None):\n        super(MultiHeadAttn, self).__init__()\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.q_net = nn.Linear(d_model, n_head * d_head, bias=False)\n        self.kv_net = nn.Linear(d_model, 2 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = LayerNorm(d_model)\n\n        self.scale = 1 / (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n        if r_r_bias is None or r_w_bias is None: # Biases are not shared\n            self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n            self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n        else:\n            self.r_r_bias = r_r_bias\n            self.r_w_bias = r_w_bias\n\n    def forward(self, h, attn_mask=None, mems=None):\n        ##### multihead attention\n        # [hlen x bsz x n_head x d_head]\n\n        if mems is not None:\n            c = torch.cat([mems, h], 0)\n        else:\n            c = h\n\n        if self.pre_lnorm:\n            ##### layer normalization\n            c = self.layer_norm(c)\n\n        head_q = self.q_net(h)\n        head_k, head_v = torch.chunk(self.kv_net(c), 2, -1)\n\n        head_q = head_q.view(h.size(0), h.size(1), self.n_head, self.d_head)\n        head_k = head_k.view(c.size(0), c.size(1), self.n_head, self.d_head)\n        head_v = head_v.view(c.size(0), c.size(1), self.n_head, self.d_head)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = torch.einsum(\'ibnd,jbnd->ijbn\', (head_q, head_k))\n        attn_score.mul_(self.scale)\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score.masked_fill_(attn_mask[None,:,:,None], -float(\'inf\'))\n            elif attn_mask.dim() == 3:\n                attn_score.masked_fill_(attn_mask[:,:,:,None], -float(\'inf\'))\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        # [qlen x klen x bsz x n_head] + [klen x bsz x n_head x d_head] -> [qlen x bsz x n_head x d_head]\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', (attn_prob, head_v))\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = h + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(h + attn_out)\n\n        return output\n\nclass RelMultiHeadAttn(nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n                 tgt_len=None, ext_len=None, mem_len=None, pre_lnorm=False,\n                 r_r_bias=None, r_w_bias=None):\n        super(RelMultiHeadAttn, self).__init__()\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = LayerNorm(d_model)\n\n        self.scale = 1 / (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n        if r_r_bias is None or r_w_bias is None: # Biases are not shared\n            self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n            self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n        else:\n            self.r_r_bias = r_r_bias\n            self.r_w_bias = r_w_bias\n\n    def _parallelogram_mask(self, h, w, left=False):\n        mask = torch.ones((h, w)).byte()\n        m = min(h, w)\n        mask[:m,:m] = torch.triu(mask[:m,:m])\n        mask[-m:,-m:] = torch.tril(mask[-m:,-m:])\n\n        if left:\n            return mask\n        else:\n            return mask.flip(0)\n\n    def _shift(self, x, qlen, klen, mask, left=False):\n        if qlen > 1:\n            zero_pad = torch.zeros((x.size(0), qlen-1, x.size(2), x.size(3)),\n                                    device=x.device, dtype=x.dtype)\n        else:\n            zero_pad = torch.zeros(0, device=x.device, dtype=x.dtype)\n\n        if left:\n            mask = mask.flip(1)\n            x_padded = torch.cat([zero_pad, x], dim=1).expand(qlen, -1, -1, -1)\n        else:\n            x_padded = torch.cat([x, zero_pad], dim=1).expand(qlen, -1, -1, -1)\n\n        x = x_padded.masked_select(mask[:,:,None,None]) \\\n                    .view(qlen, klen, x.size(2), x.size(3))\n\n        return x\n\n    def _rel_shift(self, x, zero_triu=False):\n        zero_pad_shape = (x.size(0), 1) + x.size()[2:]\n        zero_pad = torch.zeros(zero_pad_shape, device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=1)\n\n        x_padded_shape = (x.size(1) + 1, x.size(0)) + x.size()[2:]\n        x_padded = x_padded.view(*x_padded_shape)\n\n        x = x_padded[1:].view_as(x)\n\n        if zero_triu:\n            ones = torch.ones((x.size(0), x.size(1)))\n            x = x * torch.tril(ones, x.size(1) - x.size(0))[:,:,None,None]\n\n        return x\n\n    def forward(self, w, r, attn_mask=None, mems=None):\n        raise NotImplementedError\n\nclass RelPartialLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelPartialLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n        self.r_net = nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)\n\n    def forward(self, w, r, attn_mask=None, mems=None):\n        qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n\n        r_head_k = r_head_k.view(rlen, self.n_head, self.d_head)                # qlen x n_head x d_head\n\n        #### compute attention score\n        rw_head_q = w_head_q + self.r_w_bias                                    # qlen x bsz x n_head x d_head\n        AC = torch.einsum(\'ibnd,jbnd->ijbn\', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n\n        rr_head_q = w_head_q + self.r_r_bias\n        BD = torch.einsum(\'ibnd,jnd->ijbn\', (rr_head_q, r_head_k))              # qlen x klen x bsz x n_head\n        BD = self._rel_shift(BD)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[None,:,:,None], -1e30).type_as(attn_score)\n            elif attn_mask.dim() == 3:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[:,:,:,None], -1e30).type_as(attn_score)\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output\n\nclass RelLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n    def forward(self, w, r_emb, r_w_bias, r_bias, attn_mask=None, mems=None):\n        # r_emb: [klen, n_head, d_head], used for term B\n        # r_w_bias: [n_head, d_head], used for term C\n        # r_bias: [klen, n_head], used for term D\n\n        qlen, bsz = w.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)\n\n        if klen > r_emb.size(0):\n            r_emb_pad = r_emb[0:1].expand(klen-r_emb.size(0), -1, -1)\n            r_emb = torch.cat([r_emb_pad, r_emb], 0)\n            r_bias_pad = r_bias[0:1].expand(klen-r_bias.size(0), -1)\n            r_bias = torch.cat([r_bias_pad, r_bias], 0)\n        else:\n            r_emb = r_emb[-klen:]\n            r_bias = r_bias[-klen:]\n\n        #### compute attention score\n        rw_head_q = w_head_q + r_w_bias[None]                                   # qlen x bsz x n_head x d_head\n\n        AC = torch.einsum(\'ibnd,jbnd->ijbn\', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n        B_ = torch.einsum(\'ibnd,jnd->ijbn\', (w_head_q, r_emb))                  # qlen x klen x bsz x n_head\n        D_ = r_bias[None, :, None]                                              # 1    x klen x 1   x n_head\n        BD = self._rel_shift(B_ + D_)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score.masked_fill_(attn_mask[None,:,:,None], -float(\'inf\'))\n            elif attn_mask.dim() == 3:\n                attn_score.masked_fill_(attn_mask[:,:,:,None], -float(\'inf\'))\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout, **kwargs):\n        super(DecoderLayer, self).__init__()\n\n        self.dec_attn = MultiHeadAttn(n_head, d_model, d_head, dropout, **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get(\'pre_lnorm\'))\n\n    def forward(self, dec_inp, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\nclass RelLearnableDecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelLearnableMultiHeadAttn(n_head, d_model, d_head, dropout,\n                                         **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get(\'pre_lnorm\'))\n\n    def forward(self, dec_inp, r_emb, r_w_bias, r_bias, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r_emb, r_w_bias, r_bias,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\nclass RelPartialLearnableDecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelPartialLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelPartialLearnableMultiHeadAttn(n_head, d_model,\n                            d_head, dropout, **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get(\'pre_lnorm\'))\n\n    def forward(self, dec_inp, r, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\n\nclass AdaptiveEmbedding(nn.Module):\n    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, \n                 sample_softmax=False):\n        super(AdaptiveEmbedding, self).__init__()\n\n        self.n_token = n_token\n        self.d_embed = d_embed\n\n        self.cutoffs = cutoffs + [n_token]\n        self.div_val = div_val\n        self.d_proj = d_proj\n\n        self.emb_scale = d_proj ** 0.5\n\n        self.cutoff_ends = [0] + self.cutoffs\n\n        self.emb_layers = nn.ModuleList()\n        self.emb_projs = nn.ParameterList()\n        if div_val == 1:\n            self.emb_layers.append(\n                nn.Embedding(n_token, d_embed, sparse=sample_softmax>0)\n            )\n            if d_proj != d_embed:\n                self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_embed)))\n        else:\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]\n                d_emb_i = d_embed // (div_val ** i)\n                self.emb_layers.append(nn.Embedding(r_idx-l_idx, d_emb_i))\n                self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_emb_i)))\n\n    def forward(self, inp):\n        if self.div_val == 1:\n            embed = self.emb_layers[0](inp)\n            if self.d_proj != self.d_embed:\n                embed  = F.linear(embed, self.emb_projs[0])\n        else:\n            param = next(self.parameters())\n            inp_flat = inp.view(-1)\n            emb_flat = torch.zeros([inp_flat.size(0), self.d_proj], \n                dtype=param.dtype, device=param.device)\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n\n                mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n\n                if indices_i.numel() == 0:\n                    continue\n\n                inp_i = inp_flat.index_select(0, indices_i) - l_idx\n                emb_i = self.emb_layers[i](inp_i)\n                emb_i = F.linear(emb_i, self.emb_projs[i])\n\n                emb_flat.index_copy_(0, indices_i, emb_i)\n\n            embed_shape = inp.size() + (self.d_proj,)\n            embed = emb_flat.view(embed_shape)\n\n        embed.mul_(self.emb_scale)\n\n        return embed\n\n\nclass TransfoXLPreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    def __init__(self, config, *inputs, **kwargs):\n        super(TransfoXLPreTrainedModel, self).__init__()\n        if not isinstance(config, TransfoXLConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `TransfoXLConfig`. ""\n                ""To create a model from a pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        self.config = config\n\n    def init_weight(self, weight):\n        if self.config.init == \'uniform\':\n            nn.init.uniform_(weight, -self.config.init_range, self.config.init_range)\n        elif self.config.init == \'normal\':\n            nn.init.normal_(weight, 0.0, self.config.init_std)\n\n    def init_bias(self, bias):\n        nn.init.constant_(bias, 0.0)\n\n    def init_weights(self, m):\n        """""" Initialize the weights.\n        """"""\n        classname = m.__class__.__name__\n        if classname.find(\'Linear\') != -1:\n            if hasattr(m, \'weight\') and m.weight is not None:\n                self.init_weight(m.weight)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                self.init_bias(m.bias)\n        elif classname.find(\'AdaptiveEmbedding\') != -1:\n            if hasattr(m, \'emb_projs\'):\n                for i in range(len(m.emb_projs)):\n                    if m.emb_projs[i] is not None:\n                        nn.init.normal_(m.emb_projs[i], 0.0, self.config.proj_init_std)\n        elif classname.find(\'Embedding\') != -1:\n            if hasattr(m, \'weight\'):\n                self.init_weight(m.weight)\n        elif classname.find(\'ProjectedAdaptiveLogSoftmax\') != -1:\n            if hasattr(m, \'cluster_weight\') and m.cluster_weight is not None:\n                self.init_weight(m.cluster_weight)\n            if hasattr(m, \'cluster_bias\') and m.cluster_bias is not None:\n                self.init_bias(m.cluster_bias)\n            if hasattr(m, \'out_projs\'):\n                for i in range(len(m.out_projs)):\n                    if m.out_projs[i] is not None:\n                        nn.init.normal_(m.out_projs[i], 0.0, self.config.proj_init_std)\n        elif classname.find(\'LayerNorm\') != -1:\n            if hasattr(m, \'weight\'):\n                nn.init.normal_(m.weight, 1.0, self.config.init_std)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                self.init_bias(m.bias)\n        elif classname.find(\'TransformerLM\') != -1:\n            if hasattr(m, \'r_emb\'):\n                self.init_weight(m.r_emb)\n            if hasattr(m, \'r_w_bias\'):\n                self.init_weight(m.r_w_bias)\n            if hasattr(m, \'r_r_bias\'):\n                self.init_weight(m.r_r_bias)\n            if hasattr(m, \'r_bias\'):\n                self.init_bias(m.r_bias)\n\n    def set_num_special_tokens(self, num_special_tokens):\n        pass\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None,\n                        from_tf=False, *inputs, **kwargs):\n        """"""\n        Instantiate a TransfoXLPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `transfo-xl`\n                - a path or url to a pretrained model archive containing:\n                    . `transfo_xl_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a TransfoXLModel instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n            config_file = PRETRAINED_CONFIG_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} and {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n                    pretrained_model_name_or_path,\n                    archive_file, config_file))\n            return None\n        if resolved_archive_file == archive_file and resolved_config_file == config_file:\n            logger.info(""loading weights file {}"".format(archive_file))\n            logger.info(""loading configuration file {}"".format(config_file))\n        else:\n            logger.info(""loading weights file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n            logger.info(""loading configuration file {} from cache at {}"".format(\n                config_file, resolved_config_file))\n        # Load config\n        config = TransfoXLConfig.from_json_file(resolved_config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            state_dict = torch.load(resolved_archive_file, map_location=\'cpu\')\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint\n            return load_tf_weights_in_transfo_xl(model, config, pretrained_model_name_or_path)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \'_metadata\', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=\'\'):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + \'.\')\n\n        start_prefix = \'\'\n        if not hasattr(model, \'transformer\') and any(s.startswith(\'transformer.\') for s in state_dict.keys()):\n            start_prefix = \'transformer.\'\n        load(model, prefix=start_prefix)\n\n        if len(missing_keys) > 0:\n            logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(""Weights from pretrained model not used in {}: {}"".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) > 0:\n            raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\n                               model.__class__.__name__, ""\\n\\t"".join(error_msgs)))\n        # Make sure we are still sharing the input and output embeddings\n        if hasattr(model, \'tie_weights\'):\n            model.tie_weights()\n        return model\n\n\nclass TransfoXLModel(TransfoXLPreTrainedModel):\n    """"""Transformer XL model (""Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"").\n\n    Transformer XL use a relative positioning (with sinusiodal patterns) and adaptive softmax inputs which means that:\n    - you don\'t need to specify positioning embeddings indices\n    - the tokens in the vocabulary have to be sorted to decreasing frequency.\n\n    Params:\n        config: a TransfoXLConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the token indices selected in the range [0, self.config.n_token[\n        `mems`: optional memomry of hidden states from previous forward passes\n            as a list (num layers) of hidden states at the entry of each layer\n            each hidden states has shape [self.config.mem_len, bsz, self.config.d_model]\n            Note that the first two dimensions are transposed in `mems` with regards to `input_ids` and `target`\n    Outputs:\n        A tuple of (last_hidden_state, new_mems)\n        `last_hidden_state`: the encoded-hidden-states at the top of the model\n            as a torch.FloatTensor of size [batch_size, sequence_length, self.config.d_model]\n        `new_mems`: list (num layers) of updated mem states at the entry of each layer\n            each mem state is a torch.FloatTensor of size [self.config.mem_len, batch_size, self.config.d_model]\n            Note that the first two dimensions are transposed in `mems` with regards to `input_ids` and `target`\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_ids_next = torch.LongTensor([[53, 21, 1], [64, 23, 100]])\n\n    config = TransfoXLConfig()\n\n    model = TransfoXLModel(config)\n    last_hidden_state, new_mems = model(input_ids)\n\n    # Another time on input_ids_next using the memory:\n    last_hidden_state, new_mems = model(input_ids_next, new_mems)\n    ```\n    """"""\n    def __init__(self, config):\n        super(TransfoXLModel, self).__init__(config)\n        self.n_token = config.n_token\n\n        self.d_embed = config.d_embed\n        self.d_model = config.d_model\n        self.n_head = config.n_head\n        self.d_head = config.d_head\n\n        self.word_emb = AdaptiveEmbedding(config.n_token, config.d_embed, config.d_model, config.cutoffs, \n                                          div_val=config.div_val)\n\n        self.drop = nn.Dropout(config.dropout)\n\n        self.n_layer = config.n_layer\n\n        self.tgt_len = config.tgt_len\n        self.mem_len = config.mem_len\n        self.ext_len = config.ext_len\n        self.max_klen = config.tgt_len + config.ext_len + config.mem_len\n\n        self.attn_type = config.attn_type\n\n        if not config.untie_r:\n            self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n            self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n\n        self.layers = nn.ModuleList()\n        if config.attn_type == 0: # the default attention\n            for i in range(config.n_layer):\n                self.layers.append(\n                    RelPartialLearnableDecoderLayer(\n                        config.n_head, config.d_model, config.d_head, config.d_inner, config.dropout,\n                        tgt_len=config.tgt_len, ext_len=config.ext_len, mem_len=config.mem_len,\n                        dropatt=config.dropatt, pre_lnorm=config.pre_lnorm,\n                        r_w_bias=None if config.untie_r else self.r_w_bias,\n                        r_r_bias=None if config.untie_r else self.r_r_bias)\n                )\n        elif config.attn_type == 1: # learnable embeddings\n            for i in range(config.n_layer):\n                self.layers.append(\n                    RelLearnableDecoderLayer(\n                        config.n_head, config.d_model, config.d_head, config.d_inner, config.dropout,\n                        tgt_len=config.tgt_len, ext_len=config.ext_len, mem_len=config.mem_len,\n                        dropatt=config.dropatt, pre_lnorm=config.pre_lnorm,\n                        r_w_bias=None if config.untie_r else self.r_w_bias,\n                        r_r_bias=None if config.untie_r else self.r_r_bias)\n                )\n        elif config.attn_type in [2, 3]: # absolute embeddings\n            for i in range(config.n_layer):\n                self.layers.append(\n                    DecoderLayer(\n                        config.n_head, config.d_model, config.d_head, config.d_inner, config.dropout,\n                        dropatt=config.dropatt, pre_lnorm=config.pre_lnorm,\n                        r_w_bias=None if config.untie_r else self.r_w_bias,\n                        r_r_bias=None if config.untie_r else self.r_r_bias)\n                )\n\n        self.same_length = config.same_length\n        self.clamp_len = config.clamp_len\n\n        if self.attn_type == 0: # default attention\n            self.pos_emb = PositionalEmbedding(self.d_model)\n        elif self.attn_type == 1: # learnable\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n            self.r_bias = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head))\n        elif self.attn_type == 2: # absolute standard\n            self.pos_emb = PositionalEmbedding(self.d_model)\n        elif self.attn_type == 3: # absolute deeper SA\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n        self.apply(self.init_weights)\n\n    def backward_compatible(self):\n        self.sample_softmax = -1\n\n\n    def reset_length(self, tgt_len, ext_len, mem_len):\n        self.tgt_len = tgt_len\n        self.mem_len = mem_len\n        self.ext_len = ext_len\n\n    def init_mems(self, data):\n        if self.mem_len > 0:\n            mems = []\n            param = next(self.parameters())\n            for i in range(self.n_layer):\n                empty = torch.zeros(self.mem_len, data.size(1), self.config.d_model,\n                                    dtype=param.dtype, device=param.device)\n                mems.append(empty)\n\n            return mems\n        else:\n            return None\n\n    def _update_mems(self, hids, mems, qlen, mlen):\n        # does not deal with None\n        if mems is None: return None\n\n        # mems is not None\n        assert len(hids) == len(mems), \'len(hids) != len(mems)\'\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        # For the next step, the last `ext_len` of the `qlen` tokens\n        # will be used as the extended context. Hence, we only cache\n        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n        # to `mlen + qlen - self.ext_len`.\n        with torch.no_grad():\n            new_mems = []\n            end_idx = mlen + max(0, qlen - 0 - self.ext_len)\n            beg_idx = max(0, end_idx - self.mem_len)\n            for i in range(len(hids)):\n\n                cat = torch.cat([mems[i], hids[i]], dim=0)\n                new_mems.append(cat[beg_idx:end_idx].detach())\n\n        return new_mems\n\n    def _forward(self, dec_inp, mems=None):\n        qlen, bsz = dec_inp.size()\n\n        word_emb = self.word_emb(dec_inp)\n\n        mlen = mems[0].size(0) if mems is not None else 0\n        klen = mlen + qlen\n        if self.same_length:\n            all_ones = word_emb.new_ones(qlen, klen)\n            mask_len = klen - self.mem_len\n            if mask_len > 0:\n                mask_shift_len = qlen - mask_len\n            else:\n                mask_shift_len = qlen\n            dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n                    + torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1\n        else:\n            dec_attn_mask = torch.triu(\n                word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None]\n\n        hids = []\n        if self.attn_type == 0: # default\n            pos_seq = torch.arange(klen-1, -1, -1.0, device=word_emb.device, \n                                   dtype=word_emb.dtype)\n            if self.clamp_len > 0:\n                pos_seq.clamp_(max=self.clamp_len)\n            pos_emb = self.pos_emb(pos_seq)\n\n            core_out = self.drop(word_emb)\n            pos_emb = self.drop(pos_emb)\n\n            for i, layer in enumerate(self.layers):\n                hids.append(core_out)\n                mems_i = None if mems is None else mems[i]\n                core_out = layer(core_out, pos_emb, dec_attn_mask=dec_attn_mask, mems=mems_i)\n        elif self.attn_type == 1: # learnable\n            core_out = self.drop(word_emb)\n            for i, layer in enumerate(self.layers):\n                hids.append(core_out)\n                if self.clamp_len > 0:\n                    r_emb = self.r_emb[i][-self.clamp_len :]\n                    r_bias = self.r_bias[i][-self.clamp_len :]\n                else:\n                    r_emb, r_bias = self.r_emb[i], self.r_bias[i]\n\n                mems_i = None if mems is None else mems[i]\n                core_out = layer(core_out, r_emb, self.r_w_bias[i],\n                        r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i)\n        elif self.attn_type == 2: # absolute\n            pos_seq = torch.arange(klen - 1, -1, -1.0, device=word_emb.device,\n                                   dtype=word_emb.dtype)\n            if self.clamp_len > 0:\n                pos_seq.clamp_(max=self.clamp_len)\n            pos_emb = self.pos_emb(pos_seq)\n\n            core_out = self.drop(word_emb + pos_emb[-qlen:])\n\n            for i, layer in enumerate(self.layers):\n                hids.append(core_out)\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and i == 0:\n                    mems_i += pos_emb[:mlen]\n                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n                                 mems=mems_i)\n        elif self.attn_type == 3:\n            core_out = self.drop(word_emb)\n\n            for i, layer in enumerate(self.layers):\n                hids.append(core_out)\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and mlen > 0:\n                    cur_emb = self.r_emb[i][:-qlen]\n                    cur_size = cur_emb.size(0)\n                    if cur_size < mlen:\n                        cur_emb_pad = cur_emb[0:1].expand(mlen-cur_size, -1, -1)\n                        cur_emb = torch.cat([cur_emb_pad, cur_emb], 0)\n                    else:\n                        cur_emb = cur_emb[-mlen:]\n                    mems_i += cur_emb.view(mlen, 1, -1)\n                core_out += self.r_emb[i][-qlen:].view(qlen, 1, -1)\n\n                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n                                 mems=mems_i)\n\n        core_out = self.drop(core_out)\n\n        new_mems = self._update_mems(hids, mems, mlen, qlen)\n\n        return core_out, new_mems\n\n    def forward(self, input_ids, mems=None):\n        """""" Params:\n                input_ids :: [bsz, len]\n                mems :: optional mems from previous forwar passes (or init_mems)\n                    list (num layers) of mem states at the entry of each layer\n                        shape :: [self.config.mem_len, bsz, self.config.d_model]\n                    Note that the first two dimensions are transposed in `mems` with regards to `input_ids` and `target`\n            Returns:\n                tuple (last_hidden, new_mems) where:\n                    new_mems: list (num layers) of mem states at the entry of each layer\n                        shape :: [self.config.mem_len, bsz, self.config.d_model]\n                    last_hidden: output of the last layer:\n                        shape :: [bsz, len, self.config.d_model]\n        """"""\n        # the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library\n        # so we transpose here from shape [bsz, len] to shape [len, bsz]\n        input_ids = input_ids.transpose(0, 1).contiguous()\n\n        if mems is None:\n            mems = self.init_mems(input_ids)\n        last_hidden, new_mems = self._forward(input_ids, mems=mems)\n\n        # We transpose back here to shape [bsz, len, hidden_dim]\n        last_hidden = last_hidden.transpose(0, 1).contiguous()\n        return (last_hidden, new_mems)\n\n\nclass TransfoXLLMHeadModel(TransfoXLPreTrainedModel):\n    """"""Transformer XL model (""Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"").\n\n    This model add an (adaptive) softmax head on top of the TransfoXLModel\n\n    Transformer XL use a relative positioning (with sinusiodal patterns) and adaptive softmax inputs which means that:\n    - you don\'t need to specify positioning embeddings indices\n    - the tokens in the vocabulary have to be sorted to decreasing frequency.\n\n    Call self.tie_weights() if you update/load the weights of the transformer to keep the weights tied.\n\n    Params:\n        config: a TransfoXLConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the token indices selected in the range [0, self.config.n_token[\n        `target`: an optional torch.LongTensor of shape [batch_size, sequence_length]\n            with the target token indices selected in the range [0, self.config.n_token[\n        `mems`: an optional memory of hidden states from previous forward passes\n            as a list (num layers) of hidden states at the entry of each layer\n            each hidden states has shape [self.config.mem_len, bsz, self.config.d_model]\n            Note that the first two dimensions are transposed in `mems` with regards to `input_ids` and `target`\n\n    Outputs:\n        A tuple of (last_hidden_state, new_mems)\n        `softmax_output`: output of the (adaptive) softmax:\n            if target is None:\n                Negative log likelihood of shape [batch_size, sequence_length] \n            else:\n                log probabilities of tokens, shape [batch_size, sequence_length, n_tokens]\n        `new_mems`: list (num layers) of updated mem states at the entry of each layer\n            each mem state is a torch.FloatTensor of size [self.config.mem_len, batch_size, self.config.d_model]\n            Note that the first two dimensions are transposed in `mems` with regards to `input_ids` and `target`\n\n    Example usage:\n    ```python\n    # Already been converted into BPE token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_ids_next = torch.LongTensor([[53, 21, 1], [64, 23, 100]])\n\n    config = TransfoXLConfig()\n\n    model = TransfoXLModel(config)\n    last_hidden_state, new_mems = model(input_ids)\n\n    # Another time on input_ids_next using the memory:\n    last_hidden_state, new_mems = model(input_ids_next, mems=new_mems)\n    ```\n    """"""\n    def __init__(self, config):\n        super(TransfoXLLMHeadModel, self).__init__(config)\n        self.transformer = TransfoXLModel(config)\n        self.sample_softmax = config.sample_softmax\n        # use sampled softmax\n        if config.sample_softmax > 0:\n            self.out_layer = nn.Linear(config.d_model, config.n_token)\n            self.sampler = LogUniformSampler(config.n_token, config.sample_softmax)\n        # use adaptive softmax (including standard softmax)\n        else:\n            self.crit = ProjectedAdaptiveLogSoftmax(config.n_token, config.d_embed, config.d_model, \n                                                    config.cutoffs, div_val=config.div_val)\n        self.apply(self.init_weights)\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Run this to be sure output and input (adaptive) softmax weights are tied """"""\n        # sampled softmax\n        if self.sample_softmax > 0:\n            if self.config.tie_weight:\n                self.out_layer.weight = self.transformer.word_emb.weight\n        # adaptive softmax (including standard softmax)\n        else:\n            if self.config.tie_weight:\n                for i in range(len(self.crit.out_layers)):\n                    self.crit.out_layers[i].weight = self.transformer.word_emb.emb_layers[i].weight\n            if self.config.tie_projs:\n                for i, tie_proj in enumerate(self.config.tie_projs):\n                    if tie_proj and self.config.div_val == 1 and self.config.d_model != self.config.d_embed:\n                        self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[0]\n                    elif tie_proj and self.config.div_val != 1:\n                        self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[i]\n\n    def reset_length(self, tgt_len, ext_len, mem_len):\n        self.transformer.reset_length(tgt_len, ext_len, mem_len)\n\n    def init_mems(self, data):\n        return self.transformer.init_mems(data)\n\n    def forward(self, input_ids, target=None, mems=None):\n        """""" Params:\n                input_ids :: [bsz, len]\n                target :: [bsz, len]\n            Returns:\n                tuple(softmax_output, new_mems) where:\n                    new_mems: list (num layers) of hidden states at the entry of each layer\n                        shape :: [mem_len, bsz, self.config.d_model] :: Warning: shapes are transposed here w. regards to input_ids\n                    softmax_output: output of the (adaptive) softmax:\n                        if target is None:\n                            Negative log likelihood of shape :: [bsz, len] \n                        else:\n                            log probabilities of tokens, shape :: [bsz, len, n_tokens]\n        """"""\n        bsz = input_ids.size(0)\n        tgt_len = input_ids.size(1)\n\n        last_hidden, new_mems = self.transformer(input_ids, mems)\n\n        pred_hid = last_hidden[:, -tgt_len:]\n        if self.sample_softmax > 0 and self.training:\n            assert self.config.tie_weight\n            logit = sample_logits(self.transformer.word_emb, self.out_layer.bias, target, pred_hid, self.sampler)\n            softmax_output = -F.log_softmax(logit, -1)[:, :, 0]\n        else:\n            softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), target)\n            if target is None:\n                softmax_output = softmax_output.view(bsz, tgt_len, -1)\n            else:\n                softmax_output = softmax_output.view(bsz, tgt_len)\n\n        # We transpose back\n        return (softmax_output, new_mems)\n'"
examples/mnli_example/pytorch_pretrained_bert/modeling_transfo_xl_utilities.py,32,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Utilities for PyTorch Transformer XL model.\n    Directly adapted from https://github.com/kimiyoung/transformer-xl.\n""""""\n\nfrom collections import defaultdict\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# CUDA_MAJOR = int(torch.version.cuda.split(\'.\')[0])\n# CUDA_MINOR = int(torch.version.cuda.split(\'.\')[1])\n\nclass ProjectedAdaptiveLogSoftmax(nn.Module):\n    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1,\n                 keep_order=False):\n        super(ProjectedAdaptiveLogSoftmax, self).__init__()\n\n        self.n_token = n_token\n        self.d_embed = d_embed\n        self.d_proj = d_proj\n\n        self.cutoffs = cutoffs + [n_token]\n        self.cutoff_ends = [0] + self.cutoffs\n        self.div_val = div_val\n\n        self.shortlist_size = self.cutoffs[0]\n        self.n_clusters = len(self.cutoffs) - 1\n        self.head_size = self.shortlist_size + self.n_clusters\n\n        if self.n_clusters > 0:\n            self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))\n            self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))\n\n        self.out_layers = nn.ModuleList()\n        self.out_projs = nn.ParameterList()\n\n        if div_val == 1:\n            for i in range(len(self.cutoffs)):\n                if d_proj != d_embed:\n                    self.out_projs.append(\n                        nn.Parameter(torch.Tensor(d_proj, d_embed))\n                    )\n                else:\n                    self.out_projs.append(None)\n\n            self.out_layers.append(nn.Linear(d_embed, n_token))\n        else:\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]\n                d_emb_i = d_embed // (div_val ** i)\n\n                self.out_projs.append(\n                    nn.Parameter(torch.Tensor(d_proj, d_emb_i))\n                )\n\n                self.out_layers.append(nn.Linear(d_emb_i, r_idx-l_idx))\n\n        self.keep_order = keep_order\n\n    def _compute_logit(self, hidden, weight, bias, proj):\n        if proj is None:\n            logit = F.linear(hidden, weight, bias=bias)\n        else:\n            # if CUDA_MAJOR <= 9 and CUDA_MINOR <= 1:\n            proj_hid = F.linear(hidden, proj.t().contiguous())\n            logit = F.linear(proj_hid, weight, bias=bias)\n            # else:\n            #     logit = torch.einsum(\'bd,de,ev->bv\', (hidden, proj, weight.t()))\n            #     if bias is not None:\n            #         logit = logit + bias\n\n        return logit\n\n    def forward(self, hidden, target=None, keep_order=False):\n        \'\'\'\n            Params:\n                hidden :: [len*bsz x d_proj]\n                target :: [len*bsz]\n            Return:\n                if target is None:\n                    out :: [len*bsz] Negative log likelihood\n                else:\n                    out :: [len*bsz x n_tokens] log probabilities of tokens over the vocabulary\n            We could replace this implementation by the native PyTorch one\n            if their\'s had an option to set bias on all clusters in the native one.\n            here: https://github.com/pytorch/pytorch/blob/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da/torch/nn/modules/adaptive.py#L138\n        \'\'\'\n\n        if target is not None:\n            target = target.view(-1)\n            if hidden.size(0) != target.size(0):\n                raise RuntimeError(\'Input and target should have the same size \'\n                                \'in the batch dimension.\')\n\n        if self.n_clusters == 0:\n            logit = self._compute_logit(hidden, self.out_layers[0].weight,\n                                        self.out_layers[0].bias, self.out_projs[0])\n            if target is not None:\n                output = -F.log_softmax(logit, dim=-1) \\\n                        .gather(1, target.unsqueeze(1)).squeeze(1)\n            else:\n                output = F.log_softmax(logit, dim=-1)\n        else:\n            # construct weights and biases\n            weights, biases = [], []\n            for i in range(len(self.cutoffs)):\n                if self.div_val == 1:\n                    l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n                    weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                    bias_i = self.out_layers[0].bias[l_idx:r_idx]\n                else:\n                    weight_i = self.out_layers[i].weight\n                    bias_i = self.out_layers[i].bias\n\n                if i == 0:\n                    weight_i = torch.cat(\n                        [weight_i, self.cluster_weight], dim=0)\n                    bias_i = torch.cat(\n                        [bias_i, self.cluster_bias], dim=0)\n\n                weights.append(weight_i)\n                biases.append(bias_i)\n\n            head_weight, head_bias, head_proj = weights[0], biases[0], self.out_projs[0]\n\n            head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n            head_logprob = F.log_softmax(head_logit, dim=1)\n\n            if target is None:\n                out = hidden.new_empty((head_logit.size(0), self.n_token))\n            else:\n                out = torch.zeros_like(target, dtype=hidden.dtype, device=hidden.device)\n\n            offset = 0\n            cutoff_values = [0] + self.cutoffs\n            for i in range(len(cutoff_values) - 1):\n                l_idx, r_idx = cutoff_values[i], cutoff_values[i + 1]\n\n                if target is not None:\n                    mask_i = (target >= l_idx) & (target < r_idx)\n                    indices_i = mask_i.nonzero().squeeze()\n\n                    if indices_i.numel() == 0:\n                        continue\n\n                    target_i = target.index_select(0, indices_i) - l_idx\n                    head_logprob_i = head_logprob.index_select(0, indices_i)\n                    hidden_i = hidden.index_select(0, indices_i)\n                else:\n                    hidden_i = hidden\n\n                if i == 0:\n                    if target is not None:\n                        logprob_i = head_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                    else:\n                        out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n                else:\n                    weight_i, bias_i, proj_i = weights[i], biases[i], self.out_projs[i]\n\n                    tail_logit_i = self._compute_logit(hidden_i, weight_i, bias_i, proj_i)\n                    tail_logprob_i = F.log_softmax(tail_logit_i, dim=1)\n                    cluster_prob_idx = self.cutoffs[0] + i - 1  # No probability for the head cluster\n                    if target is not None:\n                        logprob_i = head_logprob_i[:, cluster_prob_idx] \\\n                                + tail_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                    else:\n                        logprob_i = head_logprob[:, cluster_prob_idx, None] + tail_logprob_i\n                        out[:, l_idx:r_idx] = logprob_i\n\n                if target is not None:\n                    if (hasattr(self, \'keep_order\') and self.keep_order) or keep_order:\n                        out.index_copy_(0, indices_i, -logprob_i)\n                    else:\n                        out[offset:offset+logprob_i.size(0)].copy_(-logprob_i)\n                    offset += logprob_i.size(0)\n\n        return out\n\n\n    def log_prob(self, hidden):\n        r"""""" Computes log probabilities for all :math:`n\\_classes`\n        From: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/adaptive.py\n        Args:\n            hidden (Tensor): a minibatch of examples\n        Returns:\n            log-probabilities of for each class :math:`c`\n            in range :math:`0 <= c <= n\\_classes`, where :math:`n\\_classes` is a\n            parameter passed to ``AdaptiveLogSoftmaxWithLoss`` constructor.\n        Shape:\n            - Input: :math:`(N, in\\_features)`\n            - Output: :math:`(N, n\\_classes)`\n        """"""\n        if self.n_clusters == 0:\n            logit = self._compute_logit(hidden, self.out_layers[0].weight,\n                                        self.out_layers[0].bias, self.out_projs[0])\n            return F.log_softmax(logit, dim=-1)\n        else:\n            # construct weights and biases\n            weights, biases = [], []\n            for i in range(len(self.cutoffs)):\n                if self.div_val == 1:\n                    l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n                    weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                    bias_i = self.out_layers[0].bias[l_idx:r_idx]\n                else:\n                    weight_i = self.out_layers[i].weight\n                    bias_i = self.out_layers[i].bias\n\n                if i == 0:\n                    weight_i = torch.cat(\n                        [weight_i, self.cluster_weight], dim=0)\n                    bias_i = torch.cat(\n                        [bias_i, self.cluster_bias], dim=0)\n\n                weights.append(weight_i)\n                biases.append(bias_i)\n\n            head_weight, head_bias, head_proj = weights[0], biases[0], self.out_projs[0]\n            head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n\n            out = hidden.new_empty((head_logit.size(0), self.n_token))\n            head_logprob = F.log_softmax(head_logit, dim=1)\n\n            cutoff_values = [0] + self.cutoffs\n            for i in range(len(cutoff_values) - 1):\n                start_idx, stop_idx = cutoff_values[i], cutoff_values[i + 1]\n\n                if i == 0:\n                    out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n                else:\n                    weight_i, bias_i, proj_i = weights[i], biases[i], self.out_projs[i]\n\n                    tail_logit_i = self._compute_logit(hidden, weight_i, bias_i, proj_i)\n                    tail_logprob_i = F.log_softmax(tail_logit_i, dim=1)\n\n                    logprob_i = head_logprob[:, -i] + tail_logprob_i\n                    out[:, start_idx, stop_idx] = logprob_i\n\n            return out\n\n\nclass LogUniformSampler(object):\n    def __init__(self, range_max, n_sample):\n        """"""\n        Reference : https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/candidate_sampling_ops.py\n            `P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)`\n\n        expected count can be approximated by 1 - (1 - p)^n\n        and we use a numerically stable version -expm1(num_tries * log1p(-p))\n\n        Our implementation fixes num_tries at 2 * n_sample, and the actual #samples will vary from run to run\n        """"""\n        with torch.no_grad():\n            self.range_max = range_max\n            log_indices = torch.arange(1., range_max+2., 1.).log_()\n            self.dist = (log_indices[1:] - log_indices[:-1]) / log_indices[-1]\n            # print(\'P\', self.dist.numpy().tolist()[-30:])\n\n            self.log_q = (- (-self.dist.double().log1p_() * 2 * n_sample).expm1_()).log_().float()\n\n        self.n_sample = n_sample\n\n    def sample(self, labels):\n        """"""\n            labels: [b1, b2]\n        Return\n            true_log_probs: [b1, b2]\n            samp_log_probs: [n_sample]\n            neg_samples: [n_sample]\n        """"""\n\n        # neg_samples = torch.empty(0).long()\n        n_sample = self.n_sample\n        n_tries = 2 * n_sample\n\n        with torch.no_grad():\n            neg_samples = torch.multinomial(self.dist, n_tries, replacement=True).unique()\n            device = labels.device\n            neg_samples = neg_samples.to(device)\n            true_log_probs = self.log_q[labels].to(device)\n            samp_log_probs = self.log_q[neg_samples].to(device)\n            return true_log_probs, samp_log_probs, neg_samples\n\ndef sample_logits(embedding, bias, labels, inputs, sampler):\n    """"""\n        embedding: an nn.Embedding layer\n        bias: [n_vocab]\n        labels: [b1, b2]\n        inputs: [b1, b2, n_emb]\n        sampler: you may use a LogUniformSampler\n    Return\n        logits: [b1, b2, 1 + n_sample]\n    """"""\n    true_log_probs, samp_log_probs, neg_samples = sampler.sample(labels)\n    n_sample = neg_samples.size(0)\n    b1, b2 = labels.size(0), labels.size(1)\n    all_ids = torch.cat([labels.view(-1), neg_samples])\n    all_w = embedding(all_ids)\n    true_w = all_w[: -n_sample].view(b1, b2, -1)\n    sample_w = all_w[- n_sample:].view(n_sample, -1)\n\n    all_b = bias[all_ids]\n    true_b = all_b[: -n_sample].view(b1, b2)\n    sample_b = all_b[- n_sample:]\n\n    hit = (labels[:, :, None] == neg_samples).detach()\n\n    true_logits = torch.einsum(\'ijk,ijk->ij\',\n        [true_w, inputs]) + true_b - true_log_probs\n    sample_logits = torch.einsum(\'lk,ijk->ijl\',\n        [sample_w, inputs]) + sample_b - samp_log_probs\n    sample_logits.masked_fill_(hit, -1e30)\n    logits = torch.cat([true_logits[:, :, None], sample_logits], -1)\n\n    return logits\n\n\n# class LogUniformSampler(object):\n#     def __init__(self, range_max, unique=False):\n#         """"""\n#         Reference : https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/candidate_sampling_ops.py\n#             `P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)`\n#         """"""\n#         self.range_max = range_max\n#         log_indices = torch.arange(1., range_max+2., 1.).log_()\n#         self.dist = (log_indices[1:] - log_indices[:-1]) / log_indices[-1]\n\n#         self.unique = unique\n\n#         if self.unique:\n#             self.exclude_mask = torch.ByteTensor(range_max).fill_(0)\n\n#     def sample(self, n_sample, labels):\n#         pos_sample, new_labels = labels.unique(return_inverse=True)\n#         n_pos_sample = pos_sample.size(0)\n#         n_neg_sample = n_sample - n_pos_sample\n\n#         if self.unique:\n#             self.exclude_mask.index_fill_(0, pos_sample, 1)\n#             sample_dist = self.dist.clone().masked_fill_(self.exclude_mask, 0)\n#             self.exclude_mask.index_fill_(0, pos_sample, 0)\n#         else:\n#             sample_dist = self.dist\n\n#         neg_sample = torch.multinomial(sample_dist, n_neg_sample)\n\n#         sample = torch.cat([pos_sample, neg_sample])\n#         sample_prob = self.dist[sample]\n\n#         return new_labels, sample, sample_prob\n\n\nif __name__ == \'__main__\':\n    S, B = 3, 4\n    n_vocab = 10000\n    n_sample = 5\n    H = 32\n\n    labels = torch.LongTensor(S, B).random_(0, n_vocab)\n\n    # sampler = LogUniformSampler(n_vocab, unique=False)\n    # new_labels, sample, sample_prob = sampler.sample(n_sample, labels)\n\n    sampler = LogUniformSampler(n_vocab, n_sample)#, unique=True)\n    # true_probs, samp_probs, neg_samples = sampler.sample(n_sample, labels)\n\n    # print(\'true_probs\', true_probs.numpy().tolist())\n    # print(\'samp_probs\', samp_probs.numpy().tolist())\n    # print(\'neg_samples\', neg_samples.numpy().tolist())\n\n    # print(\'sum\', torch.sum(sampler.dist).item())\n\n    # assert torch.all(torch.sort(sample.unique())[0].eq(torch.sort(sample)[0])).item()\n\n    embedding = nn.Embedding(n_vocab, H)\n    bias = torch.zeros(n_vocab)\n    inputs = torch.Tensor(S, B, H).normal_()\n\n    logits, out_labels = sample_logits(embedding, bias, labels, inputs, sampler, n_sample)\n    print(\'logits\', logits.detach().numpy().tolist())\n    print(\'logits shape\', logits.size())\n    print(\'out_labels\', out_labels.detach().numpy().tolist())\n    print(\'out_labels shape\', out_labels.size())\n\n'"
examples/mnli_example/pytorch_pretrained_bert/my_modeling.py,78,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch BERT model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom .file_utils import cached_path, WEIGHTS_NAME, CONFIG_NAME\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz"",\n}\nBERT_CONFIG_NAME = \'bert_config.json\'\nTF_WEIGHTS_NAME = \'model.ckpt\'\n\ndef load_tf_weights_in_bert(model, tf_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    print(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\'/\')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [""adam_v"", ""adam_m"", ""global_step""] for n in name):\n            print(""Skipping {}"".format(""/"".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'output_weights\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'squad\':\n                pointer = getattr(pointer, \'classifier\')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    print(""Skipping {}"".format(""/"".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \'_embeddings\':\n            pointer = getattr(pointer, \'weight\')\n        elif m_name == \'kernel\':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    """"""Implementation of the gelu activation function.\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish}\n\n\nclass BertConfig(object):\n    """"""Configuration class to store the configuration of a `BertModel`.\n    """"""\n    def __init__(self,\n                 vocab_size_or_config_json_file,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02,):\n        """"""Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             ""or the path to a pretrained model config file (str)"")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n        config = BertConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `BertConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n\ntry:\n    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\nexcept ImportError:\n    logger.info(""Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex ."")\n    class BertLayerNorm(nn.Module):\n        def __init__(self, hidden_size, eps=1e-12):\n            """"""Construct a layernorm module in the TF style (epsilon inside the square root).\n            """"""\n            super(BertLayerNorm, self).__init__()\n            self.weight = nn.Parameter(torch.ones(hidden_size))\n            self.bias = nn.Parameter(torch.zeros(hidden_size))\n            self.variance_epsilon = eps\n\n        def forward(self, x):\n            u = x.mean(-1, keepdim=True)\n            s = (x - u).pow(2).mean(-1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n            return self.weight * x + self.bias\n\nclass BertEmbeddings(nn.Module):\n    """"""Construct the embeddings from word, position and token_type embeddings.\n    """"""\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None):\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads))\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        self.output_score = config.output_score\n        self.output_sum   = config.output_sum\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n        attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        if self.output_score is True:\n            attention_probs_sum = attention_scores # (batch_size, h, l, l)\n        else:\n            attention_probs_sum = attention_probs # (batch_size, h, l, l)\n        if self.output_sum is True:\n            attention_probs_sum = attention_probs_sum.sum(dim=1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        return context_layer, attention_probs_sum\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n\n    def forward(self, input_tensor, attention_mask):\n        self_output, att_layer = self.self(input_tensor, attention_mask)\n        attention_output = self.output(self_output, input_tensor)\n        return attention_output, att_layer\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states, attention_mask):\n        attention_output, att_layer = self.attention(hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output, att_layer\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        layer = BertLayer(config)\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, output_all_attention_layers=False):\n        all_encoder_layers = [hidden_states]\n        all_attention_layers = [None]\n        for idx,layer_module in enumerate(self.layer):\n            hidden_states, att_layer = layer_module(hidden_states, attention_mask)\n            if output_all_encoded_layers:\n                all_encoder_layers.append(hidden_states)\n            if output_all_attention_layers:\n                all_attention_layers.append(att_layer)\n        if not output_all_encoded_layers:\n            all_encoder_layers.append(hidden_states)\n        return all_encoder_layers, all_attention_layers\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(BertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertLMPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n                                 bert_model_embedding_weights.size(0),\n                                 bias=False)\n        self.decoder.weight = bert_model_embedding_weights\n        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertOnlyMLMHead, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertPreTrainingHeads, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\n\nclass BertPreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    def __init__(self, config, *inputs, **kwargs):\n        super(BertPreTrainedModel, self).__init__()\n        if not isinstance(config, BertConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `BertConfig`. ""\n                ""To create a model from a Google pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        self.config = config\n\n    def init_bert_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n        """"""\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        """"""\n        state_dict = kwargs.get(\'state_dict\', None)\n        kwargs.pop(\'state_dict\', None)\n        cache_dir = kwargs.get(\'cache_dir\', None)\n        kwargs.pop(\'cache_dir\', None)\n        from_tf = kwargs.get(\'from_tf\', False)\n        kwargs.pop(\'from_tf\', None)\n\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                ""associated to this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n                    archive_file))\n            return None\n        if resolved_archive_file == archive_file:\n            logger.info(""loading archive file {}"".format(archive_file))\n        else:\n            logger.info(""loading archive file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n        tempdir = None\n        if os.path.isdir(resolved_archive_file) or from_tf:\n            serialization_dir = resolved_archive_file\n        else:\n            # Extract archive to temp dir\n            tempdir = tempfile.mkdtemp()\n            logger.info(""extracting archive file {} to temp dir {}"".format(\n                resolved_archive_file, tempdir))\n            with tarfile.open(resolved_archive_file, \'r:gz\') as archive:\n                archive.extractall(tempdir)\n            serialization_dir = tempdir\n        # Load config\n        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n        if not os.path.exists(config_file):\n            # Backward compatibility with old naming format\n            config_file = os.path.join(serialization_dir, BERT_CONFIG_NAME)\n        config = BertConfig.from_json_file(config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n            state_dict = torch.load(weights_path, map_location=\'cpu\')\n        if tempdir:\n            # Clean up temp dir\n            shutil.rmtree(tempdir)\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint\n            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n            return load_tf_weights_in_bert(model, weights_path)\n        # Load from a PyTorch state_dict\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if \'gamma\' in key:\n                new_key = key.replace(\'gamma\', \'weight\')\n            if \'beta\' in key:\n                new_key = key.replace(\'beta\', \'bias\')\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \'_metadata\', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=\'\'):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + \'.\')\n        start_prefix = \'\'\n        if not hasattr(model, \'bert\') and any(s.startswith(\'bert.\') for s in state_dict.keys()):\n            start_prefix = \'bert.\'\n        load(model, prefix=start_prefix)\n        if len(missing_keys) > 0:\n            logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(""Weights from pretrained model not used in {}: {}"".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) > 0:\n            raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\n                               model.__class__.__name__, ""\\n\\t"".join(error_msgs)))\n        return model\n\n\nclass BertModel(BertPreTrainedModel):\n    """"""BERT model (""Bidirectional Embedding Representations from a Transformer"").\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n\n    Outputs: Tuple of (encoded_layers, pooled_output)\n        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n            classifier pretrained on top of the hidden state associated to the first character of the\n            input (`CLS`) to train on the Next-Sentence task (see BERT\'s paper).\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = modeling.BertModel(config=config)\n    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, output_score=False, output_sum=False):\n        super(BertModel, self).__init__(config)\n        config.output_score = output_score\n        config.output_sum   = output_sum\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, output_all_attention_layers=False):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        embedding_output = self.embeddings(input_ids, token_type_ids)\n        encoded_layers,att_layers = self.encoder(embedding_output,\n                                      extended_attention_mask,\n                                      output_all_encoded_layers=output_all_encoded_layers,\n                                      output_all_attention_layers=output_all_attention_layers)\n        sequence_output = encoded_layers[-1]\n        pooled_output = self.pooler(sequence_output)\n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n        return encoded_layers, pooled_output, att_layers\n\n\nclass BertForPreTraining(BertPreTrainedModel):\n    """"""BERT model with pre-training heads.\n    This module comprises the BERT model followed by the two pre-training heads:\n        - the masked language modeling head, and\n        - the next sentence classification head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: optional masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n        `next_sentence_label`: optional next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n\n    Outputs:\n        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `masked_lm_labels` or `next_sentence_label` is `None`:\n            Outputs a tuple comprising\n            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n            - the next sentence classification logits of shape [batch_size, 2].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForPreTraining(config)\n    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForPreTraining, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                                   output_all_encoded_layers=False)\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            return total_loss\n        else:\n            return prediction_scores, seq_relationship_score\n\n\nclass BertForMaskedLM(BertPreTrainedModel):\n    """"""BERT model with the masked language modeling head.\n    This module comprises the BERT model followed by the masked language modeling head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n\n    Outputs:\n        if `masked_lm_labels` is  not `None`:\n            Outputs the masked language modeling loss.\n        if `masked_lm_labels` is `None`:\n            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForMaskedLM(config)\n    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForMaskedLM, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n                                       output_all_encoded_layers=False)\n        prediction_scores = self.cls(sequence_output)\n\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            return masked_lm_loss\n        else:\n            return prediction_scores\n\n\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    """"""BERT model with next sentence prediction head.\n    This module comprises the BERT model followed by the next sentence classification head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n\n    Outputs:\n        if `next_sentence_label` is not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `next_sentence_label` is `None`:\n            Outputs the next sentence classification logits of shape [batch_size, 2].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForNextSentencePrediction(config)\n    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForNextSentencePrediction, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                     output_all_encoded_layers=False)\n        seq_relationship_score = self.cls( pooled_output)\n\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            return next_sentence_loss\n        else:\n            return seq_relationship_score\n\n\nclass BertForSequenceClassification(BertPreTrainedModel):\n    """"""BERT model for classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary. Items in the batch should begin with the special ""CLS"" token. (see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_labels].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_labels = 2\n\n    model = BertForSequenceClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForMultipleChoice(BertPreTrainedModel):\n    """"""BERT model for multiple choice tasks.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_choices`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_choices = 2\n\n    model = BertForMultipleChoice(config, num_choices)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_choices):\n        super(BertForMultipleChoice, self).__init__(config)\n        self.num_choices = num_choices\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, self.num_choices)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            return loss\n        else:\n            return reshaped_logits\n\n\nclass BertForTokenClassification(BertPreTrainedModel):\n    """"""BERT model for token-level classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the full hidden state of the last layer.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [0, ..., num_labels].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_labels = 2\n\n    model = BertForTokenClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForTokenClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForQuestionAnswering(BertPreTrainedModel):\n    """"""BERT model for Question Answering (span extraction).\n    This module is composed of the BERT model with a linear layer on top of\n    the sequence output that computes start_logits and end_logits\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n            into account for computing the loss.\n        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n            into account for computing the loss.\n\n    Outputs:\n        if `start_positions` and `end_positions` are not `None`:\n            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n        if `start_positions` or `end_positions` is `None`:\n            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n            position tokens of shape [batch_size, sequence_length].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForQuestionAnswering(config)\n    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.bert = BertModel(config)\n        # TODO check with Google if it\'s normal there is no dropout on the token classifier of SQuAD in the TF version\n        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            return total_loss\n        else:\n            return start_logits, end_logits\n\nclass ALBertEncoder(nn.Module):\n    def __init__(self, config):\n        super(ALBertEncoder, self).__init__()\n        layer = BertLayer(config)\n        self.num_hidden_layers = config.num_hidden_layers\n        self.skip_connection = config.skip_connection\n        self.num_repetitions = config.num_repetitions\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(self.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, output_attention_layer=None):\n        all_encoder_layers = []\n        all_attention_probs_sum = []\n\n        for idx_outer, layer_module in enumerate(self.layer):\n            x0 = hidden_states\n            for idx_inner in range(self.num_repetitions):\n                idx = idx_inner + idx_outer * self.num_repetitions\n                if idx_inner==0 or self.skip_connection is False:\n                    hidden_states, attention_probs_sum = layer_module(hidden_states, attention_mask)\n                else:\n                    hidden_states, attention_probs_sum = layer_module(hidden_states + x0, attention_mask)\n                if output_attention_layer is not None and idx in output_attention_layer:\n                    all_attention_probs_sum.append(attention_probs_sum)\n                if output_all_encoded_layers:\n                    all_encoder_layers.append(hidden_states)\n        if not output_all_encoded_layers:\n            all_encoder_layers.append(hidden_states)\n        return  all_encoder_layers, all_attention_probs_sum\n\n\nclass ALBertModel(BertPreTrainedModel):\n\n    def __init__(self, config, output_score=False, output_sum=1, skip_connection=False, num_repetitions=1):\n        super(ALBertModel, self).__init__(config)\n        config.output_score = output_score\n        config.output_sum   = output_sum\n        config.skip_connection = skip_connection\n        config.num_repetitions = num_repetitions\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = ALBertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, output_attention_layer=None):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        embedding_output = self.embeddings(input_ids, token_type_ids)\n        encoded_layers,attention_probs_sum = self.encoder(embedding_output,\n                                      extended_attention_mask,\n                                      output_all_encoded_layers=output_all_encoded_layers,\n                                      output_attention_layer=output_attention_layer)\n        sequence_output = encoded_layers[-1]\n        pooled_output = self.pooler(sequence_output)\n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n        return encoded_layers, pooled_output, attention_probs_sum\n'"
examples/mnli_example/pytorch_pretrained_bert/optimization.py,5,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch optimization for BERT model.""""""\n\nimport math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.optimizer import required\nfrom torch.nn.utils import clip_grad_norm_\nimport logging\nimport abc\nimport sys\n\nlogger = logging.getLogger(__name__)\n\n\nif sys.version_info >= (3, 4):\n    ABC = abc.ABC\nelse:\n    ABC = abc.ABCMeta(\'ABC\', (), {})\n\n\nclass _LRSchedule(ABC):\n    """""" Parent of all LRSchedules here. """"""\n    warn_t_total = False        # is set to True for schedules where progressing beyond t_total steps doesn\'t make sense\n    def __init__(self, warmup=0.002, t_total=-1, **kw):\n        """"""\n        :param warmup:  what fraction of t_total steps will be used for linear warmup\n        :param t_total: how many training steps (updates) are planned\n        :param kw:\n        """"""\n        super(_LRSchedule, self).__init__(**kw)\n        if t_total < 0:\n            logger.warning(""t_total value of {} results in schedule not being applied"".format(t_total))\n        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n            raise ValueError(""Invalid warmup: {} - should be in [0.0, 1.0[ or -1"".format(warmup))\n        warmup = max(warmup, 0.)\n        self.warmup, self.t_total = float(warmup), float(t_total)\n        self.warned_for_t_total_at_progress = -1\n\n    def get_lr(self, step, nowarn=False):\n        """"""\n        :param step:    which of t_total steps we\'re on\n        :param nowarn:  set to True to suppress warning regarding training beyond specified \'t_total\' steps\n        :return:        learning rate multiplier for current update\n        """"""\n        if self.t_total < 0:\n            return 1.\n        progress = float(step) / self.t_total\n        ret = self.get_lr_(progress)\n        # warning for exceeding t_total (only active with warmup_linear\n        if not nowarn and self.warn_t_total and progress > 1. and progress > self.warned_for_t_total_at_progress:\n            logger.warning(\n                ""Training beyond specified \'t_total\'. Learning rate multiplier set to {}. Please set \'t_total\' of {} correctly.""\n                    .format(ret, self.__class__.__name__))\n            self.warned_for_t_total_at_progress = progress\n        # end warning\n        return ret\n\n    @abc.abstractmethod\n    def get_lr_(self, progress):\n        """"""\n        :param progress:    value between 0 and 1 (unless going beyond t_total steps) specifying training progress\n        :return:            learning rate multiplier for current update\n        """"""\n        return 1.\n\n\nclass ConstantLR(_LRSchedule):\n    def get_lr_(self, progress):\n        return 1.\n\n\nclass WarmupCosineSchedule(_LRSchedule):\n    """"""\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    Decreases learning rate from 1. to 0. over remaining `1 - warmup` steps following a cosine curve.\n    If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n    """"""\n    warn_t_total = True\n    def __init__(self, warmup=0.002, t_total=-1, cycles=.5, **kw):\n        """"""\n        :param warmup:      see LRSchedule\n        :param t_total:     see LRSchedule\n        :param cycles:      number of cycles. Default: 0.5, corresponding to cosine decay from 1. at progress==warmup and 0 at progress==1.\n        :param kw:\n        """"""\n        super(WarmupCosineSchedule, self).__init__(warmup=warmup, t_total=t_total, **kw)\n        self.cycles = cycles\n\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        else:\n            progress = (progress - self.warmup) / (1 - self.warmup)   # progress after warmup\n            return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))\n\n\nclass WarmupCosineWithHardRestartsSchedule(WarmupCosineSchedule):\n    """"""\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    If `cycles` (default=1.) is different from default, learning rate follows `cycles` times a cosine decaying\n    learning rate (with hard restarts).\n    """"""\n    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n        super(WarmupCosineWithHardRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n        assert(cycles >= 1.)\n\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        else:\n            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n            ret = 0.5 * (1. + math.cos(math.pi * ((self.cycles * progress) % 1)))\n            return ret\n\n\nclass WarmupCosineWithWarmupRestartsSchedule(WarmupCosineWithHardRestartsSchedule):\n    """"""\n    All training progress is divided in `cycles` (default=1.) parts of equal length.\n    Every part follows a schedule with the first `warmup` fraction of the training steps linearly increasing from 0. to 1.,\n    followed by a learning rate decreasing from 1. to 0. following a cosine curve.\n    """"""\n    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n        assert(warmup * cycles < 1.)\n        warmup = warmup * cycles if warmup >= 0 else warmup\n        super(WarmupCosineWithWarmupRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n\n    def get_lr_(self, progress):\n        progress = progress * self.cycles % 1.\n        if progress < self.warmup:\n            return progress / self.warmup\n        else:\n            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n            ret = 0.5 * (1. + math.cos(math.pi * progress))\n            return ret\n\n\nclass WarmupConstantSchedule(_LRSchedule):\n    """"""\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    Keeps learning rate equal to 1. after warmup.\n    """"""\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        return 1.\n\n\nclass WarmupLinearSchedule(_LRSchedule):\n    """"""\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    Linearly decreases learning rate from 1. to 0. over remaining `1 - warmup` steps.\n    """"""\n    warn_t_total = True\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        return max((progress - 1.) / (self.warmup - 1.), 0.)\n\n\nSCHEDULES = {\n    None:       ConstantLR,\n    ""none"":     ConstantLR,\n    ""warmup_cosine"": WarmupCosineSchedule,\n    ""warmup_constant"": WarmupConstantSchedule,\n    ""warmup_linear"": WarmupLinearSchedule\n}\n\n\nclass BertAdam(Optimizer):\n    """"""Implements BERT version of Adam algorithm with weight decay fix.\n    Params:\n        lr: learning rate\n        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n        t_total: total number of training steps for the learning\n            rate schedule, -1  means constant learning rate of 1. (no warmup regardless of warmup setting). Default: -1\n        schedule: schedule to use for the warmup (see above).\n            Can be `\'warmup_linear\'`, `\'warmup_constant\'`, `\'warmup_cosine\'`, `\'none\'`, `None` or a `_LRSchedule` object (see below).\n            If `None` or `\'none\'`, learning rate is always kept constant.\n            Default : `\'warmup_linear\'`\n        b1: Adams b1. Default: 0.9\n        b2: Adams b2. Default: 0.999\n        e: Adams epsilon. Default: 1e-6\n        weight_decay: Weight decay. Default: 0.01\n        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n    """"""\n    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule=\'warmup_linear\',\n                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01, max_grad_norm=1.0, **kwargs):\n        if lr is not required and lr < 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))\n        if not 0.0 <= b1 < 1.0:\n            raise ValueError(""Invalid b1 parameter: {} - should be in [0.0, 1.0["".format(b1))\n        if not 0.0 <= b2 < 1.0:\n            raise ValueError(""Invalid b2 parameter: {} - should be in [0.0, 1.0["".format(b2))\n        if not e >= 0.0:\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(e))\n        # initialize schedule object\n        if not isinstance(schedule, _LRSchedule):\n            schedule_type = SCHEDULES[schedule]\n            schedule = schedule_type(warmup=warmup, t_total=t_total)\n        else:\n            if warmup != -1 or t_total != -1:\n                logger.warning(""warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. ""\n                               ""Please specify custom warmup and t_total in _LRSchedule object."")\n        defaults = dict(lr=lr, schedule=schedule,\n                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm)\n        super(BertAdam, self).__init__(params, defaults)\n\n    def get_lr(self):\n        lr = []\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                if len(state) == 0:\n                    return [0]\n                lr_scheduled = group[\'lr\']\n                lr_scheduled *= group[\'schedule\'].get_lr(state[\'step\'])\n                lr.append(lr_scheduled)\n        return lr\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'next_m\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'next_v\'] = torch.zeros_like(p.data)\n\n                next_m, next_v = state[\'next_m\'], state[\'next_v\']\n                beta1, beta2 = group[\'b1\'], group[\'b2\']\n\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                next_m.mul_(beta1).add_(1 - beta1, grad)\n                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                update = next_m / (next_v.sqrt() + group[\'e\'])\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want to decay the weights in a manner that doesn\'t interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                if group[\'weight_decay\'] > 0.0:\n                    update += group[\'weight_decay\'] * p.data\n\n                lr_scheduled = group[\'lr\']\n                lr_scheduled *= group[\'schedule\'].get_lr(state[\'step\'])\n\n                update_with_lr = lr_scheduled * update\n                p.data.add_(-update_with_lr)\n\n                state[\'step\'] += 1\n\n                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n                # No bias correction\n                # bias_correction1 = 1 - beta1 ** state[\'step\']\n                # bias_correction2 = 1 - beta2 ** state[\'step\']\n\n        return loss\n'"
examples/mnli_example/pytorch_pretrained_bert/optimization_openai.py,5,"b'# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch optimization for OpenAI GPT model.""""""\n\nimport math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.optimizer import required\nfrom torch.nn.utils import clip_grad_norm_\nimport logging\nfrom .optimization import SCHEDULES, _LRSchedule, WarmupCosineWithWarmupRestartsSchedule, \\\n    WarmupCosineWithHardRestartsSchedule, WarmupCosineSchedule, WarmupLinearSchedule, WarmupConstantSchedule\n\nlogger = logging.getLogger(__name__)\n\n\nclass OpenAIAdam(Optimizer):\n    """"""Implements Open AI version of Adam algorithm with weight decay fix.\n    """"""\n    def __init__(self, params, lr=required, schedule=\'warmup_linear\', warmup=-1, t_total=-1,\n                 b1=0.9, b2=0.999, e=1e-8, weight_decay=0,\n                 vector_l2=False, max_grad_norm=-1, **kwargs):\n        if lr is not required and lr < 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))\n        if not 0.0 <= b1 < 1.0:\n            raise ValueError(""Invalid b1 parameter: {} - should be in [0.0, 1.0["".format(b1))\n        if not 0.0 <= b2 < 1.0:\n            raise ValueError(""Invalid b2 parameter: {} - should be in [0.0, 1.0["".format(b2))\n        if not e >= 0.0:\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(e))\n        # initialize schedule object\n        if not isinstance(schedule, _LRSchedule):\n            schedule_type = SCHEDULES[schedule]\n            schedule = schedule_type(warmup=warmup, t_total=t_total)\n        else:\n            if warmup != -1 or t_total != -1:\n                logger.warning(""warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. ""\n                               ""Please specify custom warmup and t_total in _LRSchedule object."")\n        defaults = dict(lr=lr, schedule=schedule,\n                        b1=b1, b2=b2, e=e, weight_decay=weight_decay, vector_l2=vector_l2,\n                        max_grad_norm=max_grad_norm)\n        super(OpenAIAdam, self).__init__(params, defaults)\n\n    def get_lr(self):\n        lr = []\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                if len(state) == 0:\n                    return [0]\n                lr_scheduled = group[\'lr\']\n                lr_scheduled *= group[\'schedule\'].get_lr(state[\'step\'])\n                lr.append(lr_scheduled)\n        return lr\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'b1\'], group[\'b2\']\n\n                state[\'step\'] += 1\n\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                denom = exp_avg_sq.sqrt().add_(group[\'e\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n\n                lr_scheduled = group[\'lr\']\n                lr_scheduled *= group[\'schedule\'].get_lr(state[\'step\'])\n\n                step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n                # Add weight decay at the end (fixed version)\n                if (len(p.size()) > 1 or group[\'vector_l2\']) and group[\'weight_decay\'] > 0:\n                    p.data.add_(-lr_scheduled * group[\'weight_decay\'], p.data)\n\n        return loss\n'"
examples/mnli_example/pytorch_pretrained_bert/tokenization.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport logging\nimport os\nimport unicodedata\nfrom io import open\n\nfrom .file_utils import cached_path\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt"",\n}\nPRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n    \'bert-base-uncased\': 512,\n    \'bert-large-uncased\': 512,\n    \'bert-base-cased\': 512,\n    \'bert-large-cased\': 512,\n    \'bert-base-multilingual-uncased\': 512,\n    \'bert-base-multilingual-cased\': 512,\n    \'bert-base-chinese\': 512,\n}\nVOCAB_NAME = \'vocab.txt\'\n\n\ndef load_vocab(vocab_file):\n    """"""Loads a vocabulary file into a dictionary.""""""\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, ""r"", encoding=""utf-8"") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass BertTokenizer(object):\n    """"""Runs end-to-end tokenization: punctuation splitting + wordpiece""""""\n\n    def __init__(self, vocab_file, do_lower_case=True, max_len=None, do_basic_tokenize=True,\n                 never_split=(""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]"")):\n        """"""Constructs a BertTokenizer.\n\n        Args:\n          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n          do_lower_case: Whether to lower case the input\n                         Only has an effect when do_wordpiece_only=False\n          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n          max_len: An artificial maximum length to truncate tokenized sequences to;\n                         Effective maximum length is always the minimum of this\n                         value (if specified) and the underlying BERT model\'s\n                         sequence length.\n          never_split: List of tokens which will never be split during tokenization.\n                         Only has an effect when do_wordpiece_only=False\n        """"""\n        if not os.path.isfile(vocab_file):\n            raise ValueError(\n                ""Can\'t find a vocabulary file at path \'{}\'. To load the vocabulary from a Google pretrained ""\n                ""model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(vocab_file))\n        self.vocab = load_vocab(vocab_file)\n        self.ids_to_tokens = collections.OrderedDict(\n            [(ids, tok) for tok, ids in self.vocab.items()])\n        self.do_basic_tokenize = do_basic_tokenize\n        if do_basic_tokenize:\n          self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n                                                never_split=never_split)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n        self.max_len = max_len if max_len is not None else int(1e12)\n\n    def tokenize(self, text):\n        split_tokens = []\n        if self.do_basic_tokenize:\n            for token in self.basic_tokenizer.tokenize(text):\n                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                    split_tokens.append(sub_token)\n        else:\n            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        """"""Converts a sequence of tokens into ids using the vocab.""""""\n        ids = []\n        for token in tokens:\n            ids.append(self.vocab[token])\n        if len(ids) > self.max_len:\n            logger.warning(\n                ""Token indices sequence length is longer than the specified maximum ""\n                "" sequence length for this BERT model ({} > {}). Running this""\n                "" sequence through BERT will result in indexing errors"".format(len(ids), self.max_len)\n            )\n        return ids\n\n    def convert_ids_to_tokens(self, ids):\n        """"""Converts a sequence of ids in wordpiece tokens using the vocab.""""""\n        tokens = []\n        for i in ids:\n            tokens.append(self.ids_to_tokens[i])\n        return tokens\n\n    def save_vocabulary(self, vocab_path):\n        """"""Save the tokenizer vocabulary to a directory or file.""""""\n        index = 0\n        if os.path.isdir(vocab_path):\n            vocab_file = os.path.join(vocab_path, VOCAB_NAME)\n        with open(vocab_file, ""w"", encoding=""utf-8"") as writer:\n            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: vocabulary indices are not consecutive.""\n                                   "" Please check that the vocabulary is not corrupted!"".format(vocab_file))\n                    index = token_index\n                writer.write(token + u\'\\n\')\n                index += 1\n        return vocab_file\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n            if \'-cased\' in pretrained_model_name_or_path and kwargs.get(\'do_lower_case\', True):\n                logger.warning(""The pre-trained model you are loading is a cased model but you have not set ""\n                               ""`do_lower_case` to False. We are setting `do_lower_case=False` for you but ""\n                               ""you may want to check this behavior."")\n                kwargs[\'do_lower_case\'] = False\n            elif \'-cased\' not in pretrained_model_name_or_path and not kwargs.get(\'do_lower_case\', True):\n                logger.warning(""The pre-trained model you are loading is an uncased model but you have set ""\n                               ""`do_lower_case` to False. We are setting `do_lower_case=True` for you ""\n                               ""but you may want to check this behavior."")\n                kwargs[\'do_lower_case\'] = True\n        else:\n            vocab_file = pretrained_model_name_or_path\n        if os.path.isdir(vocab_file):\n            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                ""associated to this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    vocab_file))\n            return None\n        if resolved_vocab_file == vocab_file:\n            logger.info(""loading vocabulary file {}"".format(vocab_file))\n        else:\n            logger.info(""loading vocabulary file {} from cache at {}"".format(\n                vocab_file, resolved_vocab_file))\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n            # if we\'re using a pretrained model, ensure the tokenizer wont index sequences longer\n            # than the number of positional embeddings\n            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name_or_path]\n            kwargs[\'max_len\'] = min(kwargs.get(\'max_len\', int(1e12)), max_len)\n        # Instantiate tokenizer.\n        tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)\n        return tokenizer\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self,\n                 do_lower_case=True,\n                 never_split=(""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]"")):\n        """"""Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        """"""\n        self.do_lower_case = do_lower_case\n        self.never_split = never_split\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text.""""""\n        text = self._clean_text(text)\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case and token not in self.never_split:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text):\n        """"""Splits punctuation on a piece of text.""""""\n        if text in self.never_split:\n            return [text]\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
examples/mnli_example/pytorch_pretrained_bert/tokenization_gpt2.py,0,"b'# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for OpenAI GPT.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport sys\nimport json\nimport logging\nimport os\nimport regex as re\nfrom io import open\n\ntry:\n    from functools import lru_cache\nexcept ImportError:\n    # Just a dummy decorator to get the checks to run on python2\n    # because honestly I don\'t want to support a byte-level unicode BPE tokenizer on python 2 right now.\n    def lru_cache():\n        return lambda func: func\n\nfrom .file_utils import cached_path\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\n    \'gpt2\': ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json"",\n}\nPRETRAINED_MERGES_ARCHIVE_MAP = {\n    \'gpt2\': ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"",\n}\nPRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n    \'gpt2\': 1024,\n}\nVOCAB_NAME = \'vocab.json\'\nMERGES_NAME = \'merges.txt\'\nSPECIAL_TOKENS_NAME = \'special_tokens.txt\'\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""\n    _chr = unichr if sys.version_info[0] == 2 else chr\n    bs = list(range(ord(""!""), ord(""~"")+1))+list(range(ord(""\xc2\xa1""), ord(""\xc2\xac"")+1))+list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [_chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass GPT2Tokenizer(object):\n    """"""\n    GPT-2 BPE tokenizer. Peculiarities:\n        - Byte-level BPE\n    """"""\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n            merges_file = PRETRAINED_MERGES_ARCHIVE_MAP[pretrained_model_name_or_path]\n            special_tokens_file = None\n        else:\n            vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)\n            merges_file = os.path.join(pretrained_model_name_or_path, MERGES_NAME)\n            special_tokens_file = os.path.join(pretrained_model_name_or_path, SPECIAL_TOKENS_NAME)\n            if not os.path.exists(special_tokens_file):\n                special_tokens_file = None\n            else:\n                logger.info(""loading special tokens file {}"".format(special_tokens_file))\n        # redirect to the cache, if necessary\n        try:\n            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n            resolved_merges_file = cached_path(merges_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} and {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    pretrained_model_name_or_path,\n                    vocab_file, merges_file))\n            return None\n        if resolved_vocab_file == vocab_file and resolved_merges_file == merges_file:\n            logger.info(""loading vocabulary file {}"".format(vocab_file))\n            logger.info(""loading merges file {}"".format(merges_file))\n        else:\n            logger.info(""loading vocabulary file {} from cache at {}"".format(\n                vocab_file, resolved_vocab_file))\n            logger.info(""loading merges file {} from cache at {}"".format(\n                merges_file, resolved_merges_file))\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n            # if we\'re using a pretrained model, ensure the tokenizer wont index sequences longer\n            # than the number of positional embeddings\n            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name_or_path]\n            kwargs[\'max_len\'] = min(kwargs.get(\'max_len\', int(1e12)), max_len)\n        # Instantiate tokenizer.\n        if special_tokens_file and \'special_tokens\' not in kwargs:\n            special_tokens = open(special_tokens_file, encoding=\'utf-8\').read().split(\'\\n\')[:-1]\n        else:\n            special_tokens = kwargs.pop(\'special_tokens\', [])\n        tokenizer = cls(resolved_vocab_file, resolved_merges_file, special_tokens=special_tokens, *inputs, **kwargs)\n        return tokenizer\n\n    def __init__(self, vocab_file, merges_file, errors=\'replace\', special_tokens=None, max_len=None):\n        self.max_len = max_len if max_len is not None else int(1e12)\n        self.encoder = json.load(open(vocab_file))\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        self.errors = errors # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n        bpe_data = open(merges_file, encoding=\'utf-8\').read().split(\'\\n\')[1:-1]\n        bpe_merges = [tuple(merge.split()) for merge in bpe_data]\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n        self.special_tokens = {}\n        self.special_tokens_decoder = {}\n        self.set_special_tokens(special_tokens)\n\n    def __len__(self):\n        return len(self.encoder) + len(self.special_tokens)\n\n    def set_special_tokens(self, special_tokens):\n        """""" Add a list of additional tokens to the encoder.\n            The additional tokens are indexed starting from the last index of the\n            current vocabulary in the order of the `special_tokens` list.\n        """"""\n        if not special_tokens:\n            self.special_tokens = {}\n            self.special_tokens_decoder = {}\n            return\n        self.special_tokens = dict((tok, len(self.encoder) + i) for i, tok in enumerate(special_tokens))\n        self.special_tokens_decoder = {v:k for k, v in self.special_tokens.items()}\n        logger.info(""Special tokens {}"".format(self.special_tokens))\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        self.cache[token] = word\n        return word\n\n    def tokenize(self, text):\n        """""" Tokenize a string. """"""\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = \'\'.join(self.byte_encoder[ord(b)] for b in token)\n            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\' \'))\n        return bpe_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        """""" Converts a sequence of tokens into ids using the vocab. """"""\n        ids = []\n        if isinstance(tokens, str) or (sys.version_info[0] == 2 and isinstance(tokens, unicode)):\n            if tokens in self.special_tokens:\n                return self.special_tokens[tokens]\n            else:\n                return self.encoder.get(tokens, 0)\n        for token in tokens:\n            if token in self.special_tokens:\n                ids.append(self.special_tokens[token])\n            else:\n                ids.append(self.encoder.get(token, 0))\n        if len(ids) > self.max_len:\n            logger.warning(\n                ""Token indices sequence length is longer than the specified maximum ""\n                "" sequence length for this OpenAI GPT model ({} > {}). Running this""\n                "" sequence through the model will result in indexing errors"".format(len(ids), self.max_len)\n            )\n        return ids\n\n    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n        """"""Converts a sequence of ids in BPE tokens using the vocab.""""""\n        tokens = []\n        for i in ids:\n            if i in self.special_tokens_decoder:\n                if not skip_special_tokens:\n                    tokens.append(self.special_tokens_decoder[i])\n            else:\n                tokens.append(self.decoder[i])\n        return tokens\n\n    def encode(self, text):\n        return self.convert_tokens_to_ids(self.tokenize(text))\n\n    def decode(self, tokens):\n        text = \'\'.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\'utf-8\', errors=self.errors)\n        return text\n\n    def save_vocabulary(self, vocab_path):\n        """"""Save the tokenizer vocabulary and merge files to a directory.""""""\n        if not os.path.isdir(vocab_path):\n            logger.error(""Vocabulary path ({}) should be a directory"".format(vocab_path))\n            return\n        vocab_file = os.path.join(vocab_path, VOCAB_NAME)\n        merge_file = os.path.join(vocab_path, MERGES_NAME)\n        special_tokens_file = os.path.join(vocab_path, SPECIAL_TOKENS_NAME)\n\n        with open(vocab_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.encoder, ensure_ascii=False))\n\n        index = 0\n        with open(merge_file, ""w"", encoding=""utf-8"") as writer:\n            writer.write(u\'#version: 0.2\\n\')\n            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: BPE merge indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(merge_file))\n                    index = token_index\n                writer.write(\' \'.join(bpe_tokens) + u\'\\n\')\n                index += 1\n\n        index = len(self.encoder)\n        with open(special_tokens_file, \'w\', encoding=\'utf-8\') as writer:\n            for token, token_index in sorted(self.special_tokens.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving special tokens vocabulary to {}: BPE indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(special_tokens_file))\n                    index = token_index\n                writer.write(token + u\'\\n\')\n                index += 1\n\n        return vocab_file, merge_file, special_tokens_file\n'"
examples/mnli_example/pytorch_pretrained_bert/tokenization_openai.py,0,"b'# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for OpenAI GPT.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport json\nimport logging\nimport os\nimport re\nimport sys\nfrom io import open\n\nfrom tqdm import tqdm\n\nfrom .file_utils import cached_path\nfrom .tokenization import BasicTokenizer\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\n    \'openai-gpt\': ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json"",\n}\nPRETRAINED_MERGES_ARCHIVE_MAP = {\n    \'openai-gpt\': ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt"",\n}\nPRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n    \'openai-gpt\': 512,\n}\nVOCAB_NAME = \'vocab.json\'\nMERGES_NAME = \'merges.txt\'\nSPECIAL_TOKENS_NAME = \'special_tokens.txt\'\n\ndef get_pairs(word):\n    """"""\n    Return set of symbol pairs in a word.\n    word is represented as tuple of symbols (symbols being variable-length strings)\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\ndef text_standardize(text):\n    """"""\n    fixes some issues the spacy tokenizer had on books corpus\n    also does some whitespace standardization\n    """"""\n    text = text.replace(\'\xe2\x80\x94\', \'-\')\n    text = text.replace(\'\xe2\x80\x93\', \'-\')\n    text = text.replace(\'\xe2\x80\x95\', \'-\')\n    text = text.replace(\'\xe2\x80\xa6\', \'...\')\n    text = text.replace(\'\xc2\xb4\', ""\'"")\n    text = re.sub(r\'\'\'(-+|~+|!+|""+|;+|\\?+|\\++|,+|\\)+|\\(+|\\\\+|\\/+|\\*+|\\[+|\\]+|}+|{+|\\|+|_+)\'\'\', r\' \\1 \', text)\n    text = re.sub(r\'\\s*\\n\\s*\', \' \\n \', text)\n    text = re.sub(r\'[^\\S\\n]+\', \' \', text)\n    return text.strip()\n\nclass OpenAIGPTTokenizer(object):\n    """"""\n    BPE tokenizer. Peculiarities:\n        - lower case all inputs\n        - uses SpaCy tokenizer and ftfy for pre-BPE tokenization if they are installed, fallback to BERT\'s BasicTokenizer if not.\n        - argument special_tokens and function set_special_tokens:\n            can be used to add additional symbols (ex: ""__classify__"") to a vocabulary.\n    """"""\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n            merges_file = PRETRAINED_MERGES_ARCHIVE_MAP[pretrained_model_name_or_path]\n            special_tokens_file = None\n        else:\n            vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)\n            merges_file = os.path.join(pretrained_model_name_or_path, MERGES_NAME)\n            special_tokens_file = os.path.join(pretrained_model_name_or_path, SPECIAL_TOKENS_NAME)\n            if not os.path.exists(special_tokens_file):\n                special_tokens_file = None\n            else:\n                logger.info(""loading special tokens file {}"".format(special_tokens_file))\n        # redirect to the cache, if necessary\n        try:\n            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n            resolved_merges_file = cached_path(merges_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} and {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    pretrained_model_name_or_path,\n                    vocab_file, merges_file))\n            return None\n        if resolved_vocab_file == vocab_file and resolved_merges_file == merges_file:\n            logger.info(""loading vocabulary file {}"".format(vocab_file))\n            logger.info(""loading merges file {}"".format(merges_file))\n        else:\n            logger.info(""loading vocabulary file {} from cache at {}"".format(\n                vocab_file, resolved_vocab_file))\n            logger.info(""loading merges file {} from cache at {}"".format(\n                merges_file, resolved_merges_file))\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n            # if we\'re using a pretrained model, ensure the tokenizer wont index sequences longer\n            # than the number of positional embeddings\n            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name_or_path]\n            kwargs[\'max_len\'] = min(kwargs.get(\'max_len\', int(1e12)), max_len)\n        # Instantiate tokenizer.\n        if special_tokens_file and \'special_tokens\' not in kwargs:\n            special_tokens = open(special_tokens_file, encoding=\'utf-8\').read().split(\'\\n\')[:-1]\n        else:\n            special_tokens = kwargs.pop(\'special_tokens\', [])\n        tokenizer = cls(resolved_vocab_file, resolved_merges_file, special_tokens=special_tokens, *inputs, **kwargs)\n        return tokenizer\n\n    def __init__(self, vocab_file, merges_file, special_tokens=None, max_len=None):\n        try:\n            import ftfy\n            import spacy\n            self.nlp = spacy.load(\'en\', disable=[\'parser\', \'tagger\', \'ner\', \'textcat\'])\n            self.fix_text = ftfy.fix_text\n        except ImportError:\n            logger.warning(""ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy."")\n            self.nlp = BasicTokenizer(do_lower_case=True,\n                                      never_split=special_tokens if special_tokens is not None else [])\n            self.fix_text = None\n\n        self.max_len = max_len if max_len is not None else int(1e12)\n        self.encoder = json.load(open(vocab_file, encoding=""utf-8""))\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        merges = open(merges_file, encoding=\'utf-8\').read().split(\'\\n\')[1:-1]\n        merges = [tuple(merge.split()) for merge in merges]\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {}\n        self.special_tokens = {}\n        self.special_tokens_decoder = {}\n        self.set_special_tokens(special_tokens)\n\n    def __len__(self):\n        return len(self.encoder) + len(self.special_tokens)\n\n    def set_special_tokens(self, special_tokens):\n        """""" Add a list of additional tokens to the encoder.\n            The additional tokens are indexed starting from the last index of the\n            current vocabulary in the order of the `special_tokens` list.\n        """"""\n        if not special_tokens:\n            self.special_tokens = {}\n            self.special_tokens_decoder = {}\n            return\n        self.special_tokens = dict((tok, len(self.encoder) + i) for i, tok in enumerate(special_tokens))\n        self.special_tokens_decoder = {v:k for k, v in self.special_tokens.items()}\n        if self.fix_text is None:\n            # Using BERT\'s BasicTokenizer: we can update the tokenizer\n            self.nlp.never_split = special_tokens\n        logger.info(""Special tokens {}"".format(self.special_tokens))\n\n    def bpe(self, token):\n        word = tuple(token[:-1]) + (token[-1] + \'</w>\',)\n        if token in self.cache:\n            return self.cache[token]\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+\'</w>\'\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        if word == \'\\n  </w>\':\n            word = \'\\n</w>\'\n        self.cache[token] = word\n        return word\n\n    def tokenize(self, text):\n        """""" Tokenize a string. """"""\n        split_tokens = []\n        if self.fix_text is None:\n            # Using BERT\'s BasicTokenizer\n            text = self.nlp.tokenize(text)\n            for token in text:\n                split_tokens.extend([t for t in self.bpe(token).split(\' \')])\n        else:\n            # Using SpaCy & ftfy (original tokenization process of OpenAI GPT)\n            text = self.nlp(text_standardize(self.fix_text(text)))\n            for token in text:\n                split_tokens.extend([t for t in self.bpe(token.text.lower()).split(\' \')])\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        """""" Converts a sequence of tokens into ids using the vocab. """"""\n        ids = []\n        if isinstance(tokens, str) or (sys.version_info[0] == 2 and isinstance(tokens, unicode)):\n            if tokens in self.special_tokens:\n                return self.special_tokens[tokens]\n            else:\n                return self.encoder.get(tokens, 0)\n        for token in tokens:\n            if token in self.special_tokens:\n                ids.append(self.special_tokens[token])\n            else:\n                ids.append(self.encoder.get(token, 0))\n        if len(ids) > self.max_len:\n            logger.warning(\n                ""Token indices sequence length is longer than the specified maximum ""\n                "" sequence length for this OpenAI GPT model ({} > {}). Running this""\n                "" sequence through the model will result in indexing errors"".format(len(ids), self.max_len)\n            )\n        return ids\n\n    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n        """"""Converts a sequence of ids in BPE tokens using the vocab.""""""\n        tokens = []\n        for i in ids:\n            if i in self.special_tokens_decoder:\n                if not skip_special_tokens:\n                    tokens.append(self.special_tokens_decoder[i])\n            else:\n                tokens.append(self.decoder[i])\n        return tokens\n\n    def encode(self, text):\n        return self.convert_tokens_to_ids(self.tokenize(text))\n\n    def decode(self, ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n        """"""Converts a sequence of ids in a string.""""""\n        tokens = self.convert_ids_to_tokens(ids, skip_special_tokens=skip_special_tokens)\n        out_string = \'\'.join(tokens).replace(\'</w>\', \' \').strip()\n        if clean_up_tokenization_spaces:\n            out_string = out_string.replace(\'<unk>\', \'\')\n            out_string = out_string.replace(\' .\', \'.\').replace(\' ?\', \'?\').replace(\' !\', \'!\').replace(\' ,\', \',\').replace(\' ,\', \',\'\n                    ).replace("" \' "", ""\'"").replace("" n\'t"", ""n\'t"").replace("" \'m"", ""\'m"").replace("" do not"", "" don\'t""\n                    ).replace("" \'s"", ""\'s"").replace("" \'ve"", ""\'ve"").replace("" \'re"", ""\'re"")\n        return out_string\n\n    def save_vocabulary(self, vocab_path):\n        """"""Save the tokenizer vocabulary and merge files to a directory.""""""\n        if not os.path.isdir(vocab_path):\n            logger.error(""Vocabulary path ({}) should be a directory"".format(vocab_path))\n            return\n        vocab_file = os.path.join(vocab_path, VOCAB_NAME)\n        merge_file = os.path.join(vocab_path, MERGES_NAME)\n        special_tokens_file = os.path.join(vocab_path, SPECIAL_TOKENS_NAME)\n\n        with open(vocab_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.encoder, ensure_ascii=False))\n\n        index = 0\n        with open(merge_file, ""w"", encoding=""utf-8"") as writer:\n            writer.write(u\'#version: 0.2\\n\')\n            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: BPE merge indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(merge_file))\n                    index = token_index\n                writer.write(\' \'.join(bpe_tokens) + u\'\\n\')\n                index += 1\n\n        index = len(self.encoder)\n        with open(special_tokens_file, \'w\', encoding=\'utf-8\') as writer:\n            for token, token_index in sorted(self.special_tokens.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving special tokens vocabulary to {}: BPE indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(special_tokens_file))\n                    index = token_index\n                writer.write(token + u\'\\n\')\n                index += 1\n\n        return vocab_file, merge_file, special_tokens_file\n'"
examples/mnli_example/pytorch_pretrained_bert/tokenization_transfo_xl.py,13,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Tokenization classes for Transformer XL model.\n    Adapted from https://github.com/kimiyoung/transformer-xl.\n""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport glob\nimport logging\nimport os\nimport sys\nfrom collections import Counter, OrderedDict\nfrom io import open\nimport unicodedata\n\nimport torch\nimport numpy as np\n\nfrom .file_utils import cached_path\n\nif sys.version_info[0] == 2:\n    import cPickle as pickle\nelse:\n    import pickle\n\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\n    \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-vocab.bin"",\n}\nVOCAB_NAME = \'vocab.bin\'\n\nPRETRAINED_CORPUS_ARCHIVE_MAP = {\n    \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-corpus.bin"",\n}\nCORPUS_NAME = \'corpus.bin\'\n\nclass TransfoXLTokenizer(object):\n    """"""\n    Transformer-XL tokenizer adapted from Vocab class in https://github.com/kimiyoung/transformer-xl\n    """"""\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a TransfoXLTokenizer.\n        The TransfoXLTokenizer.\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            if os.path.isdir(pretrained_model_name_or_path):\n                vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)\n            else:\n                vocab_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    pretrained_model_name_or_path,\n                    vocab_file))\n            return None\n        if resolved_vocab_file == vocab_file:\n            logger.info(""loading vocabulary file {}"".format(vocab_file))\n        else:\n            logger.info(""loading vocabulary file {} from cache at {}"".format(\n                vocab_file, resolved_vocab_file))\n\n        # Instantiate tokenizer.\n        tokenizer = cls(*inputs, **kwargs)\n        vocab_dict = torch.load(resolved_vocab_file)\n        for key, value in vocab_dict.items():\n            tokenizer.__dict__[key] = value\n        return tokenizer\n\n    def __init__(self, special=[], min_freq=0, max_size=None, lower_case=False,\n                 delimiter=None, vocab_file=None, never_split=(""<unk>"", ""<eos>"", ""<formula>"")):\n        self.counter = Counter()\n        self.special = special\n        self.min_freq = min_freq\n        self.max_size = max_size\n        self.lower_case = lower_case\n        self.delimiter = delimiter\n        self.vocab_file = vocab_file\n        self.never_split = never_split\n\n    def count_file(self, path, verbose=False, add_eos=False):\n        if verbose: print(\'counting file {} ...\'.format(path))\n        assert os.path.exists(path)\n\n        sents = []\n        with open(path, \'r\', encoding=\'utf-8\') as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                    print(\'    line {}\'.format(idx))\n                symbols = self.tokenize(line, add_eos=add_eos)\n                self.counter.update(symbols)\n                sents.append(symbols)\n\n        return sents\n\n    def count_sents(self, sents, verbose=False):\n        """"""\n            sents : a list of sentences, each a list of tokenized symbols\n        """"""\n        if verbose: print(\'counting {} sents ...\'.format(len(sents)))\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                print(\'    line {}\'.format(idx))\n            self.counter.update(symbols)\n\n    def _build_from_file(self, vocab_file):\n        self.idx2sym = []\n        self.sym2idx = OrderedDict()\n\n        with open(vocab_file, \'r\', encoding=\'utf-8\') as f:\n            for line in f:\n                symb = line.strip().split()[0]\n                self.add_symbol(symb)\n        if \'<UNK>\' in self.sym2idx:\n            self.unk_idx = self.sym2idx[\'<UNK>\']\n        elif \'<unk>\' in self.sym2idx:\n            self.unk_idx = self.sym2idx[\'<unk>\']\n        else:\n            raise ValueError(\'No <unkown> token in vocabulary\')\n\n    def save_vocabulary(self, vocab_path):\n        """"""Save the tokenizer vocabulary to a directory or file.""""""\n        index = 0\n        if os.path.isdir(vocab_path):\n            vocab_file = os.path.join(vocab_path, VOCAB_NAME)\n        torch.save(self.__dict__, vocab_file)\n        return vocab_file\n\n    def build_vocab(self):\n        if self.vocab_file:\n            print(\'building vocab from {}\'.format(self.vocab_file))\n            self._build_from_file(self.vocab_file)\n            print(\'final vocab size {}\'.format(len(self)))\n        else:\n            print(\'building vocab with min_freq={}, max_size={}\'.format(\n                self.min_freq, self.max_size))\n            self.idx2sym = []\n            self.sym2idx = OrderedDict()\n\n            for sym in self.special:\n                self.add_special(sym)\n\n            for sym, cnt in self.counter.most_common(self.max_size):\n                if cnt < self.min_freq: break\n                self.add_symbol(sym)\n\n            print(\'final vocab size {} from {} unique tokens\'.format(\n                len(self), len(self.counter)))\n\n    def encode_file(self, path, ordered=False, verbose=False, add_eos=True,\n            add_double_eos=False):\n        if verbose: print(\'encoding file {} ...\'.format(path))\n        assert os.path.exists(path)\n        encoded = []\n        with open(path, \'r\', encoding=\'utf-8\') as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                    print(\'    line {}\'.format(idx))\n                symbols = self.tokenize(line, add_eos=add_eos,\n                    add_double_eos=add_double_eos)\n                encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    def encode_sents(self, sents, ordered=False, verbose=False):\n        if verbose: print(\'encoding {} sents ...\'.format(len(sents)))\n        encoded = []\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                print(\'    line {}\'.format(idx))\n            encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    def add_special(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n            setattr(self, \'{}_idx\'.format(sym.strip(\'<>\')), self.sym2idx[sym])\n\n    def add_symbol(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n\n    def get_sym(self, idx):\n        assert 0 <= idx < len(self), \'Index {} out of vocabulary range\'.format(idx)\n        return self.idx2sym[idx]\n\n    def get_idx(self, sym):\n        if sym in self.sym2idx:\n            return self.sym2idx[sym]\n        else:\n            # print(\'encounter unk {}\'.format(sym))\n            # assert \'<eos>\' not in sym\n            if hasattr(self, \'unk_idx\'):\n                return self.sym2idx.get(sym, self.unk_idx)\n            # Backward compatibility with pre-trained models\n            elif \'<unk>\' in self.sym2idx:\n                return self.sym2idx[\'<unk>\']\n            elif \'<UNK>\' in self.sym2idx:\n                return self.sym2idx[\'<UNK>\']\n            else:\n                raise ValueError(\'Token not in vocabulary and no <unk> token in vocabulary for replacement\')\n\n    def convert_ids_to_tokens(self, indices):\n        """"""Converts a sequence of indices in symbols using the vocab.""""""\n        return [self.get_sym(idx) for idx in indices]\n\n    def convert_tokens_to_ids(self, symbols):\n        """"""Converts a sequence of symbols into ids using the vocab.""""""\n        return [self.get_idx(sym) for sym in symbols]\n\n    def convert_to_tensor(self, symbols):\n        return torch.LongTensor(self.convert_tokens_to_ids(symbols))\n\n    def decode(self, indices, exclude=None):\n        """"""Converts a sequence of indices in a string.""""""\n        if exclude is None:\n            return \' \'.join([self.get_sym(idx) for idx in indices])\n        else:\n            return \' \'.join([self.get_sym(idx) for idx in indices if idx not in exclude])\n\n    def __len__(self):\n        return len(self.idx2sym)\n\n    def tokenize(self, line, add_eos=False, add_double_eos=False):\n        line = line.strip()\n        # convert to lower case\n        if self.lower_case:\n            line = line.lower()\n\n        # empty delimiter \'\' will evaluate False\n        if self.delimiter == \'\':\n            symbols = line\n        else:\n            symbols = line.split(self.delimiter)\n\n        if add_double_eos: # lm1b\n            return [\'<S>\'] + symbols + [\'<S>\']\n        elif add_eos:\n            return symbols + [\'<eos>\']\n        else:\n            return symbols\n\n\nclass LMOrderedIterator(object):\n    def __init__(self, data, bsz, bptt, device=\'cpu\', ext_len=None):\n        """"""\n            data -- LongTensor -- the LongTensor is strictly ordered\n        """"""\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n\n        # Work out how cleanly we can divide the dataset into bsz parts.\n        self.n_step = data.size(0) // bsz\n\n        # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n        data = data.narrow(0, 0, self.n_step * bsz)\n\n        # Evenly divide the data across the bsz batches.\n        self.data = data.view(bsz, -1).t().contiguous().to(device)\n\n        # Number of mini-batches\n        self.n_batch = (self.n_step + self.bptt - 1) // self.bptt\n\n    def get_batch(self, i, bptt=None):\n        if bptt is None: bptt = self.bptt\n        seq_len = min(bptt, self.data.size(0) - 1 - i)\n\n        end_idx = i + seq_len\n        beg_idx = max(0, i - self.ext_len)\n\n        data = self.data[beg_idx:end_idx]\n        target = self.data[i+1:i+1+seq_len]\n\n        data_out = data.transpose(0, 1).contiguous().to(self.device)\n        target_out = target.transpose(0, 1).contiguous().to(self.device)\n\n        return data_out, target_out, seq_len\n\n    def get_fixlen_iter(self, start=0):\n        for i in range(start, self.data.size(0) - 1, self.bptt):\n            yield self.get_batch(i)\n\n    def get_varlen_iter(self, start=0, std=5, min_len=5, max_deviation=3):\n        max_len = self.bptt + max_deviation * std\n        i = start\n        while True:\n            bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.\n            bptt = min(max_len, max(min_len, int(np.random.normal(bptt, std))))\n            data, target, seq_len = self.get_batch(i, bptt)\n            i += seq_len\n            yield data, target, seq_len\n            if i >= self.data.size(0) - 2:\n                break\n\n    def __iter__(self):\n        return self.get_fixlen_iter()\n\n\nclass LMShuffledIterator(object):\n    def __init__(self, data, bsz, bptt, device=\'cpu\', ext_len=None, shuffle=False):\n        """"""\n            data -- list[LongTensor] -- there is no order among the LongTensors\n        """"""\n        self.data = data\n\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n        self.shuffle = shuffle\n\n    def get_sent_stream(self):\n        # index iterator\n        epoch_indices = np.random.permutation(len(self.data)) if self.shuffle \\\n            else np.array(range(len(self.data)))\n\n        # sentence iterator\n        for idx in epoch_indices:\n            yield self.data[idx]\n\n    def stream_iterator(self, sent_stream):\n        # streams for each data in the batch\n        streams = [None] * self.bsz\n\n        data = torch.LongTensor(self.bptt, self.bsz)\n        target = torch.LongTensor(self.bptt, self.bsz)\n\n        n_retain = 0\n\n        while True:\n            # data   : [n_retain+bptt x bsz]\n            # target : [bptt x bsz]\n            data[n_retain:].fill_(-1)\n            target.fill_(-1)\n\n            valid_batch = True\n\n            for i in range(self.bsz):\n                n_filled = 0\n                try:\n                    while n_filled < self.bptt:\n                        if streams[i] is None or len(streams[i]) <= 1:\n                            streams[i] = next(sent_stream)\n                        # number of new tokens to fill in\n                        n_new = min(len(streams[i]) - 1, self.bptt - n_filled)\n                        # first n_retain tokens are retained from last batch\n                        data[n_retain+n_filled:n_retain+n_filled+n_new, i] = \\\n                            streams[i][:n_new]\n                        target[n_filled:n_filled+n_new, i] = \\\n                            streams[i][1:n_new+1]\n                        streams[i] = streams[i][n_new:]\n                        n_filled += n_new\n                except StopIteration:\n                    valid_batch = False\n                    break\n\n            if not valid_batch:\n                return\n\n            data_out = data.transpose(0, 1).contiguous().to(self.device)\n            target_out = target.transpose(0, 1).contiguous().to(self.device)\n\n            yield data_out, target_out, self.bptt\n\n            n_retain = min(data.size(0), self.ext_len)\n            if n_retain > 0:\n                data[:n_retain] = data[-n_retain:]\n            data.resize_(n_retain + self.bptt, data.size(1))\n\n    def __iter__(self):\n        # sent_stream is an iterator\n        sent_stream = self.get_sent_stream()\n\n        for batch in self.stream_iterator(sent_stream):\n            yield batch\n\n\nclass LMMultiFileIterator(LMShuffledIterator):\n    def __init__(self, paths, vocab, bsz, bptt, device=\'cpu\', ext_len=None,\n        shuffle=False):\n\n        self.paths = paths\n        self.vocab = vocab\n\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n        self.shuffle = shuffle\n\n    def get_sent_stream(self, path):\n        sents = self.vocab.encode_file(path, add_double_eos=True)\n        if self.shuffle:\n            np.random.shuffle(sents)\n        sent_stream = iter(sents)\n\n        return sent_stream\n\n    def __iter__(self):\n        if self.shuffle:\n            np.random.shuffle(self.paths)\n\n        for path in self.paths:\n            # sent_stream is an iterator\n            sent_stream = self.get_sent_stream(path)\n            for batch in self.stream_iterator(sent_stream):\n                yield batch\n\n\nclass TransfoXLCorpus(object):\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a pre-processed corpus.\n        """"""\n        vocab = TransfoXLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        if pretrained_model_name_or_path in PRETRAINED_CORPUS_ARCHIVE_MAP:\n            corpus_file = PRETRAINED_CORPUS_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            corpus_file = os.path.join(pretrained_model_name_or_path, CORPUS_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_corpus_file = cached_path(corpus_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Corpus \'{}\' was not found in corpus list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    pretrained_model_name_or_path,\n                    corpus_file))\n            return None\n        if resolved_corpus_file == corpus_file:\n            logger.info(""loading corpus file {}"".format(corpus_file))\n        else:\n            logger.info(""loading corpus file {} from cache at {}"".format(\n                corpus_file, resolved_corpus_file))\n\n        # Instantiate tokenizer.\n        corpus = cls(*inputs, **kwargs)\n        corpus_dict = torch.load(resolved_corpus_file)\n        for key, value in corpus_dict.items():\n            corpus.__dict__[key] = value\n        corpus.vocab = vocab\n        if corpus.train is not None:\n            corpus.train = torch.tensor(corpus.train, dtype=torch.long)\n        if corpus.valid is not None:\n            corpus.valid = torch.tensor(corpus.valid, dtype=torch.long)\n        if corpus.test is not None:\n            corpus.test = torch.tensor(corpus.test, dtype=torch.long)\n        return corpus\n\n    def __init__(self, *args, **kwargs):\n        self.vocab = TransfoXLTokenizer(*args, **kwargs)\n        self.dataset = None\n        self.train = None\n        self.valid = None\n        self.test = None\n\n    def build_corpus(self, path, dataset):\n        self.dataset = dataset\n\n        if self.dataset in [\'ptb\', \'wt2\', \'enwik8\', \'text8\']:\n            self.vocab.count_file(os.path.join(path, \'train.txt\'))\n            self.vocab.count_file(os.path.join(path, \'valid.txt\'))\n            self.vocab.count_file(os.path.join(path, \'test.txt\'))\n        elif self.dataset == \'wt103\':\n            self.vocab.count_file(os.path.join(path, \'train.txt\'))\n        elif self.dataset == \'lm1b\':\n            train_path_pattern = os.path.join(\n                path, \'1-billion-word-language-modeling-benchmark-r13output\',\n                \'training-monolingual.tokenized.shuffled\', \'news.en-*\')\n            train_paths = glob.glob(train_path_pattern)\n            # the vocab will load from file when build_vocab() is called\n\n        self.vocab.build_vocab()\n\n        if self.dataset in [\'ptb\', \'wt2\', \'wt103\']:\n            self.train = self.vocab.encode_file(\n                os.path.join(path, \'train.txt\'), ordered=True)\n            self.valid = self.vocab.encode_file(\n                os.path.join(path, \'valid.txt\'), ordered=True)\n            self.test = self.vocab.encode_file(\n                os.path.join(path, \'test.txt\'), ordered=True)\n        elif self.dataset in [\'enwik8\', \'text8\']:\n            self.train = self.vocab.encode_file(\n                os.path.join(path, \'train.txt\'), ordered=True, add_eos=False)\n            self.valid = self.vocab.encode_file(\n                os.path.join(path, \'valid.txt\'), ordered=True, add_eos=False)\n            self.test = self.vocab.encode_file(\n                os.path.join(path, \'test.txt\'), ordered=True, add_eos=False)\n        elif self.dataset == \'lm1b\':\n            self.train = train_paths\n            self.valid = self.vocab.encode_file(\n                os.path.join(path, \'valid.txt\'), ordered=False, add_double_eos=True)\n            self.test = self.vocab.encode_file(\n                os.path.join(path, \'test.txt\'), ordered=False, add_double_eos=True)\n\n    def get_iterator(self, split, *args, **kwargs):\n        if split == \'train\':\n            if self.dataset in [\'ptb\', \'wt2\', \'wt103\', \'enwik8\', \'text8\']:\n                data_iter = LMOrderedIterator(self.train, *args, **kwargs)\n            elif self.dataset == \'lm1b\':\n                kwargs[\'shuffle\'] = True\n                data_iter = LMMultiFileIterator(self.train, self.vocab, *args, **kwargs)\n        elif split in [\'valid\', \'test\']:\n            data = self.valid if split == \'valid\' else self.test\n            if self.dataset in [\'ptb\', \'wt2\', \'wt103\', \'enwik8\', \'text8\']:\n                data_iter = LMOrderedIterator(data, *args, **kwargs)\n            elif self.dataset == \'lm1b\':\n                data_iter = LMShuffledIterator(data, *args, **kwargs)\n\n        return data_iter\n\n\ndef get_lm_corpus(datadir, dataset):\n    fn = os.path.join(datadir, \'cache.pt\')\n    fn_pickle = os.path.join(datadir, \'cache.pkl\')\n    if os.path.exists(fn):\n        print(\'Loading cached dataset...\')\n        corpus = torch.load(fn_pickle)\n    elif os.path.exists(fn):\n        print(\'Loading cached dataset from pickle...\')\n        with open(fn, ""rb"") as fp:\n            corpus = pickle.load(fp)\n    else:\n        print(\'Producing dataset {}...\'.format(dataset))\n        kwargs = {}\n        if dataset in [\'wt103\', \'wt2\']:\n            kwargs[\'special\'] = [\'<eos>\']\n            kwargs[\'lower_case\'] = False\n        elif dataset == \'ptb\':\n            kwargs[\'special\'] = [\'<eos>\']\n            kwargs[\'lower_case\'] = True\n        elif dataset == \'lm1b\':\n            kwargs[\'special\'] = []\n            kwargs[\'lower_case\'] = False\n            kwargs[\'vocab_file\'] = os.path.join(datadir, \'1b_word_vocab.txt\')\n        elif dataset in [\'enwik8\', \'text8\']:\n            pass\n\n        corpus = TransfoXLCorpus(datadir, dataset, **kwargs)\n        torch.save(corpus, fn)\n\n    return corpus\n'"
