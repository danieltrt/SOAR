file_path,api_count,code
WRN.py,4,"b""## Wide ResNet with FreezeOut\n# Based on code by xternalz: https://github.com/xternalz/WideResNet-pytorch\n# WRN by Sergey Zagoruyko and Nikos Komodakis\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nfrom utils import scale_fn\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate,layer_index):\n        super(BasicBlock, self).__init__()\n        \n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                               padding=0, bias=False) or None\n                               \n        # If the layer is being trained or not\n        self.active = True\n        \n        # The layer index relative to the rest of the net\n        self.layer_index = layer_index\n        \n    def forward(self, x):\n    \n        if not self.active:\n            self.eval()\n            \n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        out = torch.add(x if self.equalInOut else self.convShortcut(x), out)\n        if self.active:\n            return out\n        else:\n            return out.detach()\n    \n    \n\n\n\n# note: we call it DenseNet for simple compatibility with the training code.\n# similar we call it growthRate instead of widen_factor\nclass DenseNet(nn.Module):\n    def __init__(self, growthRate, depth, nClasses, epochs, t_0, scale_lr=True, how_scale = 'cubic', const_time=False, dropRate=0.0):\n        super(DenseNet, self).__init__()\n        \n        widen_factor=growthRate\n        num_classes = nClasses\n        self.epochs = epochs\n        self.t_0 = t_0\n        self.scale_lr = scale_lr\n        self.how_scale = how_scale\n        self.const_time = const_time\n        \n        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n        assert((depth - 4) % 6 == 0)\n        n = int((depth - 4) / 6)\n        # print(type(n))\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.conv1.layer_index = 0\n        self.conv1.active = True\n        self.layer_index = 1\n        \n        # 1st block\n        self.block1 = self._make_layer(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = self._make_layer(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = self._make_layer(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n        \n        self.bn1.active=True\n        self.fc.active=True\n        self.bn1.layer_index = self.layer_index\n        self.fc.layer_index = self.layer_index\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n            \n            if hasattr(m,'active'):\n                m.lr_ratio = scale_fn[self.how_scale](self.t_0 + (1 - self.t_0) * float(m.layer_index) / self.layer_index)\n                m.max_j = self.epochs * 1000 * m.lr_ratio\n                \n                # Optionally scale the learning rates to have the same total\n                # distance traveled (modulo the gradients).\n                m.lr = 1e-1 / m.lr_ratio if self.scale_lr else 1e-1\n                \n        # Optimizer\n        self.optim = optim.SGD([{'params':m.parameters(), 'lr':m.lr, 'layer_index':m.layer_index} for m in self.modules() if hasattr(m,'active')],  \n                         nesterov=True,momentum=0.9, weight_decay=1e-4)\n        # Iteration Counter            \n        self.j = 0  \n\n        # A simple dummy variable that indicates we are using an iteration-wise\n        # annealing scheme as opposed to epoch-wise. \n        self.lr_sched = {'itr':0}\n    def _make_layer(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        layers = []\n        print(nb_layers,type(nb_layers))\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate,self.layer_index))\n            self.layer_index +=1\n        return nn.Sequential(*layers)\n    \n    def update_lr(self):\n    \n        # Loop over all modules\n        for m in self.modules():\n        \n            # If a module is active:\n            if hasattr(m,'active') and m.active:\n            \n                # If we've passed this layer's freezing point, deactivate it.\n                if self.j > m.max_j: \n                    m.active = False\n                    \n                    # Also make sure we remove all this layer from the optimizer\n                    for i,group in enumerate(self.optim.param_groups):\n                        if group['layer_index']==m.layer_index:\n                            self.optim.param_groups.remove(group)\n                \n                # If not, update the LR\n                else:\n                    for i,group in enumerate(self.optim.param_groups):\n                        if group['layer_index']==m.layer_index:\n                            self.optim.param_groups[i]['lr'] = (0.05/m.lr_ratio)*(1+np.cos(np.pi*self.j/m.max_j))\\\n                                                              if self.scale_lr else 0.05 * (1+np.cos(np.pi*self.j/m.max_j))\n        self.j += 1   \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(-1, self.nChannels)\n        return F.log_softmax(self.fc(out))"""
densenet.py,8,"b""## DenseNet with FreezeOut. \n## Adopted from Brandon Amos: https://github.com/bamos/densenet.pytorch\nimport torch\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nimport torchvision.models as models\n\nimport sys\nimport math\nimport numpy as np\n\nfrom utils import scale_fn,calc_speedup\n\nclass Bottleneck(nn.Module):\n    def __init__(self, nChannels, growthRate,layer_index):\n        super(Bottleneck, self).__init__()\n        interChannels = 4*growthRate\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(interChannels)\n        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n                               padding=1, bias=False)\n\n        # If the layer is still being trained\n        self.active=True\n        \n        # The index of this layer relative to the overall net\n        self.layer_index=layer_index\n        \n    def forward(self, x):\n    \n        # If we're not training this layer, set to eval mode so that we use\n        # running batchnorm stats (both for time-saving and to avoid updating\n        # said stats).\n        if not self.active:\n            self.eval()\n            \n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = torch.cat((x, out), 1)\n        \n        # If we're not active, return a detached output to prevent backprop.\n        if self.active: \n            return out\n        else:\n            return out.detach()\n\nclass SingleLayer(nn.Module):\n    def __init__(self, nChannels, growthRate, layer_index):\n        super(SingleLayer, self).__init__()\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n                               padding=1, bias=False)\n        \n        # Current Layer Index\n        self.layer_index = layer_index\n        # If the layer is being trained or not\n        self.active = True\n            \n    def forward(self, x):\n        if not self.active:\n            self.eval()\n        out = self.conv1(self.bn1(F.relu(x)))\n        out = torch.cat((x, out), 1)\n        if self.active:\n            return out\n        else:\n            return out.detach()\n\nclass Transition(nn.Module):\n    def __init__(self, nChannels, nOutChannels, layer_index):\n        super(Transition, self).__init__()\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n                               bias=False)\n\n        # If the layer is being trained or not\n        self.active = True\n        \n        # The layer index relative to the rest of the net\n        self.layer_index = layer_index\n\n    def forward(self, x):\n    \n        # If we're not training this layer, set to eval mode so that we use\n        # running batchnorm stats (both for time-saving and to avoid updating\n        # said stats).\n        if not self.active:\n            self.eval()\n        out = self.conv1(self.bn1(F.relu(x)))\n        out = F.avg_pool2d(out, 2)\n        \n        # If we're not active, return a detached output to prevent backprop.\n        if self.active:\n            return out\n        else:\n            return out.detach()\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, growthRate, depth, nClasses, epochs, t_0, scale_lr=True, how_scale = 'cubic',const_time=False,reduction=0.5, bottleneck=True):\n        super(DenseNet, self).__init__()\n        \n        self.epochs = epochs\n        self.t_0 = t_0\n        self.scale_lr = scale_lr\n        self.how_scale = how_scale\n        self.const_time = const_time\n        \n        nDenseBlocks = (depth-4) // 3\n        if bottleneck:\n            nDenseBlocks //= 2\n            \n        # Calculate the speedup\n        speedup = calc_speedup(growthRate,nDenseBlocks,t_0,how_scale)\n        print('Estimated speedup is '+str((np.round(100*speedup)))+'%.')\n        \n        # Optionally scale the epochs based on the speedup so we train for\n        # the same approximate wall-clock time.\n        if self.const_time:\n            self.epochs /= 1-speedup    \n        \n        \n        nChannels = 2*growthRate\n        self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1,\n                               bias=False)\n        self.conv1.layer_index = 0\n        self.conv1.active=True\n        self.layer_index = 1\n        \n        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks*growthRate\n        nOutChannels = int(math.floor(nChannels*reduction))\n        self.trans1 = Transition(nChannels, nOutChannels,self.layer_index)\n        self.layer_index += 1\n\n        nChannels = nOutChannels\n        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks*growthRate\n        nOutChannels = int(math.floor(nChannels*reduction))\n        self.trans2 = Transition(nChannels, nOutChannels, self.layer_index)\n        self.layer_index += 1\n\n        nChannels = nOutChannels\n        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks*growthRate\n\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.fc = nn.Linear(nChannels, nClasses)\n        \n        # Set bn and fc layers to active, permanently. Have them share a layer\n        # index with the last conv layer.\n        self.bn1.active=True\n        self.fc.active=True\n        self.bn1.layer_index = self.layer_index\n        self.fc.layer_index = self.layer_index\n        \n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n            \n\n            # Set the layerwise scaling and annealing parameters\n            if hasattr(m,'active'):\n                m.lr_ratio = scale_fn[self.how_scale](self.t_0 + (1 - self.t_0) * float(m.layer_index) / self.layer_index)\n                m.max_j = self.epochs * 1000 * m.lr_ratio\n                \n                # Optionally scale the learning rates to have the same total\n                # distance traveled (modulo the gradients).\n                m.lr = 1e-1 / m.lr_ratio if self.scale_lr else 1e-1\n                \n        # Optimizer\n        self.optim = optim.SGD([{'params':m.parameters(), 'lr':m.lr, 'layer_index':m.layer_index} for m in self.modules() if hasattr(m,'active')],  \n                         nesterov=True,momentum=0.9, weight_decay=1e-4)\n        # Iteration Counter            \n        self.j = 0  \n\n        # A simple dummy variable that indicates we are using an iteration-wise\n        # annealing scheme as opposed to epoch-wise. \n        self.lr_sched = {'itr':0}\n                             \n    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n        layers = []\n        for i in range(int(nDenseBlocks)):\n            if bottleneck:\n                layers.append(Bottleneck(nChannels, growthRate, self.layer_index))                \n            else:\n                layers.append(SingleLayer(nChannels, growthRate, self.layer_index))\n            nChannels += growthRate\n            self.layer_index += 1\n        return nn.Sequential(*layers)\n\n    def update_lr(self):\n    \n        # Loop over all modules\n        for m in self.modules():\n        \n            # If a module is active:\n            if hasattr(m,'active') and m.active:\n            \n                # If we've passed this layer's freezing point, deactivate it.\n                if self.j > m.max_j: \n                    m.active = False\n                    \n                    # Also make sure we remove all this layer from the optimizer\n                    for i,group in enumerate(self.optim.param_groups):\n                        if group['layer_index']==m.layer_index:\n                            self.optim.param_groups.remove(group)\n                \n                # If not, update the LR\n                else:\n                    for i,group in enumerate(self.optim.param_groups):\n                        if group['layer_index']==m.layer_index:\n                            self.optim.param_groups[i]['lr'] = (0.05/m.lr_ratio)*(1+np.cos(np.pi*self.j/m.max_j))\\\n                                                              if self.scale_lr else 0.05 * (1+np.cos(np.pi*self.j/m.max_j))\n        \n        # Update the iteration counter\n        self.j += 1\n    \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.dense3(out)\n        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n        out = F.log_softmax(self.fc(out))\n        return out"""
train.py,10,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n'''\nFreezeOut Training Function\nAndy Brock, 2017\n\nThis script trains and tests a model using FreezeOut to accelerate training\nby progressively freezing early layers and excluding them from the backward\npass. It has command-line options for defining the phase-out strategy, including\nhow far into training to start phasing out layers, whether to scale \ninitial learning rates as a function of how long the layer is trained for,\nand how the phase out schedule is defined for layers after the first (i.e. are\nlayers frozen at regular intervals or is cubically more time given to later\nlayers?)\n\nBased on Jan Schl\xc3\xbcter's DenseNet training code:\nhttps://github.com/Lasagne/Recipes/blob/master/papers/densenet\n'''\n\nimport os\nimport logging\nimport sys\nfrom argparse import ArgumentParser\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nfrom utils import get_data_loader, MetricsLogger, progress\n# Set the recursion limit to avoid problems with deep nets\nsys.setrecursionlimit(5000)\n\n\ndef opts_parser():\n    usage = 'Trains and tests a FreezeOut DenseNet on CIFAR.'\n    parser = ArgumentParser(description=usage)\n    parser.add_argument(\n        '-L', '--depth', type=int, default=76,\n        help='Network depth in layers (default: %(default)s)')\n    parser.add_argument(\n        '-k', '--growth-rate', type=int, default=12,\n        help='Growth rate in dense blocks (default: %(default)s)')\n    parser.add_argument(\n        '--dropout', type=float, default=0,\n        help='Dropout rate (default: %(default)s)')\n    parser.add_argument(\n        '--augment', action='store_true', default=True,\n        help='Perform data augmentation (enabled by default)')\n    parser.add_argument(\n        '--no-augment', action='store_false', dest='augment',\n        help='Disable data augmentation')\n    parser.add_argument(\n        '--validate', action='store_true', default=True,\n        help='Perform validation on validation set (ensabled by default)')\n    parser.add_argument(\n        '--no-validate', action='store_false', dest='validate',\n        help='Disable validation')\n    parser.add_argument(\n        '--validate-test', action='store_const', dest='validate',\n        const='test', help='Evaluate on test set after every epoch.')\n    parser.add_argument(\n        '--epochs', type=int, default=100,\n        help='Number of training epochs (default: %(default)s)')\n    parser.add_argument(\n        '--t_0', type=float, default=0.8,\n        help=('How far into training to start freezing. Note that this if using'\n              +' cubic scaling then this is the uncubed value.'))\n    parser.add_argument(\n        '--scale_lr', type=bool, default=True,\n        help='Scale each layer''s start LR as a function of its t_0 value?')\n    parser.add_argument(\n        '--no_scale', action='store_false', dest='scale_lr',\n        help='Don''t scale each layer''s start LR as a function of its t_0 value')\n    parser.add_argument(\n        '--how_scale',type=str,default='cubic',\n        help=('How to relatively scale the schedule of each subsequent layer.'\n              +'options: linear, squared, cubic.'))\n    parser.add_argument(\n        '--const_time', type=bool, default=False,\n        help='Scale the #epochs as a function of ice to match wall clock time.')\n    parser.add_argument(\n        '--seed', type=int, default=0,\n        help='Random seed to use.')\n    parser.add_argument(\n        '--which_dataset', type=int, default=100,\n        help='Which Dataset to train on (default: %(default)s)')\n    parser.add_argument(\n        '--batch_size', type=int, default=50,\n        help='Images per batch (default: %(default)s)')\n    parser.add_argument(\n        '--resume', type=bool, default=False,\n        help='Whether or not to resume training')\n    parser.add_argument(\n        '--model', type=str, default='densenet', metavar='FILE',\n        help='Which model to use')\n    parser.add_argument(\n        '--save-weights', type=str, default='default_save', metavar='FILE',\n        help='Save network weights to given .pth file')\n    return parser\n\n\n\n\n\ndef train_test(depth, growth_rate, dropout, augment,\n               validate, epochs, save_weights, batch_size, \n               t_0, seed, scale_lr, how_scale, which_dataset, \n               const_time, resume, model):\n    \n    # Update save_weights:\n    if save_weights=='default_save':\n        save_weights = (model + '_k' + str(growth_rate) + 'L' + str(depth)\n                        + '_ice' + str(int(100*t_0)) + '_'+how_scale + str(scale_lr)\n                        + '_seed' + str(seed) + '_epochs' + str(epochs) \n                        + 'C' + str(which_dataset))\n    # Seed RNG\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    \n    # Name of the file to which we're saving losses and errors.\n    metrics_fname = 'logs/'+save_weights + '_log.jsonl'\n    logging.basicConfig(level=logging.INFO,\n                        format='%(asctime)s %(levelname)s| %(message)s')\n    logging.info('Metrics will be saved to {}'.format(metrics_fname))\n    logging.info('Running with seed ' + str(seed) + ', t_0 of ' + str(t_0)\n                + ', and the ' + how_scale + ' scaling method '\n                + 'with learning rate scaling set to ' + str(scale_lr) + '.') \n    mlog = MetricsLogger(metrics_fname, reinitialize=(not resume))\n   \n    # Import the model module\n    model_module = __import__(model)\n\n    # Get information specific to each dataset\n    train_loader,test_loader = get_data_loader(which_dataset, augment,\n                                               validate, batch_size)\n\n    # Build network, either by initializing it or loading a pre-trained\n    # network.\n    if resume:\n        logging.info('loading network ' + save_weights + '...')\n        net = torch.load(save_weights + '.pth')\n\n        # Which epoch we're starting from\n        start_epoch = net.epoch+1 if hasattr(net, 'epoch') else 0\n\n    #  Get net\n    else:\n        logging.info('Instantiating network with model ' + model + '...')\n        net = model_module.DenseNet(growth_rate, depth=depth,\n                                    nClasses=which_dataset,\n                                    epochs=epochs,\n                                    t_0 = t_0,\n                                    scale_lr = scale_lr,\n                                    how_scale = how_scale,\n                                    const_time = const_time)\n        net = net.cuda()\n        start_epoch = 0\n    \n\n\n    logging.info('Number of params: {}'.format(\n                 sum([p.data.nelement() for p in net.parameters()]))\n                 )\n\n    # Training Function, presently only returns training loss\n    # x: input data\n    # y: target labels\n    def train_fn(x, y):\n        net.optim.zero_grad()\n        output = net(V(x.cuda()))\n        loss = F.nll_loss(output, V(y.cuda()))\n        loss.backward()\n        net.optim.step()\n        return loss.data[0]\n\n    # Testing function, returns test loss and test error for a batch\n    # x: input data\n    # y: target labels\n    def test_fn(x, y):\n        output = net(V(x.cuda(), volatile=True))\n        test_loss = F.nll_loss(output, V(y.cuda(), volatile=True)).data[0]\n\n        # Get the index of the max log-probability as the prediction.\n        pred = output.data.max(1)[1].cpu()\n        test_error = pred.ne(y).sum()\n\n        return test_loss, test_error\n\n    # Finally, launch the training loop.\n    logging.info('Starting training at epoch '+str(start_epoch)+'...')\n    for epoch in range(start_epoch, net.epochs):\n\n        # Pin the current epoch on the network.\n        net.epoch = epoch\n\n        # shrink learning rate at scheduled intervals, if desired\n        if 'epoch' in net.lr_sched and epoch in net.lr_sched['epoch']:\n\n            logging.info('Annealing learning rate...')\n\n            # Optionally checkpoint at annealing\n            # if net.checkpoint_before_anneal:\n                # torch.save(net, str(epoch) + '_' + save_weights + '.pth')\n\n            for param_group in net.optim.param_groups:\n                param_group['lr'] *= 0.1\n\n        # List where we'll store training loss\n        train_loss = []\n\n        # Prepare the training data\n        batches = progress(\n            train_loader, desc='Epoch %d/%d, Batch ' % (epoch + 1, net.epochs),\n            total=len(train_loader.dataset) // batch_size)\n\n        # Put the network into training mode\n        net.train()\n    \n        # Execute training pass\n        for x, y in batches:\n        \n            # Update LR if using cosine annealing\n            if 'itr' in net.lr_sched:\n                net.update_lr()\n                \n            train_loss.append(train_fn(x, y))\n\n        # Report training metrics\n        train_loss = float(np.mean(train_loss))\n        print('  training loss:\\t%.6f' % train_loss)\n        mlog.log(epoch=epoch, train_loss=float(train_loss))\n        \n        # Check how many layers are active\n        actives = 0\n        for m in net.modules():\n            if hasattr(m,'active') and m.active:\n                actives += 1\n        logging.info('Currently have ' + str(actives) + ' active layers...')\n        \n        # Optionally, take a pass over the validation or test set.\n        if validate:\n\n            # Lists to store\n            val_loss = []\n            val_err = err = []\n\n            # Set network into evaluation mode\n            net.eval()\n\n            # Execute validation pass\n            for x, y in test_loader:\n                loss, err = test_fn(x, y)\n                val_loss.append(loss)\n                val_err.append(err)\n\n            # Report validation metrics\n            val_loss = float(np.mean(val_loss))\n            val_err =  100 * float(np.sum(val_err)) / len(test_loader.dataset)\n            print('  validation loss:\\t%.6f' % val_loss)\n            print('  validation error:\\t%.2f%%' % val_err)\n            mlog.log(epoch=epoch, val_loss=val_loss, val_err=val_err)\n\n        # Save weights for this epoch\n        print('saving weights to ' + save_weights + '...')\n        torch.save(net, save_weights + '.pth')\n\n    # At the end of it all, save weights even if we didn't checkpoint.\n    if save_weights:\n        torch.save(net, save_weights + '.pth')\n\n\ndef main():\n    # parse command line\n    parser = opts_parser()\n    args = parser.parse_args()\n    train_test(**vars(args))\n\n    # run\n    # train_test(**vars(args))\n\n\nif __name__ == '__main__':\n    main()\n"""
utils.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport sys\nimport time\nimport json\nimport logging\nimport path\nimport math\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n\nscale_fn = {\'linear\':lambda x: x,\n                         \'squared\': lambda x: x**2,\n                         \'cubic\': lambda x: x**3}\n                         \ndef calc_speedup(growthRate,nDenseBlocks,t_0,how_scale):\n    # Height*Width at each stage\n    HW = [32**2, 16**2, 8**2]\n\n    # FLOPs of first layer\n    c = [3* (2*growthRate)*HW[0]*9]\n    # num channels\n    n = 2\n    for i in range(3):\n        for j in range(nDenseBlocks):\n            # Calc flops for this layer\n            c.append(n*(4*growthRate*growthRate)*HW[i] + 4*9*growthRate*growthRate*HW[i])\n            n +=1\n        n = math.floor(n*0.5)\n    \n    # Total computational cost for training run without freezeout\n    C = 2*sum(c)\n\n    # Computational Cost with FreezeOut\n    C_f = sum(c)+sum([c_i*scale_fn[how_scale](\n                    (t_0 + (1 - t_0) * float(index) / len(c) ))\n                    for index,c_i in enumerate(c)])\n\n    \n    if how_scale==\'linear\':\n        return 1.3*(1-float(C_f)/C)\n    else:\n        return 1-float(C_f)/C\n\n\ndef get_data_loader(which_dataset,augment=True,validate=True,batch_size=50):\n    class CIFAR10(dset.CIFAR10):\n        def __len__(self):\n            if self.train:\n                return len(self.train_data)\n            else:\n                return 10000\n\n\n    class CIFAR100(dset.CIFAR100):\n        def __len__(self):\n            if self.train:\n                return len(self.train_data)\n            else:\n                return 10000\n\n    if which_dataset is 10:\n        print(\'Loading CIFAR-10...\')\n        norm_mean = [0.49139968, 0.48215827, 0.44653124]\n        norm_std = [0.24703233, 0.24348505, 0.26158768]\n        dataset = CIFAR10\n\n    elif which_dataset is 100:\n        print(\'Loading CIFAR-100...\')\n        norm_mean = [0.50707519, 0.48654887, 0.44091785]\n        norm_std = [0.26733428, 0.25643846, 0.27615049]\n        dataset = CIFAR100\n\n    # Prepare transforms and data augmentation\n    norm_transform = transforms.Normalize(norm_mean, norm_std)\n    train_transform = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        norm_transform\n    ])\n    test_transform = transforms.Compose([\n        transforms.ToTensor(),\n        norm_transform\n    ])\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True}\n\n    train_set = dataset(\n        root=\'cifar\',\n        train=True,\n        download=True,\n        transform=train_transform if augment else test_transform)\n    # If we\'re evaluating on the test set, load the test set\n    if validate == \'test\':\n        test_set = dataset(root=\'cifar\', train=False, download=True,\n                           transform=test_transform)\n\n    # If we\'re evaluating on the validation set, prepare validation set\n    # as the last 5,000 samples in the training set.\n    elif validate:\n        test_set = dataset(root=\'cifar\', train=True, download=True,\n                           transform=test_transform)\n        test_set.train_data = test_set.train_data[-5000:]\n        test_set.train_labels = test_set.train_labels[-5000:]\n        train_set.train_data = train_set.train_data[:-5000]\n        train_set.train_labels = train_set.train_labels[:-5000]\n\n    # Prepare data loaders\n    train_loader = DataLoader(train_set, batch_size=batch_size,\n                              shuffle=True, **kwargs)\n    test_loader = DataLoader(test_set, batch_size=batch_size,\n                             shuffle=False, **kwargs)\n    return train_loader, test_loader\n    \n\nclass MetricsLogger(object):\n\n    def __init__(self, fname, reinitialize=False):\n        self.fname = path.Path(fname)\n        self.reinitialize = reinitialize\n        if self.fname.exists():\n            if self.reinitialize:\n                logging.warn(\'{} exists, deleting\'.format(self.fname))\n                self.fname.remove()\n\n    def log(self, record=None, **kwargs):\n        """"""\n        Assumption: no newlines in the input.\n        """"""\n        if record is None:\n            record = {}\n        record.update(kwargs)\n        record[\'_stamp\'] = time.time()\n        with open(self.fname, \'a\') as f:\n            f.write(json.dumps(record, ensure_ascii=True)+\'\\n\')\n\n\ndef read_records(fname):\n    """""" convenience for reading back. """"""\n    skipped = 0\n    with open(fname, \'rb\') as f:\n        for line in f:\n            if not line.endswith(\'\\n\'):\n                skipped += 1\n                continue\n            yield json.loads(line.strip())\n        if skipped > 0:\n            logging.warn(\'skipped {} lines\'.format(skipped))\n            \n""""""\nVery basic progress indicator to wrap an iterable in.\n\nAuthor: Jan Schl\xc3\xbcter\n""""""\n\n\ndef progress(items, desc=\'\', total=None, min_delay=0.1):\n    """"""\n    Returns a generator over `items`, printing the number and percentage of\n    items processed and the estimated remaining processing time before yielding\n    the next item. `total` gives the total number of items (required if `items`\n    has no length), and `min_delay` gives the minimum time in seconds between\n    subsequent prints. `desc` gives an optional prefix text (end with a space).\n    """"""\n    total = total or len(items)\n    t_start = time.time()\n    t_last = 0\n    for n, item in enumerate(items):\n        t_now = time.time()\n        if t_now - t_last > min_delay:\n            print(""\\r%s%d/%d (%6.2f%%)"" % (\n                    desc, n+1, total, n / float(total) * 100), end="" "")\n            if n > 0:\n                t_done = t_now - t_start\n                t_total = t_done / n * total\n                print(""(ETA: %d:%02d)"" % divmod(t_total - t_done, 60), end="" "")\n            sys.stdout.flush()\n            t_last = t_now\n        yield item\n    t_total = time.time() - t_start\n    print(""\\r%s%d/%d (100.00%%) (took %d:%02d)"" % ((desc, total, total) +\n                                                   divmod(t_total, 60)))\n'"
vgg.py,4,"b""# VGG net stolen from the TorchVision package.\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport math\nimport numpy as np\n\nfrom utils import scale_fn\n\n\n\nclass Layer(nn.Module):\n    def __init__(self, n_in, n_out, layer_index):\n        super(Layer, self).__init__()\n        \n        self.conv1 = nn.Conv2d(n_in, n_out, kernel_size=3,\n                               padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(n_out)\n        \n        self.layer_index = layer_index\n        # If the layer is being trained or not\n        self.active = True\n            \n    def forward(self, x):\n        if not self.active:\n            self.eval()\n        out = F.relu(self.bn1(self.conv1(x)))\n        if self.active:\n            return out\n        else:\n            return out.detach()\n\n# Using the VGG values provided by Sergey Zagoryuko in http://torch.ch/blog/2015/07/30/cifar.html\ncfg = {\n    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    # 'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    }\n# It's VGG but we call it DenseNet for compatibility with the training loop.\n# I'll fix it later.\n# GrowthRate and Depth are ignored.\nclass DenseNet(nn.Module):\n\n    def __init__(self,growthRate, depth, nClasses, epochs, t_0, scale_lr=True, how_scale = 'cubic',const_time=False, cfg=cfg['E'],batch_norm=True):\n        super(DenseNet, self).__init__()\n        \n        self.epochs = epochs\n        self.t_0 = t_0\n        self.scale_lr = scale_lr\n        self.how_scale = how_scale\n        self.const_time = const_time\n        \n        self.layer_index = 0\n        self.features = self.make_layers(cfg,batch_norm)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(True),\n            nn.BatchNorm1d(512),\n            nn.Dropout(),\n            nn.Linear(512, nClasses),\n        )\n        self.classifier.layer_index = self.layer_index\n        self.classifier.active = True\n        self._initialize_weights()\n        \n        # Optimizer\n        self.optim = optim.SGD([{'params':m.parameters(), 'lr':m.lr, 'layer_index':m.layer_index} for m in self.modules() if hasattr(m,'active')],  \n                         nesterov=True,momentum=0.9, weight_decay=1e-4)\n        # Iteration Counter            \n        self.j = 0  \n\n        # A simple dummy variable that indicates we are using an iteration-wise\n        # annealing scheme as opposed to epoch-wise. \n        self.lr_sched = {'itr':0}\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = F.log_softmax(self.classifier(x))\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n            # Set the layerwise scaling and annealing parameters\n            if hasattr(m,'active'):\n                m.lr_ratio = scale_fn[self.how_scale](self.t_0 + (1 - self.t_0) * float(m.layer_index) / self.layer_index)\n                m.max_j = self.epochs * 1000 * m.lr_ratio\n                \n                # Optionally scale the learning rates to have the same total\n                # distance traveled (modulo the gradients).\n                m.lr = 0.1 / m.lr_ratio if self.scale_lr else 0.1\n        \n\n    def make_layers(self,cfg, batch_norm=False):\n        layers = []\n        in_channels = 3\n        for v in cfg:\n            if v == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                # conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n                # if batch_norm:\n                layers += [Layer(in_channels,v,self.layer_index)]\n                self.layer_index += 1\n                # else:\n                    # layers += [conv2d, nn.ReLU(inplace=True)]\n                in_channels = v\n        return nn.Sequential(*layers)\n\n    def update_lr(self):\n    \n        # Loop over all modules\n        for m in self.modules():\n        \n            # If a module is active:\n            if hasattr(m,'active') and m.active:\n            \n                # If we've passed this layer's freezing point, deactivate it.\n                if self.j > m.max_j: \n                    m.active = False\n                    \n                    # Also make sure we remove all this layer from the optimizer\n                    for i,group in enumerate(self.optim.param_groups):\n                        if group['layer_index']==m.layer_index:\n                            self.optim.param_groups.remove(group)\n                \n                # If not, update the LR\n                else:\n                    for i,group in enumerate(self.optim.param_groups):\n                        if group['layer_index']==m.layer_index:\n                            self.optim.param_groups[i]['lr'] = (m.lr/2)*(1+np.cos(np.pi*self.j/m.max_j))\n\n"""
