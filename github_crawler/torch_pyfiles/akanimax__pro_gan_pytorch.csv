file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nsetup(\n    name=\'pro-gan-pth\',\n    version=\'2.1.1\',\n    packages=find_packages(""."", exclude=(""test"", ""samples"")),\n    url=\'https://github.com/akanimax/pro_gan_pytorch\',\n    license=\'MIT\',\n    author=\'akanimax\',\n    author_email=\'akanimax@gmail.com\',\n    description=\'ProGAN package implemented as an extension of PyTorch nn.Module\',\n    install_requires=[\'numpy\', \'torch\', \'torchvision\']\n)\n'"
pro_gan_pytorch/CustomLayers.py,16,"b'"""""" Module containing custom layers """"""\n\nimport torch as th\n\n\n# extending Conv2D and Deconv2D layers for equalized learning rate logic\nclass _equalized_conv2d(th.nn.Module):\n    """""" conv2d with the concept of equalized learning rate\n        Args:\n            :param c_in: input channels\n            :param c_out:  output channels\n            :param k_size: kernel size (h, w) should be a tuple or a single integer\n            :param stride: stride for conv\n            :param pad: padding\n            :param bias: whether to use bias or not\n    """"""\n\n    def __init__(self, c_in, c_out, k_size, stride=1, pad=0, bias=True):\n        """""" constructor for the class """"""\n        from torch.nn.modules.utils import _pair\n        from numpy import sqrt, prod\n\n        super(_equalized_conv2d, self).__init__()\n\n        # define the weight and bias if to be used\n        self.weight = th.nn.Parameter(th.nn.init.normal_(\n            th.empty(c_out, c_in, *_pair(k_size))\n        ))\n\n        self.use_bias = bias\n        self.stride = stride\n        self.pad = pad\n\n        if self.use_bias:\n            self.bias = th.nn.Parameter(th.FloatTensor(c_out).fill_(0))\n\n        fan_in = prod(_pair(k_size)) * c_in  # value of fan_in\n        self.scale = sqrt(2) / sqrt(fan_in)\n\n    def forward(self, x):\n        """"""\n        forward pass of the network\n        :param x: input\n        :return: y => output\n        """"""\n        from torch.nn.functional import conv2d\n\n        return conv2d(input=x,\n                      weight=self.weight * self.scale,  # scale the weight on runtime\n                      bias=self.bias if self.use_bias else None,\n                      stride=self.stride,\n                      padding=self.pad)\n\n    def extra_repr(self):\n        return "", "".join(map(str, self.weight.shape))\n\n\nclass _equalized_deconv2d(th.nn.Module):\n    """""" Transpose convolution using the equalized learning rate\n        Args:\n            :param c_in: input channels\n            :param c_out: output channels\n            :param k_size: kernel size\n            :param stride: stride for convolution transpose\n            :param pad: padding\n            :param bias: whether to use bias or not\n    """"""\n\n    def __init__(self, c_in, c_out, k_size, stride=1, pad=0, bias=True):\n        """""" constructor for the class """"""\n        from torch.nn.modules.utils import _pair\n        from numpy import sqrt\n\n        super(_equalized_deconv2d, self).__init__()\n\n        # define the weight and bias if to be used\n        self.weight = th.nn.Parameter(th.nn.init.normal_(\n            th.empty(c_in, c_out, *_pair(k_size))\n        ))\n\n        self.use_bias = bias\n        self.stride = stride\n        self.pad = pad\n\n        if self.use_bias:\n            self.bias = th.nn.Parameter(th.FloatTensor(c_out).fill_(0))\n\n        fan_in = c_in  # value of fan_in for deconv\n        self.scale = sqrt(2) / sqrt(fan_in)\n\n    def forward(self, x):\n        """"""\n        forward pass of the layer\n        :param x: input\n        :return: y => output\n        """"""\n        from torch.nn.functional import conv_transpose2d\n\n        return conv_transpose2d(input=x,\n                                weight=self.weight * self.scale,  # scale the weight on runtime\n                                bias=self.bias if self.use_bias else None,\n                                stride=self.stride,\n                                padding=self.pad)\n\n    def extra_repr(self):\n        return "", "".join(map(str, self.weight.shape))\n\n\nclass _equalized_linear(th.nn.Module):\n    """""" Linear layer using equalized learning rate\n        Args:\n            :param c_in: number of input channels\n            :param c_out: number of output channels\n            :param bias: whether to use bias with the linear layer\n    """"""\n\n    def __init__(self, c_in, c_out, bias=True):\n        """"""\n        Linear layer modified for equalized learning rate\n        """"""\n        from numpy import sqrt\n\n        super(_equalized_linear, self).__init__()\n\n        self.weight = th.nn.Parameter(th.nn.init.normal_(\n            th.empty(c_out, c_in)\n        ))\n\n        self.use_bias = bias\n\n        if self.use_bias:\n            self.bias = th.nn.Parameter(th.FloatTensor(c_out).fill_(0))\n\n        fan_in = c_in\n        self.scale = sqrt(2) / sqrt(fan_in)\n\n    def forward(self, x):\n        """"""\n        forward pass of the layer\n        :param x: input\n        :return: y => output\n        """"""\n        from torch.nn.functional import linear\n        return linear(x, self.weight * self.scale,\n                      self.bias if self.use_bias else None)\n\n\n# ----------------------------------------------------------------------------\n# Pixelwise feature vector normalization.\n# reference: https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py#L120\n# ----------------------------------------------------------------------------\nclass PixelwiseNorm(th.nn.Module):\n    def __init__(self):\n        super(PixelwiseNorm, self).__init__()\n\n    def forward(self, x, alpha=1e-8):\n        """"""\n        forward pass of the module\n        :param x: input activations volume\n        :param alpha: small number for numerical stability\n        :return: y => pixel normalized activations\n        """"""\n        y = x.pow(2.).mean(dim=1, keepdim=True).add(alpha).sqrt()  # [N1HW]\n        y = x / y  # normalize the input x volume\n        return y\n\n\n# ==========================================================\n# Layers required for Building The generator and\n# discriminator\n# ==========================================================\nclass GenInitialBlock(th.nn.Module):\n    """""" Module implementing the initial block of the input """"""\n\n    def __init__(self, in_channels, use_eql):\n        """"""\n        constructor for the inner class\n        :param in_channels: number of input channels to the block\n        :param use_eql: whether to use equalized learning rate\n        """"""\n        from torch.nn import LeakyReLU\n\n        super(GenInitialBlock, self).__init__()\n\n        if use_eql:\n            self.conv_1 = _equalized_deconv2d(in_channels, in_channels, (4, 4), bias=True)\n            self.conv_2 = _equalized_conv2d(in_channels, in_channels, (3, 3),\n                                            pad=1, bias=True)\n\n        else:\n            from torch.nn import Conv2d, ConvTranspose2d\n            self.conv_1 = ConvTranspose2d(in_channels, in_channels, (4, 4), bias=True)\n            self.conv_2 = Conv2d(in_channels, in_channels, (3, 3), padding=1, bias=True)\n\n        # Pixelwise feature vector normalization operation\n        self.pixNorm = PixelwiseNorm()\n\n        # leaky_relu:\n        self.lrelu = LeakyReLU(0.2)\n\n    def forward(self, x):\n        """"""\n        forward pass of the block\n        :param x: input to the module\n        :return: y => output\n        """"""\n        # convert the tensor shape:\n        y = th.unsqueeze(th.unsqueeze(x, -1), -1)\n\n        # perform the forward computations:\n        y = self.lrelu(self.conv_1(y))\n        y = self.lrelu(self.conv_2(y))\n\n        # apply pixel norm\n        y = self.pixNorm(y)\n\n        return y\n\n\nclass GenGeneralConvBlock(th.nn.Module):\n    """""" Module implementing a general convolutional block """"""\n\n    def __init__(self, in_channels, out_channels, use_eql):\n        """"""\n        constructor for the class\n        :param in_channels: number of input channels to the block\n        :param out_channels: number of output channels required\n        :param use_eql: whether to use equalized learning rate\n        """"""\n        from torch.nn import LeakyReLU\n        from torch.nn.functional import interpolate\n\n        super(GenGeneralConvBlock, self).__init__()\n\n        self.upsample = lambda x: interpolate(x, scale_factor=2)\n\n        if use_eql:\n            self.conv_1 = _equalized_conv2d(in_channels, out_channels, (3, 3),\n                                            pad=1, bias=True)\n            self.conv_2 = _equalized_conv2d(out_channels, out_channels, (3, 3),\n                                            pad=1, bias=True)\n        else:\n            from torch.nn import Conv2d\n            self.conv_1 = Conv2d(in_channels, out_channels, (3, 3),\n                                 padding=1, bias=True)\n            self.conv_2 = Conv2d(out_channels, out_channels, (3, 3),\n                                 padding=1, bias=True)\n\n        # Pixelwise feature vector normalization operation\n        self.pixNorm = PixelwiseNorm()\n\n        # leaky_relu:\n        self.lrelu = LeakyReLU(0.2)\n\n    def forward(self, x):\n        """"""\n        forward pass of the block\n        :param x: input\n        :return: y => output\n        """"""\n        y = self.upsample(x)\n        y = self.pixNorm(self.lrelu(self.conv_1(y)))\n        y = self.pixNorm(self.lrelu(self.conv_2(y)))\n\n        return y\n\n\n# function to calculate the Exponential moving averages for the Generator weights\n# This function updates the exponential average weights based on the current training\ndef update_average(model_tgt, model_src, beta):\n    """"""\n    update the model_target using exponential moving averages\n    :param model_tgt: target model\n    :param model_src: source model\n    :param beta: value of decay beta\n    :return: None (updates the target model)\n    """"""\n\n    # utility function for toggling the gradient requirements of the models\n    def toggle_grad(model, requires_grad):\n        for p in model.parameters():\n            p.requires_grad_(requires_grad)\n\n    # turn off gradient calculation\n    toggle_grad(model_tgt, False)\n    toggle_grad(model_src, False)\n\n    param_dict_src = dict(model_src.named_parameters())\n\n    for p_name, p_tgt in model_tgt.named_parameters():\n        p_src = param_dict_src[p_name]\n        assert (p_src is not p_tgt)\n        p_tgt.copy_(beta * p_tgt + (1. - beta) * p_src)\n\n    # turn back on the gradient calculation\n    toggle_grad(model_tgt, True)\n    toggle_grad(model_src, True)\n\n\nclass MinibatchStdDev(th.nn.Module):\n    """"""\n    Minibatch standard deviation layer for the discriminator\n    """"""\n\n    def __init__(self):\n        """"""\n        derived class constructor\n        """"""\n        super(MinibatchStdDev, self).__init__()\n\n    def forward(self, x, alpha=1e-8):\n        """"""\n        forward pass of the layer\n        :param x: input activation volume\n        :param alpha: small number for numerical stability\n        :return: y => x appended with standard deviation constant map\n        """"""\n        batch_size, _, height, width = x.shape\n\n        # [B x C x H x W] Subtract mean over batch.\n        y = x - x.mean(dim=0, keepdim=True)\n\n        # [1 x C x H x W]  Calc standard deviation over batch\n        y = th.sqrt(y.pow(2.).mean(dim=0, keepdim=False) + alpha)\n\n        # [1]  Take average over feature_maps and pixels.\n        y = y.mean().view(1, 1, 1, 1)\n\n        # [B x 1 x H x W]  Replicate over group and pixels.\n        y = y.repeat(batch_size, 1, height, width)\n\n        # [B x C x H x W]  Append as new feature_map.\n        y = th.cat([x, y], 1)\n\n        # return the computed values:\n        return y\n\n\nclass DisFinalBlock(th.nn.Module):\n    """""" Final block for the Discriminator """"""\n\n    def __init__(self, in_channels, use_eql):\n        """"""\n        constructor of the class\n        :param in_channels: number of input channels\n        :param use_eql: whether to use equalized learning rate\n        """"""\n        from torch.nn import LeakyReLU\n\n        super(DisFinalBlock, self).__init__()\n\n        # declare the required modules for forward pass\n        self.batch_discriminator = MinibatchStdDev()\n        if use_eql:\n            self.conv_1 = _equalized_conv2d(in_channels + 1, in_channels, (3, 3), pad=1, bias=True)\n            self.conv_2 = _equalized_conv2d(in_channels, in_channels, (4, 4), bias=True)\n            # final conv layer emulates a fully connected layer\n            self.conv_3 = _equalized_conv2d(in_channels, 1, (1, 1), bias=True)\n        else:\n            from torch.nn import Conv2d\n            self.conv_1 = Conv2d(in_channels + 1, in_channels, (3, 3), padding=1, bias=True)\n            self.conv_2 = Conv2d(in_channels, in_channels, (4, 4), bias=True)\n            # final conv layer emulates a fully connected layer\n            self.conv_3 = Conv2d(in_channels, 1, (1, 1), bias=True)\n\n        # leaky_relu:\n        self.lrelu = LeakyReLU(0.2)\n\n    def forward(self, x):\n        """"""\n        forward pass of the FinalBlock\n        :param x: input\n        :return: y => output\n        """"""\n        # minibatch_std_dev layer\n        y = self.batch_discriminator(x)\n\n        # define the computations\n        y = self.lrelu(self.conv_1(y))\n        y = self.lrelu(self.conv_2(y))\n\n        # fully connected layer\n        y = self.conv_3(y)  # This layer has linear activation\n\n        # flatten the output raw discriminator scores\n        return y.view(-1)\n\n\nclass ConDisFinalBlock(th.nn.Module):\n    """""" Final block for the Conditional Discriminator\n        Uses the Projection mechanism from the paper -> https://arxiv.org/pdf/1802.05637.pdf\n    """"""\n\n    def __init__(self, in_channels, num_classes, use_eql):\n        """"""\n        constructor of the class\n        :param in_channels: number of input channels\n        :param num_classes: number of classes for conditional discrimination\n        :param use_eql: whether to use equalized learning rate\n        """"""\n        from torch.nn import LeakyReLU, Embedding\n\n        super(ConDisFinalBlock, self).__init__()\n\n        # declare the required modules for forward pass\n        self.batch_discriminator = MinibatchStdDev()\n        if use_eql:\n            self.conv_1 = _equalized_conv2d(in_channels + 1, in_channels, (3, 3), pad=1, bias=True)\n            self.conv_2 = _equalized_conv2d(in_channels, in_channels, (4, 4), bias=True)\n\n            # final conv layer emulates a fully connected layer\n            self.conv_3 = _equalized_conv2d(in_channels, 1, (1, 1), bias=True)\n        else:\n            from torch.nn import Conv2d\n            self.conv_1 = Conv2d(in_channels + 1, in_channels, (3, 3), padding=1, bias=True)\n            self.conv_2 = Conv2d(in_channels, in_channels, (4, 4), bias=True)\n\n            # final conv layer emulates a fully connected layer\n            self.conv_3 = Conv2d(in_channels, 1, (1, 1), bias=True)\n\n        # we also need an embedding matrix for the label vectors\n        self.label_embedder = Embedding(num_classes, in_channels, max_norm=1)\n\n        # leaky_relu:\n        self.lrelu = LeakyReLU(0.2)\n\n    def forward(self, x, labels):\n        """"""\n        forward pass of the FinalBlock\n        :param x: input\n        :param labels: samples\' labels for conditional discrimination\n                       Note that these are pure integer labels [Batch_size x 1]\n        :return: y => output\n        """"""\n        # minibatch_std_dev layer\n        y = self.batch_discriminator(x)  # [B x C x 4 x 4]\n\n        # perform the forward pass\n        y = self.lrelu(self.conv_1(y))  # [B x C x 4 x 4]\n\n        # obtain the computed features\n        y = self.lrelu(self.conv_2(y))  # [B x C x 1 x 1]\n\n        # embed the labels\n        labels = self.label_embedder(labels)  # [B x C]\n\n        # compute the inner product with the label embeddings\n        y_ = th.squeeze(th.squeeze(y, dim=-1), dim=-1)  # [B x C]\n        projection_scores = (y_ * labels).sum(dim=-1)  # [B]\n\n        # normal discrimination score\n        y = self.lrelu(self.conv_3(y))  # This layer has linear activation\n\n        # calculate the total score\n        final_score = y.view(-1) + projection_scores\n\n        # return the output raw discriminator scores\n        return final_score\n\n\nclass DisGeneralConvBlock(th.nn.Module):\n    """""" General block in the discriminator  """"""\n\n    def __init__(self, in_channels, out_channels, use_eql):\n        """"""\n        constructor of the class\n        :param in_channels: number of input channels\n        :param out_channels: number of output channels\n        :param use_eql: whether to use equalized learning rate\n        """"""\n        from torch.nn import AvgPool2d, LeakyReLU\n\n        super(DisGeneralConvBlock, self).__init__()\n\n        if use_eql:\n            self.conv_1 = _equalized_conv2d(in_channels, in_channels, (3, 3), pad=1, bias=True)\n            self.conv_2 = _equalized_conv2d(in_channels, out_channels, (3, 3), pad=1, bias=True)\n        else:\n            from torch.nn import Conv2d\n            self.conv_1 = Conv2d(in_channels, in_channels, (3, 3), padding=1, bias=True)\n            self.conv_2 = Conv2d(in_channels, out_channels, (3, 3), padding=1, bias=True)\n\n        self.downSampler = AvgPool2d(2)\n\n        # leaky_relu:\n        self.lrelu = LeakyReLU(0.2)\n\n    def forward(self, x):\n        """"""\n        forward pass of the module\n        :param x: input\n        :return: y => output\n        """"""\n        # define the computations\n        y = self.lrelu(self.conv_1(x))\n        y = self.lrelu(self.conv_2(y))\n        y = self.downSampler(y)\n\n        return y\n'"
pro_gan_pytorch/DataTools.py,1,"b'"""""" Module for the data loading pipeline for the model to train """"""\n\n\ndef get_transform(new_size=None):\n    """"""\n    obtain the image transforms required for the input data\n    :param new_size: size of the resized images\n    :return: image_transform => transform object from TorchVision\n    """"""\n    from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n\n    if new_size is not None:\n        image_transform = Compose([\n            Resize(new_size),\n            ToTensor(),\n            Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n        ])\n\n    else:\n        image_transform = Compose([\n            ToTensor(),\n            Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n        ])\n    return image_transform\n\n\ndef get_data_loader(dataset, batch_size, num_workers):\n    """"""\n    generate the data_loader from the given dataset\n    :param dataset: dataset for training (Should be a PyTorch dataset)\n                    Make sure every item is an Image\n    :param batch_size: batch size of the data\n    :param num_workers: num of parallel readers\n    :return: dl => dataloader for the dataset\n    """"""\n    from torch.utils.data import DataLoader\n\n    dl = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers\n    )\n\n    return dl\n'"
pro_gan_pytorch/Losses.py,7,"b'"""""" Module implementing various loss functions """"""\n\nimport torch as th\n\n\n# =============================================================\n# Interface for the losses\n# =============================================================\n\nclass GANLoss:\n    """""" Base class for all losses\n\n        @args:\n            dis: Discriminator used for calculating the loss\n                 Note this must be a part of the GAN framework\n    """"""\n\n    def __init__(self, dis):\n        self.dis = dis\n\n    def dis_loss(self, real_samps, fake_samps, height, alpha):\n        """"""\n        calculate the discriminator loss using the following data\n        :param real_samps: batch of real samples\n        :param fake_samps: batch of generated (fake) samples\n        :param height: current height at which training is going on\n        :param alpha: current value of the fader alpha\n        :return: loss => calculated loss Tensor\n        """"""\n        raise NotImplementedError(""dis_loss method has not been implemented"")\n\n    def gen_loss(self, real_samps, fake_samps, height, alpha):\n        """"""\n        calculate the generator loss\n        :param real_samps: batch of real samples\n        :param fake_samps: batch of generated (fake) samples\n        :param height: current height at which training is going on\n        :param alpha: current value of the fader alpha\n        :return: loss => calculated loss Tensor\n        """"""\n        raise NotImplementedError(""gen_loss method has not been implemented"")\n\n\nclass ConditionalGANLoss:\n    """""" Base class for all conditional losses """"""\n\n    def __init__(self, dis):\n        self.dis = dis\n\n    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n        raise NotImplementedError(""dis_loss method has not been implemented"")\n\n    def gen_loss(self, real_samps, fake_samps, labels, height, alpha):\n        raise NotImplementedError(""gen_loss method has not been implemented"")\n\n\n# =============================================================\n# Normal versions of the Losses:\n# =============================================================\n\nclass StandardGAN(GANLoss):\n\n    def __init__(self, dis):\n        from torch.nn import BCEWithLogitsLoss\n\n        super().__init__(dis)\n\n        # define the criterion and activation used for object\n        self.criterion = BCEWithLogitsLoss()\n\n    def dis_loss(self, real_samps, fake_samps, height, alpha):\n        # small assertion:\n        assert real_samps.device == fake_samps.device, \\\n            ""Real and Fake samples are not on the same device""\n\n        # device for computations:\n        device = fake_samps.device\n\n        # predictions for real images and fake images separately :\n        r_preds = self.dis(real_samps, height, alpha)\n        f_preds = self.dis(fake_samps, height, alpha)\n\n        # calculate the real loss:\n        real_loss = self.criterion(\n            th.squeeze(r_preds),\n            th.ones(real_samps.shape[0]).to(device))\n\n        # calculate the fake loss:\n        fake_loss = self.criterion(\n            th.squeeze(f_preds),\n            th.zeros(fake_samps.shape[0]).to(device))\n\n        # return final losses\n        return (real_loss + fake_loss) / 2\n\n    def gen_loss(self, _, fake_samps, height, alpha):\n        preds, _, _ = self.dis(fake_samps, height, alpha)\n        return self.criterion(th.squeeze(preds),\n                              th.ones(fake_samps.shape[0]).to(fake_samps.device))\n\n\nclass WGAN_GP(GANLoss):\n\n    def __init__(self, dis, drift=0.001, use_gp=False):\n        super().__init__(dis)\n        self.drift = drift\n        self.use_gp = use_gp\n\n    def __gradient_penalty(self, real_samps, fake_samps,\n                           height, alpha, reg_lambda=10):\n        """"""\n        private helper for calculating the gradient penalty\n        :param real_samps: real samples\n        :param fake_samps: fake samples\n        :param height: current depth in the optimization\n        :param alpha: current alpha for fade-in\n        :param reg_lambda: regularisation lambda\n        :return: tensor (gradient penalty)\n        """"""\n        batch_size = real_samps.shape[0]\n\n        # generate random epsilon\n        epsilon = th.rand((batch_size, 1, 1, 1)).to(fake_samps.device)\n\n        # create the merge of both real and fake samples\n        merged = epsilon * real_samps + ((1 - epsilon) * fake_samps)\n        merged.requires_grad_(True)\n\n        # forward pass\n        op = self.dis(merged, height, alpha)\n\n        # perform backward pass from op to merged for obtaining the gradients\n        gradient = th.autograd.grad(outputs=op, inputs=merged,\n                                    grad_outputs=th.ones_like(op), create_graph=True,\n                                    retain_graph=True, only_inputs=True)[0]\n\n        # calculate the penalty using these gradients\n        gradient = gradient.view(gradient.shape[0], -1)\n        penalty = reg_lambda * ((gradient.norm(p=2, dim=1) - 1) ** 2).mean()\n\n        # return the calculated penalty:\n        return penalty\n\n    def dis_loss(self, real_samps, fake_samps, height, alpha):\n        # define the (Wasserstein) loss\n        fake_out = self.dis(fake_samps, height, alpha)\n        real_out = self.dis(real_samps, height, alpha)\n\n        loss = (th.mean(fake_out) - th.mean(real_out)\n                + (self.drift * th.mean(real_out ** 2)))\n\n        if self.use_gp:\n            # calculate the WGAN-GP (gradient penalty)\n            gp = self.__gradient_penalty(real_samps, fake_samps, height, alpha)\n            loss += gp\n\n        return loss\n\n    def gen_loss(self, _, fake_samps, height, alpha):\n        # calculate the WGAN loss for generator\n        loss = -th.mean(self.dis(fake_samps, height, alpha))\n\n        return loss\n\n\nclass LSGAN(GANLoss):\n\n    def __init__(self, dis):\n        super().__init__(dis)\n\n    def dis_loss(self, real_samps, fake_samps, height, alpha):\n        return 0.5 * (th.mean((self.dis(real_samps, height, alpha) - 1) ** 2)\n                      + (th.mean(self.dis(fake_samps, height, alpha) ** 2)))\n\n    def gen_loss(self, _, fake_samps, height, alpha):\n        return 0.5 * (th.mean((self.dis(fake_samps, height, alpha) - 1) ** 2))\n\n\nclass LSGAN_SIGMOID(GANLoss):\n\n    def __init__(self, dis):\n        super().__init__(dis)\n\n    def dis_loss(self, real_samps, fake_samps, height, alpha):\n        from torch.nn.functional import sigmoid\n        real_scores = sigmoid(self.dis(real_samps, height, alpha))\n        fake_scores = sigmoid(self.dis(fake_samps, height, alpha))\n        return 0.5 * ((th.mean((real_scores - 1) ** 2)) + th.mean(fake_scores ** 2))\n\n    def gen_loss(self, _, fake_samps, height, alpha):\n        from torch.nn.functional import sigmoid\n        scores = sigmoid(self.dis(fake_samps, height, alpha))\n        return 0.5 * (th.mean((scores - 1) ** 2))\n\n\nclass HingeGAN(GANLoss):\n\n    def __init__(self, dis):\n        super().__init__(dis)\n\n    def dis_loss(self, real_samps, fake_samps, height, alpha):\n        r_preds = self.dis(real_samps, height, alpha)\n        f_preds = self.dis(fake_samps, height, alpha)\n\n        loss = (th.mean(th.nn.ReLU()(1 - r_preds)) +\n                th.mean(th.nn.ReLU()(1 + f_preds)))\n\n        return loss\n\n    def gen_loss(self, _, fake_samps, height, alpha):\n        return -th.mean(self.dis(fake_samps, height, alpha))\n\n\nclass RelativisticAverageHingeGAN(GANLoss):\n\n    def __init__(self, dis):\n        super().__init__(dis)\n\n    def dis_loss(self, real_samps, fake_samps, height, alpha):\n        # Obtain predictions\n        r_preds = self.dis(real_samps, height, alpha)\n        f_preds = self.dis(fake_samps, height, alpha)\n\n        # difference between real and fake:\n        r_f_diff = r_preds - th.mean(f_preds)\n\n        # difference between fake and real samples\n        f_r_diff = f_preds - th.mean(r_preds)\n\n        # return the loss\n        loss = (th.mean(th.nn.ReLU()(1 - r_f_diff))\n                + th.mean(th.nn.ReLU()(1 + f_r_diff)))\n\n        return loss\n\n    def gen_loss(self, real_samps, fake_samps, height, alpha):\n        # Obtain predictions\n        r_preds = self.dis(real_samps, height, alpha)\n        f_preds = self.dis(fake_samps, height, alpha)\n\n        # difference between real and fake:\n        r_f_diff = r_preds - th.mean(f_preds)\n\n        # difference between fake and real samples\n        f_r_diff = f_preds - th.mean(r_preds)\n\n        # return the loss\n        return (th.mean(th.nn.ReLU()(1 + r_f_diff))\n                + th.mean(th.nn.ReLU()(1 - f_r_diff)))\n\n\n# =============================================================\n# Conditional versions of the Losses:\n# =============================================================\n\nclass CondStandardGAN(ConditionalGANLoss):\n\n    def __init__(self, dis):\n        from torch.nn import BCEWithLogitsLoss\n\n        super().__init__(dis)\n\n        # define the criterion and activation used for object\n        self.criterion = BCEWithLogitsLoss()\n\n    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n        # small assertion:\n        assert real_samps.device == fake_samps.device, \\\n            ""Real and Fake samples are not on the same device""\n\n        # device for computations:\n        device = fake_samps.device\n\n        # predictions for real images and fake images separately:\n        r_preds = self.dis(real_samps, labels, height, alpha)\n        f_preds = self.dis(fake_samps, labels, height, alpha)\n\n        # calculate the real loss:\n        real_loss = self.criterion(\n            th.squeeze(r_preds),\n            th.ones(real_samps.shape[0]).to(device))\n\n        # calculate the fake loss:\n        fake_loss = self.criterion(\n            th.squeeze(f_preds),\n            th.zeros(fake_samps.shape[0]).to(device))\n\n        # return final loss\n        return (real_loss + fake_loss) / 2\n\n    def gen_loss(self, _, fake_samps, labels, height, alpha):\n        preds, _, _ = self.dis(fake_samps, labels, height, alpha)\n        return self.criterion(th.squeeze(preds),\n                              th.ones(fake_samps.shape[0]).to(fake_samps.device))\n\n\nclass CondWGAN_GP(ConditionalGANLoss):\n\n    def __init__(self, dis, drift=0.001, use_gp=False):\n        super().__init__(dis)\n        self.drift = drift\n        self.use_gp = use_gp\n\n    def __gradient_penalty(self, real_samps, fake_samps, labels,\n                           height, alpha, reg_lambda=10):\n        """"""\n        private helper for calculating the gradient penalty\n        :param real_samps: real samples\n        :param fake_samps: fake samples\n        :param labels: used for conditional loss calculation\n                       Note that this is just [Batch x 1] plain integer labels\n        :param height: current depth in the optimization\n        :param alpha: current alpha for fade-in\n        :param reg_lambda: regularisation lambda\n        :return: tensor (gradient penalty)\n        """"""\n        from torch.autograd import grad\n\n        batch_size = real_samps.shape[0]\n\n        # generate random epsilon\n        epsilon = th.rand((batch_size, 1, 1, 1)).to(fake_samps.device)\n\n        # create the merge of both real and fake samples\n        merged = (epsilon * real_samps) + ((1 - epsilon) * fake_samps)\n        merged.requires_grad_(True)\n\n        # forward pass\n        op = self.dis(merged, labels, height, alpha)\n\n        # obtain gradient of op wrt. merged\n        gradient = grad(outputs=op, inputs=merged,\n                        grad_outputs=th.ones_like(op), create_graph=True,\n                        retain_graph=True, only_inputs=True)[0]\n\n        # calculate the penalty using these gradients\n        gradient = gradient.view(batch_size, -1)\n        penalty = reg_lambda * ((gradient.norm(p=2, dim=1) - 1) ** 2).mean()\n\n        # return the calculated penalty:\n        return penalty\n\n    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n        # define the (Wasserstein) loss\n        fake_out = self.dis(fake_samps, labels, height, alpha)\n        real_out = self.dis(real_samps, labels, height, alpha)\n\n        loss = (th.mean(fake_out) - th.mean(real_out)\n                + (self.drift * th.mean(real_out ** 2)))\n\n        if self.use_gp:\n            # calculate the WGAN-GP (gradient penalty)\n            gp = self.__gradient_penalty(real_samps, fake_samps,\n                                         labels, height, alpha)\n            loss += gp\n\n        return loss\n\n    def gen_loss(self, _, fake_samps, labels, height, alpha):\n        # calculate the WGAN loss for generator\n        loss = -th.mean(self.dis(fake_samps, labels, height, alpha))\n\n        return loss\n\n\nclass CondLSGAN(ConditionalGANLoss):\n\n    def __init__(self, dis):\n        super().__init__(dis)\n\n    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n        return 0.5 * ((th.mean((self.dis(real_samps, labels, height, alpha) - 1) ** 2))\n                      + (th.mean(self.dis(fake_samps, labels, height, alpha) ** 2)))\n\n    def gen_loss(self, _, fake_samps, labels, height, alpha):\n        return 0.5 * (th.mean((self.dis(fake_samps, labels, height, alpha) - 1) ** 2))\n\n\nclass CondLSGAN_SIGMOID(ConditionalGANLoss):\n\n    def __init__(self, dis):\n        super().__init__(dis)\n\n    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n        from torch.nn.functional import sigmoid\n        real_scores = sigmoid(self.dis(real_samps, labels, height, alpha))\n        fake_scores = sigmoid(self.dis(fake_samps, labels, height, alpha))\n        return 0.5 * (th.mean((real_scores - 1) ** 2) + th.mean(fake_scores ** 2))\n\n    def gen_loss(self, _, fake_samps, labels, height, alpha):\n        from torch.nn.functional import sigmoid\n        scores = sigmoid(self.dis(fake_samps, labels, height, alpha))\n        return 0.5 * (th.mean((scores - 1) ** 2))\n\n\nclass CondHingeGAN(ConditionalGANLoss):\n\n    def __init__(self, dis):\n        super().__init__(dis)\n\n    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n        r_preds = self.dis(real_samps, labels, height, alpha)\n        f_preds = self.dis(fake_samps, labels, height, alpha)\n\n        loss = (th.mean(th.nn.ReLU()(1 - r_preds)) +\n                th.mean(th.nn.ReLU()(1 + f_preds)))\n\n        return loss\n\n    def gen_loss(self, _, fake_samps, labels, height, alpha):\n        return -th.mean(self.dis(fake_samps, labels, height, alpha))\n\n\nclass CondRelativisticAverageHingeGAN(ConditionalGANLoss):\n\n    def __init__(self, dis):\n        super().__init__(dis)\n\n    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n        # Obtain predictions\n        r_preds = self.dis(real_samps, labels, height, alpha)\n        f_preds = self.dis(fake_samps, labels, height, alpha)\n\n        # difference between real and fake:\n        r_f_diff = r_preds - th.mean(f_preds)\n\n        # difference between fake and real samples\n        f_r_diff = f_preds - th.mean(r_preds)\n\n        # return the loss\n        loss = (th.mean(th.nn.ReLU()(1 - r_f_diff))\n                + th.mean(th.nn.ReLU()(1 + f_r_diff)))\n\n        return loss\n\n    def gen_loss(self, real_samps, fake_samps, labels, height, alpha):\n        # Obtain predictions\n        r_preds = self.dis(real_samps, labels, height, alpha)\n        f_preds = self.dis(fake_samps, labels, height, alpha)\n\n        # difference between real and fake:\n        r_f_diff = r_preds - th.mean(f_preds)\n\n        # difference between fake and real samples\n        f_r_diff = f_preds - th.mean(r_preds)\n\n        # return the loss\n        return (th.mean(th.nn.ReLU()(1 + r_f_diff))\n                + th.mean(th.nn.ReLU()(1 - f_r_diff)))\n'"
pro_gan_pytorch/PRO_GAN.py,29,"b'"""""" Module implementing GAN which will be trained using the Progressive growing\n    technique -> https://arxiv.org/abs/1710.10196\n""""""\nimport datetime\nimport os\nimport time\nimport timeit\nimport copy\nimport numpy as np\nimport torch as th\n\n\n# ========================================================================================\n# Generator Module\n# can be used with ProGAN, ConditionalProGAN or standalone (for inference)\n# ========================================================================================\n\nclass Generator(th.nn.Module):\n    """""" Generator of the GAN network """"""\n\n    def __init__(self, depth=7, latent_size=512, use_eql=True):\n        """"""\n        constructor for the Generator class\n        :param depth: required depth of the Network\n        :param latent_size: size of the latent manifold\n        :param use_eql: whether to use equalized learning rate\n        """"""\n        from torch.nn import ModuleList\n        from pro_gan_pytorch.CustomLayers import GenGeneralConvBlock, GenInitialBlock\n        from torch.nn.functional import interpolate\n\n        super(Generator, self).__init__()\n\n        assert latent_size != 0 and ((latent_size & (latent_size - 1)) == 0), \\\n            ""latent size not a power of 2""\n        if depth >= 4:\n            assert latent_size >= np.power(2, depth - 4), ""latent size will diminish to zero""\n\n        # state of the generator:\n        self.use_eql = use_eql\n        self.depth = depth\n        self.latent_size = latent_size\n\n        # register the modules required for the GAN\n        self.initial_block = GenInitialBlock(self.latent_size, use_eql=self.use_eql)\n\n        # create a module list of the other required general convolution blocks\n        self.layers = ModuleList([])  # initialize to empty list\n\n        # create the ToRGB layers for various outputs:\n        if self.use_eql:\n            from pro_gan_pytorch.CustomLayers import _equalized_conv2d\n            self.toRGB = lambda in_channels: \\\n                _equalized_conv2d(in_channels, 3, (1, 1), bias=True)\n        else:\n            from torch.nn import Conv2d\n            self.toRGB = lambda in_channels: Conv2d(in_channels, 3, (1, 1), bias=True)\n\n        self.rgb_converters = ModuleList([self.toRGB(self.latent_size)])\n\n        # create the remaining layers\n        for i in range(self.depth - 1):\n            if i <= 2:\n                layer = GenGeneralConvBlock(self.latent_size,\n                                            self.latent_size, use_eql=self.use_eql)\n                rgb = self.toRGB(self.latent_size)\n            else:\n                layer = GenGeneralConvBlock(\n                    int(self.latent_size // np.power(2, i - 3)),\n                    int(self.latent_size // np.power(2, i - 2)),\n                    use_eql=self.use_eql\n                )\n                rgb = self.toRGB(int(self.latent_size // np.power(2, i - 2)))\n            self.layers.append(layer)\n            self.rgb_converters.append(rgb)\n\n        # register the temporary upsampler\n        self.temporaryUpsampler = lambda x: interpolate(x, scale_factor=2)\n\n    def forward(self, x, depth, alpha):\n        """"""\n        forward pass of the Generator\n        :param x: input noise\n        :param depth: current depth from where output is required\n        :param alpha: value of alpha for fade-in effect\n        :return: y => output\n        """"""\n\n        assert depth < self.depth, ""Requested output depth cannot be produced""\n\n        y = self.initial_block(x)\n\n        if depth > 0:\n            for block in self.layers[:depth - 1]:\n                y = block(y)\n\n            residual = self.rgb_converters[depth - 1](self.temporaryUpsampler(y))\n            straight = self.rgb_converters[depth](self.layers[depth - 1](y))\n\n            out = (alpha * straight) + ((1 - alpha) * residual)\n\n        else:\n            out = self.rgb_converters[0](y)\n\n        return out\n\n\n# ========================================================================================\n# Discriminator Module\n# can be used with ProGAN or standalone (for inference).\n# Note this cannot be used with ConditionalProGAN\n# ========================================================================================\n\nclass Discriminator(th.nn.Module):\n    """""" Discriminator of the GAN """"""\n\n    def __init__(self, height=7, feature_size=512, use_eql=True):\n        """"""\n        constructor for the class\n        :param height: total height of the discriminator (Must be equal to the Generator depth)\n        :param feature_size: size of the deepest features extracted\n                             (Must be equal to Generator latent_size)\n        :param use_eql: whether to use equalized learning rate\n        """"""\n        from torch.nn import ModuleList, AvgPool2d\n        from pro_gan_pytorch.CustomLayers import DisGeneralConvBlock, DisFinalBlock\n\n        super(Discriminator, self).__init__()\n\n        assert feature_size != 0 and ((feature_size & (feature_size - 1)) == 0), \\\n            ""latent size not a power of 2""\n        if height >= 4:\n            assert feature_size >= np.power(2, height - 4), ""feature size cannot be produced""\n\n        # create state of the object\n        self.use_eql = use_eql\n        self.height = height\n        self.feature_size = feature_size\n\n        self.final_block = DisFinalBlock(self.feature_size, use_eql=self.use_eql)\n\n        # create a module list of the other required general convolution blocks\n        self.layers = ModuleList([])  # initialize to empty list\n\n        # create the fromRGB layers for various inputs:\n        if self.use_eql:\n            from pro_gan_pytorch.CustomLayers import _equalized_conv2d\n            self.fromRGB = lambda out_channels: \\\n                _equalized_conv2d(3, out_channels, (1, 1), bias=True)\n        else:\n            from torch.nn import Conv2d\n            self.fromRGB = lambda out_channels: Conv2d(3, out_channels, (1, 1), bias=True)\n\n        self.rgb_to_features = ModuleList([self.fromRGB(self.feature_size)])\n\n        # create the remaining layers\n        for i in range(self.height - 1):\n            if i > 2:\n                layer = DisGeneralConvBlock(\n                    int(self.feature_size // np.power(2, i - 2)),\n                    int(self.feature_size // np.power(2, i - 3)),\n                    use_eql=self.use_eql\n                )\n                rgb = self.fromRGB(int(self.feature_size // np.power(2, i - 2)))\n            else:\n                layer = DisGeneralConvBlock(self.feature_size,\n                                            self.feature_size, use_eql=self.use_eql)\n                rgb = self.fromRGB(self.feature_size)\n\n            self.layers.append(layer)\n            self.rgb_to_features.append(rgb)\n\n        # register the temporary downSampler\n        self.temporaryDownsampler = AvgPool2d(2)\n\n    def forward(self, x, height, alpha):\n        """"""\n        forward pass of the discriminator\n        :param x: input to the network\n        :param height: current height of operation (Progressive GAN)\n        :param alpha: current value of alpha for fade-in\n        :return: out => raw prediction values (WGAN-GP)\n        """"""\n\n        assert height < self.height, ""Requested output depth cannot be produced""\n\n        if height > 0:\n            residual = self.rgb_to_features[height - 1](self.temporaryDownsampler(x))\n\n            straight = self.layers[height - 1](\n                self.rgb_to_features[height](x)\n            )\n\n            y = (alpha * straight) + ((1 - alpha) * residual)\n\n            for block in reversed(self.layers[:height - 1]):\n                y = block(y)\n        else:\n            y = self.rgb_to_features[0](x)\n\n        out = self.final_block(y)\n\n        return out\n\n\n# ========================================================================================\n# ConditionalDiscriminator Module\n# uses the projection discrimination mechanism\n# can be used with ConditionalProGAN or standalone (for inference)\n# Note that this is not to be used with ProGAN\n# ========================================================================================\n\nclass ConditionalDiscriminator(th.nn.Module):\n    """""" Discriminator of the GAN """"""\n\n    def __init__(self, num_classes, height=7, feature_size=512, use_eql=True):\n        """"""\n        constructor for the class\n        :param num_classes: number of classes for conditional discrimination\n        :param height: total height of the discriminator (Must be equal to the Generator depth)\n        :param feature_size: size of the deepest features extracted\n                             (Must be equal to Generator latent_size)\n        :param use_eql: whether to use equalized learning rate\n        """"""\n        from torch.nn import ModuleList, AvgPool2d\n        from pro_gan_pytorch.CustomLayers import DisGeneralConvBlock, ConDisFinalBlock\n\n        super(ConditionalDiscriminator, self).__init__()\n\n        assert feature_size != 0 and ((feature_size & (feature_size - 1)) == 0), \\\n            ""latent size not a power of 2""\n        if height >= 4:\n            assert feature_size >= np.power(2, height - 4), ""feature size cannot be produced""\n\n        # create state of the object\n        self.use_eql = use_eql\n        self.height = height\n        self.feature_size = feature_size\n        self.num_classes = num_classes\n\n        self.final_block = ConDisFinalBlock(self.feature_size, self.num_classes,\n                                            use_eql=self.use_eql)\n\n        # create a module list of the other required general convolution blocks\n        self.layers = ModuleList([])  # initialize to empty list\n\n        # create the fromRGB layers for various inputs:\n        if self.use_eql:\n            from pro_gan_pytorch.CustomLayers import _equalized_conv2d\n            self.fromRGB = lambda out_channels: \\\n                _equalized_conv2d(3, out_channels, (1, 1), bias=True)\n        else:\n            from torch.nn import Conv2d\n            self.fromRGB = lambda out_channels: Conv2d(3, out_channels, (1, 1), bias=True)\n\n        self.rgb_to_features = ModuleList([self.fromRGB(self.feature_size)])\n\n        # create the remaining layers\n        for i in range(self.height - 1):\n            if i > 2:\n                layer = DisGeneralConvBlock(\n                    int(self.feature_size // np.power(2, i - 2)),\n                    int(self.feature_size // np.power(2, i - 3)),\n                    use_eql=self.use_eql\n                )\n                rgb = self.fromRGB(int(self.feature_size // np.power(2, i - 2)))\n            else:\n                layer = DisGeneralConvBlock(self.feature_size,\n                                            self.feature_size, use_eql=self.use_eql)\n                rgb = self.fromRGB(self.feature_size)\n\n            self.layers.append(layer)\n            self.rgb_to_features.append(rgb)\n\n        # register the temporary downSampler\n        self.temporaryDownsampler = AvgPool2d(2)\n\n    def forward(self, x, labels, height, alpha):\n        """"""\n        forward pass of the discriminator\n        :param x: input to the network\n        :param labels: labels required for conditional discrimination\n                       note that these are pure integer labels of shape [B x 1]\n        :param height: current height of operation (Progressive GAN)\n        :param alpha: current value of alpha for fade-in\n        :return: out => raw prediction values\n        """"""\n\n        assert height < self.height, ""Requested output depth cannot be produced""\n\n        if height > 0:\n            residual = self.rgb_to_features[height - 1](self.temporaryDownsampler(x))\n\n            straight = self.layers[height - 1](\n                self.rgb_to_features[height](x)\n            )\n\n            y = (alpha * straight) + ((1 - alpha) * residual)\n\n            for block in reversed(self.layers[:height - 1]):\n                y = block(y)\n        else:\n            y = self.rgb_to_features[0](x)\n\n        out = self.final_block(y, labels)\n\n        return out\n\n\n# ========================================================================================\n# ProGAN Module (Unconditional)\n# ========================================================================================\n\nclass ProGAN:\n    """""" Wrapper around the Generator and the Discriminator """"""\n\n    def __init__(self, depth=7, latent_size=512, learning_rate=0.001, beta_1=0,\n                 beta_2=0.99, eps=1e-8, drift=0.001, n_critic=1, use_eql=True,\n                 loss=""wgan-gp"", use_ema=True, ema_decay=0.999,\n                 device=th.device(""cpu"")):\n        """"""\n        constructor for the class\n        :param depth: depth of the GAN (will be used for each generator and discriminator)\n        :param latent_size: latent size of the manifold used by the GAN\n        :param learning_rate: learning rate for Adam\n        :param beta_1: beta_1 for Adam\n        :param beta_2: beta_2 for Adam\n        :param eps: epsilon for Adam\n        :param n_critic: number of times to update discriminator per generator update\n        :param drift: drift penalty for the\n                      (Used only if loss is wgan or wgan-gp)\n        :param use_eql: whether to use equalized learning rate\n        :param loss: the loss function to be used\n                     Can either be a string =>\n                          [""wgan-gp"", ""wgan"", ""lsgan"", ""lsgan-with-sigmoid"",\n                          ""hinge"", ""standard-gan"" or ""relativistic-hinge""]\n                     Or an instance of GANLoss\n        :param use_ema: boolean for whether to use exponential moving averages\n        :param ema_decay: value of mu for ema\n        :param device: device to run the GAN on (GPU / CPU)\n        """"""\n\n        from torch.optim import Adam\n        from torch.nn import DataParallel\n\n        # Create the Generator and the Discriminator\n        self.gen = Generator(depth, latent_size, use_eql=use_eql).to(device)\n        self.dis = Discriminator(depth, latent_size, use_eql=use_eql).to(device)\n\n        # if code is to be run on GPU, we can use DataParallel:\n        if device == th.device(""cuda""):\n            self.gen = DataParallel(self.gen)\n            self.dis = DataParallel(self.dis)\n\n        # state of the object\n        self.latent_size = latent_size\n        self.depth = depth\n        self.use_ema = use_ema\n        self.ema_decay = ema_decay\n        self.n_critic = n_critic\n        self.use_eql = use_eql\n        self.device = device\n        self.drift = drift\n\n        # define the optimizers for the discriminator and generator\n        self.gen_optim = Adam(self.gen.parameters(), lr=learning_rate,\n                              betas=(beta_1, beta_2), eps=eps)\n\n        self.dis_optim = Adam(self.dis.parameters(), lr=learning_rate,\n                              betas=(beta_1, beta_2), eps=eps)\n\n        # define the loss function used for training the GAN\n        self.loss = self.__setup_loss(loss)\n\n        if self.use_ema:\n            from pro_gan_pytorch.CustomLayers import update_average\n\n            # create a shadow copy of the generator\n            self.gen_shadow = copy.deepcopy(self.gen)\n\n            # updater function:\n            self.ema_updater = update_average\n\n            # initialize the gen_shadow weights equal to the\n            # weights of gen\n            self.ema_updater(self.gen_shadow, self.gen, beta=0)\n\n    def __setup_loss(self, loss):\n        import pro_gan_pytorch.Losses as losses\n\n        if isinstance(loss, str):\n            loss = loss.lower()  # lowercase the string\n            if loss == ""wgan"":\n                loss = losses.WGAN_GP(self.dis, self.drift, use_gp=False)\n                # note if you use just wgan, you will have to use weight clipping\n                # in order to prevent gradient exploding\n                # check the optimize_discriminator method where this has been\n                # taken care of.\n\n            elif loss == ""wgan-gp"":\n                loss = losses.WGAN_GP(self.dis, self.drift, use_gp=True)\n\n            elif loss == ""standard-gan"":\n                loss = losses.StandardGAN(self.dis)\n\n            elif loss == ""lsgan"":\n                loss = losses.LSGAN(self.dis)\n\n            elif loss == ""lsgan-with-sigmoid"":\n                loss = losses.LSGAN_SIGMOID(self.dis)\n\n            elif loss == ""hinge"":\n                loss = losses.HingeGAN(self.dis)\n\n            elif loss == ""relativistic-hinge"":\n                loss = losses.RelativisticAverageHingeGAN(self.dis)\n\n            else:\n                raise ValueError(""Unknown loss function requested"")\n\n        elif not isinstance(loss, losses.GANLoss):\n            raise ValueError(""loss is neither an instance of GANLoss nor a string"")\n\n        return loss\n\n    def __progressive_downsampling(self, real_batch, depth, alpha):\n        """"""\n        private helper for downsampling the original images in order to facilitate the\n        progressive growing of the layers.\n        :param real_batch: batch of real samples\n        :param depth: depth at which training is going on\n        :param alpha: current value of the fader alpha\n        :return: real_samples => modified real batch of samples\n        """"""\n\n        from torch.nn import AvgPool2d\n        from torch.nn.functional import interpolate\n\n        # downsample the real_batch for the given depth\n        down_sample_factor = int(np.power(2, self.depth - depth - 1))\n        prior_downsample_factor = max(int(np.power(2, self.depth - depth)), 0)\n\n        ds_real_samples = AvgPool2d(down_sample_factor)(real_batch)\n\n        if depth > 0:\n            prior_ds_real_samples = interpolate(AvgPool2d(prior_downsample_factor)(real_batch),\n                                                scale_factor=2)\n        else:\n            prior_ds_real_samples = ds_real_samples\n\n        # real samples are a combination of ds_real_samples and prior_ds_real_samples\n        real_samples = (alpha * ds_real_samples) + ((1 - alpha) * prior_ds_real_samples)\n\n        # return the so computed real_samples\n        return real_samples\n\n    def optimize_discriminator(self, noise, real_batch, depth, alpha):\n        """"""\n        performs one step of weight update on discriminator using the batch of data\n        :param noise: input noise of sample generation\n        :param real_batch: real samples batch\n        :param depth: current depth of optimization\n        :param alpha: current alpha for fade-in\n        :return: current loss (Wasserstein loss)\n        """"""\n\n        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n\n        loss_val = 0\n        for _ in range(self.n_critic):\n            # generate a batch of samples\n            fake_samples = self.gen(noise, depth, alpha).detach()\n\n            loss = self.loss.dis_loss(real_samples, fake_samples, depth, alpha)\n\n            # optimize discriminator\n            self.dis_optim.zero_grad()\n            loss.backward()\n            self.dis_optim.step()\n\n            loss_val += loss.item()\n\n        return loss_val / self.n_critic\n\n    def optimize_generator(self, noise, real_batch, depth, alpha):\n        """"""\n        performs one step of weight update on generator for the given batch_size\n        :param noise: input random noise required for generating samples\n        :param real_batch: batch of real samples\n        :param depth: depth of the network at which optimization is done\n        :param alpha: value of alpha for fade-in effect\n        :return: current loss (Wasserstein estimate)\n        """"""\n\n        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n\n        # generate fake samples:\n        fake_samples = self.gen(noise, depth, alpha)\n\n        # TODO_complete:\n        # Change this implementation for making it compatible for relativisticGAN\n        loss = self.loss.gen_loss(real_samples, fake_samples, depth, alpha)\n\n        # optimize the generator\n        self.gen_optim.zero_grad()\n        loss.backward()\n        self.gen_optim.step()\n\n        # if use_ema is true, apply ema to the generator parameters\n        if self.use_ema:\n            self.ema_updater(self.gen_shadow, self.gen, self.ema_decay)\n\n        # return the loss value\n        return loss.item()\n\n    @staticmethod\n    def create_grid(samples, scale_factor, img_file):\n        """"""\n        utility function to create a grid of GAN samples\n        :param samples: generated samples for storing\n        :param scale_factor: factor for upscaling the image\n        :param img_file: name of file to write\n        :return: None (saves a file)\n        """"""\n        from torchvision.utils import save_image\n        from torch.nn.functional import interpolate\n\n        # upsample the image\n        if scale_factor > 1:\n            samples = interpolate(samples, scale_factor=scale_factor)\n\n        # save the images:\n        save_image(samples, img_file, nrow=int(np.sqrt(len(samples))),\n                   normalize=True, scale_each=True)\n\n    def train(self, dataset, epochs, batch_sizes,\n              fade_in_percentage, num_samples=16,\n              start_depth=0, num_workers=3, feedback_factor=100,\n              log_dir=""./models/"", sample_dir=""./samples/"", save_dir=""./models/"",\n              checkpoint_factor=1):\n        """"""\n        Utility method for training the ProGAN. Note that you don\'t have to necessarily use this\n        you can use the optimize_generator and optimize_discriminator for your own training routine.\n        :param dataset: object of the dataset used for training.\n                        Note that this is not the dataloader (we create dataloader in this method\n                        since the batch_sizes for resolutions can be different)\n        :param epochs: list of number of epochs to train the network for every resolution\n        :param batch_sizes: list of batch_sizes for every resolution\n        :param fade_in_percentage: list of percentages of epochs per resolution\n                                   used for fading in the new layer\n                                   not used for first resolution, but dummy value still needed.\n        :param num_samples: number of samples generated in sample_sheet. def=36\n        :param start_depth: start training from this depth. def=0\n        :param num_workers: number of workers for reading the data. def=3\n        :param feedback_factor: number of logs per epoch. def=100\n        :param log_dir: directory for saving the loss logs. def=""./models/""\n        :param sample_dir: directory for saving the generated samples. def=""./samples/""\n        :param checkpoint_factor: save model after these many epochs.\n                                  Note that only one model is stored per resolution.\n                                  during one resolution, the checkpoint will be updated (Rewritten)\n                                  according to this factor.\n        :param save_dir: directory for saving the models (.pth files)\n        :return: None (Writes multiple files to disk)\n        """"""\n        from pro_gan_pytorch.DataTools import get_data_loader\n\n        assert self.depth == len(batch_sizes), ""batch_sizes not compatible with depth""\n\n        # turn the generator and discriminator into train mode\n        self.gen.train()\n        self.dis.train()\n        if self.use_ema:\n            self.gen_shadow.train()\n\n        # create a global time counter\n        global_time = time.time()\n\n        # create fixed_input for debugging\n        fixed_input = th.randn(num_samples, self.latent_size).to(self.device)\n\n        print(""Starting the training process ... "")\n        for current_depth in range(start_depth, self.depth):\n\n            print(""\\n\\nCurrently working on Depth: "", current_depth)\n            current_res = np.power(2, current_depth + 2)\n            print(""Current resolution: %d x %d"" % (current_res, current_res))\n\n            data = get_data_loader(dataset, batch_sizes[current_depth], num_workers)\n            ticker = 1\n\n            for epoch in range(1, epochs[current_depth] + 1):\n                start = timeit.default_timer()  # record time at the start of epoch\n\n                print(""\\nEpoch: %d"" % epoch)\n                total_batches = len(iter(data))\n\n                fader_point = int((fade_in_percentage[current_depth] / 100)\n                                  * epochs[current_depth] * total_batches)\n\n                step = 0  # counter for number of iterations\n\n                for (i, batch) in enumerate(data, 1):\n                    # calculate the alpha for fading in the layers\n                    alpha = ticker / fader_point if ticker <= fader_point else 1\n\n                    # extract current batch of data for training\n                    images = batch.to(self.device)\n\n                    gan_input = th.randn(images.shape[0], self.latent_size).to(self.device)\n\n                    # optimize the discriminator:\n                    dis_loss = self.optimize_discriminator(gan_input, images,\n                                                           current_depth, alpha)\n\n                    # optimize the generator:\n                    gen_loss = self.optimize_generator(gan_input, images, current_depth, alpha)\n\n                    # provide a loss feedback\n                    if i % int(total_batches / feedback_factor) == 0 or i == 1:\n                        elapsed = time.time() - global_time\n                        elapsed = str(datetime.timedelta(seconds=elapsed))\n                        print(""Elapsed: [%s]  batch: %d  d_loss: %f  g_loss: %f""\n                              % (elapsed, i, dis_loss, gen_loss))\n\n                        # also write the losses to the log file:\n                        os.makedirs(log_dir, exist_ok=True)\n                        log_file = os.path.join(log_dir, ""loss_"" + str(current_depth) + "".log"")\n                        with open(log_file, ""a"") as log:\n                            log.write(str(step) + ""\\t"" + str(dis_loss) +\n                                      ""\\t"" + str(gen_loss) + ""\\n"")\n\n                        # create a grid of samples and save it\n                        os.makedirs(sample_dir, exist_ok=True)\n                        gen_img_file = os.path.join(sample_dir, ""gen_"" + str(current_depth) +\n                                                    ""_"" + str(epoch) + ""_"" +\n                                                    str(i) + "".png"")\n\n                        # this is done to allow for more GPU space\n                        with th.no_grad():\n                            self.create_grid(\n                                samples=self.gen(\n                                    fixed_input,\n                                    current_depth,\n                                    alpha\n                                ).detach() if not self.use_ema\n                                else self.gen_shadow(\n                                    fixed_input,\n                                    current_depth,\n                                    alpha\n                                ).detach(),\n                                scale_factor=int(np.power(2, self.depth - current_depth - 1)),\n                                img_file=gen_img_file,\n                            )\n\n                    # increment the alpha ticker and the step\n                    ticker += 1\n                    step += 1\n\n                stop = timeit.default_timer()\n                print(""Time taken for epoch: %.3f secs"" % (stop - start))\n\n                if epoch % checkpoint_factor == 0 or epoch == 1 or epoch == epochs[current_depth]:\n                    os.makedirs(save_dir, exist_ok=True)\n                    gen_save_file = os.path.join(save_dir, ""GAN_GEN_"" + str(current_depth) + "".pth"")\n                    dis_save_file = os.path.join(save_dir, ""GAN_DIS_"" + str(current_depth) + "".pth"")\n                    gen_optim_save_file = os.path.join(save_dir,\n                                                       ""GAN_GEN_OPTIM_"" + str(current_depth)\n                                                       + "".pth"")\n                    dis_optim_save_file = os.path.join(save_dir,\n                                                       ""GAN_DIS_OPTIM_"" + str(current_depth)\n                                                       + "".pth"")\n\n                    th.save(self.gen.state_dict(), gen_save_file)\n                    th.save(self.dis.state_dict(), dis_save_file)\n                    th.save(self.gen_optim.state_dict(), gen_optim_save_file)\n                    th.save(self.dis_optim.state_dict(), dis_optim_save_file)\n\n                    # also save the shadow generator if use_ema is True\n                    if self.use_ema:\n                        gen_shadow_save_file = os.path.join(save_dir, ""GAN_GEN_SHADOW_"" +\n                                                            str(current_depth) + "".pth"")\n                        th.save(self.gen_shadow.state_dict(), gen_shadow_save_file)\n\n        # put the gen, shadow_gen and dis in eval mode\n        self.gen.eval()\n        self.dis.eval()\n        if self.use_ema:\n            self.gen_shadow.eval()\n\n        print(""Training completed ..."")\n\n\n# ========================================================================================\n# ConditionalProGAN Module\n# ========================================================================================\n\nclass ConditionalProGAN:\n    """""" Wrapper around the Generator and the Conditional Discriminator """"""\n\n    def __init__(self, num_classes, depth=7, latent_size=512,\n                 learning_rate=0.001, beta_1=0, beta_2=0.99,\n                 eps=1e-8, drift=0.001, n_critic=1, use_eql=True,\n                 loss=""wgan-gp"", use_ema=True, ema_decay=0.999,\n                 device=th.device(""cpu"")):\n        """"""\n        constructor for the class\n        :param num_classes: number of classes required for the conditional gan\n        :param depth: depth of the GAN (will be used for each generator and discriminator)\n        :param latent_size: latent size of the manifold used by the GAN\n        :param learning_rate: learning rate for Adam\n        :param beta_1: beta_1 for Adam\n        :param beta_2: beta_2 for Adam\n        :param eps: epsilon for Adam\n        :param n_critic: number of times to update discriminator\n                         (Used only if loss is wgan or wgan-gp)\n        :param drift: drift penalty for the\n                      (Used only if loss is wgan or wgan-gp)\n        :param use_eql: whether to use equalized learning rate\n        :param loss: the loss function to be used\n                     Can either be a string =>\n                          [""wgan-gp"", ""wgan"", ""lsgan"", ""lsgan-with-sigmoid"",\n                          ""hinge"", ""standard-gan"" or ""relativistic-hinge""]\n                     Or an instance of ConditionalGANLoss\n        :param use_ema: boolean for whether to use exponential moving averages\n        :param ema_decay: value of mu for ema\n        :param device: device to run the GAN on (GPU / CPU)\n        """"""\n\n        from torch.optim import Adam\n        from torch.nn import DataParallel\n\n        # Create the Generator and the Discriminator\n        self.gen = Generator(depth, latent_size, use_eql=use_eql).to(device)\n        self.dis = ConditionalDiscriminator(\n            num_classes, height=depth,\n            feature_size=latent_size,\n            use_eql=use_eql).to(device)\n\n        # if code is to be run on GPU, we can use DataParallel:\n        if device == th.device(""cuda""):\n            self.gen = DataParallel(self.gen)\n            self.dis = DataParallel(self.dis)\n\n        # state of the object\n        self.latent_size = latent_size\n        self.depth = depth\n        self.use_ema = use_ema\n        self.num_classes = num_classes  # required for matching aware\n        self.ema_decay = ema_decay\n        self.n_critic = n_critic\n        self.use_eql = use_eql\n        self.device = device\n        self.drift = drift\n\n        # define the optimizers for the discriminator and generator\n        self.gen_optim = Adam(self.gen.parameters(), lr=learning_rate,\n                              betas=(beta_1, beta_2), eps=eps)\n\n        self.dis_optim = Adam(self.dis.parameters(), lr=learning_rate,\n                              betas=(beta_1, beta_2), eps=eps)\n\n        # define the loss function used for training the GAN\n        self.loss = self.__setup_loss(loss)\n\n        # setup the ema for the generator\n        if self.use_ema:\n            from pro_gan_pytorch.CustomLayers import update_average\n\n            # create a shadow copy of the generator\n            self.gen_shadow = copy.deepcopy(self.gen)\n\n            # updater function:\n            self.ema_updater = update_average\n\n            # initialize the gen_shadow weights equal to the\n            # weights of gen\n            self.ema_updater(self.gen_shadow, self.gen, beta=0)\n\n    def __setup_loss(self, loss):\n        import pro_gan_pytorch.Losses as losses\n\n        if isinstance(loss, str):\n            loss = loss.lower()  # lowercase the string\n            if loss == ""wgan"":\n                loss = losses.CondWGAN_GP(self.dis, self.drift, use_gp=False)\n                # note if you use just wgan, you will have to use weight clipping\n                # in order to prevent gradient exploding\n\n            elif loss == ""wgan-gp"":\n                loss = losses.CondWGAN_GP(self.dis, self.drift, use_gp=True)\n\n            elif loss == ""lsgan"":\n                loss = losses.CondLSGAN(self.dis)\n\n            elif loss == ""lsgan-with-sigmoid"":\n                loss = losses.CondLSGAN_SIGMOID(self.dis)\n\n            elif loss == ""hinge"":\n                loss = losses.CondHingeGAN(self.dis)\n\n            elif loss == ""standard-gan"":\n                loss = losses.CondStandardGAN(self.dis)\n\n            elif loss == ""relativistic-hinge"":\n                loss = losses.CondRelativisticAverageHingeGAN(self.dis)\n\n            else:\n                raise ValueError(""Unknown loss function requested"")\n\n        elif not isinstance(loss, losses.ConditionalGANLoss):\n            raise ValueError(""loss is neither an instance of GANLoss nor a string"")\n\n        return loss\n\n    def __progressive_downsampling(self, real_batch, depth, alpha):\n        """"""\n        private helper for downsampling the original images in order to facilitate the\n        progressive growing of the layers.\n        :param real_batch: batch of real samples\n        :param depth: depth at which training is going on\n        :param alpha: current value of the fader alpha\n        :return: real_samples => modified real batch of samples\n        """"""\n\n        from torch.nn import AvgPool2d\n        from torch.nn.functional import interpolate\n\n        # downsample the real_batch for the given depth\n        down_sample_factor = int(np.power(2, self.depth - depth - 1))\n        prior_downsample_factor = max(int(np.power(2, self.depth - depth)), 0)\n\n        ds_real_samples = AvgPool2d(down_sample_factor)(real_batch)\n\n        if depth > 0:\n            prior_ds_real_samples = interpolate(AvgPool2d(prior_downsample_factor)(real_batch),\n                                                scale_factor=2)\n        else:\n            prior_ds_real_samples = ds_real_samples\n\n        # real samples are a combination of ds_real_samples and prior_ds_real_samples\n        real_samples = (alpha * ds_real_samples) + ((1 - alpha) * prior_ds_real_samples)\n\n        # return the so computed real_samples\n        return real_samples\n\n    def optimize_discriminator(self, noise, real_batch, labels, depth, alpha):\n        """"""\n        performs one step of weight update on discriminator using the batch of data\n        :param noise: input noise of sample generation\n        :param real_batch: real samples batch\n        :param labels: (conditional classes) should be a list of integers\n        :param depth: current depth of optimization\n        :param alpha: current alpha for fade-in\n        :return: current loss value\n        """"""\n\n        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n\n        loss_val = 0\n        for _ in range(self.n_critic):\n            # generate a batch of samples\n            fake_samples = self.gen(noise, depth, alpha).detach()\n\n            loss = self.loss.dis_loss(real_samples, fake_samples,\n                                      labels, depth, alpha)\n\n            # optimize discriminator\n            self.dis_optim.zero_grad()\n            loss.backward()\n            self.dis_optim.step()\n\n            loss_val += loss.item()\n\n        return loss_val / self.n_critic\n\n    def optimize_generator(self, noise, real_batch, labels, depth, alpha):\n        """"""\n        performs one step of weight update on generator for the given batch_size\n        :param noise: input random noise required for generating samples\n        :param real_batch: real batch of samples (real samples)\n        :param labels: labels for conditional discrimination\n        :param depth: depth of the network at which optimization is done\n        :param alpha: value of alpha for fade-in effect\n        :return: current loss (Wasserstein estimate)\n        """"""\n\n        # create batch of real samples\n        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n\n        # generate fake samples:\n        fake_samples = self.gen(noise, depth, alpha)\n\n        # TODO_complete:\n        # Change this implementation for making it compatible for relativisticGAN\n        loss = self.loss.gen_loss(real_samples, fake_samples, labels, depth, alpha)\n\n        # optimize the generator\n        self.gen_optim.zero_grad()\n        loss.backward()\n        self.gen_optim.step()\n\n        # if use_ema is true, apply ema to the generator parameters\n        if self.use_ema:\n            self.ema_updater(self.gen_shadow, self.gen, self.ema_decay)\n\n        # return the loss value\n        return loss.item()\n\n    @staticmethod\n    def create_grid(samples, scale_factor, img_file):\n        """"""\n        utility function to create a grid of GAN samples\n        :param samples: generated samples for storing\n        :param scale_factor: factor for upscaling the image\n        :param img_file: name of file to write\n        :return: None (saves a file)\n        """"""\n        from torchvision.utils import save_image\n        from torch.nn.functional import interpolate\n\n        # upsample the image\n        if scale_factor > 1:\n            samples = interpolate(samples, scale_factor=scale_factor)\n\n        # save the images:\n        save_image(samples, img_file, nrow=int(np.sqrt(len(samples))),\n                   normalize=True, scale_each=True)\n\n    @staticmethod\n    def __save_label_info_file(label_file, labels):\n        """"""\n        utility method for saving a file with labels\n        :param label_file: path to the file to be written\n        :param labels: label tensor\n        :return: None (writes file to disk)\n        """"""\n        # write file with the labels written one per line\n        with open(label_file, ""w"") as fp:\n            for label in labels:\n                fp.write(str(label.item()) + ""\\n"")\n\n    def one_hot_encode(self, labels):\n        """"""\n        utility method to one-hot encode the labels\n        :param labels: tensor of labels (Batch)\n        :return: enc_label: encoded one_hot label\n        """"""\n        if not hasattr(self, ""label_oh_encoder""):\n            self.label_oh_encoder = th.nn.Embedding(self.num_classes, self.num_classes)\n            self.label_oh_encoder.weight.data = th.eye(self.num_classes)\n\n        return self.label_oh_encoder(labels.view(-1))\n\n    def train(self, dataset, epochs, batch_sizes,\n              fade_in_percentage, start_depth=0, num_workers=3, feedback_factor=100,\n              log_dir=""./models/"", sample_dir=""./samples/"", save_dir=""./models/"",\n              checkpoint_factor=1):\n        """"""\n        Utility method for training the ProGAN. Note that you don\'t have to necessarily use this\n        you can use the optimize_generator and optimize_discriminator for your own training routine.\n        :param dataset: object of the dataset used for training.\n                        Note that this is not the dataloader (we create dataloader in this method\n                        since the batch_sizes for resolutions can be different).\n                        Get_item should return (Image, label) in that order\n        :param epochs: list of number of epochs to train the network for every resolution\n        :param batch_sizes: list of batch_sizes for every resolution\n        :param fade_in_percentage: list of percentages of epochs per resolution\n                                   used for fading in the new layer\n                                   not used for first resolution, but dummy value still needed.\n        :param start_depth: start training from this depth. def=0\n        :param num_workers: number of workers for reading the data. def=3\n        :param feedback_factor: number of logs per epoch. def=100\n        :param log_dir: directory for saving the loss logs. def=""./models/""\n        :param sample_dir: directory for saving the generated samples. def=""./samples/""\n        :param checkpoint_factor: save model after these many epochs.\n                                  Note that only one model is stored per resolution.\n                                  during one resolution, the checkpoint will be updated (Rewritten)\n                                  according to this factor.\n        :param save_dir: directory for saving the models (.pth files)\n        :return: None (Writes multiple files to disk)\n        """"""\n        from pro_gan_pytorch.DataTools import get_data_loader\n\n        assert self.depth == len(batch_sizes), ""batch_sizes not compatible with depth""\n\n        # turn the generator and discriminator into train mode\n        self.gen.train()\n        self.dis.train()\n        if self.use_ema:\n            self.gen_shadow.train()\n\n        # create a global time counter\n        global_time = time.time()\n\n        # create fixed_input for debugging\n        temp_data_loader = get_data_loader(dataset, batch_sizes[0], num_workers=3)\n        _, fx_labels = next(iter(temp_data_loader))\n        # reshape them properly\n        fixed_labels = self.one_hot_encode(fx_labels.view(-1, 1)).to(self.device)\n        fixed_input = th.randn(fixed_labels.shape[0],\n                               self.latent_size - self.num_classes).to(self.device)\n        fixed_input = th.cat((fixed_labels, fixed_input), dim=-1)\n        del temp_data_loader  # delete the temp data_loader since it is not required anymore\n\n        os.makedirs(sample_dir, exist_ok=True)  # make sure the directory exists\n        self.__save_label_info_file(os.path.join(sample_dir, ""labels.txt""), fx_labels)\n\n        print(""Starting the training process ... "")\n        for current_depth in range(start_depth, self.depth):\n\n            print(""\\n\\nCurrently working on Depth: "", current_depth)\n            current_res = np.power(2, current_depth + 2)\n            print(""Current resolution: %d x %d"" % (current_res, current_res))\n\n            data = get_data_loader(dataset, batch_sizes[current_depth], num_workers)\n            ticker = 1\n\n            for epoch in range(1, epochs[current_depth] + 1):\n                start = timeit.default_timer()  # record time at the start of epoch\n\n                print(""\\nEpoch: %d"" % epoch)\n                total_batches = len(iter(data))\n\n                fader_point = int((fade_in_percentage[current_depth] / 100)\n                                  * epochs[current_depth] * total_batches)\n\n                step = 0  # counter for number of iterations\n\n                for (i, batch) in enumerate(data, 1):\n                    # calculate the alpha for fading in the layers\n                    alpha = ticker / fader_point if ticker <= fader_point else 1\n\n                    # extract current batch of data for training\n                    images, labels = batch\n                    images = images.to(self.device)\n                    labels = labels.view(-1, 1)\n\n                    # create the input to the Generator\n                    label_information = self.one_hot_encode(labels).to(self.device)\n                    latent_vector = th.randn(images.shape[0],\n                                             self.latent_size - self.num_classes).to(self.device)\n                    gan_input = th.cat((label_information, latent_vector), dim=-1)\n\n                    # optimize the discriminator:\n                    dis_loss = self.optimize_discriminator(gan_input, images,\n                                                           labels, current_depth, alpha)\n\n                    # optimize the generator:\n                    gen_loss = self.optimize_generator(gan_input, images,\n                                                       labels, current_depth, alpha)\n\n                    # provide a loss feedback\n                    if i % int(total_batches / feedback_factor) == 0 or i == 1:\n                        elapsed = time.time() - global_time\n                        elapsed = str(datetime.timedelta(seconds=elapsed))\n                        print(""Elapsed: [%s]  batch: %d  d_loss: %f  g_loss: %f""\n                              % (elapsed, i, dis_loss, gen_loss))\n\n                        # also write the losses to the log file:\n                        os.makedirs(log_dir, exist_ok=True)\n                        log_file = os.path.join(log_dir, ""loss_"" + str(current_depth) + "".log"")\n                        with open(log_file, ""a"") as log:\n                            log.write(str(step) + ""\\t"" + str(dis_loss) +\n                                      ""\\t"" + str(gen_loss) + ""\\n"")\n\n                        # create a grid of samples and save it\n                        os.makedirs(sample_dir, exist_ok=True)\n                        gen_img_file = os.path.join(sample_dir, ""gen_"" + str(current_depth) +\n                                                    ""_"" + str(epoch) + ""_"" +\n                                                    str(i) + "".png"")\n\n                        # this is done to allow for more GPU space\n                        self.gen_optim.zero_grad()\n                        self.dis_optim.zero_grad()\n                        with th.no_grad():\n                            self.create_grid(\n                                samples=self.gen(\n                                    fixed_input,\n                                    current_depth,\n                                    alpha\n                                ) if not self.use_ema\n                                else self.gen_shadow(\n                                    fixed_input,\n                                    current_depth,\n                                    alpha\n                                ),\n                                scale_factor=int(np.power(2, self.depth - current_depth - 1)),\n                                img_file=gen_img_file,\n                            )\n\n                    # increment the alpha ticker and the step\n                    ticker += 1\n                    step += 1\n\n                stop = timeit.default_timer()\n                print(""Time taken for epoch: %.3f secs"" % (stop - start))\n\n                if epoch % checkpoint_factor == 0 or epoch == 1 or epoch == epochs[current_depth]:\n                    os.makedirs(save_dir, exist_ok=True)\n                    gen_save_file = os.path.join(save_dir, ""GAN_GEN_"" + str(current_depth) + "".pth"")\n                    dis_save_file = os.path.join(save_dir, ""GAN_DIS_"" + str(current_depth) + "".pth"")\n                    gen_optim_save_file = os.path.join(save_dir,\n                                                       ""GAN_GEN_OPTIM_"" + str(current_depth)\n                                                       + "".pth"")\n                    dis_optim_save_file = os.path.join(save_dir,\n                                                       ""GAN_DIS_OPTIM_"" + str(current_depth)\n                                                       + "".pth"")\n\n                    th.save(self.gen.state_dict(), gen_save_file)\n                    th.save(self.dis.state_dict(), dis_save_file)\n                    th.save(self.gen_optim.state_dict(), gen_optim_save_file)\n                    th.save(self.dis_optim.state_dict(), dis_optim_save_file)\n\n                    # also save the shadow generator if use_ema is True\n                    if self.use_ema:\n                        gen_shadow_save_file = os.path.join(save_dir, ""GAN_GEN_SHADOW_"" +\n                                                            str(current_depth) + "".pth"")\n                        th.save(self.gen_shadow.state_dict(), gen_shadow_save_file)\n\n        # put the gen, shadow_gen and dis in eval mode\n        self.gen.eval()\n        self.dis.eval()\n        if self.use_ema:\n            self.gen_shadow.eval()\n\n        print(""Training completed ..."")\n'"
pro_gan_pytorch/__init__.py,0,"b'"""""" Package has implementation of ProGAN (progressive growing of GANs)\n    as an extension of PyTorch Module\n""""""\nfrom pro_gan_pytorch import PRO_GAN\nfrom pro_gan_pytorch import CustomLayers\nfrom pro_gan_pytorch import Losses\nfrom pro_gan_pytorch import DataTools\n'"
samples/demo.py,1,"b'"""""" live (realtime) latent space interpolations of trained models """"""\n\nimport argparse\nimport torch as th\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom pro_gan_pytorch.PRO_GAN import Generator\nfrom torchvision.utils import make_grid\nfrom math import ceil, sqrt\nfrom scipy.ndimage import gaussian_filter\n\n# create the device for running the demo:\ndevice = th.device(""cuda"" if th.cuda.is_available() else ""cpu"")\n\n\ndef parse_arguments():\n    """"""\n    command line arguments parser\n    :return: args => parsed command line arguments\n    """"""\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""--generator_file"", action=""store"", type=str,\n                        default=None, help=""path to the trained generator model"")\n\n    parser.add_argument(""--depth"", action=""store"", type=int,\n                        default=9, help=""Depth of the network"")\n\n    parser.add_argument(""--out_depth"", action=""store"", type=int,\n                        default=6,\n                        help=""output depth of images. **Starts from 0"")\n\n    parser.add_argument(""--latent_size"", action=""store"", type=int,\n                        default=512, help=""Latent size for the network"")\n\n    parser.add_argument(""--num_points"", action=""store"", type=int,\n                        default=12, help=""Number of samples to be seen"")\n\n    parser.add_argument(""--transition_points"", action=""store"", type=int,\n                        default=30,\n                        help=""Number of transition samples for interpolation"")\n\n    parser.add_argument(""--smoothing"", action=""store"", type=float,\n                        default=1.0,\n                        help=""amount of transitional smoothing"")\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef adjust_dynamic_range(data, drange_in=(-1, 1), drange_out=(0, 1)):\n    """"""\n    adjust the dynamic colour range of the given input data\n    :param data: input image data\n    :param drange_in: original range of input\n    :param drange_out: required range of output\n    :return: img => colour range adjusted images\n    """"""\n    if drange_in != drange_out:\n        scale = (np.float32(drange_out[1]) - np.float32(drange_out[0])) / (\n                np.float32(drange_in[1]) - np.float32(drange_in[0]))\n        bias = (np.float32(drange_out[0]) - np.float32(drange_in[0]) * scale)\n        data = data * scale + bias\n    return th.clamp(data, min=0, max=1)\n\n\ndef get_image(gen, point, depth, alpha):\n    """"""\n    obtain an All-resolution grid of images from the given point\n    :param gen: the generator object\n    :param point: random latent point for generation\n    :param depth: value of depth for image generation (0 indexed)\n    :param alpha: value of alpha for fade-in (between 0 and 1)\n    :return: img => generated image\n    """"""\n    image = gen(point, depth, alpha).detach()\n    image = adjust_dynamic_range(image).squeeze(dim=0)\n    return image.cpu().numpy().transpose(1, 2, 0)\n\n\ndef main(args):\n    """"""\n    Main function for the script\n    :param args: parsed command line arguments\n    :return: None\n    """"""\n\n    # load the model for the demo\n    gen = th.nn.DataParallel(\n        Generator(\n            depth=args.depth,\n            latent_size=args.latent_size))\n    gen.load_state_dict(th.load(args.generator_file, map_location=str(device)))\n\n    # generate the set of points:\n    total_frames = args.num_points * args.transition_points\n    all_latents = th.randn(total_frames, args.latent_size).to(device)\n    all_latents = th.from_numpy(\n        gaussian_filter(\n            all_latents.cpu(),\n            [args.smoothing * args.transition_points, 0], mode=""wrap""))\n    all_latents = (all_latents /\n                   all_latents.norm(dim=-1, keepdim=True)) * sqrt(args.latent_size)\n\n    start_point = th.unsqueeze(all_latents[0], dim=0)\n    points = all_latents[1:]\n\n    fig, ax = plt.subplots()\n    plt.axis(""off"")\n    shower = plt.imshow(get_image(gen, start_point, args.out_depth, 1))\n\n    def init():\n        return shower,\n\n    def update(point):\n        shower.set_data(get_image(gen, th.unsqueeze(point, dim=0), args.out_depth, 1))\n        return shower,\n\n    # define the animation function\n    ani = FuncAnimation(fig, update, frames=points, \n                        init_func=init)\n    plt.show(ani)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments())\n'"
samples/generate_samples.py,3,"b'"""""" Generate single image samples from a particular depth of a model """"""\n\nimport argparse\nimport torch as th\nimport numpy as np\nimport os\nfrom torch.backends import cudnn\nfrom pro_gan_pytorch.PRO_GAN import Generator\nfrom torch.nn.functional import interpolate\nfrom scipy.misc import imsave\nfrom tqdm import tqdm\n\n# turn on the fast GPU processing mode on\ncudnn.benchmark = True\n\n\n# set the manual seed\n# th.manual_seed(3)\n\n# define the device for the training script\ndevice = th.device(""cuda"" if th.cuda.is_available() else ""cpu"")\n\ndef parse_arguments():\n    """"""\n    default command line argument parser\n    :return: args => parsed command line arguments\n    """"""\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""--generator_file"", action=""store"", type=str,\n                        help=""pretrained weights file for generator"", required=True)\n\n    parser.add_argument(""--latent_size"", action=""store"", type=int,\n                        default=256,\n                        help=""latent size for the generator"")\n\n    parser.add_argument(""--depth"", action=""store"", type=int,\n                        default=9,\n                        help=""depth of the network. **Starts from 1"")\n\n    parser.add_argument(""--out_depth"", action=""store"", type=int,\n                        default=6,\n                        help=""output depth of images. **Starts from 0"")\n\n    parser.add_argument(""--num_samples"", action=""store"", type=int,\n                        default=300,\n                        help=""number of synchronized grids to be generated"")\n\n    parser.add_argument(""--out_dir"", action=""store"", type=str,\n                        default=""interp_animation_frames/"",\n                        help=""path to the output directory for the frames"")\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef adjust_dynamic_range(data, drange_in=(-1, 1), drange_out=(0, 1)):\n    """"""\n    adjust the dynamic colour range of the given input data\n    :param data: input image data\n    :param drange_in: original range of input\n    :param drange_out: required range of output\n    :return: img => colour range adjusted images\n    """"""\n    if drange_in != drange_out:\n        scale = (np.float32(drange_out[1]) - np.float32(drange_out[0])) / (\n                np.float32(drange_in[1]) - np.float32(drange_in[0]))\n        bias = (np.float32(drange_out[0]) - np.float32(drange_in[0]) * scale)\n        data = data * scale + bias\n    return th.clamp(data, min=0, max=1)\n\n\ndef main(args):\n    """"""\n    Main function for the script\n    :param args: parsed command line arguments\n    :return: None\n    """"""\n\n    print(""Creating generator object ..."")\n    # create the generator object\n    gen = th.nn.DataParallel(Generator(\n        depth=args.depth,\n        latent_size=args.latent_size\n    ))\n\n    print(""Loading the generator weights from:"", args.generator_file)\n    # load the weights into it\n    gen.load_state_dict(\n        th.load(args.generator_file, map_location=str(device))\n    )\n\n    # path for saving the files:\n    save_path = args.out_dir\n\n    print(""Generating scale synchronized images ..."")\n    for img_num in tqdm(range(1, args.num_samples + 1)):\n        # generate the images:\n        with th.no_grad():\n            point = th.randn(1, args.latent_size)\n            point = (point / point.norm()) * (args.latent_size ** 0.5)\n            ss_image = gen(point, depth=args.out_depth, alpha=1)\n            # color adjust the generated image:\n            ss_image = adjust_dynamic_range(ss_image)\n\n        # save the ss_image in the directory\n        imsave(os.path.join(save_path, str(img_num) + "".png""),\n               ss_image.squeeze(0).permute(1, 2, 0).cpu())\n\n    print(""Generated %d images at %s"" % (args.num_samples, save_path))\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments())\n'"
samples/latent_space_interpolation.py,2,"b'"""""" script for generating samples from a trained model """"""\n\nimport torch as th\nimport numpy as np\nimport argparse\nimport os\nfrom torch.backends import cudnn\nfrom torchvision.utils import make_grid\nfrom math import sqrt, ceil\nfrom tqdm import tqdm\nfrom scipy.ndimage import gaussian_filter\nimport cv2\n\n\n# turn fast mode on\ncudnn.benchmark = True\n\n# define the device for the training script\ndevice = th.device(""cuda"" if th.cuda.is_available() else ""cpu"")\n\n\ndef parse_arguments():\n    """"""\n    command line arguments parser\n    :return: args => parsed command line arguments\n    """"""\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""--generator_file"", action=""store"", type=str,\n                        help=""pretrained weights file for generator"", required=True)\n\n    parser.add_argument(""--latent_size"", action=""store"", type=int,\n                        default=512,\n                        help=""latent size for the generator"")\n\n    parser.add_argument(""--depth"", action=""store"", type=int,\n                        default=9,\n                        help=""latent size for the generator"")\n\n    parser.add_argument(""--out_depth"", action=""store"", type=int,\n                        default=6,\n                        help=""output depth of images. **Starts from 0"")\n\n    parser.add_argument(""--time"", action=""store"", type=float,\n                        default=300,\n                        help=""Number of seconds for the video to make"")\n\n    parser.add_argument(""--fps"", action=""store"", type=int,\n                        default=60, help=""Frames per second in the video"")\n\n    parser.add_argument(""--smoothing"", action=""store"", type=float,\n                        default=0.75, help=""Smoothing amount in transition frames"")\n\n    parser.add_argument(""--out_dir"", action=""store"", type=str,\n                        default=""interp_animation_frames/"",\n                        help=""path to the output directory for the frames"")\n\n    parser.add_argument(""--video_only"", action=\'store_true\',\n                        help=""Pass this to skip saving of individual frames."")\n\n    parser.add_argument(""--video_name"", action=""store"", type=str,\n                        default="""", help=""Filename of video"")\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef adjust_dynamic_range(data, drange_in=(-1, 1), drange_out=(0, 1)):\n    if drange_in != drange_out:\n        scale = (np.float32(drange_out[1]) - np.float32(drange_out[0])) / (\n            np.float32(drange_in[1]) - np.float32(drange_in[0]))\n        bias = (np.float32(drange_out[0]) - np.float32(drange_in[0]) * scale)\n        data = data * scale + bias\n    return th.clamp(data, min=0, max=1)\n\n\ndef get_image(gen, point, depth, alpha):\n    image = gen(point, depth, alpha).detach()\n    image = adjust_dynamic_range(image).squeeze(dim=0)\n    return image.cpu().numpy().transpose(1, 2, 0)\n\n\ndef main(args):\n    """"""\n    Main function of the script\n    :param args: parsed commandline arguments\n    :return: None\n    """"""\n    from pro_gan_pytorch.PRO_GAN import Generator\n\n    # create generator object:\n    print(""Creating a generator object ..."")\n    generator = th.nn.DataParallel(\n        Generator(depth=args.depth,\n                  latent_size=args.latent_size).to(device))\n\n    # load the trained generator weights\n    print(""loading the trained generator weights ..."")\n    generator.load_state_dict(th.load(args.generator_file, str(device)))\n\n    # total_frames in the video:\n    total_frames = int(args.time * args.fps)\n\n    # Let\'s create the animation video from the latent space interpolation\n    # all latent vectors:\n    all_latents = th.randn(total_frames, args.latent_size).to(device)\n    all_latents = gaussian_filter(\n        all_latents.cpu(), [args.smoothing * args.fps, 0])\n    all_latents = th.from_numpy(all_latents)\n    all_latents = all_latents / all_latents.norm(dim=-1, keepdim=True) \\\n        * (sqrt(args.latent_size))\n\n    # create output directory\n    os.makedirs(args.out_dir, exist_ok=True)\n\n    global_frame_counter = 1\n\n    # If we\'re saving a video, make the video object\n    if(args.video_name):\n        width = 2**(args.depth+1)\n        out_file = os.path.join(args.out_dir, args.video_name)\n        video_out = cv2.VideoWriter(args.video_name, cv2.VideoWriter_fourcc(\n            *\'mp4v\'), args.fps, (width, width))\n\n    # Run the main loop for the interpolation:\n    print(""Generating the video frames ..."")\n    for latent in tqdm(all_latents):\n        latent = th.unsqueeze(latent, dim=0)\n\n        # generate the image for this point:\n        img = get_image(generator, latent, args.out_depth, 1) * 255\n\n        if not args.video_only:\n            cv2.imwrite(os.path.join(\n                args.out_dir, ""{:05d}.png"".format(global_frame_counter)), img)\n\n        # Make an image of unsigned 8-bit integers for OpenCV\n        if(args.video_name):\n            img_int = img.astype(np.uint8)\n            video_out.write(img_int)\n\n        # Increment the counter\n        global_frame_counter += 1\n\n    # video frames have been generated\n    if not args.video_only:\n        print(""Video frames have been generated at:"", args.out_dir)\n\n    if(args.video_name):\n        print(\'Video saved to {}\'.format(out_file))\n        video_out.release()\n\n\nif __name__ == ""__main__"":\n    main(parse_arguments())\n'"
test/__init__.py,0,b''
test/test_CustomLayers.py,0,"b'import torch as th\n\nfrom unittest import TestCase\nfrom pro_gan_pytorch import CustomLayers as cL\n\ndevice = th.device(""cuda"" if th.cuda.is_available() else ""cpu"")\n\n\nclass Test_equalized_conv2d(TestCase):\n\n    def setUp(self):\n        self.conv_block = cL._equalized_conv2d(21, 3, k_size=(3, 3), pad=1)\n\n        # print the Equalized conv block\n        print(""\\nEqualized conv block:\\n%s"" % str(self.conv_block))\n\n    def test_forward(self):\n        mock_in = th.randn(32, 21, 16, 16).to(device)\n        mock_out = self.conv_block(mock_in)\n\n        # check output\n        self.assertEqual(mock_out.shape, (32, 3, 16, 16))\n        self.assertEqual(th.isnan(mock_out).sum().item(), 0)\n        self.assertEqual(th.isinf(mock_out).sum().item(), 0)\n\n        # check the weight\'s scale\n        self.assertAlmostEqual(self.conv_block.weight.data.std(), 1, delta=1e-1)\n\n    def tearDown(self):\n        # delete the computational resources\n        del self.conv_block\n\n\nclass Test_equalized_deconv2d(TestCase):\n\n    def setUp(self):\n        self.deconv_block = cL._equalized_deconv2d(21, 3, k_size=(3, 3), pad=1)\n\n        # print the Equalized conv block\n        print(""\\nEqualized conv block:\\n%s"" % str(self.deconv_block))\n\n    def test_forward(self):\n        mock_in = th.randn(32, 21, 16, 16).to(device)\n        mock_out = self.deconv_block(mock_in)\n\n        # check output\n        self.assertEqual(mock_out.shape, (32, 3, 16, 16))\n        self.assertEqual(th.isnan(mock_out).sum().item(), 0)\n        self.assertEqual(th.isinf(mock_out).sum().item(), 0)\n\n        # check the weight\'s scale\n        self.assertAlmostEqual(self.deconv_block.weight.data.std(), 1, delta=1e-1)\n\n    def tearDown(self):\n        # delete the computational resources\n        del self.deconv_block\n\n\nclass Test_equalized_linear(TestCase):\n\n    def setUp(self):\n        self.lin_block = cL._equalized_linear(13, 52)\n\n        # print the Equalized conv block\n        print(""\\nEqualized linear block:\\n%s"" % str(self.lin_block))\n\n    def test_forward(self):\n        # test the forward for the first res block\n        mock_in = th.randn(32, 13).to(device)\n        mock_out = self.lin_block(mock_in)\n\n        # check output\n        self.assertEqual(mock_out.shape, (32, 52))\n        self.assertEqual(th.isnan(mock_out).sum().item(), 0)\n        self.assertEqual(th.isinf(mock_out).sum().item(), 0)\n\n        # check the weight\'s scale\n        self.assertAlmostEqual(self.lin_block.weight.data.std(), 1, delta=1e-1)\n\n    def tearDown(self):\n        # delete the computational resources\n        del self.lin_block\n\n\nclass Test_PixelwiseNorm(TestCase):\n\n    def setUp(self):\n        self.normalizer = cL.PixelwiseNorm()\n\n    def test_forward(self):\n        mock_in = th.randn(1, 13, 1, 1).to(device)\n        mock_out = self.normalizer(mock_in)\n\n        # check output\n        self.assertEqual(mock_out.shape, mock_in.shape)\n        self.assertEqual(th.isnan(mock_out).sum().item(), 0)\n        self.assertEqual(th.isinf(mock_out).sum().item(), 0)\n\n        # we cannot comment that the norm of the output tensor\n        # will always be less than the norm of the input tensor\n        # so no more checking can be done\n\n    def tearDown(self):\n        # delete the computational resources\n        del self.normalizer\n\n\nclass Test_MinibatchStdDev(TestCase):\n\n    def setUp(self):\n        self.minStdD = cL.MinibatchStdDev()\n\n    def test_forward(self):\n        mock_in = th.randn(1, 13, 16, 16).to(device)\n        mock_out = self.minStdD(mock_in)\n\n        # check output\n        self.assertEqual(mock_out.shape[1], mock_in.shape[1] + 1)\n        self.assertEqual(th.isnan(mock_out).sum().item(), 0)\n        self.assertEqual(th.isinf(mock_out).sum().item(), 0)\n\n    def tearDown(self):\n        # delete the computational resources\n        del self.minStdD\n'"
