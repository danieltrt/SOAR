file_path,api_count,code
setup.py,0,"b'import setuptools\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=""detecto"",\n    version=""1.1.4"",\n    author=""Alan Bi"",\n    author_email=""alan.bi326@gmail.com"",\n    description=""Build fully-functioning computer vision models with 5 lines of code"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/alankbi/detecto"",\n    packages=setuptools.find_packages(),\n    install_requires=[\n        \'matplotlib\',\n        \'opencv-python\',\n        \'pandas\',\n        \'torch\',\n        \'torchvision\',\n    ],\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n    python_requires=\'>=3.6\',\n)\n'"
detecto/__init__.py,0,b''
detecto/config.py,1,"b""import torch\n\nconfig = {\n    'default_device': torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),\n    'default_classes': [\n        'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n        'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n        'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n        'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n        'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n        'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n        'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n        'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n        'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n        'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n    ]\n}\n"""
detecto/core.py,34,"b'import os\nimport pandas as pd\nimport random\nimport torch\nimport torchvision\n\nfrom detecto.config import config\nfrom detecto.utils import default_transforms, filter_top_predictions, xml_to_csv, _is_iterable, read_image\nfrom torchvision import transforms\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n\nclass DataLoader(torch.utils.data.DataLoader):\n\n    def __init__(self, dataset, **kwargs):\n        """"""Accepts a :class:`detecto.core.Dataset` object and creates\n        an iterable over the data, which can then be fed into a\n        :class:`detecto.core.Model` for training and validation.\n        Extends PyTorch\'s `DataLoader\n        <https://pytorch.org/docs/stable/data.html>`_ class with a custom\n        ``collate_fn`` function.\n\n        :param dataset: The dataset for iteration over.\n        :type dataset: detecto.core.Dataset\n        :param kwargs: (Optional) Additional arguments to customize the\n            DataLoader, such as ``batch_size`` or ``shuffle``. See `docs\n            <https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader>`_\n            for more details.\n        :type kwargs: Any\n\n        **Example**::\n\n            >>> from detecto.core import Dataset, DataLoader\n\n            >>> dataset = Dataset(\'labels.csv\', \'images/\')\n            >>> loader = DataLoader(dataset, batch_size=2, shuffle=True)\n            >>> for images, targets in loader:\n            >>>     print(images[0].shape)\n            >>>     print(targets[0])\n            torch.Size([3, 1080, 1720])\n            {\'boxes\': tensor([[884, 387, 937, 784]]), \'labels\': \'person\'}\n            torch.Size([3, 1080, 1720])\n            {\'boxes\': tensor([[   1,  410, 1657, 1079]]), \'labels\': \'car\'}\n            ...\n        """"""\n\n        super().__init__(dataset, collate_fn=DataLoader.collate_data, **kwargs)\n\n    # Converts a list of tuples into a tuple of lists so that\n    # it can properly be fed to the model for training\n    @staticmethod\n    def collate_data(batch):\n        images, targets = zip(*batch)\n        return list(images), list(targets)\n\n\nclass Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, label_data, image_folder=None, transform=None):\n        """"""Takes in the path to the label data and images and creates\n        an indexable dataset over all of the data. Applies optional\n        transforms over the data. Extends PyTorch\'s `Dataset\n        <https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset>`_.\n\n        :param label_data: Can either contain the path to a folder storing\n            the XML label files or a CSV file containing the label data.\n            If a CSV file, the file should have the following columns in\n            order: ``filename``, ``width``, ``height``, ``class``, ``xmin``,\n            ``ymin``, ``xmax``, and ``ymax``. See\n            :func:`detecto.utils.xml_to_csv` to generate CSV files in this\n            format from XML label files.\n        :type label_data: str\n        :param image_folder: (Optional) The path to the folder containing the\n            images. If not specified, it is assumed that the images and XML\n            files are in the same directory as given by `label_data`. Defaults\n            to None.\n        :type image_folder: str\n        :param transform: (Optional) A torchvision `transforms.Compose\n            <https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Compose>`__\n            object containing transformations to apply on all elements in\n            the dataset. See `PyTorch docs\n            <https://pytorch.org/docs/stable/torchvision/transforms.html>`_\n            for a list of possible transforms. When using transforms.Resize\n            and transforms.RandomHorizontalFlip, all box coordinates are\n            automatically adjusted to match the modified image. If None,\n            defaults to the transforms returned by\n            :func:`detecto.utils.default_transforms`.\n        :type transform: torchvision.transforms.Compose or None\n\n        **Indexing**:\n\n        A Dataset object can be indexed like any other Python iterable.\n        Doing so returns a tuple of length 2. The first element is the\n        image and the second element is a dict containing a \'boxes\' and\n        \'labels\' key. ``dict[\'boxes\']`` is a torch.Tensor of size\n        ``(1, 4)`` containing ``xmin``, ``ymin``, ``xmax``, and ``ymax``\n        of the box and ``dict[\'labels\']`` is the string label of the\n        detected object.\n\n        **Example**::\n\n            >>> from detecto.core import Dataset\n\n            >>> # Create dataset from separate XML and image folders\n            >>> dataset = Dataset(\'xml_labels/\', \'images/\')\n            >>> # Create dataset from a combined XML and image folder\n            >>> dataset1 = Dataset(\'images_and_labels/\')\n            >>> # Create dataset from a CSV file and image folder\n            >>> dataset2 = Dataset(\'labels.csv\', \'images/\')\n\n            >>> print(len(dataset))\n            >>> image, target = dataset[0]\n            >>> print(image.shape)\n            >>> print(target)\n            4\n            torch.Size([3, 720, 1280])\n            {\'boxes\': tensor([[564, 43, 736, 349]]), \'labels\': \'balloon\'}\n        """"""\n\n        # CSV file contains: filename, width, height, class, xmin, ymin, xmax, ymax\n        if os.path.isfile(label_data):\n            self._csv = pd.read_csv(label_data)\n        else:\n            self._csv = xml_to_csv(label_data)\n\n        # If image folder not given, set it to labels folder\n        if image_folder is None:\n            self._root_dir = label_data\n        else:\n            self._root_dir = image_folder\n\n        if transform is None:\n            self.transform = default_transforms()\n        else:\n            self.transform = transform\n\n    # Returns the length of this dataset\n    def __len__(self):\n        return len(self._csv)\n\n    # Is what allows you to index the dataset, e.g. dataset[0]\n    # dataset[index] returns a tuple containing the image and the targets dict\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        # Read in the image from the file name in the 0th column\n        img_name = os.path.join(self._root_dir, self._csv.iloc[idx, 0])\n        image = read_image(img_name)\n\n        # Read in xmin, ymin, xmax, and ymax\n        box = self._csv.iloc[idx, 4:]\n        box = torch.tensor(box).view(1, 4)\n\n        # Read in the label\n        label = self._csv.iloc[idx, 3]\n\n        targets = {\'boxes\': box, \'labels\': label}\n\n        # Perform transformations\n        if self.transform:\n            width = self._csv.loc[idx, \'width\']\n            height = self._csv.loc[idx, \'height\']\n\n            # Apply the transforms manually to be able to deal with\n            # transforms like Resize or RandomHorizontalFlip\n            updated_transforms = []\n            scale_factor = 1.0\n            random_flip = 0.0\n            for t in self.transform.transforms:\n                # Add each transformation to our list\n                updated_transforms.append(t)\n\n                # If a resize transformation exists, scale down the coordinates\n                # of the box by the same amount as the resize\n                if isinstance(t, transforms.Resize):\n                    original_size = min(height, width)\n                    scale_factor = original_size / t.size\n\n                # If a horizontal flip transformation exists, get its probability\n                # so we can apply it manually to both the image and the boxes.\n                elif isinstance(t, transforms.RandomHorizontalFlip):\n                    random_flip = t.p\n\n            # Apply each transformation manually\n            for t in updated_transforms:\n                # Handle the horizontal flip case, where we need to apply\n                # the transformation to both the image and the box labels\n                if isinstance(t, transforms.RandomHorizontalFlip):\n                    if random.random() < random_flip:\n                        image = transforms.RandomHorizontalFlip(1)(image)\n                        # Flip box\'s x-coordinates\n                        box[0, 0] = width - box[0, 0]\n                        box[0, 2] = width - box[0, 2]\n                        box[0, 0], box[0, 2] = box[0, (2, 0)]\n                else:\n                    image = t(image)\n\n            # Scale down box if necessary\n            targets[\'boxes\'] = (box / scale_factor).long()\n\n        return image, targets\n\n\nclass Model:\n\n    def __init__(self, classes=None, device=None):\n        """"""Initializes a machine learning model for object detection.\n        Models are built on top of PyTorch\'s `pre-trained models\n        <https://pytorch.org/docs/stable/torchvision/models.html>`_,\n        specifically the Faster R-CNN ResNet-50 FPN, but allow for\n        fine-tuning to predict on custom classes/labels.\n\n        :param classes: (Optional) A list of classes/labels for the model\n            to predict. If none given, uses the default classes specified\n            `here <https://pytorch.org/docs/stable/torchvision/models.html\n            #object-detection-instance-segmentation-and-person-keypoint-detection>`_.\n            Defaults to None.\n        :type classes: list or None\n        :param device: (Optional) The device on which to run the model,\n            such as the CPU or GPU. See `here\n            <https://pytorch.org/docs/stable/tensor_attributes.html#torch-device>`_\n            for details on specifying the device. Defaults to the GPU if\n            available and the CPU if not.\n        :type device: torch.device or None\n\n        **Example**::\n\n            >>> from detecto.core import Model\n\n            >>> model = Model([\'dog\', \'cat\', \'bunny\'])\n        """"""\n\n        self._device = device if device else config[\'default_device\']\n\n        # Load a model pre-trained on COCO\n        self._model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n        if classes:\n            # Get the number of input features for the classifier\n            in_features = self._model.roi_heads.box_predictor.cls_score.in_features\n            # Replace the pre-trained head with a new one (note: +1 because of the __background__ class)\n            self._model.roi_heads.box_predictor = FastRCNNPredictor(in_features, len(classes) + 1)\n            self._disable_normalize = False\n        else:\n            classes = config[\'default_classes\']\n            self._disable_normalize = True\n\n        self._model.to(self._device)\n\n        # Mappings to convert from string labels to ints and vice versa\n        self._classes = [\'__background__\'] + classes\n        self._int_mapping = {label: index for index, label in enumerate(self._classes)}\n\n    # Returns the raw predictions from feeding an image or list of images into the model\n    def _get_raw_predictions(self, images):\n        self._model.eval()\n\n        with torch.no_grad():\n            # Convert image into a list of length 1 if not already a list\n            if not _is_iterable(images):\n                images = [images]\n\n            # Convert to tensor and normalize if not already\n            if not isinstance(images[0], torch.Tensor):\n                # This is a temporary workaround to the bad accuracy\n                # when normalizing on default weights. Will need to\n                # investigate further TODO\n                if self._disable_normalize:\n                    defaults = transforms.Compose([transforms.ToTensor()])\n                else:\n                    defaults = default_transforms()\n                images = [defaults(img) for img in images]\n\n            # Send images to the specified device\n            images = [img.to(self._device) for img in images]\n\n            preds = self._model(images)\n            # Send predictions to CPU if not already\n            preds = [{k: v.to(torch.device(\'cpu\')) for k, v in p.items()} for p in preds]\n            return preds\n\n    def predict(self, images):\n        """"""Takes in an image or list of images and returns predictions\n        for object locations.\n\n        :param images: An image or list of images to predict on. If the\n            images have not already been transformed into torch.Tensor\n            objects, the default transformations contained in\n            :func:`detecto.utils.default_transforms` will be applied.\n        :type images: list or numpy.ndarray or torch.Tensor\n        :return: If given a single image, returns a tuple of size\n            three. The first element is a list of string labels of size N,\n            the number of detected objects. The second element is a\n            torch.Tensor of size (N, 4), giving the ``xmin``, ``ymin``,\n            ``xmax``, and ``ymax`` coordinates of the boxes around each\n            object. The third element is a torch.Tensor of size N containing\n            the scores of each predicted object (ranges from 0.0 to 1.0). If\n            given a list of images, returns a list of the tuples described\n            above, each tuple corresponding to a single image.\n        :rtype: tuple or list of tuple\n\n        **Example**::\n\n            >>> from detecto.core import Model\n            >>> from detecto.utils import read_image\n\n            >>> model = Model.load(\'model_weights.pth\', [\'horse\', \'zebra\'])\n            >>> image = read_image(\'image.jpg\')\n            >>> labels, boxes, scores = model.predict(image)\n            >>> print(labels[0])\n            >>> print(boxes[0])\n            >>> print(scores[0])\n            horse\n            tensor([   0.0000,  428.0744, 1617.1860, 1076.3607])\n            tensor(0.9397)\n        """"""\n\n        # Convert all to lists but keep track if a single image was given\n        is_single_image = not _is_iterable(images)\n        images = [images] if is_single_image else images\n        preds = self._get_raw_predictions(images)\n\n        results = []\n        for pred in preds:\n            # Convert predicted ints into their corresponding string labels\n            result = ([self._classes[val] for val in pred[\'labels\']], pred[\'boxes\'], pred[\'scores\'])\n            results.append(result)\n\n        return results[0] if is_single_image else results\n\n    def predict_top(self, images):\n        """"""Takes in an image or list of images and returns the top\n        scoring predictions for each detected label in each image.\n        Equivalent to running :meth:`detecto.core.Model.predict` and\n        then :func:`detecto.utils.filter_top_predictions` together.\n\n        :param images: An image or list of images to predict on. If the\n            images have not already been transformed into torch.Tensor\n            objects, the default transformations contained in\n            :func:`detecto.utils.default_transforms` will be applied.\n        :type images: list or numpy.ndarray or torch.Tensor\n        :return: If given a single image, returns a tuple of size\n            three. The first element is a list of string labels of size K,\n            the number of uniquely detected objects. The second element is\n            a torch.Tensor of size (K, 4), giving the ``xmin``, ``ymin``,\n            ``xmax``, and ``ymax`` coordinates of the top-scoring boxes\n            around each unique object. The third element is a torch.Tensor\n            of size K containing the scores of each uniquely predicted object\n            (ranges from 0.0 to 1.0). If given a list of images, returns a\n            list of the tuples described above, each tuple corresponding to\n            a single image.\n        :rtype: tuple or list of tuple\n\n\n        **Example**::\n\n            >>> from detecto.core import Model\n            >>> from detecto.utils import read_image\n\n            >>> model = Model.load(\'model_weights.pth\', [\'label1\', \'label2\'])\n            >>> image = read_image(\'image.jpg\')\n            >>> top_preds = model.predict_top(image)\n            >>> top_preds\n            ([\'label2\', \'label1\'], tensor([[   0.0000,  428.0744, 1617.1860, 1076.3607],\n            [ 875.3470,  412.1762,  949.5915,  793.3424]]), tensor([0.9397, 0.8686]))\n        """"""\n\n        predictions = self.predict(images)\n\n        # If tuple but not list, then images is a single image\n        if not isinstance(predictions, list):\n            return filter_top_predictions(*predictions)\n\n        results = []\n        for pred in predictions:\n            results.append(filter_top_predictions(*pred))\n\n        return results\n\n    def fit(self, dataset, val_dataset=None, epochs=10, learning_rate=0.005, momentum=0.9,\n            weight_decay=0.0005, gamma=0.1, lr_step_size=3, verbose=False):\n        """"""Train the model on the given dataset. If given a validation\n        dataset, returns a list of loss scores at each epoch.\n\n        :param dataset: A Dataset or DataLoader containing the dataset\n            to train on. If given a Dataset, this method automatically\n            wraps it in a DataLoader with `shuffle` set to `True`.\n        :type dataset: detecto.core.Dataset or detecto.core.DataLoader\n        :param val_dataset: (Optional) A Dataset or DataLoader containing\n            the dataset to validate on. Defaults to None, in which case no\n            validation occurs.\n        :type val_dataset: detecto.core.Dataset or detecto.core.DataLoader\n        :param epochs: (Optional) The number of runs over the data in\n            ``dataset`` to train for. Defaults to 10.\n        :type epochs: int\n        :param learning_rate: (Optional) How fast to update the model\n            weights at each step of training. Defaults to 0.005.\n        :type learning_rate: float\n        :param momentum: (Optional) The momentum used to reduce the\n            fluctuations of gradients at each step. Defaults to 0.9.\n        :type momentum: float\n        :param weight_decay: (Optional) The amount of L2 regularization\n            to apply on model parameters. Defaults to 0.0005.\n        :type weight_decay: float\n        :param gamma: (Optional) The decay factor that ``learning_rate``\n            is multiplied by every ``lr_step_size`` epochs. Defaults to 0.1.\n        :type gamma: float\n        :param lr_step_size: (Optional) The number of epochs between each\n            decay of ``learning_rate`` by ``gamma``. Defaults to 3.\n        :type lr_step_size: int\n        :param verbose: (Optional) Whether to print the current epoch and\n             loss (if given a validation dataset) at each step. Defaults\n             to False.\n        :type verbose: bool\n        :return: If ``val_dataset`` is not None and epochs is greater than 0,\n            returns a list of the validation losses at each epoch. Otherwise,\n            returns nothing.\n        :rtype: list or None\n\n        **Example**::\n\n            >>> from detecto.core import Model, Dataset, DataLoader\n\n            >>> dataset = Dataset(\'training_data/\')\n            >>> val_dataset = Dataset(\'validation_data/\')\n            >>> model = Model([\'rose\', \'tulip\'])\n\n            >>> losses = model.fit(dataset, val_dataset, epochs=5)\n\n            >>> # Alternatively, provide a custom DataLoader over your dataset\n            >>> loader = DataLoader(dataset, batch_size=2, shuffle=True)\n            >>> losses = model.fit(loader, val_dataset, epochs=5)\n\n            >>> losses\n            [0.11191498369799327, 0.09899920264606253, 0.08454859235434461,\n                0.06825731012780788, 0.06236840748117637]\n        """"""\n\n        # If doing custom training, the given images will most likely be\n        # normalized. This should fix the issue of poor performance on\n        # default classes when normalizing, so resume normalizing. TODO\n        if epochs > 0:\n            self._disable_normalize = False\n\n        # Convert dataset to data loader if not already\n        if not isinstance(dataset, DataLoader):\n            dataset = DataLoader(dataset, shuffle=True)\n\n        if val_dataset is not None and not isinstance(val_dataset, DataLoader):\n            val_dataset = DataLoader(val_dataset)\n\n        losses = []\n        # Get parameters that have grad turned on (i.e. parameters that should be trained)\n        parameters = [p for p in self._model.parameters() if p.requires_grad]\n        # Create an optimizer that uses SGD (stochastic gradient descent) to train the parameters\n        optimizer = torch.optim.SGD(parameters, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n        # Create a learning rate scheduler that decreases learning rate by gamma every lr_step_size epochs\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=gamma)\n\n        # Train on the entire dataset for the specified number of times (epochs)\n        for epoch in range(epochs):\n            if verbose:\n                print(\'Epoch {} of {}\'.format(epoch + 1, epochs))\n\n            # Training step\n            self._model.train()\n            for images, targets in dataset:\n                self._convert_to_int_labels(targets)\n                images, targets = self._to_device(images, targets)\n\n                # Calculate the model\'s loss (i.e. how well it does on the current\n                # image and target, with a lower loss being better)\n                loss_dict = self._model(images, targets)\n                total_loss = sum(loss for loss in loss_dict.values())\n\n                # Zero any old/existing gradients on the model\'s parameters\n                optimizer.zero_grad()\n                # Compute gradients for each parameter based on the current loss calculation\n                total_loss.backward()\n                # Update model parameters from gradients: param -= learning_rate * param.grad\n                optimizer.step()\n\n            # Validation step\n            if val_dataset is not None:\n                avg_loss = 0\n                with torch.no_grad():\n                    for images, targets in val_dataset:\n                        self._convert_to_int_labels(targets)\n                        images, targets = self._to_device(images, targets)\n                        loss_dict = self._model(images, targets)\n                        total_loss = sum(loss for loss in loss_dict.values())\n                        avg_loss += total_loss.item()\n\n                avg_loss /= len(val_dataset.dataset)\n                losses.append(avg_loss)\n\n                if verbose:\n                    print(\'Loss: {}\'.format(avg_loss))\n\n            # Update the learning rate every few epochs\n            lr_scheduler.step()\n\n        if len(losses) > 0:\n            return losses\n\n    def get_internal_model(self):\n        """"""Returns the internal torchvision model that this class contains\n        to allow for more advanced fine-tuning and the full use of\n        features presented in the PyTorch library.\n\n        :return: The torchvision model, which is a Faster R-CNN ResNet-50\n            FPN with a FastRCNNPredictor box predictor.\n        :rtype: torchvision.models.detection.faster_rcnn.FasterRCNN\n\n        **Example**::\n\n            >>> from detecto.core import Model\n\n            >>> model = Model.load(\'model_weights.pth\', [\'tick\', \'gate\'])\n            >>> torch_model = model.get_internal_model()\n            >>> type(torch_model)\n            <class \'torchvision.models.detection.faster_rcnn.FasterRCNN\'>\n        """"""\n\n        return self._model\n\n    def save(self, file):\n        """"""Saves the internal model weights to a file.\n\n        :param file: The name of the file. Should have a .pth file extension.\n        :type file: str\n\n        **Example**::\n\n            >>> from detecto.core import Model\n\n            >>> model = Model([\'tree\', \'bush\', \'leaf\'])\n            >>> model.save(\'model_weights.pth\')\n        """"""\n\n        torch.save(self._model.state_dict(), file)\n\n    @staticmethod\n    def load(file, classes):\n        """"""Loads a model from a .pth file containing the model weights.\n\n        :param file: The path to the .pth file containing the saved model.\n        :type file: str\n        :param classes: The list of classes/labels this model was trained\n            to predict. Must be in the same order as initially passed to\n            :meth:`detecto.core.Model.__init__` for accurate results.\n        :type classes: list\n        :return: The model loaded from the file.\n        :rtype: detecto.core.Model\n\n        **Example**::\n\n            >>> from detecto.core import Model\n\n            >>> model = Model.load(\'model_weights.pth\', [\'ant\', \'bee\'])\n        """"""\n\n        model = Model(classes)\n        model._model.load_state_dict(torch.load(file, map_location=model._device))\n        return model\n\n    # Converts all string labels in a list of target dicts to\n    # their corresponding int mappings\n    def _convert_to_int_labels(self, targets):\n        for target in targets:\n            # Convert string labels to integer mapping\n            target[\'labels\'] = torch.tensor(self._int_mapping[target[\'labels\']]).view(1)\n\n    # Sends all images and targets to the same device as the model\n    def _to_device(self, images, targets):\n        images = [image.to(self._device) for image in images]\n        targets = [{k: v.to(self._device) for k, v in t.items()} for t in targets]\n        return images, targets\n'"
detecto/utils.py,9,"b'import cv2\nimport os\nimport pandas as pd\nimport torch\nimport xml.etree.ElementTree as ET\n\nfrom glob import glob\nfrom torchvision import transforms\n\n\ndef default_transforms():\n    """"""Returns the default, bare-minimum transformations that should be\n    applied to images passed to classes in the :mod:`detecto.core` module.\n\n    :return: A torchvision `transforms.Compose\n        <https://pytorch.org/docs/stable/torchvision/transforms.html>`_\n        object containing a transforms.ToTensor object and the\n        transforms.Normalize object returned by\n        :func:`detecto.utils.normalize_transform`.\n    :rtype: torchvision.transforms.Compose\n\n    **Example**::\n\n        >>> from detecto.core import Dataset\n        >>> from detecto.utils import default_transforms\n\n        >>> # Note: if transform=None, the Dataset will automatically\n        >>> # apply these default transforms to images\n        >>> defaults = default_transforms()\n        >>> dataset = Dataset(\'labels.csv\', \'images/\', transform=defaults)\n    """"""\n\n    return transforms.Compose([transforms.ToTensor(), normalize_transform()])\n\n\ndef filter_top_predictions(labels, boxes, scores):\n    """"""Filters out the top scoring predictions of each class from the\n    given data. Note: passing the predictions from\n    :meth:`detecto.core.Model.predict` to this function produces the same\n    results as a direct call to :meth:`detecto.core.Model.predict_top`.\n\n    :param labels: A list containing the string labels.\n    :type labels: list\n    :param boxes: A tensor of size [N, 4] containing the N box coordinates.\n    :type boxes: torch.Tensor\n    :param scores: A tensor containing the score for each prediction.\n    :type scores: torch.Tensor\n    :return: Returns a tuple of the given labels, boxes, and scores, except\n        with only the top scoring prediction of each unique label kept in;\n        all other predictions are filtered out.\n    :rtype: tuple\n\n    **Example**::\n\n        >>> from detecto.core import Model\n        >>> from detecto.utils import read_image, filter_top_predictions\n\n        >>> model = Model.load(\'model_weights.pth\', [\'label1\', \'label2\'])\n        >>> image = read_image(\'image.jpg\')\n        >>> labels, boxes, scores = model.predict(image)\n        >>> top_preds = filter_top_predictions(labels, boxes, scores)\n        >>> top_preds\n        ([\'label2\', \'label1\'], tensor([[   0.0000,  428.0744, 1617.1860, 1076.3607],\n        [ 875.3470,  412.1762,  949.5915,  793.3424]]), tensor([0.9397, 0.8686]))\n    """"""\n\n    filtered_labels = []\n    filtered_boxes = []\n    filtered_scores = []\n    # Loop through each unique label\n    for label in set(labels):\n        # Get first index of label, which is also its highest scoring occurrence\n        index = labels.index(label)\n\n        filtered_labels.append(label)\n        filtered_boxes.append(boxes[index])\n        filtered_scores.append(scores[index])\n\n    if len(filtered_labels) == 0:\n        return filtered_labels, torch.empty(0, 4), torch.tensor(filtered_scores)\n    return filtered_labels, torch.stack(filtered_boxes), torch.tensor(filtered_scores)\n\n\ndef normalize_transform():\n    """"""Returns a torchvision `transforms.Normalize\n    <https://pytorch.org/docs/stable/torchvision/transforms.html>`_ object\n    with default mean and standard deviation values as required by PyTorch\'s\n    pre-trained models.\n\n    :return: A transforms.Normalize object with pre-computed values.\n    :rtype: torchvision.transforms.Normalize\n\n    **Example**::\n\n        >>> from detecto.core import Dataset\n        >>> from detecto.utils import normalize_transform\n        >>> from torchvision import transforms\n\n        >>> # Note: if transform=None, the Dataset will automatically\n        >>> # apply these default transforms to images\n        >>> defaults = transforms.Compose([\n        >>>     transforms.ToTensor(),\n        >>>     normalize_transform(),\n        >>> ])\n        >>> dataset = Dataset(\'labels.csv\', \'images/\', transform=defaults)\n    """"""\n\n    # Default for PyTorch\'s pre-trained models\n    return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n\ndef read_image(path):\n    """"""Helper function that reads in an image as a\n    `NumPy <https://numpy.org/>`_ array. Equivalent to using\n    `OpenCV <https://docs.opencv.org/master/>`_\'s cv2.imread\n    function and converting from BGR to RGB format.\n\n    :param path: The path to the image.\n    :type path: str\n    :return: Image in NumPy array format\n    :rtype: ndarray\n\n    **Example**::\n\n        >>> import matplotlib.pyplot as plt\n        >>> from detecto.utils import read_image\n\n        >>> image = read_image(\'image.jpg\')\n        >>> plt.imshow(image)\n        >>> plt.show()\n    """"""\n\n    image = cv2.imread(path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n\ndef reverse_normalize(image):\n    """"""Reverses the normalization applied on an image by the\n    :func:`detecto.utils.reverse_normalize` transformation. The image\n    must be a `torch.Tensor <https://pytorch.org/docs/stable/tensors.html>`_\n    object.\n\n    :param image: A normalized image.\n    :type image: torch.Tensor\n    :return: The image with the normalization undone.\n    :rtype: torch.Tensor\n\n\n    **Example**::\n\n        >>> import matplotlib.pyplot as plt\n        >>> from torchvision import transforms\n        >>> from detecto.utils import read_image, \\\\\n        >>>     default_transforms, reverse_normalize\n\n        >>> image = read_image(\'image.jpg\')\n        >>> defaults = default_transforms()\n        >>> image = defaults(image)\n\n        >>> image = reverse_normalize(image)\n        >>> image = transforms.ToPILImage()(image)\n        >>> plt.imshow(image)\n        >>> plt.show()\n    """"""\n\n    reverse = transforms.Normalize(mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.255],\n                                   std=[1 / 0.229, 1 / 0.224, 1 / 0.255])\n    return reverse(image)\n\n\ndef split_video(video_file, output_folder, prefix=\'frame\', step_size=1):\n    """"""Splits a video into individual frames and saves the JPG images to the\n    specified output folder.\n\n    :param video_file: The path to the video file to split.\n    :type video_file: str\n    :param output_folder: The directory in which to save the frames.\n    :type output_folder: str\n    :param prefix: (Optional) The prefix to each frame\'s file name. For\n        example, if prefix == \'image\', each frame will be saved as\n        image0.jpg, image1.jpg, etc. Defaults to \'frame\'.\n    :type prefix: str\n    :param step_size: (Optional) How many frames to skip between each save.\n        For example, if step_size == 3, it will save every third frame.\n        Defaults to 1.\n    :type step_size: int\n\n    **Example**::\n\n        >>> from detecto.utils import split_video\n\n        >>> split_video(\'video.mp4\', \'frames/\', step_size=4)\n    """"""\n\n    # Set step_size to minimum of 1\n    if step_size <= 0:\n        print(\'Invalid step_size for split_video; defaulting to 1\')\n        step_size = 1\n\n    video = cv2.VideoCapture(video_file)\n\n    count = 0\n    index = 0\n    # Loop through video frame by frame\n    while True:\n        ret, frame = video.read()\n        if not ret:\n            break\n\n        # Save every step_size frames\n        if count % step_size == 0:\n            file_name = \'{}{}.jpg\'.format(prefix, index)\n            cv2.imwrite(os.path.join(output_folder, file_name), frame)\n            index += 1\n\n        count += 1\n\n    video.release()\n    cv2.destroyAllWindows()\n\n\ndef xml_to_csv(xml_folder, output_file=None):\n    """"""Converts a folder of XML label files into a pandas DataFrame and/or\n    CSV file, which can then be used to create a :class:`detecto.core.Dataset`\n    object. Each XML file should correspond to an image and contain the image\n    name, image size, and the names and bounding boxes of the objects in the\n    image, if any. Extraneous data in the XML files will simply be ignored.\n    See :download:`here <../_static/example.xml>` for an example XML file.\n    For an image labeling tool that produces XML files in this format,\n    see `LabelImg <https://github.com/tzutalin/labelImg>`_.\n\n    :param xml_folder: The path to the folder containing the XML files.\n    :type xml_folder: str\n    :param output_file: (Optional) If given, saves a CSV file containing\n        the XML data in the file output_file. If None, does not save to\n        any file. Defaults to None.\n    :type output_file: str or None\n    :return: A pandas DataFrame containing the XML data.\n    :rtype: pandas.DataFrame\n\n    **Example**::\n\n        >>> from detecto.utils import xml_to_csv\n\n        >>> # Saves data to a file called labels.csv\n        >>> xml_to_csv(\'xml_labels/\', \'labels.csv\')\n        >>> # Returns a pandas DataFrame of the data\n        >>> df = xml_to_csv(\'xml_labels/\')\n    """"""\n\n    xml_list = []\n    # Loop through every XML file\n    for xml_file in glob(xml_folder + \'/*.xml\'):\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n\n        filename = root.find(\'filename\').text\n        size = root.find(\'size\')\n        width = int(size.find(\'width\').text)\n        height = int(size.find(\'height\').text)\n\n        # Each object represents each actual image label\n        for member in root.findall(\'object\'):\n            box = member.find(\'bndbox\')\n            label = member.find(\'name\').text\n\n            # Add image file name, image size, label, and box coordinates to CSV file\n            row = (filename, width, height, label, int(box[0].text),\n                   int(box[1].text), int(box[2].text), int(box[3].text))\n            xml_list.append(row)\n\n    # Save as a CSV file\n    column_names = [\'filename\', \'width\', \'height\', \'class\', \'xmin\', \'ymin\', \'xmax\', \'ymax\']\n    xml_df = pd.DataFrame(xml_list, columns=column_names)\n\n    if output_file is not None:\n        xml_df.to_csv(output_file, index=None)\n\n    return xml_df\n\n\n# Checks whether a variable is a list or tuple only\ndef _is_iterable(variable):\n    return isinstance(variable, list) or isinstance(variable, tuple)\n'"
detecto/visualize.py,7,"b'import cv2\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom detecto.utils import reverse_normalize, normalize_transform, _is_iterable\nfrom torchvision import transforms\n\n\ndef detect_live(model, score_filter=0.6):\n    """"""Displays in a window the given model\'s predictions on the current\n    computer\'s live webcam feed. To stop the webcam, press \'q\' or the ESC\n    key. Note that if the given model is not running on a GPU, the webcam\n    framerate could very well be under 1 FPS. Also note that you should not\n    call this function on Google Colab or other services running on virtual\n    machines as they may not have access to the webcam.\n\n    :param model: The trained model with which to run object detection.\n    :type model: detecto.core.Model\n    :param score_filter: (Optional) Minimum score required to show a\n        prediction. Defaults to 0.6.\n    :type score_filter: float\n\n    **Example**::\n\n        >>> from detecto.core import Model\n        >>> from detecto.visualize import detect_live\n\n        >>> model = Model()\n        >>> detect_live(model, score_filter=0.7)\n    """"""\n\n    cv2.namedWindow(\'Detecto\')\n    try:\n        video = cv2.VideoCapture(0)\n    except:\n        print(\'No webcam available.\')\n        return\n\n    while True:\n        ret, frame = video.read()\n        if not ret:\n            break\n\n        labels, boxes, scores = model.predict(frame)\n\n        # Plot each box with its label and score\n        for i in range(boxes.shape[0]):\n            if scores[i] < score_filter:\n                continue\n\n            box = boxes[i]\n            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 3)\n            if labels:\n                cv2.putText(frame, \'{}: {}\'.format(labels[i], round(scores[i].item(), 2)), (box[0], box[1] - 10),\n                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 3)\n\n        cv2.imshow(\'Detecto\', frame)\n\n        # If the \'q\' or ESC key is pressed, break from the loop\n        key = cv2.waitKey(1) & 0xFF\n        if key == ord(\'q\') or key == 27:\n            break\n\n    cv2.destroyWindow(\'Detecto\')\n    video.release()\n\n\ndef detect_video(model, input_file, output_file, fps=30, score_filter=0.6):\n    """"""Takes in a video and produces an output video with object detection\n    run on it (i.e. displays boxes around detected objects in real-time).\n    Output videos should have the .avi file extension. Note: some apps,\n    such as macOS\'s QuickTime Player, have difficulty viewing these\n    output videos. It\'s recommended that you download and use\n    `VLC <https://www.videolan.org/vlc/index.html>`_ if this occurs.\n\n\n    :param model: The trained model with which to run object detection.\n    :type model: detecto.core.Model\n    :param input_file: The path to the input video.\n    :type input_file: str\n    :param output_file: The name of the output file. Should have a .avi\n        file extension.\n    :type output_file: str\n    :param fps: (Optional) Frames per second of the output video.\n        Defaults to 30.\n    :type fps: int\n    :param score_filter: (Optional) Minimum score required to show a\n        prediction. Defaults to 0.6.\n    :type score_filter: float\n\n    **Example**::\n\n        >>> from detecto.core import Model\n        >>> from detecto.visualize import detect_video\n\n        >>> model = Model.load(\'model_weights.pth\', [\'tick\', \'gate\'])\n        >>> detect_video(model, \'input_vid.mp4\', \'output_vid.avi\', score_filter=0.7)\n    """"""\n\n    # Read in the video\n    video = cv2.VideoCapture(input_file)\n\n    # Video frame dimensions\n    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Scale down frames when passing into model for faster speeds\n    scaled_size = 800\n    scale_down_factor = min(frame_height, frame_width) / scaled_size\n\n    # The VideoWriter with which we\'ll write our video with the boxes and labels\n    # Parameters: filename, fourcc, fps, frame_size\n    out = cv2.VideoWriter(output_file, cv2.VideoWriter_fourcc(*\'DIVX\'), fps, (frame_width, frame_height))\n\n    # Transform to apply on individual frames of the video\n    transform_frame = transforms.Compose([  # TODO Issue #16\n        transforms.ToPILImage(),\n        transforms.Resize(scaled_size),\n        transforms.ToTensor(),\n        normalize_transform(),\n    ])\n\n    # Loop through every frame of the video\n    while True:\n        ret, frame = video.read()\n        # Stop the loop when we\'re done with the video\n        if not ret:\n            break\n\n        # The transformed frame is what we\'ll feed into our model\n        # transformed_frame = transform_frame(frame)\n        transformed_frame = frame  # TODO: Issue #16\n        predictions = model.predict(transformed_frame)\n\n        # Add the top prediction of each class to the frame\n        for label, box, score in zip(*predictions):\n            if score < score_filter:\n                continue\n\n            # Since the predictions are for scaled down frames,\n            # we need to increase the box dimensions\n            # box *= scale_down_factor  # TODO Issue #16\n\n            # Create the box around each object detected\n            # Parameters: frame, (start_x, start_y), (end_x, end_y), (r, g, b), thickness\n            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 3)\n\n            # Write the label and score for the boxes\n            # Parameters: frame, text, (start_x, start_y), font, font scale, (r, g, b), thickness\n            cv2.putText(frame, \'{}: {}\'.format(label, round(score.item(), 2)), (box[0], box[1] - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 3)\n\n        # Write this frame to our video file\n        out.write(frame)\n\n        # If the \'q\' key is pressed, break from the loop\n        key = cv2.waitKey(1) & 0xFF\n        if key == ord(\'q\'):\n            break\n\n    # When finished, release the video capture and writer objects\n    video.release()\n    out.release()\n\n    # Close all the frames\n    cv2.destroyAllWindows()\n\n\ndef plot_prediction_grid(model, images, dim=None, figsize=None, score_filter=0.6):\n    """"""Plots a grid of images with boxes drawn around predicted objects.\n\n    :param model: The trained model with which to run object detection.\n    :type model: detecto.core.Model\n    :param images: An iterable of images to plot. If the images are\n        normalized torch.Tensor images, they will automatically be\n        reverse-normalized and converted to PIL images for plotting.\n    :type images: iterable\n    :param dim: (Optional) The dimensions of the grid in the format\n        ``(rows, cols)``. If no value is given, the grid is of the shape\n        ``(len(images), 1)``. ``rows * cols`` must match the number of\n        given images, or a ValueError is raised. Defaults to None.\n    :type dim: tuple or None\n    :param figsize: (Optional) The size of the entire grid in the format\n        ``(width, height)``. Defaults to None.\n    :type figsize: tuple or None\n    :param score_filter: (Optional) Minimum score required to show a\n        prediction. Defaults to 0.6.\n    :type score_filter: float\n\n    **Example**::\n\n        >>> from detecto.core import Model\n        >>> from detecto.utils import read_image\n        >>> from detecto.visualize import plot_prediction_grid\n\n        >>> model = Model.load(\'model_weights.pth\', [\'tick\', \'gate\'])\n        >>> images = []\n        >>> for i in range(4):\n        >>>     image = read_image(\'image{}.jpg\'.format(i))\n        >>>     images.append(image)\n        >>> plot_prediction_grid(model, images, dim=(2, 2), figsize=(8, 8))\n    """"""\n\n    # If not specified, show all in one column\n    if dim is None:\n        dim = (len(images), 1)\n\n    if dim[0] * dim[1] != len(images):\n        raise ValueError(\'Grid dimensions do not match size of list of images\')\n\n    fig, axes = plt.subplots(dim[0], dim[1], figsize=figsize)\n\n    # Loop through each image and position in the grid\n    index = 0\n    for i in range(dim[0]):\n        for j in range(dim[1]):\n            image = images[index]\n            preds = model.predict(image)\n\n            # If already a tensor, reverse normalize it and turn it back\n            if isinstance(image, torch.Tensor):\n                image = transforms.ToPILImage()(reverse_normalize(image))\n            index += 1\n\n            # Get the correct axis\n            if dim[0] <= 1 and dim[1] <= 1:\n                ax = axes\n            elif dim[0] <= 1:\n                ax = axes[j]\n            elif dim[1] <= 1:\n                ax = axes[i]\n            else:\n                ax = axes[i, j]\n\n            ax.imshow(image)\n\n            # Plot boxes and labels\n            for label, box, score in zip(*preds):\n                if score >= score_filter:\n                    width, height = box[2] - box[0], box[3] - box[1]\n                    initial_pos = (box[0], box[1])\n                    rect = patches.Rectangle(initial_pos, width, height, linewidth=1,\n                                             edgecolor=\'r\', facecolor=\'none\')\n                    ax.add_patch(rect)\n\n                    ax.text(box[0] + 5, box[1] - 10, \'{}: {}\'\n                            .format(label, round(score.item(), 2)), color=\'red\')\n                ax.set_title(\'Image {}\'.format(index))\n\n    plt.show()\n\n\ndef show_labeled_image(image, boxes, labels=None):\n    """"""Show the image along with the specified boxes around detected objects.\n    Also displays each box\'s label if a list of labels is provided.\n\n    :param image: The image to plot. If the image is a normalized\n        torch.Tensor object, it will automatically be reverse-normalized\n        and converted to a PIL image for plotting.\n    :type image: numpy.ndarray or torch.Tensor\n    :param boxes: A torch tensor of size (N, 4) where N is the number\n        of boxes to plot, or simply size 4 if N is 1.\n    :type boxes: torch.Tensor\n    :param labels: (Optional) A list of size N giving the labels of\n            each box (labels[i] corresponds to boxes[i]). Defaults to None.\n    :type labels: torch.Tensor or None\n\n    **Example**::\n\n        >>> from detecto.core import Model\n        >>> from detecto.utils import read_image\n        >>> from detecto.visualize import show_labeled_image\n\n        >>> model = Model.load(\'model_weights.pth\', [\'tick\', \'gate\'])\n        >>> image = read_image(\'image.jpg\')\n        >>> labels, boxes, scores = model.predict(image)\n        >>> show_labeled_image(image, boxes, labels)\n    """"""\n\n    fig, ax = plt.subplots(1)\n    # If the image is already a tensor, convert it back to a PILImage\n    # and reverse normalize it\n    if isinstance(image, torch.Tensor):\n        image = reverse_normalize(image)\n        image = transforms.ToPILImage()(image)\n    ax.imshow(image)\n\n    # Show a single box or multiple if provided\n    if boxes.ndim == 1:\n        boxes = boxes.view(1, 4)\n\n    if labels is not None and not _is_iterable(labels):\n        labels = [labels]\n\n    # Plot each box\n    for i in range(boxes.shape[0]):\n        box = boxes[i]\n        width, height = (box[2] - box[0]).item(), (box[3] - box[1]).item()\n        initial_pos = (box[0].item(), box[1].item())\n        rect = patches.Rectangle(initial_pos,  width, height, linewidth=1,\n                                 edgecolor=\'r\', facecolor=\'none\')\n        if labels:\n            ax.text(box[0] + 5, box[1] - 5, \'{}\'.format(labels[i]), color=\'red\')\n\n        ax.add_patch(rect)\n\n    plt.show()\n'"
docs/conf.py,1,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.append(\'..\')\nsys.path.append(\'detecto\')\n\n# -- Mock module dependencies ------------------------------------------------\n\nautodoc_mock_imports = [\'cv2\', \'matplotlib\', \'matplotlib.patches\',\n                        \'matplotlib.pyplot\', \'pandas\', \'torch\',\n                        \'torch.utils\', \'torch.utils.data\', \'torchvision\',\n                        \'torchvision.transforms\',\n                        \'torchvision.models.detection.faster_rcnn\']\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'Detecto\'\ncopyright = \'2019, Alan Bi\'\nauthor = \'Alan Bi\'\n\n# The full version, including alpha/beta/rc tags\nrelease = \'1.0\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\nhtml_theme_options = {\n    \'logo\': \'logo.svg\',\n    \'logo_name\': True,\n}\n\nmaster_doc = \'index\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n'"
detecto/tests/__init__.py,0,b'from .helpers import get_image\n'
detecto/tests/helpers.py,1,"b""import os\nimport torch\n\nfrom detecto.core import Model, Dataset\nfrom detecto.utils import xml_to_csv, read_image\n\n\ndef get_dataset(**kwargs):\n    path = os.path.dirname(__file__)\n    input_folder = os.path.join(path, 'static')\n    labels_path = os.path.join(path, 'static/labels.csv')\n\n    xml_to_csv(input_folder, labels_path)\n    dataset = Dataset(labels_path, input_folder, **kwargs)\n    os.remove(labels_path)\n\n    return dataset\n\n\ndef get_image():\n    path = os.path.dirname(__file__)\n    file = 'static/image.jpg'\n    return read_image(os.path.join(path, file))\n\n\ndef get_model():\n    return Model(['test1', 'test2', 'test3'])\n\n\ndef empty_predictor(x):\n    return [{'labels': torch.empty(0), 'boxes': torch.empty(0, 4), 'scores': torch.empty(0)}]\n"""
detecto/tests/test_core.py,12,"b""import torch\n\nfrom detecto.core import *\nfrom detecto.utils import read_image\nfrom .helpers import get_dataset, get_image, get_model, empty_predictor\nfrom torchvision import transforms\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n\n# Test that the dataset returns the correct things and properly\n# applies the default or given transforms\ndef test_dataset():\n    # Test the format of the values returned by indexing the dataset\n    dataset = get_dataset()\n    assert len(dataset) == 2\n    assert isinstance(dataset[0][0], torch.Tensor)\n    assert isinstance(dataset[0][1], dict)\n    assert dataset[0][0].shape == (3, 1080, 1720)\n    assert 'boxes' in dataset[0][1] and 'labels' in dataset[0][1]\n    assert dataset[0][1]['boxes'].shape == (1, 4)\n    assert dataset[0][1]['labels'] == 'start_tick'\n\n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize(108),\n        transforms.RandomHorizontalFlip(1),\n        transforms.ToTensor()\n    ])\n\n    # Test that the transforms are properly applied\n    dataset = get_dataset(transform=transform)\n    assert dataset[1][0].shape == (3, 108, 172)\n    assert torch.all(dataset[1][1]['boxes'][0] == torch.tensor([6, 41, 171, 107]))\n\n    # Test works when given an XML folder\n    path = os.path.dirname(__file__)\n    input_folder = os.path.join(path, 'static')\n\n    dataset = Dataset(input_folder, input_folder)\n    assert len(dataset) == 2\n    assert dataset[0][0].shape == (3, 1080, 1720)\n    assert 'boxes' in dataset[0][1] and 'labels' in dataset[0][1]\n\n    dataset = Dataset(input_folder)\n    assert len(dataset) == 2\n    assert dataset[0][0].shape == (3, 1080, 1720)\n    assert 'boxes' in dataset[0][1] and 'labels' in dataset[0][1]\n\n\n# Ensure that the collate function of the DataLoader properly\n# converts a list of tuples into a tuple of lists\ndef test_collate_fn():\n    test_input = [(1, 10), (2, 20), (3, 30)]\n    test_output = DataLoader.collate_data(test_input)\n\n    assert isinstance(test_output, tuple)\n    assert len(test_output) == 2\n    assert isinstance(test_output[0], list)\n    assert len(test_output[0]) == 3\n\n    assert test_output[0][0] == 1\n    assert test_output[0][2] == 3\n    assert test_output[1][0] == 10\n    assert test_output[1][2] == 30\n\n\n# Test that the dataloader correctly loops through every element\n# in the dataset and returns them in the right format\ndef test_dataloader():\n    dataset = get_dataset()\n    loader = DataLoader(dataset, batch_size=2)\n\n    iterations = 0\n    for data in loader:\n        iterations += 1\n\n        assert isinstance(data, tuple)\n        assert len(data) == 2\n        assert isinstance(data[0], list)\n        assert len(data[0]) == 2\n\n        assert isinstance(data[0][0], torch.Tensor)\n        assert isinstance(data[0][1], torch.Tensor)\n        assert 'boxes' in dataset[0][1] and 'labels' in dataset[1][1]\n\n    assert iterations == 1\n\n\n# Ensure that the model's internal parameters are properly set\ndef test_model_internal():\n    model = get_model()\n\n    assert model._device == torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    box_predictor = model._model.roi_heads.box_predictor\n    assert isinstance(box_predictor, FastRCNNPredictor)\n    assert box_predictor.cls_score.out_features == 4\n\n    assert model._classes == ['__background__', 'test1', 'test2', 'test3']\n    assert model._int_mapping['test1'] == 1\n\n    # _int_mapping should give the right index of each class\n    for k in model._int_mapping:\n        assert model._classes[model._int_mapping[k]] == k\n\n\n# def test_model_default():\n#     path = os.path.dirname(__file__)\n#     file = os.path.join(path, 'static/apple_orange.jpg')\n#\n#     model = Model()\n#     preds = model.predict_top(read_image(file))\n#\n#     assert len(preds[0]) >= 2\n#     assert 'orange' in preds[0] and 'apple' in preds[0]\n#     assert sum(preds[2]) / len(preds[2]) > 0.50\n\n\n# Ensure that fitting the model increases accuracy and returns the losses\ndef test_model_fit():\n    model = Model(['start_tick', 'start_gate'])\n\n    dataset = get_dataset()\n    loader = DataLoader(dataset, batch_size=1)\n\n    initial_loss = 0\n    with torch.no_grad():\n        for images, targets in loader:\n            model._convert_to_int_labels(targets)\n            images, targets = model._to_device(images, targets)\n            loss_dict = model._model(images, targets)\n            total_loss = sum(loss for loss in loss_dict.values())\n            initial_loss += total_loss.item()\n    initial_loss /= len(loader.dataset)\n\n    losses = model.fit(loader, val_dataset=loader, epochs=1)\n\n    # Average loss during training should be lower than initial loss\n    assert len(losses) == 1\n    assert losses[0] < initial_loss\n\n    # Should not return anything if not validation losses are produced\n    losses = model.fit(loader, loader, epochs=0)\n    assert losses is None\n\n    # Works when given datasets\n    losses = model.fit(dataset, val_dataset=dataset, epochs=1)\n    assert len(losses) == 1\n\n\n# Test both the predict and predict_top methods with both single\n# images and lists of images to predict on\n# TODO: test applying transforms on images to predict\ndef test_model_predict():\n    classes = ['start_tick', 'start_gate']\n    path = os.path.dirname(__file__)\n    file = os.path.join(path, 'static/model.pth')\n\n    # Load in a pre-fitted model so it can actually make predictions\n    model = Model.load(file, classes)\n    image = get_image()\n\n    # Test predict method on a single image\n    pred = model.predict(image)\n    assert isinstance(pred, tuple)\n    assert set(pred[0]) == set(classes)\n    assert isinstance(pred[1][0], torch.Tensor) and pred[1][0].shape[0] == 4\n    assert pred[2][0] > 0.5\n\n    # Test predict method on a list of images\n    preds = model.predict([image])\n    assert len(preds) == 1\n    assert preds[0][0] == pred[0]\n    assert torch.all(preds[0][1] == pred[1])\n    assert torch.all(preds[0][2] == pred[2])\n\n    # Test predict_top method on a single image\n    top_pred = model.predict_top(image)\n    assert isinstance(top_pred, tuple)\n    assert len(top_pred[0]) == 2\n    assert isinstance(top_pred[1], torch.Tensor)\n    assert top_pred[1].shape[0] == len(top_pred[0]) and top_pred[1].shape[1] == 4\n    assert top_pred[2][0] > 0.5\n\n    # Test predict_top method on a list of images\n    top_preds = model.predict_top([image])\n    assert len(top_preds) == 1\n    assert set(top_preds[0][0]) == set(top_pred[0])\n    assert torch.all(top_preds[0][1][0] == top_pred[1][0]) or \\\n        torch.all(top_preds[0][1][0] == top_pred[1][1])\n    assert top_preds[0][2][0] == top_pred[2][0] or \\\n        top_preds[0][2][0] == top_pred[2][1]\n\n    # Test return values when no predictions are made\n    model._model.forward = empty_predictor\n    preds = model.predict([image])\n    assert len(preds) == 1\n    assert preds[0][0] == [] and preds[0][1].nelement() == 0 and preds[0][2].nelement() == 0\n\n    preds = model.predict_top([image])\n    assert len(preds) == 1\n    assert preds[0][0] == [] and preds[0][1].nelement() == 0 and preds[0][2].nelement() == 0\n\n\n# Test that save, load, and get_internal_model all work properly\ndef test_model_helpers():\n    path = os.path.dirname(__file__)\n    file = os.path.join(path, 'static/saved_model.pth')\n\n    model = get_model()\n\n    model.save(file)\n    model = Model.load(file, ['test1', 'test2', 'test3'])\n\n    assert model._model is not None\n    assert model.get_internal_model() is model._model\n\n    os.remove(file)\n"""
detecto/tests/test_utils.py,3,"b""import os\nimport pandas as pd\nimport torch\nimport torchvision\n\nfrom .helpers import get_image\nfrom detecto.utils import *\nfrom detecto.utils import _is_iterable\n\n\ndef test_filter_top_predictions():\n    labels = ['test1', 'test2', 'test1', 'test2', 'test2']\n    boxes = torch.ones(5, 4)\n    scores = torch.tensor([5., 4, 3, 2, 1])\n    for i in range(5):\n        boxes[i] *= i\n\n    preds = filter_top_predictions(labels, boxes, scores)\n\n    assert isinstance(preds, tuple) and len(preds) == 3\n    # Correct labels\n    assert len(preds[0]) == 2 and set(preds[0]) == {'test1', 'test2'}\n\n    # Correct box coordinates\n    assert {preds[1][0][0].item(), preds[1][1][0].item()} == {0, 1}\n\n    # Correct scores\n    assert {preds[2][0].item(), preds[2][1].item()} == {5, 4}\n\n\ndef test_default_transforms():\n    transforms = default_transforms()\n\n    assert isinstance(transforms.transforms[0], torchvision.transforms.ToTensor)\n    assert isinstance(transforms.transforms[1], torchvision.transforms.Normalize)\n    assert transforms.transforms[1].mean == normalize_transform().mean\n    assert transforms.transforms[1].std == normalize_transform().std\n\n\ndef test_normalize_functions():\n    transform = normalize_transform()\n    image = transforms.ToTensor()(get_image())\n\n    normalized_img = transform(image)\n    reversed_img = reverse_normalize(normalized_img)\n\n    # Normalized image that's then reversed should be close to original\n    assert (image - reversed_img).max() < 0.05\n\n\ndef test_read_image():\n    path = os.path.dirname(__file__)\n    file = 'static/image.jpg'\n    image_path = os.path.join(path, file)\n\n    image = get_image()\n\n    assert (read_image(image_path) == image).all()\n\n\ndef test_split_video():\n    path = os.path.dirname(__file__)\n    input_video = os.path.join(path, 'static/input_video.mp4')\n    output_path = os.path.join(path, 'static/split_frames')\n\n    os.mkdir(output_path)\n\n    split_video(input_video, output_path, prefix='testing', step_size=-1)\n\n    files = [f for f in os.listdir(output_path)]\n\n    assert len(files) == 1\n    assert files[0] == 'testing0.jpg'\n\n    os.remove(os.path.join(output_path, 'testing0.jpg'))\n    os.rmdir(output_path)\n\n\ndef test_xml_to_csv():\n    path = os.path.dirname(__file__)\n    input_folder = os.path.join(path, 'static')\n    output_path = os.path.join(path, 'static/labels.csv')\n\n    df = xml_to_csv(input_folder, output_path)\n    csv = pd.read_csv(output_path)\n\n    assert len(csv) == 2\n    assert len(df) == 2\n    assert csv.loc[0, 'filename'] == 'image.jpg'\n    assert df.loc[1, 'class'] == 'start_gate'\n    assert csv.loc[0, 'width'] == 1720\n    assert df.loc[1, 'height'] == 1080\n    assert csv.loc[0, 'ymax'] == 784\n    assert df.loc[1, 'xmin'] == 1\n\n    os.remove(output_path)\n\n    xml_to_csv(input_folder)\n    assert not os.path.isfile(output_path)\n\n\ndef test__is_iterable():\n    test1 = [1, 2]\n    test2 = (3, 4)\n    test3 = torch.ones(2)\n    test4 = 5\n\n    assert _is_iterable(test1)\n    assert _is_iterable(test2)\n    assert not _is_iterable(test3)\n    assert not _is_iterable(test4)\n"""
detecto/tests/test_visualize.py,3,"b""import os\n\nfrom .helpers import get_image, get_model, empty_predictor\nfrom detecto.visualize import *\n\n\ndef test_detect_video():\n    path = os.path.dirname(__file__)\n    input_video = os.path.join(path, 'static/input_video.mp4')\n    output_video = os.path.join(path, 'static/output_video.avi')\n\n    model = get_model()\n    detect_video(model, input_video, output_video)\n\n    assert os.path.isfile(output_video)\n    os.remove(output_video)\n\n    # Ensure it works when the model makes no predictions\n    model._model.forward = empty_predictor\n    detect_video(model, input_video, output_video)\n\n    assert os.path.isfile(output_video)\n    os.remove(output_video)\n\n\ndef test_plot_prediction_grid():\n    model = get_model()\n    try:\n        plot_prediction_grid(model, [3, 4], (3, 5))\n        assert False  # Above should throw a value error\n    except ValueError:\n        pass\n    except Exception:\n        assert False  # An error occurred\n\n    plt.show = lambda: None\n\n    image = get_image()\n    plot_prediction_grid(model, [image], (1, 1), figsize=(20, 10))  # Shouldn't throw an error\n\n    # Ensure it works when the model makes no predictions\n    model._model.forward = empty_predictor\n    plot_prediction_grid(model, [image], dim=None)  # Shouldn't throw an error\n\n\ndef test_show_labeled_image():\n    image = get_image()\n\n    plt.show = lambda: None\n\n    # Shouldn't throw any errors\n    show_labeled_image(image, torch.ones(4))\n    show_labeled_image(image, torch.ones(10, 4))\n    show_labeled_image(image, torch.empty(0, 4))\n"""
