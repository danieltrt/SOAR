file_path,api_count,code
config.py,0,"b'""""""Configuration file.\nNote that this config file does not contain all constants, some are\nin the train.py scripts.""""""\nfrom __future__ import print_function, division\nimport os\n\nclass Config(object):\n    """"""Class containing some global configuration constants.""""""\n\n    # path to the directory of the project\n    MAIN_DIR = os.path.dirname(__file__)\n\n    # GPU id to use\n    GPU = 0\n\n    # height and width at which ingame screenshots are saved to memory\n    MODEL_HEIGHT = 720 // 4 # 180\n    MODEL_WIDTH = 1280 // 4 # 320\n\n    # directory of the annotations\n    ANNOTATIONS_DIR = ""/media/aj/grab/ml/ets2ai/annotations"" if os.path.isdir(""/media/aj/grab/ml/ets2ai/annotations"") else os.path.join(MAIN_DIR, ""annotations/"")\n    # filepath to the speed image annotations\n    SPEED_IMAGE_SEGMENTS_DB_FILEPATH = os.path.join(MAIN_DIR, ""annotations/speed_image_segments.txt"")\n\n    # directory where the replay memory files are saved\n    REPLAY_MEMORIES_DIR = MAIN_DIR\n\n    # configurations of the replay memories\n    REPLAY_MEMORY_CFGS = {\n        ""reinforced-train"": {""filepath"": os.path.join(REPLAY_MEMORIES_DIR, ""replay_memory_reinforced.sqlite""), ""max_size"": 650*1000, ""max_size_tolerance"": 20000},\n        ""reinforced-val"": {""filepath"": os.path.join(REPLAY_MEMORIES_DIR, ""replay_memory_reinforced_val.sqlite""), ""max_size"": 150*1000, ""max_size_tolerance"": 20000},\n        ""supervised-train"": {""filepath"": os.path.join(REPLAY_MEMORIES_DIR, ""replay_memory_supervised.sqlite""), ""max_size"": 200*1000, ""max_size_tolerance"": 20000},\n        ""supervised-val"": {""filepath"": os.path.join(REPLAY_MEMORIES_DIR, ""replay_memory_supervised_val.sqlite""), ""max_size"": 50*1000, ""max_size_tolerance"": 20000},\n    }\n\n    # max speed, model inputs are clipped to that maximum\n    MAX_SPEED = 150\n\n    # max/min steering wheel positions to use for the classical and CNN-based\n    # steering wheel prediction. values beyond this are viewed as somehow wrong\n    # and either ignored or lead to the assumption that the current approximated\n    # position is wrong.\n    STEERING_WHEEL_MAX = 360+90+40 # wheel can be turned by roughly +/- 360+90 degrees, add 40deg tolerance\n    STEERING_WHEEL_MIN = -STEERING_WHEEL_MAX # steering_wheel.py currently only looks at max value\n    STEERING_WHEEL_RAW_ONE_MAX = 180\n    STEERING_WHEEL_RAW_ONE_MIN = -STEERING_WHEEL_RAW_ONE_MAX\n    STEERING_WHEEL_RAW_TWO_MAX = STEERING_WHEEL_RAW_ONE_MAX\n    STEERING_WHEEL_RAW_TWO_MIN = STEERING_WHEEL_RAW_ONE_MIN\n    STEERING_WHEEL_CNN_MAX = 360+90+40\n    STEERING_WHEEL_CNN_MIN = -STEERING_WHEEL_CNN_MAX\n    STEERING_WHEEL_RAW_CNN_MAX = 180\n    STEERING_WHEEL_RAW_CNN_MIN = -STEERING_WHEEL_RAW_CNN_MAX\n\n    # min and maximum direct reward range, used for computing bins\n    MIN_REWARD = -100\n    MAX_REWARD = 100\n\n    # keys for pause and quickload to use\n    KEY_PAUSE = ""F1""\n    KEY_QUICKLOAD = ""F11""\n\n    # Number of savegames to use during training. If set to N, the AI will\n    # load a random one of the first N savegames during reinforcement learning.\n    # (This happens many times during the training.)\n    RELOAD_MAX_SAVEGAME_NUMBER = 6\n\n    # Directory of an example image from an offence message.\n    OFFENCE_FF_IMAGE = os.path.join(MAIN_DIR, ""images/offence_ff.png"")\n    # Directory of an example image from a damage message (XX% damage).\n    DAMAGE_SINGLE_DIGIT_IMAGE = os.path.join(MAIN_DIR, ""images/damage_single_digit.png"")\n    # Directory of an example image from a damage message (X% damage).\n    DAMAGE_DOUBLE_DIGIT_IMAGE = os.path.join(MAIN_DIR, ""images/damage_double_digit.png"")\n    # Directory of an example image showing and activated reverse gear.\n    REVERSE_IMAGE = os.path.join(MAIN_DIR, ""images/reverse.png"")\n'"
annotate/__init__.py,0,b''
annotate/annotate_attributes.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom common import load_annotations, numpy_to_tk_image\nimport Tkinter\nfrom collections import OrderedDict\nimport imgaug as ia\nimport cPickle as pickle\n\nclass AttributeGroup(object):\n    def __init__(self, name, name_shown, attributes=None, default_attribute=None):\n        self.name = name\n        self.name_shown = name_shown\n        self.attributes = attributes if attributes is not None else []\n        self.default_attribute = default_attribute\n\n    def append(self, att):\n        self.attributes.append(att)\n\n    def get_by_name(self, name):\n        atts = [att for att in self.attributes if att.name == name]\n        return atts[0] if len(atts) > 0 else None\n\n    def set_default_by_name(self, name):\n        self.default_attribute = self.get_by_name(name)\n\nclass Attribute(object):\n    def __init__(self, name, name_shown):\n        self.name = name\n        self.name_shown = name_shown\n\n""""""\nATTRIBUTE_GROUPS = [\n    (\n        ""Road Type"",\n        ""highway"",\n        OrderedDict([\n            (""country_road"", ""Country Road""),\n            (""highway"", ""Highway""),\n            (""highway_entry_exit"", ""Highway entry/exit""),\n            (""open_area"", ""Open Area / Parking Lot""),\n            (""fuel_station"", ""Fuel Station""),\n            (""hotel"", ""Hotel""),\n            (""rest_area"", ""Rest Area""),\n            (""city_road"", ""City Road""),\n            (""toll_booth"", ""Toll Booth"")\n        ])\n    )\n]\n""""""\n\nATTRIBUTE_GROUP_ROAD_TYPE = AttributeGroup(""road_type"", ""Road Type"")\nATTRIBUTE_GROUP_ROAD_TYPE.append(Attribute(""country_road"", ""Country Road""))\nATTRIBUTE_GROUP_ROAD_TYPE.append(Attribute(""highway"", ""Highway""))\nATTRIBUTE_GROUP_ROAD_TYPE.append(Attribute(""highway_entry_exit"", ""Highway entry/exit""))\nATTRIBUTE_GROUP_ROAD_TYPE.append(Attribute(""open_area"", ""Open Area / Parking Lot""))\nATTRIBUTE_GROUP_ROAD_TYPE.append(Attribute(""fuel_station"", ""Fuel Station""))\nATTRIBUTE_GROUP_ROAD_TYPE.append(Attribute(""hotel"", ""Hotel""))\nATTRIBUTE_GROUP_ROAD_TYPE.append(Attribute(""rest_area"", ""Rest Area""))\nATTRIBUTE_GROUP_ROAD_TYPE.append(Attribute(""city_road"", ""City Road""))\nATTRIBUTE_GROUP_ROAD_TYPE.append(Attribute(""toll_booth"", ""Toll Booth""))\nATTRIBUTE_GROUP_ROAD_TYPE.append(Attribute(""other"", ""Other""))\nATTRIBUTE_GROUP_ROAD_TYPE.set_default_by_name(""highway"")\n\nATTRIBUTE_GROUP_INTERSECTION = AttributeGroup(""intersection"", ""Intersection"")\nATTRIBUTE_GROUP_INTERSECTION.append(Attribute(""none"", ""None""))\nATTRIBUTE_GROUP_INTERSECTION.append(Attribute(""t-left"", ""T (left -|)""))\nATTRIBUTE_GROUP_INTERSECTION.append(Attribute(""t-right"", ""T (right |-)""))\nATTRIBUTE_GROUP_INTERSECTION.append(Attribute(""t-frontal"", ""T (frontal -.-)""))\nATTRIBUTE_GROUP_INTERSECTION.append(Attribute(""cross"", ""Cross""))\nATTRIBUTE_GROUP_INTERSECTION.append(Attribute(""roundabout"", ""Roundabout""))\nATTRIBUTE_GROUP_INTERSECTION.append(Attribute(""other"", ""Other""))\nATTRIBUTE_GROUP_INTERSECTION.set_default_by_name(""none"")\n\nATTRIBUTE_GROUP_DIRECTION = AttributeGroup(""direction"", ""Direction"")\nATTRIBUTE_GROUP_DIRECTION.append(Attribute(""unidirection"", ""Unidirection""))\nATTRIBUTE_GROUP_DIRECTION.append(Attribute(""bidirection"", ""Bidirection""))\nATTRIBUTE_GROUP_DIRECTION.append(Attribute(""other"", ""Other""))\nATTRIBUTE_GROUP_DIRECTION.set_default_by_name(""bidirection"")\n\nATTRIBUTE_GROUP_LANE_COUNT = AttributeGroup(""lane-count"", ""Lane Count (current dir.)"")\nATTRIBUTE_GROUP_LANE_COUNT.append(Attribute(""1"", ""1""))\nATTRIBUTE_GROUP_LANE_COUNT.append(Attribute(""2"", ""2""))\nATTRIBUTE_GROUP_LANE_COUNT.append(Attribute(""3"", ""3""))\nATTRIBUTE_GROUP_LANE_COUNT.append(Attribute(""4+"", ""4+""))\nATTRIBUTE_GROUP_LANE_COUNT.append(Attribute(""other"", ""Other""))\nATTRIBUTE_GROUP_LANE_COUNT.set_default_by_name(""2"")\n\nATTRIBUTE_GROUP_CURVE = AttributeGroup(""curve"", ""Curve (current lane)"")\nATTRIBUTE_GROUP_CURVE.append(Attribute(""straight"", ""Straight""))\nATTRIBUTE_GROUP_CURVE.append(Attribute(""left-slight"", ""Left (slight)""))\nATTRIBUTE_GROUP_CURVE.append(Attribute(""left-medium"", ""Left (medium)""))\nATTRIBUTE_GROUP_CURVE.append(Attribute(""left-strong"", ""Left (strong)""))\nATTRIBUTE_GROUP_CURVE.append(Attribute(""right-slight"", ""Right (slight)""))\nATTRIBUTE_GROUP_CURVE.append(Attribute(""right-medium"", ""Right (medium)""))\nATTRIBUTE_GROUP_CURVE.append(Attribute(""right-strong"", ""Right (strong)""))\nATTRIBUTE_GROUP_CURVE.append(Attribute(""other"", ""Other""))\nATTRIBUTE_GROUP_CURVE.set_default_by_name(""straight"")\n\nATTRIBUTE_GROUP_SPACE_FRONT = AttributeGroup(""space-front"", ""Space (Front)"")\nATTRIBUTE_GROUP_SPACE_FRONT.append(Attribute(""plenty"", ""plenty (>3s)""))\nATTRIBUTE_GROUP_SPACE_FRONT.append(Attribute(""some"", ""some (1-3s)""))\nATTRIBUTE_GROUP_SPACE_FRONT.append(Attribute(""minimal"", ""minimal (<1s)""))\nATTRIBUTE_GROUP_SPACE_FRONT.append(Attribute(""none"", ""none (crashing)""))\nATTRIBUTE_GROUP_SPACE_FRONT.set_default_by_name(""plenty"")\n\nATTRIBUTE_GROUP_SPACE_LEFT = AttributeGroup(""space-left"", ""Space (Left)"")\nATTRIBUTE_GROUP_SPACE_LEFT.append(Attribute(""plenty"", ""plenty (good)""))\nATTRIBUTE_GROUP_SPACE_LEFT.append(Attribute(""some"", ""some (meh)""))\nATTRIBUTE_GROUP_SPACE_LEFT.append(Attribute(""minimal"", ""minimal (bad)""))\nATTRIBUTE_GROUP_SPACE_LEFT.append(Attribute(""none"", ""none (crashing)""))\nATTRIBUTE_GROUP_SPACE_LEFT.set_default_by_name(""plenty"")\n\nATTRIBUTE_GROUP_SPACE_RIGHT = AttributeGroup(""space-right"", ""Space (Right)"")\nATTRIBUTE_GROUP_SPACE_RIGHT.append(Attribute(""plenty"", ""plenty (good)""))\nATTRIBUTE_GROUP_SPACE_RIGHT.append(Attribute(""some"", ""some (meh)""))\nATTRIBUTE_GROUP_SPACE_RIGHT.append(Attribute(""minimal"", ""minimal (bad)""))\nATTRIBUTE_GROUP_SPACE_RIGHT.append(Attribute(""none"", ""none (crashing)""))\nATTRIBUTE_GROUP_SPACE_RIGHT.set_default_by_name(""plenty"")\n\nATTRIBUTE_GROUP_OFFROAD = AttributeGroup(""offroad"", ""Offroad"")\nATTRIBUTE_GROUP_OFFROAD.append(Attribute(""onroad"", ""Onroad""))\nATTRIBUTE_GROUP_OFFROAD.append(Attribute(""slightly"", ""Slightly""))\nATTRIBUTE_GROUP_OFFROAD.append(Attribute(""significantly"", ""Significantly""))\nATTRIBUTE_GROUP_OFFROAD.set_default_by_name(""onroad"")\n\nATTRIBUTE_GROUPS = [\n    ATTRIBUTE_GROUP_ROAD_TYPE,\n    ATTRIBUTE_GROUP_INTERSECTION,\n    ATTRIBUTE_GROUP_DIRECTION,\n    ATTRIBUTE_GROUP_LANE_COUNT,\n    ATTRIBUTE_GROUP_CURVE,\n    ATTRIBUTE_GROUP_SPACE_FRONT,\n    ATTRIBUTE_GROUP_SPACE_LEFT,\n    ATTRIBUTE_GROUP_SPACE_RIGHT,\n    ATTRIBUTE_GROUP_OFFROAD\n]\n\ndef main():\n    print(""Loading replay memory..."")\n    memory = replay_memory.ReplayMemory.create_instance_supervised()\n\n    win = AttributesAnnotationWindow.create(\n        memory,\n        save_to_fp=""annotations_attributes.pickle"",\n        every_nth_example=25\n    )\n    win.autosave_every_nth = 100\n    win.master.wm_title(""Annotate attributes"")\n\n    Tkinter.mainloop()\n\nclass AttributesAnnotationWindow(object):\n    def __init__(self, master, canvas, memory, current_state_idx, annotations, save_to_fp, every_nth_example=10, zoom_factor=4):\n        self.master = master\n        self.canvas = canvas\n        self.memory = memory\n        self.current_state_idx = current_state_idx\n        self.annotations = annotations if annotations is not None else dict()\n        self.current_annotation = None\n        self.background_label = None\n\n        self.dirty = False\n        self.last_autosave = 0\n        self.every_nth_example = every_nth_example\n        self.zoom_factor = zoom_factor\n        self.autosave_every_nth = 20\n        self.save_to_fp = save_to_fp\n\n        self.is_showing_directly_previous_state = False\n        self.directly_previous_state = None\n        self.current_state = None\n        self.att_group_to_variable = dict()\n        #self.switch_to_state(self.current_state_idx, autosave=False)\n        #self.current_state = memory.get_state_by_id(current_state_idx)\n\n    @staticmethod\n    def create(memory, save_to_fp, every_nth_example=10, zoom_factor=2):\n        colcount = max([len(att_group.attributes) for att_group in ATTRIBUTE_GROUPS])\n\n        print(""Loading previous annotations..."")\n        annotations = load_annotations(save_to_fp)\n        #is_annotated = dict([(str(annotation.idx), True) for annotation in annotations])\n\n        current_state_idx = memory.id_min\n        if annotations is not None:\n            while current_state_idx < memory.id_max:\n                key = str(current_state_idx)\n                if key not in annotations:\n                    break\n                current_state_idx += every_nth_example\n        print(""ID of first unannotated state: %d"" % (current_state_idx,))\n\n        master = Tkinter.Tk()\n        master.grid()\n        state = memory.get_state_by_id(current_state_idx)\n        canvas_height = state.screenshot_rs.shape[0] * zoom_factor\n        canvas_width = state.screenshot_rs.shape[1] * zoom_factor\n        print(""canvas height, width:"", canvas_height, canvas_width)\n        canvas = Tkinter.Canvas(master, width=canvas_width, height=canvas_height)\n        #canvas.pack()\n        canvas.grid(row=0, column=0, columnspan=colcount)\n        canvas.focus_set()\n\n        #y = int(canvas_height / 2)\n        #w.create_line(0, y, canvas_width, y, fill=""#476042"")\n        message = Tkinter.Label(master, text=""Press S to save."")\n        #message.pack(side=Tkinter.BOTTOM)\n        message.grid(row=1, column=0, columnspan=colcount)\n\n        window_state = AttributesAnnotationWindow(\n            master,\n            canvas,\n            memory,\n            current_state_idx,\n            annotations,\n            save_to_fp,\n            every_nth_example,\n            zoom_factor\n        )\n\n        def build_lambda(att_group, att):\n            return lambda: window_state.on_radio_click(att_group, att)\n\n        for row_idx, att_group in enumerate(ATTRIBUTE_GROUPS):\n            print(row_idx)\n            var = Tkinter.StringVar()\n            window_state.att_group_to_variable[att_group.name] = var\n            var.set(att_group.default_attribute.name)\n            lab = Tkinter.Label(master, text=att_group.name_shown)\n            lab.grid(row=2+row_idx, column=0, sticky=Tkinter.W)\n            #lab = Tkinter.Label(master, text=att_group.name_shown).pack(side=Tkinter.LEFT)\n            #lab = Tkinter.Label(master, text=att_group.name_shown).pack(anchor=Tkinter.S)\n            print(""default:"", att_group.default_attribute.name)\n            for col_idx, att in enumerate(att_group.attributes):\n                print(""ns/n"", att.name_shown, att.name)\n                c = Tkinter.Radiobutton(\n                    master, text=att.name_shown, variable=var,\n                    value=att.name,\n                    command=build_lambda(att_group, att)\n                )\n                #c.pack(side=Tkinter.LEFT)\n                c.grid(row=2+row_idx, column=col_idx+1, sticky=Tkinter.W)\n                print(row_idx, col_idx)\n\n        canvas.bind(""<s>"", lambda event: window_state.save_annotations(force=True))\n        canvas.bind(""<p>"", lambda event: window_state.toggle_previous_screenshot())\n        canvas.bind(""<Left>"", lambda event: window_state.previous_state(autosave=True))\n        canvas.bind(""<Right>"", lambda event: window_state.next_state(autosave=True))\n\n        window_state.switch_to_state(window_state.current_state_idx, autosave=False)\n\n        return window_state\n\n    def on_radio_click(self, att_group, att):\n        print(""radio click"", att_group.name, att.name)\n        var = self.att_group_to_variable[att_group.name]\n        var.set(att.name)\n        self.current_annotation[""attributes""][att_group.name] = att.name\n        self.dirty = True\n\n    #def update_annotations(self):\n\n\n    def toggle_previous_screenshot(self):\n        if self.directly_previous_state is not None:\n            if self.is_showing_directly_previous_state:\n                self.set_canvas_background(self._generate_heatmap())\n            else:\n                self.set_canvas_background(self.directly_previous_state.screenshot_rs)\n            self.is_showing_directly_previous_state = not self.is_showing_directly_previous_state\n\n    def previous_state(self, autosave):\n        print(""Switching to previous state..."")\n        self.current_state_idx -= self.every_nth_example\n        assert self.current_state_idx >= self.memory.id_min, ""Start of memory reached (%d vs %d)"" % (self.current_state_idx, self.memory.id_min)\n        self.switch_to_state(self.current_state_idx, autosave=autosave)\n\n    def next_state(self, autosave):\n        print(""Switching to next state..."")\n        self.current_state_idx += self.every_nth_example\n        assert self.current_state_idx <= self.memory.id_max, ""End of memory reached (%d vs %d)"" % (self.current_state_idx, self.memory.id_max)\n        self.switch_to_state(self.current_state_idx, autosave=autosave)\n\n    def switch_to_state(self, idx, autosave):\n        print(""Switching to state %d (autosave=%s)..."" % (idx, str(autosave)))\n        self.directly_previous_state = self.memory.get_state_by_id(idx-1)\n        self.current_state = self.memory.get_state_by_id(idx)\n        assert self.current_state is not None\n        self.current_state_idx = idx\n\n        if autosave:\n            if (self.last_autosave+1) % self.autosave_every_nth == 0:\n                # only autosaves if dirty flag is true, ie any example was changed\n                self.save_annotations()\n                self.last_autosave = 0\n            else:\n                self.last_autosave += 1\n            print(""last_autosave="", self.last_autosave)\n\n        key = str(self.current_state_idx)\n        if key in self.annotations:\n            self.current_annotation = self.annotations[key]\n            print(""Annotation for state "", key, "" available."")\n            print(""Attributes: "", self.annotations[key][""attributes""])\n        else:\n            print(""No annotation yet for state "", key)\n            last_annotation = self.current_annotation\n            self.current_annotation = {\n                ""idx"": self.current_state_idx,\n                ""from_datetime"": self.current_state.from_datetime,\n                ""screenshot_rs"": self.current_state.screenshot_rs,\n                ""attributes"": dict()\n            }\n            for att_group in ATTRIBUTE_GROUPS:\n                if last_annotation is not None:\n                    self.current_annotation[""attributes""][att_group.name] = last_annotation[""attributes""][att_group.name]\n                else:\n                    self.current_annotation[""attributes""][att_group.name] = att_group.default_attribute.name\n            self.annotations[key] = self.current_annotation\n            print(""Initializing attributes to "", self.current_annotation[""attributes""])\n\n        # set variables to new annotation state\n        for att_group_name in self.current_annotation[""attributes""]:\n            var = self.att_group_to_variable[att_group_name]\n            val = self.current_annotation[""attributes""][att_group_name]\n            var.set(val)\n\n        self.is_showing_directly_previous_state = False\n        self.set_canvas_background(self._generate_heatmap())\n        #self.update_annotation_grid(self.grid, initial=True)\n\n    def save_annotations(self, force=False):\n        #print(self.annotations)\n        if self.dirty or force:\n            print(""Saving to %s..."" % (self.save_to_fp,))\n            with open(self.save_to_fp, ""w"") as f:\n                pickle.dump(self.annotations, f, protocol=-1)\n            self.dirty = False\n            print(""Finished saving."")\n        else:\n            print(""Not saved (not marked dirty)"")\n\n    def set_canvas_background(self, image):\n        if self.background_label is None:\n            # initialize background image label (first call)\n            #img = self.current_state.screenshot_rs\n            #bg_img_tk = numpy_to_tk_image(np.zeros(img.shape))\n            img_heatmap = self._generate_heatmap()\n            img_heatmap_rs = ia.imresize_single_image(img_heatmap, (img_heatmap.shape[0]*self.zoom_factor, img_heatmap.shape[1]*self.zoom_factor), interpolation=""nearest"")\n            bg_img_tk = numpy_to_tk_image(img_heatmap_rs)\n            self.background_label = Tkinter.Label(self.canvas, image=bg_img_tk)\n            self.background_label.place(x=0, y=0, relwidth=1, relheight=1, anchor=Tkinter.NW)\n            self.background_label.image = bg_img_tk\n\n        #print(""image size"", image.shape)\n        #print(""image height, width"", image.to_array().shape)\n        image_rs = ia.imresize_single_image(image, (image.shape[0]*self.zoom_factor, image.shape[1]*self.zoom_factor), interpolation=""nearest"")\n        image_tk = numpy_to_tk_image(image_rs)\n        self.background_label.configure(image=image_tk)\n        self.background_label.image = image_tk\n\n    def _generate_heatmap(self):\n        #return util.draw_heatmap_overlay(self.current_state.screenshot_rs, self.grid, alpha=self.heatmap_alpha)\n        return self.current_state.screenshot_rs\n\nif __name__ == ""__main__"":\n    main()\n'"
annotate/annotate_cars.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom common import GridAnnotationWindow\nimport Tkinter\n\ndef main():\n    print(""Loading replay memory..."")\n    memory = replay_memory.ReplayMemory.create_instance_supervised()\n\n    win = GridAnnotationWindow.create(\n        memory,\n        current_anno_attribute_name=""cars_grid"",\n        save_to_fp=""annotations_cars.pickle"",\n        every_nth_example=5\n    )\n    win.brush_size = 3\n    win.autosave_every_nth = 200\n    win.master.wm_title(""Annotate cars"")\n\n    Tkinter.mainloop()\n\nif __name__ == ""__main__"":\n    main()\n'"
annotate/annotate_cars_mirrors.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom common import GridAnnotationWindow\nimport Tkinter\n\ndef main():\n    print(""Loading replay memory..."")\n    memory = replay_memory.ReplayMemory.create_instance_supervised()\n\n    win = GridAnnotationWindow.create(\n        memory,\n        current_anno_attribute_name=""cars_mirrors_grid"",\n        save_to_fp=""annotations_cars_mirrors.pickle"",\n        every_nth_example=5\n    )\n    win.brush_size = 3\n    win.autosave_every_nth = 200\n    win.master.wm_title(""Annotate cars mirrors"")\n\n    Tkinter.mainloop()\n\nif __name__ == ""__main__"":\n    main()\n'"
annotate/annotate_crashables.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom common import GridAnnotationWindow\nimport Tkinter\n\ndef main():\n    print(""Loading replay memory..."")\n    memory = replay_memory.ReplayMemory.create_instance_supervised()\n\n    win = GridAnnotationWindow.create(\n        memory,\n        current_anno_attribute_name=""crashables_grid"",\n        save_to_fp=""annotations_crashables.pickle"",\n        every_nth_example=20\n    )\n    win.brush_size = 3\n    win.autosave_every_nth = 100\n    win.master.wm_title(""Annotate crashables"")\n\n    Tkinter.mainloop()\n\nif __name__ == ""__main__"":\n    main()\n'"
annotate/annotate_current_lane.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom common import GridAnnotationWindow\nimport Tkinter\n\ndef main():\n    print(""Loading replay memory..."")\n    memory = replay_memory.ReplayMemory.create_instance_supervised()\n\n    win = GridAnnotationWindow.create(\n        memory,\n        current_anno_attribute_name=""current_lane_grid"",\n        save_to_fp=""annotations_current_lane.pickle"",\n        every_nth_example=20\n    )\n    win.brush_size = 2\n    win.autosave_every_nth = 100\n    win.master.wm_title(""Annotate current lane"")\n\n    Tkinter.mainloop()\n\nif __name__ == ""__main__"":\n    main()\n'"
annotate/annotate_lanes_same_direction.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom common import GridAnnotationWindow\nimport Tkinter\n\ndef main():\n    print(""Loading replay memory..."")\n    memory = replay_memory.ReplayMemory.create_instance_supervised()\n\n    win = GridAnnotationWindow.create(\n        memory,\n        current_anno_attribute_name=""lanes_same_direction_grid"",\n        save_to_fp=""annotations_lanes_same_direction.pickle"",\n        every_nth_example=20\n    )\n    win.brush_size = 2\n    win.autosave_every_nth = 100\n    win.master.wm_title(""Annotate lanes same direction"")\n\n    Tkinter.mainloop()\n\nif __name__ == ""__main__"":\n    main()\n'"
annotate/annotate_speed_segments.py,0,"b'from __future__ import print_function, division\nimport cv2\nimport numpy as np\nimport sys\nfrom lib import speed as speedlib\n\nif sys.version_info[0] == 3:\n    raw_input = input\n\ndef main():\n    cv2.namedWindow(""segment"", cv2.WINDOW_NORMAL)\n\n    db = speedlib.SpeedSegmentsDatabase.get_instance()\n    for key in db.segments:\n        seg = db.segments[key]\n        cv2.imshow(""segment"", seg.get_image())\n        cv2.waitKey(100)\n        label = raw_input(""Enter label [current label: \'%s\']: "" % (str(seg.label),))\n        seg.label = label\n        db.save()\n\n    print(""Finished."")\n\nif __name__ == ""__main__"":\n    main()\n'"
annotate/annotate_steering_wheel.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom common import GridAnnotationWindow\nimport Tkinter\n\ndef main():\n    print(""Loading replay memory..."")\n    memory = replay_memory.ReplayMemory.create_instance_supervised()\n\n    win = GridAnnotationWindow.create(\n        memory,\n        current_anno_attribute_name=""steering_wheel_grid"",\n        save_to_fp=""annotations_steering_wheel.pickle"",\n        every_nth_example=200\n    )\n    win.brush_size = 2\n    win.autosave_every_nth = 100\n    win.master.wm_title(""Annotate steering wheel"")\n\n    Tkinter.mainloop()\n\nif __name__ == ""__main__"":\n    main()\n'"
annotate/annotate_street_boundaries.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom lib import util\nimport Tkinter\nfrom PIL import Image, ImageTk\nimport numpy as np\nimport cPickle as pickle\nimport time\nimport imgaug as ia\n\nEVERY_NTH = 10\nANNOTATIONS_FP = ""annotations.pickle""\nZOOM_FACTOR = 4\nAUTOSAVE_EVERY = 20\n\ndef main():\n    print(""Loading replay memory..."")\n    memory = replay_memory.ReplayMemory.create_instance_supervised()\n\n    print(""Loading previous annotations..."")\n    annotations = load_annotations()\n    #is_annotated = dict([(str(annotation.idx), True) for annotation in annotations])\n\n    current_state_idx = memory.id_min\n    if annotations is not None:\n        while current_state_idx < memory.id_max:\n            key = str(current_state_idx)\n            if not key in annotations:\n                break\n            current_state_idx += EVERY_NTH\n    print(""ID of first unannotated state: %d"" % (current_state_idx,))\n\n    master = Tkinter.Tk()\n    state = memory.get_state_by_id(current_state_idx)\n    canvas_height = state.screenshot_rs.shape[0] * ZOOM_FACTOR\n    canvas_width = state.screenshot_rs.shape[1] * ZOOM_FACTOR\n    print(""canvas height, width:"", canvas_height, canvas_width)\n    canvas = Tkinter.Canvas(master, width=canvas_width, height=canvas_height)\n    canvas.pack()\n    canvas.focus_set()\n\n    #y = int(canvas_height / 2)\n    #w.create_line(0, y, canvas_width, y, fill=""#476042"")\n    message = Tkinter.Label(master, text=""Click to draw annotation. Press E to switch to eraser mode. Press S to save. Use Numpad +/- for brush size."")\n    message.pack(side=Tkinter.BOTTOM)\n\n    window_state = WindowState(master, canvas, memory, current_state_idx, annotations)\n\n    #canvas.bind(""<Button-1>"", OnPaint(window_state))\n    #master.bind(""<Button-1>"", lambda event: print(event))\n    #master.bind(""<Button-3>"", lambda event: print(""right"", event))\n    #master.bind(""<ButtonPress-1>"", lambda event: print(""press"", event))\n    master.bind(""<B1-Motion>"", OnLeftMouseButton(window_state))\n    #master.bind(""<ButtonRelease-1>"", lambda event: print(""release"", event))\n    master.bind(""<B3-Motion>"", OnRightMouseButton(window_state))\n    canvas.bind(""<e>"", lambda event: window_state.toggle_eraser())\n    canvas.bind(""<s>"", lambda event: window_state.save_annotations(force=True))\n    canvas.bind(""<w>"", lambda event: window_state.toggle_heatmap())\n    canvas.bind(""<Left>"", lambda event: window_state.previous_state(autosave=True))\n    canvas.bind(""<Right>"", lambda event: window_state.next_state(autosave=True))\n    canvas.bind(""<KP_Add>"", lambda event: window_state.increase_brush_size())\n    canvas.bind(""<KP_Subtract>"", lambda event: window_state.decrease_brush_size())\n\n    Tkinter.mainloop()\n\ndef load_annotations():\n    if os.path.isfile(ANNOTATIONS_FP):\n        return pickle.load(open(ANNOTATIONS_FP, ""r""))\n    else:\n        return None\n\nclass WindowState(object):\n    def __init__(self, master, canvas, memory, current_state_idx, annotations):\n        self.master = master\n        self.canvas = canvas\n        self.memory = memory\n        self.current_state_idx = current_state_idx\n        self.annotations = annotations if annotations is not None else dict()\n        self.current_annotation = None\n        self.background_label = None\n\n        self.switch_to_state(self.current_state_idx, autosave=False)\n        self.current_state = memory.get_state_by_id(current_state_idx)\n\n        self.eraser = False\n        self.dirty = False\n        self.brush_size = 3\n        self.heatmap_visible = True\n        self.last_autosave = 0\n\n    @property\n    def grid(self):\n        return self.current_annotation[""street_boundary_grid""]\n\n    def toggle_eraser(self):\n        self.eraser = not self.eraser\n        print(""Eraser set to %s"" % (self.eraser,))\n\n    def toggle_heatmap(self):\n        if self.heatmap_visible:\n            self.set_canvas_background(self.current_state.screenshot_rs)\n        else:\n            self.set_canvas_background(self._generate_heatmap())\n        self.heatmap_visible = not self.heatmap_visible\n\n    def increase_brush_size(self):\n        self.brush_size = np.clip(self.brush_size+1, 1, 100)\n        print(""Increased brush size to %d"" % (self.brush_size,))\n\n    def decrease_brush_size(self):\n        self.brush_size = np.clip(self.brush_size-1, 1, 100)\n        print(""Decreased brush size to %d"" % (self.brush_size,))\n\n    def previous_state(self, autosave):\n        print(""Switching to previous state..."")\n        self.current_state_idx -= EVERY_NTH\n        assert self.current_state_idx >= self.memory.id_min, ""Start of memory reached (%d vs %d)"" % (self.current_state_idx, self.memory.id_min)\n        self.switch_to_state(self.current_state_idx, autosave=autosave)\n\n    def next_state(self, autosave):\n        print(""Switching to next state..."")\n        self.current_state_idx += EVERY_NTH\n        assert self.current_state_idx <= self.memory.id_max, ""End of memory reached (%d vs %d)"" % (self.current_state_idx, self.memory.id_max)\n        self.switch_to_state(self.current_state_idx, autosave=autosave)\n\n    def switch_to_state(self, idx, autosave):\n        print(""Switching to state %d (autosave=%s)..."" % (idx, str(autosave)))\n        self.current_state = self.memory.get_state_by_id(idx)\n        assert self.current_state is not None\n        self.current_state_idx = idx\n\n        if autosave:\n            if self.last_autosave+1 % AUTOSAVE_EVERY == 0:\n                self.save_annotations()\n                self.last_autosave = 0\n            else:\n                self.last_autosave += 1\n\n        key = str(self.current_state_idx)\n        if key in self.annotations:\n            self.current_annotation = self.annotations[key]\n        else:\n            img = self.current_state.screenshot_rs\n            grid = np.zeros((img.shape[0], img.shape[1]), dtype=np.float32)\n            self.current_annotation = {\n                ""idx"": self.current_state_idx,\n                ""from_datetime"": self.current_state.from_datetime,\n                ""screenshot_rs"": self.current_state.screenshot_rs,\n                ""street_boundary_grid"": grid\n            }\n            self.annotations[key] = self.current_annotation\n            print(""This state has not yet been annotated."")\n\n        self.update_annotation_grid(self.grid, initial=True)\n\n    def save_annotations(self, force=False):\n        #print(self.annotations)\n        if self.dirty or force:\n            print(""Saving..."")\n            with open(ANNOTATIONS_FP, ""w"") as f:\n                pickle.dump(self.annotations, f, protocol=-1)\n            self.dirty = False\n            print(""Finished saving."")\n        else:\n            print(""Not saved (not marked dirty)"")\n\n    """"""\n    def redraw_canvas(self):\n        img = generate_canvas_image(self.current_state.screenshot_rs, self.grid)\n        self.canvas.delete(Tkinter.ALL)\n        self.set_canvas_background(self.canvas, img)\n    """"""\n\n    def update_annotation_grid(self, annotation_grid, initial=False):\n        self.current_annotation[""street_boundary_grid""] = annotation_grid\n        #self.redraw_canvas()\n        #img = generate_canvas_image(self.current_state.screenshot_rs, annotation_grid)\n        img_heatmap = self._generate_heatmap()\n        self.set_canvas_background(img_heatmap)\n        self.heatmap_visible = True\n        if not initial:\n            self.dirty = True\n\n    def set_canvas_background(self, image):\n        if self.background_label is None:\n            # initialize background image label (first call)\n            #img = self.current_state.screenshot_rs\n            #bg_img_tk = numpy_to_tk_image(np.zeros(img.shape))\n            img_heatmap = self._generate_heatmap()\n            img_heatmap_rs = ia.imresize_single_image(img_heatmap, (img_heatmap.shape[0]*ZOOM_FACTOR, img_heatmap.shape[1]*ZOOM_FACTOR), interpolation=""nearest"")\n            bg_img_tk = numpy_to_tk_image(img_heatmap_rs)\n            self.background_label = Tkinter.Label(self.canvas, image=bg_img_tk)\n            self.background_label.place(x=0, y=0, relwidth=1, relheight=1, anchor=Tkinter.NW)\n            self.background_label.image = bg_img_tk\n\n        #print(""image size"", image.shape)\n        #print(""image height, width"", image.to_array().shape)\n        image_rs = ia.imresize_single_image(image, (image.shape[0]*ZOOM_FACTOR, image.shape[1]*ZOOM_FACTOR), interpolation=""nearest"")\n        image_tk = numpy_to_tk_image(image_rs)\n        self.background_label.configure(image=image_tk)\n        self.background_label.image = image_tk\n\n    def _generate_heatmap(self):\n        return util.draw_heatmap_overlay(self.current_state.screenshot_rs, self.grid, alpha=0.3)\n\ndef numpy_to_tk_image(image):\n    image_pil = Image.fromarray(image)\n    image_tk = ImageTk.PhotoImage(image_pil)\n    return image_tk\n\nclass OnLeftMouseButton(object):\n    def __init__(self, window_state):\n        self.window_state = window_state\n\n    def __call__(self, event):\n        #canvas = event.widget\n        x = self.window_state.canvas.canvasx(event.x) / ZOOM_FACTOR\n        y = self.window_state.canvas.canvasy(event.y) / ZOOM_FACTOR\n        height, width = self.window_state.current_state.screenshot_rs.shape[0:2]\n        #x = event.x\n        #y = event.y\n        #canvas.delete(Tkinter.ALL)\n\n        grid = self.window_state.grid\n        normal = draw_normal_distribution(height, width, int(x), int(y), self.window_state.brush_size)\n        #normal = np.zeros_like(grid)\n        #normal[int(y)-2:int(y)+2, int(x)-2:int(x)+2] = 1.0\n        if not self.window_state.eraser:\n            #grid = np.clip(grid + normal, 0, 1)\n            grid = np.maximum(grid, normal)\n        else:\n            grid = grid - normal\n        grid = np.clip(grid, 0, 1)\n        self.window_state.update_annotation_grid(grid)\n        #time.sleep(0.1)\n\nclass OnRightMouseButton(object):\n    def __init__(self, window_state):\n        self.window_state = window_state\n\n    def __call__(self, event):\n        x = self.window_state.canvas.canvasx(event.x) / ZOOM_FACTOR\n        y = self.window_state.canvas.canvasy(event.y) / ZOOM_FACTOR\n        height, width = self.window_state.current_state.screenshot_rs.shape[0:2]\n        grid = self.window_state.grid\n        normal = draw_normal_distribution(height, width, int(x), int(y), self.window_state.brush_size)\n        grid = grid - normal\n        grid = np.clip(grid, 0, 1)\n        self.window_state.update_annotation_grid(grid)\n\n""""""\nclass OnToggleEraser(object):\n    def __init__(self, window_state):\n        self.window_state = window_state\n\n    def __call__(self, event):\n        self.window_state.toggle_eraser(save=True)\n\nclass OnPreviousState(object):\n    def __init__(self, window_state):\n        self.window_state = window_state\n\n    def __call__(self, event):\n        self.window_state.previous_state()\n\nclass OnNextState(object):\n    def __init__(self, window_state):\n        self.window_state = window_state\n\n    def __call__(self, event):\n        self.window_state.next_state()\n""""""\n\ndef draw_normal_distribution(height, width, x, y, size):\n    if 0 <= y < height and 0 <= x < width:\n        pad_by = size * 10\n        img = np.zeros((pad_by+height+pad_by, pad_by+width+pad_by), dtype=np.float32)\n        #img = img.pad(img, ((20, 20), (20, 20)))\n        #normal = util.create_2d_gaussian(size=size*2, fwhm=size)\n        normal = util.create_2d_gaussian(size=size*4, sigma=size)\n        #print(normal)\n        normal_h, normal_w = normal.shape\n        normal_hh, normal_wh = normal_h//2, normal_w//2\n        #print(""normal size"", normal.shape)\n        #print(""img.shape"", img.shape)\n        #print(""img[y-normal_hh:y+normal_hh, x-normal_wh:x+normal_wh]"", img[y-normal_hh:y+normal_hh, x-normal_wh:x+normal_wh].shape)\n        y1 = np.clip(y-normal_hh+pad_by, 0, img.shape[0]-1) #-(2*pad_by))\n        y2 = np.clip(y+normal_hh+pad_by, 0, img.shape[0]-1) #-(2*pad_by))\n        x1 = np.clip(x-normal_wh+pad_by, 0, img.shape[1]-1) #-(2*pad_by))\n        x2 = np.clip(x+normal_wh+pad_by, 0, img.shape[1]-1) #-(2*pad_by))\n        if x2 - x1 > 0 and y2 - y1 > 0:\n            img[y1:y2, x1:x2] = normal\n        return img[pad_by:-pad_by, pad_by:-pad_by]\n    else:\n        return np.zeros((height, width), dtype=np.float32)\n\nif __name__ == ""__main__"":\n    main()\n'"
annotate/annotate_street_markings.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom common import GridAnnotationWindow\nimport Tkinter\n\ndef main():\n    print(""Loading replay memory..."")\n    memory = replay_memory.ReplayMemory.create_instance_supervised()\n\n    win = GridAnnotationWindow.create(\n        memory,\n        current_anno_attribute_name=""street_markings_grid"",\n        save_to_fp=""annotations_street_markings.pickle"",\n        every_nth_example=20\n    )\n    win.brush_size = 2\n    win.autosave_every_nth = 100\n    win.master.wm_title(""Annotate street markings"")\n\n    Tkinter.mainloop()\n\nif __name__ == ""__main__"":\n    main()\n'"
annotate/common.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom lib import util\nimport Tkinter\nfrom PIL import Image, ImageTk\nimport numpy as np\nimport cPickle as pickle\nimport time\nimport imgaug as ia\n\ndef numpy_to_tk_image(image):\n    image_pil = Image.fromarray(image)\n    image_tk = ImageTk.PhotoImage(image_pil)\n    return image_tk\n\ndef load_annotations(fp):\n    if os.path.isfile(fp):\n        return pickle.load(open(fp, ""r""))\n    else:\n        return None\n\ndef draw_normal_distribution(height, width, x, y, size):\n    if 0 <= y < height and 0 <= x < width:\n        pad_by = size * 10\n        img = np.zeros((pad_by+height+pad_by, pad_by+width+pad_by), dtype=np.float32)\n        #img = img.pad(img, ((20, 20), (20, 20)))\n        #normal = util.create_2d_gaussian(size=size*2, fwhm=size)\n        normal = util.create_2d_gaussian(size=size*4, sigma=size)\n        #print(normal)\n        normal_h, normal_w = normal.shape\n        normal_hh, normal_wh = normal_h//2, normal_w//2\n        #print(""normal size"", normal.shape)\n        #print(""img.shape"", img.shape)\n        #print(""img[y-normal_hh:y+normal_hh, x-normal_wh:x+normal_wh]"", img[y-normal_hh:y+normal_hh, x-normal_wh:x+normal_wh].shape)\n        y1 = np.clip(y-normal_hh+pad_by, 0, img.shape[0]-1) #-(2*pad_by))\n        y2 = np.clip(y+normal_hh+pad_by, 0, img.shape[0]-1) #-(2*pad_by))\n        x1 = np.clip(x-normal_wh+pad_by, 0, img.shape[1]-1) #-(2*pad_by))\n        x2 = np.clip(x+normal_wh+pad_by, 0, img.shape[1]-1) #-(2*pad_by))\n        if x2 - x1 > 0 and y2 - y1 > 0:\n            img[y1:y2, x1:x2] = normal\n        return img[pad_by:-pad_by, pad_by:-pad_by]\n    else:\n        return np.zeros((height, width), dtype=np.float32)\n\nclass GridAnnotationWindow(object):\n    def __init__(self, master, canvas, memory, current_state_idx, annotations, current_anno_attribute_name, save_to_fp, every_nth_example=10, zoom_factor=4):\n        self.master = master\n        self.canvas = canvas\n        self.memory = memory\n        self.current_state_idx = current_state_idx\n        self.annotations = annotations if annotations is not None else dict()\n        self.current_annotation = None\n        self.background_label = None\n\n        self.eraser = False\n        self.dirty = False\n        self.brush_size = 3\n        self.last_autosave = 0\n        self.heatmap_alpha = 0.3\n        self.heatmap_alphas = (0.1, 0.3)\n        self.current_anno_attribute_name = current_anno_attribute_name\n        self.every_nth_example = every_nth_example\n        self.zoom_factor = zoom_factor\n        self.autosave_every_nth = 20\n        self.save_to_fp = save_to_fp\n\n        self.is_showing_directly_previous_state = False\n        self.directly_previous_state = None\n        self.current_state = None\n        self.switch_to_state(self.current_state_idx, autosave=False)\n        #self.current_state = memory.get_state_by_id(current_state_idx)\n\n    @staticmethod\n    def create(memory, current_anno_attribute_name, save_to_fp, every_nth_example=10, zoom_factor=4):\n        print(""Loading previous annotations..."")\n        annotations = load_annotations(save_to_fp)\n        #is_annotated = dict([(str(annotation.idx), True) for annotation in annotations])\n\n        current_state_idx = memory.id_min\n        if annotations is not None:\n            while current_state_idx < memory.id_max:\n                key = str(current_state_idx)\n                if key not in annotations or current_anno_attribute_name not in annotations[key]:\n                    break\n                current_state_idx += every_nth_example\n        print(""ID of first unannotated state: %d"" % (current_state_idx,))\n\n        master = Tkinter.Tk()\n        state = memory.get_state_by_id(current_state_idx)\n        canvas_height = state.screenshot_rs.shape[0] * zoom_factor\n        canvas_width = state.screenshot_rs.shape[1] * zoom_factor\n        print(""canvas height, width:"", canvas_height, canvas_width)\n        canvas = Tkinter.Canvas(master, width=canvas_width, height=canvas_height)\n        canvas.pack()\n        canvas.focus_set()\n\n        #y = int(canvas_height / 2)\n        #w.create_line(0, y, canvas_width, y, fill=""#476042"")\n        message = Tkinter.Label(master, text=""Click to draw annotation. Press E to switch to eraser mode. Press S to save. Use Numpad +/- for brush size."")\n        message.pack(side=Tkinter.BOTTOM)\n\n        window_state = GridAnnotationWindow(\n            master,\n            canvas,\n            memory,\n            current_state_idx,\n            annotations,\n            current_anno_attribute_name,\n            save_to_fp,\n            every_nth_example,\n            zoom_factor\n        )\n\n        #canvas.bind(""<Button-1>"", OnPaint(window_state))\n        #master.bind(""<Button-1>"", lambda event: print(event))\n        #master.bind(""<Button-3>"", lambda event: print(""right"", event))\n        #master.bind(""<ButtonPress-1>"", lambda event: print(""press"", event))\n        master.bind(""<B1-Motion>"", window_state.on_left_mouse_button)\n        #master.bind(""<ButtonRelease-1>"", lambda event: print(""release"", event))\n        master.bind(""<B3-Motion>"", window_state.on_right_mouse_button)\n        canvas.bind(""<e>"", lambda event: window_state.toggle_eraser())\n        canvas.bind(""<s>"", lambda event: window_state.save_annotations(force=True))\n        canvas.bind(""<w>"", lambda event: window_state.toggle_heatmap())\n        canvas.bind(""<p>"", lambda event: window_state.toggle_previous_screenshot())\n        canvas.bind(""<Left>"", lambda event: window_state.previous_state(autosave=True))\n        canvas.bind(""<Right>"", lambda event: window_state.next_state(autosave=True))\n        canvas.bind(""<KP_Add>"", lambda event: window_state.increase_brush_size())\n        canvas.bind(""<KP_Subtract>"", lambda event: window_state.decrease_brush_size())\n\n        return window_state\n\n    @property\n    def grid(self):\n        return self.current_annotation[self.current_anno_attribute_name]\n\n    def toggle_eraser(self):\n        self.eraser = not self.eraser\n        print(""Eraser set to %s"" % (self.eraser,))\n\n    def toggle_heatmap(self):\n        pos = self.heatmap_alphas.index(self.heatmap_alpha)\n        self.heatmap_alpha = self.heatmap_alphas[(pos+1) % len(self.heatmap_alphas)]\n\n        self.set_canvas_background(self._generate_heatmap())\n\n    def toggle_previous_screenshot(self):\n        if self.directly_previous_state is not None:\n            if self.is_showing_directly_previous_state:\n                self.set_canvas_background(self._generate_heatmap())\n            else:\n                self.set_canvas_background(self.directly_previous_state.screenshot_rs)\n            self.is_showing_directly_previous_state = not self.is_showing_directly_previous_state\n\n    def increase_brush_size(self):\n        self.brush_size = np.clip(self.brush_size+1, 1, 100)\n        print(""Increased brush size to %d"" % (self.brush_size,))\n\n    def decrease_brush_size(self):\n        self.brush_size = np.clip(self.brush_size-1, 1, 100)\n        print(""Decreased brush size to %d"" % (self.brush_size,))\n\n    def previous_state(self, autosave):\n        print(""Switching to previous state..."")\n        self.current_state_idx -= self.every_nth_example\n        assert self.current_state_idx >= self.memory.id_min, ""Start of memory reached (%d vs %d)"" % (self.current_state_idx, self.memory.id_min)\n        self.switch_to_state(self.current_state_idx, autosave=autosave)\n\n    def next_state(self, autosave):\n        print(""Switching to next state..."")\n        self.current_state_idx += self.every_nth_example\n        assert self.current_state_idx <= self.memory.id_max, ""End of memory reached (%d vs %d)"" % (self.current_state_idx, self.memory.id_max)\n        self.switch_to_state(self.current_state_idx, autosave=autosave)\n\n    def switch_to_state(self, idx, autosave):\n        print(""Switching to state %d (autosave=%s)..."" % (idx, str(autosave)))\n        self.directly_previous_state = self.memory.get_state_by_id(idx-1)\n        self.current_state = self.memory.get_state_by_id(idx)\n        assert self.current_state is not None\n        self.current_state_idx = idx\n\n        if autosave:\n            if (self.last_autosave+1) % self.autosave_every_nth == 0:\n                # only autosaves if dirty flag is true, ie any example was changed\n                self.save_annotations()\n                self.last_autosave = 0\n            else:\n                self.last_autosave += 1\n            print(""last_autosave="", self.last_autosave)\n\n        key = str(self.current_state_idx)\n        if key in self.annotations:\n            self.current_annotation = self.annotations[key]\n        else:\n            self.current_annotation = {\n                ""idx"": self.current_state_idx,\n                ""from_datetime"": self.current_state.from_datetime,\n                ""screenshot_rs"": self.current_state.screenshot_rs,\n            }\n            self.annotations[key] = self.current_annotation\n\n        annos_done = [key for key in self.current_annotation.keys() if key not in [""idx"", ""from_datetime"", ""screenshot_rs""]]\n        print(""Annotations added to this state: %s"" % ("", "".join(annos_done)))\n        if self.current_anno_attribute_name not in annos_done:\n            print(""This state has not yet been annotated with \'%s\'."" % (self.current_anno_attribute_name,))\n            img = self.current_state.screenshot_rs\n            empty_grid = np.zeros((img.shape[0], img.shape[1]), dtype=np.float32)\n            self.current_annotation[self.current_anno_attribute_name] = empty_grid\n\n        self.is_showing_directly_previous_state = False\n        self.update_annotation_grid(self.grid, initial=True)\n\n    def save_annotations(self, force=False):\n        #print(self.annotations)\n        if self.dirty or force:\n            print(""Saving..."")\n            with open(self.save_to_fp, ""w"") as f:\n                pickle.dump(self.annotations, f, protocol=-1)\n            self.dirty = False\n            print(""Finished saving."")\n        else:\n            print(""Not saved (not marked dirty)"")\n\n    """"""\n    def redraw_canvas(self):\n        img = generate_canvas_image(self.current_state.screenshot_rs, self.grid)\n        self.canvas.delete(Tkinter.ALL)\n        self.set_canvas_background(self.canvas, img)\n    """"""\n\n    def update_annotation_grid(self, annotation_grid, initial=False):\n        self.current_annotation[self.current_anno_attribute_name] = annotation_grid\n        #self.redraw_canvas()\n        #img = generate_canvas_image(self.current_state.screenshot_rs, annotation_grid)\n        img_heatmap = self._generate_heatmap()\n        self.set_canvas_background(img_heatmap)\n        if not initial:\n            self.dirty = True\n\n    def set_canvas_background(self, image):\n        if self.background_label is None:\n            # initialize background image label (first call)\n            #img = self.current_state.screenshot_rs\n            #bg_img_tk = numpy_to_tk_image(np.zeros(img.shape))\n            img_heatmap = self._generate_heatmap()\n            img_heatmap_rs = ia.imresize_single_image(img_heatmap, (img_heatmap.shape[0]*self.zoom_factor, img_heatmap.shape[1]*self.zoom_factor), interpolation=""nearest"")\n            bg_img_tk = numpy_to_tk_image(img_heatmap_rs)\n            self.background_label = Tkinter.Label(self.canvas, image=bg_img_tk)\n            self.background_label.place(x=0, y=0, relwidth=1, relheight=1, anchor=Tkinter.NW)\n            self.background_label.image = bg_img_tk\n\n        #print(""image size"", image.shape)\n        #print(""image height, width"", image.to_array().shape)\n        image_rs = ia.imresize_single_image(image, (image.shape[0]*self.zoom_factor, image.shape[1]*self.zoom_factor), interpolation=""nearest"")\n        image_tk = numpy_to_tk_image(image_rs)\n        self.background_label.configure(image=image_tk)\n        self.background_label.image = image_tk\n\n    def _generate_heatmap(self):\n        return util.draw_heatmap_overlay(self.current_state.screenshot_rs, self.grid, alpha=self.heatmap_alpha)\n\n    def on_left_mouse_button(self, event):\n        #canvas = event.widget\n        x = self.canvas.canvasx(event.x) / self.zoom_factor\n        y = self.canvas.canvasy(event.y) / self.zoom_factor\n        height, width = self.current_state.screenshot_rs.shape[0:2]\n        #x = event.x\n        #y = event.y\n        #canvas.delete(Tkinter.ALL)\n\n        grid = self.grid\n        normal = draw_normal_distribution(height, width, int(x), int(y), self.brush_size)\n        #normal = np.zeros_like(grid)\n        #normal[int(y)-2:int(y)+2, int(x)-2:int(x)+2] = 1.0\n        if not self.eraser:\n            #grid = np.clip(grid + normal, 0, 1)\n            grid = np.maximum(grid, normal)\n        else:\n            grid = grid - normal\n        grid = np.clip(grid, 0, 1)\n        self.update_annotation_grid(grid)\n        #time.sleep(0.1)\n\n    def on_right_mouse_button(self, event):\n        x = self.canvas.canvasx(event.x) / self.zoom_factor\n        y = self.canvas.canvasy(event.y) / self.zoom_factor\n        height, width = self.current_state.screenshot_rs.shape[0:2]\n        grid = self.grid\n        normal = draw_normal_distribution(height, width, int(x), int(y), self.brush_size)\n        grid = grid - normal\n        grid = np.clip(grid, 0, 1)\n        self.update_annotation_grid(grid)\n'"
lib/__init__.py,0,b''
lib/actions.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport random\nimport numpy as np\n\nACTION_UP = ""W""\nACTION_DOWN = ""S""\nACTION_LEFT = ""A""\nACTION_RIGHT = ""D""\nACTION_UP_DOWN_NONE = ""~WS""\nACTION_LEFT_RIGHT_NONE = ""~AD""\n\nACTIONS_UP_DOWN = [ACTION_UP, ACTION_DOWN, ACTION_UP_DOWN_NONE]\nACTIONS_LEFT_RIGHT = [ACTION_LEFT, ACTION_RIGHT, ACTION_LEFT_RIGHT_NONE]\nALL_ACTIONS = ACTIONS_UP_DOWN + ACTIONS_LEFT_RIGHT\n\nACTION_TO_KEY = {\n    ACTION_UP: ""w"",\n    ACTION_DOWN: ""s"",\n    ACTION_LEFT: ""a"",\n    ACTION_RIGHT: ""d"",\n    ACTION_UP_DOWN_NONE: None,\n    ACTION_LEFT_RIGHT_NONE: None\n}\n\nACTION_UP_VECTOR_INDEX = 0\nACTION_DOWN_VECTOR_INDEX = 1\nACTION_LEFT_VECTOR_INDEX = 0\nACTION_RIGHT_VECTOR_INDEX = 1\nACTION_UP_DOWN_NONE_VECTOR_INDEX = 2\nACTION_LEFT_RIGHT_NONE_VECTOR_INDEX = 2\n\ndef make_one_hot_vector(size, idx_one):\n    vec = np.zeros((size,), dtype=np.int32)\n    vec[idx_one] = 1\n    return vec\n\nACTION_UP_VECTOR = make_one_hot_vector(3, ACTION_UP_VECTOR_INDEX)\nACTION_DOWN_VECTOR = make_one_hot_vector(3, ACTION_DOWN_VECTOR_INDEX)\nACTION_LEFT_VECTOR = make_one_hot_vector(3, ACTION_LEFT_VECTOR_INDEX)\nACTION_RIGHT_VECTOR = make_one_hot_vector(3, ACTION_RIGHT_VECTOR_INDEX)\nACTION_UP_DOWN_NONE_VECTOR = make_one_hot_vector(3, ACTION_UP_DOWN_NONE_VECTOR_INDEX)\nACTION_LEFT_RIGHT_NONE_VECTOR = make_one_hot_vector(3, ACTION_LEFT_RIGHT_NONE_VECTOR_INDEX)\n\nACTION_UP_DOWN_TO_VECTOR_INDEX = {\n    ACTION_UP: ACTION_UP_VECTOR_INDEX,\n    ACTION_DOWN: ACTION_DOWN_VECTOR_INDEX,\n    ACTION_UP_DOWN_NONE: ACTION_UP_DOWN_NONE_VECTOR_INDEX\n}\n\nACTION_LEFT_RIGHT_TO_VECTOR_INDEX = {\n    ACTION_LEFT: ACTION_LEFT_VECTOR_INDEX,\n    ACTION_RIGHT: ACTION_RIGHT_VECTOR_INDEX,\n    ACTION_LEFT_RIGHT_NONE: ACTION_LEFT_RIGHT_NONE_VECTOR_INDEX\n}\n\nACTION_UP_DOWN_FROM_VECTOR_INDEX = [\n    ACTION_UP,\n    ACTION_DOWN,\n    ACTION_UP_DOWN_NONE\n]\n\nACTION_LEFT_RIGHT_FROM_VECTOR_INDEX = [\n    ACTION_LEFT,\n    ACTION_RIGHT,\n    ACTION_LEFT_RIGHT_NONE\n]\n\nACTION_UP_DOWN_TO_VECTOR = {\n    ACTION_UP: ACTION_UP_VECTOR,\n    ACTION_DOWN: ACTION_DOWN_VECTOR,\n    ACTION_UP_DOWN_NONE: ACTION_UP_DOWN_NONE_VECTOR\n}\n\nACTION_LEFT_RIGHT_TO_VECTOR = {\n    ACTION_LEFT: ACTION_LEFT_VECTOR,\n    ACTION_RIGHT: ACTION_RIGHT_VECTOR,\n    ACTION_LEFT_RIGHT_NONE: ACTION_LEFT_RIGHT_NONE_VECTOR\n}\n\nALL_MULTIACTIONS = []\nACTIONS_TO_MULTIVEC = dict()\nACTIONS_FROM_MULTIVEC_INDEX = []\ni = 0\nfor action_up_down in ACTIONS_UP_DOWN:\n    for action_left_right in ACTIONS_LEFT_RIGHT:\n        ALL_MULTIACTIONS.append((action_up_down, action_left_right))\n        vec = np.zeros((len(ACTIONS_UP_DOWN) * len(ACTIONS_LEFT_RIGHT),), dtype=np.float32)\n        vec[i] = 1\n        ACTIONS_TO_MULTIVEC[(action_up_down, action_left_right)] = vec\n        ACTIONS_FROM_MULTIVEC_INDEX.append((action_up_down, action_left_right))\n        i += 1\n\ndef get_random_action_up_down():\n    return random.choice(ACTIONS_UP_DOWN)\n\ndef get_random_action_left_right():\n    return random.choice(ACTIONS_LEFT_RIGHT)\n\ndef keys_to_action_up_down(keys):\n    if keys is None:\n        return ACTION_UP_DOWN_NONE\n    elif ""w"" in keys:\n        return ACTION_UP\n    elif ""s"" in keys:\n        return ACTION_DOWN\n    else:\n        return ACTION_UP_DOWN_NONE\n\ndef keys_to_action_left_right(keys):\n    if keys is None:\n        return ACTION_LEFT_RIGHT_NONE\n    elif ""a"" in keys:\n        return ACTION_LEFT\n    elif ""d"" in keys:\n        return ACTION_RIGHT\n    else:\n        return ACTION_LEFT_RIGHT_NONE\n\ndef action_up_down_to_vector(action):\n    return np.copy(ACTION_UP_DOWN_TO_VECTOR[action])\n\ndef action_left_right_to_vector(action):\n    return np.copy(ACTION_LEFT_RIGHT_TO_VECTOR[action])\n\ndef action_up_down_to_vector_index(action):\n    return np.copy(ACTION_UP_DOWN_TO_VECTOR_INDEX[action])\n\ndef action_left_right_to_vector_index(action):\n    return np.copy(ACTION_LEFT_RIGHT_TO_VECTOR_INDEX[action])\n\ndef action_up_down_from_vector_index(index):\n    return ACTION_UP_DOWN_FROM_VECTOR_INDEX[index]\n\ndef action_left_right_from_vector_index(index):\n    return ACTION_LEFT_RIGHT_FROM_VECTOR_INDEX[index]\n\ndef action_to_key(action):\n    return ACTION_TO_KEY[action]\n\ndef actions_to_multivec(action_up_down, action_left_right):\n    return ACTIONS_TO_MULTIVEC[(action_up_down, action_left_right)]\n'"
lib/ets2game.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\n#import thread\nimport time\nimport ets2window\nimport windowhandling\nimport actions as actionslib\nfrom config import Config\nimport random\n\nclass ETS2Game(object):\n    def __init__(self, win=None):\n        if win is None:\n            while True:\n                win_id = ets2window.find_ets2_window_id()\n                if win_id is None:\n                    print(""[ETS2Game] ETS2 window not found. Expected window with name \'Euro Truck Simulator 2\'. (Not started yet?)"")\n                    time.sleep(1)\n                else:\n                    break\n\n            win = ets2window.ETS2Window(win_id)\n\n            print(""---------------------"")\n            print(""ETS2 window found."")\n            print(""Window ID: %d"" % (win.win_id,))\n            print(""Coordinates:"", win.get_coordinates())\n            print(""Activated:"", win.is_activated())\n            print(""--------------------"")\n\n        self.win = win\n        x1, y1, x2, y2 = win.coordinates\n        h = y2 - y1\n        w = x2 - x1\n        assert w == 1280 and h == 720, ""Detected %dx%d resolution instead of expected 1280x720"" % (h, w)\n\n        self.on_screenshot = None\n        self.on_route_advisor_visible = None\n        self.min_interval = 100 / 1000 # 100ms steps\n        self.timeout_after_screenshot = 0\n        self.tick_idx = 0\n        self.tick_ingame_idx = 0\n        self.tick_ingame_route_advisor_idx = 0\n\n        self.scheduled_actions_of_interval = []\n        self.past_actions_of_interval = []\n\n    def run(self):\n        win_is_activated = False\n\n        while True:\n            time_start = time.time()\n\n            time_start_wa = time.time()\n            if self.tick_idx % 20 == 0 or win_is_activated == False:\n                win_is_activated = self.win.is_activated()\n            time_req_wa = time.time() - time_start_wa\n\n            if not win_is_activated:\n                print(""[ETS2Game.run] ETS2 window not activated (ETS2 win id: %d, active win id: %d)"" % (self.win.win_id, windowhandling.get_active_window_id()))\n                time.sleep(2)\n\n                time_req_scr = 0\n                time_req_onscr = 0\n                time_req_onra = 0\n                time_req_at = 0\n            else:\n                time_start_scr = time.time()\n                scr = self.win.get_image()\n                self.last_scr_time = time_start_scr\n                time_req_scr = time.time() - time_start_scr\n\n                time_start_onscr = time.time()\n                if self.on_screenshot is not None:\n                    self.on_screenshot(self, scr)\n                time_req_scr = time.time() - time_start_onscr\n\n                time_start_onra = time.time()\n                if not self.win.is_route_advisor_visible(scr):\n                    print(""[ETS2Game.run] Route Advisor not visible. When ingame, press F3 to show it."")\n                else:\n                    if self.on_route_advisor_visible is not None:\n                        self.on_route_advisor_visible(self, scr)\n                    self.tick_ingame_route_advisor_idx += 1\n                time_req_onra = time.time() - time_start_onra\n\n                self.tick_ingame_idx += 1\n\n                time_start_at = time.time()\n                self._actions_tick()\n                time_req_at = time.time() - time_start_at\n\n            time_done = time.time() - time_start\n\n            while time.time() - time_start < self.min_interval:\n                time.sleep(1/1000)\n\n            print(""[ETS2Game.run] Step done in %.4fs, finished in %.4fs. (wa %dms, scr %dms, onscr %dms, onra %dms, at %dms)"" % (\n                    time_done,\n                    time.time() - time_start,\n                    int(time_req_wa*1000),\n                    int(time_req_scr*1000),\n                    int(time_req_scr*1000),\n                    int(time_req_onra*1000),\n                    int(time_req_at*1000)\n                )\n            )\n            self.tick_idx += 1\n\n    def reset_actions(self):\n        self.set_actions_of_interval([actionslib.ACTION_UP_DOWN_NONE, actionslib.ACTION_LEFT_RIGHT_NONE])\n        self._actions_tick()\n\n    def set_actions_of_interval(self, actions):\n        self.scheduled_actions_of_interval = actions\n\n    """"""\n    def _end_actions(self):\n        keys = []\n        for action in self.past_actions_of_interval:\n            key = actionslib.action_to_key(action)\n            if key is not None:\n                keys.append(key)\n        self.win.keyup(keyups)\n\n    def _apply_actions(self):\n        keys = []\n        for action in self.scheduled_actions_of_interval:\n            key = actionslib.action_to_key(action)\n            if key is not None:\n                keys.append(key)\n        self.win.keydown(keys)\n        self.past_actions_of_interval = self.scheduled_actions_of_interval\n        self.scheduled_actions_of_interval = []\n    """"""\n\n    def _actions_tick(self):\n        #print(""[_actions_tick] "", self.past_actions_of_interval, ""||"", self.scheduled_actions_of_interval)\n        keysup = set()\n        for action in self.past_actions_of_interval:\n            key = actionslib.action_to_key(action)\n            if key is not None:\n                keysup.add(key)\n\n        keysdown = set()\n        for action in self.scheduled_actions_of_interval:\n            key = actionslib.action_to_key(action)\n            if key is not None:\n                keysdown.add(key)\n\n        self.win.keys(up=keysup, down=keysdown)\n        self.past_actions_of_interval = self.scheduled_actions_of_interval\n        self.scheduled_actions_of_interval = []\n\n    def pause(self):\n        self.scheduled_actions_of_interval = []\n        self.win.keys(press=set([Config.KEY_PAUSE]))\n\n    def unpause(self):\n        self.scheduled_actions_of_interval = []\n        self.win.keys(press=set([Config.KEY_PAUSE]))\n\n    def load_random_save_game(self):\n        self.past_actions_of_interval = []\n        self.scheduled_actions_of_interval = []\n        self.win.keys(press=set([Config.KEY_QUICKLOAD]))\n        time.sleep(5)\n        nth_save = random.randint(0, Config.RELOAD_MAX_SAVEGAME_NUMBER-1) # randint is a<=x<=b\n        i = 0\n        while i < nth_save:\n            print(""Next savegame (switching to %d)"" % (nth_save,))\n            self.win.keys(press=set([""Down""]))\n            time.sleep(1)\n            i += 1\n        time.sleep(3.0)\n        for i in range(60):\n            self.win.keys(press=set([""Return""]))\n            time.sleep(2.0)\n            scr = self.win.get_image()\n            if self.win.is_route_advisor_visible(scr):\n                print(""Route advisor visible, save game apparently loaded."")\n                break\n'"
lib/ets2window.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport windowhandling\nimport screenshot\nimport numpy as np\nfrom scipy import misc, ndimage\nimport cv2\nimport util\nfrom config import Config\nimport time\n\nclass ETS2Window(object):\n    def __init__(self, win_id, coordinates=None):\n        self.win_id = win_id\n        if coordinates is None:\n            self.coordinates = self.get_coordinates()\n        else:\n            self.coordinates = coordinates\n\n        self.offence_ff_image = ndimage.imread(Config.OFFENCE_FF_IMAGE, mode=""RGB"")\n        self.damage_single_digit_image = ndimage.imread(Config.DAMAGE_SINGLE_DIGIT_IMAGE, mode=""RGB"")\n        self.damage_double_digit_image = ndimage.imread(Config.DAMAGE_DOUBLE_DIGIT_IMAGE, mode=""RGB"")\n        self.reverse_image = ndimage.imread(Config.REVERSE_IMAGE, mode=""RGB"")\n\n    def is_activated(self):\n        active_win_id = windowhandling.get_active_window_id()\n        return active_win_id == self.win_id\n\n    def get_coordinates(self):\n        # x1, y1, x2, y2\n        return windowhandling.get_window_coordinates(self.win_id)\n\n    def get_image(self):\n        x1, y1, x2, y2 = self.coordinates\n        return screenshot.make_screenshot(x1=x1, y1=y1, x2=x2, y2=y2)\n\n    def get_speed_image(self, scr=None):\n        scr = scr if scr is not None else self.get_image()\n        return get_speed_image(scr)\n\n    def get_route_advisor_image(self, scr=None):\n        scr = scr if scr is not None else self.get_image()\n        return get_route_advisor_image(scr)\n\n    def is_route_advisor_visible(self, scr, threshold=2):\n        ra = self.get_route_advisor_image(scr)\n        #misc.imshow(ra)\n        #print(""ra_shape"", ra.shape)\n        #assert ra.shape == (9, 3)\n        #ra1d = np.average(ra, axis=2)\n        ra_rows = np.average(ra, axis=1)\n        #print(""ra_rows.shape"", ra_rows.shape)\n        #print(""ra_rows"", ra_rows)\n        expected = np.array([[ 25.33766234,  22.92207792,  21.94805195],\n                    [ 31.79220779,  29.50649351,  28.58441558],\n                    [ 70.32467532,  68.96103896,  68.32467532],\n                    [ 63.51948052,  61.97402597,  61.2987013 ],\n                    [ 66.20779221,  64.72727273,  64.14285714],\n                    [ 64.12987013,  62.51948052,  62.01298701],\n                    [ 60.61038961,  58.94805195,  58.20779221],\n                    [ 65.31168831,  63.74025974,  63.12987013],\n                    [ 18.18181818,  15.66233766,  14.51948052]], dtype=np.float32)\n\n        #print(""expected"", ra_rows)\n        #print(""diff"", ra_rows - expected)\n\n        # evade brightness differences\n        observed_normalized = ra_rows - np.average(ra_rows)\n        expected_normalized = expected - np.average(expected)\n\n        #print(""observed_normalized"", observed_normalized)\n        #print(""expected_normalized"", expected_normalized)\n\n        dist = np.abs(observed_normalized - expected_normalized)\n        dist_avg = np.average(dist)\n        #print(""dist"", dist)\n        #print(""dist_avg"", dist_avg)\n        return dist_avg < threshold\n\n    # quite close scores even for some non-paused images\n    def is_paused(self, scr, threshold=4):\n        """"""\n        Pause mode shows a message at the top center and at\n        the bottom right (above the route advisor)\n        Top center message:\n           Yellowish string ""F1""\n             top left     | x=605 y=164\n             top right    | x=617 y=164\n             bottom left  | x=605 y=174\n             bottom right | x=617 y=174\n           Yellowish string ""Pause""\n             top left     | x=639 y=164\n             top right    | x=675 y=164\n             bottom left  | x=639 y=174\n             bottom right | x=675 y=174\n        Bottom right message\n           Yellowish string ""Advisor""\n             top left     | x=1100, y=433\n             top right    | x=1146, y=433\n             bottom left  | x=1100, y=440\n             bottom right | x=1146, y=440\n        """"""\n        y1 = 430\n        y2 = 439 + 1\n        x1 = 1100\n        x2 = 1148 + 1\n        bt_right_img = scr[y1:y2, x1:x2, :]\n        #misc.imsave(""images/pause_bottom_right_advisor.png"", bt_right_img)\n        #misc.imshow(bt_right_img)\n        #print(np.average(bt_right_img, axis=1))\n        """"""\n        import cv2\n        import matplotlib.pyplot as plt\n        chans = cv2.split(bt_right_img)\n        for col, chan in zip([""red"", ""green"", ""blue""], chans):\n            hist = cv2.calcHist([chan], [0], None, [256], [0, 256])\n\n            print(""chan="", col)\n            print(hist)\n\n            plt.plot(hist, color=col)\n            plt.xlim([0, 256])\n        plt.show()\n        """"""\n        expected = [[  42.40816327,   34.91836735,    3.26530612],\n                    [  42.63265306,   34.57142857,    3.10204082],\n                    [  48.06122449,   37.08163265,    3.28571429],\n                    [ 130.0,          91.02040816,    2.18367347],\n                    [ 130.36734694,   91.14285714,    2.44897959],\n                    [ 112.44897959,   78.85714286,    3.24489796],\n                    [ 131.65306122,   91.57142857,    3.20408163],\n                    [ 120.75510204,   84.30612245,    4.        ],\n                    [ 127.97959184,   89.28571429,    3.59183673],\n                    [ 111.57142857,   78.24489796,    4.69387755]]\n        observed = np.average(bt_right_img, axis=1)\n        expected_normalized = expected - np.average(expected, axis=0)\n        observed_normalized = observed - np.average(observed, axis=0)\n        dist = np.abs(observed_normalized - expected_normalized)\n        dist_avg = np.average(dist)\n        #print(""is_paused"", dist_avg)\n        return dist_avg < threshold\n\n    # seems to also fire for ""You have a new mail""\n    def is_offence_shown(self, scr, threshold=0.97):\n        time_start = time.time()\n        y1 = 584\n        y2 = 591 + 1\n        x1 = 1119\n        x2 = 1180 + 1\n        offence_area = scr[y1:y2, x1:x2, :]\n        x, y, score = util.template_match(needle=self.offence_ff_image, haystack=offence_area)\n        time_req = time.time() - time_start\n        #print(""in %.4fs"" % (time_req,))\n        #print(""is_offence_shown"", x, y, score)\n        #misc.imshow(offence_area)\n        return score >= threshold\n\n    def is_damage_shown(self, scr, threshold=2):\n        time_start = time.time()\n        coords_double = (1092, 567)\n        coords_single = (1095, 567)\n        h_double, w_double = self.damage_double_digit_image.shape[0:2]\n        h_single, w_single = self.damage_single_digit_image.shape[0:2]\n        for (w, h), (x1, y1), expected in zip([(w_double, h_double), (w_single, h_single)], [coords_double, coords_single], [self.damage_double_digit_image, self.damage_single_digit_image]):\n            observed = scr[y1:y1+h, x1:x1+w, :]\n            expected_normalized = expected - np.average(expected, axis=(0, 1))\n            observed_normalized = observed - np.average(observed, axis=(0, 1))\n            dist = np.abs(observed_normalized - expected_normalized)\n            dist_avg = np.average(dist)\n            #print(""is_damage_shown"", dist_avg)\n            if dist_avg < threshold:\n                return True\n        time_req = time.time() - time_start\n        return False\n\n    def is_reverse(self, scr, threshold=2):\n        expected = self.reverse_image\n        x1 = 1070\n        y1 = 481\n        #x2 = 1076\n        #y2 = 489\n        h, w = expected.shape[0:2]\n        observed = scr[y1:y1+h, x1:x1+w, :]\n        expected_normalized = expected - np.average(expected, axis=(0, 1))\n        observed_normalized = observed - np.average(observed, axis=(0, 1))\n        dist = np.abs(observed_normalized - expected_normalized)\n        dist_avg = np.average(dist)\n        #print(""is_reverse"", dist_avg)\n        return dist_avg < threshold\n\n    def keys(self, up=None, press=None, down=None):\n        #windowhandling.sendkeys(self.win_id, up=up, press=press, down=down)\n        windowhandling.sendkeys_pykb(up=up, tap=press, down=down)\n\ndef find_ets2_window_id():\n    win_ids = windowhandling.find_window_ids(""Truck"")\n    # for some reason, there always seem to be listed multiple ETS2 windows\n    # only one of them is correct and has height and width >1,\n    # all other\'s height and width seems to always be 1\n    # so we find here all windows with name ETS2 and pick the one with the highest\n    # size (height*width)\n    candidates = []\n    for win_id in win_ids:\n        winname = windowhandling.get_window_name(win_id)\n        if winname == ""Euro Truck Simulator 2"":\n            #print(""window %d has title \'Euro Truck Simulator 2\'"" % (win_id,))\n            x1, y1, x2, y2 = windowhandling.get_window_coordinates(win_id)\n            h, w = y2 - y1, x2 - x1\n            size = h * w\n            candidates.append((win_id, size))\n    if len(candidates) == 0:\n        return None\n    else:\n        candidates = sorted(candidates, key=lambda t: t[1], reverse=True)\n        best = candidates[0]\n        if best[1] > 1:\n            return best[0]\n        else:\n            return None\n\ndef get_speed_image_coords(scr, mode=""extended"", border=(2, 2, 2, 2)):\n    h, w = scr.shape[0:2]\n    assert w == 1280 and h == 720\n\n    """"""\n    if mode == ""whole"":\n        x1_speed_rel = (994 - border[3]) / 1280\n        x2_speed_rel = (1052 + border[1]) / 1280\n        y1_speed_rel = (477 - border[0]) / 720\n        y2_speed_rel = (494 + border[2]) / 720\n    elif mode == ""small"":\n        x1_speed_rel = (1005 - border[3]) / 1280\n        x2_speed_rel = (1017 + border[1]) / 1280\n        y1_speed_rel = (481 - border[0]) / 720\n        y2_speed_rel = (489 + border[2]) / 720\n\n    x1_speed = int(x1_speed_rel * w)\n    x2_speed = int(x2_speed_rel * w)\n    y1_speed = int(y1_speed_rel * h)\n    y2_speed = int(y2_speed_rel * h)\n    """"""\n\n    if mode == ""whole"":\n        x1 = 994 - border[3]\n        x2 = 1052 + border[1]\n        y1 = 477 - border[0]\n        y2 = 494 + border[2]\n    elif mode == ""extended"":\n        x1 = 997 - border[3]\n        x2 = 1048 + border[1]\n        y1 = 481 - border[0]\n        y2 = 489 + border[2]\n    elif mode == ""small"":\n        x1 = 1005 - border[3]\n        x2 = 1017 + border[1]\n        y1 = 481 - border[0]\n        y2 = 489 + border[2]\n\n    return x1, y1, x2, y2\n\ndef get_route_advisor_image_coords(scr):\n    assert scr.shape[0:2] == (720, 1280)\n    x1 = 1086\n    x2 = 1162\n    y1 = 689\n    y2 = 697\n    return x1, y1, x2, y2\n\ndef get_route_advisor_image(scr):\n    x1, y1, x2, y2 = get_route_advisor_image_coords(scr)\n    return scr[y1:y2+1, x1:x2+1, :]\n\ndef get_speed_image(scr):\n    x1, y1, x2, y2 = get_speed_image_coords(scr)\n    return scr[y1:y2+1, x1:x2+1, :]\n'"
lib/plotting.py,0,"b'""""""Classes to handle plotting during the training.""""""\nfrom __future__ import print_function, division\nimport math\nimport cPickle as pickle\nfrom collections import OrderedDict\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\nGROWTH_BY = 500\n\nclass History(object):\n    def __init__(self):\n        self.line_groups = OrderedDict()\n\n    @staticmethod\n    def from_string(s):\n        return pickle.loads(s)\n\n    def to_string(self):\n        return pickle.dumps(self, protocol=-1)\n\n    @staticmethod\n    def load_from_filepath(fp):\n        #return json.loads(open(, ""r"").read())\n        with open(fp, ""r"") as f:\n            history = pickle.load(f)\n        return history\n\n    def save_to_filepath(self, fp):\n        with open(fp, ""w"") as f:\n            pickle.dump(self, f, protocol=-1)\n\n    def add_group(self, group_name, line_names, increasing=True):\n        self.line_groups[group_name] = LineGroup(group_name, line_names, increasing=increasing)\n\n    def add_value(self, group_name, line_name, x, y, average=False):\n        self.line_groups[group_name].lines[line_name].append(x, y, average=average)\n\n    def get_group_names(self):\n        return list(self.line_groups.iterkeys())\n\n    def get_groups_increasing(self):\n        return [group.increasing for group in self.line_groups.itervalues()]\n\n    def get_max_x(self):\n        return max([group.get_max_x() for group in self.line_groups.itervalues()])\n\n    def get_recent_average(self, group_name, line_name, nb_points):\n        ys = self.line_groups[group_name].lines[line_name].ys[-nb_points:]\n        return np.average(ys)\n\nclass LineGroup(object):\n    def __init__(self, group_name, line_names, increasing=True):\n        self.group_name = group_name\n        self.lines = OrderedDict([(name, Line()) for name in line_names])\n        self.increasing = increasing\n        self.xlim = (None, None)\n\n    def get_line_names(self):\n        return list(self.lines.iterkeys())\n\n    def get_line_xs(self):\n        #return [line.xs for line in self.lines.itervalues()]\n        """"""\n        for key, line in self.lines.items():\n            if not hasattr(line, ""last_index""):\n                print(self.group_name, key, ""no last index"")\n            else:\n                print(self.group_name, key, ""OK"")\n            print(type(line.xs), type(line.ys), type(line.counts), type(line.datetimes))\n        """"""\n        return [line.get_xs() for line in self.lines.itervalues()]\n\n    def get_line_ys(self):\n        #return [line.ys for line in self.lines.itervalues()]\n        return [line.get_ys() for line in self.lines.itervalues()]\n\n    def get_max_x(self):\n        #return max([max(line.xs) if len(line.xs) > 0 else 0 for line in self.lines.itervalues()])\n        return max([np.maximum(line.get_xs()) if line.last_index > -1 else 0 for line in self.lines.itervalues()])\n\n""""""\nclass Line(object):\n    def __init__(self, xs=None, ys=None, counts=None, datetimes=None):\n        self.xs = xs if xs is not None else []\n        self.ys = ys if ys is not None else []\n        self.counts = counts if counts is not None else []\n        self.datetimes = datetimes if datetimes is not None else []\n        self.last_index = -1\n\n    def append(self, x, y, average=False):\n        # legacy (for loading from pickle)\n        #if not hasattr(self, ""counts""):\n        #    self.counts = [1] * len(self.xs)\n        # ---\n\n        if not average or len(self.xs) == 0 or self.xs[-1] != x:\n            self.xs.append(x)\n            self.ys.append(float(y)) # float to get rid of numpy\n            self.counts.append(1)\n            self.datetimes.append(time.time())\n        else:\n            count = self.counts[-1]\n            self.ys[-1] = ((self.ys[-1] * count) + y) / (count+1)\n            self.counts[-1] += 1\n            self.datetimes[-1] = time.time()\n""""""\n\nclass Line(object):\n    def __init__(self, xs=None, ys=None, counts=None, datetimes=None):\n        zeros = np.tile(np.array([0], dtype=np.int32), GROWTH_BY)\n        self.xs = xs if xs is not None else np.copy(zeros)\n        self.ys = ys if ys is not None else zeros.astype(np.float32)\n        self.counts = counts if counts is not None else zeros.astype(np.uint16)\n        self.datetimes = datetimes if datetimes is not None else zeros.astype(np.uint64)\n        self.last_index = -1\n\n    # for legacy as functions, replace with properties\n    def get_xs(self):\n        # legacy\n        if isinstance(self.xs, list):\n            self._legacy_convert_from_list_to_np()\n\n        return self.xs[0:self.last_index+1]\n\n    def get_ys(self):\n        return self.ys[0:self.last_index+1]\n\n    def get_counts(self):\n        return self.counts[0:self.last_index+1]\n\n    def get_datetimes(self):\n        return self.datetimes[0:self.last_index+1]\n\n    def _legacy_convert_from_list_to_np(self):\n        #print(""is list!"")\n        print(""[plotting] Converting from list to numpy..."")\n        self.last_index = len(self.xs) - 1\n        self.xs = np.array(self.xs, dtype=np.int32)\n        self.ys = np.array(self.ys, dtype=np.float32)\n        self.counts = np.array(self.counts, dtype=np.uint16)\n        self.datetimes = np.array([int(dt*1000) for dt in self.datetimes], dtype=np.uint64)\n\n    def append(self, x, y, average=False):\n        # legacy (for loading from pickle)\n        #if not hasattr(self, ""counts""):\n        #    self.counts = [1] * len(self.xs)\n        # ---\n\n        #legacy\n        if isinstance(self.xs, list):\n            self._legacy_convert_from_list_to_np()\n\n        if (self.last_index+1) == self.xs.shape[0]:\n            #print(""growing from %d by %d..."" % (self.xs.shape[0], GROWTH_BY), self.xs.shape, self.ys.shape, self.counts.shape, self.datetimes.shape)\n            zeros = np.tile(np.array([0], dtype=np.int32), GROWTH_BY)\n            self.xs = np.append(self.xs, np.copy(zeros))\n            self.ys = np.append(self.ys, zeros.astype(np.float32))\n            self.counts = np.append(self.counts, zeros.astype(np.uint16))\n            self.datetimes = np.append(self.datetimes, zeros.astype(np.uint64))\n            #print(""growing done"", self.xs.shape, self.ys.shape, self.counts.shape, self.datetimes.shape)\n\n        first_entry = (self.last_index == -1)\n        if not average or first_entry or self.xs[self.last_index] != x:\n            idx = self.last_index + 1\n            self.xs[idx] = x\n            self.ys[idx] = y\n            self.counts[idx] = 1\n            self.datetimes[idx] = int(time.time()*1000)\n            self.last_index = idx\n        else:\n            idx = self.last_index\n            count = self.counts[idx]\n            self.ys[idx] = ((self.ys[idx] * count) + y) / (count+1)\n            self.counts[idx] = count + 1\n            self.datetimes[idx] = int(time.time()*1000)\n\n        #print(""added"", x, y, average)\n        #print(self.xs[self.last_index-10:self.last_index+10+1])\n        #print(self.ys[self.last_index-10:self.last_index+10+1])\n        #print(self.counts[self.last_index-10:self.last_index+10+1])\n        #print(self.datetimes[self.last_index-10:self.last_index+10+1])\n\nclass LossPlotter(object):\n    def __init__(self, titles, increasing, save_to_fp):\n        assert len(titles) == len(increasing)\n        n_plots = len(titles)\n        self.titles = titles\n        self.increasing = dict([(title, incr) for title, incr in zip(titles, increasing)])\n        self.xlim = dict([(title, (None, None)) for title in titles])\n        self.colors = [""red"", ""blue"", ""cyan"", ""magenta"", ""orange"", ""black""]\n\n        self.nb_points_max = 500\n        self.save_to_fp = save_to_fp\n        self.start_batch_idx = 0\n        self.autolimit_y = False\n        self.autolimit_y_multiplier = 5\n\n        #self.fig, self.axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\n        nrows = max(1, int(math.sqrt(n_plots)))\n        ncols = int(math.ceil(n_plots / nrows))\n        width = ncols * 10\n        height = nrows * 10\n\n        self.fig, self.axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(width, height))\n\n        if nrows == 1 and ncols == 1:\n            self.axes = [self.axes]\n        else:\n            self.axes = self.axes.flat\n\n        title_to_ax = dict()\n        for idx, (title, ax) in enumerate(zip(self.titles, self.axes)):\n            title_to_ax[title] = ax\n        self.title_to_ax = title_to_ax\n\n        self.fig.tight_layout()\n        self.fig.subplots_adjust(left=0.05)\n\n    def plot(self, history):\n        for plot_idx, title in enumerate(self.titles):\n            ax = self.title_to_ax[title]\n            group_name = title\n            group_increasing = self.increasing[title]\n            group = history.line_groups[title]\n            line_names = group.get_line_names()\n            #print(""getting line x/y..."", time.time())\n            line_xs = group.get_line_xs()\n            line_ys = group.get_line_ys()\n            #print(""getting line x/y FIN"", time.time())\n\n            """"""\n            print(""title"", title)\n            print(""line_names"", line_names)\n            for i, xx in enumerate(line_xs):\n                print(""line_xs i: "", xx)\n            for i, yy in enumerate(line_ys):\n                print(""line_ys i: "", yy)\n            """"""\n            if any([len(xx) > 0 for xx in line_xs]):\n                xs_min = min([min(xx) for xx in line_xs if len(xx) > 0])\n                xs_max = max([max(xx) for xx in line_xs if len(xx) > 0])\n                xlim = self.xlim[title]\n                xlim = [\n                    max(xs_min, self.start_batch_idx) if xlim[0] is None else min(xlim[0], xs_max-1),\n                    xs_max+1 if xlim[1] is None else xlim[1]\n                ]\n                if xlim[0] < 0:\n                    xlim[0] = max(xs_max - abs(xlim[0]), 0)\n                if xlim[1] < 0:\n                    xlim[1] = max(xs_max - abs(xlim[1]), 1)\n            else:\n                # none of the lines has any value, so just use dummy values\n                # to avoid min/max of empty sequence errors\n                xlim = [\n                    0 if self.xlim[title][0] is None else self.xlim[title][0],\n                    1 if self.xlim[title][1] is None else self.xlim[title][1]\n                ]\n\n            self._plot_group(ax, group_name, group_increasing, line_names, line_xs, line_ys, xlim)\n        self.fig.savefig(self.save_to_fp)\n\n    # this seems to be slow sometimes\n    def _line_to_xy(self, line_x, line_y, xlim, limit_y_min=None, limit_y_max=None):\n        def _add_point(points_x, points_y, curr_sum, counter):\n            points_x.append(batch_idx)\n            y = curr_sum / counter\n            if limit_y_min is not None and limit_y_max is not None:\n                y = np.clip(y, limit_y_min, limit_y_max)\n            elif limit_y_min is not None:\n                y = max(y, limit_y_min)\n            elif limit_y_max is not None:\n                y = min(y, limit_y_max)\n            points_y.append(y)\n\n        nb_points = 0\n        for i in range(len(line_x)):\n            batch_idx = line_x[i]\n            if xlim[0] <= batch_idx < xlim[1]:\n                nb_points += 1\n\n        point_every = max(1, int(nb_points / self.nb_points_max))\n        points_x = []\n        points_y = []\n        curr_sum = 0\n        counter = 0\n        for i in range(len(line_x)):\n            batch_idx = line_x[i]\n            if xlim[0] <= batch_idx < xlim[1]:\n                curr_sum += line_y[i]\n                counter += 1\n                if counter >= point_every:\n                    _add_point(points_x, points_y, curr_sum, counter)\n                    counter = 0\n                    curr_sum = 0\n        if counter > 0:\n            _add_point(points_x, points_y, curr_sum, counter)\n\n        return points_x, points_y\n\n    def _plot_group(self, ax, group_name, group_increasing, line_names, line_xs, line_ys, xlim):\n        ax.cla()\n        ax.grid()\n\n        if self.autolimit_y and any([len(line_xs) > 0 for line_xs in line_xs]):\n            min_x = min([np.min(line_x) for line_x in line_xs])\n            max_x = max([np.max(line_x) for line_x in line_xs])\n            min_y = min([np.min(line_y) for line_y in line_ys])\n            max_y = max([np.max(line_y) for line_y in line_ys])\n\n            if group_increasing:\n                if max_y > 0:\n                    limit_y_max = None\n                    limit_y_min = max_y / self.autolimit_y_multiplier\n                    if min_y > limit_y_min:\n                        limit_y_min = None\n            else:\n                if min_y > 0:\n                    limit_y_max = min_y * self.autolimit_y_multiplier\n                    limit_y_min = None\n                    if max_y < limit_y_max:\n                        limit_y_max = None\n\n            if limit_y_min is not None:\n                ax.plot((min_x, max_x), (limit_y_min, limit_y_min), c=""purple"")\n\n            if limit_y_max is not None:\n                ax.plot((min_x, max_x), (limit_y_max, limit_y_max), c=""purple"")\n\n            # y achse range begrenzen\n            yaxmin = min_y if limit_y_min is None else limit_y_min\n            yaxmax = max_y if limit_y_max is None else limit_y_max\n            yrange = yaxmax - yaxmin\n            yaxmin = yaxmin - (0.05 * yrange)\n            yaxmax = yaxmax + (0.05 * yrange)\n            ax.set_ylim([yaxmin, yaxmax])\n        else:\n            limit_y_min = None\n            limit_y_max = None\n\n        for line_name, line_x, line_y, line_col in zip(line_names, line_xs, line_ys, self.colors):\n            #print(""line to xy..."", time.time())\n            x, y = self._line_to_xy(line_x, line_y, xlim, limit_y_min=limit_y_min, limit_y_max=limit_y_max)\n            #print(""line to xy FIN"", time.time())\n            #print(""plotting ax..."", time.time())\n            ax.plot(x, y, color=line_col, linewidth=1.0)\n            #print(""plotting ax FIN"", time.time())\n\n        ax.set_title(group_name)\n'"
lib/pykeylogger.py,0,"b'# Copyright (c) 2011, Andrew Moffat\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in the\n#       documentation and/or other materials provided with the distribution.\n#     * Neither the name of the <organization> nor the\n#       names of its contributors may be used to endorse or promote products\n#       derived from this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> BE LIABLE FOR ANY\n# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n# from: https://github.com/amoffat/pykeylogger\n\nimport sys\nfrom time import sleep, time\nimport ctypes as ct\nfrom ctypes.util import find_library\n\n\n# linux only!\nassert(""linux"" in sys.platform)\n\n\nx11 = ct.cdll.LoadLibrary(find_library(""X11""))\ndisplay = x11.XOpenDisplay(None)\n\n\n# this will hold the keyboard state.  32 bytes, with each\n# bit representing the state for a single key.\nkeyboard = (ct.c_char * 32)()\n\n# these are the locations (byte, byte value) of special\n# keys to watch\nshift_keys = ((6,4), (7,64))\nmodifiers = {\n    ""left shift"": (6,4),\n    ""right shift"": (7,64),\n    ""left ctrl"": (4,32),\n    ""right ctrl"": (13,2),\n    ""left alt"": (8,1),\n    ""right alt"": (13,16)\n}\nlast_pressed = set()\nlast_pressed_adjusted = set()\nlast_modifier_state = {}\ncaps_lock_state = 0\n\n# key is byte number, value is a dictionary whose\n# keys are values for that byte, and values are the\n# keys corresponding to those byte values\nkey_mapping = {\n    1: {\n        0b00000010: ""<esc>"",\n        0b00000100: (""1"", ""!""),\n        0b00001000: (""2"", ""@""),\n        0b00010000: (""3"", ""#""),\n        0b00100000: (""4"", ""$""),\n        0b01000000: (""5"", ""%""),\n        0b10000000: (""6"", ""^""),\n    },\n    2: {\n        0b00000001: (""7"", ""&""),\n        0b00000010: (""8"", ""*""),\n        0b00000100: (""9"", ""(""),\n        0b00001000: (""0"", "")""),\n        0b00010000: (""-"", ""_""),\n        0b00100000: (""="", ""+""),\n        0b01000000: ""<backspace>"",\n        0b10000000: ""<tab>"",\n    },\n    3: {\n        0b00000001: (""q"", ""Q""),\n        0b00000010: (""w"", ""W""),\n        0b00000100: (""e"", ""E""),\n        0b00001000: (""r"", ""R""),\n        0b00010000: (""t"", ""T""),\n        0b00100000: (""y"", ""Y""),\n        0b01000000: (""u"", ""U""),\n        0b10000000: (""i"", ""I""),\n    },\n    4: {\n        0b00000001: (""o"", ""O""),\n        0b00000010: (""p"", ""P""),\n        0b00000100: (""["", ""{""),\n        0b00001000: (""]"", ""}""),\n        0b00010000: ""<enter>"",\n        #0b00100000: ""<left ctrl>"",\n        0b01000000: (""a"", ""A""),\n        0b10000000: (""s"", ""S""),\n    },\n    5: {\n        0b00000001: (""d"", ""D""),\n        0b00000010: (""f"", ""F""),\n        0b00000100: (""g"", ""G""),\n        0b00001000: (""h"", ""H""),\n        0b00010000: (""j"", ""J""),\n        0b00100000: (""k"", ""K""),\n        0b01000000: (""l"", ""L""),\n        0b10000000: ("";"", "":""),\n    },\n    6: {\n        0b00000001: (""\'"", ""\\""""),\n        0b00000010: (""`"", ""~""),\n        #0b00000100: ""<left shift>"",\n        0b00001000: (""\\\\"", ""|""),\n        0b00010000: (""z"", ""Z""),\n        0b00100000: (""x"", ""X""),\n        0b01000000: (""c"", ""C""),\n        0b10000000: (""v"", ""V""),\n    },\n    7: {\n        0b00000001: (""b"", ""B""),\n        0b00000010: (""n"", ""N""),\n        0b00000100: (""m"", ""M""),\n        0b00001000: ("","", ""<""),\n        0b00010000: (""."", "">""),\n        0b00100000: (""/"", ""?""),\n        #0b01000000: ""<right shift>"",\n    },\n    8: {\n        #0b00000001: ""<left alt>"",\n        0b00000010: "" "",\n        0b00000100: ""<caps lock>"",\n    },\n    13: {\n        #0b00000010: ""<right ctrl>"",\n        #0b00010000: ""<right alt>"",\n    },\n}\n\n\n\n\ndef fetch_keys_raw():\n    x11.XQueryKeymap(display, keyboard)\n    return keyboard\n\n\n\ndef fetch_keys():\n    global caps_lock_state, last_pressed, last_pressed_adjusted, last_modifier_state\n    keypresses_raw = fetch_keys_raw()\n\n\n    # check modifier states (ctrl, alt, shift keys)\n    modifier_state = {}\n    for mod, (i, byte) in modifiers.iteritems():\n        modifier_state[mod] = bool(ord(keypresses_raw[i]) & byte)\n\n    # shift pressed?\n    shift = 0\n    for i, byte in shift_keys:\n        if ord(keypresses_raw[i]) & byte:\n            shift = 1\n            break\n\n    # caps lock state\n    if ord(keypresses_raw[8]) & 4: caps_lock_state = int(not caps_lock_state)\n\n\n    # aggregate the pressed keys\n    pressed = []\n    for i, k in enumerate(keypresses_raw):\n        o = ord(k)\n        if o:\n            for byte,key in key_mapping.get(i, {}).iteritems():\n                if byte & o:\n                    if isinstance(key, tuple): key = key[shift or caps_lock_state]\n                    pressed.append(key)\n\n\n    tmp = pressed\n    pressed = list(set(pressed).difference(last_pressed))\n    state_changed = tmp != last_pressed and (pressed or last_pressed_adjusted)\n    last_pressed = tmp\n    last_pressed_adjusted = pressed\n\n    if pressed: pressed = pressed[0]\n    else: pressed = None\n\n\n    state_changed = last_modifier_state and (state_changed or modifier_state != last_modifier_state)\n    last_modifier_state = modifier_state\n\n    return state_changed, modifier_state, pressed\n\ndef fetch_keydowns():\n    global caps_lock_state\n    keypresses_raw = fetch_keys_raw()\n\n\n    # check modifier states (ctrl, alt, shift keys)\n    modifier_state = {}\n    for mod, (i, byte) in modifiers.iteritems():\n        modifier_state[mod] = bool(ord(keypresses_raw[i]) & byte)\n\n    # shift pressed?\n    shift = 0\n    for i, byte in shift_keys:\n        if ord(keypresses_raw[i]) & byte:\n            shift = 1\n            break\n\n    # caps lock state\n    if ord(keypresses_raw[8]) & 4: caps_lock_state = int(not caps_lock_state)\n\n\n    # aggregate the pressed keys\n    pressed = []\n    for i, k in enumerate(keypresses_raw):\n        o = ord(k)\n        if o:\n            for byte,key in key_mapping.get(i, {}).iteritems():\n                if byte & o:\n                    if isinstance(key, tuple): key = key[shift or caps_lock_state]\n                    pressed.append(key)\n\n    return modifier_state, pressed\n\n\ndef log(done, callback, sleep_interval=.005):\n    while not done():\n        sleep(sleep_interval)\n        changed, modifiers, keys = fetch_keys()\n        if changed: callback(time(), modifiers, keys)\n\n\n\n\nif __name__ == ""__main__"":\n    now = time()\n    done = lambda: time() > now + 60\n    def print_keys(t, modifiers, keys): print ""%.2f   %r   %r"" % (t, keys, modifiers)\n\n    """"""Sample output:\n    1314238675.42   \'o\'   {\'left shift\': False, \'right alt\': False, \'right shift\': False, \'left alt\': False, \'left ctrl\': False, \'right ctrl\': False}\n    1314238675.51   \'m\'   {\'left shift\': False, \'right alt\': False, \'right shift\': False, \'left alt\': False, \'left ctrl\': False, \'right ctrl\': False}\n    1314238675.65   \'g\'   {\'left shift\': False, \'right alt\': False, \'right shift\': False, \'left alt\': False, \'left ctrl\': False, \'right ctrl\': False}\n    """"""\n    log(done, print_keys)\n'"
lib/replay_memory.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport sqlite3\nimport states\nfrom config import Config\nimport random\nimport numpy as np\n\nclass ReplayMemory(object):\n    #instances = dict()\n\n    def __init__(self, filepath, max_size, max_size_tolerance):\n        self.filepath = filepath\n        self.conn = sqlite3.connect(filepath, detect_types=sqlite3.PARSE_DECLTYPES)\n        self.max_size = max_size\n        self.max_size_tolerance = max_size_tolerance\n\n        self.conn.execute(""""""\n            CREATE TABLE IF NOT EXISTS states (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                from_datetime TIMESTAMP NOT NULL,\n                screenshot_rs_jpg BLOB NOT NULL,\n                speed INTEGER,\n                is_reverse INTEGER(1),\n                is_offence_shown INTEGER(1),\n                is_damage_shown INTEGER(1),\n                reward REAL NOT NULL,\n                action_left_right VARCHAR(3),\n                action_up_down VARCHAR(3),\n                p_explore REAL NOT NULL,\n                steering_wheel REAL,\n                steering_wheel_raw_one REAL,\n                steering_wheel_raw_two REAL,\n                steering_wheel_cnn REAL,\n                steering_wheel_raw_cnn REAL\n            )\n        """""")\n\n        self.size = 0\n        self.nb_states_added = 0\n        self.id_min = None\n        self.id_max = None\n        self.update_caches()\n\n    """"""\n    @staticmethod\n    def get_instance(name, filepath, max_size, max_size_tolerance):\n        if not name in ReplayMemory.instances:\n            ReplayMemory.instances[name] = ReplayMemory(filepath=filepath, max_size=max_size, max_size_tolerance=max_size_tolerance)\n        return ReplayMemory.instances[name]\n\n    @staticmethod\n    def get_instance_supervised():\n        return ReplayMemory.get_instance(\n            name=""supervised"",\n            filepath=Config.REPLAY_MEMORY_SUPERVISED_FILEPATH,\n            max_size=Config.REPLAY_MEMORY_SUPERVISED_MAX_SIZE,\n            max_size_tolerance=Config.REPLAY_MEMORY_SUPERVISED_MAX_SIZE_TOLERANCE\n        )\n\n    @staticmethod\n    def get_instance_reinforced():\n        return ReplayMemory.get_instance(\n            name=""reinforced"",\n            filepath=Config.REPLAY_MEMORY_REINFORCED_FILEPATH,\n            max_size=Config.REPLAY_MEMORY_REINFORCED_MAX_SIZE,\n            max_size_tolerance=Config.REPLAY_MEMORY_REINFORCED_MAX_SIZE_TOLERANCE\n        )\n    """"""\n\n    @staticmethod\n    def create_instance_supervised(val=False):\n        return ReplayMemory.create_instance_by_config(""supervised%s"" % (""-val"" if val else ""-train"",))\n\n    @staticmethod\n    def create_instance_reinforced(val=False):\n        return ReplayMemory.create_instance_by_config(""reinforced%s"" % (""-val"" if val else ""-train"",))\n\n    @staticmethod\n    def create_instance_by_config(cfg_name):\n        return ReplayMemory(\n            filepath=Config.REPLAY_MEMORY_CFGS[cfg_name][""filepath""],\n            max_size=Config.REPLAY_MEMORY_CFGS[cfg_name][""max_size""],\n            max_size_tolerance=Config.REPLAY_MEMORY_CFGS[cfg_name][""max_size_tolerance""]\n        )\n\n    def add_states(self, states, shrink=True):\n        for state in states:\n            self.add_state(state, commit=False, shrink=False)\n        if shrink:\n            self.shrink_to_max_size()\n        self.commit()\n\n    def add_state(self, state, commit=True, shrink=True):\n        idx = self.id_max + 1 if self.id_max is not None else 1\n        from_datetime = state.from_datetime\n        #scr_rs = state.screenshot_rs\n        scr_rs_jpg = sqlite3.Binary(state.screenshot_rs_jpg)\n        speed = state.speed\n        is_reverse = state.is_reverse\n        is_offence_shown = state.is_offence_shown\n        is_damage_shown = state.is_damage_shown\n        reward = state.reward\n        action_left_right = state.action_left_right\n        action_up_down = state.action_up_down\n        p_explore = state.p_explore\n        steering_wheel_classical = state.steering_wheel_classical\n        steering_wheel_raw_one_classical = state.steering_wheel_raw_one_classical\n        steering_wheel_raw_two_classical = state.steering_wheel_raw_two_classical\n        steering_wheel_cnn = state.steering_wheel_cnn\n        steering_wheel_raw_cnn = state.steering_wheel_raw_cnn\n\n        stmt = """"""\n        INSERT INTO states\n               (id, from_datetime, screenshot_rs_jpg, speed, is_reverse, is_offence_shown, is_damage_shown, reward, action_left_right, action_up_down, p_explore, steering_wheel, steering_wheel_raw_one, steering_wheel_raw_two, steering_wheel_cnn, steering_wheel_raw_cnn)\n        VALUES ( ?,             ?,                 ?,     ?,          ?,                ?,               ?,      ?,                 ?,              ?,         ?,              ?,                      ?,                      ?,                  ?,                      ?)\n        """"""\n\n        assert isinstance(action_left_right, str)\n        assert isinstance(action_up_down, str)\n        self.conn.execute(stmt, (\n            idx, from_datetime, scr_rs_jpg,\n            speed, is_reverse, is_offence_shown, is_damage_shown, reward,\n            action_left_right, action_up_down, p_explore,\n            steering_wheel_classical, steering_wheel_raw_one_classical, steering_wheel_raw_two_classical,\n            steering_wheel_cnn, steering_wheel_raw_cnn\n        ))\n        self.id_max = idx\n        self.size += 1\n        self.nb_states_added += 1\n        #print(""[ReplayMemory] Added state (new size: %d)"" % (self.size,))\n\n        if commit:\n            self.commit()\n        if shrink:\n            self.shrink_to_max_size()\n\n    def update_state(self, idx, state, commit=True):\n        new_idx = idx\n        old_idx = state.idx\n        from_datetime = state.from_datetime\n        scr_rs_jpg = sqlite3.Binary(state.screenshot_rs_jpg)\n        speed = state.speed\n        is_reverse = state.is_reverse\n        is_offence_shown = state.is_offence_shown\n        is_damage_shown = state.is_damage_shown\n        reward = state.reward\n        action_left_right = state.action_left_right\n        action_up_down = state.action_up_down\n        p_explore = state.p_explore\n        steering_wheel_classical = state.steering_wheel_classical\n        steering_wheel_raw_one_classical = state.steering_wheel_raw_one_classical\n        steering_wheel_raw_two_classical = state.steering_wheel_raw_two_classical\n        steering_wheel_cnn = state.steering_wheel_cnn\n        steering_wheel_raw_cnn = state.steering_wheel_raw_cnn\n\n        stmt = """"""\n        UPDATE states\n        SET id=?,\n            from_datetime=?,\n            screenshot_rs_jpg=?,\n            speed=?,\n            is_reverse=?,\n            is_offence_shown=?,\n            is_damage_shown=?,\n            reward=?,\n            action_left_right=?,\n            action_up_down=?,\n            p_explore=?,\n            steering_wheel=?,\n            steering_wheel_raw_one=?,\n            steering_wheel_raw_two=?,\n            steering_wheel_cnn=?,\n            steering_wheel_raw_cnn=?\n        WHERE id=?\n        """"""\n        assert isinstance(action_left_right, str)\n        assert isinstance(action_up_down, str)\n        self.conn.execute(stmt, (\n            new_idx, from_datetime, scr_rs_jpg,\n            speed, is_reverse, is_offence_shown, is_damage_shown, reward,\n            action_left_right, action_up_down, p_explore,\n            steering_wheel_classical, steering_wheel_raw_one_classical, steering_wheel_raw_two_classical,\n            steering_wheel_cnn, steering_wheel_raw_cnn,\n            old_idx))\n\n        if commit:\n            self.commit()\n\n    def get_state_by_id(self, idx):\n        cur = self.conn.cursor()\n        cur.execute(""""""\n            SELECT\n                id, from_datetime, screenshot_rs_jpg, speed, is_reverse,\n                is_offence_shown, is_damage_shown, reward,\n                action_up_down, action_left_right, p_explore,\n                steering_wheel, steering_wheel_raw_one, steering_wheel_raw_two,\n                steering_wheel_cnn, steering_wheel_raw_cnn\n            FROM states\n            WHERE id=?\n        """""", (idx,))\n        rows = cur.fetchall()\n        result = []\n        for row in rows:\n            result.append(states.State.from_row(row))\n        if len(result) == 0:\n            return None\n        else:\n            assert len(result) == 1\n            return result[0]\n\n    def get_states_by_ids(self, ids):\n        id_to_pos = dict([(idx, i) for i, idx in enumerate(ids)])\n\n        cur = self.conn.cursor()\n        cur.execute(""""""\n            SELECT\n                id, from_datetime, screenshot_rs_jpg, speed, is_reverse,\n                is_offence_shown, is_damage_shown, reward,\n                action_up_down, action_left_right, p_explore,\n                steering_wheel, steering_wheel_raw_one, steering_wheel_raw_two,\n                steering_wheel_cnn, steering_wheel_raw_cnn\n            FROM states\n            WHERE id IN (%s)\n        """""" % ("", "".join([str(idx) for idx in ids]),))\n        rows = cur.fetchall()\n\n\n        result = [None] * len(ids)\n        for row in rows:\n            state = states.State.from_row(row)\n            pos = id_to_pos[state.idx]\n            result[pos] = state\n        return result\n\n    def get_random_states(self, count):\n        assert self.size > 0\n        id_min = self.id_min\n        id_max = self.id_max\n        ids = [str(v) for v in np.random.randint(id_min, id_max, size=(count,))]\n        cur = self.conn.cursor()\n        cur.execute(""""""\n            SELECT\n                id, from_datetime, screenshot_rs_jpg, speed, is_reverse,\n                is_offence_shown, is_damage_shown, reward,\n                action_up_down, action_left_right, p_explore,\n                steering_wheel, steering_wheel_raw_one, steering_wheel_raw_two,\n                steering_wheel_cnn, steering_wheel_raw_cnn\n            FROM states\n            WHERE id IN (%s)\n        """""" % ("", "".join(ids)))\n        rows = cur.fetchall()\n        result = []\n        for row in rows:\n            result.append(states.State.from_row(row))\n        while len(result) < count:\n            result.append(random.choice(result))\n        return result\n\n    def get_states_range(self, pos_start, length):\n        assert self.id_min is not None\n        assert pos_start is not None\n        id_start = self.id_min + pos_start\n        id_end = id_start + length\n        #print(""[get_states_range] pos_start"", pos_start, ""length"", length, ""id_start"", id_start, ""id_end"", id_end)\n        cur = self.conn.cursor()\n        cur.execute(""""""\n            SELECT\n                id, from_datetime, screenshot_rs_jpg, speed, is_reverse,\n                is_offence_shown, is_damage_shown, reward,\n                action_up_down, action_left_right, p_explore,\n                steering_wheel, steering_wheel_raw_one, steering_wheel_raw_two,\n                steering_wheel_cnn, steering_wheel_raw_cnn\n            FROM states\n            WHERE id >= ? and id < ?\n            ORDER BY id ASC\n        """""", (id_start, id_end))\n        rows = cur.fetchall()\n        result = []\n        for row in rows:\n            result.append(states.State.from_row(row))\n        #print(""[get_states_range]"", self.filepath, [state.idx for state in result])\n        assert len(result) == length, ""Wrong number of states found: pos_start=%d length=%d id_start=%d id_end=%d id_min=%d id_max=%d size=%d"" % (pos_start, length, id_start, id_end, self.id_min, self.id_max, self.size)\n        return result\n\n    def get_random_state_chain(self, length, from_pos_range=None):\n        #print(""[get_random_state_chain] from_pos_range"", from_pos_range)\n        if length > self.size:\n            raise Exception(""Requested state chain length is larger than size of replay memory. Gather more experiences/states."")\n\n        from_pos_range = list(from_pos_range) if from_pos_range is not None else [0, self.size]\n        if from_pos_range[0] is None:\n            from_pos_range[0] = 0\n        #else:\n        #    from_pos_range[0] = from_pos_range[0] if from_pos_range[0] >= self.id_min else self.id_min\n        if from_pos_range[1] is None:\n            from_pos_range[1] = self.size\n        else:\n            from_pos_range[1] = from_pos_range[1] if from_pos_range[1] <= self.size else self.size\n        if length > (from_pos_range[1] - from_pos_range[0]):\n            raise Exception(""Requested state chain length is larger than allowed id range."")\n\n        start_pos_min = from_pos_range[0]\n        start_pos_max = from_pos_range[1] - length\n        start_pos = random.randint(start_pos_min, start_pos_max)\n        #print(""get_random_state_chain"", self.filepath, start_pos, length, start_pos_min, start_pos_max)\n        return self.get_states_range(pos_start=start_pos, length=length)\n\n    def get_random_state_chain_timediff(self, length, max_timediff_ms=500, depth=50):\n        states = self.get_random_state_chain(length)\n        maxdiff = 0\n        last_time = states[0].from_datetime\n        for state in states[1:]:\n            if last_time < state.from_datetime:\n                timediff = state.from_datetime - last_time\n                timediff = timediff.total_seconds() * 1000\n            else:\n                print(""[WARNING] load_random_state_chain: state from datetime %s is after state %s, expected reversed order"" % (last_time, state.from_datetime))\n                timediff = max_timediff_ms + 1\n            maxdiff = max(timediff, maxdiff)\n            last_time = state.from_datetime\n        #print(""maxdiff:"", maxdiff)\n        if maxdiff > max_timediff_ms:\n            if depth == 0:\n                print(""[WARNING] reached max depth in load_random_state_chain"", from_pos_range)\n                return states\n            else:\n                return self.get_random_state_chain_timediff(length, max_timediff_ms=max_timediff_ms, depth=depth-1)\n        else:\n            return states\n\n    def update_caches(self):\n        cur = self.conn.cursor()\n        cur.execute(""SELECT COUNT(*) as c FROM states"")\n        row = cur.fetchone()\n        count = row[0]\n\n        if count > 0:\n            cur = self.conn.cursor()\n            cur.execute(""SELECT MIN(id) as id_min, MAX(id) as id_max FROM states"")\n            row = cur.fetchone()\n            id_min = row[0]\n            id_max = row[1]\n        else:\n            id_min = None\n            id_max = None\n\n        self.size = count\n        self.id_min = id_min\n        self.id_max = id_max\n\n    def is_above_tolerance(self):\n        return self.size > (self.max_size + self.max_size_tolerance)\n\n    def shrink_to_max_size(self, force=False):\n        is_over_size = (self.size > self.max_size)\n        #is_over_tolerance = (self.size > self.max_size + self.max_size_tolerance)\n        if self.is_above_tolerance() or (is_over_size and force):\n            print(""[ReplayMemory] Shrink to size (from %d to %d)"" % (self.size, self.max_size))\n            diff = self.size - self.max_size\n            del_start = self.id_min\n            del_end = self.id_min + diff\n            cur = self.conn.cursor()\n            cur.execute(""DELETE FROM states WHERE id >= ? AND id < ?"", (del_start, del_end))\n            self.commit()\n            #self.size -= diff\n            #self.id_min += diff\n            self.update_caches()\n            print(""[ReplayMemory] New size is %d"" % (self.size,))\n\n    def commit(self):\n        self.conn.commit()\n\n    def close(self):\n        self.conn.close()\n\n    def connect(self):\n        self.conn = sqlite3.connect(self.filepath, detect_types=sqlite3.PARSE_DECLTYPES)\n'"
lib/rewards.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom config import Config\nimport numpy as np\n\ndef calculate_reward(state, next_state):\n    if next_state.speed is not None:\n        next_speed = next_state.speed\n    elif state.speed is not None:\n        next_speed = state.speed\n    else:\n        next_speed = 0\n    return calculate_reward_raw(next_speed, next_state.is_reverse, next_state.is_offence_shown, next_state.is_damage_shown)\n\ndef calculate_reward_raw(next_speed, next_is_reverse, next_is_offence_shown, next_is_damage_shown):\n    assert next_speed is not None\n    #print(next_speed, next_is_reverse, next_is_offence_shown, next_is_damage_shown)\n    #if next_is_damage_shown:\n    #    return Config.MIN_REWARD\n    #else:\n    #speed_reward = Config.MAX_SPEED - (Config.MAX_SPEED / (1 + 0.1*np.clip(next_speed, 0, Config.MAX_SPEED))) - 20\n    #speed_reward = 100 * (speed_reward / Config.MAX_SPEED)\n    #lowest = (-20/Config.MAX_SPEED)*100\n\n    speed_reward = next_speed\n    lowest = 0\n\n    speed_reward = np.clip(speed_reward, lowest, 100)\n    if next_is_reverse:\n        if speed_reward > 0:\n            speed_reward *= 0.25\n\n    #offence_reward = -50 if next_is_offence_shown else 0\n    offence_reward = -10 if next_is_offence_shown else 0\n    damage_reward = -50 if next_is_damage_shown else 0\n\n    reward = speed_reward + offence_reward + damage_reward\n    return np.clip(reward, Config.MIN_REWARD, Config.MAX_REWARD)\n\n""""""\nclass GridToRewardConverter(object):\n    def __init__(self, gamma=Config.GAMMA):\n        bins = Config.MODEL_NB_REWARD_BINS\n        blocks = Config.MODEL_NB_FUTURE_BLOCKS\n        block_sizes = Config.MODEL_FUTURE_BLOCK_SIZE\n\n        # weight all bins the same way (might also give more weight to negatives)\n        self.base_weighting = np.ones((blocks, bins), dtype=np.float32)\n        #print(""self.base_weighting"", self.base_weighting)\n\n        # average value of each bin\n        bin_averages = []\n        bin_start = Config.MAX_REWARD\n        bin_size = (Config.MAX_REWARD - Config.MIN_REWARD) / Config.MODEL_NB_REWARD_BINS\n        for i in range(Config.MODEL_NB_REWARD_BINS):\n            bin_end = bin_start - bin_size\n            bin_avg = (bin_end + bin_start) / 2\n            bin_averages.append(bin_avg)\n            bin_start = bin_end\n        self.bin_averages = np.tile(np.array(bin_averages)[np.newaxis, :], (blocks, 1))\n        #print(""self.bin_averages"", self.bin_averages)\n\n        # correct for block sizes\n        self.block_size_correct = np.tile(np.array(block_sizes, dtype=np.float32)[:, np.newaxis], (1, bins))\n        #print(""self.block_size_correct"", self.block_size_correct)\n\n        # gamma influence (weight future rewards less)\n        # for each block of N states, calculate the gamma value of each\n        # state and take their average\n        gammas = []\n        gamma_current = 1\n        for bsz in block_sizes:\n            gammas_block = []\n            for i in range(bsz):\n                gammas_block.append(gamma_current)\n                gamma_current = gamma_current * gamma\n            gammas.append(np.average(gammas_block))\n\n            #gamma_end = gamma_start * (gamma ** bsz)\n            #gamma_avg = (gamma_start + gamma_end) / 2\n            #gammas.append(gamma_avg)\n            #gamma_start = gamma_end * gamma\n        self.gammas = np.tile(np.array(gammas, dtype=np.float32)[:, np.newaxis], (1, bins))\n        #print(""self.gammas"", self.gammas)\n\n        self.weighting = self.base_weighting * self.bin_averages * self.block_size_correct * self.gammas\n        #print(""self.weighting"", self.weighting)\n\n    def __call__(self, output_grid):\n        return np.sum(output_grid * self.weighting)\n""""""\n\nif __name__ == ""__main__"":\n    import matplotlib.pyplot as plt\n    speeds = np.arange(150)\n    ys = []\n    for speed in speeds:\n        ys.append(calculate_reward_raw(speed, False, False, False))\n    plt.plot(speeds, ys)\n    plt.show()\n'"
lib/screenshot.py,0,"b'# code largely from\n#   http://stackoverflow.com/questions/69645/take-a-screenshot-via-a-python-script-linux\nfrom __future__ import print_function, division\nimport ctypes\nimport os\nfrom PIL import Image\nimport numpy as np\nimport time\nfrom scipy import misc\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\nLIB_NAME = ""screenshot_c.so"" # naming this just ""screenshot.so"" causes problem for ""import screenshot"" (seems to prefer the .so over .py?)\nABS_LIB_PATH = os.path.join(CURRENT_DIR, LIB_NAME)\nSCREENSHOT_LIB = ctypes.CDLL(ABS_LIB_PATH)\n\ndef make_screenshot(x1, y1, x2, y2):\n    #w, h = x1+x2, y1+y2\n    w = x2 - x1\n    h = y2 - y1\n    size = w * h\n    objlength = size * 3\n\n    SCREENSHOT_LIB.getScreen.argtypes = []\n    result = (ctypes.c_ubyte * objlength)()\n\n    SCREENSHOT_LIB.getScreen(x1, y1, w, h, result)\n    #return Image.frombuffer(\'RGB\', (w, h), result, \'raw\', \'RGB\', 0, 1)\n    img_flat = np.frombuffer(result, dtype=np.uint8)\n    return img_flat.reshape((h, w, 3))\n\nif __name__ == \'__main__\':\n    start_time = time.time()\n    im = make_screenshot(100, 100, 320, 320)\n    im = np.array(im)\n    end_time = time.time()\n    print(""Made screenshot in %.4fs"" % (end_time - start_time,))\n    print(im.ravel()[0:50], im.shape)\n    misc.imshow(im)\n'"
lib/speed.py,0,"b'""""""See ets2window.py for reading out the speed-o-meter from screen.""""""\nfrom __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom config import Config\nfrom skimage import morphology\nimport numpy as np\nimport cv2\nimport os\n\nclass SpeedSegmentsDatabase(object):\n    instance = None\n\n    def __init__(self, filepath=Config.SPEED_IMAGE_SEGMENTS_DB_FILEPATH):\n        self.filepath = filepath\n        self.segments = SpeedSegmentsDatabase.load_dict_from_file(filepath)\n\n    @staticmethod\n    def get_instance():\n        if SpeedSegmentsDatabase.instance is None:\n            SpeedSegmentsDatabase.instance = SpeedSegmentsDatabase()\n        return SpeedSegmentsDatabase.instance\n\n    @staticmethod\n    def load_dict_from_file(filepath):\n        if not os.path.isfile(filepath):\n            print(""No SpeedSegments database found at file \'%s\'. Initializing database empty."" % (filepath,))\n            return dict()\n        else:\n            lines = open(filepath, ""r"").readlines()\n            #lines = [line.strip().split(""\\t"") for line in lines]\n            lines = [line.strip() for line in lines]\n            segments = dict()\n            nb_found = 0\n            nb_labeled = 0\n            #segments_found_keys = set()\n            #segments_found = []\n            for line in lines:\n                seg = SpeedSegment.from_string(line)\n                segments[seg.get_key()] = seg\n                nb_found += 1\n                nb_labeled += (seg.label is not None)\n            """"""\n            for h, w, vals in lines:\n                seg = np.zeros((int(h) * int(w)), dtype=np.bool)\n                for i in range(len(vals)):\n                    seg[i] = int(vals[i])\n                seg = seg.reshape((int(h), int(w)))\n                seg_key = SpeedSegment.segment_image_to_key(seg)\n                segments[seg_key] = seg\n                #segments_found.append(seg)\n                #segments_found_keys.add(seg_key)\n                nb_found += 1\n                nb_labeled += (seg.label is not None)\n            #return segments_found_keys, segments_found\n            """"""\n\n            print(""Loaded SpeedSegments database from file \'%s\' with %d segments, %d of which are labeled."" % (filepath, nb_found, nb_labeled))\n            return segments\n\n    def save(self):\n        with open(self.filename, ""w"") as f:\n            for key in self.segments:\n                segment = self.segments[key]\n\n                #sr = segment.arr.astype(np.int32).ravel()\n                #vals = []\n                #for v in sr:\n                #    vals.append(str(v))\n                #label = segment.label if segment.label is not None else ""?""\n                #f.write(""%s\\t%d\\t%d\\t%s"" % (label, segment.shape[0], segment.shape[1], """".join(vals)))\n                f.write(""%s\\n"" % (segment.to_string(),))\n\n    def add(self, segment, save=True, force=False):\n        key = segment.get_key()\n        if key not in self.segments or force:\n            self.segments[key] = segment\n            if save:\n                self.save()\n\n    def get_by_key(self, key):\n        #print(""get_by_key(%s) => %s"" % (key, str(self.segments.get(key))))\n        return self.segments.get(key)\n\n    def contains_key(self, key):\n        #print(""contains_key(%s) => %s"" % (key, str(self.get_by_key(key) is not None)))\n        return self.get_by_key(key) is not None\n\nclass SpeedOMeter(object):\n    def __init__(self, image):\n        assert image.dtype == np.uint8\n        assert image.ndim == 3\n        self.image = image\n\n    def get_postprocessed_image(self):\n        return SpeedOMeter.postprocess_speed_image(self.image)\n\n    def get_postprocessed_image_rgb(self):\n        img_bin = self.get_postprocessed_image()\n        img2d = img_bin.astype(np.uint8) * 255\n        return np.tile(img2d[:, :, np.newaxis], (1, 1, 3))\n\n    def split_to_segments(self):\n        post = self.get_postprocessed_image()\n        return SpeedOMeter.segment_speed_image(post)\n\n    def predict_speed_raw(self):\n        segs = self.split_to_segments()\n        #assert len(segs) > 0\n        if len(segs) == 0:\n            print(""[WARNING] [SpeedOMeter.predict_speed_raw()] length of segs is zero"")\n            return 0\n\n        result = []\n        for seg in segs:\n            label = seg.predict_label()\n            result.append(label)\n        return result\n\n    def predict_speed(self):\n        labels = self.predict_speed_raw()\n        digits = set(""0123456789"")\n        result = []\n        for label in labels:\n            if label is None:\n                return None\n            for c in label:\n                if c in digits:\n                    result.append(c)\n        if len(result) == 0:\n            return None\n        else:\n            return int("""".join(result))\n\n    @staticmethod\n    def postprocess_speed_image(speed_image):\n        speed_image_gray = cv2.cvtColor(speed_image, cv2.COLOR_RGB2GRAY)\n        #assert speed_image_gray.dtype == np.uint8\n        #speed_image_edges = filters.prewitt_v(speed_image_gray) # float, roughly -1 to 1\n\n        #speed_image = speed_image[:, :, 1:] # remove red\n        #speed_image_avg = np.average(speed_image, axis=2)\n        return (speed_image_gray > 140)\n        #print(""speed_image_edges.dtype"", speed_image_edges.dtype, np.min(speed_image_edges), np.max(speed_image_edges))\n        #return np.logical_or(speed_image_edges < (-64/255), speed_image_edges > (64/255))\n\n    @staticmethod\n    def segment_speed_image(speed_image_bin):\n        speed_image_labeled, num_labels = morphology.label(\n            speed_image_bin, background=0, connectivity=1, return_num=True\n        )\n\n        segments = []\n        for label in range(1, num_labels+1):\n            (yy, xx) = np.nonzero(speed_image_labeled == label)\n            min_y, max_y = np.min(yy), np.max(yy)\n            min_x, max_x = np.min(xx), np.max(xx)\n            seg_img = speed_image_bin[min_y:max_y+1, min_x:max_x+1]\n            segments.append(SpeedSegment(seg_img))\n\n        return segments\n\nclass SpeedSegment(object):\n    def __init__(self, arr, label=None):\n        assert arr.dtype == np.bool\n        assert arr.ndim == 2\n        self.arr = arr\n        self.label = label\n\n    def get_image(self):\n        return self.arr.astype(np.uint8) * 255\n\n    def predict_label(self):\n        key = self.get_key()\n        annotated_seg = SpeedSegmentsDatabase.get_instance().get_by_key(key)\n        if annotated_seg is not None:\n            return annotated_seg.label\n        else:\n            return None\n\n    def is_in_database(self):\n        key = self.get_key()\n        return SpeedSegmentsDatabase.get_instance().contains_key(key)\n\n    def get_key(self):\n        return SpeedSegment.segment_image_to_key(self.arr)\n\n    def to_string(self):\n        sr = self.arr.astype(np.int32).ravel()\n        vals = []\n        for v in sr:\n            vals.append(str(v))\n        label = self.label if self.label is not None else ""?""\n        h, w = self.arr.shape\n        return ""%s||%d||%d||%s"" % (label, h, w, """".join(vals))\n\n    @staticmethod\n    def from_string(line):\n        label, h, w, vals = line.split(""||"")\n        seg_image = np.zeros((int(h) * int(w)), dtype=np.bool)\n        for i in range(len(vals)):\n            seg_image[i] = int(vals[i])\n        seg_image = seg_image.reshape((int(h), int(w)))\n        if label == ""?"":\n            label = None\n        seg = SpeedSegment(seg_image, label=label)\n        return seg\n\n    @staticmethod\n    def segment_image_to_key(arr):\n        assert arr.dtype == np.bool\n        assert arr.ndim == 2\n        # int conversion here so that the string does not become\n        # ""TrueFalseFalseTrue..."" but instead ""1001...""\n        sr = arr.ravel().astype(np.int32)\n        vals = []\n        for v in sr:\n            vals.append(str(v))\n        return ""%dx%d_%s"" % (arr.shape[0], arr.shape[1], str("""".join(vals)),)\n'"
lib/states.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport actions\nimport util\nfrom config import Config\nimport imgaug as ia\n\nclass State(object):\n    def __init__(self, from_datetime, screenshot_rs_jpg, speed, is_reverse, \\\n        is_offence_shown, is_damage_shown, reward, \\\n        action_left_right, action_up_down, p_explore, \\\n        steering_wheel_classical, steering_wheel_raw_one_classical, steering_wheel_raw_two_classical, \\\n        steering_wheel_cnn, steering_wheel_raw_cnn,\n        allow_cache=False, idx=None):\n        assert action_up_down is None or action_up_down in actions.ACTIONS_UP_DOWN\n        assert action_left_right is None or action_left_right in actions.ACTIONS_LEFT_RIGHT\n        self.from_datetime = from_datetime\n        self.screenshot_rs_jpg = screenshot_rs_jpg\n        self.speed = speed\n        self.is_reverse = bool(is_reverse) # convert sqlite int to bool if necessary\n        self.is_offence_shown = bool(is_offence_shown)\n        self.is_damage_shown = bool(is_damage_shown)\n        self.reward = reward\n        self.action_left_right = action_left_right\n        self.action_up_down = action_up_down\n        self.p_explore = p_explore\n        self.steering_wheel_classical = steering_wheel_classical\n        self.steering_wheel_raw_one_classical = steering_wheel_raw_one_classical\n        self.steering_wheel_raw_two_classical = steering_wheel_raw_two_classical\n        self.steering_wheel_cnn = steering_wheel_cnn\n        self.steering_wheel_raw_cnn = steering_wheel_raw_cnn\n\n        self.allow_cache = allow_cache\n        self._screenshot_rs = None\n        self._screenshot_rs_small = None\n\n        self.idx = idx\n\n    @property\n    def screenshot_rs(self):\n        if self.allow_cache:\n            if self._screenshot_rs is None:\n                self._screenshot_rs = util.decompress_img(self.screenshot_rs_jpg)\n            return self._screenshot_rs\n        else:\n            return util.decompress_img(self.screenshot_rs_jpg)\n\n    """"""\n    @property\n    def screenshot_rs_small(self):\n        if self.allow_cache:\n            if self._screenshot_rs_small is None:\n                scr_large = self.screenshot_rs\n                h, w = Config.MODEL_HEIGHT_SMALL, Config.MODEL_WIDTH_SMALL\n                self._screenshot_rs_small = ia.imresize_single_image(scr_large, (h, w), interpolation=""cubic"")\n            return self._screenshot_rs_small\n        else:\n            scr_large = self.screenshot_rs\n            h, w = Config.MODEL_HEIGHT_SMALL, Config.MODEL_WIDTH_SMALL\n            return ia.imresize_single_image(scr_large, (h, w), interpolation=""cubic"")\n    """"""\n\n    @property\n    def multiaction(self):\n        return (self.action_up_down, self.action_left_right)\n\n    @property\n    def actions_multivec(self):\n        return actions.actions_to_multivec(action_up_down=self.action_up_down, action_left_right=self.action_left_right)\n\n    @staticmethod\n    def from_row(row):\n        # sqlite3 returns unicode strings, which causes problems in python2\n        # when they are compared to non-unicode strings\n        # in python3, all strings should be by default unicode so it shouldnt\n        # cause any problems there\n        if sys.version_info[0] >= 3:\n            return State(\n                idx=row[0],\n                from_datetime=row[1],\n                screenshot_rs_jpg=str(row[2]), # sqlite3 returns buffer object instead of string\n                speed=row[3],\n                is_reverse=row[4],\n                is_offence_shown=row[5],\n                is_damage_shown=row[6],\n                reward=row[7],\n                action_up_down=row[8],\n                action_left_right=row[9],\n                p_explore=row[10],\n                steering_wheel_classical=row[11],\n                steering_wheel_raw_one_classical=row[12],\n                steering_wheel_raw_two_classical=row[13],\n                steering_wheel_cnn=row[14],\n                steering_wheel_raw_cnn=row[15]\n            )\n        else:\n            return State(\n                idx=row[0],\n                from_datetime=row[1],\n                screenshot_rs_jpg=str(row[2]), # sqlite3 returns buffer object instead of string\n                speed=row[3],\n                is_reverse=row[4],\n                is_offence_shown=row[5],\n                is_damage_shown=row[6],\n                reward=row[7],\n                action_up_down=str(row[8]), # make sure that these are not unicode\n                action_left_right=str(row[9]), # make sure that these are not unicode\n                p_explore=row[10],\n                steering_wheel_classical=row[11],\n                steering_wheel_raw_one_classical=row[12],\n                steering_wheel_raw_two_classical=row[13],\n                steering_wheel_cnn=row[14],\n                steering_wheel_raw_cnn=row[15]\n            )\n\n    def to_string(self):\n        return ""State(dt=%s, img_shape=%s, speed=%s, off=%d, dmg=%d, reward=%.2f, alr=%s, aud=%s, pe=%.2f, w=%.2f, wr1=%.2f, wr2=%.2f)"" % (self.from_datetime, self.screenshot_rs.shape, str(self.speed), int(self.is_offence_shown), int(self.is_damage_shown), self.reward, self.action_left_right, self.action_up_down, self.p_explore, self.steering_wheel, self.steering_wheel_raw_one, self.steering_wheel_raw_two)\n'"
lib/steering_wheel.py,1,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom train_steering_wheel.train import (\n    MODEL_HEIGHT as CNN_MODEL_HEIGHT,\n    MODEL_HEIGHT as CNN_MODEL_WIDTH,\n    ANGLE_BIN_SIZE as CNN_ANGLE_BIN_SIZE,\n    extract_steering_wheel_image as cnn_extract_steering_wheel_image,\n    downscale_image as cnn_downscale_image\n)\nfrom train_steering_wheel import models as cnn_models\nfrom lib import util\nfrom config import Config\n\nfrom scipy import misc, ndimage\nimport cv2\nimport imgaug as ia\nimport numpy as np\nfrom skimage.transform import hough_line, hough_line_peaks\nfrom skimage import filters\nfrom skimage import morphology\nimport math\nimport torch\n\n#cv2.namedWindow(""thresh"", cv2.WINDOW_NORMAL)\n\nSTEERING_WHEEL_TRACKER_CNN_FP = os.path.join(Config.MAIN_DIR, ""train_steering_wheel/steering_wheel.tar"")\n\nclass SteeringWheelTrackerCNN(object):\n    def __init__(self, default_angle=0):\n        assert os.path.isfile(STEERING_WHEEL_TRACKER_CNN_FP)\n        checkpoint = torch.load(STEERING_WHEEL_TRACKER_CNN_FP)\n        self.model = cnn_models.SteeringWheelTrackerCNNModel()\n        self.model.load_state_dict(checkpoint[""tracker_cnn_state_dict""])\n        self.model.cuda(Config.GPU)\n        self.model.eval()\n\n        self.default_angle = default_angle\n\n        self.reset()\n\n    def reset(self):\n        self.last_match = None\n        self.last_angle = self.default_angle\n        self.last_angle_raw = self.default_angle\n        self.overflow_degrees = 45\n        self.overflow_max_count = 3\n        self.overflow_counter = 0\n\n    def estimate_angle(self, image):\n        #from scipy import misc\n        subimg = cnn_extract_steering_wheel_image(image)\n        #misc.imshow(subimg)\n        subimg = cnn_downscale_image(subimg)\n        #misc.imshow(subimg)\n        angle_raw_bins = self.model.forward_image(subimg, volatile=True, requires_grad=False, gpu=Config.GPU, softmax=True)\n        angle_raw_bins = angle_raw_bins.data[0].cpu().numpy()\n        angle_raw_bin = np.argmax(angle_raw_bins)\n        #print(angle_raw_bins.data.cpu().numpy())\n\n        """"""\n        angle_raw_center = angle_raw_bin * CNN_ANGLE_BIN_SIZE + CNN_ANGLE_BIN_SIZE * 0.5 - 180\n        angle_raw_left = angle_raw_center - CNN_ANGLE_BIN_SIZE\n        angle_raw_right = angle_raw_center + CNN_ANGLE_BIN_SIZE\n        angle_raw_center_p = angle_raw_bins[angle_raw_bin]\n        angle_raw_left_p = angle_raw_bins[angle_raw_bin-1] if angle_raw_bin-1 > 0 else 0\n        angle_raw_right_p = angle_raw_bins[angle_raw_bin+1] if angle_raw_bin+1 < angle_raw_bins.shape[0] else 0\n\n        angle_raw = angle_raw_left_p * angle_raw_left + angle_raw_center_p * angle_raw_center + angle_raw_right_p * angle_raw_right\n        """"""\n        angle_raw = angle_raw_bin * CNN_ANGLE_BIN_SIZE + CNN_ANGLE_BIN_SIZE * 0.5 - 180\n\n        #print(angle_raw)\n        possible_angles = [angle_raw]\n        if angle_raw < 0:\n            possible_angles.append(180+(180-abs(angle_raw)))\n            possible_angles.append(-360-abs(angle_raw))\n        if angle_raw > 0:\n            possible_angles.append(-180-(180-abs(angle_raw)))\n            possible_angles.append(360+abs(angle_raw))\n        possible_angles_dist = [(a, abs(self.last_angle - a)) for a in possible_angles]\n        possible_angles_dist_sort = sorted(possible_angles_dist, key=lambda t: t[1])\n        angle = possible_angles_dist_sort[0][0]\n\n        if angle > Config.STEERING_WHEEL_MAX:\n            angle = angle - 360\n        elif angle < Config.STEERING_WHEEL_MIN:\n            angle = angle + 360\n\n        if abs(angle - self.last_angle) >= self.overflow_degrees:\n            if self.overflow_counter >= self.overflow_max_count:\n                self.last_angle = angle\n                self.last_angle_raw = angle_raw\n                self.overflow_counter = 0\n            else:\n                angle = self.last_angle\n                angle_raw = self.last_angle_raw\n            self.overflow_counter += 1\n        else:\n            self.last_angle = angle\n            self.last_angle_raw = angle_raw\n            self.overflow_counter = 0\n\n        return angle, angle_raw\n\nclass SteeringWheelTracker(object):\n    def __init__(self, default_angle=0):\n        self.default_angle = default_angle\n        self.max_angle = Config.STEERING_WHEEL_MAX # wheel can be turned by roughly +/- 360+90 degrees, add 40deg tolerance\n        self.reset()\n\n    def reset(self):\n        self.last_match = None\n        self.last_angle = self.default_angle\n        self.last_angle_raw1 = self.default_angle\n        self.last_angle_raw2 = self.default_angle\n        self.overflow_degrees = 45\n        self.overflow_max_count = 3\n        self.overflow_counter = 0\n\n    def estimate_angle(self, image, visualize=False):\n        if visualize:\n            match, image_viz = estimate_by_last_match(image, last_match=self.last_match, visualize=visualize)\n            #if match is None and self.last_match is not None:\n            #    match, image_viz = estimate_by_last_match(image, last_match=None, visualize=visualize)\n        else:\n            match = estimate_by_last_match(image, last_match=self.last_match, visualize=visualize)\n            #if match is None and self.last_match is not None:\n            #    match = estimate_by_last_match(image, last_match=None, visualize=visualize)\n\n        if match is None:\n            #print(""no match"")\n            self.reset()\n        else:\n            v1 = np.float32([1, 0])\n            #print(match)\n            v2a = np.float32([\n                match[""right_x""] - match[""left_x""],\n                match[""right_y""] - match[""left_y""]\n            ])\n            v2b = np.float32([\n                match[""left_x""] - match[""right_x""],\n                match[""left_y""] - match[""right_y""]\n            ])\n            angle_raw1 = get_angle(v1, v2a)\n            angle_raw2 = get_angle(v1, v2b)\n            #print(""early angle_raw1"", angle_raw1, ""angle_raw2"", angle_raw2)\n            if angle_raw1 > 180:\n                angle_raw1 = -(360 - angle_raw1)\n            if angle_raw2 > 180:\n                angle_raw2 = -(360 - angle_raw2)\n            #distance_from_90 = (angle_raw1 % 90)\n            #p_flipped = 1 - (min(distance_from_90, 90-distance_from_90) / 45)\n            #flip_distance = min(abs(abs(angle_raw1) - 270), abs(abs(angle_raw1) - 90)) / 90\n            #maxp = 0.95\n            #p_flipped = np.clip(1 - flip_distance, 0, maxp)\n\n            # maxp and p_flipped is legacy stuff, can be removed\n            maxp = 0.95\n            p_flipped = maxp\n            possible_angles = [\n                (-360+angle_raw1, maxp),\n                (-360+angle_raw2, p_flipped),\n                (angle_raw1, maxp),\n                (angle_raw2, p_flipped),\n                (360+angle_raw1, maxp),\n                (360+angle_raw2, p_flipped),\n            ]\n            possible_angles = [(r, p) for (r, p) in possible_angles if r < self.max_angle]\n            possible_angles_dist = [(poss, abs(poss - self.last_angle), p) for (poss, p) in possible_angles]\n            possible_angles_dist_sort = sorted(possible_angles_dist, key=lambda t: t[1]*(1-t[2]))\n            angle = possible_angles_dist_sort[0][0]\n\n            #print(""angle_raw1 %.2f | angle_raw2 %.2f | after add %.2f | poss %s | poss sort %s"" % (angle_raw1, angle_raw2, angle, str(possible_angles_dist), str(possible_angles_dist_sort)))\n            #print(""after add"", angle)\n\n            if abs(angle - self.last_angle) >= self.overflow_degrees:\n                if self.overflow_counter >= self.overflow_max_count:\n                    self.last_match = match\n                    self.last_angle = angle\n                    self.last_angle_raw1 = angle_raw1\n                    self.last_angle_raw2 = angle_raw2\n                    self.overflow_counter = 0\n                else:\n                    self.last_match = None\n                    angle = self.last_angle\n                    angle_raw1 = self.last_angle_raw1\n                    angle_raw2 = self.last_angle_raw2\n                self.overflow_counter += 1\n            else:\n                self.last_match = match\n                self.last_angle = angle\n                self.last_angle_raw1 = angle_raw1\n                self.last_angle_raw2 = angle_raw2\n                self.overflow_counter = 0\n        if visualize:\n            return self.last_angle, (self.last_angle_raw1, self.last_angle_raw2), image_viz\n        else:\n            return self.last_angle, (self.last_angle_raw1, self.last_angle_raw2)\n\n\ndef get_angle(v1, v2):\n    v1_theta = math.atan2(v1[1], v1[0])\n    v2_theta = math.atan2(v2[1], v2[0])\n    r = (v2_theta - v1_theta) * (180.0 / math.pi)\n    if r < 0:\n        r += 360.0\n    return r\n\ndef estimate_by_last_match(image, last_match=None, visualize=False):\n    if last_match is None:\n        return estimate(image, visualize=visualize)\n    else:\n        #search_rect = (\n        #    last_match[""min_x""], last_match[""min_y""],\n        #    last_match[""max_x""], last_match[""max_y""]\n        #)\n        search_rect = None\n        #expected_position = (last_match[""center_x""], last_match[""center_y""])\n        expected_position = None\n        return estimate(image, expected_position=expected_position, search_rect=search_rect, visualize=visualize)\n\n#@profile\ndef estimate(image, expected_position=None, search_rect=None, search_rect_border=0.05, expected_pixels=(10, 200), optimal_size=90, visualize=False):\n    downscale_factor = 1 # legacy stuff\n    h, w = image.shape[0:2]\n\n    if search_rect is not None:\n        x1, y1, x2, y2 = search_rect[0], search_rect[1], search_rect[2], search_rect[3]\n        if search_rect_border > 0:\n            clip = np.clip\n            bx = int(w * search_rect_border)\n            by = int(h * search_rect_border)\n            x1 = clip(x1 - bx, 0, w-2)\n            y1 = clip(y1 - by, 0, h-2)\n            x2 = clip(x2 + bx, x1, w-1)\n            y2 = clip(y2 + by, y1, h-1)\n    else:\n        # full wheel: x1=440, x2=870, y1=440, y2=720\n        # wheel w=440, h=270\n        x1 = int(w * (480/1280))\n        x2 = int(w * (830/1280))\n        y1 = int(h * (520/720))\n        y2 = int(h * (720/720))\n    rect_h = y2 - y1\n    rect_w = x2 - x1\n\n    if expected_position is None:\n        expected_position = (\n            int(w * (646/1280)),\n            int(h * (684/720))\n        )\n\n    img_wheel = image[y1:y2+1, x1:x2+1, :]\n    img_wheel_rs = img_wheel\n    img_wheel_rsy = cv2.cvtColor(img_wheel_rs, cv2.COLOR_RGB2GRAY)\n    expected_position_rs = (\n        int((expected_position[0]-x1) * downscale_factor),\n        int((expected_position[1]-y1) * downscale_factor)\n    )\n\n    #thresh_mask = filters.threshold_li(img_wheel_rsy)\n    thresh_mask = filters.threshold_isodata(img_wheel_rsy)\n    thresh = img_wheel_rsy > thresh_mask #40\n    #cv2.imshow(""thresh"", thresh.astype(np.uint8)*255)\n    #cv2.waitKey(10)\n    thresh = morphology.binary_dilation(thresh, morphology.square(3))\n\n    img_labeled, num_labels = morphology.label(\n        thresh, background=0, connectivity=1, return_num=True\n    )\n\n    segments = []\n    for label in range(1, num_labels+1):\n        img_seg = (img_labeled == label)\n        (yy, xx) = np.nonzero(img_seg)\n\n        # size of correct segment is around 60 pixels without dilation and 90 with dilation\n        # position is at around x=21, y=13\n        # (both numbers for screenshots after jpg-compression/decompression at 1/4 the original\n        # size, i.e. 1280/4 x 720/4)\n        if expected_pixels[0] <= len(yy) <= expected_pixels[1]:\n            center_x = np.average(xx)\n            center_y = np.average(yy)\n\n            # euclidean distance to expected position\n            # segments which\'s center is at the expected position get a 0\n            # segments which a further away get higher values\n            dist_pos = 0.1 * math.sqrt((center_x - expected_position_rs[0]) ** 2 + (center_y - expected_position_rs[1])**2)\n\n            # distance to optimal size (number of pixels)\n            # segments that have the same number of pixels as the expected size\n            # get a 0, segments with 50pecent more/less pixels get a 0\n            dist_size = np.clip(\n                1/(optimal_size*0.5) * abs(len(yy) - optimal_size),\n                0, 1\n            )\n            dist = dist_pos + dist_size\n\n            segments.append({\n                ""xx"": xx,\n                ""yy"": yy,\n                ""center_x"": center_x,\n                ""center_y"": center_y,\n                ""dist_pos"": dist_pos,\n                ""dist_size"": dist_size,\n                ""dist"": dist,\n                ""img_seg"": img_seg\n            })\n\n    if len(segments) == 0:\n        return (None, None) if visualize else None\n\n    segments = sorted(segments, key=lambda d: d[""dist""])\n    best_match = segments[0]\n    xx = x1 + (best_match[""xx""].astype(np.float32) * (1/downscale_factor)).astype(np.int32)\n    yy = y1 + (best_match[""yy""].astype(np.float32) * (1/downscale_factor)).astype(np.int32)\n\n    image_segment = best_match[""img_seg""]\n    image_segment = morphology.binary_erosion(image_segment, morphology.square(3))\n\n    cx, cy = int(best_match[""center_x""]), int(best_match[""center_y""])\n    sy, sx = 10, 10\n    hx1 = np.clip(cx - sx, 0, image_segment.shape[1])\n    hx2 = np.clip(cx + sx + 1, 0, image_segment.shape[1])\n    hy1 = np.clip(cy - sy, 0, image_segment.shape[0])\n    hy2 = np.clip(cy + sy + 1, 0, image_segment.shape[0])\n    hough_segment = image_segment[hy1:hy2, hx1:hx2]\n    h, theta, d = hough_line(hough_segment)\n    if len(h) == 0:\n        return (None, None) if visualize else None\n\n    hspaces, angles, dists = hough_line_peaks(h, theta, d, num_peaks=1)\n    if len(hspaces) == 0:\n        return (None, None) if visualize else None\n\n    hspace, angle, dist = hspaces[0], angles[0], dists[0]\n    line_y0 = (dist - 0 * np.cos(angle)) / np.sin(angle)\n    line_y1 = (dist - hough_segment.shape[1] * np.cos(angle)) / np.sin(angle)\n    slope = (line_y1 - line_y0) / (hx2 - hx1)\n\n    left_x = cx - 3\n    right_x = cx + 3\n    left_y = cy + (-3) * slope\n    right_y = cy + 3 * slope\n\n    #print(""x1 %d x2 %d y1 %d y2 %d | cx %d cy %d | hx1 %d hx2 %d hy1 %d hy2 %d | line_y0 %.2f line_y1 %.2f | left_x %d right_x %d left_y %d right_y %d | hs %s | is %s"" % (\n    #    x1, x2, y1, y2, cx, cy, hx1, hx2, hy1, hy2, line_y0, line_y1, left_x, right_x, left_y, right_y, hough_segment.shape, image_segment.shape\n    #))\n\n    #fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n    #ax.imshow(image_segment, cmap=plt.cm.gray)\n    #ax.plot([left_x, right_x], [left_y, right_y], ""-r"")\n    #plt.show()\n\n    best_match[""min_x""] = int(np.min(xx))\n    best_match[""max_x""] = int(np.max(xx))\n    best_match[""min_y""] = int(np.min(yy))\n    best_match[""max_y""] = int(np.max(yy))\n    best_match[""center_x""] = x1 + (best_match[""center_x""] * (1/downscale_factor))\n    best_match[""center_y""] = y1 + (best_match[""center_y""] * (1/downscale_factor))\n    best_match[""left_x""] = x1 + (left_x * (1/downscale_factor))\n    best_match[""right_x""] = x1 + (right_x * (1/downscale_factor))\n    best_match[""left_y""] = y1 + (left_y * (1/downscale_factor))\n    best_match[""right_y""] = y1 + (right_y * (1/downscale_factor))\n\n    if visualize:\n        upf = 2\n        image_viz = ia.imresize_single_image(np.copy(image), (image.shape[0]*upf, image.shape[1]*upf))\n        image_viz = util.draw_point(image_viz, x=int(x1)*upf, y=int(y1)*upf, size=7, color=[255, 0, 0])\n        image_viz = util.draw_point(image_viz, x=(int(x2)-1)*upf, y=int(y1)*upf, size=7, color=[255, 0, 0])\n        image_viz = util.draw_point(image_viz, x=(int(x2)-1)*upf, y=(int(y2)-1)*upf, size=7, color=[255, 0, 0])\n        image_viz = util.draw_point(image_viz, x=int(x1)*upf, y=(int(y2)-1)*upf, size=7, color=[255, 0, 0])\n        image_viz = util.draw_point(image_viz, x=int(expected_position[0])*upf, y=int(expected_position[1])*upf, size=7, color=[0, 0, 255])\n        image_viz = util.draw_point(image_viz, x=best_match[""min_x""]*upf, y=best_match[""min_y""]*upf, size=7, color=[128, 0, 0])\n        image_viz = util.draw_point(image_viz, x=(best_match[""max_x""]-1)*upf, y=best_match[""min_y""]*upf, size=7, color=[128, 0, 0])\n        image_viz = util.draw_point(image_viz, x=(best_match[""max_x""]-1)*upf, y=(best_match[""max_y""]-1)*upf, size=7, color=[128, 0, 0])\n        image_viz = util.draw_point(image_viz, x=best_match[""min_x""]*upf, y=(best_match[""max_y""]-1)*upf, size=7, color=[128, 0, 0])\n        image_viz = util.draw_point(image_viz, x=int(best_match[""center_x""])*upf, y=int(best_match[""center_y""])*upf, size=7, color=[0, 0, 128])\n        image_viz = util.draw_point(image_viz, x=int(best_match[""left_x""])*upf, y=int(best_match[""left_y""])*upf, size=7, color=[0, 255, 0])\n        image_viz = util.draw_point(image_viz, x=int(best_match[""right_x""])*upf, y=int(best_match[""right_y""])*upf, size=7, color=[0, 128, 0])\n        return best_match, image_viz\n    else:\n        return best_match\n'"
lib/util.py,5,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport cStringIO as StringIO\nfrom scipy import misc, ndimage\nfrom skimage import feature\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom config import Config\nfrom matplotlib import pyplot as plt\nimport imgaug as ia\nfrom PIL import Image\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nimport cv2\n\ndef template_match(needle, haystack):\n    result = feature.match_template(haystack, needle)\n    ij = np.unravel_index(np.argmax(result), result.shape)\n    x, y = ij[1], ij[0]\n    score = result[y, x]\n    return x, y, score\n\ndef compress_to_jpg(img):\n    return compress_img(img, method=""JPEG"")\n\ndef compress_img(img, method):\n    img_compressed_buffer = StringIO.StringIO()\n    im = misc.toimage(img)\n    im.save(img_compressed_buffer, format=method)\n    img_compressed = img_compressed_buffer.getvalue()\n    img_compressed_buffer.close()\n    return img_compressed\n\ndef decompress_img(img_compressed):\n    img_compressed_buffer = StringIO.StringIO()\n    img_compressed_buffer.write(img_compressed)\n    img = ndimage.imread(img_compressed_buffer, mode=""RGB"")\n    img_compressed_buffer.close()\n    return img\n\ndef draw_image(img, other_img, x, y, copy=True, raise_if_oob=False):\n    if img.ndim == 3 and other_img.ndim == 2:\n        other_img = np.tile(np.copy(other_img)[:, :, np.newaxis], (1, 1, 3))\n    elif img.ndim == 3 and other_img.ndim == 3 and other_img.shape[2] == 1:\n        other_img = np.tile(other_img, (1, 1, 3))\n    else:\n        assert img.ndim == other_img.ndim\n        assert img.shape[2] == other_img.shape[2]\n\n    result = np.copy(img) if copy else img\n    imgh, imgw = img.shape[0], img.shape[1]\n    if y < 0 or y >= imgh or x < 0 or x >= imgw:\n        if raise_if_oob:\n            raise Exception(""Invalid coordinates y=%d, x=%d for img shape h=%d, w=%d"" % (y, x, imgh, imgw))\n        else:\n            return result\n\n    y2 = np.clip(y + other_img.shape[0], 0, imgh-1)\n    x2 = np.clip(x + other_img.shape[1], 0, imgw-1)\n\n    result[y:y2, x:x2, :] = other_img[0:y2-y, 0:x2-x, :]\n\n    return result\n\ndef draw_point(img, y, x, size=5, color=[255, 0, 0], alpha=1.0, copy=True):\n    assert img.dtype == np.uint8\n    assert isinstance(y, int), ""Invalid y-coordinate (type=%s val=%s)"" % (type(y), str(y))\n    assert isinstance(x, int), ""Invalid x-coordinate (type=%s val=%s)"" % (type(x), str(x))\n    assert size % 2 != 0\n\n    if copy:\n        img = np.copy(img)\n\n    if alpha < 0.99:\n        img = img.astype(np.float32, copy=False)\n\n    height, width = img.shape[0], img.shape[1]\n\n    if y < 0 or y >= height:\n        print(""[WARNING] draw_point: y is out of bounds (y=%d vs bounds 0 to %d)"" % (y, height,))\n    if x < 0 or x >= width:\n        print(""[WARNING] draw_point: x is out of bounds (x=%d vs bounds 0 to %d)"" % (x, width,))\n\n    coords = []\n    if size == 1:\n        coords.append((y, x))\n    else:\n        sizeh = (size-1) // 2\n        for yy in range(-sizeh, sizeh+1):\n            for xx in range(-sizeh, sizeh+1):\n                coords.append((y + yy, x + xx))\n\n    coords = [(yy, xx) for (yy, xx) in coords if 0 <= yy < height and 0 <= xx < width]\n\n    if len(coords) > 0:\n        coords_y = [yy for yy, xx in coords]\n        coords_x = [xx for yy, xx in coords]\n\n        if alpha >= 0.99:\n            img[coords_y, coords_x, :] = np.uint8(color)\n        else:\n            img[coords_y, coords_x, :] = (1-alpha) * img[coords_y, coords_x, :] + alpha * np.float32(color)\n\n    if alpha < 0.99:\n        img = np.clip(img, 0, 255, copy=False)\n        img = img.astype(np.uint8)\n\n    return img\n\ndef draw_line(img, y1, x1, y2, x2, color=[0, 255, 0], alpha=1.0, thickness=1, clip_result=True, copy=True):\n    assert img.dtype == np.uint8\n    assert 0 <= alpha <= 1.0\n    assert thickness >= 1\n    assert (thickness == 1 and alpha <= 1.0) or (thickness > 1 and alpha == 1.0) # wenn thickness > 1 dann muss auf PIL ausgewichen werden, welches derzeit kein alpha unterstuetzt\n\n    if copy:\n        result = np.copy(img)\n    else:\n        result = img\n\n    img_height, img_width = result.shape[0], result.shape[1]\n    y1 = np.clip(y1, 0, img_height-1)\n    y2 = np.clip(y2, 0, img_height-1)\n    x1 = np.clip(x1, 0, img_width-1)\n    x2 = np.clip(x2, 0, img_width-1)\n\n    if thickness == 1:\n        rr, cc = draw.line(y1, x1, y2, x2)\n        rr = np.clip(rr, 0, img_height - 1)\n        cc = np.clip(cc, 0, img_width - 1)\n\n        if alpha >= 1.0-0.01:\n            result[rr, cc, :] = np.array(color)\n        else:\n            result = result.astype(np.float32)\n            result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * np.array(color)\n            if clip_result:\n                result = np.clip(result, 0, 255)\n            result = result.astype(np.uint8)\n    else:\n        # skimage does not support thickness, so use PIL here\n        # here without alpha for now, would need RGBA image\n        assert alpha >= 0.99\n        result = Image.fromarray(result)\n        context = ImageDraw.Draw(result)\n        context.line([(x1, y1), (x2, y2)], fill=tuple(color), width=thickness)\n        result = np.asarray(result)\n        result.setflags(write=True) # PIL/asarray returns read-only array\n        result = result.astype(np.uint8)\n\n    return result\n\ndef draw_direction_circle(img, y, x, r_inner, r_outer, angle_start, angle_end, color_border=[0, 255, 0], color_fill=[0, 200, 0], alpha=1.0):\n    assert r_inner <= r_outer\n    input_dtype = img.dtype\n    img = np.copy(img).astype(np.float32)\n    color_border = np.array(color_border)\n    color_fill = np.array(color_fill)\n\n    #print(y, x, r_inner, r_outer, angle_start, angle_end)\n\n    center = (x, y)\n    axes_outer = (r_outer, r_outer)\n    axes_inner = (r_inner, r_inner)\n    angle = -90\n    thickness = 1\n\n    #cv2.ellipse(img, center=center, axes=axes_outer, angle=angle, startAngle=angle_start, endAngle=angle_end, color=color, thickness=thickness)\n    outer_points = cv2.ellipse2Poly(center=center, axes=axes_outer, angle=angle, arcStart=angle_start, arcEnd=angle_end, delta=1)\n    inner_points = cv2.ellipse2Poly(center=center, axes=axes_inner, angle=angle, arcStart=angle_start, arcEnd=angle_end, delta=1)\n    # inner_points in umgekehrter reihenfolge, da sonst zwei linien schraeg durch den bogen gehen,\n    # um inneren und aeusseren bogen zu verbinden\n    full_arc = np.concatenate((outer_points, inner_points[::-1]), axis=0).astype(np.int32)\n\n    if alpha >= 0.99:\n        #points = [outer_points, inner_points]\n        # polylines expects points as [(N, 2)] of dtype int32 (not int64)\n        #cv2.polylines(img, points, isClosed=False, color=color, thickness=thickness)\n\n        #points_connectors = [np.int32([outer_points[0], inner_points[0]]), np.int32([outer_points[-1], inner_points[-1]])]\n        #cv2.polylines(img, points_connectors, isClosed=False, color=color, thickness=thickness)\n        #cv2.fillConvexPoly(img, full_arc, color=color_fill.astype(np.int64))\n        cv2.fillPoly(img, [full_arc], color=color_fill.astype(np.int64))\n        cv2.polylines(img, [full_arc], isClosed=True, color=color_border.astype(np.int64), thickness=thickness, lineType=cv2.CV_AA)\n        np.clip(img, 0, 255, out=img)\n    else:\n        """"""\n        img_draw = np.zeros_like(img)\n\n        #cv2.fillConvexPoly(img_draw, full_arc, color=color_fill.astype(np.int64))\n        cv2.fillPoly(img_draw, [full_arc], color=color_fill.astype(np.int64))\n\n        cv2.polylines(img_draw, [full_arc], isClosed=True, color=color_border.astype(np.int64), thickness=thickness, lineType=cv2.CV_AA)\n\n        mask = img_draw > 0\n\n        #mask_1d = np.tile(np.max(mask, axis=2), (1, 1, 3))\n        mask_1d = np.tile(np.max(mask, axis=2)[:, :, np.newaxis], (1, 1, 3))\n        img_outside_mask = img * (~mask_1d)\n        img_inside_mask = img * mask_1d\n        img = img_outside_mask + (1 - alpha) * img_inside_mask + alpha * img_draw\n        """"""\n\n        arc_x1, arc_y1 = np.min(full_arc, axis=0)\n        arc_x2, arc_y2 = np.max(full_arc, axis=0)\n        full_arc_shifted = full_arc - np.array([arc_x1, arc_y1])\n\n        subimg = img[arc_y1:arc_y2+1, arc_x1:arc_x2+1, :]\n        subimg_draw = np.zeros_like(subimg)\n\n        #cv2.fillConvexPoly(img_draw, full_arc, color=color_fill.astype(np.int64))\n        cv2.fillPoly(subimg_draw, [full_arc_shifted], color=color_fill.astype(np.int64))\n\n        cv2.polylines(subimg_draw, [full_arc_shifted], isClosed=True, color=color_border.astype(np.int64), thickness=thickness, lineType=cv2.CV_AA)\n\n        mask = subimg_draw > 0\n\n        mask_1d = np.any(mask, axis=2)\n        mask_3d = mask_1d[:, :, np.newaxis]\n        subimg_outside_mask = subimg * (~mask_3d)\n        subimg_inside_mask = subimg * mask_3d\n        subimg = subimg_outside_mask + (1 - alpha) * subimg_inside_mask + alpha * subimg_draw\n\n        np.clip(subimg, 0, 255, out=subimg)\n\n        img[arc_y1:arc_y2+1, arc_x1:arc_x2+1, :] = subimg\n\n    img = img.astype(input_dtype)\n    return img\n\nFONT_CACHE = dict()\ndef draw_text(img, y, x, text, color=[0, 255, 0], size=25):\n    assert img.dtype in [np.uint8, np.int32, np.int64, np.float32, np.float64]\n\n    input_dtype = img.dtype\n    if img.dtype != np.uint8:\n        img = img.astype(np.uint8)\n\n    for i in range(len(color)):\n        val = color[i]\n        if isinstance(val, float):\n            val = int(val * 255)\n            val = np.clip(val, 0, 255)\n            color[i] = val\n\n    shape = img.shape\n    img = Image.fromarray(img)\n    if size not in FONT_CACHE:\n        FONT_CACHE[size] = ImageFont.truetype(""DejaVuSans.ttf"", size)\n    context = ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=FONT_CACHE[size])\n    img_np = np.asarray(img)\n    img_np.setflags(write=True)  # PIL/asarray returns read only array\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\ndef to_variable(inputs, volatile=False, requires_grad=True):\n    if volatile:\n        make_var = lambda x: Variable(x, volatile=True)\n    else:\n        make_var = lambda x: Variable(x, requires_grad=requires_grad)\n\n    if isinstance(inputs, np.ndarray):\n        return make_var(torch.from_numpy(inputs))\n    elif isinstance(inputs, list):\n        return [make_var(torch.from_numpy(el)) for el in inputs]\n    elif isinstance(inputs, tuple):\n        return [make_var(torch.from_numpy(el)) for el in inputs]\n    elif isinstance(inputs, dict):\n        return dict([(key, make_var(torch.from_numpy(inputs[key]))) for key in inputs])\n    else:\n        raise Exception(""unknown input %s"" % (type(inputs),))\n\ndef to_variables(inputs, volatile=False, requires_grad=True):\n    return to_variable(inputs, volatile=volatile, requires_grad=requires_grad)\n\ndef to_cuda(inputs, gpu=Config.GPU):\n    if gpu <= -1:\n        return inputs\n    else:\n        if isinstance(inputs, Variable):\n            return inputs.cuda(gpu)\n        elif isinstance(inputs, list):\n            return [el.cuda(gpu) for el in inputs]\n        elif isinstance(inputs, tuple):\n            return tuple([el.cuda(gpu) for el in inputs])\n        elif isinstance(inputs, dict):\n            return dict([(key, inputs[key].cuda(gpu)) for key in inputs])\n        else:\n            raise Exception(""unknown input %s"" % (type(inputs),))\n\ndef to_numpy(var):\n    #if ia.is_numpy_array(var):\n    if isinstance(var, (np.ndarray, np.generic)):\n        return var\n    else:\n        return var.data.cpu().numpy()\n\ndef draw_heatmap_overlay(img, heatmap, alpha=0.5):\n    #assert img.shape[0:2] == heatmap.shape[0:2]\n    assert len(heatmap.shape) == 2 or (heatmap.ndim == 3 and heatmap.shape[2] == 1)\n    assert img.dtype in [np.uint8, np.int32, np.int64]\n    assert heatmap.dtype in [np.float32, np.float64]\n\n    if heatmap.ndim == 3 and heatmap.shape[2] == 1:\n        heatmap = np.squeeze(heatmap)\n\n    if img.shape[0:2] != heatmap.shape[0:2]:\n        heatmap_rs = np.clip(heatmap * 255, 0, 255).astype(np.uint8)\n        heatmap_rs = ia.imresize_single_image(heatmap_rs[..., np.newaxis], img.shape[0:2], interpolation=""nearest"")\n        heatmap = np.squeeze(heatmap_rs) / 255.0\n\n    cmap = plt.get_cmap(\'jet\')\n    heatmap_cmapped = cmap(heatmap)\n    #img_heatmaps_cmapped = img_heatmaps_cmapped[:, :, 0:3]\n    heatmap_cmapped = np.delete(heatmap_cmapped, 3, 2)\n    #heatmap_cmapped = np.clip(heatmap_cmapped * 255, 0, 255).astype(np.uint8)\n    heatmap_cmapped = heatmap_cmapped * 255\n    mix = (1-alpha) * img + alpha * heatmap_cmapped\n    mix = np.clip(mix, 0, 255).astype(np.uint8)\n    return mix\n\ndef create_2d_gaussian_old(size, fwhm=3, center=None):\n    """""" Make a square gaussian kernel.\n    size is the length of a side of the square\n    fwhm is full-width-half-maximum, which\n    can be thought of as an effective radius.\n    """"""\n\n    x = np.arange(0, size, 1, float)\n    y = x[:,np.newaxis]\n\n    if center is None:\n        x0 = y0 = size // 2\n    else:\n        x0 = center[0]\n        y0 = center[1]\n\n    unnormalized = np.exp(-4*np.log(2) * ((x-x0)**2 + (y-y0)**2) / fwhm**2)\n    return unnormalized / np.max(unnormalized)\n\ndef create_2d_gaussian(size, sigma):\n    sigma_x = sigma_y = sigma\n    x = np.arange(-size, size, step=1)\n    y = np.arange(-size, size, step=1)\n\n    x, y = np.meshgrid(x, y)\n    z = (1/(2*np.pi*sigma_x*sigma_y) * np.exp(-(x**2/(2*sigma_x**2) + y**2/(2*sigma_y**2))))\n    z = z.reshape((size*2, size*2))\n    z_norm = z / np.max(z)\n    return z_norm\n'"
lib/windowhandling.py,0,"b'from __future__ import print_function, division\nimport screenshot\nimport numpy as np\nimport subprocess\nimport time\nfrom pykeyboard import PyKeyboard\n\nKEYBOARD = PyKeyboard()\n\ndef sendkeys(win_id, up=None, press=None, down=None):\n    assert up is None or isinstance(up, (list, tuple, set))\n    assert press is None or isinstance(press, (list, tuple, set))\n    assert down is None or isinstance(down, (list, tuple, set))\n    up = set(up) if up is not None else set()\n    down = set(down) if down is not None else set()\n    press = set(press) if press is not None else set()\n    up = up - down\n    press = press - down\n    if len(up) > 0 or len(press) > 0 or len(down) > 0:\n        process = subprocess.Popen([""xte""], stdin=subprocess.PIPE)\n        commands = []\n        for key in up:\n            commands.append(""keyup "" + key)\n        for key in press:\n            commands.append(""key "" + key)\n        for key in down:\n            commands.append(""keydown "" + key)\n        sequence = ""\\n"".join(commands) + ""\\n""\n        process.communicate(input=sequence)\n        process.wait()\n\nPYKB_KEYS_DOWN = set()\ndef sendkeys_pykb(up=None, tap=None, down=None):\n    assert up is None or isinstance(up, (list, tuple, set))\n    assert tap is None or isinstance(tap, (list, tuple, set))\n    assert down is None or isinstance(down, (list, tuple, set))\n    up = set(up) if up is not None else set()\n    down = set(down) if down is not None else set()\n    tap = set(tap) if tap is not None else set()\n    #up = up - down\n    tap = tap - down\n    #print(""up"", up, ""down"", down, ""tap"", tap)\n    #up = up.union(set([""a"", ""w"", ""s"", ""d""]))\n    up = up.union(PYKB_KEYS_DOWN)\n\n    #down = [""w""]\n    if len(up) > 0:\n        for key in up:\n            #print(""up"", key)\n            KEYBOARD.release_key(key)\n    if len(tap) > 0:\n        for key in tap:\n            KEYBOARD.tap_key(key)\n    if len(down) > 0:\n        #KEYBOARD.press_keys(list(down))\n        for key in down:\n            #print(""down"", key)\n            #pass\n            #KEYBOARD.tap_key(key)\n            #KEYBOARD.release_key(key)\n            KEYBOARD.press_key(key)\n    #time.sleep(0.1)\n\n    PYKB_KEYS_DOWN.clear()\n    for key in down:\n        PYKB_KEYS_DOWN.add(key)\n\ndef xwininfo(win_id):\n    process = subprocess.Popen([""xwininfo"", ""-id"", str(win_id)], stdout=subprocess.PIPE)\n    process.wait()\n    assert process.returncode == 0\n    lines = process.stdout.readlines()\n    lines = [line.strip() for line in lines]\n    lines = [line for line in lines if len(line) > 0]\n    return lines\n\ndef get_window_coordinates(win_id):\n    lines = xwininfo(win_id)\n\n    #for line in lines:\n    #    print(line)\n\n    x1 = None\n    y1 = None\n    h = None\n    w = None\n    for line in lines:\n        line = line.lower()\n        if line.startswith(""absolute upper-left x:""):\n            x1 = int(line[len(""absolute upper-left x:""):].strip())\n        elif line.startswith(""absolute upper-left y:""):\n            y1 = int(line[len(""absolute upper-left y:""):].strip())\n        elif line.startswith(""width:""):\n            w = int(line[len(""width:""):].strip())\n        elif line.startswith(""height:""):\n            h = int(line[len(""height:""):].strip())\n    assert x1 is not None\n    assert y1 is not None\n    assert h is not None\n    assert w is not None\n    return x1, y1, x1+w, y1+h\n\ndef find_window_ids(needle_name):\n    assert len(needle_name) > 0\n    process = subprocess.Popen([""xdotool"", ""search"", ""--name"", needle_name], stdout=subprocess.PIPE)\n    process.wait()\n    result = process.stdout.readlines()\n    #print(""process.returncode"", process.returncode)\n    #print(""lines"", result)\n    if process.returncode == 0:\n        win_ids = [int(line.strip()) for line in result]\n        return win_ids\n    elif process.returncode == 1:\n        if len(result) == 0:\n            # xdotool exists, but window not found\n            return []\n        else:\n            raise Exception(""Received error return code when calling xdotool and non-zero output (xdotool not installed?): %s"" % (result,))\n    else:\n        raise Exception(""Unexpected returncode from xdotool: %s"" % (process.returncode,))\n\ndef get_window_name(win_id):\n    wininfo = xwininfo(win_id)\n    pos = wininfo[0].find(\'""\')\n    assert pos > -1\n    return wininfo[0][pos+1:-1]\n\ndef get_active_window_id():\n    process = subprocess.Popen([""xdotool"", ""getactivewindow""], stdout=subprocess.PIPE)\n    process.wait()\n    assert process.returncode == 0\n    result = process.stdout.readlines()\n    win_ids = [int(line.strip()) for line in result]\n    return win_ids[-1]\n\ndef activate_window(win_id):\n    process = subprocess.Popen([""xdotool"", ""windowactivate"", ""--sync"", str(win_id)])\n    process.wait()\n    assert process.returncode == 0\n'"
scripts/add_steering_wheel_to_replay_memory.py,0,"b'from __future__ import division, print_function\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom lib import steering_wheel as swlib\n\nimport numpy as np\nimport time\nimport sqlite3\nfrom scipy import misc\nimport cv2\nimport imgaug as ia\n\nif sys.version_info[0] == 3:\n    raw_input = input\n\ntry:\n    xrange\nexcept NameError:\n    xrange = range\n\nMAX_TIMEDIFF_MS = 500\nMAX_PIXELDIFF = 800\n\ndef main():\n    #cv2.namedWindow(""overview"", cv2.WINDOW_NORMAL)\n\n    key = raw_input(""This will add the approximated steering wheel location to all states in the database. Press \'y\' to continue."")\n    assert key == ""y""\n\n    tracker_classical = swlib.SteeringWheelTracker()\n    tracker_cnn = swlib.SteeringWheelTrackerCNN()\n    memories = [\n        (replay_memory.ReplayMemory.create_instance_reinforced(val=False), 10**6),\n        (replay_memory.ReplayMemory.create_instance_reinforced(val=True), 10**6),\n        (replay_memory.ReplayMemory.create_instance_supervised(val=False), 60*10),\n        (replay_memory.ReplayMemory.create_instance_supervised(val=True), 60*10)\n    ]\n    for memory, memory_reset_every in memories:\n        add_columns(memory)\n\n        last_datetime = None\n        last_scr = None\n        last_was_new_scene = False\n        last_autoreset = 0\n\n        for idx in xrange(memory.id_min, memory.id_max+1):\n            state = memory.get_state_by_id(idx)\n            if state.steering_wheel_classical is None or state.steering_wheel_cnn is None:\n                timediff_ms = 0 if last_datetime is None else (state.from_datetime - last_datetime).total_seconds() * 1000\n                """"""\n                if last_scr is not None:\n                    from scipy import misc\n                    print(state.screenshot_rs.shape, last_scr.shape)\n                    print(state.screenshot_rs[20:-30, ...] - last_scr[20:-30, ...])\n                    misc.imshow(state.screenshot_rs[20:-30, ...])\n                """"""\n                if last_scr is None:\n                    #pixeldiff = 0\n                    same_scene = True\n                else:\n                    img1 = np.average(np.average(state.screenshot_rs[20:-30, ...], axis=1), axis=0)\n                    img2 = np.average(np.average(last_scr[20:-30, ...], axis=1), axis=0)\n                    #pixeldiff = 0 if last_scr is None else np.average(np.abs(img1 - img2))\n                    same_scene = screens_show_same_scene(last_scr, state.screenshot_rs)\n\n                if last_was_new_scene:\n                    print(""resetting (last was new)"")\n                    tracker_classical.reset()\n                    tracker_cnn.reset()\n                    last_was_new_scene = False\n                elif last_autoreset > memory_reset_every:\n                    print(""resetting (auto)..."")\n                    tracker_classical.reset()\n                    tracker_cnn.reset()\n                    last_autoreset = 0\n                    last_was_new_scene = False\n                elif timediff_ms > MAX_TIMEDIFF_MS or not same_scene:\n                    print(""resetting (new scene)..."")\n                    tracker_classical.reset()\n                    tracker_cnn.reset()\n                    last_was_new_scene = True\n                else:\n                    last_was_new_scene = False\n                (wheel_deg_classical, (wheel_deg_raw1_classical, wheel_deg_raw2_classical)), time_req_classical = track(tracker_classical, state.screenshot_rs)\n                (wheel_deg_cnn, wheel_deg_raw_cnn), time_req_cnn = track(tracker_cnn, state.screenshot_rs)\n\n                img_viz = np.copy(state.screenshot_rs)\n                img_viz = util.draw_text(img_viz, x=0, y=0, text=""%.2f | %.2f / %.2f\\n%.2f | %.2f"" % (wheel_deg_classical, wheel_deg_raw1_classical, wheel_deg_raw2_classical, wheel_deg_cnn, wheel_deg_raw_cnn))\n                #cv2.imshow(""overview"", img_viz[:,:,::-1])\n                #cv2.waitKey(150)\n                #if timediff_ms > 1000:\n                #    from scipy import misc\n                #    misc.imshow(np.hstack([last_scr, state.screenshot_rs]))\n\n                state.steering_wheel_classical = wheel_deg_classical\n                state.steering_wheel_raw_one_classical = wheel_deg_raw1_classical\n                state.steering_wheel_raw_two_classical = wheel_deg_raw2_classical\n                state.steering_wheel_cnn = wheel_deg_cnn\n                state.steering_wheel_raw_cnn = wheel_deg_raw_cnn\n                memory.update_state(state.idx, state, commit=False)\n\n                last_datetime = state.from_datetime\n                last_scr = state.screenshot_rs\n                last_autoreset += 1\n            else:\n                timediff_ms = 0\n                wheel_deg_classical = 0\n                wheel_deg_cnn = 0\n                time_req_classical = 0\n                time_req_cnn = 0\n\n            if idx % 1000 == 0:\n                print(""#%d/%d timediff=%d wheel_cl=%.2f wheel_cnn=%.2f in_cl=%.4fs in_cnn=%.4f"" % (idx, memory.id_max, timediff_ms, wheel_deg_classical, wheel_deg_cnn, time_req_classical, time_req_cnn))\n                memory.commit()\n\n        memory.commit()\n\ndef track(tracker, scr):\n    time_start = time.time()\n    result = tracker.estimate_angle(scr)\n    time_end = time.time()\n    return result, time_end - time_start\n\ndef screens_show_same_scene(scr1, scr2):\n    scr1 = ia.imresize_single_image(scr1, (50, 70))\n    scr2 = ia.imresize_single_image(scr2, (50, 70))\n    hist1 = cv2.calcHist([scr1[...,0], scr1[...,1], scr1[...,2]], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    hist2 = cv2.calcHist([scr2[...,0], scr2[...,1], scr2[...,2]], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    diff = np.sum(np.abs(hist1 - hist2))\n    return diff <= MAX_PIXELDIFF\n\ndef add_columns(memory):\n    column_names = [\n        ""steering_wheel"",\n        ""steering_wheel_raw_one"",\n        ""steering_wheel_raw_two"",\n        ""steering_wheel_cnn"",\n        ""steering_wheel_raw_cnn""\n    ]\n    for name in column_names:\n        try:\n            memory.conn.execute(""ALTER TABLE states ADD COLUMN %s REAL"" % (name,))\n        except sqlite3.OperationalError as exc:\n            if ""duplicate column"" in str(exc):\n                print(""Column \'%s\' already added to table."" % (name,))\n            else:\n                raise exc\n\n    memory.commit()\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/collect_experiences_supervised.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import windowhandling\nfrom lib import ets2window\nfrom lib import ets2game\nfrom lib import screenshot\nfrom lib import speed as speedlib\nfrom lib import replay_memory\nfrom lib import states as stateslib\nfrom lib import actions as actionslib\nfrom lib import util\nfrom lib import pykeylogger\nfrom lib import steering_wheel as swlib\nfrom lib import rewards as rewardslib\nfrom config import Config\nimport time\nfrom datetime import datetime\nimport cv2\nimport imgaug as ia\nimport numpy as np\nimport random\nimport argparse\nnp.random.seed(42)\nrandom.seed(42)\n\ndef main():\n    parser = argparse.ArgumentParser(description=""Train trucker via reinforcement learning."")\n    parser.add_argument(""--val"", default=False, action=""store_true"", help=""Whether to add experiences to the validation memory."", required=False)\n    args = parser.parse_args()\n\n    cv2.namedWindow(""overview"", cv2.WINDOW_NORMAL)\n    cv2.resizeWindow(""overview"", Config.MODEL_WIDTH + 256, Config.MODEL_HEIGHT + 128)\n    cv2.moveWindow(""overview"", 55, 1080-720)\n    cv2.waitKey(100)\n\n    class OnRouteAdvisorVisible(object):\n        def __init__(self):\n            self.memory = replay_memory.ReplayMemory.create_instance_supervised(val=args.val)\n            self.is_collecting = False\n            self.is_collecting_keylisten = True\n            self.last_state = None\n            self.sw_tracker = swlib.SteeringWheelTrackerCNN()\n            self.sw_tracker_classical = swlib.SteeringWheelTracker()\n\n        def __call__(self, game, scr):\n            speed_image = game.win.get_speed_image(scr)\n            som = speedlib.SpeedOMeter(speed_image)\n\n            keys_modifiers, keys = pykeylogger.fetch_keydowns()\n            ctrl_pressed = (keys_modifiers[""left ctrl""] == True or keys_modifiers[""right ctrl""] == True)\n            if ctrl_pressed:\n                if self.is_collecting_keylisten:\n                    self.is_collecting = not self.is_collecting\n                self.is_collecting_keylisten = False\n            else:\n                self.is_collecting_keylisten = True\n            #print(""keys_any_changes"", keys_any_changes)\n            #print(""keys:"", keys)\n            #print(""keys_modifiers:"", keys_modifiers)\n\n            reward = None\n            action_up_down = actionslib.keys_to_action_up_down(keys)\n            action_left_right = actionslib.keys_to_action_left_right(keys)\n            p_explore = 0\n\n            from_datetime = datetime.utcnow()\n            screenshot_rs = ia.imresize_single_image(scr, (Config.MODEL_HEIGHT, Config.MODEL_WIDTH), interpolation=""cubic"")\n            screenshot_rs_jpg = util.compress_to_jpg(screenshot_rs)\n            speed = som.predict_speed()\n            is_reverse = game.win.is_reverse(scr)\n            is_offence_shown = game.win.is_offence_shown(scr)\n            is_damage_shown = game.win.is_damage_shown(scr)\n\n            scr_de = util.decompress_img(screenshot_rs_jpg)\n            sw_classical, (sw_raw_one_classical, sw_raw_two_classical) = self.sw_tracker_classical.estimate_angle(scr_de)\n            sw, sw_raw = self.sw_tracker.estimate_angle(scr_de)\n\n            current_state = stateslib.State(\n                from_datetime=from_datetime,\n                screenshot_rs_jpg=screenshot_rs_jpg,\n                speed=speed,\n                is_reverse=is_reverse,\n                is_offence_shown=is_offence_shown,\n                is_damage_shown=is_damage_shown,\n                reward=None,\n                action_left_right=action_left_right,\n                action_up_down=action_up_down,\n                p_explore=p_explore,\n                steering_wheel_classical=sw_classical,\n                steering_wheel_raw_one_classical=sw_raw_one_classical,\n                steering_wheel_raw_two_classical=sw_raw_two_classical,\n                steering_wheel_cnn=sw,\n                steering_wheel_raw_cnn=sw_raw,\n                allow_cache=False\n            )\n\n            if self.last_state is not None:\n                self.last_state.reward = rewardslib.calculate_reward(self.last_state, current_state)\n                if self.is_collecting:\n                    self.memory.add_state(self.last_state, commit=False)\n\n            do_commit = (game.tick_ingame_route_advisor_idx % 20 == 0)\n            if do_commit:\n                self.memory.commit()\n\n            img = generate_overview_image(\n                screenshot_rs,\n                som, self.memory, do_commit,\n                action_up_down, action_left_right,\n                self.is_collecting\n            )\n            cv2.imshow(""overview"", img[:,:,::-1])\n            cv2.waitKey(1)\n\n            #print(""[Main] Speed:"", som.predict_speed(), som.predict_speed_raw())\n            self.last_state = current_state\n\n    game = ets2game.ETS2Game()\n    game.on_route_advisor_visible = OnRouteAdvisorVisible()\n    game.min_interval = 100 / 1000\n    game.run()\n\ndef generate_overview_image(screenshot_rs, som, memory, do_commit, action_up_down, action_left_right, is_collecting):\n    current_image = screenshot_rs\n    speed_image = som.get_postprocessed_image_rgb()\n\n    """"""\n    time_start = time.time()\n    screenshot_rs_small = ia.imresize_single_image(screenshot_rs, (Config.MODEL_HEIGHT//2, Config.MODEL_WIDTH//2), interpolation=""cubic"")\n    screenshot_rs_small = cv2.cvtColor(screenshot_rs_small, cv2.COLOR_RGB2GRAY)\n    screenshot_rs_small = np.tile(screenshot_rs_small[:, :, np.newaxis], (1, 1, 3))\n    print(""RS %.4f"" % (time.time() - time_start,))\n\n    from lib import util\n    screenshot_rs_small_jpg = util.compress_to_jpg(screenshot_rs_small)\n    time_start = time.time()\n    screenshot_rs_small = util.decompress_img(screenshot_rs_small_jpg)\n    print(""JPG %.4f"" % (time.time() - time_start,))\n    """"""\n\n    h, w = current_image.shape[0:2]\n    #screenshot_rs = ia.imresize_single_image(screenshot_rs, ())\n    h_small, w_small = screenshot_rs.shape[0:2]\n    h_speed, w_speed = speed_image.shape[0:2]\n\n    current_image = np.pad(current_image, ((0, h_small+h_speed), (0, 256), (0, 0)), mode=""constant"")\n    current_image[h:h+h_small, 0:w_small, :] = screenshot_rs\n    current_image[h+h_small:h+h_small+h_speed, 0:w_speed, :] = speed_image\n\n    speed = som.predict_speed()\n\n    texts = [\n        ""memory size: %06d"" % (memory.size,),\n        ""commit: yes"" if do_commit else ""commit: no"",\n        ""action u/d: %s"" % (action_up_down,),\n        ""action l/r: %s"" % (action_left_right,),\n        ""speed: %03d"" % (speed,) if speed is not None else ""speed: NONE"",\n        ""is tracking (press CTRL to toggle)"" if is_collecting else ""NOT tracking (press CTRL to toggle)""\n    ]\n    texts = ""\\n"".join(texts)\n\n    for i, text in enumerate([texts]):\n        current_image = util.draw_text(current_image, x=w+10, y=5+(i*15), size=10, text=text)\n\n    return current_image\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/collect_speed_segments.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import windowhandling\nfrom lib import ets2window\nfrom lib import ets2game\nfrom lib import screenshot\nfrom lib import speed as speedlib\n\nimport time\nimport cv2\nimport numpy as np\nimport os\n\ndef main():\n    cv2.namedWindow(""speed"", cv2.WINDOW_NORMAL)\n    cv2.namedWindow(""speed_bin"", cv2.WINDOW_NORMAL)\n\n    def on_route_advisor_visible(game, scr):\n        speed_image = game.win.get_speed_image(scr)\n        som = speedlib.SpeedOMeter(speed_image)\n\n        speed_image_bin = som.get_postprocessed_image()\n\n        cv2.imshow(""speed"", speed_image)\n        cv2.imshow(""speed_bin"", speed_image_bin.astype(np.uint8)*255)\n        cv2.waitKey(1)\n\n        segments = som.split_to_segments()\n        print(""Found %d segments"" % (len(segments),), [s.arr.shape for s in segments])\n\n        for seg in segments:\n            if not seg.is_in_database():\n                speedlib.SpeedSegmentsDatabase.get_instance().add(seg)\n                print(""Added new segment with key: %s"" % (seg.get_key(),))\n            else:\n                print(""Segment already in database."")\n\n    game = ets2game.ETS2Game()\n    game.on_route_advisor_visible = on_route_advisor_visible\n    game.run()\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/recalculate_rewards.py,0,"b'from __future__ import division, print_function\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import replay_memory\nfrom lib import rewards as rewardslib\n\nimport collections\n\nif sys.version_info[0] == 3:\n    raw_input = input\n\ntry:\n    xrange\nexcept NameError:\n    xrange = range\n\ndef main():\n    key = raw_input(""This will recalculate the rewards of all states in the database. Press \'y\' to continue."")\n    assert key == ""y""\n\n    memories = [\n        replay_memory.ReplayMemory.create_instance_reinforced(val=False),\n        #replay_memory.ReplayMemory.create_instance_reinforced(val=True)\n    ]\n    for memory in memories:\n        state_curr = memory.get_state_by_id(memory.id_min)\n        last_speeds = collections.deque(maxlen=20)\n        for idx in xrange(memory.id_min+1, memory.id_max+1):\n            state_next = memory.get_state_by_id(idx)\n            assert state_curr is not None\n            assert state_next is not None\n            reward = rewardslib.calculate_reward(state_curr, state_next)\n            print(""Changing reward of state %d from %03.2f to %03.2f"" % (idx-1, state_curr.reward, reward))\n            state_curr.reward = reward\n            memory.update_state(state_curr.idx, state_curr, commit=False)\n\n            if state_curr.speed is None:\n                print(""[NOTE] speed is none, last speeds:"", last_speeds)\n            last_speeds.append(state_curr.speed)\n\n            state_prev = state_curr\n\n            if idx % 100 == 0:\n                memory.commit()\n\nif __name__ == ""__main__"":\n    main()\n'"
train_reinforced/batching.py,0,"b'""""""Functions to generate batches for the reinforcement learning part.\nMainly intended for training, though during the playing phase, the same\nfunctions are used.""""""\nfrom __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport models as models_reinforced\n\nimport cv2\nfrom config import Config\nimport imgaug as ia\nfrom lib.util import to_variable, to_cuda, to_numpy\nfrom lib import util\nfrom lib import actions as actionslib\nfrom lib import replay_memory\nimport numpy as np\nfrom scipy import misc\nimport multiprocessing\nimport threading\nimport random\nimport time\n\nif sys.version_info[0] == 2:\n    import cPickle as pickle\n    from Queue import Full as QueueFull\nelif sys.version_info[0] == 3:\n    import pickle\n    from queue import Full as QueueFull\n    xrange = range\n\nGPU = Config.GPU\nNB_REWARD_BINS = 101\n\nclass BatchData(object):\n    """"""Method encapsulating the data of a single batch.\n\n    TODO some of the functions are named like properties, rename\n    """"""\n\n    def __init__(self, curr_idx, images_by_timestep, images_prev_by_timestep, multiactions, rewards, speeds, is_reverse, steering_wheel, steering_wheel_raw, previous_states_distances):\n        self.curr_idx = curr_idx\n        self.images_by_timestep = images_by_timestep\n        self.images_prev_by_timestep = images_prev_by_timestep\n        self.multiactions = multiactions\n        self.rewards = rewards\n        self.speeds = speeds\n        self.is_reverse = is_reverse\n        self.steering_wheel = steering_wheel\n        self.steering_wheel_raw = steering_wheel_raw\n        self.previous_states_distances = previous_states_distances\n\n    @property\n    def batch_size(self):\n        return self.images_by_timestep.shape[1]\n\n    @property\n    def nb_future(self):\n        return self.images_prev_by_timestep.shape[0] - 1\n\n    @property\n    def nb_prev_per_image(self):\n        return self.images_prev_by_timestep.shape[2]\n\n    def reward_bin_idx(self, timestep, inbatch_idx):\n        timestep = self.curr_idx + timestep\n        reward = self.rewards[timestep, inbatch_idx]\n        reward_norm = (reward - Config.MIN_REWARD) / (Config.MAX_REWARD - Config.MIN_REWARD)\n        reward_norm = 1 - reward_norm # top to bottom\n        rewbin = np.clip(int(reward_norm * NB_REWARD_BINS), 0, NB_REWARD_BINS-1) # clip here, because MAX_REWARD ends up giving bin NB_REWARD_BINS, which is 1 too high\n        return rewbin\n\n    def rewards_bins(self, timestep):\n        timestep = self.curr_idx + timestep\n        T, B = self.rewards.shape\n        result = np.zeros((B, NB_REWARD_BINS), dtype=np.float32)\n        for b in xrange(B):\n            rewbin = self.reward_bin_idx(timestep-self.curr_idx, b)\n            result[b, rewbin] = 1\n        return result\n\n    def rewards_bins_all(self):\n        T, B = self.rewards.shape\n        bins_over_time = [self.rewards_bins(t) for t in xrange(-self.curr_idx, T-self.curr_idx)]\n        return np.array(bins_over_time, dtype=np.float32)\n\n    def inputs_supervised(self, volatile=False, requires_grad=True, gpu=GPU):\n        images = to_cuda(to_variable(self.images_by_timestep[0], volatile=volatile, requires_grad=requires_grad), gpu)\n        images_prev = to_cuda(to_variable(self.images_prev_by_timestep[0], volatile=volatile, requires_grad=requires_grad), gpu)\n        return images, images_prev\n\n    def inputs_reinforced_add_numpy(self, timestep=0):\n        timestep = self.curr_idx + timestep\n        B = self.batch_size\n\n        prev_indices_exclusive = [timestep - d for d in self.previous_states_distances]\n        prev_indices_inclusive = [timestep] + prev_indices_exclusive\n\n        ma_vecs = np.zeros((self.batch_size, len(prev_indices_exclusive), 9), dtype=np.float32)\n        for i, idx in enumerate(prev_indices_exclusive):\n            mas = self.multiactions[idx]\n            for b, ma in enumerate(mas):\n                ma_vecs[b, i, :] = actionslib.ACTIONS_TO_MULTIVEC[ma]\n        ma_vecs = ma_vecs.reshape(self.batch_size, -1) # (B, P*9) with P=number of previous images\n\n        speeds = self.speeds[prev_indices_inclusive, :]\n        steering_wheel = (self.steering_wheel[prev_indices_inclusive, :] - Config.STEERING_WHEEL_CNN_MIN) / (Config.STEERING_WHEEL_CNN_MAX - Config.STEERING_WHEEL_CNN_MIN)\n        steering_wheel_raw = (self.steering_wheel_raw[prev_indices_inclusive, :] - Config.STEERING_WHEEL_RAW_CNN_MIN) / (Config.STEERING_WHEEL_RAW_CNN_MAX - Config.STEERING_WHEEL_RAW_CNN_MIN)\n        vals = {\n            ""speeds"": np.squeeze(np.clip(speeds / Config.MAX_SPEED, 0, 1)),\n            ""is_reverse"": np.squeeze(self.is_reverse[prev_indices_inclusive, :]),\n            ""steering_wheel"": np.squeeze(steering_wheel*2 - 1),\n            ""steering_wheel_raw"": np.squeeze(steering_wheel_raw*2 - 1),\n            ""multiactions_vecs"": ma_vecs\n        }\n        if B == 1:\n            vals[""speeds""] = vals[""speeds""][:, np.newaxis]\n            vals[""is_reverse""] = vals[""is_reverse""][:, np.newaxis]\n            vals[""steering_wheel""] = vals[""steering_wheel""][:, np.newaxis]\n            vals[""steering_wheel_raw""] = vals[""steering_wheel_raw""][:, np.newaxis]\n        vals[""speeds""] = vals[""speeds""].transpose((1, 0)) # (P, B) => (B, P) with P=number of previous images\n        vals[""is_reverse""] = vals[""is_reverse""].transpose((1, 0)) # (P, B) => (B, P) with P=number of previous images\n        vals[""steering_wheel""] = vals[""steering_wheel""].transpose((1, 0)) # (P, B) => (B, P) with P=number of previous images\n        vals[""steering_wheel_raw""] = vals[""steering_wheel_raw""].transpose((1, 0)) # (P, B) => (B, P) with P=number of previous images\n\n        return vals\n\n    def inputs_reinforced_add(self, volatile=False, requires_grad=True, gpu=GPU):\n        return to_cuda(to_variable(self.inputs_reinforced_add_numpy(), volatile=volatile, requires_grad=requires_grad), gpu)\n\n    def future_inputs_supervised(self, volatile=False, requires_grad=True, gpu=GPU):\n        images = to_cuda(to_variable(self.images_by_timestep[1:], volatile=volatile, requires_grad=requires_grad), gpu)\n        images_prev = to_cuda(to_variable(self.images_prev_by_timestep[1:], volatile=volatile, requires_grad=requires_grad), gpu)\n        return images, images_prev\n\n    def future_reinforced_add(self, volatile=False, requires_grad=True, gpu=GPU):\n        vals = {\n            ""speeds"": [],\n            ""is_reverse"": [],\n            ""steering_wheel"": [],\n            ""steering_wheel_raw"": [],\n            ""multiactions_vecs"": []\n        }\n        for timestep in xrange(1, self.nb_future+1):\n            inputs_ts = self.inputs_reinforced_add_numpy(timestep=timestep)\n            vals[""speeds""].append(inputs_ts[""speeds""])\n            vals[""is_reverse""].append(inputs_ts[""is_reverse""])\n            vals[""steering_wheel""].append(inputs_ts[""steering_wheel""])\n            vals[""steering_wheel_raw""].append(inputs_ts[""steering_wheel_raw""])\n            vals[""multiactions_vecs""].append(inputs_ts[""multiactions_vecs""])\n        vals[""speeds""] = np.array(vals[""speeds""], dtype=np.float32)\n        vals[""is_reverse""] = np.array(vals[""is_reverse""], dtype=np.float32)\n        vals[""steering_wheel""] = np.array(vals[""steering_wheel""], dtype=np.float32)\n        vals[""steering_wheel_raw""] = np.array(vals[""steering_wheel_raw""], dtype=np.float32)\n        vals[""multiactions_vecs""] = np.array(vals[""multiactions_vecs""], dtype=np.float32)\n\n        T, B, _ = vals[""speeds""].shape\n        vals_flat = {\n            ""speeds"": vals[""speeds""].reshape((T*B, -1)),\n            ""is_reverse"": vals[""is_reverse""].reshape((T*B, -1)),\n            ""steering_wheel"": vals[""steering_wheel""].reshape((T*B, -1)),\n            ""steering_wheel_raw"": vals[""steering_wheel_raw""].reshape((T*B, -1)),\n            ""multiactions_vecs"": vals[""multiactions_vecs""].reshape((T*B, -1))\n        }\n\n        return to_cuda(to_variable(vals_flat, volatile=volatile, requires_grad=requires_grad), gpu)\n\n    def inputs_successor_multiactions_vecs(self, volatile=False, requires_grad=True, gpu=GPU):\n        # the successor gets in actions a and has to predict the next\n        # state, i.e. for tuples (s, a, r, s\') it gets a and predicts s\',\n        # hence the future actions here start at curr_idx (current state index)\n        # and end at -1\n        arr = models_reinforced.SuccessorPredictor.multiactions_to_vecs(self.multiactions[self.curr_idx:-1])\n        assert arr.shape == (self.nb_future, self.batch_size, 9)\n        return to_cuda(to_variable(arr, volatile=volatile, requires_grad=requires_grad), gpu)\n\n    def direct_rewards_values(self, volatile=False, requires_grad=True, gpu=GPU):\n        rews = self.rewards[self.curr_idx, :][:,np.newaxis]\n        rews = np.tile(rews, (1, 9))\n        return to_cuda(to_variable(rews, volatile=volatile, requires_grad=requires_grad), gpu)\n\n    def future_direct_rewards_values(self, volatile=False, requires_grad=True, gpu=GPU):\n        rews = self.rewards[self.curr_idx+1:, :][:, :, np.newaxis]\n        rews = np.tile(rews, (1, 1, 9))\n        return to_cuda(to_variable(rews, volatile=volatile, requires_grad=requires_grad), gpu)\n\n    def outputs_dr_gt(self, volatile=False, requires_grad=True, gpu=GPU):\n        # for a tuple (s, a, r, s\'), the reward r is ought to be predicted\n        # that is, the reward for the previous action, which is dependent\n        # on the new state s\' that it created\n        # it is saved at the previous timestep, i.e. at state s, hence\n        # here -1\n        bins = self.rewards_bins(-1)\n        return to_cuda(to_variable(bins, volatile=volatile, requires_grad=requires_grad), gpu)\n\n    def outputs_dr_future_gt(self, volatile=False, requires_grad=True, gpu=GPU):\n        # starting at curr_idx and ending at -1 here for the same reason\n        # as above\n        bins = self.rewards_bins_all()\n        bins = bins[self.curr_idx:-1]\n        return to_cuda(to_variable(bins, volatile=volatile, requires_grad=requires_grad), gpu)\n\n    def outputs_ae_gt(self, volatile=False, requires_grad=True, gpu=GPU):\n        imgs = self.images_by_timestep[0, ...]\n        imgs = np.clip(imgs*255, 0, 255).astype(np.uint8).transpose((0, 2, 3, 1))\n        imgs_rs = ia.imresize_many_images(imgs, (45, 80))\n        imgs_rs = (imgs_rs / 255.0).astype(np.float32).transpose((0, 3, 1, 2))\n        return to_cuda(to_variable(imgs_rs, volatile=volatile, requires_grad=requires_grad), gpu)\n\n    def chosen_action_indices(self):\n        mas_timestep = self.multiactions[self.curr_idx]\n        indices = [np.argmax(actionslib.ACTIONS_TO_MULTIVEC[ma]) for ma in mas_timestep]\n        return indices\n\n    def chosen_action_indices_future(self):\n        indices_by_timestep = []\n        for t_idx in xrange(self.nb_future):\n            mas_timestep = self.multiactions[t_idx]\n            indices = [np.argmax(actionslib.ACTIONS_TO_MULTIVEC[ma]) for ma in mas_timestep]\n            indices_by_timestep.append(indices)\n        return indices_by_timestep\n\n    def draw(self, timestep=0, inbatch_idx=0):\n        timestep = self.curr_idx + timestep\n        img = self.images_by_timestep[timestep-self.curr_idx, inbatch_idx, :, :, :]\n        img = (img.transpose((1, 2, 0))*255).astype(np.uint8)\n        imgs_prev = self.images_prev_by_timestep[timestep-self.curr_idx, inbatch_idx, :, :, :]\n        imgs_prev = (imgs_prev.transpose((1, 2, 0))*255).astype(np.uint8)\n\n        h, w = img.shape[0:2]\n        imgs_viz = [img] + [np.tile(imgs_prev[..., i][:, :, np.newaxis], (1, 1, 3)) for i in xrange(imgs_prev.shape[2])]\n        imgs_viz = [ia.imresize_single_image(im, (h, w), interpolation=""cubic"") for im in imgs_viz]\n        imgs_viz = np.hstack(imgs_viz)\n\n        rewards_bins = self.rewards_bins_all()\n        mas = [self.multiactions[i][inbatch_idx] for i in xrange(timestep-self.nb_prev_per_image, timestep)]\n        pos = [timestep] + [timestep-d for d in self.previous_states_distances]\n        reinforced_add = self.inputs_reinforced_add_numpy(timestep=timestep-self.curr_idx)\n        outputs_dr_gt = self.outputs_dr_gt()[inbatch_idx]\n        texts = [\n            ""pos: "" + "" "".join([str(i) for i in pos]),\n            ""Rewards:      "" + "" "".join([""%.2f"" % (self.rewards[i, inbatch_idx],) for i in pos]),\n            ""Rewards bins: "" + "" "".join([""%d"" % (np.argmax(rewards_bins[i, inbatch_idx]),) for i in pos]),\n            ""Speeds:       "" + "" "".join([""%.2f"" % (self.speeds[i, inbatch_idx],) for i in pos]),\n            ""Multiactions: "" + "" "".join([""%s%s"" % (ma[0], ma[1]) for ma in mas]),\n            ""Speeds RA:    "" + "" "".join([""%.3f"" % (reinforced_add[""speeds""][inbatch_idx, i],) for i in xrange(reinforced_add[""speeds""].shape[1])]),\n            ""outputs_dr_gt[t=-1]: "" + ""%d"" % (np.argmax(to_numpy(outputs_dr_gt)),)\n        ]\n        texts = ""\\n"".join(texts)\n\n        result = np.zeros((imgs_viz.shape[0]*3, imgs_viz.shape[1], 3), dtype=np.uint8)\n        util.draw_image(result, x=0, y=0, other_img=imgs_viz, copy=False)\n        result = util.draw_text(result, x=0, y=imgs_viz.shape[0]+4, text=texts, size=9)\n        return result\n\ndef states_to_batch(previous_states_list, states_list, augseq, previous_states_distances, model_height, model_width, model_prev_height, model_prev_width):\n    """"""Convert multiple chains of states into a batch.\n\n    Parameters\n    ----------\n    previous_states_list : list of list of State\n        Per chain of states a list of the previous states.\n        First index of the list is the batch index,\n        second index is the timestep. The oldest states come first.\n    states_list : list of list of State\n        Per chain of states a list of states that contain the ""current""\n        state at the start, followed by future states.\n        First index is batch index, second timestep.\n    augseq : Augmenter\n        Sequence of augmenters to apply to each image. Use Noop() to make\n        no changes.\n    previous_states_distances : list of int\n        List of distances relative to the current state. Each distance\n        refers to one previous state to add to the model input.\n        E.g. [2, 1] adds the state 200ms and 100ms before the current ""state"".\n    model_height : int\n        Height of the model input images (current state).\n    model_width : int\n        Width of the model input images (current state).\n    model_prev_height : int\n        Height of the model input images (previous states).\n    model_prev_width : int\n        Width of the model input images (previous states).\n\n    Returns\n    ----------\n    List of BatchData\n    """"""\n    assert isinstance(previous_states_list, list)\n    assert isinstance(states_list, list)\n    assert isinstance(previous_states_list[0], list)\n    assert isinstance(states_list[0], list)\n    assert len(previous_states_list) == len(states_list)\n\n    B = len(states_list)\n    H, W = model_height, model_width\n    Hp, Wp = model_prev_height, model_prev_width\n\n    nb_prev_load = max(previous_states_distances)\n    nb_future_states = len(states_list[0]) - 1\n    nb_timesteps = nb_prev_load + 1 + nb_future_states\n    #images = np.zeros((nb_timesteps, B, H, W, 3), dtype=np.uint8)\n    #images_gray = np.zeros((nb_timesteps, B, Hp, Wp), dtype=np.float32)\n    images_by_timestep = np.zeros((1+nb_future_states, B, H, W, 3), dtype=np.float32)\n    images_gray = np.zeros((nb_timesteps, B, Hp, Wp), dtype=np.float32)\n    multiactions = [[] for i in xrange(nb_timesteps)]\n    rewards = np.zeros((nb_timesteps, B), dtype=np.float32)\n    speeds = np.zeros((nb_timesteps, B), dtype=np.float32)\n    is_reverse = np.zeros((nb_timesteps, B), dtype=np.float32)\n    steering_wheel = np.zeros((nb_timesteps, B), dtype=np.float32)\n    steering_wheel_raw = np.zeros((nb_timesteps, B), dtype=np.float32)\n\n    augseqs_det = [augseq.to_deterministic() for _ in xrange(len(states_list))]\n\n    for b, (previous_states, states) in enumerate(zip(previous_states_list, states_list)):\n        augseq_det = augseqs_det[b]\n\n        all_states = previous_states + states\n        for t, state in enumerate(all_states):\n            imgy = cv2.cvtColor(state.screenshot_rs, cv2.COLOR_RGB2GRAY)\n            imgy_rs = downscale(imgy, Hp, Wp)\n            imgy_rs_aug = augseq_det.augment_image(imgy_rs)\n            images_gray[t, b, ...] = imgy_rs\n\n            multiactions[t].append(state.multiaction)\n            rewards[t, b] = state.reward\n            if state.speed is not None:\n                speeds[t, b] = state.speed\n            if state.is_reverse is not None:\n                is_reverse[t, b] = int(state.is_reverse)\n            if state.steering_wheel_cnn is not None:\n                steering_wheel[t, b] = state.steering_wheel_cnn\n            if state.steering_wheel_raw_cnn is not None:\n                steering_wheel_raw[t, b] = state.steering_wheel_raw_cnn\n    images_gray = images_gray[..., np.newaxis]\n\n    for b, states in enumerate(states_list):\n        augseq_det = augseqs_det[b]\n\n        for i, state in enumerate(states):\n            state = states[i]\n            images_by_timestep[i, b, ...] = augseq_det.augment_image(downscale(state.screenshot_rs, H, W))\n\n    nb_prev_per_img = len(previous_states_distances)\n    images_prev_by_timestep = np.zeros((1+nb_future_states, B, Hp, Wp, nb_prev_per_img), dtype=np.float32)\n    for t in xrange(1 + nb_future_states):\n        indices = [nb_prev_load+t-d for d in previous_states_distances]\n        prev = images_gray[indices]\n        prev = prev.transpose((1, 2, 3, 4, 0)).reshape((B, Hp, Wp, nb_prev_per_img))\n        images_prev_by_timestep[t] = prev\n    images_by_timestep = (images_by_timestep.astype(np.float32) / 255.0).transpose((0, 1, 4, 2, 3))\n    images_prev_by_timestep = (images_prev_by_timestep.astype(np.float32) / 255.0).transpose((0, 1, 4, 2, 3))\n\n    return BatchData(nb_prev_load, images_by_timestep, images_prev_by_timestep, multiactions, rewards, speeds, is_reverse, steering_wheel, steering_wheel_raw, previous_states_distances)\n\ndef downscale(im, h, w):\n    if im.ndim == 2:\n        im = im[:, :, np.newaxis]\n        return np.squeeze(ia.imresize_single_image(im, (h, w), interpolation=""cubic""))\n    else:\n        return ia.imresize_single_image(im, (h, w), interpolation=""cubic"")\n\nclass BatchLoader(object):\n    """"""Class to load batches from the replay memory.""""""\n\n    def __init__(self, val, batch_size, augseq, previous_states_distances, nb_future_states, model_height, model_width, model_prev_height, model_prev_width):\n        self.val = val\n        self.batch_size = batch_size\n        self.augseq = augseq.deepcopy()\n        self.augseq.reseed(random.randint(0, 10**6))\n        self.previous_states_distances = previous_states_distances\n        self.nb_future_states = nb_future_states\n        self.model_height = model_height\n        self.model_width = model_width\n        self.model_prev_height = model_prev_height\n        self.model_prev_width = model_prev_width\n        self._memory = None\n\n    def load_random_batch(self):\n        if self._memory is None:\n            self._memory = replay_memory.ReplayMemory.create_instance_reinforced(val=self.val)\n            self._memory.update_caches()\n            print(""Connected memory to %s, idmin=%d, idmax=%d"" % (""val"" if self.val else ""train"", self._memory.id_min, self._memory.id_max))\n        memory = self._memory\n\n        nb_prev = max(self.previous_states_distances)\n        nb_timesteps = nb_prev + 1 + self.nb_future_states\n\n        previous_states_list = []\n        states_list = []\n        for b in xrange(self.batch_size):\n            statechain = memory.get_random_state_chain(nb_timesteps)\n            previous_states_list.append(statechain[:nb_prev])\n            states_list.append(statechain[nb_prev:])\n\n        return states_to_batch(previous_states_list, states_list, self.augseq, self.previous_states_distances, self.model_height, self.model_width, self.model_prev_height, self.model_prev_width)\n\nclass BackgroundBatchLoader(object):\n    """"""Class that takes a BatchLoader and executes it many times in background\n    processes.""""""\n\n    def __init__(self, batch_loader, queue_size, nb_workers, threaded=False):\n        self.queue = multiprocessing.Queue(queue_size)\n        self.workers = []\n        self.exit_signal = multiprocessing.Event()\n        for i in range(nb_workers):\n            seed = random.randint(1, 10**6)\n            if threaded:\n                worker = threading.Thread(target=self._load_batches, args=(batch_loader, self.queue, self.exit_signal, None))\n            else:\n                worker = multiprocessing.Process(target=self._load_batches, args=(batch_loader, self.queue, self.exit_signal, seed))\n            worker.daemon = True\n            worker.start()\n            self.workers.append(worker)\n\n    def get_batch(self):\n        return pickle.loads(self.queue.get())\n\n    def _load_batches(self, batch_loader, queue, exit_signal, seed=None):\n        if seed is not None:\n            random.seed(seed)\n            np.random.seed(seed)\n            batch_loader.augseq.reseed(seed)\n            ia.seed(seed)\n\n        while not exit_signal.is_set():\n            batch = batch_loader.load_random_batch()\n\n            start_time = time.time()\n            batch_str = pickle.dumps(batch, protocol=-1)\n            added_to_queue = False # without this, it will add the batch countless times to the queue\n            while not added_to_queue and not exit_signal.is_set():\n                try:\n                    queue.put(batch_str, timeout=1)\n                    added_to_queue = True\n                except QueueFull as e:\n                    pass\n            end_time = time.time()\n        batch_loader._memory.close()\n\n    def join(self):\n        self.exit_signal.set()\n        time.sleep(5)\n\n        while not self.queue.empty():\n            _ = self.queue.get()\n        #self.queue.join()\n\n        for worker in self.workers:\n            #worker.join()\n            worker.terminate()\n\nif __name__ == ""__main__"":\n    from scipy import misc\n    from imgaug import augmenters as iaa\n\n    MODEL_HEIGHT = 90\n    MODEL_WIDTH = 160\n    MODEL_PREV_HEIGHT = 45\n    MODEL_PREV_WIDTH = 80\n\n    loader = BatchLoader(\n        val=False, batch_size=8, augseq=iaa.Noop(),\n        previous_states_distances=[2, 4, 6, 8, 10],\n        nb_future_states=10,\n        model_height=MODEL_HEIGHT, model_width=MODEL_WIDTH,\n        model_prev_height=MODEL_PREV_HEIGHT, model_prev_width=MODEL_PREV_HEIGHT\n    )\n    for _ in xrange(1000):\n        for t in xrange(3):\n            imgs = []\n            for b in xrange(3):\n                print(t, b)\n                batch = loader.load_random_batch()\n                imgs.append(batch.draw(timestep=t, inbatch_idx=b))\n            misc.imshow(np.vstack(imgs))\n'"
train_reinforced/generate_video_frames.py,2,"b'from __future__ import division, print_function\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom train_semisupervised.train import (\n    PREVIOUS_STATES_DISTANCES,\n    MODEL_HEIGHT,\n    MODEL_WIDTH,\n    MODEL_PREV_HEIGHT,\n    MODEL_PREV_WIDTH\n)\nfrom batching import states_to_batch\nfrom train_semisupervised import models as models_semisupervised\nfrom lib.util import to_variable, to_cuda, to_numpy\nfrom lib import util\nfrom lib import actions as actionslib\nfrom config import Config\n\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nimport cPickle as pickle\nimport argparse\nimport numpy as np\nimport cv2\nimport gzip as gz\nimport torch\nimport torch.nn.functional as F\nimport collections\nfrom scipy import misc, ndimage\nfrom skimage import draw\nimport glob\n\ntry:\n    xrange\nexcept NameError:\n    xrange = range\n\ndef main():\n    parser = argparse.ArgumentParser(description=""Generate frames for a video. This requires a previous recording done via train.py --record=\'filename\'."")\n    parser.add_argument(""--record"", default=None, help=""Filepath to recording."", required=True)\n    parser.add_argument(""--outdir"", default=""video_output"", help=""Output directory name."", required=False)\n    parser.add_argument(""--y"", default=False, action=""store_true"", help=""Always let ffmpeg/avconv overwrite existing video files without asking."", required=False)\n    parser.add_argument(""--noredraw"", default=False, action=""store_true"", help=""Skip drawing frames for videos for which the target directory already exists."", required=False)\n    args = parser.parse_args()\n    assert args.record is not None\n\n    if ""*"" in args.record:\n        record_fps = glob.glob(args.record)\n        assert len(record_fps) > 0\n        for record_fp in record_fps:\n            fn = os.path.basename(record_fp)\n            assert fn != """"\n            fn_root = fn[0:fn.index(""."")] #os.path.splitext(fn)\n            assert fn_root != """"\n            outdir = os.path.join(args.outdir, fn_root + ""-"" + str(abs(hash(record_fp)))[0:8])\n\n            process(record_fp, outdir, args.y, args.noredraw)\n    else:\n        process(args.record, args.outdir, args.y, args.noredraw)\n\ndef process(record_fp, outdir_frames, y, noredraw):\n    if outdir_frames[-1] == ""/"":\n        outdir_videos = os.path.dirname(outdir_frames[:-1])\n        dirname_frames_last = os.path.basename(outdir_frames[:-1])\n    else:\n        outdir_videos = os.path.dirname(outdir_frames)\n        dirname_frames_last = os.path.basename(outdir_frames)\n    assert os.path.isfile(record_fp)\n\n    print(""Processing recording \'%s\'..."" % (record_fp,))\n    print(""Writing frames to \'%s\', videos to \'%s\' with filename start \'%s\'..."" % (outdir_frames, outdir_videos, dirname_frames_last))\n\n    with gz.open(record_fp, ""rb"") as f:\n        recording = pickle.load(f)\n\n    if os.path.exists(outdir_frames) and noredraw:\n        print(""Video frames were already drawn, not redrawing"")\n    else:\n        if not os.path.exists(outdir_frames):\n            print(""Target directory for frames does not exist, creating it..."")\n            os.makedirs(outdir_frames)\n\n        fd = FrameDrawer(outdir_videos)\n        for fidx, frames_drawn in enumerate(fd.draw_frames(recording)):\n            print(""Frame %06d of around %06d..."" % (fidx, len(recording[""frames""])))\n            frame_plans = frames_drawn[0]\n            frame_atts = frames_drawn[1]\n            frame_grids = frames_drawn[2]\n            if frame_plans is not None:\n                misc.imsave(os.path.join(outdir_frames, ""plans_%06d.jpg"" % (fidx,)), frame_plans)\n            if frame_atts is not None:\n                misc.imsave(os.path.join(outdir_frames, ""atts_%06d.jpg"" % (fidx,)), frame_atts)\n            if frame_grids is not None:\n                misc.imsave(os.path.join(outdir_frames, ""grids_%06d.jpg"" % (fidx,)), frame_grids)\n            #if fidx > 200:\n            #    break\n\n    if not os.path.exists(outdir_videos):\n        print(""Target directory for videos does not exist, creating it..."")\n        os.makedirs(outdir_videos)\n\n    frame_fps = [""plans_%06d.jpg"", ""atts_%06d.jpg"", ""grids_%06d.jpg""]\n    frame_fps = [os.path.join(outdir_frames, fp) for fp in frame_fps]\n    video_fps = [""plans.mp4"", ""atts.mp4"", ""grids.mp4""]\n    video_fps = [os.path.join(outdir_videos, ""%s-%s"" % (dirname_frames_last, fp)) for fp in video_fps]\n\n    for frame_fp, video_fp in zip(frame_fps, video_fps):\n        #os.system(\'avconv %s -framerate 10 -i ""%s"" -crf 25 -b:v 2000k -vcodec mpeg4 %s\' % (""-y"" if y else """", frame_fp, video_fp))\n        os.system(\'avconv %s -framerate 10 -i ""%s"" -crf 25 -b:v 2000k -vcodec h264 %s\' % (""-y"" if y else """", frame_fp, video_fp))\n\nclass FrameDrawer(object):\n    def __init__(self, outdir):\n        self.outdir = outdir\n\n        checkpoint_supervised = torch.load(""../train_semisupervised/train_semisupervised_model_withshortcuts.tar"")\n        embedder_supervised = models_semisupervised.PredictorWithShortcuts()\n        embedder_supervised.eval()\n        embedder_supervised.load_state_dict(checkpoint_supervised[""predictor_state_dict""])\n        if Config.GPU >= 0:\n            embedder_supervised.cuda(Config.GPU)\n        self.embedder_supervised = embedder_supervised\n\n    def draw_frames(self, recording):\n        previous_states = collections.deque(maxlen=max(PREVIOUS_STATES_DISTANCES))\n        for frame in recording[""frames""]:\n            scr = util.decompress_img(frame[""scr""])\n            scr = np.clip(scr.astype(np.float32) * 1.5, 0, 255).astype(np.uint8)\n            current_state = frame[""state""]\n\n            current_plan_idx = frame[""current_plan_idx""]\n            current_plan_step_idx = frame[""current_plan_step_idx""]\n            idr_v = frame[""idr_v""]\n            idr_adv = frame[""idr_adv""]\n            plan_to_rewards_direct = frame[""plan_to_rewards_direct""]\n            plan_to_reward_indirect = frame[""plan_to_reward_indirect""]\n            plan_to_reward = frame[""plan_to_reward""]\n            plans_ranking = frame[""plans_ranking""]\n\n            if current_plan_idx is not None:\n                frame_plans = self.draw_frame_plans(\n                    scr, current_state,\n                    recording[""plans""],\n                    current_plan_idx, current_plan_step_idx,\n                    idr_v, idr_adv,\n                    plan_to_rewards_direct, plan_to_reward_indirect, plan_to_reward,\n                    plans_ranking\n                )\n            else:\n                frame_plans = None\n\n            if len(previous_states) == previous_states.maxlen:\n                batch = states_to_batch([list(previous_states)], [[current_state]], iaa.Noop(), PREVIOUS_STATES_DISTANCES, MODEL_HEIGHT, MODEL_WIDTH, MODEL_PREV_HEIGHT, MODEL_PREV_WIDTH)\n                inputs_supervised = batch.inputs_supervised(volatile=True, gpu=Config.GPU)\n\n                x_ae, x_grids, x_atts, x_ma, x_flow, x_canny, x_flipped, x_emb = self.embedder_supervised.forward(inputs_supervised[0], inputs_supervised[1])\n\n                frame_attributes = self.draw_frame_attributes(scr, x_atts)\n                frame_grids = self.draw_frame_grids(scr, x_grids)\n            else:\n                frame_attributes = None\n                frame_grids = None\n\n            yield (frame_plans, frame_attributes, frame_grids)\n\n            previous_states.append(current_state)\n\n    def draw_frame_plans(self, scr, state, plans, current_plan_idx, current_plan_step_idx, idr_v, idr_adv, plan_to_rewards_direct, plan_to_reward_indirect, plan_to_reward, plans_ranking):\n        mincolf = 0.2\n        bgcolor = [0, 0, 0]\n        image = np.zeros((720, 1280, 3), dtype=np.uint8)\n        scr_main = ia.imresize_single_image(scr, (int(720*0.58), int(1280*0.58)))\n        util.draw_image(\n            image,\n            y=int((image.shape[0]-scr_main.shape[0])/2),\n            x=1280-scr_main.shape[1]-2,\n            other_img=scr_main,\n            copy=False\n        )\n        image = util.draw_text(\n            image,\n            x=1280-(scr_main.shape[1]//2)-125,\n            y=image.shape[0] - int((image.shape[0]-scr_main.shape[0])/2) + 10,\n            text=""Framerate matches the one that the model sees (10fps)."",\n            size=10,\n            color=[128, 128, 128]\n        )\n\n        def draw_key(key):\n            btn = [\n                [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n                [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n                [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n                [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n            ]\n            btn = np.array(btn, dtype=np.uint8) * 255\n            btn = np.tile(btn[:, :, np.newaxis], (1, 1, 3))\n            if key is None:\n                return np.zeros_like(btn)\n            elif key == """":\n                return btn\n            else:\n                return util.draw_text(btn, x=3, y=3, text=key, size=9, color=[255, 255, 255])\n\n        def multiaction_idx_to_image(multiaction_idx):\n            #btn = np.pad(btn, ((0, 0), (0, 4), (0, 0)), mode=""constant"", constant_values=0)\n            key_to_img = dict()\n            for key in [""W"", ""A"", ""S"", ""D"", None]:\n                key_to_img[key] = draw_key(key)\n\n            multiaction = actionslib.ALL_MULTIACTIONS[multiaction_idx]\n            sw = 1.0 if multiaction[0] == ""W"" else mincolf\n            sa = 1.0 if multiaction[1] == ""A"" else mincolf\n            ss = 1.0 if multiaction[0] == ""S"" else mincolf\n            sd = 1.0 if multiaction[1] == ""D"" else mincolf\n            buttons = [\n                [key_to_img[None], key_to_img[""W""]*sw, key_to_img[None]],\n                [key_to_img[""A""]*sa, key_to_img[""S""]*ss, key_to_img[""D""]*sd]\n            ]\n            buttons_img = np.vstack([\n                np.hstack([btn.astype(np.uint8) for btn in buttons[0]]),\n                np.hstack([btn.astype(np.uint8) for btn in buttons[1]])\n            ])\n            buttons_img = np.pad(buttons_img, ((0, 0), (0, 4), (0, 0)), mode=""constant"", constant_values=0)\n            return buttons_img\n\n        multiaction_idx_to_image_dict = dict([(i, multiaction_idx_to_image(i)) for i in range(len(actionslib.ALL_MULTIACTIONS))])\n        multiaction_to_image_dict = dict([(ma, multiaction_idx_to_image(i)) for i, ma in enumerate(actionslib.ALL_MULTIACTIONS)])\n\n        def plan_to_image(p_multiactions, p_direct_rewards, p_v, padding_bottom=8, minwidth=200):\n            plan_viz = [multiaction_to_image_dict[ma] for ma in p_multiactions]\n            #plan_viz = [np.pad(a, ((0, 20), (2, 2), (0, 0)), mode=""constant"", constant_values=0) for a in plan_viz]\n            plan_viz = [np.pad(a, ((0, 20), (0, 1), (0, 0)), mode=""constant"", constant_values=0) for a in plan_viz]\n            if p_direct_rewards is not None:\n                for j in xrange(len(plan_viz)):\n                    #plan_viz[j] = util.draw_text(plan_viz[j], x=9, y=plan_viz[j].shape[0]-16, text=""r"", size=9, color=[128, 128, 128])\n                    plan_viz[j] = util.draw_text(plan_viz[j], x=11, y=plan_viz[j].shape[0]-13, text=""r %.1f"" % (p_direct_rewards[j],), size=9, color=[128, 128, 128])\n\n            if p_v is not None:\n                plan_viz.append(np.zeros_like(plan_viz[-1]))\n                #plan_viz[-1] = util.draw_text(plan_viz[-1], x=3, y=5, text=""V"", size=9, color=[128, 128, 128])\n                #plan_viz[-1] = util.draw_text(plan_viz[-1], x=9, y=11, text=""V %.1f"" % (p_v,), size=9, color=[255, 255, 255])\n                plan_viz[-1] = util.draw_text(plan_viz[-1], x=5, y=16, text=""V %.1f"" % (p_v,), size=9, color=[255, 255, 255])\n\n            plan_viz = np.hstack(plan_viz)\n            width_extend = minwidth - plan_viz.shape[1] if plan_viz.shape[1] < minwidth else 0\n            #print(""width_extend"", width_extend, minwidth, plan_viz.shape[0])\n            plan_viz = np.pad(plan_viz, ((0, padding_bottom), (0, width_extend), (0, 0)), mode=""constant"", constant_values=0)\n\n            return plan_viz\n\n        # -------------\n        # current plan\n        # -------------\n        current_plan_viz = plan_to_image(\n            plans[current_plan_idx][current_plan_step_idx:],\n            None, None\n        )\n        #current_plan_viz = np.pad(current_plan_viz, ((50, 0), (20, 0), (0, 0)), mode=""constant"", constant_values=0)\n        current_plan_viz = np.pad(current_plan_viz, ((50, 0), (2, 0), (0, 0)), mode=""constant"", constant_values=0)\n        current_plan_viz = util.draw_text(current_plan_viz, x=4, y=4, text=""Current Plan"", color=[255, 255, 255])\n        util.draw_image(image, y=10, x=10, other_img=current_plan_viz, copy=False)\n\n        # -------------\n        # best plans\n        # -------------\n        best_plans_viz = []\n        for i in range(4):\n            plan_idx = plans_ranking[::-1][i]\n            plan = plans[plan_idx]\n            r = plan_to_rewards_direct[plan_idx]\n            v = plan_to_reward_indirect[plan_idx]\n            plan_viz = plan_to_image(plan, r, v)\n            best_plans_viz.append(plan_viz)\n\n        best_plans_viz = np.vstack(best_plans_viz)\n\n        #best_plans_viz = np.pad(best_plans_viz, ((50, 30), (20, 0), (0, 0)), mode=""constant"", constant_values=0)\n        best_plans_viz = np.pad(best_plans_viz, ((50, 30), (2, 0), (0, 0)), mode=""constant"", constant_values=0)\n        best_plans_viz = util.draw_text(best_plans_viz, x=4, y=4, text=""Best Plans"", color=[255, 255, 255])\n        best_plans_viz = util.draw_text(best_plans_viz, x=30, y=best_plans_viz.shape[0]-20, text=""r = expected direct reward at timestep (discounted)\\nV = expected indirect reward at last timestep (discounted)"", color=[128, 128, 128], size=9)\n\n        util.draw_image(image, y=110, x=10, other_img=best_plans_viz, copy=False)\n\n        # --------------\n        # top15\n        # --------------\n        n = 15\n        top_viz = []\n        counts_ud = dict([(action, 0) for action in actionslib.ACTIONS_UP_DOWN])\n        counts_lr = dict([(action, 0) for action in actionslib.ACTIONS_LEFT_RIGHT])\n        for i in range(n):\n            plan_idx = plans_ranking[::-1][i]\n            plan = plans[plan_idx]\n            for ma in plan:\n                counts_ud[ma[0]] += 1\n                counts_lr[ma[1]] += 1\n        sum_ud = np.sum(list(counts_ud.values()))\n        sum_lr = np.sum(list(counts_lr.values()))\n        fracs_ud = [counts_ud[""W""]/sum_ud, counts_ud[""S""]/sum_ud, counts_ud[""~WS""]/sum_ud]\n        fracs_lr = [counts_lr[""A""]/sum_lr, counts_lr[""D""]/sum_lr, counts_lr[""~AD""]/sum_lr]\n        def draw_bar(frac, key, h=30, w=20, margin_right=15):\n            bar = np.zeros((h, 1), dtype=np.uint8) + 32\n            bar[0:int(h*frac)+1] = 255\n            bar = np.flipud(bar)\n            bar = np.tile(bar[:, :, np.newaxis], (1, w, 3))\n            bar = np.pad(bar, ((20, 30), (0, margin_right), (0, 0)), mode=""constant"", constant_values=0)\n            textx = 5\n            if frac*100 >= 10:\n                textx = textx - 3\n            elif frac*100 >= 100:\n                textx = textx - 6\n            bar = ia.draw_text(bar, x=textx, y=2, text=""%.0f%%"" % (frac*100,), size=8, color=[255, 255, 255])\n            keyimg = draw_key(key)\n            util.draw_image(bar, x=(w//2)-keyimg.shape[1]//2, y=bar.shape[0]-keyimg.shape[0]-8, other_img=keyimg, copy=False)\n            return bar\n\n        bars_ud = [draw_bar(fracs_ud[0], ""W""), draw_bar(fracs_ud[1], ""S""), draw_bar(fracs_ud[2], """", margin_right=55)]\n        bars_lr = [draw_bar(fracs_lr[0], ""A""), draw_bar(fracs_lr[1], ""D""), draw_bar(fracs_lr[2], """")]\n        top_viz = np.hstack(bars_ud + bars_lr)\n        top_viz = np.pad(top_viz, ((50, 30), (20, 180), (0, 0)), mode=""constant"", constant_values=0)\n        top_viz = util.draw_text(top_viz, x=4, y=4, text=""Share Of Keys (Top %d Plans)"" % (n,), color=[255, 255, 255])\n        top_viz = util.draw_text(top_viz, x=4, y=top_viz.shape[0]-20, text=""Percent of actions among top %d plans that contain a top/down or left/right key"" % (n,), color=[128, 128, 128], size=9)\n\n        util.draw_image(image, y=430, x=10, other_img=top_viz, copy=False)\n\n        # --------------\n        # other\n        # --------------\n        other_viz = np.zeros((300, 500, 3), dtype=np.uint8)\n        other_viz = util.draw_text(other_viz, x=4, y=4, text=""Speed"", color=[255, 255, 255])\n        other_viz = util.draw_text(other_viz, x=150, y=4, text=""Steering Wheel"", color=[255, 255, 255])\n\n        other_viz = util.draw_text(other_viz, x=12, y=65, text=""%d km/h"" % (state.speed if state.speed is not None else -1), color=[255, 255, 255])\n\n        sw_angle = state.steering_wheel_cnn if state.steering_wheel_cnn is not None else 0\n        sw_circle = np.zeros((80, 80, 3), dtype=np.int32)\n        if sw_angle <= -360 or sw_angle >= 360:\n            rr, cc = draw.circle(r=40, c=40, radius=30)\n            sw_circle[rr, cc, :] = 128\n        col = [128, 128, 128] if -360 < sw_angle < 360 else [255, 255, 255]\n        if abs(sw_angle % 360) > 1:\n            if sw_angle < 0:\n                sw_circle = util.draw_direction_circle(\n                    sw_circle,\n                    y=40, x=40,\n                    r_inner=0, r_outer=30,\n                    angle_start=360-(abs(int(sw_angle)) % 360), angle_end=360,\n                    color_border=col,\n                    color_fill=col\n                    #color_fill=[255,0,0]\n                )\n                #sw_circle = util.draw_text(sw_circle, x=5, y=5, text=""%.2f\\n%.2f"" % (abs(int(sw_angle)) % 360, 360-(abs(int(sw_angle)) % 360)), size=12, color=[255, 255, 255])\n            else:\n                sw_circle = util.draw_direction_circle(\n                    sw_circle,\n                    y=40, x=40,\n                    r_inner=0, r_outer=30,\n                    angle_start=0, angle_end=int(sw_angle) % 360,\n                    color_border=col,\n                    color_fill=col\n                    #color_fill=[0,255,0]\n                )\n        rr, cc, val = draw.circle_perimeter_aa(40, 40, radius=30)\n        #sw_circle[rr, cc, :] = sw_circle[rr, cc, :] + np.tile((val * 255)[:,:,np.newaxis], (1, 1, 3))\n        sw_circle[rr, cc, :] += np.tile((val * 255).astype(np.int32)[:,np.newaxis], (1, 3))\n        sw_circle = np.clip(sw_circle, 0, 255).astype(np.uint8)\n        sw_circle = np.pad(sw_circle, ((0, 0), (0, 140), (0, 0)), mode=""constant"", constant_values=0)\n        sw_circle = util.draw_text(sw_circle, x=92, y=27, text=""%d deg"" % (sw_angle,), color=[255, 255, 255])\n        util.draw_image(other_viz, x=150, y=40, other_img=sw_circle, copy=False)\n\n        util.draw_image(image, y=590, x=10, other_img=other_viz, copy=False)\n\n        return image\n\n    def draw_frame_attributes(self, scr, atts):\n        atts = atts[0]\n        mincolf = 0.2\n\n        #print(""space_front raw"", atts[33:37], F.softmax(atts[33:37]))\n        #print(""space_left raw"", atts[37:41], F.softmax(atts[37:41]))\n        #print(""space_right raw"", atts[41:45], F.softmax(atts[41:45].unsqueeze(0)).squeeze(0))\n        road_type = simplesoftmax(to_numpy(atts[0:10]))\n        intersection = simplesoftmax(to_numpy(atts[10:17]))\n        direction = simplesoftmax(to_numpy(atts[17:20]))\n        lane_count = simplesoftmax(to_numpy(atts[20:25]))\n        curve = simplesoftmax(to_numpy(atts[25:33]))\n        space_front = simplesoftmax(to_numpy(atts[33:37]))\n        space_left = simplesoftmax(to_numpy(atts[37:41]))\n        space_right = simplesoftmax(to_numpy(atts[41:45]))\n        offroad = simplesoftmax(to_numpy(atts[45:48]))\n\n        bgcolor = [0, 0, 0]\n        image = np.zeros((720, 1280, 3), dtype=np.uint8) + bgcolor\n        scr_main = ia.imresize_single_image(scr, (int(720*0.58), int(1280*0.58)))\n        util.draw_image(\n            image,\n            y=int((image.shape[0]-scr_main.shape[0])/2),\n            x=1280-scr_main.shape[1]-2,\n            other_img=scr_main,\n            copy=False\n        )\n        image = util.draw_text(\n            image,\n            x=1280-(scr_main.shape[1]//2)-125,\n            y=image.shape[0] - int((image.shape[0]-scr_main.shape[0])/2) + 10,\n            text=""Framerate matches the one that the model sees (10fps)."",\n            size=10,\n            color=[128, 128, 128]\n        )\n\n        # ---------------\n        # Curve\n        # ---------------\n        """"""\n        street = np.zeros((65, 65, 3), dtype=np.float32)\n        street[:, 0:2, :] = 255\n        street[:, -2:, :] = 255\n        street[:, 32:35, :] = 255\n\n        street_left_strong = curve(street\n        """"""\n        curve_left_strong = 255 - ndimage.imread(""../images/video/curve-left-strong.png"", mode=""RGB"")\n        curve_left_medium = 255 - ndimage.imread(""../images/video/curve-left-medium.png"", mode=""RGB"")\n        curve_left_slight = 255 - ndimage.imread(""../images/video/curve-left-slight.png"", mode=""RGB"")\n        curve_straight = 255 - ndimage.imread(""../images/video/curve-straight.png"", mode=""RGB"")\n        curve_right_strong = np.fliplr(curve_left_strong)\n        curve_right_medium = np.fliplr(curve_left_medium)\n        curve_right_slight = np.fliplr(curve_left_slight)\n\n        curve_straight = (curve_straight * np.clip(curve[0], mincolf, 1.0)).astype(np.uint8)\n        curve_left_slight = (curve_left_slight * np.clip(curve[1], mincolf, 1.0)).astype(np.uint8)\n        curve_left_medium = (curve_left_medium * np.clip(curve[2], mincolf, 1.0)).astype(np.uint8)\n        curve_left_strong = (curve_left_strong * np.clip(curve[3], mincolf, 1.0)).astype(np.uint8)\n        curve_right_slight = (curve_right_slight * np.clip(curve[4], mincolf, 1.0)).astype(np.uint8)\n        curve_right_medium = (curve_right_medium * np.clip(curve[5], mincolf, 1.0)).astype(np.uint8)\n        curve_right_strong = (curve_right_strong * np.clip(curve[6], mincolf, 1.0)).astype(np.uint8)\n\n        def add_perc(curve_img, perc, x_correct):\n            col = np.clip(255 * perc, mincolf*255, 255)\n            col = np.array([col, col, col], dtype=np.uint8)\n\n            curve_img_pad = np.pad(curve_img, ((0, 20), (0, 0), (0, 0)), mode=""constant"", constant_values=0)\n\n            x = int(curve_img_pad.shape[1]/2) - 6\n            if (perc*100) >= 100:\n                x = x - 9\n            elif (perc*100) >= 10:\n                x = x - 6\n            x = x + x_correct\n\n            curve_img_pad = util.draw_text(\n                curve_img_pad,\n                x=x,\n                y=curve_img_pad.shape[0]-15,\n                text=""%.0f%%"" % (perc*100,),\n                color=col,\n                size=9\n            )\n            return curve_img_pad\n\n        curve_straight = add_perc(curve_straight, curve[0], x_correct=0)\n        curve_left_slight = add_perc(curve_left_slight, curve[1], x_correct=3)\n        curve_left_medium = add_perc(curve_left_medium, curve[2], x_correct=1)\n        curve_left_strong = add_perc(curve_left_strong, curve[3], x_correct=-1)\n        curve_right_slight = add_perc(curve_right_slight, curve[4], x_correct=-3)\n        curve_right_medium = add_perc(curve_right_medium, curve[5], x_correct=-2)\n        curve_right_strong = add_perc(curve_right_strong, curve[6], x_correct=0)\n\n        curves = np.hstack([\n            curve_left_strong, curve_left_medium, curve_left_slight,\n            curve_straight,\n            curve_right_slight, curve_right_medium, curve_right_strong\n        ])\n\n        curves = np.pad(curves, ((50, 0), (20, 0), (0, 0)), mode=""constant"", constant_values=0)\n        curves = util.draw_text(curves, x=4, y=4, text=""Curve"", color=[255, 255, 255])\n\n        util.draw_image(image, y=50, x=2, other_img=curves, copy=False)\n\n        # ---------------\n        # Lane count\n        # ---------------\n        pics = []\n        for lc_idx in range(4):\n            col = int(np.clip(255*lane_count[lc_idx], 255*mincolf, 255))\n            col = np.array([col, col, col], dtype=np.uint8)\n            lc = lc_idx + 1\n            marking_width = 2\n            street = np.zeros((64, 64, 3), dtype=np.float32)\n            street[:, 0:marking_width, :] = col\n            street[:, -marking_width:, :] = col\n            inner_width = street.shape[1] - 2*marking_width\n            lane_width = int((inner_width - (lc-1)*marking_width) // lc)\n            start = marking_width\n            for i in range(lc-1):\n                mstart = start + lane_width\n                mend = mstart + marking_width\n                street[1::6, mstart:mend, :] = col\n                street[2::6, mstart:mend, :] = col\n                street[3::6, mstart:mend, :] = col\n                start = mend\n\n            x = 14 + 24\n            if lane_count[lc_idx]*100 >= 10:\n                x = x - 8\n            elif lane_count[lc_idx]*100 >= 100:\n                x = x - 12\n\n            street = np.pad(street, ((0, 20), (14, 14), (0, 0)), mode=""constant"", constant_values=0)\n            street = util.draw_text(street, x=x, y=street.shape[0]-14, text=""%.0f%%"" % (lane_count[lc_idx]*100,), size=9, color=col)\n            pics.append(street)\n\n        pics = np.hstack(pics)\n        pics = np.pad(pics, ((55, 0), (20, 0), (0, 0)), mode=""constant"", constant_values=0)\n        pics = util.draw_text(pics, x=4, y=4, text=""Lane Count"", color=[255, 255, 255])\n        util.draw_image(image, y=250, x=2, other_img=pics, copy=False)\n\n        # ---------------\n        # Space\n        # ---------------\n        truck = np.zeros((100, 55, 3), dtype=np.uint8)\n        truck[0:2, :, :] = 255\n        truck[0:20, 0:2, :] = 255\n        truck[0:20, -2:, :] = 255\n        truck[20:22, :, :] = 255\n\n        truck[22:25, 25:27, :] = 255\n        truck[22:25, 29:31, :] = 255\n\n        truck[24:26, :, :] = 255\n        truck[24:, 0:2, :] = 255\n        truck[24:, -2:, :] = 255\n        truck[24:, -2:, :] = 255\n        truck[-2:, :, :] = 255\n\n        truck_full = np.pad(truck, ((50, 50), (100, 50), (0, 0)), mode=""constant"", constant_values=np.average(bgcolor))\n\n        #print(""space_front"", space_front)\n        #print(""space_right"", space_right)\n        #print(""space_left"", space_left)\n        fill_top = 1 * space_front[0] + 0.6 * space_front[1] + 0.25 * space_front[2] + 0 * space_front[3]\n        fill_right = 1 * space_right[0] + 0.6 * space_right[1] + 0.25 * space_right[2] + 0 * space_right[3]\n        fill_left = 1 * space_left[0] + 0.6 * space_left[1] + 0.25 * space_left[2] + 0 * space_left[3]\n\n        r_outer_top = 8 + int((30-8) * fill_top)\n        r_outer_right = 8 + int((30-8) * fill_right)\n        r_outer_left = 8 + int((30-8) * fill_left)\n\n        def fill_to_text(fill):\n            col = np.array([255, 255, 255], dtype=np.uint8)\n            if fill > 0.75:\n                text = ""plenty""\n            elif fill > 0.5:\n                text = ""some""\n            elif fill > 0.25:\n                text = ""low""\n            else:\n                text = ""minimal""\n            return text, col\n\n        #top\n        truck_full = util.draw_direction_circle(\n            truck_full,\n            y=33, x=100+27,\n            r_inner=8, r_outer=30,\n            angle_start=-60, angle_end=60,\n            color_border=[255, 255, 255],\n            color_fill=[0, 0, 0]\n        )\n        truck_full = util.draw_direction_circle(\n            truck_full,\n            y=33, x=100+27,\n            r_inner=8, r_outer=r_outer_top,\n            angle_start=-60, angle_end=60,\n            color_border=[255, 255, 255],\n            color_fill=[255, 255, 255]\n        )\n        #text, col = fill_to_text(fill_top)\n        #truck_full = util.draw_text(truck_full, x=100+27, y=15, text=text, size=9, color=col)\n\n        # right\n        truck_full = util.draw_direction_circle(\n            truck_full,\n            y=100, x=170,\n            r_inner=8, r_outer=30,\n            angle_start=30, angle_end=180-30,\n            color_border=[255, 255, 255],\n            color_fill=[0, 0, 0]\n        )\n        truck_full = util.draw_direction_circle(\n            truck_full,\n            y=100, x=170,\n            r_inner=8, r_outer=r_outer_right,\n            angle_start=30, angle_end=180-30,\n            color_border=[255, 255, 255],\n            color_fill=[255, 255, 255]\n        )\n        #text, col = fill_to_text(fill_right)\n        #truck_full = util.draw_text(truck_full, x=170, y=100, text=text, size=9, color=col)\n\n        # left\n        truck_full = util.draw_direction_circle(\n            truck_full,\n            y=100, x=83,\n            r_inner=8, r_outer=30,\n            angle_start=180+30, angle_end=360-30,\n            color_border=[255, 255, 255],\n            color_fill=[0, 0, 0]\n        )\n        truck_full = util.draw_direction_circle(\n            truck_full,\n            y=100, x=83,\n            r_inner=8, r_outer=r_outer_left,\n            angle_start=180+30, angle_end=360-30,\n            color_border=[255, 255, 255],\n            color_fill=[255, 255, 255]\n        )\n        #text, col = fill_to_text(fill_left)\n        #truck_full = util.draw_text(truck_full, x=75, y=100, text=text, size=9, color=col)\n\n        truck_full = np.pad(truck_full, ((50, 0), (110, 0), (0, 0)), mode=""constant"", constant_values=0)\n        truck_full = util.draw_text(truck_full, x=4, y=4, text=""Space"", color=[255, 255, 255])\n\n        util.draw_image(image, y=450, x=10, other_img=truck_full, copy=False)\n\n        return image\n\n    def draw_frame_grids(self, scr, grids):\n        grids_meta = [\n            (0, ""street boundaries""),\n            (3, ""crashables (except cars)""),\n            (7, ""street markings""),\n            (4, ""current lane""),\n            (1, ""cars""),\n            (2, ""cars in mirrors"")\n        ]\n        titles = [title for idx, title in grids_meta]\n        grids = to_numpy(grids[0])\n        grids = [grids[idx] for idx, title in grids_meta]\n        #self.grid_to_graph(scr, grids[0])\n\n        bgcolor = [0, 0, 0]\n        image = np.zeros((720, 1280, 3), dtype=np.uint8) + bgcolor\n        scr_main = ia.imresize_single_image(scr, (int(720*0.58), int(1280*0.58)))\n        #util.draw_image(image, y=720-scr_main.shape[0], x=1080-scr_main.shape[1], other_img=scr_main, copy=False)\n        util.draw_image(\n            image,\n            y=int((image.shape[0]-scr_main.shape[0])/2),\n            x=1280-scr_main.shape[1]-2,\n            other_img=scr_main,\n            copy=False\n        )\n        image = util.draw_text(\n            image,\n            x=1280-(scr_main.shape[1]//2)-125,\n            y=image.shape[0] - int((image.shape[0]-scr_main.shape[0])/2) + 10,\n            text=""Framerate matches the one that the model sees (10fps)."",\n            size=10,\n            color=[128, 128, 128]\n        )\n\n        grid_rel_size = 0.19\n        scr_small = ia.imresize_single_image(scr, (int(720*grid_rel_size), int(1280*grid_rel_size)))\n        grid_hms = []\n        for grid, title in zip(grids, titles):\n            grid = (grid*255).astype(np.uint8)[:,:,np.newaxis]\n            grid = ia.imresize_single_image(grid, (int(720*grid_rel_size), int(1280*grid_rel_size)), interpolation=""nearest"")\n            grid_hm = util.draw_heatmap_overlay(scr_small, grid/255)\n            grid_hm = np.pad(grid_hm, ((2, 0), (2, 2), (0, 0)), mode=""constant"", constant_values=np.average(bgcolor))\n            #grid_hm = np.pad(grid_hm, ((0, 20), (0, 0), (0, 0)), mode=""constant"", constant_values=0)\n            #grid_hm[-20:, 2:-2, :] = [128, 128, 255]\n            #grid_hm = util.draw_text(grid_hm, x=4, y=grid_hm.shape[0]-16, text=title, size=10, color=[255, 255, 255])\n            grid_hm = np.pad(grid_hm, ((40, 0), (0, 0), (0, 0)), mode=""constant"", constant_values=0)\n            grid_hm = util.draw_text(grid_hm, x=4, y=20, text=title, size=12, color=[255, 255, 255])\n            grid_hms.append(grid_hm)\n        grid_hms = ia.draw_grid(grid_hms, cols=2)\n\n        util.draw_image(image, y=70, x=0, other_img=grid_hms, copy=False)\n\n        return image\n\n    def tsalesman(self, graph):\n        if len(graph) <= 2:\n            return graph\n\n        paths = []\n        for _ in xrange(1000):\n            ids = list(range(len(graph)))\n            np.random.shuffle(ids)\n            path = [graph[idx] for idx in ids]\n            paths.append(path)\n\n        def length_edge(node1, node2):\n            d = math.sqrt((node1[0]-node2[0])**2 + (node1[1]-node2[1])**2)\n            return d\n\n        def length_path(path):\n            length_sum = length_edge(path[0], path[-1])\n            for i in xrange(1, len(path)):\n                length_sum += length_edge(path[i-1], path[i])\n            return length_sum\n\n        paths_l = [(path, length_path(path)) for path in paths]\n        paths = sorted(paths_l, key=lambda t: t[1])\n        return paths[0][0]\n\n    def grid_to_graph(self, scr, grid):\n        import scipy.ndimage as ndimage\n        import scipy.ndimage.filters as filters\n        data = grid\n        neighborhood_size = 7\n        #threshold_max = 0.5\n        threshold_diff = 0.1\n        threshold_score = 0.2\n\n        data_max = filters.maximum_filter(data, neighborhood_size)\n        maxima = (data == data_max)\n        data_min = filters.minimum_filter(data, neighborhood_size)\n        #diff = ((data_max - data_min) > threshold_diff)\n        #maxima[diff == 0] = 0\n        maxima[data_max < 0.2] = 0\n\n        labeled, num_objects = ndimage.label(maxima)\n        slices = ndimage.find_objects(labeled)\n        xx, yy, score = [], [], []\n        for dy, dx in slices:\n            x_center = (dx.start + dx.stop - 1)/2\n            y_center = (dy.start + dy.stop - 1)/2\n            s = np.average(data[dy.start:dy.stop+1, dx.start:dx.stop+1])\n            if s > threshold_score:\n                xx.append(x_center / grid.shape[1])\n                yy.append(y_center / grid.shape[0])\n                score.append(s)\n\n        graph = list(zip(xx, yy, score))\n        path = tsalesman(graph)\n        paths_final = [path]\n\n        scr_viz = np.copy(scr)\n        h, w = scr.shape[0:2]\n        #hup, wup = h/grid.shape[0], w/grid.shape[1]\n        hup, wup = h, w\n        for i, (x, y, s) in enumerate(zip(xx, yy, score)):\n            size = 3*int(s*10)\n            size = size if size % 2 != 0 else size - 1\n            scr_viz = util.draw_point(scr_viz, y=int(y*hup), x=int(x*wup), size=size, color=[0, 255, 0])\n            scr_viz = util.draw_text(scr_viz, y=int(y*hup), x=int(x*wup), text=str(i), color=[0, 255, 0])\n\n        colors = [[255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 255], [0, 0, 0]]\n        for path, col in zip(paths_final, colors):\n            last_x = None\n            last_y = None\n            for (x, y, s) in path:\n                if last_x is not None:\n                    scr_viz = util.draw_line(scr_viz, y1=int(last_y*hup), x1=int(last_x*wup), y2=int(y*hup), x2=int(x*wup), color=col, thickness=2)\n                last_x = x\n                last_y = y\n        misc.imshow(scr_viz)\n\n        """"""\n        paths_final = []\n        graph = list(zip(xx, yy, score))\n        for _ in range(1):\n            paths_final_flat = flatten_list(paths_final)\n            print(""paths_final_flat"", paths_final_flat)\n            source_candidates = [(i, (x, y, s)) for i, (x, y, s) in enumerate(graph) if (x, y, s) not in paths_final_flat]\n            if len(source_candidates) == 0:\n                break\n            else:\n                #print(""source_candidates"", source_candidates)\n                #source_score = max([s for (i, (x, y, s)) in source_candidates])\n                #print(""source_score"", source_score)\n                #source_id = [i for (i, (x, y, s)) in source_candidates if s == source_score][0]\n                source_val = min([x for (i, (x, y, s)) in source_candidates])\n                source_id = [i for (i, (x, y, s)) in source_candidates if x == source_val][0]\n                print(""source_id"", source_id)\n                _, _, paths = self.dijkstra(graph, source_id, already_done=[i for i, (x, y, s) in enumerate(paths_final_flat)])\n                if len(paths) == 0:\n                    break\n                else:\n                    print(""paths"", paths)\n                    #best_path = sorted(paths, key=lambda t: -t[1]+t[2], reverse=True)[0]\n                    best_path = sorted(paths, key=lambda t: t[2], reverse=True)[0]\n                    best_path = best_path[0]\n                    print(""best_path ids"", best_path)\n                    best_path = [graph[idx] for idx in best_path]\n                    print(""best_path"", best_path)\n                    paths_final.append(best_path)\n        paths_final = [path for path in paths_final if len(path) > 1]\n\n        scr_viz = np.copy(scr)\n        h, w = scr.shape[0:2]\n        #hup, wup = h/grid.shape[0], w/grid.shape[1]\n        hup, wup = h, w\n        for i, (x, y, s) in enumerate(zip(xx, yy, score)):\n            size = 3*int(s*10)\n            size = size if size % 2 != 0 else size - 1\n            scr_viz = util.draw_point(scr_viz, y=int(y*hup), x=int(x*wup), size=size, color=[0, 255, 0])\n            scr_viz = util.draw_text(scr_viz, y=int(y*hup), x=int(x*wup), text=str(i), color=[0, 255, 0])\n\n        colors = [[255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 255], [0, 0, 0]]\n        for path, col in zip(paths_final, colors):\n            last_x = None\n            last_y = None\n            for (x, y, s) in path:\n                if last_x is not None:\n                    scr_viz = util.draw_line(scr_viz, y1=int(last_y*hup), x1=int(last_x*wup), y2=int(y*hup), x2=int(x*wup), color=col, thickness=2)\n                last_x = x\n                last_y = y\n        misc.imshow(scr_viz)\n        """"""\n\n        #misc.imshow(np.hstack([scr, scr_viz]))\n\n    """"""\n    def shortest(self, graph):\n        edges = []\n\n\n    def dijkstra(self, graph, source_id, distance_threshold=0.5, already_done=None):\n        already_done = set() if already_done is None else set(already_done)\n        id_to_vertex = dict([(i, v) for i, v in enumerate(graph)])\n        vertex_to_id = dict([(v, i) for i, v in enumerate(graph)])\n        graph_ids = [i for i, _ in enumerate(graph)]\n        def length(id1, id2):\n            d = (graph[id1][0]-graph[id2][0])**2 + (graph[id1][1]-graph[id2][1])**2\n            if id1 in already_done or id2 in already_done:\n                d = d + 0.2\n            d = d / (0.5*(id_to_vertex[id1][2] + id_to_vertex[id2][2]))\n            print(""length"", id1, id2, d)\n            return d\n        def neighbours(id1):\n            n = []\n            for id2, v in id_to_vertex.items():\n                #print(id1, id2, length(id1, id2))\n                if id1 != id2 and length(id1, id2) < distance_threshold:\n                    n.append(id2)\n            return n\n        def mindist(dist, Q):\n            mindist_val = 999999\n            mindist_id = -1\n            for vid in Q:\n                if dist[vid] < mindist_val:\n                    mindist_val = mindist_val\n                    mindist_id = vid\n            return mindist_id\n\n        dist = dict()\n        prev = dict()\n        Q = set()\n        for vid in graph_ids:\n            dist[vid] = 999999\n            prev[vid] = None\n            Q.add(vid)\n\n        dist[source_id] = 0\n        prev[source_id] = None\n\n        while len(Q) > 0:\n            uid = mindist(dist, Q)\n            print(""do"", uid)\n            if uid == -1:\n                print(Q)\n                print(dist)\n            if uid == -1:\n                break\n            else:\n                Q.remove(uid)\n\n                for vid in neighbours(uid):\n                    alt = dist[uid] + length(uid, vid)\n                    print(""neighbour %d -> %d | d=%.2f"" % (uid, vid, alt))\n                    if alt < dist[vid]:\n                        print(""closer!"")\n                        dist[vid] = alt\n                        prev[vid] = uid\n\n        print(""dist"", dist)\n        print(""prev"", prev)\n        # paths\n        ps = []\n        for i, v in enumerate(graph):\n            last_node_id = i\n            p = [last_node_id]\n            sum_dist = 0\n            count_nodes = 1\n            while True:\n                curr_node_id = prev[last_node_id]\n                if curr_node_id is None:\n                    break\n                else:\n                    p.append(curr_node_id)\n                    count_nodes += 1\n                    sum_dist += length(last_node_id, curr_node_id)\n                last_node_id = curr_node_id\n            ps.append((p, sum_dist, count_nodes))\n        print(""ps"", ps)\n        ps = [p for p in ps if p[0][-1] == source_id]\n        print(""ps red"", ps)\n\n        return dist, prev, ps\n    """"""\n\ndef flatten_list(l):\n    return [item for sublist in l for item in sublist]\n\ndef simplesoftmax(l):\n    s = np.sum(l)\n    if s > 0:\n        return l/s\n    else:\n        return np.zeros_like(s)\n\nif __name__ == ""__main__"":\n    main()\n'"
train_reinforced/models.py,7,"b'""""""Models used during the reinforcement learning.""""""\nfrom __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd.function import InplaceFunction\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom config import Config\nimport numpy as np\nfrom lib import actions as actionslib\nfrom lib.util import to_cuda, to_variable\nimport imgaug as ia\nimport random\nimport math\n\ndef add_white_noise(x, std, training):\n    """"""Layer that adds white/gaussian noise to its input.""""""\n    if training:\n        noise = Variable(\n            x.data.new().resize_as_(x.data).normal_(mean=0, std=std),\n            volatile=x.volatile, requires_grad=False\n        ).type_as(x)\n        x = x + noise\n    return x\n\ndef init_weights(module):\n    """"""Weight initializer.""""""\n    for m in module.modules():\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            m.weight.data.normal_(0, math.sqrt(2. / n))\n        elif classname.find(\'Linear\') != -1:\n            m.weight.data.normal_(0, 0.02)\n        elif classname.find(\'BatchNorm\') != -1:\n            m.weight.data.normal_(1.0, 0.02)\n            m.bias.data.fill_(0)\n\nclass Embedder(nn.Module):\n    """"""Model that converts the (B, C, H, W) outputs from the semi-supervised\n    embedder to vectors (B, S).\n    This also adds information regarding speed, steering wheel, gear and\n    previous actions.""""""\n\n    def __init__(self):\n        super(Embedder, self).__init__()\n\n        def identity(v):\n            return lambda x: x\n        bn2d = nn.InstanceNorm2d\n        bn1d = identity\n\n        self.nb_previous_images = 2\n\n        self.emb_sup_c1    = nn.Conv2d(512, 1024, kernel_size=3, padding=0, stride=1)\n        self.emb_sup_c1_bn = bn2d(1024)\n        self.emb_sup_c1_sd = nn.Dropout2d(0.0)\n\n        self.emb_add_fc1 = nn.Linear(\n            (self.nb_previous_images+1) # speeds\n            + (self.nb_previous_images+1) # is_reverse\n            + (self.nb_previous_images+1) # steering wheel\n            + (self.nb_previous_images+1) # steering wheel raw\n            + self.nb_previous_images*9,\n            128\n        )\n        self.emb_add_fc1_bn = bn1d(128)\n\n        self.emb_fc1 = nn.Linear(1024*3 + 128, 512)\n        self.emb_fc1_bn = bn1d(512)\n\n        init_weights(self)\n\n    def forward(self, embeddings_supervised, speeds, is_reverse, steering_wheel, steering_wheel_raw, multiactions_vecs):\n        def act(x):\n            return F.leaky_relu(x, negative_slope=0.2, inplace=True)\n\n        x_emb_sup = embeddings_supervised # 512x3x5\n        x_emb_sup = act(self.emb_sup_c1_sd(self.emb_sup_c1_bn(self.emb_sup_c1(x_emb_sup)))) # 1024x1x3\n        x_emb_sup = x_emb_sup.view(-1, 1024*1*3)\n        x_emb_sup = add_white_noise(x_emb_sup, 0.005, self.training)\n\n        x_emb_add = torch.cat([speeds, is_reverse, steering_wheel, steering_wheel_raw, multiactions_vecs], 1)\n        x_emb_add = act(self.emb_add_fc1_bn(self.emb_add_fc1(x_emb_add)))\n        x_emb_add = add_white_noise(x_emb_add, 0.005, self.training)\n\n        x_emb = torch.cat([x_emb_sup, x_emb_add], 1)\n        x_emb = F.dropout(x_emb, p=0.05, training=self.training)\n\n        embs = F.relu(self.emb_fc1_bn(self.emb_fc1(x_emb)))\n        # this is currently always on, to decrease the likelihood of systematic\n        # errors that are repeated over many frames\n        embs = add_white_noise(embs, 0.005, True)\n\n        return embs\n\n    def forward_dict(self, embeddings_supervised, inputs_reinforced_add):\n        return self.forward(\n            embeddings_supervised,\n            inputs_reinforced_add[""speeds""],\n            inputs_reinforced_add[""is_reverse""],\n            inputs_reinforced_add[""steering_wheel""],\n            inputs_reinforced_add[""steering_wheel_raw""],\n            inputs_reinforced_add[""multiactions_vecs""]\n        )\n\nclass DirectRewardPredictor(nn.Module):\n    """"""Model that predicts the direct reward of a state.\n    For (s, a, r), (s\', a\', r\') this would predict r in s\'. It does not predict\n    r\', because that is dependent on a\'.\n    The prediction happens via a softmax over bins instead of via regression.\n    """"""\n    def __init__(self, nb_bins):\n        super(DirectRewardPredictor, self).__init__()\n\n        def identity(v):\n            return lambda x: x\n        bn2d = nn.InstanceNorm2d\n        bn1d = identity\n\n        self.fc1 = nn.Linear(512, 128)\n        self.fc1_bn = bn1d(128)\n        self.fc2 = nn.Linear(128, nb_bins)\n\n        init_weights(self)\n\n    def forward(self, embeddings, softmax):\n        def act(x):\n            return F.leaky_relu(x, negative_slope=0.2, inplace=True)\n\n        x = act(self.fc1_bn(self.fc1(embeddings)))\n        x = add_white_noise(x, 0.005, self.training)\n        x = F.dropout(x, p=0.1, training=self.training)\n        x = self.fc2(x)\n        if softmax:\n            return F.softmax(x)\n        else:\n            return x\n\nclass IndirectRewardPredictor(nn.Module):\n    """"""Model that predicts indirect rewards / Q-values.\n    For\n      (s, a, r), (s\', a\', r\'), ...\n    that is\n      r + g^1*r\' + g^2*r\'\' + ...\n    in state s,\n    where g is gamma (discount term).\n    The prediction happens via regression.\n    It predicts one value per action a.\n    The prediction is split into V (mean value of a state) and A (advantage\n    of each action).\n    """"""\n    def __init__(self):\n        super(IndirectRewardPredictor, self).__init__()\n\n        def identity(v):\n            return lambda x: x\n        bn2d = nn.InstanceNorm2d\n        bn1d = identity\n\n        self.fc1 = nn.Linear(512, 128)\n        self.fc1_bn = bn1d(128)\n\n        self.fc_v = nn.Linear(128, 1)\n        self.fc_advantage = nn.Linear(128, 9)\n\n        init_weights(self)\n\n    def forward(self, embeddings, return_v_adv=False):\n        def act(x):\n            return F.leaky_relu(x, negative_slope=0.2, inplace=True)\n\n        B, _ = embeddings.size()\n\n        x = act(self.fc1_bn(self.fc1(embeddings)))\n        x = add_white_noise(x, 0.005, self.training)\n        x = F.dropout(x, p=0.1, training=self.training)\n\n        x_v = self.fc_v(x)\n        x_v_expanded = x_v.expand(B, 9)\n\n        x_adv = self.fc_advantage(x)\n        x_adv_mean = x_adv.mean(dim=1)\n        x_adv_mean = x_adv_mean.expand(B, 9)\n        x_adv = x_adv - x_adv_mean\n\n        x = x_v_expanded + x_adv\n\n        if return_v_adv:\n            return x, (x_v, x_adv)\n        else:\n            return x\n\nclass SuccessorPredictor(nn.Module):\n    """"""Model that predicts future embeddings from a given current one and given\n    future actions.\n    I.e.\n        input: 1 current embedding, T future actions\n        output: T future embeddings\n\n    The prediction happens via LSTMs.\n    Instead of predicting the T future embeddings directly, it predicts the\n    change of each action with respect to the input embedding and then adds\n    the input embedding to that (residual architecture).\n    """"""\n    def __init__(self):\n        super(SuccessorPredictor, self).__init__()\n\n        def identity(v):\n            return lambda x: x\n        bn2d = nn.InstanceNorm2d\n        bn1d = identity\n\n        self.input_size = 9\n        self.hidden_size = 512\n        self.nb_layers = 1\n\n        self.hidden_fc1 = nn.Linear(512, self.nb_layers*2*self.hidden_size)\n        self.hidden_fc1_bn = bn1d(self.nb_layers*2*self.hidden_size)\n\n        self.rnn = nn.LSTM(self.input_size, self.hidden_size, self.nb_layers, dropout=0.1, batch_first=False)\n\n        self.fc1 = nn.Linear(self.hidden_size, 512)\n\n        init_weights(self)\n\n    @staticmethod\n    def multiactions_to_vecs(multiactions_list):\n        vecs = []\n        for multiactions in multiactions_list:\n            vecs_this = [actionslib.ACTIONS_TO_MULTIVEC[multiaction] for multiaction in multiactions]\n            vecs.append(vecs_this)\n        vecs = np.array(vecs, dtype=np.float32) # (T, B, 9) with B=number of plans, T=timesteps\n        return vecs\n\n    def forward_apply(self, embeddings_reinforced, multiactions_vecs, gpu=Config.GPU):\n        assert len(embeddings_reinforced.size()) == 2\n        assert embeddings_reinforced.size(0) == 1\n        T, B, _ = multiactions_vecs.size()\n        _, S = embeddings_reinforced.size()\n        # extend embeddings from (1, 256) to (N_PLANS, 256)\n        embeddings_reinforced = embeddings_reinforced.expand(B, S)\n        return self.forward(embeddings_reinforced, multiactions_vecs, volatile=True, gpu=gpu)\n\n    def forward(self, embeddings_reinforced, multiactions_vecs, hidden=None, volatile=False, gpu=Config.GPU):\n        inputs, embeddings_over_time = self._to_inputs(embeddings_reinforced, multiactions_vecs)\n        if hidden is None:\n            B = embeddings_reinforced.size(0)\n            hidden_vecs = self.hidden_fc1_bn(self.hidden_fc1(embeddings_reinforced))\n            hidden_vecs = F.tanh(hidden_vecs)\n            h = hidden_vecs[:, 0:self.nb_layers*self.hidden_size]\n            c = hidden_vecs[:, self.nb_layers*self.hidden_size:]\n\n            hidden = (\n                h.contiguous().view(self.nb_layers, B, self.hidden_size),\n                c.contiguous().view(self.nb_layers, B, self.hidden_size)\n            )\n\n        output_lstm, hidden = self.rnn(inputs, hidden)\n\n        T, B, S = output_lstm.size()\n        x = output_lstm\n        x = x.view(T*B, S)\n        x = add_white_noise(x, 0.005, self.training)\n        x = F.dropout(x, p=0.1, training=self.training)\n        x = self.fc1(x)\n\n        x = embeddings_over_time + x\n        x = x.view(T, B, 512)\n\n        x = F.relu(x, inplace=True)\n\n        return x, hidden\n\n    def create_hidden(self, batch_size, volatile=False, gpu=Config.GPU):\n        weight = next(self.parameters()).data\n        return (\n            to_cuda(Variable(weight.new(self.nb_layers, batch_size, self.hidden_size).zero_(), volatile=volatile), gpu),\n            to_cuda(Variable(weight.new(self.nb_layers, batch_size, self.hidden_size).zero_(), volatile=volatile), gpu)\n        )\n\n    def _to_inputs(self, embeddings_reinforced, multiactions_vecs):\n        B, S = embeddings_reinforced.size()\n        T = multiactions_vecs.size(0)\n        # identical batch size\n        assert embeddings_reinforced.size(0) == multiactions_vecs.size(1), ""%s / %s"" % (str(embeddings_reinforced.size()), str(multiactions_vecs.size()))\n        embeddings_over_time = embeddings_reinforced.contiguous().view(1, B, S).expand(T, B, S)\n        return multiactions_vecs, embeddings_over_time\n\nclass AEDecoder(nn.Module):\n    """"""Decoder part of an autoencoder that takes an embedding vector\n    and converts it into an image.""""""\n    def __init__(self):\n        super(AEDecoder, self).__init__()\n\n        def identity(v):\n            return lambda x: x\n        bn2d = nn.InstanceNorm2d\n        bn1d = identity\n\n        self.ae_fc1 = nn.Linear(512, 128*3*5)\n        self.ae_fc1_bn = bn1d(128*3*5)\n        self.ae_c1 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.ae_c1_bn = bn2d(128)\n        self.ae_c2 = nn.Conv2d(128, 128, kernel_size=3, padding=(0, 1))\n        self.ae_c2_bn = bn2d(128)\n        self.ae_c3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.ae_c3_bn = bn2d(128)\n        self.ae_c4 = nn.Conv2d(128, 3, kernel_size=3, padding=1)\n\n        init_weights(self)\n\n    def forward(self, embedding):\n        def act(x):\n            return F.relu(x, inplace=True)\n        def up(x):\n            m = nn.UpsamplingNearest2d(scale_factor=2)\n            return m(x)\n        x_ae = embedding # Bx256\n        x_ae = act(self.ae_fc1_bn(self.ae_fc1(x_ae))) # 128x3x5\n        x_ae = x_ae.view(-1, 128, 3, 5)\n        x_ae = up(x_ae) # 6x10\n        x_ae = act(self.ae_c1_bn(self.ae_c1(x_ae))) # 6x10\n        x_ae = up(x_ae) # 12x20\n        x_ae = act(self.ae_c2_bn(self.ae_c2(x_ae))) # 12x20 -> 10x20\n        x_ae = F.pad(x_ae, (0, 0, 1, 0)) # 11x20\n        x_ae = up(x_ae) # 22x40\n        x_ae = act(self.ae_c3_bn(self.ae_c3(x_ae))) # 22x40\n        x_ae = up(x_ae) # 44x80\n        x_ae = F.pad(x_ae, (0, 0, 1, 0)) # add 1px at top (from 44 to 45)\n        x_ae = F.sigmoid(self.ae_c4(x_ae))\n        return x_ae\n'"
train_reinforced/plans.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport models as models_reinforced\nfrom lib import actions as actionslib\n\ntry:\n    xrange\nexcept NameError:\n    xrange = range\n\ndef make_plans_unique(plans):\n    added = set()\n    result = []\n    for plan in plans:\n        plan_str = "" "".join([""%s%s"" % (a, b) for (a, b) in plan])\n        if plan_str not in added:\n            result.append(plan)\n            added.add(plan_str)\n    return result\n\ndef create_plans(nb_future_states):\n    PLANS = []\n    # plans that contain only a specific multiaction\n    for multiaction in actionslib.ALL_MULTIACTIONS:\n        plan = []\n        for i in xrange(nb_future_states):\n            plan.append(multiaction)\n        PLANS.append(plan)\n    # plans that contain multiaction A followed by B at every Nth step\n    # e.g [U U UR U U UR U U UR] (U=up, UR=up+right)\n    for n in [1, 2, 3]:\n        for ma1 in actionslib.ALL_MULTIACTIONS:\n            for ma2 in actionslib.ALL_MULTIACTIONS:\n                if ma1 != ma2:\n                    plan = []\n                    for i in xrange(nb_future_states):\n                        if i % n == 0:\n                            plan.append(ma2)\n                        else:\n                            plan.append(ma1)\n                    PLANS.append(plan)\n    # plans that start with multiaction A followed by B after P percent of timesteps\n    # e.g. [U U U U U UR UR UR UR UR] for P=50percent\n    for p in [0.25, 0.5, 0.75]:\n        for ma1 in actionslib.ALL_MULTIACTIONS:\n            for ma2 in actionslib.ALL_MULTIACTIONS:\n                if ma1 != ma2:\n                    plan = []\n                    for i in xrange(nb_future_states):\n                        if i >= int(nb_future_states*p):\n                            plan.append(ma1)\n                        else:\n                            plan.append(ma2)\n                    PLANS.append(plan)\n\n    print(""Generated %d possible plans of multiactions."" % (len(PLANS),))\n    PLANS = make_plans_unique(PLANS)\n    print(""Reduced plans to %d after unique."" % (len(PLANS),))\n\n    PLANS_VECS = models_reinforced.SuccessorPredictor.multiactions_to_vecs(PLANS).transpose((1, 0, 2)) # BT9 -> TB9\n\n    return PLANS, PLANS_VECS\n'"
train_reinforced/train.py,11,"b'""""""Train a self-driving truck via reinforcement learning.""""""\nfrom __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport models as models_reinforced\nfrom plans import create_plans\nfrom batching import BatchLoader, BackgroundBatchLoader, states_to_batch\nfrom visualization import generate_overview_image, generate_training_debug_image\n\nfrom train_semisupervised.train import (\n    PREVIOUS_STATES_DISTANCES,\n    MODEL_HEIGHT,\n    MODEL_WIDTH,\n    MODEL_PREV_HEIGHT,\n    MODEL_PREV_WIDTH\n)\nfrom train_semisupervised import models as models_semisupervised\nfrom lib import ets2game\nfrom lib import speed as speedlib\nfrom lib import replay_memory\nfrom lib import states as stateslib\nfrom lib import actions as actionslib\nfrom lib import steering_wheel as swlib\nfrom lib import util\nfrom lib.util import to_variable, to_cuda, to_numpy\nfrom lib import plotting\nfrom lib import rewards as rewardslib\nfrom config import Config\n\nfrom scipy import misc\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nfrom imgaug import parameters as iap\nimport time\nimport cv2\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport argparse\nimport collections\nfrom datetime import datetime\nimport copy\nimport gzip as gz\n\nnp.random.seed(42)\nrandom.seed(42)\ntorch.manual_seed(42)\ntorch.backends.cudnn.benchmark = True\n\nif sys.version_info[0] == 2:\n    pass\nelif sys.version_info[0] == 3:\n    xrange = range\n\n# number of batches per training sessions and batch size\nNB_BATCHES_PER_TRAIN = 30000\nBATCH_SIZE = 8\n\n# when to start training sessions\n# wait until minimum number of experiences, then every 30k experiences\ndef DO_TRAIN(ora, game):\n    if game.tick_ingame_route_advisor_idx == 0 and ora.args.instanttrain:\n        return True\n    elif ora.args.onlyapply:\n        return False\n    elif ora.memory.size < 30000 or ora.memory_val.size < 2000:\n        return False\n    else:\n        return (game.tick_ingame_route_advisor_idx+1) % 30000 == 0\n# when to save during training\nDO_SAVE = lambda epoch, batch_idx, batch_idx_total: (batch_idx+1) % min(2000, NB_BATCHES_PER_TRAIN-1) == 0\n# when to validate during training\nDO_VALIDATE = lambda epoch, batch_idx, batch_idx_total: (batch_idx+1) % min(500, NB_BATCHES_PER_TRAIN-1) == 0\n# when to plot loss curves during training\nDO_PLOT = lambda epoch, batch_idx, batch_idx_total: (batch_idx+1) % min(500, NB_BATCHES_PER_TRAIN-1) == 0\n# whether to train the model which was initialized by semi-supervised training\nDO_TRAIN_SUPERVISED = lambda epoch, batch_idx, batch_idx_total: batch_idx_total > 50000\n# whether to backpropagate losses of direct and indirect reward prediction\n# applied to the successors. if false, the reward prediction models are only\n# trained to handle the true embeddings, not the noisier successor embeddings.\nDO_BACKPROP_SUCCESSOR_REWARDS = lambda epoch, batch_idx, batch_idx_total: batch_idx_total > 2000\n# whether to NOT backpropagate the reward gradients from the successors beyond\n# the direct/indirect reward models. If False, the successors should become more\n# suited towards predicting correctly factors that influence rewards.\nDO_CUTOFF_BACKPROP_SUCCESSOR_REWARDS = lambda epoch, batch_idx, batch_idx_total: batch_idx_total < 5000\n# whether to NOT backpropagate all successor gradietns beyond the successors (into\n# the embeddings). If False, the embeddings should become more suited for\n# successor prediction.\nDO_CUTOFF_BACKPROP_SUCCESSOR = lambda epoch, batch_idx, batch_idx_total: batch_idx_total < 10000\n# whether to train an autoencoder on the embeddings\nDO_TRAIN_AE = lambda epoch, batch_idx, batch_idx_total: batch_idx_total < 25000\n\n# weightings of gradients\n# idr is here lowered to avoid problems from high gradients related to MSE\nLOSS_DR_WEIGHTING = 1\nLOSS_DR_FUTURE_WEIGHTING = 1\nLOSS_IDR_WEIGHTING = 0.01\nLOSS_IDR_FUTURE_WEIGHTING = 0.01\nLOSS_SUCCESSOR_WEIGHTING = 0.5\nLOSS_DR_SUCCESSORS_WEIGHTING = 1\nLOSS_IDR_SUCCESSORS_WEIGHTING = 0.01\nLOSS_AE_WEIGHTING = 1\n\n# multiplier for all Adam learning rates\n# might be a good idea to increase this value to 1 at the start\nLEARNING_RATE_MULTIPLIER = 0.1\n\n# number of validation batches to run per validation\nNB_VAL_BATCHES = 128\n# how often to fix weights for IDR/Q-value models during training\nTRAIN_FIX_EVERY_N_BATCHES = 100\n# whether to add a state to the training or validation replay memory\nMEMORY_ADD_STATE_TO_TRAIN = lambda tick_idx: True if tick_idx % 10000 > 1000 else False\n# whether to commit changes to the databases (replay memories)\nENABLE_COMMITS = True\n# commit every Nth state\nCOMMIT_EVERY = 110\n\n# number of future timesteps to predict when applying the mdoel, training it\n# or validating it\nNB_FUTURE_STATES_APPLY = 10\nNB_FUTURE_STATES_TRAIN = 10\nNB_FUTURE_STATES_VAL = 10\nPICK_NEW_PLAN_EVERY_N_STEPS = 10\n\n# number of reward bins for the direct reward prediction,\n# the sizes of these bins and their average values\nNB_REWARD_BINS = 101 # Note: this constant is also in batching.py\nBIN_SIZE = (Config.MAX_REWARD - Config.MIN_REWARD) / NB_REWARD_BINS\nBIN_AVERAGES = np.array([Config.MIN_REWARD + (i*BIN_SIZE + (i+1)*BIN_SIZE)/2 for i in xrange(NB_REWARD_BINS)], dtype=np.float32)[::-1]\n\n# discount factor and discount over time\nGAMMA = 0.95\nREWARDS_FUTURE_GAMMAS = np.power(\n    np.array([GAMMA] * NB_FUTURE_STATES_APPLY, dtype=np.float32),\n    1 + np.arange(NB_FUTURE_STATES_APPLY)\n)\n\n# whether to use argmax on the predicted direct reward bins to compute the\n# direct reward (otherwise: average of activated bins, multiplied by probability\n# of each bin)\nDIRECT_REWARD_ARGMAX = False\n\n# whether to add the computed direct reward to the final reward, which is used\n# to determine the ranking of plans (and therefore the plan to pick)\n# if false, the ranking is only determined by the final timesteps V-value\n# (indirect reward)\nREWARD_ADD_DIRECT_REWARD = False\n\n# plans of actions to check during validation (for each plan, expected rewards\n# are calculated, then the best plan is chosen)\nPLANS, PLANS_VECS = create_plans(NB_FUTURE_STATES_APPLY)\nPLANS_VECS_VAR = to_cuda(to_variable(PLANS_VECS, volatile=True), Config.GPU)\n\n# epsilon greedy constants (=p_explore)\n# chance in case p_explore hits that the last state\'s action is repeated\nP_EXPLORE_CHANCE_REDO = 0.85\n# minimum and maximum chances to explore\nP_EXPLORE_MIN = 0.2\nP_EXPLORE_MAX = 0.8\n# reach minimum p_explore after around 12 hours / 430k experiences\nP_EXPLORE_STEPSIZE = 1/(12*60*60*10)\n\ndef main():\n    """"""Initialize/load model, replay memories, optimizers, history and loss\n    plotter, augmentation sequence. Then start play and train loop.""""""\n\n    # -----------\n    # test loading a batch, for faster debugging\n    loader = BatchLoader(\n        val=False, batch_size=BATCH_SIZE, augseq=iaa.Noop(),\n        previous_states_distances=PREVIOUS_STATES_DISTANCES,\n        nb_future_states=NB_FUTURE_STATES_TRAIN,\n        model_height=MODEL_HEIGHT, model_width=MODEL_WIDTH,\n        model_prev_height=MODEL_PREV_HEIGHT, model_prev_width=MODEL_PREV_HEIGHT\n    )\n    batch = loader.load_random_batch()\n    # -----------\n\n    parser = argparse.ArgumentParser(description=""Train trucker via reinforcement learning."")\n    parser.add_argument(""--nocontinue"", default=False, action=""store_true"", help=""Whether to NOT continue the previous experiment"", required=False)\n    parser.add_argument(""--instanttrain"", default=False, action=""store_true"", help=""Start first training instantly"", required=False)\n    parser.add_argument(""--onlytrain"", default=False, action=""store_true"", help=""Do nothing but training, ignore the game"", required=False)\n    parser.add_argument(""--onlyapply"", default=False, action=""store_true"", help=""Never train the models"", required=False)\n    parser.add_argument(""--noinsert"", default=False, action=""store_true"", help=""Whether to not add experiences to database."", required=False)\n    parser.add_argument(""--nospeedkill"", default=False, action=""store_true"", help=""Whether to not autokill tries that end in low speed for a long time."", required=False)\n    # TODO this uses the standard semi-supervised model, not the one with shortcuts\n    parser.add_argument(""--showgrids"", default=False, action=""store_true"", help=""Whether to show grids in the overview window (takes extra time per state)."", required=False)\n    parser.add_argument(""--p_explore"", default=None, help=""Constant p_explore value to use."", required=False)\n    parser.add_argument(""--record"", default=None, help=""Record screenshots and states, then save them to the given filepath, e.g. record=\'records/testrun17.pickle.gz\'."", required=False)\n    args = parser.parse_args()\n    if args.p_explore is not None:\n        args.p_explore = float(args.p_explore)\n    if args.record and os.path.isfile(args.record):\n        print(""[WARNING] --record file already exists. This will overwrite the file!"")\n\n    if not args.onlytrain:\n        cv2.namedWindow(""overview"", cv2.WINDOW_NORMAL)\n        cv2.resizeWindow(""overview"", 590, 720)\n        cv2.moveWindow(""overview"", 30, 1080-720)\n        cv2.waitKey(100)\n\n    # ---------------------\n    # initialize variables\n    # ---------------------\n    checkpoint_reinforced = torch.load(""train_reinforced_model.tar"") if os.path.isfile(""train_reinforced_model.tar"") and not args.nocontinue else None\n    checkpoint_supervised = torch.load(""../train_semisupervised/train_semisupervised_model.tar"") if os.path.isfile(""../train_semisupervised/train_semisupervised_model.tar"") else None\n    if checkpoint_reinforced is None:\n        print(""[WARNING] No checkpoint from reinforcement training found."")\n\n    # connection to write states and check size\n    # training creates several new connections\n    memory = replay_memory.ReplayMemory.create_instance_reinforced()\n    memory_val = replay_memory.ReplayMemory.create_instance_reinforced(val=True)\n\n    history = plotting.History()\n    embedder_supervised_orig = models_semisupervised.Predictor()\n    models = {\n        ""embedder_supervised"": models_semisupervised.Predictor(),\n        ""embedder_reinforced"": models_reinforced.Embedder(),\n        ""direct_reward_predictor"": models_reinforced.DirectRewardPredictor(NB_REWARD_BINS),\n        ""indirect_reward_predictor"": models_reinforced.IndirectRewardPredictor(),\n        ""successor_predictor"": models_reinforced.SuccessorPredictor(),\n        ""ae_decoder"": models_reinforced.AEDecoder()\n    }\n\n    embedder_supervised_orig.eval()\n\n    print(""Embedder supervised:"")\n    print(models[""embedder_supervised""])\n    print(""Embedder reinforced:"")\n    print(models[""embedder_reinforced""])\n    optimizers = {\n        ""embedder_supervised"": optim.Adam(models[""embedder_supervised""].parameters(), lr=0.001*LEARNING_RATE_MULTIPLIER),\n        ""embedder_reinforced"": optim.Adam(models[""embedder_reinforced""].parameters(), lr=0.001*LEARNING_RATE_MULTIPLIER),\n        ""direct_reward_predictor"": optim.Adam(models[""direct_reward_predictor""].parameters(), lr=0.001*LEARNING_RATE_MULTIPLIER),\n        ""indirect_reward_predictor"": optim.Adam(models[""indirect_reward_predictor""].parameters(), lr=0.001*LEARNING_RATE_MULTIPLIER),\n        ""successor_predictor"": optim.Adam(models[""successor_predictor""].parameters(), lr=0.001*LEARNING_RATE_MULTIPLIER),\n        ""ae_decoder"": optim.Adam(models[""ae_decoder""].parameters(), lr=0.001*LEARNING_RATE_MULTIPLIER)\n    }\n\n    # initialize loss value hsitory\n    history.add_group(""loss-dr"", [""train"", ""val""], increasing=False)\n    history.add_group(""loss-dr-future"", [""train"", ""val""], increasing=False)\n    history.add_group(""loss-dr-successors"", [""train"", ""val""], increasing=False)\n    history.add_group(""loss-idr"", [""train"", ""val""], increasing=False)\n    history.add_group(""loss-idr-future"", [""train"", ""val""], increasing=False)\n    history.add_group(""loss-idr-successors"", [""train"", ""val""], increasing=False)\n    history.add_group(""loss-successor"", [""train"", ""val""], increasing=False)\n    history.add_group(""loss-ae"", [""train"", ""val""], increasing=False)\n\n    # initialize loss plotter\n    loss_plotter = plotting.LossPlotter(\n        history.get_group_names(),\n        history.get_groups_increasing(),\n        save_to_fp=""train_reinforced_plot.jpg""\n    )\n    # this limits the plots to the last 100k batches\n    for group_name in history.get_group_names():\n        loss_plotter.xlim[group_name] = (-100000, None)\n    loss_plotter.start_batch_idx = 100\n\n    # load model parameters and loss history\n    if checkpoint_reinforced is not None:\n        print(""Loading old checkpoint from reinforced training...."")\n        models[""embedder_supervised""].load_state_dict(checkpoint_reinforced[""embedder_supervised_state_dict""])\n        models[""embedder_reinforced""].load_state_dict(checkpoint_reinforced[""embedder_reinforced_state_dict""])\n        models[""direct_reward_predictor""].load_state_dict(checkpoint_reinforced[""direct_reward_predictor_state_dict""])\n        models[""indirect_reward_predictor""].load_state_dict(checkpoint_reinforced[""indirect_reward_predictor_state_dict""])\n        models[""successor_predictor""].load_state_dict(checkpoint_reinforced[""successor_predictor_state_dict""])\n        models[""ae_decoder""].load_state_dict(checkpoint_reinforced[""ae_decoder_state_dict""])\n        history = plotting.History.from_string(checkpoint_reinforced[""history""])\n    elif checkpoint_supervised is not None:\n        models[""embedder_supervised""].load_state_dict(checkpoint_supervised[""predictor_state_dict""])\n    else:\n        print(""[WARNING] No checkpoint from reinforcement training or semisupervised training found. Starting from zero."")\n\n    if checkpoint_supervised is not None:\n        embedder_supervised_orig.load_state_dict(checkpoint_supervised[""predictor_state_dict""])\n\n    # initialize loss criterions\n    criterions = {\n        ""direct_reward_predictor"": nn.CrossEntropyLoss(),\n        ""indirect_reward_predictor"": nn.MSELoss(),\n        ""successor_predictor"": nn.MSELoss(),\n        ""ae_decoder"": nn.MSELoss(),\n    }\n\n    # send everything to the GPU\n    if Config.GPU >= 0:\n        embedder_supervised_orig.cuda(Config.GPU)\n        for key in models:\n            models[key].cuda(Config.GPU)\n        for key in criterions:\n            criterions[key].cuda(Config.GPU)\n\n    # initialize image augmentation cascade\n    rarely = lambda aug: iaa.Sometimes(0.1, aug)\n    sometimes = lambda aug: iaa.Sometimes(0.2, aug)\n    often = lambda aug: iaa.Sometimes(0.3, aug)\n    augseq = iaa.Sequential([\n            often(iaa.Crop(percent=(0, 0.05))),\n            sometimes(iaa.GaussianBlur((0, 0.2))), # blur images with a sigma between 0 and 3.0\n            often(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5)), # add gaussian noise to images\n            often(\n                iaa.Dropout(\n                    iap.FromLowerResolution(\n                        other_param=iap.Binomial(1 - 0.2),\n                        size_px=(2, 16)\n                    ),\n                    per_channel=0.2\n                )\n            ),\n            rarely(iaa.Sharpen(alpha=(0, 0.7), lightness=(0.75, 1.5))), # sharpen images\n            rarely(iaa.Emboss(alpha=(0, 0.7), strength=(0, 2.0))), # emboss images\n            rarely(iaa.Sometimes(0.5,\n                iaa.EdgeDetect(alpha=(0, 0.4)),\n                iaa.DirectedEdgeDetect(alpha=(0, 0.4), direction=(0.0, 1.0)),\n            )),\n            often(iaa.Add((-20, 20), per_channel=0.5)), # change brightness of images (by -10 to 10 of original value)\n            often(iaa.Multiply((0.8, 1.2), per_channel=0.25)), # change brightness of images (50-150% of original value)\n            often(iaa.ContrastNormalization((0.8, 1.2), per_channel=0.5)), # improve or worsen the contrast\n            sometimes(iaa.Affine(\n                scale={""x"": (0.9, 1.1), ""y"": (0.9, 1.1)},\n                translate_percent={""x"": (-0.05, 0.05), ""y"": (-0.05, 0.05)},\n                rotate=(0, 0),\n                shear=(0, 0),\n                order=[0, 1],\n                cval=(0, 255),\n                mode=ia.ALL\n            ))\n        ],\n        random_order=True # do all of the above in random order\n    )\n\n    # ---------------------\n    # game loop\n    # ---------------------\n    ora = OnRouteAdvisorVisible(\n        memory,\n        memory_val,\n        models,\n        embedder_supervised_orig,\n        optimizers,\n        criterions,\n        history,\n        loss_plotter,\n        augseq,\n        nb_ticks_done=0 if checkpoint_reinforced is None else checkpoint_reinforced[""nb_ticks_done""],\n        last_train_tick=-1 if checkpoint_reinforced is None else checkpoint_reinforced[""last_train_tick""],\n        batch_idx_total=0 if checkpoint_reinforced is None else checkpoint_reinforced[""batch_idx_total""],\n        epoch=0 if checkpoint_reinforced is None else checkpoint_reinforced[""epoch""],\n        p_explore=P_EXPLORE_MAX if checkpoint_reinforced is None else checkpoint_reinforced[""p_explore""],\n        args=args\n    )\n    if args.onlytrain:\n        while True:\n            ora.train()\n    game = ets2game.ETS2Game()\n    game.on_route_advisor_visible = ora\n    game.min_interval = 100 / 1000\n    try:\n        game.run()\n    except KeyboardInterrupt:\n        if args.record is not None and len(ora.recording[""frames""]) > 0:\n            wait_sec = 10\n            print(""Saving recording of %d frames to filepath \'%s\' in %d seconds. Press CTRL+C to cancel."" % (len(ora.recording[""frames""]), args.record, wait_sec))\n            if os.path.isfile(args.record):\n                print(""[WARNING] File already exists! This will overwrite the file!"")\n            else:\n                print(""The file \'%s\' does to not exist yet."" % (args.record,))\n            time.sleep(wait_sec)\n            print(""Saving..."")\n            #path_pieces = os.path.split(args.record)\n            directory = os.path.dirname(args.record)\n            #if len(path_pieces) > 1:\n            if directory != """":\n                if not os.path.exists(directory):\n                    os.makedirs(directory)\n            for frame in ora.recording[""frames""]:\n                frame[""scr""] = util.compress_to_jpg(frame[""scr""])\n            with gz.open(args.record, ""wb"") as f:\n                pickle.dump(ora.recording, f, protocol=-1)\n\nclass OnRouteAdvisorVisible(object):\n    def __init__(self, memory, memory_val, models, embedder_supervised_orig, optimizers, criterions, history, loss_plotter, augseq, nb_ticks_done, last_train_tick, batch_idx_total, epoch, args, p_explore=P_EXPLORE_MAX):\n        self.memory = memory\n        self.memory_val = memory_val\n        self.models = models\n        self.embedder_supervised_orig = embedder_supervised_orig\n        self.optimizers = optimizers\n        self.criterions = criterions\n        self.history = history\n        self.loss_plotter = loss_plotter\n        self.augseq = augseq\n        self.previous_states = collections.deque(maxlen=max(PREVIOUS_STATES_DISTANCES))\n        self.last_scr = None # used for recording\n        self.p_explore = collections.deque([p_explore], maxlen=NB_FUTURE_STATES_APPLY)\n        self._fill_p_explore()\n        self.last_speeds = collections.deque(maxlen=500)\n        self.nb_ticks_done = nb_ticks_done\n        self.last_train_tick = last_train_tick\n        self.batch_idx_total = batch_idx_total\n        self.epoch = epoch\n        self.sw_tracker = swlib.SteeringWheelTrackerCNN()\n\n        self.current_plan_idx = None\n        self.current_plan_step_idx = None\n        self.current_plan_reward = None\n\n        self.args = args\n        self.recording = {\n            ""time"": time.time(),\n            ""args"": args,\n            ""plans"": PLANS,\n            ""frames"": []\n        }\n\n        self.switch_models_to_eval()\n        #self.switch_models_to_train()\n\n    def switch_models_to_train(self):\n        for key in self.models:\n            self.models[key].train()\n\n    def switch_models_to_eval(self):\n        for key in self.models:\n            self.models[key].eval()\n\n    def _fill_p_explore(self):\n        while len(self.p_explore) < self.p_explore.maxlen:\n            self._add_p_explore()\n\n    def _add_p_explore(self):\n        self.p_explore.append(np.clip(self.p_explore[-1] - P_EXPLORE_STEPSIZE, P_EXPLORE_MIN, P_EXPLORE_MAX))\n\n    def __call__(self, game, scr):\n        """"""Function called every 100ms while playing.\n        Applies the model in order to play, records experiences/states to\n        the replay memory and calls the training function every now and then.""""""\n\n        if game.win.is_paused(scr):\n            print(""[OnRouteAdvisorVisible] Game paused, skipping execution/training. Unpause the game."")\n            return\n\n        # decrease replay memory size to max size\n        if self.memory.is_above_tolerance() or self.memory_val.is_above_tolerance():\n            game.reset_actions()\n            time.sleep(1)\n            game.pause()\n            time.sleep(1)\n            self.memory.shrink_to_max_size(force=True)\n            self.memory_val.shrink_to_max_size(force=True)\n            time.sleep(1)\n            game.unpause()\n            time.sleep(1)\n            return\n\n        # train the model\n        load_save = False\n        if DO_TRAIN(self, game):\n            game.reset_actions()\n            time.sleep(1)\n            self.memory.commit()\n            self.memory_val.commit()\n            time.sleep(0.5)\n            self.memory.close()\n            self.memory_val.close()\n            game.pause()\n            self.train()\n            game.unpause()\n            time.sleep(0.5)\n            self.memory.connect()\n            self.memory_val.connect()\n            time.sleep(1)\n            load_save = True\n            self.last_train_tick = game.tick_ingame_route_advisor_idx\n\n        # game load must come after save, because loading might take too long,\n        # then training pause is called during load screen, which then causes problems\n        if not self.args.nospeedkill and len(self.last_speeds) == self.last_speeds.maxlen and np.average(self.last_speeds) < 3:\n            load_save = True\n            print(""[OnRouteAdvisorVisible] Low speeds for a long time, auto-reloading save game."")\n            print(""[OnRouteAdvisorVisible] average speed: %.4f, max speed: %d"" % (np.average(self.last_speeds), np.max(self.last_speeds)))\n            print(""[OnRouteAdvisorVisible] last speeds: "", self.last_speeds)\n\n        # load savegame\n        if load_save:\n            game.reset_actions()\n            time.sleep(1)\n            self.last_speeds.clear()\n            self.previous_states.clear()\n            self.sw_tracker.reset()\n            game.load_random_save_game()\n            # if return is not used here, then the screenshot (scr) should be\n            # update as some time might have passed\n            return\n\n        times = dict()\n\n        # estimate current speed\n        times[""images""] = time.time()\n        speed_image = game.win.get_speed_image(scr)\n        som = speedlib.SpeedOMeter(speed_image)\n        times[""images""] = time.time() - times[""images""]\n\n        # initialize current state\n        times[""state""] = time.time()\n        from_datetime = datetime.utcnow()\n        screenshot_rs = ia.imresize_single_image(scr, (Config.MODEL_HEIGHT, Config.MODEL_WIDTH), interpolation=""cubic"")\n        screenshot_rs_jpg = util.compress_to_jpg(screenshot_rs)\n        speed = som.predict_speed()\n        is_reverse = game.win.is_reverse(scr)\n        is_offence_shown = game.win.is_offence_shown(scr) # 4-6ms\n        is_damage_shown = game.win.is_damage_shown(scr)\n        sw, sw_raw = self.sw_tracker.estimate_angle(util.decompress_img(screenshot_rs_jpg)) # ~2.5ms\n\n        current_state = stateslib.State(\n            from_datetime=from_datetime,\n            screenshot_rs_jpg=screenshot_rs_jpg,\n            speed=speed,\n            is_reverse=is_reverse,\n            is_offence_shown=is_offence_shown,\n            is_damage_shown=is_damage_shown,\n            reward=None,\n            action_left_right=None,\n            action_up_down=None,\n            p_explore=self.p_explore[0],\n            steering_wheel_classical=None,\n            steering_wheel_raw_one_classical=None,\n            steering_wheel_raw_two_classical=None,\n            steering_wheel_cnn=sw,\n            steering_wheel_raw_cnn=sw_raw,\n            allow_cache=True\n        )\n        times[""state""] = time.time() - times[""state""]\n\n        # apply model once enough previous states were observed\n        if len(self.previous_states) >= self.previous_states.maxlen:\n            times[""model""] = time.time()\n\n            # transform current and previous states to batch\n            # same function here as in training to reduce probability of bugs\n            #batch = states_to_batch([list(self.previous_states)], [[current_state]], self.augseq, PREVIOUS_STATES_DISTANCES, MODEL_HEIGHT, MODEL_WIDTH, MODEL_PREV_HEIGHT, MODEL_PREV_WIDTH)\n            batch = states_to_batch([list(self.previous_states)], [[current_state]], iaa.Noop(), PREVIOUS_STATES_DISTANCES, MODEL_HEIGHT, MODEL_WIDTH, MODEL_PREV_HEIGHT, MODEL_PREV_WIDTH)\n\n            # generate embeddings of semi-supervised model part\n            inputs_supervised = batch.inputs_supervised(volatile=True, gpu=Config.GPU)\n            embeddings_supervised = self.models[""embedder_supervised""].embed(inputs_supervised[0], inputs_supervised[1])\n\n            # generate embeddings of reinforced model part\n            inputs_reinforced_add = batch.inputs_reinforced_add(volatile=True, gpu=Config.GPU)\n            embeddings_reinforced = self.models[""embedder_reinforced""].forward_dict(embeddings_supervised, inputs_reinforced_add)\n\n            # predict successors for each plan (chain of future actions)\n            # embeddings_reinforced: (1, 512), PLANS_VECS_VAR: (T, N_PLANS, 9), future_embeddings_rf: (T, N_PLANS, 512)\n            future_embeddings_rf, hidden = self.models[""successor_predictor""].forward_apply(embeddings_reinforced, PLANS_VECS_VAR, gpu=Config.GPU)\n            # must transpose here first, otherwise the flattened array is\n            #   <B=0,T=0>,<B=1,T=0>,...\n            # instead of\n            #   <B=0,T=0>,<B=0,T=1>,...\n            future_embeddings_rf_flat = future_embeddings_rf.transpose(0, 1).contiguous().view(PLANS_VECS_VAR.size(1) * NB_FUTURE_STATES_APPLY, 512) # (N_PLANS*T, 512)\n\n            # predict future direct rewards\n            direct_rewards = self.models[""direct_reward_predictor""].forward(future_embeddings_rf_flat, softmax=True)\n\n            # predict current state indirect rewards (V(s) and A(s, a))\n            _, (idr_v, idr_adv) = self.models[""indirect_reward_predictor""].forward(embeddings_reinforced, return_v_adv=True)\n\n            # predict future/successor indirect rewards\n            _, (plan_to_v, _) = self.models[""indirect_reward_predictor""].forward(future_embeddings_rf[-1, ...].view(PLANS_VECS_VAR.size(1), -1), return_v_adv=True)\n\n            # estimate plan ranking based on predicted rewards\n            plan_to_reward_bins = to_numpy(direct_rewards.view(len(PLANS), NB_FUTURE_STATES_APPLY, -1))\n            plan_to_rewards_direct, plan_to_reward_indirect, plan_to_reward = calculate_future_rewards(plan_to_reward_bins, to_numpy(plan_to_v))\n            plans_ranking = np.argsort(plan_to_reward, axis=0, kind=""mergesort"")\n            best_plan = PLANS[plans_ranking[-1]]\n\n            # predict AE decodings for future states of best plan\n            #best_plan_ae_decodings = self.models[""ae_decoder""](future_embeddings_rf[:, plans_ranking[-1], :].contiguous().view(NB_FUTURE_STATES_APPLY, 512))\n            best_plan_ae_decodings = None\n            if self.args.showgrids:\n                grids = self.embedder_supervised_orig.predict_grids(inputs_supervised[0], inputs_supervised[1])\n            else:\n                grids = None\n\n            # use the calculated plan or stick with the old one?\n            use_this_plan = (self.current_plan_idx is None) or (self.current_plan_step_idx >= PICK_NEW_PLAN_EVERY_N_STEPS-1)\n            if use_this_plan:\n                self.current_plan_idx = plans_ranking[-1]\n                self.current_plan_step_idx = 0\n            else:\n                best_plan = PLANS[self.current_plan_idx]\n                self.current_plan_step_idx += 1\n            times[""model""] = time.time() - times[""model""]\n\n            # choose actions accordings to current plan,\n            # add epsilon-greedy to that\n            times[""ai-control""] = time.time()\n            action_up_down_bpe, action_left_right_bpe = best_plan[self.current_plan_step_idx]\n            action_up_down, action_left_right = p_explore_to_actions(\n                self.p_explore[0], action_up_down_bpe, action_left_right_bpe,\n                self.previous_states[-1] if len(self.previous_states) > 0 else None,\n                constant_p_explore=self.args.p_explore)\n            times[""ai-control""] = time.time() - times[""ai-control""]\n        else:\n            plan_to_rewards_direct = None\n            plan_to_reward_indirect = None\n            plan_to_reward = None\n            plans_ranking = None\n            best_plan_ae_decodings = None\n            grids = None\n            action_up_down = action_up_down_bpe = actionslib.ACTION_UP_DOWN_NONE\n            action_left_right = action_left_right_bpe = actionslib.ACTION_LEFT_RIGHT_NONE\n            outputs_to_rewards = None\n            idr_v, idr_adv = None, None\n\n        game.set_actions_of_interval([action_up_down, action_left_right])\n        current_state.action_up_down = action_up_down\n        current_state.action_left_right = action_left_right\n\n        times[""reward""] = time.time()\n        # last state reward\n        if len(self.previous_states) > 0:\n            # calculate last state\'s reward, i.e. for\n            #   (s, a, r) (s\', a\', r\')\n            # we compute r (derived from s\') and attach it to s\n            last_state = self.previous_states[-1]\n            last_state_reward = rewardslib.calculate_reward(last_state, current_state)\n            last_state.reward = last_state_reward\n\n            # save last state to train/val DB\n            if not self.args.noinsert:\n                if MEMORY_ADD_STATE_TO_TRAIN(game.tick_ingame_route_advisor_idx):\n                    self.memory.add_state(last_state, commit=False, shrink=False)\n                else:\n                    self.memory_val.add_state(last_state, commit=False, shrink=False)\n        else:\n            last_state = None\n            last_state_reward = None\n        times[""reward""] = time.time() - times[""reward""]\n\n        # commit last DB inserts of states\n        times[""commit""] = time.time()\n        do_commit = (game.tick_ingame_route_advisor_idx % COMMIT_EVERY == 0)\n        if not self.args.noinsert and do_commit:\n            self.memory.commit()\n            self.memory_val.commit()\n        times[""commit""] = time.time() - times[""commit""]\n\n        if self.args.record is not None and len(self.previous_states) >= 1:\n            self.recording[""frames""].append({\n                #""scr"": util.compress_to_jpg(self.last_scr),\n                ""scr"": self.last_scr,\n                ""scr_time"": game.last_scr_time,\n                ""state"": self.previous_states[-1],\n                ""current_plan_idx"": self.current_plan_idx,\n                ""current_plan_step_idx"": self.current_plan_step_idx,\n                ""idr_v"": to_numpy(idr_v[0]) if idr_v is not None else None,\n                ""idr_adv"": to_numpy(idr_adv[0]) if idr_adv is not None else None,\n                ""plan_to_rewards_direct"": plan_to_rewards_direct,\n                ""plan_to_reward_indirect"": plan_to_reward_indirect,\n                ""plan_to_reward"": plan_to_reward,\n                ""plans_ranking"": plans_ranking\n            })\n\n        self.previous_states.append(current_state)\n        self.last_scr = scr\n        self._add_p_explore()\n        if speed is not None:\n            self.last_speeds.append(speed)\n        else:\n            # do not add None to last speeds, otherwise functions like mean()\n            # dont work any more\n            print(""[WARNING] Speed is None! Last speeds: "", self.last_speeds)\n            if len(self.last_speeds) > 0:\n                self.last_speeds.append(self.last_speeds[-1])\n            else:\n                self.last_speeds.append(0)\n\n        # generate overview image\n        if game.tick_ingame_route_advisor_idx % 1 == 0:\n            times[""overview-img""] = time.time()\n            img = generate_overview_image(\n                current_state, last_state,\n                action_up_down_bpe, action_left_right_bpe,\n                self.memory,\n                self.memory_val,\n                game.tick_ingame_route_advisor_idx,\n                self.last_train_tick,\n                PLANS,\n                plan_to_rewards_direct,\n                plan_to_reward_indirect,\n                plan_to_reward,\n                plans_ranking,\n                PLANS[self.current_plan_idx][self.current_plan_step_idx:] if self.current_plan_idx is not None else None,\n                best_plan_ae_decodings,\n                idr_v, idr_adv,\n                grids,\n                self.args\n            )\n            times[""overview-img""] = time.time() - times[""overview-img""]\n\n            times[""overview-img-show""] = time.time()\n            cv2.imshow(""overview"", img[:,:,::-1])\n            cv2.waitKey(1)\n            times[""overview-img-show""] = time.time() - times[""overview-img-show""]\n\n        #for time_name in times:\n        #    print(""%s: %.4fs"" % (time_name, times[time_name]))\n\n        self.nb_ticks_done += 1\n\n    def train(self):\n        """"""Training function.""""""\n\n        print(""[OnRouteAdvisorVisible] Training."")\n        print(""[OnRouteAdvisorVisible] Memory size: %d train, %d val"" % (self.memory.size, self.memory_val.size))\n\n        # initialize background batch loaders that generate batches on other\n        # CPU cores\n        batch_loader_train = BatchLoader(\n            val=False, batch_size=BATCH_SIZE, augseq=self.augseq,\n            previous_states_distances=PREVIOUS_STATES_DISTANCES, nb_future_states=NB_FUTURE_STATES_TRAIN, model_height=MODEL_HEIGHT, model_width=MODEL_WIDTH, model_prev_height=MODEL_PREV_HEIGHT, model_prev_width=MODEL_PREV_WIDTH\n        )\n        batch_loader_val = BatchLoader(\n            val=True, batch_size=BATCH_SIZE, augseq=iaa.Noop(),\n            previous_states_distances=PREVIOUS_STATES_DISTANCES, nb_future_states=NB_FUTURE_STATES_VAL, model_height=MODEL_HEIGHT, model_width=MODEL_WIDTH, model_prev_height=MODEL_PREV_HEIGHT, model_prev_width=MODEL_PREV_WIDTH\n        )\n        batch_loader_train = BackgroundBatchLoader(batch_loader_train, queue_size=25, nb_workers=6)\n        batch_loader_val = BackgroundBatchLoader(batch_loader_val, queue_size=15, nb_workers=2)\n\n        self.switch_models_to_train()\n\n        for batch_idx in xrange(NB_BATCHES_PER_TRAIN):\n            # fix model parameters every N batches\n            if batch_idx == 0 or batch_idx % TRAIN_FIX_EVERY_N_BATCHES == 0:\n                models_fixed = dict([(key, copy.deepcopy(model)) for (key, model) in self.models.items() if key in set([""indirect_reward_predictor""])])\n\n            self._run_batch(batch_loader_train, batch_idx, models_fixed, val=False)\n\n            if DO_VALIDATE(self.epoch, batch_idx, self.batch_idx_total):\n                self.switch_models_to_eval()\n                for i in xrange(NB_VAL_BATCHES):\n                    self._run_batch(batch_loader_val, batch_idx, models_fixed, val=True)\n                self.switch_models_to_train()\n\n            # every N batches, plot loss curves\n            if DO_PLOT(self.epoch, batch_idx, self.batch_idx_total):\n                self.loss_plotter.plot(self.history)\n\n            # every N batches, save a checkpoint\n            if DO_SAVE(self.epoch, batch_idx, self.batch_idx_total):\n                self._save()\n\n            self.batch_idx_total += 1\n\n        print(""Joining batch loaders..."")\n        batch_loader_train.join()\n        batch_loader_val.join()\n\n        print(""Saving..."")\n        self._save()\n\n        self.switch_models_to_eval()\n        self.epoch += 1\n\n        print(""Finished training."")\n\n    def _run_batch(self, batch_loader, batch_idx, models_fixed, val):\n        """"""Train/Validate on a single batch.""""""\n        time_prep_start = time.time()\n        train = not val\n\n        embedder_supervised = self.models[""embedder_supervised""]\n        embedder_reinforced = self.models[""embedder_reinforced""]\n        direct_reward_predictor = self.models[""direct_reward_predictor""]\n        indirect_reward_predictor = self.models[""indirect_reward_predictor""]\n        successor_predictor = self.models[""successor_predictor""]\n        ae_decoder = self.models[""ae_decoder""]\n\n        #embedder_supervised_fixed = models_fixed[""embedder_supervised""]\n        #embedder_reinforced_fixed = models_fixed[""embedder_reinforced""]\n        #direct_reward_predictor_fixed = models_fixed[""direct_reward_predictor""]\n        indirect_reward_predictor_fixed = models_fixed[""indirect_reward_predictor""]\n        #successor_predictor_fixed = models_fixed[""successor_predictor""]\n        #ae_decoder_fixed = models_fixed[""ae_decoder""]\n\n        backward_supervised = not val and DO_TRAIN_SUPERVISED(self.epoch, batch_idx, self.batch_idx_total)\n        backward_successor_rewards = DO_BACKPROP_SUCCESSOR_REWARDS(self.epoch, batch_idx, self.batch_idx_total)\n        cutoff_backward_successor = DO_CUTOFF_BACKPROP_SUCCESSOR(self.epoch, batch_idx, self.batch_idx_total)\n        cutoff_backward_successor_rewards = DO_CUTOFF_BACKPROP_SUCCESSOR_REWARDS(self.epoch, batch_idx, self.batch_idx_total)\n        train_ae = DO_TRAIN_AE(self.epoch, batch_idx, self.batch_idx_total)\n        #print(\n        #    ""backward_supervised"", backward_supervised,\n        #    ""backward_successor_rewards"", backward_successor_rewards,\n        #    ""cutoff_backward_successor"", cutoff_backward_successor,\n        #    ""cutoff_backward_successor_rewards"", cutoff_backward_successor_rewards,\n        #    ""train_ae"", train_ae\n        #)\n        time_prep_end = time.time()\n\n        # ----------\n        # collect batch\n        # ----------\n        time_cbatch_start = time.time()\n        batch = batch_loader.get_batch()\n        time_cbatch_end = time.time()\n\n        time_cbatchf_start = time.time()\n        inputs_supervised = batch.inputs_supervised(volatile=not backward_supervised, requires_grad=False, gpu=Config.GPU)\n        inputs_reinforced_add = batch.inputs_reinforced_add(volatile=val, requires_grad=False, gpu=Config.GPU)\n        future_inputs_supervised = batch.future_inputs_supervised(volatile=not backward_supervised, requires_grad=False, gpu=Config.GPU)\n        future_reinforced_add = batch.future_reinforced_add(volatile=val, requires_grad=False, gpu=Config.GPU)\n        inputs_successor_multiactions_vecs = batch.inputs_successor_multiactions_vecs(volatile=val, requires_grad=False, gpu=Config.GPU)\n        outputs_dr_gt = batch.outputs_dr_gt(volatile=val, requires_grad=False, gpu=Config.GPU)\n        outputs_dr_future_gt = batch.outputs_dr_future_gt(volatile=val, requires_grad=False, gpu=Config.GPU)\n        direct_reward_values = batch.direct_rewards_values(volatile=val, requires_grad=False, gpu=Config.GPU)\n        future_direct_reward_values = batch.future_direct_rewards_values(volatile=val, requires_grad=False, gpu=Config.GPU)\n        #future_direct_reward_values = batch.future_direct_rewards_values(volatile=val, requires_grad=False, gpu=GPU)\n        #next_direct_rewards_values = future_direct_reward_values[0, ...]\n        chosen_action_indices = batch.chosen_action_indices()\n        chosen_action_indices_future = batch.chosen_action_indices_future()\n        outputs_ae_gt = batch.outputs_ae_gt(volatile=val, requires_grad=False, gpu=Config.GPU)\n        time_cbatchf_end = time.time()\n\n        # ----------\n        # forward/backward\n        # ----------\n        time_fwbw_start = time.time()\n\n        if train:\n            # zero grad of all optimizers\n            for key in self.optimizers:\n                self.optimizers[key].zero_grad()\n\n        # embed future states\n        T, B, C, H, W = future_inputs_supervised[0].size()\n        T, B, Cprev, Hprev, Wprev = future_inputs_supervised[1].size()\n        future_emb_sup = embedder_supervised.embed(\n            future_inputs_supervised[0].view(T*B, C, H, W),\n            future_inputs_supervised[1].view(T*B, Cprev, Hprev, Wprev)\n        )\n        if not backward_supervised:\n            future_emb_sup = Variable(future_emb_sup.data, volatile=val, requires_grad=True if not val else False)\n        future_emb_rf = embedder_reinforced.forward_dict(future_emb_sup, future_reinforced_add)\n        future_emb_rf_by_time = future_emb_rf.view(T, B, -1)\n        outputs_successor_gt = to_cuda(Variable(future_emb_rf_by_time.data, volatile=val, requires_grad=False), Config.GPU)\n\n        # embed current states\n        emb_sup = embedder_supervised.embed(inputs_supervised[0], inputs_supervised[1])\n        if not backward_supervised:\n            emb_sup = to_cuda(Variable(emb_sup.data, volatile=val, requires_grad=True if not val else False), Config.GPU)\n        emb_rf = embedder_reinforced.forward_dict(emb_sup, inputs_reinforced_add)\n\n        # predict indirect rewards\n        outputs_idr_preds = indirect_reward_predictor(emb_rf)\n        outputs_idr_preds_next_fixed = indirect_reward_predictor_fixed(future_emb_rf_by_time[0])\n\n        outputs_idr_gt = direct_reward_values + GAMMA * outputs_idr_preds_next_fixed\n        outputs_idr_gt = idr_zerograd_unchosen_actions(outputs_idr_gt, outputs_idr_preds, chosen_action_indices)\n        outputs_idr_gt = to_cuda(Variable(outputs_idr_gt.data, volatile=val, requires_grad=False), Config.GPU)\n\n        # predict direct reward, successor states, AE decodings\n        outputs_dr_preds = direct_reward_predictor(emb_rf, softmax=False)\n        successor_input = emb_rf\n        if cutoff_backward_successor:\n            successor_input = to_cuda(Variable(successor_input.data, volatile=val, requires_grad=True if not val else False), Config.GPU)\n        outputs_successor_preds = successor_predictor(\n            successor_input,\n            inputs_successor_multiactions_vecs,\n            volatile=False, gpu=Config.GPU\n        )[0]\n        if train_ae:\n            outputs_ae_preds = ae_decoder(emb_rf)\n        else:\n            outputs_ae_preds = outputs_ae_gt\n\n        # predict direct rewards on future states\n        # (embeddings for these are already available for successors anyways)\n        outputs_dr_future_preds = direct_reward_predictor(future_emb_rf_by_time.view(T*B, -1), softmax=False)\n\n        outputs_idr_future_preds_all = indirect_reward_predictor(future_emb_rf_by_time.view(T*B, -1)).view(T, B, -1)\n        outputs_idr_future_preds_all_fixed = indirect_reward_predictor_fixed(future_emb_rf_by_time.view(T*B, -1)).view(T, B, -1)\n        outputs_idr_future_preds = outputs_idr_future_preds_all[0:-1]\n        outputs_idr_future_preds_next = outputs_idr_future_preds_all_fixed[1:]\n        outputs_idr_future_gt = future_direct_reward_values[:-1] + GAMMA * outputs_idr_future_preds_next\n        outputs_idr_future_gt = idr_zerograd_unchosen_actions_future(outputs_idr_future_gt, outputs_idr_future_preds, chosen_action_indices_future)\n        outputs_idr_future_preds = outputs_idr_future_preds.view((T-1)*B, -1)\n        outputs_idr_future_gt = outputs_idr_future_gt.view((T-1)*B, -1)\n        outputs_idr_future_gt = to_cuda(Variable(outputs_idr_future_gt.data, volatile=val, requires_grad=False), gpu=Config.GPU)\n\n        # predict the direct and indirect reward for successors\n        # (i.e. predictions of future states)\n        # these losses are currently not backpropagated through the successor\n        # and embedder\n        dr_successors_input = outputs_successor_preds\n        idr_successors_input = outputs_successor_preds[0:-1]\n        if cutoff_backward_successor_rewards:\n            dr_successors_input = to_cuda(Variable(dr_successors_input.data, volatile=val, requires_grad=True if not val else False), Config.GPU)\n            idr_successors_preds = to_cuda(Variable(idr_successors_input.data, volatile=val, requires_grad=True if not val else False), Config.GPU)\n        outputs_dr_successors_preds = direct_reward_predictor(dr_successors_input.view(T*B, -1), softmax=False)\n        outputs_idr_successors_preds = indirect_reward_predictor(idr_successors_input.view((T-1)*B, -1)).view((T-1), B, -1)\n        outputs_dr_successors_gt = outputs_dr_future_gt\n        outputs_idr_successors_gt = outputs_idr_future_gt.clone().view((T-1), B, -1)\n        outputs_idr_successors_gt = idr_zerograd_unchosen_actions_future(outputs_idr_successors_gt, outputs_idr_successors_preds, chosen_action_indices_future)\n        outputs_idr_successors_gt = outputs_idr_successors_gt.view((T-1)*B, -1)\n        outputs_idr_successors_gt = to_cuda(Variable(outputs_idr_successors_gt.data, volatile=val, requires_grad=False), gpu=Config.GPU)\n\n        #print(""outputs_dr_preds"", outputs_dr_preds.size(), outputs_dr_gt.size())\n        #print(""outputs_dr_gt.max(dim=1)[1]"", outputs_dr_gt.max(dim=1)[1])\n        #print(""outputs_dr_future_preds"", outputs_dr_future_preds.size(), outputs_dr_future_gt.size())\n        #print(""outputs_dr_successors_preds"", outputs_dr_successors_preds.size(), outputs_dr_successors_gt.size())\n        #print(""outputs_idr_successors_preds"", outputs_idr_successors_preds.size(), outputs_idr_successors_gt.size())\n\n        loss_dr = self.criterions[""direct_reward_predictor""](outputs_dr_preds, outputs_dr_gt.max(dim=1)[1].squeeze(dim=1))\n        loss_dr_future = self.criterions[""direct_reward_predictor""](outputs_dr_future_preds, outputs_dr_future_gt.view(T*B, -1).max(dim=1)[1].squeeze(dim=1))\n        loss_dr_successors = self.criterions[""direct_reward_predictor""](outputs_dr_successors_preds, outputs_dr_successors_gt.view(T*B, -1).max(dim=1)[1].squeeze(dim=1))\n        loss_idr = self.criterions[""indirect_reward_predictor""](outputs_idr_preds, outputs_idr_gt)\n        loss_idr_future = self.criterions[""indirect_reward_predictor""](outputs_idr_future_preds, outputs_idr_future_gt)\n        loss_idr_successors = self.criterions[""indirect_reward_predictor""](outputs_idr_successors_preds, outputs_idr_successors_gt)\n        loss_successor = self.criterions[""successor_predictor""](outputs_successor_preds, outputs_successor_gt)\n        loss_ae = self.criterions[""ae_decoder""](outputs_ae_preds, outputs_ae_gt) if train_ae else None\n\n        if train:\n            lw = [\n                (loss_dr, LOSS_DR_WEIGHTING),\n                (loss_dr_future, LOSS_DR_FUTURE_WEIGHTING),\n                (loss_idr, LOSS_IDR_WEIGHTING),\n                (loss_idr_future, LOSS_IDR_FUTURE_WEIGHTING),\n                (loss_successor, LOSS_SUCCESSOR_WEIGHTING),\n                (loss_dr_successors, LOSS_DR_SUCCESSORS_WEIGHTING if backward_successor_rewards else 0),\n                (loss_idr_successors, LOSS_IDR_SUCCESSORS_WEIGHTING if backward_successor_rewards else 0),\n                (loss_ae, LOSS_AE_WEIGHTING if train_ae else 0)\n            ]\n            lw_reduced = [(loss, weighting) for (loss, weighting) in lw if loss is not None and weighting > 0]\n\n            losses = [loss for (loss, weighting) in lw_reduced]\n            losses_grad = [loss.data.new().resize_as_(loss.data).fill_(weighting) for (loss, weighting) in lw_reduced]\n\n            if len(lw_reduced) == 0:\n                print(""[WARNING] After removing losses without weights there was nothing left to train on."")\n            else:\n                torch.autograd.backward(losses, losses_grad)\n                for key in self.optimizers:\n                    if key == ""embedder_supervised"":\n                        if backward_supervised:\n                            self.optimizers[key].step()\n                    elif key == ""ae_decoder"":\n                        if train_ae:\n                            self.optimizers[key].step()\n                    else:\n                        self.optimizers[key].step()\n\n        torch.cuda.synchronize()\n        time_fwbw_end = time.time()\n\n        time_finish_start = time.time()\n        # add average loss values to history and output message\n        self.history.add_value(""loss-dr"", ""train"" if train else ""val"", self.batch_idx_total, loss_dr.data[0], average=val)\n        self.history.add_value(""loss-dr-future"", ""train"" if train else ""val"", self.batch_idx_total, loss_dr_future.data[0], average=val)\n        self.history.add_value(""loss-dr-successors"", ""train"" if train else ""val"", self.batch_idx_total, loss_dr_successors.data[0], average=val)\n        self.history.add_value(""loss-idr"", ""train"" if train else ""val"", self.batch_idx_total, loss_idr.data[0], average=val)\n        self.history.add_value(""loss-idr-future"", ""train"" if train else ""val"", self.batch_idx_total, loss_idr_future.data[0], average=val)\n        self.history.add_value(""loss-idr-successors"", ""train"" if train else ""val"", self.batch_idx_total, loss_idr_successors.data[0], average=val)\n        self.history.add_value(""loss-successor"", ""train"" if train else ""val"", self.batch_idx_total, loss_successor.data[0], average=val)\n        self.history.add_value(""loss-ae"", ""train"" if train else ""val"", self.batch_idx_total, loss_ae.data[0] if train_ae else 0, average=val)\n\n        msg_losses = ""dr=%.4f dr-future=%.4f dr-successors=%.4f idr=%.4f idr-future=%.4f idr-successors=%.4f successor=%.4f ae=%.4f"" % (loss_dr.data[0], loss_dr_future.data[0], loss_dr_successors.data[0], loss_idr.data[0], loss_idr_future.data[0], loss_idr_successors.data[0], loss_successor.data[0], loss_ae.data[0] if train_ae else 0,)\n\n        # generate an overview image of the last batch\n        if (batch_idx+1) % 20 == 0 or val:\n            debug_img = generate_training_debug_image(\n                inputs_supervised[0], inputs_supervised[1],\n                outputs_dr_preds, outputs_dr_gt,\n                outputs_idr_preds, outputs_idr_gt,\n                outputs_successor_preds, outputs_successor_gt,\n                outputs_ae_preds, outputs_ae_gt,\n                outputs_dr_successors_preds.view(T, B, -1), outputs_dr_successors_gt,\n                outputs_idr_successors_preds.view((T-1), B, 9), outputs_idr_successors_gt.view((T-1), B, 9),\n                batch.multiactions\n            )\n            misc.imsave(""train_reinforced_debug_img_%s.png"" % (""train"" if train else ""val"",), debug_img)\n        time_finish_end = time.time()\n\n        print(""%s%d/%04d L[%s] T[prep=%.02fs, cb=%.02fs cbf=%.2fs fwbw=%.02fs, fin=%.2fs]"" % (\n            ""T"" if train else ""V"",\n            self.epoch,\n            batch_idx,\n            msg_losses,\n            time_prep_end - time_prep_start,\n            time_cbatch_end - time_cbatch_start,\n            time_cbatchf_end - time_cbatchf_start,\n            time_fwbw_end - time_fwbw_start,\n            time_finish_end - time_finish_start\n        ))\n\n    def _save(self):\n        """"""Function to save a checkpoint.""""""\n        torch.save({\n            ""history"": self.history.to_string(),\n            ""embedder_supervised_state_dict"": self.models[""embedder_supervised""].state_dict(),\n            ""embedder_reinforced_state_dict"": self.models[""embedder_reinforced""].state_dict(),\n            ""direct_reward_predictor_state_dict"": self.models[""direct_reward_predictor""].state_dict(),\n            ""indirect_reward_predictor_state_dict"": self.models[""indirect_reward_predictor""].state_dict(),\n            ""successor_predictor_state_dict"": self.models[""successor_predictor""].state_dict(),\n            ""ae_decoder_state_dict"": self.models[""ae_decoder""].state_dict(),\n            ""p_explore"": self.p_explore[0],\n            ""nb_ticks_done"": self.nb_ticks_done,\n            ""last_train_tick"": self.last_train_tick,\n            ""batch_idx_total"": self.batch_idx_total,\n            ""epoch"": self.epoch\n        }, ""train_reinforced_model.tar"")\n\ndef idr_zerograd_unchosen_actions(outputs_idr_gt, outputs_idr_preds, chosen_action_indices):\n    """"""Set gradients to zero for indirect rewards which\'s actions were not\n    chosen. This is done by setting the ground truth to the prediction.""""""\n    B, _ = outputs_idr_gt.size()\n    for b_idx in xrange(B):\n        pred = outputs_idr_preds[b_idx]\n        for a_idx in xrange(9):\n            chosen_action_idx = chosen_action_indices[b_idx]\n            if a_idx != chosen_action_idx:\n                outputs_idr_gt[b_idx, a_idx] = pred[a_idx]\n    return outputs_idr_gt\n\ndef idr_zerograd_unchosen_actions_future(outputs_idr_future_gt, outputs_idr_future_preds, chosen_action_indices_future):\n    """"""Set gradients to zero for future indirect rewards which\'s actions were not\n    chosen. This is done by setting the ground truth to the prediction.""""""\n    T, B, _ = outputs_idr_future_gt.size()\n    for t_idx in xrange(T):\n        for b_idx in xrange(B):\n            pred = outputs_idr_future_preds[t_idx, b_idx]\n            for a_idx in xrange(9):\n                chosen_action_idx = chosen_action_indices_future[t_idx][b_idx]\n                if a_idx != chosen_action_idx:\n                    outputs_idr_future_gt[t_idx, b_idx, a_idx] = pred[a_idx]\n    return outputs_idr_future_gt\n\n""""""\ndef calculate_future_rewards_argmax(plan_to_reward_bins):\n    B, T, S = plan_to_reward_bins.shape\n    plan_to_reward_bins_ids = np.argmax(plan_to_reward_bins, axis=2)\n    plan_to_rewards = BIN_AVERAGES[plan_to_reward_bins_ids.flatten()].reshape(B, T)\n    plan_to_rewards_direct = plan_to_rewards * REWARDS_FUTURE_GAMMAS\n    plan_to_reward = np.sum(plan_to_rewards_direct, axis=1)\n    return plan_to_rewards_direct, plan_to_reward\n""""""\n\ndef calculate_future_rewards(plan_to_reward_bins, indirect_rewards):\n    """"""Compute expected rewards of plans by predicted direct and indirect\n    rewards (latter: only of last successor).""""""\n    B, T, S = plan_to_reward_bins.shape\n    if DIRECT_REWARD_ARGMAX:\n        plan_to_reward_bins_ids = np.argmax(plan_to_reward_bins, axis=2)\n        plan_to_rewards = BIN_AVERAGES[plan_to_reward_bins_ids.flatten()].reshape(B, T)\n    else:\n        plan_to_rewards = np.sum(plan_to_reward_bins * BIN_AVERAGES, axis=2)\n    plan_to_rewards_direct = plan_to_rewards * REWARDS_FUTURE_GAMMAS\n    #plan_to_reward_indirect = np.average(indirect_rewards, axis=1) * np.power(GAMMA, REWARDS_FUTURE_GAMMAS.shape[0]+1)\n    plan_to_reward_indirect = np.squeeze(indirect_rewards[:, 0]) * np.power(GAMMA, REWARDS_FUTURE_GAMMAS.shape[0]+1)\n    if REWARD_ADD_DIRECT_REWARD:\n        plan_to_reward = np.sum(plan_to_rewards_direct, axis=1) + plan_to_reward_indirect\n    else:\n        plan_to_reward = plan_to_reward_indirect\n    #plan_to_reward = np.sum(plan_to_rewards_direct, axis=1)\n    #plan_to_reward = np.squeeze(plan_to_reward_indirect)\n    return plan_to_rewards_direct, plan_to_reward_indirect, plan_to_reward\n\n""""""\ndef calculate_future_rewards(plan_to_reward_bins, indirect_rewards):\n    plan_to_rewards = np.sum(plan_to_reward_bins * BIN_AVERAGES, axis=2)\n    plan_to_rewards_direct = plan_to_rewards * REWARDS_FUTURE_GAMMAS\n    plan_to_reward_indirect = indirect_rewards * np.power(GAMMA, REWARDS_FUTURE_GAMMAS.shape[0]+1)\n    plan_to_reward = np.sum(plan_to_rewards_direct, axis=1) + np.squeeze(plan_to_reward_indirect)\n    #plan_to_reward = np.squeeze(plan_to_reward_indirect)\n    return plan_to_rewards_direct, plan_to_reward_indirect, plan_to_reward\n""""""\n\ndef p_explore_to_actions(p_explore, action_up_down, action_left_right, last_state, constant_p_explore):\n    """"""Change actions according to epsilon-greedy policy.""""""\n    # if a constant p_explore value was chosen via --p_explore flag,\n    # then use that value\n    if constant_p_explore is not None:\n        p_explore = constant_p_explore\n\n    # in p_explore of all cases randomize up/down action\n    if random.random() < p_explore:\n        if last_state is not None and random.random() < P_EXPLORE_CHANCE_REDO:\n            # in p percent of all cases, simply repeat the last action\n            action_up_down = last_state.action_up_down\n        else:\n            # in (1-p) percent of all cases, chose a random action\n            action_up_down = random.choice(actionslib.ACTIONS_UP_DOWN)\n\n    # same for left/right action\n    if random.random() < p_explore:\n        if last_state is not None and random.random() < P_EXPLORE_CHANCE_REDO:\n            action_left_right = last_state.action_left_right\n        else:\n            action_left_right = random.choice(actionslib.ACTIONS_LEFT_RIGHT)\n\n    return action_up_down, action_left_right\n\nif __name__ == ""__main__"":\n    main()\n'"
train_reinforced/visualization.py,1,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom lib import actions as actionslib\nfrom lib import util\nfrom lib.util import to_numpy\n\nimport imgaug as ia\nimport numpy as np\nimport torch.nn.functional as F\n\ntry:\n    xrange\nexcept NameError:\n    xrange = range\n\ndef generate_overview_image(current_state, last_state, \\\n    action_up_down_bpe, action_left_right_bpe, \\\n    memory, memory_val, \\\n    ticks, last_train_tick, \\\n    plans, plan_to_rewards_direct, plan_to_reward_indirect, \\\n    plan_to_reward, plans_ranking, current_plan, best_plan_ae_decodings,\n    idr_v, idr_adv,\n    grids, args):\n    h, w = current_state.screenshot_rs.shape[0:2]\n    scr = np.copy(current_state.screenshot_rs)\n    scr = ia.imresize_single_image(scr, (h//2, w//2))\n\n    if best_plan_ae_decodings is not None:\n        ae_decodings = (to_numpy(best_plan_ae_decodings) * 255).astype(np.uint8).transpose((0, 2, 3, 1))\n        ae_decodings = [ia.imresize_single_image(ae_decodings[i, ...], (h//4, w//4)) for i in xrange(ae_decodings.shape[0])]\n        ae_decodings = ia.draw_grid(ae_decodings, cols=5)\n        #ae_decodings = np.vstack([\n        #    np.hstack(ae_decodings[0:5]),\n        #    np.hstack(ae_decodings[5:10])\n        #])\n    else:\n        ae_decodings = np.zeros((1, 1, 3), dtype=np.uint8)\n\n    if grids is not None:\n        scr_rs = ia.imresize_single_image(scr, (h//4, w//4))\n        grids = (to_numpy(grids)[0] * 255).astype(np.uint8)\n        grids = [ia.imresize_single_image(grids[i, ...][:,:,np.newaxis], (h//4, w//4)) for i in xrange(grids.shape[0])]\n        grids = [util.draw_heatmap_overlay(scr_rs, np.squeeze(grid/255).astype(np.float32)) for grid in grids]\n        grids = ia.draw_grid(grids, cols=4)\n    else:\n        grids = np.zeros((1, 1, 3), dtype=np.uint8)\n\n    plans_text = []\n\n    if idr_v is not None and idr_adv is not None:\n        idr_v = to_numpy(idr_v[0])\n        idr_adv = to_numpy(idr_adv[0])\n        plans_text.append(""V(s): %+07.2f"" % (idr_v[0],))\n        adv_texts = []\n        curr = []\n        for i, ma in enumerate(actionslib.ALL_MULTIACTIONS):\n            curr.append(""A(%s%s): %+07.2f"" % (ma[0] if ma[0] != ""~WS"" else ""_"", ma[1] if ma[1] != ""~AD"" else ""_"", idr_adv[i]))\n            if (i+1) % 3 == 0 or (i+1) == len(actionslib.ALL_MULTIACTIONS):\n                adv_texts.append("" "".join(curr))\n                curr = []\n        plans_text.extend(adv_texts)\n\n    if current_plan is not None:\n        plans_text.append("""")\n        plans_text.append(""Current Plan:"")\n        actions_ud_text = []\n        actions_lr_text = []\n        for multiaction in current_plan:\n            actions_ud_text.append(""%s"" % (multiaction[0] if multiaction[0] != ""~WS"" else ""_"",))\n            actions_lr_text.append(""%s"" % (multiaction[1] if multiaction[1] != ""~AD"" else ""_"",))\n        plans_text.extend(["" "".join(actions_ud_text), "" "".join(actions_lr_text)])\n\n    plans_text.append("""")\n    plans_text.append(""Best Plans:"")\n    if plan_to_rewards_direct is not None:\n        for plan_idx in plans_ranking[::-1][0:5]:\n            plan = plans[plan_idx]\n            rewards_direct = plan_to_rewards_direct[plan_idx]\n            reward_indirect = plan_to_reward_indirect[plan_idx]\n            reward = plan_to_reward[plan_idx]\n            actions_ud_text = []\n            actions_lr_text = []\n            rewards_text = []\n            for multiaction in plan:\n                actions_ud_text.append(""%s"" % (multiaction[0] if multiaction[0] != ""~WS"" else ""_"",))\n                actions_lr_text.append(""%s"" % (multiaction[1] if multiaction[1] != ""~AD"" else ""_"",))\n            for rewards_t in rewards_direct:\n                rewards_text.append(""%+04.1f"" % (rewards_t,))\n            rewards_text.append(""| %+07.2f (V(s\')=%+07.2f)"" % (reward, reward_indirect))\n            plans_text.extend(["""", "" "".join(actions_ud_text), "" "".join(actions_lr_text), "" "".join(rewards_text)])\n    plans_text = ""\\n"".join(plans_text)\n\n    stats_texts = [\n        ""u/d bpe: %s"" % (action_up_down_bpe.rjust(5)),\n        ""  l/r bpe: %s"" % (action_left_right_bpe.rjust(5)),\n        ""u/d ape: %s %s"" % (current_state.action_up_down.rjust(5), ""[C]"" if action_up_down_bpe != current_state.action_up_down else """"),\n        ""  l/r ape: %s %s"" % (current_state.action_left_right.rjust(5), ""[C]"" if action_left_right_bpe != current_state.action_left_right else """"),\n        ""speed: %03d"" % (current_state.speed,) if current_state.speed is not None else ""speed: None"",\n        ""is_reverse: yes"" if current_state.is_reverse else ""is_reverse: no"",\n        ""is_damage_shown: yes"" if current_state.is_damage_shown else ""is_damage_shown: no"",\n        ""is_offence_shown: yes"" if current_state.is_offence_shown else ""is_offence_shown: no"",\n        ""steering wheel: %05.2f (%05.2f)"" % (current_state.steering_wheel_cnn, current_state.steering_wheel_raw_cnn),\n        ""reward for last state: %05.2f"" % (last_state.reward,) if last_state is not None else ""reward for last state: None"",\n        ""p_explore: %.2f%s"" % (current_state.p_explore if args.p_explore is None else args.p_explore, """" if args.p_explore is None else "" (constant)""),\n        ""memory size (train/val): %06d / %06d"" % (memory.size, memory_val.size),\n        ""ticks: %06d"" % (ticks,),\n        ""last train: %06d"" % (last_train_tick,)\n    ]\n    stats_text = ""\\n"".join(stats_texts)\n\n    all_texts = plans_text + ""\\n\\n\\n"" + stats_text\n\n    result = np.zeros((720, 590, 3), dtype=np.uint8)\n    util.draw_image(result, x=0, y=0, other_img=scr, copy=False)\n    util.draw_image(result, x=0, y=scr.shape[0]+10, other_img=ae_decodings, copy=False)\n    util.draw_image(result, x=0, y=scr.shape[0]+10+ae_decodings.shape[0]+10, other_img=grids, copy=False)\n    result = util.draw_text(result, x=0, y=scr.shape[0]+10+ae_decodings.shape[0]+10+grids.shape[0]+10, size=8, text=all_texts, color=[255, 255, 255])\n    return result\n\ndef generate_training_debug_image(inputs_supervised, inputs_supervised_prev, \\\n    outputs_dr_preds, outputs_dr_gt, \\\n    outputs_idr_preds, outputs_idr_gt, \\\n    outputs_successor_preds, outputs_successor_gt, \\\n    outputs_ae_preds, outputs_ae_gt, \\\n    outputs_dr_successors_preds, outputs_dr_successors_gt, \\\n    outputs_idr_successors_preds, outputs_idr_successors_gt,\n    multiactions):\n    imgs_in = to_numpy(inputs_supervised)[0]\n    imgs_in = np.clip(imgs_in * 255, 0, 255).astype(np.uint8).transpose((1, 2, 0))\n    imgs_in_prev = to_numpy(inputs_supervised_prev)[0]\n    imgs_in_prev = np.clip(imgs_in_prev * 255, 0, 255).astype(np.uint8).transpose((1, 2, 0))\n    h, w = imgs_in.shape[0:2]\n    imgs_in = np.vstack([\n        np.hstack([downscale(imgs_in[..., 0:3]), downscale(to_rgb(imgs_in_prev[..., 0]))]),\n        #np.hstack([downscale(to_rgb(imgs_in_prev[..., 1])), downscale(to_rgb(imgs_in_prev[..., 2]))])\n        np.hstack([downscale(to_rgb(imgs_in_prev[..., 1])), np.zeros_like(imgs_in[..., 0:3])])\n    ])\n    h_imgs = imgs_in.shape[0]\n\n    ae_gt = np.clip(to_numpy(outputs_ae_gt)[0] * 255, 0, 255).astype(np.uint8).transpose((1, 2, 0))\n    ae_preds = np.clip(to_numpy(outputs_ae_preds)[0] * 255, 0, 255).astype(np.uint8).transpose((1, 2, 0))\n    """"""\n    imgs_ae = np.vstack([\n        downscale(ae_preds[..., 0:3]),\n        downscale(to_rgb(ae_preds[..., 3])),\n        downscale(to_rgb(ae_preds[..., 4])),\n        downscale(to_rgb(ae_preds[..., 5]))\n    ])\n    """"""\n    imgs_ae = np.hstack([downscale(ae_gt), downscale(ae_preds)])\n    h_ae = imgs_ae.shape[0]\n\n    outputs_successor_dr_grid = draw_successor_dr_grid(\n        to_numpy(F.softmax(outputs_dr_successors_preds[:, 0, :])),\n        to_numpy(outputs_dr_successors_gt[:, 0]),\n        upscale_factor=(2, 4)\n    )\n\n    outputs_dr_preds = to_numpy(F.softmax(outputs_dr_preds))[0]\n    outputs_dr_gt = to_numpy(outputs_dr_gt)[0]\n    grid_preds = output_grid_to_image(outputs_dr_preds[np.newaxis, :], upscale_factor=(2, 4))\n    grid_gt = output_grid_to_image(outputs_dr_gt[np.newaxis, :], upscale_factor=(2, 4))\n    imgs_dr = np.hstack([\n        grid_gt,\n        np.zeros((grid_gt.shape[0], 4, 3), dtype=np.uint8),\n        grid_preds,\n        np.zeros((grid_gt.shape[0], 8, 3), dtype=np.uint8),\n        outputs_successor_dr_grid\n    ])\n    successor_multiactions_str = "" "".join([""%s%s"" % (ma[0] if ma[0] != ""~WS"" else ""_"", ma[1] if ma[1] != ""~AD"" else ""_"") for ma in multiactions[0]])\n    imgs_dr = np.pad(imgs_dr, ((30, 0), (0, 300), (0, 0)), mode=""constant"", constant_values=0)\n    imgs_dr = util.draw_text(imgs_dr, x=0, y=0, text=""DR curr bins gt:%s, pred:%s | successor preds\\nsucc. mas: %s"" % (str(np.argmax(outputs_dr_gt)), str(np.argmax(outputs_dr_preds)), successor_multiactions_str), size=9)\n    h_dr = imgs_dr.shape[0]\n\n    outputs_idr_preds = np.squeeze(to_numpy(outputs_idr_preds)[0])\n    outputs_idr_gt = np.squeeze(to_numpy(outputs_idr_gt)[0])\n    idr_text = [\n        ""[IndirectReward A0]"",\n        ""  gt: %.2f"" % (outputs_idr_gt[..., 0],),\n        ""  pr: %.2f"" % (outputs_idr_preds[..., 0],),\n        ""[IndirectReward A1]"",\n        ""  gt: %.2f"" % (outputs_idr_gt[..., 1],),\n        ""  pr: %.2f"" % (outputs_idr_preds[..., 1],),\n        ""[IndirectReward A2]"",\n        ""  gt: %.2f"" % (outputs_idr_gt[..., 2],),\n        ""  pr: %.2f"" % (outputs_idr_preds[..., 2],)\n    ]\n    idr_text = ""\\n"".join(idr_text)\n\n    outputs_successor_preds = np.squeeze(to_numpy(outputs_successor_preds)[:, 0, :])\n    outputs_successor_gt = np.squeeze(to_numpy(outputs_successor_gt)[:, 0, :])\n    distances = np.average((outputs_successor_preds - outputs_successor_gt) ** 2, axis=1)\n    successors_text = [\n        ""[Successors]"",\n        ""  Distances:"",\n        ""    "" + "" "".join([""%02.2f"" % (d,) for d in distances]),\n        ""  T=0 gt/pred:"",\n        ""    "" + "" "".join([""%+02.2f"" % (val,) for val in outputs_successor_gt[0, 0:25]]),\n        ""    "" + "" "".join([""%+02.2f"" % (val,) for val in outputs_successor_preds[0, 0:25]]),\n        ""  T=1 gt/pred:"",\n        ""    "" + "" "".join([""%+02.2f"" % (val,) for val in outputs_successor_gt[1, 0:25]]),\n        ""    "" + "" "".join([""%+02.2f"" % (val,) for val in outputs_successor_preds[1, 0:25]]),\n        ""  T=2 gt/pred:"",\n        ""    "" + "" "".join([""%+02.2f"" % (val,) for val in outputs_successor_gt[2, 0:25]]),\n        ""    "" + "" "".join([""%+02.2f"" % (val,) for val in outputs_successor_preds[2, 0:25]]),\n    ]\n    successors_text = ""\\n"".join(successors_text)\n\n    outputs_dr_successors_preds = np.squeeze(to_numpy(outputs_dr_successors_preds)[:, 0, :])\n    outputs_dr_successors_gt = np.squeeze(to_numpy(outputs_dr_successors_gt)[:, 0, :])\n    bins_dr_successors_preds = np.argmax(outputs_dr_successors_preds, axis=1)\n    bins_dr_successors_gt = np.argmax(outputs_dr_successors_gt, axis=1)\n    successors_dr_text = [\n        ""[Direct rewards bins of successors]"",\n        ""  gt:   "" + "" "".join([""%d"" % (b,) for b in bins_dr_successors_gt]),\n        ""  pred: "" + "" "".join([""%d"" % (b,) for b in bins_dr_successors_preds])\n    ]\n    successors_dr_text = ""\\n"".join(successors_dr_text)\n\n    outputs_idr_successors_preds = np.squeeze(to_numpy(outputs_idr_successors_preds)[:, 0, :])\n    outputs_idr_successors_gt = np.squeeze(to_numpy(outputs_idr_successors_gt)[:, 0, :])\n    successors_idr_text = [\n        ""[Indirect rewards of successors A0]"",\n        ""  gt:   "" + "" "".join([""%+03.2f"" % (v,) for v in outputs_idr_successors_gt[..., 0]]),\n        ""  pred: "" + "" "".join([""%+03.2f"" % (v,) for v in outputs_idr_successors_preds[..., 0]]),\n        ""[Indirect rewards of successors A1]"",\n        ""  gt:   "" + "" "".join([""%+03.2f"" % (v,) for v in outputs_idr_successors_gt[..., 1]]),\n        ""  pred: "" + "" "".join([""%+03.2f"" % (v,) for v in outputs_idr_successors_preds[..., 1]]),\n        ""[Indirect rewards of successors A2]"",\n        ""  gt:   "" + "" "".join([""%+03.2f"" % (v,) for v in outputs_idr_successors_gt[..., 2]]),\n        ""  pred: "" + "" "".join([""%+03.2f"" % (v,) for v in outputs_idr_successors_preds[..., 2]])\n    ]\n    successors_idr_text = ""\\n"".join(successors_idr_text)\n\n    result = np.zeros((950, 320, 3), dtype=np.uint8)\n    spacing = 4\n    util.draw_image(result, x=0, y=0, other_img=imgs_in, copy=False)\n    util.draw_image(result, x=0, y=h_imgs+spacing, other_img=imgs_ae, copy=False)\n    util.draw_image(result, x=0, y=h_imgs+spacing+h_ae+spacing, other_img=imgs_dr, copy=False)\n    result = util.draw_text(result, x=0, y=h_imgs+spacing+h_ae+spacing+h_dr+spacing, text=idr_text + ""\\n"" + successors_text + ""\\n"" + successors_dr_text + ""\\n"" + successors_idr_text, size=9)\n\n    return result\n\ndef to_rgb(im):\n    return np.tile(im[:,:,np.newaxis], (1, 1, 3))\n\ndef downscale(im):\n    return ia.imresize_single_image(im, (90, 160), interpolation=""cubic"")\n\ndef output_grid_to_image(output_grid, upscale_factor=(4, 4)):\n    if output_grid is None:\n        grid_vis = np.zeros((Config.MODEL_NB_REWARD_BINS, Config.MODEL_NB_FUTURE_BLOCKS), dtype=np.uint8)\n    else:\n        if output_grid.ndim == 3:\n            output_grid = output_grid[0]\n        grid_vis = (output_grid.transpose((1, 0)) * 255).astype(np.uint8)\n    grid_vis = np.tile(grid_vis[:, :, np.newaxis], (1, 1, 3))\n    if output_grid is None:\n        grid_vis[::2, ::2, :] = [255, 0, 0]\n    grid_vis = ia.imresize_single_image(grid_vis, (grid_vis.shape[0]*upscale_factor[0], grid_vis.shape[1]*upscale_factor[1]), interpolation=""nearest"")\n    grid_vis = np.pad(grid_vis, ((1, 1), (1, 1), (0, 0)), mode=""constant"", constant_values=128)\n    return grid_vis\n\ndef draw_successor_dr_grid(outputs_dr_successors_preds, outputs_dr_successors_gt, upscale_factor=(4, 4)):\n    T, S = outputs_dr_successors_preds.shape\n    cols = []\n    for t in range(T):\n        col = (outputs_dr_successors_preds[t][np.newaxis, :].transpose((1, 0)) * 255).astype(np.uint8)\n        col = np.tile(col[:, :, np.newaxis], (1, 1, 3))\n        correct_bin_idx = np.argmax(outputs_dr_successors_gt[t])\n        col[correct_bin_idx, 0, 2] = 255\n        col = ia.imresize_single_image(col, (col.shape[0]*upscale_factor[0], col.shape[1]*upscale_factor[1]), interpolation=""nearest"")\n        col = np.pad(col, ((1, 1), (1, 1), (0, 0)), mode=""constant"", constant_values=128)\n        cols.append(col)\n    return np.hstack(cols)\n'"
train_semisupervised/__init__.py,0,b''
train_semisupervised/batching.py,0,"b'""""""Functions/classes related to loading batches for training.""""""\nfrom __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport train\n\nimport numpy as np\nimport cv2\nimport imgaug as ia\nimport multiprocessing\nimport threading\nimport cPickle as pickle\nimport random\nfrom skimage import feature\n\nif sys.version_info[0] == 2:\n    import cPickle as pickle\nelif sys.version_info[0] == 3:\n    import pickle\n    xrange = range\n\ndef create_batch(examples, examples_autogen, augseq):\n    """"""Convert datasets of examples to batches.\n\n    This method will pick random examples from the datasets ""examples""\n    (25 percent per batch) and ""examples_autogen"" (75 percent per batch).\n\n    Parameters\n    ----------\n    examples : list of Example\n        One or more hand-annotated examples to use. The Example class\n        is defined in dataset.py.\n    examples_autogen : list of Example\n        One or more automatically annoated examples to use.\n    augseq : Augmenter\n        Augmentation sequence to use.\n\n    Returns\n    ----------\n    (images, images_prev), (outputs_ae, outputs_grids, outputs_atts, outputs_multiactions, outputs_flow, outputs_canny, outputs_flips), (grids_annotated, atts_annotated)\n    where\n        images are the current timestep input images\n        images_prev are for each batch element the previous timestep input images\n        outputs_ae are the ground truth outputs for the autoencoder (decoder)\n        outputs_grids are the ground truth hand-annotated grids (e.g. car positions in images)\n        outputs_atts are the ground truth hand-annotated attributes (e.g. number of lanes)\n        outputs_multiactions are the ground truth action outputs (e.g. next action one-hot-vector)\n        outputs_flow are the ground truth optical flow outputs\n        outputs_canny are the ground truth canny edge outputs\n        outputs_flips are the ground truth of flipped input images (temporal flips)\n        grids_annotated is a (B, G) array that contains a 1 if for batch element b grid g was annotated\n        atts_annotated is a (B, 1) array that contains a 1 if for batch element b attributes were annotated\n    """"""\n    B = train.BATCH_SIZE\n\n    img_h, img_w = train.MODEL_HEIGHT, train.MODEL_WIDTH\n    img_prev_h, img_prev_w = train.MODEL_PREV_HEIGHT, train.MODEL_PREV_WIDTH\n    ae_h, ae_w = int(img_h*(1/train.AE_DOWNSCALE_FACTOR)), int(img_w*(1/train.AE_DOWNSCALE_FACTOR))\n    grids_h, grids_w = int(img_h*(1/train.GRIDS_DOWNSCALE_FACTOR)), int(img_w*(1/train.GRIDS_DOWNSCALE_FACTOR))\n    flow_h, flow_w = int(img_h*(1/train.FLOW_DOWNSCALE_FACTOR)), int(img_w*(1/train.FLOW_DOWNSCALE_FACTOR))\n    canny_h, canny_w = int(img_h*(1/train.CANNY_DOWNSCALE_FACTOR)), int(img_w*(1/train.CANNY_DOWNSCALE_FACTOR))\n\n    images = np.zeros((B, img_h, img_w, 3), dtype=np.float32)\n    images_prev = np.zeros((B, img_prev_h, img_prev_w, len(train.PREVIOUS_STATES_DISTANCES)), dtype=np.float32)\n    outputs_ae = np.zeros((B, ae_h, ae_w, 3 + len(train.PREVIOUS_STATES_DISTANCES)), dtype=np.float32)\n    outputs_grids = np.zeros((B, grids_h, grids_w, len(train.ANNOTATIONS_GRIDS_FPS)), dtype=np.float32)\n    outputs_atts = np.zeros((B, train.NB_ATTRIBUTE_VALUES), dtype=np.float32)\n    outputs_multiactions = np.zeros((B, 9 + 9 + 9 + 9), dtype=np.float32)\n    outputs_flow = np.zeros((B, flow_h, flow_w, 1), dtype=np.float32)\n    outputs_canny = np.zeros((B, canny_h, canny_w, len(train.CANNY_SIGMAS)), dtype=np.float32)\n    outputs_flips = np.zeros((B, len(train.PREVIOUS_STATES_DISTANCES)), dtype=np.float32)\n    grids_annotated = np.zeros((B, len(train.GRIDS_ORDER)), dtype=np.int32)\n    atts_annotated = np.zeros((B, 1), dtype=np.int32)\n    for b_idx in xrange(B):\n        if b_idx == 0 or b_idx % 4 == 0:\n            rnd_idx = random.randint(0, len(examples)-1)\n            example = examples[rnd_idx]\n        else:\n            rnd_idx = random.randint(0, len(examples_autogen)-1)\n            example = examples_autogen[rnd_idx]\n\n        img_curr = downscale(example.screenshot_rs, img_h, img_w)\n        img_curr_y = gray(img_curr)\n        imgs_prev = downscale(example.previous_screenshots_rs, img_prev_h, img_prev_w)\n        imgs_prev_y = gray(imgs_prev)\n\n        if random.random() < train.P_FLIP:\n            flip_ids = random.sample(range(len(imgs_prev)), 2)\n            imgs_prev[flip_ids[1]], imgs_prev[flip_ids[0]] = imgs_prev[flip_ids[0]], imgs_prev[flip_ids[1]]\n            outputs_flips[b_idx, flip_ids[0]] = 1\n            outputs_flips[b_idx, flip_ids[1]] = 1\n            #misc.imshow(\n            #    np.vstack([\n            #        np.hstack(imgs_prev_orig),\n            #        np.hstack(imgs_prev)\n            #    ])\n            #)\n\n        grids, grids_annotated_one = example.get_grids_array()\n        grids = (grids*255).astype(np.uint8)\n        atts, has_attributes = example.get_attributes_array()\n\n        flow_settings = [\n            {""pyr_scale"": 0.5, ""levels"": 3, ""winsize"": 3, ""iterations"": 2, ""poly_n"": 5, ""poly_sigma"": 1.2, ""flags"": 0}\n        ]\n        flow_imgs = []\n        flow_curr_y_rs = downscale(img_curr_y, flow_h, flow_w)\n        flow_prev_y_rs = gray(downscale(example.previous_screenshots_rs[0], flow_h, flow_w))\n        for settings in flow_settings:\n            # API of calcOpticalFlowFarneback is apparently different in python 3\n            if sys.version_info[0] == 2:\n                flow = cv2.calcOpticalFlowFarneback(\n                    flow_prev_y_rs,\n                    flow_curr_y_rs,\n                    **settings\n                )\n            else:\n                flow = cv2.calcOpticalFlowFarneback(\n                    flow_prev_y_rs,\n                    flow_curr_y_rs,\n                    None,\n                    **settings\n                )\n            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n\n            """"""\n            def to_rgb(im):\n                return np.clip(np.tile(im[...,np.newaxis], (1, 1, 3)), 0, 255).astype(np.uint8)\n            misc.imshow(np.vstack([\n                np.hstack([imgs_prev[0], img_curr]),\n                np.hstack([to_rgb(mag*2), to_rgb(ang*2)]),\n                np.hstack([to_rgb(np.log(mag+0.00001)*255), to_rgb(np.log(ang*0.5)*255)]),\n                np.hstack([util.draw_heatmap_overlay(img_curr, np.log(mag+0.00001)), util.draw_heatmap_overlay(img_curr, np.log(ang*0.5))])\n            ]))\n            """"""\n\n            mag = np.clip(np.log(mag + 0.00001), 0, 1)\n            ang = np.clip(np.log(ang*0.5 + 0.00001), 0, 1)\n            flow_img = mag[:, :, np.newaxis]\n            flow_img = (flow_img * 255).astype(np.uint8)\n            #misc.imshow(np.hstack([to_rgb(flow_img[...,0]), to_rgb(flow_img[...,1])]))\n            flow_imgs.append(flow_img)\n\n        augseq_det = augseq.to_deterministic()\n        augseq_det_alt = augseq_det.deepcopy()\n        augs_to_keep = augseq_det_alt.find_augmenters_by_name("".*(Affine|Crop|Fliplr).*"", regex=True)\n        augseq_det_alt = augseq_det_alt.remove_augmenters(lambda a, parents: a not in augs_to_keep)\n        img_curr_aug = augseq_det.augment_image(img_curr)\n\n        imgs_prev_y_aug = [augseq_det.augment_image(img) for img in imgs_prev_y]\n        imgs_prev_y_aug = np.array(imgs_prev_y_aug, dtype=np.uint8).transpose((1, 2, 0))\n\n        img_aug = img_curr_aug\n        img_ae_aug = np.dstack([img_curr_aug, downscale(imgs_prev_y_aug, ae_h, ae_w)])\n\n        img_ae_aug_rs = downscale(img_ae_aug, ae_h, ae_w)\n        grids_rs = downscale(grids, grids_h, grids_w)\n        flow_imgs_rs = downscale(flow_imgs, flow_h, flow_w)\n\n        flow_imgs_rs_aug = [augseq_det_alt.augment_image(flow_img) for flow_img in flow_imgs_rs]\n        grids_rs_aug = augseq_det_alt.augment_image(grids_rs)\n\n        #misc.imshow(np.hstack([to_rgb(flow_img_aug_rs[...,0]*255), to_rgb(flow_img_aug_rs[...,1]*255)]))\n\n        img_curr_y_canny = downscale(img_curr_y, canny_h, canny_w)\n        imgs_canny = []\n        for sidx, sigma in enumerate(train.CANNY_SIGMAS):\n            imgs_canny.append(feature.canny(img_curr_y_canny, sigma=sigma))\n        imgs_canny = (np.array(imgs_canny) * 255).astype(np.uint8).transpose((1, 2, 0))\n        imgs_canny_aug = augseq_det_alt.augment_image(imgs_canny)\n        """"""\n        misc.imshow(\n            np.vstack([\n                np.hstack([imgs_canny[...,i] for i in xrange(imgs_canny.shape[2])]),\n                np.hstack([imgs_canny_aug[...,i] for i in xrange(imgs_canny_aug.shape[2])])\n            ])\n        )\n        """"""\n\n        images[b_idx, ...] = img_aug / 255\n        images_prev[b_idx, ...] = imgs_prev_y_aug / 255\n        outputs_ae[b_idx, ...] = img_ae_aug_rs / 255\n        outputs_grids[b_idx, ...] = grids_rs_aug / 255\n        outputs_atts[b_idx, ...] = atts\n        outputs_multiactions[b_idx, 0:9] = example.previous_multiaction_vecs_avg\n        outputs_multiactions[b_idx, 9:18] = example.next_multiaction_vecs_avg\n        outputs_multiactions[b_idx, 18:27] = example.multiaction_vec\n        outputs_multiactions[b_idx, 27:36] = example.next_multiaction_vec\n        for i, flow_img_rs_aug in enumerate(flow_imgs_rs_aug):\n            outputs_flow[b_idx, :, :, i] = np.squeeze(flow_img_rs_aug / 255)\n        outputs_canny[b_idx, ...] = imgs_canny_aug / 255\n        grids_annotated[b_idx, ...] = grids_annotated_one\n        atts_annotated[b_idx, ...] = has_attributes\n\n        """"""\n        def to_rgb(im):\n            return np.tile(im[:,:,np.newaxis], (1, 1, 3))\n        misc.imshow(\n            np.hstack([\n                images[b_idx, ..., 0:3],\n                to_rgb(images[b_idx, ..., 3]),\n                to_rgb(images[b_idx, ..., 4]),\n                to_rgb(images[b_idx, ..., 5])\n            ])\n        )\n        """"""\n\n    images = images.transpose(0, 3, 1, 2)\n    images_prev = images_prev.transpose(0, 3, 1, 2)\n    outputs_ae = outputs_ae.transpose(0, 3, 1, 2)\n    outputs_grids = outputs_grids.transpose(0, 3, 1, 2)\n    outputs_flow = outputs_flow.transpose(0, 3, 1, 2)\n    outputs_canny = outputs_canny.transpose(0, 3, 1, 2)\n\n    #for arr in [images, outputs_ae, outputs_grids, outputs_atts, outputs_multiactions, outputs_flow, outputs_flips]:\n    #    print(np.min(arr), np.average(arr), np.max(arr))\n\n    return (images, images_prev), (outputs_ae, outputs_grids, outputs_atts, outputs_multiactions, outputs_flow, outputs_canny, outputs_flips), (grids_annotated, atts_annotated)\n\ndef downscale(im, h, w):\n    """"""Downscale one or more images to size (h, w).""""""\n    if isinstance(im, list):\n        return [downscale(i, h, w) for i in im]\n    else:\n        if im.ndim == 2:\n            im = im[:, :, np.newaxis]\n            im_rs = ia.imresize_single_image(im, (h, w), interpolation=""cubic"")\n            return np.squeeze(im)\n        else:\n            return ia.imresize_single_image(im, (h, w), interpolation=""cubic"")\n\ndef gray(im):\n    """"""Convert one or more images to grayscale.""""""\n    if isinstance(im, list):\n        return [gray(i) for i in im]\n    else:\n        return cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n\ndef to_rgb(im):\n    """"""Convert an image from (h, w) to (h, w, 3).""""""\n    if im.ndim == 3:\n        if im.shape[2] == 3:\n            return im\n        else:\n            return np.tile(im, (1, 1, 3))\n    else:\n        return np.tile(im[:, :, np.newaxis], (1, 1, 3))\n\nclass BatchLoader(object):\n    """"""Class to generate batches in multiple background processes.""""""\n\n    def __init__(self, dataset, dataset_autogen, queue_size, augseq, nb_workers, threaded=False):\n        self.queue = multiprocessing.Queue(queue_size)\n        self.workers = []\n        for i in range(nb_workers):\n            seed = random.randint(0, 10**6)\n            augseq_worker = augseq.deepcopy()\n            if threaded:\n                worker = threading.Thread(target=self._load_batches, args=(dataset, dataset_autogen, self.queue, augseq_worker, None))\n            else:\n                worker = multiprocessing.Process(target=self._load_batches, args=(dataset, dataset_autogen, self.queue, augseq_worker, seed))\n            worker.daemon = True\n            worker.start()\n            self.workers.append(worker)\n\n    def get_batch(self):\n        return pickle.loads(self.queue.get())\n\n    def _load_batches(self, dataset, dataset_autogen, queue, augseq_worker, seed):\n        if seed is not None:\n            random.seed(seed)\n            np.random.seed(seed)\n            augseq_worker.reseed(seed)\n            ia.seed(seed)\n\n        while True:\n            batch = create_batch(dataset, dataset_autogen, augseq_worker)\n            queue.put(pickle.dumps(batch, protocol=-1))\n\n    def join(self):\n        for worker in self.workers:\n            worker.terminate()\n'"
train_semisupervised/compress_annotations.py,0,"b'from __future__ import division, print_function\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport train\nfrom dataset import load_dataset_annotated\n\nimport cPickle as pickle\nimport gzip as gz\nfrom lib import util\n\ndef main():\n    examples = load_dataset_annotated()\n    print(""Loaded %d examples."" % (len(examples),))\n\n    """"""\n    data = []\n    for ex in examples:\n        print(""scr"", type(ex.screenshot_rs_jpg))\n        print(""scrp"", [type(scr) for scr in ex.previous_screenshots_rs_jpg])\n        print(type(util.decompress_img(ex.screenshot_rs_jpg)))\n        print(type(util.compress_to_jpg(util.decompress_img(ex.screenshot_rs_jpg))))\n        data.append({\n            ""state_idx"": ex.state_idx,\n            ""screenshot_rs_jpg"": str(ex.screenshot_rs_jpg),\n            ""previous_screenshots_rs_jpg"": [str(scr) for scr in ex.previous_screenshots_rs_jpg],\n            ""previous_multiaction_vec"": ex.previous_multiaction_vec,\n            ""previous_multiaction_vecs"": ex.previous_multiaction_vecs,\n            ""multiaction_vec"": ex.multiaction_vec,\n            ""next_multiaction_vec"": ex.next_multiaction_vec,\n            ""next_multiaction_vecs"": ex.next_multiaction_vecs,\n            ""grids"": ex.grids,\n            ""attributes"": ex.attributes\n        })\n    """"""\n\n    print(""Writing to file..."")\n    with gz.open(train.ANNOTATIONS_COMPRESSED_FP, ""wb"") as f:\n        pickle.dump(examples, f, protocol=-1)\n\nif __name__ == ""__main__"":\n    main()\n'"
train_semisupervised/dataset.py,0,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport train\nfrom lib import replay_memory\nfrom lib import util\n\nimport numpy as np\nimport cPickle as pickle\nimport gzip as gz\nimport random\n\ntry:\n    xrange\nexcept NameError:\n    xrange = range\n\ndef dataset_state_idx_to_example(state_idx, memory):\n    prev = max(max(train.PREVIOUS_STATES_DISTANCES), 10)\n    states = memory.get_states_by_ids(range(state_idx-prev, state_idx+1+10))\n    states_past = states[0:prev]\n    state = states[prev]\n    states_future = states[prev+1:]\n\n    if any([s is None for s in states_past]):\n        print(""[INFO] a state past is None for idx %d"" % (state_idx,))\n    if any([s is None for s in states_future]):\n        print(""[INFO] a state future is None for idx %d"" % (state_idx,))\n    states_past = [s if s is not None else state for s in states_past]\n    states_future = [s if s is not None else state for s in states_future]\n\n    previous_multiaction_vecs = [state_past.actions_multivec for state_past in states_past]\n    previous_multiaction_vec = previous_multiaction_vecs[0]\n    previous_multiaction_vecs_avg = np.average(np.array(previous_multiaction_vecs, dtype=np.float32), 0)\n    multiaction_vec = state.actions_multivec\n    next_multiaction_vecs = [state_future.actions_multivec for state_future in states_future]\n    next_multiaction_vec = next_multiaction_vecs[0]\n    next_multiaction_vecs_avg = np.average(np.array(next_multiaction_vecs, dtype=np.float32), 0)\n\n    ex = Example(state_idx, state.screenshot_rs_jpg)\n    ex.previous_screenshots_rs_jpg = [states_past[len(states_past)-d].screenshot_rs_jpg for d in train.PREVIOUS_STATES_DISTANCES]\n    ex.previous_multiaction_vec = previous_multiaction_vec\n    ex.previous_multiaction_vecs_avg = previous_multiaction_vecs_avg\n    ex.multiaction_vec = multiaction_vec\n    ex.next_multiaction_vec = next_multiaction_vec\n    ex.next_multiaction_vecs_avg = next_multiaction_vecs_avg\n\n    return ex\n\ndef load_dataset_annotated():\n    memory = replay_memory.ReplayMemory.create_instance_supervised(val=False)\n\n    examples = dict()\n    if not train.DEBUG:\n        print(""[load_dataset] Loading grids examples..."")\n        for (grid_key, fp) in train.ANNOTATIONS_GRIDS_FPS:\n            if not os.path.isfile(fp):\n                print(""[WARNING] Could not find annotations (grids) file \'%s\'"" % (fp,))\n            else:\n                dataset = pickle.load(open(fp, ""r""))\n                for key in dataset:\n                    state_idx = dataset[key][""idx""]\n                    from_datetime = dataset[key][""from_datetime""]\n                    screenshot_rs = dataset[key][""screenshot_rs""]\n                    grid = dataset[key][grid_key]\n                    if not key in examples:\n                        examples[key] = dataset_state_idx_to_example(state_idx, memory)\n                    examples[key].add_grid(grid_key, grid)\n\n    print(""[load_dataset] Loading atts examples..."")\n    for (att_group_key, fp) in train.ANNOTATIONS_ATTS_FPS:\n        dataset = pickle.load(open(fp, ""r""))\n        for key in dataset:\n            if not os.path.isfile(fp):\n                print(""[WARNING] Could not find annotations (atts) file \'%s\'"" % (fp,))\n            else:\n                state_idx = dataset[key][""idx""]\n                from_datetime = dataset[key][""from_datetime""]\n                screenshot_rs = dataset[key][""screenshot_rs""]\n                attributes = dataset[key][att_group_key]\n                if not key in examples:\n                    examples[key] = dataset_state_idx_to_example(state_idx, memory)\n                examples[key].add_attributes(attributes)\n\n    return examples.values()\n\ndef load_dataset_annotated_compressed():\n    fp = train.ANNOTATIONS_COMPRESSED_FP\n    assert os.path.isfile(fp)\n    with gz.open(fp, ""rb"") as f:\n        examples = pickle.load(f)\n    return examples\n\ndef load_dataset_autogen(val, nb_load, not_in=None):\n    not_in = not_in if not_in is not None else set()\n    memory = replay_memory.ReplayMemory.create_instance_supervised(val=val)\n\n    print(""[load_dataset_autogen] Loading random examples..."")\n    examples = dict()\n    for i in xrange(nb_load):\n        rnd_idx = random.randint(memory.id_min+max(train.PREVIOUS_STATES_DISTANCES), memory.id_max-10)\n        key = str(rnd_idx)\n        if key not in not_in:\n            examples[key] = dataset_state_idx_to_example(rnd_idx, memory)\n\n    return examples.values()\n\nclass Example(object):\n    def __init__(self, state_idx, screenshot_rs_jpg):\n        self.state_idx = state_idx\n        self.screenshot_rs_jpg = screenshot_rs_jpg\n        self.previous_screenshots_rs_jpg = None\n        self.previous_multiaction_vec = None\n        #self.previous_multiaction_vecs = None\n        self.previous_multiaction_vecs_avg = None\n        self.multiaction_vec = None\n        self.next_multiaction_vec = None\n        #self.next_multiaction_vecs = None\n        self.next_multiaction_vecs_avg = None\n        self.grids = dict()\n        self.attributes = dict()\n\n    @property\n    def screenshot_rs(self):\n        return util.decompress_img(self.screenshot_rs_jpg)\n\n    @property\n    def previous_screenshots_rs(self):\n        return [util.decompress_img(scr) for scr in self.previous_screenshots_rs_jpg]\n\n    def add_grid(self, grid_key, grid):\n        self.grids[grid_key] = grid\n\n    def add_attributes(self, atts):\n        for att_group_name in atts:\n            self.attributes[att_group_name] = atts[att_group_name]\n\n    def get_grids_array(self):\n        h, w = self.screenshot_rs.shape[0:2]\n        lst = []\n        grids_annotated = []\n        for key in train.GRIDS_ORDER:\n            if key in self.grids:\n                lst.append(self.grids[key])\n                grids_annotated.append(True)\n            else:\n                lst.append(np.zeros((h, w), dtype=np.float32))\n                grids_annotated.append(False)\n        arr = np.array(lst, dtype=np.float32)\n        return arr.transpose((1, 2, 0)), grids_annotated\n\n    def get_attributes_array(self):\n        if not self.has_attributes():\n            return np.zeros((train.NB_ATTRIBUTE_VALUES,), dtype=np.float32), False\n        else:\n            lst = []\n            for att_group_name in train.ATTRIBUTES_ORDER:\n                vec = attribute_to_onehot(att_group_name, self.attributes[att_group_name])\n                lst.extend(vec)\n            return np.array(lst, dtype=np.float32), True\n\n    def has_attributes(self):\n        return len(self.attributes) > 0\n\ndef attribute_to_onehot(att_group_name, att_name):\n    group = train.ATTRIBUTE_GROUPS_BY_NAME[att_group_name]\n    l = len(group.attributes)\n    idx = [i for i, att in enumerate(group.attributes) if att.name == att_name]\n    #print(att_group_name, att_name, [(i, att.name) for i, att in enumerate(group.attributes)])\n    assert len(idx) == 1, len(idx)\n    vec = np.zeros((l,), dtype=np.float32)\n    vec[idx[0]] = 1\n    return vec\n'"
train_semisupervised/models.py,10,"b'from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport train\nfrom lib import actions\nfrom lib.util import to_cuda, to_variable\nfrom config import Config\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd.function import InplaceFunction\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport numpy as np\nimport imgaug as ia\nimport random\nimport math\nimport cv2\n\nclass Predictor(nn.Module):\n    def __init__(self):\n        super(Predictor, self).__init__()\n\n        def identity(_):\n            return lambda x: x\n        #bn2d = nn.BatchNorm2d\n        #bn1d = nn.BatchNorm1d\n        bn2d = nn.InstanceNorm2d\n        bn1d = nn.InstanceNorm1d\n        #bn2d = identity\n        #bn1d = identity\n        #bn2d = InstanceNormalization\n\n        self.nb_previous_images = 2\n\n        self.emb_c1_curr = nn.Conv2d(3, 128, kernel_size=7, padding=3, stride=2)\n        self.emb_c1_bn_curr = bn2d(128)\n        self.emb_c1_sd_curr = nn.Dropout2d(0.0)\n\n        self.emb_c2_curr = nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1)\n        self.emb_c2_bn_curr = bn2d(128)\n        self.emb_c2_sd_curr = nn.Dropout2d(0.0)\n\n        self.emb_c3_curr = nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1)\n        self.emb_c3_bn_curr = bn2d(256)\n        self.emb_c3_sd_curr = nn.Dropout2d(0.0)\n\n\n        self.emb_c1_prev = nn.Conv2d(self.nb_previous_images, 64, kernel_size=3, padding=1, stride=1)\n        self.emb_c1_bn_prev = bn2d(64)\n        self.emb_c1_sd_prev = nn.Dropout2d(0.0)\n\n        self.emb_c2_prev = nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1)\n        self.emb_c2_bn_prev = bn2d(128)\n        self.emb_c2_sd_prev = nn.Dropout2d(0.0)\n\n\n        self.emb_c4 = nn.Conv2d(256+128+4, 256, kernel_size=5, padding=2, stride=2)\n        self.emb_c4_bn = bn2d(256)\n        self.emb_c4_sd = nn.Dropout2d(0.0)\n\n        self.emb_c5 = nn.Conv2d(256, 256, kernel_size=5, padding=2, stride=2)\n        self.emb_c5_bn = bn2d(256)\n        self.emb_c5_sd = nn.Dropout2d(0.0)\n\n        self.emb_c6 = nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2)\n        self.emb_c6_bn = bn2d(512)\n        self.emb_c6_sd = nn.Dropout2d(0.0)\n\n        self.emb_c7 = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n        self.emb_c7_bn = bn2d(512)\n        self.emb_c7_sd = nn.Dropout2d(0.0)\n\n        self.maps_c1 = nn.Conv2d(512, 256, kernel_size=5, padding=2)\n        self.maps_c1_bn = bn2d(256)\n        self.maps_c2 = nn.Conv2d(256, 256, kernel_size=5, padding=(0, 2))\n        self.maps_c2_bn = bn2d(256)\n        self.maps_c3 = nn.Conv2d(256, 8+3+self.nb_previous_images+1+1, kernel_size=5, padding=2) # 8 grids, 3 for RGB AE, N prev for N grayscale AE, 1 flow, 1 canny\n\n        # road_type: 10\n        # intersection: 7\n        # direction: 3\n        # lane count: 5\n        # curve: 8\n        # space-front: 4\n        # space-left: 4\n        # space-right: 4\n        # offroad: 3\n        atts_size = 10 + 7 + 3 + 5 + 8 + 4 + 4 + 4 + 3\n        ma_size = 9 + 9 + 9 + 9\n        flipped_size = self.nb_previous_images\n        self.vec_fc1 = nn.Linear(512*3*5, atts_size+ma_size+flipped_size, bias=False)\n\n        for m in self.modules():\n            classname = m.__class__.__name__\n            if classname.find(\'Conv\') != -1:\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif classname.find(\'Linear\') != -1:\n                m.weight.data.normal_(0, 0.02)\n            elif classname.find(\'BatchNorm\') != -1:\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                #m.weight.data.normal_(1.0, 0.02)\n                #m.bias.data.fill_(0)\n\n    def downscale(self, img):\n        return ia.imresize_single_image(img, (train.MODEL_HEIGHT, train.MODEL_WIDTH), interpolation=""cubic"")\n\n    def downscale_prev(self, img):\n        return ia.imresize_single_image(img, (train.MODEL_PREV_HEIGHT, train.MODEL_PREV_WIDTH), interpolation=""cubic"")\n\n    def embed_state(self, previous_states, state, volatile=False, requires_grad=True, gpu=-1):\n        prev_scrs = [self.downscale_prev(s.screenshot_rs) for s in previous_states]\n        prev_scrs_y = [cv2.cvtColor(scr, cv2.COLOR_RGB2GRAY) for scr in prev_scrs]\n\n        #inputs = np.dstack([self.downscale(state.screenshot_rs)] + list(reversed(prev_scrs_y)))\n        inputs = np.array(self.downscale(state.screenshot_rs), dtype=np.float32)\n        inputs = inputs / 255.0\n        inputs = inputs.transpose((2, 0, 1))\n        inputs = inputs[np.newaxis, ...]\n        inputs = to_cuda(to_variable(inputs, volatile=volatile, requires_grad=requires_grad), gpu)\n\n        inputs_prev = np.dstack(prev_scrs_y)\n        inputs_prev = inputs_prev.astype(np.float32) / 255.0\n        inputs_prev = inputs_prev.transpose((2, 0, 1))\n        inputs_prev = inputs_prev[np.newaxis, ...]\n        inputs_prev = to_cuda(to_variable(inputs_prev, volatile=volatile, requires_grad=requires_grad), gpu)\n\n        return self.embed(inputs, inputs_prev)\n\n    def embed(self, inputs, inputs_prev):\n        return self.forward(inputs, inputs_prev, only_embed=True)\n\n    def forward(self, inputs, inputs_prev, only_embed=False):\n        def act(x):\n            return F.relu(x, inplace=True)\n        def lrelu(x, negative_slope=0.2):\n            return F.leaky_relu(x, negative_slope=negative_slope, inplace=True)\n        def up(x, f=2):\n            m = nn.UpsamplingNearest2d(scale_factor=f)\n            return m(x)\n        def maxp(x):\n            return F.max_pool2d(x, 2)\n\n        B = inputs.size(0)\n        pos_x = np.tile(np.linspace(0, 1, 40).astype(np.float32).reshape(1, 1, 40), (B, 1, 23, 1))\n        pos_x = np.concatenate([pos_x, np.fliplr(pos_x)], axis=1)\n        pos_y = np.tile(np.linspace(0, 1, 23).astype(np.float32).reshape(1, 23, 1), (B, 1, 1, 40))\n        pos_y = np.concatenate([pos_y, np.flipud(pos_y)], axis=1)\n\n        """"""\n        print(pos_x_curr[0, 0, 0, 0])\n        print(pos_x_curr[0, 0, 0, int(MODEL_WIDTH*(1/4))-1])\n        print(pos_x_curr[0, 0, 0, int(MODEL_WIDTH*(2/4))-1])\n        print(pos_x_curr[0, 0, 0, int(MODEL_WIDTH*(3/4))-1])\n        print(pos_x_curr[0, 0, 0, int(MODEL_WIDTH*(4/4))-1])\n        from scipy import misc\n        misc.imshow(\n            np.vstack([\n                np.squeeze(pos_x_curr[0].transpose((1, 2, 0))) * 255,\n                np.squeeze(pos_y_curr[0].transpose((1, 2, 0))) * 255\n            ])\n        )\n        """"""\n\n        pos_x = to_cuda(to_variable(pos_x, volatile=inputs.volatile, requires_grad=inputs.requires_grad), Config.GPU)\n        pos_y = to_cuda(to_variable(pos_y, volatile=inputs.volatile, requires_grad=inputs.requires_grad), Config.GPU)\n\n        x_emb0_curr = inputs # 3x90x160\n        x_emb1_curr = lrelu(self.emb_c1_sd_curr(self.emb_c1_bn_curr(self.emb_c1_curr(x_emb0_curr)))) # 45x80\n        x_emb2_curr = lrelu(self.emb_c2_sd_curr(self.emb_c2_bn_curr(self.emb_c2_curr(x_emb1_curr)))) # 45x80\n        x_emb2_curr = F.pad(x_emb2_curr, (0, 0, 1, 0)) # 45x80 -> 46x80\n        x_emb2_curr = maxp(x_emb2_curr) # 23x40\n        x_emb3_curr = lrelu(self.emb_c3_sd_curr(self.emb_c3_bn_curr(self.emb_c3_curr(x_emb2_curr)))) # 23x40\n\n        x_emb0_prev = inputs_prev # 2x45x80\n        x_emb1_prev = lrelu(self.emb_c1_sd_prev(self.emb_c1_bn_prev(self.emb_c1_prev(x_emb0_prev)))) # 45x80\n        x_emb1_prev = F.pad(x_emb1_prev, (0, 0, 1, 0)) # 45x80 -> 46x80\n        x_emb1_prev = maxp(x_emb1_prev) # 23x40\n        x_emb2_prev = lrelu(self.emb_c2_sd_prev(self.emb_c2_bn_prev(self.emb_c2_prev(x_emb1_prev)))) # 23x40\n\n        x_emb3 = torch.cat([x_emb3_curr, x_emb2_prev, pos_x, pos_y], 1)\n        x_emb3 = F.pad(x_emb3, (0, 0, 1, 0)) # 23x40 -> 24x40\n\n        x_emb4 = lrelu(self.emb_c4_sd(self.emb_c4_bn(self.emb_c4(x_emb3)))) # 12x20\n        x_emb5 = lrelu(self.emb_c5_sd(self.emb_c5_bn(self.emb_c5(x_emb4)))) # 6x10\n        x_emb6 = lrelu(self.emb_c6_sd(self.emb_c6_bn(self.emb_c6(x_emb5)))) # 3x5\n        x_emb7 = lrelu(self.emb_c7_sd(self.emb_c7_bn(self.emb_c7(x_emb6)))) # 3x5\n        x_emb = x_emb7\n\n        if only_embed:\n            return x_emb\n        else:\n            x_maps = x_emb # 3x5\n            x_maps = up(x_maps, 4) # 12x20\n            x_maps = lrelu(self.maps_c1_bn(self.maps_c1(x_maps))) # 12x20\n            x_maps = up(x_maps, 4) # 48x80\n            x_maps = lrelu(self.maps_c2_bn(self.maps_c2(x_maps))) # 48x80 -> 44x80\n            x_maps = F.pad(x_maps, (0, 0, 1, 0)) # 45x80\n            x_maps = up(x_maps) # 90x160\n            x_maps = F.sigmoid(self.maps_c3(x_maps)) # 90x160\n\n            ae_size = 3 + self.nb_previous_images\n            x_grids = x_maps[:, 0:8, ...]\n            x_ae = x_maps[:, 8:8+ae_size, ...]\n            x_flow = x_maps[:, 8+ae_size:8+ae_size+1, ...]\n            x_canny = x_maps[:, 8+ae_size+1:8+ae_size+2, ...]\n\n            x_vec = x_emb\n            x_vec = x_vec.view(-1, 512*3*5)\n            x_vec = F.dropout(x_vec, p=0.5, training=self.training)\n            x_vec = F.sigmoid(self.vec_fc1(x_vec))\n\n            atts_size = 10 + 7 + 3 + 5 + 8 + 4 + 4 + 4 + 3\n            ma_size = 9 + 9 + 9 + 9\n            x_atts = x_vec[:, 0:atts_size]\n            x_ma = x_vec[:, atts_size:atts_size+ma_size]\n            x_flipped = x_vec[:, atts_size+ma_size:]\n\n            return x_ae, x_grids, x_atts, x_ma, x_flow, x_canny, x_flipped, x_emb\n\n    def predict_grids(self, inputs, inputs_prev):\n        x_ae, x_grids, x_atts, x_ma, x_flow, x_canny, x_flipped, x_emb = self.forward(inputs, inputs_prev)\n        return x_grids\n\nclass PredictorWithShortcuts(nn.Module):\n    def __init__(self):\n        super(PredictorWithShortcuts, self).__init__()\n\n        def identity(_):\n            return lambda x: x\n        #bn2d = nn.BatchNorm2d\n        #bn1d = nn.BatchNorm1d\n        bn2d = nn.InstanceNorm2d\n        bn1d = nn.InstanceNorm1d\n        #bn2d = identity\n        #bn1d = identity\n        #bn2d = InstanceNormalization\n\n        self.nb_previous_images = 2\n\n        self.emb_c1_curr    = nn.Conv2d(3, 128, kernel_size=7, padding=3, stride=2)\n        self.emb_c1_bn_curr = bn2d(128)\n        self.emb_c1_sd_curr = nn.Dropout2d(0.0)\n\n        self.emb_c2_curr    = nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1)\n        self.emb_c2_bn_curr = bn2d(128)\n        self.emb_c2_sd_curr = nn.Dropout2d(0.0)\n\n        self.emb_c3_curr    = nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1)\n        self.emb_c3_bn_curr = bn2d(256)\n        self.emb_c3_sd_curr = nn.Dropout2d(0.0)\n\n\n        self.emb_c1_prev    = nn.Conv2d(self.nb_previous_images, 64, kernel_size=3, padding=1, stride=1)\n        self.emb_c1_bn_prev = bn2d(64)\n        self.emb_c1_sd_prev = nn.Dropout2d(0.0)\n\n        self.emb_c2_prev    = nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1)\n        self.emb_c2_bn_prev = bn2d(128)\n        self.emb_c2_sd_prev = nn.Dropout2d(0.0)\n\n\n        self.emb_c4    = nn.Conv2d(256+128+4, 256, kernel_size=5, padding=2, stride=2)\n        self.emb_c4_bn = bn2d(256)\n        self.emb_c4_sd = nn.Dropout2d(0.0)\n\n        self.emb_c5    = nn.Conv2d(256, 256, kernel_size=5, padding=2, stride=2)\n        self.emb_c5_bn = bn2d(256)\n        self.emb_c5_sd = nn.Dropout2d(0.0)\n\n        self.emb_c6    = nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2)\n        self.emb_c6_bn = bn2d(512)\n        self.emb_c6_sd = nn.Dropout2d(0.0)\n\n        self.emb_c7    = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n        self.emb_c7_bn = bn2d(512)\n        self.emb_c7_sd = nn.Dropout2d(0.0)\n\n        self.maps_c1 = nn.Conv2d(512+256, 256, kernel_size=5, padding=2)\n        self.maps_c1_bn = bn2d(256)\n        self.maps_c2 = nn.Conv2d(256+128, 256, kernel_size=5, padding=(0, 2))\n        self.maps_c2_bn = bn2d(256)\n        self.maps_c3 = nn.Conv2d(256+3, 8+3+self.nb_previous_images+1+1, kernel_size=5, padding=2) # 8 grids, 3 for RGB AE, N prev for N grayscale AE, 1 flow, 1 canny\n\n        # road_type: 10\n        # intersection: 7\n        # direction: 3\n        # lane count: 5\n        # curve: 8\n        # space-front: 4\n        # space-left: 4\n        # space-right: 4\n        # offroad: 3\n        atts_size = 10 + 7 + 3 + 5 + 8 + 4 + 4 + 4 + 3\n        ma_size = 9 + 9 + 9 + 9\n        flipped_size = self.nb_previous_images\n        self.vec_fc1 = nn.Linear(512*3*5, atts_size+ma_size+flipped_size, bias=False)\n\n        for m in self.modules():\n            classname = m.__class__.__name__\n            if classname.find(\'Conv\') != -1:\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif classname.find(\'Linear\') != -1:\n                m.weight.data.normal_(0, 0.02)\n            elif classname.find(\'BatchNorm\') != -1:\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                #m.weight.data.normal_(1.0, 0.02)\n                #m.bias.data.fill_(0)\n\n    def downscale(self, img):\n        return ia.imresize_single_image(img, (train.MODEL_HEIGHT, train.MODEL_WIDTH), interpolation=""cubic"")\n\n    def downscale_prev(self, img):\n        return ia.imresize_single_image(img, (train.MODEL_PREV_HEIGHT, train.MODEL_PREV_WIDTH), interpolation=""cubic"")\n\n    def embed_state(self, previous_states, state, volatile=False, requires_grad=True, gpu=-1):\n        prev_scrs = [self.downscale_prev(s.screenshot_rs) for s in previous_states]\n        prev_scrs_y = [cv2.cvtColor(scr, cv2.COLOR_RGB2GRAY) for scr in prev_scrs]\n\n        #inputs = np.dstack([self.downscale(state.screenshot_rs)] + list(reversed(prev_scrs_y)))\n        inputs = np.array(self.downscale(state.screenshot_rs), dtype=np.float32)\n        inputs = inputs / 255.0\n        inputs = inputs.transpose((2, 0, 1))\n        inputs = inputs[np.newaxis, ...]\n        inputs = to_cuda(to_variable(inputs, volatile=volatile, requires_grad=requires_grad), gpu)\n\n        inputs_prev = np.dstack(prev_scrs_y)\n        inputs_prev = inputs_prev.astype(np.float32) / 255.0\n        inputs_prev = inputs_prev.transpose((2, 0, 1))\n        inputs_prev = inputs_prev[np.newaxis, ...]\n        inputs_prev = to_cuda(to_variable(inputs_prev, volatile=volatile, requires_grad=requires_grad), gpu)\n\n        return self.embed(inputs, inputs_prev)\n\n    def embed(self, inputs, inputs_prev):\n        return self.forward(inputs, inputs_prev, only_embed=True)\n\n    def forward(self, inputs, inputs_prev, only_embed=False):\n        def act(x):\n            return F.relu(x, inplace=True)\n        def lrelu(x, negative_slope=0.2):\n            return F.leaky_relu(x, negative_slope=negative_slope, inplace=True)\n        def up(x, f=2):\n            m = nn.UpsamplingNearest2d(scale_factor=f)\n            return m(x)\n        def maxp(x):\n            return F.max_pool2d(x, 2)\n\n        B = inputs.size(0)\n        pos_x = np.tile(np.linspace(0, 1, 40).astype(np.float32).reshape(1, 1, 40), (B, 1, 23, 1))\n        pos_x = np.concatenate([pos_x, np.fliplr(pos_x)], axis=1)\n        pos_y = np.tile(np.linspace(0, 1, 23).astype(np.float32).reshape(1, 23, 1), (B, 1, 1, 40))\n        pos_y = np.concatenate([pos_y, np.flipud(pos_y)], axis=1)\n\n        pos_x = to_cuda(to_variable(pos_x, volatile=inputs.volatile, requires_grad=inputs.requires_grad), Config.GPU)\n        pos_y = to_cuda(to_variable(pos_y, volatile=inputs.volatile, requires_grad=inputs.requires_grad), Config.GPU)\n\n        x_emb0_curr = inputs # 3x90x160\n        x_emb1_curr = lrelu(self.emb_c1_sd_curr(self.emb_c1_bn_curr(self.emb_c1_curr(x_emb0_curr)))) # 45x80\n        x_emb2_curr = lrelu(self.emb_c2_sd_curr(self.emb_c2_bn_curr(self.emb_c2_curr(x_emb1_curr)))) # 45x80\n        x_emb2_curr = F.pad(x_emb2_curr, (0, 0, 1, 0)) # 45x80 -> 46x80\n        x_emb2_curr_pool = maxp(x_emb2_curr) # 23x40\n        x_emb3_curr = lrelu(self.emb_c3_sd_curr(self.emb_c3_bn_curr(self.emb_c3_curr(x_emb2_curr_pool)))) # 23x40\n\n        x_emb0_prev = inputs_prev # 2x45x80\n        x_emb1_prev = lrelu(self.emb_c1_sd_prev(self.emb_c1_bn_prev(self.emb_c1_prev(x_emb0_prev)))) # 45x80\n        x_emb1_prev = F.pad(x_emb1_prev, (0, 0, 1, 0)) # 45x80 -> 46x80\n        x_emb1_prev = maxp(x_emb1_prev) # 23x40\n        x_emb2_prev = lrelu(self.emb_c2_sd_prev(self.emb_c2_bn_prev(self.emb_c2_prev(x_emb1_prev)))) # 23x40\n\n        x_emb3 = torch.cat([x_emb3_curr, x_emb2_prev, pos_x, pos_y], 1)\n        x_emb3 = F.pad(x_emb3, (0, 0, 1, 0)) # 23x40 -> 24x40\n\n        x_emb4 = lrelu(self.emb_c4_sd(self.emb_c4_bn(self.emb_c4(x_emb3)))) # 12x20\n        x_emb5 = lrelu(self.emb_c5_sd(self.emb_c5_bn(self.emb_c5(x_emb4)))) # 6x10\n        x_emb6 = lrelu(self.emb_c6_sd(self.emb_c6_bn(self.emb_c6(x_emb5)))) # 3x5\n        x_emb7 = lrelu(self.emb_c7_sd(self.emb_c7_bn(self.emb_c7(x_emb6)))) # 3x5\n        x_emb = x_emb7\n\n        if only_embed:\n            return x_emb\n        else:\n            x_maps = x_emb # 3x5\n            x_maps = up(x_maps, 4) # 12x20\n            x_maps = lrelu(self.maps_c1_bn(self.maps_c1(\n                torch.cat([x_maps, x_emb4], 1)\n            ))) # 12x20\n            x_maps = up(x_maps, 4) # 48x80\n            x_maps = lrelu(self.maps_c2_bn(self.maps_c2(\n                torch.cat([x_maps, F.pad(x_emb2_curr, (0, 0, 1, 1))], 1)\n            ))) # 48x80 -> 44x80\n            x_maps = F.pad(x_maps, (0, 0, 1, 0)) # 45x80\n            x_maps = up(x_maps) # 90x160\n            x_maps = F.sigmoid(self.maps_c3(\n                torch.cat([x_maps, inputs], 1)\n            )) # 90x160\n\n            ae_size = 3 + self.nb_previous_images\n            x_grids = x_maps[:, 0:8, ...]\n            x_ae = x_maps[:, 8:8+ae_size, ...]\n            x_flow = x_maps[:, 8+ae_size:8+ae_size+1, ...]\n            x_canny = x_maps[:, 8+ae_size+1:8+ae_size+2, ...]\n\n            x_vec = x_emb\n            x_vec = x_vec.view(-1, 512*3*5)\n            x_vec = F.dropout(x_vec, p=0.5, training=self.training)\n            x_vec = F.sigmoid(self.vec_fc1(x_vec))\n\n            atts_size = 10 + 7 + 3 + 5 + 8 + 4 + 4 + 4 + 3\n            ma_size = 9 + 9 + 9 + 9\n            x_atts = x_vec[:, 0:atts_size]\n            x_ma = x_vec[:, atts_size:atts_size+ma_size]\n            x_flipped = x_vec[:, atts_size+ma_size:]\n\n            return x_ae, x_grids, x_atts, x_ma, x_flow, x_canny, x_flipped, x_emb\n\n    def predict_grids(self, inputs, inputs_prev):\n        x_ae, x_grids, x_atts, x_ma, x_flow, x_canny, x_flipped, x_emb = self.forward(inputs, inputs_prev)\n        return x_grids\n'"
train_semisupervised/train.py,7,"b'""""""Train a semi-supervised model.""""""\nfrom __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport models\nfrom batching import BatchLoader\nfrom visualization import generate_debug_image\nfrom dataset import (\n    load_dataset_annotated,\n    load_dataset_annotated_compressed,\n    load_dataset_autogen\n)\n\nfrom annotate.annotate_attributes import ATTRIBUTE_GROUPS\nfrom lib.util import to_variable, to_cuda, to_numpy\nfrom lib import plotting\nfrom config import Config\n\nfrom scipy import misc\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nimport time\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nnp.random.seed(42)\nrandom.seed(42)\ntorch.manual_seed(42)\n\ntry:\n    xrange\nexcept NameError:\n    xrange = range\n\n# whether to load less data\nDEBUG = False\n\n# model input height/width of the screenshot that shows the current state\nMODEL_HEIGHT = 90\nMODEL_WIDTH = 160\n\n# model input height/width of screenshots that show previous states\nMODEL_PREV_HEIGHT = 45\nMODEL_PREV_WIDTH = 80\n\n# number of batches to train for\nNB_BATCHES = 50000\n\n# batch size during train/val\nBATCH_SIZE = 32\n\n# gpu to use\nGPU = Config.GPU\n\n# save/validate/plot every N batches\nSAVE_EVERY = 100\nVAL_EVERY = 250\nPLOT_EVERY = 100\n\n# number of batches to run during each validation (loss will be averaged)\nNB_VAL_BATCHES = 16\n\n# number of examples to split of from the hand-annotated examples\nNB_VAL_SPLIT = 128\n\n# number of examples to split of from the automatically generated examples\nNB_AUTOGEN_VAL = 512\n\n# number of automatically generated examples to use during training\n# these will be regularly re-generated (filled with new examples)\nNB_AUTOGEN_TRAIN = 20000 if not DEBUG else 256\n\n# weightings of losses for autoencoder, hand-annotated grids, hand-annotated\n# attributes, actions, optical flow, canny edges, temporal flips\nLOSS_AE_WEIGHTING = 0.2\nLOSS_GRIDS_WEIGHTING = 0.3\nLOSS_ATTRIBUTES_WEIGHTING = 0.1\nLOSS_MULTIACTIONS_WEIGHTING = 0.1\nLOSS_FLOW_WEIGHTING = 0.1\nLOSS_CANNY_WEIGHTING = 0.1\nLOSS_FLIPPED_WEIGHTING = 0.1\n\n# probability to flip previous states inputs during training\nP_FLIP = 0.2\n\n# sigmas to use for canny edge images\nCANNY_SIGMAS = [1.0]\n\n# filepaths to grid annotation files\nANNOTATIONS_GRIDS_FPS = [\n    (""street_boundary_grid"", os.path.join(Config.ANNOTATIONS_DIR, ""annotations.pickle"")),\n    (""cars_grid"", os.path.join(Config.ANNOTATIONS_DIR, ""annotations_cars.pickle"")),\n    (""cars_mirrors_grid"", os.path.join(Config.ANNOTATIONS_DIR, ""annotations_cars_mirrors.pickle"")),\n    (""crashables_grid"", os.path.join(Config.ANNOTATIONS_DIR, ""annotations_crashables.pickle"")),\n    (""current_lane_grid"", os.path.join(Config.ANNOTATIONS_DIR, ""annotations_current_lane.pickle"")),\n    (""lanes_same_direction_grid"", os.path.join(Config.ANNOTATIONS_DIR, ""annotations_lanes_same_direction.pickle"")),\n    (""steering_wheel_grid"", os.path.join(Config.ANNOTATIONS_DIR, ""annotations_steering_wheel.pickle"")),\n    (""street_markings_grid"", os.path.join(Config.ANNOTATIONS_DIR, ""annotations_street_markings.pickle""))\n]\n\n# filepath to attribute annotation files\nANNOTATIONS_ATTS_FPS = [\n    (""attributes"", os.path.join(Config.ANNOTATIONS_DIR, ""annotations_attributes.pickle""))\n]\n\n# filepath to the compressed annotations (contains grids + attributes)\nANNOTATIONS_COMPRESSED_FP = os.path.join(Config.ANNOTATIONS_DIR, ""semisupervised_annotations.pickle.gz"")\n\n# whether to use the compressed annotations instead of loading them from the\n# filepaths\nUSE_COMPRESSED_ANNOTATIONS = True\n\n# order of the hand-annotated grids (was this only for visualization?)\nGRIDS_ORDER = [key for (key, fp) in ANNOTATIONS_GRIDS_FPS]\n\n# order of the hand-annotated attributes (was this only for visualization?)\nATTRIBUTES_ORDER = []\nfor att_group in ATTRIBUTE_GROUPS:\n    ATTRIBUTES_ORDER.append(att_group.name)\n\n# dictionary mapping from attribute group names (e.g. ""lane count"" to attributes)\nATTRIBUTE_GROUPS_BY_NAME = dict([(att_group.name, att_group) for att_group in ATTRIBUTE_GROUPS])\n\n# total number of attributes\nNB_ATTRIBUTE_VALUES = sum([len(att_group.attributes) for att_group in ATTRIBUTE_GROUPS])\n\n# downscaling factors (relative to input image size) for hand-annotated grids,\n# autoencoder esults, optical flow, canny edge images\n# changing this may also mean that you have to change the model outputs in\n# model.py\nGRIDS_DOWNSCALE_FACTOR = 1\nAE_DOWNSCALE_FACTOR = 1\nFLOW_DOWNSCALE_FACTOR = 1\nCANNY_DOWNSCALE_FACTOR = 1\n\n# distances (in states) for the previous state inputs\n# e.g. [1, 2] means that each example (during train/test) will contain the\n# current state (always added), the state 100ms ago and the state 200ms ago\n# (more precisely: the screenshots of these states)\nPREVIOUS_STATES_DISTANCES = [1, 2]\n\ndef main():\n    """"""Initialize/load model, dataset, optimizers, history and loss\n    plotter, augmentation sequence. Then start training loop.""""""\n\n    parser = argparse.ArgumentParser(description=""Train semisupervised model"")\n    parser.add_argument(\'--nocontinue\', default=False, action=""store_true"", help=""Whether to NOT continue the previous experiment"", required=False)\n    parser.add_argument(\'--withshortcuts\', default=False, action=""store_true"", help=""Whether to train a model with shortcuts from downscaling to upscaling layers."", required=False)\n    args = parser.parse_args()\n\n    checkpoint_fp = ""train_semisupervised_model%s.tar"" % (""_withshortcuts"" if args.withshortcuts else """",)\n    if os.path.isfile(checkpoint_fp) and not args.nocontinue:\n        checkpoint = torch.load(checkpoint_fp)\n    else:\n        checkpoint = None\n\n    # load or initialize loss history\n    if checkpoint is not None:\n        history = plotting.History.from_string(checkpoint[""history""])\n    else:\n        history = plotting.History()\n        history.add_group(""loss-ae"", [""train"", ""val""], increasing=False)\n        history.add_group(""loss-grids"", [""train"", ""val""], increasing=False)\n        history.add_group(""loss-atts"", [""train"", ""val""], increasing=False)\n        history.add_group(""loss-multiactions"", [""train"", ""val""], increasing=False)\n        history.add_group(""loss-flow"", [""train"", ""val""], increasing=False)\n        history.add_group(""loss-canny"", [""train"", ""val""], increasing=False)\n        history.add_group(""loss-flipped"", [""train"", ""val""], increasing=False)\n\n    # initialize loss plotter\n    loss_plotter = plotting.LossPlotter(\n        history.get_group_names(),\n        history.get_groups_increasing(),\n        save_to_fp=""train_semisupervised_plot%s.jpg"" % (""_withshortcuts"" if args.withshortcuts else """",)\n    )\n    loss_plotter.start_batch_idx = 100\n\n    # initialize and load model\n    predictor = models.Predictor() if not args.withshortcuts else models.PredictorWithShortcuts()\n    if checkpoint is not None:\n        predictor.load_state_dict(checkpoint[""predictor_state_dict""])\n    predictor.train()\n\n    # initialize optimizer\n    optimizer_predictor = optim.Adam(predictor.parameters())\n\n    # initialize losses\n    criterion_ae = nn.MSELoss()\n    criterion_grids = nn.BCELoss()\n    criterion_atts = nn.BCELoss()\n    criterion_multiactions = nn.BCELoss()\n    criterion_flow = nn.BCELoss()\n    criterion_canny = nn.BCELoss()\n    criterion_flipped = nn.BCELoss()\n\n    # send everything to gpu\n    if GPU >= 0:\n        predictor.cuda(GPU)\n        criterion_ae.cuda(GPU)\n        criterion_grids.cuda(GPU)\n        criterion_atts.cuda(GPU)\n        criterion_multiactions.cuda(GPU)\n        criterion_flow.cuda(GPU)\n        criterion_canny.cuda(GPU)\n        criterion_flipped.cuda(GPU)\n\n    # initialize image augmentation cascade\n    rarely = lambda aug: iaa.Sometimes(0.1, aug)\n    sometimes = lambda aug: iaa.Sometimes(0.2, aug)\n    often = lambda aug: iaa.Sometimes(0.3, aug)\n    # no hflips here, because that would mess up the optimal steering direction\n    # no grayscale here, because that doesn\'t play well with the grayscale\n    # previous images\n    # no coarse dropout, because then the model would have to magically guess\n    # things like edges or flow\n    augseq = iaa.Sequential([\n            often(iaa.Crop(percent=(0, 0.05))),\n            sometimes(iaa.GaussianBlur((0, 0.2))), # blur images with a sigma between 0 and 3.0\n            often(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5)), # add gaussian noise to images\n            often(iaa.Dropout((0.0, 0.05), per_channel=0.5)),\n            rarely(iaa.Sharpen(alpha=(0, 0.7), lightness=(0.75, 1.5))), # sharpen images\n            rarely(iaa.Emboss(alpha=(0, 0.7), strength=(0, 2.0))), # emboss images\n            rarely(iaa.Sometimes(0.5,\n                iaa.EdgeDetect(alpha=(0, 0.4)),\n                iaa.DirectedEdgeDetect(alpha=(0, 0.4), direction=(0.0, 1.0)),\n            )),\n            often(iaa.Add((-20, 20), per_channel=0.5)), # change brightness of images (by -10 to 10 of original value)\n            often(iaa.Multiply((0.8, 1.2), per_channel=0.25)), # change brightness of images (50-150% of original value)\n            often(iaa.ContrastNormalization((0.8, 1.2), per_channel=0.5)), # improve or worsen the contrast\n            sometimes(iaa.Affine(\n                scale={""x"": (0.9, 1.1), ""y"": (0.9, 1.1)},\n                translate_percent={""x"": (-0.07, 0.07), ""y"": (-0.07, 0.07)},\n                rotate=(0, 0),\n                shear=(0, 0),\n                order=[0, 1],\n                cval=(0, 255),\n                mode=ia.ALL\n            ))\n        ],\n        random_order=True # do all of the above in random order\n    )\n\n    # load datasets\n    print(""Loading dataset..."")\n    if USE_COMPRESSED_ANNOTATIONS:\n        examples = load_dataset_annotated_compressed()\n    else:\n        examples = load_dataset_annotated()\n    #examples_annotated_ids = set([ex.state_idx for ex in examples])\n    examples_annotated_ids = set()\n    examples_autogen_val = load_dataset_autogen(val=True, nb_load=NB_AUTOGEN_VAL, not_in=examples_annotated_ids)\n    examples_autogen_train = load_dataset_autogen(val=False, nb_load=NB_AUTOGEN_TRAIN, not_in=examples_annotated_ids)\n    random.shuffle(examples)\n    random.shuffle(examples_autogen_val)\n    random.shuffle(examples_autogen_train)\n    examples_val = examples[0:NB_VAL_SPLIT]\n    examples_train = examples[NB_VAL_SPLIT:]\n\n    # initialize background batch loaders\n    #memory = replay_memory.ReplayMemory.get_instance_supervised()\n    batch_loader_train = BatchLoader(examples_train, examples_autogen_train, augseq=augseq, queue_size=15, nb_workers=4, threaded=False)\n    batch_loader_val = BatchLoader(examples_val, examples_autogen_val, augseq=iaa.Noop(), queue_size=NB_VAL_BATCHES, nb_workers=1, threaded=False)\n\n    # training loop\n    print(""Training..."")\n    start_batch_idx = 0 if checkpoint is None else checkpoint[""batch_idx""] + 1\n    for batch_idx in xrange(start_batch_idx, NB_BATCHES):\n        # train on batch\n\n        # load batch data\n        time_cbatch_start = time.time()\n        (inputs, inputs_prev), (outputs_ae_gt, outputs_grids_gt_orig, outputs_atts_gt_orig, outputs_multiactions_gt, outputs_flow_gt, outputs_canny_gt, outputs_flipped_gt), (grids_annotated, atts_annotated) = batch_loader_train.get_batch()\n        inputs = to_cuda(to_variable(inputs), GPU)\n        inputs_prev = to_cuda(to_variable(inputs_prev), GPU)\n        outputs_ae_gt = to_cuda(to_variable(outputs_ae_gt, requires_grad=False), GPU)\n        outputs_multiactions_gt = to_cuda(to_variable(outputs_multiactions_gt, requires_grad=False), GPU)\n        outputs_flow_gt = to_cuda(to_variable(outputs_flow_gt, requires_grad=False), GPU)\n        outputs_canny_gt = to_cuda(to_variable(outputs_canny_gt, requires_grad=False), GPU)\n        outputs_flipped_gt = to_cuda(to_variable(outputs_flipped_gt, requires_grad=False), GPU)\n        time_cbatch_end = time.time()\n\n        # predict and compute losses\n        time_fwbw_start = time.time()\n        optimizer_predictor.zero_grad()\n        (outputs_ae_pred, outputs_grids_pred, outputs_atts_pred, outputs_multiactions_pred, outputs_flow_pred, outputs_canny_pred, outputs_flipped_pred, emb) = predictor(inputs, inputs_prev)\n        # zero-grad some outputs where annotations are not available for specific examples\n        outputs_grids_gt = remove_unannotated_grids_gt(outputs_grids_pred, outputs_grids_gt_orig, grids_annotated)\n        outputs_grids_gt = to_cuda(to_variable(outputs_grids_gt, requires_grad=False), GPU)\n        outputs_atts_gt = remove_unannotated_atts_gt(outputs_atts_pred, outputs_atts_gt_orig, atts_annotated)\n        outputs_atts_gt = to_cuda(to_variable(outputs_atts_gt, requires_grad=False), GPU)\n        loss_ae = criterion_ae(outputs_ae_pred, outputs_ae_gt)\n        loss_grids = criterion_grids(outputs_grids_pred, outputs_grids_gt)\n        loss_atts = criterion_atts(outputs_atts_pred, outputs_atts_gt)\n        loss_multiactions = criterion_multiactions(outputs_multiactions_pred, outputs_multiactions_gt)\n        loss_flow = criterion_flow(outputs_flow_pred, outputs_flow_gt)\n        loss_canny = criterion_canny(outputs_canny_pred, outputs_canny_gt)\n        loss_flipped = criterion_flipped(outputs_flipped_pred, outputs_flipped_gt)\n        losses_grad_lst = [\n            loss.data.new().resize_as_(loss.data).fill_(w) for loss, w in zip(\n                [loss_ae, loss_grids, loss_atts, loss_multiactions, loss_flow, loss_canny, loss_flipped],\n                [LOSS_AE_WEIGHTING, LOSS_GRIDS_WEIGHTING, LOSS_ATTRIBUTES_WEIGHTING, LOSS_MULTIACTIONS_WEIGHTING, LOSS_FLOW_WEIGHTING, LOSS_CANNY_WEIGHTING, LOSS_FLIPPED_WEIGHTING]\n            )\n        ]\n        torch.autograd.backward([loss_ae, loss_grids, loss_atts, loss_multiactions, loss_flow, loss_canny, loss_flipped], losses_grad_lst)\n        optimizer_predictor.step()\n        time_fwbw_end = time.time()\n\n        # add losses to history and output a message\n        loss_ae_value = to_numpy(loss_ae)[0]\n        loss_grids_value = to_numpy(loss_grids)[0]\n        loss_atts_value = to_numpy(loss_atts)[0]\n        loss_multiactions_value = to_numpy(loss_multiactions)[0]\n        loss_flow_value = to_numpy(loss_flow)[0]\n        loss_canny_value = to_numpy(loss_canny)[0]\n        loss_flipped_value = to_numpy(loss_flipped)[0]\n        history.add_value(""loss-ae"", ""train"", batch_idx, loss_ae_value)\n        history.add_value(""loss-grids"", ""train"", batch_idx, loss_grids_value)\n        history.add_value(""loss-atts"", ""train"", batch_idx, loss_atts_value)\n        history.add_value(""loss-multiactions"", ""train"", batch_idx, loss_multiactions_value)\n        history.add_value(""loss-flow"", ""train"", batch_idx, loss_flow_value)\n        history.add_value(""loss-canny"", ""train"", batch_idx, loss_canny_value)\n        history.add_value(""loss-flipped"", ""train"", batch_idx, loss_flipped_value)\n        print(""[T] Batch %05d L[ae=%.4f, grids=%.4f, atts=%.4f, multiactions=%.4f, flow=%.4f, canny=%.4f, flipped=%.4f] T[cbatch=%.04fs, fwbw=%.04fs]"" % (batch_idx, loss_ae_value, loss_grids_value, loss_atts_value, loss_multiactions_value, loss_flow_value, loss_canny_value, loss_flipped_value, time_cbatch_end - time_cbatch_start, time_fwbw_end - time_fwbw_start))\n\n        # genrate a debug image showing batch predictions and ground truths\n        if (batch_idx+1) % 20 == 0:\n            debug_img = generate_debug_image(\n                inputs, inputs_prev,\n                outputs_ae_gt, outputs_grids_gt_orig, outputs_atts_gt_orig, outputs_multiactions_gt, outputs_flow_gt, outputs_canny_gt, outputs_flipped_gt,\n                outputs_ae_pred, outputs_grids_pred, outputs_atts_pred, outputs_multiactions_pred, outputs_flow_pred, outputs_canny_pred, outputs_flipped_pred,\n                grids_annotated, atts_annotated\n            )\n            misc.imsave(\n                ""train_semisupervised_debug_img%s.jpg"" % (""_withshortcuts"" if args.withshortcuts else """",),\n                debug_img\n            )\n\n        # run N validation batches\n        # TODO merge this with training stuff above (one function for both)\n        if (batch_idx+1) % VAL_EVERY == 0:\n            predictor.eval()\n            loss_ae_total = 0\n            loss_grids_total = 0\n            loss_atts_total = 0\n            loss_multiactions_total = 0\n            loss_flow_total = 0\n            loss_canny_total = 0\n            loss_flipped_total = 0\n            for i in xrange(NB_VAL_BATCHES):\n                time_cbatch_start = time.time()\n                (inputs, inputs_prev), (outputs_ae_gt, outputs_grids_gt_orig, outputs_atts_gt_orig, outputs_multiactions_gt, outputs_flow_gt, outputs_canny_gt, outputs_flipped_gt), (grids_annotated, atts_annotated) = batch_loader_val.get_batch()\n                inputs = to_cuda(to_variable(inputs, volatile=True), GPU)\n                inputs_prev = to_cuda(to_variable(inputs_prev, volatile=True), GPU)\n                outputs_ae_gt = to_cuda(to_variable(outputs_ae_gt, volatile=True), GPU)\n                outputs_multiactions_gt = to_cuda(to_variable(outputs_multiactions_gt, volatile=True), GPU)\n                outputs_flow_gt = to_cuda(to_variable(outputs_flow_gt, volatile=True), GPU)\n                outputs_canny_gt = to_cuda(to_variable(outputs_canny_gt, volatile=True), GPU)\n                outputs_flipped_gt = to_cuda(to_variable(outputs_flipped_gt, volatile=True), GPU)\n                time_cbatch_end = time.time()\n\n                time_fwbw_start = time.time()\n                (outputs_ae_pred, outputs_grids_pred, outputs_atts_pred, outputs_multiactions_pred, outputs_flow_pred, outputs_canny_pred, outputs_flipped_pred, emb) = predictor(inputs, inputs_prev)\n                outputs_grids_gt = remove_unannotated_grids_gt(outputs_grids_pred, outputs_grids_gt_orig, grids_annotated)\n                outputs_grids_gt = to_cuda(to_variable(outputs_grids_gt, volatile=True), GPU)\n                outputs_atts_gt = remove_unannotated_atts_gt(outputs_atts_pred, outputs_atts_gt_orig, atts_annotated)\n                outputs_atts_gt = to_cuda(to_variable(outputs_atts_gt, volatile=True), GPU)\n                loss_ae = criterion_ae(outputs_ae_pred, outputs_ae_gt)\n                loss_grids = criterion_grids(outputs_grids_pred, outputs_grids_gt)\n                loss_atts = criterion_atts(outputs_atts_pred, outputs_atts_gt)\n                loss_multiactions = criterion_multiactions(outputs_multiactions_pred, outputs_multiactions_gt)\n                loss_flow = criterion_flow(outputs_flow_pred, outputs_flow_gt)\n                loss_canny = criterion_canny(outputs_canny_pred, outputs_canny_gt)\n                loss_flipped = criterion_flipped(outputs_flipped_pred, outputs_flipped_gt)\n                time_fwbw_end = time.time()\n\n                loss_ae_value = to_numpy(loss_ae)[0]\n                loss_grids_value = to_numpy(loss_grids)[0]\n                loss_atts_value = to_numpy(loss_atts)[0]\n                loss_multiactions_value = to_numpy(loss_multiactions)[0]\n                loss_flow_value = to_numpy(loss_flow)[0]\n                loss_canny_value = to_numpy(loss_canny)[0]\n                loss_flipped_value = to_numpy(loss_flipped)[0]\n                loss_ae_total += loss_ae_value\n                loss_grids_total += loss_grids_value\n                loss_atts_total += loss_atts_value\n                loss_multiactions_total += loss_multiactions_value\n                loss_flow_total += loss_flow_value\n                loss_canny_total += loss_canny_value\n                loss_flipped_total += loss_flipped_value\n                print(""[V] Batch %05d L[ae=%.4f, grids=%.4f, atts=%.4f, multiactions=%.4f, flow=%.4f, canny=%.4f, flipped=%.4f] T[cbatch=%.04fs, fwbw=%.04fs]"" % (batch_idx, loss_ae_value, loss_grids_value, loss_atts_value, loss_multiactions_value, loss_flow_value, loss_canny_value, loss_flipped_value, time_cbatch_end - time_cbatch_start, time_fwbw_end - time_fwbw_start))\n\n                if i == 0:\n                    debug_img = generate_debug_image(\n                        inputs, inputs_prev,\n                        outputs_ae_gt, outputs_grids_gt_orig, outputs_atts_gt_orig, outputs_multiactions_gt, outputs_flow_gt, outputs_canny_gt, outputs_flipped_gt,\n                        outputs_ae_pred, outputs_grids_pred, outputs_atts_pred, outputs_multiactions_pred, outputs_flow_pred, outputs_canny_pred, outputs_flipped_pred,\n                        grids_annotated, atts_annotated\n                    )\n                    misc.imsave(\n                        ""train_semisupervised_debug_img_val%s.jpg"" % (""_withshortcuts"" if args.withshortcuts else """",),\n                        debug_img\n                    )\n            history.add_value(""loss-ae"", ""val"", batch_idx, loss_ae_total / NB_VAL_BATCHES)\n            history.add_value(""loss-grids"", ""val"", batch_idx, loss_grids_total / NB_VAL_BATCHES)\n            history.add_value(""loss-atts"", ""val"", batch_idx, loss_atts_total / NB_VAL_BATCHES)\n            history.add_value(""loss-multiactions"", ""val"", batch_idx, loss_multiactions_total / NB_VAL_BATCHES)\n            history.add_value(""loss-flow"", ""val"", batch_idx, loss_flow_total / NB_VAL_BATCHES)\n            history.add_value(""loss-canny"", ""val"", batch_idx, loss_canny_total / NB_VAL_BATCHES)\n            history.add_value(""loss-flipped"", ""val"", batch_idx, loss_flipped_total / NB_VAL_BATCHES)\n            predictor.train()\n\n        # generate loss plot\n        if (batch_idx+1) % PLOT_EVERY == 0:\n            loss_plotter.plot(history)\n\n        # every N batches, save a checkpoint\n        if (batch_idx+1) % SAVE_EVERY == 0:\n            checkpoint_fp = ""train_semisupervised_model%s.tar"" % (""_withshortcuts"" if args.withshortcuts else """",)\n            torch.save({\n                ""batch_idx"": batch_idx,\n                ""history"": history.to_string(),\n                ""predictor_state_dict"": predictor.state_dict(),\n            }, checkpoint_fp)\n\n        # refresh automatically generated examples (autoencoder, canny edge stuff etc.)\n        if (batch_idx+1) % 1000 == 0:\n            print(""Refreshing autogen dataset..."")\n            batch_loader_train.join()\n            examples_autogen_train = load_dataset_autogen(val=False, nb_load=NB_AUTOGEN_TRAIN, not_in=examples_annotated_ids)\n            batch_loader_train = BatchLoader(examples_train, examples_autogen_train, augseq=augseq, queue_size=15, nb_workers=4, threaded=False)\n\ndef remove_unannotated_grids_gt(outputs_grids_pred, outputs_grids_gt, grids_annotated):\n    """"""Zero-grad grid outputs for which there is no annotation data for an\n    example.""""""\n    gt2 = np.copy(outputs_grids_gt)\n    pred = to_numpy(outputs_grids_pred)\n    for b_idx in xrange(grids_annotated.shape[0]):\n        for grid_idx in xrange(grids_annotated.shape[1]):\n            if grids_annotated[b_idx, grid_idx] == 0:\n                gt2[b_idx, grid_idx, ...] = pred[b_idx, grid_idx, ...]\n    return gt2\n\ndef remove_unannotated_atts_gt(outputs_atts_pred, outputs_atts_gt, atts_annotated):\n    """"""Zero-grad attribute outputs for which there is no annotation data for an\n    example.""""""\n    gt2 = np.copy(outputs_atts_gt)\n    pred = to_numpy(outputs_atts_pred)\n    for b_idx in xrange(atts_annotated.shape[0]):\n        if atts_annotated[b_idx, 0] == 0:\n            gt2[b_idx, ...] = pred[b_idx, ...]\n    return gt2\n\nif __name__ == ""__main__"":\n    main()\n'"
train_semisupervised/visualization.py,0,"b'from __future__ import division, print_function\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport train\nfrom annotate.annotate_attributes import ATTRIBUTE_GROUPS\nfrom lib.util import to_numpy\nfrom lib import util\n\nimport imgaug as ia\nimport numpy as np\n\ntry:\n    xrange\nexcept NameError:\n    xrange = range\n\ndef generate_debug_image(images, images_prev, \\\n    outputs_ae_gt, outputs_grids_gt, outputs_atts_gt, \\\n    outputs_multiactions_gt, outputs_flow_gt, outputs_canny_gt, \\\n    outputs_flipped_gt, \\\n    outputs_ae_pred, outputs_grids_pred, outputs_atts_pred, \\\n    outputs_multiactions_pred, outputs_flow_pred, outputs_canny_pred, \\\n    outputs_flipped_pred, \\\n    grids_annotated, atts_annotated):\n    image = to_numpy(images)[0]\n    grids_annotated = grids_annotated[0]\n    atts_annotated = atts_annotated[0]\n\n    ae_gt = to_numpy(outputs_ae_gt)[0]\n    grids_gt = to_numpy(outputs_grids_gt)[0]\n    atts_gt = to_numpy(outputs_atts_gt)[0]\n    multiactions_gt = to_numpy(outputs_multiactions_gt)[0]\n    flow_gt = to_numpy(outputs_flow_gt)[0]\n    canny_gt = to_numpy(outputs_canny_gt)[0]\n    flipped_gt = to_numpy(outputs_flipped_gt)[0]\n\n    ae_pred = to_numpy(outputs_ae_pred)[0]\n    grids_pred = to_numpy(outputs_grids_pred)[0]\n    atts_pred = to_numpy(outputs_atts_pred)[0]\n    multiactions_pred = to_numpy(outputs_multiactions_pred)[0]\n    flow_pred = to_numpy(outputs_flow_pred)[0]\n    canny_pred = to_numpy(outputs_canny_pred)[0]\n    flipped_pred = to_numpy(outputs_flipped_pred)[0]\n\n    image = (np.squeeze(image).transpose(1, 2, 0) * 255).astype(np.uint8)\n    ae_pred = (np.squeeze(ae_pred).transpose(1, 2, 0) * 255).astype(np.uint8)\n    grids_gt = (np.squeeze(grids_gt).transpose(1, 2, 0) * 255).astype(np.uint8)\n    grids_pred = (np.squeeze(grids_pred).transpose(1, 2, 0) * 255).astype(np.uint8)\n    atts_gt = np.squeeze(atts_gt)\n    atts_pred = np.squeeze(atts_pred)\n    multiactions_gt = np.squeeze(multiactions_gt)\n    multiactions_pred = np.squeeze(multiactions_pred)\n    flow_gt = (flow_gt.transpose(1, 2, 0) * 255).astype(np.uint8)\n    flow_pred = (flow_pred.transpose(1, 2, 0) * 255).astype(np.uint8)\n    canny_gt = (canny_gt.transpose(1, 2, 0) * 255).astype(np.uint8)\n    canny_pred = (canny_pred.transpose(1, 2, 0) * 255).astype(np.uint8)\n\n    #print(image.shape, grid_gt.shape, grid_pred.shape)\n\n    h, w = int(image.shape[0]*0.5), int(image.shape[1]*0.5)\n    #h, w = image.shape[0:2]\n    image_rs = ia.imresize_single_image(image, (h, w), interpolation=""cubic"")\n    grids_vis = []\n    for i in xrange(grids_gt.shape[2]):\n        grid_gt_rs = ia.imresize_single_image(grids_gt[..., i][:, :, np.newaxis], (h, w), interpolation=""cubic"")\n        grid_pred_rs = ia.imresize_single_image(grids_pred[..., i][:, :, np.newaxis], (h, w), interpolation=""cubic"")\n        grid_gt_hm = util.draw_heatmap_overlay(image_rs, np.squeeze(grid_gt_rs) / 255)\n        grid_pred_hm = util.draw_heatmap_overlay(image_rs, np.squeeze(grid_pred_rs) / 255)\n        if grids_annotated[i] == 0:\n            grid_gt_hm[::4, ::4, :] = [255, 0, 0]\n        grids_vis.append(np.hstack((grid_gt_hm, grid_pred_hm)))\n\n    """"""\n    lst = [image[0:3]] \\\n        + [image[3:6]] \\\n        + [ia.imresize_single_image(ae_pred[:, :, 0:3], (image.shape[0], image.shape[1]), interpolation=""cubic"")] \\\n        + [ia.imresize_single_image(ae_pred[:, :, 3:6], (image.shape[0], image.shape[1]), interpolation=""cubic"")] \\\n        + [ia.imresize_single_image(np.tile(flow_pred[:, :, 0][:, :, np.newaxis], (1, 1, 3)), (image.shape[0], image.shape[1]), interpolation=""cubic"")] \\\n        + [ia.imresize_single_image(np.tile(flow_pred[:, :, 1][:, :, np.newaxis], (1, 1, 3)), (image.shape[0], image.shape[1]), interpolation=""cubic"")] \\\n        + grids_vis\n    print([s.shape for s in lst])\n    """"""\n\n    def downscale(im):\n        return ia.imresize_single_image(im, (image.shape[0]//2, image.shape[1]//2), interpolation=""cubic"")\n\n    def to_rgb(im):\n        if im.ndim == 2:\n            im = im[:, :, np.newaxis]\n        return np.tile(im, (1, 1, 3))\n\n    #print(canny_gt.shape, canny_gt[...,0].shape, to_rgb(canny_gt[...,0]).shape, downscale(to_rgb(canny_gt[...,0])).shape)\n    #print(canny_pred.shape, canny_gt[...,0].shape, to_rgb(canny_pred[...,0]).shape, downscale(to_rgb(canny_pred[...,0])).shape)\n    current_image = np.vstack(\n        #[image[:, :, 0:3]]\n        [image[:, :, 0:3]]\n        + grids_vis\n        + [np.hstack([\n            downscale(ae_pred[:, :, 0:3]),\n            downscale(to_rgb(ae_pred[:, :, 3]))\n        ])]\n        + [np.hstack([\n            downscale(to_rgb(ae_pred[:, :, 4])),\n        #    downscale(to_rgb(ae_pred[:, :, 5]))\n            np.zeros_like(downscale(to_rgb(ae_pred[:, :, 4])))\n        ])]\n        + [np.hstack([\n            downscale(to_rgb(flow_gt[..., 0])),\n            downscale(to_rgb(flow_pred[..., 0]))\n        ])]\n        + [np.hstack([\n            downscale(to_rgb(canny_gt[..., 0])),\n            downscale(to_rgb(canny_pred[..., 0]))\n        ])]\n    )\n    y_grids_start = image.shape[0]\n    grid_height = grids_vis[0].shape[0]\n    for i, name in enumerate(train.GRIDS_ORDER):\n        current_image = util.draw_text(current_image, x=2, y=y_grids_start+(i+1)*grid_height-12, text=name, size=8, color=[0, 255, 0])\n\n    current_image = np.pad(current_image, ((0, 280), (0, 280), (0, 0)), mode=""constant"", constant_values=0)\n    texts = []\n    att_idx = 0\n    for i, att_group in enumerate(ATTRIBUTE_GROUPS):\n        texts.append(att_group.name_shown)\n        for j, att in enumerate(att_group.attributes):\n            if atts_annotated[0] == 0:\n                texts.append("" %s | ? | %.2f"" % (att.name, atts_pred[att_idx]))\n            else:\n                texts.append("" %s | %.2f | %.2f"" % (att.name, atts_gt[att_idx], atts_pred[att_idx]))\n            att_idx += 1\n    current_image = util.draw_text(current_image, x=current_image.shape[1]-256+1, y=1, text=""\\n"".join(texts), size=8, color=[0, 255, 0])\n\n    ma_texts = [""multiactions (prev avg, next avg, curr, next)""]\n    counter = 0\n    while counter < multiactions_gt.shape[0]:\n        ma_texts_sub = ([], [])\n        for j in xrange(9):\n            ma_texts_sub[0].append(""%.2f"" % (multiactions_gt[counter],))\n            ma_texts_sub[1].append(""%.2f"" % (multiactions_pred[counter],))\n            counter += 1\n        ma_texts.append("" "".join(ma_texts_sub[0]))\n        ma_texts.append("" "".join(ma_texts_sub[1]))\n        ma_texts.append("""")\n    current_image = util.draw_text(current_image, x=current_image.shape[1]-256+1, y=650, text=""\\n"".join(ma_texts), size=8, color=[0, 255, 0])\n\n    flipped_texts = [\n        ""flipped"",\n        "" "".join([""%.2f"" % (flipped_gt[i],) for i in xrange(flipped_gt.shape[0])]),\n        "" "".join([""%.2f"" % (flipped_pred[i],) for i in xrange(flipped_pred.shape[0])])\n    ]\n    current_image = util.draw_text(current_image, x=current_image.shape[1]-256+1, y=810, text=""\\n"".join(flipped_texts), size=8, color=[0, 255, 0])\n\n    return current_image\n'"
train_steering_wheel/__init__.py,0,b''
train_steering_wheel/models.py,5,"b""from __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd.function import InplaceFunction\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom config import Config\nimport numpy as np\nfrom lib import actions as actionslib\nfrom lib.util import to_cuda, to_variable\nimport imgaug as ia\nimport random\n\nANGLE_BIN_SIZE = 5\nGPU = 0\n\nclass SteeringWheelTrackerCNNModel(nn.Module):\n    def __init__(self):\n        super(SteeringWheelTrackerCNNModel, self).__init__()\n\n        self.c1 = nn.Conv2d(3, 32, kernel_size=7, padding=3, stride=1)\n        self.fc1 = nn.Linear(32*(32//4)*(64//4), 16)\n        self.fc2 = nn.Linear(16, 360//ANGLE_BIN_SIZE)\n\n    def forward(self, inputs, softmax=False):\n        x = inputs\n        x = F.relu(self.c1(x))\n        x = F.avg_pool2d(x, 4)\n        x = x.view(-1, 32*(32//4)*(64//4))\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        if softmax:\n            x = F.softmax(x)\n        return x\n\n    def forward_image(self, subimg, softmax=False, volatile=False, requires_grad=True, gpu=GPU):\n        subimg = np.float32([subimg/255]).transpose((0, 3, 1, 2))\n        subimg = to_cuda(to_variable(subimg, volatile=volatile, requires_grad=requires_grad), GPU)\n        return self.forward(subimg, softmax=softmax)\n"""
train_steering_wheel/train.py,6,"b'""""""Trains a CNN to detect the current steering wheel angle from images.""""""\nfrom __future__ import print_function, division\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nimport models\nfrom lib import replay_memory\nfrom lib import util\nfrom lib.util import to_variable, to_cuda, to_numpy\nfrom lib import plotting\nfrom config import Config\n\nfrom scipy import misc\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nfrom imgaug import parameters as iap\nimport time\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport multiprocessing\nimport threading\nimport argparse\nnp.random.seed(42)\nrandom.seed(42)\ntorch.manual_seed(42)\n\nif sys.version_info[0] == 2:\n    import cPickle as pickle\nelif sys.version_info[0] == 3:\n    import pickle\n    xrange = range\n\n# train for N batches\nNB_BATCHES = 50000\n\n# size of each batch\nBATCH_SIZE = 128\n\n# save/val/plot every N batches\nSAVE_EVERY = 500\nVAL_EVERY = 500\nPLOT_EVERY = 500\n\n# use N batches for validation (loss will be averaged)\nNB_VAL_BATCHES = 128\n\n# input image height/width\nMODEL_HEIGHT = 32\nMODEL_WIDTH = 64\n\n# size of each bin in degrees\nANGLE_BIN_SIZE = 5\n\ndef main():\n    """"""Function that initializes the training (e.g. models)\n    and runs the batches.""""""\n\n    parser = argparse.ArgumentParser(description=""Train steering wheel tracker"")\n    parser.add_argument(\'--nocontinue\', default=False, action=""store_true"", help=""Whether to NOT continue the previous experiment"", required=False)\n    args = parser.parse_args()\n\n    if os.path.isfile(""steering_wheel.tar"") and not args.nocontinue:\n        checkpoint = torch.load(""steering_wheel.tar"")\n    else:\n        checkpoint = None\n\n    if checkpoint is not None:\n        history = plotting.History.from_string(checkpoint[""history""])\n    else:\n        history = plotting.History()\n        history.add_group(""loss"", [""train"", ""val""], increasing=False)\n        history.add_group(""acc"", [""train"", ""val""], increasing=True)\n    loss_plotter = plotting.LossPlotter(\n        history.get_group_names(),\n        history.get_groups_increasing(),\n        save_to_fp=""train_plot.jpg""\n    )\n    loss_plotter.start_batch_idx = 100\n\n    tracker_cnn = models.SteeringWheelTrackerCNNModel()\n    tracker_cnn.train()\n\n    optimizer = optim.Adam(tracker_cnn.parameters())\n\n    criterion = nn.CrossEntropyLoss()\n    #criterion = nn.BCELoss()\n    if checkpoint is not None:\n        tracker_cnn.load_state_dict(checkpoint[""tracker_cnn_state_dict""])\n\n    if Config.GPU >= 0:\n        tracker_cnn.cuda(Config.GPU)\n        criterion.cuda(Config.GPU)\n\n    # initialize image augmentation cascade\n    rarely = lambda aug: iaa.Sometimes(0.1, aug)\n    sometimes = lambda aug: iaa.Sometimes(0.2, aug)\n    often = lambda aug: iaa.Sometimes(0.4, aug)\n    augseq = iaa.Sequential([\n            sometimes(iaa.Crop(percent=(0, 0.025))),\n            rarely(iaa.GaussianBlur((0, 1.0))), # blur images with a sigma between 0 and 3.0\n            rarely(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.02*255), per_channel=0.5)), # add gaussian noise to images\n            often(iaa.Dropout(\n                iap.FromLowerResolution(\n                    other_param=iap.Binomial(1 - 0.2),\n                    size_px=(2, 16)\n                ),\n                per_channel=0.2\n            )),\n            often(iaa.Add((-20, 20), per_channel=0.5)), # change brightness of images (by -10 to 10 of original value)\n            often(iaa.Multiply((0.8, 1.2), per_channel=0.25)), # change brightness of images (50-150% of original value)\n            often(iaa.ContrastNormalization((0.8, 1.2), per_channel=0.5)), # improve or worsen the contrast\n            often(iaa.Affine(\n                scale={""x"": (0.8, 1.3), ""y"": (0.8, 1.3)},\n                translate_percent={""x"": (-0.2, 0.2), ""y"": (-0.2, 0.2)},\n                rotate=(-0, 0),\n                shear=(-0, 0),\n                order=[0, 1],\n                cval=(0, 255),\n                mode=[""constant"", ""edge""]\n            )),\n            rarely(iaa.Grayscale(alpha=(0.0, 1.0)))\n        ],\n        random_order=True # do all of the above in random order\n    )\n\n    #memory = replay_memory.ReplayMemory.get_instance_supervised()\n    batch_loader_train = BatchLoader(val=False, augseq=augseq, queue_size=15, nb_workers=4)\n    batch_loader_val = BatchLoader(val=True, augseq=iaa.Noop(), queue_size=NB_VAL_BATCHES, nb_workers=2)\n\n    start_batch_idx = 0 if checkpoint is None else checkpoint[""batch_idx""] + 1\n    for batch_idx in xrange(start_batch_idx, NB_BATCHES):\n        run_batch(batch_idx, False, batch_loader_train, tracker_cnn, criterion, optimizer, history, (batch_idx % 20) == 0)\n\n        if (batch_idx+1) % VAL_EVERY == 0:\n            for i in xrange(NB_VAL_BATCHES):\n                run_batch(batch_idx, True, batch_loader_val, tracker_cnn, criterion, optimizer, history, i == 0)\n\n        if (batch_idx+1) % PLOT_EVERY == 0:\n            loss_plotter.plot(history)\n\n        # every N batches, save a checkpoint\n        if (batch_idx+1) % SAVE_EVERY == 0:\n            torch.save({\n                ""batch_idx"": batch_idx,\n                ""history"": history.to_string(),\n                ""tracker_cnn_state_dict"": tracker_cnn.state_dict()\n            }, ""steering_wheel.tar"")\n\ndef run_batch(batch_idx, val, batch_loader, tracker_cnn, criterion, optimizer, history, save_debug_image):\n    """"""Train or validate on a single batch.""""""\n    train = not val\n    time_cbatch_start = time.time()\n    inputs, outputs_gt = batch_loader.get_batch()\n    if Config.GPU >= 0:\n        inputs = to_cuda(to_variable(inputs, volatile=val), Config.GPU)\n        outputs_gt_bins = to_cuda(to_variable(np.argmax(outputs_gt, axis=1), volatile=val, requires_grad=False), Config.GPU)\n        outputs_gt = to_cuda(to_variable(outputs_gt, volatile=val, requires_grad=False), Config.GPU)\n    time_cbatch_end = time.time()\n\n    time_fwbw_start = time.time()\n    if train:\n        optimizer.zero_grad()\n    outputs_pred = tracker_cnn(inputs)\n    outputs_pred_sm = F.softmax(outputs_pred)\n    loss = criterion(outputs_pred, outputs_gt_bins)\n    if train:\n        loss.backward()\n        optimizer.step()\n    time_fwbw_end = time.time()\n\n    loss = loss.data.cpu().numpy()[0]\n    outputs_pred_np = to_numpy(outputs_pred_sm)\n    outputs_gt_np = to_numpy(outputs_gt)\n    acc = np.sum(np.equal(np.argmax(outputs_pred_np, axis=1), np.argmax(outputs_gt_np, axis=1))) / BATCH_SIZE\n    history.add_value(""loss"", ""train"" if train else ""val"", batch_idx, loss, average=val)\n    history.add_value(""acc"", ""train"" if train else ""val"", batch_idx, acc, average=val)\n    print(""[%s] Batch %05d | loss %.8f | acc %.2f | cbatch %.04fs | fwbw %.04fs"" % (""T"" if train else ""V"", batch_idx, loss, acc, time_cbatch_end - time_cbatch_start, time_fwbw_end - time_fwbw_start))\n\n    if save_debug_image:\n        debug_img = generate_debug_image(inputs, outputs_gt, outputs_pred_sm)\n        misc.imsave(""debug_img_%s.jpg"" % (""train"" if train else ""val""), debug_img)\n\ndef generate_debug_image(inputs, outputs_gt, outputs_pred):\n    """"""Draw an image with current ground truth and predictions for debug purposes.""""""\n    current_image = inputs.data[0].cpu().numpy()\n    current_image = np.clip(current_image * 255, 0, 255).astype(np.uint8).transpose((1, 2, 0))\n    current_image = ia.imresize_single_image(current_image, (32*4, 64*4))\n    h, w = current_image.shape[0:2]\n    outputs_gt = to_numpy(outputs_gt)[0]\n    outputs_pred = to_numpy(outputs_pred)[0]\n\n    binwidth = 6\n    outputs_grid = np.zeros((20+2, outputs_gt.shape[0]*binwidth, 3), dtype=np.uint8)\n    for angle_bin_idx in xrange(outputs_gt.shape[0]):\n        val = outputs_pred[angle_bin_idx]\n        x_start = angle_bin_idx*binwidth\n        x_end = (angle_bin_idx+1)*binwidth\n        fill_start = 1\n        fill_end = 1 + int(20*val)\n        #print(angle_bin_idx, x_start, x_end, fill_start, fill_end, outputs_grid.shape, outputs_grid[fill_start:fill_end, x_start+1:x_end].shape)\n        if fill_start < fill_end:\n            outputs_grid[fill_start:fill_end, x_start+1:x_end] = [255, 255, 255]\n\n        bordercol = [128, 128, 128] if outputs_gt[angle_bin_idx] < 1 else [0, 0, 255]\n        outputs_grid[0:22, x_start:x_start+1] = bordercol\n        outputs_grid[0:22, x_end:x_end+1] = bordercol\n        outputs_grid[0, x_start:x_end+1] = bordercol\n        outputs_grid[21, x_start:x_end+1] = bordercol\n\n    outputs_grid = outputs_grid[::-1, :, :]\n\n    bin_gt = np.argmax(outputs_gt)\n    bin_pred = np.argmax(outputs_pred)\n    angles = [(binidx*ANGLE_BIN_SIZE) - 180 for binidx in [bin_gt, bin_pred]]\n\n    #print(outputs_grid.shape)\n    current_image = np.pad(current_image, ((0, 128), (0, 400), (0, 0)), mode=""constant"")\n    current_image[h+4:h+4+22, 4:4+outputs_grid.shape[1], :] = outputs_grid\n    current_image = util.draw_text(current_image, x=4, y=h+4+22+4, text=""GT: %03.2fdeg\\nPR: %03.2fdeg"" % (angles[0], angles[1]), size=10)\n\n    return current_image\n\ndef extract_steering_wheel_image(screenshot_rs):\n    """"""Extract the part of a screenshot (resized to 180x320 HxW) that usually\n    contains the steering wheel.""""""\n    h, w = screenshot_rs.shape[0:2]\n    x1 = int(w * (470/1280))\n    x2 = int(w * (840/1280))\n    y1 = int(h * (500/720))\n    y2 = int(h * (720/720))\n    return screenshot_rs[y1:y2+1, x1:x2+1, :]\n\ndef downscale_image(steering_wheel_image):\n    """"""Downscale an image to the model\'s input sizes (height, width).""""""\n    return ia.imresize_single_image(\n        steering_wheel_image,\n        (MODEL_HEIGHT, MODEL_WIDTH),\n        interpolation=""linear""\n    )\n\ndef load_random_state(memory, depth=0):\n    """"""Load a single random state from the replay memory which has a steering\n    wheel position attached (estimated via classical means).""""""\n    rndidx = random.randint(memory.id_min, memory.id_max)\n    state = memory.get_state_by_id(rndidx)\n    if state.steering_wheel_classical is None:\n        if depth+1 >= 200:\n            raise Exception(""Maximum depth reached in load_random_state(), \\\n                too many states with None in column steering_wheel_classical. \\\n                Use scripts/add_steering_wheel.py to recalculate missing values."")\n        return load_random_state(memory, depth=depth+1)\n    else:\n        return state\n\ndef load_random_batch(memory, augseq, batch_size):\n    """"""Load a random batch from the replay memory for training.\n    augseq contains the image augmentation sequence to use.""""""\n    inputs = np.zeros((batch_size, MODEL_HEIGHT, MODEL_WIDTH, 3), dtype=np.uint8)\n    outputs = np.zeros((batch_size, 360//ANGLE_BIN_SIZE), dtype=np.float32)\n\n    for b_idx in xrange(batch_size):\n        state = load_random_state(memory)\n        subimg = extract_steering_wheel_image(state.screenshot_rs)\n        subimg = augseq.augment_image(subimg)\n        subimg = downscale_image(subimg)\n        inputs[b_idx] = subimg\n        deg = state.steering_wheel_classical % 360\n        if -360 <= deg < -180:\n            deg = 360 - deg\n        elif -180 <= deg < 0:\n            pass\n        elif 0 <= deg < 180:\n            pass\n        elif 180 <= deg < 360:\n            deg = -360 + deg\n        deg = 180 + deg\n        bin_idx = int(deg / ANGLE_BIN_SIZE)\n        outputs[b_idx, bin_idx] = 1\n\n    inputs = (inputs / 255).astype(np.float32).transpose((0, 3, 1, 2))\n\n    return inputs, outputs\n\nclass BatchLoader(object):\n    """"""Class to load batches in multiple background processes.""""""\n    def __init__(self, val, queue_size, augseq, nb_workers, threaded=False):\n        self.queue = multiprocessing.Queue(queue_size)\n        self.workers = []\n        for i in range(nb_workers):\n            seed = random.randint(0, 10**6)\n            augseq_worker = augseq.deepcopy()\n            if threaded:\n                worker = threading.Thread(target=self._load_batches, args=(val, self.queue, augseq_worker, None))\n            else:\n                worker = multiprocessing.Process(target=self._load_batches, args=(val, self.queue, augseq_worker, seed))\n            worker.daemon = True\n            worker.start()\n            self.workers.append(worker)\n\n    def get_batch(self):\n        return pickle.loads(self.queue.get())\n\n    def _load_batches(self, val, queue, augseq_worker, seed):\n        if seed is None:\n            random.seed(seed)\n            np.random.seed(seed)\n            augseq_worker.reseed(seed)\n            ia.seed(seed)\n        memory = replay_memory.ReplayMemory.create_instance_reinforced(val=val)\n\n        while True:\n            batch = load_random_batch(memory, augseq_worker, BATCH_SIZE)\n            queue.put(pickle.dumps(batch, protocol=-1))\n\nif __name__ == ""__main__"":\n    main()\n'"
