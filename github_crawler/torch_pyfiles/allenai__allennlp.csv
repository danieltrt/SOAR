file_path,api_count,code
setup.py,0,"b'from setuptools import find_packages, setup\n\n# PEP0440 compatible formatted version, see:\n# https://www.python.org/dev/peps/pep-0440/\n#\n# release markers:\n#   X.Y\n#   X.Y.Z   # For bugfix releases\n#\n# pre-release markers:\n#   X.YaN   # Alpha release\n#   X.YbN   # Beta release\n#   X.YrcN  # Release Candidate\n#   X.Y     # Final release\n\n# version.py defines the VERSION and VERSION_SHORT variables.\n# We use exec here so we don\'t import allennlp whilst setting up.\nVERSION = {}  # type: ignore\nwith open(""allennlp/version.py"", ""r"") as version_file:\n    exec(version_file.read(), VERSION)\n\nsetup(\n    name=""allennlp"",\n    version=VERSION[""VERSION""],\n    description=""An open-source NLP research library, built on PyTorch."",\n    long_description=open(""README.md"").read(),\n    long_description_content_type=""text/markdown"",\n    classifiers=[\n        ""Intended Audience :: Science/Research"",\n        ""Development Status :: 3 - Alpha"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n    keywords=""allennlp NLP deep learning machine reading"",\n    url=""https://github.com/allenai/allennlp"",\n    author=""Allen Institute for Artificial Intelligence"",\n    author_email=""allennlp@allenai.org"",\n    license=""Apache"",\n    packages=find_packages(\n        exclude=[\n            ""*.tests"",\n            ""*.tests.*"",\n            ""tests.*"",\n            ""tests"",\n            ""test_fixtures"",\n            ""test_fixtures.*"",\n            ""benchmarks"",\n            ""benchmarks.*"",\n        ]\n    ),\n    install_requires=[\n        ""torch>=1.5.0,<1.6.0"",\n        ""jsonnet>=0.10.0 ; sys.platform != \'win32\'"",\n        ""overrides==3.0.0"",\n        ""nltk"",\n        ""spacy>=2.1.0,<2.3"",\n        ""numpy"",\n        ""tensorboardX>=1.2"",\n        ""boto3"",\n        ""requests>=2.18"",\n        ""tqdm>=4.19"",\n        ""h5py"",\n        ""scikit-learn"",\n        ""scipy"",\n        ""pytest"",\n        ""transformers>=2.9,<2.12"",\n        ""jsonpickle"",\n        ""dataclasses;python_version<\'3.7\'"",\n        ""filelock>=3.0,<3.1"",\n    ],\n    entry_points={""console_scripts"": [""allennlp=allennlp.__main__:run""]},\n    include_package_data=True,\n    python_requires="">=3.6.1"",\n    zip_safe=False,\n)\n'"
allennlp/__init__.py,0,"b'# Make sure that allennlp is running on Python 3.6.1 or later\n# (to avoid running into this bug: https://bugs.python.org/issue29246)\nimport sys\n\nif sys.version_info < (3, 6, 1):\n    raise RuntimeError(""AllenNLP requires Python 3.6.1 or later"")\n\n# We get a lot of these spurious warnings,\n# see https://github.com/ContinuumIO/anaconda-issues/issues/6678\nimport warnings  # noqa\n\nwarnings.filterwarnings(""ignore"", message=""numpy.dtype size changed"")\nwarnings.filterwarnings(""ignore"", message=""numpy.ufunc size changed"")\n\ntry:\n    # On some systems this prevents the dreaded\n    # ImportError: dlopen: cannot load any more object with static TLS\n    import spacy, torch, numpy  # noqa\n\nexcept ModuleNotFoundError:\n    print(\n        ""Using AllenNLP requires the python packages Spacy, ""\n        ""Pytorch and Numpy to be installed. Please see ""\n        ""https://github.com/allenai/allennlp for installation instructions.""\n    )\n    raise\n\nfrom allennlp.version import VERSION as __version__  # noqa\n'"
allennlp/__main__.py,0,"b'#!/usr/bin/env python\nimport logging\nimport os\nimport sys\n\nif os.environ.get(""ALLENNLP_DEBUG""):\n    LEVEL = logging.DEBUG\nelse:\n    level_name = os.environ.get(""ALLENNLP_LOG_LEVEL"")\n    LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n\nsys.path.insert(0, os.path.dirname(os.path.abspath(os.path.join(__file__, os.pardir))))\nlogging.basicConfig(format=""%(asctime)s - %(levelname)s - %(name)s - %(message)s"", level=LEVEL)\n\nfrom allennlp.commands import main  # noqa\n\n\ndef run():\n    main(prog=""allennlp"")\n\n\nif __name__ == ""__main__"":\n    run()\n'"
allennlp/version.py,0,"b'import os\n\n_MAJOR = ""1""\n_MINOR = ""0""\n# On master and in a nightly release the patch should be one ahead of the last\n# released build.\n_PATCH = ""0rc6""\n# This is mainly for nightly builds which have the suffix "".dev$DATE"". See\n# https://semver.org/#is-v123-a-semantic-version for the semantics.\n_SUFFIX = os.environ.get(""ALLENNLP_VERSION_SUFFIX"", """")\n\nVERSION_SHORT = ""{0}.{1}"".format(_MAJOR, _MINOR)\nVERSION = ""{0}.{1}.{2}{3}"".format(_MAJOR, _MINOR, _PATCH, _SUFFIX)\n'"
benchmarks/__init__.py,0,b''
scripts/build_docs_config.py,0,"b'#!/usr/bin/env python\n\n""""""\nThis script is used to populate the table of contents for the API in the mkdocs config file.\n""""""\n\nimport argparse\nfrom pathlib import Path\nfrom typing import Any, List\n\nfrom ruamel.yaml import YAML\n\nfrom allennlp.version import VERSION\n\n\nAPI_TOC_KEY = ""API""\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""target_yaml"", help=""Path to the target mkdocs config file."")\n    parser.add_argument(""source_yaml"", help=""Path to the mkdocs skeleton config file."")\n    parser.add_argument(""docs_root"", help=""The root of the markdown docs folder."")\n    parser.add_argument(\n        ""api_docs_path"", help=""The root of the API docs within the markdown docs root folder.""\n    )\n    parser.add_argument(""--docs-version"", type=str, default=f""v{VERSION}"")\n    return parser.parse_args()\n\n\ndef build_api_toc(source_path: Path, docs_root: Path):\n    nav_entries: List[Any] = []\n\n    for child in source_path.iterdir():\n        if child.is_dir():\n            nav_subsection = build_api_toc(child, docs_root)\n        elif child.suffix == "".md"":\n            nav_subsection = str(child.relative_to(docs_root))\n        nav_entries.append({child.stem: nav_subsection})\n\n    nav_entries.sort(key=lambda x: list(x)[0], reverse=False)\n    return nav_entries\n\n\ndef main():\n    yaml = YAML()\n    opts = parse_args()\n\n    source_yaml = yaml.load(Path(opts.source_yaml))\n\n    nav_entries = build_api_toc(Path(opts.api_docs_path), Path(opts.docs_root))\n\n    # Add version to name.\n    source_yaml[""site_name""] = f""AllenNLP {opts.docs_version}""\n\n    # Find the yaml sub-object corresponding to the API table of contents.\n    site_nav = source_yaml[""nav""]\n    for nav_obj in site_nav:\n        if API_TOC_KEY in nav_obj:\n            break\n    nav_obj[API_TOC_KEY] = nav_entries\n\n    with open(opts.target_yaml, ""w"") as f:\n        yaml.dump(source_yaml, f)\n\n    print(f""{opts.target_yaml} created"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/check_links.py,0,"b'#!/usr/bin/env python\n# encoding: UTF-8\n\n""""""\nGoes through all the inline-links in markdown files and reports the breakages.\n""""""\n\nimport re\nimport sys\nimport pathlib\nimport os\nfrom multiprocessing.dummy import Pool\nfrom typing import Tuple, NamedTuple, Optional\n\nimport requests\n\n\nOK_STATUS_CODES = (\n    200,\n    401,  # the resource exists but may require some sort of login.\n    403,  # ^ same\n    405,  # HEAD method not allowed.\n    406,  # the resource exists, but our default \'Accept-\' header may not match what the server can provide.\n)\n\nTHREADS = 10\n\nhttp_session = requests.Session()\nfor resource_prefix in (""http://"", ""https://""):\n    http_session.mount(\n        resource_prefix,\n        requests.adapters.HTTPAdapter(max_retries=5, pool_connections=20, pool_maxsize=THREADS),\n    )\n\n\nclass MatchTuple(NamedTuple):\n    source: str\n    name: str\n    link: str\n\n\ndef url_ok(match_tuple: MatchTuple) -> Tuple[bool, str]:\n    """"""Check if a URL is reachable.""""""\n    try:\n        result = http_session.head(match_tuple.link, timeout=5, allow_redirects=True)\n        return (\n            result.ok or result.status_code in OK_STATUS_CODES,\n            f""status code = {result.status_code}"",\n        )\n    except (requests.ConnectionError, requests.Timeout):\n        return False, ""connection error""\n\n\ndef path_ok(match_tuple: MatchTuple) -> bool:\n    """"""Check if a file in this repository exists.""""""\n    relative_path = match_tuple.link.split(""#"")[0]\n    full_path = os.path.join(os.path.dirname(str(match_tuple.source)), relative_path)\n    return os.path.exists(full_path)\n\n\ndef link_ok(match_tuple: MatchTuple) -> Tuple[MatchTuple, bool, Optional[str]]:\n    reason: Optional[str] = None\n    if match_tuple.link.startswith(""http""):\n        result_ok, reason = url_ok(match_tuple)\n    else:\n        result_ok = path_ok(match_tuple)\n    print(f""  {\'\xe2\x9c\x93\' if result_ok else \'\xe2\x9c\x97\'} {match_tuple.link}"")\n    return match_tuple, result_ok, reason\n\n\ndef main():\n    print(""Finding all markdown files in the current directory..."")\n\n    project_root = (pathlib.Path(__file__).parent / "".."").resolve()\n    markdown_files = project_root.glob(""**/*.md"")\n\n    all_matches = set()\n    url_regex = re.compile(r""\\[([^!][^\\]]+)\\]\\(([^)(]+)\\)"")\n    for markdown_file in markdown_files:\n        with open(markdown_file) as handle:\n            for line in handle.readlines():\n                matches = url_regex.findall(line)\n                for name, link in matches:\n                    if ""localhost"" not in link:\n                        all_matches.add(MatchTuple(source=str(markdown_file), name=name, link=link))\n\n    print(f""  {len(all_matches)} markdown files found"")\n    print(""Checking to make sure we can retrieve each link..."")\n\n    with Pool(processes=THREADS) as pool:\n        results = pool.map(link_ok, [match for match in list(all_matches)])\n    unreachable_results = [\n        (match_tuple, reason) for match_tuple, success, reason in results if not success\n    ]\n\n    if unreachable_results:\n        print(f""Unreachable links ({len(unreachable_results)}):"")\n        for match_tuple, reason in unreachable_results:\n            print(""  > Source: "" + match_tuple.source)\n            print(""    Name: "" + match_tuple.name)\n            print(""    Link: "" + match_tuple.link)\n            if reason is not None:\n                print(""    Reason: "" + reason)\n        sys.exit(1)\n    print(""No Unreachable link found."")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/get_version.py,0,"b'#!/usr/bin/env python\n\nimport argparse\nfrom typing import Dict\n\nimport requests\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""version_type"", choices=[""stable"", ""latest"", ""current""])\n    return parser.parse_args()\n\n\ndef get_current_version() -> str:\n    VERSION: Dict[str, str] = {}\n    with open(""allennlp/version.py"", ""r"") as version_file:\n        exec(version_file.read(), VERSION)\n    return ""v"" + VERSION[""VERSION""]\n\n\ndef get_latest_version() -> str:\n    resp = requests.get(""https://api.github.com/repos/allenai/allennlp/tags"")\n    return resp.json()[0][""name""]\n\n\ndef get_stable_version() -> str:\n    resp = requests.get(""https://api.github.com/repos/allenai/allennlp/releases/latest"")\n    return resp.json()[""tag_name""]\n\n\ndef main() -> None:\n    opts = parse_args()\n    if opts.version_type == ""stable"":\n        print(get_stable_version())\n    elif opts.version_type == ""latest"":\n        print(get_latest_version())\n    elif opts.version_type == ""current"":\n        print(get_current_version())\n    else:\n        raise NotImplementedError\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/py2md.py,0,"b'#!/usr/bin/env python\n\n""""""\nTurn docstrings from a single module into a markdown file.\n\nWe do this with PydocMarkdown, using custom processors and renderers defined here.\n""""""\n\nimport argparse\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport logging\nfrom multiprocessing import Pool, cpu_count\nimport os\nfrom pathlib import Path\nimport re\nimport sys\nfrom typing import Optional, Tuple, List\n\nfrom nr.databind.core import Struct\nfrom nr.interface import implements, override\nfrom pydoc_markdown import PydocMarkdown\nfrom pydoc_markdown.contrib.loaders.python import PythonLoader\nfrom pydoc_markdown.contrib.renderers.markdown import MarkdownRenderer\nfrom pydoc_markdown.interfaces import Processor, Renderer\nfrom pydoc_markdown.reflection import Argument, Module, Function, Class\n\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(""py2md"")\n\n\nclass DocstringError(Exception):\n    pass\n\n\ndef emphasize(s: str) -> str:\n    # Need to escape underscores.\n    s = s.replace(""_"", ""\\\\_"")\n    return f""__{s}__""\n\n\nclass Section(Enum):\n    ARGUMENTS = ""ARGUMENTS""\n    PARAMETERS = ""PARAMETERS""\n    ATTRIBUTES = ""ATTRIBUTES""\n    MEMBERS = ""MEMBERS""\n    RETURNS = ""RETURNS""\n    RAISES = ""RAISES""\n    EXAMPLES = ""EXAMPLES""\n    OTHER = ""OTHER""\n\n    @classmethod\n    def from_str(cls, section: str) -> ""Section"":\n        section = section.upper()\n        for member in cls:\n            if section == member.value:\n                return member\n        return cls.OTHER\n\n\nREQUIRED_PARAM_RE = re.compile(r""^`([^`]+)`(, required\\.?)?$"")\n\nOPTIONAL_PARAM_RE = re.compile(\n    r""^`([^`]+)`,?\\s+(optional,?\\s)?\\(\\s?(optional,\\s)?default\\s?=\\s?`([^`]+)`\\s?\\)\\.?$""\n)\n\nOPTIONAL_PARAM_NO_DEFAULT_RE = re.compile(r""^`([^`]+)`,?\\s+optional\\.?$"")\n\n\n@dataclass\nclass Param:\n    ident: str\n    ty: Optional[str] = None\n    required: bool = False\n    default: Optional[str] = None\n\n    @classmethod\n    def from_line(cls, line: str) -> Optional[""Param""]:\n        if "":"" not in line:\n            return None\n\n        ident, description = line.split("":"", 1)\n        ident = ident.strip()\n        description = description.strip()\n\n        if "" "" in ident:\n            return None\n\n        maybe_match = REQUIRED_PARAM_RE.match(description)\n        if maybe_match:\n            ty = maybe_match.group(1)\n            return cls(ident=ident, ty=ty, required=True)\n\n        maybe_match = OPTIONAL_PARAM_RE.match(description)\n        if maybe_match:\n            ty = maybe_match.group(1)\n            default = maybe_match.group(4)\n            return cls(ident=ident, ty=ty, required=False, default=default)\n\n        maybe_match = OPTIONAL_PARAM_NO_DEFAULT_RE.match(description)\n        if maybe_match:\n            ty = maybe_match.group(1)\n            return cls(ident=ident, ty=ty, required=False)\n\n        raise DocstringError(\n            f""Invalid parameter / attribute description: \'{line}\'\\n""\n            ""Make sure types are enclosed in backticks.\\n""\n            ""Required parameters should be documented like: \'{ident} : `{type}`\'\\n""\n            ""Optional parameters should be documented like: \'{ident} : `{type}`, optional (default = `{expr}`)\'\\n""\n        )\n\n    def to_line(self) -> str:\n        line: str = f""- {emphasize(self.ident)} :""\n        if self.ty:\n            line += f"" `{self.ty}`""\n            if not self.required:\n                line += "", optional""\n                if self.default:\n                    line += f"" (default = `{self.default}`)""\n        line += "" <br>""\n        return line\n\n\n# For now we handle attributes / members in the same way as parameters / arguments.\nAttrib = Param\n\n\n@dataclass\nclass RetVal:\n    description: Optional[str] = None\n    ident: Optional[str] = None\n    ty: Optional[str] = None\n\n    @classmethod\n    def from_line(cls, line: str) -> ""RetVal"":\n        if "": "" not in line:\n            return cls(description=line)\n        ident, ty = line.split("":"", 1)\n        ident = ident.strip()\n        ty = ty.strip()\n        if ty and not ty.startswith(""`""):\n            raise DocstringError(f""Type should be enclosed in backticks: \'{line}\'"")\n        return cls(ident=ident, ty=ty)\n\n    def to_line(self) -> str:\n        if self.description:\n            line = f""- {self.description} <br>""\n        elif self.ident:\n            line = f""- {emphasize(self.ident)}""\n            if self.ty:\n                line += f"" : {self.ty} <br>""\n            else:\n                line += "" <br>""\n        else:\n            raise DocstringError(""RetVal must have either description or ident"")\n        return line\n\n\n@dataclass\nclass ProcessorState:\n    parameters: ""OrderedDict[str, Param]""\n    current_section: Optional[Section] = None\n    codeblock_opened: bool = False\n    consecutive_blank_line_count: int = 0\n\n\n@implements(Processor)\nclass AllenNlpDocstringProcessor(Struct):\n    """"""\n    Use to turn our docstrings into Markdown.\n    """"""\n\n    CROSS_REF_RE = re.compile(""(:(class|func|mod):`~?([a-zA-Z0-9_.]+)`)"")\n\n    @override\n    def process(self, graph, resolver):\n        graph.visit(self.process_node)\n\n    def process_node(self, node):\n        if not getattr(node, ""docstring"", None):\n            return\n\n        lines: List[str] = []\n        state: ProcessorState = ProcessorState(parameters=OrderedDict())\n\n        for line in node.docstring.split(""\\n""):\n            # Check if we\'re starting or ending a codeblock.\n            if line.startswith(""```""):\n                state.codeblock_opened = not state.codeblock_opened\n\n            if not state.codeblock_opened:\n                # If we\'re not in a codeblock, we\'ll do some pre-processing.\n                if not line.strip():\n                    state.consecutive_blank_line_count += 1\n                    if state.consecutive_blank_line_count >= 2:\n                        state.current_section = None\n                else:\n                    state.consecutive_blank_line_count = 0\n                line = self._preprocess_line(line, state)\n\n            lines.append(line)\n\n        # Now set the docstring to our preprocessed version of it.\n        node.docstring = ""\\n"".join(lines)\n\n    def _preprocess_line(self, line, state: ProcessorState) -> str:\n        match = re.match(r""#+ (.*)$"", line)\n        if match:\n            state.current_section = Section.from_str(match.group(1).strip())\n            line = re.sub(r""#+ (.*)$"", r""<strong>\\1</strong>\\n"", line)\n        else:\n            if line and not line.startswith("" "") and not line.startswith(""!!! ""):\n                if state.current_section in (Section.ARGUMENTS, Section.PARAMETERS,):\n                    param = Param.from_line(line)\n                    if param:\n                        line = param.to_line()\n                elif state.current_section in (Section.ATTRIBUTES, Section.MEMBERS):\n                    attrib = Attrib.from_line(line)\n                    if attrib:\n                        line = attrib.to_line()\n                elif state.current_section in (Section.RETURNS, Section.RAISES):\n                    retval = RetVal.from_line(line)\n                    line = retval.to_line()\n\n            line = self._transform_cross_references(line)\n\n        return line\n\n    def _transform_cross_references(self, line: str) -> str:\n        """"""\n        Replace sphinx style crossreferences with markdown links.\n        """"""\n        for match, ty, name in self.CROSS_REF_RE.findall(line):\n            if name.startswith(""allennlp.""):\n                path = name.split(""."")\n                if ty == ""mod"":\n                    href = ""/api/"" + ""/"".join(path[1:])\n                else:\n                    href = ""/api/"" + ""/"".join(path[1:-1]) + ""#"" + path[-1].lower()\n                cross_ref = f""[`{path[-1]}`]({href})""\n            elif ""."" not in name:\n                cross_ref = f""[`{name}`](#{name.lower()})""\n            else:\n                cross_ref = f""`{name}`""\n            line = line.replace(match, cross_ref)\n        return line\n\n\n@implements(Processor)\nclass AllenNlpFilterProcessor(Struct):\n    """"""\n    Used to filter out nodes that we don\'t want to document.\n    """"""\n\n    def process(self, graph, _resolver):\n        graph.visit(self._process_node)\n\n    def _process_node(self, node):\n        def _check(node):\n            if node.parent and node.parent.name.startswith(""_""):\n                return False\n            if node.name.startswith(""_""):\n                return False\n            if node.name == ""logger"" and isinstance(node.parent, Module):\n                return False\n            return True\n\n        if not _check(node):\n            node.visible = False\n\n\n@implements(Renderer)\nclass AllenNlpRenderer(MarkdownRenderer):\n    def _format_function_signature(\n        self, func: Function, override_name: str = None, add_method_bar: bool = True\n    ) -> str:\n        parts = []\n        for dec in func.decorators:\n            parts.append(""@{}{}\\n"".format(dec.name, dec.args or """"))\n        if self.signature_python_help_style and not func.is_method():\n            parts.append(""{} = "".format(func.path()))\n        if func.is_async:\n            parts.append(""async "")\n        if self.signature_with_def:\n            parts.append(""def "")\n        if self.signature_class_prefix and (\n            func.is_function() and func.parent and func.parent.is_class()\n        ):\n            parts.append(func.parent.name + ""."")\n        parts.append((override_name or func.name))\n        signature_args = Argument.format_arglist(func.args)\n        if signature_args.endswith("",""):\n            signature_args = signature_args[:-1].strip()\n        if (\n            len(parts[-1])\n            + len(signature_args)\n            + (0 if not func.return_ else len(str(func.return_)))\n            > 60\n        ):\n            signature_args = "",\\n    "".join(\n                filter(lambda s: s.strip() not in ("""", "",""), (str(arg) for arg in func.args))\n            )\n            parts.append(""(\\n    "" + signature_args + ""\\n)"")\n        else:\n            parts.append(""("" + signature_args + "")"")\n        if func.return_:\n            parts.append("" -> {}"".format(func.return_))\n        result = """".join(parts)\n        if add_method_bar and func.is_method():\n            result = ""\\n"".join("" | "" + line for line in result.split(""\\n""))\n        return result\n\n    def _format_classdef_signature(self, cls: Class) -> str:\n        bases = "", "".join(map(str, cls.bases))\n        if cls.metaclass:\n            bases += "", metaclass="" + str(cls.metaclass)\n        code = ""class {}({})"".format(cls.name, bases)\n        if self.signature_python_help_style:\n            code = cls.path() + "" = "" + code\n        if self.classdef_render_init_signature_if_needed and (\n            ""__init__"" in cls.members and not cls.members[""__init__""].visible\n        ):\n            code += "":\\n"" + self._format_function_signature(\n                cls.members[""__init__""], add_method_bar=True\n            )\n        return code\n\n    def _render_module_breadcrumbs(self, fp, mod: Module):\n        submods = mod.name.split(""."")\n        if submods[0] != ""allennlp"":\n            return\n        breadcrumbs = []\n        for i, submod_name in enumerate(submods):\n            if i == 0:\n                title = f""*{submod_name}*""\n            elif i == len(submods) - 1:\n                title = f""**.{submod_name}**""\n            else:\n                title = f""*.{submod_name}*""\n            #  href = ""/api/"" + ""/"".join(submods[1 : i + 1])\n            #  breadcrumbs.append(f""[{title}]({href})"")\n            breadcrumbs.append(title)\n        fp.write(""[ "" + """".join(breadcrumbs) + "" ]\\n\\n---\\n\\n"")\n\n    def _render_object(self, fp, level, obj):\n        if not isinstance(obj, Module) or self.render_module_header:\n            self._render_header(fp, level, obj)\n        if isinstance(obj, Module):\n            self._render_module_breadcrumbs(fp, obj)\n        self._render_signature_block(fp, obj)\n        if obj.docstring:\n            lines = obj.docstring.split(""\\n"")\n            if self.docstrings_as_blockquote:\n                lines = [""> "" + x for x in lines]\n            fp.write(""\\n"".join(lines))\n            fp.write(""\\n\\n"")\n\n\ndef py2md(module: str, out: Optional[str] = None) -> bool:\n    """"""\n    Returns `True` if module successfully processed, otherwise `False`.\n    """"""\n    logger.debug(""Processing %s"", module)\n    pydocmd = PydocMarkdown(\n        loaders=[PythonLoader(modules=[module])],\n        processors=[AllenNlpFilterProcessor(), AllenNlpDocstringProcessor()],\n        renderer=AllenNlpRenderer(\n            filename=out,\n            add_method_class_prefix=False,\n            add_member_class_prefix=False,\n            data_code_block=True,\n            signature_with_def=True,\n            use_fixed_header_levels=False,\n            render_module_header=False,\n        ),\n    )\n    if out:\n        out_path = Path(out)\n        os.makedirs(out_path.parent, exist_ok=True)\n\n    pydocmd.load_modules()\n    try:\n        pydocmd.process()\n    except DocstringError as err:\n        logger.exception(""Failed to process %s.\\n%s"", module, err)\n        return False\n    pydocmd.render()\n    return True\n\n\ndef _py2md_wrapper(x: Tuple[str, str]) -> bool:\n    """"""\n    Used to wrap py2md since we can\'t pickle a lambda (needed for multiprocessing).\n    """"""\n    return py2md(x[0], x[1])\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""modules"", nargs=""+"", type=str, help=""""""The Python modules to parse."""""")\n    parser.add_argument(\n        ""-o"",\n        ""--out"",\n        nargs=""+"",\n        type=str,\n        help=""""""Output files.\n                If given, must have the same number of items as \'modules\'.\n                If not given, stdout is used."""""",\n    )\n    return parser.parse_args()\n\n\ndef main():\n    opts = parse_args()\n    outputs = opts.out if opts.out else [None] * len(opts.modules)\n    if len(outputs) != len(opts.modules):\n        raise ValueError(""Number inputs and outputs should be the same."")\n    n_threads = cpu_count()\n    errors: int = 0\n    if len(opts.modules) > n_threads and opts.out:\n        # If writing to files, can process in parallel.\n        chunk_size = max([1, int(len(outputs) / n_threads)])\n        logger.info(""Using %d threads"", n_threads)\n        with Pool(n_threads) as p:\n            for result in p.imap(_py2md_wrapper, zip(opts.modules, outputs), chunk_size):\n                if not result:\n                    errors += 1\n    else:\n        # If writing to stdout, need to process sequentially. Otherwise the output\n        # could get intertwined.\n        for module, out in zip(opts.modules, outputs):\n            result = py2md(module, out)\n            if not result:\n                errors += 1\n    logger.info(""Processed %d modules"", len(opts.modules))\n    if errors:\n        logger.error(""Found %d errors"", errors)\n        sys.exit(1)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/train_fixtures.py,0,"b'#!/usr/bin/env python\n\nimport glob\nimport logging\nimport os\nimport re\nimport shutil\nimport sys\nimport tempfile\n\nsys.path.insert(0, os.path.dirname(os.path.abspath(os.path.join(__file__, os.pardir))))\nfrom allennlp.commands.test_install import _get_module_root\nfrom allennlp.commands.train import train_model_from_file, train_model\nfrom allennlp.common import Params\nfrom allennlp.common.util import pushd\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef train_fixture(config_prefix: str, config_filename: str = ""experiment.json"") -> None:\n    config_file = config_prefix + config_filename\n    serialization_dir = config_prefix + ""serialization""\n    # Train model doesn\'t like it if we have incomplete serialization\n    # directories, so remove them if they exist.\n    if os.path.exists(serialization_dir):\n        shutil.rmtree(serialization_dir)\n\n    # train the model\n    train_model_from_file(config_file, serialization_dir)\n\n    # remove unnecessary files\n    shutil.rmtree(os.path.join(serialization_dir, ""log""))\n\n    for filename in glob.glob(os.path.join(serialization_dir, ""*"")):\n        if (\n            filename.endswith("".log"")\n            or filename.endswith("".json"")\n            or re.search(r""epoch_[0-9]+\\.th$"", filename)\n        ):\n            os.remove(filename)\n\n\ndef train_fixture_gpu(config_prefix: str) -> None:\n    config_file = config_prefix + ""experiment.json""\n    serialization_dir = config_prefix + ""serialization""\n    params = Params.from_file(config_file)\n    params[""trainer""][""cuda_device""] = 0\n\n    # train this one to a tempdir\n    tempdir = tempfile.gettempdir()\n    train_model(params, tempdir)\n\n    # now copy back the weights and and archived model\n    shutil.copy(os.path.join(tempdir, ""best.th""), os.path.join(serialization_dir, ""best_gpu.th""))\n    shutil.copy(\n        os.path.join(tempdir, ""model.tar.gz""), os.path.join(serialization_dir, ""model_gpu.tar.gz"")\n    )\n\n\nif __name__ == ""__main__"":\n    module_root = _get_module_root().parent\n    with pushd(module_root, verbose=True):\n        models = [\n            (""basic_classifier"", ""experiment_seq2seq.jsonnet""),\n            ""simple_tagger"",\n            ""simple_tagger_with_elmo"",\n            ""simple_tagger_with_span_f1"",\n        ]\n        for model in models:\n            if isinstance(model, tuple):\n                model, config_filename = model\n                train_fixture(f""allennlp/tests/fixtures/{model}/"", config_filename)\n            else:\n                train_fixture(f""allennlp/tests/fixtures/{model}/"")\n'"
test_fixtures/__init__.py,0,b''
tests/__init__.py,0,b''
tests/version_test.py,0,"b'import re\n\nimport pytest\n\nfrom allennlp.version import VERSION\n\n\n# Regex to check that the current version set in `allennlp.version` adheres to\n# PEP 440, as well as some of our own internal conventions, such as the `.dev`\n# suffix being used only for nightly builds.\n# 0.0.0rc0.post0.dev20200424\nVALID_VERSION_RE = re.compile(\n    r""^""\n    r""(0|[1-9]\\d*)""  # major\n    r""\\.(0|[1-9]\\d*)""  # minor\n    r""\\.(0|[1-9]\\d*)""  # patch\n    r""(rc(0|[1-9]\\d*))?""  # patch suffix\n    r""(\\.post(0|[1-9]\\d*))?""  # [.postN]\n    r""(\\.dev2020[0-9]{4})?""  # [.devDATE]\n    r""$""\n)\n\n\ndef is_valid(version: str) -> bool:\n    return VALID_VERSION_RE.match(version) is not None\n\n\n@pytest.mark.parametrize(\n    ""version, valid"",\n    [\n        # Valid versions:\n        (""1.0.0"", True),\n        (""1.0.0rc3"", True),\n        (""1.0.0.post0"", True),\n        (""1.0.0.post1"", True),\n        (""1.0.0rc3.post0"", True),\n        (""1.0.0rc3.post0.dev20200424"", True),\n        # Invalid versions:\n        (""1.0.0.rc3"", False),\n        (""1.0.0rc01"", False),\n        (""1.0.0rc3.dev2020424"", False),\n    ],\n)\ndef test_is_valid_helper(version: str, valid: bool):\n    assert is_valid(version) is valid\n\n\ndef test_version():\n    """"""\n    Ensures current version is consistent with our conventions.\n    """"""\n    assert is_valid(VERSION)\n'"
allennlp/commands/__init__.py,0,"b'import argparse\nimport logging\nfrom typing import Any, Optional\n\nfrom overrides import overrides\n\nfrom allennlp import __version__\nfrom allennlp.commands.evaluate import Evaluate\nfrom allennlp.commands.find_learning_rate import FindLearningRate\nfrom allennlp.commands.predict import Predict\nfrom allennlp.commands.print_results import PrintResults\nfrom allennlp.commands.subcommand import Subcommand\nfrom allennlp.commands.test_install import TestInstall\nfrom allennlp.commands.train import Train\nfrom allennlp.common.plugins import import_plugins\nfrom allennlp.common.util import import_module_and_submodules\n\nlogger = logging.getLogger(__name__)\n\n\nclass ArgumentParserWithDefaults(argparse.ArgumentParser):\n    """"""\n    Custom argument parser that will display the default value for an argument\n    in the help message.\n    """"""\n\n    _action_defaults_to_ignore = {""help"", ""store_true"", ""store_false"", ""store_const""}\n\n    @staticmethod\n    def _is_empty_default(default: Any) -> bool:\n        if default is None:\n            return True\n        if isinstance(default, (str, list, tuple, set)):\n            return not bool(default)\n        return False\n\n    @overrides\n    def add_argument(self, *args, **kwargs):\n        # Add default value to the help message when the default is meaningful.\n        default = kwargs.get(""default"")\n        if kwargs.get(\n            ""action""\n        ) not in self._action_defaults_to_ignore and not self._is_empty_default(default):\n            description = kwargs.get(""help"", """")\n            kwargs[""help""] = f""{description} (default = {default})""\n        super().add_argument(*args, **kwargs)\n\n\ndef create_parser(prog: Optional[str] = None) -> argparse.ArgumentParser:\n    """"""\n    Creates the argument parser for the main program.\n    """"""\n    parser = ArgumentParserWithDefaults(description=""Run AllenNLP"", prog=prog)\n    parser.add_argument(""--version"", action=""version"", version=f""%(prog)s {__version__}"")\n\n    subparsers = parser.add_subparsers(title=""Commands"", metavar="""")\n\n    for subcommand_name in sorted(Subcommand.list_available()):\n        subcommand_class = Subcommand.by_name(subcommand_name)\n        subcommand = subcommand_class()\n        subparser = subcommand.add_subparser(subparsers)\n        subparser.add_argument(\n            ""--include-package"",\n            type=str,\n            action=""append"",\n            default=[],\n            help=""additional packages to include"",\n        )\n\n    return parser\n\n\ndef main(prog: Optional[str] = None) -> None:\n    """"""\n    The [`run`](./train.md#run) command only knows about the registered classes in the ``allennlp``\n    codebase. In particular, once you start creating your own `Model` s and so forth, it won\'t\n    work for them, unless you use the ``--include-package`` flag or you make your code available\n    as a plugin (see [`plugins`](./plugins.md)).\n    """"""\n    import_plugins()\n\n    parser = create_parser(prog)\n    args = parser.parse_args()\n\n    # If a subparser is triggered, it adds its work as `args.func`.\n    # So if no such attribute has been added, no subparser was triggered,\n    # so give the user some help.\n    if ""func"" in dir(args):\n        # Import any additional modules needed (to register custom classes).\n        for package_name in args.include_package:\n            import_module_and_submodules(package_name)\n        args.func(args)\n    else:\n        parser.print_help()\n'"
allennlp/commands/evaluate.py,0,"b'""""""\nThe `evaluate` subcommand can be used to\nevaluate a trained model against a dataset\nand report any metrics calculated by the model.\n""""""\n\nimport argparse\nimport json\nimport logging\nfrom typing import Any, Dict\n\nfrom overrides import overrides\n\nfrom allennlp.commands.subcommand import Subcommand\nfrom allennlp.common.util import dump_metrics, prepare_environment\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data import DataLoader\nfrom allennlp.models.archival import load_archive\nfrom allennlp.training.util import evaluate\n\nlogger = logging.getLogger(__name__)\n\n\n@Subcommand.register(""evaluate"")\nclass Evaluate(Subcommand):\n    @overrides\n    def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        description = """"""Evaluate the specified model + dataset""""""\n        subparser = parser.add_parser(\n            self.name, description=description, help=""Evaluate the specified model + dataset.""\n        )\n\n        subparser.add_argument(""archive_file"", type=str, help=""path to an archived trained model"")\n\n        subparser.add_argument(\n            ""input_file"", type=str, help=""path to the file containing the evaluation data""\n        )\n\n        subparser.add_argument(""--output-file"", type=str, help=""path to output file"")\n\n        subparser.add_argument(\n            ""--weights-file"", type=str, help=""a path that overrides which weights file to use""\n        )\n\n        cuda_device = subparser.add_mutually_exclusive_group(required=False)\n        cuda_device.add_argument(\n            ""--cuda-device"", type=int, default=-1, help=""id of GPU to use (if any)""\n        )\n\n        subparser.add_argument(\n            ""-o"",\n            ""--overrides"",\n            type=str,\n            default="""",\n            help=""a JSON structure used to override the experiment configuration"",\n        )\n\n        subparser.add_argument(\n            ""--batch-size"", type=int, help=""If non-empty, the batch size to use during evaluation.""\n        )\n\n        subparser.add_argument(\n            ""--batch-weight-key"",\n            type=str,\n            default="""",\n            help=""If non-empty, name of metric used to weight the loss on a per-batch basis."",\n        )\n\n        subparser.add_argument(\n            ""--extend-vocab"",\n            action=""store_true"",\n            default=False,\n            help=""if specified, we will use the instances in your new dataset to ""\n            ""extend your vocabulary. If pretrained-file was used to initialize ""\n            ""embedding layers, you may also need to pass --embedding-sources-mapping."",\n        )\n\n        subparser.add_argument(\n            ""--embedding-sources-mapping"",\n            type=str,\n            default="""",\n            help=""a JSON dict defining mapping from embedding module path to embedding ""\n            ""pretrained-file used during training. If not passed, and embedding needs to be ""\n            ""extended, we will try to use the original file paths used during training. If ""\n            ""they are not available we will use random vectors for embedding extension."",\n        )\n\n        subparser.set_defaults(func=evaluate_from_args)\n\n        return subparser\n\n\ndef evaluate_from_args(args: argparse.Namespace) -> Dict[str, Any]:\n    # Disable some of the more verbose logging statements\n    logging.getLogger(""allennlp.common.params"").disabled = True\n    logging.getLogger(""allennlp.nn.initializers"").disabled = True\n    logging.getLogger(""allennlp.modules.token_embedders.embedding"").setLevel(logging.INFO)\n\n    # Load from archive\n    archive = load_archive(\n        args.archive_file,\n        weights_file=args.weights_file,\n        cuda_device=args.cuda_device,\n        overrides=args.overrides,\n    )\n    config = archive.config\n    prepare_environment(config)\n    model = archive.model\n    model.eval()\n\n    # Load the evaluation data\n\n    # Try to use the validation dataset reader if there is one - otherwise fall back\n    # to the default dataset_reader used for both training and validation.\n    validation_dataset_reader_params = config.pop(""validation_dataset_reader"", None)\n    if validation_dataset_reader_params is not None:\n        dataset_reader = DatasetReader.from_params(validation_dataset_reader_params)\n    else:\n        dataset_reader = DatasetReader.from_params(config.pop(""dataset_reader""))\n    evaluation_data_path = args.input_file\n    logger.info(""Reading evaluation data from %s"", evaluation_data_path)\n    instances = dataset_reader.read(evaluation_data_path)\n\n    embedding_sources = (\n        json.loads(args.embedding_sources_mapping) if args.embedding_sources_mapping else {}\n    )\n\n    if args.extend_vocab:\n        logger.info(""Vocabulary is being extended with test instances."")\n        model.vocab.extend_from_instances(instances=instances)\n        model.extend_embedder_vocab(embedding_sources)\n\n    instances.index_with(model.vocab)\n    data_loader_params = config.pop(""validation_data_loader"", None)\n    if data_loader_params is None:\n        data_loader_params = config.pop(""data_loader"")\n    if args.batch_size:\n        data_loader_params[""batch_size""] = args.batch_size\n    data_loader = DataLoader.from_params(dataset=instances, params=data_loader_params)\n\n    metrics = evaluate(model, data_loader, args.cuda_device, args.batch_weight_key)\n\n    logger.info(""Finished evaluating."")\n\n    dump_metrics(args.output_file, metrics, log=True)\n\n    return metrics\n'"
allennlp/commands/find_learning_rate.py,0,"b'""""""\nThe `find-lr` subcommand can be used to find a good learning rate for a model.\nIt requires a configuration file and a directory in\nwhich to write the results.\n""""""\n\nimport argparse\nimport logging\nimport math\nimport os\nimport re\nfrom typing import List, Tuple\nimport itertools\n\nfrom overrides import overrides\n\nfrom allennlp.commands.subcommand import Subcommand\nfrom allennlp.common import Params, Tqdm\nfrom allennlp.common.checks import check_for_gpu, ConfigurationError\nfrom allennlp.common.util import prepare_environment\nfrom allennlp.data import Vocabulary\nfrom allennlp.data import DataLoader\nfrom allennlp.models import Model\nfrom allennlp.training import GradientDescentTrainer, Trainer\nfrom allennlp.training.util import create_serialization_dir, datasets_from_params\n\nlogger = logging.getLogger(__name__)\n\n\n@Subcommand.register(""find-lr"")\nclass FindLearningRate(Subcommand):\n    @overrides\n    def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n\n        description = """"""Find a learning rate range where loss decreases quickly\n                         for the specified model and dataset.""""""\n        subparser = parser.add_parser(\n            self.name, description=description, help=""Find a learning rate range.""\n        )\n\n        subparser.add_argument(\n            ""param_path"", type=str, help=""path to parameter file describing the model to be trained""\n        )\n        subparser.add_argument(\n            ""-s"",\n            ""--serialization-dir"",\n            required=True,\n            type=str,\n            help=""The directory in which to save results."",\n        )\n        subparser.add_argument(\n            ""-o"",\n            ""--overrides"",\n            type=str,\n            default="""",\n            help=""a JSON structure used to override the experiment configuration"",\n        )\n        subparser.add_argument(\n            ""--start-lr"", type=float, default=1e-5, help=""learning rate to start the search""\n        )\n        subparser.add_argument(\n            ""--end-lr"", type=float, default=10, help=""learning rate up to which search is done""\n        )\n        subparser.add_argument(\n            ""--num-batches"",\n            type=int,\n            default=100,\n            help=""number of mini-batches to run learning rate finder"",\n        )\n        subparser.add_argument(\n            ""--stopping-factor"",\n            type=float,\n            default=None,\n            help=""stop the search when the current loss exceeds the best loss recorded by ""\n            ""multiple of stopping factor"",\n        )\n        subparser.add_argument(\n            ""--linear"",\n            action=""store_true"",\n            help=""increase learning rate linearly instead of exponential increase"",\n        )\n        subparser.add_argument(\n            ""-f"",\n            ""--force"",\n            action=""store_true"",\n            required=False,\n            help=""overwrite the output directory if it exists"",\n        )\n\n        subparser.set_defaults(func=find_learning_rate_from_args)\n\n        return subparser\n\n\ndef find_learning_rate_from_args(args: argparse.Namespace) -> None:\n    """"""\n    Start learning rate finder for given args\n    """"""\n    params = Params.from_file(args.param_path, args.overrides)\n    find_learning_rate_model(\n        params,\n        args.serialization_dir,\n        start_lr=args.start_lr,\n        end_lr=args.end_lr,\n        num_batches=args.num_batches,\n        linear_steps=args.linear,\n        stopping_factor=args.stopping_factor,\n        force=args.force,\n    )\n\n\ndef find_learning_rate_model(\n    params: Params,\n    serialization_dir: str,\n    start_lr: float = 1e-5,\n    end_lr: float = 10,\n    num_batches: int = 100,\n    linear_steps: bool = False,\n    stopping_factor: float = None,\n    force: bool = False,\n) -> None:\n    """"""\n    Runs learning rate search for given `num_batches` and saves the results in ``serialization_dir``\n\n    # Parameters\n\n    params : `Params`\n        A parameter object specifying an AllenNLP Experiment.\n    serialization_dir : `str`\n        The directory in which to save results.\n    start_lr : `float`\n        Learning rate to start the search.\n    end_lr : `float`\n        Learning rate upto which search is done.\n    num_batches : `int`\n        Number of mini-batches to run Learning rate finder.\n    linear_steps : `bool`\n        Increase learning rate linearly if False exponentially.\n    stopping_factor : `float`\n        Stop the search when the current loss exceeds the best loss recorded by\n        multiple of stopping factor. If `None` search proceeds till the `end_lr`\n    force : `bool`\n        If True and the serialization directory already exists, everything in it will\n        be removed prior to finding the learning rate.\n    """"""\n    create_serialization_dir(params, serialization_dir, recover=False, force=force)\n\n    prepare_environment(params)\n\n    cuda_device = params.params.get(""trainer"").get(""cuda_device"", -1)\n    check_for_gpu(cuda_device)\n    distributed_params = params.params.get(""distributed"")\n    # See https://github.com/allenai/allennlp/issues/3658\n    assert not distributed_params, ""find-lr is not compatible with DistributedDataParallel.""\n\n    all_datasets = datasets_from_params(params)\n    datasets_for_vocab_creation = set(params.pop(""datasets_for_vocab_creation"", all_datasets))\n\n    for dataset in datasets_for_vocab_creation:\n        if dataset not in all_datasets:\n            raise ConfigurationError(f""invalid \'dataset_for_vocab_creation\' {dataset}"")\n\n    logger.info(\n        ""From dataset instances, %s will be considered for vocabulary creation."",\n        "", "".join(datasets_for_vocab_creation),\n    )\n    vocab = Vocabulary.from_params(\n        params.pop(""vocabulary"", {}),\n        instances=(\n            instance\n            for key, dataset in all_datasets.items()\n            for instance in dataset\n            if key in datasets_for_vocab_creation\n        ),\n    )\n\n    train_data = all_datasets[""train""]\n    train_data.index_with(vocab)\n    model = Model.from_params(vocab=vocab, params=params.pop(""model""))\n    data_loader = DataLoader.from_params(dataset=train_data, params=params.pop(""data_loader""))\n\n    trainer_params = params.pop(""trainer"")\n\n    no_grad_regexes = trainer_params.pop(""no_grad"", ())\n    for name, parameter in model.named_parameters():\n        if any(re.search(regex, name) for regex in no_grad_regexes):\n            parameter.requires_grad_(False)\n\n    trainer_choice = trainer_params.pop(""type"", ""gradient_descent"")\n    if trainer_choice != ""gradient_descent"":\n        raise ConfigurationError(\n            ""currently find-learning-rate only works with the GradientDescentTrainer""\n        )\n    trainer: GradientDescentTrainer = Trainer.from_params(  # type: ignore\n        model=model,\n        serialization_dir=serialization_dir,\n        data_loader=data_loader,\n        params=trainer_params,\n    )\n\n    logger.info(\n        f""Starting learning rate search from {start_lr} to {end_lr} in {num_batches} iterations.""\n    )\n    learning_rates, losses = search_learning_rate(\n        trainer,\n        start_lr=start_lr,\n        end_lr=end_lr,\n        num_batches=num_batches,\n        linear_steps=linear_steps,\n        stopping_factor=stopping_factor,\n    )\n    logger.info(""Finished learning rate search."")\n    losses = _smooth(losses, 0.98)\n\n    _save_plot(learning_rates, losses, os.path.join(serialization_dir, ""lr-losses.png""))\n\n\ndef search_learning_rate(\n    trainer: GradientDescentTrainer,\n    start_lr: float = 1e-5,\n    end_lr: float = 10,\n    num_batches: int = 100,\n    linear_steps: bool = False,\n    stopping_factor: float = None,\n) -> Tuple[List[float], List[float]]:\n    """"""\n    Runs training loop on the model using [`GradientDescentTrainer`](../training/trainer.md#gradientdescenttrainer)\n    increasing learning rate from `start_lr` to `end_lr` recording the losses.\n\n    # Parameters\n\n    trainer: `GradientDescentTrainer`\n    start_lr : `float`\n        The learning rate to start the search.\n    end_lr : `float`\n        The learning rate upto which search is done.\n    num_batches : `int`\n        Number of batches to run the learning rate finder.\n    linear_steps : `bool`\n        Increase learning rate linearly if False exponentially.\n    stopping_factor : `float`\n        Stop the search when the current loss exceeds the best loss recorded by\n        multiple of stopping factor. If `None` search proceeds till the `end_lr`\n\n    # Returns\n\n    (learning_rates, losses) : `Tuple[List[float], List[float]]`\n        Returns list of learning rates and corresponding losses.\n        Note: The losses are recorded before applying the corresponding learning rate\n    """"""\n    if num_batches <= 10:\n        raise ConfigurationError(\n            ""The number of iterations for learning rate finder should be greater than 10.""\n        )\n\n    trainer.model.train()\n\n    infinite_generator = itertools.cycle(trainer.data_loader)\n    train_generator_tqdm = Tqdm.tqdm(infinite_generator, total=num_batches)\n\n    learning_rates = []\n    losses = []\n    best = 1e9\n    if linear_steps:\n        lr_update_factor = (end_lr - start_lr) / num_batches\n    else:\n        lr_update_factor = (end_lr / start_lr) ** (1.0 / num_batches)\n\n    for i, batch in enumerate(train_generator_tqdm):\n\n        if linear_steps:\n            current_lr = start_lr + (lr_update_factor * i)\n        else:\n            current_lr = start_lr * (lr_update_factor ** i)\n\n        for param_group in trainer.optimizer.param_groups:\n            param_group[""lr""] = current_lr\n\n        trainer.optimizer.zero_grad()\n        loss = trainer.batch_outputs(batch, for_training=True)[""loss""]\n        loss.backward()\n        loss = loss.detach().cpu().item()\n\n        if stopping_factor is not None and (math.isnan(loss) or loss > stopping_factor * best):\n            logger.info(f""Loss ({loss}) exceeds stopping_factor * lowest recorded loss."")\n            break\n\n        trainer.rescale_gradients()\n        trainer.optimizer.step()\n\n        learning_rates.append(current_lr)\n        losses.append(loss)\n\n        if loss < best and i > 10:\n            best = loss\n\n        if i == num_batches:\n            break\n\n    return learning_rates, losses\n\n\ndef _smooth(values: List[float], beta: float) -> List[float]:\n    """""" Exponential smoothing of values """"""\n    avg_value = 0.0\n    smoothed = []\n    for i, value in enumerate(values):\n        avg_value = beta * avg_value + (1 - beta) * value\n        smoothed.append(avg_value / (1 - beta ** (i + 1)))\n    return smoothed\n\n\ndef _save_plot(learning_rates: List[float], losses: List[float], save_path: str):\n\n    try:\n        import matplotlib\n\n        matplotlib.use(""Agg"")  # noqa\n        import matplotlib.pyplot as plt\n\n    except ModuleNotFoundError as error:\n\n        logger.warn(\n            ""To use allennlp find-learning-rate, please install matplotlib: pip install matplotlib>=2.2.3 .""\n        )\n        raise error\n\n    plt.ylabel(""loss"")\n    plt.xlabel(""learning rate (log10 scale)"")\n    plt.xscale(""log"")\n    plt.plot(learning_rates, losses)\n    logger.info(f""Saving learning_rate vs loss plot to {save_path}."")\n    plt.savefig(save_path)\n'"
allennlp/commands/predict.py,0,"b'""""""\nThe `predict` subcommand allows you to make bulk JSON-to-JSON\nor dataset to JSON predictions using a trained model and its\n[`Predictor`](../predictors/predictor.md#predictor) wrapper.\n""""""\n\nfrom typing import List, Iterator, Optional\nimport argparse\nimport sys\nimport json\n\nfrom overrides import overrides\n\nfrom allennlp.commands.subcommand import Subcommand\nfrom allennlp.common.checks import check_for_gpu, ConfigurationError\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.common.util import lazy_groups_of\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors.predictor import Predictor, JsonDict\nfrom allennlp.data import Instance\n\n\n@Subcommand.register(""predict"")\nclass Predict(Subcommand):\n    @overrides\n    def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n\n        description = """"""Run the specified model against a JSON-lines input file.""""""\n        subparser = parser.add_parser(\n            self.name, description=description, help=""Use a trained model to make predictions.""\n        )\n\n        subparser.add_argument(\n            ""archive_file"", type=str, help=""the archived model to make predictions with""\n        )\n        subparser.add_argument(""input_file"", type=str, help=""path to or url of the input file"")\n\n        subparser.add_argument(""--output-file"", type=str, help=""path to output file"")\n        subparser.add_argument(\n            ""--weights-file"", type=str, help=""a path that overrides which weights file to use""\n        )\n\n        batch_size = subparser.add_mutually_exclusive_group(required=False)\n        batch_size.add_argument(\n            ""--batch-size"", type=int, default=1, help=""The batch size to use for processing""\n        )\n\n        subparser.add_argument(\n            ""--silent"", action=""store_true"", help=""do not print output to stdout""\n        )\n\n        cuda_device = subparser.add_mutually_exclusive_group(required=False)\n        cuda_device.add_argument(\n            ""--cuda-device"", type=int, default=-1, help=""id of GPU to use (if any)""\n        )\n\n        subparser.add_argument(\n            ""--use-dataset-reader"",\n            action=""store_true"",\n            help=""Whether to use the dataset reader of the original model to load Instances. ""\n            ""The validation dataset reader will be used if it exists, otherwise it will ""\n            ""fall back to the train dataset reader. This behavior can be overridden ""\n            ""with the --dataset-reader-choice flag."",\n        )\n\n        subparser.add_argument(\n            ""--dataset-reader-choice"",\n            type=str,\n            choices=[""train"", ""validation""],\n            default=""validation"",\n            help=""Indicates which model dataset reader to use if the --use-dataset-reader ""\n            ""flag is set."",\n        )\n\n        subparser.add_argument(\n            ""-o"",\n            ""--overrides"",\n            type=str,\n            default="""",\n            help=""a JSON structure used to override the experiment configuration"",\n        )\n\n        subparser.add_argument(\n            ""--predictor"", type=str, help=""optionally specify a specific predictor to use""\n        )\n\n        subparser.set_defaults(func=_predict)\n\n        return subparser\n\n\ndef _get_predictor(args: argparse.Namespace) -> Predictor:\n    check_for_gpu(args.cuda_device)\n    archive = load_archive(\n        args.archive_file,\n        weights_file=args.weights_file,\n        cuda_device=args.cuda_device,\n        overrides=args.overrides,\n    )\n\n    return Predictor.from_archive(\n        archive, args.predictor, dataset_reader_to_load=args.dataset_reader_choice\n    )\n\n\nclass _PredictManager:\n    def __init__(\n        self,\n        predictor: Predictor,\n        input_file: str,\n        output_file: Optional[str],\n        batch_size: int,\n        print_to_console: bool,\n        has_dataset_reader: bool,\n    ) -> None:\n\n        self._predictor = predictor\n        self._input_file = input_file\n        if output_file is not None:\n            self._output_file = open(output_file, ""w"")\n        else:\n            self._output_file = None\n        self._batch_size = batch_size\n        self._print_to_console = print_to_console\n        if has_dataset_reader:\n            self._dataset_reader = predictor._dataset_reader\n        else:\n            self._dataset_reader = None\n\n    def _predict_json(self, batch_data: List[JsonDict]) -> Iterator[str]:\n        if len(batch_data) == 1:\n            results = [self._predictor.predict_json(batch_data[0])]\n        else:\n            results = self._predictor.predict_batch_json(batch_data)\n        for output in results:\n            yield self._predictor.dump_line(output)\n\n    def _predict_instances(self, batch_data: List[Instance]) -> Iterator[str]:\n        if len(batch_data) == 1:\n            results = [self._predictor.predict_instance(batch_data[0])]\n        else:\n            results = self._predictor.predict_batch_instance(batch_data)\n        for output in results:\n            yield self._predictor.dump_line(output)\n\n    def _maybe_print_to_console_and_file(\n        self, index: int, prediction: str, model_input: str = None\n    ) -> None:\n        if self._print_to_console:\n            if model_input is not None:\n                print(f""input {index}: "", model_input)\n            print(""prediction: "", prediction)\n        if self._output_file is not None:\n            self._output_file.write(prediction)\n\n    def _get_json_data(self) -> Iterator[JsonDict]:\n        if self._input_file == ""-"":\n            for line in sys.stdin:\n                if not line.isspace():\n                    yield self._predictor.load_line(line)\n        else:\n            input_file = cached_path(self._input_file)\n            with open(input_file, ""r"") as file_input:\n                for line in file_input:\n                    if not line.isspace():\n                        yield self._predictor.load_line(line)\n\n    def _get_instance_data(self) -> Iterator[Instance]:\n        if self._input_file == ""-"":\n            raise ConfigurationError(""stdin is not an option when using a DatasetReader."")\n        elif self._dataset_reader is None:\n            raise ConfigurationError(""To generate instances directly, pass a DatasetReader."")\n        else:\n            yield from self._dataset_reader.read(self._input_file)\n\n    def run(self) -> None:\n        has_reader = self._dataset_reader is not None\n        index = 0\n        if has_reader:\n            for batch in lazy_groups_of(self._get_instance_data(), self._batch_size):\n                for model_input_instance, result in zip(batch, self._predict_instances(batch)):\n                    self._maybe_print_to_console_and_file(index, result, str(model_input_instance))\n                    index = index + 1\n        else:\n            for batch_json in lazy_groups_of(self._get_json_data(), self._batch_size):\n                for model_input_json, result in zip(batch_json, self._predict_json(batch_json)):\n                    self._maybe_print_to_console_and_file(\n                        index, result, json.dumps(model_input_json)\n                    )\n                    index = index + 1\n\n        if self._output_file is not None:\n            self._output_file.close()\n\n\ndef _predict(args: argparse.Namespace) -> None:\n    predictor = _get_predictor(args)\n\n    if args.silent and not args.output_file:\n        print(""--silent specified without --output-file."")\n        print(""Exiting early because no output will be created."")\n        sys.exit(0)\n\n    manager = _PredictManager(\n        predictor,\n        args.input_file,\n        args.output_file,\n        args.batch_size,\n        not args.silent,\n        args.use_dataset_reader,\n    )\n    manager.run()\n'"
allennlp/commands/print_results.py,0,"b'""""""\nThe `print-results` subcommand allows you to print results from multiple\nallennlp serialization directories to the console in a helpful csv format.\n""""""\n\nimport argparse\nimport json\nimport logging\nimport os\n\nfrom overrides import overrides\n\nfrom allennlp.commands.subcommand import Subcommand\n\nlogger = logging.getLogger(__name__)\n\n\n@Subcommand.register(""print-results"")\nclass PrintResults(Subcommand):\n    @overrides\n    def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n\n        description = """"""Print results from allennlp training runs in a helpful CSV format.""""""\n        subparser = parser.add_parser(\n            self.name,\n            description=description,\n            help=""Print results from allennlp serialization directories to the console."",\n        )\n        subparser.add_argument(\n            ""path"",\n            type=str,\n            help=""Path to recursively search for allennlp serialization directories."",\n        )\n\n        subparser.add_argument(\n            ""-k"",\n            ""--keys"",\n            type=str,\n            nargs=""+"",\n            help=""Keys to print from metrics.json.""\n            \'Keys not present in all metrics.json will result in ""N/A""\',\n            default=None,\n            required=False,\n        )\n        subparser.add_argument(\n            ""-m"",\n            ""--metrics-filename"",\n            type=str,\n            help=""Name of the metrics file to inspect."",\n            default=""metrics.json"",\n            required=False,\n        )\n\n        subparser.set_defaults(func=print_results_from_args)\n        return subparser\n\n\ndef print_results_from_args(args: argparse.Namespace):\n    """"""\n    Prints results from an `argparse.Namespace` object.\n    """"""\n    path = args.path\n    metrics_name = args.metrics_filename\n    keys = args.keys\n\n    results_dict = {}\n    for root, _, files in os.walk(path):\n        if metrics_name in files:\n            full_name = os.path.join(root, metrics_name)\n            with open(full_name) as file_:\n                metrics = json.load(file_)\n            results_dict[full_name] = metrics\n\n    sorted_keys = sorted(list(results_dict.keys()))\n    print(f""model_run, {\', \'.join(keys)}"")\n    for name in sorted_keys:\n        results = results_dict[name]\n        keys_to_print = (str(results.get(key, ""N/A"")) for key in keys)\n        print(f""{name}, {\', \'.join(keys_to_print)}"")\n'"
allennlp/commands/subcommand.py,0,"b'""""""\nBase class for subcommands under `allennlp.run`.\n""""""\n\nimport argparse\nfrom typing import Callable, Dict, Optional, Type, TypeVar\n\nfrom overrides import overrides\n\nfrom allennlp.common import Registrable\n\n\nT = TypeVar(""T"", bound=""Subcommand"")\n\n\nclass Subcommand(Registrable):\n    """"""\n    An abstract class representing subcommands for allennlp.run.\n    If you wanted to (for example) create your own custom `special-evaluate` command to use like\n\n    `allennlp special-evaluate ...`\n\n    you would create a `Subcommand` subclass and then pass it as an override to\n    [`main`](#main).\n    """"""\n\n    reverse_registry: Dict[Type, str] = {}\n\n    def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        raise NotImplementedError\n\n    @classmethod\n    @overrides\n    def register(\n        cls: Type[T], name: str, constructor: Optional[str] = None, exist_ok: bool = False\n    ) -> Callable[[Type[T]], Type[T]]:\n        super_register_fn = super().register(name, constructor=constructor, exist_ok=exist_ok)\n\n        def add_name_to_reverse_registry(subclass: Type[T]) -> Type[T]:\n            subclass = super_register_fn(subclass)\n            # Don\'t need to check `exist_ok`, as it\'s done by super.\n            # Also, don\'t need to delete previous entries if overridden, they can just stay there.\n            cls.reverse_registry[subclass] = name\n            return subclass\n\n        return add_name_to_reverse_registry\n\n    @property\n    def name(self) -> str:\n        return self.reverse_registry[self.__class__]\n'"
allennlp/commands/test_install.py,1,"b'""""""\nThe `test-install` subcommand provides a programmatic way to verify\nthat AllenNLP has been successfully installed.\n""""""\n\nimport argparse\nimport logging\nimport pathlib\n\nfrom overrides import overrides\nimport torch\n\nimport allennlp\nfrom allennlp.common.util import import_module_and_submodules\nfrom allennlp.commands.subcommand import Subcommand\nfrom allennlp.version import VERSION\n\n\nlogger = logging.getLogger(__name__)\n\n\n@Subcommand.register(""test-install"")\nclass TestInstall(Subcommand):\n    @overrides\n    def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        description = """"""Test that AllenNLP is installed correctly.""""""\n        subparser = parser.add_parser(\n            self.name, description=description, help=""Test AllenNLP installation.""\n        )\n        subparser.set_defaults(func=_run_test)\n        return subparser\n\n\ndef _get_module_root():\n    return pathlib.Path(allennlp.__file__).parent\n\n\ndef _run_test(args: argparse.Namespace):\n    # Make sure we can actually import the main modules without errors.\n    import_module_and_submodules(""allennlp.common"")\n    import_module_and_submodules(""allennlp.data"")\n    import_module_and_submodules(""allennlp.interpret"")\n    import_module_and_submodules(""allennlp.models"")\n    import_module_and_submodules(""allennlp.modules"")\n    import_module_and_submodules(""allennlp.nn"")\n    import_module_and_submodules(""allennlp.predictors"")\n    import_module_and_submodules(""allennlp.training"")\n    logger.info(""AllenNLP version %s installed to %s"", VERSION, _get_module_root())\n    logger.info(""Cuda devices available: %s"", torch.cuda.device_count())\n'"
allennlp/commands/train.py,3,"b'""""""\nThe `train` subcommand can be used to train a model.\nIt requires a configuration file and a directory in\nwhich to write the results.\n""""""\n\nimport argparse\nimport logging\nimport os\nfrom typing import Any, Dict, List, Optional\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom overrides import overrides\n\nfrom allennlp.commands.subcommand import Subcommand\nfrom allennlp.common import Params, Registrable, Lazy\nfrom allennlp.common.checks import check_for_gpu, ConfigurationError\nfrom allennlp.common.logging import prepare_global_logging\nfrom allennlp.common import util as common_util\nfrom allennlp.common.plugins import import_plugins\nfrom allennlp.data import DatasetReader, Vocabulary\nfrom allennlp.data import DataLoader\nfrom allennlp.models.archival import archive_model, CONFIG_NAME\nfrom allennlp.models.model import _DEFAULT_WEIGHTS, Model\nfrom allennlp.training.trainer import Trainer\nfrom allennlp.training import util as training_util\n\nlogger = logging.getLogger(__name__)\n\n\n@Subcommand.register(""train"")\nclass Train(Subcommand):\n    @overrides\n    def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        description = """"""Train the specified model on the specified dataset.""""""\n        subparser = parser.add_parser(self.name, description=description, help=""Train a model."")\n\n        subparser.add_argument(\n            ""param_path"", type=str, help=""path to parameter file describing the model to be trained""\n        )\n\n        subparser.add_argument(\n            ""-s"",\n            ""--serialization-dir"",\n            required=True,\n            type=str,\n            help=""directory in which to save the model and its logs"",\n        )\n\n        subparser.add_argument(\n            ""-r"",\n            ""--recover"",\n            action=""store_true"",\n            default=False,\n            help=""recover training from the state in serialization_dir"",\n        )\n\n        subparser.add_argument(\n            ""-f"",\n            ""--force"",\n            action=""store_true"",\n            required=False,\n            help=""overwrite the output directory if it exists"",\n        )\n\n        subparser.add_argument(\n            ""-o"",\n            ""--overrides"",\n            type=str,\n            default="""",\n            help=""a JSON structure used to override the experiment configuration"",\n        )\n\n        subparser.add_argument(\n            ""--file-friendly-logging"",\n            action=""store_true"",\n            default=False,\n            help=""outputs tqdm status on separate lines and slows tqdm refresh rate"",\n        )\n\n        subparser.add_argument(\n            ""--node-rank"", type=int, default=0, help=""rank of this node in the distributed setup""\n        )\n\n        subparser.add_argument(\n            ""--dry-run"",\n            action=""store_true"",\n            help=""do not train a model, but create a vocabulary, show dataset statistics and ""\n            ""other training information"",\n        )\n\n        subparser.set_defaults(func=train_model_from_args)\n\n        return subparser\n\n\ndef train_model_from_args(args: argparse.Namespace):\n    """"""\n    Just converts from an `argparse.Namespace` object to string paths.\n    """"""\n    train_model_from_file(\n        parameter_filename=args.param_path,\n        serialization_dir=args.serialization_dir,\n        overrides=args.overrides,\n        file_friendly_logging=args.file_friendly_logging,\n        recover=args.recover,\n        force=args.force,\n        node_rank=args.node_rank,\n        include_package=args.include_package,\n        dry_run=args.dry_run,\n    )\n\n\ndef train_model_from_file(\n    parameter_filename: str,\n    serialization_dir: str,\n    overrides: str = """",\n    file_friendly_logging: bool = False,\n    recover: bool = False,\n    force: bool = False,\n    node_rank: int = 0,\n    include_package: List[str] = None,\n    dry_run: bool = False,\n) -> Optional[Model]:\n    """"""\n    A wrapper around [`train_model`](#train_model) which loads the params from a file.\n\n    # Parameters\n\n    parameter_filename : `str`\n        A json parameter file specifying an AllenNLP experiment.\n    serialization_dir : `str`\n        The directory in which to save results and logs. We just pass this along to\n        [`train_model`](#train_model).\n    overrides : `str`\n        A JSON string that we will use to override values in the input parameter file.\n    file_friendly_logging : `bool`, optional (default=`False`)\n        If `True`, we make our output more friendly to saved model files.  We just pass this\n        along to [`train_model`](#train_model).\n    recover : `bool`, optional (default=`False`)\n        If `True`, we will try to recover a training run from an existing serialization\n        directory.  This is only intended for use when something actually crashed during the middle\n        of a run.  For continuing training a model on new data, see `Model.from_archive`.\n    force : `bool`, optional (default=`False`)\n        If `True`, we will overwrite the serialization directory if it already exists.\n    node_rank : `int`, optional\n        Rank of the current node in distributed training\n    include_package : `str`, optional\n        In distributed mode, extra packages mentioned will be imported in trainer workers.\n    dry_run : `bool`, optional (default=`False`)\n        Do not train a model, but create a vocabulary, show dataset statistics and other training\n        information.\n\n    # Returns\n\n    best_model : `Optional[Model]`\n        The model with the best epoch weights or `None` if in dry run.\n    """"""\n    # Load the experiment config from a file and pass it to `train_model`.\n    params = Params.from_file(parameter_filename, overrides)\n    return train_model(\n        params=params,\n        serialization_dir=serialization_dir,\n        file_friendly_logging=file_friendly_logging,\n        recover=recover,\n        force=force,\n        node_rank=node_rank,\n        include_package=include_package,\n        dry_run=dry_run,\n    )\n\n\ndef train_model(\n    params: Params,\n    serialization_dir: str,\n    file_friendly_logging: bool = False,\n    recover: bool = False,\n    force: bool = False,\n    node_rank: int = 0,\n    include_package: List[str] = None,\n    dry_run: bool = False,\n) -> Optional[Model]:\n    """"""\n    Trains the model specified in the given [`Params`](../common/params.md#params) object, using the data\n    and training parameters also specified in that object, and saves the results in `serialization_dir`.\n\n    # Parameters\n\n    params : `Params`\n        A parameter object specifying an AllenNLP Experiment.\n    serialization_dir : `str`\n        The directory in which to save results and logs.\n    file_friendly_logging : `bool`, optional (default=`False`)\n        If `True`, we add newlines to tqdm output, even on an interactive terminal, and we slow\n        down tqdm\'s output to only once every 10 seconds.\n    recover : `bool`, optional (default=`False`)\n        If `True`, we will try to recover a training run from an existing serialization\n        directory.  This is only intended for use when something actually crashed during the middle\n        of a run.  For continuing training a model on new data, see `Model.from_archive`.\n    force : `bool`, optional (default=`False`)\n        If `True`, we will overwrite the serialization directory if it already exists.\n    node_rank : `int`, optional\n        Rank of the current node in distributed training\n    include_package : `List[str]`, optional\n        In distributed mode, extra packages mentioned will be imported in trainer workers.\n    dry_run : `bool`, optional (default=`False`)\n        Do not train a model, but create a vocabulary, show dataset statistics and other training\n        information.\n\n    # Returns\n\n    best_model : `Optional[Model]`\n        The model with the best epoch weights or `None` if in dry run.\n    """"""\n    training_util.create_serialization_dir(params, serialization_dir, recover, force)\n    params.to_file(os.path.join(serialization_dir, CONFIG_NAME))\n\n    distributed_params = params.params.pop(""distributed"", None)\n    # If distributed isn\'t in the config and the config contains strictly\n    # one cuda device, we just run a single training process.\n    if distributed_params is None:\n        model = _train_worker(\n            process_rank=0,\n            params=params,\n            serialization_dir=serialization_dir,\n            file_friendly_logging=file_friendly_logging,\n            include_package=include_package,\n            dry_run=dry_run,\n        )\n        if not dry_run:\n            archive_model(serialization_dir)\n        return model\n\n    # Otherwise, we are running multiple processes for training.\n    else:\n        # We are careful here so that we can raise a good error if someone\n        # passed the wrong thing - cuda_devices are required.\n        device_ids = distributed_params.pop(""cuda_devices"", None)\n        multi_device = isinstance(device_ids, list) and len(device_ids) > 1\n        num_nodes = distributed_params.pop(""num_nodes"", 1)\n\n        if not (multi_device or num_nodes > 1):\n            raise ConfigurationError(\n                ""Multiple cuda devices/nodes need to be configured to run distributed training.""\n            )\n        check_for_gpu(device_ids)\n\n        master_addr = distributed_params.pop(""master_address"", ""127.0.0.1"")\n        master_port = distributed_params.pop(""master_port"", 29500)\n        num_procs = len(device_ids)\n        world_size = num_nodes * num_procs\n\n        logging.info(\n            f""Switching to distributed training mode since multiple GPUs are configured""\n            f""Master is at: {master_addr}:{master_port} | Rank of this node: {node_rank} | ""\n            f""Number of workers in this node: {num_procs} | Number of nodes: {num_nodes} | ""\n            f""World size: {world_size}""\n        )\n\n        # Creating `Vocabulary` objects from workers could be problematic since\n        # the data loaders in each worker will yield only `rank` specific\n        # instances. Hence it is safe to construct the vocabulary and write it\n        # to disk before initializing the distributed context. The workers will\n        # load the vocabulary from the path specified.\n        vocab_dir = os.path.join(serialization_dir, ""vocabulary"")\n        if recover:\n            vocab = Vocabulary.from_files(vocab_dir)\n        else:\n            vocab = training_util.make_vocab_from_params(\n                params.duplicate(), serialization_dir, print_statistics=dry_run\n            )\n        params[""vocabulary""] = {\n            ""type"": ""from_files"",\n            ""directory"": vocab_dir,\n            ""padding_token"": vocab._padding_token,\n            ""oov_token"": vocab._oov_token,\n        }\n\n        mp.spawn(\n            _train_worker,\n            args=(\n                params.duplicate(),\n                serialization_dir,\n                file_friendly_logging,\n                include_package,\n                dry_run,\n                node_rank,\n                master_addr,\n                master_port,\n                world_size,\n                device_ids,\n            ),\n            nprocs=num_procs,\n        )\n        if dry_run:\n            return None\n        else:\n            archive_model(serialization_dir)\n            model = Model.load(params, serialization_dir)\n            return model\n\n\ndef _train_worker(\n    process_rank: int,\n    params: Params,\n    serialization_dir: str,\n    file_friendly_logging: bool = False,\n    include_package: List[str] = None,\n    dry_run: bool = False,\n    node_rank: int = 0,\n    master_addr: str = ""127.0.0.1"",\n    master_port: int = 29500,\n    world_size: int = 1,\n    distributed_device_ids: List[int] = None,\n) -> Optional[Model]:\n    """"""\n    Helper to train the configured model/experiment. In distributed mode, this is spawned as a\n    worker process. In a single GPU experiment, this returns the `Model` object and in distributed\n    training, nothing is returned.\n\n    # Parameters\n\n    process_rank : `int`\n        The process index that is initialized using the GPU device id.\n    params : `Params`\n        A parameter object specifying an AllenNLP Experiment.\n    serialization_dir : `str`\n        The directory in which to save results and logs.\n    file_friendly_logging : `bool`, optional (default=`False`)\n        If `True`, we add newlines to tqdm output, even on an interactive terminal, and we slow\n        down tqdm\'s output to only once every 10 seconds.\n    include_package : `List[str]`, optional\n        In distributed mode, since this function would have been spawned as a separate process,\n        the extra imports need to be done again. NOTE: This does not have any effect in single\n        GPU training.\n    dry_run : `bool`, optional (default=`False`)\n        Do not train a model, but create a vocabulary, show dataset statistics and other training\n        information.\n    node_rank : `int`, optional\n        Rank of the node.\n    master_addr : `str`, optional (default=`""127.0.0.1""`)\n        Address of the master node for distributed training.\n    master_port : `str`, optional (default=`""29500""`)\n        Port of the master node for distributed training.\n    world_size : `int`, optional\n        The number of processes involved in distributed training.\n    distributed_device_ids: `List[str]`, optional\n        IDs of the devices used involved in distributed training.\n\n    # Returns\n\n    best_model : `Optional[Model]`\n        The model with the best epoch weights or `None` if in distributed training or in dry run.\n    """"""\n    prepare_global_logging(\n        serialization_dir, file_friendly_logging, rank=process_rank, world_size=world_size\n    )\n    common_util.prepare_environment(params)\n\n    distributed = world_size > 1\n\n    # not using `allennlp.common.util.is_master` as the process group is yet to be initialized\n    master = process_rank == 0\n\n    include_package = include_package or []\n\n    if distributed:\n        # Since the worker is spawned and not forked, the extra imports need to be done again.\n        # Both the ones from the plugins and the ones from `include_package`.\n        import_plugins()\n        for package_name in include_package:\n            common_util.import_module_and_submodules(package_name)\n\n        num_procs_per_node = len(distributed_device_ids)\n        # The Unique identifier of the worker process among all the processes in the\n        # distributed training group is computed here. This is used while initializing\n        # the process group using `init_process_group`\n        global_rank = node_rank * num_procs_per_node + process_rank\n\n        # Number of processes per node is useful to know if a process\n        # is a master in the local node(node in which it is running)\n        os.environ[""ALLENNLP_PROCS_PER_NODE""] = str(num_procs_per_node)\n\n        # In distributed training, the configured device is always going to be a list.\n        # The corresponding gpu id for the particular worker is obtained by picking the id\n        # from the device list with the rank as index\n        gpu_id = distributed_device_ids[process_rank]  # type: ignore\n\n        # Till now, ""cuda_device"" might not be set in the trainer params.\n        # But a worker trainer needs to only know about its specific GPU id.\n        params[""trainer""][""cuda_device""] = gpu_id\n        params[""trainer""][""world_size""] = world_size\n        params[""trainer""][""distributed""] = True\n\n        if gpu_id >= 0:\n            torch.cuda.set_device(int(gpu_id))\n            dist.init_process_group(\n                backend=""nccl"",\n                init_method=f""tcp://{master_addr}:{master_port}"",\n                world_size=world_size,\n                rank=global_rank,\n            )\n        else:\n            dist.init_process_group(\n                backend=""gloo"",\n                init_method=f""tcp://{master_addr}:{master_port}"",\n                world_size=world_size,\n                rank=global_rank,\n            )\n        logging.info(\n            f""Process group of world size {world_size} initialized ""\n            f""for distributed training in worker {global_rank}""\n        )\n\n    train_loop = TrainModel.from_params(\n        params=params, serialization_dir=serialization_dir, local_rank=process_rank,\n    )\n\n    if dry_run:\n        return None\n\n    try:\n        if distributed:  # let the setup get ready for all the workers\n            dist.barrier()\n\n        metrics = train_loop.run()\n    except KeyboardInterrupt:\n        # if we have completed an epoch, try to create a model archive.\n        if master and os.path.exists(os.path.join(serialization_dir, _DEFAULT_WEIGHTS)):\n            logging.info(\n                ""Training interrupted by the user. Attempting to create ""\n                ""a model archive using the current best epoch weights.""\n            )\n            archive_model(serialization_dir)\n        raise\n\n    if master:\n        train_loop.finish(metrics)\n\n    if not distributed:\n        return train_loop.model\n\n    return None\n\n\nclass TrainModel(Registrable):\n    """"""\n    This class exists so that we can easily read a configuration file with the `allennlp train`\n    command.  The basic logic is that we call `train_loop =\n    TrainModel.from_params(params_from_config_file)`, then `train_loop.run()`.  This class performs\n    very little logic, pushing most of it to the `Trainer` that has a `train()` method.  The\n    point here is to construct all of the dependencies for the `Trainer` in a way that we can do\n    it using `from_params()`, while having all of those dependencies transparently documented and\n    not hidden in calls to `params.pop()`.  If you are writing your own training loop, you almost\n    certainly should not use this class, but you might look at the code for this class to see what\n    we do, to make writing your training loop easier.\n\n    In particular, if you are tempted to call the `__init__` method of this class, you are probably\n    doing something unnecessary.  Literally all we do after `__init__` is call `trainer.train()`.  You\n    can do that yourself, if you\'ve constructed a `Trainer` already.  What this class gives you is a\n    way to construct the `Trainer` by means of a config file.  The actual constructor that we use\n    with `from_params` in this class is `from_partial_objects`.  See that method for a description\n    of all of the allowed top-level keys in a configuration file used with `allennlp train`.\n    """"""\n\n    default_implementation = ""default""\n    """"""\n    The default implementation is registered as \'default\'.\n    """"""\n\n    def __init__(\n        self,\n        serialization_dir: str,\n        model: Model,\n        trainer: Trainer,\n        evaluation_data_loader: DataLoader = None,\n        evaluate_on_test: bool = False,\n        batch_weight_key: str = """",\n    ) -> None:\n        self.serialization_dir = serialization_dir\n        self.model = model\n        self.trainer = trainer\n        self.evaluation_data_loader = evaluation_data_loader\n        self.evaluate_on_test = evaluate_on_test\n        self.batch_weight_key = batch_weight_key\n\n    def run(self) -> Dict[str, Any]:\n        return self.trainer.train()\n\n    def finish(self, metrics: Dict[str, Any]):\n        if self.evaluation_data_loader is not None and self.evaluate_on_test:\n            logger.info(""The model will be evaluated using the best epoch weights."")\n            test_metrics = training_util.evaluate(\n                self.model,\n                self.evaluation_data_loader,\n                cuda_device=self.trainer.cuda_device,\n                batch_weight_key=self.batch_weight_key,\n            )\n\n            for key, value in test_metrics.items():\n                metrics[""test_"" + key] = value\n        elif self.evaluation_data_loader:\n            logger.info(\n                ""To evaluate on the test set after training, pass the ""\n                ""\'evaluate_on_test\' flag, or use the \'allennlp evaluate\' command.""\n            )\n        common_util.dump_metrics(\n            os.path.join(self.serialization_dir, ""metrics.json""), metrics, log=True\n        )\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        serialization_dir: str,\n        local_rank: int,\n        dataset_reader: DatasetReader,\n        train_data_path: str,\n        model: Lazy[Model],\n        data_loader: Lazy[DataLoader],\n        trainer: Lazy[Trainer],\n        vocabulary: Lazy[Vocabulary] = None,\n        datasets_for_vocab_creation: List[str] = None,\n        validation_dataset_reader: DatasetReader = None,\n        validation_data_path: str = None,\n        validation_data_loader: Lazy[DataLoader] = None,\n        test_data_path: str = None,\n        evaluate_on_test: bool = False,\n        batch_weight_key: str = """",\n    ) -> ""TrainModel"":\n        """"""\n        This method is intended for use with our `FromParams` logic, to construct a `TrainModel`\n        object from a config file passed to the `allennlp train` command.  The arguments to this\n        method are the allowed top-level keys in a configuration file (except for the first three,\n        which are obtained separately).\n\n        You *could* use this outside of our `FromParams` logic if you really want to, but there\n        might be easier ways to accomplish your goal than instantiating `Lazy` objects.  If you are\n        writing your own training loop, we recommend that you look at the implementation of this\n        method for inspiration and possibly some utility functions you can call, but you very likely\n        should not use this method directly.\n\n        The `Lazy` type annotations here are a mechanism for building dependencies to an object\n        sequentially - the `TrainModel` object needs data, a model, and a trainer, but the model\n        needs to see the data before it\'s constructed (to create a vocabulary) and the trainer needs\n        the data and the model before it\'s constructed.  Objects that have sequential dependencies\n        like this are labeled as `Lazy` in their type annotations, and we pass the missing\n        dependencies when we call their `construct()` method, which you can see in the code below.\n\n        # Parameters\n        serialization_dir: `str`\n            The directory where logs and model archives will be saved.\n\n            In a typical AllenNLP configuration file, this parameter does not get an entry as a\n            top-level key, it gets passed in separately.\n        local_rank: `int`\n            The process index that is initialized using the GPU device id.\n\n            In a typical AllenNLP configuration file, this parameter does not get an entry as a\n            top-level key, it gets passed in separately.\n        dataset_reader: `DatasetReader`\n            The `DatasetReader` that will be used for training and (by default) for validation.\n        train_data_path: `str`\n            The file (or directory) that will be passed to `dataset_reader.read()` to construct the\n            training data.\n        model: `Lazy[Model]`\n            The model that we will train.  This is lazy because it depends on the `Vocabulary`;\n            after constructing the vocabulary we call `model.construct(vocab=vocabulary)`.\n        data_loader: `Lazy[DataLoader]`\n            The data_loader we use to batch instances from the dataset reader at training and (by\n            default) validation time. This is lazy because it takes a dataset in it\'s constructor.\n        trainer: `Lazy[Trainer]`\n            The `Trainer` that actually implements the training loop.  This is a lazy object because\n            it depends on the model that\'s going to be trained.\n        vocabulary: `Lazy[Vocabulary]`, optional (default=`None`)\n            The `Vocabulary` that we will use to convert strings in the data to integer ids (and\n            possibly set sizes of embedding matrices in the `Model`).  By default we construct the\n            vocabulary from the instances that we read.\n        datasets_for_vocab_creation: `List[str]`, optional (default=`None`)\n            If you pass in more than one dataset but don\'t want to use all of them to construct a\n            vocabulary, you can pass in this key to limit it.  Valid entries in the list are\n            ""train"", ""validation"" and ""test"".\n        validation_dataset_reader: `DatasetReader`, optional (default=`None`)\n            If given, we will use this dataset reader for the validation data instead of\n            `dataset_reader`.\n        validation_data_path: `str`, optional (default=`None`)\n            If given, we will use this data for computing validation metrics and early stopping.\n        validation_data_loader: `Lazy[DataLoader]`, optional (default=`None`)\n            If given, the data_loader we use to batch instances from the dataset reader at\n            validation and test time. This is lazy because it takes a dataset in it\'s constructor.\n        test_data_path: `str`, optional (default=`None`)\n            If given, we will use this as test data.  This makes it available for vocab creation by\n            default, but nothing else.\n        evaluate_on_test: `bool`, optional (default=`False`)\n            If given, we will evaluate the final model on this data at the end of training.  Note\n            that we do not recommend using this for actual test data in every-day experimentation;\n            you should only very rarely evaluate your model on actual test data.\n        batch_weight_key: `str`, optional (default=`""""`)\n            The name of metric used to weight the loss on a per-batch basis.  This is only used\n            during evaluation on final test data, if you\'ve specified `evaluate_on_test=True`.\n        """"""\n\n        datasets = training_util.read_all_datasets(\n            train_data_path=train_data_path,\n            dataset_reader=dataset_reader,\n            validation_dataset_reader=validation_dataset_reader,\n            validation_data_path=validation_data_path,\n            test_data_path=test_data_path,\n        )\n\n        if datasets_for_vocab_creation:\n            for key in datasets_for_vocab_creation:\n                if key not in datasets:\n                    raise ConfigurationError(f""invalid \'dataset_for_vocab_creation\' {key}"")\n\n        instance_generator = (\n            instance\n            for key, dataset in datasets.items()\n            if not datasets_for_vocab_creation or key in datasets_for_vocab_creation\n            for instance in dataset\n        )\n\n        vocabulary_ = vocabulary.construct(instances=instance_generator)\n        if not vocabulary_:\n            vocabulary_ = Vocabulary.from_instances(instance_generator)\n        model_ = model.construct(vocab=vocabulary_)\n\n        # Initializing the model can have side effect of expanding the vocabulary.\n        # Save the vocab only in the master. In the degenerate non-distributed\n        # case, we\'re trivially the master. In the distributed case this is safe\n        # to do without worrying about race conditions since saving and loading\n        # the vocab involves acquiring a file lock.\n        if common_util.is_master():\n            vocabulary_path = os.path.join(serialization_dir, ""vocabulary"")\n            vocabulary_.save_to_files(vocabulary_path)\n\n        for dataset in datasets.values():\n            dataset.index_with(model_.vocab)\n\n        data_loader_ = data_loader.construct(dataset=datasets[""train""])\n        validation_data = datasets.get(""validation"")\n        if validation_data is not None:\n            # Because of the way Lazy[T] works, we can\'t check it\'s existence\n            # _before_ we\'ve tried to construct it. It returns None if it is not\n            # present, so we try to construct it first, and then afterward back off\n            # to the data_loader configuration used for training if it returns None.\n            validation_data_loader_ = validation_data_loader.construct(dataset=validation_data)\n            if validation_data_loader_ is None:\n                validation_data_loader_ = data_loader.construct(dataset=validation_data)\n        else:\n            validation_data_loader_ = None\n\n        test_data = datasets.get(""test"")\n        if test_data is not None:\n            test_data_loader = validation_data_loader.construct(dataset=test_data)\n            if test_data_loader is None:\n                test_data_loader = data_loader.construct(dataset=test_data)\n        else:\n            test_data_loader = None\n\n        # We don\'t need to pass serialization_dir and local_rank here, because they will have been\n        # passed through the trainer by from_params already, because they were keyword arguments to\n        # construct this class in the first place.\n        trainer_ = trainer.construct(\n            model=model_, data_loader=data_loader_, validation_data_loader=validation_data_loader_,\n        )\n\n        return cls(\n            serialization_dir=serialization_dir,\n            model=model_,\n            trainer=trainer_,\n            evaluation_data_loader=test_data_loader,\n            evaluate_on_test=evaluate_on_test,\n            batch_weight_key=batch_weight_key,\n        )\n\n\nTrainModel.register(""default"", constructor=""from_partial_objects"")(TrainModel)\n'"
allennlp/common/__init__.py,0,b'from allennlp.common.from_params import FromParams\nfrom allennlp.common.lazy import Lazy\nfrom allennlp.common.params import Params\nfrom allennlp.common.registrable import Registrable\nfrom allennlp.common.tqdm import Tqdm\nfrom allennlp.common.util import JsonDict\n'
allennlp/common/checks.py,3,"b'""""""\nFunctions and exceptions for checking that\nAllenNLP and its models are configured correctly.\n""""""\nfrom typing import Union, List\n\nimport logging\nimport re\nimport subprocess\n\nimport torch\nfrom torch import cuda\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConfigurationError(Exception):\n    """"""\n    The exception raised by any AllenNLP object when it\'s misconfigured\n    (e.g. missing properties, invalid properties, unknown properties).\n    """"""\n\n    def __init__(self, message):\n        super().__init__()\n        self.message = message\n\n    def __str__(self):\n        # TODO(brendanr): Is there some reason why we need repr here? It\n        # produces horrible output for simple multi-line error messages.\n        return self.message\n\n\nclass ExperimentalFeatureWarning(RuntimeWarning):\n    """"""\n    A warning that you are using an experimental feature\n    that may change or be deleted.\n    """"""\n\n    pass\n\n\ndef log_pytorch_version_info():\n    import torch\n\n    logger.info(""Pytorch version: %s"", torch.__version__)\n\n\ndef check_dimensions_match(\n    dimension_1: int, dimension_2: int, dim_1_name: str, dim_2_name: str\n) -> None:\n    if dimension_1 != dimension_2:\n        raise ConfigurationError(\n            f""{dim_1_name} must match {dim_2_name}, but got {dimension_1} ""\n            f""and {dimension_2} instead""\n        )\n\n\ndef parse_cuda_device(cuda_device: Union[str, int, List[int]]) -> int:\n    """"""\n    Disambiguates single GPU and multiple GPU settings for cuda_device param.\n    """"""\n\n    message = """"""\n    In allennlp 1.0, the Trainer cannot be passed multiple cuda devices.\n    Instead, use the faster Distributed Data Parallel. For instance, if you previously had config like:\n        {\n          ""trainer"": {\n            ""cuda_device"": [0, 1, 2, 3],\n            ""num_epochs"": 20,\n            ...\n          }\n        }\n        simply change it to:\n        {\n          ""distributed"": {\n            ""cuda_devices"": [0, 1, 2, 3],\n          },\n          ""trainer"": {\n            ""num_epochs"": 20,\n            ...\n          }\n        }\n        """"""\n\n    def from_list(strings):\n        if len(strings) > 1:\n            raise ConfigurationError(message)\n        elif len(strings) == 1:\n            return int(strings[0])\n        else:\n            return -1\n\n    if isinstance(cuda_device, str):\n        return from_list(re.split(r"",\\s*"", cuda_device))\n    elif isinstance(cuda_device, int):\n        return cuda_device\n    elif isinstance(cuda_device, list):\n        return from_list(cuda_device)\n    else:\n        # TODO(brendanr): Determine why mypy can\'t tell that this matches the Union.\n        return int(cuda_device)  # type: ignore\n\n\ndef check_for_gpu(device: Union[int, torch.device, List[Union[int, torch.device]]]):\n    if isinstance(device, list):\n        for did in device:\n            check_for_gpu(did)\n    elif device is None:\n        return\n    else:\n        from allennlp.common.util import int_to_device\n\n        device = int_to_device(device)\n        if device != torch.device(""cpu""):\n            num_devices_available = cuda.device_count()\n            if num_devices_available == 0:\n                # Torch will give a more informative exception than ours, so we want to include\n                # that context as well if it\'s available.  For example, if you try to run torch 1.5\n                # on a machine with CUDA10.1 you\'ll get the following:\n                #\n                #     The NVIDIA driver on your system is too old (found version 10010).\n                #\n                torch_gpu_error = """"\n                try:\n                    cuda._check_driver()\n                except Exception as e:\n                    torch_gpu_error = ""\\n{0}"".format(e)\n\n                raise ConfigurationError(\n                    ""Experiment specified a GPU but none is available;""\n                    "" if you want to run on CPU use the override""\n                    "" \'trainer.cuda_device=-1\' in the json config file."" + torch_gpu_error\n                )\n            elif device.index >= num_devices_available:\n                raise ConfigurationError(\n                    f""Experiment specified GPU device {device.index}""\n                    f"" but there are only {num_devices_available} devices ""\n                    f"" available.""\n                )\n\n\ndef check_for_java() -> bool:\n    try:\n        java_version = subprocess.check_output([""java"", ""-version""], stderr=subprocess.STDOUT)\n        return ""version"" in java_version.decode()\n    except FileNotFoundError:\n        return False\n'"
allennlp/common/file_utils.py,0,"b'""""""\nUtilities for working with the local dataset cache.\n""""""\n\nimport glob\nimport os\nimport logging\nimport shutil\nimport tempfile\nimport json\nfrom urllib.parse import urlparse\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, IO, Callable, Set, List\nfrom hashlib import sha256\nfrom functools import wraps\n\nimport boto3\nimport botocore\nfrom botocore.exceptions import ClientError, EndpointConnectionError\nfrom filelock import FileLock\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.exceptions import ConnectionError\nfrom requests.packages.urllib3.util.retry import Retry\n\nfrom allennlp.common.tqdm import Tqdm\n\nlogger = logging.getLogger(__name__)\n\nCACHE_ROOT = Path(os.getenv(""ALLENNLP_CACHE_ROOT"", Path.home() / "".allennlp""))\nCACHE_DIRECTORY = str(CACHE_ROOT / ""cache"")\nDEPRECATED_CACHE_DIRECTORY = str(CACHE_ROOT / ""datasets"")\n\n# This variable was deprecated in 0.7.2 since we use a single folder for caching\n# all types of files (datasets, models, etc.)\nDATASET_CACHE = CACHE_DIRECTORY\n\n# Warn if the user is still using the deprecated cache directory.\nif os.path.exists(DEPRECATED_CACHE_DIRECTORY):\n    logger = logging.getLogger(__name__)\n    logger.warning(\n        f""Deprecated cache directory found ({DEPRECATED_CACHE_DIRECTORY}).  ""\n        f""Please remove this directory from your system to free up space.""\n    )\n\n\ndef url_to_filename(url: str, etag: str = None) -> str:\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    """"""\n    url_bytes = url.encode(""utf-8"")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(""utf-8"")\n        etag_hash = sha256(etag_bytes)\n        filename += ""."" + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename: str, cache_dir: str = None) -> Tuple[str, str]:\n    """"""\n    Return the url and etag (which may be `None`) stored for `filename`.\n    Raise `FileNotFoundError` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = CACHE_DIRECTORY\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise FileNotFoundError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + "".json""\n    if not os.path.exists(meta_path):\n        raise FileNotFoundError(""file {} not found"".format(meta_path))\n\n    with open(meta_path) as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[""url""]\n    etag = metadata[""etag""]\n\n    return url, etag\n\n\ndef cached_path(url_or_filename: Union[str, Path], cache_dir: str = None) -> str:\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    """"""\n    if cache_dir is None:\n        cache_dir = CACHE_DIRECTORY\n    if isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n\n    url_or_filename = os.path.expanduser(url_or_filename)\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (""http"", ""https"", ""s3""):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == """":\n        # File, but it doesn\'t exist.\n        raise FileNotFoundError(""file {} not found"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\n\n\ndef is_url_or_existing_file(url_or_filename: Union[str, Path, None]) -> bool:\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine check if it\'s url or an existing file path.\n    """"""\n    if url_or_filename is None:\n        return False\n    url_or_filename = os.path.expanduser(str(url_or_filename))\n    parsed = urlparse(url_or_filename)\n    return parsed.scheme in (""http"", ""https"", ""s3"") or os.path.exists(url_or_filename)\n\n\ndef _split_s3_path(url: str) -> Tuple[str, str]:\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(""bad s3 path {}"".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef _s3_request(func: Callable):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url: str, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise FileNotFoundError(""file {} not found"".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\ndef _get_s3_resource():\n    session = boto3.session.Session()\n    if session.get_credentials() is None:\n        # Use unsigned requests.\n        s3_resource = session.resource(\n            ""s3"", config=botocore.client.Config(signature_version=botocore.UNSIGNED)\n        )\n    else:\n        s3_resource = session.resource(""s3"")\n    return s3_resource\n\n\n@_s3_request\ndef _s3_etag(url: str) -> Optional[str]:\n    """"""Check ETag on S3 object.""""""\n    s3_resource = _get_s3_resource()\n    bucket_name, s3_path = _split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@_s3_request\ndef _s3_get(url: str, temp_file: IO) -> None:\n    """"""Pull a file directly from S3.""""""\n    s3_resource = _get_s3_resource()\n    bucket_name, s3_path = _split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef _session_with_backoff() -> requests.Session:\n    """"""\n    We ran into an issue where http requests to s3 were timing out,\n    possibly because we were making too many requests too quickly.\n    This helper function returns a requests session that has retry-with-backoff\n    built in. See\n    <https://stackoverflow.com/questions/23267409/how-to-implement-retry-mechanism-into-python-requests-library>.\n    """"""\n    session = requests.Session()\n    retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504])\n    session.mount(""http://"", HTTPAdapter(max_retries=retries))\n    session.mount(""https://"", HTTPAdapter(max_retries=retries))\n\n    return session\n\n\ndef _http_etag(url: str) -> Optional[str]:\n    with _session_with_backoff() as session:\n        response = session.head(url, allow_redirects=True)\n    if response.status_code != 200:\n        raise IOError(\n            ""HEAD request failed for url {} with status code {}"".format(url, response.status_code)\n        )\n    return response.headers.get(""ETag"")\n\n\ndef _http_get(url: str, temp_file: IO) -> None:\n    with _session_with_backoff() as session:\n        req = session.get(url, stream=True)\n        content_length = req.headers.get(""Content-Length"")\n        total = int(content_length) if content_length is not None else None\n        progress = Tqdm.tqdm(unit=""B"", total=total)\n        for chunk in req.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                progress.update(len(chunk))\n                temp_file.write(chunk)\n        progress.close()\n\n\ndef _find_latest_cached(url: str, cache_dir: str) -> Optional[str]:\n    filename = url_to_filename(url)\n    cache_path = os.path.join(cache_dir, filename)\n    candidates: List[Tuple[str, float]] = []\n    for path in glob.glob(cache_path + ""*""):\n        if path.endswith("".json""):\n            continue\n        mtime = os.path.getmtime(path)\n        candidates.append((path, mtime))\n    # Sort candidates by modification time, neweste first.\n    candidates.sort(key=lambda x: x[1], reverse=True)\n    if candidates:\n        return candidates[0][0]\n    return None\n\n\n# TODO(joelgrus): do we want to do checksums or anything like that?\ndef get_from_cache(url: str, cache_dir: str = None) -> str:\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = CACHE_DIRECTORY\n\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Get eTag to add to filename, if it exists.\n    try:\n        if url.startswith(""s3://""):\n            etag = _s3_etag(url)\n        else:\n            etag = _http_etag(url)\n    except (ConnectionError, EndpointConnectionError):\n        # We might be offline, in which case we don\'t want to throw an error\n        # just yet. Instead, we\'ll try to use the latest cached version of the\n        # target resource, if it exists. We\'ll only throw an exception if we\n        # haven\'t cached the resource at all yet.\n        logger.warning(\n            ""Connection error occured while trying to fetch ETag for %s. ""\n            ""Will attempt to use latest cached version of resource"",\n            url,\n        )\n        latest_cached = _find_latest_cached(url, cache_dir)\n        if latest_cached:\n            logger.info(\n                ""ETag request failed with connection error, using latest cached ""\n                ""version of %s: %s"",\n                url,\n                latest_cached,\n            )\n            return latest_cached\n        else:\n            logger.error(\n                ""Connection failed while trying to fetch ETag, ""\n                ""and no cached version of %s could be found"",\n                url,\n            )\n            raise\n\n    filename = url_to_filename(url, etag)\n\n    # Get cache path to put the file.\n    cache_path = os.path.join(cache_dir, filename)\n\n    # Multiple processes may be trying to cache the same file at once, so we need\n    # to be a little careful to avoid race conditions. We do this using a lock file.\n    # Only one process can own this lock file at a time, and a process will block\n    # on the call to `lock.acquire()` until the process currently holding the lock\n    # releases it.\n    logger.info(""checking cache for %s at %s"", url, cache_path)\n    logger.info(""waiting to acquire lock on %s"", cache_path)\n    with FileLock(cache_path + "".lock""):\n        if os.path.exists(cache_path):\n            logger.info(""cache of %s is up-to-date"", url)\n        else:\n            # Download to temporary file, then copy to cache dir once finished.\n            # Otherwise you get corrupt cache entries if the download gets interrupted.\n            with tempfile.NamedTemporaryFile() as temp_file:\n                logger.info(""%s not found in cache, downloading to %s"", url, temp_file.name)\n\n                # GET file object\n                if url.startswith(""s3://""):\n                    _s3_get(url, temp_file)\n                else:\n                    _http_get(url, temp_file)\n\n                # we are copying the file before closing it, so flush to avoid truncation\n                temp_file.flush()\n                # shutil.copyfileobj() starts at the current position, so go to the start\n                temp_file.seek(0)\n\n                logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n                with open(cache_path, ""wb"") as cache_file:\n                    shutil.copyfileobj(temp_file, cache_file)  # type: ignore\n\n                logger.info(""creating metadata file for %s"", cache_path)\n                meta = {""url"": url, ""etag"": etag}\n                meta_path = cache_path + "".json""\n                with open(meta_path, ""w"") as meta_file:\n                    json.dump(meta, meta_file)\n\n                logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path\n\n\ndef read_set_from_file(filename: str) -> Set[str]:\n    """"""\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    """"""\n    collection = set()\n    with open(filename, ""r"") as file_:\n        for line in file_:\n            collection.add(line.rstrip())\n    return collection\n\n\ndef get_file_extension(path: str, dot=True, lower: bool = True):\n    ext = os.path.splitext(path)[1]\n    ext = ext if dot else ext[1:]\n    return ext.lower() if lower else ext\n\n\ndef open_compressed(\n    filename: Union[str, Path], mode: str = ""rt"", encoding: Optional[str] = ""UTF-8"", **kwargs\n):\n    if isinstance(filename, Path):\n        filename = str(filename)\n    open_fn: Callable = open\n\n    if filename.endswith("".gz""):\n        import gzip\n\n        open_fn = gzip.open\n    elif filename.endswith("".bz2""):\n        import bz2\n\n        open_fn = bz2.open\n    return open_fn(filename, mode=mode, encoding=encoding, **kwargs)\n'"
allennlp/common/from_params.py,0,"b'import collections.abc\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    Callable,\n    cast,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\nimport inspect\nimport logging\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.lazy import Lazy\nfrom allennlp.common.params import Params\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(""T"", bound=""FromParams"")\n\n# If a function parameter has no default value specified,\n# this is what the inspect module returns.\n_NO_DEFAULT = inspect.Parameter.empty\n\n\ndef takes_arg(obj, arg: str) -> bool:\n    """"""\n    Checks whether the provided obj takes a certain arg.\n    If it\'s a class, we\'re really checking whether its constructor does.\n    If it\'s a function or method, we\'re checking the object itself.\n    Otherwise, we raise an error.\n    """"""\n    if inspect.isclass(obj):\n        signature = inspect.signature(obj.__init__)\n    elif inspect.ismethod(obj) or inspect.isfunction(obj):\n        signature = inspect.signature(obj)\n    else:\n        raise ConfigurationError(f""object {obj} is not callable"")\n    return arg in signature.parameters\n\n\ndef takes_kwargs(obj) -> bool:\n    """"""\n    Checks whether a provided object takes in any positional arguments.\n    Similar to takes_arg, we do this for both the __init__ function of\n    the class or a function / method\n    Otherwise, we raise an error\n    """"""\n    if inspect.isclass(obj):\n        signature = inspect.signature(obj.__init__)\n    elif inspect.ismethod(obj) or inspect.isfunction(obj):\n        signature = inspect.signature(obj)\n    else:\n        raise ConfigurationError(f""object {obj} is not callable"")\n    return any(\n        p.kind == inspect.Parameter.VAR_KEYWORD  # type: ignore\n        for p in signature.parameters.values()\n    )\n\n\ndef can_construct_from_params(type_: Type) -> bool:\n    if type_ in [str, int, float, bool]:\n        return True\n    origin = getattr(type_, ""__origin__"", None)\n    if origin == Lazy:\n        return True\n    elif origin:\n        if hasattr(type_, ""from_params""):\n            return True\n        args = getattr(type_, ""__args__"")\n        return all(can_construct_from_params(arg) for arg in args)\n\n    return hasattr(type_, ""from_params"")\n\n\ndef is_base_registrable(cls) -> bool:\n    """"""\n    Checks whether this is a class that directly inherits from Registrable, or is a subclass of such\n    a class.\n    """"""\n    from allennlp.common.registrable import Registrable  # import here to avoid circular imports\n\n    if not issubclass(cls, Registrable):\n        return False\n    method_resolution_order = inspect.getmro(cls)[1:]\n    for base_class in method_resolution_order:\n        if issubclass(base_class, Registrable) and base_class is not Registrable:\n            return False\n    return True\n\n\ndef remove_optional(annotation: type):\n    """"""\n    Optional[X] annotations are actually represented as Union[X, NoneType].\n    For our purposes, the ""Optional"" part is not interesting, so here we\n    throw it away.\n    """"""\n    origin = getattr(annotation, ""__origin__"", None)\n    args = getattr(annotation, ""__args__"", ())\n    if origin == Union and len(args) == 2 and args[1] == type(None):  # noqa\n        return args[0]\n    else:\n        return annotation\n\n\ndef infer_params(cls: Type[T], constructor: Callable[..., T] = None):\n    if constructor is None:\n        constructor = cls.__init__\n\n    signature = inspect.signature(constructor)\n    parameters = dict(signature.parameters)\n\n    has_kwargs = False\n    for param in parameters.values():\n        if param.kind == param.VAR_KEYWORD:\n            has_kwargs = True\n\n    if not has_kwargs:\n        return parameters\n\n    # ""mro"" is ""method resolution order"".  The first one is the current class, the next is the\n    # first superclass, and so on.  We take the first superclass we find that inherits from\n    # FromParams.\n    super_class = None\n    for super_class_candidate in cls.mro()[1:]:\n        if issubclass(super_class_candidate, FromParams):\n            super_class = super_class_candidate\n            break\n    if not super_class:\n        raise RuntimeError(""found a kwargs parameter with no inspectable super class"")\n    super_parameters = infer_params(super_class)\n\n    return {**super_parameters, **parameters}  # Subclass parameters overwrite superclass ones\n\n\ndef create_kwargs(\n    constructor: Callable[..., T], cls: Type[T], params: Params, **extras\n) -> Dict[str, Any]:\n    """"""\n    Given some class, a `Params` object, and potentially other keyword arguments,\n    create a dict of keyword args suitable for passing to the class\'s constructor.\n\n    The function does this by finding the class\'s constructor, matching the constructor\n    arguments to entries in the `params` object, and instantiating values for the parameters\n    using the type annotation and possibly a from_params method.\n\n    Any values that are provided in the `extras` will just be used as is.\n    For instance, you might provide an existing `Vocabulary` this way.\n    """"""\n    # Get the signature of the constructor.\n\n    kwargs: Dict[str, Any] = {}\n\n    parameters = infer_params(cls, constructor)\n\n    # Iterate over all the constructor parameters and their annotations.\n    for param_name, param in parameters.items():\n        # Skip ""self"". You\'re not *required* to call the first parameter ""self"",\n        # so in theory this logic is fragile, but if you don\'t call the self parameter\n        # ""self"" you kind of deserve what happens.\n        if param_name == ""self"":\n            continue\n        # Also skip **kwargs parameters; we handled them above.\n        if param.kind == param.VAR_KEYWORD:\n            continue\n\n        # If the annotation is a compound type like typing.Dict[str, int],\n        # it will have an __origin__ field indicating `typing.Dict`\n        # and an __args__ field indicating `(str, int)`. We capture both.\n        annotation = remove_optional(param.annotation)\n\n        constructed_arg = pop_and_construct_arg(\n            cls.__name__, param_name, annotation, param.default, params, **extras\n        )\n\n        # If we just ended up constructing the default value for the parameter, we can just omit it.\n        # Leaving it in can cause issues with **kwargs in some corner cases, where you might end up\n        # with multiple values for a single parameter (e.g., the default value gives you lazy=False\n        # for a dataset reader inside **kwargs, but a particular dataset reader actually hard-codes\n        # lazy=True - the superclass sees both lazy=True and lazy=False in its constructor).\n        if constructed_arg is not param.default:\n            kwargs[param_name] = constructed_arg\n\n    params.assert_empty(cls.__name__)\n    return kwargs\n\n\ndef create_extras(cls: Type[T], extras: Dict[str, Any]) -> Dict[str, Any]:\n    """"""\n    Given a dictionary of extra arguments, returns a dictionary of\n    kwargs that actually are a part of the signature of the cls.from_params\n    (or cls) method.\n    """"""\n    subextras: Dict[str, Any] = {}\n    if hasattr(cls, ""from_params""):\n        from_params_method = cls.from_params  # type: ignore\n    else:\n        # In some rare cases, we get a registered subclass that does _not_ have a\n        # from_params method (this happens with Activations, for instance, where we\n        # register pytorch modules directly).  This is a bit of a hack to make those work,\n        # instead of adding a `from_params` method for them somehow. Then the extras\n        # in the class constructor are what we are looking for, to pass on.\n        from_params_method = cls\n    if takes_kwargs(from_params_method):\n        # If annotation.params accepts **kwargs, we need to pass them all along.\n        # For example, `BasicTextFieldEmbedder.from_params` requires a Vocabulary\n        # object, but `TextFieldEmbedder.from_params` does not.\n        subextras = extras\n    else:\n        # Otherwise, only supply the ones that are actual args; any additional ones\n        # will cause a TypeError.\n        subextras = {k: v for k, v in extras.items() if takes_arg(from_params_method, k)}\n    return subextras\n\n\ndef pop_and_construct_arg(\n    class_name: str, argument_name: str, annotation: Type, default: Any, params: Params, **extras\n) -> Any:\n    """"""\n    Does the work of actually constructing an individual argument for\n    [`create_kwargs`](./#create_kwargs).\n\n    Here we\'re in the inner loop of iterating over the parameters to a particular constructor,\n    trying to construct just one of them.  The information we get for that parameter is its name,\n    its type annotation, and its default value; we also get the full set of `Params` for\n    constructing the object (which we may mutate), and any `extras` that the constructor might\n    need.\n\n    We take the type annotation and default value here separately, instead of using an\n    `inspect.Parameter` object directly, so that we can handle `Union` types using recursion on\n    this method, trying the different annotation types in the union in turn.\n    """"""\n    from allennlp.models.archival import load_archive  # import here to avoid circular imports\n\n    # We used `argument_name` as the method argument to avoid conflicts with \'name\' being a key in\n    # `extras`, which isn\'t _that_ unlikely.  Now that we are inside the method, we can switch back\n    # to using `name`.\n    name = argument_name\n\n    # Some constructors expect extra non-parameter items, e.g. vocab: Vocabulary.\n    # We check the provided `extras` for these and just use them if they exist.\n    if name in extras:\n        if name not in params:\n            return extras[name]\n        else:\n            logger.warning(\n                f""Parameter {name} for class {class_name} was found in both ""\n                ""**extras and in params. Using the specification found in params, ""\n                ""but you probably put a key in a config file that you didn\'t need, ""\n                ""and if it is different from what we get from **extras, you might ""\n                ""get unexpected behavior.""\n            )\n    # Next case is when argument should be loaded from pretrained archive.\n    elif (\n        name in params\n        and isinstance(params.get(name), Params)\n        and ""_pretrained"" in params.get(name)\n    ):\n        load_module_params = params.pop(name).pop(""_pretrained"")\n        archive_file = load_module_params.pop(""archive_file"")\n        module_path = load_module_params.pop(""module_path"")\n        freeze = load_module_params.pop(""freeze"", True)\n        archive = load_archive(archive_file)\n        result = archive.extract_module(module_path, freeze)\n        if not isinstance(result, annotation):\n            raise ConfigurationError(\n                f""The module from model at {archive_file} at path {module_path} ""\n                f""was expected of type {annotation} but is of type {type(result)}""\n            )\n        return result\n\n    popped_params = params.pop(name, default) if default != _NO_DEFAULT else params.pop(name)\n    if popped_params is None:\n        origin = getattr(annotation, ""__origin__"", None)\n        if origin == Lazy:\n            return Lazy(lambda **kwargs: None)\n        return None\n\n    return construct_arg(class_name, name, popped_params, annotation, default, **extras)\n\n\ndef construct_arg(\n    class_name: str,\n    argument_name: str,\n    popped_params: Params,\n    annotation: Type,\n    default: Any,\n    **extras,\n) -> Any:\n    """"""\n    The first two parameters here are only used for logging if we encounter an error.\n    """"""\n    origin = getattr(annotation, ""__origin__"", None)\n    args = getattr(annotation, ""__args__"", [])\n\n    # The parameter is optional if its default value is not the ""no default"" sentinel.\n    optional = default != _NO_DEFAULT\n\n    if hasattr(annotation, ""from_params""):\n        if popped_params is default:\n            return default\n        elif popped_params is not None:\n            # Our params have an entry for this, so we use that.\n\n            subextras = create_extras(annotation, extras)\n\n            # In some cases we allow a string instead of a param dict, so\n            # we need to handle that case separately.\n            if isinstance(popped_params, str):\n                popped_params = Params({""type"": popped_params})\n            elif isinstance(popped_params, dict):\n                popped_params = Params(popped_params)\n            return annotation.from_params(params=popped_params, **subextras)\n        elif not optional:\n            # Not optional and not supplied, that\'s an error!\n            raise ConfigurationError(f""expected key {argument_name} for {class_name}"")\n        else:\n            return default\n\n    # If the parameter type is a Python primitive, just pop it off\n    # using the correct casting pop_xyz operation.\n    elif annotation in {int, bool}:\n        if type(popped_params) in {int, bool}:\n            return annotation(popped_params)\n        else:\n            raise TypeError(f""Expected {argument_name} to be a {annotation.__name__}."")\n    elif annotation == str:\n        # Strings are special because we allow casting from Path to str.\n        if type(popped_params) == str or isinstance(popped_params, Path):\n            return str(popped_params)  # type: ignore\n        else:\n            raise TypeError(f""Expected {argument_name} to be a string."")\n    elif annotation == float:\n        # Floats are special because in Python, you can put an int wherever you can put a float.\n        # https://mypy.readthedocs.io/en/stable/duck_type_compatibility.html\n        if type(popped_params) in {int, float}:\n            return popped_params\n        else:\n            raise TypeError(f""Expected {argument_name} to be numeric."")\n\n    # This is special logic for handling types like Dict[str, TokenIndexer],\n    # List[TokenIndexer], Tuple[TokenIndexer, Tokenizer], and Set[TokenIndexer],\n    # which it creates by instantiating each value from_params and returning the resulting structure.\n    elif (\n        origin in {collections.abc.Mapping, Mapping, Dict, dict}\n        and len(args) == 2\n        and can_construct_from_params(args[-1])\n    ):\n        value_cls = annotation.__args__[-1]\n\n        value_dict = {}\n\n        for key, value_params in popped_params.items():\n            value_dict[key] = construct_arg(\n                str(value_cls),\n                argument_name + ""."" + key,\n                value_params,\n                value_cls,\n                _NO_DEFAULT,\n                **extras,\n            )\n\n        return value_dict\n\n    elif origin in (Tuple, tuple) and all(can_construct_from_params(arg) for arg in args):\n        value_list = []\n\n        for i, (value_cls, value_params) in enumerate(zip(annotation.__args__, popped_params)):\n            value = construct_arg(\n                str(value_cls),\n                argument_name + f"".{i}"",\n                value_params,\n                value_cls,\n                _NO_DEFAULT,\n                **extras,\n            )\n            value_list.append(value)\n\n        return tuple(value_list)\n\n    elif origin in (Set, set) and len(args) == 1 and can_construct_from_params(args[0]):\n        value_cls = annotation.__args__[0]\n\n        value_set = set()\n\n        for i, value_params in enumerate(popped_params):\n            value = construct_arg(\n                str(value_cls),\n                argument_name + f"".{i}"",\n                value_params,\n                value_cls,\n                _NO_DEFAULT,\n                **extras,\n            )\n            value_set.add(value)\n\n        return value_set\n\n    elif origin == Union:\n        # Storing this so we can recover it later if we need to.\n        backup_params = deepcopy(popped_params)\n\n        # We\'ll try each of the given types in the union sequentially, returning the first one that\n        # succeeds.\n        for arg_annotation in args:\n            try:\n                return construct_arg(\n                    str(arg_annotation),\n                    argument_name,\n                    popped_params,\n                    arg_annotation,\n                    default,\n                    **extras,\n                )\n            except (ValueError, TypeError, ConfigurationError, AttributeError):\n                # Our attempt to construct the argument may have modified popped_params, so we\n                # restore it here.\n                popped_params = deepcopy(backup_params)\n\n        # If none of them succeeded, we crash.\n        raise ConfigurationError(\n            f""Failed to construct argument {argument_name} with type {annotation}""\n        )\n    elif origin == Lazy:\n        if popped_params is default:\n            return Lazy(lambda **kwargs: default)\n        value_cls = args[0]\n        subextras = create_extras(value_cls, extras)\n\n        def constructor(**kwargs):\n            # If there are duplicate keys between subextras and kwargs, this will overwrite the ones\n            # in subextras with what\'s in kwargs.  If an argument shows up twice, we should take it\n            # from what\'s passed to Lazy.construct() instead of what we got from create_extras().\n            # Almost certainly these will be identical objects, anyway.\n            # We do this by constructing a new dictionary, instead of mutating subextras, just in\n            # case this constructor is called multiple times.\n            constructor_extras = {**subextras, **kwargs}\n            return value_cls.from_params(params=deepcopy(popped_params), **constructor_extras)\n\n        return Lazy(constructor)  # type: ignore\n\n    # For any other kind of iterable, we will just assume that a list is good enough, and treat\n    # it the same as List. This condition needs to be at the end, so we don\'t catch other kinds\n    # of Iterables with this branch.\n    elif (\n        origin in {collections.abc.Iterable, Iterable, List, list}\n        and len(args) == 1\n        and can_construct_from_params(args[0])\n    ):\n        value_cls = annotation.__args__[0]\n\n        value_list = []\n\n        for i, value_params in enumerate(popped_params):\n            value = construct_arg(\n                str(value_cls),\n                argument_name + f"".{i}"",\n                value_params,\n                value_cls,\n                _NO_DEFAULT,\n                **extras,\n            )\n            value_list.append(value)\n\n        return value_list\n\n    else:\n        # Pass it on as is and hope for the best.   \xc2\xaf\\_(\xe3\x83\x84)_/\xc2\xaf\n        if isinstance(popped_params, Params):\n            return popped_params.as_dict(quiet=True)\n        return popped_params\n\n\nclass FromParams:\n    """"""\n    Mixin to give a from_params method to classes. We create a distinct base class for this\n    because sometimes we want non-Registrable classes to be instantiatable from_params.\n    """"""\n\n    @classmethod\n    def from_params(\n        cls: Type[T],\n        params: Params,\n        constructor_to_call: Callable[..., T] = None,\n        constructor_to_inspect: Callable[..., T] = None,\n        **extras,\n    ) -> T:\n        """"""\n        This is the automatic implementation of `from_params`. Any class that subclasses\n        `FromParams` (or `Registrable`, which itself subclasses `FromParams`) gets this\n        implementation for free.  If you want your class to be instantiated from params in the\n        ""obvious"" way -- pop off parameters and hand them to your constructor with the same names --\n        this provides that functionality.\n\n        If you need more complex logic in your from `from_params` method, you\'ll have to implement\n        your own method that overrides this one.\n\n        The `constructor_to_call` and `constructor_to_inspect` arguments deal with a bit of\n        redirection that we do.  We allow you to register particular `@classmethods` on a class as\n        the constructor to use for a registered name.  This lets you, e.g., have a single\n        `Vocabulary` class that can be constructed in two different ways, with different names\n        registered to each constructor.  In order to handle this, we need to know not just the class\n        we\'re trying to construct (`cls`), but also what method we should inspect to find its\n        arguments (`constructor_to_inspect`), and what method to call when we\'re done constructing\n        arguments (`constructor_to_call`).  These two methods are the same when you\'ve used a\n        `@classmethod` as your constructor, but they are `different` when you use the default\n        constructor (because you inspect `__init__`, but call `cls()`).\n        """"""\n\n        from allennlp.common.registrable import Registrable  # import here to avoid circular imports\n\n        logger.debug(\n            f""instantiating class {cls} from params {getattr(params, \'params\', params)} ""\n            f""and extras {set(extras.keys())}""\n        )\n\n        if params is None:\n            return None\n\n        if isinstance(params, str):\n            params = Params({""type"": params})\n\n        if not isinstance(params, Params):\n            raise ConfigurationError(\n                ""from_params was passed a `params` object that was not a `Params`. This probably ""\n                ""indicates malformed parameters in a configuration file, where something that ""\n                ""should have been a dictionary was actually a list, or something else. ""\n                f""This happened when constructing an object of type {cls}.""\n            )\n\n        registered_subclasses = Registrable._registry.get(cls)\n\n        if is_base_registrable(cls) and registered_subclasses is None:\n            # NOTE(mattg): There are some potential corner cases in this logic if you have nested\n            # Registrable types.  We don\'t currently have any of those, but if we ever get them,\n            # adding some logic to check `constructor_to_call` should solve the issue.  Not\n            # bothering to add that unnecessary complexity for now.\n            raise ConfigurationError(\n                ""Tried to construct an abstract Registrable base class that has no registered ""\n                ""concrete types. This might mean that you need to use --include-package to get ""\n                ""your concrete classes actually registered.""\n            )\n\n        if registered_subclasses is not None and not constructor_to_call:\n            # We know `cls` inherits from Registrable, so we\'ll use a cast to make mypy happy.\n\n            as_registrable = cast(Type[Registrable], cls)\n            default_to_first_choice = as_registrable.default_implementation is not None\n            choice = params.pop_choice(\n                ""type"",\n                choices=as_registrable.list_available(),\n                default_to_first_choice=default_to_first_choice,\n            )\n            subclass, constructor_name = as_registrable.resolve_class_name(choice)\n            # See the docstring for an explanation of what\'s going on here.\n            if not constructor_name:\n                constructor_to_inspect = subclass.__init__\n                constructor_to_call = subclass  # type: ignore\n            else:\n                constructor_to_inspect = getattr(subclass, constructor_name)\n                constructor_to_call = constructor_to_inspect\n\n            if hasattr(subclass, ""from_params""):\n                # We want to call subclass.from_params.\n                extras = create_extras(subclass, extras)\n                # mypy can\'t follow the typing redirection that we do, so we explicitly cast here.\n                retyped_subclass = cast(Type[T], subclass)\n                return retyped_subclass.from_params(\n                    params=params,\n                    constructor_to_call=constructor_to_call,\n                    constructor_to_inspect=constructor_to_inspect,\n                    **extras,\n                )\n            else:\n                # In some rare cases, we get a registered subclass that does _not_ have a\n                # from_params method (this happens with Activations, for instance, where we\n                # register pytorch modules directly).  This is a bit of a hack to make those work,\n                # instead of adding a `from_params` method for them somehow.  We just trust that\n                # you\'ve done the right thing in passing your parameters, and nothing else needs to\n                # be recursively constructed.\n                extras = create_extras(subclass, extras)\n                constructor_args = {**params, **extras}\n                return subclass(**constructor_args)  # type: ignore\n        else:\n            # This is not a base class, so convert our params and extras into a dict of kwargs.\n\n            # See the docstring for an explanation of what\'s going on here.\n            if not constructor_to_inspect:\n                constructor_to_inspect = cls.__init__\n            if not constructor_to_call:\n                constructor_to_call = cls\n\n            if constructor_to_inspect == object.__init__:\n                # This class does not have an explicit constructor, so don\'t give it any kwargs.\n                # Without this logic, create_kwargs will look at object.__init__ and see that\n                # it takes *args and **kwargs and look for those.\n                kwargs: Dict[str, Any] = {}\n                params.assert_empty(cls.__name__)\n            else:\n                # This class has a constructor, so create kwargs for it.\n                kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)\n\n            return constructor_to_call(**kwargs)  # type: ignore\n'"
allennlp/common/lazy.py,0,"b'from typing import Callable, Generic, TypeVar, Optional\n\nT = TypeVar(""T"")\n\n\nclass Lazy(Generic[T]):\n    """"""\n    This class is for use when constructing objects using `FromParams`, when an argument to a\n    constructor has a _sequential dependency_ with another argument to the same constructor.  For\n    example, in a `Trainer` class you might want to take a `Model` and an `Optimizer` as arguments,\n    but the `Optimizer` needs to be constructed using the parameters from the `Model`.  You can give\n    the type annotation `Lazy[Optimizer]` to the optimizer argument, then inside the constructor\n    call `optimizer.construct(parameters=model.parameters)`.\n\n    This is only recommended for use when you have registered a `@classmethod` as the constructor\n    for your class, instead of using `__init__`.  Having a `Lazy[]` type annotation on an argument\n    to an `__init__` method makes your class completely dependent on being constructed using the\n    `FromParams` pipeline, which is not a good idea.\n\n    The actual implementation here is incredibly simple; the logic that handles the lazy\n    construction is actually found in `FromParams`, where we have a special case for a `Lazy` type\n    annotation.\n\n    !!! Warning\n        The way this class is used in from_params means that optional constructor arguments CANNOT\n        be compared to `None` _before_ it is constructed. See the example below for correct usage.\n\n    ```\n    @classmethod\n    def my_constructor(cls, some_object: Lazy[MyObject] = None) -> MyClass:\n        ...\n        # WRONG! some_object will never be None at this point, it will be\n        # a Lazy[] that returns None\n        obj = some_object or MyObjectDefault()\n        # CORRECT:\n        obj = some_object.construct(kwarg=kwarg) or MyObjectDefault()\n        ...\n    ```\n\n    """"""\n\n    def __init__(self, constructor: Callable[..., T]):\n        self._constructor = constructor\n\n    def construct(self, **kwargs) -> Optional[T]:\n        return self._constructor(**kwargs)\n'"
allennlp/common/logging.py,0,"b'import logging\nfrom logging import Filter\nimport os\nimport sys\nfrom typing import Optional\n\nfrom allennlp.common.tee import TeeHandler\nfrom allennlp.common.tqdm import Tqdm\n\n\nclass AllenNlpLogger(logging.Logger):\n    """"""\n    A custom subclass of \'logging.Logger\' that keeps a set of messages to\n    implement {debug,info,etc.}_once() methods.\n    """"""\n\n    def __init__(self, name):\n        super().__init__(name)\n        self._seen_msgs = set()\n\n    def debug_once(self, msg, *args, **kwargs):\n        if msg not in self._seen_msgs:\n            self.debug(msg, *args, **kwargs)\n            self._seen_msgs.add(msg)\n\n    def info_once(self, msg, *args, **kwargs):\n        if msg not in self._seen_msgs:\n            self.info(msg, *args, **kwargs)\n            self._seen_msgs.add(msg)\n\n    def warning_once(self, msg, *args, **kwargs):\n        if msg not in self._seen_msgs:\n            self.warning(msg, *args, **kwargs)\n            self._seen_msgs.add(msg)\n\n    def error_once(self, msg, *args, **kwargs):\n        if msg not in self._seen_msgs:\n            self.error(msg, *args, **kwargs)\n            self._seen_msgs.add(msg)\n\n    def critical_once(self, msg, *args, **kwargs):\n        if msg not in self._seen_msgs:\n            self.critical(msg, *args, **kwargs)\n            self._seen_msgs.add(msg)\n\n\nlogging.setLoggerClass(AllenNlpLogger)\n\n\nclass ErrorFilter(Filter):\n    """"""\n    Filters out everything that is at the ERROR level or higher. This is meant to be used\n    with a stdout handler when a stderr handler is also configured. That way ERROR\n    messages aren\'t duplicated.\n    """"""\n\n    def filter(self, record):\n        return record.levelno < logging.ERROR\n\n\nclass WorkerLogFilter(Filter):\n    def __init__(self, rank=-1):\n        super().__init__()\n        self._rank = rank\n\n    def filter(self, record):\n        if self._rank != -1:\n            record.msg = f""Rank {self._rank} | {record.msg}""\n        return True\n\n\ndef prepare_global_logging(\n    serialization_dir: str, file_friendly_logging: bool, rank: int = 0, world_size: int = 1\n) -> None:\n    # If we don\'t have a terminal as stdout,\n    # force tqdm to be nicer.\n    if not sys.stdout.isatty():\n        file_friendly_logging = True\n\n    Tqdm.set_slower_interval(file_friendly_logging)\n\n    stdout_file: str\n    stderr_file: str\n    worker_filter: Optional[WorkerLogFilter] = None\n    if world_size == 1:\n        # This case is not distributed training and hence will stick to the older\n        # log file names\n        stdout_file = os.path.join(serialization_dir, ""stdout.log"")\n        stderr_file = os.path.join(serialization_dir, ""stderr.log"")\n    else:\n        # Create log files with worker ids\n        stdout_file = os.path.join(serialization_dir, f""stdout_worker{rank}.log"")\n        stderr_file = os.path.join(serialization_dir, f""stderr_worker{rank}.log"")\n\n        # This adds the worker\'s rank to messages being logged to files.\n        # This will help when combining multiple worker log files using `less` command.\n        worker_filter = WorkerLogFilter(rank)\n\n    # Patch stdout/err.\n    stdout_patch = TeeHandler(\n        stdout_file,\n        sys.stdout,\n        file_friendly_terminal_output=file_friendly_logging,\n        silent=rank != 0,  # don\'t print to terminal from non-master workers.\n    )\n    sys.stdout = stdout_patch  # type: ignore\n    stderr_patch = TeeHandler(\n        stderr_file,\n        sys.stderr,\n        file_friendly_terminal_output=file_friendly_logging,\n        silent=rank != 0,  # don\'t print to terminal from non-master workers.\n    )\n    sys.stderr = stderr_patch  # type: ignore\n\n    # Handlers for stdout/err logging\n    output_handler = logging.StreamHandler(sys.stdout)\n    error_handler = logging.StreamHandler(sys.stderr)\n\n    if worker_filter is not None:\n        output_handler.addFilter(worker_filter)\n        error_handler.addFilter(worker_filter)\n\n    root_logger = logging.getLogger()\n\n    # Remove the already set stream handler in root logger.\n    # Not doing this will result in duplicate log messages\n    # printed in the console\n    if len(root_logger.handlers) > 0:\n        for handler in root_logger.handlers:\n            root_logger.removeHandler(handler)\n\n    formatter = logging.Formatter(""%(asctime)s - %(levelname)s - %(name)s - %(message)s"")\n    output_handler.setFormatter(formatter)\n    error_handler.setFormatter(formatter)\n\n    if os.environ.get(""ALLENNLP_DEBUG""):\n        LEVEL = logging.DEBUG\n    else:\n        level_name = os.environ.get(""ALLENNLP_LOG_LEVEL"")\n        LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n\n    output_handler.setLevel(LEVEL)\n    error_handler.setLevel(logging.ERROR)\n\n    # filter out everything at the ERROR or higher level for output stream\n    # so that error messages don\'t appear twice in the logs.\n    output_handler.addFilter(ErrorFilter())\n\n    root_logger.addHandler(output_handler)\n    root_logger.addHandler(error_handler)\n\n    root_logger.setLevel(LEVEL)\n'"
allennlp/common/params.py,0,"b'from typing import Any, Dict, List\nfrom collections.abc import MutableMapping\nfrom collections import OrderedDict\nimport copy\nimport json\nimport logging\nimport os\nimport zlib\n\nfrom overrides import overrides\n\n# _jsonnet doesn\'t work on Windows, so we have to use fakes.\ntry:\n    from _jsonnet import evaluate_file, evaluate_snippet\nexcept ImportError:\n\n    def evaluate_file(filename: str, **_kwargs) -> str:\n        logger.warning(\n            f""error loading _jsonnet (this is expected on Windows), treating {filename} as plain json""\n        )\n        with open(filename, ""r"") as evaluation_file:\n            return evaluation_file.read()\n\n    def evaluate_snippet(_filename: str, expr: str, **_kwargs) -> str:\n        logger.warning(\n            ""error loading _jsonnet (this is expected on Windows), treating snippet as plain json""\n        )\n        return expr\n\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.file_utils import cached_path\n\nlogger = logging.getLogger(__name__)\n\n\ndef infer_and_cast(value: Any):\n    """"""\n    In some cases we\'ll be feeding params dicts to functions we don\'t own;\n    for example, PyTorch optimizers. In that case we can\'t use `pop_int`\n    or similar to force casts (which means you can\'t specify `int` parameters\n    using environment variables). This function takes something that looks JSON-like\n    and recursively casts things that look like (bool, int, float) to (bool, int, float).\n    """"""\n\n    if isinstance(value, (int, float, bool)):\n        # Already one of our desired types, so leave as is.\n        return value\n    elif isinstance(value, list):\n        # Recursively call on each list element.\n        return [infer_and_cast(item) for item in value]\n    elif isinstance(value, dict):\n        # Recursively call on each dict value.\n        return {key: infer_and_cast(item) for key, item in value.items()}\n    elif isinstance(value, str):\n        # If it looks like a bool, make it a bool.\n        if value.lower() == ""true"":\n            return True\n        elif value.lower() == ""false"":\n            return False\n        else:\n            # See if it could be an int.\n            try:\n                return int(value)\n            except ValueError:\n                pass\n            # See if it could be a float.\n            try:\n                return float(value)\n            except ValueError:\n                # Just return it as a string.\n                return value\n    else:\n        raise ValueError(f""cannot infer type of {value}"")\n\n\ndef _is_encodable(value: str) -> bool:\n    """"""\n    We need to filter out environment variables that can\'t\n    be unicode-encoded to avoid a ""surrogates not allowed""\n    error in jsonnet.\n    """"""\n    # Idiomatically you\'d like to not check the != b""""\n    # but mypy doesn\'t like that.\n    return (value == """") or (value.encode(""utf-8"", ""ignore"") != b"""")\n\n\ndef _environment_variables() -> Dict[str, str]:\n    """"""\n    Wraps `os.environ` to filter out non-encodable values.\n    """"""\n    return {key: value for key, value in os.environ.items() if _is_encodable(value)}\n\n\ndef unflatten(flat_dict: Dict[str, Any]) -> Dict[str, Any]:\n    """"""\n    Given a ""flattened"" dict with compound keys, e.g.\n        {""a.b"": 0}\n    unflatten it:\n        {""a"": {""b"": 0}}\n    """"""\n    unflat: Dict[str, Any] = {}\n\n    for compound_key, value in flat_dict.items():\n        curr_dict = unflat\n        parts = compound_key.split(""."")\n        for key in parts[:-1]:\n            curr_value = curr_dict.get(key)\n            if key not in curr_dict:\n                curr_dict[key] = {}\n                curr_dict = curr_dict[key]\n            elif isinstance(curr_value, dict):\n                curr_dict = curr_value\n            else:\n                raise ConfigurationError(""flattened dictionary is invalid"")\n        if not isinstance(curr_dict, dict) or parts[-1] in curr_dict:\n            raise ConfigurationError(""flattened dictionary is invalid"")\n        curr_dict[parts[-1]] = value\n\n    return unflat\n\n\ndef with_fallback(preferred: Dict[str, Any], fallback: Dict[str, Any]) -> Dict[str, Any]:\n    """"""\n    Deep merge two dicts, preferring values from `preferred`.\n    """"""\n\n    def merge(preferred_value: Any, fallback_value: Any) -> Any:\n        if isinstance(preferred_value, dict) and isinstance(fallback_value, dict):\n            return with_fallback(preferred_value, fallback_value)\n        elif isinstance(preferred_value, dict) and isinstance(fallback_value, list):\n            # treat preferred_value as a sparse list, where each key is an index to be overridden\n            merged_list = fallback_value\n            for elem_key, preferred_element in preferred_value.items():\n                try:\n                    index = int(elem_key)\n                    merged_list[index] = merge(preferred_element, fallback_value[index])\n                except ValueError:\n                    raise ConfigurationError(\n                        ""could not merge dicts - the preferred dict contains ""\n                        f""invalid keys (key {elem_key} is not a valid list index)""\n                    )\n                except IndexError:\n                    raise ConfigurationError(\n                        ""could not merge dicts - the preferred dict contains ""\n                        f""invalid keys (key {index} is out of bounds)""\n                    )\n            return merged_list\n        else:\n            return copy.deepcopy(preferred_value)\n\n    preferred_keys = set(preferred.keys())\n    fallback_keys = set(fallback.keys())\n    common_keys = preferred_keys & fallback_keys\n\n    merged: Dict[str, Any] = {}\n\n    for key in preferred_keys - fallback_keys:\n        merged[key] = copy.deepcopy(preferred[key])\n    for key in fallback_keys - preferred_keys:\n        merged[key] = copy.deepcopy(fallback[key])\n\n    for key in common_keys:\n        preferred_value = preferred[key]\n        fallback_value = fallback[key]\n\n        merged[key] = merge(preferred_value, fallback_value)\n    return merged\n\n\ndef parse_overrides(serialized_overrides: str) -> Dict[str, Any]:\n    if serialized_overrides:\n        ext_vars = _environment_variables()\n\n        return unflatten(json.loads(evaluate_snippet("""", serialized_overrides, ext_vars=ext_vars)))\n    else:\n        return {}\n\n\ndef _is_dict_free(obj: Any) -> bool:\n    """"""\n    Returns False if obj is a dict, or if it\'s a list with an element that _has_dict.\n    """"""\n    if isinstance(obj, dict):\n        return False\n    elif isinstance(obj, list):\n        return all(_is_dict_free(item) for item in obj)\n    else:\n        return True\n\n\nclass Params(MutableMapping):\n    """"""\n    Represents a parameter dictionary with a history, and contains other functionality around\n    parameter passing and validation for AllenNLP.\n\n    There are currently two benefits of a `Params` object over a plain dictionary for parameter\n    passing:\n\n    1. We handle a few kinds of parameter validation, including making sure that parameters\n       representing discrete choices actually have acceptable values, and making sure no extra\n       parameters are passed.\n    2. We log all parameter reads, including default values.  This gives a more complete\n       specification of the actual parameters used than is given in a JSON file, because\n       those may not specify what default values were used, whereas this will log them.\n\n    !!! Consumption\n        The convention for using a `Params` object in AllenNLP is that you will consume the parameters\n        as you read them, so that there are none left when you\'ve read everything you expect.  This\n        lets us easily validate that you didn\'t pass in any `extra` parameters, just by making sure\n        that the parameter dictionary is empty.  You should do this when you\'re done handling\n        parameters, by calling `Params.assert_empty`.\n    """"""\n\n    # This allows us to check for the presence of ""None"" as a default argument,\n    # which we require because we make a distinction between passing a value of ""None""\n    # and passing no value to the default parameter of ""pop"".\n    DEFAULT = object()\n\n    def __init__(self, params: Dict[str, Any], history: str = """") -> None:\n        self.params = _replace_none(params)\n        self.history = history\n\n    @overrides\n    def pop(self, key: str, default: Any = DEFAULT, keep_as_dict: bool = False) -> Any:\n\n        """"""\n        Performs the functionality associated with dict.pop(key), along with checking for\n        returned dictionaries, replacing them with Param objects with an updated history\n        (unless keep_as_dict is True, in which case we leave them as dictionaries).\n\n        If `key` is not present in the dictionary, and no default was specified, we raise a\n        `ConfigurationError`, instead of the typical `KeyError`.\n        """"""\n        if default is self.DEFAULT:\n            try:\n                value = self.params.pop(key)\n            except KeyError:\n                msg = f\'key ""{key}"" is required\'\n                if self.history:\n                    msg += f\' at location ""{self.history}""\'\n                raise ConfigurationError(msg)\n        else:\n            value = self.params.pop(key, default)\n\n        if keep_as_dict or _is_dict_free(value):\n            logger.info(f""{self.history}{key} = {value}"")\n            return value\n        else:\n            return self._check_is_dict(key, value)\n\n    def pop_int(self, key: str, default: Any = DEFAULT) -> int:\n        """"""\n        Performs a pop and coerces to an int.\n        """"""\n        value = self.pop(key, default)\n        if value is None:\n            return None\n        else:\n            return int(value)\n\n    def pop_float(self, key: str, default: Any = DEFAULT) -> float:\n        """"""\n        Performs a pop and coerces to a float.\n        """"""\n        value = self.pop(key, default)\n        if value is None:\n            return None\n        else:\n            return float(value)\n\n    def pop_bool(self, key: str, default: Any = DEFAULT) -> bool:\n        """"""\n        Performs a pop and coerces to a bool.\n        """"""\n        value = self.pop(key, default)\n        if value is None:\n            return None\n        elif isinstance(value, bool):\n            return value\n        elif value == ""true"":\n            return True\n        elif value == ""false"":\n            return False\n        else:\n            raise ValueError(""Cannot convert variable to bool: "" + value)\n\n    @overrides\n    def get(self, key: str, default: Any = DEFAULT):\n        """"""\n        Performs the functionality associated with dict.get(key) but also checks for returned\n        dicts and returns a Params object in their place with an updated history.\n        """"""\n        default = None if default is self.DEFAULT else default\n        value = self.params.get(key, default)\n        return self._check_is_dict(key, value)\n\n    def pop_choice(\n        self,\n        key: str,\n        choices: List[Any],\n        default_to_first_choice: bool = False,\n        allow_class_names: bool = True,\n    ) -> Any:\n        """"""\n        Gets the value of `key` in the `params` dictionary, ensuring that the value is one of\n        the given choices. Note that this `pops` the key from params, modifying the dictionary,\n        consistent with how parameters are processed in this codebase.\n\n        # Parameters\n\n        key: `str`\n\n            Key to get the value from in the param dictionary\n\n        choices: `List[Any]`\n\n            A list of valid options for values corresponding to `key`.  For example, if you\'re\n            specifying the type of encoder to use for some part of your model, the choices might be\n            the list of encoder classes we know about and can instantiate.  If the value we find in\n            the param dictionary is not in `choices`, we raise a `ConfigurationError`, because\n            the user specified an invalid value in their parameter file.\n\n        default_to_first_choice: `bool`, optional (default = `False`)\n\n            If this is `True`, we allow the `key` to not be present in the parameter\n            dictionary.  If the key is not present, we will use the return as the value the first\n            choice in the `choices` list.  If this is `False`, we raise a\n            `ConfigurationError`, because specifying the `key` is required (e.g., you `have` to\n            specify your model class when running an experiment, but you can feel free to use\n            default settings for encoders if you want).\n\n        allow_class_names: `bool`, optional (default = `True`)\n\n            If this is `True`, then we allow unknown choices that look like fully-qualified class names.\n            This is to allow e.g. specifying a model type as my_library.my_model.MyModel\n            and importing it on the fly. Our check for ""looks like"" is extremely lenient\n            and consists of checking that the value contains a \'.\'.\n        """"""\n        default = choices[0] if default_to_first_choice else self.DEFAULT\n        value = self.pop(key, default)\n        ok_because_class_name = allow_class_names and ""."" in value\n        if value not in choices and not ok_because_class_name:\n            key_str = self.history + key\n            message = (\n                f""{value} not in acceptable choices for {key_str}: {choices}. ""\n                ""You should either use the --include-package flag to make sure the correct module ""\n                ""is loaded, or use a fully qualified class name in your config file like ""\n                """"""{""model"": ""my_module.models.MyModel""} to have it imported automatically.""""""\n            )\n            raise ConfigurationError(message)\n        return value\n\n    def as_dict(self, quiet: bool = False, infer_type_and_cast: bool = False):\n        """"""\n        Sometimes we need to just represent the parameters as a dict, for instance when we pass\n        them to PyTorch code.\n\n        # Parameters\n\n        quiet: `bool`, optional (default = `False`)\n\n            Whether to log the parameters before returning them as a dict.\n\n        infer_type_and_cast: `bool`, optional (default = `False`)\n\n            If True, we infer types and cast (e.g. things that look like floats to floats).\n        """"""\n        if infer_type_and_cast:\n            params_as_dict = infer_and_cast(self.params)\n        else:\n            params_as_dict = self.params\n\n        if quiet:\n            return params_as_dict\n\n        def log_recursively(parameters, history):\n            for key, value in parameters.items():\n                if isinstance(value, dict):\n                    new_local_history = history + key + "".""\n                    log_recursively(value, new_local_history)\n                else:\n                    logger.info(f""{history}{key} = {value}"")\n\n        logger.info(\n            ""Converting Params object to dict; logging of default ""\n            ""values will not occur when dictionary parameters are ""\n            ""used subsequently.""\n        )\n        logger.info(""CURRENTLY DEFINED PARAMETERS: "")\n        log_recursively(self.params, self.history)\n        return params_as_dict\n\n    def as_flat_dict(self):\n        """"""\n        Returns the parameters of a flat dictionary from keys to values.\n        Nested structure is collapsed with periods.\n        """"""\n        flat_params = {}\n\n        def recurse(parameters, path):\n            for key, value in parameters.items():\n                newpath = path + [key]\n                if isinstance(value, dict):\n                    recurse(value, newpath)\n                else:\n                    flat_params[""."".join(newpath)] = value\n\n        recurse(self.params, [])\n        return flat_params\n\n    def duplicate(self) -> ""Params"":\n        """"""\n        Uses `copy.deepcopy()` to create a duplicate (but fully distinct)\n        copy of these Params.\n        """"""\n        return copy.deepcopy(self)\n\n    def assert_empty(self, class_name: str):\n        """"""\n        Raises a `ConfigurationError` if `self.params` is not empty.  We take `class_name` as\n        an argument so that the error message gives some idea of where an error happened, if there\n        was one.  `class_name` should be the name of the `calling` class, the one that got extra\n        parameters (if there are any).\n        """"""\n        if self.params:\n            raise ConfigurationError(\n                ""Extra parameters passed to {}: {}"".format(class_name, self.params)\n            )\n\n    def __getitem__(self, key):\n        if key in self.params:\n            return self._check_is_dict(key, self.params[key])\n        else:\n            raise KeyError\n\n    def __setitem__(self, key, value):\n        self.params[key] = value\n\n    def __delitem__(self, key):\n        del self.params[key]\n\n    def __iter__(self):\n        return iter(self.params)\n\n    def __len__(self):\n        return len(self.params)\n\n    def _check_is_dict(self, new_history, value):\n        if isinstance(value, dict):\n            new_history = self.history + new_history + "".""\n            return Params(value, history=new_history)\n        if isinstance(value, list):\n            value = [self._check_is_dict(f""{new_history}.{i}"", v) for i, v in enumerate(value)]\n        return value\n\n    @classmethod\n    def from_file(\n        cls, params_file: str, params_overrides: str = """", ext_vars: dict = None\n    ) -> ""Params"":\n        """"""\n        Load a `Params` object from a configuration file.\n\n        # Parameters\n\n        params_file: `str`\n\n            The path to the configuration file to load.\n\n        params_overrides: `str`, optional\n\n            A dict of overrides that can be applied to final object.\n            e.g. {""model.embedding_dim"": 10}\n\n        ext_vars: `dict`, optional\n\n            Our config files are Jsonnet, which allows specifying external variables\n            for later substitution. Typically we substitute these using environment\n            variables; however, you can also specify them here, in which case they\n            take priority over environment variables.\n            e.g. {""HOME_DIR"": ""/Users/allennlp/home""}\n        """"""\n        if ext_vars is None:\n            ext_vars = {}\n\n        # redirect to cache, if necessary\n        params_file = cached_path(params_file)\n        ext_vars = {**_environment_variables(), **ext_vars}\n\n        file_dict = json.loads(evaluate_file(params_file, ext_vars=ext_vars))\n\n        overrides_dict = parse_overrides(params_overrides)\n        param_dict = with_fallback(preferred=overrides_dict, fallback=file_dict)\n\n        return cls(param_dict)\n\n    def to_file(self, params_file: str, preference_orders: List[List[str]] = None) -> None:\n        with open(params_file, ""w"") as handle:\n            json.dump(self.as_ordered_dict(preference_orders), handle, indent=4)\n\n    def as_ordered_dict(self, preference_orders: List[List[str]] = None) -> OrderedDict:\n        """"""\n        Returns Ordered Dict of Params from list of partial order preferences.\n\n        # Parameters\n\n        preference_orders: `List[List[str]]`, optional\n\n            `preference_orders` is list of partial preference orders. [""A"", ""B"", ""C""] means\n            ""A"" > ""B"" > ""C"". For multiple preference_orders first will be considered first.\n            Keys not found, will have last but alphabetical preference. Default Preferences:\n            `[[""dataset_reader"", ""iterator"", ""model"", ""train_data_path"", ""validation_data_path"",\n            ""test_data_path"", ""trainer"", ""vocabulary""], [""type""]]`\n        """"""\n        params_dict = self.as_dict(quiet=True)\n        if not preference_orders:\n            preference_orders = []\n            preference_orders.append(\n                [\n                    ""dataset_reader"",\n                    ""iterator"",\n                    ""model"",\n                    ""train_data_path"",\n                    ""validation_data_path"",\n                    ""test_data_path"",\n                    ""trainer"",\n                    ""vocabulary"",\n                ]\n            )\n            preference_orders.append([""type""])\n\n        def order_func(key):\n            # Makes a tuple to use for ordering.  The tuple is an index into each of the `preference_orders`,\n            # followed by the key itself.  This gives us integer sorting if you have a key in one of the\n            # `preference_orders`, followed by alphabetical ordering if not.\n            order_tuple = [\n                order.index(key) if key in order else len(order) for order in preference_orders\n            ]\n            return order_tuple + [key]\n\n        def order_dict(dictionary, order_func):\n            # Recursively orders dictionary according to scoring order_func\n            result = OrderedDict()\n            for key, val in sorted(dictionary.items(), key=lambda item: order_func(item[0])):\n                result[key] = order_dict(val, order_func) if isinstance(val, dict) else val\n            return result\n\n        return order_dict(params_dict, order_func)\n\n    def get_hash(self) -> str:\n        """"""\n        Returns a hash code representing the current state of this `Params` object.  We don\'t\n        want to implement `__hash__` because that has deeper python implications (and this is a\n        mutable object), but this will give you a representation of the current state.\n        We use `zlib.adler32` instead of Python\'s builtin `hash` because the random seed for the\n        latter is reset on each new program invocation, as discussed here:\n        https://stackoverflow.com/questions/27954892/deterministic-hashing-in-python-3.\n        """"""\n        dumped = json.dumps(self.params, sort_keys=True)\n        hashed = zlib.adler32(dumped.encode())\n        return str(hashed)\n\n    def __str__(self) -> str:\n        return f""{self.history}Params({self.params})""\n\n\ndef pop_choice(\n    params: Dict[str, Any],\n    key: str,\n    choices: List[Any],\n    default_to_first_choice: bool = False,\n    history: str = ""?."",\n    allow_class_names: bool = True,\n) -> Any:\n    """"""\n    Performs the same function as `Params.pop_choice`, but is required in order to deal with\n    places that the Params object is not welcome, such as inside Keras layers.  See the docstring\n    of that method for more detail on how this function works.\n\n    This method adds a `history` parameter, in the off-chance that you know it, so that we can\n    reproduce `Params.pop_choice` exactly.  We default to using ""?."" if you don\'t know the\n    history, so you\'ll have to fix that in the log if you want to actually recover the logged\n    parameters.\n    """"""\n    value = Params(params, history).pop_choice(\n        key, choices, default_to_first_choice, allow_class_names=allow_class_names\n    )\n    return value\n\n\ndef _replace_none(params: Any) -> Any:\n    if params == ""None"":\n        return None\n    elif isinstance(params, dict):\n        for key, value in params.items():\n            params[key] = _replace_none(value)\n        return params\n    elif isinstance(params, list):\n        return [_replace_none(value) for value in params]\n    return params\n'"
allennlp/common/plugins.py,0,"b'""""""\nPlugin management.\n\nAllenNLP supports loading ""plugins"" dynamically. A plugin is just a Python package that\ncan be found and imported by AllenNLP. This is done by creating a file named `.allennlp_plugins`\nin the directory where the `allennlp` command is run that lists the modules that should be loaded,\none per line.\n""""""\n\nimport importlib\nimport logging\nimport os\nfrom typing import Iterable\n\nfrom allennlp.common.util import push_python_path, import_module_and_submodules\n\nlogger = logging.getLogger(__name__)\n\n\nDEFAULT_PLUGINS = (""allennlp_models"",)\n\n\ndef discover_file_plugins(plugins_filename: str = "".allennlp_plugins"") -> Iterable[str]:\n    """"""\n    Returns an iterable of the plugins found, declared within a file whose path is `plugins_filename`.\n    """"""\n    if os.path.isfile(plugins_filename):\n        with open(plugins_filename) as file_:\n            for module_name in file_.readlines():\n                module_name = module_name.strip()\n                if module_name:\n                    yield module_name\n    else:\n        return []\n\n\ndef discover_plugins() -> Iterable[str]:\n    """"""\n    Returns an iterable of the plugins found.\n    """"""\n    with push_python_path("".""):\n        yield from discover_file_plugins()\n\n\ndef import_plugins() -> None:\n    """"""\n    Imports the plugins found with `discover_plugins()`.\n    """"""\n    for module in DEFAULT_PLUGINS:\n        try:\n            # For default plugins we recursively import everything.\n            import_module_and_submodules(module)\n        except ModuleNotFoundError:\n            pass\n    for module_name in discover_plugins():\n        try:\n            importlib.import_module(module_name)\n        except ModuleNotFoundError as e:\n            logger.error(f""Plugin {module_name} could not be loaded: {e}"")\n'"
allennlp/common/registrable.py,0,"b'""""""\n`allennlp.common.registrable.Registrable` is a ""mixin"" for endowing\nany base class with a named registry for its subclasses and a decorator\nfor registering them.\n""""""\nfrom collections import defaultdict\nfrom typing import TypeVar, Type, Callable, Dict, List, Optional, Tuple\nimport importlib\nimport logging\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.from_params import FromParams\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(""T"", bound=""Registrable"")\n\n\nclass Registrable(FromParams):\n    """"""\n    Any class that inherits from `Registrable` gains access to a named registry for its\n    subclasses. To register them, just decorate them with the classmethod\n    `@BaseClass.register(name)`.\n\n    After which you can call `BaseClass.list_available()` to get the keys for the\n    registered subclasses, and `BaseClass.by_name(name)` to get the corresponding subclass.\n    Note that the registry stores the subclasses themselves; not class instances.\n    In most cases you would then call `from_params(params)` on the returned subclass.\n\n    You can specify a default by setting `BaseClass.default_implementation`.\n    If it is set, it will be the first element of `list_available()`.\n\n    Note that if you use this class to implement a new `Registrable` abstract class,\n    you must ensure that all subclasses of the abstract class are loaded when the module is\n    loaded, because the subclasses register themselves in their respective files. You can\n    achieve this by having the abstract class and all subclasses in the __init__.py of the\n    module in which they reside (as this causes any import of either the abstract class or\n    a subclass to load all other subclasses and the abstract class).\n    """"""\n\n    _registry: Dict[Type, Dict[str, Tuple[Type, str]]] = defaultdict(dict)\n    default_implementation: str = None\n\n    @classmethod\n    def register(cls: Type[T], name: str, constructor: str = None, exist_ok: bool = False):\n        """"""\n        Register a class under a particular name.\n\n        # Parameters\n\n        name : `str`\n            The name to register the class under.\n        constructor : `str`, optional (default=`None`)\n            The name of the method to use on the class to construct the object.  If this is given,\n            we will use this method (which must be a `@classmethod`) instead of the default\n            constructor.\n        exist_ok : `bool`, optional (default=`False`)\n            If True, overwrites any existing models registered under `name`. Else,\n            throws an error if a model is already registered under `name`.\n\n        # Examples\n\n        To use this class, you would typically have a base class that inherits from `Registrable`:\n\n        ```python\n        class Vocabulary(Registrable):\n            ...\n        ```\n\n        Then, if you want to register a subclass, you decorate it like this:\n\n        ```python\n        @Vocabulary.register(""my-vocabulary"")\n        class MyVocabulary(Vocabulary):\n            def __init__(self, param1: int, param2: str):\n                ...\n        ```\n\n        Registering a class like this will let you instantiate a class from a config file, where you\n        give `""type"": ""my-vocabulary""`, and keys corresponding to the parameters of the `__init__`\n        method (note that for this to work, those parameters must have type annotations).\n\n        If you want to have the instantiation from a config file call a method other than the\n        constructor, either because you have several different construction paths that could be\n        taken for the same object (as we do in `Vocabulary`) or because you have logic you want to\n        happen before you get to the constructor (as we do in `Embedding`), you can register a\n        specific `@classmethod` as the constructor to use, like this:\n\n        ```python\n        @Vocabulary.register(""my-vocabulary-from-instances"", constructor=""from_instances"")\n        @Vocabulary.register(""my-vocabulary-from-files"", constructor=""from_files"")\n        class MyVocabulary(Vocabulary):\n            def __init__(self, some_params):\n                ...\n\n            @classmethod\n            def from_instances(cls, some_other_params) -> MyVocabulary:\n                ...  # construct some_params from instances\n                return cls(some_params)\n\n            @classmethod\n            def from_files(cls, still_other_params) -> MyVocabulary:\n                ...  # construct some_params from files\n                return cls(some_params)\n        ```\n        """"""\n        registry = Registrable._registry[cls]\n\n        def add_subclass_to_registry(subclass: Type[T]):\n            # Add to registry, raise an error if key has already been used.\n            if name in registry:\n                if exist_ok:\n                    message = (\n                        f""{name} has already been registered as {registry[name][0].__name__}, but ""\n                        f""exist_ok=True, so overwriting with {cls.__name__}""\n                    )\n                    logger.info(message)\n                else:\n                    message = (\n                        f""Cannot register {name} as {cls.__name__}; ""\n                        f""name already in use for {registry[name][0].__name__}""\n                    )\n                    raise ConfigurationError(message)\n            registry[name] = (subclass, constructor)\n            return subclass\n\n        return add_subclass_to_registry\n\n    @classmethod\n    def by_name(cls: Type[T], name: str) -> Callable[..., T]:\n        """"""\n        Returns a callable function that constructs an argument of the registered class.  Because\n        you can register particular functions as constructors for specific names, this isn\'t\n        necessarily the `__init__` method of some class.\n        """"""\n        logger.debug(f""instantiating registered subclass {name} of {cls}"")\n        subclass, constructor = cls.resolve_class_name(name)\n        if not constructor:\n            return subclass\n        else:\n            return getattr(subclass, constructor)\n\n    @classmethod\n    def resolve_class_name(cls: Type[T], name: str) -> Tuple[Type[T], Optional[str]]:\n        """"""\n        Returns the subclass that corresponds to the given `name`, along with the name of the\n        method that was registered as a constructor for that `name`, if any.\n\n        This method also allows `name` to be a fully-specified module name, instead of a name that\n        was already added to the `Registry`.  In that case, you cannot use a separate function as\n        a constructor (as you need to call `cls.register()` in order to tell us what separate\n        function to use).\n        """"""\n        if name in Registrable._registry[cls]:\n            subclass, constructor = Registrable._registry[cls].get(name)\n            return subclass, constructor\n        elif ""."" in name:\n            # This might be a fully qualified class name, so we\'ll try importing its ""module""\n            # and finding it there.\n            parts = name.split(""."")\n            submodule = ""."".join(parts[:-1])\n            class_name = parts[-1]\n\n            try:\n                module = importlib.import_module(submodule)\n            except ModuleNotFoundError:\n                raise ConfigurationError(\n                    f""tried to interpret {name} as a path to a class ""\n                    f""but unable to import module {submodule}""\n                )\n\n            try:\n                subclass = getattr(module, class_name)\n                constructor = None\n                return subclass, constructor\n            except AttributeError:\n                raise ConfigurationError(\n                    f""tried to interpret {name} as a path to a class ""\n                    f""but unable to find class {class_name} in {submodule}""\n                )\n\n        else:\n            # is not a qualified class name\n            raise ConfigurationError(\n                f""{name} is not a registered name for {cls.__name__}. ""\n                ""You probably need to use the --include-package flag ""\n                ""to load your custom code. Alternatively, you can specify your choices ""\n                """"""using fully-qualified paths, e.g. {""model"": ""my_module.models.MyModel""} """"""\n                ""in which case they will be automatically imported correctly.""\n            )\n\n    @classmethod\n    def list_available(cls) -> List[str]:\n        """"""List default first if it exists""""""\n        keys = list(Registrable._registry[cls].keys())\n        default = cls.default_implementation\n\n        if default is None:\n            return keys\n        elif default not in keys:\n            raise ConfigurationError(f""Default implementation {default} is not registered"")\n        else:\n            return [default] + [k for k in keys if k != default]\n'"
allennlp/common/tee.py,0,"b'from typing import TextIO\nimport os\n\n\ndef replace_cr_with_newline(message: str) -> str:\n    """"""\n    TQDM and requests use carriage returns to get the training line to update for each batch\n    without adding more lines to the terminal output. Displaying those in a file won\'t work\n    correctly, so we\'ll just make sure that each batch shows up on its one line.\n    """"""\n    if ""\\r"" in message:\n        message = message.replace(""\\r"", """")\n        if not message or message[-1] != ""\\n"":\n            message += ""\\n""\n    return message\n\n\nclass TeeHandler:\n    """"""\n    This class behaves similar to the `tee` command-line utility by writing messages\n    to both a terminal and a file.\n\n    It\'s meant to be used like this to patch sys.stdout and sys.stderr.\n\n    # Examples\n\n    ```python\n    sys.stdout = TeeHandler(""stdout.log"", sys.stdout)\n    sys.stderr = TeeHandler(""stdout.log"", sys.stderr)\n    ```\n    """"""\n\n    def __init__(\n        self,\n        filename: str,\n        terminal: TextIO,\n        file_friendly_terminal_output: bool = False,\n        silent: bool = False,\n    ) -> None:\n        self.terminal = terminal\n        self.file_friendly_terminal_output = file_friendly_terminal_output\n        self.silent = silent\n        parent_directory = os.path.dirname(filename)\n        os.makedirs(parent_directory, exist_ok=True)\n        self.log = open(filename, ""a"")\n\n    def write(self, message):\n        cleaned = replace_cr_with_newline(message)\n\n        if not self.silent:\n            if self.file_friendly_terminal_output:\n                self.terminal.write(cleaned)\n            else:\n                self.terminal.write(message)\n\n        self.log.write(cleaned)\n\n    def flush(self):\n        self.terminal.flush()\n        self.log.flush()\n\n    def isatty(self):\n        # Mirror the API of sys.stdout so that we can\n        # check for the presence of a terminal easily.\n        return not self.file_friendly_terminal_output\n\n    def cleanup(self) -> TextIO:\n        self.log.close()\n        return self.terminal\n'"
allennlp/common/tqdm.py,0,"b'""""""\n`allennlp.common.tqdm.Tqdm` wraps tqdm so we can add configurable\nglobal defaults for certain tqdm parameters.\n""""""\n\ntry:\n    SHELL = str(type(get_ipython()))  # type:ignore # noqa: F821\nexcept:  # noqa: E722\n    SHELL = """"\n\nif ""zmqshell.ZMQInteractiveShell"" in SHELL:\n    from tqdm import tqdm_notebook as _tqdm\nelse:\n    from tqdm import tqdm as _tqdm\n\n# This is neccesary to stop tqdm from hanging\n# when exceptions are raised inside iterators.\n# It should have been fixed in 4.2.1, but it still\n# occurs.\n# TODO(Mark): Remove this once tqdm cleans up after itself properly.\n# https://github.com/tqdm/tqdm/issues/469\n_tqdm.monitor_interval = 0\n\n\nclass Tqdm:\n    # These defaults are the same as the argument defaults in tqdm.\n    default_mininterval: float = 0.1\n\n    @staticmethod\n    def set_default_mininterval(value: float) -> None:\n        Tqdm.default_mininterval = value\n\n    @staticmethod\n    def set_slower_interval(use_slower_interval: bool) -> None:\n        """"""\n        If `use_slower_interval` is `True`, we will dramatically slow down `tqdm\'s` default\n        output rate.  `tqdm\'s` default output rate is great for interactively watching progress,\n        but it is not great for log files.  You might want to set this if you are primarily going\n        to be looking at output through log files, not the terminal.\n        """"""\n        if use_slower_interval:\n            Tqdm.default_mininterval = 10.0\n        else:\n            Tqdm.default_mininterval = 0.1\n\n    @staticmethod\n    def tqdm(*args, **kwargs):\n        new_kwargs = {""mininterval"": Tqdm.default_mininterval, **kwargs}\n\n        return _tqdm(*args, **new_kwargs)\n'"
allennlp/common/util.py,18,"b'""""""\nVarious utilities that don\'t fit anywhere else.\n""""""\nimport importlib\nimport json\nimport logging\nimport os\nimport pkgutil\nimport random\nimport subprocess\nimport sys\nfrom contextlib import contextmanager\nfrom itertools import islice, zip_longest\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Generator,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Tuple,\n    TypeVar,\n    Union,\n)\n\nimport numpy\nimport spacy\nimport torch\nimport torch.distributed as dist\nfrom spacy.cli.download import download as spacy_download\nfrom spacy.language import Language as SpacyModelType\n\nfrom allennlp.common.checks import log_pytorch_version_info\nfrom allennlp.common.params import Params\n\ntry:\n    import resource\nexcept ImportError:\n    # resource doesn\'t exist on Windows systems\n    resource = None\n\nlogger = logging.getLogger(__name__)\n\nJsonDict = Dict[str, Any]\n\n# If you want to have start and/or end symbols for any reason in your code, we recommend you use\n# these, to have a common place to import from.  Also, it\'s important for some edge cases in how\n# data is processed for these symbols to be lowercase, not uppercase (because we have code that\n# will lowercase tokens for you in some circumstances, and we need this symbol to not change in\n# those cases).\nSTART_SYMBOL = ""@start@""\nEND_SYMBOL = ""@end@""\n\n\nPathType = Union[os.PathLike, str]\nT = TypeVar(""T"")\nContextManagerFunctionReturnType = Generator[T, None, None]\n\n\ndef sanitize(x: Any) -> Any:\n    """"""\n    Sanitize turns PyTorch and Numpy types into basic Python types so they\n    can be serialized into JSON.\n    """"""\n    # Import here to avoid circular references\n    from allennlp.data.tokenizers.token import Token\n\n    if isinstance(x, (str, float, int, bool)):\n        # x is already serializable\n        return x\n    elif isinstance(x, torch.Tensor):\n        # tensor needs to be converted to a list (and moved to cpu if necessary)\n        return x.cpu().tolist()\n    elif isinstance(x, numpy.ndarray):\n        # array needs to be converted to a list\n        return x.tolist()\n    elif isinstance(x, numpy.number):\n        # NumPy numbers need to be converted to Python numbers\n        return x.item()\n    elif isinstance(x, dict):\n        # Dicts need their values sanitized\n        return {key: sanitize(value) for key, value in x.items()}\n    elif isinstance(x, numpy.bool_):\n        # Numpy bool_ need to be converted to python bool.\n        return bool(x)\n    elif isinstance(x, (spacy.tokens.Token, Token)):\n        # Tokens get sanitized to just their text.\n        return x.text\n    elif isinstance(x, (list, tuple)):\n        # Lists and Tuples need their values sanitized\n        return [sanitize(x_i) for x_i in x]\n    elif x is None:\n        return ""None""\n    elif hasattr(x, ""to_json""):\n        return x.to_json()\n    else:\n        raise ValueError(\n            f""Cannot sanitize {x} of type {type(x)}. ""\n            ""If this is your own custom class, add a `to_json(self)` method ""\n            ""that returns a JSON-like object.""\n        )\n\n\ndef group_by_count(iterable: List[Any], count: int, default_value: Any) -> List[List[Any]]:\n    """"""\n    Takes a list and groups it into sublists of size `count`, using `default_value` to pad the\n    list at the end if the list is not divisable by `count`.\n\n    For example:\n\n    ```\n    >>> group_by_count([1, 2, 3, 4, 5, 6, 7], 3, 0)\n    [[1, 2, 3], [4, 5, 6], [7, 0, 0]]\n    ```\n\n    This is a short method, but it\'s complicated and hard to remember as a one-liner, so we just\n    make a function out of it.\n    """"""\n    return [list(x) for x in zip_longest(*[iter(iterable)] * count, fillvalue=default_value)]\n\n\nA = TypeVar(""A"")\n\n\ndef lazy_groups_of(iterable: Iterable[A], group_size: int) -> Iterator[List[A]]:\n    """"""\n    Takes an iterable and batches the individual instances into lists of the\n    specified size. The last list may be smaller if there are instances left over.\n    """"""\n    iterator = iter(iterable)\n    while True:\n        s = list(islice(iterator, group_size))\n        if len(s) > 0:\n            yield s\n        else:\n            break\n\n\ndef pad_sequence_to_length(\n    sequence: List,\n    desired_length: int,\n    default_value: Callable[[], Any] = lambda: 0,\n    padding_on_right: bool = True,\n) -> List:\n    """"""\n    Take a list of objects and pads it to the desired length, returning the padded list.  The\n    original list is not modified.\n\n    # Parameters\n\n    sequence : `List`\n        A list of objects to be padded.\n\n    desired_length : `int`\n        Maximum length of each sequence. Longer sequences are truncated to this length, and\n        shorter ones are padded to it.\n\n    default_value: `Callable`, optional (default=`lambda: 0`)\n        Callable that outputs a default value (of any type) to use as padding values.  This is\n        a lambda to avoid using the same object when the default value is more complex, like a\n        list.\n\n    padding_on_right : `bool`, optional (default=`True`)\n        When we add padding tokens (or truncate the sequence), should we do it on the right or\n        the left?\n\n    # Returns\n\n    padded_sequence : `List`\n    """"""\n    # Truncates the sequence to the desired length.\n    if padding_on_right:\n        padded_sequence = sequence[:desired_length]\n    else:\n        padded_sequence = sequence[-desired_length:]\n    # Continues to pad with default_value() until we reach the desired length.\n    pad_length = desired_length - len(padded_sequence)\n    # This just creates the default value once, so if it\'s a list, and if it gets mutated\n    # later, it could cause subtle bugs. But the risk there is low, and this is much faster.\n    values_to_pad = [default_value()] * pad_length\n    if padding_on_right:\n        padded_sequence = padded_sequence + values_to_pad\n    else:\n        padded_sequence = values_to_pad + padded_sequence\n    return padded_sequence\n\n\ndef add_noise_to_dict_values(dictionary: Dict[A, float], noise_param: float) -> Dict[A, float]:\n    """"""\n    Returns a new dictionary with noise added to every key in `dictionary`.  The noise is\n    uniformly distributed within `noise_param` percent of the value for every value in the\n    dictionary.\n    """"""\n    new_dict = {}\n    for key, value in dictionary.items():\n        noise_value = value * noise_param\n        noise = random.uniform(-noise_value, noise_value)\n        new_dict[key] = value + noise\n    return new_dict\n\n\ndef namespace_match(pattern: str, namespace: str):\n    """"""\n    Matches a namespace pattern against a namespace string.  For example, `*tags` matches\n    `passage_tags` and `question_tags` and `tokens` matches `tokens` but not\n    `stemmed_tokens`.\n    """"""\n    if pattern[0] == ""*"" and namespace.endswith(pattern[1:]):\n        return True\n    elif pattern == namespace:\n        return True\n    return False\n\n\ndef prepare_environment(params: Params):\n    """"""\n    Sets random seeds for reproducible experiments. This may not work as expected\n    if you use this from within a python project in which you have already imported Pytorch.\n    If you use the scripts/run_model.py entry point to training models with this library,\n    your experiments should be reasonably reproducible. If you are using this from your own\n    project, you will want to call this function before importing Pytorch. Complete determinism\n    is very difficult to achieve with libraries doing optimized linear algebra due to massively\n    parallel execution, which is exacerbated by using GPUs.\n\n    # Parameters\n\n    params: `Params`\n        A `Params` object or dict holding the json parameters.\n    """"""\n    seed = params.pop_int(""random_seed"", 13370)\n    numpy_seed = params.pop_int(""numpy_seed"", 1337)\n    torch_seed = params.pop_int(""pytorch_seed"", 133)\n\n    if seed is not None:\n        random.seed(seed)\n    if numpy_seed is not None:\n        numpy.random.seed(numpy_seed)\n    if torch_seed is not None:\n        torch.manual_seed(torch_seed)\n        # Seed all GPUs with the same seed if available.\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(torch_seed)\n\n    log_pytorch_version_info()\n\n\nLOADED_SPACY_MODELS: Dict[Tuple[str, bool, bool, bool], SpacyModelType] = {}\n\n\ndef get_spacy_model(\n    spacy_model_name: str, pos_tags: bool, parse: bool, ner: bool\n) -> SpacyModelType:\n    """"""\n    In order to avoid loading spacy models a whole bunch of times, we\'ll save references to them,\n    keyed by the options we used to create the spacy model, so any particular configuration only\n    gets loaded once.\n    """"""\n\n    options = (spacy_model_name, pos_tags, parse, ner)\n    if options not in LOADED_SPACY_MODELS:\n        disable = [""vectors"", ""textcat""]\n        if not pos_tags:\n            disable.append(""tagger"")\n        if not parse:\n            disable.append(""parser"")\n        if not ner:\n            disable.append(""ner"")\n        try:\n            spacy_model = spacy.load(spacy_model_name, disable=disable)\n        except OSError:\n            logger.warning(\n                f""Spacy models \'{spacy_model_name}\' not found.  Downloading and installing.""\n            )\n            spacy_download(spacy_model_name)\n\n            # Import the downloaded model module directly and load from there\n            spacy_model_module = __import__(spacy_model_name)\n            spacy_model = spacy_model_module.load(disable=disable)  # type: ignore\n\n        LOADED_SPACY_MODELS[options] = spacy_model\n    return LOADED_SPACY_MODELS[options]\n\n\n@contextmanager\ndef pushd(new_dir: PathType, verbose: bool = False) -> ContextManagerFunctionReturnType[None]:\n    """"""\n    Changes the current directory to the given path and prepends it to `sys.path`.\n\n    This method is intended to use with `with`, so after its usage, the current directory will be\n    set to the previous value.\n    """"""\n    previous_dir = os.getcwd()\n    if verbose:\n        logger.info(f""Changing directory to {new_dir}"")  # type: ignore\n    os.chdir(new_dir)\n    try:\n        yield\n    finally:\n        if verbose:\n            logger.info(f""Changing directory back to {previous_dir}"")\n        os.chdir(previous_dir)\n\n\n@contextmanager\ndef push_python_path(path: PathType) -> ContextManagerFunctionReturnType[None]:\n    """"""\n    Prepends the given path to `sys.path`.\n\n    This method is intended to use with `with`, so after its usage, its value willbe removed from\n    `sys.path`.\n    """"""\n    # In some environments, such as TC, it fails when sys.path contains a relative path, such as ""."".\n    path = Path(path).resolve()\n    path = str(path)\n    sys.path.insert(0, path)\n    try:\n        yield\n    finally:\n        # Better to remove by value, in case `sys.path` was manipulated in between.\n        sys.path.remove(path)\n\n\ndef import_module_and_submodules(package_name: str) -> None:\n    """"""\n    Import all submodules under the given package.\n    Primarily useful so that people using AllenNLP as a library\n    can specify their own custom packages and have their custom\n    classes get loaded and registered.\n    """"""\n    importlib.invalidate_caches()\n\n    # For some reason, python doesn\'t always add this by default to your path, but you pretty much\n    # always want it when using `--include-package`.  And if it\'s already there, adding it again at\n    # the end won\'t hurt anything.\n    with push_python_path("".""):\n        # Import at top level\n        module = importlib.import_module(package_name)\n        path = getattr(module, ""__path__"", [])\n        path_string = """" if not path else path[0]\n\n        # walk_packages only finds immediate children, so need to recurse.\n        for module_finder, name, _ in pkgutil.walk_packages(path):\n            # Sometimes when you import third-party libraries that are on your path,\n            # `pkgutil.walk_packages` returns those too, so we need to skip them.\n            if path_string and module_finder.path != path_string:\n                continue\n            subpackage = f""{package_name}.{name}""\n            import_module_and_submodules(subpackage)\n\n\ndef peak_memory_mb() -> Dict[int, float]:\n    """"""\n    Get peak memory usage for each worker, as measured by max-resident-set size:\n\n    https://unix.stackexchange.com/questions/30940/getrusage-system-call-what-is-maximum-resident-set-size\n\n    Only works on OSX and Linux, otherwise the result will be 0.0 for every worker.\n    """"""\n    if resource is None or sys.platform not in (""linux"", ""darwin""):\n        peak_mb = 0.0\n    else:\n        peak = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n        if sys.platform == ""darwin"":\n            # On OSX the result is in bytes.\n            peak_mb = peak / 1_000_000\n        else:\n            # On Linux the result is in kilobytes.\n            peak_mb = peak / 1_000\n\n    if is_distributed():\n        global_rank = dist.get_rank()\n        world_size = dist.get_world_size()\n\n        peak_mb_tensor = torch.tensor([float(global_rank), peak_mb])\n        # All of these tensors will be gathered into this list.\n        gather_results = [torch.tensor([0.0, 0.0]) for _ in range(world_size)]\n\n        # If the backend is \'nccl\', this means we\'re training on GPUs, so these tensors\n        # need to be on GPU.\n        if dist.get_backend() == ""nccl"":\n            peak_mb_tensor = peak_mb_tensor.cuda()\n            gather_results = [x.cuda() for x in gather_results]\n\n        dist.all_gather(gather_results, peak_mb_tensor)\n\n        results_dict: Dict[int, float] = {}\n        for peak_mb_tensor in gather_results:\n            worker = int(peak_mb_tensor[0])\n            peak_mb = round(float(peak_mb_tensor[1]), 3)\n            results_dict[worker] = peak_mb\n\n        return results_dict\n    else:\n        return {0: peak_mb}\n\n\ndef gpu_memory_mb() -> Dict[int, int]:\n    """"""\n    Get the current GPU memory usage.\n    Based on https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4\n\n    # Returns\n\n    `Dict[int, int]`\n        Keys are device ids as integers.\n        Values are memory usage as integers in MB.\n        Returns an empty `dict` if GPUs are not available.\n    """"""\n    try:\n        result = subprocess.check_output(\n            [""nvidia-smi"", ""--query-gpu=memory.used"", ""--format=csv,nounits,noheader""],\n            encoding=""utf-8"",\n        )\n        gpu_memory = [int(x) for x in result.strip().split(""\\n"")]\n        return {gpu: memory for gpu, memory in enumerate(gpu_memory)}\n    except FileNotFoundError:\n        # `nvidia-smi` doesn\'t exist, assume that means no GPU.\n        return {}\n    except:  # noqa\n        # Catch *all* exceptions, because this memory check is a nice-to-have\n        # and we\'d never want a training run to fail because of it.\n        logger.warning(\n            ""unable to check gpu_memory_mb() due to occasional failure, continuing"", exc_info=True\n        )\n        return {}\n\n\ndef ensure_list(iterable: Iterable[A]) -> List[A]:\n    """"""\n    An Iterable may be a list or a generator.\n    This ensures we get a list without making an unnecessary copy.\n    """"""\n    if isinstance(iterable, list):\n        return iterable\n    else:\n        return list(iterable)\n\n\ndef is_lazy(iterable: Iterable[A]) -> bool:\n    """"""\n    Checks if the given iterable is lazy,\n    which here just means it\'s not a list.\n    """"""\n    return not isinstance(iterable, list)\n\n\ndef int_to_device(device: Union[int, torch.device]) -> torch.device:\n    if isinstance(device, torch.device):\n        return device\n    if device < 0:\n        return torch.device(""cpu"")\n    return torch.device(device)\n\n\ndef log_frozen_and_tunable_parameter_names(model: torch.nn.Module) -> None:\n    frozen_parameter_names, tunable_parameter_names = get_frozen_and_tunable_parameter_names(model)\n\n    logger.info(""The following parameters are Frozen (without gradient):"")\n    for name in frozen_parameter_names:\n        logger.info(name)\n\n    logger.info(""The following parameters are Tunable (with gradient):"")\n    for name in tunable_parameter_names:\n        logger.info(name)\n\n\ndef get_frozen_and_tunable_parameter_names(\n    model: torch.nn.Module,\n) -> Tuple[Iterable[str], Iterable[str]]:\n    frozen_parameter_names = (\n        name for name, parameter in model.named_parameters() if not parameter.requires_grad\n    )\n    tunable_parameter_names = (\n        name for name, parameter in model.named_parameters() if parameter.requires_grad\n    )\n    return frozen_parameter_names, tunable_parameter_names\n\n\ndef dump_metrics(file_path: Optional[str], metrics: Dict[str, Any], log: bool = False) -> None:\n    metrics_json = json.dumps(metrics, indent=2)\n    if file_path:\n        with open(file_path, ""w"") as metrics_file:\n            metrics_file.write(metrics_json)\n    if log:\n        logger.info(""Metrics: %s"", metrics_json)\n\n\ndef flatten_filename(file_path: str) -> str:\n    return file_path.replace(""/"", ""_SLASH_"")\n\n\ndef is_master(\n    global_rank: int = None, world_size: int = None, num_procs_per_node: int = None\n) -> bool:\n    """"""\n    Checks if the process is a ""master"" of its node in a distributed process group. If a\n    process group is not initialized, this returns `True`.\n\n    # Parameters\n\n    global_rank : `int` ( default = `None` )\n        Global rank of the process if in a distributed process group. If not\n        given, rank is obtained using `torch.distributed.get_rank()`\n    world_size : `int` ( default = `None` )\n        Number of processes in the distributed group. If not\n        given, this is obtained using `torch.distributed.get_world_size()`\n    num_procs_per_node: `int` ( default = `None` )\n        Number of GPU processes running per node\n    """"""\n\n    # In non-distributed case, a ""master"" process doesn\'t make any\n    # sense. So instead of raising an error, returning True would\n    # make things less painful\n    if not is_distributed():\n        return True\n\n    if global_rank is None:\n        global_rank = dist.get_rank()\n\n    if world_size is None:\n        world_size = dist.get_world_size()\n\n    if num_procs_per_node is None and os.environ:\n        num_procs_per_node = int(os.environ.get(""ALLENNLP_PROCS_PER_NODE"", world_size))\n\n    # rank == 0 would do in a single-node multi-GPU setup. However,\n    # in a multi-node case, every node has a logical master and hence\n    # the mod(%) op.\n    return global_rank % (world_size / num_procs_per_node) == 0\n\n\ndef is_distributed() -> bool:\n    """"""\n    Checks if the distributed process group is available and has been initialized\n    """"""\n    return dist.is_available() and dist.is_initialized()\n\n\ndef sanitize_wordpiece(wordpiece: str) -> str:\n    """"""\n    Sanitizes wordpieces from BERT, RoBERTa or ALBERT tokenizers.\n    """"""\n    if wordpiece.startswith(""##""):\n        return wordpiece[2:]\n    elif wordpiece.startswith(""\xc4\xa0""):\n        return wordpiece[1:]\n    elif wordpiece.startswith(""\xe2\x96\x81""):\n        return wordpiece[1:]\n    else:\n        return wordpiece\n\n\ndef sanitize_ptb_tokenized_string(text: str) -> str:\n    """"""\n    Sanitizes string that was tokenized using PTBTokenizer\n    """"""\n    tokens = text.split("" "")\n    if len(tokens) == 0:\n        return text\n\n    # Replace quotation marks and parentheses\n    token_map = {\n        ""``"": \'""\',\n        ""\'\'"": \'""\',\n        ""-lrb-"": ""("",\n        ""-rrb-"": "")"",\n        ""-lsb-"": ""["",\n        ""-rsb-"": ""]"",\n        ""-lcb-"": ""{"",\n        ""-rcb-"": ""}"",\n        ""<s>"": """",\n        ""</s>"": """",\n    }\n\n    # Merge punctuation with previous tokens\n    punct_forward = {""`"", ""$"", ""#""}\n    punct_backward = {""."", "","", ""!"", ""?"", "":"", "";"", ""%"", ""\'""}\n\n    # Exact matches that get merged forward or backward\n    em_forward = {""("", ""["", ""{""}\n    em_backward = {""n\'t"", ""na"", "")"", ""]"", ""}""}\n\n    new_tokens: List[str] = []\n\n    merge_fwd = False\n    for i, orig_token in enumerate(tokens):\n        tokens[i] = token_map[orig_token.lower()] if orig_token.lower() in token_map else orig_token\n        new_token = tokens[i].lower()\n\n        # merge_fwd was set by previous token, so it should be prepended to current token\n        if merge_fwd:\n            tokens[i] = tokens[i - 1] + tokens[i]\n\n        if len(tokens[i]) == 0:\n            continue\n\n        # Special cases for `` and \'\', those tells us if "" is the start or end of a quotation.\n        # Also always merge tokens starting with \' backward and don\'t merge back if we just merged forward\n        merge_bckwd = not merge_fwd and (\n            orig_token == ""\'\'""\n            or new_token in em_backward\n            or new_token.startswith(""\'"")\n            or all(c in punct_backward for c in new_token)\n        )\n        merge_fwd = (\n            orig_token == ""``""\n            or new_token in em_forward\n            or all(c in punct_forward for c in new_token)\n        )\n\n        if merge_bckwd and new_tokens:\n            new_tokens[-1] += tokens[i]\n        elif not new_tokens or not merge_fwd or i == len(tokens) - 1:\n            new_tokens.append(tokens[i])\n\n    return "" "".join(new_tokens)\n'"
allennlp/data/__init__.py,0,"b'from allennlp.data.dataloader import DataLoader, allennlp_collate\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.fields.field import DataArray, Field\nfrom allennlp.data.fields.text_field import TextFieldTensors\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.samplers import BatchSampler, Sampler\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.tokenizers.tokenizer import Tokenizer\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.data.batch import Batch\n'"
allennlp/data/batch.py,1,"b'""""""\nA :class:`Batch` represents a collection of `Instance` s to be fed\nthrough a model.\n""""""\n\nimport logging\nfrom collections import defaultdict\nfrom typing import Dict, Iterable, Iterator, List, Union\n\nimport numpy\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.util import ensure_list\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.vocabulary import Vocabulary\n\nlogger = logging.getLogger(__name__)\n\n\nclass Batch(Iterable):\n    """"""\n    A batch of Instances. In addition to containing the instances themselves,\n    it contains helper functions for converting the data into tensors.\n\n    A Batch just takes an iterable of instances in its constructor and hangs onto them\n    in a list.\n    """"""\n\n    __slots__ = [""instances""]\n\n    def __init__(self, instances: Iterable[Instance]) -> None:\n        super().__init__()\n\n        self.instances = ensure_list(instances)\n        self._check_types()\n\n    def _check_types(self) -> None:\n        """"""\n        Check that all the instances have the same types.\n        """"""\n        all_instance_fields_and_types: List[Dict[str, str]] = [\n            {k: v.__class__.__name__ for k, v in x.fields.items()} for x in self.instances\n        ]\n        # Check all the field names and Field types are the same for every instance.\n        if not all(all_instance_fields_and_types[0] == x for x in all_instance_fields_and_types):\n            raise ConfigurationError(""You cannot construct a Batch with non-homogeneous Instances."")\n\n    def get_padding_lengths(self) -> Dict[str, Dict[str, int]]:\n        """"""\n        Gets the maximum padding lengths from all `Instances` in this batch.  Each `Instance`\n        has multiple `Fields`, and each `Field` could have multiple things that need padding.\n        We look at all fields in all instances, and find the max values for each (field_name,\n        padding_key) pair, returning them in a dictionary.\n\n        This can then be used to convert this batch into arrays of consistent length, or to set\n        model parameters, etc.\n        """"""\n        padding_lengths: Dict[str, Dict[str, int]] = defaultdict(dict)\n        all_instance_lengths: List[Dict[str, Dict[str, int]]] = [\n            instance.get_padding_lengths() for instance in self.instances\n        ]\n        all_field_lengths: Dict[str, List[Dict[str, int]]] = defaultdict(list)\n        for instance_lengths in all_instance_lengths:\n            for field_name, instance_field_lengths in instance_lengths.items():\n                all_field_lengths[field_name].append(instance_field_lengths)\n        for field_name, field_lengths in all_field_lengths.items():\n            for padding_key in field_lengths[0].keys():\n                max_value = max(x.get(padding_key, 0) for x in field_lengths)\n                padding_lengths[field_name][padding_key] = max_value\n        return {**padding_lengths}\n\n    def as_tensor_dict(\n        self, padding_lengths: Dict[str, Dict[str, int]] = None, verbose: bool = False\n    ) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]:\n        # This complex return type is actually predefined elsewhere as a DataArray,\n        # but we can\'t use it because mypy doesn\'t like it.\n        """"""\n        This method converts this `Batch` into a set of pytorch Tensors that can be passed\n        through a model.  In order for the tensors to be valid tensors, all `Instances` in this\n        batch need to be padded to the same lengths wherever padding is necessary, so we do that\n        first, then we combine all of the tensors for each field in each instance into a set of\n        batched tensors for each field.\n\n        # Parameters\n\n        padding_lengths : `Dict[str, Dict[str, int]]`\n            If a key is present in this dictionary with a non-`None` value, we will pad to that\n            length instead of the length calculated from the data.  This lets you, e.g., set a\n            maximum value for sentence length if you want to throw out long sequences.\n\n            Entries in this dictionary are keyed first by field name (e.g., ""question""), then by\n            padding key (e.g., ""num_tokens"").\n\n        verbose : `bool`, optional (default=`False`)\n            Should we output logging information when we\'re doing this padding?  If the batch is\n            large, this is nice to have, because padding a large batch could take a long time.\n            But if you\'re doing this inside of a data generator, having all of this output per\n            batch is a bit obnoxious (and really slow).\n\n        # Returns\n\n        tensors : `Dict[str, DataArray]`\n            A dictionary of tensors, keyed by field name, suitable for passing as input to a model.\n            This is a `batch` of instances, so, e.g., if the instances have a ""question"" field and\n            an ""answer"" field, the ""question"" fields for all of the instances will be grouped\n            together into a single tensor, and the ""answer"" fields for all instances will be\n            similarly grouped in a parallel set of tensors, for batched computation. Additionally,\n            for complex `Fields`, the value of the dictionary key is not necessarily a single\n            tensor.  For example, with the `TextField`, the output is a dictionary mapping\n            `TokenIndexer` keys to tensors. The number of elements in this sub-dictionary\n            therefore corresponds to the number of `TokenIndexers` used to index the\n            `TextField`.  Each `Field` class is responsible for batching its own output.\n        """"""\n        padding_lengths = padding_lengths or defaultdict(dict)\n        # First we need to decide _how much_ to pad.  To do that, we find the max length for all\n        # relevant padding decisions from the instances themselves.  Then we check whether we were\n        # given a max length for a particular field and padding key.  If we were, we use that\n        # instead of the instance-based one.\n        if verbose:\n            logger.info(f""Padding batch of size {len(self.instances)} to lengths {padding_lengths}"")\n            logger.info(""Getting max lengths from instances"")\n        instance_padding_lengths = self.get_padding_lengths()\n        if verbose:\n            logger.info(f""Instance max lengths: {instance_padding_lengths}"")\n        lengths_to_use: Dict[str, Dict[str, int]] = defaultdict(dict)\n        for field_name, instance_field_lengths in instance_padding_lengths.items():\n            for padding_key in instance_field_lengths.keys():\n                if padding_key in padding_lengths[field_name]:\n                    lengths_to_use[field_name][padding_key] = padding_lengths[field_name][\n                        padding_key\n                    ]\n                else:\n                    lengths_to_use[field_name][padding_key] = instance_field_lengths[padding_key]\n\n        # Now we actually pad the instances to tensors.\n        field_tensors: Dict[str, list] = defaultdict(list)\n        if verbose:\n            logger.info(f""Now actually padding instances to length: {lengths_to_use}"")\n        for instance in self.instances:\n            for field, tensors in instance.as_tensor_dict(lengths_to_use).items():\n                field_tensors[field].append(tensors)\n\n        # Finally, we combine the tensors that we got for each instance into one big tensor (or set\n        # of tensors) per field.  The `Field` classes themselves have the logic for batching the\n        # tensors together, so we grab a dictionary of field_name -> field class from the first\n        # instance in the batch.\n        field_classes = self.instances[0].fields\n        return {\n            field_name: field_classes[field_name].batch_tensors(field_tensor_list)\n            for field_name, field_tensor_list in field_tensors.items()\n        }\n\n    def __iter__(self) -> Iterator[Instance]:\n        return iter(self.instances)\n\n    def index_instances(self, vocab: Vocabulary) -> None:\n        for instance in self.instances:\n            instance.index_fields(vocab)\n\n    def print_statistics(self) -> None:\n        # Make sure if has been indexed first\n        sequence_field_lengths: Dict[str, List] = defaultdict(list)\n        for instance in self.instances:\n            if not instance.indexed:\n                raise ConfigurationError(\n                    ""Instances must be indexed with vocabulary ""\n                    ""before asking to print dataset statistics.""\n                )\n            for field, field_padding_lengths in instance.get_padding_lengths().items():\n                for key, value in field_padding_lengths.items():\n                    sequence_field_lengths[f""{field}.{key}""].append(value)\n\n        print(""\\n\\n----Dataset Statistics----\\n"")\n        for name, lengths in sequence_field_lengths.items():\n            print(f""Statistics for {name}:"")\n            print(\n                f""\\tLengths: Mean: {numpy.mean(lengths)}, Standard Dev: {numpy.std(lengths)}, ""\n                f""Max: {numpy.max(lengths)}, Min: {numpy.min(lengths)}""\n            )\n\n        print(""\\n10 Random instances:"")\n        for i in numpy.random.randint(len(self.instances), size=10):\n            print(f""Instance {i}:"")\n            print(f""\\t{self.instances[i]}"")\n'"
allennlp/data/dataloader.py,4,"b'from typing import List, Dict, Union\n\nimport torch\nfrom torch.utils import data\n\nfrom allennlp.common.registrable import Registrable\nfrom allennlp.data.instance import Instance\n\nfrom allennlp.common.lazy import Lazy\nfrom allennlp.data.batch import Batch\nfrom allennlp.data.samplers import Sampler, BatchSampler\n\n\nTensorDict = Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]\n\n\ndef allennlp_collate(instances: List[Instance]) -> TensorDict:\n    batch = Batch(instances)\n    return batch.as_tensor_dict(batch.get_padding_lengths())\n\n\nclass DataLoader(Registrable, data.DataLoader):\n    """"""\n    A registrable version of the pytorch\n    [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n    Firstly, this class exists is so that we can construct a DataLoader\n    from a configuration file and have a different default `collate_fn`.\n    You can use this class directly in python code, but it is identical to using\n    pytorch dataloader with allennlp\'s custom collate function:\n\n    ```\n    from torch.utils.data import DataLoader\n\n    from allennlp.data.samplers import allennlp_collate\n    # Construct a dataloader directly for a dataset which contains allennlp\n    # Instances which have _already_ been indexed.\n    my_loader = DataLoader(dataset, batch_size=32, collate_fn=allennlp_collate)\n    ```\n\n    Secondly, this class adds a `batches_per_epoch` parameter which, if given, determines the number\n    of batches after which an epoch ends.  If this is `None`, then an epoch is set to be one full pass\n    through your data.  You might use this if you have a very large dataset and want more frequent\n    checkpoints and evaluations on validation data, for instance.\n\n    In a typical AllenNLP configuration file, the `dataset` parameter does not get an entry under\n    the ""data_loader"", it gets constructed separately.\n    """"""\n\n    def __init__(\n        self,\n        dataset: data.Dataset,\n        batch_size: int = 1,\n        shuffle: bool = False,\n        sampler: Sampler = None,\n        batch_sampler: BatchSampler = None,\n        num_workers: int = 0,\n        # NOTE: The default for collate_fn is different from the normal `None`.\n        # We assume that if you are using this class you are using an\n        # allennlp dataset of instances, which would require this.\n        collate_fn=allennlp_collate,\n        pin_memory: bool = False,\n        drop_last: bool = False,\n        timeout: int = 0,\n        worker_init_fn=None,\n        multiprocessing_context: str = None,\n        batches_per_epoch: int = None,\n    ):\n        super().__init__(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            sampler=sampler,\n            batch_sampler=batch_sampler,\n            num_workers=num_workers,\n            collate_fn=collate_fn,\n            pin_memory=pin_memory,\n            drop_last=drop_last,\n            timeout=timeout,\n            worker_init_fn=worker_init_fn,\n            multiprocessing_context=multiprocessing_context,\n        )\n        self._data_generator = super().__iter__()\n        self._batches_per_epoch = batches_per_epoch\n\n    def __len__(self):\n        if self._batches_per_epoch is not None:\n            return self._batches_per_epoch\n        return super().__len__()\n\n    def __iter__(self):\n        if self._batches_per_epoch is None:\n            yield from super().__iter__()\n        else:\n            for i in range(self._batches_per_epoch):\n                try:\n                    yield next(self._data_generator)\n                except StopIteration:  # data_generator is exhausted\n                    self._data_generator = super().__iter__()  # so refresh it\n                    yield next(self._data_generator)  # and yield required instance\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        dataset: data.Dataset,\n        batch_size: int = 1,\n        shuffle: bool = False,\n        sampler: Lazy[Sampler] = None,\n        batch_sampler: Lazy[BatchSampler] = None,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n        drop_last: bool = False,\n        timeout: int = 0,\n        worker_init_fn=None,\n        multiprocessing_context: str = None,\n        batches_per_epoch: int = None,\n    ) -> ""DataLoader"":\n\n        if batch_sampler is not None:\n            batch_sampler_ = batch_sampler.construct(data_source=dataset)\n        else:\n            batch_sampler_ = None\n        if sampler is not None:\n            sampler_ = sampler.construct(data_source=dataset)\n        else:\n            sampler_ = None\n\n        return cls(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            sampler=sampler_,\n            batch_sampler=batch_sampler_,\n            num_workers=num_workers,\n            # NOTE: The default for collate_fn is different from the normal `None`.\n            # We assume that if you are using this class you are using an\n            # allennlp dataset of instances, which would require this.\n            collate_fn=allennlp_collate,\n            pin_memory=pin_memory,\n            drop_last=drop_last,\n            timeout=timeout,\n            worker_init_fn=worker_init_fn,\n            multiprocessing_context=multiprocessing_context,\n            batches_per_epoch=batches_per_epoch,\n        )\n\n\nDataLoader.register(""default"", ""from_partial_objects"")(DataLoader)\nDataLoader.default_implementation = ""default""\n'"
allennlp/data/instance.py,0,"b'from typing import Dict, MutableMapping, Mapping\n\nfrom allennlp.data.fields.field import DataArray, Field\nfrom allennlp.data.vocabulary import Vocabulary\n\n\nclass Instance(Mapping[str, Field]):\n    """"""\n    An `Instance` is a collection of :class:`~allennlp.data.fields.field.Field` objects,\n    specifying the inputs and outputs to\n    some model.  We don\'t make a distinction between inputs and outputs here, though - all\n    operations are done on all fields, and when we return arrays, we return them as dictionaries\n    keyed by field name.  A model can then decide which fields it wants to use as inputs as which\n    as outputs.\n\n    The `Fields` in an `Instance` can start out either indexed or un-indexed.  During the data\n    processing pipeline, all fields will be indexed, after which multiple instances can be combined\n    into a `Batch` and then converted into padded arrays.\n\n    # Parameters\n\n    fields : `Dict[str, Field]`\n        The `Field` objects that will be used to produce data arrays for this instance.\n    """"""\n\n    __slots__ = [""fields"", ""indexed""]\n\n    def __init__(self, fields: MutableMapping[str, Field]) -> None:\n        self.fields = fields\n        self.indexed = False\n\n    # Add methods for `Mapping`.  Note, even though the fields are\n    # mutable, we don\'t implement `MutableMapping` because we want\n    # you to use `add_field` and supply a vocabulary.\n    def __getitem__(self, key: str) -> Field:\n        return self.fields[key]\n\n    def __iter__(self):\n        return iter(self.fields)\n\n    def __len__(self) -> int:\n        return len(self.fields)\n\n    def add_field(self, field_name: str, field: Field, vocab: Vocabulary = None) -> None:\n        """"""\n        Add the field to the existing fields mapping.\n        If we have already indexed the Instance, then we also index `field`, so\n        it is necessary to supply the vocab.\n        """"""\n        self.fields[field_name] = field\n        if self.indexed:\n            field.index(vocab)\n\n    def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        """"""\n        Increments counts in the given `counter` for all of the vocabulary items in all of the\n        `Fields` in this `Instance`.\n        """"""\n        for field in self.fields.values():\n            field.count_vocab_items(counter)\n\n    def index_fields(self, vocab: Vocabulary) -> None:\n        """"""\n        Indexes all fields in this `Instance` using the provided `Vocabulary`.\n        This `mutates` the current object, it does not return a new `Instance`.\n        A `DataLoader` will call this on each pass through a dataset; we use the `indexed`\n        flag to make sure that indexing only happens once.\n\n        This means that if for some reason you modify your vocabulary after you\'ve\n        indexed your instances, you might get unexpected behavior.\n        """"""\n        if not self.indexed:\n            self.indexed = True\n            for field in self.fields.values():\n                field.index(vocab)\n\n    def get_padding_lengths(self) -> Dict[str, Dict[str, int]]:\n        """"""\n        Returns a dictionary of padding lengths, keyed by field name.  Each `Field` returns a\n        mapping from padding keys to actual lengths, and we just key that dictionary by field name.\n        """"""\n        lengths = {}\n        for field_name, field in self.fields.items():\n            lengths[field_name] = field.get_padding_lengths()\n        return lengths\n\n    def as_tensor_dict(\n        self, padding_lengths: Dict[str, Dict[str, int]] = None\n    ) -> Dict[str, DataArray]:\n        """"""\n        Pads each `Field` in this instance to the lengths given in `padding_lengths` (which is\n        keyed by field name, then by padding key, the same as the return value in\n        :func:`get_padding_lengths`), returning a list of torch tensors for each field.\n\n        If `padding_lengths` is omitted, we will call `self.get_padding_lengths()` to get the\n        sizes of the tensors to create.\n        """"""\n        padding_lengths = padding_lengths or self.get_padding_lengths()\n        tensors = {}\n        for field_name, field in self.fields.items():\n            tensors[field_name] = field.as_tensor(padding_lengths[field_name])\n        return tensors\n\n    def __str__(self) -> str:\n        base_string = ""Instance with fields:\\n""\n        return "" "".join(\n            [base_string] + [f""\\t {name}: {field} \\n"" for name, field in self.fields.items()]\n        )\n\n    def duplicate(self) -> ""Instance"":\n        new = Instance({k: field.duplicate() for k, field in self.fields.items()})\n        new.indexed = self.indexed\n        return new\n'"
allennlp/data/vocabulary.py,0,"b'""""""\nA Vocabulary maps strings to integers, allowing for strings to be mapped to an\nout-of-vocabulary token.\n""""""\n\nimport codecs\nimport copy\nimport logging\nimport os\nfrom collections import defaultdict\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Set, Union, TYPE_CHECKING\n\nfrom filelock import FileLock\n\nfrom allennlp.common.util import namespace_match\nfrom allennlp.common import Registrable\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.tqdm import Tqdm\n\nif TYPE_CHECKING:\n    from allennlp.data import instance as adi  # noqa\n\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_NON_PADDED_NAMESPACES = (""*tags"", ""*labels"")\nDEFAULT_PADDING_TOKEN = ""@@PADDING@@""\nDEFAULT_OOV_TOKEN = ""@@UNKNOWN@@""\nNAMESPACE_PADDING_FILE = ""non_padded_namespaces.txt""\n\n\nclass _NamespaceDependentDefaultDict(defaultdict):\n    """"""\n    This is a [defaultdict]\n    (https://docs.python.org/2/library/collections.html#collections.defaultdict) where the\n    default value is dependent on the key that is passed.\n\n    We use ""namespaces"" in the :class:`Vocabulary` object to keep track of several different\n    mappings from strings to integers, so that we have a consistent API for mapping words, tags,\n    labels, characters, or whatever else you want, into integers.  The issue is that some of those\n    namespaces (words and characters) should have integers reserved for padding and\n    out-of-vocabulary tokens, while others (labels and tags) shouldn\'t.  This class allows you to\n    specify filters on the namespace (the key used in the `defaultdict`), and use different\n    default values depending on whether the namespace passes the filter.\n\n    To do filtering, we take a set of `non_padded_namespaces`.  This is a set of strings\n    that are either matched exactly against the keys, or treated as suffixes, if the\n    string starts with `*`.  In other words, if `*tags` is in `non_padded_namespaces` then\n    `passage_tags`, `question_tags`, etc. (anything that ends with `tags`) will have the\n    `non_padded` default value.\n\n    # Parameters\n\n    non_padded_namespaces : `Iterable[str]`\n        A set / list / tuple of strings describing which namespaces are not padded.  If a namespace\n        (key) is missing from this dictionary, we will use :func:`namespace_match` to see whether\n        the namespace should be padded.  If the given namespace matches any of the strings in this\n        list, we will use `non_padded_function` to initialize the value for that namespace, and\n        we will use `padded_function` otherwise.\n    padded_function : `Callable[[], Any]`\n        A zero-argument function to call to initialize a value for a namespace that `should` be\n        padded.\n    non_padded_function : `Callable[[], Any]`\n        A zero-argument function to call to initialize a value for a namespace that should `not` be\n        padded.\n    """"""\n\n    def __init__(\n        self,\n        non_padded_namespaces: Iterable[str],\n        padded_function: Callable[[], Any],\n        non_padded_function: Callable[[], Any],\n    ) -> None:\n        self._non_padded_namespaces = set(non_padded_namespaces)\n        self._padded_function = padded_function\n        self._non_padded_function = non_padded_function\n        super().__init__()\n\n    def __missing__(self, key: str):\n        if any(namespace_match(pattern, key) for pattern in self._non_padded_namespaces):\n            value = self._non_padded_function()\n        else:\n            value = self._padded_function()\n        dict.__setitem__(self, key, value)\n        return value\n\n    def add_non_padded_namespaces(self, non_padded_namespaces: Set[str]):\n        # add non_padded_namespaces which weren\'t already present\n        self._non_padded_namespaces.update(non_padded_namespaces)\n\n\nclass _TokenToIndexDefaultDict(_NamespaceDependentDefaultDict):\n    def __init__(self, non_padded_namespaces: Set[str], padding_token: str, oov_token: str) -> None:\n        super().__init__(\n            non_padded_namespaces, lambda: {padding_token: 0, oov_token: 1}, lambda: {}\n        )\n\n\nclass _IndexToTokenDefaultDict(_NamespaceDependentDefaultDict):\n    def __init__(self, non_padded_namespaces: Set[str], padding_token: str, oov_token: str) -> None:\n        super().__init__(\n            non_padded_namespaces, lambda: {0: padding_token, 1: oov_token}, lambda: {}\n        )\n\n\ndef _read_pretrained_tokens(embeddings_file_uri: str) -> List[str]:\n    # Moving this import to the top breaks everything (cycling import, I guess)\n    from allennlp.modules.token_embedders.embedding import EmbeddingsTextFile\n\n    logger.info(""Reading pretrained tokens from: %s"", embeddings_file_uri)\n    tokens: List[str] = []\n    with EmbeddingsTextFile(embeddings_file_uri) as embeddings_file:\n        for line_number, line in enumerate(Tqdm.tqdm(embeddings_file), start=1):\n            token_end = line.find("" "")\n            if token_end >= 0:\n                token = line[:token_end]\n                tokens.append(token)\n            else:\n                line_begin = line[:20] + ""..."" if len(line) > 20 else line\n                logger.warning(""Skipping line number %d: %s"", line_number, line_begin)\n    return tokens\n\n\nclass Vocabulary(Registrable):\n    """"""\n    A Vocabulary maps strings to integers, allowing for strings to be mapped to an\n    out-of-vocabulary token.\n\n    Vocabularies are fit to a particular dataset, which we use to decide which tokens are\n    in-vocabulary.\n\n    Vocabularies also allow for several different namespaces, so you can have separate indices for\n    \'a\' as a word, and \'a\' as a character, for instance, and so we can use this object to also map\n    tag and label strings to indices, for a unified :class:`~.fields.field.Field` API.  Most of the\n    methods on this class allow you to pass in a namespace; by default we use the \'tokens\'\n    namespace, and you can omit the namespace argument everywhere and just use the default.\n\n    This class is registered as a `Vocabulary` with four different names, which all point to\n    different `@classmethod` constructors found in this class.  `from_instances` is registered as\n    ""from_instances"", `from_files` is registered as ""from_files"", `from_files_and_instances` is\n    registered as ""extend"", and `empty` is registered as ""empty"".  If you are using a configuration\n    file to construct a vocabulary, you can use any of those strings as the ""type"" key in the\n    configuration file to use the corresponding `@classmethod` to construct the object.\n    ""from_instances"" is the default.  Look at the docstring for the `@classmethod` to see what keys\n    are allowed in the configuration file (when there is an `instances` argument to the\n    `@classmethod`, it will be passed in separately and does not need a corresponding key in the\n    configuration file).\n\n    # Parameters\n\n    counter : `Dict[str, Dict[str, int]]`, optional (default=`None`)\n        A collection of counts from which to initialize this vocabulary.  We will examine the\n        counts and, together with the other parameters to this class, use them to decide which\n        words are in-vocabulary.  If this is `None`, we just won\'t initialize the vocabulary with\n        anything.\n\n    min_count : `Dict[str, int]`, optional (default=`None`)\n        When initializing the vocab from a counter, you can specify a minimum count, and every\n        token with a count less than this will not be added to the dictionary.  These minimum\n        counts are `namespace-specific`, so you can specify different minimums for labels versus\n        words tokens, for example.  If a namespace does not have a key in the given dictionary, we\n        will add all seen tokens to that namespace.\n\n    max_vocab_size : `Union[int, Dict[str, int]]`, optional (default=`None`)\n        If you want to cap the number of tokens in your vocabulary, you can do so with this\n        parameter.  If you specify a single integer, every namespace will have its vocabulary fixed\n        to be no larger than this.  If you specify a dictionary, then each namespace in the\n        `counter` can have a separate maximum vocabulary size.  Any missing key will have a value\n        of `None`, which means no cap on the vocabulary size.\n\n    non_padded_namespaces : `Iterable[str]`, optional\n        By default, we assume you are mapping word / character tokens to integers, and so you want\n        to reserve word indices for padding and out-of-vocabulary tokens.  However, if you are\n        mapping NER or SRL tags, or class labels, to integers, you probably do not want to reserve\n        indices for padding and out-of-vocabulary tokens.  Use this field to specify which\n        namespaces should `not` have padding and OOV tokens added.\n\n        The format of each element of this is either a string, which must match field names\n        exactly,  or `*` followed by a string, which we match as a suffix against field names.\n\n        We try to make the default here reasonable, so that you don\'t have to think about this.\n        The default is `(""*tags"", ""*labels"")`, so as long as your namespace ends in ""tags"" or\n        ""labels"" (which is true by default for all tag and label fields in this code), you don\'t\n        have to specify anything here.\n\n    pretrained_files : `Dict[str, str]`, optional\n        If provided, this map specifies the path to optional pretrained embedding files for each\n        namespace. This can be used to either restrict the vocabulary to only words which appear\n        in this file, or to ensure that any words in this file are included in the vocabulary\n        regardless of their count, depending on the value of `only_include_pretrained_words`.\n        Words which appear in the pretrained embedding file but not in the data are NOT included\n        in the Vocabulary.\n\n    min_pretrained_embeddings : `Dict[str, int]`, optional\n        If provided, specifies for each namespace a minimum number of lines (typically the\n        most common words) to keep from pretrained embedding files, even for words not\n        appearing in the data.\n\n    only_include_pretrained_words : `bool`, optional (default=`False`)\n        This defines the strategy for using any pretrained embedding files which may have been\n        specified in `pretrained_files`. If False, an inclusive strategy is used: and words\n        which are in the `counter` and in the pretrained file are added to the `Vocabulary`,\n        regardless of whether their count exceeds `min_count` or not. If True, we use an\n        exclusive strategy: words are only included in the Vocabulary if they are in the pretrained\n        embedding file (their count must still be at least `min_count`).\n\n    tokens_to_add : `Dict[str, List[str]]`, optional (default=`None`)\n        If given, this is a list of tokens to add to the vocabulary, keyed by the namespace to add\n        the tokens to.  This is a way to be sure that certain items appear in your vocabulary,\n        regardless of any other vocabulary computation.\n\n    padding_token : `str`,  optional (default=`DEFAULT_PADDING_TOKEN`)\n        If given, this the string used for padding.\n\n    oov_token : `str`,  optional (default=`DEFAULT_OOV_TOKEN`)\n        If given, this the string used for the out of vocabulary (OOVs) tokens.\n\n    """"""\n\n    default_implementation = ""from_instances""\n\n    def __init__(\n        self,\n        counter: Dict[str, Dict[str, int]] = None,\n        min_count: Dict[str, int] = None,\n        max_vocab_size: Union[int, Dict[str, int]] = None,\n        non_padded_namespaces: Iterable[str] = DEFAULT_NON_PADDED_NAMESPACES,\n        pretrained_files: Optional[Dict[str, str]] = None,\n        only_include_pretrained_words: bool = False,\n        tokens_to_add: Dict[str, List[str]] = None,\n        min_pretrained_embeddings: Dict[str, int] = None,\n        padding_token: Optional[str] = DEFAULT_PADDING_TOKEN,\n        oov_token: Optional[str] = DEFAULT_OOV_TOKEN,\n    ) -> None:\n        self._padding_token = padding_token if padding_token is not None else DEFAULT_PADDING_TOKEN\n        self._oov_token = oov_token if oov_token is not None else DEFAULT_OOV_TOKEN\n\n        self._non_padded_namespaces = set(non_padded_namespaces)\n\n        self._token_to_index = _TokenToIndexDefaultDict(\n            self._non_padded_namespaces, self._padding_token, self._oov_token\n        )\n        self._index_to_token = _IndexToTokenDefaultDict(\n            self._non_padded_namespaces, self._padding_token, self._oov_token\n        )\n\n        self._retained_counter: Optional[Dict[str, Dict[str, int]]] = None\n\n        # Made an empty vocabulary, now extend it.\n        self._extend(\n            counter,\n            min_count,\n            max_vocab_size,\n            non_padded_namespaces,\n            pretrained_files,\n            only_include_pretrained_words,\n            tokens_to_add,\n            min_pretrained_embeddings,\n        )\n\n    @classmethod\n    def from_instances(\n        cls,\n        instances: Iterable[""adi.Instance""],\n        min_count: Dict[str, int] = None,\n        max_vocab_size: Union[int, Dict[str, int]] = None,\n        non_padded_namespaces: Iterable[str] = DEFAULT_NON_PADDED_NAMESPACES,\n        pretrained_files: Optional[Dict[str, str]] = None,\n        only_include_pretrained_words: bool = False,\n        tokens_to_add: Dict[str, List[str]] = None,\n        min_pretrained_embeddings: Dict[str, int] = None,\n        padding_token: Optional[str] = DEFAULT_PADDING_TOKEN,\n        oov_token: Optional[str] = DEFAULT_OOV_TOKEN,\n    ) -> ""Vocabulary"":\n        """"""\n        Constructs a vocabulary given a collection of `Instances` and some parameters.\n        We count all of the vocabulary items in the instances, then pass those counts\n        and the other parameters, to :func:`__init__`.  See that method for a description\n        of what the other parameters do.\n\n        The `instances` parameter does not get an entry in a typical AllenNLP configuration file,\n        but the other parameters do (if you want non-default parameters).\n        """"""\n        logger.info(""Fitting token dictionary from dataset."")\n        padding_token = padding_token if padding_token is not None else DEFAULT_PADDING_TOKEN\n        oov_token = oov_token if oov_token is not None else DEFAULT_OOV_TOKEN\n        namespace_token_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))\n        for instance in Tqdm.tqdm(instances):\n            instance.count_vocab_items(namespace_token_counts)\n\n        return cls(\n            counter=namespace_token_counts,\n            min_count=min_count,\n            max_vocab_size=max_vocab_size,\n            non_padded_namespaces=non_padded_namespaces,\n            pretrained_files=pretrained_files,\n            only_include_pretrained_words=only_include_pretrained_words,\n            tokens_to_add=tokens_to_add,\n            min_pretrained_embeddings=min_pretrained_embeddings,\n            padding_token=padding_token,\n            oov_token=oov_token,\n        )\n\n    @classmethod\n    def from_files(\n        cls,\n        directory: str,\n        padding_token: Optional[str] = DEFAULT_PADDING_TOKEN,\n        oov_token: Optional[str] = DEFAULT_OOV_TOKEN,\n    ) -> ""Vocabulary"":\n        """"""\n        Loads a `Vocabulary` that was serialized using `save_to_files`.\n\n        # Parameters\n\n        directory : `str`\n            The directory containing the serialized vocabulary.\n        """"""\n        logger.info(""Loading token dictionary from %s."", directory)\n        padding_token = padding_token if padding_token is not None else DEFAULT_PADDING_TOKEN\n        oov_token = oov_token if oov_token is not None else DEFAULT_OOV_TOKEN\n\n        # We use a lock file to avoid race conditions where multiple processes\n        # might be reading/writing from/to the same vocab files at once.\n        with FileLock(os.path.join(directory, "".lock"")):\n            with codecs.open(\n                os.path.join(directory, NAMESPACE_PADDING_FILE), ""r"", ""utf-8""\n            ) as namespace_file:\n                non_padded_namespaces = [namespace_str.strip() for namespace_str in namespace_file]\n\n            vocab = cls(\n                non_padded_namespaces=non_padded_namespaces,\n                padding_token=padding_token,\n                oov_token=oov_token,\n            )\n\n            # Check every file in the directory.\n            for namespace_filename in os.listdir(directory):\n                if namespace_filename == NAMESPACE_PADDING_FILE:\n                    continue\n                if namespace_filename.startswith("".""):\n                    continue\n                namespace = namespace_filename.replace("".txt"", """")\n                if any(namespace_match(pattern, namespace) for pattern in non_padded_namespaces):\n                    is_padded = False\n                else:\n                    is_padded = True\n                filename = os.path.join(directory, namespace_filename)\n                vocab.set_from_file(filename, is_padded, namespace=namespace, oov_token=oov_token)\n\n        return vocab\n\n    @classmethod\n    def from_files_and_instances(\n        cls,\n        instances: Iterable[""adi.Instance""],\n        directory: str,\n        padding_token: Optional[str] = DEFAULT_PADDING_TOKEN,\n        oov_token: Optional[str] = DEFAULT_OOV_TOKEN,\n        min_count: Dict[str, int] = None,\n        max_vocab_size: Union[int, Dict[str, int]] = None,\n        non_padded_namespaces: Iterable[str] = DEFAULT_NON_PADDED_NAMESPACES,\n        pretrained_files: Optional[Dict[str, str]] = None,\n        only_include_pretrained_words: bool = False,\n        tokens_to_add: Dict[str, List[str]] = None,\n        min_pretrained_embeddings: Dict[str, int] = None,\n    ) -> ""Vocabulary"":\n        """"""\n        Extends an already generated vocabulary using a collection of instances.\n\n        The `instances` parameter does not get an entry in a typical AllenNLP configuration file,\n        but the other parameters do (if you want non-default parameters).  See `__init__` for a\n        description of what the other parameters mean.\n        """"""\n        vocab = cls.from_files(directory, padding_token, oov_token)\n        logger.info(""Fitting token dictionary from dataset."")\n        namespace_token_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))\n        for instance in Tqdm.tqdm(instances):\n            instance.count_vocab_items(namespace_token_counts)\n        vocab._extend(\n            counter=namespace_token_counts,\n            min_count=min_count,\n            max_vocab_size=max_vocab_size,\n            non_padded_namespaces=non_padded_namespaces,\n            pretrained_files=pretrained_files,\n            only_include_pretrained_words=only_include_pretrained_words,\n            tokens_to_add=tokens_to_add,\n            min_pretrained_embeddings=min_pretrained_embeddings,\n        )\n        return vocab\n\n    @classmethod\n    def empty(cls) -> ""Vocabulary"":\n        """"""\n        This method returns a bare vocabulary instantiated with `cls()` (so, `Vocabulary()` if you\n        haven\'t made a subclass of this object).  The only reason to call `Vocabulary.empty()`\n        instead of `Vocabulary()` is if you are instantiating this object from a config file.  We\n        register this constructor with the key ""empty"", so if you know that you don\'t need to\n        compute a vocabulary (either because you\'re loading a pre-trained model from an archive\n        file, you\'re using a pre-trained transformer that has its own vocabulary, or something\n        else), you can use this to avoid having the default vocabulary construction code iterate\n        through the data.\n        """"""\n        return cls()\n\n    def set_from_file(\n        self,\n        filename: str,\n        is_padded: bool = True,\n        oov_token: str = DEFAULT_OOV_TOKEN,\n        namespace: str = ""tokens"",\n    ):\n        """"""\n        If you already have a vocabulary file for a trained model somewhere, and you really want to\n        use that vocabulary file instead of just setting the vocabulary from a dataset, for\n        whatever reason, you can do that with this method.  You must specify the namespace to use,\n        and we assume that you want to use padding and OOV tokens for this.\n\n        # Parameters\n\n        filename : `str`\n            The file containing the vocabulary to load.  It should be formatted as one token per\n            line, with nothing else in the line.  The index we assign to the token is the line\n            number in the file (1-indexed if `is_padded`, 0-indexed otherwise).  Note that this\n            file should contain the OOV token string!\n        is_padded : `bool`, optional (default=`True`)\n            Is this vocabulary padded?  For token / word / character vocabularies, this should be\n            `True`; while for tag or label vocabularies, this should typically be `False`.  If\n            `True`, we add a padding token with index 0, and we enforce that the `oov_token` is\n            present in the file.\n        oov_token : `str`, optional (default=`DEFAULT_OOV_TOKEN`)\n            What token does this vocabulary use to represent out-of-vocabulary characters?  This\n            must show up as a line in the vocabulary file.  When we find it, we replace\n            `oov_token` with `self._oov_token`, because we only use one OOV token across\n            namespaces.\n        namespace : `str`, optional (default=`""tokens""`)\n            What namespace should we overwrite with this vocab file?\n        """"""\n        if is_padded:\n            self._token_to_index[namespace] = {self._padding_token: 0}\n            self._index_to_token[namespace] = {0: self._padding_token}\n        else:\n            self._token_to_index[namespace] = {}\n            self._index_to_token[namespace] = {}\n        with codecs.open(filename, ""r"", ""utf-8"") as input_file:\n            lines = input_file.read().split(""\\n"")\n            # Be flexible about having final newline or not\n            if lines and lines[-1] == """":\n                lines = lines[:-1]\n            for i, line in enumerate(lines):\n                index = i + 1 if is_padded else i\n                token = line.replace(""@@NEWLINE@@"", ""\\n"")\n                if token == oov_token:\n                    token = self._oov_token\n                self._token_to_index[namespace][token] = index\n                self._index_to_token[namespace][index] = token\n        if is_padded:\n            assert self._oov_token in self._token_to_index[namespace], ""OOV token not found!""\n\n    def extend_from_instances(self, instances: Iterable[""adi.Instance""]) -> None:\n        logger.info(""Fitting token dictionary from dataset."")\n        namespace_token_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))\n        for instance in Tqdm.tqdm(instances):\n            instance.count_vocab_items(namespace_token_counts)\n        self._extend(counter=namespace_token_counts)\n\n    def extend_from_vocab(self, vocab: ""Vocabulary"") -> None:\n        """"""\n        Adds all vocabulary items from all namespaces in the given vocabulary to this vocabulary.\n        Useful if you want to load a model and extends its vocabulary from new instances.\n\n        We also add all non-padded namespaces from the given vocabulary to this vocabulary.\n        """"""\n        self._non_padded_namespaces.update(vocab._non_padded_namespaces)\n        self._token_to_index._non_padded_namespaces.update(vocab._non_padded_namespaces)\n        self._index_to_token._non_padded_namespaces.update(vocab._non_padded_namespaces)\n        for namespace in vocab.get_namespaces():\n            for token in vocab.get_token_to_index_vocabulary(namespace):\n                self.add_token_to_namespace(token, namespace)\n\n    def _extend(\n        self,\n        counter: Dict[str, Dict[str, int]] = None,\n        min_count: Dict[str, int] = None,\n        max_vocab_size: Union[int, Dict[str, int]] = None,\n        non_padded_namespaces: Iterable[str] = DEFAULT_NON_PADDED_NAMESPACES,\n        pretrained_files: Optional[Dict[str, str]] = None,\n        only_include_pretrained_words: bool = False,\n        tokens_to_add: Dict[str, List[str]] = None,\n        min_pretrained_embeddings: Dict[str, int] = None,\n    ) -> None:\n        """"""\n        This method can be used for extending already generated vocabulary.  It takes same\n        parameters as Vocabulary initializer. The `_token_to_index` and `_index_to_token`\n        mappings of calling vocabulary will be retained.  It is an inplace operation so None will be\n        returned.\n        """"""\n        if not isinstance(max_vocab_size, dict):\n            int_max_vocab_size = max_vocab_size\n            max_vocab_size = defaultdict(lambda: int_max_vocab_size)  # type: ignore\n        min_count = min_count or {}\n        pretrained_files = pretrained_files or {}\n        min_pretrained_embeddings = min_pretrained_embeddings or {}\n        non_padded_namespaces = set(non_padded_namespaces)\n        counter = counter or {}\n        tokens_to_add = tokens_to_add or {}\n\n        self._retained_counter = counter\n        # Make sure vocabulary extension is safe.\n        current_namespaces = {*self._token_to_index}\n        extension_namespaces = {*counter, *tokens_to_add}\n\n        for namespace in current_namespaces & extension_namespaces:\n            # if new namespace was already present\n            # Either both should be padded or none should be.\n            original_padded = not any(\n                namespace_match(pattern, namespace) for pattern in self._non_padded_namespaces\n            )\n            extension_padded = not any(\n                namespace_match(pattern, namespace) for pattern in non_padded_namespaces\n            )\n            if original_padded != extension_padded:\n                raise ConfigurationError(\n                    ""Common namespace {} has conflicting "".format(namespace)\n                    + ""setting of padded = True/False. ""\n                    + ""Hence extension cannot be done.""\n                )\n\n        # Add new non-padded namespaces for extension\n        self._token_to_index.add_non_padded_namespaces(non_padded_namespaces)\n        self._index_to_token.add_non_padded_namespaces(non_padded_namespaces)\n        self._non_padded_namespaces.update(non_padded_namespaces)\n\n        for namespace in counter:\n            if namespace in pretrained_files:\n                pretrained_list = _read_pretrained_tokens(pretrained_files[namespace])\n                min_embeddings = min_pretrained_embeddings.get(namespace, 0)\n                if min_embeddings > 0:\n                    tokens_old = tokens_to_add.get(namespace, [])\n                    tokens_new = pretrained_list[:min_embeddings]\n                    tokens_to_add[namespace] = tokens_old + tokens_new\n                pretrained_set = set(pretrained_list)\n            else:\n                pretrained_set = None\n            token_counts = list(counter[namespace].items())\n            token_counts.sort(key=lambda x: x[1], reverse=True)\n            try:\n                max_vocab = max_vocab_size[namespace]\n            except KeyError:\n                max_vocab = None\n            if max_vocab:\n                token_counts = token_counts[:max_vocab]\n            for token, count in token_counts:\n                if pretrained_set is not None:\n                    if only_include_pretrained_words:\n                        if token in pretrained_set and count >= min_count.get(namespace, 1):\n                            self.add_token_to_namespace(token, namespace)\n                    elif token in pretrained_set or count >= min_count.get(namespace, 1):\n                        self.add_token_to_namespace(token, namespace)\n                elif count >= min_count.get(namespace, 1):\n                    self.add_token_to_namespace(token, namespace)\n\n        for namespace, tokens in tokens_to_add.items():\n            for token in tokens:\n                self.add_token_to_namespace(token, namespace)\n\n    def __getstate__(self):\n        """"""\n        Need to sanitize defaultdict and defaultdict-like objects\n        by converting them to vanilla dicts when we pickle the vocabulary.\n        """"""\n        state = copy.copy(self.__dict__)\n        state[""_token_to_index""] = dict(state[""_token_to_index""])\n        state[""_index_to_token""] = dict(state[""_index_to_token""])\n\n        if ""_retained_counter"" in state:\n            state[""_retained_counter""] = {\n                key: dict(value) for key, value in state[""_retained_counter""].items()\n            }\n\n        return state\n\n    def __setstate__(self, state):\n        """"""\n        Conversely, when we unpickle, we need to reload the plain dicts\n        into our special DefaultDict subclasses.\n        """"""\n\n        self.__dict__ = copy.copy(state)\n        self._token_to_index = _TokenToIndexDefaultDict(\n            self._non_padded_namespaces, self._padding_token, self._oov_token\n        )\n        self._token_to_index.update(state[""_token_to_index""])\n        self._index_to_token = _IndexToTokenDefaultDict(\n            self._non_padded_namespaces, self._padding_token, self._oov_token\n        )\n        self._index_to_token.update(state[""_index_to_token""])\n\n    def save_to_files(self, directory: str) -> None:\n        """"""\n        Persist this Vocabulary to files so it can be reloaded later.\n        Each namespace corresponds to one file.\n\n        # Parameters\n\n        directory : `str`\n            The directory where we save the serialized vocabulary.\n        """"""\n        os.makedirs(directory, exist_ok=True)\n        if os.listdir(directory):\n            logging.warning(""vocabulary serialization directory %s is not empty"", directory)\n\n        # We use a lock file to avoid race conditions where multiple processes\n        # might be reading/writing from/to the same vocab files at once.\n        with FileLock(os.path.join(directory, "".lock"")):\n            with codecs.open(\n                os.path.join(directory, NAMESPACE_PADDING_FILE), ""w"", ""utf-8""\n            ) as namespace_file:\n                for namespace_str in self._non_padded_namespaces:\n                    print(namespace_str, file=namespace_file)\n\n            for namespace, mapping in self._index_to_token.items():\n                # Each namespace gets written to its own file, in index order.\n                with codecs.open(\n                    os.path.join(directory, namespace + "".txt""), ""w"", ""utf-8""\n                ) as token_file:\n                    num_tokens = len(mapping)\n                    start_index = 1 if mapping[0] == self._padding_token else 0\n                    for i in range(start_index, num_tokens):\n                        print(mapping[i].replace(""\\n"", ""@@NEWLINE@@""), file=token_file)\n\n    def is_padded(self, namespace: str) -> bool:\n        """"""\n        Returns whether or not there are padding and OOV tokens added to the given namespace.\n        """"""\n        return self._index_to_token[namespace][0] == self._padding_token\n\n    def add_token_to_namespace(self, token: str, namespace: str = ""tokens"") -> int:\n        """"""\n        Adds `token` to the index, if it is not already present.  Either way, we return the index of\n        the token.\n        """"""\n        if not isinstance(token, str):\n            raise ValueError(\n                ""Vocabulary tokens must be strings, or saving and loading will break.""\n                ""  Got %s (with type %s)"" % (repr(token), type(token))\n            )\n        if token not in self._token_to_index[namespace]:\n            index = len(self._token_to_index[namespace])\n            self._token_to_index[namespace][token] = index\n            self._index_to_token[namespace][index] = token\n            return index\n        else:\n            return self._token_to_index[namespace][token]\n\n    def add_tokens_to_namespace(self, tokens: List[str], namespace: str = ""tokens"") -> List[int]:\n        """"""\n        Adds `tokens` to the index, if they are not already present.  Either way, we return the\n        indices of the tokens in the order that they were given.\n        """"""\n        return [self.add_token_to_namespace(token, namespace) for token in tokens]\n\n    def get_index_to_token_vocabulary(self, namespace: str = ""tokens"") -> Dict[int, str]:\n        return self._index_to_token[namespace]\n\n    def get_token_to_index_vocabulary(self, namespace: str = ""tokens"") -> Dict[str, int]:\n        return self._token_to_index[namespace]\n\n    def get_token_index(self, token: str, namespace: str = ""tokens"") -> int:\n        try:\n            return self._token_to_index[namespace][token]\n        except KeyError:\n            try:\n                return self._token_to_index[namespace][self._oov_token]\n            except KeyError:\n                logger.error(""Namespace: %s"", namespace)\n                logger.error(""Token: %s"", token)\n                raise KeyError(\n                    f""\'{token}\' not found in vocab namespace \'{namespace}\', and namespace ""\n                    f""does not contain the default OOV token (\'{self._oov_token}\')""\n                )\n\n    def get_token_from_index(self, index: int, namespace: str = ""tokens"") -> str:\n        return self._index_to_token[namespace][index]\n\n    def get_vocab_size(self, namespace: str = ""tokens"") -> int:\n        return len(self._token_to_index[namespace])\n\n    def get_namespaces(self) -> Set[str]:\n        return set(self._index_to_token.keys())\n\n    def __eq__(self, other):\n        if isinstance(self, other.__class__):\n            return self.__dict__ == other.__dict__\n        return False\n\n    def __str__(self) -> str:\n        base_string = ""Vocabulary with namespaces:\\n""\n        non_padded_namespaces = f""\\tNon Padded Namespaces: {self._non_padded_namespaces}\\n""\n        namespaces = [\n            f""\\tNamespace: {name}, Size: {self.get_vocab_size(name)} \\n""\n            for name in self._index_to_token\n        ]\n        return "" "".join([base_string, non_padded_namespaces] + namespaces)\n\n    def __repr__(self) -> str:\n        # This is essentially the same as __str__, but with no newlines\n        base_string = ""Vocabulary with namespaces: ""\n        namespaces = [\n            f""{name}, Size: {self.get_vocab_size(name)} ||"" for name in self._index_to_token\n        ]\n        non_padded_namespaces = f""Non Padded Namespaces: {self._non_padded_namespaces}""\n        return "" "".join([base_string] + namespaces + [non_padded_namespaces])\n\n    def print_statistics(self) -> None:\n        if self._retained_counter:\n            logger.info(\n                ""Printed vocabulary statistics are only for the part of the vocabulary generated ""\n                ""from instances. If vocabulary is constructed by extending saved vocabulary with ""\n                ""dataset instances, the directly loaded portion won\'t be considered here.""\n            )\n            print(""\\n\\n----Vocabulary Statistics----\\n"")\n            # Since we don\'t saved counter info, it is impossible to consider pre-saved portion.\n            for namespace in self._retained_counter:\n                tokens_with_counts = list(self._retained_counter[namespace].items())\n                tokens_with_counts.sort(key=lambda x: x[1], reverse=True)\n                print(f""\\nTop 10 most frequent tokens in namespace \'{namespace}\':"")\n                for token, freq in tokens_with_counts[:10]:\n                    print(f""\\tToken: {token}\\t\\tFrequency: {freq}"")\n                # Now sort by token length, not frequency\n                tokens_with_counts.sort(key=lambda x: len(x[0]), reverse=True)\n\n                print(f""\\nTop 10 longest tokens in namespace \'{namespace}\':"")\n                for token, freq in tokens_with_counts[:10]:\n                    print(f""\\tToken: {token}\\t\\tlength: {len(token)}\\tFrequency: {freq}"")\n\n                print(f""\\nTop 10 shortest tokens in namespace \'{namespace}\':"")\n                for token, freq in reversed(tokens_with_counts[-10:]):\n                    print(f""\\tToken: {token}\\t\\tlength: {len(token)}\\tFrequency: {freq}"")\n        else:\n            # _retained_counter would be set only if instances were used for vocabulary construction.\n            logger.info(\n                ""Vocabulary statistics cannot be printed since ""\n                ""dataset instances were not used for its construction.""\n            )\n\n\n# We can\'t decorate `Vocabulary` with `Vocabulary.register()`, because `Vocabulary` hasn\'t been\n# defined yet.  So we put these down here.\nVocabulary.register(""from_instances"", constructor=""from_instances"")(Vocabulary)\nVocabulary.register(""from_files"", constructor=""from_files"")(Vocabulary)\nVocabulary.register(""extend"", constructor=""from_files_and_instances"")(Vocabulary)\nVocabulary.register(""empty"", constructor=""empty"")(Vocabulary)\n'"
allennlp/interpret/__init__.py,0,b'from allennlp.interpret.attackers.attacker import Attacker\nfrom allennlp.interpret.saliency_interpreters.saliency_interpreter import SaliencyInterpreter\n'
allennlp/models/__init__.py,0,"b'""""""\nThese submodules contain the classes for AllenNLP models,\nall of which are subclasses of `Model`.\n""""""\n\nfrom allennlp.models.model import Model\nfrom allennlp.models.archival import archive_model, load_archive, Archive\nfrom allennlp.models.simple_tagger import SimpleTagger\nfrom allennlp.models.basic_classifier import BasicClassifier\n'"
allennlp/models/archival.py,1,"b'""""""\nHelper functions for archiving models and restoring archived models.\n""""""\n\nfrom typing import NamedTuple\nimport atexit\nimport logging\nimport os\nimport tempfile\nimport tarfile\nimport shutil\n\nfrom torch.nn import Module\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.common.params import Params\nfrom allennlp.models.model import Model, _DEFAULT_WEIGHTS\n\nlogger = logging.getLogger(__name__)\n\n\nclass Archive(NamedTuple):\n    """""" An archive comprises a Model and its experimental config""""""\n\n    model: Model\n    config: Params\n\n    def extract_module(self, path: str, freeze: bool = True) -> Module:\n        """"""\n        This method can be used to load a module from the pretrained model archive.\n\n        It is also used implicitly in FromParams based construction. So instead of using standard\n        params to construct a module, you can instead load a pretrained module from the model\n        archive directly. For eg, instead of using params like {""type"": ""module_type"", ...}, you\n        can use the following template::\n\n            {\n                ""_pretrained"": {\n                    ""archive_file"": ""../path/to/model.tar.gz"",\n                    ""path"": ""path.to.module.in.model"",\n                    ""freeze"": False\n                }\n            }\n\n        If you use this feature with FromParams, take care of the following caveat: Call to\n        initializer(self) at end of model initializer can potentially wipe the transferred parameters\n        by reinitializing them. This can happen if you have setup initializer regex that also\n        matches parameters of the transferred module. To safe-guard against this, you can either\n        update your initializer regex to prevent conflicting match or add extra initializer::\n\n            [\n                ["".*transferred_module_name.*"", ""prevent""]]\n            ]\n\n        # Parameters\n\n        path : `str`, required\n            Path of target module to be loaded from the model.\n            Eg. ""_textfield_embedder.token_embedder_tokens""\n        freeze : `bool`, optional (default=`True`)\n            Whether to freeze the module parameters or not.\n\n        """"""\n        modules_dict = {path: module for path, module in self.model.named_modules()}\n        module = modules_dict.get(path)\n\n        if not module:\n            raise ConfigurationError(\n                f""You asked to transfer module at path {path} from ""\n                f""the model {type(self.model)}. But it\'s not present.""\n            )\n        if not isinstance(module, Module):\n            raise ConfigurationError(\n                f""The transferred object from model {type(self.model)} at path ""\n                f""{path} is not a PyTorch Module.""\n            )\n\n        for parameter in module.parameters():  # type: ignore\n            parameter.requires_grad_(not freeze)\n        return module\n\n\n# We archive a model by creating a tar.gz file with its weights, config, and vocabulary.\n#\n# These constants are the *known names* under which we archive them.\nCONFIG_NAME = ""config.json""\n_WEIGHTS_NAME = ""weights.th""\n\n\ndef archive_model(\n    serialization_dir: str, weights: str = _DEFAULT_WEIGHTS, archive_path: str = None\n) -> None:\n    """"""\n    Archive the model weights, its training configuration, and its vocabulary to `model.tar.gz`.\n\n    # Parameters\n\n    serialization_dir : `str`\n        The directory where the weights and vocabulary are written out.\n    weights : `str`, optional (default=`_DEFAULT_WEIGHTS`)\n        Which weights file to include in the archive. The default is `best.th`.\n    archive_path : `str`, optional, (default = `None`)\n        A full path to serialize the model to. The default is ""model.tar.gz"" inside the\n        serialization_dir. If you pass a directory here, we\'ll serialize the model\n        to ""model.tar.gz"" inside the directory.\n    """"""\n    weights_file = os.path.join(serialization_dir, weights)\n    if not os.path.exists(weights_file):\n        logger.error(""weights file %s does not exist, unable to archive model"", weights_file)\n        return\n\n    config_file = os.path.join(serialization_dir, CONFIG_NAME)\n    if not os.path.exists(config_file):\n        logger.error(""config file %s does not exist, unable to archive model"", config_file)\n\n    if archive_path is not None:\n        archive_file = archive_path\n        if os.path.isdir(archive_file):\n            archive_file = os.path.join(archive_file, ""model.tar.gz"")\n    else:\n        archive_file = os.path.join(serialization_dir, ""model.tar.gz"")\n    logger.info(""archiving weights and vocabulary to %s"", archive_file)\n    with tarfile.open(archive_file, ""w:gz"") as archive:\n        archive.add(config_file, arcname=CONFIG_NAME)\n        archive.add(weights_file, arcname=_WEIGHTS_NAME)\n        archive.add(os.path.join(serialization_dir, ""vocabulary""), arcname=""vocabulary"")\n\n\ndef load_archive(\n    archive_file: str,\n    cuda_device: int = -1,\n    opt_level: str = None,\n    overrides: str = """",\n    weights_file: str = None,\n) -> Archive:\n    """"""\n    Instantiates an Archive from an archived `tar.gz` file.\n\n    # Parameters\n\n    archive_file : `str`\n        The archive file to load the model from.\n    cuda_device : `int`, optional (default = `-1`)\n        If `cuda_device` is >= 0, the model will be loaded onto the\n        corresponding GPU. Otherwise it will be loaded onto the CPU.\n    opt_level : `str`, optional, (default = `None`)\n        Each `opt_level` establishes a set of properties that govern Amp\xe2\x80\x99s implementation of pure or mixed\n        precision training. Must be a choice of `""O0""`, `""O1""`, `""O2""`, or `""O3""`.\n        See the Apex [documentation](https://nvidia.github.io/apex/amp.html#opt-levels-and-properties) for\n        more details. If `None`, defaults to the `opt_level` found in the model params. If `cuda_device==-1`,\n        Amp is not used and this argument is ignored.\n    overrides : `str`, optional (default = `""""`)\n        JSON overrides to apply to the unarchived `Params` object.\n    weights_file : `str`, optional (default = `None`)\n        The weights file to use.  If unspecified, weights.th in the archive_file will be used.\n    """"""\n    # redirect to the cache, if necessary\n    resolved_archive_file = cached_path(archive_file)\n\n    if resolved_archive_file == archive_file:\n        logger.info(f""loading archive file {archive_file}"")\n    else:\n        logger.info(f""loading archive file {archive_file} from cache at {resolved_archive_file}"")\n\n    if os.path.isdir(resolved_archive_file):\n        serialization_dir = resolved_archive_file\n    else:\n        # Extract archive to temp dir\n        tempdir = tempfile.mkdtemp()\n        logger.info(f""extracting archive file {resolved_archive_file} to temp dir {tempdir}"")\n        with tarfile.open(resolved_archive_file, ""r:gz"") as archive:\n            archive.extractall(tempdir)\n        # Postpone cleanup until exit in case the unarchived contents are needed outside\n        # this function.\n        atexit.register(_cleanup_archive_dir, tempdir)\n\n        serialization_dir = tempdir\n\n    # Load config\n    config = Params.from_file(os.path.join(serialization_dir, CONFIG_NAME), overrides)\n\n    if weights_file:\n        weights_path = weights_file\n    else:\n        weights_path = os.path.join(serialization_dir, _WEIGHTS_NAME)\n        # Fallback for serialization directories.\n        if not os.path.exists(weights_path):\n            weights_path = os.path.join(serialization_dir, _DEFAULT_WEIGHTS)\n\n    # Instantiate model. Use a duplicate of the config, as it will get consumed.\n    model = Model.load(\n        config.duplicate(),\n        weights_file=weights_path,\n        serialization_dir=serialization_dir,\n        cuda_device=cuda_device,\n        opt_level=opt_level,\n    )\n\n    return Archive(model=model, config=config)\n\n\ndef _cleanup_archive_dir(path: str):\n    if os.path.exists(path):\n        logger.info(""removing temporary unarchived model dir at %s"", path)\n        shutil.rmtree(path)\n'"
allennlp/models/basic_classifier.py,12,"b'from typing import Dict, Optional\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.data import TextFieldTensors, Vocabulary\nfrom allennlp.models.model import Model\nfrom allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder\nfrom allennlp.nn import InitializerApplicator, util\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.training.metrics import CategoricalAccuracy\n\n\n@Model.register(""basic_classifier"")\nclass BasicClassifier(Model):\n    """"""\n    This `Model` implements a basic text classifier. After embedding the text into\n    a text field, we will optionally encode the embeddings with a `Seq2SeqEncoder`. The\n    resulting sequence is pooled using a `Seq2VecEncoder` and then passed to\n    a linear classification layer, which projects into the label space. If a\n    `Seq2SeqEncoder` is not provided, we will pass the embedded text directly to the\n    `Seq2VecEncoder`.\n\n    Registered as a `Model` with name ""basic_classifier"".\n\n    # Parameters\n\n    vocab : `Vocabulary`\n    text_field_embedder : `TextFieldEmbedder`\n        Used to embed the input text into a `TextField`\n    seq2seq_encoder : `Seq2SeqEncoder`, optional (default=`None`)\n        Optional Seq2Seq encoder layer for the input text.\n    seq2vec_encoder : `Seq2VecEncoder`\n        Required Seq2Vec encoder layer. If `seq2seq_encoder` is provided, this encoder\n        will pool its output. Otherwise, this encoder will operate directly on the output\n        of the `text_field_embedder`.\n    feedforward : `FeedForward`, optional, (default = `None`)\n        An optional feedforward layer to apply after the seq2vec_encoder.\n    dropout : `float`, optional (default = `None`)\n        Dropout percentage to use.\n    num_labels : `int`, optional (default = `None`)\n        Number of labels to project to in classification layer. By default, the classification layer will\n        project to the size of the vocabulary namespace corresponding to labels.\n    label_namespace : `str`, optional (default = `""labels""`)\n        Vocabulary namespace corresponding to labels. By default, we use the ""labels"" namespace.\n    initializer : `InitializerApplicator`, optional (default=`InitializerApplicator()`)\n        If provided, will be used to initialize the model parameters.\n    """"""\n\n    def __init__(\n        self,\n        vocab: Vocabulary,\n        text_field_embedder: TextFieldEmbedder,\n        seq2vec_encoder: Seq2VecEncoder,\n        seq2seq_encoder: Seq2SeqEncoder = None,\n        feedforward: Optional[FeedForward] = None,\n        dropout: float = None,\n        num_labels: int = None,\n        label_namespace: str = ""labels"",\n        namespace: str = ""tokens"",\n        initializer: InitializerApplicator = InitializerApplicator(),\n        **kwargs,\n    ) -> None:\n\n        super().__init__(vocab, **kwargs)\n        self._text_field_embedder = text_field_embedder\n\n        if seq2seq_encoder:\n            self._seq2seq_encoder = seq2seq_encoder\n        else:\n            self._seq2seq_encoder = None\n\n        self._seq2vec_encoder = seq2vec_encoder\n        self._feedforward = feedforward\n        if feedforward is not None:\n            self._classifier_input_dim = self._feedforward.get_output_dim()\n        else:\n            self._classifier_input_dim = self._seq2vec_encoder.get_output_dim()\n\n        if dropout:\n            self._dropout = torch.nn.Dropout(dropout)\n        else:\n            self._dropout = None\n        self._label_namespace = label_namespace\n        self._namespace = namespace\n\n        if num_labels:\n            self._num_labels = num_labels\n        else:\n            self._num_labels = vocab.get_vocab_size(namespace=self._label_namespace)\n        self._classification_layer = torch.nn.Linear(self._classifier_input_dim, self._num_labels)\n        self._accuracy = CategoricalAccuracy()\n        self._loss = torch.nn.CrossEntropyLoss()\n        initializer(self)\n\n    def forward(  # type: ignore\n        self, tokens: TextFieldTensors, label: torch.IntTensor = None\n    ) -> Dict[str, torch.Tensor]:\n\n        """"""\n        # Parameters\n\n        tokens : `TextFieldTensors`\n            From a `TextField`\n        label : `torch.IntTensor`, optional (default = `None`)\n            From a `LabelField`\n\n        # Returns\n\n        An output dictionary consisting of:\n\n            - `logits` (`torch.FloatTensor`) :\n                A tensor of shape `(batch_size, num_labels)` representing\n                unnormalized log probabilities of the label.\n            - `probs` (`torch.FloatTensor`) :\n                A tensor of shape `(batch_size, num_labels)` representing\n                probabilities of the label.\n            - `loss` : (`torch.FloatTensor`, optional) :\n                A scalar loss to be optimised.\n        """"""\n        embedded_text = self._text_field_embedder(tokens)\n        mask = get_text_field_mask(tokens)\n\n        if self._seq2seq_encoder:\n            embedded_text = self._seq2seq_encoder(embedded_text, mask=mask)\n\n        embedded_text = self._seq2vec_encoder(embedded_text, mask=mask)\n\n        if self._dropout:\n            embedded_text = self._dropout(embedded_text)\n\n        if self._feedforward is not None:\n            embedded_text = self._feedforward(embedded_text)\n\n        logits = self._classification_layer(embedded_text)\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n\n        output_dict = {""logits"": logits, ""probs"": probs}\n        output_dict[""token_ids""] = util.get_token_ids_from_text_field_tensors(tokens)\n        if label is not None:\n            loss = self._loss(logits, label.long().view(-1))\n            output_dict[""loss""] = loss\n            self._accuracy(logits, label)\n\n        return output_dict\n\n    @overrides\n    def make_output_human_readable(\n        self, output_dict: Dict[str, torch.Tensor]\n    ) -> Dict[str, torch.Tensor]:\n        """"""\n        Does a simple argmax over the probabilities, converts index to string label, and\n        add `""label""` key to the dictionary with the result.\n        """"""\n        predictions = output_dict[""probs""]\n        if predictions.dim() == 2:\n            predictions_list = [predictions[i] for i in range(predictions.shape[0])]\n        else:\n            predictions_list = [predictions]\n        classes = []\n        for prediction in predictions_list:\n            label_idx = prediction.argmax(dim=-1).item()\n            label_str = self.vocab.get_index_to_token_vocabulary(self._label_namespace).get(\n                label_idx, str(label_idx)\n            )\n            classes.append(label_str)\n        output_dict[""label""] = classes\n        tokens = []\n        for instance_tokens in output_dict[""token_ids""]:\n            tokens.append(\n                [\n                    self.vocab.get_token_from_index(token_id.item(), namespace=self._namespace)\n                    for token_id in instance_tokens\n                ]\n            )\n        output_dict[""tokens""] = tokens\n        return output_dict\n\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        metrics = {""accuracy"": self._accuracy.get_metric(reset)}\n        return metrics\n\n    default_predictor = ""text_classifier""\n'"
allennlp/models/model.py,13,"b'""""""\n`Model` is an abstract class representing\nan AllenNLP model.\n""""""\n\nimport logging\nimport os\nfrom typing import Dict, Union, List, Set, Type, Optional\n\ntry:\n    from apex import amp\nexcept ImportError:\n    amp = None\nimport numpy\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.params import Params\nfrom allennlp.common.registrable import Registrable\nfrom allennlp.data import Instance, Vocabulary\nfrom allennlp.data.batch import Batch\nfrom allennlp.nn import util\nfrom allennlp.nn.regularizers import RegularizerApplicator\n\nlogger = logging.getLogger(__name__)\n\n# When training a model, many sets of weights are saved. By default we want to\n# save/load this set of weights.\n_DEFAULT_WEIGHTS = ""best.th""\n\n\nclass Model(torch.nn.Module, Registrable):\n    """"""\n    This abstract class represents a model to be trained. Rather than relying completely\n    on the Pytorch Module, we modify the output spec of `forward` to be a dictionary.\n\n    Models built using this API are still compatible with other pytorch models and can\n    be used naturally as modules within other models - outputs are dictionaries, which\n    can be unpacked and passed into other layers. One caveat to this is that if you\n    wish to use an AllenNLP model inside a Container (such as nn.Sequential), you must\n    interleave the models with a wrapper module which unpacks the dictionary into\n    a list of tensors.\n\n    In order for your model to be trained using the [`Trainer`](../training/trainer.md)\n    api, the output dictionary of your Model must include a ""loss"" key, which will be\n    optimised during the training process.\n\n    Finally, you can optionally implement :func:`Model.get_metrics` in order to make use\n    of early stopping and best-model serialization based on a validation metric in\n    `Trainer`. Metrics that begin with ""_"" will not be logged\n    to the progress bar by `Trainer`.\n\n    The `from_archive` method on this class is registered as a `Model` with name ""from_archive"".\n    So, if you are using a configuration file, you can specify a model as `{""type"": ""from_archive"",\n    ""archive_file"": ""/path/to/archive.tar.gz""}`, which will pull out the model from the given\n    location and return it.\n\n    # Parameters\n\n    vocab: `Vocabulary`\n        There are two typical use-cases for the `Vocabulary` in a `Model`: getting vocabulary sizes\n        when constructing embedding matrices or output classifiers (as the vocabulary holds the\n        number of classes in your output, also), and translating model output into human-readable\n        form.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""model"", it gets specified as a top-level parameter, then is passed in to the model\n        separately.\n    regularizer: `RegularizerApplicator`, optional\n        If given, the `Trainer` will use this to regularize model parameters.\n    """"""\n\n    _warn_for_unseparable_batches: Set[str] = set()\n    default_predictor: Optional[str] = None\n\n    def __init__(self, vocab: Vocabulary, regularizer: RegularizerApplicator = None) -> None:\n        super().__init__()\n        self.vocab = vocab\n        self._regularizer = regularizer\n\n    def get_regularization_penalty(self) -> Union[float, torch.Tensor]:\n        """"""\n        Computes the regularization penalty for the model.\n        Returns 0 if the model was not configured to use regularization.\n        """"""\n        if self._regularizer is None:\n            return 0.0\n        else:\n            return self._regularizer(self)\n\n    def get_parameters_for_histogram_tensorboard_logging(self) -> List[str]:\n        """"""\n        Returns the name of model parameters used for logging histograms to tensorboard.\n        """"""\n        return [name for name, _ in self.named_parameters()]\n\n    def forward(self, *inputs) -> Dict[str, torch.Tensor]:\n        """"""\n        Defines the forward pass of the model. In addition, to facilitate easy training,\n        this method is designed to compute a loss function defined by a user.\n\n        The input is comprised of everything required to perform a\n        training update, `including` labels - you define the signature here!\n        It is down to the user to ensure that inference can be performed\n        without the presence of these labels. Hence, any inputs not available at\n        inference time should only be used inside a conditional block.\n\n        The intended sketch of this method is as follows::\n\n            def forward(self, input1, input2, targets=None):\n                ....\n                ....\n                output1 = self.layer1(input1)\n                output2 = self.layer2(input2)\n                output_dict = {""output1"": output1, ""output2"": output2}\n                if targets is not None:\n                    # Function returning a scalar torch.Tensor, defined by the user.\n                    loss = self._compute_loss(output1, output2, targets)\n                    output_dict[""loss""] = loss\n                return output_dict\n\n        # Parameters\n\n        *inputs : `Any`\n            Tensors comprising everything needed to perform a training update, `including` labels,\n            which should be optional (i.e have a default value of `None`).  At inference time,\n            simply pass the relevant inputs, not including the labels.\n\n        # Returns\n\n        output_dict : `Dict[str, torch.Tensor]`\n            The outputs from the model. In order to train a model using the\n            `Trainer` api, you must provide a ""loss"" key pointing to a\n            scalar `torch.Tensor` representing the loss to be optimized.\n        """"""\n        raise NotImplementedError\n\n    def forward_on_instance(self, instance: Instance) -> Dict[str, numpy.ndarray]:\n        """"""\n        Takes an [`Instance`](../data/instance.md), which typically has raw text in it, converts\n        that text into arrays using this model\'s [`Vocabulary`](../data/vocabulary.md), passes those\n        arrays through `self.forward()` and `self.make_output_human_readable()` (which by default\n        does nothing) and returns the result.  Before returning the result, we convert any\n        `torch.Tensors` into numpy arrays and remove the batch dimension.\n        """"""\n        return self.forward_on_instances([instance])[0]\n\n    def forward_on_instances(self, instances: List[Instance]) -> List[Dict[str, numpy.ndarray]]:\n        """"""\n        Takes a list of `Instances`, converts that text into arrays using this model\'s `Vocabulary`,\n        passes those arrays through `self.forward()` and `self.make_output_human_readable()` (which\n        by default does nothing) and returns the result.  Before returning the result, we convert\n        any `torch.Tensors` into numpy arrays and separate the batched output into a list of\n        individual dicts per instance. Note that typically this will be faster on a GPU (and\n        conditionally, on a CPU) than repeated calls to `forward_on_instance`.\n\n        # Parameters\n\n        instances : `List[Instance]`, required\n            The instances to run the model on.\n\n        # Returns\n\n        A list of the models output for each instance.\n        """"""\n        batch_size = len(instances)\n        with torch.no_grad():\n            cuda_device = self._get_prediction_device()\n            dataset = Batch(instances)\n            dataset.index_instances(self.vocab)\n            model_input = util.move_to_device(dataset.as_tensor_dict(), cuda_device)\n            outputs = self.make_output_human_readable(self(**model_input))\n\n            instance_separated_output: List[Dict[str, numpy.ndarray]] = [\n                {} for _ in dataset.instances\n            ]\n            for name, output in list(outputs.items()):\n                if isinstance(output, torch.Tensor):\n                    # NOTE(markn): This is a hack because 0-dim pytorch tensors are not iterable.\n                    # This occurs with batch size 1, because we still want to include the loss in that case.\n                    if output.dim() == 0:\n                        output = output.unsqueeze(0)\n\n                    if output.size(0) != batch_size:\n                        self._maybe_warn_for_unseparable_batches(name)\n                        continue\n                    output = output.detach().cpu().numpy()\n                elif len(output) != batch_size:\n                    self._maybe_warn_for_unseparable_batches(name)\n                    continue\n                for instance_output, batch_element in zip(instance_separated_output, output):\n                    instance_output[name] = batch_element\n            return instance_separated_output\n\n    def make_output_human_readable(\n        self, output_dict: Dict[str, torch.Tensor]\n    ) -> Dict[str, torch.Tensor]:\n        """"""\n        Takes the result of `forward` and makes it human readable.  Most of the time, the only thing\n        this method does is convert tokens / predicted labels from tensors to strings that humans\n        might actually understand.  Somtimes you\'ll also do an argmax or something in here, too, but\n        that most often happens in `Model.forward`, before you compute your metrics.\n\n        This method `modifies` the input dictionary, and also `returns` the same dictionary.\n\n        By default in the base class we do nothing.\n        """"""\n\n        return output_dict\n\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        """"""\n        Returns a dictionary of metrics. This method will be called by\n        `allennlp.training.Trainer` in order to compute and use model metrics for early\n        stopping and model serialization.  We return an empty dictionary here rather than raising\n        as it is not required to implement metrics for a new model.  A boolean `reset` parameter is\n        passed, as frequently a metric accumulator will have some state which should be reset\n        between epochs. This is also compatible with [`Metric`s](../training/metrics/metric.md). Metrics\n        should be populated during the call to `forward`, with the `Metric` handling the accumulation of\n        the metric until this method is called.\n        """"""\n\n        return {}\n\n    def _get_prediction_device(self) -> int:\n        """"""\n        This method checks the device of the model parameters to determine the cuda_device\n        this model should be run on for predictions.  If there are no parameters, it returns -1.\n\n        # Returns\n\n        The cuda device this model should run on for predictions.\n        """"""\n        devices = {util.get_device_of(param) for param in self.parameters()}\n\n        if len(devices) > 1:\n            devices_string = "", "".join(str(x) for x in devices)\n            raise ConfigurationError(f""Parameters have mismatching cuda_devices: {devices_string}"")\n        elif len(devices) == 1:\n            return devices.pop()\n        else:\n            return -1\n\n    def _maybe_warn_for_unseparable_batches(self, output_key: str):\n        """"""\n        This method warns once if a user implements a model which returns a dictionary with\n        values which we are unable to split back up into elements of the batch. This is controlled\n        by a class attribute `_warn_for_unseperable_batches` because it would be extremely verbose\n        otherwise.\n        """"""\n        if output_key not in self._warn_for_unseparable_batches:\n            logger.warning(\n                f""Encountered the {output_key} key in the model\'s return dictionary which ""\n                ""couldn\'t be split by the batch size. Key will be ignored.""\n            )\n            # We only want to warn once for this key,\n            # so we set this to false so we don\'t warn again.\n            self._warn_for_unseparable_batches.add(output_key)\n\n    @classmethod\n    def _load(\n        cls,\n        config: Params,\n        serialization_dir: str,\n        weights_file: Optional[str] = None,\n        cuda_device: int = -1,\n        opt_level: Optional[str] = None,\n    ) -> ""Model"":\n        """"""\n        Instantiates an already-trained model, based on the experiment\n        configuration and some optional overrides.\n        """"""\n        weights_file = weights_file or os.path.join(serialization_dir, _DEFAULT_WEIGHTS)\n\n        # Load vocabulary from file\n        vocab_dir = os.path.join(serialization_dir, ""vocabulary"")\n        # If the config specifies a vocabulary subclass, we need to use it.\n        vocab_params = config.get(""vocabulary"", Params({}))\n        vocab_choice = vocab_params.pop_choice(""type"", Vocabulary.list_available(), True)\n        vocab_class, _ = Vocabulary.resolve_class_name(vocab_choice)\n        vocab = vocab_class.from_files(\n            vocab_dir, vocab_params.get(""padding_token""), vocab_params.get(""oov_token"")\n        )\n\n        model_params = config.get(""model"")\n\n        training_params = config.get(""trainer"", Params({}))\n        opt_level = opt_level or training_params.get(""opt_level"")\n\n        # The experiment config tells us how to _train_ a model, including where to get pre-trained\n        # embeddings from.  We\'re now _loading_ the model, so those embeddings will already be\n        # stored in our weights.  We don\'t need any pretrained weight file anymore, and we don\'t\n        # want the code to look for it, so we remove it from the parameters here.\n        remove_pretrained_embedding_params(model_params)\n        model = Model.from_params(vocab=vocab, params=model_params)\n\n        # Force model to cpu or gpu, as appropriate, to make sure that the embeddings are\n        # in sync with the weights\n        if cuda_device >= 0:\n            model.cuda(cuda_device)\n        else:\n            model.cpu()\n\n        # If opt_level is not None (i.e. it exists in the loaded models params or was provided\n        # as argument to this method), call amp.initialize on the loaded model.\n        # Log a warning if amp is not installed or we are loading onto the cpu so that these\n        # cases do not pass silently.\n        if opt_level is not None:\n            if amp is None:\n                logger.warning(\n                    (\n                        f""Apex must be installed to enable mixed-precision via amp.""\n                        f"" Got opt_level is not None (opt_level={opt_level}) but Apex is not installed.""\n                        "" Any further training or inference will happen at full-precision.""\n                    )\n                )\n            if cuda_device == -1:\n                logger.warning(\n                    (\n                        f""A CUDA device must be specified to enable mixed-precision via amp.""\n                        f"" Got cuda_device=={cuda_device} but opt_level is not None (opt_level={opt_level}).""\n                        "" Any further training or inference will happen at full-precision.""\n                    )\n                )\n            if amp is not None and cuda_device >= 0:\n                model = amp.initialize(model, opt_level=opt_level)\n\n        # If vocab+embedding extension was done, the model initialized from from_params\n        # and one defined by state dict in weights_file might not have same embedding shapes.\n        # Eg. when model embedder module was transferred along with vocab extension, the\n        # initialized embedding weight shape would be smaller than one in the state_dict.\n        # So calling model embedding extension is required before load_state_dict.\n        # If vocab and model embeddings are in sync, following would be just a no-op.\n        model.extend_embedder_vocab()\n\n        model_state = torch.load(weights_file, map_location=util.device_mapping(cuda_device))\n        model.load_state_dict(model_state)\n\n        return model\n\n    @classmethod\n    def load(\n        cls,\n        config: Params,\n        serialization_dir: str,\n        weights_file: Optional[str] = None,\n        cuda_device: int = -1,\n        opt_level: Optional[str] = None,\n    ) -> ""Model"":\n        """"""\n        Instantiates an already-trained model, based on the experiment\n        configuration and some optional overrides.\n\n        # Parameters\n\n        config : `Params`\n            The configuration that was used to train the model. It should definitely\n            have a `model` section, and should probably have a `trainer` section\n            as well.\n        serialization_dir: `str = None`\n            The directory containing the serialized weights, parameters, and vocabulary\n            of the model.\n        weights_file: `str = None`\n            By default we load the weights from `best.th` in the serialization\n            directory, but you can override that value here.\n        cuda_device: `int = -1`\n            By default we load the model on the CPU, but if you want to load it\n            for GPU usage you can specify the id of your GPU here\n        opt_level : `str`, optional (default = `None`)\n            Each `opt_level` establishes a set of properties that govern Amp\xe2\x80\x99s implementation of pure or mixed\n            precision training. Must be a choice of `""O0""`, `""O1""`, `""O2""`, or `""O3""`.\n            See the Apex [documentation](https://nvidia.github.io/apex/amp.html#opt-levels-and-properties) for\n            more details. If `None`, defaults to the `opt_level` found in the model params. If `cuda_device==-1`,\n            Amp is not used and this argument is ignored.\n\n        # Returns\n\n        model : `Model`\n            The model specified in the configuration, loaded with the serialized\n            vocabulary and the trained weights.\n        """"""\n\n        # Peak at the class of the model.\n        model_type = (\n            config[""model""] if isinstance(config[""model""], str) else config[""model""][""type""]\n        )\n\n        # Load using an overridable _load method.\n        # This allows subclasses of Model to override _load.\n\n        model_class: Type[Model] = cls.by_name(model_type)  # type: ignore\n        if not isinstance(model_class, type):\n            # If you\'re using from_archive to specify your model (e.g., for fine tuning), then you\n            # can\'t currently override the behavior of _load; we just use the default Model._load.\n            # If we really need to change this, we would need to implement a recursive\n            # get_model_class method, that recurses whenever it finds a from_archive model type.\n            model_class = Model\n        return model_class._load(config, serialization_dir, weights_file, cuda_device, opt_level)\n\n    def extend_embedder_vocab(self, embedding_sources_mapping: Dict[str, str] = None) -> None:\n        """"""\n        Iterates through all embedding modules in the model and assures it can embed\n        with the extended vocab. This is required in fine-tuning or transfer learning\n        scenarios where model was trained with original vocabulary but during\n        fine-tuning/transfer-learning, it will have it work with extended vocabulary\n        (original + new-data vocabulary).\n\n        # Parameters\n\n        embedding_sources_mapping : `Dict[str, str]`, optional (default = `None`)\n            Mapping from model_path to pretrained-file path of the embedding\n            modules. If pretrained-file used at time of embedding initialization\n            isn\'t available now, user should pass this mapping. Model path is\n            path traversing the model attributes upto this embedding module.\n            Eg. ""_text_field_embedder.token_embedder_tokens"".\n        """"""\n        # self.named_modules() gives all sub-modules (including nested children)\n        # The path nesting is already separated by ""."": eg. parent_module_name.child_module_name\n        embedding_sources_mapping = embedding_sources_mapping or {}\n        for model_path, module in self.named_modules():\n            if hasattr(module, ""extend_vocab""):\n                pretrained_file = embedding_sources_mapping.get(model_path)\n                module.extend_vocab(\n                    self.vocab, extension_pretrained_file=pretrained_file, model_path=model_path,\n                )\n\n    @classmethod\n    def from_archive(cls, archive_file: str, vocab: Vocabulary = None) -> ""Model"":\n        """"""\n        Loads a model from an archive file.  This basically just calls\n        `return archival.load_archive(archive_file).model`.  It exists as a method here for\n        convenience, and so that we can register it for easy use for fine tuning an existing model\n        from a config file.\n\n        If `vocab` is given, we will extend the loaded model\'s vocabulary using the passed vocab\n        object (including calling `extend_embedder_vocab`, which extends embedding layers).\n        """"""\n        from allennlp.models.archival import load_archive  # here to avoid circular imports\n\n        model = load_archive(archive_file).model\n        if vocab:\n            model.vocab.extend_from_vocab(vocab)\n            model.extend_embedder_vocab()\n        return model\n\n\n# We can\'t decorate `Model` with `Model.register()`, because `Model` hasn\'t been defined yet.  So we\n# put this down here.\nModel.register(""from_archive"", constructor=""from_archive"")(Model)\n\n\ndef remove_pretrained_embedding_params(params: Params):\n    if isinstance(params, Params):  # The model could possibly be a string, for example.\n        keys = params.keys()\n        if ""pretrained_file"" in keys:\n            del params[""pretrained_file""]\n        for value in params.values():\n            if isinstance(value, Params):\n                remove_pretrained_embedding_params(value)\n'"
allennlp/models/simple_tagger.py,10,"b'from typing import Dict, Optional, List, Any\n\nimport numpy\nfrom overrides import overrides\nimport torch\nfrom torch.nn.modules.linear import Linear\nimport torch.nn.functional as F\n\nfrom allennlp.common.checks import check_dimensions_match, ConfigurationError\nfrom allennlp.data import TextFieldTensors, Vocabulary\nfrom allennlp.modules import Seq2SeqEncoder, TimeDistributed, TextFieldEmbedder\nfrom allennlp.models.model import Model\nfrom allennlp.nn import InitializerApplicator\nfrom allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\nfrom allennlp.training.metrics import CategoricalAccuracy, SpanBasedF1Measure\n\n\n@Model.register(""simple_tagger"")\nclass SimpleTagger(Model):\n    """"""\n    This `SimpleTagger` simply encodes a sequence of text with a stacked `Seq2SeqEncoder`, then\n    predicts a tag for each token in the sequence.\n\n    Registered as a `Model` with name ""simple_tagger"".\n\n    # Parameters\n\n    vocab : `Vocabulary`, required\n        A Vocabulary, required in order to compute sizes for input/output projections.\n    text_field_embedder : `TextFieldEmbedder`, required\n        Used to embed the `tokens` `TextField` we get as input to the model.\n    encoder : `Seq2SeqEncoder`\n        The encoder (with its own internal stacking) that we will use in between embedding tokens\n        and predicting output tags.\n    calculate_span_f1 : `bool`, optional (default=`None`)\n        Calculate span-level F1 metrics during training. If this is `True`, then\n        `label_encoding` is required. If `None` and\n        label_encoding is specified, this is set to `True`.\n        If `None` and label_encoding is not specified, it defaults\n        to `False`.\n    label_encoding : `str`, optional (default=`None`)\n        Label encoding to use when calculating span f1.\n        Valid options are ""BIO"", ""BIOUL"", ""IOB1"", ""BMES"".\n        Required if `calculate_span_f1` is true.\n    label_namespace : `str`, optional (default=`labels`)\n        This is needed to compute the SpanBasedF1Measure metric, if desired.\n        Unless you did something unusual, the default value should be what you want.\n    verbose_metrics : `bool`, optional (default = `False`)\n        If true, metrics will be returned per label class in addition\n        to the overall statistics.\n    initializer : `InitializerApplicator`, optional (default=`InitializerApplicator()`)\n        Used to initialize the model parameters.\n    """"""\n\n    def __init__(\n        self,\n        vocab: Vocabulary,\n        text_field_embedder: TextFieldEmbedder,\n        encoder: Seq2SeqEncoder,\n        calculate_span_f1: bool = None,\n        label_encoding: Optional[str] = None,\n        label_namespace: str = ""labels"",\n        verbose_metrics: bool = False,\n        initializer: InitializerApplicator = InitializerApplicator(),\n        **kwargs,\n    ) -> None:\n        super().__init__(vocab, **kwargs)\n\n        self.label_namespace = label_namespace\n        self.text_field_embedder = text_field_embedder\n        self.num_classes = self.vocab.get_vocab_size(label_namespace)\n        self.encoder = encoder\n        self._verbose_metrics = verbose_metrics\n        self.tag_projection_layer = TimeDistributed(\n            Linear(self.encoder.get_output_dim(), self.num_classes)\n        )\n\n        check_dimensions_match(\n            text_field_embedder.get_output_dim(),\n            encoder.get_input_dim(),\n            ""text field embedding dim"",\n            ""encoder input dim"",\n        )\n\n        self.metrics = {\n            ""accuracy"": CategoricalAccuracy(),\n            ""accuracy3"": CategoricalAccuracy(top_k=3),\n        }\n\n        # We keep calculate_span_f1 as a constructor argument for API consistency with\n        # the CrfTagger, even it is redundant in this class\n        # (label_encoding serves the same purpose).\n        if calculate_span_f1 is None:\n            calculate_span_f1 = label_encoding is not None\n\n        self.calculate_span_f1 = calculate_span_f1\n        if calculate_span_f1:\n            if not label_encoding:\n                raise ConfigurationError(\n                    ""calculate_span_f1 is True, but no label_encoding was specified.""\n                )\n            self._f1_metric = SpanBasedF1Measure(\n                vocab, tag_namespace=label_namespace, label_encoding=label_encoding\n            )\n        else:\n            self._f1_metric = None\n\n        initializer(self)\n\n    @overrides\n    def forward(\n        self,  # type: ignore\n        tokens: TextFieldTensors,\n        tags: torch.LongTensor = None,\n        metadata: List[Dict[str, Any]] = None,\n        ignore_loss_on_o_tags: bool = False,\n    ) -> Dict[str, torch.Tensor]:\n\n        """"""\n        # Parameters\n\n        tokens : `TextFieldTensors`, required\n            The output of `TextField.as_array()`, which should typically be passed directly to a\n            `TextFieldEmbedder`. This output is a dictionary mapping keys to `TokenIndexer`\n            tensors.  At its most basic, using a `SingleIdTokenIndexer` this is : `{""tokens"":\n            Tensor(batch_size, num_tokens)}`. This dictionary will have the same keys as were used\n            for the `TokenIndexers` when you created the `TextField` representing your\n            sequence.  The dictionary is designed to be passed directly to a `TextFieldEmbedder`,\n            which knows how to combine different word representations into a single vector per\n            token in your input.\n        tags : `torch.LongTensor`, optional (default = `None`)\n            A torch tensor representing the sequence of integer gold class labels of shape\n            `(batch_size, num_tokens)`.\n        metadata : `List[Dict[str, Any]]`, optional, (default = `None`)\n            metadata containing the original words in the sentence to be tagged under a \'words\' key.\n        ignore_loss_on_o_tags : `bool`, optional (default = `False`)\n            If True, we compute the loss only for actual spans in `tags`, and not on `O` tokens.\n            This is useful for computing gradients of the loss on a _single span_, for\n            interpretation / attacking.\n\n        # Returns\n\n        An output dictionary consisting of:\n            - `logits` (`torch.FloatTensor`) :\n                A tensor of shape `(batch_size, num_tokens, tag_vocab_size)` representing\n                unnormalised log probabilities of the tag classes.\n            - `class_probabilities` (`torch.FloatTensor`) :\n                A tensor of shape `(batch_size, num_tokens, tag_vocab_size)` representing\n                a distribution of the tag classes per word.\n            - `loss` (`torch.FloatTensor`, optional) :\n                A scalar loss to be optimised.\n\n        """"""\n        embedded_text_input = self.text_field_embedder(tokens)\n        batch_size, sequence_length, _ = embedded_text_input.size()\n        mask = get_text_field_mask(tokens)\n        encoded_text = self.encoder(embedded_text_input, mask)\n\n        logits = self.tag_projection_layer(encoded_text)\n        reshaped_log_probs = logits.view(-1, self.num_classes)\n        class_probabilities = F.softmax(reshaped_log_probs, dim=-1).view(\n            [batch_size, sequence_length, self.num_classes]\n        )\n\n        output_dict = {""logits"": logits, ""class_probabilities"": class_probabilities}\n\n        if tags is not None:\n            if ignore_loss_on_o_tags:\n                o_tag_index = self.vocab.get_token_index(""O"", namespace=self.label_namespace)\n                tag_mask = mask & (tags != o_tag_index)\n            else:\n                tag_mask = mask\n            loss = sequence_cross_entropy_with_logits(logits, tags, tag_mask)\n            for metric in self.metrics.values():\n                metric(logits, tags, mask)\n            if self.calculate_span_f1:\n                self._f1_metric(logits, tags, mask)\n            output_dict[""loss""] = loss\n\n        if metadata is not None:\n            output_dict[""words""] = [x[""words""] for x in metadata]\n        return output_dict\n\n    @overrides\n    def make_output_human_readable(\n        self, output_dict: Dict[str, torch.Tensor]\n    ) -> Dict[str, torch.Tensor]:\n        """"""\n        Does a simple position-wise argmax over each token, converts indices to string labels, and\n        adds a `""tags""` key to the dictionary with the result.\n        """"""\n        all_predictions = output_dict[""class_probabilities""]\n        all_predictions = all_predictions.cpu().data.numpy()\n        if all_predictions.ndim == 3:\n            predictions_list = [all_predictions[i] for i in range(all_predictions.shape[0])]\n        else:\n            predictions_list = [all_predictions]\n        all_tags = []\n        for predictions in predictions_list:\n            argmax_indices = numpy.argmax(predictions, axis=-1)\n            tags = [\n                self.vocab.get_token_from_index(x, namespace=self.label_namespace)\n                for x in argmax_indices\n            ]\n            all_tags.append(tags)\n        output_dict[""tags""] = all_tags\n        return output_dict\n\n    @overrides\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        metrics_to_return = {\n            metric_name: metric.get_metric(reset) for metric_name, metric in self.metrics.items()\n        }\n\n        if self.calculate_span_f1:\n            f1_dict = self._f1_metric.get_metric(reset=reset)\n            if self._verbose_metrics:\n                metrics_to_return.update(f1_dict)\n            else:\n                metrics_to_return.update({x: y for x, y in f1_dict.items() if ""overall"" in x})\n        return metrics_to_return\n\n    default_predictor = ""sentence_tagger""\n'"
allennlp/modules/__init__.py,1,"b'""""""\nCustom PyTorch\n`Module <https://pytorch.org/docs/master/nn.html#torch.nn.Module>`_ s\nthat are used as components in AllenNLP `Model` s.\n""""""\n\nfrom allennlp.modules.attention import Attention\nfrom allennlp.modules.bimpm_matching import BiMpmMatching\nfrom allennlp.modules.conditional_random_field import ConditionalRandomField\nfrom allennlp.modules.elmo import Elmo\nfrom allennlp.modules.feedforward import FeedForward\nfrom allennlp.modules.gated_sum import GatedSum\nfrom allennlp.modules.highway import Highway\nfrom allennlp.modules.input_variational_dropout import InputVariationalDropout\nfrom allennlp.modules.layer_norm import LayerNorm\nfrom allennlp.modules.matrix_attention import MatrixAttention\nfrom allennlp.modules.maxout import Maxout\nfrom allennlp.modules.residual_with_layer_dropout import ResidualWithLayerDropout\nfrom allennlp.modules.scalar_mix import ScalarMix\nfrom allennlp.modules.seq2seq_encoders import Seq2SeqEncoder\nfrom allennlp.modules.seq2vec_encoders import Seq2VecEncoder\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder\nfrom allennlp.modules.time_distributed import TimeDistributed\nfrom allennlp.modules.token_embedders import TokenEmbedder, Embedding\nfrom allennlp.modules.softmax_loss import SoftmaxLoss\n'"
allennlp/modules/augmented_lstm.py,40,"b'""""""\nAn LSTM with Recurrent Dropout and the option to use highway\nconnections between layers.\nBased on PyText version (that was based on a previous AllenNLP version)\n""""""\n\nfrom typing import Optional, Tuple\n\nimport torch\nfrom allennlp.common.checks import ConfigurationError\nfrom torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence\n\nfrom allennlp.nn.initializers import block_orthogonal\nfrom allennlp.nn.util import get_dropout_mask\n\n\nclass AugmentedLSTMCell(torch.nn.Module):\n    """"""\n    `AugmentedLSTMCell` implements a AugmentedLSTM cell.\n\n    # Parameters\n\n    embed_dim : `int`\n        The number of expected features in the input.\n    lstm_dim : `int`\n        Number of features in the hidden state of the LSTM.\n    use_highway : `bool`, optional (default = `True`)\n        If `True` we append a highway network to the outputs of the LSTM.\n    use_bias : `bool`, optional (default = `True`)\n        If `True` we use a bias in our LSTM calculations, otherwise we don\'t.\n\n    # Attributes\n\n    input_linearity : `nn.Module`\n        Fused weight matrix which computes a linear function over the input.\n    state_linearity : `nn.Module`\n        Fused weight matrix which computes a linear function over the states.\n    """"""\n\n    def __init__(\n        self, embed_dim: int, lstm_dim: int, use_highway: bool = True, use_bias: bool = True\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.lstm_dim = lstm_dim\n        self.use_highway = use_highway\n        self.use_bias = use_bias\n\n        if use_highway:\n            self._highway_inp_proj_start = 5 * self.lstm_dim\n            self._highway_inp_proj_end = 6 * self.lstm_dim\n\n            # fused linearity of input to input_gate,\n            # forget_gate, memory_init, output_gate, highway_gate,\n            # and the actual highway value\n            self.input_linearity = torch.nn.Linear(\n                self.embed_dim, self._highway_inp_proj_end, bias=self.use_bias\n            )\n            # fused linearity of input to input_gate,\n            # forget_gate, memory_init, output_gate, highway_gate\n            self.state_linearity = torch.nn.Linear(\n                self.lstm_dim, self._highway_inp_proj_start, bias=True\n            )\n        else:\n            # If there\'s no highway layer then we have a standard\n            # LSTM. The 4 comes from fusing input, forget, memory, output\n            # gates/inputs.\n            self.input_linearity = torch.nn.Linear(\n                self.embed_dim, 4 * self.lstm_dim, bias=self.use_bias\n            )\n            self.state_linearity = torch.nn.Linear(self.lstm_dim, 4 * self.lstm_dim, bias=True)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        # Use sensible default initializations for parameters.\n        block_orthogonal(self.input_linearity.weight.data, [self.lstm_dim, self.embed_dim])\n        block_orthogonal(self.state_linearity.weight.data, [self.lstm_dim, self.lstm_dim])\n\n        self.state_linearity.bias.data.fill_(0.0)\n        # Initialize forget gate biases to 1.0 as per An Empirical\n        # Exploration of Recurrent Network Architectures, (Jozefowicz, 2015).\n        self.state_linearity.bias.data[self.lstm_dim : 2 * self.lstm_dim].fill_(1.0)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        states=Tuple[torch.Tensor, torch.Tensor],\n        variational_dropout_mask: Optional[torch.BoolTensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""\n        !!! Warning\n            DO NOT USE THIS LAYER DIRECTLY, instead use the AugmentedLSTM class\n\n        # Parameters\n\n        x : `torch.Tensor`\n            Input tensor of shape (bsize x input_dim).\n        states : `Tuple[torch.Tensor, torch.Tensor]`\n            Tuple of tensors containing\n            the hidden state and the cell state of each element in\n            the batch. Each of these tensors have a dimension of\n            (bsize x nhid). Defaults to `None`.\n\n        # Returns\n\n        `Tuple[torch.Tensor, torch.Tensor]`\n            Returned states. Shape of each state is (bsize x nhid).\n\n        """"""\n        hidden_state, memory_state = states\n\n        # In Pytext this was done as the last step of the cell.\n        # But the original AugmentedLSTM from AllenNLP this was done before the processing\n        if variational_dropout_mask is not None and self.training:\n            hidden_state = hidden_state * variational_dropout_mask\n\n        projected_input = self.input_linearity(x)\n        projected_state = self.state_linearity(hidden_state)\n\n        input_gate = forget_gate = memory_init = output_gate = highway_gate = None\n        if self.use_highway:\n            fused_op = projected_input[:, : 5 * self.lstm_dim] + projected_state\n            fused_chunked = torch.chunk(fused_op, 5, 1)\n            (input_gate, forget_gate, memory_init, output_gate, highway_gate) = fused_chunked\n            highway_gate = torch.sigmoid(highway_gate)\n        else:\n            fused_op = projected_input + projected_state\n            input_gate, forget_gate, memory_init, output_gate = torch.chunk(fused_op, 4, 1)\n        input_gate = torch.sigmoid(input_gate)\n        forget_gate = torch.sigmoid(forget_gate)\n        memory_init = torch.tanh(memory_init)\n        output_gate = torch.sigmoid(output_gate)\n        memory = input_gate * memory_init + forget_gate * memory_state\n        timestep_output: torch.Tensor = output_gate * torch.tanh(memory)\n\n        if self.use_highway:\n            highway_input_projection = projected_input[\n                :, self._highway_inp_proj_start : self._highway_inp_proj_end\n            ]\n            timestep_output = (\n                highway_gate * timestep_output\n                + (1 - highway_gate) * highway_input_projection  # noqa\n            )\n\n        return timestep_output, memory\n\n\nclass AugmentedLstm(torch.nn.Module):\n    """"""\n    `AugmentedLstm` implements a one-layer single directional\n    AugmentedLSTM layer. AugmentedLSTM is an LSTM which optionally\n    appends an optional highway network to the output layer. Furthermore the\n    dropout controls the level of variational dropout done.\n\n    # Parameters\n\n    input_size : `int`\n        The number of expected features in the input.\n    hidden_size : `int`\n        Number of features in the hidden state of the LSTM.\n        Defaults to 32.\n    go_forward : `bool`\n        Whether to compute features left to right (forward)\n        or right to left (backward).\n    recurrent_dropout_probability : `float`\n        Variational dropout probability to use. Defaults to 0.0.\n    use_highway : `bool`\n        If `True` we append a highway network to the outputs of the LSTM.\n    use_input_projection_bias : `bool`\n        If `True` we use a bias in our LSTM calculations, otherwise we don\'t.\n\n    # Attributes\n\n    cell : `AugmentedLSTMCell`\n        `AugmentedLSTMCell` that is applied at every timestep.\n\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        go_forward: bool = True,\n        recurrent_dropout_probability: float = 0.0,\n        use_highway: bool = True,\n        use_input_projection_bias: bool = True,\n    ):\n        super().__init__()\n\n        self.embed_dim = input_size\n        self.lstm_dim = hidden_size\n\n        self.go_forward = go_forward\n        self.use_highway = use_highway\n        self.recurrent_dropout_probability = recurrent_dropout_probability\n\n        self.cell = AugmentedLSTMCell(\n            self.embed_dim, self.lstm_dim, self.use_highway, use_input_projection_bias\n        )\n\n    def forward(\n        self, inputs: PackedSequence, states: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[PackedSequence, Tuple[torch.Tensor, torch.Tensor]]:\n        """"""\n        Warning: Would be better to use the BiAugmentedLstm class in a regular model\n\n        Given an input batch of sequential data such as word embeddings, produces a single layer unidirectional\n        AugmentedLSTM representation of the sequential input and new state tensors.\n\n        # Parameters\n\n        inputs : `PackedSequence`\n            `bsize` sequences of shape `(len, input_dim)` each, in PackedSequence format\n        states : `Tuple[torch.Tensor, torch.Tensor]`\n            Tuple of tensors containing the initial hidden state and\n            the cell state of each element in the batch. Each of these tensors have a dimension of\n            (1 x bsize x nhid). Defaults to `None`.\n\n        # Returns\n\n        `Tuple[PackedSequence, Tuple[torch.Tensor, torch.Tensor]]`\n            AugmentedLSTM representation of input and the state of the LSTM `t = seq_len`.\n            Shape of representation is (bsize x seq_len x representation_dim).\n            Shape of each state is (1 x bsize x nhid).\n\n        """"""\n        if not isinstance(inputs, PackedSequence):\n            raise ConfigurationError(""inputs must be PackedSequence but got %s"" % (type(inputs)))\n\n        sequence_tensor, batch_lengths = pad_packed_sequence(inputs, batch_first=True)\n        batch_size = sequence_tensor.size()[0]\n        total_timesteps = sequence_tensor.size()[1]\n        output_accumulator = sequence_tensor.new_zeros(batch_size, total_timesteps, self.lstm_dim)\n        if states is None:\n            full_batch_previous_memory = sequence_tensor.new_zeros(batch_size, self.lstm_dim)\n            full_batch_previous_state = sequence_tensor.data.new_zeros(batch_size, self.lstm_dim)\n        else:\n            full_batch_previous_state = states[0].squeeze(0)\n            full_batch_previous_memory = states[1].squeeze(0)\n        current_length_index = batch_size - 1 if self.go_forward else 0\n        if self.recurrent_dropout_probability > 0.0:\n            dropout_mask = get_dropout_mask(\n                self.recurrent_dropout_probability, full_batch_previous_memory\n            )\n        else:\n            dropout_mask = None\n\n        for timestep in range(total_timesteps):\n            index = timestep if self.go_forward else total_timesteps - timestep - 1\n\n            if self.go_forward:\n                while batch_lengths[current_length_index] <= index:\n                    current_length_index -= 1\n            # If we\'re going backwards, we are _picking up_ more indices.\n            else:\n                # First conditional: Are we already at the maximum\n                # number of elements in the batch?\n                # Second conditional: Does the next shortest\n                # sequence beyond the current batch\n                # index require computation use this timestep?\n                while (\n                    current_length_index < (len(batch_lengths) - 1)\n                    and batch_lengths[current_length_index + 1] > index\n                ):\n                    current_length_index += 1\n\n            previous_memory = full_batch_previous_memory[0 : current_length_index + 1].clone()\n            previous_state = full_batch_previous_state[0 : current_length_index + 1].clone()\n            timestep_input = sequence_tensor[0 : current_length_index + 1, index]\n            timestep_output, memory = self.cell(\n                timestep_input,\n                (previous_state, previous_memory),\n                dropout_mask[0 : current_length_index + 1] if dropout_mask is not None else None,\n            )\n            full_batch_previous_memory = full_batch_previous_memory.data.clone()\n            full_batch_previous_state = full_batch_previous_state.data.clone()\n            full_batch_previous_memory[0 : current_length_index + 1] = memory\n            full_batch_previous_state[0 : current_length_index + 1] = timestep_output\n            output_accumulator[0 : current_length_index + 1, index, :] = timestep_output\n\n        output_accumulator = pack_padded_sequence(\n            output_accumulator, batch_lengths, batch_first=True\n        )\n\n        # Mimic the pytorch API by returning state in the following shape:\n        # (num_layers * num_directions, batch_size, lstm_dim). As this\n        # LSTM cannot be stacked, the first dimension here is just 1.\n        final_state = (\n            full_batch_previous_state.unsqueeze(0),\n            full_batch_previous_memory.unsqueeze(0),\n        )\n        return output_accumulator, final_state\n\n\nclass BiAugmentedLstm(torch.nn.Module):\n    """"""\n    `BiAugmentedLstm` implements a generic AugmentedLSTM representation layer.\n    BiAugmentedLstm is an LSTM which optionally appends an optional highway network to the output layer.\n    Furthermore the dropout controls the level of variational dropout done.\n\n    # Parameters\n\n    input_size : `int`, required\n        The dimension of the inputs to the LSTM.\n    hidden_size : `int`, required.\n        The dimension of the outputs of the LSTM.\n    num_layers : `int`\n        Number of recurrent layers. Eg. setting `num_layers=2`\n        would mean stacking two LSTMs together to form a stacked LSTM,\n        with the second LSTM taking in the outputs of the first LSTM and\n        computing the final result. Defaults to 1.\n    bias : `bool`\n        If `True` we use a bias in our LSTM calculations, otherwise we don\'t.\n    recurrent_dropout_probability : `float`, optional (default = `0.0`)\n        Variational dropout probability to use.\n    bidirectional : `bool`\n        If `True`, becomes a bidirectional LSTM. Defaults to `True`.\n    padding_value : `float`, optional (default = `0.0`)\n        Value for the padded elements. Defaults to 0.0.\n    use_highway : `bool`, optional (default = `True`)\n        Whether or not to use highway connections between layers. This effectively involves\n        reparameterising the normal output of an LSTM as::\n\n            gate = sigmoid(W_x1 * x_t + W_h * h_t)\n            output = gate * h_t  + (1 - gate) * (W_x2 * x_t)\n\n    # Returns\n\n    output_accumulator : `PackedSequence`\n        The outputs of the LSTM for each timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where\n        for a given batch element, all outputs past the sequence length for that batch are zero tensors.\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int = 1,\n        bias: bool = True,\n        recurrent_dropout_probability: float = 0.0,\n        bidirectional: bool = False,\n        padding_value: float = 0.0,\n        use_highway: bool = True,\n    ) -> None:\n        super().__init__()\n        self.input_size = input_size\n        self.padding_value = padding_value\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        self.recurrent_dropout_probability = recurrent_dropout_probability\n        self.use_highway = use_highway\n        self.use_bias = bias\n\n        num_directions = int(self.bidirectional) + 1\n        self.forward_layers = torch.nn.ModuleList()\n        if self.bidirectional:\n            self.backward_layers = torch.nn.ModuleList()\n\n        lstm_embed_dim = self.input_size\n        for _ in range(self.num_layers):\n            self.forward_layers.append(\n                AugmentedLstm(\n                    lstm_embed_dim,\n                    self.hidden_size,\n                    go_forward=True,\n                    recurrent_dropout_probability=self.recurrent_dropout_probability,\n                    use_highway=self.use_highway,\n                    use_input_projection_bias=self.use_bias,\n                )\n            )\n            if self.bidirectional:\n                self.backward_layers.append(\n                    AugmentedLstm(\n                        lstm_embed_dim,\n                        self.hidden_size,\n                        go_forward=False,\n                        recurrent_dropout_probability=self.recurrent_dropout_probability,\n                        use_highway=self.use_highway,\n                        use_input_projection_bias=self.use_bias,\n                    )\n                )\n\n            lstm_embed_dim = self.hidden_size * num_directions\n        self.representation_dim = lstm_embed_dim\n\n    def forward(\n        self, inputs: torch.Tensor, states: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        """"""\n        Given an input batch of sequential data such as word embeddings, produces\n        a AugmentedLSTM representation of the sequential input and new state\n        tensors.\n\n        # Parameters\n\n        inputs : `PackedSequence`, required.\n            A tensor of shape (batch_size, num_timesteps, input_size)\n            to apply the LSTM over.\n        states : `Tuple[torch.Tensor, torch.Tensor]`\n            Tuple of tensors containing\n            the initial hidden state and the cell state of each element in\n            the batch. Each of these tensors have a dimension of\n            (bsize x num_layers x num_directions * nhid). Defaults to `None`.\n\n        # Returns\n\n        `Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]`\n            AgumentedLSTM representation of input and\n            the state of the LSTM `t = seq_len`.\n            Shape of representation is (bsize x seq_len x representation_dim).\n            Shape of each state is (bsize x num_layers * num_directions x nhid).\n\n        """"""\n\n        if not isinstance(inputs, PackedSequence):\n            raise ConfigurationError(""inputs must be PackedSequence but got %s"" % (type(inputs)))\n\n        # if states is not None:\n        #    states = (states[0].transpose(0, 1), states[1].transpose(0, 1))\n        if self.bidirectional:\n            return self._forward_bidirectional(inputs, states)\n        return self._forward_unidirectional(inputs, states)\n\n    def _forward_bidirectional(\n        self, inputs: PackedSequence, states: Optional[Tuple[torch.Tensor, torch.Tensor]]\n    ):\n        output_sequence = inputs\n        final_h = []\n        final_c = []\n\n        if not states:\n            hidden_states = [None] * self.num_layers\n        elif states[0].size()[0] != self.num_layers:\n            raise RuntimeError(\n                ""Initial states were passed to forward() but the number of ""\n                ""initial states does not match the number of layers.""\n            )\n        else:\n            hidden_states = list(\n                zip(  # type: ignore\n                    states[0].chunk(self.num_layers, 0), states[1].chunk(self.num_layers, 0)\n                )\n            )\n        for i, state in enumerate(hidden_states):\n            if state:\n                forward_state = state[0].chunk(2, -1)\n                backward_state = state[1].chunk(2, -1)\n            else:\n                forward_state = backward_state = None\n\n            forward_layer = self.forward_layers[i]\n            backward_layer = self.backward_layers[i]\n            # The state is duplicated to mirror the Pytorch API for LSTMs.\n            forward_output, final_forward_state = forward_layer(output_sequence, forward_state)\n            backward_output, final_backward_state = backward_layer(output_sequence, backward_state)\n            forward_output, lengths = pad_packed_sequence(forward_output, batch_first=True)\n            backward_output, _ = pad_packed_sequence(backward_output, batch_first=True)\n            output_sequence = torch.cat([forward_output, backward_output], -1)\n            output_sequence = pack_padded_sequence(output_sequence, lengths, batch_first=True)\n\n            final_h.extend([final_forward_state[0], final_backward_state[0]])\n            final_c.extend([final_forward_state[1], final_backward_state[1]])\n\n        final_h = torch.cat(final_h, dim=0)\n        final_c = torch.cat(final_c, dim=0)\n        final_state_tuple = (final_h, final_c)\n        output_sequence, batch_lengths = pad_packed_sequence(\n            output_sequence, padding_value=self.padding_value, batch_first=True\n        )\n\n        output_sequence = pack_padded_sequence(output_sequence, batch_lengths, batch_first=True)\n        return output_sequence, final_state_tuple\n\n    def _forward_unidirectional(\n        self, inputs: PackedSequence, states: Optional[Tuple[torch.Tensor, torch.Tensor]]\n    ):\n        output_sequence = inputs\n        final_h = []\n        final_c = []\n\n        if not states:\n            hidden_states = [None] * self.num_layers\n        elif states[0].size()[0] != self.num_layers:\n            raise RuntimeError(\n                ""Initial states were passed to forward() but the number of ""\n                ""initial states does not match the number of layers.""\n            )\n        else:\n            hidden_states = list(\n                zip(  # type: ignore\n                    states[0].chunk(self.num_layers, 0), states[1].chunk(self.num_layers, 0)\n                )  # type: ignore\n            )\n\n        for i, state in enumerate(hidden_states):\n            forward_layer = self.forward_layers[i]\n            # The state is duplicated to mirror the Pytorch API for LSTMs.\n            forward_output, final_forward_state = forward_layer(output_sequence, state)\n            output_sequence = forward_output\n            final_h.append(final_forward_state[0])\n            final_c.append(final_forward_state[1])\n\n        final_h = torch.cat(final_h, dim=0)\n        final_c = torch.cat(final_c, dim=0)\n        final_state_tuple = (final_h, final_c)\n        output_sequence, batch_lengths = pad_packed_sequence(\n            output_sequence, padding_value=self.padding_value, batch_first=True\n        )\n\n        output_sequence = pack_padded_sequence(output_sequence, batch_lengths, batch_first=True)\n\n        return output_sequence, final_state_tuple\n'"
allennlp/modules/bimpm_matching.py,30,"b'""""""\nMulti-perspective matching layer\n""""""\n\nfrom typing import Tuple, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.registrable import FromParams\nfrom allennlp.nn.util import (\n    get_lengths_from_binary_sequence_mask,\n    masked_max,\n    masked_mean,\n    masked_softmax,\n    tiny_value_of_dtype,\n)\n\n\ndef multi_perspective_match(\n    vector1: torch.Tensor, vector2: torch.Tensor, weight: torch.Tensor\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    """"""\n    Calculate multi-perspective cosine matching between time-steps of vectors\n    of the same length.\n\n    # Parameters\n\n    vector1 : `torch.Tensor`\n        A tensor of shape `(batch, seq_len, hidden_size)`\n    vector2 : `torch.Tensor`\n        A tensor of shape `(batch, seq_len or 1, hidden_size)`\n    weight : `torch.Tensor`\n        A tensor of shape `(num_perspectives, hidden_size)`\n\n    # Returns\n\n    `torch.Tensor` :\n        Shape `(batch, seq_len, 1)`.\n    `torch.Tensor` :\n        Shape `(batch, seq_len, num_perspectives)`.\n    """"""\n    assert vector1.size(0) == vector2.size(0)\n    assert weight.size(1) == vector1.size(2) == vector1.size(2)\n\n    # (batch, seq_len, 1)\n    similarity_single = F.cosine_similarity(vector1, vector2, 2).unsqueeze(2)\n\n    # (1, 1, num_perspectives, hidden_size)\n    weight = weight.unsqueeze(0).unsqueeze(0)\n\n    # (batch, seq_len, num_perspectives, hidden_size)\n    vector1 = weight * vector1.unsqueeze(2)\n    vector2 = weight * vector2.unsqueeze(2)\n\n    similarity_multi = F.cosine_similarity(vector1, vector2, dim=3)\n\n    return similarity_single, similarity_multi\n\n\ndef multi_perspective_match_pairwise(\n    vector1: torch.Tensor, vector2: torch.Tensor, weight: torch.Tensor\n) -> torch.Tensor:\n    """"""\n    Calculate multi-perspective cosine matching between each time step of\n    one vector and each time step of another vector.\n\n    # Parameters\n\n    vector1 : `torch.Tensor`\n        A tensor of shape `(batch, seq_len1, hidden_size)`\n    vector2 : `torch.Tensor`\n        A tensor of shape `(batch, seq_len2, hidden_size)`\n    weight : `torch.Tensor`\n        A tensor of shape `(num_perspectives, hidden_size)`\n\n    # Returns\n\n    `torch.Tensor` :\n        A tensor of shape `(batch, seq_len1, seq_len2, num_perspectives)` consisting\n        multi-perspective matching results\n    """"""\n    num_perspectives = weight.size(0)\n\n    # (1, num_perspectives, 1, hidden_size)\n    weight = weight.unsqueeze(0).unsqueeze(2)\n\n    # (batch, num_perspectives, seq_len*, hidden_size)\n    vector1 = weight * vector1.unsqueeze(1).expand(-1, num_perspectives, -1, -1)\n    vector2 = weight * vector2.unsqueeze(1).expand(-1, num_perspectives, -1, -1)\n\n    # (batch, num_perspectives, seq_len*, 1)\n    vector1_norm = vector1.norm(p=2, dim=3, keepdim=True)\n    vector2_norm = vector2.norm(p=2, dim=3, keepdim=True)\n\n    # (batch, num_perspectives, seq_len1, seq_len2)\n    mul_result = torch.matmul(vector1, vector2.transpose(2, 3))\n    norm_value = vector1_norm * vector2_norm.transpose(2, 3)\n\n    # (batch, seq_len1, seq_len2, num_perspectives)\n    return (mul_result / norm_value.clamp(min=tiny_value_of_dtype(norm_value.dtype))).permute(\n        0, 2, 3, 1\n    )\n\n\nclass BiMpmMatching(nn.Module, FromParams):\n    """"""\n    This `Module` implements the matching layer of BiMPM model described in [Bilateral\n    Multi-Perspective Matching for Natural Language Sentences](https://arxiv.org/abs/1702.03814)\n    by Zhiguo Wang et al., 2017.\n    Also please refer to the [TensorFlow implementation](https://github.com/zhiguowang/BiMPM/) and\n    [PyTorch implementation](https://github.com/galsang/BIMPM-pytorch).\n\n    # Parameters\n\n    hidden_dim : `int`, optional (default = `100`)\n        The hidden dimension of the representations\n    num_perspectives : `int`, optional (default = `20`)\n        The number of perspectives for matching\n    share_weights_between_directions : `bool`, optional (default = `True`)\n        If True, share weight between matching from sentence1 to sentence2 and from sentence2\n        to sentence1, useful for non-symmetric tasks\n    is_forward : `bool`, optional (default = `None`)\n        Whether the matching is for forward sequence or backward sequence, useful in finding last\n        token in full matching. It can not be None if with_full_match is True.\n    with_full_match : `bool`, optional (default = `True`)\n        If True, include full match\n    with_maxpool_match : `bool`, optional (default = `True`)\n        If True, include max pool match\n    with_attentive_match : `bool`, optional (default = `True`)\n        If True, include attentive match\n    with_max_attentive_match : `bool`, optional (default = `True`)\n        If True, include max attentive match\n    """"""\n\n    def __init__(\n        self,\n        hidden_dim: int = 100,\n        num_perspectives: int = 20,\n        share_weights_between_directions: bool = True,\n        is_forward: bool = None,\n        with_full_match: bool = True,\n        with_maxpool_match: bool = True,\n        with_attentive_match: bool = True,\n        with_max_attentive_match: bool = True,\n    ) -> None:\n        super().__init__()\n\n        self.hidden_dim = hidden_dim\n        self.num_perspectives = num_perspectives\n        self.is_forward = is_forward\n\n        self.with_full_match = with_full_match\n        self.with_maxpool_match = with_maxpool_match\n        self.with_attentive_match = with_attentive_match\n        self.with_max_attentive_match = with_max_attentive_match\n\n        if not (\n            with_full_match\n            or with_maxpool_match\n            or with_attentive_match\n            or with_max_attentive_match\n        ):\n            raise ConfigurationError(""At least one of the matching method should be enabled"")\n\n        def create_parameter():  # utility function to create and initialize a parameter\n            param = nn.Parameter(torch.zeros(num_perspectives, hidden_dim))\n            torch.nn.init.kaiming_normal_(param)\n            return param\n\n        def share_or_create(weights_to_share):  # utility function to create or share the weights\n            return weights_to_share if share_weights_between_directions else create_parameter()\n\n        output_dim = (\n            2  # used to calculate total output dimension, 2 is for cosine max and cosine min\n        )\n        if with_full_match:\n            if is_forward is None:\n                raise ConfigurationError(""Must specify is_forward to enable full matching"")\n            self.full_match_weights = create_parameter()\n            self.full_match_weights_reversed = share_or_create(self.full_match_weights)\n            output_dim += num_perspectives + 1\n\n        if with_maxpool_match:\n            self.maxpool_match_weights = create_parameter()\n            output_dim += num_perspectives * 2\n\n        if with_attentive_match:\n            self.attentive_match_weights = create_parameter()\n            self.attentive_match_weights_reversed = share_or_create(self.attentive_match_weights)\n            output_dim += num_perspectives + 1\n\n        if with_max_attentive_match:\n            self.max_attentive_match_weights = create_parameter()\n            self.max_attentive_match_weights_reversed = share_or_create(\n                self.max_attentive_match_weights\n            )\n            output_dim += num_perspectives + 1\n\n        self.output_dim = output_dim\n\n    def get_output_dim(self) -> int:\n        return self.output_dim\n\n    def forward(\n        self,\n        context_1: torch.Tensor,\n        mask_1: torch.BoolTensor,\n        context_2: torch.Tensor,\n        mask_2: torch.BoolTensor,\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n\n        """"""\n        Given the forward (or backward) representations of sentence1 and sentence2, apply four bilateral\n        matching functions between them in one direction.\n\n        # Parameters\n\n        context_1 : `torch.Tensor`\n            Tensor of shape (batch_size, seq_len1, hidden_dim) representing the encoding of the first sentence.\n        mask_1 : `torch.BoolTensor`\n            Boolean Tensor of shape (batch_size, seq_len1), indicating which\n            positions in the first sentence are padding (0) and which are not (1).\n        context_2 : `torch.Tensor`\n            Tensor of shape (batch_size, seq_len2, hidden_dim) representing the encoding of the second sentence.\n        mask_2 : `torch.BoolTensor`\n            Boolean Tensor of shape (batch_size, seq_len2), indicating which\n            positions in the second sentence are padding (0) and which are not (1).\n\n        # Returns\n\n        `Tuple[List[torch.Tensor], List[torch.Tensor]]` :\n            A tuple of matching vectors for the two sentences. Each of which is a list of\n            matching vectors of shape (batch, seq_len, num_perspectives or 1)\n        """"""\n        assert (not mask_2.requires_grad) and (not mask_1.requires_grad)\n        assert context_1.size(-1) == context_2.size(-1) == self.hidden_dim\n\n        # (batch,)\n        len_1 = get_lengths_from_binary_sequence_mask(mask_1)\n        len_2 = get_lengths_from_binary_sequence_mask(mask_2)\n\n        # explicitly set masked weights to zero\n        # (batch_size, seq_len*, hidden_dim)\n        context_1 = context_1 * mask_1.unsqueeze(-1)\n        context_2 = context_2 * mask_2.unsqueeze(-1)\n\n        # array to keep the matching vectors for the two sentences\n        matching_vector_1: List[torch.Tensor] = []\n        matching_vector_2: List[torch.Tensor] = []\n\n        # Step 0. unweighted cosine\n        # First calculate the cosine similarities between each forward\n        # (or backward) contextual embedding and every forward (or backward)\n        # contextual embedding of the other sentence.\n\n        # (batch, seq_len1, seq_len2)\n        cosine_sim = F.cosine_similarity(context_1.unsqueeze(-2), context_2.unsqueeze(-3), dim=3)\n\n        # (batch, seq_len*, 1)\n        cosine_max_1 = masked_max(cosine_sim, mask_2.unsqueeze(-2), dim=2, keepdim=True)\n        cosine_mean_1 = masked_mean(cosine_sim, mask_2.unsqueeze(-2), dim=2, keepdim=True)\n        cosine_max_2 = masked_max(\n            cosine_sim.permute(0, 2, 1), mask_1.unsqueeze(-2), dim=2, keepdim=True\n        )\n        cosine_mean_2 = masked_mean(\n            cosine_sim.permute(0, 2, 1), mask_1.unsqueeze(-2), dim=2, keepdim=True\n        )\n\n        matching_vector_1.extend([cosine_max_1, cosine_mean_1])\n        matching_vector_2.extend([cosine_max_2, cosine_mean_2])\n\n        # Step 1. Full-Matching\n        # Each time step of forward (or backward) contextual embedding of one sentence\n        # is compared with the last time step of the forward (or backward)\n        # contextual embedding of the other sentence\n        if self.with_full_match:\n\n            # (batch, 1, hidden_dim)\n            if self.is_forward:\n                # (batch, 1, hidden_dim)\n                last_position_1 = (len_1 - 1).clamp(min=0)\n                last_position_1 = last_position_1.view(-1, 1, 1).expand(-1, 1, self.hidden_dim)\n                last_position_2 = (len_2 - 1).clamp(min=0)\n                last_position_2 = last_position_2.view(-1, 1, 1).expand(-1, 1, self.hidden_dim)\n\n                context_1_last = context_1.gather(1, last_position_1)\n                context_2_last = context_2.gather(1, last_position_2)\n            else:\n                context_1_last = context_1[:, 0:1, :]\n                context_2_last = context_2[:, 0:1, :]\n\n            # (batch, seq_len*, num_perspectives)\n            matching_vector_1_full = multi_perspective_match(\n                context_1, context_2_last, self.full_match_weights\n            )\n            matching_vector_2_full = multi_perspective_match(\n                context_2, context_1_last, self.full_match_weights_reversed\n            )\n\n            matching_vector_1.extend(matching_vector_1_full)\n            matching_vector_2.extend(matching_vector_2_full)\n\n        # Step 2. Maxpooling-Matching\n        # Each time step of forward (or backward) contextual embedding of one sentence\n        # is compared with every time step of the forward (or backward)\n        # contextual embedding of the other sentence, and only the max value of each\n        # dimension is retained.\n        if self.with_maxpool_match:\n            # (batch, seq_len1, seq_len2, num_perspectives)\n            matching_vector_max = multi_perspective_match_pairwise(\n                context_1, context_2, self.maxpool_match_weights\n            )\n\n            # (batch, seq_len*, num_perspectives)\n            matching_vector_1_max = masked_max(\n                matching_vector_max, mask_2.unsqueeze(-2).unsqueeze(-1), dim=2\n            )\n            matching_vector_1_mean = masked_mean(\n                matching_vector_max, mask_2.unsqueeze(-2).unsqueeze(-1), dim=2\n            )\n            matching_vector_2_max = masked_max(\n                matching_vector_max.permute(0, 2, 1, 3), mask_1.unsqueeze(-2).unsqueeze(-1), dim=2\n            )\n            matching_vector_2_mean = masked_mean(\n                matching_vector_max.permute(0, 2, 1, 3), mask_1.unsqueeze(-2).unsqueeze(-1), dim=2\n            )\n\n            matching_vector_1.extend([matching_vector_1_max, matching_vector_1_mean])\n            matching_vector_2.extend([matching_vector_2_max, matching_vector_2_mean])\n\n        # Step 3. Attentive-Matching\n        # Each forward (or backward) similarity is taken as the weight\n        # of the forward (or backward) contextual embedding, and calculate an\n        # attentive vector for the sentence by weighted summing all its\n        # contextual embeddings.\n        # Finally match each forward (or backward) contextual embedding\n        # with its corresponding attentive vector.\n\n        # (batch, seq_len1, seq_len2, hidden_dim)\n        att_2 = context_2.unsqueeze(-3) * cosine_sim.unsqueeze(-1)\n\n        # (batch, seq_len1, seq_len2, hidden_dim)\n        att_1 = context_1.unsqueeze(-2) * cosine_sim.unsqueeze(-1)\n\n        if self.with_attentive_match:\n            # (batch, seq_len*, hidden_dim)\n            att_mean_2 = masked_softmax(att_2.sum(dim=2), mask_1.unsqueeze(-1))\n            att_mean_1 = masked_softmax(att_1.sum(dim=1), mask_2.unsqueeze(-1))\n\n            # (batch, seq_len*, num_perspectives)\n            matching_vector_1_att_mean = multi_perspective_match(\n                context_1, att_mean_2, self.attentive_match_weights\n            )\n            matching_vector_2_att_mean = multi_perspective_match(\n                context_2, att_mean_1, self.attentive_match_weights_reversed\n            )\n            matching_vector_1.extend(matching_vector_1_att_mean)\n            matching_vector_2.extend(matching_vector_2_att_mean)\n\n        # Step 4. Max-Attentive-Matching\n        # Pick the contextual embeddings with the highest cosine similarity as the attentive\n        # vector, and match each forward (or backward) contextual embedding with its\n        # corresponding attentive vector.\n        if self.with_max_attentive_match:\n            # (batch, seq_len*, hidden_dim)\n            att_max_2 = masked_max(att_2, mask_2.unsqueeze(-2).unsqueeze(-1), dim=2)\n            att_max_1 = masked_max(\n                att_1.permute(0, 2, 1, 3), mask_1.unsqueeze(-2).unsqueeze(-1), dim=2\n            )\n\n            # (batch, seq_len*, num_perspectives)\n            matching_vector_1_att_max = multi_perspective_match(\n                context_1, att_max_2, self.max_attentive_match_weights\n            )\n            matching_vector_2_att_max = multi_perspective_match(\n                context_2, att_max_1, self.max_attentive_match_weights_reversed\n            )\n\n            matching_vector_1.extend(matching_vector_1_att_max)\n            matching_vector_2.extend(matching_vector_2_att_max)\n\n        return matching_vector_1, matching_vector_2\n'"
allennlp/modules/conditional_random_field.py,22,"b'""""""\nConditional random field\n""""""\nfrom typing import List, Tuple, Dict, Union\n\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nimport allennlp.nn.util as util\n\nVITERBI_DECODING = Tuple[List[int], float]  # a list of tags, and a viterbi score\n\n\ndef allowed_transitions(constraint_type: str, labels: Dict[int, str]) -> List[Tuple[int, int]]:\n    """"""\n    Given labels and a constraint type, returns the allowed transitions. It will\n    additionally include transitions for the start and end states, which are used\n    by the conditional random field.\n\n    # Parameters\n\n    constraint_type : `str`, required\n        Indicates which constraint to apply. Current choices are\n        ""BIO"", ""IOB1"", ""BIOUL"", and ""BMES"".\n    labels : `Dict[int, str]`, required\n        A mapping {label_id -> label}. Most commonly this would be the value from\n        Vocabulary.get_index_to_token_vocabulary()\n\n    # Returns\n\n    `List[Tuple[int, int]]`\n        The allowed transitions (from_label_id, to_label_id).\n    """"""\n    num_labels = len(labels)\n    start_tag = num_labels\n    end_tag = num_labels + 1\n    labels_with_boundaries = list(labels.items()) + [(start_tag, ""START""), (end_tag, ""END"")]\n\n    allowed = []\n    for from_label_index, from_label in labels_with_boundaries:\n        if from_label in (""START"", ""END""):\n            from_tag = from_label\n            from_entity = """"\n        else:\n            from_tag = from_label[0]\n            from_entity = from_label[1:]\n        for to_label_index, to_label in labels_with_boundaries:\n            if to_label in (""START"", ""END""):\n                to_tag = to_label\n                to_entity = """"\n            else:\n                to_tag = to_label[0]\n                to_entity = to_label[1:]\n            if is_transition_allowed(constraint_type, from_tag, from_entity, to_tag, to_entity):\n                allowed.append((from_label_index, to_label_index))\n    return allowed\n\n\ndef is_transition_allowed(\n    constraint_type: str, from_tag: str, from_entity: str, to_tag: str, to_entity: str\n):\n    """"""\n    Given a constraint type and strings `from_tag` and `to_tag` that\n    represent the origin and destination of the transition, return whether\n    the transition is allowed under the given constraint type.\n\n    # Parameters\n\n    constraint_type : `str`, required\n        Indicates which constraint to apply. Current choices are\n        ""BIO"", ""IOB1"", ""BIOUL"", and ""BMES"".\n    from_tag : `str`, required\n        The tag that the transition originates from. For example, if the\n        label is `I-PER`, the `from_tag` is `I`.\n    from_entity : `str`, required\n        The entity corresponding to the `from_tag`. For example, if the\n        label is `I-PER`, the `from_entity` is `PER`.\n    to_tag : `str`, required\n        The tag that the transition leads to. For example, if the\n        label is `I-PER`, the `to_tag` is `I`.\n    to_entity : `str`, required\n        The entity corresponding to the `to_tag`. For example, if the\n        label is `I-PER`, the `to_entity` is `PER`.\n\n    # Returns\n\n    `bool`\n        Whether the transition is allowed under the given `constraint_type`.\n    """"""\n\n    if to_tag == ""START"" or from_tag == ""END"":\n        # Cannot transition into START or from END\n        return False\n\n    if constraint_type == ""BIOUL"":\n        if from_tag == ""START"":\n            return to_tag in (""O"", ""B"", ""U"")\n        if to_tag == ""END"":\n            return from_tag in (""O"", ""L"", ""U"")\n        return any(\n            [\n                # O can transition to O, B-* or U-*\n                # L-x can transition to O, B-*, or U-*\n                # U-x can transition to O, B-*, or U-*\n                from_tag in (""O"", ""L"", ""U"") and to_tag in (""O"", ""B"", ""U""),\n                # B-x can only transition to I-x or L-x\n                # I-x can only transition to I-x or L-x\n                from_tag in (""B"", ""I"") and to_tag in (""I"", ""L"") and from_entity == to_entity,\n            ]\n        )\n    elif constraint_type == ""BIO"":\n        if from_tag == ""START"":\n            return to_tag in (""O"", ""B"")\n        if to_tag == ""END"":\n            return from_tag in (""O"", ""B"", ""I"")\n        return any(\n            [\n                # Can always transition to O or B-x\n                to_tag in (""O"", ""B""),\n                # Can only transition to I-x from B-x or I-x\n                to_tag == ""I"" and from_tag in (""B"", ""I"") and from_entity == to_entity,\n            ]\n        )\n    elif constraint_type == ""IOB1"":\n        if from_tag == ""START"":\n            return to_tag in (""O"", ""I"")\n        if to_tag == ""END"":\n            return from_tag in (""O"", ""B"", ""I"")\n        return any(\n            [\n                # Can always transition to O or I-x\n                to_tag in (""O"", ""I""),\n                # Can only transition to B-x from B-x or I-x, where\n                # x is the same tag.\n                to_tag == ""B"" and from_tag in (""B"", ""I"") and from_entity == to_entity,\n            ]\n        )\n    elif constraint_type == ""BMES"":\n        if from_tag == ""START"":\n            return to_tag in (""B"", ""S"")\n        if to_tag == ""END"":\n            return from_tag in (""E"", ""S"")\n        return any(\n            [\n                # Can only transition to B or S from E or S.\n                to_tag in (""B"", ""S"") and from_tag in (""E"", ""S""),\n                # Can only transition to M-x from B-x, where\n                # x is the same tag.\n                to_tag == ""M"" and from_tag in (""B"", ""M"") and from_entity == to_entity,\n                # Can only transition to E-x from B-x or M-x, where\n                # x is the same tag.\n                to_tag == ""E"" and from_tag in (""B"", ""M"") and from_entity == to_entity,\n            ]\n        )\n    else:\n        raise ConfigurationError(f""Unknown constraint type: {constraint_type}"")\n\n\nclass ConditionalRandomField(torch.nn.Module):\n    """"""\n    This module uses the ""forward-backward"" algorithm to compute\n    the log-likelihood of its inputs assuming a conditional random field model.\n\n    See, e.g. http://www.cs.columbia.edu/~mcollins/fb.pdf\n\n    # Parameters\n\n    num_tags : `int`, required\n        The number of tags.\n    constraints : `List[Tuple[int, int]]`, optional (default = `None`)\n        An optional list of allowed transitions (from_tag_id, to_tag_id).\n        These are applied to `viterbi_tags()` but do not affect `forward()`.\n        These should be derived from `allowed_transitions` so that the\n        start and end transitions are handled correctly for your tag type.\n    include_start_end_transitions : `bool`, optional (default = `True`)\n        Whether to include the start and end transition parameters.\n    """"""\n\n    def __init__(\n        self,\n        num_tags: int,\n        constraints: List[Tuple[int, int]] = None,\n        include_start_end_transitions: bool = True,\n    ) -> None:\n        super().__init__()\n        self.num_tags = num_tags\n\n        # transitions[i, j] is the logit for transitioning from state i to state j.\n        self.transitions = torch.nn.Parameter(torch.Tensor(num_tags, num_tags))\n\n        # _constraint_mask indicates valid transitions (based on supplied constraints).\n        # Include special start of sequence (num_tags + 1) and end of sequence tags (num_tags + 2)\n        if constraints is None:\n            # All transitions are valid.\n            constraint_mask = torch.Tensor(num_tags + 2, num_tags + 2).fill_(1.0)\n        else:\n            constraint_mask = torch.Tensor(num_tags + 2, num_tags + 2).fill_(0.0)\n            for i, j in constraints:\n                constraint_mask[i, j] = 1.0\n\n        self._constraint_mask = torch.nn.Parameter(constraint_mask, requires_grad=False)\n\n        # Also need logits for transitioning from ""start"" state and to ""end"" state.\n        self.include_start_end_transitions = include_start_end_transitions\n        if include_start_end_transitions:\n            self.start_transitions = torch.nn.Parameter(torch.Tensor(num_tags))\n            self.end_transitions = torch.nn.Parameter(torch.Tensor(num_tags))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.xavier_normal_(self.transitions)\n        if self.include_start_end_transitions:\n            torch.nn.init.normal_(self.start_transitions)\n            torch.nn.init.normal_(self.end_transitions)\n\n    def _input_likelihood(self, logits: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor:\n        """"""\n        Computes the (batch_size,) denominator term for the log-likelihood, which is the\n        sum of the likelihoods across all possible state sequences.\n        """"""\n        batch_size, sequence_length, num_tags = logits.size()\n\n        # Transpose batch size and sequence dimensions\n        mask = mask.transpose(0, 1).contiguous()\n        logits = logits.transpose(0, 1).contiguous()\n\n        # Initial alpha is the (batch_size, num_tags) tensor of likelihoods combining the\n        # transitions to the initial states and the logits for the first timestep.\n        if self.include_start_end_transitions:\n            alpha = self.start_transitions.view(1, num_tags) + logits[0]\n        else:\n            alpha = logits[0]\n\n        # For each i we compute logits for the transitions from timestep i-1 to timestep i.\n        # We do so in a (batch_size, num_tags, num_tags) tensor where the axes are\n        # (instance, current_tag, next_tag)\n        for i in range(1, sequence_length):\n            # The emit scores are for time i (""next_tag"") so we broadcast along the current_tag axis.\n            emit_scores = logits[i].view(batch_size, 1, num_tags)\n            # Transition scores are (current_tag, next_tag) so we broadcast along the instance axis.\n            transition_scores = self.transitions.view(1, num_tags, num_tags)\n            # Alpha is for the current_tag, so we broadcast along the next_tag axis.\n            broadcast_alpha = alpha.view(batch_size, num_tags, 1)\n\n            # Add all the scores together and logexp over the current_tag axis.\n            inner = broadcast_alpha + emit_scores + transition_scores\n\n            # In valid positions (mask == True) we want to take the logsumexp over the current_tag dimension\n            # of `inner`. Otherwise (mask == False) we want to retain the previous alpha.\n            alpha = util.logsumexp(inner, 1) * mask[i].view(batch_size, 1) + alpha * (\n                ~mask[i]\n            ).view(batch_size, 1)\n\n        # Every sequence needs to end with a transition to the stop_tag.\n        if self.include_start_end_transitions:\n            stops = alpha + self.end_transitions.view(1, num_tags)\n        else:\n            stops = alpha\n\n        # Finally we log_sum_exp along the num_tags dim, result is (batch_size,)\n        return util.logsumexp(stops)\n\n    def _joint_likelihood(\n        self, logits: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor\n    ) -> torch.Tensor:\n        """"""\n        Computes the numerator term for the log-likelihood, which is just score(inputs, tags)\n        """"""\n        batch_size, sequence_length, _ = logits.data.shape\n\n        # Transpose batch size and sequence dimensions:\n        logits = logits.transpose(0, 1).contiguous()\n        mask = mask.transpose(0, 1).contiguous()\n        tags = tags.transpose(0, 1).contiguous()\n\n        # Start with the transition scores from start_tag to the first tag in each input\n        if self.include_start_end_transitions:\n            score = self.start_transitions.index_select(0, tags[0])\n        else:\n            score = 0.0\n\n        # Add up the scores for the observed transitions and all the inputs but the last\n        for i in range(sequence_length - 1):\n            # Each is shape (batch_size,)\n            current_tag, next_tag = tags[i], tags[i + 1]\n\n            # The scores for transitioning from current_tag to next_tag\n            transition_score = self.transitions[current_tag.view(-1), next_tag.view(-1)]\n\n            # The score for using current_tag\n            emit_score = logits[i].gather(1, current_tag.view(batch_size, 1)).squeeze(1)\n\n            # Include transition score if next element is unmasked,\n            # input_score if this element is unmasked.\n            score = score + transition_score * mask[i + 1] + emit_score * mask[i]\n\n        # Transition from last state to ""stop"" state. To start with, we need to find the last tag\n        # for each instance.\n        last_tag_index = mask.sum(0).long() - 1\n        last_tags = tags.gather(0, last_tag_index.view(1, batch_size)).squeeze(0)\n\n        # Compute score of transitioning to `stop_tag` from each ""last tag"".\n        if self.include_start_end_transitions:\n            last_transition_score = self.end_transitions.index_select(0, last_tags)\n        else:\n            last_transition_score = 0.0\n\n        # Add the last input if it\'s not masked.\n        last_inputs = logits[-1]  # (batch_size, num_tags)\n        last_input_score = last_inputs.gather(1, last_tags.view(-1, 1))  # (batch_size, 1)\n        last_input_score = last_input_score.squeeze()  # (batch_size,)\n\n        score = score + last_transition_score + last_input_score * mask[-1]\n\n        return score\n\n    def forward(\n        self, inputs: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor = None\n    ) -> torch.Tensor:\n        """"""\n        Computes the log likelihood.\n        """"""\n\n        if mask is None:\n            mask = torch.ones(*tags.size(), dtype=torch.bool)\n\n        log_denominator = self._input_likelihood(inputs, mask)\n        log_numerator = self._joint_likelihood(inputs, tags, mask)\n\n        return torch.sum(log_numerator - log_denominator)\n\n    def viterbi_tags(\n        self, logits: torch.Tensor, mask: torch.BoolTensor = None, top_k: int = None\n    ) -> Union[List[VITERBI_DECODING], List[List[VITERBI_DECODING]]]:\n        """"""\n        Uses viterbi algorithm to find most likely tags for the given inputs.\n        If constraints are applied, disallows all other transitions.\n\n        Returns a list of results, of the same size as the batch (one result per batch member)\n        Each result is a List of length top_k, containing the top K viterbi decodings\n        Each decoding is a tuple  (tag_sequence, viterbi_score)\n\n        For backwards compatibility, if top_k is None, then instead returns a flat list of\n        tag sequences (the top tag sequence for each batch item).\n        """"""\n        if mask is None:\n            mask = torch.ones(*logits.shape[:2], dtype=torch.bool, device=logits.device)\n\n        if top_k is None:\n            top_k = 1\n            flatten_output = True\n        else:\n            flatten_output = False\n\n        _, max_seq_length, num_tags = logits.size()\n\n        # Get the tensors out of the variables\n        logits, mask = logits.data, mask.data\n\n        # Augment transitions matrix with start and end transitions\n        start_tag = num_tags\n        end_tag = num_tags + 1\n        transitions = torch.Tensor(num_tags + 2, num_tags + 2).fill_(-10000.0)\n\n        # Apply transition constraints\n        constrained_transitions = self.transitions * self._constraint_mask[\n            :num_tags, :num_tags\n        ] + -10000.0 * (1 - self._constraint_mask[:num_tags, :num_tags])\n        transitions[:num_tags, :num_tags] = constrained_transitions.data\n\n        if self.include_start_end_transitions:\n            transitions[\n                start_tag, :num_tags\n            ] = self.start_transitions.detach() * self._constraint_mask[\n                start_tag, :num_tags\n            ].data + -10000.0 * (\n                1 - self._constraint_mask[start_tag, :num_tags].detach()\n            )\n            transitions[:num_tags, end_tag] = self.end_transitions.detach() * self._constraint_mask[\n                :num_tags, end_tag\n            ].data + -10000.0 * (1 - self._constraint_mask[:num_tags, end_tag].detach())\n        else:\n            transitions[start_tag, :num_tags] = -10000.0 * (\n                1 - self._constraint_mask[start_tag, :num_tags].detach()\n            )\n            transitions[:num_tags, end_tag] = -10000.0 * (\n                1 - self._constraint_mask[:num_tags, end_tag].detach()\n            )\n\n        best_paths = []\n        # Pad the max sequence length by 2 to account for start_tag + end_tag.\n        tag_sequence = torch.Tensor(max_seq_length + 2, num_tags + 2)\n\n        for prediction, prediction_mask in zip(logits, mask):\n            mask_indices = prediction_mask.nonzero().squeeze()\n            masked_prediction = torch.index_select(prediction, 0, mask_indices)\n            sequence_length = masked_prediction.shape[0]\n\n            # Start with everything totally unlikely\n            tag_sequence.fill_(-10000.0)\n            # At timestep 0 we must have the START_TAG\n            tag_sequence[0, start_tag] = 0.0\n            # At steps 1, ..., sequence_length we just use the incoming prediction\n            tag_sequence[1 : (sequence_length + 1), :num_tags] = masked_prediction\n            # And at the last timestep we must have the END_TAG\n            tag_sequence[sequence_length + 1, end_tag] = 0.0\n\n            # We pass the tags and the transitions to `viterbi_decode`.\n            viterbi_paths, viterbi_scores = util.viterbi_decode(\n                tag_sequence=tag_sequence[: (sequence_length + 2)],\n                transition_matrix=transitions,\n                top_k=top_k,\n            )\n            top_k_paths = []\n            for viterbi_path, viterbi_score in zip(viterbi_paths, viterbi_scores):\n                # Get rid of START and END sentinels and append.\n                viterbi_path = viterbi_path[1:-1]\n                top_k_paths.append((viterbi_path, viterbi_score.item()))\n            best_paths.append(top_k_paths)\n\n        if flatten_output:\n            return [top_k_paths[0] for top_k_paths in best_paths]\n\n        return best_paths\n'"
allennlp/modules/elmo.py,52,"b'import json\nimport logging\nimport warnings\nfrom typing import Any, Dict, List, Union\n\nimport numpy\nimport torch\nfrom overrides import overrides\nfrom torch.nn.modules import Dropout\n\nfrom allennlp.common import FromParams\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.common.util import lazy_groups_of\nfrom allennlp.data import Instance, Token, Vocabulary\nfrom allennlp.data.batch import Batch\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.token_indexers.elmo_indexer import (\n    ELMoCharacterMapper,\n    ELMoTokenCharactersIndexer,\n)\nfrom allennlp.modules.elmo_lstm import ElmoLstm\nfrom allennlp.modules.highway import Highway\nfrom allennlp.modules.scalar_mix import ScalarMix\nfrom allennlp.nn.util import (\n    add_sentence_boundary_token_ids,\n    get_device_of,\n    remove_sentence_boundaries,\n)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(""ignore"", category=FutureWarning)\n    import h5py\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Elmo(torch.nn.Module, FromParams):\n    """"""\n    Compute ELMo representations using a pre-trained bidirectional language model.\n\n    See ""Deep contextualized word representations"", Peters et al. for details.\n\n    This module takes character id input and computes `num_output_representations` different layers\n    of ELMo representations.  Typically `num_output_representations` is 1 or 2.  For example, in\n    the case of the SRL model in the above paper, `num_output_representations=1` where ELMo was included at\n    the input token representation layer.  In the case of the SQuAD model, `num_output_representations=2`\n    as ELMo was also included at the GRU output layer.\n\n    In the implementation below, we learn separate scalar weights for each output layer,\n    but only run the biLM once on each input sequence for efficiency.\n\n    # Parameters\n\n    options_file : `str`, required.\n        ELMo JSON options file\n    weight_file : `str`, required.\n        ELMo hdf5 weight file\n    num_output_representations : `int`, required.\n        The number of ELMo representation to output with\n        different linear weighted combination of the 3 layers (i.e.,\n        character-convnet output, 1st lstm output, 2nd lstm output).\n    requires_grad : `bool`, optional\n        If True, compute gradient of ELMo parameters for fine tuning.\n    do_layer_norm : `bool`, optional, (default = `False`).\n        Should we apply layer normalization (passed to `ScalarMix`)?\n    dropout : `float`, optional, (default = `0.5`).\n        The dropout to be applied to the ELMo representations.\n    vocab_to_cache : `List[str]`, optional, (default = `None`).\n        A list of words to pre-compute and cache character convolutions\n        for. If you use this option, Elmo expects that you pass word\n        indices of shape (batch_size, timesteps) to forward, instead\n        of character indices. If you use this option and pass a word which\n        wasn\'t pre-cached, this will break.\n    keep_sentence_boundaries : `bool`, optional, (default = `False`)\n        If True, the representation of the sentence boundary tokens are\n        not removed.\n    scalar_mix_parameters : `List[float]`, optional, (default = `None`)\n        If not `None`, use these scalar mix parameters to weight the representations\n        produced by different layers. These mixing weights are not updated during\n        training. The mixing weights here should be the unnormalized (i.e., pre-softmax)\n        weights. So, if you wanted to use only the 1st layer of a 2-layer ELMo,\n        you can set this to [-9e10, 1, -9e10 ].\n    module : `torch.nn.Module`, optional, (default = `None`).\n        If provided, then use this module instead of the pre-trained ELMo biLM.\n        If using this option, then pass `None` for both `options_file`\n        and `weight_file`.  The module must provide a public attribute\n        `num_layers` with the number of internal layers and its `forward`\n        method must return a `dict` with `activations` and `mask` keys\n        (see `_ElmoBilm` for an example).  Note that `requires_grad` is also\n        ignored with this option.\n    """"""\n\n    def __init__(\n        self,\n        options_file: str,\n        weight_file: str,\n        num_output_representations: int,\n        requires_grad: bool = False,\n        do_layer_norm: bool = False,\n        dropout: float = 0.5,\n        vocab_to_cache: List[str] = None,\n        keep_sentence_boundaries: bool = False,\n        scalar_mix_parameters: List[float] = None,\n        module: torch.nn.Module = None,\n    ) -> None:\n        super().__init__()\n\n        logger.info(""Initializing ELMo"")\n        if module is not None:\n            if options_file is not None or weight_file is not None:\n                raise ConfigurationError(""Don\'t provide options_file or weight_file with module"")\n            self._elmo_lstm = module\n        else:\n            self._elmo_lstm = _ElmoBiLm(\n                options_file,\n                weight_file,\n                requires_grad=requires_grad,\n                vocab_to_cache=vocab_to_cache,\n            )\n        self._has_cached_vocab = vocab_to_cache is not None\n        self._keep_sentence_boundaries = keep_sentence_boundaries\n        self._dropout = Dropout(p=dropout)\n        self._scalar_mixes: Any = []\n        for k in range(num_output_representations):\n            scalar_mix = ScalarMix(\n                self._elmo_lstm.num_layers,\n                do_layer_norm=do_layer_norm,\n                initial_scalar_parameters=scalar_mix_parameters,\n                trainable=scalar_mix_parameters is None,\n            )\n            self.add_module(""scalar_mix_{}"".format(k), scalar_mix)\n            self._scalar_mixes.append(scalar_mix)\n\n    def get_output_dim(self):\n        return self._elmo_lstm.get_output_dim()\n\n    def forward(\n        self, inputs: torch.Tensor, word_inputs: torch.Tensor = None\n    ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:\n        """"""\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n        Shape `(batch_size, timesteps, 50)` of character ids representing the current batch.\n        word_inputs : `torch.Tensor`, required.\n            If you passed a cached vocab, you can in addition pass a tensor of shape\n            `(batch_size, timesteps)`, which represent word ids which have been pre-cached.\n\n        # Returns\n\n        `Dict[str, Union[torch.Tensor, List[torch.Tensor]]]`\n            A dict with the following keys:\n            - `\'elmo_representations\'` (`List[torch.Tensor]`) :\n              A `num_output_representations` list of ELMo representations for the input sequence.\n              Each representation is shape `(batch_size, timesteps, embedding_dim)`\n            - `\'mask\'` (`torch.BoolTensor`) :\n              Shape `(batch_size, timesteps)` long tensor with sequence mask.\n        """"""\n        # reshape the input if needed\n        original_shape = inputs.size()\n        if len(original_shape) > 3:\n            timesteps, num_characters = original_shape[-2:]\n            reshaped_inputs = inputs.view(-1, timesteps, num_characters)\n        else:\n            reshaped_inputs = inputs\n\n        if word_inputs is not None:\n            original_word_size = word_inputs.size()\n            if self._has_cached_vocab and len(original_word_size) > 2:\n                reshaped_word_inputs = word_inputs.view(-1, original_word_size[-1])\n            elif not self._has_cached_vocab:\n                logger.warning(\n                    ""Word inputs were passed to ELMo but it does not have a cached vocab.""\n                )\n                reshaped_word_inputs = None\n            else:\n                reshaped_word_inputs = word_inputs\n        else:\n            reshaped_word_inputs = word_inputs\n\n        # run the biLM\n        bilm_output = self._elmo_lstm(reshaped_inputs, reshaped_word_inputs)\n        layer_activations = bilm_output[""activations""]\n        mask_with_bos_eos = bilm_output[""mask""]\n\n        # compute the elmo representations\n        representations = []\n        for i in range(len(self._scalar_mixes)):\n            scalar_mix = getattr(self, ""scalar_mix_{}"".format(i))\n            representation_with_bos_eos = scalar_mix(layer_activations, mask_with_bos_eos)\n            if self._keep_sentence_boundaries:\n                processed_representation = representation_with_bos_eos\n                processed_mask = mask_with_bos_eos\n            else:\n                representation_without_bos_eos, mask_without_bos_eos = remove_sentence_boundaries(\n                    representation_with_bos_eos, mask_with_bos_eos\n                )\n                processed_representation = representation_without_bos_eos\n                processed_mask = mask_without_bos_eos\n            representations.append(self._dropout(processed_representation))\n\n        # reshape if necessary\n        if word_inputs is not None and len(original_word_size) > 2:\n            mask = processed_mask.view(original_word_size)\n            elmo_representations = [\n                representation.view(original_word_size + (-1,))\n                for representation in representations\n            ]\n        elif len(original_shape) > 3:\n            mask = processed_mask.view(original_shape[:-1])\n            elmo_representations = [\n                representation.view(original_shape[:-1] + (-1,))\n                for representation in representations\n            ]\n        else:\n            mask = processed_mask\n            elmo_representations = representations\n\n        return {""elmo_representations"": elmo_representations, ""mask"": mask}\n\n\ndef batch_to_ids(batch: List[List[str]]) -> torch.Tensor:\n    """"""\n    Converts a batch of tokenized sentences to a tensor representing the sentences with encoded characters\n    (len(batch), max sentence length, max word length).\n\n    # Parameters\n\n    batch : `List[List[str]]`, required\n        A list of tokenized sentences.\n\n    # Returns\n\n        A tensor of padded character ids.\n    """"""\n    instances = []\n    indexer = ELMoTokenCharactersIndexer()\n    for sentence in batch:\n        tokens = [Token(token) for token in sentence]\n        field = TextField(tokens, {""character_ids"": indexer})\n        instance = Instance({""elmo"": field})\n        instances.append(instance)\n\n    dataset = Batch(instances)\n    vocab = Vocabulary()\n    dataset.index_instances(vocab)\n    return dataset.as_tensor_dict()[""elmo""][""character_ids""][""elmo_tokens""]\n\n\nclass _ElmoCharacterEncoder(torch.nn.Module):\n    """"""\n    Compute context insensitive token representation using pretrained biLM.\n\n    This embedder has input character ids of size (batch_size, sequence_length, 50)\n    and returns (batch_size, sequence_length + 2, embedding_dim), where embedding_dim\n    is specified in the options file (typically 512).\n\n    We add special entries at the beginning and end of each sequence corresponding\n    to <S> and </S>, the beginning and end of sentence tokens.\n\n    Note: this is a lower level class useful for advanced usage.  Most users should\n    use `ElmoTokenEmbedder` or `allennlp.modules.Elmo` instead.\n\n    # Parameters\n\n    options_file : `str`\n        ELMo JSON options file\n    weight_file : `str`\n        ELMo hdf5 weight file\n    requires_grad : `bool`, optional, (default = `False`).\n        If True, compute gradient of ELMo parameters for fine tuning.\n\n\n    The relevant section of the options file is something like:\n\n    ```\n    {\'char_cnn\': {\n        \'activation\': \'relu\',\n        \'embedding\': {\'dim\': 4},\n        \'filters\': [[1, 4], [2, 8], [3, 16], [4, 32], [5, 64]],\n        \'max_characters_per_token\': 50,\n        \'n_characters\': 262,\n        \'n_highway\': 2\n        }\n    }\n    ```\n    """"""\n\n    def __init__(self, options_file: str, weight_file: str, requires_grad: bool = False) -> None:\n        super().__init__()\n\n        with open(cached_path(options_file), ""r"") as fin:\n            self._options = json.load(fin)\n        self._weight_file = weight_file\n\n        self.output_dim = self._options[""lstm""][""projection_dim""]\n        self.requires_grad = requires_grad\n\n        self._load_weights()\n\n        # Cache the arrays for use in forward -- +1 due to masking.\n        self._beginning_of_sentence_characters = torch.from_numpy(\n            numpy.array(ELMoCharacterMapper.beginning_of_sentence_characters) + 1\n        )\n        self._end_of_sentence_characters = torch.from_numpy(\n            numpy.array(ELMoCharacterMapper.end_of_sentence_characters) + 1\n        )\n\n    def get_output_dim(self):\n        return self.output_dim\n\n    @overrides\n    def forward(self, inputs: torch.Tensor) -> Dict[str, torch.Tensor]:\n        """"""\n        Compute context insensitive token embeddings for ELMo representations.\n\n        # Parameters\n\n        inputs : `torch.Tensor`\n            Shape `(batch_size, sequence_length, 50)` of character ids representing the\n            current batch.\n\n        # Returns\n\n        Dict with keys:\n        `\'token_embedding\'` : `torch.Tensor`\n            Shape `(batch_size, sequence_length + 2, embedding_dim)` tensor with context\n            insensitive token representations.\n        `\'mask\'`:  `torch.BoolTensor`\n            Shape `(batch_size, sequence_length + 2)` long tensor with sequence mask.\n        """"""\n        # Add BOS/EOS\n        mask = (inputs > 0).sum(dim=-1) > 0\n        character_ids_with_bos_eos, mask_with_bos_eos = add_sentence_boundary_token_ids(\n            inputs, mask, self._beginning_of_sentence_characters, self._end_of_sentence_characters\n        )\n\n        # the character id embedding\n        max_chars_per_token = self._options[""char_cnn""][""max_characters_per_token""]\n        # (batch_size * sequence_length, max_chars_per_token, embed_dim)\n        character_embedding = torch.nn.functional.embedding(\n            character_ids_with_bos_eos.view(-1, max_chars_per_token), self._char_embedding_weights\n        )\n\n        # run convolutions\n        cnn_options = self._options[""char_cnn""]\n        if cnn_options[""activation""] == ""tanh"":\n            activation = torch.tanh\n        elif cnn_options[""activation""] == ""relu"":\n            activation = torch.nn.functional.relu\n        else:\n            raise ConfigurationError(""Unknown activation"")\n\n        # (batch_size * sequence_length, embed_dim, max_chars_per_token)\n        character_embedding = torch.transpose(character_embedding, 1, 2)\n        convs = []\n        for i in range(len(self._convolutions)):\n            conv = getattr(self, ""char_conv_{}"".format(i))\n            convolved = conv(character_embedding)\n            # (batch_size * sequence_length, n_filters for this width)\n            convolved, _ = torch.max(convolved, dim=-1)\n            convolved = activation(convolved)\n            convs.append(convolved)\n\n        # (batch_size * sequence_length, n_filters)\n        token_embedding = torch.cat(convs, dim=-1)\n\n        # apply the highway layers (batch_size * sequence_length, n_filters)\n        token_embedding = self._highways(token_embedding)\n\n        # final projection  (batch_size * sequence_length, embedding_dim)\n        token_embedding = self._projection(token_embedding)\n\n        # reshape to (batch_size, sequence_length, embedding_dim)\n        batch_size, sequence_length, _ = character_ids_with_bos_eos.size()\n\n        return {\n            ""mask"": mask_with_bos_eos,\n            ""token_embedding"": token_embedding.view(batch_size, sequence_length, -1),\n        }\n\n    def _load_weights(self):\n        self._load_char_embedding()\n        self._load_cnn_weights()\n        self._load_highway()\n        self._load_projection()\n\n    def _load_char_embedding(self):\n        with h5py.File(cached_path(self._weight_file), ""r"") as fin:\n            char_embed_weights = fin[""char_embed""][...]\n\n        weights = numpy.zeros(\n            (char_embed_weights.shape[0] + 1, char_embed_weights.shape[1]), dtype=""float32""\n        )\n        weights[1:, :] = char_embed_weights\n\n        self._char_embedding_weights = torch.nn.Parameter(\n            torch.FloatTensor(weights), requires_grad=self.requires_grad\n        )\n\n    def _load_cnn_weights(self):\n        cnn_options = self._options[""char_cnn""]\n        filters = cnn_options[""filters""]\n        char_embed_dim = cnn_options[""embedding""][""dim""]\n\n        convolutions = []\n        for i, (width, num) in enumerate(filters):\n            conv = torch.nn.Conv1d(\n                in_channels=char_embed_dim, out_channels=num, kernel_size=width, bias=True\n            )\n            # load the weights\n            with h5py.File(cached_path(self._weight_file), ""r"") as fin:\n                weight = fin[""CNN""][""W_cnn_{}"".format(i)][...]\n                bias = fin[""CNN""][""b_cnn_{}"".format(i)][...]\n\n            w_reshaped = numpy.transpose(weight.squeeze(axis=0), axes=(2, 1, 0))\n            if w_reshaped.shape != tuple(conv.weight.data.shape):\n                raise ValueError(""Invalid weight file"")\n            conv.weight.data.copy_(torch.FloatTensor(w_reshaped))\n            conv.bias.data.copy_(torch.FloatTensor(bias))\n\n            conv.weight.requires_grad = self.requires_grad\n            conv.bias.requires_grad = self.requires_grad\n\n            convolutions.append(conv)\n            self.add_module(""char_conv_{}"".format(i), conv)\n\n        self._convolutions = convolutions\n\n    def _load_highway(self):\n\n        # the highway layers have same dimensionality as the number of cnn filters\n        cnn_options = self._options[""char_cnn""]\n        filters = cnn_options[""filters""]\n        n_filters = sum(f[1] for f in filters)\n        n_highway = cnn_options[""n_highway""]\n\n        # create the layers, and load the weights\n        self._highways = Highway(n_filters, n_highway, activation=torch.nn.functional.relu)\n        for k in range(n_highway):\n            # The AllenNLP highway is one matrix multplication with concatenation of\n            # transform and carry weights.\n            with h5py.File(cached_path(self._weight_file), ""r"") as fin:\n                # The weights are transposed due to multiplication order assumptions in tf\n                # vs pytorch (tf.matmul(X, W) vs pytorch.matmul(W, X))\n                w_transform = numpy.transpose(fin[""CNN_high_{}"".format(k)][""W_transform""][...])\n                # -1.0 since AllenNLP is g * x + (1 - g) * f(x) but tf is (1 - g) * x + g * f(x)\n                w_carry = -1.0 * numpy.transpose(fin[""CNN_high_{}"".format(k)][""W_carry""][...])\n                weight = numpy.concatenate([w_transform, w_carry], axis=0)\n                self._highways._layers[k].weight.data.copy_(torch.FloatTensor(weight))\n                self._highways._layers[k].weight.requires_grad = self.requires_grad\n\n                b_transform = fin[""CNN_high_{}"".format(k)][""b_transform""][...]\n                b_carry = -1.0 * fin[""CNN_high_{}"".format(k)][""b_carry""][...]\n                bias = numpy.concatenate([b_transform, b_carry], axis=0)\n                self._highways._layers[k].bias.data.copy_(torch.FloatTensor(bias))\n                self._highways._layers[k].bias.requires_grad = self.requires_grad\n\n    def _load_projection(self):\n        cnn_options = self._options[""char_cnn""]\n        filters = cnn_options[""filters""]\n        n_filters = sum(f[1] for f in filters)\n\n        self._projection = torch.nn.Linear(n_filters, self.output_dim, bias=True)\n        with h5py.File(cached_path(self._weight_file), ""r"") as fin:\n            weight = fin[""CNN_proj""][""W_proj""][...]\n            bias = fin[""CNN_proj""][""b_proj""][...]\n            self._projection.weight.data.copy_(torch.FloatTensor(numpy.transpose(weight)))\n            self._projection.bias.data.copy_(torch.FloatTensor(bias))\n\n            self._projection.weight.requires_grad = self.requires_grad\n            self._projection.bias.requires_grad = self.requires_grad\n\n\nclass _ElmoBiLm(torch.nn.Module):\n    """"""\n    Run a pre-trained bidirectional language model, outputting the activations at each\n    layer for weighting together into an ELMo representation (with\n    `allennlp.modules.seq2seq_encoders.Elmo`).  This is a lower level class, useful\n    for advanced uses, but most users should use `allennlp.modules.Elmo` directly.\n\n    # Parameters\n\n    options_file : `str`\n        ELMo JSON options file\n    weight_file : `str`\n        ELMo hdf5 weight file\n    requires_grad : `bool`, optional, (default = `False`).\n        If True, compute gradient of ELMo parameters for fine tuning.\n    vocab_to_cache : `List[str]`, optional, (default = `None`).\n        A list of words to pre-compute and cache character convolutions\n        for. If you use this option, _ElmoBiLm expects that you pass word\n        indices of shape (batch_size, timesteps) to forward, instead\n        of character indices. If you use this option and pass a word which\n        wasn\'t pre-cached, this will break.\n    """"""\n\n    def __init__(\n        self,\n        options_file: str,\n        weight_file: str,\n        requires_grad: bool = False,\n        vocab_to_cache: List[str] = None,\n    ) -> None:\n        super().__init__()\n\n        self._token_embedder = _ElmoCharacterEncoder(\n            options_file, weight_file, requires_grad=requires_grad\n        )\n\n        self._requires_grad = requires_grad\n        if requires_grad and vocab_to_cache:\n            logging.warning(\n                ""You are fine tuning ELMo and caching char CNN word vectors. ""\n                ""This behaviour is not guaranteed to be well defined, particularly. ""\n                ""if not all of your inputs will occur in the vocabulary cache.""\n            )\n        # This is an embedding, used to look up cached\n        # word vectors built from character level cnn embeddings.\n        self._word_embedding = None\n        self._bos_embedding: torch.Tensor = None\n        self._eos_embedding: torch.Tensor = None\n        if vocab_to_cache:\n            logging.info(""Caching character cnn layers for words in vocabulary."")\n            # This sets 3 attributes, _word_embedding, _bos_embedding and _eos_embedding.\n            # They are set in the method so they can be accessed from outside the\n            # constructor.\n            self.create_cached_cnn_embeddings(vocab_to_cache)\n\n        with open(cached_path(options_file), ""r"") as fin:\n            options = json.load(fin)\n        if not options[""lstm""].get(""use_skip_connections""):\n            raise ConfigurationError(""We only support pretrained biLMs with residual connections"")\n        self._elmo_lstm = ElmoLstm(\n            input_size=options[""lstm""][""projection_dim""],\n            hidden_size=options[""lstm""][""projection_dim""],\n            cell_size=options[""lstm""][""dim""],\n            num_layers=options[""lstm""][""n_layers""],\n            memory_cell_clip_value=options[""lstm""][""cell_clip""],\n            state_projection_clip_value=options[""lstm""][""proj_clip""],\n            requires_grad=requires_grad,\n        )\n        self._elmo_lstm.load_weights(weight_file)\n        # Number of representation layers including context independent layer\n        self.num_layers = options[""lstm""][""n_layers""] + 1\n\n    def get_output_dim(self):\n        return 2 * self._token_embedder.get_output_dim()\n\n    def forward(\n        self, inputs: torch.Tensor, word_inputs: torch.Tensor = None\n    ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:\n        """"""\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n            Shape `(batch_size, timesteps, 50)` of character ids representing the current batch.\n        word_inputs : `torch.Tensor`, required.\n            If you passed a cached vocab, you can in addition pass a tensor of shape `(batch_size, timesteps)`,\n            which represent word ids which have been pre-cached.\n\n        # Returns\n\n        Dict with keys:\n\n        `\'activations\'` : `List[torch.Tensor]`\n            A list of activations at each layer of the network, each of shape\n            `(batch_size, timesteps + 2, embedding_dim)`\n        `\'mask\'`:  `torch.BoolTensor`\n            Shape `(batch_size, timesteps + 2)` long tensor with sequence mask.\n\n        Note that the output tensors all include additional special begin and end of sequence\n        markers.\n        """"""\n        if self._word_embedding is not None and word_inputs is not None:\n            try:\n                mask_without_bos_eos = word_inputs > 0\n                # The character cnn part is cached - just look it up.\n                embedded_inputs = self._word_embedding(word_inputs)  # type: ignore\n                # shape (batch_size, timesteps + 2, embedding_dim)\n                type_representation, mask = add_sentence_boundary_token_ids(\n                    embedded_inputs, mask_without_bos_eos, self._bos_embedding, self._eos_embedding\n                )\n            except (RuntimeError, IndexError):\n                # Back off to running the character convolutions,\n                # as we might not have the words in the cache.\n                token_embedding = self._token_embedder(inputs)\n                mask = token_embedding[""mask""]\n                type_representation = token_embedding[""token_embedding""]\n        else:\n            token_embedding = self._token_embedder(inputs)\n            mask = token_embedding[""mask""]\n            type_representation = token_embedding[""token_embedding""]\n        lstm_outputs = self._elmo_lstm(type_representation, mask)\n\n        # Prepare the output.  The first layer is duplicated.\n        # Because of minor differences in how masking is applied depending\n        # on whether the char cnn layers are cached, we\'ll be defensive and\n        # multiply by the mask here. It\'s not strictly necessary, as the\n        # mask passed on is correct, but the values in the padded areas\n        # of the char cnn representations can change.\n        output_tensors = [\n            torch.cat([type_representation, type_representation], dim=-1) * mask.unsqueeze(-1)\n        ]\n        for layer_activations in torch.chunk(lstm_outputs, lstm_outputs.size(0), dim=0):\n            output_tensors.append(layer_activations.squeeze(0))\n\n        return {""activations"": output_tensors, ""mask"": mask}\n\n    def create_cached_cnn_embeddings(self, tokens: List[str]) -> None:\n        """"""\n        Given a list of tokens, this method precomputes word representations\n        by running just the character convolutions and highway layers of elmo,\n        essentially creating uncontextual word vectors. On subsequent forward passes,\n        the word ids are looked up from an embedding, rather than being computed on\n        the fly via the CNN encoder.\n\n        This function sets 3 attributes:\n\n        _word_embedding : `torch.Tensor`\n            The word embedding for each word in the tokens passed to this method.\n        _bos_embedding : `torch.Tensor`\n            The embedding for the BOS token.\n        _eos_embedding : `torch.Tensor`\n            The embedding for the EOS token.\n\n        # Parameters\n\n        tokens : `List[str]`, required.\n            A list of tokens to precompute character convolutions for.\n        """"""\n        tokens = [ELMoCharacterMapper.bos_token, ELMoCharacterMapper.eos_token] + tokens\n        timesteps = 32\n        batch_size = 32\n        chunked_tokens = lazy_groups_of(iter(tokens), timesteps)\n\n        all_embeddings = []\n        device = get_device_of(next(self.parameters()))\n        for batch in lazy_groups_of(chunked_tokens, batch_size):\n            # Shape (batch_size, timesteps, 50)\n            batched_tensor = batch_to_ids(batch)\n            # NOTE: This device check is for when a user calls this method having\n            # already placed the model on a device. If this is called in the\n            # constructor, it will probably happen on the CPU. This isn\'t too bad,\n            # because it\'s only a few convolutions and will likely be very fast.\n            if device >= 0:\n                batched_tensor = batched_tensor.cuda(device)\n            output = self._token_embedder(batched_tensor)\n            token_embedding = output[""token_embedding""]\n            mask = output[""mask""]\n            token_embedding, _ = remove_sentence_boundaries(token_embedding, mask)\n            all_embeddings.append(token_embedding.view(-1, token_embedding.size(-1)))\n        full_embedding = torch.cat(all_embeddings, 0)\n\n        # We might have some trailing embeddings from padding in the batch, so\n        # we clip the embedding and lookup to the right size.\n        full_embedding = full_embedding[: len(tokens), :]\n        embedding = full_embedding[2 : len(tokens), :]\n        vocab_size, embedding_dim = list(embedding.size())\n\n        from allennlp.modules.token_embedders import Embedding  # type: ignore\n\n        self._bos_embedding = full_embedding[0, :]\n        self._eos_embedding = full_embedding[1, :]\n        self._word_embedding = Embedding(  # type: ignore\n            num_embeddings=vocab_size,\n            embedding_dim=embedding_dim,\n            weight=embedding.data,\n            trainable=self._requires_grad,\n            padding_index=0,\n        )\n'"
allennlp/modules/elmo_lstm.py,26,"b'""""""\nA stacked bidirectional LSTM with skip connections between layers.\n""""""\nimport warnings\nfrom typing import List, Optional, Tuple\n\nimport numpy\nimport torch\nfrom torch.nn.utils.rnn import PackedSequence, pad_packed_sequence\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.modules.encoder_base import _EncoderBase\nfrom allennlp.modules.lstm_cell_with_projection import LstmCellWithProjection\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(""ignore"", category=FutureWarning)\n    import h5py\n\n\nclass ElmoLstm(_EncoderBase):\n    """"""\n    A stacked, bidirectional LSTM which uses\n    [`LstmCellWithProjection`\'s](./lstm_cell_with_projection.md)\n    with highway layers between the inputs to layers.\n    The inputs to the forward and backward directions are independent - forward and backward\n    states are not concatenated between layers.\n\n    Additionally, this LSTM maintains its `own` state, which is updated every time\n    `forward` is called. It is dynamically resized for different batch sizes and is\n    designed for use with non-continuous inputs (i.e inputs which aren\'t formatted as a stream,\n    such as text used for a language modeling task, which is how stateful RNNs are typically used).\n    This is non-standard, but can be thought of as having an ""end of sentence"" state, which is\n    carried across different sentences.\n\n    [0]: https://arxiv.org/abs/1512.05287\n\n    # Parameters\n\n    input_size : `int`, required\n        The dimension of the inputs to the LSTM.\n    hidden_size : `int`, required\n        The dimension of the outputs of the LSTM.\n    cell_size : `int`, required.\n        The dimension of the memory cell of the `LstmCellWithProjection`.\n    num_layers : `int`, required\n        The number of bidirectional LSTMs to use.\n    requires_grad : `bool`, optional\n        If True, compute gradient of ELMo parameters for fine tuning.\n    recurrent_dropout_probability : `float`, optional (default = `0.0`)\n        The dropout probability to be used in a dropout scheme as stated in\n        [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks][0].\n    state_projection_clip_value : `float`, optional, (default = `None`)\n        The magnitude with which to clip the hidden_state after projecting it.\n    memory_cell_clip_value : `float`, optional, (default = `None`)\n        The magnitude with which to clip the memory cell.\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        cell_size: int,\n        num_layers: int,\n        requires_grad: bool = False,\n        recurrent_dropout_probability: float = 0.0,\n        memory_cell_clip_value: Optional[float] = None,\n        state_projection_clip_value: Optional[float] = None,\n    ) -> None:\n        super().__init__(stateful=True)\n\n        # Required to be wrapped with a `PytorchSeq2SeqWrapper`.\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_size = cell_size\n        self.requires_grad = requires_grad\n\n        forward_layers = []\n        backward_layers = []\n\n        lstm_input_size = input_size\n        go_forward = True\n        for layer_index in range(num_layers):\n            forward_layer = LstmCellWithProjection(\n                lstm_input_size,\n                hidden_size,\n                cell_size,\n                go_forward,\n                recurrent_dropout_probability,\n                memory_cell_clip_value,\n                state_projection_clip_value,\n            )\n            backward_layer = LstmCellWithProjection(\n                lstm_input_size,\n                hidden_size,\n                cell_size,\n                not go_forward,\n                recurrent_dropout_probability,\n                memory_cell_clip_value,\n                state_projection_clip_value,\n            )\n            lstm_input_size = hidden_size\n\n            self.add_module(""forward_layer_{}"".format(layer_index), forward_layer)\n            self.add_module(""backward_layer_{}"".format(layer_index), backward_layer)\n            forward_layers.append(forward_layer)\n            backward_layers.append(backward_layer)\n        self.forward_layers = forward_layers\n        self.backward_layers = backward_layers\n\n    def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor:\n        """"""\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n            A Tensor of shape `(batch_size, sequence_length, hidden_size)`.\n        mask : `torch.BoolTensor`, required.\n            A binary mask of shape `(batch_size, sequence_length)` representing the\n            non-padded elements in each sequence in the batch.\n\n        # Returns\n\n        `torch.Tensor`\n            A `torch.Tensor` of shape (num_layers, batch_size, sequence_length, hidden_size),\n            where the num_layers dimension represents the LSTM output from that layer.\n        """"""\n        batch_size, total_sequence_length = mask.size()\n        stacked_sequence_output, final_states, restoration_indices = self.sort_and_run_forward(\n            self._lstm_forward, inputs, mask\n        )\n\n        num_layers, num_valid, returned_timesteps, encoder_dim = stacked_sequence_output.size()\n        # Add back invalid rows which were removed in the call to sort_and_run_forward.\n        if num_valid < batch_size:\n            zeros = stacked_sequence_output.new_zeros(\n                num_layers, batch_size - num_valid, returned_timesteps, encoder_dim\n            )\n            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 1)\n\n            # The states also need to have invalid rows added back.\n            new_states = []\n            for state in final_states:\n                state_dim = state.size(-1)\n                zeros = state.new_zeros(num_layers, batch_size - num_valid, state_dim)\n                new_states.append(torch.cat([state, zeros], 1))\n            final_states = new_states\n\n        # It\'s possible to need to pass sequences which are padded to longer than the\n        # max length of the sequence to a Seq2StackEncoder. However, packing and unpacking\n        # the sequences mean that the returned tensor won\'t include these dimensions, because\n        # the RNN did not need to process them. We add them back on in the form of zeros here.\n        sequence_length_difference = total_sequence_length - returned_timesteps\n        if sequence_length_difference > 0:\n            zeros = stacked_sequence_output.new_zeros(\n                num_layers,\n                batch_size,\n                sequence_length_difference,\n                stacked_sequence_output[0].size(-1),\n            )\n            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 2)\n\n        self._update_states(final_states, restoration_indices)\n\n        # Restore the original indices and return the sequence.\n        # Has shape (num_layers, batch_size, sequence_length, hidden_size)\n        return stacked_sequence_output.index_select(1, restoration_indices)\n\n    def _lstm_forward(\n        self,\n        inputs: PackedSequence,\n        initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        """"""\n        # Parameters\n\n        inputs : `PackedSequence`, required.\n            A batch first `PackedSequence` to run the stacked LSTM over.\n        initial_state : `Tuple[torch.Tensor, torch.Tensor]`, optional, (default = `None`)\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM, with shape (num_layers, batch_size, 2 * hidden_size) and\n            (num_layers, batch_size, 2 * cell_size) respectively.\n\n        # Returns\n\n        output_sequence : `torch.FloatTensor`\n            The encoded sequence of shape (num_layers, batch_size, sequence_length, hidden_size)\n        final_states : `Tuple[torch.FloatTensor, torch.FloatTensor]`\n            The per-layer final (state, memory) states of the LSTM, with shape\n            (num_layers, batch_size, 2 * hidden_size) and  (num_layers, batch_size, 2 * cell_size)\n            respectively. The last dimension is duplicated because it contains the state/memory\n            for both the forward and backward layers.\n        """"""\n        if initial_state is None:\n            hidden_states: List[Optional[Tuple[torch.Tensor, torch.Tensor]]] = [None] * len(\n                self.forward_layers\n            )\n        elif initial_state[0].size()[0] != len(self.forward_layers):\n            raise ConfigurationError(\n                ""Initial states were passed to forward() but the number of ""\n                ""initial states does not match the number of layers.""\n            )\n        else:\n            hidden_states = list(zip(initial_state[0].split(1, 0), initial_state[1].split(1, 0)))\n\n        inputs, batch_lengths = pad_packed_sequence(inputs, batch_first=True)\n        forward_output_sequence = inputs\n        backward_output_sequence = inputs\n\n        final_states = []\n        sequence_outputs = []\n        for layer_index, state in enumerate(hidden_states):\n            forward_layer = getattr(self, ""forward_layer_{}"".format(layer_index))\n            backward_layer = getattr(self, ""backward_layer_{}"".format(layer_index))\n\n            forward_cache = forward_output_sequence\n            backward_cache = backward_output_sequence\n\n            if state is not None:\n                forward_hidden_state, backward_hidden_state = state[0].split(self.hidden_size, 2)\n                forward_memory_state, backward_memory_state = state[1].split(self.cell_size, 2)\n                forward_state = (forward_hidden_state, forward_memory_state)\n                backward_state = (backward_hidden_state, backward_memory_state)\n            else:\n                forward_state = None\n                backward_state = None\n\n            forward_output_sequence, forward_state = forward_layer(\n                forward_output_sequence, batch_lengths, forward_state\n            )\n            backward_output_sequence, backward_state = backward_layer(\n                backward_output_sequence, batch_lengths, backward_state\n            )\n            # Skip connections, just adding the input to the output.\n            if layer_index != 0:\n                forward_output_sequence += forward_cache\n                backward_output_sequence += backward_cache\n\n            sequence_outputs.append(\n                torch.cat([forward_output_sequence, backward_output_sequence], -1)\n            )\n            # Append the state tuples in a list, so that we can return\n            # the final states for all the layers.\n            final_states.append(\n                (\n                    torch.cat([forward_state[0], backward_state[0]], -1),\n                    torch.cat([forward_state[1], backward_state[1]], -1),\n                )\n            )\n\n        stacked_sequence_outputs: torch.FloatTensor = torch.stack(sequence_outputs)\n        # Stack the hidden state and memory for each layer into 2 tensors of shape\n        # (num_layers, batch_size, hidden_size) and (num_layers, batch_size, cell_size)\n        # respectively.\n        final_hidden_states, final_memory_states = zip(*final_states)\n        final_state_tuple: Tuple[torch.FloatTensor, torch.FloatTensor] = (\n            torch.cat(final_hidden_states, 0),\n            torch.cat(final_memory_states, 0),\n        )\n        return stacked_sequence_outputs, final_state_tuple\n\n    def load_weights(self, weight_file: str) -> None:\n        """"""\n        Load the pre-trained weights from the file.\n        """"""\n        requires_grad = self.requires_grad\n\n        with h5py.File(cached_path(weight_file), ""r"") as fin:\n            for i_layer, lstms in enumerate(zip(self.forward_layers, self.backward_layers)):\n                for j_direction, lstm in enumerate(lstms):\n                    # lstm is an instance of LSTMCellWithProjection\n                    cell_size = lstm.cell_size\n\n                    dataset = fin[""RNN_%s"" % j_direction][""RNN""][""MultiRNNCell""][\n                        ""Cell%s"" % i_layer\n                    ][""LSTMCell""]\n\n                    # tensorflow packs together both W and U matrices into one matrix,\n                    # but pytorch maintains individual matrices.  In addition, tensorflow\n                    # packs the gates as input, memory, forget, output but pytorch\n                    # uses input, forget, memory, output.  So we need to modify the weights.\n                    tf_weights = numpy.transpose(dataset[""W_0""][...])\n                    torch_weights = tf_weights.copy()\n\n                    # split the W from U matrices\n                    input_size = lstm.input_size\n                    input_weights = torch_weights[:, :input_size]\n                    recurrent_weights = torch_weights[:, input_size:]\n                    tf_input_weights = tf_weights[:, :input_size]\n                    tf_recurrent_weights = tf_weights[:, input_size:]\n\n                    # handle the different gate order convention\n                    for torch_w, tf_w in [\n                        [input_weights, tf_input_weights],\n                        [recurrent_weights, tf_recurrent_weights],\n                    ]:\n                        torch_w[(1 * cell_size) : (2 * cell_size), :] = tf_w[\n                            (2 * cell_size) : (3 * cell_size), :\n                        ]\n                        torch_w[(2 * cell_size) : (3 * cell_size), :] = tf_w[\n                            (1 * cell_size) : (2 * cell_size), :\n                        ]\n\n                    lstm.input_linearity.weight.data.copy_(torch.FloatTensor(input_weights))\n                    lstm.state_linearity.weight.data.copy_(torch.FloatTensor(recurrent_weights))\n                    lstm.input_linearity.weight.requires_grad = requires_grad\n                    lstm.state_linearity.weight.requires_grad = requires_grad\n\n                    # the bias weights\n                    tf_bias = dataset[""B""][...]\n                    # tensorflow adds 1.0 to forget gate bias instead of modifying the\n                    # parameters...\n                    tf_bias[(2 * cell_size) : (3 * cell_size)] += 1\n                    torch_bias = tf_bias.copy()\n                    torch_bias[(1 * cell_size) : (2 * cell_size)] = tf_bias[\n                        (2 * cell_size) : (3 * cell_size)\n                    ]\n                    torch_bias[(2 * cell_size) : (3 * cell_size)] = tf_bias[\n                        (1 * cell_size) : (2 * cell_size)\n                    ]\n                    lstm.state_linearity.bias.data.copy_(torch.FloatTensor(torch_bias))\n                    lstm.state_linearity.bias.requires_grad = requires_grad\n\n                    # the projection weights\n                    proj_weights = numpy.transpose(dataset[""W_P_0""][...])\n                    lstm.state_projection.weight.data.copy_(torch.FloatTensor(proj_weights))\n                    lstm.state_projection.weight.requires_grad = requires_grad\n'"
allennlp/modules/encoder_base.py,23,"b'from typing import Tuple, Union, Optional, Callable, Any\nimport torch\nfrom torch.nn.utils.rnn import pack_padded_sequence, PackedSequence\n\nfrom allennlp.nn.util import get_lengths_from_binary_sequence_mask, sort_batch_by_length\n\n# We have two types here for the state, because storing the state in something\n# which is Iterable (like a tuple, below), is helpful for internal manipulation\n# - however, the states are consumed as either Tensors or a Tuple of Tensors, so\n# returning them in this format is unhelpful.\nRnnState = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\nRnnStateStorage = Tuple[torch.Tensor, ...]\n\n\nclass _EncoderBase(torch.nn.Module):\n\n    """"""\n    This abstract class serves as a base for the 3 `Encoder` abstractions in AllenNLP.\n    - [`Seq2SeqEncoders`](./seq2seq_encoders/seq2seq_encoder.md)\n    - [`Seq2VecEncoders`](./seq2vec_encoders/seq2vec_encoder.md)\n\n    Additionally, this class provides functionality for sorting sequences by length\n    so they can be consumed by Pytorch RNN classes, which require their inputs to be\n    sorted by length. Finally, it also provides optional statefulness to all of it\'s\n    subclasses by allowing the caching and retrieving of the hidden states of RNNs.\n    """"""\n\n    def __init__(self, stateful: bool = False) -> None:\n        super().__init__()\n        self.stateful = stateful\n        self._states: Optional[RnnStateStorage] = None\n\n    def sort_and_run_forward(\n        self,\n        module: Callable[\n            [PackedSequence, Optional[RnnState]],\n            Tuple[Union[PackedSequence, torch.Tensor], RnnState],\n        ],\n        inputs: torch.Tensor,\n        mask: torch.BoolTensor,\n        hidden_state: Optional[RnnState] = None,\n    ):\n        """"""\n        This function exists because Pytorch RNNs require that their inputs be sorted\n        before being passed as input. As all of our Seq2xxxEncoders use this functionality,\n        it is provided in a base class. This method can be called on any module which\n        takes as input a `PackedSequence` and some `hidden_state`, which can either be a\n        tuple of tensors or a tensor.\n\n        As all of our Seq2xxxEncoders have different return types, we return `sorted`\n        outputs from the module, which is called directly. Additionally, we return the\n        indices into the batch dimension required to restore the tensor to it\'s correct,\n        unsorted order and the number of valid batch elements (i.e the number of elements\n        in the batch which are not completely masked). This un-sorting and re-padding\n        of the module outputs is left to the subclasses because their outputs have different\n        types and handling them smoothly here is difficult.\n\n        # Parameters\n\n        module : `Callable[RnnInputs, RnnOutputs]`\n            A function to run on the inputs, where\n            `RnnInputs: [PackedSequence, Optional[RnnState]]` and\n            `RnnOutputs: Tuple[Union[PackedSequence, torch.Tensor], RnnState]`.\n            In most cases, this is a `torch.nn.Module`.\n        inputs : `torch.Tensor`, required.\n            A tensor of shape `(batch_size, sequence_length, embedding_size)` representing\n            the inputs to the Encoder.\n        mask : `torch.BoolTensor`, required.\n            A tensor of shape `(batch_size, sequence_length)`, representing masked and\n            non-masked elements of the sequence for each element in the batch.\n        hidden_state : `Optional[RnnState]`, (default = `None`).\n            A single tensor of shape (num_layers, batch_size, hidden_size) representing the\n            state of an RNN with or a tuple of\n            tensors of shapes (num_layers, batch_size, hidden_size) and\n            (num_layers, batch_size, memory_size), representing the hidden state and memory\n            state of an LSTM-like RNN.\n\n        # Returns\n\n        module_output : `Union[torch.Tensor, PackedSequence]`.\n            A Tensor or PackedSequence representing the output of the Pytorch Module.\n            The batch size dimension will be equal to `num_valid`, as sequences of zero\n            length are clipped off before the module is called, as Pytorch cannot handle\n            zero length sequences.\n        final_states : `Optional[RnnState]`\n            A Tensor representing the hidden state of the Pytorch Module. This can either\n            be a single tensor of shape (num_layers, num_valid, hidden_size), for instance in\n            the case of a GRU, or a tuple of tensors, such as those required for an LSTM.\n        restoration_indices : `torch.LongTensor`\n            A tensor of shape `(batch_size,)`, describing the re-indexing required to transform\n            the outputs back to their original batch order.\n        """"""\n        # In some circumstances you may have sequences of zero length. `pack_padded_sequence`\n        # requires all sequence lengths to be > 0, so remove sequences of zero length before\n        # calling self._module, then fill with zeros.\n\n        # First count how many sequences are empty.\n        batch_size = mask.size(0)\n        num_valid = torch.sum(mask[:, 0]).int().item()\n\n        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n        (\n            sorted_inputs,\n            sorted_sequence_lengths,\n            restoration_indices,\n            sorting_indices,\n        ) = sort_batch_by_length(inputs, sequence_lengths)\n\n        # Now create a PackedSequence with only the non-empty, sorted sequences.\n        packed_sequence_input = pack_padded_sequence(\n            sorted_inputs[:num_valid, :, :],\n            sorted_sequence_lengths[:num_valid].data.tolist(),\n            batch_first=True,\n        )\n        # Prepare the initial states.\n        if not self.stateful:\n            if hidden_state is None:\n                initial_states: Any = hidden_state\n            elif isinstance(hidden_state, tuple):\n                initial_states = [\n                    state.index_select(1, sorting_indices)[:, :num_valid, :].contiguous()\n                    for state in hidden_state\n                ]\n            else:\n                initial_states = hidden_state.index_select(1, sorting_indices)[\n                    :, :num_valid, :\n                ].contiguous()\n\n        else:\n            initial_states = self._get_initial_states(batch_size, num_valid, sorting_indices)\n\n        # Actually call the module on the sorted PackedSequence.\n        module_output, final_states = module(packed_sequence_input, initial_states)\n\n        return module_output, final_states, restoration_indices\n\n    def _get_initial_states(\n        self, batch_size: int, num_valid: int, sorting_indices: torch.LongTensor\n    ) -> Optional[RnnState]:\n        """"""\n        Returns an initial state for use in an RNN. Additionally, this method handles\n        the batch size changing across calls by mutating the state to append initial states\n        for new elements in the batch. Finally, it also handles sorting the states\n        with respect to the sequence lengths of elements in the batch and removing rows\n        which are completely padded. Importantly, this `mutates` the state if the\n        current batch size is larger than when it was previously called.\n\n        # Parameters\n\n        batch_size : `int`, required.\n            The batch size can change size across calls to stateful RNNs, so we need\n            to know if we need to expand or shrink the states before returning them.\n            Expanded states will be set to zero.\n        num_valid : `int`, required.\n            The batch may contain completely padded sequences which get removed before\n            the sequence is passed through the encoder. We also need to clip these off\n            of the state too.\n        sorting_indices `torch.LongTensor`, required.\n            Pytorch RNNs take sequences sorted by length. When we return the states to be\n            used for a given call to `module.forward`, we need the states to match up to\n            the sorted sequences, so before returning them, we sort the states using the\n            same indices used to sort the sequences.\n\n        # Returns\n\n        This method has a complex return type because it has to deal with the first time it\n        is called, when it has no state, and the fact that types of RNN have heterogeneous\n        states.\n\n        If it is the first time the module has been called, it returns `None`, regardless\n        of the type of the `Module`.\n\n        Otherwise, for LSTMs, it returns a tuple of `torch.Tensors` with shape\n        `(num_layers, num_valid, state_size)` and `(num_layers, num_valid, memory_size)`\n        respectively, or for GRUs, it returns a single `torch.Tensor` of shape\n        `(num_layers, num_valid, state_size)`.\n        """"""\n        # We don\'t know the state sizes the first time calling forward,\n        # so we let the module define what it\'s initial hidden state looks like.\n        if self._states is None:\n            return None\n\n        # Otherwise, we have some previous states.\n        if batch_size > self._states[0].size(1):\n            # This batch is larger than the all previous states.\n            # If so, resize the states.\n            num_states_to_concat = batch_size - self._states[0].size(1)\n            resized_states = []\n            # state has shape (num_layers, batch_size, hidden_size)\n            for state in self._states:\n                # This _must_ be inside the loop because some\n                # RNNs have states with different last dimension sizes.\n                zeros = state.new_zeros(state.size(0), num_states_to_concat, state.size(2))\n                resized_states.append(torch.cat([state, zeros], 1))\n            self._states = tuple(resized_states)\n            correctly_shaped_states = self._states\n\n        elif batch_size < self._states[0].size(1):\n            # This batch is smaller than the previous one.\n            correctly_shaped_states = tuple(state[:, :batch_size, :] for state in self._states)\n        else:\n            correctly_shaped_states = self._states\n\n        # At this point, our states are of shape (num_layers, batch_size, hidden_size).\n        # However, the encoder uses sorted sequences and additionally removes elements\n        # of the batch which are fully padded. We need the states to match up to these\n        # sorted and filtered sequences, so we do that in the next two blocks before\n        # returning the state/s.\n        if len(self._states) == 1:\n            # GRUs only have a single state. This `unpacks` it from the\n            # tuple and returns the tensor directly.\n            correctly_shaped_state = correctly_shaped_states[0]\n            sorted_state = correctly_shaped_state.index_select(1, sorting_indices)\n            return sorted_state[:, :num_valid, :].contiguous()\n        else:\n            # LSTMs have a state tuple of (state, memory).\n            sorted_states = [\n                state.index_select(1, sorting_indices) for state in correctly_shaped_states\n            ]\n            return tuple(state[:, :num_valid, :].contiguous() for state in sorted_states)\n\n    def _update_states(\n        self, final_states: RnnStateStorage, restoration_indices: torch.LongTensor\n    ) -> None:\n        """"""\n        After the RNN has run forward, the states need to be updated.\n        This method just sets the state to the updated new state, performing\n        several pieces of book-keeping along the way - namely, unsorting the\n        states and ensuring that the states of completely padded sequences are\n        not updated. Finally, it also detaches the state variable from the\n        computational graph, such that the graph can be garbage collected after\n        each batch iteration.\n\n        # Parameters\n\n        final_states : `RnnStateStorage`, required.\n            The hidden states returned as output from the RNN.\n        restoration_indices : `torch.LongTensor`, required.\n            The indices that invert the sorting used in `sort_and_run_forward`\n            to order the states with respect to the lengths of the sequences in\n            the batch.\n        """"""\n        # TODO(Mark): seems weird to sort here, but append zeros in the subclasses.\n        # which way around is best?\n        new_unsorted_states = [state.index_select(1, restoration_indices) for state in final_states]\n\n        if self._states is None:\n            # We don\'t already have states, so just set the\n            # ones we receive to be the current state.\n            self._states = tuple(state.data for state in new_unsorted_states)\n        else:\n            # Now we\'ve sorted the states back so that they correspond to the original\n            # indices, we need to figure out what states we need to update, because if we\n            # didn\'t use a state for a particular row, we want to preserve its state.\n            # Thankfully, the rows which are all zero in the state correspond exactly\n            # to those which aren\'t used, so we create masks of shape (new_batch_size,),\n            # denoting which states were used in the RNN computation.\n            current_state_batch_size = self._states[0].size(1)\n            new_state_batch_size = final_states[0].size(1)\n            # Masks for the unused states of shape (1, new_batch_size, 1)\n            used_new_rows_mask = [\n                (state[0, :, :].sum(-1) != 0.0).float().view(1, new_state_batch_size, 1)\n                for state in new_unsorted_states\n            ]\n            new_states = []\n            if current_state_batch_size > new_state_batch_size:\n                # The new state is smaller than the old one,\n                # so just update the indices which we used.\n                for old_state, new_state, used_mask in zip(\n                    self._states, new_unsorted_states, used_new_rows_mask\n                ):\n                    # zero out all rows in the previous state\n                    # which _were_ used in the current state.\n                    masked_old_state = old_state[:, :new_state_batch_size, :] * (1 - used_mask)\n                    # The old state is larger, so update the relevant parts of it.\n                    old_state[:, :new_state_batch_size, :] = new_state + masked_old_state\n                    new_states.append(old_state.detach())\n            else:\n                # The states are the same size, so we just have to\n                # deal with the possibility that some rows weren\'t used.\n                new_states = []\n                for old_state, new_state, used_mask in zip(\n                    self._states, new_unsorted_states, used_new_rows_mask\n                ):\n                    # zero out all rows which _were_ used in the current state.\n                    masked_old_state = old_state * (1 - used_mask)\n                    # The old state is larger, so update the relevant parts of it.\n                    new_state += masked_old_state\n                    new_states.append(new_state.detach())\n\n            # It looks like there should be another case handled here - when\n            # the current_state_batch_size < new_state_batch_size. However,\n            # this never happens, because the states themeselves are mutated\n            # by appending zeros when calling _get_inital_states, meaning that\n            # the new states are either of equal size, or smaller, in the case\n            # that there are some unused elements (zero-length) for the RNN computation.\n            self._states = tuple(new_states)\n\n    def reset_states(self, mask: torch.BoolTensor = None) -> None:\n        """"""\n        Resets the internal states of a stateful encoder.\n\n        # Parameters\n\n        mask : `torch.BoolTensor`, optional.\n            A tensor of shape `(batch_size,)` indicating which states should\n            be reset. If not provided, all states will be reset.\n        """"""\n        if mask is None:\n            self._states = None\n        else:\n            # state has shape (num_layers, batch_size, hidden_size). We reshape\n            # mask to have shape (1, batch_size, 1) so that operations\n            # broadcast properly.\n            mask_batch_size = mask.size(0)\n            mask = mask.view(1, mask_batch_size, 1)\n            new_states = []\n            for old_state in self._states:\n                old_state_batch_size = old_state.size(1)\n                if old_state_batch_size != mask_batch_size:\n                    raise ValueError(\n                        f""Trying to reset states using mask with incorrect batch size. ""\n                        f""Expected batch size: {old_state_batch_size}. ""\n                        f""Provided batch size: {mask_batch_size}.""\n                    )\n                new_state = ~mask * old_state\n                new_states.append(new_state.detach())\n            self._states = tuple(new_states)\n'"
allennlp/modules/feedforward.py,9,"b'""""""\nA feed-forward neural network.\n""""""\nfrom typing import List, Union\n\nimport torch\n\nfrom allennlp.common import FromParams\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.nn import Activation\n\n\nclass FeedForward(torch.nn.Module, FromParams):\n    """"""\n    This `Module` is a feed-forward neural network, just a sequence of `Linear` layers with\n    activation functions in between.\n\n    # Parameters\n\n    input_dim : `int`, required\n        The dimensionality of the input.  We assume the input has shape `(batch_size, input_dim)`.\n    num_layers : `int`, required\n        The number of `Linear` layers to apply to the input.\n    hidden_dims : `Union[int, List[int]]`, required\n        The output dimension of each of the `Linear` layers.  If this is a single `int`, we use\n        it for all `Linear` layers.  If it is a `List[int]`, `len(hidden_dims)` must be\n        `num_layers`.\n    activations : `Union[Activation, List[Activation]]`, required\n        The activation function to use after each `Linear` layer.  If this is a single function,\n        we use it after all `Linear` layers.  If it is a `List[Activation]`,\n        `len(activations)` must be `num_layers`. Activation must have torch.nn.Module type.\n    dropout : `Union[float, List[float]]`, optional (default = `0.0`)\n        If given, we will apply this amount of dropout after each layer.  Semantics of `float`\n        versus `List[float]` is the same as with other parameters.\n\n    # Examples\n\n    ```python\n    FeedForward(124, 2, [64, 32], torch.nn.ReLU(), 0.2)\n    #> FeedForward(\n    #>   (_activations): ModuleList(\n    #>     (0): ReLU()\n    #>     (1): ReLU()\n    #>   )\n    #>   (_linear_layers): ModuleList(\n    #>     (0): Linear(in_features=124, out_features=64, bias=True)\n    #>     (1): Linear(in_features=64, out_features=32, bias=True)\n    #>   )\n    #>   (_dropout): ModuleList(\n    #>     (0): Dropout(p=0.2, inplace=False)\n    #>     (1): Dropout(p=0.2, inplace=False)\n    #>   )\n    #> )\n    ```\n    """"""\n\n    def __init__(\n        self,\n        input_dim: int,\n        num_layers: int,\n        hidden_dims: Union[int, List[int]],\n        activations: Union[Activation, List[Activation]],\n        dropout: Union[float, List[float]] = 0.0,\n    ) -> None:\n\n        super().__init__()\n        if not isinstance(hidden_dims, list):\n            hidden_dims = [hidden_dims] * num_layers  # type: ignore\n        if not isinstance(activations, list):\n            activations = [activations] * num_layers  # type: ignore\n        if not isinstance(dropout, list):\n            dropout = [dropout] * num_layers  # type: ignore\n        if len(hidden_dims) != num_layers:\n            raise ConfigurationError(\n                ""len(hidden_dims) (%d) != num_layers (%d)"" % (len(hidden_dims), num_layers)\n            )\n        if len(activations) != num_layers:\n            raise ConfigurationError(\n                ""len(activations) (%d) != num_layers (%d)"" % (len(activations), num_layers)\n            )\n        if len(dropout) != num_layers:\n            raise ConfigurationError(\n                ""len(dropout) (%d) != num_layers (%d)"" % (len(dropout), num_layers)\n            )\n        self._activations = torch.nn.ModuleList(activations)\n        input_dims = [input_dim] + hidden_dims[:-1]\n        linear_layers = []\n        for layer_input_dim, layer_output_dim in zip(input_dims, hidden_dims):\n            linear_layers.append(torch.nn.Linear(layer_input_dim, layer_output_dim))\n        self._linear_layers = torch.nn.ModuleList(linear_layers)\n        dropout_layers = [torch.nn.Dropout(p=value) for value in dropout]\n        self._dropout = torch.nn.ModuleList(dropout_layers)\n        self._output_dim = hidden_dims[-1]\n        self.input_dim = input_dim\n\n    def get_output_dim(self):\n        return self._output_dim\n\n    def get_input_dim(self):\n        return self.input_dim\n\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n\n        output = inputs\n        for layer, activation, dropout in zip(\n            self._linear_layers, self._activations, self._dropout\n        ):\n            output = dropout(activation(layer(output)))\n        return output\n'"
allennlp/modules/gated_sum.py,6,"b'import torch\n\nfrom allennlp.nn import Activation\n\n\nclass GatedSum(torch.nn.Module):\n    """"""\n    This `Module` represents a gated sum of two tensors `a` and `b`. Specifically:\n    ```\n    f = activation(W [a; b])\n    out = f * a + (1 - f) * b\n    ```\n\n    # Parameters\n\n    input_dim : `int`, required\n        The dimensionality of the input. We assume the input have shape `(..., input_dim)`.\n    activation : `Activation`, optional (default = `torch.nn.Sigmoid()`)\n        The activation function to use.\n    """"""\n\n    def __init__(self, input_dim: int, activation: Activation = torch.nn.Sigmoid()) -> None:\n        super().__init__()\n        self.input_dim = input_dim\n        self._gate = torch.nn.Linear(input_dim * 2, 1)\n        self._activation = activation\n\n    def get_input_dim(self):\n        return self.input_dim\n\n    def get_output_dim(self):\n        return self.input_dim\n\n    def forward(self, input_a: torch.Tensor, input_b: torch.Tensor) -> torch.Tensor:\n        if input_a.size() != input_b.size():\n            raise ValueError(""The input must have the same size."")\n        if input_a.size(-1) != self.input_dim:\n            raise ValueError(""Input size must match `input_dim`."")\n        gate_value = self._activation(self._gate(torch.cat([input_a, input_b], -1)))\n        return gate_value * input_a + (1 - gate_value) * input_b\n'"
allennlp/modules/highway.py,7,"b'""""""\nA [Highway layer](https://arxiv.org/abs/1505.00387) that does a gated combination of a linear\ntransformation and a non-linear transformation of its input.\n""""""\n\nfrom typing import Callable\n\nimport torch\nfrom overrides import overrides\n\n\nclass Highway(torch.nn.Module):\n    """"""\n    A [Highway layer](https://arxiv.org/abs/1505.00387) does a gated combination of a linear\n    transformation and a non-linear transformation of its input.  :math:`y = g * x + (1 - g) *\n    f(A(x))`, where :math:`A` is a linear transformation, :math:`f` is an element-wise\n    non-linearity, and :math:`g` is an element-wise gate, computed as :math:`sigmoid(B(x))`.\n\n    This module will apply a fixed number of highway layers to its input, returning the final\n    result.\n\n    # Parameters\n\n    input_dim : `int`, required\n        The dimensionality of :math:`x`.  We assume the input has shape `(batch_size, ...,\n        input_dim)`.\n    num_layers : `int`, optional (default=`1`)\n        The number of highway layers to apply to the input.\n    activation : `Callable[[torch.Tensor], torch.Tensor]`, optional (default=`torch.nn.functional.relu`)\n        The non-linearity to use in the highway layers.\n    """"""\n\n    def __init__(\n        self,\n        input_dim: int,\n        num_layers: int = 1,\n        activation: Callable[[torch.Tensor], torch.Tensor] = torch.nn.functional.relu,\n    ) -> None:\n        super().__init__()\n        self._input_dim = input_dim\n        self._layers = torch.nn.ModuleList(\n            [torch.nn.Linear(input_dim, input_dim * 2) for _ in range(num_layers)]\n        )\n        self._activation = activation\n        for layer in self._layers:\n            # We should bias the highway layer to just carry its input forward.  We do that by\n            # setting the bias on `B(x)` to be positive, because that means `g` will be biased to\n            # be high, so we will carry the input forward.  The bias on `B(x)` is the second half\n            # of the bias vector in each Linear layer.\n            layer.bias[input_dim:].data.fill_(1)\n\n    @overrides\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n        current_input = inputs\n        for layer in self._layers:\n            projected_input = layer(current_input)\n            linear_part = current_input\n            # NOTE: if you modify this, think about whether you should modify the initialization\n            # above, too.\n            nonlinear_part, gate = projected_input.chunk(2, dim=-1)\n            nonlinear_part = self._activation(nonlinear_part)\n            gate = torch.sigmoid(gate)\n            current_input = gate * linear_part + (1 - gate) * nonlinear_part\n        return current_input\n'"
allennlp/modules/input_variational_dropout.py,4,"b'import torch\n\n\nclass InputVariationalDropout(torch.nn.Dropout):\n    """"""\n    Apply the dropout technique in Gal and Ghahramani, [Dropout as a Bayesian Approximation:\n    Representing Model Uncertainty in Deep Learning](https://arxiv.org/abs/1506.02142) to a\n    3D tensor.\n\n    This module accepts a 3D tensor of shape `(batch_size, num_timesteps, embedding_dim)`\n    and samples a single dropout mask of shape `(batch_size, embedding_dim)` and applies\n    it to every time step.\n    """"""\n\n    def forward(self, input_tensor):\n\n        """"""\n        Apply dropout to input tensor.\n\n        # Parameters\n\n        input_tensor : `torch.FloatTensor`\n            A tensor of shape `(batch_size, num_timesteps, embedding_dim)`\n\n        # Returns\n\n        output : `torch.FloatTensor`\n            A tensor of shape `(batch_size, num_timesteps, embedding_dim)` with dropout applied.\n        """"""\n        ones = input_tensor.data.new_ones(input_tensor.shape[0], input_tensor.shape[-1])\n        dropout_mask = torch.nn.functional.dropout(ones, self.p, self.training, inplace=False)\n        if self.inplace:\n            input_tensor *= dropout_mask.unsqueeze(1)\n            return None\n        else:\n            return dropout_mask.unsqueeze(1) * input_tensor\n'"
allennlp/modules/layer_norm.py,4,"b'import torch\n\nfrom allennlp.nn import util\n\n\nclass LayerNorm(torch.nn.Module):\n\n    """"""\n    An implementation of [Layer Normalization](\n    https://www.semanticscholar.org/paper/Layer-Normalization-Ba-Kiros/97fb4e3d45bb098e27e0071448b6152217bd35a5).\n\n    Layer Normalization stabilises the training of deep neural networks by\n    normalising the outputs of neurons from a particular layer. It computes:\n\n    output = (gamma * (tensor - mean) / (std + eps)) + beta\n\n    # Parameters\n\n    dimension : `int`, required.\n        The dimension of the layer output to normalize.\n\n    # Returns\n\n    The normalized layer output.\n    """"""  # noqa\n\n    def __init__(self, dimension: int) -> None:\n        super().__init__()\n        self.gamma = torch.nn.Parameter(torch.ones(dimension))\n        self.beta = torch.nn.Parameter(torch.zeros(dimension))\n\n    def forward(self, tensor: torch.Tensor):\n        mean = tensor.mean(-1, keepdim=True)\n        std = tensor.std(-1, unbiased=False, keepdim=True)\n        return (\n            self.gamma * (tensor - mean) / (std + util.tiny_value_of_dtype(std.dtype)) + self.beta\n        )\n'"
allennlp/modules/lstm_cell_with_projection.py,19,"b'""""""\nAn LSTM with Recurrent Dropout, a hidden_state which is projected and\nclipping on both the hidden state and the memory state of the LSTM.\n""""""\n\nfrom typing import Optional, Tuple, List\n\nimport torch\n\nfrom allennlp.nn.util import get_dropout_mask\nfrom allennlp.nn.initializers import block_orthogonal\n\n\nclass LstmCellWithProjection(torch.nn.Module):\n    """"""\n    An LSTM with Recurrent Dropout and a projected and clipped hidden state and\n    memory. Note: this implementation is slower than the native Pytorch LSTM because\n    it cannot make use of CUDNN optimizations for stacked RNNs due to and\n    variational dropout and the custom nature of the cell state.\n\n    [0]: https://arxiv.org/abs/1512.05287\n\n    # Parameters\n\n    input_size : `int`, required.\n        The dimension of the inputs to the LSTM.\n    hidden_size : `int`, required.\n        The dimension of the outputs of the LSTM.\n    cell_size : `int`, required.\n        The dimension of the memory cell used for the LSTM.\n    go_forward : `bool`, optional (default = `True`)\n        The direction in which the LSTM is applied to the sequence.\n        Forwards by default, or backwards if False.\n    recurrent_dropout_probability : `float`, optional (default = `0.0`)\n        The dropout probability to be used in a dropout scheme as stated in\n        [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks]\n        [0]. Implementation wise, this simply\n        applies a fixed dropout mask per sequence to the recurrent connection of the\n        LSTM.\n    state_projection_clip_value : `float`, optional, (default = `None`)\n        The magnitude with which to clip the hidden_state after projecting it.\n    memory_cell_clip_value : `float`, optional, (default = `None`)\n        The magnitude with which to clip the memory cell.\n\n    # Returns\n\n    output_accumulator : `torch.FloatTensor`\n        The outputs of the LSTM for each timestep. A tensor of shape\n        (batch_size, max_timesteps, hidden_size) where for a given batch\n        element, all outputs past the sequence length for that batch are\n        zero tensors.\n    final_state : `Tuple[torch.FloatTensor, torch.FloatTensor]`\n        The final (state, memory) states of the LSTM, with shape\n        (1, batch_size, hidden_size) and  (1, batch_size, cell_size)\n        respectively. The first dimension is 1 in order to match the Pytorch\n        API for returning stacked LSTM states.\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        cell_size: int,\n        go_forward: bool = True,\n        recurrent_dropout_probability: float = 0.0,\n        memory_cell_clip_value: Optional[float] = None,\n        state_projection_clip_value: Optional[float] = None,\n    ) -> None:\n        super().__init__()\n        # Required to be wrapped with a `PytorchSeq2SeqWrapper`.\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.cell_size = cell_size\n\n        self.go_forward = go_forward\n        self.state_projection_clip_value = state_projection_clip_value\n        self.memory_cell_clip_value = memory_cell_clip_value\n        self.recurrent_dropout_probability = recurrent_dropout_probability\n\n        # We do the projections for all the gates all at once.\n        self.input_linearity = torch.nn.Linear(input_size, 4 * cell_size, bias=False)\n        self.state_linearity = torch.nn.Linear(hidden_size, 4 * cell_size, bias=True)\n\n        # Additional projection matrix for making the hidden state smaller.\n        self.state_projection = torch.nn.Linear(cell_size, hidden_size, bias=False)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        # Use sensible default initializations for parameters.\n        block_orthogonal(self.input_linearity.weight.data, [self.cell_size, self.input_size])\n        block_orthogonal(self.state_linearity.weight.data, [self.cell_size, self.hidden_size])\n\n        self.state_linearity.bias.data.fill_(0.0)\n        # Initialize forget gate biases to 1.0 as per An Empirical\n        # Exploration of Recurrent Network Architectures, (Jozefowicz, 2015).\n        self.state_linearity.bias.data[self.cell_size : 2 * self.cell_size].fill_(1.0)\n\n    def forward(\n        self,\n        inputs: torch.FloatTensor,\n        batch_lengths: List[int],\n        initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n    ):\n        """"""\n        # Parameters\n\n        inputs : `torch.FloatTensor`, required.\n            A tensor of shape (batch_size, num_timesteps, input_size)\n            to apply the LSTM over.\n        batch_lengths : `List[int]`, required.\n            A list of length batch_size containing the lengths of the sequences in batch.\n        initial_state : `Tuple[torch.Tensor, torch.Tensor]`, optional, (default = `None`)\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM. The `state` has shape (1, batch_size, hidden_size) and the\n            `memory` has shape (1, batch_size, cell_size).\n\n        # Returns\n\n        output_accumulator : `torch.FloatTensor`\n            The outputs of the LSTM for each timestep. A tensor of shape\n            (batch_size, max_timesteps, hidden_size) where for a given batch\n            element, all outputs past the sequence length for that batch are\n            zero tensors.\n        final_state : `Tuple[torch.FloatTensor, torch.FloatTensor]`\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM. The `state` has shape (1, batch_size, hidden_size) and the\n            `memory` has shape (1, batch_size, cell_size).\n        """"""\n        batch_size = inputs.size()[0]\n        total_timesteps = inputs.size()[1]\n\n        output_accumulator = inputs.new_zeros(batch_size, total_timesteps, self.hidden_size)\n\n        if initial_state is None:\n            full_batch_previous_memory = inputs.new_zeros(batch_size, self.cell_size)\n            full_batch_previous_state = inputs.new_zeros(batch_size, self.hidden_size)\n        else:\n            full_batch_previous_state = initial_state[0].squeeze(0)\n            full_batch_previous_memory = initial_state[1].squeeze(0)\n\n        current_length_index = batch_size - 1 if self.go_forward else 0\n        if self.recurrent_dropout_probability > 0.0 and self.training:\n            dropout_mask = get_dropout_mask(\n                self.recurrent_dropout_probability, full_batch_previous_state\n            )\n        else:\n            dropout_mask = None\n\n        for timestep in range(total_timesteps):\n            # The index depends on which end we start.\n            index = timestep if self.go_forward else total_timesteps - timestep - 1\n\n            # What we are doing here is finding the index into the batch dimension\n            # which we need to use for this timestep, because the sequences have\n            # variable length, so once the index is greater than the length of this\n            # particular batch sequence, we no longer need to do the computation for\n            # this sequence. The key thing to recognise here is that the batch inputs\n            # must be _ordered_ by length from longest (first in batch) to shortest\n            # (last) so initially, we are going forwards with every sequence and as we\n            # pass the index at which the shortest elements of the batch finish,\n            # we stop picking them up for the computation.\n            if self.go_forward:\n                while batch_lengths[current_length_index] <= index:\n                    current_length_index -= 1\n            # If we\'re going backwards, we are _picking up_ more indices.\n            else:\n                # First conditional: Are we already at the maximum number of elements in the batch?\n                # Second conditional: Does the next shortest sequence beyond the current batch\n                # index require computation use this timestep?\n                while (\n                    current_length_index < (len(batch_lengths) - 1)\n                    and batch_lengths[current_length_index + 1] > index\n                ):\n                    current_length_index += 1\n\n            # Actually get the slices of the batch which we\n            # need for the computation at this timestep.\n            # shape (batch_size, cell_size)\n            previous_memory = full_batch_previous_memory[0 : current_length_index + 1].clone()\n            # Shape (batch_size, hidden_size)\n            previous_state = full_batch_previous_state[0 : current_length_index + 1].clone()\n            # Shape (batch_size, input_size)\n            timestep_input = inputs[0 : current_length_index + 1, index]\n\n            # Do the projections for all the gates all at once.\n            # Both have shape (batch_size, 4 * cell_size)\n            projected_input = self.input_linearity(timestep_input)\n            projected_state = self.state_linearity(previous_state)\n\n            # Main LSTM equations using relevant chunks of the big linear\n            # projections of the hidden state and inputs.\n            input_gate = torch.sigmoid(\n                projected_input[:, (0 * self.cell_size) : (1 * self.cell_size)]\n                + projected_state[:, (0 * self.cell_size) : (1 * self.cell_size)]\n            )\n            forget_gate = torch.sigmoid(\n                projected_input[:, (1 * self.cell_size) : (2 * self.cell_size)]\n                + projected_state[:, (1 * self.cell_size) : (2 * self.cell_size)]\n            )\n            memory_init = torch.tanh(\n                projected_input[:, (2 * self.cell_size) : (3 * self.cell_size)]\n                + projected_state[:, (2 * self.cell_size) : (3 * self.cell_size)]\n            )\n            output_gate = torch.sigmoid(\n                projected_input[:, (3 * self.cell_size) : (4 * self.cell_size)]\n                + projected_state[:, (3 * self.cell_size) : (4 * self.cell_size)]\n            )\n            memory = input_gate * memory_init + forget_gate * previous_memory\n\n            # Here is the non-standard part of this LSTM cell; first, we clip the\n            # memory cell, then we project the output of the timestep to a smaller size\n            # and again clip it.\n\n            if self.memory_cell_clip_value:\n\n                memory = torch.clamp(\n                    memory, -self.memory_cell_clip_value, self.memory_cell_clip_value\n                )\n\n            # shape (current_length_index, cell_size)\n            pre_projection_timestep_output = output_gate * torch.tanh(memory)\n\n            # shape (current_length_index, hidden_size)\n            timestep_output = self.state_projection(pre_projection_timestep_output)\n            if self.state_projection_clip_value:\n\n                timestep_output = torch.clamp(\n                    timestep_output,\n                    -self.state_projection_clip_value,\n                    self.state_projection_clip_value,\n                )\n\n            # Only do dropout if the dropout prob is > 0.0 and we are in training mode.\n            if dropout_mask is not None:\n                timestep_output = timestep_output * dropout_mask[0 : current_length_index + 1]\n\n            # We\'ve been doing computation with less than the full batch, so here we create a new\n            # variable for the the whole batch at this timestep and insert the result for the\n            # relevant elements of the batch into it.\n            full_batch_previous_memory = full_batch_previous_memory.clone()\n            full_batch_previous_state = full_batch_previous_state.clone()\n            full_batch_previous_memory[0 : current_length_index + 1] = memory\n            full_batch_previous_state[0 : current_length_index + 1] = timestep_output\n            output_accumulator[0 : current_length_index + 1, index] = timestep_output\n\n        # Mimic the pytorch API by returning state in the following shape:\n        # (num_layers * num_directions, batch_size, ...). As this\n        # LSTM cell cannot be stacked, the first dimension here is just 1.\n        final_state = (\n            full_batch_previous_state.unsqueeze(0),\n            full_batch_previous_memory.unsqueeze(0),\n        )\n\n        return output_accumulator, final_state\n'"
allennlp/modules/masked_layer_norm.py,5,"b'import torch\n\nfrom allennlp.nn import util\n\n\nclass MaskedLayerNorm(torch.nn.Module):\n    """"""\n    See LayerNorm for details.\n\n    Note, however, that unlike LayerNorm this norm includes a batch component.\n    """"""\n\n    def __init__(self, size: int, gamma0: float = 0.1) -> None:\n        super().__init__()\n        self.gamma = torch.nn.Parameter(torch.ones(1, 1, size) * gamma0)\n        self.beta = torch.nn.Parameter(torch.zeros(1, 1, size))\n        self.size = size\n\n    def forward(self, tensor: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor:\n\n        broadcast_mask = mask.unsqueeze(-1)\n        num_elements = broadcast_mask.sum() * self.size\n        mean = (tensor * broadcast_mask).sum() / num_elements\n        masked_centered = (tensor - mean) * broadcast_mask\n        std = torch.sqrt(\n            (masked_centered * masked_centered).sum() / num_elements\n            + util.tiny_value_of_dtype(tensor.dtype)\n        )\n        return (\n            self.gamma * (tensor - mean) / (std + util.tiny_value_of_dtype(tensor.dtype))\n            + self.beta\n        )\n'"
allennlp/modules/maxout.py,7,"b'""""""\nA maxout neural network.\n""""""\nfrom typing import Sequence, Union\n\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.registrable import FromParams\n\n\nclass Maxout(torch.nn.Module, FromParams):\n    """"""\n    This `Module` is a maxout neural network.\n\n    # Parameters\n\n    input_dim : `int`, required\n        The dimensionality of the input.  We assume the input has shape `(batch_size, input_dim)`.\n    num_layers : `int`, required\n        The number of maxout layers to apply to the input.\n    output_dims : `Union[int, Sequence[int]]`, required\n        The output dimension of each of the maxout layers.  If this is a single `int`, we use\n        it for all maxout layers.  If it is a `Sequence[int]`, `len(output_dims)` must be\n        `num_layers`.\n    pool_sizes : `Union[int, Sequence[int]]`, required\n        The size of max-pools.  If this is a single `int`, we use\n        it for all maxout layers.  If it is a `Sequence[int]`, `len(pool_sizes)` must be\n        `num_layers`.\n    dropout : `Union[float, Sequence[float]]`, optional (default = `0.0`)\n        If given, we will apply this amount of dropout after each layer.  Semantics of `float`\n        versus `Sequence[float]` is the same as with other parameters.\n    """"""\n\n    def __init__(\n        self,\n        input_dim: int,\n        num_layers: int,\n        output_dims: Union[int, Sequence[int]],\n        pool_sizes: Union[int, Sequence[int]],\n        dropout: Union[float, Sequence[float]] = 0.0,\n    ) -> None:\n        super().__init__()\n        if not isinstance(output_dims, list):\n            output_dims = [output_dims] * num_layers  # type: ignore\n        if not isinstance(pool_sizes, list):\n            pool_sizes = [pool_sizes] * num_layers  # type: ignore\n        if not isinstance(dropout, list):\n            dropout = [dropout] * num_layers  # type: ignore\n        if len(output_dims) != num_layers:\n            raise ConfigurationError(\n                ""len(output_dims) (%d) != num_layers (%d)"" % (len(output_dims), num_layers)\n            )\n        if len(pool_sizes) != num_layers:\n            raise ConfigurationError(\n                ""len(pool_sizes) (%d) != num_layers (%d)"" % (len(pool_sizes), num_layers)\n            )\n        if len(dropout) != num_layers:\n            raise ConfigurationError(\n                ""len(dropout) (%d) != num_layers (%d)"" % (len(dropout), num_layers)\n            )\n\n        self._pool_sizes = pool_sizes\n        input_dims = [input_dim] + output_dims[:-1]\n        linear_layers = []\n        for layer_input_dim, layer_output_dim, pool_size in zip(\n            input_dims, output_dims, pool_sizes\n        ):\n            linear_layers.append(torch.nn.Linear(layer_input_dim, layer_output_dim * pool_size))\n        self._linear_layers = torch.nn.ModuleList(linear_layers)\n        dropout_layers = [torch.nn.Dropout(p=value) for value in dropout]\n        self._dropout = torch.nn.ModuleList(dropout_layers)\n        self._output_dims = output_dims\n        self._output_dim = output_dims[-1]\n        self._input_dim = input_dim\n\n    def get_output_dim(self):\n        return self._output_dim\n\n    def get_input_dim(self):\n        return self._input_dim\n\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n\n        output = inputs\n        for layer, layer_output_dim, dropout, pool_size in zip(\n            self._linear_layers, self._output_dims, self._dropout, self._pool_sizes\n        ):\n            affine_output = layer(output)\n            # Compute and apply the proper shape for the max.\n            shape = list(inputs.size())\n            shape[-1] = layer_output_dim\n            shape.append(pool_size)\n\n            maxed_output = torch.max(affine_output.view(*shape), dim=-1)[0]\n            dropped_output = dropout(maxed_output)\n            output = dropped_output\n        return output\n'"
allennlp/modules/residual_with_layer_dropout.py,8,"b'import torch\n\n\nclass ResidualWithLayerDropout(torch.nn.Module):\n    """"""\n    A residual connection with the layer dropout technique [Deep Networks with Stochastic\n    Depth](https://arxiv.org/pdf/1603.09382.pdf).\n\n    This module accepts the input and output of a layer, decides whether this layer should\n    be stochastically dropped, returns either the input or output + input. During testing,\n    it will re-calibrate the outputs of this layer by the expected number of times it\n    participates in training.\n    """"""\n\n    def __init__(self, undecayed_dropout_prob: float = 0.5) -> None:\n        super().__init__()\n        if undecayed_dropout_prob < 0 or undecayed_dropout_prob > 1:\n            raise ValueError(\n                f""undecayed dropout probability has to be between 0 and 1, ""\n                f""but got {undecayed_dropout_prob}""\n            )\n        self.undecayed_dropout_prob = undecayed_dropout_prob\n\n    def forward(\n        self,  # type: ignore\n        layer_input: torch.Tensor,\n        layer_output: torch.Tensor,\n        layer_index: int = None,\n        total_layers: int = None,\n    ) -> torch.Tensor:\n\n        """"""\n        Apply dropout to this layer, for this whole mini-batch.\n        dropout_prob = layer_index / total_layers * undecayed_dropout_prob if layer_idx and\n        total_layers is specified, else it will use the undecayed_dropout_prob directly.\n\n        # Parameters\n\n        layer_input `torch.FloatTensor` required\n            The input tensor of this layer.\n        layer_output `torch.FloatTensor` required\n            The output tensor of this layer, with the same shape as the layer_input.\n        layer_index `int`\n            The layer index, starting from 1. This is used to calcuate the dropout prob\n            together with the `total_layers` parameter.\n        total_layers `int`\n            The total number of layers.\n\n        # Returns\n\n        output : `torch.FloatTensor`\n            A tensor with the same shape as `layer_input` and `layer_output`.\n        """"""\n        if layer_index is not None and total_layers is not None:\n            dropout_prob = 1.0 * self.undecayed_dropout_prob * layer_index / total_layers\n        else:\n            dropout_prob = 1.0 * self.undecayed_dropout_prob\n        if self.training:\n            if torch.rand(1) < dropout_prob:\n                return layer_input\n            else:\n                return layer_output + layer_input\n        else:\n            return (1 - dropout_prob) * layer_output + layer_input\n'"
allennlp/modules/sampled_softmax_loss.py,29,"b'# https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/nn_impl.py#L885\nfrom typing import Set, Tuple\n\nimport numpy as np\n\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.nn import util\n\n\ndef _choice(num_words: int, num_samples: int) -> Tuple[np.ndarray, int]:\n    """"""\n    Chooses `num_samples` samples without replacement from [0, ..., num_words).\n    Returns a tuple (samples, num_tries).\n    """"""\n    num_tries = 0\n    num_chosen = 0\n\n    def get_buffer() -> np.ndarray:\n        log_samples = np.random.rand(num_samples) * np.log(num_words + 1)\n        samples = np.exp(log_samples).astype(""int64"") - 1\n        return np.clip(samples, a_min=0, a_max=num_words - 1)\n\n    sample_buffer = get_buffer()\n    buffer_index = 0\n    samples: Set[int] = set()\n\n    while num_chosen < num_samples:\n        num_tries += 1\n        # choose sample\n        sample_id = sample_buffer[buffer_index]\n        if sample_id not in samples:\n            samples.add(sample_id)\n            num_chosen += 1\n\n        buffer_index += 1\n        if buffer_index == num_samples:\n            # Reset the buffer\n            sample_buffer = get_buffer()\n            buffer_index = 0\n\n    return np.array(list(samples)), num_tries\n\n\nclass SampledSoftmaxLoss(torch.nn.Module):\n    """"""\n    Based on the default log_uniform_candidate_sampler in tensorflow.\n\n    !!! NOTE\n        num_words DOES NOT include padding id.\n\n    !!! NOTE\n        In all cases except (tie_embeddings=True and use_character_inputs=False)\n        the weights are dimensioned as num_words and do not include an entry for the padding (0) id.\n        For the (tie_embeddings=True and use_character_inputs=False) case,\n        then the embeddings DO include the extra 0 padding, to be consistent with the word embedding layer.\n\n    # Parameters\n\n    num_words, `int`, required\n        The number of words in the vocabulary\n    embedding_dim, `int`, required\n        The dimension to softmax over\n    num_samples, `int`, required\n        During training take this many samples. Must be less than num_words.\n    sparse, `bool`, optional (default = `False`)\n        If this is true, we use a sparse embedding matrix.\n    unk_id, `int`, optional (default = `None`)\n        If provided, the id that represents unknown characters.\n    use_character_inputs, `bool`, optional (default = `True`)\n        Whether to use character inputs\n    use_fast_sampler, `bool`, optional (default = `False`)\n        Whether to use the fast cython sampler.\n    """"""\n\n    def __init__(\n        self,\n        num_words: int,\n        embedding_dim: int,\n        num_samples: int,\n        sparse: bool = False,\n        unk_id: int = None,\n        use_character_inputs: bool = True,\n        use_fast_sampler: bool = False,\n    ) -> None:\n        super().__init__()\n\n        # TODO(joelgrus): implement tie_embeddings (maybe)\n        self.tie_embeddings = False\n\n        assert num_samples < num_words\n\n        if use_fast_sampler:\n            raise ConfigurationError(""fast sampler is not implemented"")\n        else:\n            self.choice_func = _choice\n\n        # Glorit init (std=(1.0 / sqrt(fan_in))\n        if sparse:\n            # create our own sparse embedding\n            self.softmax_w = torch.nn.Embedding(\n                num_embeddings=num_words, embedding_dim=embedding_dim, sparse=True\n            )\n            self.softmax_w.weight.data.normal_(mean=0.0, std=1.0 / np.sqrt(embedding_dim))\n            self.softmax_b = torch.nn.Embedding(\n                num_embeddings=num_words, embedding_dim=1, sparse=True\n            )\n            self.softmax_b.weight.data.fill_(0.0)\n        else:\n            # just create tensors to use as the embeddings\n            # Glorit init (std=(1.0 / sqrt(fan_in))\n            self.softmax_w = torch.nn.Parameter(\n                torch.randn(num_words, embedding_dim) / np.sqrt(embedding_dim)\n            )\n            self.softmax_b = torch.nn.Parameter(torch.zeros(num_words))\n\n        self.sparse = sparse\n        self.use_character_inputs = use_character_inputs\n\n        if use_character_inputs:\n            self._unk_id = unk_id\n\n        self._num_samples = num_samples\n        self._embedding_dim = embedding_dim\n        self._num_words = num_words\n        self.initialize_num_words()\n\n    def initialize_num_words(self):\n        if self.sparse:\n            num_words = self.softmax_w.weight.size(0)\n        else:\n            num_words = self.softmax_w.size(0)\n\n        self._num_words = num_words\n        self._log_num_words_p1 = np.log(num_words + 1)\n\n        # compute the probability of each sampled id\n        self._probs = (\n            np.log(np.arange(num_words) + 2) - np.log(np.arange(num_words) + 1)\n        ) / self._log_num_words_p1\n\n    def forward(\n        self,\n        embeddings: torch.Tensor,\n        targets: torch.Tensor,\n        target_token_embedding: torch.Tensor = None,\n    ) -> torch.Tensor:\n        # embeddings is size (n, embedding_dim)\n        # targets is (n_words, ) with the index of the actual target\n        # when tieing weights, target_token_embedding is required.\n        # it is size (n_words, embedding_dim)\n        # returns log likelihood loss (batch_size, )\n        # Does not do any count normalization / divide by batch size\n\n        if embeddings.shape[0] == 0:\n            # empty batch\n            return torch.tensor(0.0).to(embeddings.device)\n\n        if not self.training:\n            return self._forward_eval(embeddings, targets)\n        else:\n            return self._forward_train(embeddings, targets, target_token_embedding)\n\n    def _forward_train(\n        self, embeddings: torch.Tensor, targets: torch.Tensor, target_token_embedding: torch.Tensor\n    ) -> torch.Tensor:\n\n        # (target_token_embedding is only used in the tie_embeddings case,\n        #  which is not implemented)\n\n        # want to compute (n, n_samples + 1) array with the log\n        # probabilities where the first index is the true target\n        # and the remaining ones are the the negative samples.\n        # then we can just select the first column\n\n        # NOTE: targets input has padding removed (so 0 == the first id, NOT the padding id)\n\n        (\n            sampled_ids,\n            target_expected_count,\n            sampled_expected_count,\n        ) = self.log_uniform_candidate_sampler(targets, choice_func=self.choice_func)\n\n        long_targets = targets.long()\n        long_targets.requires_grad_(False)\n\n        # Get the softmax weights (so we can compute logits)\n        # shape (batch_size * max_sequence_length + num_samples)\n        all_ids = torch.cat([long_targets, sampled_ids], dim=0)\n\n        if self.sparse:\n            all_ids_1 = all_ids.unsqueeze(1)\n            all_w = self.softmax_w(all_ids_1).squeeze(1)\n            all_b = self.softmax_b(all_ids_1).squeeze(2).squeeze(1)\n        else:\n            all_w = torch.nn.functional.embedding(all_ids, self.softmax_w)\n            # the unsqueeze / squeeze works around an issue with 1 dim\n            # embeddings\n            all_b = torch.nn.functional.embedding(all_ids, self.softmax_b.unsqueeze(1)).squeeze(1)\n\n        batch_size = long_targets.size(0)\n        true_w = all_w[:batch_size, :]\n        sampled_w = all_w[batch_size:, :]\n        true_b = all_b[:batch_size]\n        sampled_b = all_b[batch_size:]\n\n        # compute the logits and remove log expected counts\n        # [batch_size, ]\n        true_logits = (\n            (true_w * embeddings).sum(dim=1)\n            + true_b\n            - torch.log(\n                target_expected_count + util.tiny_value_of_dtype(target_expected_count.dtype)\n            )\n        )\n        # [batch_size, n_samples]\n        sampled_logits = (\n            torch.matmul(embeddings, sampled_w.t())\n            + sampled_b\n            - torch.log(\n                sampled_expected_count + util.tiny_value_of_dtype(sampled_expected_count.dtype)\n            )\n        )\n\n        # remove true labels -- we will take\n        # softmax, so set the sampled logits of true values to a large\n        # negative number\n        # [batch_size, n_samples]\n        true_in_sample_mask = sampled_ids == long_targets.unsqueeze(1)\n        masked_sampled_logits = sampled_logits.masked_fill(true_in_sample_mask, -10000.0)\n        # now concat the true logits as index 0\n        # [batch_size, n_samples + 1]\n        logits = torch.cat([true_logits.unsqueeze(1), masked_sampled_logits], dim=1)\n\n        # finally take log_softmax\n        log_softmax = torch.nn.functional.log_softmax(logits, dim=1)\n        # true log likelihood is index 0, loss = -1.0 * sum over batch\n        # the likelihood loss can become very large if the corresponding\n        # true logit is very small, so we apply a per-target cap here\n        # so that a single logit for a very rare word won\'t dominate the batch.\n        nll_loss = -1.0 * log_softmax[:, 0].sum()\n        return nll_loss\n\n    def _forward_eval(self, embeddings: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n\n        # evaluation mode, use full softmax\n        if self.sparse:\n            w = self.softmax_w.weight\n            b = self.softmax_b.weight.squeeze(1)\n        else:\n            w = self.softmax_w\n            b = self.softmax_b\n\n        log_softmax = torch.nn.functional.log_softmax(torch.matmul(embeddings, w.t()) + b, dim=-1)\n        if self.tie_embeddings and not self.use_character_inputs:\n            targets_ = targets + 1\n        else:\n            targets_ = targets\n        return torch.nn.functional.nll_loss(log_softmax, targets_.long(), reduction=""sum"")\n\n    def log_uniform_candidate_sampler(self, targets, choice_func=_choice):\n        # returns sampled, true_expected_count, sampled_expected_count\n        # targets = (batch_size, )\n        #\n        #  samples = (n_samples, )\n        #  true_expected_count = (batch_size, )\n        #  sampled_expected_count = (n_samples, )\n\n        # see: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/range_sampler.h\n        # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/range_sampler.cc\n\n        # algorithm: keep track of number of tries when doing sampling,\n        #   then expected count is\n        #   -expm1(num_tries * log1p(-p))\n        # = (1 - (1-p)^num_tries) where p is self._probs[id]\n\n        np_sampled_ids, num_tries = choice_func(self._num_words, self._num_samples)\n\n        sampled_ids = torch.from_numpy(np_sampled_ids).to(targets.device)\n\n        # Compute expected count = (1 - (1-p)^num_tries) = -expm1(num_tries * log1p(-p))\n        # P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)\n        target_probs = (\n            torch.log((targets.float() + 2.0) / (targets.float() + 1.0)) / self._log_num_words_p1\n        )\n        target_expected_count = -1.0 * (torch.exp(num_tries * torch.log1p(-target_probs)) - 1.0)\n        sampled_probs = (\n            torch.log((sampled_ids.float() + 2.0) / (sampled_ids.float() + 1.0))\n            / self._log_num_words_p1\n        )\n        sampled_expected_count = -1.0 * (torch.exp(num_tries * torch.log1p(-sampled_probs)) - 1.0)\n\n        sampled_ids.requires_grad_(False)\n        target_expected_count.requires_grad_(False)\n        sampled_expected_count.requires_grad_(False)\n\n        return sampled_ids, target_expected_count, sampled_expected_count\n'"
allennlp/modules/scalar_mix.py,12,"b'from typing import List\n\nimport torch\nfrom torch.nn import ParameterList, Parameter\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.nn import util\n\n\nclass ScalarMix(torch.nn.Module):\n    """"""\n    Computes a parameterised scalar mixture of N tensors, `mixture = gamma * sum(s_k * tensor_k)`\n    where `s = softmax(w)`, with `w` and `gamma` scalar parameters.\n\n    In addition, if `do_layer_norm=True` then apply layer normalization to each tensor\n    before weighting.\n    """"""\n\n    def __init__(\n        self,\n        mixture_size: int,\n        do_layer_norm: bool = False,\n        initial_scalar_parameters: List[float] = None,\n        trainable: bool = True,\n    ) -> None:\n        super().__init__()\n        self.mixture_size = mixture_size\n        self.do_layer_norm = do_layer_norm\n\n        if initial_scalar_parameters is None:\n            initial_scalar_parameters = [0.0] * mixture_size\n        elif len(initial_scalar_parameters) != mixture_size:\n            raise ConfigurationError(\n                ""Length of initial_scalar_parameters {} differs ""\n                ""from mixture_size {}"".format(initial_scalar_parameters, mixture_size)\n            )\n\n        self.scalar_parameters = ParameterList(\n            [\n                Parameter(\n                    torch.FloatTensor([initial_scalar_parameters[i]]), requires_grad=trainable\n                )\n                for i in range(mixture_size)\n            ]\n        )\n        self.gamma = Parameter(torch.FloatTensor([1.0]), requires_grad=trainable)\n\n    def forward(self, tensors: List[torch.Tensor], mask: torch.BoolTensor = None) -> torch.Tensor:\n        """"""\n        Compute a weighted average of the `tensors`.  The input tensors an be any shape\n        with at least two dimensions, but must all be the same shape.\n\n        When `do_layer_norm=True`, the `mask` is required input.  If the `tensors` are\n        dimensioned  `(dim_0, ..., dim_{n-1}, dim_n)`, then the `mask` is dimensioned\n        `(dim_0, ..., dim_{n-1})`, as in the typical case with `tensors` of shape\n        `(batch_size, timesteps, dim)` and `mask` of shape `(batch_size, timesteps)`.\n\n        When `do_layer_norm=False` the `mask` is ignored.\n        """"""\n        if len(tensors) != self.mixture_size:\n            raise ConfigurationError(\n                ""{} tensors were passed, but the module was initialized to ""\n                ""mix {} tensors."".format(len(tensors), self.mixture_size)\n            )\n\n        def _do_layer_norm(tensor, broadcast_mask, num_elements_not_masked):\n            tensor_masked = tensor * broadcast_mask\n            mean = torch.sum(tensor_masked) / num_elements_not_masked\n            variance = (\n                torch.sum(((tensor_masked - mean) * broadcast_mask) ** 2) / num_elements_not_masked\n            )\n            return (tensor - mean) / torch.sqrt(variance + util.tiny_value_of_dtype(variance.dtype))\n\n        normed_weights = torch.nn.functional.softmax(\n            torch.cat([parameter for parameter in self.scalar_parameters]), dim=0\n        )\n        normed_weights = torch.split(normed_weights, split_size_or_sections=1)\n\n        if not self.do_layer_norm:\n            pieces = []\n            for weight, tensor in zip(normed_weights, tensors):\n                pieces.append(weight * tensor)\n            return self.gamma * sum(pieces)\n\n        else:\n            broadcast_mask = mask.unsqueeze(-1)\n            input_dim = tensors[0].size(-1)\n            num_elements_not_masked = torch.sum(mask) * input_dim\n\n            pieces = []\n            for weight, tensor in zip(normed_weights, tensors):\n                pieces.append(\n                    weight * _do_layer_norm(tensor, broadcast_mask, num_elements_not_masked)\n                )\n            return self.gamma * sum(pieces)\n'"
allennlp/modules/softmax_loss.py,8,"b'import torch\nimport numpy as np\n\n\nclass SoftmaxLoss(torch.nn.Module):\n    """"""\n    Given some embeddings and some targets, applies a linear layer\n    to create logits over possible words and then returns the\n    negative log likelihood.\n    """"""\n\n    def __init__(self, num_words: int, embedding_dim: int) -> None:\n        super().__init__()\n\n        # TODO(joelgrus): implement tie_embeddings (maybe)\n        self.tie_embeddings = False\n\n        self.softmax_w = torch.nn.Parameter(\n            torch.randn(embedding_dim, num_words) / np.sqrt(embedding_dim)\n        )\n        self.softmax_b = torch.nn.Parameter(torch.zeros(num_words))\n\n    def forward(self, embeddings: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n\n        # embeddings is size (n, embedding_dim)\n        # targets is (batch_size, ) with the correct class id\n        # Does not do any count normalization / divide by batch size\n        probs = torch.nn.functional.log_softmax(\n            torch.matmul(embeddings, self.softmax_w) + self.softmax_b, dim=-1\n        )\n\n        return torch.nn.functional.nll_loss(probs, targets.long(), reduction=""sum"")\n'"
allennlp/modules/stacked_alternating_lstm.py,7,"b'""""""\nA stacked LSTM with LSTM layers which alternate between going forwards over\nthe sequence and going backwards.\n""""""\n\nfrom typing import Optional, Tuple, Union, List\nimport torch\nfrom torch.nn.utils.rnn import PackedSequence\nfrom allennlp.modules.augmented_lstm import AugmentedLstm\nfrom allennlp.common.checks import ConfigurationError\n\nTensorPair = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass StackedAlternatingLstm(torch.nn.Module):\n    """"""\n    A stacked LSTM with LSTM layers which alternate between going forwards over\n    the sequence and going backwards. This implementation is based on the\n    description in [Deep Semantic Role Labelling - What works and what\'s next][0].\n\n    [0]: https://www.aclweb.org/anthology/P17-1044.pdf\n    [1]: https://arxiv.org/abs/1512.05287\n\n    # Parameters\n\n    input_size : `int`, required\n        The dimension of the inputs to the LSTM.\n    hidden_size : `int`, required\n        The dimension of the outputs of the LSTM.\n    num_layers : `int`, required\n        The number of stacked LSTMs to use.\n    recurrent_dropout_probability : `float`, optional (default = `0.0`)\n        The dropout probability to be used in a dropout scheme as stated in\n        [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks][1].\n    use_input_projection_bias : `bool`, optional (default = `True`)\n        Whether or not to use a bias on the input projection layer. This is mainly here\n        for backwards compatibility reasons and will be removed (and set to False)\n        in future releases.\n\n    # Returns\n\n    output_accumulator : `PackedSequence`\n        The outputs of the interleaved LSTMs per timestep. A tensor of shape\n        (batch_size, max_timesteps, hidden_size) where for a given batch\n        element, all outputs past the sequence length for that batch are\n        zero tensors.\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int,\n        recurrent_dropout_probability: float = 0.0,\n        use_highway: bool = True,\n        use_input_projection_bias: bool = True,\n    ) -> None:\n        super().__init__()\n\n        # Required to be wrapped with a `PytorchSeq2SeqWrapper`.\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        layers = []\n        lstm_input_size = input_size\n        for layer_index in range(num_layers):\n            go_forward = layer_index % 2 == 0\n            layer = AugmentedLstm(\n                lstm_input_size,\n                hidden_size,\n                go_forward,\n                recurrent_dropout_probability=recurrent_dropout_probability,\n                use_highway=use_highway,\n                use_input_projection_bias=use_input_projection_bias,\n            )\n            lstm_input_size = hidden_size\n            self.add_module(""layer_{}"".format(layer_index), layer)\n            layers.append(layer)\n        self.lstm_layers = layers\n\n    def forward(\n        self, inputs: PackedSequence, initial_state: Optional[TensorPair] = None\n    ) -> Tuple[Union[torch.Tensor, PackedSequence], TensorPair]:\n        """"""\n        # Parameters\n\n        inputs : `PackedSequence`, required.\n            A batch first `PackedSequence` to run the stacked LSTM over.\n        initial_state : `Tuple[torch.Tensor, torch.Tensor]`, optional, (default = `None`)\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM. Each tensor has shape (1, batch_size, output_dimension).\n\n        # Returns\n\n        output_sequence : `PackedSequence`\n            The encoded sequence of shape (batch_size, sequence_length, hidden_size)\n        final_states: `Tuple[torch.Tensor, torch.Tensor]`\n            The per-layer final (state, memory) states of the LSTM, each with shape\n            (num_layers, batch_size, hidden_size).\n        """"""\n        if not initial_state:\n            hidden_states: List[Optional[TensorPair]] = [None] * len(self.lstm_layers)\n        elif initial_state[0].size()[0] != len(self.lstm_layers):\n            raise ConfigurationError(\n                ""Initial states were passed to forward() but the number of ""\n                ""initial states does not match the number of layers.""\n            )\n        else:\n            hidden_states = list(zip(initial_state[0].split(1, 0), initial_state[1].split(1, 0)))\n\n        output_sequence = inputs\n        final_states = []\n        for i, state in enumerate(hidden_states):\n            layer = getattr(self, ""layer_{}"".format(i))\n            # The state is duplicated to mirror the Pytorch API for LSTMs.\n            output_sequence, final_state = layer(output_sequence, state)\n            final_states.append(final_state)\n\n        final_hidden_state, final_cell_state = tuple(\n            torch.cat(state_list, 0) for state_list in zip(*final_states)\n        )\n        return output_sequence, (final_hidden_state, final_cell_state)\n'"
allennlp/modules/stacked_bidirectional_lstm.py,8,"b'from typing import Optional, Tuple, List\nimport torch\nfrom torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence\nfrom allennlp.modules.augmented_lstm import AugmentedLstm\nfrom allennlp.modules.input_variational_dropout import InputVariationalDropout\nfrom allennlp.common.checks import ConfigurationError\n\n\nTensorPair = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass StackedBidirectionalLstm(torch.nn.Module):\n    """"""\n    A standard stacked Bidirectional LSTM where the LSTM layers\n    are concatenated between each layer. The only difference between\n    this and a regular bidirectional LSTM is the application of\n    variational dropout to the hidden states and outputs of each layer apart\n    from the last layer of the LSTM. Note that this will be slower, as it\n    doesn\'t use CUDNN.\n\n    [0]: https://arxiv.org/abs/1512.05287\n\n    # Parameters\n\n    input_size : `int`, required\n        The dimension of the inputs to the LSTM.\n    hidden_size : `int`, required\n        The dimension of the outputs of the LSTM.\n    num_layers : `int`, required\n        The number of stacked Bidirectional LSTMs to use.\n    recurrent_dropout_probability : `float`, optional (default = `0.0`)\n        The recurrent dropout probability to be used in a dropout scheme as\n        stated in [A Theoretically Grounded Application of Dropout in Recurrent\n        Neural Networks][0].\n    layer_dropout_probability : `float`, optional (default = `0.0`)\n        The layer wise dropout probability to be used in a dropout scheme as\n        stated in [A Theoretically Grounded Application of Dropout in Recurrent\n        Neural Networks][0].\n    use_highway : `bool`, optional (default = `True`)\n        Whether or not to use highway connections between layers. This effectively involves\n        reparameterising the normal output of an LSTM as::\n\n            gate = sigmoid(W_x1 * x_t + W_h * h_t)\n            output = gate * h_t  + (1 - gate) * (W_x2 * x_t)\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int,\n        recurrent_dropout_probability: float = 0.0,\n        layer_dropout_probability: float = 0.0,\n        use_highway: bool = True,\n    ) -> None:\n        super().__init__()\n\n        # Required to be wrapped with a `PytorchSeq2SeqWrapper`.\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bidirectional = True\n\n        layers = []\n        lstm_input_size = input_size\n        for layer_index in range(num_layers):\n\n            forward_layer = AugmentedLstm(\n                lstm_input_size,\n                hidden_size,\n                go_forward=True,\n                recurrent_dropout_probability=recurrent_dropout_probability,\n                use_highway=use_highway,\n                use_input_projection_bias=False,\n            )\n            backward_layer = AugmentedLstm(\n                lstm_input_size,\n                hidden_size,\n                go_forward=False,\n                recurrent_dropout_probability=recurrent_dropout_probability,\n                use_highway=use_highway,\n                use_input_projection_bias=False,\n            )\n\n            lstm_input_size = hidden_size * 2\n            self.add_module(""forward_layer_{}"".format(layer_index), forward_layer)\n            self.add_module(""backward_layer_{}"".format(layer_index), backward_layer)\n            layers.append([forward_layer, backward_layer])\n        self.lstm_layers = layers\n        self.layer_dropout = InputVariationalDropout(layer_dropout_probability)\n\n    def forward(\n        self, inputs: PackedSequence, initial_state: Optional[TensorPair] = None\n    ) -> Tuple[PackedSequence, TensorPair]:\n        """"""\n        # Parameters\n\n        inputs : `PackedSequence`, required.\n            A batch first `PackedSequence` to run the stacked LSTM over.\n        initial_state : `Tuple[torch.Tensor, torch.Tensor]`, optional, (default = `None`)\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM. Each tensor has shape (num_layers, batch_size, output_dimension * 2).\n\n        # Returns\n\n        output_sequence : `PackedSequence`\n            The encoded sequence of shape (batch_size, sequence_length, hidden_size * 2)\n        final_states: `torch.Tensor`\n            The per-layer final (state, memory) states of the LSTM, each with shape\n            (num_layers * 2, batch_size, hidden_size * 2).\n        """"""\n        if initial_state is None:\n            hidden_states: List[Optional[TensorPair]] = [None] * len(self.lstm_layers)\n        elif initial_state[0].size()[0] != len(self.lstm_layers):\n            raise ConfigurationError(\n                ""Initial states were passed to forward() but the number of ""\n                ""initial states does not match the number of layers.""\n            )\n        else:\n            hidden_states = list(zip(initial_state[0].split(1, 0), initial_state[1].split(1, 0)))\n\n        output_sequence = inputs\n        final_h = []\n        final_c = []\n        for i, state in enumerate(hidden_states):\n            forward_layer = getattr(self, ""forward_layer_{}"".format(i))\n            backward_layer = getattr(self, ""backward_layer_{}"".format(i))\n            # The state is duplicated to mirror the Pytorch API for LSTMs.\n            forward_output, final_forward_state = forward_layer(output_sequence, state)\n            backward_output, final_backward_state = backward_layer(output_sequence, state)\n\n            forward_output, lengths = pad_packed_sequence(forward_output, batch_first=True)\n            backward_output, _ = pad_packed_sequence(backward_output, batch_first=True)\n\n            output_sequence = torch.cat([forward_output, backward_output], -1)\n            # Apply layer wise dropout on each output sequence apart from the\n            # first (input) and last\n            if i < (self.num_layers - 1):\n                output_sequence = self.layer_dropout(output_sequence)\n            output_sequence = pack_padded_sequence(output_sequence, lengths, batch_first=True)\n\n            final_h.extend([final_forward_state[0], final_backward_state[0]])\n            final_c.extend([final_forward_state[1], final_backward_state[1]])\n\n        final_h = torch.cat(final_h, dim=0)\n        final_c = torch.cat(final_c, dim=0)\n        final_state_tuple = (final_h, final_c)\n        return output_sequence, final_state_tuple\n'"
allennlp/modules/time_distributed.py,2,"b'""""""\nA wrapper that unrolls the second (time) dimension of a tensor\ninto the first (batch) dimension, applies some other `Module`,\nand then rolls the time dimension back up.\n""""""\n\nfrom typing import List\n\nfrom overrides import overrides\nimport torch\n\n\nclass TimeDistributed(torch.nn.Module):\n    """"""\n    Given an input shaped like `(batch_size, time_steps, [rest])` and a `Module` that takes\n    inputs like `(batch_size, [rest])`, `TimeDistributed` reshapes the input to be\n    `(batch_size * time_steps, [rest])`, applies the contained `Module`, then reshapes it back.\n\n    Note that while the above gives shapes with `batch_size` first, this `Module` also works if\n    `batch_size` is second - we always just combine the first two dimensions, then split them.\n\n    It also reshapes keyword arguments unless they are not tensors or their name is specified in\n    the optional `pass_through` iterable.\n    """"""\n\n    def __init__(self, module):\n        super().__init__()\n        self._module = module\n\n    @overrides\n    def forward(self, *inputs, pass_through: List[str] = None, **kwargs):\n\n        pass_through = pass_through or []\n\n        reshaped_inputs = [self._reshape_tensor(input_tensor) for input_tensor in inputs]\n\n        # Need some input to then get the batch_size and time_steps.\n        some_input = None\n        if inputs:\n            some_input = inputs[-1]\n\n        reshaped_kwargs = {}\n        for key, value in kwargs.items():\n            if isinstance(value, torch.Tensor) and key not in pass_through:\n                if some_input is None:\n                    some_input = value\n\n                value = self._reshape_tensor(value)\n\n            reshaped_kwargs[key] = value\n\n        reshaped_outputs = self._module(*reshaped_inputs, **reshaped_kwargs)\n\n        if some_input is None:\n            raise RuntimeError(""No input tensor to time-distribute"")\n\n        # Now get the output back into the right shape.\n        # (batch_size, time_steps, **output_size)\n        new_size = some_input.size()[:2] + reshaped_outputs.size()[1:]\n        outputs = reshaped_outputs.contiguous().view(new_size)\n\n        return outputs\n\n    @staticmethod\n    def _reshape_tensor(input_tensor):\n        input_size = input_tensor.size()\n        if len(input_size) <= 2:\n            raise RuntimeError(f""No dimension to distribute: {input_size}"")\n        # Squash batch_size and time_steps into a single axis; result has shape\n        # (batch_size * time_steps, **input_size).\n        squashed_shape = [-1] + list(input_size[2:])\n        return input_tensor.contiguous().view(*squashed_shape)\n'"
allennlp/nn/__init__.py,0,"b'from allennlp.nn.activations import Activation\nfrom allennlp.nn.initializers import Initializer, InitializerApplicator\nfrom allennlp.nn.regularizers import RegularizerApplicator\n'"
allennlp/nn/activations.py,38,"b'""""""\nAn `Activation` is just a function\nthat takes some parameters and returns an element-wise activation function.\nFor the most part we just use\n[PyTorch activations](https://pytorch.org/docs/master/nn.html#non-linear-activations).\nHere we provide a thin wrapper to allow registering them and instantiating them `from_params`.\n\nThe available activation functions are\n\n* ""linear""\n* [""mish""](https://arxiv.org/abs/1908.08681)\n* [""swish""](https://arxiv.org/abs/1710.05941)\n* [""relu""](https://pytorch.org/docs/master/nn.html#torch.nn.ReLU)\n* [""relu6""](https://pytorch.org/docs/master/nn.html#torch.nn.ReLU6)\n* [""elu""](https://pytorch.org/docs/master/nn.html#torch.nn.ELU)\n* [""prelu""](https://pytorch.org/docs/master/nn.html#torch.nn.PReLU)\n* [""leaky_relu""](https://pytorch.org/docs/master/nn.html#torch.nn.LeakyReLU)\n* [""threshold""](https://pytorch.org/docs/master/nn.html#torch.nn.Threshold)\n* [""hardtanh""](https://pytorch.org/docs/master/nn.html#torch.nn.Hardtanh)\n* [""sigmoid""](https://pytorch.org/docs/master/nn.html#torch.nn.Sigmoid)\n* [""tanh""](https://pytorch.org/docs/master/nn.html#torch.nn.Tanh)\n* [""log_sigmoid""](https://pytorch.org/docs/master/nn.html#torch.nn.LogSigmoid)\n* [""softplus""](https://pytorch.org/docs/master/nn.html#torch.nn.Softplus)\n* [""softshrink""](https://pytorch.org/docs/master/nn.html#torch.nn.Softshrink)\n* [""softsign""](https://pytorch.org/docs/master/nn.html#torch.nn.Softsign)\n* [""tanhshrink""](https://pytorch.org/docs/master/nn.html#torch.nn.Tanhshrink)\n* [""selu""](https://pytorch.org/docs/master/nn.html#torch.nn.SELU)\n""""""\nfrom typing import Callable\n\nimport torch\nfrom overrides import overrides\n\nfrom allennlp.common import Registrable\n\n\nclass Activation(torch.nn.Module, Registrable):\n    """"""\n    Pytorch has a number of built-in activation functions.  We group those here under a common\n    type, just to make it easier to configure and instantiate them `from_params` using\n    `Registrable`.\n\n    Note that we\'re only including element-wise activation functions in this list.  You really need\n    to think about masking when you do a softmax or other similar activation function, so it\n    requires a different API.\n    """"""\n\n    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:\n        """"""\n        This function is here just to make mypy happy.  We expect activation functions to follow\n        this API; the builtin pytorch activation functions follow this just fine, even though they\n        don\'t subclass `Activation`.  We\'re just making it explicit here, so mypy knows that\n        activations are callable like this.\n        """"""\n        raise NotImplementedError\n\n\nclass _ActivationLambda(torch.nn.Module):\n    """"""Wrapper around non PyTorch, lambda based activations to display them as modules whenever printing model.""""""\n\n    def __init__(self, func: Callable[[torch.Tensor], torch.Tensor], name: str):\n        super().__init__()\n        self._name = name\n        self._func = func\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self._func(x)\n\n    @overrides\n    def _get_name(self):\n        return self._name\n\n\n# There are no classes to decorate, so we hack these into Registrable._registry.\n# If you want to instantiate it, you can do like this:\n# Activation.by_name(\'relu\')()\nRegistrable._registry[Activation] = {\n    ""linear"": (lambda: _ActivationLambda(lambda x: x, ""Linear""), None),  # type: ignore\n    ""mish"": (  # type: ignore\n        lambda: _ActivationLambda(\n            lambda x: x * torch.tanh(torch.nn.functional.softplus(x)), ""Mish""\n        ),\n        None,\n    ),\n    ""swish"": (lambda: _ActivationLambda(lambda x: x * torch.sigmoid(x), ""Swish""), None),  # type: ignore\n    ""relu"": (torch.nn.ReLU, None),\n    ""relu6"": (torch.nn.ReLU6, None),\n    ""elu"": (torch.nn.ELU, None),\n    ""prelu"": (torch.nn.PReLU, None),\n    ""leaky_relu"": (torch.nn.LeakyReLU, None),\n    ""threshold"": (torch.nn.Threshold, None),\n    ""hardtanh"": (torch.nn.Hardtanh, None),\n    ""sigmoid"": (torch.nn.Sigmoid, None),\n    ""tanh"": (torch.nn.Tanh, None),\n    ""log_sigmoid"": (torch.nn.LogSigmoid, None),\n    ""softplus"": (torch.nn.Softplus, None),\n    ""softshrink"": (torch.nn.Softshrink, None),\n    ""softsign"": (torch.nn.Softsign, None),\n    ""tanhshrink"": (torch.nn.Tanhshrink, None),\n    ""selu"": (torch.nn.SELU, None),\n}\n'"
allennlp/nn/beam_search.py,14,"b'from typing import List, Callable, Tuple, Dict, cast\nimport warnings\n\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\n\n\nStateType = Dict[str, torch.Tensor]\nStepFunctionType = Callable[[torch.Tensor, StateType, int], Tuple[torch.Tensor, StateType]]\nStepFunctionTypeNoTimestep = Callable[[torch.Tensor, StateType], Tuple[torch.Tensor, StateType]]\n\n\nclass BeamSearch:\n    """"""\n    Implements the beam search algorithm for decoding the most likely sequences.\n\n    [0]: https://arxiv.org/abs/1702.01806\n\n    # Parameters\n\n    end_index : `int`\n        The index of the ""stop"" or ""end"" token in the target vocabulary.\n    max_steps : `int`, optional (default = `50`)\n        The maximum number of decoding steps to take, i.e. the maximum length\n        of the predicted sequences.\n    beam_size : `int`, optional (default = `10`)\n        The width of the beam used.\n    per_node_beam_size : `int`, optional (default = `beam_size`)\n        The maximum number of candidates to consider per node, at each step in the search.\n        If not given, this just defaults to `beam_size`. Setting this parameter\n        to a number smaller than `beam_size` may give better results, as it can introduce\n        more diversity into the search. See [Beam Search Strategies for Neural Machine Translation.\n        Freitag and Al-Onaizan, 2017][0].\n    """"""\n\n    def __init__(\n        self,\n        end_index: int,\n        max_steps: int = 50,\n        beam_size: int = 10,\n        per_node_beam_size: int = None,\n    ) -> None:\n        self._end_index = end_index\n        self.max_steps = max_steps\n        self.beam_size = beam_size\n        self.per_node_beam_size = per_node_beam_size or beam_size\n\n    @staticmethod\n    def reconstruct_sequences(predictions, backpointers):\n        # Reconstruct the sequences.\n        # shape: [(batch_size, beam_size, 1)]\n        reconstructed_predictions = [predictions[-1].unsqueeze(2)]\n\n        # shape: (batch_size, beam_size)\n        cur_backpointers = backpointers[-1]\n\n        for timestep in range(len(predictions) - 2, 0, -1):\n            # shape: (batch_size, beam_size, 1)\n            cur_preds = predictions[timestep].gather(1, cur_backpointers).unsqueeze(2)\n\n            reconstructed_predictions.append(cur_preds)\n\n            # shape: (batch_size, beam_size)\n            cur_backpointers = backpointers[timestep - 1].gather(1, cur_backpointers)\n\n        # shape: (batch_size, beam_size, 1)\n        final_preds = predictions[0].gather(1, cur_backpointers).unsqueeze(2)\n\n        reconstructed_predictions.append(final_preds)\n\n        return reconstructed_predictions\n\n    @torch.no_grad()\n    def search(\n        self, start_predictions: torch.Tensor, start_state: StateType, step: StepFunctionType\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""\n        Given a starting state and a step function, apply beam search to find the\n        most likely target sequences.\n\n        Notes\n        -----\n        If your step function returns `-inf` for some log probabilities\n        (like if you\'re using a masked log-softmax) then some of the ""best""\n        sequences returned may also have `-inf` log probability. Specifically\n        this happens when the beam size is smaller than the number of actions\n        with finite log probability (non-zero probability) returned by the step function.\n        Therefore if you\'re using a mask you may want to check the results from `search`\n        and potentially discard sequences with non-finite log probability.\n\n        # Parameters\n\n        start_predictions : `torch.Tensor`\n            A tensor containing the initial predictions with shape `(batch_size,)`.\n            Usually the initial predictions are just the index of the ""start"" token\n            in the target vocabulary.\n        start_state : `StateType`\n            The initial state passed to the `step` function. Each value of the state dict\n            should be a tensor of shape `(batch_size, *)`, where `*` means any other\n            number of dimensions.\n        step : `StepFunctionType`\n            A function that is responsible for computing the next most likely tokens,\n            given the current state and the predictions from the last time step.\n            The function should accept two arguments. The first being a tensor\n            of shape `(group_size,)`, representing the index of the predicted\n            tokens from the last time step, and the second being the current state.\n            The `group_size` will be `batch_size * beam_size`, except in the initial\n            step, for which it will just be `batch_size`.\n            The function is expected to return a tuple, where the first element\n            is a tensor of shape `(group_size, target_vocab_size)` containing\n            the log probabilities of the tokens for the next step, and the second\n            element is the updated state. The tensor in the state should have shape\n            `(group_size, *)`, where `*` means any other number of dimensions.\n\n        # Returns\n\n        `Tuple[torch.Tensor, torch.Tensor]`\n            Tuple of `(predictions, log_probabilities)`, where `predictions`\n            has shape `(batch_size, beam_size, max_steps)` and `log_probabilities`\n            has shape `(batch_size, beam_size)`.\n        """"""\n\n        # If the step function we\'re given does not take the time step argument, wrap it\n        # in one that does.\n        from inspect import signature\n\n        step_signature = signature(step)\n        if len(step_signature.parameters) < 3:\n            old_step = cast(StepFunctionTypeNoTimestep, step)\n\n            def new_step(\n                last_predictions: torch.Tensor, state: Dict[str, torch.Tensor], time_step: int\n            ):\n                return old_step(last_predictions, state)\n\n            step = new_step\n\n        batch_size = start_predictions.size()[0]\n\n        # List of (batch_size, beam_size) tensors. One for each time step. Does not\n        # include the start symbols, which are implicit.\n        predictions: List[torch.Tensor] = []\n\n        # List of (batch_size, beam_size) tensors. One for each time step. None for\n        # the first.  Stores the index n for the parent prediction, i.e.\n        # predictions[t-1][i][n], that it came from.\n        backpointers: List[torch.Tensor] = []\n\n        # Calculate the first timestep. This is done outside the main loop\n        # because we are going from a single decoder input (the output from the\n        # encoder) to the top `beam_size` decoder outputs. On the other hand,\n        # within the main loop we are going from the `beam_size` elements of the\n        # beam to `beam_size`^2 candidates from which we will select the top\n        # `beam_size` elements for the next iteration.\n        # shape: (batch_size, num_classes)\n        start_class_log_probabilities, state = step(start_predictions, start_state, 0)\n\n        num_classes = start_class_log_probabilities.size()[1]\n\n        # Make sure `per_node_beam_size` is not larger than `num_classes`.\n        if self.per_node_beam_size > num_classes:\n            raise ConfigurationError(\n                f""Target vocab size ({num_classes:d}) too small ""\n                f""relative to per_node_beam_size ({self.per_node_beam_size:d}).\\n""\n                f""Please decrease beam_size or per_node_beam_size.""\n            )\n\n        # shape: (batch_size, beam_size), (batch_size, beam_size)\n        start_top_log_probabilities, start_predicted_classes = start_class_log_probabilities.topk(\n            self.beam_size\n        )\n        if self.beam_size == 1 and (start_predicted_classes == self._end_index).all():\n            warnings.warn(\n                ""Empty sequences predicted. You may want to increase the beam size or ensure ""\n                ""your step function is working properly."",\n                RuntimeWarning,\n            )\n            return start_predicted_classes.unsqueeze(-1), start_top_log_probabilities\n\n        # The log probabilities for the last time step.\n        # shape: (batch_size, beam_size)\n        last_log_probabilities = start_top_log_probabilities\n\n        # shape: [(batch_size, beam_size)]\n        predictions.append(start_predicted_classes)\n\n        # Log probability tensor that mandates that the end token is selected.\n        # shape: (batch_size * beam_size, num_classes)\n        log_probs_after_end = start_class_log_probabilities.new_full(\n            (batch_size * self.beam_size, num_classes), float(""-inf"")\n        )\n        log_probs_after_end[:, self._end_index] = 0.0\n\n        # Set the same state for each element in the beam.\n        for key, state_tensor in state.items():\n            if state_tensor is None:\n                continue\n            _, *last_dims = state_tensor.size()\n            # shape: (batch_size * beam_size, *)\n            state[key] = (\n                state_tensor.unsqueeze(1)\n                .expand(batch_size, self.beam_size, *last_dims)\n                .reshape(batch_size * self.beam_size, *last_dims)\n            )\n\n        for timestep in range(self.max_steps - 1):\n            # shape: (batch_size * beam_size,)\n            last_predictions = predictions[-1].reshape(batch_size * self.beam_size)\n\n            # If every predicted token from the last step is `self._end_index`,\n            # then we can stop early.\n            if (last_predictions == self._end_index).all():\n                break\n\n            # Take a step. This get the predicted log probs of the next classes\n            # and updates the state.\n            # shape: (batch_size * beam_size, num_classes)\n            class_log_probabilities, state = step(last_predictions, state, timestep + 1)\n\n            # shape: (batch_size * beam_size, num_classes)\n            last_predictions_expanded = last_predictions.unsqueeze(-1).expand(\n                batch_size * self.beam_size, num_classes\n            )\n\n            # Here we are finding any beams where we predicted the end token in\n            # the previous timestep and replacing the distribution with a\n            # one-hot distribution, forcing the beam to predict the end token\n            # this timestep as well.\n            # shape: (batch_size * beam_size, num_classes)\n            cleaned_log_probabilities = torch.where(\n                last_predictions_expanded == self._end_index,\n                log_probs_after_end,\n                class_log_probabilities,\n            )\n\n            # shape (both): (batch_size * beam_size, per_node_beam_size)\n            top_log_probabilities, predicted_classes = cleaned_log_probabilities.topk(\n                self.per_node_beam_size\n            )\n\n            # Here we expand the last log probabilities to (batch_size * beam_size, per_node_beam_size)\n            # so that we can add them to the current log probs for this timestep.\n            # This lets us maintain the log probability of each element on the beam.\n            # shape: (batch_size * beam_size, per_node_beam_size)\n            expanded_last_log_probabilities = (\n                last_log_probabilities.unsqueeze(2)\n                .expand(batch_size, self.beam_size, self.per_node_beam_size)\n                .reshape(batch_size * self.beam_size, self.per_node_beam_size)\n            )\n\n            # shape: (batch_size * beam_size, per_node_beam_size)\n            summed_top_log_probabilities = top_log_probabilities + expanded_last_log_probabilities\n\n            # shape: (batch_size, beam_size * per_node_beam_size)\n            reshaped_summed = summed_top_log_probabilities.reshape(\n                batch_size, self.beam_size * self.per_node_beam_size\n            )\n\n            # shape: (batch_size, beam_size * per_node_beam_size)\n            reshaped_predicted_classes = predicted_classes.reshape(\n                batch_size, self.beam_size * self.per_node_beam_size\n            )\n\n            # Keep only the top `beam_size` beam indices.\n            # shape: (batch_size, beam_size), (batch_size, beam_size)\n            restricted_beam_log_probs, restricted_beam_indices = reshaped_summed.topk(\n                self.beam_size\n            )\n\n            # Use the beam indices to extract the corresponding classes.\n            # shape: (batch_size, beam_size)\n            restricted_predicted_classes = reshaped_predicted_classes.gather(\n                1, restricted_beam_indices\n            )\n\n            predictions.append(restricted_predicted_classes)\n\n            # shape: (batch_size, beam_size)\n            last_log_probabilities = restricted_beam_log_probs\n\n            # The beam indices come from a `beam_size * per_node_beam_size` dimension where the\n            # indices with a common ancestor are grouped together. Hence\n            # dividing by per_node_beam_size gives the ancestor. (Note that this is integer\n            # division as the tensor is a LongTensor.)\n            # shape: (batch_size, beam_size)\n            backpointer = restricted_beam_indices / self.per_node_beam_size\n\n            backpointers.append(backpointer)\n\n            # Keep only the pieces of the state tensors corresponding to the\n            # ancestors created this iteration.\n            for key, state_tensor in state.items():\n                if state_tensor is None:\n                    continue\n                _, *last_dims = state_tensor.size()\n                # shape: (batch_size, beam_size, *)\n                expanded_backpointer = backpointer.view(\n                    batch_size, self.beam_size, *([1] * len(last_dims))\n                ).expand(batch_size, self.beam_size, *last_dims)\n\n                # shape: (batch_size * beam_size, *)\n                state[key] = (\n                    state_tensor.reshape(batch_size, self.beam_size, *last_dims)\n                    .gather(1, expanded_backpointer)\n                    .reshape(batch_size * self.beam_size, *last_dims)\n                )\n\n        if not torch.isfinite(last_log_probabilities).all():\n            warnings.warn(\n                ""Infinite log probabilities encountered. Some final sequences may not make sense. ""\n                ""This can happen when the beam size is larger than the number of valid (non-zero ""\n                ""probability) transitions that the step function produces."",\n                RuntimeWarning,\n            )\n\n        reconstructed_predictions = self.reconstruct_sequences(predictions, backpointers)\n\n        # shape: (batch_size, beam_size, max_steps)\n        all_predictions = torch.cat(list(reversed(reconstructed_predictions)), 2)\n\n        return all_predictions, last_log_probabilities\n'"
allennlp/nn/chu_liu_edmonds.py,0,"b'from typing import List, Set, Tuple, Dict\nimport numpy\n\nfrom allennlp.common.checks import ConfigurationError\n\n\ndef decode_mst(\n    energy: numpy.ndarray, length: int, has_labels: bool = True\n) -> Tuple[numpy.ndarray, numpy.ndarray]:\n    """"""\n    Note: Counter to typical intuition, this function decodes the _maximum_\n    spanning tree.\n\n    Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for\n    maximum spanning arborescences on graphs.\n\n    # Parameters\n\n    energy : `numpy.ndarray`, required.\n        A tensor with shape (num_labels, timesteps, timesteps)\n        containing the energy of each edge. If has_labels is `False`,\n        the tensor should have shape (timesteps, timesteps) instead.\n    length : `int`, required.\n        The length of this sequence, as the energy may have come\n        from a padded batch.\n    has_labels : `bool`, optional, (default = `True`)\n        Whether the graph has labels or not.\n    """"""\n    if has_labels and energy.ndim != 3:\n        raise ConfigurationError(""The dimension of the energy array is not equal to 3."")\n    elif not has_labels and energy.ndim != 2:\n        raise ConfigurationError(""The dimension of the energy array is not equal to 2."")\n    input_shape = energy.shape\n    max_length = input_shape[-1]\n\n    # Our energy matrix might have been batched -\n    # here we clip it to contain only non padded tokens.\n    if has_labels:\n        energy = energy[:, :length, :length]\n        # get best label for each edge.\n        label_id_matrix = energy.argmax(axis=0)\n        energy = energy.max(axis=0)\n    else:\n        energy = energy[:length, :length]\n        label_id_matrix = None\n    # get original score matrix\n    original_score_matrix = energy\n    # initialize score matrix to original score matrix\n    score_matrix = numpy.array(original_score_matrix, copy=True)\n\n    old_input = numpy.zeros([length, length], dtype=numpy.int32)\n    old_output = numpy.zeros([length, length], dtype=numpy.int32)\n    current_nodes = [True for _ in range(length)]\n    representatives: List[Set[int]] = []\n\n    for node1 in range(length):\n        original_score_matrix[node1, node1] = 0.0\n        score_matrix[node1, node1] = 0.0\n        representatives.append({node1})\n\n        for node2 in range(node1 + 1, length):\n            old_input[node1, node2] = node1\n            old_output[node1, node2] = node2\n\n            old_input[node2, node1] = node2\n            old_output[node2, node1] = node1\n\n    final_edges: Dict[int, int] = {}\n\n    # The main algorithm operates inplace.\n    chu_liu_edmonds(\n        length, score_matrix, current_nodes, final_edges, old_input, old_output, representatives\n    )\n\n    heads = numpy.zeros([max_length], numpy.int32)\n    if has_labels:\n        head_type = numpy.ones([max_length], numpy.int32)\n    else:\n        head_type = None\n\n    for child, parent in final_edges.items():\n        heads[child] = parent\n        if has_labels:\n            head_type[child] = label_id_matrix[parent, child]\n\n    return heads, head_type\n\n\ndef chu_liu_edmonds(\n    length: int,\n    score_matrix: numpy.ndarray,\n    current_nodes: List[bool],\n    final_edges: Dict[int, int],\n    old_input: numpy.ndarray,\n    old_output: numpy.ndarray,\n    representatives: List[Set[int]],\n):\n    """"""\n    Applies the chu-liu-edmonds algorithm recursively\n    to a graph with edge weights defined by score_matrix.\n\n    Note that this function operates in place, so variables\n    will be modified.\n\n    # Parameters\n\n    length : `int`, required.\n        The number of nodes.\n    score_matrix : `numpy.ndarray`, required.\n        The score matrix representing the scores for pairs\n        of nodes.\n    current_nodes : `List[bool]`, required.\n        The nodes which are representatives in the graph.\n        A representative at it\'s most basic represents a node,\n        but as the algorithm progresses, individual nodes will\n        represent collapsed cycles in the graph.\n    final_edges : `Dict[int, int]`, required.\n        An empty dictionary which will be populated with the\n        nodes which are connected in the maximum spanning tree.\n    old_input : `numpy.ndarray`, required.\n    old_output : `numpy.ndarray`, required.\n    representatives : `List[Set[int]]`, required.\n        A list containing the nodes that a particular node\n        is representing at this iteration in the graph.\n\n    # Returns\n\n    Nothing - all variables are modified in place.\n\n    """"""\n    # Set the initial graph to be the greedy best one.\n    parents = [-1]\n    for node1 in range(1, length):\n        parents.append(0)\n        if current_nodes[node1]:\n            max_score = score_matrix[0, node1]\n            for node2 in range(1, length):\n                if node2 == node1 or not current_nodes[node2]:\n                    continue\n\n                new_score = score_matrix[node2, node1]\n                if new_score > max_score:\n                    max_score = new_score\n                    parents[node1] = node2\n\n    # Check if this solution has a cycle.\n    has_cycle, cycle = _find_cycle(parents, length, current_nodes)\n    # If there are no cycles, find all edges and return.\n    if not has_cycle:\n        final_edges[0] = -1\n        for node in range(1, length):\n            if not current_nodes[node]:\n                continue\n\n            parent = old_input[parents[node], node]\n            child = old_output[parents[node], node]\n            final_edges[child] = parent\n        return\n\n    # Otherwise, we have a cycle so we need to remove an edge.\n    # From here until the recursive call is the contraction stage of the algorithm.\n    cycle_weight = 0.0\n    # Find the weight of the cycle.\n    index = 0\n    for node in cycle:\n        index += 1\n        cycle_weight += score_matrix[parents[node], node]\n\n    # For each node in the graph, find the maximum weight incoming\n    # and outgoing edge into the cycle.\n    cycle_representative = cycle[0]\n    for node in range(length):\n        if not current_nodes[node] or node in cycle:\n            continue\n\n        in_edge_weight = float(""-inf"")\n        in_edge = -1\n        out_edge_weight = float(""-inf"")\n        out_edge = -1\n\n        for node_in_cycle in cycle:\n            if score_matrix[node_in_cycle, node] > in_edge_weight:\n                in_edge_weight = score_matrix[node_in_cycle, node]\n                in_edge = node_in_cycle\n\n            # Add the new edge score to the cycle weight\n            # and subtract the edge we\'re considering removing.\n            score = (\n                cycle_weight\n                + score_matrix[node, node_in_cycle]\n                - score_matrix[parents[node_in_cycle], node_in_cycle]\n            )\n\n            if score > out_edge_weight:\n                out_edge_weight = score\n                out_edge = node_in_cycle\n\n        score_matrix[cycle_representative, node] = in_edge_weight\n        old_input[cycle_representative, node] = old_input[in_edge, node]\n        old_output[cycle_representative, node] = old_output[in_edge, node]\n\n        score_matrix[node, cycle_representative] = out_edge_weight\n        old_output[node, cycle_representative] = old_output[node, out_edge]\n        old_input[node, cycle_representative] = old_input[node, out_edge]\n\n    # For the next recursive iteration, we want to consider the cycle as a\n    # single node. Here we collapse the cycle into the first node in the\n    # cycle (first node is arbitrary), set all the other nodes not be\n    # considered in the next iteration. We also keep track of which\n    # representatives we are considering this iteration because we need\n    # them below to check if we\'re done.\n    considered_representatives: List[Set[int]] = []\n    for i, node_in_cycle in enumerate(cycle):\n        considered_representatives.append(set())\n        if i > 0:\n            # We need to consider at least one\n            # node in the cycle, arbitrarily choose\n            # the first.\n            current_nodes[node_in_cycle] = False\n\n        for node in representatives[node_in_cycle]:\n            considered_representatives[i].add(node)\n            if i > 0:\n                representatives[cycle_representative].add(node)\n\n    chu_liu_edmonds(\n        length, score_matrix, current_nodes, final_edges, old_input, old_output, representatives\n    )\n\n    # Expansion stage.\n    # check each node in cycle, if one of its representatives\n    # is a key in the final_edges, it is the one we need.\n    found = False\n    key_node = -1\n    for i, node in enumerate(cycle):\n        for cycle_rep in considered_representatives[i]:\n            if cycle_rep in final_edges:\n                key_node = node\n                found = True\n                break\n        if found:\n            break\n\n    previous = parents[key_node]\n    while previous != key_node:\n        child = old_output[parents[previous], previous]\n        parent = old_input[parents[previous], previous]\n        final_edges[child] = parent\n        previous = parents[previous]\n\n\ndef _find_cycle(\n    parents: List[int], length: int, current_nodes: List[bool]\n) -> Tuple[bool, List[int]]:\n\n    added = [False for _ in range(length)]\n    added[0] = True\n    cycle = set()\n    has_cycle = False\n    for i in range(1, length):\n        if has_cycle:\n            break\n        # don\'t redo nodes we\'ve already\n        # visited or aren\'t considering.\n        if added[i] or not current_nodes[i]:\n            continue\n        # Initialize a new possible cycle.\n        this_cycle = set()\n        this_cycle.add(i)\n        added[i] = True\n        has_cycle = True\n        next_node = i\n        while parents[next_node] not in this_cycle:\n            next_node = parents[next_node]\n            # If we see a node we\'ve already processed,\n            # we can stop, because the node we are\n            # processing would have been in that cycle.\n            if added[next_node]:\n                has_cycle = False\n                break\n            added[next_node] = True\n            this_cycle.add(next_node)\n\n        if has_cycle:\n            original = next_node\n            cycle.add(original)\n            next_node = parents[original]\n            while next_node != original:\n                cycle.add(next_node)\n                next_node = parents[next_node]\n            break\n\n    return has_cycle, list(cycle)\n'"
allennlp/nn/initializers.py,39,"b'""""""\nAn initializer is just a PyTorch function.\nHere we implement a proxy class that allows us\nto register them and supply any additional function arguments\n(for example, the `mean` and `std` of a normal initializer)\nas named arguments to the constructor.\n\nThe available initialization functions are\n\n* [""normal""](https://pytorch.org/docs/master/nn.html?highlight=orthogonal#torch.nn.init.normal_)\n* [""uniform""](https://pytorch.org/docs/master/nn.html?highlight=orthogonal#torch.nn.init.uniform_)\n* [""constant""](https://pytorch.org/docs/master/nn.html?highlight=orthogonal#torch.nn.init.constant_)\n* [""eye""](https://pytorch.org/docs/master/nn.html?highlight=orthogonal#torch.nn.init.eye_)\n* [""dirac""](https://pytorch.org/docs/master/nn.html?highlight=orthogonal#torch.nn.init.dirac_)\n* [""xavier_uniform""](https://pytorch.org/docs/master/nn.html?highlight=orthogonal#torch.nn.init.xavier_uniform_)\n* [""xavier_normal""](https://pytorch.org/docs/master/nn.html?highlight=orthogonal#torch.nn.init.xavier_normal_)\n* [""kaiming_uniform""](https://pytorch.org/docs/master/nn.html?highlight=orthogonal#torch.nn.init.kaiming_uniform_)\n* [""kaiming_normal""](https://pytorch.org/docs/master/nn.html?highlight=orthogonal#torch.nn.init.kaiming_normal_)\n* [""orthogonal""](https://pytorch.org/docs/master/nn.html?highlight=orthogonal#torch.nn.init.orthogonal_)\n* [""sparse""](https://pytorch.org/docs/master/nn.html?highlight=orthogonal#torch.nn.init.sparse_)\n* [""block_orthogonal""](./initializers.md#block_orthogonal)\n* [""uniform_unit_scaling""](./initializers.md#uniform_unit_scaling)\n* [""pretrained""](./initializers.md#PretrainedModelInitializer)\n""""""\nimport logging\nimport re\nimport math\nfrom typing import Callable, List, Tuple, Dict\nimport itertools\nfrom overrides import overrides\n\nimport torch\nimport torch.nn.init\n\nfrom allennlp.common import FromParams, Registrable\nfrom allennlp.common.checks import ConfigurationError\n\nlogger = logging.getLogger(__name__)\n\n\nclass Initializer(Registrable):\n    """"""\n    An initializer is really just a bare pytorch function. This class\n    is a proxy that allows us to implement `Registrable` for those functions.\n    """"""\n\n    default_implementation = ""normal""\n\n    def __call__(self, tensor: torch.Tensor, **kwargs) -> None:\n        """"""\n        This function is here just to make mypy happy.  We expect initialization functions to\n        follow this API; the builtin pytorch initialization functions follow this just fine, even\n        though they don\'t subclass `Initialization`.  We\'re just making it explicit here, so mypy\n        knows that initializers are callable like this.\n        """"""\n        raise NotImplementedError\n\n\ndef uniform_unit_scaling(tensor: torch.Tensor, nonlinearity: str = ""linear""):\n    """"""\n    An initaliser which preserves output variance for approximately gaussian\n    distributed inputs. This boils down to initialising layers using a uniform\n    distribution in the range `(-sqrt(3/dim[0]) * scale, sqrt(3 / dim[0]) * scale)`, where\n    `dim[0]` is equal to the input dimension of the parameter and the `scale`\n    is a constant scaling factor which depends on the non-linearity used.\n\n    See `Random Walk Initialisation for Training Very Deep Feedforward Networks\n    <https://www.semanticscholar.org/paper/Random-Walk-Initialization-for-Training-Very-Deep-Sussillo-Abbott/be9728a0728b6acf7a485225b1e41592176eda0b>`_\n    for more information.\n\n    # Parameters\n\n    tensor : `torch.Tensor`, required.\n        The tensor to initialise.\n    nonlinearity : `str`, optional (default = `""linear""`)\n        The non-linearity which is performed after the projection that this\n        tensor is involved in. This must be the name of a function contained\n        in the `torch.nn.functional` package.\n\n    # Returns\n\n    The initialised tensor.\n    """"""\n    size = 1.0\n    # Estimate the input size. This won\'t work perfectly,\n    # but it covers almost all use cases where this initialiser\n    # would be expected to be useful, i.e in large linear and\n    # convolutional layers, as the last dimension will almost\n    # always be the output size.\n    for dimension in list(tensor.size())[:-1]:\n        size *= dimension\n\n    activation_scaling = torch.nn.init.calculate_gain(nonlinearity, tensor)\n    max_value = math.sqrt(3 / size) * activation_scaling\n\n    return tensor.data.uniform_(-max_value, max_value)\n\n\ndef block_orthogonal(tensor: torch.Tensor, split_sizes: List[int], gain: float = 1.0) -> None:\n    """"""\n    An initializer which allows initializing model parameters in ""blocks"". This is helpful\n    in the case of recurrent models which use multiple gates applied to linear projections,\n    which can be computed efficiently if they are concatenated together. However, they are\n    separate parameters which should be initialized independently.\n\n    # Parameters\n\n    tensor : `torch.Tensor`, required.\n        A tensor to initialize.\n    split_sizes : `List[int]`, required.\n        A list of length `tensor.ndim()` specifying the size of the\n        blocks along that particular dimension. E.g. `[10, 20]` would\n        result in the tensor being split into chunks of size 10 along the\n        first dimension and 20 along the second.\n    gain : `float`, optional (default = `1.0`)\n        The gain (scaling) applied to the orthogonal initialization.\n    """"""\n    data = tensor.data\n    sizes = list(tensor.size())\n    if any(a % b != 0 for a, b in zip(sizes, split_sizes)):\n        raise ConfigurationError(\n            ""tensor dimensions must be divisible by their respective ""\n            ""split_sizes. Found size: {} and split_sizes: {}"".format(sizes, split_sizes)\n        )\n    indexes = [list(range(0, max_size, split)) for max_size, split in zip(sizes, split_sizes)]\n    # Iterate over all possible blocks within the tensor.\n    for block_start_indices in itertools.product(*indexes):\n        # A list of tuples containing the index to start at for this block\n        # and the appropriate step size (i.e split_size[i] for dimension i).\n        index_and_step_tuples = zip(block_start_indices, split_sizes)\n        # This is a tuple of slices corresponding to:\n        # tensor[index: index + step_size, ...]. This is\n        # required because we could have an arbitrary number\n        # of dimensions. The actual slices we need are the\n        # start_index: start_index + step for each dimension in the tensor.\n        block_slice = tuple(\n            slice(start_index, start_index + step) for start_index, step in index_and_step_tuples\n        )\n        data[block_slice] = torch.nn.init.orthogonal_(tensor[block_slice].contiguous(), gain=gain)\n\n\ndef zero(tensor: torch.Tensor) -> None:\n    return tensor.data.zero_()\n\n\ndef lstm_hidden_bias(tensor: torch.Tensor) -> None:\n    """"""\n    Initialize the biases of the forget gate to 1, and all other gates to 0,\n    following Jozefowicz et al., An Empirical Exploration of Recurrent Network Architectures\n    """"""\n    # gates are (b_hi|b_hf|b_hg|b_ho) of shape (4*hidden_size)\n    tensor.data.zero_()\n    hidden_size = tensor.shape[0] // 4\n    tensor.data[hidden_size : (2 * hidden_size)] = 1.0\n\n\nclass _InitializerWrapper(Initializer):\n    def __init__(self, init_function: Callable[..., None], **kwargs):\n        self._init_function = init_function\n        self._kwargs = kwargs\n\n    def __call__(self, tensor: torch.Tensor, **kwargs) -> None:\n        self._init_function(tensor, **self._kwargs)\n\n    def __repr__(self):\n        return ""Init: %s, with params: %s"" % (self._init_function, self._kwargs)\n\n\n@Initializer.register(""normal"")\nclass NormalInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""normal"".\n    """"""\n\n    def __init__(self, mean: float = 0.0, std: float = 0.1):\n        super().__init__(init_function=torch.nn.init.normal_, mean=mean, std=std)\n\n\n@Initializer.register(""orthogonal"")\nclass OrthogonalInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""orthogonal"".\n    """"""\n\n    def __init__(self, gain: float = 1.0):\n        super().__init__(init_function=torch.nn.init.orthogonal_, gain=gain)\n\n\n@Initializer.register(""uniform"")\nclass UniformInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""uniform"".\n    """"""\n\n    def __init__(self, a: float = 0.0, b: float = 1.0):\n        super().__init__(init_function=torch.nn.init.uniform_, a=a, b=b)\n\n\n@Initializer.register(""constant"")\nclass ConstantInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""constant"".\n    """"""\n\n    def __init__(self, val: float):\n        super().__init__(init_function=torch.nn.init.constant_, val=val)\n\n\n@Initializer.register(""dirac"")\nclass DiracInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""dirac"".\n    """"""\n\n    def __init__(self):\n        super().__init__(init_function=torch.nn.init.dirac_)\n\n\n@Initializer.register(""xavier_uniform"")\nclass XavierUniformInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""xavir_uniform"".\n    """"""\n\n    def __init__(self, gain: float = 1.0):\n        super().__init__(init_function=torch.nn.init.xavier_uniform_, gain=gain)\n\n\n@Initializer.register(""xavier_normal"")\nclass XavierNormalInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""xavier_normal"".\n    """"""\n\n    def __init__(self, gain: float = 1.0):\n        super().__init__(init_function=torch.nn.init.xavier_normal_, gain=gain)\n\n\n@Initializer.register(""kaiming_uniform"")\nclass KaimingUniformInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""kaiming_uniform"".\n    """"""\n\n    def __init__(self, a: float = 0.0, mode: str = ""fan_in"", nonlinearity: str = ""leaky_relu""):\n        super().__init__(\n            init_function=torch.nn.init.kaiming_uniform_, a=a, mode=mode, nonlinearity=nonlinearity\n        )\n\n\n@Initializer.register(""kaiming_normal"")\nclass KaimingNormalInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""kaiming_normal"".\n    """"""\n\n    def __init__(self, a: float = 0.0, mode: str = ""fan_in"", nonlinearity: str = ""leaky_relu""):\n        super().__init__(\n            init_function=torch.nn.init.kaiming_normal_, a=a, mode=mode, nonlinearity=nonlinearity\n        )\n\n\n@Initializer.register(""sparse"")\nclass SparseInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""sparse"".\n    """"""\n\n    def __init__(self, sparsity: float, std: float = 0.01):\n        super().__init__(init_function=torch.nn.init.sparse_, sparsity=sparsity, std=std)\n\n\n@Initializer.register(""eye"")\nclass EyeInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""eye"".\n    """"""\n\n    def __init__(self):\n        super().__init__(init_function=torch.nn.init.eye_)\n\n\n@Initializer.register(""block_orthogonal"")\nclass BlockOrthogonalInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""block_orthogonal"".\n    """"""\n\n    def __init__(self, split_sizes: List[int], gain: float = 1.0):\n        super().__init__(init_function=block_orthogonal, split_sizes=split_sizes, gain=gain)\n\n\n@Initializer.register(""uniform_unit_scaling"")\nclass UniformUnitScalingInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""uniform_unit_scaling"".\n    """"""\n\n    def __init__(self, nonlinearity: str = ""linear""):\n        super().__init__(init_function=uniform_unit_scaling, nonlinearity=nonlinearity)\n\n\n@Initializer.register(""zero"")\nclass ZeroInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""zero"".\n    """"""\n\n    def __init__(self):\n        super().__init__(init_function=zero)\n\n\n@Initializer.register(""lstm_hidden_bias"")\nclass LstmHiddenBiasInitializer(_InitializerWrapper):\n    """"""\n    Registered as an `Initializer` with name ""lstm_hidden_bias"".\n    """"""\n\n    def __init__(self):\n        super().__init__(init_function=lstm_hidden_bias)\n\n\n@Initializer.register(""pretrained"")\nclass PretrainedModelInitializer(Initializer):\n    """"""\n    An initializer which allows initializing parameters using a pretrained model. The\n    initializer will load all of the weights from the `weights_file_path` and use the\n    name of the new parameters to index into the pretrained parameters. Therefore,\n    by default, the names of the new and pretrained parameters must be the same.\n    However, this behavior can be overridden using the `parameter_name_overrides`,\n    which remaps the name of the new parameter to the key which should be used\n    to index into the pretrained parameters.\n\n    The initializer will load all of the weights from the `weights_file_path`\n    regardless of which parameters will actually be used to initialize the new model.\n    So, if you need to initialize several parameters using a pretrained model, the most\n    memory-efficient way to do this is to use one `PretrainedModelInitializer` per\n    weights file and use a regex to match all of the new parameters which need to be\n    initialized.\n\n    If you are using a configuration file to instantiate this object, the below entry\n    in the `InitializerApplicator` parameters will initialize `linear_1.weight` and\n    `linear_2.weight` using a pretrained model.  `linear_1.weight` will be initialized\n    to the pretrained parameters called `linear_1.weight`, but `linear_2.weight` will\n    be initialized to the pretrained parameters called `linear_3.weight`::\n\n    ```\n       [""linear_1.weight|linear_2.weight"",\n           {\n               ""type"": ""pretrained"",\n               ""weights_file_path"": ""best.th"",\n               ""parameter_name_overrides"": {\n                   ""linear_2.weight"": ""linear_3.weight""\n               }\n           }\n       ]\n    ```\n\n    To initialize weights for all the parameters from a pretrained model (assuming their names\n    remain unchanged), use the following instead:\n\n    ```\n            ["".*"",\n                {\n                    ""type"": ""pretrained"",\n                    ""weights_file_path"": ""best.th"",\n                    ""parameter_name_overrides"": {}\n                }\n            ]\n    ```\n\n    Registered as an `Initializer` with name ""pretrained"".\n\n    # Parameters\n\n    weights_file_path : `str`, required\n        The path to the weights file which has the pretrained model parameters.\n    parameter_name_overrides : `Dict[str, str]`, optional (default = `None`)\n        The mapping from the new parameter name to the name which should be used\n        to index into the pretrained model parameters. If a parameter name is not\n        specified, the initializer will use the parameter\'s default name as the key.\n    """"""\n\n    def __init__(\n        self, weights_file_path: str, parameter_name_overrides: Dict[str, str] = None\n    ) -> None:\n        self.weights: Dict[str, torch.Tensor] = torch.load(weights_file_path)\n        self.parameter_name_overrides = parameter_name_overrides or {}\n\n    @overrides\n    def __call__(self, tensor: torch.Tensor, parameter_name: str, **kwargs) -> None:  # type: ignore\n        # Select the new parameter name if it\'s being overridden\n        if parameter_name in self.parameter_name_overrides:\n            parameter_name = self.parameter_name_overrides[parameter_name]\n\n        # If the size of the source and destination tensors are not the\n        # same, then we need to raise an error\n        source_weights = self.weights[parameter_name]\n        if tensor.data.size() != source_weights.size():\n            raise ConfigurationError(\n                ""Incompatible sizes found for parameter %s. ""\n                ""Found %s and %s"" % (parameter_name, tensor.data.size(), source_weights.size())\n            )\n\n        # Copy the parameters from the source to the destination\n        tensor.data[:] = source_weights[:]\n\n\nclass InitializerApplicator(FromParams):\n    """"""\n    Applies initializers to the parameters of a Module based on regex matches.  Any parameter not\n    explicitly matching a regex will not be initialized, instead using whatever the default\n    initialization was in the module\'s code.\n\n    If you are instantiating this object from a config file, an example configuration is as\n    follows:\n\n    ```json\n    {\n        ""regexes"": [\n            [""parameter_regex_match1"",\n                {\n                    ""type"": ""normal""\n                    ""mean"": 0.01\n                    ""std"": 0.1\n                }\n            ],\n            [""parameter_regex_match2"", ""uniform""]\n        ],\n        ""prevent_regexes"": [""prevent_init_regex""]\n    }\n    ```\n\n    where the first item in each tuple under the `regexes` parameters is the regex that matches to\n    parameters, and the second item specifies an `Initializer.` These values can either be strings,\n    in which case they correspond to the names of initializers, or dictionaries, in which case they\n    must contain the ""type"" key, corresponding to the name of an initializer.  In addition, they may\n    contain auxiliary named parameters which will be fed to the initializer itself. To determine\n    valid auxiliary parameters, please refer to the torch.nn.init documentation.\n\n    # Parameters\n\n    regexes : `List[Tuple[str, Initializer]]`, optional (default = `[]`)\n        A list mapping parameter regexes to initializers.  We will check each parameter against\n        each regex in turn, and apply the initializer paired with the first matching regex, if\n        any.\n\n    prevent_regexes: `List[str]`, optional (default=`None`)\n        Any parameter name matching one of these regexes will not be initialized, regardless of\n        whether it matches one of the regexes passed in the `regexes` parameter.\n    """"""\n\n    def __init__(\n        self, regexes: List[Tuple[str, Initializer]] = None, prevent_regexes: List[str] = None\n    ) -> None:\n        self._initializers = regexes or []\n        self._prevent_regex = None\n        if prevent_regexes:\n            self._prevent_regex = ""("" + "")|("".join(prevent_regexes) + "")""\n\n    def __call__(self, module: torch.nn.Module) -> None:\n        """"""\n        Applies an initializer to all parameters in a module that match one of the regexes we were\n        given in this object\'s constructor.  Does nothing to parameters that do not match.\n\n        # Parameters\n\n        module : `torch.nn.Module`, required.\n            The Pytorch module to apply the initializers to.\n        """"""\n        logger.info(""Initializing parameters"")\n        unused_regexes = {initializer[0] for initializer in self._initializers}\n        uninitialized_parameters = set()\n        # Store which initializers were applied to which parameters.\n        for name, parameter in module.named_parameters():\n            for initializer_regex, initializer in self._initializers:\n                allow = self._prevent_regex is None or not bool(\n                    re.search(self._prevent_regex, name)\n                )\n                if allow and re.search(initializer_regex, name):\n                    logger.info(""Initializing %s using %s initializer"", name, initializer_regex)\n                    initializer(parameter, parameter_name=name)\n                    unused_regexes.discard(initializer_regex)\n                    break\n            else:  # no break\n                uninitialized_parameters.add(name)\n        for regex in unused_regexes:\n            logger.warning(""Did not use initialization regex that was passed: %s"", regex)\n        logger.info(\n            ""Done initializing parameters; the following parameters are using their ""\n            ""default initialization from their code""\n        )\n        uninitialized_parameter_list = list(uninitialized_parameters)\n        uninitialized_parameter_list.sort()\n        for name in uninitialized_parameter_list:\n            logger.info(""   %s"", name)\n'"
allennlp/nn/util.py,241,"b'""""""\nAssorted utilities for working with neural networks in AllenNLP.\n""""""\n\nimport copy\nimport json\nimport logging\nfrom collections import defaultdict\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, TypeVar, Union\n\nimport math\nimport numpy\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(""T"")\n\n\ndef has_tensor(obj) -> bool:\n    """"""\n    Given a possibly complex data structure,\n    check if it has any torch.Tensors in it.\n    """"""\n    if isinstance(obj, torch.Tensor):\n        return True\n    elif isinstance(obj, dict):\n        return any(has_tensor(value) for value in obj.values())\n    elif isinstance(obj, (list, tuple)):\n        return any(has_tensor(item) for item in obj)\n    else:\n        return False\n\n\ndef move_to_device(obj, cuda_device: Union[torch.device, int]):\n    """"""\n    Given a structure (possibly) containing Tensors on the CPU,\n    move all the Tensors to the specified GPU (or do nothing, if they should be on the CPU).\n    """"""\n    from allennlp.common.util import int_to_device\n\n    cuda_device = int_to_device(cuda_device)\n\n    if cuda_device == torch.device(""cpu"") or not has_tensor(obj):\n        return obj\n    elif isinstance(obj, torch.Tensor):\n        return obj.cuda(cuda_device)\n    elif isinstance(obj, dict):\n        return {key: move_to_device(value, cuda_device) for key, value in obj.items()}\n    elif isinstance(obj, list):\n        return [move_to_device(item, cuda_device) for item in obj]\n    elif isinstance(obj, tuple) and hasattr(obj, ""_fields""):\n        # This is the best way to detect a NamedTuple, it turns out.\n        return obj.__class__(*(move_to_device(item, cuda_device) for item in obj))\n    elif isinstance(obj, tuple):\n        return tuple(move_to_device(item, cuda_device) for item in obj)\n    else:\n        return obj\n\n\ndef clamp_tensor(tensor, minimum, maximum):\n    """"""\n    Supports sparse and dense tensors.\n    Returns a tensor with values clamped between the provided minimum and maximum,\n    without modifying the original tensor.\n    """"""\n    if tensor.is_sparse:\n        coalesced_tensor = tensor.coalesce()\n\n        coalesced_tensor._values().clamp_(minimum, maximum)\n        return coalesced_tensor\n    else:\n        return tensor.clamp(minimum, maximum)\n\n\ndef batch_tensor_dicts(\n    tensor_dicts: List[Dict[str, torch.Tensor]], remove_trailing_dimension: bool = False\n) -> Dict[str, torch.Tensor]:\n    """"""\n    Takes a list of tensor dictionaries, where each dictionary is assumed to have matching keys,\n    and returns a single dictionary with all tensors with the same key batched together.\n\n    # Parameters\n\n    tensor_dicts : `List[Dict[str, torch.Tensor]]`\n        The list of tensor dictionaries to batch.\n    remove_trailing_dimension : `bool`\n        If `True`, we will check for a trailing dimension of size 1 on the tensors that are being\n        batched, and remove it if we find it.\n    """"""\n    key_to_tensors: Dict[str, List[torch.Tensor]] = defaultdict(list)\n    for tensor_dict in tensor_dicts:\n        for key, tensor in tensor_dict.items():\n            key_to_tensors[key].append(tensor)\n    batched_tensors = {}\n    for key, tensor_list in key_to_tensors.items():\n        batched_tensor = torch.stack(tensor_list)\n        if remove_trailing_dimension and all(tensor.size(-1) == 1 for tensor in tensor_list):\n            batched_tensor = batched_tensor.squeeze(-1)\n        batched_tensors[key] = batched_tensor\n    return batched_tensors\n\n\ndef get_lengths_from_binary_sequence_mask(mask: torch.BoolTensor) -> torch.LongTensor:\n    """"""\n    Compute sequence lengths for each batch element in a tensor using a\n    binary mask.\n\n    # Parameters\n\n    mask : `torch.BoolTensor`, required.\n        A 2D binary mask of shape (batch_size, sequence_length) to\n        calculate the per-batch sequence lengths from.\n\n    # Returns\n\n    `torch.LongTensor`\n        A torch.LongTensor of shape (batch_size,) representing the lengths\n        of the sequences in the batch.\n    """"""\n    return mask.sum(-1)\n\n\ndef get_mask_from_sequence_lengths(\n    sequence_lengths: torch.Tensor, max_length: int\n) -> torch.BoolTensor:\n    """"""\n    Given a variable of shape `(batch_size,)` that represents the sequence lengths of each batch\n    element, this function returns a `(batch_size, max_length)` mask variable.  For example, if\n    our input was `[2, 2, 3]`, with a `max_length` of 4, we\'d return\n    `[[1, 1, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0]]`.\n\n    We require `max_length` here instead of just computing it from the input `sequence_lengths`\n    because it lets us avoid finding the max, then copying that value from the GPU to the CPU so\n    that we can use it to construct a new tensor.\n    """"""\n    # (batch_size, max_length)\n    ones = sequence_lengths.new_ones(sequence_lengths.size(0), max_length)\n    range_tensor = ones.cumsum(dim=1)\n    return sequence_lengths.unsqueeze(1) >= range_tensor\n\n\ndef sort_batch_by_length(tensor: torch.Tensor, sequence_lengths: torch.Tensor):\n    """"""\n    Sort a batch first tensor by some specified lengths.\n\n    # Parameters\n\n    tensor : `torch.FloatTensor`, required.\n        A batch first Pytorch tensor.\n    sequence_lengths : `torch.LongTensor`, required.\n        A tensor representing the lengths of some dimension of the tensor which\n        we want to sort by.\n\n    # Returns\n\n    sorted_tensor : `torch.FloatTensor`\n        The original tensor sorted along the batch dimension with respect to sequence_lengths.\n    sorted_sequence_lengths : `torch.LongTensor`\n        The original sequence_lengths sorted by decreasing size.\n    restoration_indices : `torch.LongTensor`\n        Indices into the sorted_tensor such that\n        `sorted_tensor.index_select(0, restoration_indices) == original_tensor`\n    permutation_index : `torch.LongTensor`\n        The indices used to sort the tensor. This is useful if you want to sort many\n        tensors using the same ordering.\n    """"""\n\n    if not isinstance(tensor, torch.Tensor) or not isinstance(sequence_lengths, torch.Tensor):\n        raise ConfigurationError(""Both the tensor and sequence lengths must be torch.Tensors."")\n\n    sorted_sequence_lengths, permutation_index = sequence_lengths.sort(0, descending=True)\n    sorted_tensor = tensor.index_select(0, permutation_index)\n\n    index_range = torch.arange(0, len(sequence_lengths), device=sequence_lengths.device)\n    # This is the equivalent of zipping with index, sorting by the original\n    # sequence lengths and returning the now sorted indices.\n    _, reverse_mapping = permutation_index.sort(0, descending=False)\n    restoration_indices = index_range.index_select(0, reverse_mapping)\n    return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index\n\n\ndef get_final_encoder_states(\n    encoder_outputs: torch.Tensor, mask: torch.BoolTensor, bidirectional: bool = False\n) -> torch.Tensor:\n    """"""\n    Given the output from a `Seq2SeqEncoder`, with shape `(batch_size, sequence_length,\n    encoding_dim)`, this method returns the final hidden state for each element of the batch,\n    giving a tensor of shape `(batch_size, encoding_dim)`.  This is not as simple as\n    `encoder_outputs[:, -1]`, because the sequences could have different lengths.  We use the\n    mask (which has shape `(batch_size, sequence_length)`) to find the final state for each batch\n    instance.\n\n    Additionally, if `bidirectional` is `True`, we will split the final dimension of the\n    `encoder_outputs` into two and assume that the first half is for the forward direction of the\n    encoder and the second half is for the backward direction.  We will concatenate the last state\n    for each encoder dimension, giving `encoder_outputs[:, -1, :encoding_dim/2]` concatenated with\n    `encoder_outputs[:, 0, encoding_dim/2:]`.\n    """"""\n    # These are the indices of the last words in the sequences (i.e. length sans padding - 1).  We\n    # are assuming sequences are right padded.\n    # Shape: (batch_size,)\n    last_word_indices = mask.sum(1) - 1\n    batch_size, _, encoder_output_dim = encoder_outputs.size()\n    expanded_indices = last_word_indices.view(-1, 1, 1).expand(batch_size, 1, encoder_output_dim)\n    # Shape: (batch_size, 1, encoder_output_dim)\n    final_encoder_output = encoder_outputs.gather(1, expanded_indices)\n    final_encoder_output = final_encoder_output.squeeze(1)  # (batch_size, encoder_output_dim)\n    if bidirectional:\n        final_forward_output = final_encoder_output[:, : (encoder_output_dim // 2)]\n        final_backward_output = encoder_outputs[:, 0, (encoder_output_dim // 2) :]\n        final_encoder_output = torch.cat([final_forward_output, final_backward_output], dim=-1)\n    return final_encoder_output\n\n\ndef get_dropout_mask(dropout_probability: float, tensor_for_masking: torch.Tensor):\n    """"""\n    Computes and returns an element-wise dropout mask for a given tensor, where\n    each element in the mask is dropped out with probability dropout_probability.\n    Note that the mask is NOT applied to the tensor - the tensor is passed to retain\n    the correct CUDA tensor type for the mask.\n\n    # Parameters\n\n    dropout_probability : `float`, required.\n        Probability of dropping a dimension of the input.\n    tensor_for_masking : `torch.Tensor`, required.\n\n\n    # Returns\n\n    `torch.FloatTensor`\n        A torch.FloatTensor consisting of the binary mask scaled by 1/ (1 - dropout_probability).\n        This scaling ensures expected values and variances of the output of applying this mask\n        and the original tensor are the same.\n    """"""\n    binary_mask = (torch.rand(tensor_for_masking.size()) > dropout_probability).to(\n        tensor_for_masking.device\n    )\n    # Scale mask by 1/keep_prob to preserve output statistics.\n    dropout_mask = binary_mask.float().div(1.0 - dropout_probability)\n    return dropout_mask\n\n\ndef masked_softmax(\n    vector: torch.Tensor, mask: torch.BoolTensor, dim: int = -1, memory_efficient: bool = False,\n) -> torch.Tensor:\n    """"""\n    `torch.nn.functional.softmax(vector)` does not work if some elements of `vector` should be\n    masked.  This performs a softmax on just the non-masked portions of `vector`.  Passing\n    `None` in for the mask is also acceptable; you\'ll just get a regular softmax.\n\n    `vector` can have an arbitrary number of dimensions; the only requirement is that `mask` is\n    broadcastable to `vector\'s` shape.  If `mask` has fewer dimensions than `vector`, we will\n    unsqueeze on dimension 1 until they match.  If you need a different unsqueezing of your mask,\n    do it yourself before passing the mask into this function.\n\n    If `memory_efficient` is set to true, we will simply use a very large negative number for those\n    masked positions so that the probabilities of those positions would be approximately 0.\n    This is not accurate in math, but works for most cases and consumes less memory.\n\n    In the case that the input vector is completely masked and `memory_efficient` is false, this function\n    returns an array of `0.0`. This behavior may cause `NaN` if this is used as the last layer of\n    a model that uses categorical cross-entropy loss. Instead, if `memory_efficient` is true, this function\n    will treat every element as equal, and do softmax over equal numbers.\n    """"""\n    if mask is None:\n        result = torch.nn.functional.softmax(vector, dim=dim)\n    else:\n        while mask.dim() < vector.dim():\n            mask = mask.unsqueeze(1)\n        if not memory_efficient:\n            # To limit numerical errors from large vector elements outside the mask, we zero these out.\n            result = torch.nn.functional.softmax(vector * mask, dim=dim)\n            result = result * mask\n            result = result / (\n                result.sum(dim=dim, keepdim=True) + tiny_value_of_dtype(result.dtype)\n            )\n        else:\n            masked_vector = vector.masked_fill(~mask, min_value_of_dtype(vector.dtype))\n            result = torch.nn.functional.softmax(masked_vector, dim=dim)\n    return result\n\n\ndef masked_log_softmax(vector: torch.Tensor, mask: torch.BoolTensor, dim: int = -1) -> torch.Tensor:\n    """"""\n    `torch.nn.functional.log_softmax(vector)` does not work if some elements of `vector` should be\n    masked.  This performs a log_softmax on just the non-masked portions of `vector`.  Passing\n    `None` in for the mask is also acceptable; you\'ll just get a regular log_softmax.\n\n    `vector` can have an arbitrary number of dimensions; the only requirement is that `mask` is\n    broadcastable to `vector\'s` shape.  If `mask` has fewer dimensions than `vector`, we will\n    unsqueeze on dimension 1 until they match.  If you need a different unsqueezing of your mask,\n    do it yourself before passing the mask into this function.\n\n    In the case that the input vector is completely masked, the return value of this function is\n    arbitrary, but not `nan`.  You should be masking the result of whatever computation comes out\n    of this in that case, anyway, so the specific values returned shouldn\'t matter.  Also, the way\n    that we deal with this case relies on having single-precision floats; mixing half-precision\n    floats with fully-masked vectors will likely give you `nans`.\n\n    If your logits are all extremely negative (i.e., the max value in your logit vector is -50 or\n    lower), the way we handle masking here could mess you up.  But if you\'ve got logit values that\n    extreme, you\'ve got bigger problems than this.\n    """"""\n    if mask is not None:\n        while mask.dim() < vector.dim():\n            mask = mask.unsqueeze(1)\n        # vector + mask.log() is an easy way to zero out masked elements in logspace, but it\n        # results in nans when the whole vector is masked.  We need a very small value instead of a\n        # zero in the mask for these cases.\n        vector = vector + (mask + tiny_value_of_dtype(vector.dtype)).log()\n    return torch.nn.functional.log_softmax(vector, dim=dim)\n\n\ndef masked_max(\n    vector: torch.Tensor, mask: torch.BoolTensor, dim: int, keepdim: bool = False,\n) -> torch.Tensor:\n    """"""\n    To calculate max along certain dimensions on masked values\n\n    # Parameters\n\n    vector : `torch.Tensor`\n        The vector to calculate max, assume unmasked parts are already zeros\n    mask : `torch.BoolTensor`\n        The mask of the vector. It must be broadcastable with vector.\n    dim : `int`\n        The dimension to calculate max\n    keepdim : `bool`\n        Whether to keep dimension\n\n    # Returns\n\n    `torch.Tensor`\n        A `torch.Tensor` of including the maximum values.\n    """"""\n    replaced_vector = vector.masked_fill(~mask, min_value_of_dtype(vector.dtype))\n    max_value, _ = replaced_vector.max(dim=dim, keepdim=keepdim)\n    return max_value\n\n\ndef masked_mean(\n    vector: torch.Tensor, mask: torch.BoolTensor, dim: int, keepdim: bool = False\n) -> torch.Tensor:\n    """"""\n    To calculate mean along certain dimensions on masked values\n\n    # Parameters\n\n    vector : `torch.Tensor`\n        The vector to calculate mean.\n    mask : `torch.BoolTensor`\n        The mask of the vector. It must be broadcastable with vector.\n    dim : `int`\n        The dimension to calculate mean\n    keepdim : `bool`\n        Whether to keep dimension\n\n    # Returns\n\n    `torch.Tensor`\n        A `torch.Tensor` of including the mean values.\n    """"""\n    replaced_vector = vector.masked_fill(~mask, 0.0)\n\n    value_sum = torch.sum(replaced_vector, dim=dim, keepdim=keepdim)\n    value_count = torch.sum(mask, dim=dim, keepdim=keepdim)\n    return value_sum / value_count.float().clamp(min=tiny_value_of_dtype(torch.float))\n\n\ndef masked_flip(padded_sequence: torch.Tensor, sequence_lengths: List[int]) -> torch.Tensor:\n    """"""\n    Flips a padded tensor along the time dimension without affecting masked entries.\n\n    # Parameters\n\n    padded_sequence : `torch.Tensor`\n        The tensor to flip along the time dimension.\n        Assumed to be of dimensions (batch size, num timesteps, ...)\n    sequence_lengths : `torch.Tensor`\n        A list containing the lengths of each unpadded sequence in the batch.\n\n    # Returns\n\n    `torch.Tensor`\n        A `torch.Tensor` of the same shape as padded_sequence.\n    """"""\n    assert padded_sequence.size(0) == len(\n        sequence_lengths\n    ), f""sequence_lengths length ${len(sequence_lengths)} does not match batch size ${padded_sequence.size(0)}""\n    num_timesteps = padded_sequence.size(1)\n    flipped_padded_sequence = torch.flip(padded_sequence, [1])\n    sequences = [\n        flipped_padded_sequence[i, num_timesteps - length :]\n        for i, length in enumerate(sequence_lengths)\n    ]\n    return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n\n\ndef viterbi_decode(\n    tag_sequence: torch.Tensor,\n    transition_matrix: torch.Tensor,\n    tag_observations: Optional[List[int]] = None,\n    allowed_start_transitions: torch.Tensor = None,\n    allowed_end_transitions: torch.Tensor = None,\n    top_k: int = None,\n):\n    """"""\n    Perform Viterbi decoding in log space over a sequence given a transition matrix\n    specifying pairwise (transition) potentials between tags and a matrix of shape\n    (sequence_length, num_tags) specifying unary potentials for possible tags per\n    timestep.\n\n    # Parameters\n\n    tag_sequence : `torch.Tensor`, required.\n        A tensor of shape (sequence_length, num_tags) representing scores for\n        a set of tags over a given sequence.\n    transition_matrix : `torch.Tensor`, required.\n        A tensor of shape (num_tags, num_tags) representing the binary potentials\n        for transitioning between a given pair of tags.\n    tag_observations : `Optional[List[int]]`, optional, (default = `None`)\n        A list of length `sequence_length` containing the class ids of observed\n        elements in the sequence, with unobserved elements being set to -1. Note that\n        it is possible to provide evidence which results in degenerate labelings if\n        the sequences of tags you provide as evidence cannot transition between each\n        other, or those transitions are extremely unlikely. In this situation we log a\n        warning, but the responsibility for providing self-consistent evidence ultimately\n        lies with the user.\n    allowed_start_transitions : `torch.Tensor`, optional, (default = `None`)\n        An optional tensor of shape (num_tags,) describing which tags the START token\n        may transition *to*. If provided, additional transition constraints will be used for\n        determining the start element of the sequence.\n    allowed_end_transitions : `torch.Tensor`, optional, (default = `None`)\n        An optional tensor of shape (num_tags,) describing which tags may transition *to* the\n        end tag. If provided, additional transition constraints will be used for determining\n        the end element of the sequence.\n    top_k : `int`, optional, (default = `None`)\n        Optional integer specifying how many of the top paths to return. For top_k>=1, returns\n        a tuple of two lists: top_k_paths, top_k_scores, For top_k==None, returns a flattened\n        tuple with just the top path and its score (not in lists, for backwards compatibility).\n\n    # Returns\n\n    viterbi_path : `List[int]`\n        The tag indices of the maximum likelihood tag sequence.\n    viterbi_score : `torch.Tensor`\n        The score of the viterbi path.\n    """"""\n    if top_k is None:\n        top_k = 1\n        flatten_output = True\n    elif top_k >= 1:\n        flatten_output = False\n    else:\n        raise ValueError(f""top_k must be either None or an integer >=1. Instead received {top_k}"")\n\n    sequence_length, num_tags = list(tag_sequence.size())\n\n    has_start_end_restrictions = (\n        allowed_end_transitions is not None or allowed_start_transitions is not None\n    )\n\n    if has_start_end_restrictions:\n\n        if allowed_end_transitions is None:\n            allowed_end_transitions = torch.zeros(num_tags)\n        if allowed_start_transitions is None:\n            allowed_start_transitions = torch.zeros(num_tags)\n\n        num_tags = num_tags + 2\n        new_transition_matrix = torch.zeros(num_tags, num_tags)\n        new_transition_matrix[:-2, :-2] = transition_matrix\n\n        # Start and end transitions are fully defined, but cannot transition between each other.\n\n        allowed_start_transitions = torch.cat(\n            [allowed_start_transitions, torch.tensor([-math.inf, -math.inf])]\n        )\n        allowed_end_transitions = torch.cat(\n            [allowed_end_transitions, torch.tensor([-math.inf, -math.inf])]\n        )\n\n        # First define how we may transition FROM the start and end tags.\n        new_transition_matrix[-2, :] = allowed_start_transitions\n        # We cannot transition from the end tag to any tag.\n        new_transition_matrix[-1, :] = -math.inf\n\n        new_transition_matrix[:, -1] = allowed_end_transitions\n        # We cannot transition to the start tag from any tag.\n        new_transition_matrix[:, -2] = -math.inf\n\n        transition_matrix = new_transition_matrix\n\n    if tag_observations:\n        if len(tag_observations) != sequence_length:\n            raise ConfigurationError(\n                ""Observations were provided, but they were not the same length ""\n                ""as the sequence. Found sequence of length: {} and evidence: {}"".format(\n                    sequence_length, tag_observations\n                )\n            )\n    else:\n        tag_observations = [-1 for _ in range(sequence_length)]\n\n    if has_start_end_restrictions:\n        tag_observations = [num_tags - 2] + tag_observations + [num_tags - 1]\n        zero_sentinel = torch.zeros(1, num_tags)\n        extra_tags_sentinel = torch.ones(sequence_length, 2) * -math.inf\n        tag_sequence = torch.cat([tag_sequence, extra_tags_sentinel], -1)\n        tag_sequence = torch.cat([zero_sentinel, tag_sequence, zero_sentinel], 0)\n        sequence_length = tag_sequence.size(0)\n\n    path_scores = []\n    path_indices = []\n\n    if tag_observations[0] != -1:\n        one_hot = torch.zeros(num_tags)\n        one_hot[tag_observations[0]] = 100000.0\n        path_scores.append(one_hot.unsqueeze(0))\n    else:\n        path_scores.append(tag_sequence[0, :].unsqueeze(0))\n\n    # Evaluate the scores for all possible paths.\n    for timestep in range(1, sequence_length):\n        # Add pairwise potentials to current scores.\n        summed_potentials = path_scores[timestep - 1].unsqueeze(2) + transition_matrix\n        summed_potentials = summed_potentials.view(-1, num_tags)\n\n        # Best pairwise potential path score from the previous timestep.\n        max_k = min(summed_potentials.size()[0], top_k)\n        scores, paths = torch.topk(summed_potentials, k=max_k, dim=0)\n\n        # If we have an observation for this timestep, use it\n        # instead of the distribution over tags.\n        observation = tag_observations[timestep]\n        # Warn the user if they have passed\n        # invalid/extremely unlikely evidence.\n        if tag_observations[timestep - 1] != -1 and observation != -1:\n            if transition_matrix[tag_observations[timestep - 1], observation] < -10000:\n                logger.warning(\n                    ""The pairwise potential between tags you have passed as ""\n                    ""observations is extremely unlikely. Double check your evidence ""\n                    ""or transition potentials!""\n                )\n        if observation != -1:\n            one_hot = torch.zeros(num_tags)\n            one_hot[observation] = 100000.0\n            path_scores.append(one_hot.unsqueeze(0))\n        else:\n            path_scores.append(tag_sequence[timestep, :] + scores)\n        path_indices.append(paths.squeeze())\n\n    # Construct the most likely sequence backwards.\n    path_scores_v = path_scores[-1].view(-1)\n    max_k = min(path_scores_v.size()[0], top_k)\n    viterbi_scores, best_paths = torch.topk(path_scores_v, k=max_k, dim=0)\n    viterbi_paths = []\n    for i in range(max_k):\n        viterbi_path = [best_paths[i]]\n        for backward_timestep in reversed(path_indices):\n            viterbi_path.append(int(backward_timestep.view(-1)[viterbi_path[-1]]))\n        # Reverse the backward path.\n        viterbi_path.reverse()\n\n        if has_start_end_restrictions:\n            viterbi_path = viterbi_path[1:-1]\n\n        # Viterbi paths uses (num_tags * n_permutations) nodes; therefore, we need to modulo.\n        viterbi_path = [j % num_tags for j in viterbi_path]\n        viterbi_paths.append(viterbi_path)\n\n    if flatten_output:\n        return viterbi_paths[0], viterbi_scores[0]\n\n    return viterbi_paths, viterbi_scores\n\n\ndef get_text_field_mask(\n    text_field_tensors: Dict[str, Dict[str, torch.Tensor]],\n    num_wrapping_dims: int = 0,\n    padding_id: int = 0,\n) -> torch.BoolTensor:\n    """"""\n    Takes the dictionary of tensors produced by a `TextField` and returns a mask\n    with 0 where the tokens are padding, and 1 otherwise. `padding_id` specifies the id of padding tokens.\n    We also handle `TextFields` wrapped by an arbitrary number of `ListFields`, where the number of wrapping\n    `ListFields` is given by `num_wrapping_dims`.\n\n    If `num_wrapping_dims == 0`, the returned mask has shape `(batch_size, num_tokens)`.\n    If `num_wrapping_dims > 0` then the returned mask has `num_wrapping_dims` extra\n    dimensions, so the shape will be `(batch_size, ..., num_tokens)`.\n\n    There could be several entries in the tensor dictionary with different shapes (e.g., one for\n    word ids, one for character ids).  In order to get a token mask, we use the tensor in\n    the dictionary with the lowest number of dimensions.  After subtracting `num_wrapping_dims`,\n    if this tensor has two dimensions we assume it has shape `(batch_size, ..., num_tokens)`,\n    and use it for the mask.  If instead it has three dimensions, we assume it has shape\n    `(batch_size, ..., num_tokens, num_features)`, and sum over the last dimension to produce\n    the mask.  Most frequently this will be a character id tensor, but it could also be a\n    featurized representation of each token, etc.\n\n    If the input `text_field_tensors` contains the ""mask"" key, this is returned instead of inferring the mask.\n    """"""\n    masks = []\n    for indexer_name, indexer_tensors in text_field_tensors.items():\n        if ""mask"" in indexer_tensors:\n            masks.append(indexer_tensors[""mask""].bool())\n    if len(masks) == 1:\n        return masks[0]\n    elif len(masks) > 1:\n        # TODO(mattg): My guess is this will basically never happen, so I\'m not writing logic to\n        # handle it.  Should be straightforward to handle, though.  If you see this error in\n        # practice, open an issue on github.\n        raise ValueError(""found two mask outputs; not sure which to use!"")\n\n    tensor_dims = [\n        (tensor.dim(), tensor)\n        for indexer_output in text_field_tensors.values()\n        for tensor in indexer_output.values()\n    ]\n    tensor_dims.sort(key=lambda x: x[0])\n\n    smallest_dim = tensor_dims[0][0] - num_wrapping_dims\n    if smallest_dim == 2:\n        token_tensor = tensor_dims[0][1]\n        return token_tensor != padding_id\n    elif smallest_dim == 3:\n        character_tensor = tensor_dims[0][1]\n        return (character_tensor != padding_id).any(dim=-1)\n    else:\n        raise ValueError(""Expected a tensor with dimension 2 or 3, found {}"".format(smallest_dim))\n\n\ndef get_token_ids_from_text_field_tensors(\n    text_field_tensors: Dict[str, Dict[str, torch.Tensor]],\n) -> torch.Tensor:\n    """"""\n    Our `TextFieldTensors` are complex output structures, because they try to handle a lot of\n    potential variation. Sometimes, you just want to grab the token ids from this data structure,\n    and that\'s not trivial without hard-coding assumptions about your data processing, which defeats\n    the entire purpose of that generality. This method tries to let you get the token ids out of the\n    data structure in your model without hard-coding any assumptions.\n    """"""\n    for indexer_name, indexer_tensors in text_field_tensors.items():\n        for argument_name, tensor in indexer_tensors.items():\n            if argument_name in [""tokens"", ""token_ids"", ""input_ids""]:\n                return tensor\n    raise NotImplementedError(\n        ""Our heuristic for guessing the right token ids failed. Please open an issue on ""\n        ""github with more detail on how you got this error, so we can implement more robust ""\n        ""logic in this method.""\n    )\n\n\ndef weighted_sum(matrix: torch.Tensor, attention: torch.Tensor) -> torch.Tensor:\n    """"""\n    Takes a matrix of vectors and a set of weights over the rows in the matrix (which we call an\n    ""attention"" vector), and returns a weighted sum of the rows in the matrix.  This is the typical\n    computation performed after an attention mechanism.\n\n    Note that while we call this a ""matrix"" of vectors and an attention ""vector"", we also handle\n    higher-order tensors.  We always sum over the second-to-last dimension of the ""matrix"", and we\n    assume that all dimensions in the ""matrix"" prior to the last dimension are matched in the\n    ""vector"".  Non-matched dimensions in the ""vector"" must be `directly after the batch dimension`.\n\n    For example, say I have a ""matrix"" with dimensions `(batch_size, num_queries, num_words,\n    embedding_dim)`.  The attention ""vector"" then must have at least those dimensions, and could\n    have more. Both:\n\n        - `(batch_size, num_queries, num_words)` (distribution over words for each query)\n        - `(batch_size, num_documents, num_queries, num_words)` (distribution over words in a\n          query for each document)\n\n    are valid input ""vectors"", producing tensors of shape:\n    `(batch_size, num_queries, embedding_dim)` and\n    `(batch_size, num_documents, num_queries, embedding_dim)` respectively.\n    """"""\n    # We\'ll special-case a few settings here, where there are efficient (but poorly-named)\n    # operations in pytorch that already do the computation we need.\n    if attention.dim() == 2 and matrix.dim() == 3:\n        return attention.unsqueeze(1).bmm(matrix).squeeze(1)\n    if attention.dim() == 3 and matrix.dim() == 3:\n        return attention.bmm(matrix)\n    if matrix.dim() - 1 < attention.dim():\n        expanded_size = list(matrix.size())\n        for i in range(attention.dim() - matrix.dim() + 1):\n            matrix = matrix.unsqueeze(1)\n            expanded_size.insert(i + 1, attention.size(i + 1))\n        matrix = matrix.expand(*expanded_size)\n    intermediate = attention.unsqueeze(-1).expand_as(matrix) * matrix\n    return intermediate.sum(dim=-2)\n\n\ndef sequence_cross_entropy_with_logits(\n    logits: torch.FloatTensor,\n    targets: torch.LongTensor,\n    weights: Union[torch.FloatTensor, torch.BoolTensor],\n    average: str = ""batch"",\n    label_smoothing: float = None,\n    gamma: float = None,\n    alpha: Union[float, List[float], torch.FloatTensor] = None,\n) -> torch.FloatTensor:\n    """"""\n    Computes the cross entropy loss of a sequence, weighted with respect to\n    some user provided weights. Note that the weighting here is not the same as\n    in the `torch.nn.CrossEntropyLoss()` criterion, which is weighting\n    classes; here we are weighting the loss contribution from particular elements\n    in the sequence. This allows loss computations for models which use padding.\n\n    # Parameters\n\n    logits : `torch.FloatTensor`, required.\n        A `torch.FloatTensor` of size (batch_size, sequence_length, num_classes)\n        which contains the unnormalized probability for each class.\n    targets : `torch.LongTensor`, required.\n        A `torch.LongTensor` of size (batch, sequence_length) which contains the\n        index of the true class for each corresponding step.\n    weights : `Union[torch.FloatTensor, torch.BoolTensor]`, required.\n        A `torch.FloatTensor` of size (batch, sequence_length)\n    average: `str`, optional (default = `""batch""`)\n        If ""batch"", average the loss across the batches. If ""token"", average\n        the loss across each item in the input. If `None`, return a vector\n        of losses per batch element.\n    label_smoothing : `float`, optional (default = `None`)\n        Whether or not to apply label smoothing to the cross-entropy loss.\n        For example, with a label smoothing value of 0.2, a 4 class classification\n        target would look like `[0.05, 0.05, 0.85, 0.05]` if the 3rd class was\n        the correct label.\n    gamma : `float`, optional (default = `None`)\n        Focal loss[*] focusing parameter `gamma` to reduces the relative loss for\n        well-classified examples and put more focus on hard. The greater value\n        `gamma` is, the more focus on hard examples.\n    alpha : `Union[float, List[float]]`, optional (default = `None`)\n        Focal loss[*] weighting factor `alpha` to balance between classes. Can be\n        used independently with `gamma`. If a single `float` is provided, it\n        is assumed binary case using `alpha` and `1 - alpha` for positive and\n        negative respectively. If a list of `float` is provided, with the same\n        length as the number of classes, the weights will match the classes.\n        [*] T. Lin, P. Goyal, R. Girshick, K. He and P. Doll\xc3\xa1r, ""Focal Loss for\n        Dense Object Detection,"" 2017 IEEE International Conference on Computer\n        Vision (ICCV), Venice, 2017, pp. 2999-3007.\n\n    # Returns\n\n    `torch.FloatTensor`\n        A torch.FloatTensor representing the cross entropy loss.\n        If `average==""batch""` or `average==""token""`, the returned loss is a scalar.\n        If `average is None`, the returned loss is a vector of shape (batch_size,).\n\n    """"""\n    if average not in {None, ""token"", ""batch""}:\n        raise ValueError(""Got average f{average}, expected one of None, \'token\', or \'batch\'"")\n\n    # make sure weights are float\n    weights = weights.to(logits.dtype)\n    # sum all dim except batch\n    non_batch_dims = tuple(range(1, len(weights.shape)))\n    # shape : (batch_size,)\n    weights_batch_sum = weights.sum(dim=non_batch_dims)\n    # shape : (batch * sequence_length, num_classes)\n    logits_flat = logits.view(-1, logits.size(-1))\n    # shape : (batch * sequence_length, num_classes)\n    log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n    # shape : (batch * max_len, 1)\n    targets_flat = targets.view(-1, 1).long()\n    # focal loss coefficient\n    if gamma:\n        # shape : (batch * sequence_length, num_classes)\n        probs_flat = log_probs_flat.exp()\n        # shape : (batch * sequence_length,)\n        probs_flat = torch.gather(probs_flat, dim=1, index=targets_flat)\n        # shape : (batch * sequence_length,)\n        focal_factor = (1.0 - probs_flat) ** gamma\n        # shape : (batch, sequence_length)\n        focal_factor = focal_factor.view(*targets.size())\n        weights = weights * focal_factor\n\n    if alpha is not None:\n        # shape : () / (num_classes,)\n        if isinstance(alpha, (float, int)):\n\n            # shape : (2,)\n            alpha_factor = torch.tensor(\n                [1.0 - float(alpha), float(alpha)], dtype=weights.dtype, device=weights.device\n            )\n\n        elif isinstance(alpha, (list, numpy.ndarray, torch.Tensor)):\n\n            # shape : (c,)\n            alpha_factor = torch.tensor(alpha, dtype=weights.dtype, device=weights.device)\n\n            if not alpha_factor.size():\n                # shape : (1,)\n                alpha_factor = alpha_factor.view(1)\n                # shape : (2,)\n                alpha_factor = torch.cat([1 - alpha_factor, alpha_factor])\n        else:\n            raise TypeError(\n                (""alpha must be float, list of float, or torch.FloatTensor, {} provided."").format(\n                    type(alpha)\n                )\n            )\n        # shape : (batch, max_len)\n        alpha_factor = torch.gather(alpha_factor, dim=0, index=targets_flat.view(-1)).view(\n            *targets.size()\n        )\n        weights = weights * alpha_factor\n\n    if label_smoothing is not None and label_smoothing > 0.0:\n        num_classes = logits.size(-1)\n        smoothing_value = label_smoothing / num_classes\n        # Fill all the correct indices with 1 - smoothing value.\n        one_hot_targets = torch.zeros_like(log_probs_flat).scatter_(\n            -1, targets_flat, 1.0 - label_smoothing\n        )\n        smoothed_targets = one_hot_targets + smoothing_value\n        negative_log_likelihood_flat = -log_probs_flat * smoothed_targets\n        negative_log_likelihood_flat = negative_log_likelihood_flat.sum(-1, keepdim=True)\n    else:\n        # Contribution to the negative log likelihood only comes from the exact indices\n        # of the targets, as the target distributions are one-hot. Here we use torch.gather\n        # to extract the indices of the num_classes dimension which contribute to the loss.\n        # shape : (batch * sequence_length, 1)\n        negative_log_likelihood_flat = -torch.gather(log_probs_flat, dim=1, index=targets_flat)\n    # shape : (batch, sequence_length)\n    negative_log_likelihood = negative_log_likelihood_flat.view(*targets.size())\n    # shape : (batch, sequence_length)\n    negative_log_likelihood = negative_log_likelihood * weights\n\n    if average == ""batch"":\n        # shape : (batch_size,)\n        per_batch_loss = negative_log_likelihood.sum(non_batch_dims) / (\n            weights_batch_sum + tiny_value_of_dtype(negative_log_likelihood.dtype)\n        )\n        num_non_empty_sequences = (weights_batch_sum > 0).sum() + tiny_value_of_dtype(\n            negative_log_likelihood.dtype\n        )\n        return per_batch_loss.sum() / num_non_empty_sequences\n    elif average == ""token"":\n        return negative_log_likelihood.sum() / (\n            weights_batch_sum.sum() + tiny_value_of_dtype(negative_log_likelihood.dtype)\n        )\n    else:\n        # shape : (batch_size,)\n        per_batch_loss = negative_log_likelihood.sum(non_batch_dims) / (\n            weights_batch_sum + tiny_value_of_dtype(negative_log_likelihood.dtype)\n        )\n        return per_batch_loss\n\n\ndef replace_masked_values(\n    tensor: torch.Tensor, mask: torch.BoolTensor, replace_with: float\n) -> torch.Tensor:\n    """"""\n    Replaces all masked values in `tensor` with `replace_with`.  `mask` must be broadcastable\n    to the same shape as `tensor`. We require that `tensor.dim() == mask.dim()`, as otherwise we\n    won\'t know which dimensions of the mask to unsqueeze.\n\n    This just does `tensor.masked_fill()`, except the pytorch method fills in things with a mask\n    value of 1, where we want the opposite.  You can do this in your own code with\n    `tensor.masked_fill(~mask, replace_with)`.\n    """"""\n    if tensor.dim() != mask.dim():\n        raise ConfigurationError(\n            ""tensor.dim() (%d) != mask.dim() (%d)"" % (tensor.dim(), mask.dim())\n        )\n    return tensor.masked_fill(~mask, replace_with)\n\n\ndef tensors_equal(tensor1: torch.Tensor, tensor2: torch.Tensor, tolerance: float = 1e-12) -> bool:\n    """"""\n    A check for tensor equality (by value).  We make sure that the tensors have the same shape,\n    then check all of the entries in the tensor for equality.  We additionally allow the input\n    tensors to be lists or dictionaries, where we then do the above check on every position in the\n    list / item in the dictionary.  If we find objects that aren\'t tensors as we\'re doing that, we\n    just defer to their equality check.\n\n    This is kind of a catch-all method that\'s designed to make implementing `__eq__` methods\n    easier, in a way that\'s really only intended to be useful for tests.\n    """"""\n\n    if isinstance(tensor1, (list, tuple)):\n        if not isinstance(tensor2, (list, tuple)) or len(tensor1) != len(tensor2):\n            return False\n        return all(tensors_equal(t1, t2, tolerance) for t1, t2 in zip(tensor1, tensor2))\n    elif isinstance(tensor1, dict):\n        if not isinstance(tensor2, dict):\n            return False\n        if tensor1.keys() != tensor2.keys():\n            return False\n        return all(tensors_equal(tensor1[key], tensor2[key], tolerance) for key in tensor1)\n    elif isinstance(tensor1, torch.Tensor):\n        if not isinstance(tensor2, torch.Tensor):\n            return False\n        if tensor1.size() != tensor2.size():\n            return False\n        # Special case for bools since they don\'t support subtraction\n        if tensor1.dtype == torch.bool or tensor2.dtype == torch.bool:\n            return (tensor1 == tensor2).all()\n        return ((tensor1 - tensor2).abs().float() < tolerance).all()\n    else:\n        try:\n            return tensor1 == tensor2\n        except RuntimeError:\n            print(type(tensor1), type(tensor2))\n            raise\n\n\ndef device_mapping(cuda_device: int):\n    """"""\n    In order to `torch.load()` a GPU-trained model onto a CPU (or specific GPU),\n    you have to supply a `map_location` function. Call this with\n    the desired `cuda_device` to get the function that `torch.load()` needs.\n    """"""\n\n    def inner_device_mapping(storage: torch.Storage, location) -> torch.Storage:\n        if cuda_device >= 0:\n            return storage.cuda(cuda_device)\n        else:\n            return storage\n\n    return inner_device_mapping\n\n\ndef combine_tensors(combination: str, tensors: List[torch.Tensor]) -> torch.Tensor:\n    """"""\n    Combines a list of tensors using element-wise operations and concatenation, specified by a\n    `combination` string.  The string refers to (1-indexed) positions in the input tensor list,\n    and looks like `""1,2,1+2,3-1""`.\n\n    We allow the following kinds of combinations : `x`, `x*y`, `x+y`, `x-y`, and `x/y`,\n    where `x` and `y` are positive integers less than or equal to `len(tensors)`.  Each of\n    the binary operations is performed elementwise.  You can give as many combinations as you want\n    in the `combination` string.  For example, for the input string `""1,2,1*2""`, the result\n    would be `[1;2;1*2]`, as you would expect, where `[;]` is concatenation along the last\n    dimension.\n\n    If you have a fixed, known way to combine tensors that you use in a model, you should probably\n    just use something like `torch.cat([x_tensor, y_tensor, x_tensor * y_tensor])`.  This\n    function adds some complexity that is only necessary if you want the specific combination used\n    to be `configurable`.\n\n    If you want to do any element-wise operations, the tensors involved in each element-wise\n    operation must have the same shape.\n\n    This function also accepts `x` and `y` in place of `1` and `2` in the combination\n    string.\n    """"""\n    if len(tensors) > 9:\n        raise ConfigurationError(""Double-digit tensor lists not currently supported"")\n    combination = combination.replace(""x"", ""1"").replace(""y"", ""2"")\n    to_concatenate = [_get_combination(piece, tensors) for piece in combination.split("","")]\n    return torch.cat(to_concatenate, dim=-1)\n\n\ndef _rindex(sequence: Sequence[T], obj: T) -> int:\n    """"""\n    Return zero-based index in the sequence of the last item whose value is equal to obj.  Raises a\n    ValueError if there is no such item.\n\n    # Parameters\n\n    sequence : `Sequence[T]`\n    obj : `T`\n\n    # Returns\n\n    `int`\n        zero-based index associated to the position of the last item equal to obj\n    """"""\n    for i in range(len(sequence) - 1, -1, -1):\n        if sequence[i] == obj:\n            return i\n\n    raise ValueError(f""Unable to find {obj} in sequence {sequence}."")\n\n\ndef _get_combination(combination: str, tensors: List[torch.Tensor]) -> torch.Tensor:\n    if combination.isdigit():\n        index = int(combination) - 1\n        return tensors[index]\n    else:\n        if len(combination) != 3:\n            raise ConfigurationError(""Invalid combination: "" + combination)\n        first_tensor = _get_combination(combination[0], tensors)\n        second_tensor = _get_combination(combination[2], tensors)\n        operation = combination[1]\n        if operation == ""*"":\n            return first_tensor * second_tensor\n        elif operation == ""/"":\n            return first_tensor / second_tensor\n        elif operation == ""+"":\n            return first_tensor + second_tensor\n        elif operation == ""-"":\n            return first_tensor - second_tensor\n        else:\n            raise ConfigurationError(""Invalid operation: "" + operation)\n\n\ndef combine_tensors_and_multiply(\n    combination: str, tensors: List[torch.Tensor], weights: torch.nn.Parameter\n) -> torch.Tensor:\n    """"""\n    Like [`combine_tensors`](./util.md#combine_tensors), but does a weighted (linear)\n    multiplication while combining. This is a separate function from `combine_tensors`\n    because we try to avoid instantiating large intermediate tensors during the combination,\n    which is possible because we know that we\'re going to be multiplying by a weight vector in the end.\n\n    # Parameters\n\n    combination : `str`\n        Same as in `combine_tensors`\n    tensors : `List[torch.Tensor]`\n        A list of tensors to combine, where the integers in the `combination` are (1-indexed)\n        positions in this list of tensors.  These tensors are all expected to have either three or\n        four dimensions, with the final dimension being an embedding.  If there are four\n        dimensions, one of them must have length 1.\n    weights : `torch.nn.Parameter`\n        A vector of weights to use for the combinations.  This should have shape (combined_dim,),\n        as calculated by `get_combined_dim`.\n    """"""\n    if len(tensors) > 9:\n        raise ConfigurationError(""Double-digit tensor lists not currently supported"")\n    combination = combination.replace(""x"", ""1"").replace(""y"", ""2"")\n    pieces = combination.split("","")\n    tensor_dims = [tensor.size(-1) for tensor in tensors]\n    combination_dims = [_get_combination_dim(piece, tensor_dims) for piece in pieces]\n    dims_so_far = 0\n    to_sum = []\n    for piece, combination_dim in zip(pieces, combination_dims):\n        weight = weights[dims_so_far : (dims_so_far + combination_dim)]\n        dims_so_far += combination_dim\n        to_sum.append(_get_combination_and_multiply(piece, tensors, weight))\n    result = to_sum[0]\n    for result_piece in to_sum[1:]:\n        result = result + result_piece\n    return result\n\n\ndef _get_combination_and_multiply(\n    combination: str, tensors: List[torch.Tensor], weight: torch.nn.Parameter\n) -> torch.Tensor:\n    if combination.isdigit():\n        index = int(combination) - 1\n        return torch.matmul(tensors[index], weight)\n    else:\n        if len(combination) != 3:\n            raise ConfigurationError(""Invalid combination: "" + combination)\n        first_tensor = _get_combination(combination[0], tensors)\n        second_tensor = _get_combination(combination[2], tensors)\n        operation = combination[1]\n        if operation == ""*"":\n            if first_tensor.dim() > 4 or second_tensor.dim() > 4:\n                raise ValueError(""Tensors with dim > 4 not currently supported"")\n            desired_dim = max(first_tensor.dim(), second_tensor.dim()) - 1\n            if first_tensor.dim() == 4:\n                expanded_dim = _rindex(first_tensor.size(), 1)\n                first_tensor = first_tensor.squeeze(expanded_dim)\n            if second_tensor.dim() == 4:\n                expanded_dim = _rindex(second_tensor.size(), 1)\n                second_tensor = second_tensor.squeeze(expanded_dim)\n            intermediate = first_tensor * weight\n            result = torch.matmul(intermediate, second_tensor.transpose(-1, -2))\n            if result.dim() == desired_dim + 1:\n                result = result.squeeze(-1)\n            return result\n        elif operation == ""/"":\n            if first_tensor.dim() > 4 or second_tensor.dim() > 4:\n                raise ValueError(""Tensors with dim > 4 not currently supported"")\n            desired_dim = max(first_tensor.dim(), second_tensor.dim()) - 1\n            if first_tensor.dim() == 4:\n                expanded_dim = _rindex(first_tensor.size(), 1)\n                first_tensor = first_tensor.squeeze(expanded_dim)\n            if second_tensor.dim() == 4:\n                expanded_dim = _rindex(second_tensor.size(), 1)\n                second_tensor = second_tensor.squeeze(expanded_dim)\n            intermediate = first_tensor * weight\n            result = torch.matmul(intermediate, second_tensor.pow(-1).transpose(-1, -2))\n            if result.dim() == desired_dim + 1:\n                result = result.squeeze(-1)\n            return result\n        elif operation == ""+"":\n            return torch.matmul(first_tensor, weight) + torch.matmul(second_tensor, weight)\n        elif operation == ""-"":\n            return torch.matmul(first_tensor, weight) - torch.matmul(second_tensor, weight)\n        else:\n            raise ConfigurationError(""Invalid operation: "" + operation)\n\n\ndef get_combined_dim(combination: str, tensor_dims: List[int]) -> int:\n    """"""\n    For use with [`combine_tensors`](./util.md#combine_tensors).\n    This function computes the resultant dimension when calling `combine_tensors(combination, tensors)`,\n    when the tensor dimension is known.  This is necessary for knowing the sizes of weight matrices\n    when building models that use `combine_tensors`.\n\n    # Parameters\n\n    combination : `str`\n        A comma-separated list of combination pieces, like `""1,2,1*2""`, specified identically to\n        `combination` in `combine_tensors`.\n    tensor_dims : `List[int]`\n        A list of tensor dimensions, where each dimension is from the `last axis` of the tensors\n        that will be input to `combine_tensors`.\n    """"""\n    if len(tensor_dims) > 9:\n        raise ConfigurationError(""Double-digit tensor lists not currently supported"")\n    combination = combination.replace(""x"", ""1"").replace(""y"", ""2"")\n    return sum(_get_combination_dim(piece, tensor_dims) for piece in combination.split("",""))\n\n\ndef _get_combination_dim(combination: str, tensor_dims: List[int]) -> int:\n    if combination.isdigit():\n        index = int(combination) - 1\n        return tensor_dims[index]\n    else:\n        if len(combination) != 3:\n            raise ConfigurationError(""Invalid combination: "" + combination)\n        first_tensor_dim = _get_combination_dim(combination[0], tensor_dims)\n        second_tensor_dim = _get_combination_dim(combination[2], tensor_dims)\n        operation = combination[1]\n        if first_tensor_dim != second_tensor_dim:\n            raise ConfigurationError(\'Tensor dims must match for operation ""{}""\'.format(operation))\n        return first_tensor_dim\n\n\ndef logsumexp(tensor: torch.Tensor, dim: int = -1, keepdim: bool = False) -> torch.Tensor:\n    """"""\n    A numerically stable computation of logsumexp. This is mathematically equivalent to\n    `tensor.exp().sum(dim, keep=keepdim).log()`.  This function is typically used for summing log\n    probabilities.\n\n    # Parameters\n\n    tensor : `torch.FloatTensor`, required.\n        A tensor of arbitrary size.\n    dim : `int`, optional (default = `-1`)\n        The dimension of the tensor to apply the logsumexp to.\n    keepdim: `bool`, optional (default = `False`)\n        Whether to retain a dimension of size one at the dimension we reduce over.\n    """"""\n    max_score, _ = tensor.max(dim, keepdim=keepdim)\n    if keepdim:\n        stable_vec = tensor - max_score\n    else:\n        stable_vec = tensor - max_score.unsqueeze(dim)\n    return max_score + (stable_vec.exp().sum(dim, keepdim=keepdim)).log()\n\n\ndef get_device_of(tensor: torch.Tensor) -> int:\n    """"""\n    Returns the device of the tensor.\n    """"""\n    if not tensor.is_cuda:\n        return -1\n    else:\n        return tensor.get_device()\n\n\ndef flatten_and_batch_shift_indices(indices: torch.Tensor, sequence_length: int) -> torch.Tensor:\n    """"""\n    This is a subroutine for [`batched_index_select`](./util.md#batched_index_select).\n    The given `indices` of size `(batch_size, d_1, ..., d_n)` indexes into dimension 2 of a\n    target tensor, which has size `(batch_size, sequence_length, embedding_size)`. This\n    function returns a vector that correctly indexes into the flattened target. The sequence\n    length of the target must be provided to compute the appropriate offsets.\n\n    ```python\n        indices = torch.ones([2,3], dtype=torch.long)\n        # Sequence length of the target tensor.\n        sequence_length = 10\n        shifted_indices = flatten_and_batch_shift_indices(indices, sequence_length)\n        # Indices into the second element in the batch are correctly shifted\n        # to take into account that the target tensor will be flattened before\n        # the indices are applied.\n        assert shifted_indices == [1, 1, 1, 11, 11, 11]\n    ```\n\n    # Parameters\n\n    indices : `torch.LongTensor`, required.\n    sequence_length : `int`, required.\n        The length of the sequence the indices index into.\n        This must be the second dimension of the tensor.\n\n    # Returns\n\n    offset_indices : `torch.LongTensor`\n    """"""\n    # Shape: (batch_size)\n    if torch.max(indices) >= sequence_length or torch.min(indices) < 0:\n        raise ConfigurationError(\n            f""All elements in indices should be in range (0, {sequence_length - 1})""\n        )\n    offsets = get_range_vector(indices.size(0), get_device_of(indices)) * sequence_length\n    for _ in range(len(indices.size()) - 1):\n        offsets = offsets.unsqueeze(1)\n\n    # Shape: (batch_size, d_1, ..., d_n)\n    offset_indices = indices + offsets\n\n    # Shape: (batch_size * d_1 * ... * d_n)\n    offset_indices = offset_indices.view(-1)\n    return offset_indices\n\n\ndef batched_index_select(\n    target: torch.Tensor,\n    indices: torch.LongTensor,\n    flattened_indices: Optional[torch.LongTensor] = None,\n) -> torch.Tensor:\n    """"""\n    The given `indices` of size `(batch_size, d_1, ..., d_n)` indexes into the sequence\n    dimension (dimension 2) of the target, which has size `(batch_size, sequence_length,\n    embedding_size)`.\n\n    This function returns selected values in the target with respect to the provided indices, which\n    have size `(batch_size, d_1, ..., d_n, embedding_size)`. This can use the optionally\n    precomputed `flattened_indices` with size `(batch_size * d_1 * ... * d_n)` if given.\n\n    An example use case of this function is looking up the start and end indices of spans in a\n    sequence tensor. This is used in the\n    [CoreferenceResolver](../models/coreference_resolution/coref.md). Model to select\n    contextual word representations corresponding to the start and end indices of mentions. The key\n    reason this can\'t be done with basic torch functions is that we want to be able to use look-up\n    tensors with an arbitrary number of dimensions (for example, in the coref model, we don\'t know\n    a-priori how many spans we are looking up).\n\n    # Parameters\n\n    target : `torch.Tensor`, required.\n        A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size).\n        This is the tensor to be indexed.\n    indices : `torch.LongTensor`\n        A tensor of shape (batch_size, ...), where each element is an index into the\n        `sequence_length` dimension of the `target` tensor.\n    flattened_indices : `Optional[torch.Tensor]`, optional (default = `None`)\n        An optional tensor representing the result of calling `flatten_and_batch_shift_indices`\n        on `indices`. This is helpful in the case that the indices can be flattened once and\n        cached for many batch lookups.\n\n    # Returns\n\n    selected_targets : `torch.Tensor`\n        A tensor with shape [indices.size(), target.size(-1)] representing the embedded indices\n        extracted from the batch flattened target tensor.\n    """"""\n    if flattened_indices is None:\n        # Shape: (batch_size * d_1 * ... * d_n)\n        flattened_indices = flatten_and_batch_shift_indices(indices, target.size(1))\n\n    # Shape: (batch_size * sequence_length, embedding_size)\n    flattened_target = target.view(-1, target.size(-1))\n\n    # Shape: (batch_size * d_1 * ... * d_n, embedding_size)\n    flattened_selected = flattened_target.index_select(0, flattened_indices)\n    selected_shape = list(indices.size()) + [target.size(-1)]\n    # Shape: (batch_size, d_1, ..., d_n, embedding_size)\n    selected_targets = flattened_selected.view(*selected_shape)\n    return selected_targets\n\n\ndef batched_span_select(target: torch.Tensor, spans: torch.LongTensor) -> torch.Tensor:\n    """"""\n    The given `spans` of size `(batch_size, num_spans, 2)` indexes into the sequence\n    dimension (dimension 2) of the target, which has size `(batch_size, sequence_length,\n    embedding_size)`.\n\n    This function returns segmented spans in the target with respect to the provided span indices.\n    It does not guarantee element order within each span.\n\n    # Parameters\n\n    target : `torch.Tensor`, required.\n        A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size).\n        This is the tensor to be indexed.\n    indices : `torch.LongTensor`\n        A 3 dimensional tensor of shape (batch_size, num_spans, 2) representing start and end\n        indices (both inclusive) into the `sequence_length` dimension of the `target` tensor.\n\n    # Returns\n\n    span_embeddings : `torch.Tensor`\n        A tensor with shape (batch_size, num_spans, max_batch_span_width, embedding_size]\n        representing the embedded spans extracted from the batch flattened target tensor.\n    span_mask: `torch.BoolTensor`\n        A tensor with shape (batch_size, num_spans, max_batch_span_width) representing the mask on\n        the returned span embeddings.\n    """"""\n    # both of shape (batch_size, num_spans, 1)\n    span_starts, span_ends = spans.split(1, dim=-1)\n\n    # shape (batch_size, num_spans, 1)\n    # These span widths are off by 1, because the span ends are `inclusive`.\n    span_widths = span_ends - span_starts\n\n    # We need to know the maximum span width so we can\n    # generate indices to extract the spans from the sequence tensor.\n    # These indices will then get masked below, such that if the length\n    # of a given span is smaller than the max, the rest of the values\n    # are masked.\n    max_batch_span_width = span_widths.max().item() + 1\n\n    # Shape: (1, 1, max_batch_span_width)\n    max_span_range_indices = get_range_vector(max_batch_span_width, get_device_of(target)).view(\n        1, 1, -1\n    )\n    # Shape: (batch_size, num_spans, max_batch_span_width)\n    # This is a broadcasted comparison - for each span we are considering,\n    # we are creating a range vector of size max_span_width, but masking values\n    # which are greater than the actual length of the span.\n    #\n    # We\'re using <= here (and for the mask below) because the span ends are\n    # inclusive, so we want to include indices which are equal to span_widths rather\n    # than using it as a non-inclusive upper bound.\n    span_mask = max_span_range_indices <= span_widths\n    raw_span_indices = span_ends - max_span_range_indices\n    # We also don\'t want to include span indices which are less than zero,\n    # which happens because some spans near the beginning of the sequence\n    # have an end index < max_batch_span_width, so we add this to the mask here.\n    span_mask = span_mask & (raw_span_indices >= 0)\n    span_indices = torch.nn.functional.relu(raw_span_indices.float()).long()\n\n    # Shape: (batch_size, num_spans, max_batch_span_width, embedding_dim)\n    span_embeddings = batched_index_select(target, span_indices)\n\n    return span_embeddings, span_mask\n\n\ndef flattened_index_select(target: torch.Tensor, indices: torch.LongTensor) -> torch.Tensor:\n    """"""\n    The given `indices` of size `(set_size, subset_size)` specifies subsets of the `target`\n    that each of the set_size rows should select. The `target` has size\n    `(batch_size, sequence_length, embedding_size)`, and the resulting selected tensor has size\n    `(batch_size, set_size, subset_size, embedding_size)`.\n\n    # Parameters\n\n    target : `torch.Tensor`, required.\n        A Tensor of shape (batch_size, sequence_length, embedding_size).\n    indices : `torch.LongTensor`, required.\n        A LongTensor of shape (set_size, subset_size). All indices must be < sequence_length\n        as this tensor is an index into the sequence_length dimension of the target.\n\n    # Returns\n\n    selected : `torch.Tensor`, required.\n        A Tensor of shape (batch_size, set_size, subset_size, embedding_size).\n    """"""\n    if indices.dim() != 2:\n        raise ConfigurationError(\n            ""Indices passed to flattened_index_select had shape {} but ""\n            ""only 2 dimensional inputs are supported."".format(indices.size())\n        )\n    # Shape: (batch_size, set_size * subset_size, embedding_size)\n    flattened_selected = target.index_select(1, indices.view(-1))\n\n    # Shape: (batch_size, set_size, subset_size, embedding_size)\n    selected = flattened_selected.view(target.size(0), indices.size(0), indices.size(1), -1)\n    return selected\n\n\ndef get_range_vector(size: int, device: int) -> torch.Tensor:\n    """"""\n    Returns a range vector with the desired size, starting at 0. The CUDA implementation\n    is meant to avoid copy data from CPU to GPU.\n    """"""\n    if device > -1:\n        return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1\n    else:\n        return torch.arange(0, size, dtype=torch.long)\n\n\ndef bucket_values(\n    distances: torch.Tensor, num_identity_buckets: int = 4, num_total_buckets: int = 10\n) -> torch.Tensor:\n    """"""\n    Places the given values (designed for distances) into `num_total_buckets`semi-logscale\n    buckets, with `num_identity_buckets` of these capturing single values.\n\n    The default settings will bucket values into the following buckets:\n    [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+].\n\n    # Parameters\n\n    distances : `torch.Tensor`, required.\n        A Tensor of any size, to be bucketed.\n    num_identity_buckets: `int`, optional (default = `4`).\n        The number of identity buckets (those only holding a single value).\n    num_total_buckets : `int`, (default = `10`)\n        The total number of buckets to bucket values into.\n\n    # Returns\n\n    `torch.Tensor`\n        A tensor of the same shape as the input, containing the indices of the buckets\n        the values were placed in.\n    """"""\n    # Chunk the values into semi-logscale buckets using .floor().\n    # This is a semi-logscale bucketing because we divide by log(2) after taking the log.\n    # We do this to make the buckets more granular in the initial range, where we expect\n    # most values to fall. We then add (num_identity_buckets - 1) because we want these indices\n    # to start _after_ the fixed number of buckets which we specified would only hold single values.\n    logspace_index = (distances.float().log() / math.log(2)).floor().long() + (\n        num_identity_buckets - 1\n    )\n    # create a mask for values which will go into single number buckets (i.e not a range).\n    use_identity_mask = (distances <= num_identity_buckets).long()\n    use_buckets_mask = 1 + (-1 * use_identity_mask)\n    # Use the original values if they are less than num_identity_buckets, otherwise\n    # use the logspace indices.\n    combined_index = use_identity_mask * distances + use_buckets_mask * logspace_index\n    # Clamp to put anything > num_total_buckets into the final bucket.\n    return combined_index.clamp(0, num_total_buckets - 1)\n\n\ndef add_sentence_boundary_token_ids(\n    tensor: torch.Tensor, mask: torch.BoolTensor, sentence_begin_token: Any, sentence_end_token: Any\n) -> Tuple[torch.Tensor, torch.BoolTensor]:\n    """"""\n    Add begin/end of sentence tokens to the batch of sentences.\n    Given a batch of sentences with size `(batch_size, timesteps)` or\n    `(batch_size, timesteps, dim)` this returns a tensor of shape\n    `(batch_size, timesteps + 2)` or `(batch_size, timesteps + 2, dim)` respectively.\n\n    Returns both the new tensor and updated mask.\n\n    # Parameters\n\n    tensor : `torch.Tensor`\n        A tensor of shape `(batch_size, timesteps)` or `(batch_size, timesteps, dim)`\n    mask : `torch.BoolTensor`\n         A tensor of shape `(batch_size, timesteps)`\n    sentence_begin_token: `Any`\n        Can be anything that can be broadcast in torch for assignment.\n        For 2D input, a scalar with the `<S>` id. For 3D input, a tensor with length dim.\n    sentence_end_token: `Any`\n        Can be anything that can be broadcast in torch for assignment.\n        For 2D input, a scalar with the `</S>` id. For 3D input, a tensor with length dim.\n\n    # Returns\n\n    tensor_with_boundary_tokens : `torch.Tensor`\n        The tensor with the appended and prepended boundary tokens. If the input was 2D,\n        it has shape (batch_size, timesteps + 2) and if the input was 3D, it has shape\n        (batch_size, timesteps + 2, dim).\n    new_mask : `torch.BoolTensor`\n        The new mask for the tensor, taking into account the appended tokens\n        marking the beginning and end of the sentence.\n    """"""\n    # TODO: matthewp, profile this transfer\n    sequence_lengths = mask.sum(dim=1).detach().cpu().numpy()\n    tensor_shape = list(tensor.data.shape)\n    new_shape = list(tensor_shape)\n    new_shape[1] = tensor_shape[1] + 2\n    tensor_with_boundary_tokens = tensor.new_zeros(*new_shape)\n    if len(tensor_shape) == 2:\n        tensor_with_boundary_tokens[:, 1:-1] = tensor\n        tensor_with_boundary_tokens[:, 0] = sentence_begin_token\n        for i, j in enumerate(sequence_lengths):\n            tensor_with_boundary_tokens[i, j + 1] = sentence_end_token\n        new_mask = tensor_with_boundary_tokens != 0\n    elif len(tensor_shape) == 3:\n        tensor_with_boundary_tokens[:, 1:-1, :] = tensor\n        for i, j in enumerate(sequence_lengths):\n            tensor_with_boundary_tokens[i, 0, :] = sentence_begin_token\n            tensor_with_boundary_tokens[i, j + 1, :] = sentence_end_token\n        new_mask = (tensor_with_boundary_tokens > 0).sum(dim=-1) > 0\n    else:\n        raise ValueError(""add_sentence_boundary_token_ids only accepts 2D and 3D input"")\n\n    return tensor_with_boundary_tokens, new_mask\n\n\ndef remove_sentence_boundaries(\n    tensor: torch.Tensor, mask: torch.BoolTensor\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    """"""\n    Remove begin/end of sentence embeddings from the batch of sentences.\n    Given a batch of sentences with size `(batch_size, timesteps, dim)`\n    this returns a tensor of shape `(batch_size, timesteps - 2, dim)` after removing\n    the beginning and end sentence markers.  The sentences are assumed to be padded on the right,\n    with the beginning of each sentence assumed to occur at index 0 (i.e., `mask[:, 0]` is assumed\n    to be 1).\n\n    Returns both the new tensor and updated mask.\n\n    This function is the inverse of `add_sentence_boundary_token_ids`.\n\n    # Parameters\n\n    tensor : `torch.Tensor`\n        A tensor of shape `(batch_size, timesteps, dim)`\n    mask : `torch.BoolTensor`\n         A tensor of shape `(batch_size, timesteps)`\n\n    # Returns\n\n    tensor_without_boundary_tokens : `torch.Tensor`\n        The tensor after removing the boundary tokens of shape `(batch_size, timesteps - 2, dim)`\n    new_mask : `torch.BoolTensor`\n        The new mask for the tensor of shape `(batch_size, timesteps - 2)`.\n    """"""\n    # TODO: matthewp, profile this transfer\n    sequence_lengths = mask.sum(dim=1).detach().cpu().numpy()\n    tensor_shape = list(tensor.data.shape)\n    new_shape = list(tensor_shape)\n    new_shape[1] = tensor_shape[1] - 2\n    tensor_without_boundary_tokens = tensor.new_zeros(*new_shape)\n    new_mask = tensor.new_zeros((new_shape[0], new_shape[1]), dtype=torch.bool)\n    for i, j in enumerate(sequence_lengths):\n        if j > 2:\n            tensor_without_boundary_tokens[i, : (j - 2), :] = tensor[i, 1 : (j - 1), :]\n            new_mask[i, : (j - 2)] = True\n\n    return tensor_without_boundary_tokens, new_mask\n\n\ndef add_positional_features(\n    tensor: torch.Tensor, min_timescale: float = 1.0, max_timescale: float = 1.0e4\n):\n\n    """"""\n    Implements the frequency-based positional encoding described\n    in [Attention is All you Need][0].\n\n    Adds sinusoids of different frequencies to a `Tensor`. A sinusoid of a\n    different frequency and phase is added to each dimension of the input `Tensor`.\n    This allows the attention heads to use absolute and relative positions.\n\n    The number of timescales is equal to hidden_dim / 2 within the range\n    (min_timescale, max_timescale). For each timescale, the two sinusoidal\n    signals sin(timestep / timescale) and cos(timestep / timescale) are\n    generated and concatenated along the hidden_dim dimension.\n\n    [0]: https://www.semanticscholar.org/paper/Attention-Is-All-You-Need-Vaswani-Shazeer/0737da0767d77606169cbf4187b83e1ab62f6077\n\n    # Parameters\n\n    tensor : `torch.Tensor`\n        a Tensor with shape (batch_size, timesteps, hidden_dim).\n    min_timescale : `float`, optional (default = `1.0`)\n        The smallest timescale to use.\n    max_timescale : `float`, optional (default = `1.0e4`)\n        The largest timescale to use.\n\n    # Returns\n\n    `torch.Tensor`\n        The input tensor augmented with the sinusoidal frequencies.\n    """"""  # noqa\n    _, timesteps, hidden_dim = tensor.size()\n\n    timestep_range = get_range_vector(timesteps, get_device_of(tensor)).data.float()\n    # We\'re generating both cos and sin frequencies,\n    # so half for each.\n    num_timescales = hidden_dim // 2\n    timescale_range = get_range_vector(num_timescales, get_device_of(tensor)).data.float()\n\n    log_timescale_increments = math.log(float(max_timescale) / float(min_timescale)) / float(\n        num_timescales - 1\n    )\n    inverse_timescales = min_timescale * torch.exp(timescale_range * -log_timescale_increments)\n\n    # Broadcasted multiplication - shape (timesteps, num_timescales)\n    scaled_time = timestep_range.unsqueeze(1) * inverse_timescales.unsqueeze(0)\n    # shape (timesteps, 2 * num_timescales)\n    sinusoids = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 1)\n    if hidden_dim % 2 != 0:\n        # if the number of dimensions is odd, the cos and sin\n        # timescales had size (hidden_dim - 1) / 2, so we need\n        # to add a row of zeros to make up the difference.\n        sinusoids = torch.cat([sinusoids, sinusoids.new_zeros(timesteps, 1)], 1)\n    return tensor + sinusoids.unsqueeze(0)\n\n\ndef clone(module: torch.nn.Module, num_copies: int) -> torch.nn.ModuleList:\n    """"""Produce N identical layers.""""""\n    return torch.nn.ModuleList(copy.deepcopy(module) for _ in range(num_copies))\n\n\ndef combine_initial_dims(tensor: torch.Tensor) -> torch.Tensor:\n    """"""\n    Given a (possibly higher order) tensor of ids with shape\n    (d1, ..., dn, sequence_length)\n    Return a view that\'s (d1 * ... * dn, sequence_length).\n    If original tensor is 1-d or 2-d, return it as is.\n    """"""\n    if tensor.dim() <= 2:\n        return tensor\n    else:\n        return tensor.view(-1, tensor.size(-1))\n\n\ndef uncombine_initial_dims(tensor: torch.Tensor, original_size: torch.Size) -> torch.Tensor:\n    """"""\n    Given a tensor of embeddings with shape\n    (d1 * ... * dn, sequence_length, embedding_dim)\n    and the original shape\n    (d1, ..., dn, sequence_length),\n    return the reshaped tensor of embeddings with shape\n    (d1, ..., dn, sequence_length, embedding_dim).\n    If original size is 1-d or 2-d, return it as is.\n    """"""\n    if len(original_size) <= 2:\n        return tensor\n    else:\n        view_args = list(original_size) + [tensor.size(-1)]\n        return tensor.view(*view_args)\n\n\ndef inspect_parameters(module: torch.nn.Module, quiet: bool = False) -> Dict[str, Any]:\n    """"""\n    Inspects the model/module parameters and their tunability. The output is structured\n    in a nested dict so that parameters in same sub-modules are grouped together.\n    This can be helpful to setup module path based regex, for example in initializer.\n    It prints it by default (optional) and returns the inspection dict. Eg. output::\n\n        {\n            ""_text_field_embedder"": {\n                ""token_embedder_tokens"": {\n                    ""_projection"": {\n                        ""bias"": ""tunable"",\n                        ""weight"": ""tunable""\n                    },\n                    ""weight"": ""frozen""\n                }\n            }\n        }\n\n    """"""\n    results: Dict[str, Any] = {}\n    for name, param in sorted(module.named_parameters()):\n        keys = name.split(""."")\n        write_to = results\n        for key in keys[:-1]:\n            if key not in write_to:\n                write_to[key] = {}\n            write_to = write_to[key]\n        write_to[keys[-1]] = ""tunable"" if param.requires_grad else ""frozen""\n    if not quiet:\n        print(json.dumps(results, indent=4))\n    return results\n\n\ndef find_embedding_layer(model: torch.nn.Module) -> torch.nn.Module:\n    """"""\n    Takes a model (typically an AllenNLP `Model`, but this works for any `torch.nn.Module`) and\n    makes a best guess about which module is the embedding layer.  For typical AllenNLP models,\n    this often is the `TextFieldEmbedder`, but if you\'re using a pre-trained contextualizer, we\n    really want layer 0 of that contextualizer, not the output.  So there are a bunch of hacks in\n    here for specific pre-trained contextualizers.\n    """"""\n    # We\'ll look for a few special cases in a first pass, then fall back to just finding a\n    # TextFieldEmbedder in a second pass if we didn\'t find a special case.\n    from transformers.modeling_gpt2 import GPT2Model\n    from transformers.modeling_bert import BertEmbeddings\n    from allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder\n    from allennlp.modules.text_field_embedders.basic_text_field_embedder import (\n        BasicTextFieldEmbedder,\n    )\n    from allennlp.modules.token_embedders.embedding import Embedding\n\n    # The special case only works if we don\'t have mismatched embedding.  What we essentially want\n    # to do is grab a transformer\'s wordpiece embedding layer, because using that is a lot easier\n    # for something like hotflip than running a network to embed all tokens in a vocabulary.  If\n    # you\'ve used a mismatched embedder, though, your input tokens are actually *words*, so that\'s\n    # what we\'ll be visualizing and attacking, even though you\'re modeling things at the wordpiece\n    # level.  In this case, we need to return gradients and things at the word level, so we can\'t\n    # use our shortcut of just returning the wordpiece embedding.\n    mismatched = False\n    for module in model.modules():\n        if ""Mismatched"" in module.__class__.__name__:\n            # We don\'t currently have a good way to check whether an embedder is mismatched, and it\n            # doesn\'t seem like it\'s worth it to try to add an API call for this somewhere,\n            # especially as we can\'t really call it here in a type-safe way, anyway, as we\'re\n            # iterating over plain pytorch Modules.  This check should work for now (v1.0), but it\'s\n            # possible that some class gets added later that will require this check to change.\n            mismatched = True\n\n    if not mismatched:\n        for module in model.modules():\n            if isinstance(module, BertEmbeddings):\n                return module.word_embeddings\n            if isinstance(module, GPT2Model):\n                return module.wte\n\n    for module in model.modules():\n        if isinstance(module, TextFieldEmbedder):\n\n            if isinstance(module, BasicTextFieldEmbedder):\n                # We\'ll have a check for single Embedding cases, because we can be more efficient\n                # in cases like this.  If this check fails, then for something like hotflip we need\n                # to actually run the text field embedder and construct a vector for each token.\n                if len(module._token_embedders) == 1:\n                    embedder = list(module._token_embedders.values())[0]\n                    if isinstance(embedder, Embedding):\n                        if embedder._projection is None:\n                            # If there\'s a projection inside the Embedding, then we need to return\n                            # the whole TextFieldEmbedder, because there\'s more computation that\n                            # needs to be done than just multiply by an embedding matrix.\n                            return embedder\n            return module\n    raise RuntimeError(""No embedding module found!"")\n\n\ndef extend_layer(layer: torch.nn.Module, new_dim: int) -> None:\n    valid_layers = [torch.nn.Linear, torch.nn.Bilinear]\n    if not any([isinstance(layer, i) for i in valid_layers]):\n        raise ConfigurationError(""Inappropriate layer type"")\n\n    extend_dim = new_dim - layer.out_features\n    if not extend_dim:\n        return layer\n\n    if isinstance(layer, torch.nn.Linear):\n        new_weight = torch.FloatTensor(extend_dim, layer.in_features)\n    elif isinstance(layer, torch.nn.Bilinear):\n        new_weight = torch.FloatTensor(extend_dim, layer.in1_features, layer.in2_features)\n\n    new_bias = torch.FloatTensor(extend_dim)\n    torch.nn.init.xavier_uniform_(new_weight)\n    torch.nn.init.zeros_(new_bias)\n\n    device = layer.weight.device\n    layer.weight = torch.nn.Parameter(\n        torch.cat([layer.weight.data, new_weight.to(device)], dim=0),\n        requires_grad=layer.weight.requires_grad,\n    )\n    layer.bias = torch.nn.Parameter(\n        torch.cat([layer.bias.data, new_bias.to(device)], dim=0),\n        requires_grad=layer.bias.requires_grad,\n    )\n    layer.out_features = new_dim\n\n\ndef masked_topk(\n    input_: torch.FloatTensor,\n    mask: torch.BoolTensor,\n    k: Union[int, torch.LongTensor],\n    dim: int = -1,\n) -> Tuple[torch.LongTensor, torch.LongTensor, torch.FloatTensor]:\n    """"""\n    Extracts the top-k items along a certain dimension. This is similar to `torch.topk` except:\n    (1) we allow of a `mask` that makes the function not consider certain elements;\n    (2) the returned top input, mask, and indices are sorted in their original order in the input;\n    (3) May use the same k for all dimensions, or different k for each.\n\n    # Parameters\n\n    input_ : `torch.FloatTensor`, required.\n        A tensor containing the items that we want to prune.\n    mask : `torch.BoolTensor`, required.\n        A tensor with the same shape as `input_` that makes the function not consider masked out\n        (i.e. False) elements.\n    k : `Union[int, torch.LongTensor]`, required.\n        If a tensor of shape as `input_` except without dimension `dim`, specifies the number of\n        items to keep for each dimension.\n        If an int, keep the same number of items for all dimensions.\n\n    # Returns\n\n    top_input : `torch.FloatTensor`\n        The values of the top-k scoring items.\n        Has the same shape as `input_` except dimension `dim` has value `k` when it\'s an `int`\n        or `k.max()` when it\'s a tensor.\n    top_mask : `torch.BoolTensor`\n        The corresponding mask for `top_input`.\n        Has the shape as `top_input`.\n    top_indices : `torch.IntTensor`\n        The indices of the top-k scoring items into the original `input_`\n        tensor. This is returned because it can be useful to retain pointers to\n        the original items, if each item is being scored by multiple distinct\n        scorers, for instance.\n        Has the shape as `top_input`.\n    """"""\n    if input_.size() != mask.size():\n        raise ValueError(""`input_` and `mask` must have the same shape."")\n    if not -input_.dim() <= dim < input_.dim():\n        raise ValueError(""`dim` must be in `[-input_.dim(), input_.dim())`"")\n    dim = (dim + input_.dim()) % input_.dim()\n\n    max_k = k if isinstance(k, int) else k.max()\n\n    # We put the dim in question to the last dimension by permutation, and squash all leading dims.\n\n    # [0, 1, ..., dim - 1, dim + 1, ..., input.dim() - 1, dim]\n    permutation = list(range(input_.dim()))\n    permutation.pop(dim)\n    permutation += [dim]\n\n    # [0, 1, ..., dim - 1, -1, dim, ..., input.dim() - 2]; for restoration\n    reverse_permutation = list(range(input_.dim() - 1))\n    reverse_permutation.insert(dim, -1)\n\n    other_dims_size = list(input_.size())\n    other_dims_size.pop(dim)\n    permuted_size = other_dims_size + [max_k]  # for restoration\n\n    # If an int was given for number of items to keep, construct tensor by repeating the value.\n    if isinstance(k, int):\n        # Put the tensor on same device as the mask.\n        k = k * torch.ones(*other_dims_size, dtype=torch.long, device=mask.device)\n    else:\n        if list(k.size()) != other_dims_size:\n            raise ValueError(\n                ""`k` must have the same shape as `input_` with dimension `dim` removed.""\n            )\n\n    num_items = input_.size(dim)\n    # (batch_size, num_items)  -- ""batch_size"" refers to all other dimensions stacked together\n    input_ = input_.permute(*permutation).reshape(-1, num_items)\n    mask = mask.permute(*permutation).reshape(-1, num_items)\n    k = k.reshape(-1)\n\n    # Make sure that we don\'t select any masked items by setting their scores to be very\n    # negative.\n    input_ = replace_masked_values(input_, mask, min_value_of_dtype(input_.dtype))\n\n    # Shape: (batch_size, max_k)\n    _, top_indices = input_.topk(max_k, 1)\n\n    # Mask based on number of items to keep for each sentence.\n    # Shape: (batch_size, max_k)\n    top_indices_mask = get_mask_from_sequence_lengths(k, max_k).bool()\n\n    # Fill all masked indices with largest ""top"" index for that sentence, so that all masked\n    # indices will be sorted to the end.\n    # Shape: (batch_size, 1)\n    fill_value, _ = top_indices.max(dim=1, keepdim=True)\n    # Shape: (batch_size, max_num_items_to_keep)\n    top_indices = torch.where(top_indices_mask, top_indices, fill_value)\n\n    # Now we order the selected indices in increasing order with\n    # respect to their indices (and hence, with respect to the\n    # order they originally appeared in the `embeddings` tensor).\n    top_indices, _ = top_indices.sort(1)\n\n    # Combine the masks on spans that are out-of-bounds, and the mask on spans that are outside\n    # the top k for each sentence.\n    # Shape: (batch_size, max_k)\n    sequence_mask = mask.gather(1, top_indices)\n    top_mask = top_indices_mask & sequence_mask\n\n    # Shape: (batch_size, max_k)\n    top_input = input_.gather(1, top_indices)\n\n    return (\n        top_input.reshape(*permuted_size).permute(*reverse_permutation),\n        top_mask.reshape(*permuted_size).permute(*reverse_permutation),\n        top_indices.reshape(*permuted_size).permute(*reverse_permutation),\n    )\n\n\ndef info_value_of_dtype(dtype: torch.dtype):\n    """"""\n    Returns the `finfo` or `iinfo` object of a given PyTorch data type. Does not allow torch.bool.\n    """"""\n    if dtype == torch.bool:\n        raise TypeError(""Does not support torch.bool"")\n    elif dtype.is_floating_point:\n        return torch.finfo(dtype)\n    else:\n        return torch.iinfo(dtype)\n\n\ndef min_value_of_dtype(dtype: torch.dtype):\n    """"""\n    Returns the minimum value of a given PyTorch data type. Does not allow torch.bool.\n    """"""\n    return info_value_of_dtype(dtype).min\n\n\ndef max_value_of_dtype(dtype: torch.dtype):\n    """"""\n    Returns the maximum value of a given PyTorch data type. Does not allow torch.bool.\n    """"""\n    return info_value_of_dtype(dtype).max\n\n\ndef tiny_value_of_dtype(dtype: torch.dtype):\n    """"""\n    Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical\n    issues such as division by zero.\n    This is different from `info_value_of_dtype(dtype).tiny` because it causes some NaN bugs.\n    Only supports floating point dtypes.\n    """"""\n    if not dtype.is_floating_point:\n        raise TypeError(""Only supports floating point dtypes."")\n    if dtype == torch.float or dtype == torch.double:\n        return 1e-13\n    elif dtype == torch.half:\n        return 1e-4\n    else:\n        raise TypeError(""Does not support dtype "" + str(dtype))\n'"
allennlp/predictors/__init__.py,0,"b'""""""\nA `Predictor` is\na wrapper for an AllenNLP `Model`\nthat makes JSON predictions using JSON inputs. If you\nwant to serve up a model through the web service\n(or using `allennlp.commands.predict`), you\'ll need\na `Predictor` that wraps it.\n""""""\nfrom allennlp.predictors.predictor import Predictor\nfrom allennlp.predictors.sentence_tagger import SentenceTaggerPredictor\nfrom allennlp.predictors.text_classifier import TextClassifierPredictor\n'"
allennlp/predictors/predictor.py,1,"b'from typing import List, Iterator, Dict, Tuple, Any, Type\nimport json\nfrom contextlib import contextmanager\n\nimport numpy\nfrom torch.utils.hooks import RemovableHandle\nfrom torch import Tensor\nfrom torch import backends\n\nfrom allennlp.common import Registrable, plugins\nfrom allennlp.common.util import JsonDict, sanitize\nfrom allennlp.data import DatasetReader, Instance\nfrom allennlp.data.batch import Batch\nfrom allennlp.models import Model\nfrom allennlp.models.archival import Archive, load_archive\nfrom allennlp.nn import util\n\n\nclass Predictor(Registrable):\n    """"""\n    a `Predictor` is a thin wrapper around an AllenNLP model that handles JSON -> JSON predictions\n    that can be used for serving models through the web API or making predictions in bulk.\n    """"""\n\n    def __init__(self, model: Model, dataset_reader: DatasetReader, frozen: bool = True) -> None:\n        if frozen:\n            model.eval()\n        self._model = model\n        self._dataset_reader = dataset_reader\n        self.cuda_device = next(self._model.named_parameters())[1].get_device()\n\n    def load_line(self, line: str) -> JsonDict:\n        """"""\n        If your inputs are not in JSON-lines format (e.g. you have a CSV)\n        you can override this function to parse them correctly.\n        """"""\n        return json.loads(line)\n\n    def dump_line(self, outputs: JsonDict) -> str:\n        """"""\n        If you don\'t want your outputs in JSON-lines format\n        you can override this function to output them differently.\n        """"""\n        return json.dumps(outputs) + ""\\n""\n\n    def predict_json(self, inputs: JsonDict) -> JsonDict:\n        instance = self._json_to_instance(inputs)\n        return self.predict_instance(instance)\n\n    def json_to_labeled_instances(self, inputs: JsonDict) -> List[Instance]:\n        """"""\n        Converts incoming json to a [`Instance`](../data/instance.md),\n        runs the model on the newly created instance, and adds labels to the\n        `Instance`s given by the model\'s output.\n\n        # Returns\n\n        `List[instance]`\n            A list of `Instance`\'s.\n        """"""\n\n        instance = self._json_to_instance(inputs)\n        outputs = self._model.forward_on_instance(instance)\n        new_instances = self.predictions_to_labeled_instances(instance, outputs)\n        return new_instances\n\n    def get_gradients(self, instances: List[Instance]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n        """"""\n        Gets the gradients of the loss with respect to the model inputs.\n\n        # Parameters\n\n        instances : `List[Instance]`\n\n        # Returns\n\n        `Tuple[Dict[str, Any], Dict[str, Any]]`\n            The first item is a Dict of gradient entries for each input.\n            The keys have the form  `{grad_input_1: ..., grad_input_2: ... }`\n            up to the number of inputs given. The second item is the model\'s output.\n\n        # Notes\n\n        Takes a `JsonDict` representing the inputs of the model and converts\n        them to [`Instances`](../data/instance.md)), sends these through\n        the model [`forward`](../models/model.md#forward) function after registering hooks on the embedding\n        layer of the model. Calls `backward` on the loss and then removes the\n        hooks.\n        """"""\n        # set requires_grad to true for all parameters, but save original values to\n        # restore them later\n        original_param_name_to_requires_grad_dict = {}\n        for param_name, param in self._model.named_parameters():\n            original_param_name_to_requires_grad_dict[param_name] = param.requires_grad\n            param.requires_grad = True\n\n        embedding_gradients: List[Tensor] = []\n        hooks: List[RemovableHandle] = self._register_embedding_gradient_hooks(embedding_gradients)\n\n        dataset = Batch(instances)\n        dataset.index_instances(self._model.vocab)\n        dataset_tensor_dict = util.move_to_device(dataset.as_tensor_dict(), self.cuda_device)\n        # To bypass ""RuntimeError: cudnn RNN backward can only be called in training mode""\n        with backends.cudnn.flags(enabled=False):\n            outputs = self._model.make_output_human_readable(\n                self._model.forward(**dataset_tensor_dict)  # type: ignore\n            )\n\n            loss = outputs[""loss""]\n            self._model.zero_grad()\n            loss.backward()\n\n        for hook in hooks:\n            hook.remove()\n\n        grad_dict = dict()\n        for idx, grad in enumerate(embedding_gradients):\n            key = ""grad_input_"" + str(idx + 1)\n            grad_dict[key] = grad.detach().cpu().numpy()\n\n        # restore the original requires_grad values of the parameters\n        for param_name, param in self._model.named_parameters():\n            param.requires_grad = original_param_name_to_requires_grad_dict[param_name]\n\n        return grad_dict, outputs\n\n    def _register_embedding_gradient_hooks(self, embedding_gradients):\n        """"""\n        Registers a backward hook on the\n        [`BasicTextFieldEmbedder`](../modules/text_field_embedders/basic_text_field_embedder.md)\n        class. Used to save the gradients of the embeddings for use in get_gradients()\n\n        When there are multiple inputs (e.g., a passage and question), the hook\n        will be called multiple times. We append all the embeddings gradients\n        to a list.\n        """"""\n\n        def hook_layers(module, grad_in, grad_out):\n            embedding_gradients.append(grad_out[0])\n\n        backward_hooks = []\n        embedding_layer = util.find_embedding_layer(self._model)\n        backward_hooks.append(embedding_layer.register_backward_hook(hook_layers))\n        return backward_hooks\n\n    @contextmanager\n    def capture_model_internals(self) -> Iterator[dict]:\n        """"""\n        Context manager that captures the internal-module outputs of\n        this predictor\'s model. The idea is that you could use it as follows:\n\n        ```\n            with predictor.capture_model_internals() as internals:\n                outputs = predictor.predict_json(inputs)\n\n            return {**outputs, ""model_internals"": internals}\n        ```\n        """"""\n        results = {}\n        hooks = []\n\n        # First we\'ll register hooks to add the outputs of each module to the results dict.\n        def add_output(idx: int):\n            def _add_output(mod, _, outputs):\n                results[idx] = {""name"": str(mod), ""output"": sanitize(outputs)}\n\n            return _add_output\n\n        for idx, module in enumerate(self._model.modules()):\n            if module != self._model:\n                hook = module.register_forward_hook(add_output(idx))\n                hooks.append(hook)\n\n        # If you capture the return value of the context manager, you get the results dict.\n        yield results\n\n        # And then when you exit the context we remove all the hooks.\n        for hook in hooks:\n            hook.remove()\n\n    def predict_instance(self, instance: Instance) -> JsonDict:\n        outputs = self._model.forward_on_instance(instance)\n        return sanitize(outputs)\n\n    def predictions_to_labeled_instances(\n        self, instance: Instance, outputs: Dict[str, numpy.ndarray]\n    ) -> List[Instance]:\n        """"""\n        This function takes a model\'s outputs for an Instance, and it labels that instance according\n        to the output. For example, in classification this function labels the instance according\n        to the class with the highest probability. This function is used to to compute gradients\n        of what the model predicted. The return type is a list because in some tasks there are\n        multiple predictions in the output (e.g., in NER a model predicts multiple spans). In this\n        case, each instance in the returned list of Instances contains an individual\n        entity prediction as the label.\n        """"""\n\n        raise RuntimeError(""implement this method for model interpretations or attacks"")\n\n    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n        """"""\n        Converts a JSON object into an [`Instance`](../data/instance.md)\n        and a `JsonDict` of information which the `Predictor` should pass through,\n        such as tokenised inputs.\n        """"""\n        raise NotImplementedError\n\n    def predict_batch_json(self, inputs: List[JsonDict]) -> List[JsonDict]:\n        instances = self._batch_json_to_instances(inputs)\n        return self.predict_batch_instance(instances)\n\n    def predict_batch_instance(self, instances: List[Instance]) -> List[JsonDict]:\n        outputs = self._model.forward_on_instances(instances)\n        return sanitize(outputs)\n\n    def _batch_json_to_instances(self, json_dicts: List[JsonDict]) -> List[Instance]:\n        """"""\n        Converts a list of JSON objects into a list of `Instance`s.\n        By default, this expects that a ""batch"" consists of a list of JSON blobs which would\n        individually be predicted by `predict_json`. In order to use this method for\n        batch prediction, `_json_to_instance` should be implemented by the subclass, or\n        if the instances have some dependency on each other, this method should be overridden\n        directly.\n        """"""\n        instances = []\n        for json_dict in json_dicts:\n            instances.append(self._json_to_instance(json_dict))\n        return instances\n\n    @classmethod\n    def from_path(\n        cls,\n        archive_path: str,\n        predictor_name: str = None,\n        cuda_device: int = -1,\n        dataset_reader_to_load: str = ""validation"",\n        frozen: bool = True,\n        import_plugins: bool = True,\n    ) -> ""Predictor"":\n        """"""\n        Instantiate a `Predictor` from an archive path.\n\n        If you need more detailed configuration options, such as overrides,\n        please use `from_archive`.\n\n        # Parameters\n\n        archive_path : `str`\n            The path to the archive.\n        predictor_name : `str`, optional (default=`None`)\n            Name that the predictor is registered as, or None to use the\n            predictor associated with the model.\n        cuda_device : `int`, optional (default=`-1`)\n            If `cuda_device` is >= 0, the model will be loaded onto the\n            corresponding GPU. Otherwise it will be loaded onto the CPU.\n        dataset_reader_to_load : `str`, optional (default=`""validation""`)\n            Which dataset reader to load from the archive, either ""train"" or\n            ""validation"".\n        frozen : `bool`, optional (default=`True`)\n            If we should call `model.eval()` when building the predictor.\n        import_plugins : `bool`, optional (default=`True`)\n            If `True`, we attempt to import plugins before loading the predictor.\n            This comes with additional overhead, but means you don\'t need to explicitly\n            import the modules that your predictor depends on as long as those modules\n            can be found by `allennlp.common.plugins.import_plugins()`.\n\n        # Returns\n\n        `Predictor`\n            A Predictor instance.\n        """"""\n        if import_plugins:\n            plugins.import_plugins()\n        return Predictor.from_archive(\n            load_archive(archive_path, cuda_device=cuda_device),\n            predictor_name,\n            dataset_reader_to_load=dataset_reader_to_load,\n            frozen=frozen,\n        )\n\n    @classmethod\n    def from_archive(\n        cls,\n        archive: Archive,\n        predictor_name: str = None,\n        dataset_reader_to_load: str = ""validation"",\n        frozen: bool = True,\n    ) -> ""Predictor"":\n        """"""\n        Instantiate a `Predictor` from an [`Archive`](../models/archival.md);\n        that is, from the result of training a model. Optionally specify which `Predictor`\n        subclass; otherwise, we try to find a corresponding predictor in `DEFAULT_PREDICTORS`, or if\n        one is not found, the base class (i.e. `Predictor`) will be used. Optionally specify\n        which [`DatasetReader`](../data/dataset_readers/dataset_reader.md) should be loaded;\n        otherwise, the validation one will be used if it exists followed by the training dataset reader.\n        Optionally specify if the loaded model should be frozen, meaning `model.eval()` will be called.\n        """"""\n        # Duplicate the config so that the config inside the archive doesn\'t get consumed\n        config = archive.config.duplicate()\n\n        if not predictor_name:\n            model_type = config.get(""model"").get(""type"")\n            model_class, _ = Model.resolve_class_name(model_type)\n            predictor_name = model_class.default_predictor\n        predictor_class: Type[Predictor] = Predictor.by_name(  # type: ignore\n            predictor_name\n        ) if predictor_name is not None else cls\n\n        if dataset_reader_to_load == ""validation"" and ""validation_dataset_reader"" in config:\n            dataset_reader_params = config[""validation_dataset_reader""]\n        else:\n            dataset_reader_params = config[""dataset_reader""]\n        dataset_reader = DatasetReader.from_params(dataset_reader_params)\n\n        model = archive.model\n        if frozen:\n            model.eval()\n\n        return predictor_class(model, dataset_reader)\n'"
allennlp/predictors/sentence_tagger.py,0,"b'from typing import List, Dict\n\nfrom overrides import overrides\nimport numpy\n\nfrom allennlp.common.util import JsonDict\nfrom allennlp.data import DatasetReader, Instance\nfrom allennlp.data.fields import FlagField, TextField, SequenceLabelField\nfrom allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\nfrom allennlp.models import Model\nfrom allennlp.predictors.predictor import Predictor\n\n\n@Predictor.register(""sentence_tagger"")\nclass SentenceTaggerPredictor(Predictor):\n    """"""\n    Predictor for any model that takes in a sentence and returns\n    a single set of tags for it.  In particular, it can be used with\n    the [`CrfTagger`](../models/crf_tagger.md) model\n    and also the [`SimpleTagger`](../models/simple_tagger.md) model.\n\n    Registered as a `Predictor` with name ""sentence_tagger"".\n    """"""\n\n    def __init__(\n        self, model: Model, dataset_reader: DatasetReader, language: str = ""en_core_web_sm""\n    ) -> None:\n        super().__init__(model, dataset_reader)\n        self._tokenizer = SpacyTokenizer(language=language, pos_tags=True)\n\n    def predict(self, sentence: str) -> JsonDict:\n        return self.predict_json({""sentence"": sentence})\n\n    @overrides\n    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n        """"""\n        Expects JSON that looks like `{""sentence"": ""...""}`.\n        Runs the underlying model, and adds the `""words""` to the output.\n        """"""\n        sentence = json_dict[""sentence""]\n        tokens = self._tokenizer.tokenize(sentence)\n        return self._dataset_reader.text_to_instance(tokens)\n\n    @overrides\n    def predictions_to_labeled_instances(\n        self, instance: Instance, outputs: Dict[str, numpy.ndarray]\n    ) -> List[Instance]:\n        """"""\n        This function currently only handles BIOUL tags.\n\n        Imagine an NER model predicts three named entities (each one with potentially\n        multiple tokens). For each individual entity, we create a new Instance that has\n        the label set to only that entity and the rest of the tokens are labeled as outside.\n        We then return a list of those Instances.\n\n        For example:\n\n        ```text\n        Mary  went to Seattle to visit Microsoft Research\n        U-Per  O    O   U-Loc  O   O     B-Org     L-Org\n        ```\n\n        We create three instances.\n\n        ```text\n        Mary  went to Seattle to visit Microsoft Research\n        U-Per  O    O    O     O   O       O         O\n\n        Mary  went to Seattle to visit Microsoft Research\n        O      O    O   U-LOC  O   O       O         O\n\n        Mary  went to Seattle to visit Microsoft Research\n        O      O    O    O     O   O     B-Org     L-Org\n        ```\n\n        We additionally add a flag to these instances to tell the model to only compute loss on\n        non-O tags, so that we get gradients that are specific to the particular span prediction\n        that each instance represents.\n        """"""\n        predicted_tags = outputs[""tags""]\n        predicted_spans = []\n\n        i = 0\n        while i < len(predicted_tags):\n            tag = predicted_tags[i]\n            # if its a U, add it to the list\n            if tag[0] == ""U"":\n                current_tags = [t if idx == i else ""O"" for idx, t in enumerate(predicted_tags)]\n                predicted_spans.append(current_tags)\n            # if its a B, keep going until you hit an L.\n            elif tag[0] == ""B"":\n                begin_idx = i\n                while tag[0] != ""L"":\n                    i += 1\n                    tag = predicted_tags[i]\n                end_idx = i\n                current_tags = [\n                    t if begin_idx <= idx <= end_idx else ""O""\n                    for idx, t in enumerate(predicted_tags)\n                ]\n                predicted_spans.append(current_tags)\n            i += 1\n\n        # Creates a new instance for each contiguous tag\n        instances = []\n        for labels in predicted_spans:\n            new_instance = instance.duplicate()\n            text_field: TextField = instance[""tokens""]  # type: ignore\n            new_instance.add_field(\n                ""tags"", SequenceLabelField(labels, text_field), self._model.vocab\n            )\n            new_instance.add_field(""ignore_loss_on_o_tags"", FlagField(True))\n            instances.append(new_instance)\n\n        return instances\n'"
allennlp/predictors/text_classifier.py,0,"b'from typing import List, Dict\n\nfrom overrides import overrides\nimport numpy\n\nfrom allennlp.common.util import JsonDict\nfrom allennlp.data import Instance\nfrom allennlp.predictors.predictor import Predictor\nfrom allennlp.data.fields import LabelField\nfrom allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\n\n\n@Predictor.register(""text_classifier"")\nclass TextClassifierPredictor(Predictor):\n    """"""\n    Predictor for any model that takes in a sentence and returns\n    a single class for it.  In particular, it can be used with\n    the [`BasicClassifier`](../models/basic_classifier.md) model.\n\n    Registered as a `Predictor` with name ""text_classifier"".\n    """"""\n\n    def predict(self, sentence: str) -> JsonDict:\n        return self.predict_json({""sentence"": sentence})\n\n    @overrides\n    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n        """"""\n        Expects JSON that looks like `{""sentence"": ""...""}`.\n        Runs the underlying model, and adds the `""label""` to the output.\n        """"""\n        sentence = json_dict[""sentence""]\n        if not hasattr(self._dataset_reader, ""tokenizer"") and not hasattr(\n            self._dataset_reader, ""_tokenizer""\n        ):\n            tokenizer = SpacyTokenizer()\n            sentence = [str(t) for t in tokenizer.tokenize(sentence)]\n        return self._dataset_reader.text_to_instance(sentence)\n\n    @overrides\n    def predictions_to_labeled_instances(\n        self, instance: Instance, outputs: Dict[str, numpy.ndarray]\n    ) -> List[Instance]:\n        new_instance = instance.duplicate()\n        label = numpy.argmax(outputs[""probs""])\n        new_instance.add_field(""label"", LabelField(int(label), skip_indexing=True))\n        return [new_instance]\n'"
allennlp/tools/__init__.py,0,b''
allennlp/tools/archive_surgery.py,0,"b'#! /usr/bin/env python\n""""""\nHelper script for modifying config.json files that are locked inside\nmodel.tar.gz archives. This is useful if you need to rename things or\nadd or remove values, usually because of changes to the library.\n\nThis script will untar the archive to a temp directory, launch an editor\nto modify the config.json, and then re-tar everything to a new archive.\nIf your $EDITOR environment variable is not set, you\'ll have to explicitly\nspecify which editor to use.\n""""""\n\nimport argparse\nimport atexit\nimport logging\nimport os\nimport shutil\nimport subprocess\nimport tempfile\nimport tarfile\n\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.models.archival import CONFIG_NAME\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.ERROR)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=""Perform surgery on a model.tar.gz archive"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    parser.add_argument(""--input-file"", required=True, help=""path to input file"")\n    parser.add_argument(\n        ""--editor"",\n        default=os.environ.get(""EDITOR""),\n        help=""editor to launch, whose default value is `$EDITOR` the environment variable"",\n    )\n    output = parser.add_mutually_exclusive_group()\n    output.add_argument(""--output-file"", help=""path to output file"")\n    output.add_argument(\n        ""--inplace"",\n        action=""store_true"",\n        help=""overwrite the input file with the modified configuration"",\n    )\n    parser.add_argument(\n        ""-f"", ""--force"", action=""store_true"", help=""overwrite the output file if it exists""\n    )\n\n    args = parser.parse_args()\n\n    if args.editor is None:\n        raise RuntimeError(""please specify an editor or set the $EDITOR environment variable"")\n\n    if not args.inplace and os.path.exists(args.output_file) and not args.force:\n        raise ValueError(""output file already exists, use --force to override"")\n\n    archive_file = cached_path(args.input_file)\n    if not os.path.exists(archive_file):\n        raise ValueError(""input file doesn\'t exist"")\n    if args.inplace:\n        output_file = archive_file\n    else:\n        output_file = args.output_file\n\n    # Extract archive to temp dir\n    tempdir = tempfile.mkdtemp()\n    with tarfile.open(archive_file, ""r:gz"") as archive:\n        archive.extractall(tempdir)\n    atexit.register(lambda: shutil.rmtree(tempdir))\n\n    config_path = os.path.join(tempdir, CONFIG_NAME)\n    subprocess.run([args.editor, config_path], check=False)\n\n    with tarfile.open(output_file, ""w:gz"") as tar:\n        tar.add(tempdir, arcname=os.path.sep)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
allennlp/tools/create_elmo_embeddings_from_vocab.py,2,"b'import argparse\nimport gzip\nimport os\n\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data import Token, Vocabulary\nfrom allennlp.data.token_indexers import ELMoTokenCharactersIndexer\nfrom allennlp.data.vocabulary import DEFAULT_OOV_TOKEN\nfrom allennlp.modules.elmo import _ElmoCharacterEncoder\n\n\ndef main(\n    vocab_path: str,\n    elmo_config_path: str,\n    elmo_weights_path: str,\n    output_dir: str,\n    batch_size: int,\n    device: int,\n    use_custom_oov_token: bool = False,\n):\n    """"""\n    Creates ELMo word representations from a vocabulary file. These\n    word representations are _independent_ - they are the result of running\n    the CNN and Highway layers of the ELMo model, but not the Bidirectional LSTM.\n    ELMo requires 2 additional tokens: <S> and </S>. The first token\n    in this file is assumed to be an unknown token.\n\n    This script produces two artifacts: A new vocabulary file\n    with the <S> and </S> tokens inserted and a glove formatted embedding\n    file containing word : vector pairs, one per line, with all values\n    separated by a space.\n    """"""\n\n    # Load the vocabulary words and convert to char ids\n    with open(vocab_path, ""r"") as vocab_file:\n        tokens = vocab_file.read().strip().split(""\\n"")\n\n    # Insert the sentence boundary tokens which elmo uses at positions 1 and 2.\n    if tokens[0] != DEFAULT_OOV_TOKEN and not use_custom_oov_token:\n        raise ConfigurationError(""ELMo embeddings require the use of a OOV token."")\n\n    tokens = [tokens[0]] + [""<S>"", ""</S>""] + tokens[1:]\n\n    indexer = ELMoTokenCharactersIndexer()\n    indices = indexer.tokens_to_indices([Token(token) for token in tokens], Vocabulary())[""tokens""]\n    sentences = []\n    for k in range((len(indices) // 50) + 1):\n        sentences.append(\n            indexer.as_padded_tensor_dict(\n                indices[(k * 50) : ((k + 1) * 50)], padding_lengths={""tokens"": 50}\n            )\n        )\n\n    last_batch_remainder = 50 - (len(indices) % 50)\n    if device != -1:\n        elmo_token_embedder = _ElmoCharacterEncoder(elmo_config_path, elmo_weights_path).cuda(\n            device\n        )\n    else:\n        elmo_token_embedder = _ElmoCharacterEncoder(elmo_config_path, elmo_weights_path)\n\n    all_embeddings = []\n    for i in range((len(sentences) // batch_size) + 1):\n        batch = torch.stack(sentences[i * batch_size : (i + 1) * batch_size])\n        if device != -1:\n            batch = batch.cuda(device)\n\n        token_embedding = elmo_token_embedder(batch)[""token_embedding""].data\n\n        # Reshape back to a list of words of shape (batch_size * 50, encoding_dim)\n        # We also need to remove the <S>, </S> tokens appended by the encoder.\n        per_word_embeddings = (\n            token_embedding[:, 1:-1, :].contiguous().view(-1, token_embedding.size(-1))\n        )\n\n        all_embeddings.append(per_word_embeddings)\n\n    # Remove the embeddings associated with padding in the last batch.\n    all_embeddings[-1] = all_embeddings[-1][:-last_batch_remainder, :]\n\n    embedding_weight = torch.cat(all_embeddings, 0).cpu().numpy()\n\n    # Write out the embedding in a glove format.\n    os.makedirs(output_dir, exist_ok=True)\n    with gzip.open(os.path.join(output_dir, ""elmo_embeddings.txt.gz""), ""wb"") as embeddings_file:\n        for i, word in enumerate(tokens):\n            string_array = "" "".join(str(x) for x in list(embedding_weight[i, :]))\n            embeddings_file.write(f""{word} {string_array}\\n"".encode(""utf-8""))\n\n    # Write out the new vocab with the <S> and </S> tokens.\n    _, vocab_file_name = os.path.split(vocab_path)\n    with open(os.path.join(output_dir, vocab_file_name), ""w"") as new_vocab_file:\n        for word in tokens:\n            new_vocab_file.write(f""{word}\\n"")\n\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser(\n        description=""Generate CNN representations for a vocabulary using ELMo"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--vocab_path"",\n        type=str,\n        help=""A path to a vocabulary file to generate representations for."",\n    )\n    parser.add_argument(\n        ""--elmo_config"", type=str, help=""The path to a directory containing an ELMo config file.""\n    )\n    parser.add_argument(\n        ""--elmo_weights"", type=str, help=""The path to a directory containing an ELMo weight file.""\n    )\n    parser.add_argument(\n        ""--output_dir"", type=str, help=""The output directory to store the serialised embeddings.""\n    )\n    parser.add_argument(""--batch_size"", type=int, default=64, help=""The batch size to use."")\n    parser.add_argument(""--device"", type=int, default=-1, help=""The device to run on."")\n    parser.add_argument(\n        ""--use_custom_oov_token"",\n        type=bool,\n        default=False,\n        help=""AllenNLP requires a particular OOV token.""\n        ""To generate embeddings with a custom OOV token,""\n        ""add this flag."",\n    )\n\n    args = parser.parse_args()\n    main(\n        args.vocab_path,\n        args.elmo_config,\n        args.elmo_weights,\n        args.output_dir,\n        args.batch_size,\n        args.device,\n        args.use_custom_oov_token,\n    )\n'"
allennlp/tools/inspect_cache.py,0,"b'import os\n\nfrom allennlp.common.file_utils import CACHE_DIRECTORY\nfrom allennlp.common.file_utils import filename_to_url\n\n\ndef main():\n    print(f""Looking for datasets in {CACHE_DIRECTORY}..."")\n    if not os.path.exists(CACHE_DIRECTORY):\n        print(""Directory does not exist."")\n        print(""No cached datasets found."")\n\n    cached_files = os.listdir(CACHE_DIRECTORY)\n\n    if not cached_files:\n        print(""Directory is empty."")\n        print(""No cached datasets found."")\n\n    for filename in cached_files:\n        if not filename.endswith(""json""):\n            url, etag = filename_to_url(filename)\n            print(""Filename: %s"" % filename)\n            print(""Url: %s"" % url)\n            print(""ETag: %s"" % etag)\n            print()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
allennlp/training/__init__.py,0,"b'from allennlp.training.checkpointer import Checkpointer\nfrom allennlp.training.tensorboard_writer import TensorboardWriter\nfrom allennlp.training.no_op_trainer import NoOpTrainer\nfrom allennlp.training.trainer import Trainer, GradientDescentTrainer, BatchCallback, EpochCallback\n'"
allennlp/training/checkpointer.py,7,"b'from typing import Union, Dict, Any, List, Tuple\n\nimport logging\nimport os\nimport re\nimport shutil\nimport time\n\nimport torch\n\nimport allennlp\nfrom allennlp.common import Registrable\nfrom allennlp.nn import util as nn_util\nfrom allennlp.training import util as training_util\n\nlogger = logging.getLogger(__name__)\n\n\nclass Checkpointer(Registrable):\n    """"""\n    This class implements the functionality for checkpointing your model and trainer state\n    during training. It is agnostic as to what those states look like (they are typed as\n    Dict[str, Any]), but they will be fed to `torch.save` so they should be serializable\n    in that sense. They will also be restored as Dict[str, Any], which means the calling\n    code is responsible for knowing what to do with them.\n\n    # Parameters\n\n    num_serialized_models_to_keep : `int`, optional (default=`2`)\n        Number of previous model checkpoints to retain.  Default is to keep 2 checkpoints.\n        A value of None or -1 means all checkpoints will be kept.\n\n        In a typical AllenNLP configuration file, this argument does not get an entry under the\n        ""checkpointer"", it gets passed in separately.\n    keep_serialized_model_every_num_seconds : `int`, optional (default=`None`)\n        If num_serialized_models_to_keep is not None, then occasionally it\'s useful to\n        save models at a given interval in addition to the last num_serialized_models_to_keep.\n        To do so, specify keep_serialized_model_every_num_seconds as the number of seconds\n        between permanently saved checkpoints.  Note that this option is only used if\n        num_serialized_models_to_keep is not None, otherwise all checkpoints are kept.\n    model_save_interval : `float`, optional (default=`None`)\n        If provided, then serialize models every `model_save_interval`\n        seconds within single epochs.  In all cases, models are also saved\n        at the end of every epoch if `serialization_dir` is provided.\n    """"""\n\n    default_implementation = ""default""\n\n    def __init__(\n        self,\n        serialization_dir: str = None,\n        keep_serialized_model_every_num_seconds: int = None,\n        num_serialized_models_to_keep: int = 2,\n        model_save_interval: float = None,\n    ) -> None:\n        self._serialization_dir = serialization_dir\n        self._keep_serialized_model_every_num_seconds = keep_serialized_model_every_num_seconds\n        self._num_serialized_models_to_keep = num_serialized_models_to_keep\n        self._model_save_interval = model_save_interval\n\n        self._last_permanent_saved_checkpoint_time = time.time()\n        self._serialized_paths: List[Tuple[float, str, str]] = []\n        self._last_save_time = time.time()\n\n    def maybe_save_checkpoint(\n        self, trainer: ""allennlp.training.trainer.Trainer"", epoch: int, batches_this_epoch: int\n    ) -> None:\n        """"""\n        Given amount of time lapsed between the last save and now (tracked internally), the\n        current epoch, and the number of batches seen so far this epoch, this method decides whether\n        to save a checkpoint or not.  If we decide to save a checkpoint, we grab whatever state we\n        need out of the `Trainer` and save it.\n\n        This function is intended to be called at the end of each batch in an epoch (perhaps because\n        your data is large enough that you don\'t really have ""epochs"").  The default implementation\n        only looks at time, not batch or epoch number, though those parameters are available to you\n        if you want to customize the behavior of this function.\n        """"""\n        if self._model_save_interval is None:\n            return\n        if time.time() - self._last_save_time < self._model_save_interval:\n            return\n\n        self._last_save_time = time.time()\n        epoch_str = f""{epoch}.{training_util.time_to_str(int(self._last_save_time))}""\n        self.save_checkpoint(epoch_str, trainer)\n\n    def save_checkpoint(\n        self,\n        epoch: Union[int, str],\n        trainer: ""allennlp.training.trainer.Trainer"",\n        is_best_so_far: bool = False,\n    ) -> None:\n        if self._serialization_dir is not None:\n            with trainer.get_checkpoint_state() as state:\n                model_state, training_states = state\n                model_path = os.path.join(\n                    self._serialization_dir, ""model_state_epoch_{}.th"".format(epoch)\n                )\n                torch.save(model_state, model_path)\n                training_path = os.path.join(\n                    self._serialization_dir, ""training_state_epoch_{}.th"".format(epoch)\n                )\n                torch.save({**training_states, ""epoch"": epoch}, training_path)\n\n            # The main checkpointing logic is now done, this is just shuffling files around, to keep\n            # track of best weights, and to remove old checkpoints, if desired.\n            if is_best_so_far:\n                logger.info(\n                    ""Best validation performance so far. Copying weights to \'%s/best.th\'."",\n                    self._serialization_dir,\n                )\n                shutil.copyfile(model_path, os.path.join(self._serialization_dir, ""best.th""))\n\n            if (\n                self._num_serialized_models_to_keep is not None\n                and self._num_serialized_models_to_keep >= 0\n            ):\n                self._serialized_paths.append((time.time(), model_path, training_path))\n                if len(self._serialized_paths) > self._num_serialized_models_to_keep:\n                    paths_to_remove = self._serialized_paths.pop(0)\n                    # Check to see if we should keep this checkpoint, if it has been longer\n                    # then self._keep_serialized_model_every_num_seconds since the last\n                    # kept checkpoint.\n                    remove_path = True\n                    if self._keep_serialized_model_every_num_seconds is not None:\n                        save_time = paths_to_remove[0]\n                        time_since_checkpoint_kept = (\n                            save_time - self._last_permanent_saved_checkpoint_time\n                        )\n                        if (\n                            time_since_checkpoint_kept\n                            > self._keep_serialized_model_every_num_seconds\n                        ):\n                            # We want to keep this checkpoint.\n                            remove_path = False\n                            self._last_permanent_saved_checkpoint_time = save_time\n                    if remove_path:\n                        for fname in paths_to_remove[1:]:\n                            if os.path.isfile(fname):\n                                os.remove(fname)\n\n    def find_latest_checkpoint(self) -> Tuple[str, str]:\n        """"""\n        Return the location of the latest model and training state files.\n        If there isn\'t a valid checkpoint then return None.\n        """"""\n        have_checkpoint = self._serialization_dir is not None and any(\n            ""model_state_epoch_"" in x for x in os.listdir(self._serialization_dir)\n        )\n\n        if not have_checkpoint:\n            return None\n\n        serialization_files = os.listdir(self._serialization_dir)\n        model_checkpoints = [x for x in serialization_files if ""model_state_epoch"" in x]\n        # Get the last checkpoint file.  Epochs are specified as either an\n        # int (for end of epoch files) or with epoch and timestamp for\n        # within epoch checkpoints, e.g. 5.2018-02-02-15-33-42\n        found_epochs = [\n            re.search(r""model_state_epoch_([0-9\\.\\-]+)\\.th"", x).group(1) for x in model_checkpoints\n        ]\n        int_epochs: Any = []\n        for epoch in found_epochs:\n            pieces = epoch.split(""."")\n            if len(pieces) == 1:\n                # Just a single epoch without timestamp\n                int_epochs.append([int(pieces[0]), ""0""])\n            else:\n                # has a timestamp\n                int_epochs.append([int(pieces[0]), pieces[1]])\n        last_epoch = sorted(int_epochs, reverse=True)[0]\n        if last_epoch[1] == ""0"":\n            epoch_to_load = str(last_epoch[0])\n        else:\n            epoch_to_load = ""{0}.{1}"".format(last_epoch[0], last_epoch[1])\n\n        model_path = os.path.join(\n            self._serialization_dir, ""model_state_epoch_{}.th"".format(epoch_to_load)\n        )\n        training_state_path = os.path.join(\n            self._serialization_dir, ""training_state_epoch_{}.th"".format(epoch_to_load)\n        )\n\n        return (model_path, training_state_path)\n\n    def restore_checkpoint(self) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n        """"""\n        Restores a model from a serialization_dir to the last saved checkpoint.\n        This includes a training state (typically consisting of an epoch count and optimizer state),\n        which is serialized separately from  model parameters. This function should only be used to\n        continue training - if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(""/path/to/model/weights.th""))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return empty dicts.\n\n        # Returns\n\n        states : `Tuple[Dict[str, Any], Dict[str, Any]]`\n            The model state and the training state.\n        """"""\n        latest_checkpoint = self.find_latest_checkpoint()\n\n        if latest_checkpoint is None:\n            # No checkpoint to restore, start at 0\n            return {}, {}\n\n        model_path, training_state_path = latest_checkpoint\n\n        # Load the parameters onto CPU, then transfer to GPU.\n        # This avoids potential OOM on GPU for large models that\n        # load parameters onto GPU then make a new GPU copy into the parameter\n        # buffer. The GPU transfer happens implicitly in load_state_dict.\n        model_state = torch.load(model_path, map_location=nn_util.device_mapping(-1))\n        training_state = torch.load(training_state_path, map_location=nn_util.device_mapping(-1))\n        return model_state, training_state\n\n    def best_model_state(self) -> Dict[str, Any]:\n        if self._serialization_dir:\n            logger.info(""loading best weights"")\n            best_model_state_path = os.path.join(self._serialization_dir, ""best.th"")\n            return torch.load(best_model_state_path, map_location=nn_util.device_mapping(-1))\n        else:\n            logger.info(\n                ""cannot load best weights without `serialization_dir`, ""\n                ""so you\'re just getting the last weights""\n            )\n            return {}\n\n\nCheckpointer.register(""default"")(Checkpointer)\n'"
allennlp/training/metric_tracker.py,0,"b'from typing import Optional, Iterable, Dict, Any\n\nfrom allennlp.common.checks import ConfigurationError\n\n\nclass MetricTracker:\n    """"""\n    This class tracks a metric during training for the dual purposes of early stopping\n    and for knowing whether the current value is the best so far. It mimics the PyTorch\n    `state_dict` / `load_state_dict` interface, so that it can be checkpointed along with\n    your model and optimizer.\n\n    Some metrics improve by increasing; others by decreasing. Here you can either explicitly\n    supply `should_decrease`, or you can provide a `metric_name` in which case ""should decrease""\n    is inferred from the first character, which must be ""+"" or ""-"".\n\n    # Parameters\n\n    patience : `int`, optional (default = `None`)\n        If provided, then `should_stop_early()` returns True if we go this\n        many epochs without seeing a new best value.\n    metric_name : `str`, optional (default = `None`)\n        If provided, it\'s used to infer whether we expect the metric values to\n        increase (if it starts with ""+"") or decrease (if it starts with ""-"").\n        It\'s an error if it doesn\'t start with one of those. If it\'s not provided,\n        you should specify `should_decrease` instead.\n    should_decrease : `str`, optional (default = `None`)\n        If `metric_name` isn\'t provided (in which case we can\'t infer `should_decrease`),\n        then you have to specify it here.\n    """"""\n\n    def __init__(\n        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None\n    ) -> None:\n        self._best_so_far: float = None\n        self._patience = patience\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self.best_epoch_metrics: Dict[str, float] = {}\n        self._epoch_number = 0\n        self.best_epoch: int = None\n\n        # If the metric name starts with ""+"", we want it to increase.\n        # If the metric name starts with ""-"", we want it to decrease.\n        # We also allow you to not specify a metric name and just set `should_decrease` directly.\n        if should_decrease is None and metric_name is None:\n            raise ConfigurationError(\n                ""must specify either `should_decrease` or `metric_name` (but not both)""\n            )\n        elif should_decrease is not None and metric_name is not None:\n            raise ConfigurationError(\n                ""must specify either `should_decrease` or `metric_name` (but not both)""\n            )\n        elif metric_name is not None:\n            if metric_name[0] == ""-"":\n                self._should_decrease = True\n            elif metric_name[0] == ""+"":\n                self._should_decrease = False\n            else:\n                raise ConfigurationError(""metric_name must start with + or -"")\n        else:\n            self._should_decrease = should_decrease\n\n    def clear(self) -> None:\n        """"""\n        Clears out the tracked metrics, but keeps the patience and should_decrease settings.\n        """"""\n        self._best_so_far = None\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self._epoch_number = 0\n        self.best_epoch = None\n\n    def state_dict(self) -> Dict[str, Any]:\n        """"""\n        A `Trainer` can use this to serialize the state of the metric tracker.\n        """"""\n        return {\n            ""best_so_far"": self._best_so_far,\n            ""patience"": self._patience,\n            ""epochs_with_no_improvement"": self._epochs_with_no_improvement,\n            ""is_best_so_far"": self._is_best_so_far,\n            ""should_decrease"": self._should_decrease,\n            ""best_epoch_metrics"": self.best_epoch_metrics,\n            ""epoch_number"": self._epoch_number,\n            ""best_epoch"": self.best_epoch,\n        }\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        """"""\n        A `Trainer` can use this to hydrate a metric tracker from a serialized state.\n        """"""\n        self._best_so_far = state_dict[""best_so_far""]\n        self._patience = state_dict[""patience""]\n        self._epochs_with_no_improvement = state_dict[""epochs_with_no_improvement""]\n        self._is_best_so_far = state_dict[""is_best_so_far""]\n        self._should_decrease = state_dict[""should_decrease""]\n        self.best_epoch_metrics = state_dict[""best_epoch_metrics""]\n        self._epoch_number = state_dict[""epoch_number""]\n        self.best_epoch = state_dict[""best_epoch""]\n\n    def add_metric(self, metric: float) -> None:\n        """"""\n        Record a new value of the metric and update the various things that depend on it.\n        """"""\n        new_best = (\n            (self._best_so_far is None)\n            or (self._should_decrease and metric < self._best_so_far)\n            or (not self._should_decrease and metric > self._best_so_far)\n        )\n\n        if new_best:\n            self.best_epoch = self._epoch_number\n            self._is_best_so_far = True\n            self._best_so_far = metric\n            self._epochs_with_no_improvement = 0\n        else:\n            self._is_best_so_far = False\n            self._epochs_with_no_improvement += 1\n        self._epoch_number += 1\n\n    def add_metrics(self, metrics: Iterable[float]) -> None:\n        """"""\n        Helper to add multiple metrics at once.\n        """"""\n        for metric in metrics:\n            self.add_metric(metric)\n\n    def is_best_so_far(self) -> bool:\n        """"""\n        Returns true if the most recent value of the metric is the best so far.\n        """"""\n        return self._is_best_so_far\n\n    def should_stop_early(self) -> bool:\n        """"""\n        Returns true if improvement has stopped for long enough.\n        """"""\n        if self._patience is None:\n            return False\n        else:\n            return self._epochs_with_no_improvement >= self._patience\n'"
allennlp/training/moving_average.py,1,"b'from typing import Iterable, Tuple, Optional\n\nimport torch\n\nfrom allennlp.common.registrable import Registrable\n\nNamedParameter = Tuple[str, torch.Tensor]\n\n\nclass MovingAverage(Registrable):\n    """"""\n    Tracks a moving average of model parameters.\n    """"""\n\n    default_implementation = ""exponential""\n\n    def __init__(self, parameters: Iterable[NamedParameter]) -> None:\n        self._parameters = list(parameters)\n        self._shadows = {name: parameter.data.clone() for name, parameter in self._parameters}\n        self._backups = {name: parameter.data.clone() for name, parameter in self._parameters}\n\n    def apply(self, num_updates: Optional[int] = None):\n        """"""\n        Update the moving averages based on the latest values of the parameters.\n        """"""\n        raise NotImplementedError\n\n    def assign_average_value(self) -> None:\n        """"""\n        Replace all the parameter values with the averages.\n        Save the current parameter values to restore later.\n        """"""\n        for name, parameter in self._parameters:\n            self._backups[name].copy_(parameter.data)\n            parameter.data.copy_(self._shadows[name])\n\n    def restore(self) -> None:\n        """"""\n        Restore the backed-up (non-average) parameter values.\n        """"""\n        for name, parameter in self._parameters:\n            parameter.data.copy_(self._backups[name])\n\n\n@MovingAverage.register(""exponential"")\nclass ExponentialMovingAverage(MovingAverage):\n    """"""\n    Create shadow variables and maintain exponential moving average for model parameters.\n\n    Registered as a `MovingAverage` with name ""exponential"".\n\n    # Parameters\n\n    parameters : `Iterable[Tuple[str, Parameter]]`, required\n        The parameters whose averages we\'ll be tracking.\n\n        In a typical AllenNLP configuration file, this argument does not get an entry under the\n        ""moving_average"", it gets passed in separately.\n    decay : `float`, optional (default = `0.9999`)\n        The decay rate that will be used if `num_updates` is not passed\n        (and that will be used as an upper bound if `num_updates` is passed).\n    numerator : `float`, optional (default = `1.0`)\n        The numerator used to compute the decay rate if `num_updates` is passed.\n    denominator : `float`, optional (default = `10.0`)\n        The denominator used to compute the decay rate if `num_updates` is passed.\n    """"""\n\n    def __init__(\n        self,\n        parameters: Iterable[NamedParameter],\n        decay: float = 0.9999,\n        numerator: float = 1.0,\n        denominator: float = 10.0,\n    ) -> None:\n        super().__init__(parameters)\n        self._decay = decay\n        self._numerator = numerator\n        self._denominator = denominator\n\n    def apply(self, num_updates: Optional[int] = None) -> None:\n        """"""\n        Apply exponential moving average to `named_parameters` if specified,\n        or we will apply this to all the trainable parameters of the model.\n\n        The optional `num_updates` parameter allows one to tweak the decay rate\n        dynamically. If passed, the actual decay rate used is:\n\n            `min(decay, (numerator + num_updates) / (denominator + num_updates))`\n\n        (This logic is based on the Tensorflow exponential moving average\n         <https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage>)\n        """"""\n        if num_updates is not None:\n            decay = min(\n                self._decay, (self._numerator + num_updates) / (self._denominator + num_updates)\n            )\n        else:\n            decay = self._decay\n\n        for name, parameter in self._parameters:\n            self._shadows[name].mul_(decay).add_((1 - decay) * parameter.data)\n'"
allennlp/training/no_op_trainer.py,0,"b'import os\nfrom contextlib import contextmanager\nfrom typing import Any, Dict, Iterator, Tuple\n\nfrom allennlp.models import Model\nfrom allennlp.training.checkpointer import Checkpointer\nfrom allennlp.training.trainer import Trainer\n\n\n@Trainer.register(""no_op"")\nclass NoOpTrainer(Trainer):\n    """"""\n    Registered as a `Trainer` with name ""no_op"".\n    """"""\n\n    def __init__(self, serialization_dir: str, model: Model) -> None:\n        """"""\n        A trivial trainer to assist in making model archives for models that do not actually\n        require training. For instance, a majority class baseline.\n\n        In a typical AllenNLP configuration file, neither the `serialization_dir` nor the `model`\n        arguments would need an entry.\n        """"""\n\n        super().__init__(serialization_dir, cuda_device=-1)\n        self.model = model\n\n    def train(self) -> Dict[str, Any]:\n        self.model.vocab.save_to_files(os.path.join(self._serialization_dir, ""vocabulary""))\n\n        checkpointer = Checkpointer(self._serialization_dir)\n        checkpointer.save_checkpoint(epoch=0, trainer=self, is_best_so_far=True)\n        return {}\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        yield self.model.state_dict(), {}\n'"
allennlp/training/optimizers.py,41,"b'""""""\nAllenNLP just uses\n[PyTorch optimizers](https://pytorch.org/docs/master/optim.html),\nwith a thin wrapper to allow registering them and instantiating them `from_params`.\n\nThe available optimizers are\n\n* [adadelta](https://pytorch.org/docs/master/optim.html#torch.optim.Adadelta)\n* [adagrad](https://pytorch.org/docs/master/optim.html#torch.optim.Adagrad)\n* [adam](https://pytorch.org/docs/master/optim.html#torch.optim.Adam)\n* [adamw](https://pytorch.org/docs/master/optim.html#torch.optim.AdamW)\n* [huggingface_adamw](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.AdamW)\n* [sparse_adam](https://pytorch.org/docs/master/optim.html#torch.optim.SparseAdam)\n* [sgd](https://pytorch.org/docs/master/optim.html#torch.optim.SGD)\n* [rmsprop](https://pytorch.org/docs/master/optim.html#torch.optim.RMSprop)\n* [adamax](https://pytorch.org/docs/master/optim.html#torch.optim.Adamax)\n* [averaged_sgd](https://pytorch.org/docs/master/optim.html#torch.optim.ASGD)\n""""""\n\nimport logging\nimport re\nimport math\nfrom typing import Any, Dict, List, Tuple, Union\n\nimport torch\nimport transformers\n\nfrom allennlp.common import Params, Registrable\n\nlogger = logging.getLogger(__name__)\n\n\ndef make_parameter_groups(\n    model_parameters: List[Tuple[str, torch.nn.Parameter]],\n    groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n) -> Union[List[Dict[str, Any]], List[torch.nn.Parameter]]:\n    """"""\n    Takes a list of model parameters with associated names (typically coming from something like\n    `model.parameters`), along with a grouping (as specified below), and prepares them to be passed\n    to the `__init__` function of a `torch.Optimizer`.  This means separating the parameters into\n    groups with the given regexes, and prepping whatever keyword arguments are given for those\n    regexes in `groups`.\n\n    `groups` contains something like:\n\n    ```\n    [\n        ([""regex1"", ""regex2""], {""lr"": 1e-3}),\n        ([""regex3""], {""lr"": 1e-4})\n    ]\n    ```\n\n    The return value in the right format to be passed directly as the `params` argument to a pytorch\n    `Optimizer`.  If there are multiple groups specified, this is list of dictionaries, where each\n    dict contains a ""parameter group"" and groups specific options, e.g., {\'params\': [list of\n    parameters], \'lr\': 1e-3, ...}.  Any config option not specified in the additional options (e.g.\n    for the default group) is inherited from the top level arguments given in the constructor.  See:\n    <https://pytorch.org/docs/0.3.0/optim.html?#per-parameter-options>.  See also our\n    `test_optimizer_parameter_groups` test for an example of how this works in this code.\n\n    The dictionary\'s return type is labeled as `Any`, because it can be a `List[torch.nn.Parameter]`\n    (for the ""params"" key), or anything else (typically a float) for the other keys.\n    """"""\n    if groups:\n        # In addition to any parameters that match group specific regex,\n        # we also need a group for the remaining ""default"" group.\n        # Those will be included in the last entry of parameter_groups.\n        parameter_groups: Union[List[Dict[str, Any]], List[torch.nn.Parameter]] = [\n            {""params"": []} for _ in range(len(groups) + 1)\n        ]\n        # add the group specific kwargs\n        for k in range(len(groups)):\n            parameter_groups[k].update(groups[k][1])\n\n        regex_use_counts: Dict[str, int] = {}\n        parameter_group_names: List[set] = [set() for _ in range(len(groups) + 1)]\n        for name, param in model_parameters:\n            # Determine the group for this parameter.\n            group_index = None\n            for k, group_regexes in enumerate(groups):\n                for regex in group_regexes[0]:\n                    if regex not in regex_use_counts:\n                        regex_use_counts[regex] = 0\n                    if re.search(regex, name):\n                        if group_index is not None and group_index != k:\n                            raise ValueError(\n                                ""{} was specified in two separate parameter groups"".format(name)\n                            )\n                        group_index = k\n                        regex_use_counts[regex] += 1\n\n            if group_index is not None:\n                parameter_groups[group_index][""params""].append(param)\n                parameter_group_names[group_index].add(name)\n            else:\n                # the default group\n                parameter_groups[-1][""params""].append(param)\n                parameter_group_names[-1].add(name)\n\n        # log the parameter groups\n        logger.info(""Done constructing parameter groups."")\n        for k in range(len(groups) + 1):\n            group_options = {\n                key: val for key, val in parameter_groups[k].items() if key != ""params""\n            }\n            logger.info(""Group %s: %s, %s"", k, list(parameter_group_names[k]), group_options)\n        # check for unused regex\n        for regex, count in regex_use_counts.items():\n            if count == 0:\n                logger.warning(\n                    ""When constructing parameter groups, %s does not match any parameter name"",\n                    regex,\n                )\n\n    else:\n        parameter_groups = [param for name, param in model_parameters]\n\n    # Log the number of parameters to optimize\n    num_parameters = 0\n    for parameter_group in parameter_groups:\n        if isinstance(parameter_group, dict):\n            num_parameters += sum(parameter.numel() for parameter in parameter_group[""params""])\n        else:\n            num_parameters += parameter_group.numel()  # type: ignore\n    logger.info(""Number of trainable parameters: %s"", num_parameters)\n    return parameter_groups\n\n\nclass Optimizer(Registrable):\n    """"""\n    This class just allows us to implement `Registrable` for Pytorch Optimizers.  We do something a\n    little bit different with `Optimizers`, because they are implemented as classes in PyTorch, and\n    we want to use those classes.  To make things easy, we just inherit from those classes, using\n    multiple inheritance to also inherit from `Optimizer`.  The only reason we do this is to make\n    type inference on parameters possible, so we can construct these objects using our configuration\n    framework.  If you are writing your own script, you can safely ignore these classes and just use\n    the `torch.optim` classes directly.\n\n    If you are implementing one of these classes, the `model_parameters` and `parameter_groups`\n    arguments to `__init__` are important, and should always be present.  The trainer will pass\n    the trainable parameters in the model to the optimizer using the name `model_parameters`, so if\n    you use a different name, your code will crash.  Nothing will technically crash if you use a\n    name other than `parameter_groups` for your second argument, it will just be annoyingly\n    inconsistent.\n\n    Most subclasses of `Optimizer` take both a `model_parameters` and a `parameter_groups`\n    constructor argument.  The `model_parameters` argument does not get an entry in a typical\n    AllenNLP configuration file, but the `parameter_groups` argument does (if you want a non-default\n    value).  See the documentation for the `make_parameter_groups` function for more information on\n    how the `parameter_groups` argument should be specified.\n    """"""\n\n    default_implementation = ""adam""\n\n    @staticmethod\n    def default(model_parameters: List) -> ""Optimizer"":\n        return Optimizer.from_params(model_parameters=model_parameters, params=Params({}))\n\n\n@Optimizer.register(""adam"")\nclass AdamOptimizer(Optimizer, torch.optim.Adam):\n    """"""\n    Registered as an `Optimizer` with name ""adam"".\n    """"""\n\n    def __init__(\n        self,\n        model_parameters: List[Tuple[str, torch.nn.Parameter]],\n        parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n        lr: float = 0.001,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-08,\n        weight_decay: float = 0.0,\n        amsgrad: bool = False,\n    ):\n        super().__init__(\n            params=make_parameter_groups(model_parameters, parameter_groups),\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            amsgrad=amsgrad,\n        )\n\n\n@Optimizer.register(""sparse_adam"")\nclass SparseAdamOptimizer(Optimizer, torch.optim.SparseAdam):\n    """"""\n    Registered as an `Optimizer` with name ""sparse_adam"".\n    """"""\n\n    def __init__(\n        self,\n        model_parameters: List[Tuple[str, torch.nn.Parameter]],\n        parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n        lr: float = 0.001,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-08,\n    ):\n        super().__init__(\n            params=make_parameter_groups(model_parameters, parameter_groups),\n            lr=lr,\n            betas=betas,\n            eps=eps,\n        )\n\n\n@Optimizer.register(""adamax"")\nclass AdamaxOptimizer(Optimizer, torch.optim.Adamax):\n    """"""\n    Registered as an `Optimizer` with name ""adamax"".\n    """"""\n\n    def __init__(\n        self,\n        model_parameters: List[Tuple[str, torch.nn.Parameter]],\n        parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n        lr: float = 0.002,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-08,\n        weight_decay: float = 0.0,\n    ):\n        super().__init__(\n            params=make_parameter_groups(model_parameters, parameter_groups),\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n        )\n\n\n@Optimizer.register(""adamw"")\nclass AdamWOptimizer(Optimizer, torch.optim.AdamW):\n    """"""\n    Registered as an `Optimizer` with name ""adamw"".\n    """"""\n\n    def __init__(\n        self,\n        model_parameters: List[Tuple[str, torch.nn.Parameter]],\n        parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n        lr: float = 0.001,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-08,\n        weight_decay: float = 0.01,\n        amsgrad: bool = False,\n    ):\n        super().__init__(\n            params=make_parameter_groups(model_parameters, parameter_groups),\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            amsgrad=amsgrad,\n        )\n\n\n@Optimizer.register(""huggingface_adamw"")\nclass HuggingfaceAdamWOptimizer(Optimizer, transformers.AdamW):\n    """"""\n    Registered as an `Optimizer` with name ""huggingface_adamw"".\n    """"""\n\n    def __init__(\n        self,\n        model_parameters: List[Tuple[str, torch.nn.Parameter]],\n        parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n        lr: float = 0.001,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-06,\n        weight_decay: float = 0.0,\n        correct_bias: bool = False,\n    ):\n        super().__init__(\n            params=make_parameter_groups(model_parameters, parameter_groups),\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            correct_bias=correct_bias,\n        )\n\n\n@Optimizer.register(""adagrad"")\nclass AdagradOptimizer(Optimizer, torch.optim.Adagrad):\n    """"""\n    Registered as an `Optimizer` with name ""adagrad"".\n    """"""\n\n    def __init__(\n        self,\n        model_parameters: List[Tuple[str, torch.nn.Parameter]],\n        parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n        lr: float = 0.01,\n        lr_decay: float = 0.0,\n        weight_decay: float = 0.0,\n        initial_accumulator_value: float = 0.0,\n        eps: float = 1e-10,\n    ):\n        super().__init__(\n            params=make_parameter_groups(model_parameters, parameter_groups),\n            lr=lr,\n            lr_decay=lr_decay,\n            weight_decay=weight_decay,\n            initial_accumulator_value=initial_accumulator_value,\n            eps=eps,\n        )\n\n\n@Optimizer.register(""adadelta"")\nclass AdadeltaOptimizer(Optimizer, torch.optim.Adadelta):\n    """"""\n    Registered as an `Optimizer` with name ""adadelta"".\n    """"""\n\n    def __init__(\n        self,\n        model_parameters: List[Tuple[str, torch.nn.Parameter]],\n        parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n        lr: float = 1.0,\n        rho: float = 0.9,\n        eps: float = 1e-06,\n        weight_decay: float = 0.0,\n    ):\n        super().__init__(\n            params=make_parameter_groups(model_parameters, parameter_groups),\n            lr=lr,\n            rho=rho,\n            eps=eps,\n            weight_decay=weight_decay,\n        )\n\n\n@Optimizer.register(""sgd"")\nclass SgdOptimizer(Optimizer, torch.optim.SGD):\n    """"""\n    Registered as an `Optimizer` with name ""sgd"".\n    """"""\n\n    def __init__(\n        self,\n        model_parameters: List[Tuple[str, torch.nn.Parameter]],\n        lr: float,\n        parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n        momentum: float = 0.0,\n        dampening: float = 0,\n        weight_decay: float = 0.0,\n        nesterov: bool = False,\n    ):\n        super().__init__(\n            params=make_parameter_groups(model_parameters, parameter_groups),\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n        )\n\n\n@Optimizer.register(""rmsprop"")\nclass RmsPropOptimizer(Optimizer, torch.optim.RMSprop):\n    """"""\n    Registered as an `Optimizer` with name ""rmsprop"".\n    """"""\n\n    def __init__(\n        self,\n        model_parameters: List[Tuple[str, torch.nn.Parameter]],\n        parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n        lr: float = 0.01,\n        alpha: float = 0.99,\n        eps: float = 1e-08,\n        weight_decay: float = 0.0,\n        momentum: float = 0.0,\n        centered: bool = False,\n    ):\n        super().__init__(\n            params=make_parameter_groups(model_parameters, parameter_groups),\n            lr=lr,\n            alpha=alpha,\n            eps=eps,\n            weight_decay=weight_decay,\n            momentum=momentum,\n            centered=centered,\n        )\n\n\n@Optimizer.register(""averaged_sgd"")\nclass AveragedSgdOptimizer(Optimizer, torch.optim.ASGD):\n    """"""\n    Registered as an `Optimizer` with name ""averaged_sgd"".\n    """"""\n\n    def __init__(\n        self,\n        model_parameters: List[Tuple[str, torch.nn.Parameter]],\n        parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n        lr: float = 0.01,\n        lambd: float = 0.0001,\n        alpha: float = 0.75,\n        t0: float = 1000000.0,\n        weight_decay: float = 0.0,\n    ):\n        super().__init__(\n            params=make_parameter_groups(model_parameters, parameter_groups),\n            lr=lr,\n            lambd=lambd,\n            alpha=alpha,\n            t0=t0,\n            weight_decay=weight_decay,\n        )\n\n\n@Optimizer.register(""dense_sparse_adam"")\nclass DenseSparseAdam(Optimizer, torch.optim.Optimizer):\n    """"""\n    NOTE: This class has been copied verbatim from the separate Dense and\n    Sparse versions of Adam in Pytorch.\n\n    Implements Adam algorithm with dense & sparse gradients.\n    It has been proposed in Adam: A Method for Stochastic Optimization.\n\n    Registered as an `Optimizer` with name ""dense_sparse_adam"".\n\n    # Parameters\n\n    params : `iterable`\n        iterable of parameters to optimize or dicts defining parameter groups\n    lr : `float`, optional (default = `1e-3`)\n        The learning rate.\n    betas : `Tuple[float, float]`, optional (default = `(0.9, 0.999)`)\n        coefficients used for computing running averages of gradient\n        and its square.\n    eps : `float`, optional, (default = `1e-8`)\n        A term added to the denominator to improve numerical stability.\n    """"""\n\n    def __init__(\n        self,\n        model_parameters: List[Tuple[str, torch.nn.Parameter]],\n        parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,\n        lr=1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n    ):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps)\n        super().__init__(make_parameter_groups(model_parameters, parameter_groups), defaults)\n\n    def step(self, closure=None):\n        """"""\n        Performs a single optimization step.\n\n        # Parameters\n\n        closure : `callable`, optional.\n            A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[""params""]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[""step""] = 0\n                    # Exponential moving average of gradient values\n                    state[""exp_avg""] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[""exp_avg_sq""] = torch.zeros_like(p.data)\n\n                state[""step""] += 1\n\n                exp_avg, exp_avg_sq = state[""exp_avg""], state[""exp_avg_sq""]\n                beta1, beta2 = group[""betas""]\n\n                if grad.is_sparse:\n                    grad = grad.coalesce()  # the update is non-linear so indices must be unique\n                    grad_indices = grad._indices()\n                    grad_values = grad._values()\n                    size = grad.size()\n\n                    def make_sparse(values):\n                        constructor = grad.new\n                        if grad_indices.dim() == 0 or values.dim() == 0:\n                            return constructor().resize_as_(grad)\n                        return constructor(grad_indices, values, size)\n\n                    # Decay the first and second moment running average coefficient\n                    #      old <- b * old + (1 - b) * new\n                    # <==> old += (1 - b) * (new - old)\n                    old_exp_avg_values = exp_avg.sparse_mask(grad)._values()\n                    exp_avg_update_values = grad_values.sub(old_exp_avg_values).mul_(1 - beta1)\n                    exp_avg.add_(make_sparse(exp_avg_update_values))\n                    old_exp_avg_sq_values = exp_avg_sq.sparse_mask(grad)._values()\n                    exp_avg_sq_update_values = (\n                        grad_values.pow(2).sub_(old_exp_avg_sq_values).mul_(1 - beta2)\n                    )\n                    exp_avg_sq.add_(make_sparse(exp_avg_sq_update_values))\n\n                    # Dense addition again is intended, avoiding another sparse_mask\n                    numer = exp_avg_update_values.add_(old_exp_avg_values)\n                    exp_avg_sq_update_values.add_(old_exp_avg_sq_values)\n                    denom = exp_avg_sq_update_values.sqrt_().add_(group[""eps""])\n                    del exp_avg_update_values, exp_avg_sq_update_values\n\n                    bias_correction1 = 1 - beta1 ** state[""step""]\n                    bias_correction2 = 1 - beta2 ** state[""step""]\n                    step_size = group[""lr""] * math.sqrt(bias_correction2) / bias_correction1\n\n                    p.data.add_(make_sparse(-step_size * numer.div_(denom)))\n\n                else:\n                    # Decay the first and second moment running average coefficient\n                    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                    denom = exp_avg_sq.sqrt().add_(group[""eps""])\n\n                    bias_correction1 = 1 - beta1 ** state[""step""]\n                    bias_correction2 = 1 - beta2 ** state[""step""]\n                    step_size = group[""lr""] * math.sqrt(bias_correction2) / bias_correction1\n\n                    p.data.addcdiv_(exp_avg, denom, value=-step_size)\n\n        return loss\n'"
allennlp/training/scheduler.py,1,"b'from typing import Dict, Any\n\nimport torch\n\n\nclass Scheduler:\n    """"""\n    A `Scheduler` is a generalization of PyTorch learning rate schedulers.\n\n    A scheduler can be used to update any field in an optimizer\'s parameter groups,\n    not just the learning rate.\n\n    During training using the AllenNLP `Trainer`, this is the API and calling\n    sequence for `step` and `step_batch`::\n\n       scheduler = ... # creates scheduler, calls self.step(epoch=-1) in __init__\n\n       batch_num_total = 0\n       for epoch in range(num_epochs):\n           for batch in batchs_in_epoch:\n               # compute loss, update parameters with current learning rates\n               # call step_batch AFTER updating parameters\n               batch_num_total += 1\n               scheduler.step_batch(batch_num_total)\n           # call step() at the END of each epoch\n           scheduler.step(validation_metrics, epoch)\n    """"""\n\n    def __init__(\n        self, optimizer: torch.optim.Optimizer, param_group_field: str, last_epoch: int = -1\n    ) -> None:\n        self.optimizer = optimizer\n        self.param_group_field = param_group_field\n        self._initial_param_group_field = f""initial_{param_group_field}""\n        if last_epoch == -1:\n            for i, group in enumerate(self.optimizer.param_groups):\n                if param_group_field not in group:\n                    raise KeyError(f""{param_group_field} missing from param_groups[{i}]"")\n                group.setdefault(self._initial_param_group_field, group[param_group_field])\n        else:\n            for i, group in enumerate(self.optimizer.param_groups):\n                if self._initial_param_group_field not in group:\n                    raise KeyError(\n                        f""{self._initial_param_group_field} missing from param_groups[{i}]""\n                    )\n        self.base_values = [\n            group[self._initial_param_group_field] for group in self.optimizer.param_groups\n        ]\n        self.last_epoch = last_epoch\n\n    def state_dict(self) -> Dict[str, Any]:\n        """"""\n        Returns the state of the scheduler as a `dict`.\n        """"""\n        return {key: value for key, value in self.__dict__.items() if key != ""optimizer""}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        """"""\n        Load the schedulers state.\n\n        # Parameters\n\n        state_dict : `Dict[str, Any]`\n            Scheduler state. Should be an object returned from a call to `state_dict`.\n        """"""\n        self.__dict__.update(state_dict)\n\n    def get_values(self):\n        raise NotImplementedError\n\n    def step(self, metric: float = None) -> None:\n        self.last_epoch += 1\n        self.metric = metric\n        for param_group, value in zip(self.optimizer.param_groups, self.get_values()):\n            param_group[self.param_group_field] = value\n\n    def step_batch(self, batch_num_total: int = None) -> None:\n        """"""\n        By default, a scheduler is assumed to only update every epoch, not every batch.\n        So this does nothing unless it\'s overriden.\n        """"""\n\n        return\n'"
allennlp/training/tensorboard_writer.py,9,"b'from typing import Any, Callable, Dict, List, Optional, Set\nimport logging\nimport os\n\nfrom tensorboardX import SummaryWriter\nimport torch\n\nfrom allennlp.common.from_params import FromParams\nfrom allennlp.data.dataloader import TensorDict\nfrom allennlp.nn import util as nn_util\nfrom allennlp.training.optimizers import Optimizer\nfrom allennlp.training import util as training_util\nfrom allennlp.models.model import Model\n\nlogger = logging.getLogger(__name__)\n\n\nclass TensorboardWriter(FromParams):\n    """"""\n    Class that handles Tensorboard (and other) logging.\n\n    # Parameters\n\n    serialization_dir : `str`, optional (default = `None`)\n        If provided, this is where the Tensorboard logs will be written.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""tensorboard_writer"", it gets passed in separately.\n    summary_interval : `int`, optional (default = `100`)\n        Most statistics will be written out only every this many batches.\n    histogram_interval : `int`, optional (default = `None`)\n        If provided, activation histograms will be written out every this many batches.\n        If None, activation histograms will not be written out.\n        When this parameter is specified, the following additional logging is enabled:\n            * Histograms of model parameters\n            * The ratio of parameter update norm to parameter norm\n            * Histogram of layer activations\n        We log histograms of the parameters returned by\n        `model.get_parameters_for_histogram_tensorboard_logging`.\n        The layer activations are logged for any modules in the `Model` that have\n        the attribute `should_log_activations` set to `True`.  Logging\n        histograms requires a number of GPU-CPU copies during training and is typically\n        slow, so we recommend logging histograms relatively infrequently.\n        Note: only Modules that return tensors, tuples of tensors or dicts\n        with tensors as values currently support activation logging.\n    batch_size_interval : `int`, optional, (default = `None`)\n        If defined, how often to log the average batch size.\n    should_log_parameter_statistics : `bool`, optional (default = `True`)\n        Whether to log parameter statistics (mean and standard deviation of parameters and\n        gradients).\n    should_log_learning_rate : `bool`, optional (default = `False`)\n        Whether to log (parameter-specific) learning rate.\n    get_batch_num_total : `Callable[[], int]`, optional (default = `None`)\n        A thunk that returns the number of batches so far. Most likely this will\n        be a closure around an instance variable in your `Trainer` class.  Because of circular\n        dependencies in constructing this object and the `Trainer`, this is typically `None` when\n        you construct the object, but it gets set inside the constructor of our `Trainer`.\n    """"""\n\n    def __init__(\n        self,\n        serialization_dir: Optional[str] = None,\n        summary_interval: int = 100,\n        histogram_interval: int = None,\n        batch_size_interval: Optional[int] = None,\n        should_log_parameter_statistics: bool = True,\n        should_log_learning_rate: bool = False,\n        get_batch_num_total: Callable[[], int] = None,\n    ) -> None:\n        if serialization_dir is not None:\n            # Create log directories prior to creating SummaryWriter objects\n            # in order to avoid race conditions during distributed training.\n            train_ser_dir = os.path.join(serialization_dir, ""log"", ""train"")\n            os.makedirs(train_ser_dir, exist_ok=True)\n            self._train_log = SummaryWriter(train_ser_dir)\n            val_ser_dir = os.path.join(serialization_dir, ""log"", ""validation"")\n            os.makedirs(val_ser_dir, exist_ok=True)\n            self._validation_log = SummaryWriter(val_ser_dir)\n        else:\n            self._train_log = self._validation_log = None\n\n        self._summary_interval = summary_interval\n        self._histogram_interval = histogram_interval\n        self._batch_size_interval = batch_size_interval\n        self._should_log_parameter_statistics = should_log_parameter_statistics\n        self._should_log_learning_rate = should_log_learning_rate\n        self.get_batch_num_total = get_batch_num_total\n\n        self._cumulative_batch_group_size = 0\n        self._batches_this_epoch = 0\n        self._histogram_parameters: Set[str] = None\n\n    @staticmethod\n    def _item(value: Any):\n        if hasattr(value, ""item""):\n            val = value.item()\n        else:\n            val = value\n        return val\n\n    def log_memory_usage(\n        self, cpu_memory_usage: Dict[int, float], gpu_memory_usage: Dict[int, int]\n    ):\n        cpu_memory_usage_total = 0.0\n        for worker, memory in cpu_memory_usage.items():\n            self.add_train_scalar(f""memory_usage/worker_{worker}_cpu"", memory)\n            cpu_memory_usage_total += memory\n        self.add_train_scalar(""memory_usage/cpu"", cpu_memory_usage_total)\n        for gpu, memory in gpu_memory_usage.items():\n            self.add_train_scalar(f""memory_usage/gpu_{gpu}"", memory)\n\n    def log_batch(\n        self,\n        model: Model,\n        optimizer: Optimizer,\n        batch_grad_norm: Optional[float],\n        metrics: Dict[str, float],\n        batch_group: List[List[TensorDict]],\n        param_updates: Optional[Dict[str, torch.Tensor]],\n    ) -> None:\n        if self.should_log_this_batch():\n            self.log_parameter_and_gradient_statistics(model, batch_grad_norm)\n            self.log_learning_rates(model, optimizer)\n\n            self.add_train_scalar(""loss/loss_train"", metrics[""loss""])\n            self.log_metrics({""epoch_metrics/"" + k: v for k, v in metrics.items()})\n\n        if self.should_log_histograms_this_batch():\n            self.log_histograms(model)\n            self.log_gradient_updates(model, param_updates)\n\n        if self._batch_size_interval:\n            # We\'re assuming here that `log_batch` will get called every batch, and only every\n            # batch.  This is true with our current usage of this code (version 1.0); if that\n            # assumption becomes wrong, this code will break.\n            batch_group_size = sum(training_util.get_batch_size(batch) for batch in batch_group)\n            self._batches_this_epoch += 1\n            self._cumulative_batch_group_size += batch_group_size\n\n            if (self._batches_this_epoch - 1) % self._batch_size_interval == 0:\n                average = self._cumulative_batch_group_size / self._batches_this_epoch\n                logger.info(f""current batch size: {batch_group_size} mean batch size: {average}"")\n                self.add_train_scalar(""current_batch_size"", batch_group_size)\n                self.add_train_scalar(""mean_batch_size"", average)\n\n    def reset_epoch(self) -> None:\n        self._cumulative_batch_group_size = 0\n        self._batches_this_epoch = 0\n\n    def should_log_this_batch(self) -> bool:\n        return self.get_batch_num_total() % self._summary_interval == 0\n\n    def should_log_histograms_this_batch(self) -> bool:\n        return (\n            self._histogram_interval is not None\n            and self.get_batch_num_total() % self._histogram_interval == 0\n        )\n\n    def add_train_scalar(self, name: str, value: float, timestep: int = None) -> None:\n        timestep = timestep or self.get_batch_num_total()\n        # get the scalar\n        if self._train_log is not None:\n            self._train_log.add_scalar(name, self._item(value), timestep)\n\n    def add_train_histogram(self, name: str, values: torch.Tensor) -> None:\n        if self._train_log is not None:\n            if isinstance(values, torch.Tensor):\n                values_to_write = values.cpu().data.numpy().flatten()\n                self._train_log.add_histogram(name, values_to_write, self.get_batch_num_total())\n\n    def add_validation_scalar(self, name: str, value: float, timestep: int = None) -> None:\n        timestep = timestep or self.get_batch_num_total()\n        if self._validation_log is not None:\n            self._validation_log.add_scalar(name, self._item(value), timestep)\n\n    def log_parameter_and_gradient_statistics(self, model: Model, batch_grad_norm: float) -> None:\n        """"""\n        Send the mean and std of all parameters and gradients to tensorboard, as well\n        as logging the average gradient norm.\n        """"""\n        if self._should_log_parameter_statistics:\n            # Log parameter values to Tensorboard\n            for name, param in model.named_parameters():\n                if param.data.numel() > 0:\n                    self.add_train_scalar(""parameter_mean/"" + name, param.data.mean())\n                if param.data.numel() > 1:\n                    self.add_train_scalar(""parameter_std/"" + name, param.data.std())\n                if param.grad is not None:\n                    if param.grad.is_sparse:\n\n                        grad_data = param.grad.data._values()\n                    else:\n                        grad_data = param.grad.data\n\n                    # skip empty gradients\n                    if torch.prod(torch.tensor(grad_data.shape)).item() > 0:\n                        self.add_train_scalar(""gradient_mean/"" + name, grad_data.mean())\n                        if grad_data.numel() > 1:\n                            self.add_train_scalar(""gradient_std/"" + name, grad_data.std())\n                    else:\n                        # no gradient for a parameter with sparse gradients\n                        logger.info(""No gradient for %s, skipping tensorboard logging."", name)\n            # norm of gradients\n            if batch_grad_norm is not None:\n                self.add_train_scalar(""gradient_norm"", batch_grad_norm)\n\n    def log_learning_rates(self, model: Model, optimizer: torch.optim.Optimizer):\n        """"""\n        Send current parameter specific learning rates to tensorboard\n        """"""\n        if self._should_log_learning_rate:\n            # optimizer stores lr info keyed by parameter tensor\n            # we want to log with parameter name\n            names = {param: name for name, param in model.named_parameters()}\n            for group in optimizer.param_groups:\n                if ""lr"" not in group:\n                    continue\n                rate = group[""lr""]\n                for param in group[""params""]:\n                    # check whether params has requires grad or not\n                    effective_rate = rate * float(param.requires_grad)\n                    self.add_train_scalar(""learning_rate/"" + names[param], effective_rate)\n\n    def log_histograms(self, model: Model) -> None:\n        """"""\n        Send histograms of parameters to tensorboard.\n        """"""\n        if not self._histogram_parameters:\n            # Avoiding calling this every batch.  If we ever use two separate models with a single\n            # writer, this is wrong, but I doubt that will ever happen.\n            self._histogram_parameters = set(\n                model.get_parameters_for_histogram_tensorboard_logging()\n            )\n        for name, param in model.named_parameters():\n            if name in self._histogram_parameters:\n                self.add_train_histogram(""parameter_histogram/"" + name, param)\n\n    def log_gradient_updates(self, model: Model, param_updates: Dict[str, torch.Tensor]) -> None:\n        for name, param in model.named_parameters():\n            update_norm = torch.norm(param_updates[name].view(-1))\n            param_norm = torch.norm(param.view(-1)).cpu()\n            self.add_train_scalar(\n                ""gradient_update/"" + name,\n                update_norm / (param_norm + nn_util.tiny_value_of_dtype(param_norm.dtype)),\n            )\n\n    def log_metrics(\n        self,\n        train_metrics: dict,\n        val_metrics: dict = None,\n        epoch: int = None,\n        log_to_console: bool = False,\n    ) -> None:\n        """"""\n        Sends all of the train metrics (and validation metrics, if provided) to tensorboard.\n        """"""\n        metric_names = set(train_metrics.keys())\n        if val_metrics is not None:\n            metric_names.update(val_metrics.keys())\n        val_metrics = val_metrics or {}\n\n        # For logging to the console\n        if log_to_console:\n            dual_message_template = ""%s |  %8.3f  |  %8.3f""\n            no_val_message_template = ""%s |  %8.3f  |  %8s""\n            no_train_message_template = ""%s |  %8s  |  %8.3f""\n            header_template = ""%s |  %-10s""\n            name_length = max(len(x) for x in metric_names)\n            logger.info(header_template, ""Training"".rjust(name_length + 13), ""Validation"")\n\n        for name in sorted(metric_names):\n            # Log to tensorboard\n            train_metric = train_metrics.get(name)\n            if train_metric is not None:\n                self.add_train_scalar(name, train_metric, timestep=epoch)\n            val_metric = val_metrics.get(name)\n            if val_metric is not None:\n                self.add_validation_scalar(name, val_metric, timestep=epoch)\n\n            # And maybe log to console\n            if log_to_console and val_metric is not None and train_metric is not None:\n                logger.info(\n                    dual_message_template, name.ljust(name_length), train_metric, val_metric\n                )\n            elif log_to_console and val_metric is not None:\n                logger.info(no_train_message_template, name.ljust(name_length), ""N/A"", val_metric)\n            elif log_to_console and train_metric is not None:\n                logger.info(no_val_message_template, name.ljust(name_length), train_metric, ""N/A"")\n\n    def enable_activation_logging(self, model: Model) -> None:\n        if self._histogram_interval is not None:\n            # To log activation histograms to the forward pass, we register\n            # a hook on forward to capture the output tensors.\n            # This uses a closure to determine whether to log the activations,\n            # since we don\'t want them on every call.\n            for _, module in model.named_modules():\n                if not getattr(module, ""should_log_activations"", False):\n                    # skip it\n                    continue\n\n                def hook(module_, inputs, outputs):\n\n                    log_prefix = ""activation_histogram/{0}"".format(module_.__class__)\n                    if self.should_log_histograms_this_batch():\n                        self.log_activation_histogram(outputs, log_prefix)\n\n                module.register_forward_hook(hook)\n\n    def log_activation_histogram(self, outputs, log_prefix: str) -> None:\n        if isinstance(outputs, torch.Tensor):\n            log_name = log_prefix\n            self.add_train_histogram(log_name, outputs)\n        elif isinstance(outputs, (list, tuple)):\n            for i, output in enumerate(outputs):\n                log_name = ""{0}_{1}"".format(log_prefix, i)\n                self.add_train_histogram(log_name, output)\n        elif isinstance(outputs, dict):\n            for k, tensor in outputs.items():\n                log_name = ""{0}_{1}"".format(log_prefix, k)\n                self.add_train_histogram(log_name, tensor)\n        else:\n            # skip it\n            pass\n\n    def close(self) -> None:\n        """"""\n        Calls the `close` method of the `SummaryWriter` s which makes sure that pending\n        scalars are flushed to disk and the tensorboard event files are closed properly.\n        """"""\n        if self._train_log is not None:\n            self._train_log.close()\n        if self._validation_log is not None:\n            self._validation_log.close()\n'"
allennlp/training/trainer.py,27,"b'import datetime\nimport logging\nimport math\nimport os\nimport re\nimport time\nimport traceback\nfrom contextlib import contextmanager\nfrom typing import Any, Dict, Iterator, List, Optional, Tuple, Union\n\nfrom allennlp.common.util import int_to_device\n\ntry:\n    from apex import amp\nexcept ImportError:\n    amp = None\nimport torch\nimport torch.distributed as dist\nimport torch.optim.lr_scheduler\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom allennlp.common import Lazy, Registrable, Tqdm\nfrom allennlp.common import util as common_util\nfrom allennlp.common.checks import ConfigurationError, check_for_gpu\nfrom allennlp.data import DataLoader\nfrom allennlp.data.dataloader import TensorDict\nfrom allennlp.models.model import Model\nfrom allennlp.nn import util as nn_util\nfrom allennlp.training import util as training_util\nfrom allennlp.training.checkpointer import Checkpointer\nfrom allennlp.training.learning_rate_schedulers import LearningRateScheduler\nfrom allennlp.training.metric_tracker import MetricTracker\nfrom allennlp.training.momentum_schedulers import MomentumScheduler\nfrom allennlp.training.moving_average import MovingAverage\nfrom allennlp.training.optimizers import Optimizer\nfrom allennlp.training.tensorboard_writer import TensorboardWriter\n\nlogger = logging.getLogger(__name__)\n\n\nclass Trainer(Registrable):\n    """"""\n    The base class for an AllenNLP trainer. It can do pretty much\n    anything you want. Your subclass should implement `train`\n    and also probably `from_params`.\n    """"""\n\n    default_implementation = ""gradient_descent""\n\n    def __init__(\n        self,\n        serialization_dir: str,\n        cuda_device: Union[int, torch.device] = -1,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n    ) -> None:\n\n        check_for_gpu(cuda_device)\n        self._serialization_dir = serialization_dir\n\n        if isinstance(cuda_device, list):\n            raise ConfigurationError(\n                ""In allennlp 1.0, the Trainer can only be assigned a single `cuda_device`. ""\n                ""Instead, we use torch\'s DistributedDataParallel at the command level, meaning ""\n                ""our Trainer always uses a single GPU per process.""\n            )\n\n        if distributed and world_size <= 1:\n            raise ConfigurationError(\n                ""Distributed training can be performed only with more than 1 device. Check ""\n                ""`cuda_device` key in the experiment configuration.""\n            )\n\n        self.cuda_device = int_to_device(cuda_device)\n\n        self._distributed = distributed\n        self._rank = local_rank\n        self._master = self._rank == 0\n        self._world_size = world_size\n\n    def train(self) -> Dict[str, Any]:\n        """"""\n        Train a model and return the results.\n        """"""\n        raise NotImplementedError\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        """"""\n        Returns a tuple of (model state, training state), where training state could have several\n        internal components (e.g., for an, optimizer, learning rate scheduler, etc.).\n\n        This is a context manager, and should be called as `with trainer.get_checkpoint_state() as\n        state:`, so that the trainer has the opportunity to change and restore its internal state\n        for checkpointing.  This is used, e.g., for moving averages of model weights.\n        """"""\n        raise NotImplementedError\n\n\nclass BatchCallback(Registrable):\n    """"""\n    An optional callback that you can pass to the `GradientDescentTrainer` that will be called at\n    the end of every batch, during both training and validation.  The default implementation\n    does nothing. You can implement your own callback and do whatever you want, such as saving\n    predictions to disk or extra logging.\n    """"""\n\n    def __call__(\n        self,\n        trainer: ""GradientDescentTrainer"",\n        batch_inputs: List[List[TensorDict]],\n        batch_outputs: List[Dict[str, Any]],\n        epoch: int,\n        batch_number: int,\n        is_training: bool,\n        is_master: bool,\n    ) -> None:\n        pass\n\n\n@BatchCallback.register(""tensorboard-memory-usage"")\nclass TensoboardBatchMemoryUsage(BatchCallback):\n    """"""\n    Logs the CPU and GPU memory usage to tensorboard on every batch.\n\n    This is mainly used for debugging as it can cause a significant slowdown in training.\n    """"""\n\n    def __call__(\n        self,\n        trainer: ""GradientDescentTrainer"",\n        batch_inputs: List[List[TensorDict]],\n        batch_outputs: List[Dict[str, Any]],\n        epoch: int,\n        batch_number: int,\n        is_training: bool,\n        is_master: bool,\n    ) -> None:\n        # In the distributed case we need to call this from every worker, since every\n        # worker reports its own memory usage.\n        cpu_memory_usage = common_util.peak_memory_mb()\n        # But we only want to call `gpu_memory_mb` and `log_memory_usage` from the\n        # master process.\n        if is_master:\n            gpu_memory_usage = common_util.gpu_memory_mb()\n            trainer._tensorboard.log_memory_usage(cpu_memory_usage, gpu_memory_usage)\n\n\nBatchCallback.register(""null"")(BatchCallback)\n\n\nclass EpochCallback(Registrable):\n    """"""\n    An optional callback that you can pass to the `GradientDescentTrainer` that will be called at\n    the end of every epoch (and before the start of training, with `epoch=-1`). The default\n    implementation does nothing. You can implement your own callback and do whatever you want, such\n    as additional modifications of the trainer\'s state in between epochs.\n    """"""\n\n    def __call__(\n        self,\n        trainer: ""GradientDescentTrainer"",\n        metrics: Dict[str, Any],\n        epoch: int,\n        is_master: bool,\n    ) -> None:\n        pass\n\n\nEpochCallback.register(""null"")(EpochCallback)\n\n\n@Trainer.register(""gradient_descent"", constructor=""from_partial_objects"")\nclass GradientDescentTrainer(Trainer):\n    """"""\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation dataloader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name ""gradient_descent"" (and is also the default `Trainer`).\n    The constructor that is registered is `from_partial_objects` - see the arguments to that\n    function for the exact keys that should be used, if you are using a configuration file.  They\n    largely match the arguments to `__init__`, and we don\'t repeat their docstrings in\n    `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n    [1]: https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a ""loss"" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""trainer"", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A pytorch `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""trainer"", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `str`, optional (default=`""loss""`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either ""+"" or ""-"", which specifies whether the metric\n        is an increasing or decreasing function.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""trainer"", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""trainer"", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `int`, optional (default = `-1`)\n        An integer specifying the CUDA device(s) to use for this process. If -1, the CPU is used.\n        Data parallelism is controlled at the allennlp train level, so each trainer will have a single\n        GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    tensorboard_writer : `TensorboardWriter`, optional\n        If this is not provided, we will construct a `TensorboardWriter` with default\n        parameters and use that.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    batch_callbacks : `List[BatchCallback]`, optional (default = `None`)\n        A list of callbacks that will be called at the end of every batch, during both train and\n        validation.\n\n    epoch_callbacks : `List[EpochCallback]`, optional (default = `None`)\n        A list of callbacks that will be called at the end of every epoch, and at the start of\n        training (with epoch = -1).\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch\'s `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""trainer"", it gets constructed separately (you need a top-level ""distributed"" key, next to\n        the ""trainer"" entry, that specifies a list of ""cuda_devices"").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""trainer"", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""trainer"", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf\'s\n        post][0] for details on Gradient Accumulation.\n\n    opt_level : `str`, optional, (default = `None`)\n        Each opt_level establishes a set of properties that govern Amp\xe2\x80\x99s implementation of pure or mixed\n        precision training. Must be a choice of `""O0""`, `""O1""`, `""O2""`, or `""O3""`.\n        See [the Apex documentation][1] for\n        more details. If `None`, Amp is not used. Defaults to `None`.\n\n    """"""\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: torch.utils.data.DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: str = ""-loss"",\n        validation_data_loader: torch.utils.data.DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: int = -1,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        tensorboard_writer: TensorboardWriter = None,\n        moving_average: Optional[MovingAverage] = None,\n        batch_callbacks: List[BatchCallback] = None,\n        epoch_callbacks: List[EpochCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        opt_level: Optional[str] = None,\n    ) -> None:\n        super().__init__(serialization_dir, cuda_device, distributed, local_rank, world_size)\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self._validation_data_loader = validation_data_loader\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    ""You provided a validation dataset but patience was set to None, ""\n                    ""meaning that early stopping is disabled""\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                \'{} is an invalid value for ""patience"": it must be a positive integer \'\n                ""or None (if you want to disable early stopping)"".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(patience, validation_metric)\n        # Get rid of + or -\n        self._validation_metric = validation_metric[1:]\n\n        self._num_epochs = num_epochs\n\n        if checkpointer is not None:\n            self._checkpointer = checkpointer\n        else:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n        self._batch_callbacks = batch_callbacks or []\n        self._epoch_callbacks = epoch_callbacks or []\n\n        # We keep the total batch number as an instance variable because it\n        # is used inside a closure for the hook which logs activations in\n        # `_enable_activation_logging`.\n        self._batch_num_total = 0\n\n        self._tensorboard = tensorboard_writer or TensorboardWriter(serialization_dir)\n        self._tensorboard.get_batch_num_total = lambda: self._batch_num_total\n        self._tensorboard.enable_activation_logging(self.model)\n\n        self._last_log = 0.0  # time of last logging\n\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training with NVIDIA Apex.\n        self._opt_level = opt_level\n        if self._opt_level is not None:\n            if amp is None:\n                raise ConfigurationError(\n                    (\n                        ""Apex not installed but opt_level was provided. Please install NVIDIA\'s Apex to enable""\n                        "" automatic mixed precision (AMP) training. See: https://github.com/NVIDIA/apex.""\n                    )\n                )\n\n            self.model, self.optimizer = amp.initialize(\n                self.model, self.optimizer, opt_level=self._opt_level\n            )\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP\'s `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch\'s object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(""cpu"") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> Optional[float]:\n        """"""\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n        """"""\n        if self._grad_norm:\n            if self._opt_level is not None:\n                # See: https://nvidia.github.io/apex/advanced.html#gradient-clipping\n                parameters_to_clip = [\n                    p for p in amp.master_params(self.optimizer) if p.grad is not None\n                ]\n            else:\n                parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return None\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        """"""\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        """"""\n        batch = nn_util.move_to_device(batch, self.cuda_device)\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                regularization_penalty = self.model.get_regularization_penalty()\n                loss = output_dict[""loss""]\n\n                # Handle model without regularization\n                if regularization_penalty == 0.0:\n                    regularization_penalty = loss.new_full(size=[], fill_value=0.0)\n\n                output_dict[""reg_loss""] = regularization_penalty\n                output_dict[""loss""] += regularization_penalty\n            except KeyError:\n                if for_training:\n                    raise RuntimeError(\n                        ""The model you are trying to optimize does not contain a""\n                        "" \'loss\' key in the output of model.forward(inputs).""\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        """"""\n        Trains one epoch and returns metrics.\n        """"""\n        logger.info(""Epoch %d/%d"", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_memory_mb().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f""Worker {worker} memory usage MB: {memory}"")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.gpu_memory_mb().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f""GPU {gpu} memory usage MB: {memory}"")\n\n        train_loss = 0.0\n        train_reg_loss = 0.0\n        # Set the model to ""train"" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(""Training"")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(""inf"")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the master\'s\n        # progress is shown\n        if self._master:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can\'t proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don\'t support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f""Worker {torch.distributed.get_rank()} finishing training early! ""\n                        ""This implies that there is an imbalance in your training ""\n                        ""data across the workers and that some amount of it will be ""\n                        ""ignored. A small amount of this is fine, but a major imbalance ""\n                        ""should be avoided. Note: This warning will appear unless your ""\n                        ""data is perfectly balanced.""\n                    )\n                    break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            self.optimizer.zero_grad()\n\n            batch_group_outputs = []\n            for batch in batch_group:\n                batch_outputs = self.batch_outputs(batch, for_training=True)\n                batch_group_outputs.append(batch_outputs)\n                loss = batch_outputs[""loss""]\n                reg_loss = batch_outputs[""reg_loss""]\n                if torch.isnan(loss):\n                    raise ValueError(""nan loss encountered"")\n                loss = loss / len(batch_group)\n                reg_loss = reg_loss / len(batch_group)\n                if self._opt_level is not None:\n                    with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                        scaled_loss.backward()\n                else:\n                    loss.backward()\n                train_loss += loss.item()\n                train_reg_loss += reg_loss.item()\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn\'t update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            param_updates = None\n            if self._tensorboard.should_log_histograms_this_batch() and self._master:\n                # Get the magnitude of parameter updates for logging.  We need to do some\n                # computation before and after the optimizer step, and it\'s expensive because of\n                # GPU/CPU copies (necessary for large models, and for shipping to tensorboard), so\n                # we don\'t do this every batch, only when it\'s requested.\n                param_updates = {\n                    name: param.detach().cpu().clone()\n                    for name, param in self.model.named_parameters()\n                }\n                self.optimizer.step()\n                for name, param in self.model.named_parameters():\n                    param_updates[name].sub_(param.detach().cpu())\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._master:\n                # Updating tqdm only for the master as the trainers wouldn\'t have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n                self._tensorboard.log_batch(\n                    self.model,\n                    self.optimizer,\n                    batch_grad_norm,\n                    metrics,\n                    batch_group,\n                    param_updates,\n                )\n\n                self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n            for callback in self._batch_callbacks:\n                callback(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_master=self._master,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f""Worker {torch.distributed.get_rank()} completed its entire epoch (training).""\n            )\n            # Indicate that we\'re done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n        for (worker, memory) in cpu_memory_usage:\n            metrics[""worker_"" + str(worker) + ""_memory_MB""] = memory\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[""gpu_"" + str(gpu_num) + ""_memory_MB""] = memory\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, float, int]:\n        """"""\n        Computes the validation loss. Returns it and the number of batches.\n        """"""\n        logger.info(""Validating"")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                ""Validation results cannot be calculated without a validation_data_loader""\n            )\n\n        val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        batches_this_epoch = 0\n        val_loss = 0\n        val_reg_loss = 0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can\'t proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don\'t support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f""Worker {torch.distributed.get_rank()} finishing validation early! ""\n                        ""This implies that there is an imbalance in your validation ""\n                        ""data across the workers and that some amount of it will be ""\n                        ""ignored. A small amount of this is fine, but a major imbalance ""\n                        ""should be avoided. Note: This warning will appear unless your ""\n                        ""data is perfectly balanced.""\n                    )\n                    break\n\n            batch_outputs = self.batch_outputs(batch, for_training=False)\n            loss = batch_outputs.get(""loss"")\n            reg_loss = batch_outputs.get(""reg_loss"")\n            if loss is not None:\n                # You shouldn\'t necessarily have to compute a loss for validation, so we allow for\n                # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                # currently only used as the divisor for the loss function, so we can safely only\n                # count those batches for which we actually have a loss.  If this variable ever\n                # gets used for something else, we might need to change things around a bit.\n                batches_this_epoch += 1\n                val_loss += loss.detach().cpu().numpy()\n                if reg_loss is not None:\n                    val_reg_loss += reg_loss.detach().cpu().numpy()\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n            description = training_util.description_from_metrics(val_metrics)\n            val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._batch_callbacks:\n                callback(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_master=self._master,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f""Worker {torch.distributed.get_rank()} completed its entire epoch (validation).""\n            )\n            # Indicate that we\'re done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        """"""\n        Trains the supplied model with the supplied parameters.\n        """"""\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                ""Could not recover training from the checkpoint.  Did you mean to output to ""\n                ""a different serialization directory or delete the existing serialization ""\n                ""directory?""\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(""Beginning training."")\n\n        val_metrics: Dict[str, float] = {}\n        this_epoch_val_metric: float = None\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[""best_epoch""] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[""best_validation_"" + key] = value\n\n        for callback in self._epoch_callbacks:\n            callback(self, metrics={}, epoch=-1, is_master=self._master)\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(""gpu_"") and key.endswith(""_memory_MB""):\n                    metrics[""peak_"" + key] = max(metrics.get(""peak_"" + key, 0), value)\n                elif key.startswith(""worker_"") and key.endswith(""_memory_MB""):\n                    metrics[""peak_"" + key] = max(metrics.get(""peak_"" + key, 0), value)\n\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = val_metrics[self._validation_metric]\n                    self._metric_tracker.add_metric(this_epoch_val_metric)\n\n                    if self._metric_tracker.should_stop_early():\n                        logger.info(""Ran out of patience.  Stopping training."")\n                        break\n\n            if self._master:\n                self._tensorboard.log_metrics(\n                    train_metrics, val_metrics=val_metrics, log_to_console=True, epoch=epoch + 1\n                )  # +1 because tensorboard doesn\'t like 0\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[""training_duration""] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[""training_start_epoch""] = epoch_counter\n            metrics[""training_epochs""] = epochs_trained\n            metrics[""epoch""] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[""training_"" + key] = value\n            for key, value in val_metrics.items():\n                metrics[""validation_"" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[""best_epoch""] = epoch\n                for key, value in val_metrics.items():\n                    metrics[""best_validation_"" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._master:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f""metrics_epoch_{epoch}.json""), metrics\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn\'t, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            if self._master:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n\n            # Wait for the master to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._epoch_callbacks:\n                callback(self, metrics=metrics, epoch=epoch, is_master=self._master)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(""Epoch duration: %s"", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(""Estimated training time remaining: %s"", formatted_time)\n\n            epochs_trained += 1\n\n        # make sure pending events are flushed to disk and files are closed properly\n        self._tensorboard.close()\n\n        # Load the best model state before returning\n        best_model_state = self._checkpointer.best_model_state()\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            ""metric_tracker"": self._metric_tracker.state_dict(),\n            ""optimizer"": self.optimizer.state_dict(),\n            ""batch_num_total"": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[""learning_rate_scheduler""] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[""momentum_scheduler""] = self._momentum_scheduler.state_dict()\n        # If model was trained with amp, we should persist the amp state.\n        if self._opt_level is not None:\n            training_states[""amp""] = amp.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        """"""\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(""/path/to/model/weights.th""))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        """"""\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        # The apex docs recommend calling amp.initialize before calling load_state_dict.\n        if self._opt_level is not None and ""amp"" in training_state:\n            self.model, self.optimizer = amp.initialize(\n                self.model, self.optimizer, opt_level=self._opt_level\n            )\n            amp.load_state_dict(training_state[""amp""])\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[""optimizer""])\n        if (\n            self._learning_rate_scheduler is not None\n            and ""learning_rate_scheduler"" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[""learning_rate_scheduler""])\n        if self._momentum_scheduler is not None and ""momentum_scheduler"" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[""momentum_scheduler""])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if ""metric_tracker"" in training_state:\n            self._metric_tracker.load_state_dict(training_state[""metric_tracker""])\n        # It used to be the case that we tracked `val_metric_per_epoch`.\n        elif ""val_metric_per_epoch"" in training_state:\n            self._metric_tracker.clear()\n            self._metric_tracker.add_metrics(training_state[""val_metric_per_epoch""])\n        # And before that we didn\'t track anything.\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[""epoch""], int):\n            epoch_to_return = training_state[""epoch""] + 1\n        else:\n            epoch_to_return = int(training_state[""epoch""].split(""."")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(""batch_num_total"")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: str = ""-loss"",\n        num_epochs: int = 20,\n        cuda_device: int = -1,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = None,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        opt_level: Optional[str] = None,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = None,\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = None,\n        batch_callbacks: List[BatchCallback] = None,\n        epoch_callbacks: List[EpochCallback] = None,\n    ) -> ""Trainer"":\n        """"""\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can\'t just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class\'s arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it\'s constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn\'t work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you\'re not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        """"""\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n        if not optimizer_:\n            optimizer_ = Optimizer.default(parameters)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = moving_average.construct(parameters=parameters)\n        learning_rate_scheduler_ = learning_rate_scheduler.construct(\n            optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n        )\n        momentum_scheduler_ = momentum_scheduler.construct(optimizer=optimizer_)\n\n        checkpointer_ = checkpointer.construct() or Checkpointer(serialization_dir)\n        tensorboard_writer_ = tensorboard_writer.construct() or TensorboardWriter(serialization_dir)\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            tensorboard_writer=tensorboard_writer_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            batch_callbacks=batch_callbacks,\n            epoch_callbacks=epoch_callbacks,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            opt_level=opt_level,\n        )\n'"
allennlp/training/util.py,12,"b'""""""\nHelper functions for Trainers\n""""""\nimport datetime\nimport logging\nimport os\nimport shutil\nfrom typing import Any, Dict, Iterable, Optional, Union, Tuple, Set\nfrom collections import Counter\n\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom allennlp.common.checks import check_for_gpu, ConfigurationError\nfrom allennlp.common.params import Params\nfrom allennlp.common.tqdm import Tqdm\nfrom allennlp.data import Instance, Vocabulary\nfrom allennlp.data.batch import Batch\nfrom allennlp.data.dataset_readers import DatasetReader\nfrom allennlp.models.archival import CONFIG_NAME\nfrom allennlp.models.model import Model\nfrom allennlp.nn import util as nn_util\n\nlogger = logging.getLogger(__name__)\n\n\n# We want to warn people that tqdm ignores metrics that start with underscores\n# exactly once. This variable keeps track of whether we have.\nclass HasBeenWarned:\n    tqdm_ignores_underscores = False\n\n\ndef move_optimizer_to_cuda(optimizer):\n    """"""\n    Move the optimizer state to GPU, if necessary.\n    After calling, any parameter specific state in the optimizer\n    will be located on the same device as the parameter.\n    """"""\n    for param_group in optimizer.param_groups:\n        for param in param_group[""params""]:\n            if param.is_cuda:\n                param_state = optimizer.state[param]\n                for k in param_state.keys():\n                    if isinstance(param_state[k], torch.Tensor):\n                        param_state[k] = param_state[k].cuda(device=param.get_device())\n\n\ndef get_batch_size(batch: Union[Dict, torch.Tensor]) -> int:\n    """"""\n    Returns the size of the batch dimension. Assumes a well-formed batch,\n    returns 0 otherwise.\n    """"""\n    if isinstance(batch, torch.Tensor):\n        return batch.size(0)  # type: ignore\n    elif isinstance(batch, Dict):\n        return get_batch_size(next(iter(batch.values())))\n    else:\n        return 0\n\n\ndef time_to_str(timestamp: int) -> str:\n    """"""\n    Convert seconds past Epoch to human readable string.\n    """"""\n    datetimestamp = datetime.datetime.fromtimestamp(timestamp)\n    return ""{:04d}-{:02d}-{:02d}-{:02d}-{:02d}-{:02d}"".format(\n        datetimestamp.year,\n        datetimestamp.month,\n        datetimestamp.day,\n        datetimestamp.hour,\n        datetimestamp.minute,\n        datetimestamp.second,\n    )\n\n\ndef str_to_time(time_str: str) -> datetime.datetime:\n    """"""\n    Convert human readable string to datetime.datetime.\n    """"""\n    pieces: Any = [int(piece) for piece in time_str.split(""-"")]\n    return datetime.datetime(*pieces)\n\n\ndef read_all_datasets(\n    train_data_path: str,\n    dataset_reader: DatasetReader,\n    validation_dataset_reader: DatasetReader = None,\n    validation_data_path: str = None,\n    test_data_path: str = None,\n) -> Dict[str, Dataset]:\n    """"""\n    Reads all datasets (perhaps lazily, if the corresponding dataset readers are lazy) and returns a\n    dictionary mapping dataset name (""train"", ""validation"" or ""test"") to the iterable resulting from\n    `reader.read(filename)`.\n    """"""\n\n    logger.info(""Reading training data from %s"", train_data_path)\n    train_data = dataset_reader.read(train_data_path)\n\n    datasets: Dict[str, Dataset] = {""train"": train_data}\n\n    validation_dataset_reader = validation_dataset_reader or dataset_reader\n\n    if validation_data_path is not None:\n        logger.info(""Reading validation data from %s"", validation_data_path)\n        validation_data = validation_dataset_reader.read(validation_data_path)\n        datasets[""validation""] = validation_data\n\n    if test_data_path is not None:\n        logger.info(""Reading test data from %s"", test_data_path)\n        test_data = validation_dataset_reader.read(test_data_path)\n        datasets[""test""] = test_data\n\n    return datasets\n\n\ndef datasets_from_params(params: Params) -> Dict[str, Dataset]:\n    """"""\n    Load all the datasets specified by the config.\n\n    # Parameters\n\n    params : `Params`\n    cache_directory : `str`, optional\n        If given, we will instruct the `DatasetReaders` that we construct to cache their\n        instances in this location (or read their instances from caches in this location, if a\n        suitable cache already exists).  This is essentially a `base` directory for the cache, as\n        we will additionally add the `cache_prefix` to this directory, giving an actual cache\n        location of `cache_directory + cache_prefix`.\n    cache_prefix : `str`, optional\n        This works in conjunction with the `cache_directory`.  The idea is that the\n        `cache_directory` contains caches for all different parameter settings, while the\n        `cache_prefix` captures a specific set of parameters that led to a particular cache file.\n        That is, if you change the tokenization settings inside your `DatasetReader`, you don\'t\n        want to read cached data that used the old settings.  In order to avoid this, we compute a\n        hash of the parameters used to construct each `DatasetReader` and use that as a ""prefix""\n        to the cache files inside the base `cache_directory`.  So, a given `input_file` would\n        be cached essentially as `cache_directory + cache_prefix + input_file`, where you specify\n        a `cache_directory`, the `cache_prefix` is based on the dataset reader parameters, and\n        the `input_file` is whatever path you provided to `DatasetReader.read()`.  In order to\n        allow you to give recognizable names to these prefixes if you want them, you can manually\n        specify the `cache_prefix`.  Note that in some rare cases this can be dangerous, as we\'ll\n        use the `same` prefix for both train and validation dataset readers.\n    """"""\n    dataset_reader_params = params.pop(""dataset_reader"")\n    validation_dataset_reader_params = params.pop(""validation_dataset_reader"", None)\n\n    dataset_reader = DatasetReader.from_params(dataset_reader_params)\n\n    validation_and_test_dataset_reader: DatasetReader = dataset_reader\n    if validation_dataset_reader_params is not None:\n        logger.info(""Using a separate dataset reader to load validation and test data."")\n        validation_and_test_dataset_reader = DatasetReader.from_params(\n            validation_dataset_reader_params\n        )\n\n    train_data_path = params.pop(""train_data_path"")\n    logger.info(""Reading training data from %s"", train_data_path)\n    train_data = dataset_reader.read(train_data_path)\n\n    datasets: Dict[str, Iterable[Instance]] = {""train"": train_data}\n\n    validation_data_path = params.pop(""validation_data_path"", None)\n    if validation_data_path is not None:\n        logger.info(""Reading validation data from %s"", validation_data_path)\n        validation_data = validation_and_test_dataset_reader.read(validation_data_path)\n        datasets[""validation""] = validation_data\n\n    test_data_path = params.pop(""test_data_path"", None)\n    if test_data_path is not None:\n        logger.info(""Reading test data from %s"", test_data_path)\n        test_data = validation_and_test_dataset_reader.read(test_data_path)\n        datasets[""test""] = test_data\n\n    return datasets\n\n\ndef create_serialization_dir(\n    params: Params, serialization_dir: str, recover: bool, force: bool\n) -> None:\n    """"""\n    This function creates the serialization directory if it doesn\'t exist.  If it already exists\n    and is non-empty, then it verifies that we\'re recovering from a training with an identical configuration.\n\n    # Parameters\n\n    params : `Params`\n        A parameter object specifying an AllenNLP Experiment.\n    serialization_dir : `str`\n        The directory in which to save results and logs.\n    recover : `bool`\n        If `True`, we will try to recover from an existing serialization directory, and crash if\n        the directory doesn\'t exist, or doesn\'t match the configuration we\'re given.\n    force : `bool`\n        If `True`, we will overwrite the serialization directory if it already exists.\n    """"""\n    if recover and force:\n        raise ConfigurationError(""Illegal arguments: both force and recover are true."")\n\n    if os.path.exists(serialization_dir) and force:\n        shutil.rmtree(serialization_dir)\n\n    if os.path.exists(serialization_dir) and os.listdir(serialization_dir):\n        if not recover:\n            raise ConfigurationError(\n                f""Serialization directory ({serialization_dir}) already exists and is ""\n                f""not empty. Specify --recover to recover from an existing output folder.""\n            )\n\n        logger.info(f""Recovering from prior training at {serialization_dir}."")\n\n        recovered_config_file = os.path.join(serialization_dir, CONFIG_NAME)\n        if not os.path.exists(recovered_config_file):\n            raise ConfigurationError(\n                ""The serialization directory already exists but doesn\'t ""\n                ""contain a config.json. You probably gave the wrong directory.""\n            )\n        loaded_params = Params.from_file(recovered_config_file)\n\n        # Check whether any of the training configuration differs from the configuration we are\n        # resuming.  If so, warn the user that training may fail.\n        fail = False\n        flat_params = params.as_flat_dict()\n        flat_loaded = loaded_params.as_flat_dict()\n        for key in flat_params.keys() - flat_loaded.keys():\n            logger.error(\n                f""Key \'{key}\' found in training configuration but not in the serialization ""\n                f""directory we\'re recovering from.""\n            )\n            fail = True\n        for key in flat_loaded.keys() - flat_params.keys():\n            logger.error(\n                f""Key \'{key}\' found in the serialization directory we\'re recovering from ""\n                f""but not in the training config.""\n            )\n            fail = True\n        for key in flat_params.keys():\n            if flat_params.get(key) != flat_loaded.get(key):\n                logger.error(\n                    f""Value for \'{key}\' in training configuration does not match that the value in ""\n                    f""the serialization directory we\'re recovering from: ""\n                    f""{flat_params[key]} != {flat_loaded[key]}""\n                )\n                fail = True\n        if fail:\n            raise ConfigurationError(\n                ""Training configuration does not match the configuration we\'re recovering from.""\n            )\n    else:\n        if recover:\n            raise ConfigurationError(\n                f""--recover specified but serialization_dir ({serialization_dir}) ""\n                ""does not exist.  There is nothing to recover from.""\n            )\n        os.makedirs(serialization_dir, exist_ok=True)\n\n\ndef enable_gradient_clipping(model: Model, grad_clipping: Optional[float]) -> None:\n    if grad_clipping is not None:\n        for parameter in model.parameters():\n            if parameter.requires_grad:\n                parameter.register_hook(\n                    lambda grad: nn_util.clamp_tensor(\n                        grad, minimum=-grad_clipping, maximum=grad_clipping\n                    )\n                )\n\n\ndef rescale_gradients(model: Model, grad_norm: Optional[float] = None) -> Optional[float]:\n    """"""\n    Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n    """"""\n    if grad_norm:\n        parameters_to_clip = [p for p in model.parameters() if p.grad is not None]\n        return clip_grad_norm_(parameters_to_clip, grad_norm)\n    return None\n\n\ndef get_metrics(\n    model: Model,\n    total_loss: float,\n    total_reg_loss: float,\n    num_batches: int,\n    reset: bool = False,\n    world_size: int = 1,\n    cuda_device: Union[int, torch.device] = torch.device(""cpu""),\n) -> Dict[str, float]:\n    """"""\n    Gets the metrics but sets `""loss""` to\n    the total loss divided by the `num_batches` so that\n    the `""loss""` metric is ""average loss per batch"".\n    """"""\n    metrics = model.get_metrics(reset=reset)\n    metrics[""loss""] = float(total_loss / num_batches) if num_batches > 0 else 0.0\n    metrics[""reg_loss""] = float(total_reg_loss / num_batches) if num_batches > 0 else 0.0\n\n    if world_size > 1:\n        # In distributed mode, average out all metrics across GPUs\n        aggregated_metrics = {}\n        for metric_name, metric_val in metrics.items():\n            metric_tensor = torch.tensor(metric_val).to(cuda_device)\n            dist.all_reduce(metric_tensor, op=dist.ReduceOp.SUM)\n            reduced_metric = metric_tensor.item() / world_size\n            aggregated_metrics[metric_name] = reduced_metric\n        return aggregated_metrics\n    else:\n        return metrics\n\n\ndef evaluate(\n    model: Model, data_loader: DataLoader, cuda_device: int = -1, batch_weight_key: str = None,\n) -> Dict[str, Any]:\n    """"""\n    # Parameters\n\n    model : `Model`\n        The model to evaluate\n    data_loader : `DataLoader`\n        The `DataLoader` that will iterate over the evaluation data (data loaders already contain\n        their data).\n    cuda_device : `int`, optional (default=`-1`)\n        The cuda device to use for this evaluation.  The model is assumed to already be using this\n        device; this parameter is only used for moving the input data to the correct device.\n    batch_weight_key : `str`, optional (default=`None`)\n        If given, this is a key in the output dictionary for each batch that specifies how to weight\n        the loss for that batch.  If this is not given, we use a weight of 1 for every batch.\n    """"""\n    check_for_gpu(cuda_device)\n    with torch.no_grad():\n        model.eval()\n\n        iterator = iter(data_loader)\n        logger.info(""Iterating over dataset"")\n        generator_tqdm = Tqdm.tqdm(iterator)\n\n        # Number of batches in instances.\n        batch_count = 0\n        # Number of batches where the model produces a loss.\n        loss_count = 0\n        # Cumulative weighted loss\n        total_loss = 0.0\n        # Cumulative weight across all batches.\n        total_weight = 0.0\n\n        for batch in generator_tqdm:\n            batch_count += 1\n            batch = nn_util.move_to_device(batch, cuda_device)\n            output_dict = model(**batch)\n            loss = output_dict.get(""loss"")\n\n            metrics = model.get_metrics()\n\n            if loss is not None:\n                loss_count += 1\n                if batch_weight_key:\n                    weight = output_dict[batch_weight_key].item()\n                else:\n                    weight = 1.0\n\n                total_weight += weight\n                total_loss += loss.item() * weight\n                # Report the average loss so far.\n                metrics[""loss""] = total_loss / total_weight\n\n            if not HasBeenWarned.tqdm_ignores_underscores and any(\n                metric_name.startswith(""_"") for metric_name in metrics\n            ):\n                logger.warning(\n                    \'Metrics with names beginning with ""_"" will \'\n                    ""not be logged to the tqdm progress bar.""\n                )\n                HasBeenWarned.tqdm_ignores_underscores = True\n            description = (\n                "", "".join(\n                    [\n                        ""%s: %.2f"" % (name, value)\n                        for name, value in metrics.items()\n                        if not name.startswith(""_"")\n                    ]\n                )\n                + "" ||""\n            )\n            generator_tqdm.set_description(description, refresh=False)\n\n        final_metrics = model.get_metrics(reset=True)\n        if loss_count > 0:\n            # Sanity check\n            if loss_count != batch_count:\n                raise RuntimeError(\n                    ""The model you are trying to evaluate only sometimes "" + ""produced a loss!""\n                )\n            final_metrics[""loss""] = total_loss / total_weight\n\n        return final_metrics\n\n\ndef description_from_metrics(metrics: Dict[str, float]) -> str:\n    if not HasBeenWarned.tqdm_ignores_underscores and any(\n        metric_name.startswith(""_"") for metric_name in metrics\n    ):\n        logger.warning(\n            \'Metrics with names beginning with ""_"" will \' ""not be logged to the tqdm progress bar.""\n        )\n        HasBeenWarned.tqdm_ignores_underscores = True\n    return (\n        "", "".join(\n            [\n                ""%s: %.4f"" % (name, value)\n                for name, value in metrics.items()\n                if not name.startswith(""_"")\n            ]\n        )\n        + "" ||""\n    )\n\n\ndef make_vocab_from_params(\n    params: Params, serialization_dir: str, print_statistics: bool = False\n) -> Vocabulary:\n    vocab_params = params.pop(""vocabulary"", {})\n    os.makedirs(serialization_dir, exist_ok=True)\n    vocab_dir = os.path.join(serialization_dir, ""vocabulary"")\n\n    if os.path.isdir(vocab_dir) and os.listdir(vocab_dir) is not None:\n        raise ConfigurationError(\n            ""The \'vocabulary\' directory in the provided serialization directory is non-empty""\n        )\n\n    all_datasets = datasets_from_params(params)\n    datasets_for_vocab_creation = set(params.pop(""datasets_for_vocab_creation"", all_datasets))\n\n    for dataset in datasets_for_vocab_creation:\n        if dataset not in all_datasets:\n            raise ConfigurationError(f""invalid \'dataset_for_vocab_creation\' {dataset}"")\n\n    logger.info(\n        ""From dataset instances, %s will be considered for vocabulary creation."",\n        "", "".join(datasets_for_vocab_creation),\n    )\n\n    instances: Iterable[Instance] = (\n        instance\n        for key, dataset in all_datasets.items()\n        if key in datasets_for_vocab_creation\n        for instance in dataset\n    )\n\n    if print_statistics:\n        instances = list(instances)\n\n    vocab = Vocabulary.from_params(vocab_params, instances=instances)\n\n    logger.info(f""writing the vocabulary to {vocab_dir}."")\n    vocab.save_to_files(vocab_dir)\n    logger.info(""done creating vocab"")\n\n    if print_statistics:\n        dataset = Batch(instances)\n        dataset.index_instances(vocab)\n        dataset.print_statistics()\n        vocab.print_statistics()\n\n    return vocab\n\n\ndef ngrams(\n    tensor: torch.LongTensor, ngram_size: int, exclude_indices: Set[int]\n) -> Dict[Tuple[int, ...], int]:\n    ngram_counts: Dict[Tuple[int, ...], int] = Counter()\n    if ngram_size > tensor.size(-1):\n        return ngram_counts\n    for start_position in range(ngram_size):\n        for tensor_slice in tensor[start_position:].split(ngram_size, dim=-1):\n            if tensor_slice.size(-1) < ngram_size:\n                break\n            ngram = tuple(x.item() for x in tensor_slice)\n            if any(x in exclude_indices for x in ngram):\n                continue\n            ngram_counts[ngram] += 1\n    return ngram_counts\n\n\ndef get_valid_tokens_mask(tensor: torch.LongTensor, exclude_indices: Set[int]) -> torch.ByteTensor:\n    valid_tokens_mask = torch.ones_like(tensor, dtype=torch.bool)\n    for index in exclude_indices:\n        valid_tokens_mask &= tensor != index\n    return valid_tokens_mask\n'"
benchmarks/data/__init__.py,0,b''
scripts/ai2_internal/resume_daemon.py,0,"b'#! /usr/bin/env python3\n\n# Tool to automatically resume preemptible beaker experiments created with run_with_beaker.py.\n#\n# Examples\n# --------\n#\n# Ensure an experiment will be resumed:\n# resume_daemon.py --action=start --experiment-id=$YOUR_EXPERIMENT_ID\n#\n# Stop resuming an experiment:\n# resume_daemon.py --action=stop --experiment-id=$YOUR_EXPERIMENT_ID\n#\n# Details\n# -------\n#\n# In order to operate, resume_daemon.py does the following:\n#\n# 1. Modifies the user\'s crontab.\n# 2. Maintains a SQLite DB in ~/.allennlp/resume.db.\n# 3. Keeps logs in ~/.allennlp/resume.log.\n#\n# The reliance on crontab means that resumes will only occur when the running\n# system is powered on. Longer term Beaker is planning on adding this\n# functionality to their service directly, which will obsolete this tool.\n\nimport argparse\nimport json\nimport logging\nimport os\nimport random\nimport sqlite3\nimport subprocess\nimport time\nfrom enum import Enum\nfrom logging.handlers import RotatingFileHandler\nfrom sqlite3 import Connection\nfrom subprocess import PIPE\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\n    fmt=""%(asctime)s %(levelname)-8s %(message)s"", datefmt=""%Y-%m-%d %H:%M:%S""\n)\ndot_allennlp_dir = f""{os.environ[\'HOME\']}/.allennlp""\n# Special case for users that haven\'t run AllenNLP locally.\nif not os.path.exists(dot_allennlp_dir):\n    os.mkdir(dot_allennlp_dir)\nhandler = RotatingFileHandler(\n    f""{dot_allennlp_dir}/resume.log"", maxBytes=1024 * 1024, backupCount=10\n)\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\nBEAKER_QUERY_INTERVAL_SECONDS = 1.0\n\n\n# See https://github.com/beaker/client/blob/master/api/task_status.go\nclass BeakerStatus(Enum):\n    submitted = ""submitted""\n    provisioning = ""provisioning""\n    initializing = ""initializing""\n    running = ""running""\n    terminating = ""terminating""\n    preempted = ""preempted""\n    succeeded = ""succeeded""\n    skipped = ""skipped""\n    stopped = ""stopped""\n    failed = ""failed""\n\n    def __str__(self):\n        return self.name\n\n    def is_end_state(self):\n        if self is BeakerStatus.preempted:\n            return True\n        elif self is BeakerStatus.succeeded:\n            return True\n        elif self is BeakerStatus.skipped:\n            return True\n        elif self is BeakerStatus.stopped:\n            return True\n        elif self is BeakerStatus.failed:\n            return True\n        else:\n            return False\n\n\nclass BeakerWrapper:\n    def get_status(self, experiment_id: str) -> BeakerStatus:\n        command = [""beaker"", ""experiment"", ""inspect"", experiment_id]\n        experiment_json = subprocess.check_output(command)\n\n        # Example output from beaker.\n        # brendanr.local$ beaker experiment inspect ex_g7knlblsjxxk\n        # [\n        #     {\n        #         ""id"": ""ex_g7knlblsjxxk"",\n        #         ""owner"": {\n        #             ""id"": ""us_a4hw8yvr3xut"",\n        #             ""name"": ""ai2"",\n        #             ""displayName"": ""AI2""\n        #         },\n        #         ""author"": {\n        #             ""id"": ""us_hl8x796649u9"",\n        #             ""name"": ""brendanr"",\n        #             ""displayName"": ""Brendan Roof""\n        #         },\n        #         ""workspace"": """",\n        #         ""user"": {\n        #             ""id"": """",\n        #             ""name"": """",\n        #             ""displayName"": """"\n        #         },\n        #         ""nodes"": [\n        #             {\n        #                 ""name"": ""training"",\n        #                 ""task_id"": """",\n        #                 ""taskId"": ""tk_64wm85lc3f0m"",\n        #                 ""result_id"": """",\n        #                 ""resultId"": ""ds_du02un92r57b"",\n        #                 ""status"": ""initializing"",\n        #                 ""child_task_ids"": null,\n        #                 ""childTaskIds"": [],\n        #                 ""parent_task_ids"": null,\n        #                 ""parentTaskIds"": []\n        #             }\n        #         ],\n        #         ""created"": ""2019-09-25T02:03:30.820437Z"",\n        #         ""archived"": false\n        #     }\n        # ]\n\n        experiment_data = json.loads(experiment_json)\n        # Beaker lets there be multiple tasks in a single experiment. Here we\n        # just try to handle the simple case of single task experiments like\n        # those created by run_with_beaker.py.\n        assert len(experiment_data) == 1, ""Experiment not created with run_with_beaker.py""\n        assert (\n            len(experiment_data[0][""nodes""]) == 1\n        ), ""Experiment not created with run_with_beaker.py""\n        status = BeakerStatus(experiment_data[0][""nodes""][0][""status""])\n        # Small delay to avoid thrashing Beaker.\n        time.sleep(BEAKER_QUERY_INTERVAL_SECONDS)\n        return status\n\n    def resume(self, experiment_id: str) -> str:\n        command = [""beaker"", ""experiment"", ""resume"", f""--experiment-name={experiment_id}""]\n        # Small delay to avoid thrashing Beaker.\n        time.sleep(BEAKER_QUERY_INTERVAL_SECONDS)\n        return subprocess.check_output(command, universal_newlines=True).strip()\n\n\ndef create_table(connection: Connection) -> None:\n    cursor = connection.cursor()\n    create_table_statement = """"""\n    CREATE TABLE active_experiments\n    (experiment_id TEXT PRIMARY KEY, original_id TEXT, max_resumes INTEGER, current_resume INTEGER)\n    """"""\n    cursor.execute(create_table_statement)\n    connection.commit()\n\n\ndef start_autoresume(connection: Connection, experiment_id: str, max_resumes: int) -> None:\n    cursor = connection.cursor()\n    cursor.execute(\n        ""INSERT INTO active_experiments VALUES (?, ?, ?, ?)"",\n        (experiment_id, experiment_id, max_resumes, 0),\n    )\n    connection.commit()\n\n\ndef stop_autoresume(connection: Connection, experiment_id: str) -> None:\n    cursor = connection.cursor()\n    cursor.execute(""SELECT * FROM active_experiments WHERE experiment_id = ?"", (experiment_id,))\n    result = cursor.fetchall()\n    assert result, f""Experiment {experiment_id} not found!""\n    cursor.execute(""DELETE FROM active_experiments WHERE experiment_id = ?"", (experiment_id,))\n    connection.commit()\n\n\ndef resume(connection: Connection, beaker: BeakerWrapper) -> None:\n    logger.info(""Checking if resumes are needed."")\n\n    cursor = connection.cursor()\n    cursor.execute(""SELECT * FROM active_experiments"")\n    experiments = cursor.fetchall()\n\n    for experiment_row in experiments:\n        experiment_id, original_id, max_resumes, current_resume = experiment_row\n        status = beaker.get_status(experiment_id)\n        if status.is_end_state():\n            stop_autoresume(connection, experiment_id)\n            if status is BeakerStatus.preempted:\n                if current_resume >= max_resumes:\n                    logger.info(\n                        f""Experiment {experiment_id} preempted too many times ""\n                        f""({max_resumes}). Original experiment: {original_id}""\n                    )\n                else:\n                    new_experiment_id = beaker.resume(experiment_id)\n                    logger.info(\n                        f""Experiment {experiment_id} preempted ""\n                        f""({current_resume}/{max_resumes}). Resuming as: ""\n                        f""{new_experiment_id} Original experiment: {original_id}""\n                    )\n                    cursor.execute(\n                        ""INSERT INTO active_experiments VALUES (?, ?, ?, ?)"",\n                        (new_experiment_id, original_id, max_resumes, current_resume + 1),\n                    )\n                    connection.commit()\n            else:\n                logger.info(\n                    f""Experiment {experiment_id} completed with status: ""\n                    f""{status}. Original experiment: {original_id}""\n                )\n\n\nclass Action(Enum):\n    start = ""start""\n    stop = ""stop""\n    resume = ""resume""\n\n    def __str__(self):\n        return self.name\n\n\ndef main(args) -> None:\n    # Smooth load from potentially many daemons on different machines.\n    time.sleep(random.randint(0, args.random_delay_seconds))\n\n    db_path = f""{dot_allennlp_dir}/resume.db""\n    connection = sqlite3.connect(db_path)\n\n    # Create the DB if needed.\n    cursor = connection.cursor()\n    cursor.execute(\n        ""SELECT name FROM sqlite_master WHERE type=\'table\' AND name=\'active_experiments\'""\n    )\n    tables = cursor.fetchall()\n    if not tables:\n        create_table(connection)\n\n    # Modify the crontab if needed.\n    crontab_l_result = subprocess.run(\n        [""crontab"", ""-l""], universal_newlines=True, stdout=PIPE, stderr=PIPE\n    )\n    if crontab_l_result.returncode == 0:\n        current_crontab = crontab_l_result.stdout\n    else:\n        # `crontab -l` fails when a crontab hasn\'t been installed previously.\n        # Sanity check the error message to guard against blowing away the\n        # crontab in some obscure failure case.\n        assert ""no crontab"" in crontab_l_result.stderr, f""crontab failed: {crontab_l_result.stderr}""\n        current_crontab = """"\n\n    full_path = os.path.abspath(__file__)\n    if full_path not in current_crontab:\n        # Execute this script every ten minutes. We set the PATH to that used\n        # to run this install step to make sure that we have access to python3\n        # and beaker.\n        cron_line = (\n            f""*/10 * * * * bash -c \'export PATH={os.environ[\'PATH\']};""\n            f"" python3 {full_path} --action=resume --random-delay-seconds=60\'\\n""\n        )\n        new_crontab = current_crontab + cron_line\n        subprocess.run([""crontab"", ""-""], input=new_crontab, encoding=""utf-8"")\n\n    if args.action is Action.start:\n        assert args.experiment_id\n        start_autoresume(connection, args.experiment_id, args.max_resumes)\n    elif args.action is Action.stop:\n        assert args.experiment_id\n        stop_autoresume(connection, args.experiment_id)\n    elif args.action is Action.resume:\n        beaker = BeakerWrapper()\n        resume(connection, beaker)\n    else:\n        raise Exception(f""Unaccounted for action {args.action}"")\n    connection.close()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--action"", type=Action, choices=list(Action), required=True)\n    parser.add_argument(""--experiment-id"", type=str)\n    parser.add_argument(""--max-resumes"", type=int, default=10)\n    parser.add_argument(""--random-delay-seconds"", type=int, default=0)\n    args = parser.parse_args()\n\n    try:\n        main(args)\n    except Exception:\n        # Ensure traces are logged.\n        # TODO(brendanr): Is there a better way to do this?\n        logger.exception(""Fatal error"")\n        raise\n'"
scripts/ai2_internal/run_with_beaker.py,0,"b'#! /usr/bin/env python\n\n# Script to launch AllenNLP Beaker jobs.\n\nimport argparse\nimport os\nimport json\nimport random\nimport tempfile\nimport subprocess\nimport sys\n\n# This has to happen before we import spacy (even indirectly), because for some crazy reason spacy\n# thought it was a good idea to set the random seed on import...\nrandom_int = random.randint(0, 2 ** 32)\n\nsys.path.insert(\n    0, os.path.dirname(os.path.abspath(os.path.join(os.path.join(__file__, os.pardir), os.pardir)))\n)\n\nfrom allennlp.common.params import Params\n\n\ndef main(param_file: str, args: argparse.Namespace):\n    commit = subprocess.check_output([""git"", ""rev-parse"", ""HEAD""], universal_newlines=True).strip()\n    docker_image = f""allennlp/allennlp:{commit}""\n    overrides = args.overrides\n\n    # Reads params and sets environment.\n    ext_vars = {}\n\n    for var in args.env:\n        key, value = var.split(""="")\n        ext_vars[key] = value\n\n    params = Params.from_file(param_file, overrides, ext_vars)\n\n    # Write params as json. Otherwise Jsonnet\'s import feature breaks.\n    params_dir = tempfile.mkdtemp(prefix=""config"")\n    compiled_params_path = os.path.join(params_dir, ""config.json"")\n    params.to_file(compiled_params_path)\n    print(f""Compiled jsonnet config written to {compiled_params_path}."")\n\n    flat_params = params.as_flat_dict()\n    env = {}\n    for k, v in flat_params.items():\n        k = str(k).replace(""."", ""_"")\n        env[k] = str(v)\n\n    # If the git repository is dirty, add a random hash.\n    result = subprocess.run(""git diff-index --quiet HEAD --"", shell=True)\n    if result.returncode != 0:\n        dirty_hash = ""%x"" % random_int\n        docker_image += ""-"" + dirty_hash\n\n    if args.image:\n        image = args.image\n        print(f""Using the specified image: {image}"")\n    else:\n        print(f""Building the Docker image ({docker_image})..."")\n        subprocess.run(f""docker build -t {docker_image} ."", shell=True, check=True)\n\n        print(""Create a Beaker image..."")\n        image = subprocess.check_output(\n            f""beaker image create --quiet {docker_image}"", shell=True, universal_newlines=True\n        ).strip()\n        print(f""  Image created: {docker_image}"")\n\n    config_dataset_id = subprocess.check_output(\n        f""beaker dataset create --quiet {params_dir}/*"", shell=True, universal_newlines=True\n    ).strip()\n\n    # Arguments that differ between preemptible and regular machine execution.\n    if args.preemptible:\n        allennlp_prefix = [""/stage/allennlp/resumable_train.sh"", ""/output"", ""/config/config.json""]\n    else:\n        allennlp_prefix = [\n            ""python"",\n            ""-m"",\n            ""allennlp.run"",\n            ""train"",\n            ""/config/config.json"",\n            ""-s"",\n            ""/output"",\n        ]\n\n    # All other arguments\n    allennlp_suffix = [""--file-friendly-logging""]\n    for package_name in args.include_package:\n        allennlp_suffix.append(""--include-package"")\n        allennlp_suffix.append(package_name)\n\n    allennlp_command = allennlp_prefix + allennlp_suffix\n\n    dataset_mounts = []\n    for source in args.source + [f""{config_dataset_id}:/config""]:\n        datasetId, containerPath = source.split("":"")\n        dataset_mounts.append({""datasetId"": datasetId, ""containerPath"": containerPath})\n\n    for var in args.env:\n        key, value = var.split(""="")\n        env[key] = value\n\n    requirements = {}\n    if args.cpu:\n        requirements[""cpu""] = float(args.cpu)\n    if args.memory:\n        requirements[""memory""] = args.memory\n    if args.gpu_count:\n        requirements[""gpuCount""] = int(args.gpu_count)\n    if args.preemptible:\n        requirements[""preemptible""] = True\n    config_spec = {\n        ""description"": args.desc,\n        ""image"": image,\n        ""resultPath"": ""/output"",\n        ""args"": allennlp_command,\n        ""datasetMounts"": dataset_mounts,\n        ""requirements"": requirements,\n        ""env"": env,\n    }\n    config_task = {""spec"": config_spec, ""name"": ""training""}\n\n    config = {""tasks"": [config_task]}\n\n    output_path = (\n        args.spec_output_path\n        if args.spec_output_path\n        else tempfile.mkstemp("".yaml"", ""beaker-config-"")[1]\n    )\n    with open(output_path, ""w"") as output:\n        output.write(json.dumps(config, indent=4))\n    print(f""Beaker spec written to {output_path}."")\n\n    experiment_command = [""beaker"", ""experiment"", ""create"", ""--quiet"", ""--file"", output_path]\n    if args.name:\n        experiment_command.append(""--name"")\n        experiment_command.append(args.name.replace("" "", ""-""))\n\n    def resume_command(experiment_id):\n        resume_daemon_path = os.path.join(os.path.dirname(__file__), ""resume_daemon.py"")\n        return [\n            # Run with python (instead of calling directly) in case the\n            # executable bit wasn\'t preserved for some reason.\n            ""python3"",\n            resume_daemon_path,\n            ""--action=start"",\n            f""--max-resumes={args.max_resumes}"",\n            f""--experiment-id={experiment_id}"",\n        ]\n\n    if args.dry_run:\n        print(""This is a dry run (--dry-run).  Launch your job with the following command:"")\n        print(""    "" + "" "".join(experiment_command))\n        if args.max_resumes > 0:\n            print(""Configure auto-resumes with the following command:"")\n            print(""    "" + "" "".join(resume_command(""$YOUR_EXPERIMENT_ID"")))\n    else:\n        print(""Running the experiment:"")\n        print(""    "" + "" "".join(experiment_command))\n        experiment_id = subprocess.check_output(experiment_command, universal_newlines=True).strip()\n        print(\n            f""Experiment {experiment_id} submitted. ""\n            f""See progress at https://beaker.org/ex/{experiment_id}""\n        )\n        if args.max_resumes > 0:\n            print(""Configuring auto-resumes:"")\n            print(""    "" + "" "".join(resume_command(experiment_id)))\n            subprocess.run(resume_command(experiment_id))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""param_file"", type=str, help=""The model configuration file."")\n    parser.add_argument(""--name"", type=str, help=""A name for the experiment."")\n    parser.add_argument(\n        ""--spec_output_path"", type=str, help=""The destination to write the experiment spec.""\n    )\n    parser.add_argument(\n        ""--dry-run"", action=""store_true"", help=""If specified, an experiment will not be created.""\n    )\n    parser.add_argument(\n        ""--image"", type=str, help=""The image to use (if unspecified one will be built)""\n    )\n    parser.add_argument(""--desc"", type=str, help=""A description for the experiment."")\n    parser.add_argument(\n        ""--env"",\n        action=""append"",\n        default=[],\n        help=""Set environment variables (e.g. NAME=value or NAME)"",\n    )\n    parser.add_argument(\n        ""--source"",\n        action=""append"",\n        default=[],\n        help=""Bind a remote data source (e.g. source-id:/target/path)"",\n    )\n    parser.add_argument(""--cpu"", help=""CPUs to reserve for this experiment (e.g., 0.5)"")\n    parser.add_argument(\n        ""--gpu-count"", default=1, help=""GPUs to use for this experiment (e.g., 1 (default))""\n    )\n    parser.add_argument(""--memory"", help=""Memory to reserve for this experiment (e.g., 1GB)"")\n    parser.add_argument(\n        ""--preemptible"", action=""store_true"", help=""Allow task to run on preemptible hardware""\n    )\n    parser.add_argument(\n        ""--max-resumes"",\n        type=int,\n        default=0,\n        help=""When running with --preemptible, use a cronjob to automatically resume this many times."",\n    )\n    parser.add_argument(\n        ""--include-package"",\n        type=str,\n        action=""append"",\n        default=[],\n        help=""Additional packages to include"",\n    )\n    parser.add_argument(\n        ""-o"",\n        ""--overrides"",\n        type=str,\n        default="""",\n        help=""a JSON structure used to override the experiment configuration"",\n    )\n\n    args = parser.parse_args()\n\n    if args.max_resumes > 0:\n        assert args.preemptible, ""--max-resumes requires --preemptible!""\n\n    main(args.param_file, args)\n'"
tests/commands/__init__.py,0,b''
tests/commands/evaluate_test.py,3,"b'import argparse\nimport json\nfrom typing import Iterator, List, Dict\n\nimport torch\nfrom flaky import flaky\nimport pytest\n\nfrom allennlp.commands.evaluate import evaluate_from_args, Evaluate, evaluate\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.dataloader import TensorDict\nfrom allennlp.models import Model\n\n\nclass DummyDataLoader:\n    def __init__(self, outputs: List[TensorDict]) -> None:\n        super().__init__()\n        self._outputs = outputs\n\n    def __iter__(self) -> Iterator[TensorDict]:\n        yield from self._outputs\n\n    def __len__(self):\n        return len(self._outputs)\n\n\nclass DummyModel(Model):\n    def __init__(self) -> None:\n        super().__init__(None)  # type: ignore\n\n    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:  # type: ignore\n        return kwargs\n\n\nclass TestEvaluate(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n\n        self.parser = argparse.ArgumentParser(description=""Testing"")\n        subparsers = self.parser.add_subparsers(title=""Commands"", metavar="""")\n        Evaluate().add_subparser(subparsers)\n\n    def test_evaluate_calculates_average_loss(self):\n        losses = [7.0, 9.0, 8.0]\n        outputs = [{""loss"": torch.Tensor([loss])} for loss in losses]\n        data_loader = DummyDataLoader(outputs)\n        metrics = evaluate(DummyModel(), data_loader, -1, """")\n        assert metrics[""loss""] == pytest.approx(8.0)\n\n    def test_evaluate_calculates_average_loss_with_weights(self):\n        losses = [7.0, 9.0, 8.0]\n        weights = [10, 2, 1.5]\n        inputs = zip(losses, weights)\n        outputs = [\n            {""loss"": torch.Tensor([loss]), ""batch_weight"": torch.Tensor([weight])}\n            for loss, weight in inputs\n        ]\n        data_loader = DummyDataLoader(outputs)\n        metrics = evaluate(DummyModel(), data_loader, -1, ""batch_weight"")\n        assert metrics[""loss""] == pytest.approx((70 + 18 + 12) / 13.5)\n\n    @flaky\n    def test_evaluate_from_args(self):\n        kebab_args = [\n            ""evaluate"",\n            str(\n                self.FIXTURES_ROOT / ""simple_tagger_with_span_f1"" / ""serialization"" / ""model.tar.gz""\n            ),\n            str(self.FIXTURES_ROOT / ""data"" / ""conll2003.txt""),\n            ""--cuda-device"",\n            ""-1"",\n        ]\n\n        args = self.parser.parse_args(kebab_args)\n        metrics = evaluate_from_args(args)\n        assert metrics.keys() == {\n            ""accuracy"",\n            ""accuracy3"",\n            ""precision-overall"",\n            ""recall-overall"",\n            ""f1-measure-overall"",\n            ""loss"",\n        }\n\n    def test_output_file_evaluate_from_args(self):\n        output_file = str(self.TEST_DIR / ""metrics.json"")\n        kebab_args = [\n            ""evaluate"",\n            str(\n                self.FIXTURES_ROOT / ""simple_tagger_with_span_f1"" / ""serialization"" / ""model.tar.gz""\n            ),\n            str(self.FIXTURES_ROOT / ""data"" / ""conll2003.txt""),\n            ""--cuda-device"",\n            ""-1"",\n            ""--output-file"",\n            output_file,\n        ]\n        args = self.parser.parse_args(kebab_args)\n        computed_metrics = evaluate_from_args(args)\n        with open(output_file, ""r"") as file:\n            saved_metrics = json.load(file)\n        assert computed_metrics == saved_metrics\n\n    def test_evaluate_works_with_vocab_expansion(self):\n        archive_path = str(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        # snli2 has a extra token (""seahorse"") in it.\n        evaluate_data_path = str(\n            self.FIXTURES_ROOT / ""data"" / ""text_classification_json"" / ""imdb_corpus2.jsonl""\n        )\n        embeddings_filename = str(\n            self.FIXTURES_ROOT / ""data"" / ""unawarded_embeddings.gz""\n        )  # has only unawarded vector\n        embedding_sources_mapping = json.dumps(\n            {""_text_field_embedder.token_embedder_tokens"": embeddings_filename}\n        )\n        kebab_args = [""evaluate"", archive_path, evaluate_data_path, ""--cuda-device"", ""-1""]\n\n        # TODO(mattg): the unawarded_embeddings.gz file above doesn\'t exist, but this test still\n        # passes.  This suggests that vocab extension in evaluate isn\'t currently doing anything,\n        # and so it is broken.\n\n        # Evaluate 1 with no vocab expansion,\n        # Evaluate 2 with vocab expansion with no pretrained embedding file.\n        # Evaluate 3 with vocab expansion with given pretrained embedding file.\n        metrics_1 = evaluate_from_args(self.parser.parse_args(kebab_args))\n        metrics_2 = evaluate_from_args(self.parser.parse_args(kebab_args + [""--extend-vocab""]))\n        metrics_3 = evaluate_from_args(\n            self.parser.parse_args(\n                kebab_args + [""--embedding-sources-mapping"", embedding_sources_mapping]\n            )\n        )\n        assert metrics_1 != metrics_2\n        assert metrics_2 != metrics_3\n'"
tests/commands/find_learning_rate_test.py,0,"b'import argparse\nimport os\n\nimport pytest\n\nfrom allennlp.common import Params\nfrom allennlp.data import Vocabulary\nfrom allennlp.data import DataLoader\nfrom allennlp.models import Model\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase, requires_multi_gpu\nfrom allennlp.commands.find_learning_rate import (\n    search_learning_rate,\n    find_learning_rate_from_args,\n    find_learning_rate_model,\n    FindLearningRate,\n)\nfrom allennlp.training import Trainer\nfrom allennlp.training.util import datasets_from_params\n\n\ndef is_matplotlib_installed():\n    try:\n        import matplotlib  # noqa: F401 - Matplotlib is optional.\n    except:  # noqa: E722. Any exception means we don\'t have a working matplotlib.\n        return False\n    return True\n\n\nclass TestFindLearningRate(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.params = lambda: Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": str(self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""),\n                ""validation_data_path"": str(self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""),\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""cuda_device"": -1, ""num_epochs"": 2, ""optimizer"": ""adam""},\n            }\n        )\n\n    @pytest.mark.skipif(not is_matplotlib_installed(), reason=""matplotlib dependency is optional"")\n    def test_find_learning_rate(self):\n        find_learning_rate_model(\n            self.params(),\n            os.path.join(self.TEST_DIR, ""test_find_learning_rate""),\n            start_lr=1e-5,\n            end_lr=1,\n            num_batches=100,\n            linear_steps=True,\n            stopping_factor=None,\n            force=False,\n        )\n\n        # It\'s OK if serialization dir exists but is empty:\n        serialization_dir2 = os.path.join(self.TEST_DIR, ""empty_directory"")\n        assert not os.path.exists(serialization_dir2)\n        os.makedirs(serialization_dir2)\n        find_learning_rate_model(\n            self.params(),\n            serialization_dir2,\n            start_lr=1e-5,\n            end_lr=1,\n            num_batches=100,\n            linear_steps=True,\n            stopping_factor=None,\n            force=False,\n        )\n\n        # It\'s not OK if serialization dir exists and has junk in it non-empty:\n        serialization_dir3 = os.path.join(self.TEST_DIR, ""non_empty_directory"")\n        assert not os.path.exists(serialization_dir3)\n        os.makedirs(serialization_dir3)\n        with open(os.path.join(serialization_dir3, ""README.md""), ""w"") as f:\n            f.write(""TEST"")\n\n        with pytest.raises(ConfigurationError):\n            find_learning_rate_model(\n                self.params(),\n                serialization_dir3,\n                start_lr=1e-5,\n                end_lr=1,\n                num_batches=100,\n                linear_steps=True,\n                stopping_factor=None,\n                force=False,\n            )\n\n        # ... unless you use the --force flag.\n        find_learning_rate_model(\n            self.params(),\n            serialization_dir3,\n            start_lr=1e-5,\n            end_lr=1,\n            num_batches=100,\n            linear_steps=True,\n            stopping_factor=None,\n            force=True,\n        )\n\n    def test_find_learning_rate_args(self):\n        parser = argparse.ArgumentParser(description=""Testing"")\n        subparsers = parser.add_subparsers(title=""Commands"", metavar="""")\n        FindLearningRate().add_subparser(subparsers)\n\n        for serialization_arg in [""-s"", ""--serialization-dir""]:\n            raw_args = [""find-lr"", ""path/to/params"", serialization_arg, ""serialization_dir""]\n\n            args = parser.parse_args(raw_args)\n\n            assert args.func == find_learning_rate_from_args\n            assert args.param_path == ""path/to/params""\n            assert args.serialization_dir == ""serialization_dir""\n\n        # config is required\n        with pytest.raises(SystemExit) as cm:\n            parser.parse_args([""find-lr"", ""-s"", ""serialization_dir""])\n            assert cm.exception.code == 2  # argparse code for incorrect usage\n\n        # serialization dir is required\n        with pytest.raises(SystemExit) as cm:\n            parser.parse_args([""find-lr"", ""path/to/params""])\n            assert cm.exception.code == 2  # argparse code for incorrect usage\n\n    @requires_multi_gpu\n    def test_find_learning_rate_multi_gpu(self):\n        params = self.params()\n        del params[""trainer""][""cuda_device""]\n        params[""distributed""] = Params({})\n        params[""distributed""][""cuda_devices""] = [0, 1]\n\n        with pytest.raises(AssertionError) as execinfo:\n            find_learning_rate_model(\n                params,\n                os.path.join(self.TEST_DIR, ""test_find_learning_rate_multi_gpu""),\n                start_lr=1e-5,\n                end_lr=1,\n                num_batches=100,\n                linear_steps=True,\n                stopping_factor=None,\n                force=False,\n            )\n        assert ""DistributedDataParallel"" in str(execinfo.value)\n\n\nclass TestSearchLearningRate(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        params = Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": str(self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""),\n                ""validation_data_path"": str(self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""),\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""cuda_device"": -1, ""num_epochs"": 2, ""optimizer"": ""adam""},\n            }\n        )\n        all_datasets = datasets_from_params(params)\n        vocab = Vocabulary.from_params(\n            params.pop(""vocabulary"", {}),\n            instances=(instance for dataset in all_datasets.values() for instance in dataset),\n        )\n        model = Model.from_params(vocab=vocab, params=params.pop(""model""))\n        train_data = all_datasets[""train""]\n        train_data.index_with(vocab)\n\n        data_loader = DataLoader.from_params(dataset=train_data, params=params.pop(""data_loader""))\n        trainer_params = params.pop(""trainer"")\n        serialization_dir = os.path.join(self.TEST_DIR, ""test_search_learning_rate"")\n\n        self.trainer = Trainer.from_params(\n            model=model,\n            serialization_dir=serialization_dir,\n            data_loader=data_loader,\n            train_data=train_data,\n            params=trainer_params,\n            validation_data=None,\n            validation_iterator=None,\n        )\n\n    def test_search_learning_rate_with_num_batches_less_than_ten(self):\n        with pytest.raises(ConfigurationError):\n            search_learning_rate(self.trainer, num_batches=9)\n\n    def test_search_learning_rate_linear_steps(self):\n        learning_rates_losses = search_learning_rate(self.trainer, linear_steps=True)\n        assert len(learning_rates_losses) > 1\n\n    def test_search_learning_rate_without_stopping_factor(self):\n        learning_rates, losses = search_learning_rate(\n            self.trainer, num_batches=100, stopping_factor=None\n        )\n        assert len(learning_rates) == 101\n        assert len(losses) == 101\n'"
tests/commands/main_test.py,0,"b'import shutil\nimport sys\n\nimport pytest\nfrom overrides import overrides\n\nfrom allennlp.commands import main\nfrom allennlp.commands.subcommand import Subcommand\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.plugins import discover_plugins\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.util import push_python_path, pushd\n\n\nclass TestMain(AllenNlpTestCase):\n    def test_fails_on_unknown_command(self):\n        sys.argv = [\n            ""bogus"",  # command\n            ""unknown_model"",  # model_name\n            ""bogus file"",  # input_file\n            ""--output-file"",\n            ""bogus out file"",\n            ""--silent"",\n        ]\n\n        with pytest.raises(SystemExit) as cm:\n            main()\n\n        assert cm.value.code == 2  # argparse code for incorrect usage\n\n    def test_subcommand_overrides(self):\n        called = False\n\n        def do_nothing(_):\n            nonlocal called\n            called = True\n\n        @Subcommand.register(""evaluate"", exist_ok=True)\n        class FakeEvaluate(Subcommand):  # noqa\n            @overrides\n            def add_subparser(self, parser):\n                subparser = parser.add_parser(self.name, description=""fake"", help=""fake help"")\n                subparser.set_defaults(func=do_nothing)\n                return subparser\n\n        sys.argv = [""allennlp"", ""evaluate""]\n        main()\n        assert called\n\n    def test_other_modules(self):\n        # Create a new package in a temporary dir\n        packagedir = self.TEST_DIR / ""testpackage""\n        packagedir.mkdir()\n        (packagedir / ""__init__.py"").touch()\n\n        # And add that directory to the path\n        with push_python_path(self.TEST_DIR):\n            # Write out a duplicate model there, but registered under a different name.\n            from allennlp.models import simple_tagger\n\n            with open(simple_tagger.__file__) as model_file:\n                code = model_file.read().replace(\n                    """"""@Model.register(""simple_tagger"")"""""",\n                    """"""@Model.register(""duplicate-test-tagger"")"""""",\n                )\n\n            with open(packagedir / ""model.py"", ""w"") as new_model_file:\n                new_model_file.write(code)\n\n            # Copy fixture there too.\n            shutil.copy(self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv"", self.TEST_DIR)\n            data_path = str(self.TEST_DIR / ""sequence_tagging.tsv"")\n\n            # Write out config file\n            config_path = self.TEST_DIR / ""config.json""\n            config_json = """"""{\n                    ""model"": {\n                            ""type"": ""duplicate-test-tagger"",\n                            ""text_field_embedder"": {\n                                    ""token_embedders"": {\n                                            ""tokens"": {\n                                                    ""type"": ""embedding"",\n                                                    ""embedding_dim"": 5\n                                            }\n                                    }\n                            },\n                            ""encoder"": {\n                                    ""type"": ""lstm"",\n                                    ""input_size"": 5,\n                                    ""hidden_size"": 7,\n                                    ""num_layers"": 2\n                            }\n                    },\n                    ""dataset_reader"": {""type"": ""sequence_tagging""},\n                    ""train_data_path"": ""$$$"",\n                    ""validation_data_path"": ""$$$"",\n                    ""data_loader"": {""batch_size"": 2},\n                    ""trainer"": {\n                            ""num_epochs"": 2,\n                            ""optimizer"": ""adam""\n                    }\n                }"""""".replace(\n                ""$$$"", data_path\n            )\n            with open(config_path, ""w"") as config_file:\n                config_file.write(config_json)\n\n            serialization_dir = self.TEST_DIR / ""serialization""\n\n            # Run train with using the non-allennlp module.\n            sys.argv = [""allennlp"", ""train"", str(config_path), ""-s"", str(serialization_dir)]\n\n            # Shouldn\'t be able to find the model.\n            with pytest.raises(ConfigurationError):\n                main()\n\n            # Now add the --include-package flag and it should work.\n            # We also need to add --recover since the output directory already exists.\n            sys.argv.extend([""--recover"", ""--include-package"", ""testpackage""])\n\n            main()\n\n            # Rewrite out config file, but change a value.\n            with open(config_path, ""w"") as new_config_file:\n                new_config_file.write(config_json.replace(\'""num_epochs"": 2,\', \'""num_epochs"": 4,\'))\n\n            # This should fail because the config.json does not match that in the serialization directory.\n            with pytest.raises(ConfigurationError):\n                main()\n\n    def test_file_plugin_loaded(self):\n        plugins_root = self.FIXTURES_ROOT / ""plugins""\n\n        sys.argv = [""allennlp""]\n\n        available_plugins = set(discover_plugins())\n        assert available_plugins == set()\n\n        with pushd(plugins_root):\n            main()\n            subcommands_available = Subcommand.list_available()\n            assert ""d"" in subcommands_available\n'"
tests/commands/no_op_train_test.py,3,"b'from typing import Dict\n\nimport torch\n\nfrom allennlp.commands.train import train_model\nfrom allennlp.common import Params\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.models import load_archive, Model\n\nSEQUENCE_TAGGING_DATA_PATH = str(AllenNlpTestCase.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv"")\n\n\n@Model.register(""constant"")\nclass ConstantModel(Model):\n    def forward(self, *inputs) -> Dict[str, torch.Tensor]:\n        return {""class"": torch.tensor(98)}\n\n\nclass TestTrain(AllenNlpTestCase):\n    def test_train_model(self):\n        params = lambda: Params(\n            {\n                ""model"": {""type"": ""constant""},\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""type"": ""no_op""},\n            }\n        )\n\n        serialization_dir = self.TEST_DIR / ""serialization_directory""\n        train_model(params(), serialization_dir=serialization_dir)\n        archive = load_archive(str(serialization_dir / ""model.tar.gz""))\n        model = archive.model\n        assert model.forward(torch.tensor([1, 2, 3]))[""class""] == torch.tensor(98)\n        assert model.vocab.get_vocab_size() == 9\n'"
tests/commands/predict_test.py,0,"b'import argparse\nimport csv\nimport io\nimport json\nimport os\nimport pathlib\nimport shutil\nimport sys\nimport tempfile\n\nimport pytest\n\nfrom allennlp.commands import main\nfrom allennlp.commands.predict import Predict\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.util import JsonDict, push_python_path\nfrom allennlp.data.dataset_readers import DatasetReader, TextClassificationJsonReader\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors import Predictor, TextClassifierPredictor\n\n\nclass TestPredict(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.classifier_model_path = (\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        self.classifier_data_path = (\n            self.FIXTURES_ROOT / ""data"" / ""text_classification_json"" / ""imdb_corpus.jsonl""\n        )\n        self.tempdir = pathlib.Path(tempfile.mkdtemp())\n        self.infile = self.tempdir / ""inputs.txt""\n        self.outfile = self.tempdir / ""outputs.txt""\n\n    def test_add_predict_subparser(self):\n        parser = argparse.ArgumentParser(description=""Testing"")\n        subparsers = parser.add_subparsers(title=""Commands"", metavar="""")\n        Predict().add_subparser(subparsers)\n\n        kebab_args = [\n            ""predict"",  # command\n            ""/path/to/archive"",  # archive\n            ""/dev/null"",  # input_file\n            ""--output-file"",\n            ""/dev/null"",\n            ""--batch-size"",\n            ""10"",\n            ""--cuda-device"",\n            ""0"",\n            ""--silent"",\n        ]\n\n        args = parser.parse_args(kebab_args)\n\n        assert args.func.__name__ == ""_predict""\n        assert args.archive_file == ""/path/to/archive""\n        assert args.output_file == ""/dev/null""\n        assert args.batch_size == 10\n        assert args.cuda_device == 0\n        assert args.silent\n\n    def test_works_with_known_model(self):\n        with open(self.infile, ""w"") as f:\n            f.write(""""""{""sentence"": ""the seahawks won the super bowl in 2016""}\\n"""""")\n            f.write(""""""{""sentence"": ""the mariners won the super bowl in 2037""}\\n"""""")\n\n        sys.argv = [\n            ""__main__.py"",  # executable\n            ""predict"",  # command\n            str(self.classifier_model_path),\n            str(self.infile),  # input_file\n            ""--output-file"",\n            str(self.outfile),\n            ""--silent"",\n        ]\n\n        main()\n\n        assert os.path.exists(self.outfile)\n\n        with open(self.outfile, ""r"") as f:\n            results = [json.loads(line) for line in f]\n\n        assert len(results) == 2\n        for result in results:\n            assert set(result.keys()) == {""label"", ""logits"", ""probs"", ""tokens"", ""token_ids""}\n\n        shutil.rmtree(self.tempdir)\n\n    def test_using_dataset_reader_works_with_known_model(self):\n\n        sys.argv = [\n            ""__main__.py"",  # executable\n            ""predict"",  # command\n            str(self.classifier_model_path),\n            str(self.classifier_data_path),  # input_file\n            ""--output-file"",\n            str(self.outfile),\n            ""--silent"",\n            ""--use-dataset-reader"",\n        ]\n\n        main()\n\n        assert os.path.exists(self.outfile)\n\n        with open(self.outfile, ""r"") as f:\n            results = [json.loads(line) for line in f]\n\n        assert len(results) == 3\n        for result in results:\n            assert set(result.keys()) == {""label"", ""logits"", ""loss"", ""probs"", ""tokens"", ""token_ids""}\n\n        shutil.rmtree(self.tempdir)\n\n    def test_uses_correct_dataset_reader(self):\n        # We\'re going to use a fake predictor for this test, just checking that we loaded the\n        # correct dataset reader.  We\'ll also create a fake dataset reader that subclasses the\n        # expected one, and specify that one for validation.\n        @Predictor.register(""test-predictor"")\n        class _TestPredictor(Predictor):\n            def dump_line(self, outputs: JsonDict) -> str:\n                data = {\n                    ""dataset_reader_type"": type(self._dataset_reader).__name__  # type: ignore\n                }\n                return json.dumps(data) + ""\\n""\n\n            def load_line(self, line: str) -> JsonDict:\n                raise NotImplementedError\n\n        @DatasetReader.register(""fake-reader"")\n        class FakeDatasetReader(TextClassificationJsonReader):\n            pass\n\n        # --use-dataset-reader argument only should use validation\n        sys.argv = [\n            ""__main__.py"",  # executable\n            ""predict"",  # command\n            str(self.classifier_model_path),\n            str(self.classifier_data_path),  # input_file\n            ""--output-file"",\n            str(self.outfile),\n            ""--overrides"",\n            \'{""validation_dataset_reader"": {""type"": ""fake-reader""}}\',\n            ""--silent"",\n            ""--predictor"",\n            ""test-predictor"",\n            ""--use-dataset-reader"",\n        ]\n        main()\n        assert os.path.exists(self.outfile)\n        with open(self.outfile, ""r"") as f:\n            results = [json.loads(line) for line in f]\n            assert results[0][""dataset_reader_type""] == ""FakeDatasetReader""\n\n        # --use-dataset-reader, override with train\n        sys.argv = [\n            ""__main__.py"",  # executable\n            ""predict"",  # command\n            str(self.classifier_model_path),\n            str(self.classifier_data_path),  # input_file\n            ""--output-file"",\n            str(self.outfile),\n            ""--overrides"",\n            \'{""validation_dataset_reader"": {""type"": ""fake-reader""}}\',\n            ""--silent"",\n            ""--predictor"",\n            ""test-predictor"",\n            ""--use-dataset-reader"",\n            ""--dataset-reader-choice"",\n            ""train"",\n        ]\n        main()\n        assert os.path.exists(self.outfile)\n        with open(self.outfile, ""r"") as f:\n            results = [json.loads(line) for line in f]\n            assert results[0][""dataset_reader_type""] == ""TextClassificationJsonReader""\n\n        # --use-dataset-reader, override with validation\n        sys.argv = [\n            ""__main__.py"",  # executable\n            ""predict"",  # command\n            str(self.classifier_model_path),\n            str(self.classifier_data_path),  # input_file\n            ""--output-file"",\n            str(self.outfile),\n            ""--overrides"",\n            \'{""validation_dataset_reader"": {""type"": ""fake-reader""}}\',\n            ""--silent"",\n            ""--predictor"",\n            ""test-predictor"",\n            ""--use-dataset-reader"",\n            ""--dataset-reader-choice"",\n            ""validation"",\n        ]\n        main()\n        assert os.path.exists(self.outfile)\n        with open(self.outfile, ""r"") as f:\n            results = [json.loads(line) for line in f]\n            assert results[0][""dataset_reader_type""] == ""FakeDatasetReader""\n\n        # No --use-dataset-reader flag, fails because the loading logic\n        # is not implemented in the testing predictor\n        sys.argv = [\n            ""__main__.py"",  # executable\n            ""predict"",  # command\n            str(self.classifier_model_path),\n            str(self.classifier_data_path),  # input_file\n            ""--output-file"",\n            str(self.outfile),\n            ""--overrides"",\n            \'{""validation_dataset_reader"": {""type"": ""fake-reader""}}\',\n            ""--silent"",\n            ""--predictor"",\n            ""test-predictor"",\n        ]\n        with pytest.raises(NotImplementedError):\n            main()\n\n    def test_base_predictor(self):\n        # Tests when no Predictor is found and the base class implementation is used\n        model_path = str(self.classifier_model_path)\n        archive = load_archive(model_path)\n        model_type = archive.config.get(""model"").get(""type"")\n        # Makes sure that we don\'t have a default_predictor for it. Otherwise the base class\n        # implementation wouldn\'t be used\n        from allennlp.models import Model\n\n        model_class, _ = Model.resolve_class_name(model_type)\n        saved_default_predictor = model_class.default_predictor\n        model_class.default_predictor = None\n        try:\n            # Doesn\'t use a --predictor\n            sys.argv = [\n                ""__main__.py"",  # executable\n                ""predict"",  # command\n                model_path,\n                str(self.classifier_data_path),  # input_file\n                ""--output-file"",\n                str(self.outfile),\n                ""--silent"",\n                ""--use-dataset-reader"",\n            ]\n            main()\n            assert os.path.exists(self.outfile)\n            with open(self.outfile, ""r"") as f:\n                results = [json.loads(line) for line in f]\n\n            assert len(results) == 3\n            for result in results:\n                assert set(result.keys()) == {\n                    ""logits"",\n                    ""probs"",\n                    ""label"",\n                    ""loss"",\n                    ""tokens"",\n                    ""token_ids"",\n                }\n        finally:\n            model_class.default_predictor = saved_default_predictor\n\n    def test_batch_prediction_works_with_known_model(self):\n        with open(self.infile, ""w"") as f:\n            f.write(""""""{""sentence"": ""the seahawks won the super bowl in 2016""}\\n"""""")\n            f.write(""""""{""sentence"": ""the mariners won the super bowl in 2037""}\\n"""""")\n\n        sys.argv = [\n            ""__main__.py"",  # executable\n            ""predict"",  # command\n            str(self.classifier_model_path),\n            str(self.infile),  # input_file\n            ""--output-file"",\n            str(self.outfile),\n            ""--silent"",\n            ""--batch-size"",\n            ""2"",\n        ]\n\n        main()\n\n        assert os.path.exists(self.outfile)\n        with open(self.outfile, ""r"") as f:\n            results = [json.loads(line) for line in f]\n\n        assert len(results) == 2\n        for result in results:\n            assert set(result.keys()) == {""label"", ""logits"", ""probs"", ""tokens"", ""token_ids""}\n\n        shutil.rmtree(self.tempdir)\n\n    def test_fails_without_required_args(self):\n        sys.argv = [\n            ""__main__.py"",\n            ""predict"",\n            ""/path/to/archive"",\n        ]  # executable  # command  # archive, but no input file\n\n        with pytest.raises(SystemExit) as cm:\n            main()\n\n        assert cm.value.code == 2  # argparse code for incorrect usage\n\n    def test_can_specify_predictor(self):\n        @Predictor.register(""classification-explicit"")\n        class ExplicitPredictor(TextClassifierPredictor):\n            """"""same as classifier predictor but with an extra field""""""\n\n            def predict_json(self, inputs: JsonDict) -> JsonDict:\n                result = super().predict_json(inputs)\n                result[""explicit""] = True\n                return result\n\n        with open(self.infile, ""w"") as f:\n            f.write(""""""{""sentence"": ""the seahawks won the super bowl in 2016""}\\n"""""")\n            f.write(""""""{""sentence"": ""the mariners won the super bowl in 2037""}\\n"""""")\n\n        sys.argv = [\n            ""__main__.py"",  # executable\n            ""predict"",  # command\n            str(self.classifier_model_path),\n            str(self.infile),  # input_file\n            ""--output-file"",\n            str(self.outfile),\n            ""--predictor"",\n            ""classification-explicit"",\n            ""--silent"",\n        ]\n\n        main()\n        assert os.path.exists(self.outfile)\n\n        with open(self.outfile, ""r"") as f:\n            results = [json.loads(line) for line in f]\n\n        assert len(results) == 2\n        # Overridden predictor should output extra field\n        for result in results:\n            assert set(result.keys()) == {\n                ""label"",\n                ""logits"",\n                ""explicit"",\n                ""probs"",\n                ""tokens"",\n                ""token_ids"",\n            }\n\n        shutil.rmtree(self.tempdir)\n\n    def test_other_modules(self):\n        # Create a new package in a temporary dir\n        packagedir = self.TEST_DIR / ""testpackage""\n        packagedir.mkdir()\n        (packagedir / ""__init__.py"").touch()\n\n        # And add that directory to the path\n        with push_python_path(self.TEST_DIR):\n            # Write out a duplicate predictor there, but registered under a different name.\n            from allennlp.predictors import text_classifier\n\n            with open(text_classifier.__file__) as f:\n                code = f.read().replace(\n                    """"""@Predictor.register(""text_classifier"")"""""",\n                    """"""@Predictor.register(""duplicate-test-predictor"")"""""",\n                )\n\n            with open(os.path.join(packagedir, ""predictor.py""), ""w"") as f:\n                f.write(code)\n\n            self.infile = os.path.join(self.TEST_DIR, ""inputs.txt"")\n            self.outfile = os.path.join(self.TEST_DIR, ""outputs.txt"")\n\n            with open(self.infile, ""w"") as f:\n                f.write(""""""{""sentence"": ""the seahawks won the super bowl in 2016""}\\n"""""")\n                f.write(""""""{""sentence"": ""the mariners won the super bowl in 2037""}\\n"""""")\n\n            sys.argv = [\n                ""__main__.py"",  # executable\n                ""predict"",  # command\n                str(self.classifier_model_path),\n                str(self.infile),  # input_file\n                ""--output-file"",\n                str(self.outfile),\n                ""--predictor"",\n                ""duplicate-test-predictor"",\n                ""--silent"",\n            ]\n\n            # Should raise ConfigurationError, because predictor is unknown\n            with pytest.raises(ConfigurationError):\n                main()\n\n            # But once we include testpackage, it should be known\n            sys.argv.extend([""--include-package"", ""testpackage""])\n            main()\n\n            assert os.path.exists(self.outfile)\n\n            with open(self.outfile, ""r"") as f:\n                results = [json.loads(line) for line in f]\n\n            assert len(results) == 2\n            # Overridden predictor should output extra field\n            for result in results:\n                assert set(result.keys()) == {""label"", ""logits"", ""probs"", ""tokens"", ""token_ids""}\n\n    def test_alternative_file_formats(self):\n        @Predictor.register(""classification-csv"")\n        class CsvPredictor(TextClassifierPredictor):\n            """"""same as classification predictor but using CSV inputs and outputs""""""\n\n            def load_line(self, line: str) -> JsonDict:\n                reader = csv.reader([line])\n                sentence, label = next(reader)\n                return {""sentence"": sentence, ""label"": label}\n\n            def dump_line(self, outputs: JsonDict) -> str:\n                output = io.StringIO()\n                writer = csv.writer(output)\n                row = [outputs[""label""], *outputs[""probs""]]\n\n                writer.writerow(row)\n                return output.getvalue()\n\n        with open(self.infile, ""w"") as f:\n            writer = csv.writer(f)\n            writer.writerow([""the seahawks won the super bowl in 2016"", ""pos""])\n            writer.writerow([""the mariners won the super bowl in 2037"", ""neg""])\n\n        sys.argv = [\n            ""__main__.py"",  # executable\n            ""predict"",  # command\n            str(self.classifier_model_path),\n            str(self.infile),  # input_file\n            ""--output-file"",\n            str(self.outfile),\n            ""--predictor"",\n            ""classification-csv"",\n            ""--silent"",\n        ]\n\n        main()\n        assert os.path.exists(self.outfile)\n\n        with open(self.outfile) as f:\n            reader = csv.reader(f)\n            results = [row for row in reader]\n\n        assert len(results) == 2\n        for row in results:\n            assert len(row) == 3  # label and 2 class probabilities\n            label, *probs = row\n            for prob in probs:\n                assert 0 <= float(prob) <= 1\n            assert label != """"\n\n        shutil.rmtree(self.tempdir)\n'"
tests/commands/print_results_test.py,0,"b'import os\nimport json\nimport sys\nimport pathlib\nimport tempfile\nimport io\nfrom contextlib import redirect_stdout\n\nfrom allennlp.commands import main\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestPrintResults(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n\n        self.out_dir1 = pathlib.Path(tempfile.mkdtemp(prefix=""hi""))\n        self.out_dir2 = pathlib.Path(tempfile.mkdtemp(prefix=""hi""))\n\n        self.directory1 = self.TEST_DIR / ""results1""\n        self.directory2 = self.TEST_DIR / ""results2""\n        self.directory3 = self.TEST_DIR / ""results3""\n        os.makedirs(self.directory1)\n        os.makedirs(self.directory2)\n        os.makedirs(self.directory3)\n        json.dump(\n            {""train"": 1, ""test"": 2, ""dev"": 3},\n            open(os.path.join(self.directory1 / ""metrics.json""), ""w+""),\n        )\n        json.dump(\n            {""train"": 4, ""dev"": 5}, open(os.path.join(self.directory2 / ""metrics.json""), ""w+"")\n        )\n        json.dump(\n            {""train"": 6, ""dev"": 7}, open(os.path.join(self.directory3 / ""cool_metrics.json""), ""w+"")\n        )\n\n    def test_print_results(self):\n        kebab_args = [\n            ""__main__.py"",\n            ""print-results"",\n            str(self.TEST_DIR),\n            ""--keys"",\n            ""train"",\n            ""dev"",\n            ""test"",\n        ]\n        sys.argv = kebab_args\n        with io.StringIO() as buf, redirect_stdout(buf):\n            main()\n            output = buf.getvalue()\n\n        lines = output.strip().split(""\\n"")\n        assert lines[0] == ""model_run, train, dev, test""\n\n        expected_results = {\n            (str(self.directory1) + ""/metrics.json"", ""1"", ""3"", ""2""),\n            (str(self.directory2) + ""/metrics.json"", ""4"", ""5"", ""N/A""),\n        }\n        results = {tuple(line.split("", "")) for line in lines[1:]}\n        assert results == expected_results\n\n    def test_print_results_with_metrics_filename(self):\n        kebab_args = [\n            ""__main__.py"",\n            ""print-results"",\n            str(self.TEST_DIR),\n            ""--keys"",\n            ""train"",\n            ""dev"",\n            ""test"",\n            ""--metrics-filename"",\n            ""cool_metrics.json"",\n        ]\n        sys.argv = kebab_args\n        with io.StringIO() as buf, redirect_stdout(buf):\n            main()\n            output = buf.getvalue()\n\n        lines = output.strip().split(""\\n"")\n        assert lines[0] == ""model_run, train, dev, test""\n\n        expected_results = {(str(self.directory3) + ""/cool_metrics.json"", ""6"", ""7"", ""N/A"")}\n        results = {tuple(line.split("", "")) for line in lines[1:]}\n        assert results == expected_results\n'"
tests/commands/test_install_test.py,0,"b'import os\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.commands.test_install import _get_module_root\n\n\nclass TestTestInstall(AllenNlpTestCase):\n    def test_get_module_root(self):\n        """"""\n        When a user runs `allennlp test-install`, we have no idea where\n        they\'re running it from, so we do an `os.chdir` to the _module_\n        root in order to get all the paths in the fixtures to resolve properly.\n\n        The logic within `allennlp test-install` is pretty hard to test in\n        its entirety, so this test is verifies that the `os.chdir` component\n        works properly by checking that we correctly find the path to\n        `os.chdir` to.\n        """"""\n        project_root = _get_module_root()\n        assert os.path.exists(os.path.join(project_root, ""__main__.py""))\n'"
tests/commands/train_test.py,5,"b'import argparse\nimport json\n\nimport logging\nimport math\nimport os\nimport re\nimport shutil\nfrom collections import OrderedDict, Counter\nfrom typing import Iterable, Optional, List, Dict, Any\n\nimport pytest\nimport torch\n\nfrom allennlp.commands.train import Train, train_model, train_model_from_args, TrainModel\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase, cpu_or_gpu\nfrom allennlp.data import DatasetReader, Instance, Vocabulary\nfrom allennlp.data.dataloader import TensorDict\nfrom allennlp.models import load_archive, Model\nfrom allennlp.models.archival import CONFIG_NAME\nfrom allennlp.training import BatchCallback, GradientDescentTrainer\nfrom allennlp.training.learning_rate_schedulers import (\n    ExponentialLearningRateScheduler,\n    LearningRateScheduler,\n)\n\nSEQUENCE_TAGGING_DATA_PATH = str(AllenNlpTestCase.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv"")\nSEQUENCE_TAGGING_SHARDS_PATH = str(AllenNlpTestCase.FIXTURES_ROOT / ""data"" / ""shards"" / ""*"")\n\n\n@BatchCallback.register(""training_data_logger"")\nclass TrainingDataLoggerBatchCallback(BatchCallback):\n    def __call__(  # type: ignore\n        self,\n        trainer: ""GradientDescentTrainer"",\n        batch_inputs: List[TensorDict],\n        batch_outputs: List[Dict[str, Any]],\n        epoch: int,\n        batch_number: int,\n        is_training: bool,\n        is_master: bool,\n    ) -> None:\n        if is_training:\n            logger = logging.getLogger(__name__)\n            for batch in batch_inputs:\n                for metadata in batch[""metadata""]:\n                    logger.info(f""First word from training data: \'{metadata[\'words\'][0]}\'"")  # type: ignore\n\n\nclass TestTrain(AllenNlpTestCase):\n    def test_train_model(self):\n        params = lambda: Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""num_epochs"": 2, ""optimizer"": ""adam""},\n            }\n        )\n\n        train_model(params(), serialization_dir=os.path.join(self.TEST_DIR, ""test_train_model""))\n\n        # It\'s OK if serialization dir exists but is empty:\n        serialization_dir2 = os.path.join(self.TEST_DIR, ""empty_directory"")\n        assert not os.path.exists(serialization_dir2)\n        os.makedirs(serialization_dir2)\n        train_model(params(), serialization_dir=serialization_dir2)\n\n        # It\'s not OK if serialization dir exists and has junk in it non-empty:\n        serialization_dir3 = os.path.join(self.TEST_DIR, ""non_empty_directory"")\n        assert not os.path.exists(serialization_dir3)\n        os.makedirs(serialization_dir3)\n        with open(os.path.join(serialization_dir3, ""README.md""), ""w"") as f:\n            f.write(""TEST"")\n\n        with pytest.raises(ConfigurationError):\n            train_model(params(), serialization_dir=serialization_dir3)\n\n        # It\'s also not OK if serialization dir is a real serialization dir:\n        with pytest.raises(ConfigurationError):\n            train_model(params(), serialization_dir=os.path.join(self.TEST_DIR, ""test_train_model""))\n\n        # But it\'s OK if serialization dir exists and --recover is specified:\n        train_model(\n            params(),\n            serialization_dir=os.path.join(self.TEST_DIR, ""test_train_model""),\n            recover=True,\n        )\n\n        # It\'s ok serialization dir exists and --force is specified (it will be deleted):\n        train_model(\n            params(), serialization_dir=os.path.join(self.TEST_DIR, ""test_train_model""), force=True\n        )\n\n        # But --force and --recover cannot both be specified\n        with pytest.raises(ConfigurationError):\n            train_model(\n                params(),\n                serialization_dir=os.path.join(self.TEST_DIR, ""test_train_model""),\n                force=True,\n                recover=True,\n            )\n\n    @cpu_or_gpu\n    def test_train_model_distributed(self):\n        if torch.cuda.device_count() >= 2:\n            devices = [0, 1]\n        else:\n            devices = [-1, -1]\n\n        params = lambda: Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""num_epochs"": 2, ""optimizer"": ""adam""},\n                ""distributed"": {""cuda_devices"": devices},\n            }\n        )\n\n        out_dir = os.path.join(self.TEST_DIR, ""test_distributed_train"")\n        train_model(params(), serialization_dir=out_dir)\n\n        # Check that some logs specific to distributed\n        # training are where we expect.\n        serialized_files = os.listdir(out_dir)\n        assert ""stderr_worker0.log"" in serialized_files\n        assert ""stdout_worker0.log"" in serialized_files\n        assert ""stderr_worker1.log"" in serialized_files\n        assert ""stdout_worker1.log"" in serialized_files\n        assert ""model.tar.gz"" in serialized_files\n\n        # Check we can load the serialized model\n        assert load_archive(out_dir).model\n\n    @cpu_or_gpu\n    @pytest.mark.parametrize(""lazy"", [True, False])\n    def test_train_model_distributed_with_sharded_reader(self, lazy):\n        if torch.cuda.device_count() >= 2:\n            devices = [0, 1]\n        else:\n            devices = [-1, -1]\n\n        params = lambda: Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {\n                    ""type"": ""sharded"",\n                    ""base_reader"": {""type"": ""sequence_tagging""},\n                    ""lazy"": lazy,\n                },\n                ""train_data_path"": SEQUENCE_TAGGING_SHARDS_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_SHARDS_PATH,\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""num_epochs"": 2, ""optimizer"": ""adam""},\n                ""distributed"": {""cuda_devices"": devices},\n            }\n        )\n\n        out_dir = os.path.join(self.TEST_DIR, ""test_distributed_train"")\n        train_model(params(), serialization_dir=out_dir)\n\n        # Check that some logs specific to distributed\n        # training are where we expect.\n        serialized_files = os.listdir(out_dir)\n        assert ""stderr_worker0.log"" in serialized_files\n        assert ""stdout_worker0.log"" in serialized_files\n        assert ""stderr_worker1.log"" in serialized_files\n        assert ""stdout_worker1.log"" in serialized_files\n        assert ""model.tar.gz"" in serialized_files\n\n        # Check we can load the serialized model\n        archive = load_archive(out_dir)\n        assert archive.model\n\n        # Check that we created a vocab from all the shards.\n        tokens = archive.model.vocab._token_to_index[""tokens""].keys()\n        assert tokens == {\n            ""@@PADDING@@"",\n            ""@@UNKNOWN@@"",\n            ""are"",\n            ""."",\n            ""animals"",\n            ""plants"",\n            ""vehicles"",\n            ""cats"",\n            ""dogs"",\n            ""snakes"",\n            ""birds"",\n            ""ferns"",\n            ""trees"",\n            ""flowers"",\n            ""vegetables"",\n            ""cars"",\n            ""buses"",\n            ""planes"",\n            ""rockets"",\n        }\n\n        # TODO: This is somewhat brittle. Make these constants in trainer.py.\n        train_early = ""finishing training early!""\n        validation_early = ""finishing validation early!""\n        train_complete = ""completed its entire epoch (training).""\n        validation_complete = ""completed its entire epoch (validation).""\n\n        # There are three shards, but only two workers, so the first worker will have to discard some data.\n        with open(os.path.join(out_dir, ""stdout_worker0.log"")) as f:\n            worker0_log = f.read()\n            assert train_early in worker0_log\n            assert validation_early in worker0_log\n            assert train_complete not in worker0_log\n            assert validation_complete not in worker0_log\n\n        with open(os.path.join(out_dir, ""stdout_worker1.log"")) as f:\n            worker1_log = f.read()\n            assert train_early not in worker1_log\n            assert validation_early not in worker1_log\n            assert train_complete in worker1_log\n            assert validation_complete in worker1_log\n\n    @cpu_or_gpu\n    @pytest.mark.parametrize(""lazy"", [True, False])\n    def test_train_model_distributed_without_sharded_reader(self, lazy: bool):\n        if torch.cuda.device_count() >= 2:\n            devices = [0, 1]\n        else:\n            devices = [-1, -1]\n\n        num_epochs = 2\n        params = lambda: Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging"", ""lazy"": lazy},\n                ""train_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""data_loader"": {""batch_size"": 1},\n                ""trainer"": {\n                    ""num_epochs"": num_epochs,\n                    ""optimizer"": ""adam"",\n                    ""batch_callbacks"": [\n                        ""tests.commands.train_test.TrainingDataLoggerBatchCallback""\n                    ],\n                },\n                ""distributed"": {""cuda_devices"": devices},\n            }\n        )\n\n        out_dir = os.path.join(self.TEST_DIR, ""test_distributed_train"")\n        train_model(params(), serialization_dir=out_dir)\n\n        # Check that some logs specific to distributed\n        # training are where we expect.\n        serialized_files = os.listdir(out_dir)\n        assert ""stderr_worker0.log"" in serialized_files\n        assert ""stdout_worker0.log"" in serialized_files\n        assert ""stderr_worker1.log"" in serialized_files\n        assert ""stdout_worker1.log"" in serialized_files\n        assert ""model.tar.gz"" in serialized_files\n\n        # Check we can load the serialized model\n        archive = load_archive(out_dir)\n        assert archive.model\n\n        # Check that we created a vocab from all the shards.\n        tokens = set(archive.model.vocab._token_to_index[""tokens""].keys())\n        assert tokens == {\n            ""@@PADDING@@"",\n            ""@@UNKNOWN@@"",\n            ""are"",\n            ""."",\n            ""animals"",\n            ""cats"",\n            ""dogs"",\n            ""snakes"",\n            ""birds"",\n        }\n\n        train_complete = ""completed its entire epoch (training).""\n        validation_complete = ""completed its entire epoch (validation).""\n\n        import re\n\n        pattern = re.compile(r""First word from training data: \'([^\']*)\'"")\n        first_word_counts = Counter()  # type: ignore\n        with open(os.path.join(out_dir, ""stdout_worker0.log"")) as f:\n            worker0_log = f.read()\n            assert train_complete in worker0_log\n            assert validation_complete in worker0_log\n            for first_word in pattern.findall(worker0_log):\n                first_word_counts[first_word] += 1\n\n        with open(os.path.join(out_dir, ""stdout_worker1.log"")) as f:\n            worker1_log = f.read()\n            assert train_complete in worker1_log\n            assert validation_complete in worker1_log\n            for first_word in pattern.findall(worker1_log):\n                first_word_counts[first_word] += 1\n\n        assert first_word_counts == {\n            ""cats"": num_epochs,\n            ""dogs"": num_epochs,\n            ""snakes"": num_epochs,\n            ""birds"": num_epochs,\n        }\n\n    def test_distributed_raises_error_with_no_gpus(self):\n        params = Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""num_epochs"": 2, ""optimizer"": ""adam""},\n                ""distributed"": {},\n            }\n        )\n        with pytest.raises(ConfigurationError):\n            train_model(params, serialization_dir=os.path.join(self.TEST_DIR, ""test_train_model""))\n\n    def test_train_saves_all_keys_in_config(self):\n        params = Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""pytorch_seed"": 42,\n                ""numpy_seed"": 42,\n                ""random_seed"": 42,\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""num_epochs"": 2, ""optimizer"": ""adam""},\n            }\n        )\n\n        serialization_dir = os.path.join(self.TEST_DIR, ""test_train_model"")\n        params_as_dict = (\n            params.as_ordered_dict()\n        )  # Do it here as train_model will pop all the values.\n        train_model(params, serialization_dir=serialization_dir)\n\n        config_path = os.path.join(serialization_dir, CONFIG_NAME)\n        with open(config_path) as config:\n            saved_config_as_dict = OrderedDict(json.load(config))\n        assert params_as_dict == saved_config_as_dict\n\n    def test_error_is_throw_when_cuda_device_is_not_available(self):\n        params = Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": ""test_fixtures/data/sequence_tagging.tsv"",\n                ""validation_data_path"": ""test_fixtures/data/sequence_tagging.tsv"",\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {\n                    ""num_epochs"": 2,\n                    ""cuda_device"": torch.cuda.device_count(),\n                    ""optimizer"": ""adam"",\n                },\n            }\n        )\n\n        with pytest.raises(ConfigurationError, match=""Experiment specified""):\n            train_model(params, serialization_dir=os.path.join(self.TEST_DIR, ""test_train_model""))\n\n    def test_train_with_test_set(self):\n        params = Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""test_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""evaluate_on_test"": True,\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""num_epochs"": 2, ""optimizer"": ""adam""},\n            }\n        )\n\n        train_model(params, serialization_dir=os.path.join(self.TEST_DIR, ""train_with_test_set""))\n\n    def test_train_number_of_steps(self):\n        number_of_epochs = 2\n\n        last_num_steps_per_epoch: Optional[int] = None\n\n        @LearningRateScheduler.register(""mock"")\n        class MockLRScheduler(ExponentialLearningRateScheduler):\n            def __init__(self, optimizer: torch.optim.Optimizer, num_steps_per_epoch: int):\n                super().__init__(optimizer)\n                nonlocal last_num_steps_per_epoch\n                last_num_steps_per_epoch = num_steps_per_epoch\n\n        batch_callback_counter = 0\n\n        @BatchCallback.register(""counter"")\n        class CounterBatchCallback(BatchCallback):\n            def __call__(\n                self,\n                trainer: GradientDescentTrainer,\n                batch_inputs: List[List[TensorDict]],\n                batch_outputs: List[Dict[str, Any]],\n                epoch: int,\n                batch_number: int,\n                is_training: bool,\n                is_master: bool,\n            ) -> None:\n                nonlocal batch_callback_counter\n                if is_training:\n                    batch_callback_counter += 1\n\n        params = Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""test_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""evaluate_on_test"": True,\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {\n                    ""num_epochs"": number_of_epochs,\n                    ""optimizer"": ""adam"",\n                    ""learning_rate_scheduler"": {""type"": ""mock""},\n                    ""batch_callbacks"": [""counter""],\n                },\n            }\n        )\n        train_model(\n            params.duplicate(), serialization_dir=os.path.join(self.TEST_DIR, ""train_normal"")\n        )\n        assert batch_callback_counter == last_num_steps_per_epoch * number_of_epochs\n        batch_callback_counter = 0\n        normal_steps_per_epoch = last_num_steps_per_epoch\n\n        original_batch_size = params[""data_loader""][""batch_size""]\n        params[""data_loader""][""batch_size""] = 1\n        train_model(\n            params.duplicate(), serialization_dir=os.path.join(self.TEST_DIR, ""train_with_bs1"")\n        )\n        assert batch_callback_counter == last_num_steps_per_epoch * number_of_epochs\n        batch_callback_counter = 0\n        assert normal_steps_per_epoch == math.ceil(last_num_steps_per_epoch / original_batch_size)\n\n        params[""data_loader""][""batch_size""] = original_batch_size\n        params[""trainer""][""num_gradient_accumulation_steps""] = 3\n        train_model(params, serialization_dir=os.path.join(self.TEST_DIR, ""train_with_ga""))\n        assert batch_callback_counter == last_num_steps_per_epoch * number_of_epochs\n        batch_callback_counter = 0\n        assert math.ceil(normal_steps_per_epoch / 3) == last_num_steps_per_epoch\n\n    def test_train_args(self):\n        parser = argparse.ArgumentParser(description=""Testing"")\n        subparsers = parser.add_subparsers(title=""Commands"", metavar="""")\n        Train().add_subparser(subparsers)\n\n        for serialization_arg in [""-s"", ""--serialization-dir""]:\n            raw_args = [""train"", ""path/to/params"", serialization_arg, ""serialization_dir""]\n\n            args = parser.parse_args(raw_args)\n\n            assert args.func == train_model_from_args\n            assert args.param_path == ""path/to/params""\n            assert args.serialization_dir == ""serialization_dir""\n\n        # config is required\n        with pytest.raises(SystemExit) as cm:\n            args = parser.parse_args([""train"", ""-s"", ""serialization_dir""])\n            assert cm.exception.code == 2  # argparse code for incorrect usage\n\n        # serialization dir is required\n        with pytest.raises(SystemExit) as cm:\n            args = parser.parse_args([""train"", ""path/to/params""])\n            assert cm.exception.code == 2  # argparse code for incorrect usage\n\n    def test_train_model_can_instantiate_from_params(self):\n        params = Params.from_file(self.FIXTURES_ROOT / ""simple_tagger"" / ""experiment.json"")\n\n        # Can instantiate from base class params\n        TrainModel.from_params(\n            params=params, serialization_dir=self.TEST_DIR, local_rank=0, batch_weight_key=""""\n        )\n\n    def test_train_can_fine_tune_model_from_archive(self):\n        params = Params.from_file(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""experiment_from_archive.jsonnet""\n        )\n        train_loop = TrainModel.from_params(\n            params=params, serialization_dir=self.TEST_DIR, local_rank=0, batch_weight_key=""""\n        )\n        train_loop.run()\n\n        model = Model.from_archive(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n\n        # This is checking that the vocabulary actually got extended.  The data that we\'re using for\n        # training is different from the data we used to produce the model archive, and we set\n        # parameters such that the vocab should have been extended.\n        assert train_loop.model.vocab.get_vocab_size() > model.vocab.get_vocab_size()\n\n\n@DatasetReader.register(""lazy-test"")\nclass LazyFakeReader(DatasetReader):\n    def __init__(self) -> None:\n        super().__init__(lazy=True)\n        self.reader = DatasetReader.from_params(Params({""type"": ""sequence_tagging"", ""lazy"": True}))\n\n    def _read(self, file_path: str) -> Iterable[Instance]:\n        """"""\n        Reads some data from the `file_path` and returns the instances.\n        """"""\n        return self.reader.read(file_path)\n\n\nclass TestTrainOnLazyDataset(AllenNlpTestCase):\n    def test_train_model(self):\n        params = Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""lazy-test""},\n                ""train_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""num_epochs"": 2, ""optimizer"": ""adam""},\n            }\n        )\n\n        train_model(params, serialization_dir=os.path.join(self.TEST_DIR, ""train_lazy_model""))\n\n    def test_train_with_test_set(self):\n        params = Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""lazy-test""},\n                ""train_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""test_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""evaluate_on_test"": True,\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""num_epochs"": 2, ""optimizer"": ""adam""},\n            }\n        )\n\n        train_model(params, serialization_dir=os.path.join(self.TEST_DIR, ""lazy_test_set""))\n\n    def test_train_nograd_regex(self):\n        params_get = lambda: Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""validation_data_path"": SEQUENCE_TAGGING_DATA_PATH,\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""num_epochs"": 2, ""optimizer"": ""adam""},\n            }\n        )\n        serialization_dir = os.path.join(self.TEST_DIR, ""test_train_nograd"")\n        regex_lists = [[], ["".*text_field_embedder.*""], ["".*text_field_embedder.*"", "".*encoder.*""]]\n        for regex_list in regex_lists:\n            params = params_get()\n            params[""trainer""][""no_grad""] = regex_list\n            shutil.rmtree(serialization_dir, ignore_errors=True)\n            model = train_model(params, serialization_dir=serialization_dir)\n            # If regex is matched, parameter name should have requires_grad False\n            # Or else True\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in regex_list):\n                    assert not parameter.requires_grad\n                else:\n                    assert parameter.requires_grad\n        # If all parameters have requires_grad=False, then error.\n        params = params_get()\n        params[""trainer""][""no_grad""] = [""*""]\n        shutil.rmtree(serialization_dir, ignore_errors=True)\n        with pytest.raises(Exception):\n            train_model(params, serialization_dir=serialization_dir)\n\n\nclass TestDryRun(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n\n        self.params = Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": str(self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""),\n                ""validation_data_path"": str(self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""),\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""num_epochs"": 2, ""optimizer"": ""adam""},\n            }\n        )\n\n    def test_dry_run_doesnt_overwrite_vocab(self):\n        vocab_path = self.TEST_DIR / ""vocabulary""\n        os.mkdir(vocab_path)\n        # Put something in the vocab directory\n        with open(vocab_path / ""test.txt"", ""a+"") as open_file:\n            open_file.write(""test"")\n        # It should raise error if vocab dir is non-empty\n        with pytest.raises(ConfigurationError):\n            train_model(self.params, self.TEST_DIR, dry_run=True)\n\n    def test_dry_run_without_vocabulary_key(self):\n        train_model(self.params, self.TEST_DIR, dry_run=True)\n\n    def test_dry_run_makes_vocab(self):\n        vocab_path = self.TEST_DIR / ""vocabulary""\n\n        train_model(self.params, self.TEST_DIR, dry_run=True)\n\n        vocab_files = os.listdir(vocab_path)\n        assert set(vocab_files) == {\n            "".lock"",\n            ""labels.txt"",\n            ""non_padded_namespaces.txt"",\n            ""tokens.txt"",\n        }\n\n        with open(vocab_path / ""tokens.txt"") as f:\n            tokens = [line.strip() for line in f]\n\n        tokens.sort()\n        assert tokens == [""."", ""@@UNKNOWN@@"", ""animals"", ""are"", ""birds"", ""cats"", ""dogs"", ""snakes""]\n\n        with open(vocab_path / ""labels.txt"") as f:\n            labels = [line.strip() for line in f]\n\n        labels.sort()\n        assert labels == [""N"", ""V""]\n\n    def test_dry_run_with_extension(self):\n        existing_serialization_dir = self.TEST_DIR / ""existing""\n        extended_serialization_dir = self.TEST_DIR / ""extended""\n        existing_vocab_path = existing_serialization_dir / ""vocabulary""\n        extended_vocab_path = extended_serialization_dir / ""vocabulary""\n\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""some_weird_token_1"", namespace=""tokens"")\n        vocab.add_token_to_namespace(""some_weird_token_2"", namespace=""tokens"")\n        os.makedirs(existing_serialization_dir, exist_ok=True)\n        vocab.save_to_files(existing_vocab_path)\n\n        self.params[""vocabulary""] = {}\n        self.params[""vocabulary""][""type""] = ""extend""\n        self.params[""vocabulary""][""directory""] = str(existing_vocab_path)\n        self.params[""vocabulary""][""min_count""] = {""tokens"": 3}\n        train_model(self.params, extended_serialization_dir, dry_run=True)\n\n        vocab_files = os.listdir(extended_vocab_path)\n        assert set(vocab_files) == {\n            "".lock"",\n            ""labels.txt"",\n            ""non_padded_namespaces.txt"",\n            ""tokens.txt"",\n        }\n\n        with open(extended_vocab_path / ""tokens.txt"") as f:\n            tokens = [line.strip() for line in f]\n\n        assert tokens[0] == ""@@UNKNOWN@@""\n        assert tokens[1] == ""some_weird_token_1""\n        assert tokens[2] == ""some_weird_token_2""\n\n        tokens.sort()\n        assert tokens == [\n            ""."",\n            ""@@UNKNOWN@@"",\n            ""animals"",\n            ""are"",\n            ""some_weird_token_1"",\n            ""some_weird_token_2"",\n        ]\n\n        with open(extended_vocab_path / ""labels.txt"") as f:\n            labels = [line.strip() for line in f]\n\n        labels.sort()\n        assert labels == [""N"", ""V""]\n\n    def test_dry_run_without_extension(self):\n        existing_serialization_dir = self.TEST_DIR / ""existing""\n        extended_serialization_dir = self.TEST_DIR / ""extended""\n        existing_vocab_path = existing_serialization_dir / ""vocabulary""\n        extended_vocab_path = extended_serialization_dir / ""vocabulary""\n\n        vocab = Vocabulary()\n        # if extend is False, its users responsibility to make sure that dataset instances\n        # will be indexible by provided vocabulary. At least @@UNKNOWN@@ should be present in\n        # namespace for which there could be OOV entries seen in dataset during indexing.\n        # For `tokens` ns, new words will be seen but `tokens` has @@UNKNOWN@@ token.\n        # but for \'labels\' ns, there is no @@UNKNOWN@@ so required to add \'N\', \'V\' upfront.\n        vocab.add_token_to_namespace(""some_weird_token_1"", namespace=""tokens"")\n        vocab.add_token_to_namespace(""some_weird_token_2"", namespace=""tokens"")\n        vocab.add_token_to_namespace(""N"", namespace=""labels"")\n        vocab.add_token_to_namespace(""V"", namespace=""labels"")\n        os.makedirs(existing_serialization_dir, exist_ok=True)\n        vocab.save_to_files(existing_vocab_path)\n\n        self.params[""vocabulary""] = {}\n        self.params[""vocabulary""][""type""] = ""from_files""\n        self.params[""vocabulary""][""directory""] = str(existing_vocab_path)\n        train_model(self.params, extended_serialization_dir, dry_run=True)\n\n        with open(extended_vocab_path / ""tokens.txt"") as f:\n            tokens = [line.strip() for line in f]\n\n        assert tokens[0] == ""@@UNKNOWN@@""\n        assert tokens[1] == ""some_weird_token_1""\n        assert tokens[2] == ""some_weird_token_2""\n        assert len(tokens) == 3\n\n    def test_make_vocab_args(self):\n        parser = argparse.ArgumentParser(description=""Testing"")\n        subparsers = parser.add_subparsers(title=""Commands"", metavar="""")\n        Train().add_subparser(subparsers)\n        for serialization_arg in [""-s"", ""--serialization-dir""]:\n            raw_args = [\n                ""train"",\n                ""path/to/params"",\n                serialization_arg,\n                ""serialization_dir"",\n                ""--dry-run"",\n            ]\n            args = parser.parse_args(raw_args)\n            assert args.func == train_model_from_args\n            assert args.param_path == ""path/to/params""\n            assert args.serialization_dir == ""serialization_dir""\n            assert args.dry_run\n'"
tests/common/__init__.py,0,b''
tests/common/file_utils_test.py,3,"b'from collections import Counter\nimport os\nimport pathlib\nimport json\n\nimport pytest\nimport responses\nfrom requests.exceptions import ConnectionError\n\nfrom allennlp.common import file_utils\nfrom allennlp.common.file_utils import (\n    url_to_filename,\n    filename_to_url,\n    get_from_cache,\n    cached_path,\n    _split_s3_path,\n    open_compressed,\n)\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\ndef set_up_glove(url: str, byt: bytes, change_etag_every: int = 1000):\n    # Mock response for the datastore url that returns glove vectors\n    responses.add(\n        responses.GET,\n        url,\n        body=byt,\n        status=200,\n        content_type=""application/gzip"",\n        stream=True,\n        headers={""Content-Length"": str(len(byt))},\n    )\n\n    etags_left = change_etag_every\n    etag = ""0""\n\n    def head_callback(_):\n        """"""\n        Writing this as a callback allows different responses to different HEAD requests.\n        In our case, we\'re going to change the ETag header every `change_etag_every`\n        requests, which will allow us to simulate having a new version of the file.\n        """"""\n        nonlocal etags_left, etag\n        headers = {""ETag"": etag}\n        # countdown and change ETag\n        etags_left -= 1\n        if etags_left <= 0:\n            etags_left = change_etag_every\n            etag = str(int(etag) + 1)\n        return (200, headers, """")\n\n    responses.add_callback(responses.HEAD, url, callback=head_callback)\n\n\nclass TestFileUtils(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.glove_file = self.FIXTURES_ROOT / ""embeddings/glove.6B.100d.sample.txt.gz""\n        with open(self.glove_file, ""rb"") as glove:\n            self.glove_bytes = glove.read()\n\n    def test_cached_path_offline(self, monkeypatch):\n        # Ensures `cached_path` just returns the path to the latest cached version\n        # of the resource when there\'s no internet connection.\n\n        # First we mock the `_http_etag` method so that it raises a `ConnectionError`,\n        # like it would if there was no internet connection.\n        def mocked_http_etag(url: str):\n            raise ConnectionError\n\n        monkeypatch.setattr(file_utils, ""_http_etag"", mocked_http_etag)\n\n        url = ""https://github.com/allenai/allennlp/blob/master/some-fake-resource""\n\n        # We\'ll create two cached versions of this fake resource using two different etags.\n        etags = [\'W/""3e5885bfcbf4c47bc4ee9e2f6e5ea916""\', \'W/""3e5885bfcbf4c47bc4ee9e2f6e5ea918""\']\n        filenames = [os.path.join(self.TEST_DIR, url_to_filename(url, etag)) for etag in etags]\n        for filename, etag in zip(filenames, etags):\n            meta_filename = filename + "".json""\n            with open(filename, ""w"") as f:\n                f.write(""some random data"")\n            with open(meta_filename, ""w"") as meta_f:\n                json.dump({""url"": url, ""etag"": etag}, meta_f)\n\n        # The version corresponding to the last etag should be returned, since\n        # that one has the latest ""last modified"" time.\n        assert get_from_cache(url, cache_dir=self.TEST_DIR) == filenames[-1]\n\n        # We also want to make sure this works when the latest cached version doesn\'t\n        # have a corresponding etag.\n        filename = os.path.join(self.TEST_DIR, url_to_filename(url))\n        meta_filename = filename + "".json""\n        with open(filename, ""w"") as f:\n            f.write(""some random data"")\n        with open(meta_filename, ""w"") as meta_f:\n            json.dump({""url"": url, ""etag"": etag}, meta_f)\n\n        assert get_from_cache(url, cache_dir=self.TEST_DIR) == filename\n\n    def test_url_to_filename(self):\n        for url in [\n            ""http://allenai.org"",\n            ""http://allennlp.org"",\n            ""https://www.google.com"",\n            ""http://pytorch.org"",\n            ""https://allennlp.s3.amazonaws.com"" + ""/long"" * 20 + ""/url"",\n        ]:\n            filename = url_to_filename(url)\n            assert ""http"" not in filename\n            with pytest.raises(FileNotFoundError):\n                filename_to_url(filename, cache_dir=self.TEST_DIR)\n            pathlib.Path(os.path.join(self.TEST_DIR, filename)).touch()\n            with pytest.raises(FileNotFoundError):\n                filename_to_url(filename, cache_dir=self.TEST_DIR)\n            json.dump(\n                {""url"": url, ""etag"": None},\n                open(os.path.join(self.TEST_DIR, filename + "".json""), ""w""),\n            )\n            back_to_url, etag = filename_to_url(filename, cache_dir=self.TEST_DIR)\n            assert back_to_url == url\n            assert etag is None\n\n    def test_url_to_filename_with_etags(self):\n        for url in [\n            ""http://allenai.org"",\n            ""http://allennlp.org"",\n            ""https://www.google.com"",\n            ""http://pytorch.org"",\n        ]:\n            filename = url_to_filename(url, etag=""mytag"")\n            assert ""http"" not in filename\n            pathlib.Path(os.path.join(self.TEST_DIR, filename)).touch()\n            json.dump(\n                {""url"": url, ""etag"": ""mytag""},\n                open(os.path.join(self.TEST_DIR, filename + "".json""), ""w""),\n            )\n            back_to_url, etag = filename_to_url(filename, cache_dir=self.TEST_DIR)\n            assert back_to_url == url\n            assert etag == ""mytag""\n        baseurl = ""http://allenai.org/""\n        assert url_to_filename(baseurl + ""1"") != url_to_filename(baseurl, etag=""1"")\n\n    def test_url_to_filename_with_etags_eliminates_quotes(self):\n        for url in [\n            ""http://allenai.org"",\n            ""http://allennlp.org"",\n            ""https://www.google.com"",\n            ""http://pytorch.org"",\n        ]:\n            filename = url_to_filename(url, etag=\'""mytag""\')\n            assert ""http"" not in filename\n            pathlib.Path(os.path.join(self.TEST_DIR, filename)).touch()\n            json.dump(\n                {""url"": url, ""etag"": ""mytag""},\n                open(os.path.join(self.TEST_DIR, filename + "".json""), ""w""),\n            )\n            back_to_url, etag = filename_to_url(filename, cache_dir=self.TEST_DIR)\n            assert back_to_url == url\n            assert etag == ""mytag""\n\n    def test_split_s3_path(self):\n        # Test splitting good urls.\n        assert _split_s3_path(""s3://my-bucket/subdir/file.txt"") == (""my-bucket"", ""subdir/file.txt"")\n        assert _split_s3_path(""s3://my-bucket/file.txt"") == (""my-bucket"", ""file.txt"")\n\n        # Test splitting bad urls.\n        with pytest.raises(ValueError):\n            _split_s3_path(""s3://"")\n            _split_s3_path(""s3://myfile.txt"")\n            _split_s3_path(""myfile.txt"")\n\n    @responses.activate\n    def test_get_from_cache(self):\n        url = ""http://fake.datastore.com/glove.txt.gz""\n        set_up_glove(url, self.glove_bytes, change_etag_every=2)\n\n        filename = get_from_cache(url, cache_dir=self.TEST_DIR)\n        assert filename == os.path.join(self.TEST_DIR, url_to_filename(url, etag=""0""))\n\n        # We should have made one HEAD request and one GET request.\n        method_counts = Counter(call.request.method for call in responses.calls)\n        assert len(method_counts) == 2\n        assert method_counts[""HEAD""] == 1\n        assert method_counts[""GET""] == 1\n\n        # And the cached file should have the correct contents\n        with open(filename, ""rb"") as cached_file:\n            assert cached_file.read() == self.glove_bytes\n\n        # A second call to `get_from_cache` should make another HEAD call\n        # but not another GET call.\n        filename2 = get_from_cache(url, cache_dir=self.TEST_DIR)\n        assert filename2 == filename\n\n        method_counts = Counter(call.request.method for call in responses.calls)\n        assert len(method_counts) == 2\n        assert method_counts[""HEAD""] == 2\n        assert method_counts[""GET""] == 1\n\n        with open(filename2, ""rb"") as cached_file:\n            assert cached_file.read() == self.glove_bytes\n\n        # A third call should have a different ETag and should force a new download,\n        # which means another HEAD call and another GET call.\n        filename3 = get_from_cache(url, cache_dir=self.TEST_DIR)\n        assert filename3 == os.path.join(self.TEST_DIR, url_to_filename(url, etag=""1""))\n\n        method_counts = Counter(call.request.method for call in responses.calls)\n        assert len(method_counts) == 2\n        assert method_counts[""HEAD""] == 3\n        assert method_counts[""GET""] == 2\n\n        with open(filename3, ""rb"") as cached_file:\n            assert cached_file.read() == self.glove_bytes\n\n    @responses.activate\n    def test_cached_path(self):\n        url = ""http://fake.datastore.com/glove.txt.gz""\n        set_up_glove(url, self.glove_bytes)\n\n        # non-existent file\n        with pytest.raises(FileNotFoundError):\n            filename = cached_path(self.FIXTURES_ROOT / ""does_not_exist"" / ""fake_file.tar.gz"")\n\n        # unparsable URI\n        with pytest.raises(ValueError):\n            filename = cached_path(""fakescheme://path/to/fake/file.tar.gz"")\n\n        # existing file as path\n        assert cached_path(self.glove_file) == str(self.glove_file)\n\n        # caches urls\n        filename = cached_path(url, cache_dir=self.TEST_DIR)\n\n        assert len(responses.calls) == 2\n        assert filename == os.path.join(self.TEST_DIR, url_to_filename(url, etag=""0""))\n\n        with open(filename, ""rb"") as cached_file:\n            assert cached_file.read() == self.glove_bytes\n\n    def test_open_compressed(self):\n        uncompressed_file = self.FIXTURES_ROOT / ""embeddings/fake_embeddings.5d.txt""\n        with open_compressed(uncompressed_file) as f:\n            uncompressed_lines = [line.strip() for line in f]\n\n        for suffix in [""bz2"", ""gz""]:\n            compressed_file = f""{uncompressed_file}.{suffix}""\n            with open_compressed(compressed_file) as f:\n                compressed_lines = [line.strip() for line in f]\n            assert compressed_lines == uncompressed_lines\n'"
tests/common/from_params_test.py,3,"b'from typing import Dict, Iterable, List, Mapping, Optional, Set, Tuple, Union\n\nimport pytest\nimport torch\n\nfrom allennlp.common import Lazy, Params, Registrable\nfrom allennlp.common.from_params import FromParams, takes_arg, remove_optional, create_kwargs\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import DataLoader, DatasetReader, Tokenizer\nfrom allennlp.models import Model\nfrom allennlp.models.archival import load_archive\nfrom allennlp.common.checks import ConfigurationError\n\n\nclass MyClass(FromParams):\n    def __init__(self, my_int: int, my_bool: bool = False) -> None:\n        self.my_int = my_int\n        self.my_bool = my_bool\n\n\nclass TestFromParams(AllenNlpTestCase):\n    def test_takes_arg(self):\n        def bare_function(some_input: int) -> int:\n            return some_input + 1\n\n        assert takes_arg(bare_function, ""some_input"")\n        assert not takes_arg(bare_function, ""some_other_input"")\n\n        class SomeClass:\n            total = 0\n\n            def __init__(self, constructor_param: str) -> None:\n                self.constructor_param = constructor_param\n\n            def check_param(self, check: str) -> bool:\n                return self.constructor_param == check\n\n            @classmethod\n            def set_total(cls, new_total: int) -> None:\n                cls.total = new_total\n\n        assert takes_arg(SomeClass, ""self"")\n        assert takes_arg(SomeClass, ""constructor_param"")\n        assert not takes_arg(SomeClass, ""check"")\n\n        assert takes_arg(SomeClass.check_param, ""check"")\n        assert not takes_arg(SomeClass.check_param, ""other_check"")\n\n        assert takes_arg(SomeClass.set_total, ""new_total"")\n        assert not takes_arg(SomeClass.set_total, ""total"")\n\n    def test_remove_optional(self):\n        optional_type = Optional[Dict[str, str]]\n        bare_type = remove_optional(optional_type)  # type: ignore\n        bare_bare_type = remove_optional(bare_type)\n\n        assert bare_type == Dict[str, str]\n        assert bare_bare_type == Dict[str, str]\n\n        assert remove_optional(Optional[str]) == str\n        assert remove_optional(str) == str\n\n    def test_from_params(self):\n        my_class = MyClass.from_params(Params({""my_int"": 10}), my_bool=True)\n\n        assert isinstance(my_class, MyClass)\n        assert my_class.my_int == 10\n        assert my_class.my_bool\n\n    def test_good_error_message_when_passing_non_params(self):\n        from allennlp.nn import InitializerApplicator\n\n        # This was how we used to take initializer params.  We want to be sure we give a reasonable\n        # error message when something like this is passed to FromParams.\n        params = Params({""initializer"": [[""regex1"", ""uniform""], [""regex2"", ""orthogonal""]]})\n        with pytest.raises(ConfigurationError, match=""dictionary.*InitializerApplicator""):\n            InitializerApplicator.from_params(params=params.pop(""initializer""))\n\n    def test_create_kwargs(self):\n        kwargs = create_kwargs(MyClass, MyClass, Params({""my_int"": 5}), my_bool=True, my_float=4.4)\n\n        # my_float should not be included because it\'s not a param of the MyClass constructor\n        assert kwargs == {""my_int"": 5, ""my_bool"": True}\n\n    def test_extras(self):\n        from allennlp.common.registrable import Registrable\n\n        class A(Registrable):\n            pass\n\n        @A.register(""b"")\n        class B(A):\n            def __init__(self, size: int, name: str) -> None:\n                self.size = size\n                self.name = name\n\n        @A.register(""c"")\n        class C(A):\n            def __init__(self, size: int, name: str) -> None:\n                self.size = size\n                self.name = name\n\n            # custom from params\n            @classmethod\n            def from_params(cls, params: Params, size: int, **extras) -> ""C"":  # type: ignore\n                name = params.pop(""name"")\n                return cls(size=size, name=name)\n\n        # Check that extras get passed, even though A doesn\'t need them.\n        params = Params({""type"": ""b"", ""size"": 10})\n        b = A.from_params(params, name=""extra"")\n\n        assert b.name == ""extra""\n        assert b.size == 10\n\n        # Check that extra extras don\'t get passed.\n        params = Params({""type"": ""b"", ""size"": 10})\n        b = A.from_params(params, name=""extra"", unwanted=True)\n\n        assert b.name == ""extra""\n        assert b.size == 10\n\n        # Now the same with a custom from_params.\n        params = Params({""type"": ""c"", ""name"": ""extra_c""})\n        c = A.from_params(params, size=20)\n        assert c.name == ""extra_c""\n        assert c.size == 20\n\n        # Check that extra extras don\'t get passed.\n        params = Params({""type"": ""c"", ""name"": ""extra_c""})\n        c = A.from_params(params, size=20, unwanted=True)\n\n        assert c.name == ""extra_c""\n        assert c.size == 20\n\n    def test_extras_for_custom_classes(self):\n\n        from allennlp.common.registrable import Registrable\n\n        class BaseClass(Registrable):\n            pass\n\n        class BaseClass2(Registrable):\n            pass\n\n        @BaseClass.register(""A"")\n        class A(BaseClass):\n            def __init__(self, a: int, b: int, val: str) -> None:\n                self.a = a\n                self.b = b\n                self.val = val\n\n            def __hash__(self):\n                return self.b\n\n            def __eq__(self, other):\n                return self.b == other.b\n\n            @classmethod\n            def from_params(cls, params: Params, a: int, **extras) -> ""A"":  # type: ignore\n                # A custom from params\n                b = params.pop_int(""b"")\n                val = params.pop(""val"", ""C"")\n                params.assert_empty(cls.__name__)\n                return cls(a=a, b=b, val=val)\n\n        @BaseClass2.register(""B"")\n        class B(BaseClass2):\n            def __init__(self, c: int, b: int) -> None:\n                self.c = c\n                self.b = b\n\n            @classmethod\n            def from_params(cls, params: Params, c: int, **extras) -> ""B"":  # type: ignore\n                b = params.pop_int(""b"")\n                params.assert_empty(cls.__name__)\n                return cls(c=c, b=b)\n\n        @BaseClass.register(""E"")\n        class E(BaseClass):\n            def __init__(self, m: int, n: int) -> None:\n                self.m = m\n                self.n = n\n\n            @classmethod\n            def from_params(cls, params: Params, **extras2) -> ""E"":  # type: ignore\n                m = params.pop_int(""m"")\n                params.assert_empty(cls.__name__)\n                n = extras2[""n""]\n                return cls(m=m, n=n)\n\n        class C:\n            pass\n\n        @BaseClass.register(""D"")\n        class D(BaseClass):\n            def __init__(\n                self,\n                arg1: List[BaseClass],\n                arg2: Tuple[BaseClass, BaseClass2],\n                arg3: Dict[str, BaseClass],\n                arg4: Set[BaseClass],\n                arg5: List[BaseClass],\n            ) -> None:\n                self.arg1 = arg1\n                self.arg2 = arg2\n                self.arg3 = arg3\n                self.arg4 = arg4\n                self.arg5 = arg5\n\n        vals = [1, 2, 3]\n        params = Params(\n            {\n                ""type"": ""D"",\n                ""arg1"": [\n                    {""type"": ""A"", ""b"": vals[0]},\n                    {""type"": ""A"", ""b"": vals[1]},\n                    {""type"": ""A"", ""b"": vals[2]},\n                ],\n                ""arg2"": [{""type"": ""A"", ""b"": vals[0]}, {""type"": ""B"", ""b"": vals[0]}],\n                ""arg3"": {\n                    ""class_1"": {""type"": ""A"", ""b"": vals[0]},\n                    ""class_2"": {""type"": ""A"", ""b"": vals[1]},\n                },\n                ""arg4"": [\n                    {""type"": ""A"", ""b"": vals[0], ""val"": ""M""},\n                    {""type"": ""A"", ""b"": vals[1], ""val"": ""N""},\n                    {""type"": ""A"", ""b"": vals[1], ""val"": ""N""},\n                ],\n                ""arg5"": [{""type"": ""E"", ""m"": 9}],\n            }\n        )\n        extra = C()\n        tval1 = 5\n        tval2 = 6\n        d = BaseClass.from_params(params=params, extra=extra, a=tval1, c=tval2, n=10)\n\n        # Tests for List # Parameters\n        assert len(d.arg1) == len(vals)\n        assert isinstance(d.arg1, list)\n        assert isinstance(d.arg1[0], A)\n        assert all(x.b == y for x, y in zip(d.arg1, vals))\n        assert all(x.a == tval1 for x in d.arg1)\n\n        # Tests for Tuple\n        assert isinstance(d.arg2, tuple)\n        assert isinstance(d.arg2[0], A)\n        assert isinstance(d.arg2[1], B)\n        assert d.arg2[0].a == tval1\n        assert d.arg2[1].c == tval2\n        assert d.arg2[0].b == d.arg2[1].b == vals[0]\n\n        # Tests for Dict\n        assert isinstance(d.arg3, dict)\n        assert isinstance(d.arg3[""class_1""], A)\n        assert d.arg3[""class_1""].a == d.arg3[""class_2""].a == tval1\n        assert d.arg3[""class_1""].b == vals[0]\n        assert d.arg3[""class_2""].b == vals[1]\n\n        # Tests for Set\n        assert isinstance(d.arg4, set)\n        assert len(d.arg4) == 2\n        assert any(x.val == ""M"" for x in d.arg4)\n        assert any(x.val == ""N"" for x in d.arg4)\n\n        # Tests for custom extras parameters\n        assert isinstance(d.arg5, list)\n        assert isinstance(d.arg5[0], E)\n        assert d.arg5[0].m == 9\n        assert d.arg5[0].n == 10\n\n    def test_no_constructor(self):\n        params = Params({""type"": ""just_spaces""})\n\n        Tokenizer.from_params(params)\n\n    def test_union(self):\n        class A(FromParams):\n            def __init__(self, a: Union[int, List[int]]) -> None:\n                self.a = a\n\n        class B(FromParams):\n            def __init__(self, b: Union[A, List[A]]) -> None:\n                # Really you would want to be sure that `self.b` has a consistent type, but for\n                # this test we\'ll ignore that.\n                self.b = b\n\n        params = Params({""a"": 3})\n        a = A.from_params(params)\n        assert a.a == 3\n\n        params = Params({""a"": [3, 4, 5]})\n        a = A.from_params(params)\n        assert a.a == [3, 4, 5]\n\n        params = Params({""b"": {""a"": 3}})\n        b = B.from_params(params)\n        assert isinstance(b.b, A)\n        assert b.b.a == 3\n\n        params = Params({""b"": [{""a"": 3}, {""a"": [4, 5]}]})\n        b = B.from_params(params)\n        assert isinstance(b.b, list)\n        assert b.b[0].a == 3\n        assert b.b[1].a == [4, 5]\n\n    def test_crazy_nested_union(self):\n        class A(FromParams):\n            def __init__(self, a: Union[int, List[int]]) -> None:\n                self.a = a\n\n        class B(FromParams):\n            def __init__(self, b: Union[A, List[A]]) -> None:\n                # Really you would want to be sure that `self.b` has a consistent type, but for\n                # this test we\'ll ignore that.\n                self.b = b\n\n        class C(FromParams):\n            def __init__(self, c: Union[A, B, Dict[str, A]]) -> None:\n                # Really you would want to be sure that `self.c` has a consistent type, but for\n                # this test we\'ll ignore that.\n                self.c = c\n\n        # This is a contrived, ugly example (why would you want to duplicate names in a nested\n        # structure like this??), but it demonstrates a potential bug when dealing with mutatable\n        # parameters.  If you\'re not careful about keeping the parameters un-mutated in two\n        # separate places, you\'ll end up with a B, or with a dict that\'s missing the \'b\' key.\n        params = Params({""c"": {""a"": {""a"": 3}, ""b"": {""a"": [4, 5]}}})\n        c = C.from_params(params)\n        assert isinstance(c.c, dict)\n        assert c.c[""a""].a == 3\n        assert c.c[""b""].a == [4, 5]\n\n    def test_union_of_castable_types(self):\n        class IntFloat(FromParams):\n            def __init__(self, a: Union[int, float]) -> None:\n                self.a = a\n\n        class FloatInt(FromParams):\n            def __init__(self, a: Union[float, int]) -> None:\n                self.a = a\n\n        float_param_str = \'{""a"": 1.0}\'\n        int_param_str = \'{""a"": 1}\'\n        import json\n\n        for expected_type, param_str in [(int, int_param_str), (float, float_param_str)]:\n            for cls in [IntFloat, FloatInt]:\n                c = cls.from_params(Params(json.loads(param_str)))\n                assert type(c.a) == expected_type\n\n    def test_invalid_type_conversions(self):\n        class A(FromParams):\n            def __init__(self, a: int) -> None:\n                self.a = a\n\n        with pytest.raises(TypeError):\n            A.from_params(Params({""a"": ""1""}))\n        with pytest.raises(TypeError):\n            A.from_params(Params({""a"": 1.0}))\n\n    def test_dict(self):\n\n        from allennlp.common.registrable import Registrable\n\n        class A(Registrable):\n            pass\n\n        @A.register(""b"")\n        class B(A):\n            def __init__(self, size: int) -> None:\n                self.size = size\n\n        class C(Registrable):\n            pass\n\n        @C.register(""d"")\n        class D(C):\n            def __init__(self, items: Dict[str, A]) -> None:\n                self.items = items\n\n        params = Params(\n            {\n                ""type"": ""d"",\n                ""items"": {""first"": {""type"": ""b"", ""size"": 1}, ""second"": {""type"": ""b"", ""size"": 2}},\n            }\n        )\n        d = C.from_params(params)\n\n        assert isinstance(d.items, dict)\n        assert len(d.items) == 2\n        assert all(isinstance(key, str) for key in d.items.keys())\n        assert all(isinstance(value, B) for value in d.items.values())\n        assert d.items[""first""].size == 1\n        assert d.items[""second""].size == 2\n\n    def test_dict_not_params(self):\n        class A(FromParams):\n            def __init__(self, counts: Dict[str, int]) -> None:\n                self.counts = counts\n\n        params = Params({""counts"": {""a"": 10, ""b"": 20}})\n        a = A.from_params(params)\n\n        assert isinstance(a.counts, dict)\n        assert not isinstance(a.counts, Params)\n\n    def test_list(self):\n\n        from allennlp.common.registrable import Registrable\n\n        class A(Registrable):\n            pass\n\n        @A.register(""b"")\n        class B(A):\n            def __init__(self, size: int) -> None:\n                self.size = size\n\n        class C(Registrable):\n            pass\n\n        @C.register(""d"")\n        class D(C):\n            def __init__(self, items: List[A]) -> None:\n                self.items = items\n\n        params = Params(\n            {""type"": ""d"", ""items"": [{""type"": ""b"", ""size"": 1}, {""type"": ""b"", ""size"": 2}]}\n        )\n        d = C.from_params(params)\n\n        assert isinstance(d.items, list)\n        assert len(d.items) == 2\n        assert all(isinstance(item, B) for item in d.items)\n        assert d.items[0].size == 1\n        assert d.items[1].size == 2\n\n    def test_tuple(self):\n\n        from allennlp.common.registrable import Registrable\n\n        class A(Registrable):\n            pass\n\n        @A.register(""b"")\n        class B(A):\n            def __init__(self, size: int) -> None:\n                self.size = size\n\n        class C(Registrable):\n            pass\n\n        @C.register(""d"")\n        class D(C):\n            def __init__(self, name: str) -> None:\n                self.name = name\n\n        class E(Registrable):\n            pass\n\n        @E.register(""f"")\n        class F(E):\n            def __init__(self, items: Tuple[A, C]) -> None:\n                self.items = items\n\n        params = Params(\n            {""type"": ""f"", ""items"": [{""type"": ""b"", ""size"": 1}, {""type"": ""d"", ""name"": ""item2""}]}\n        )\n        f = E.from_params(params)\n\n        assert isinstance(f.items, tuple)\n        assert len(f.items) == 2\n        assert isinstance(f.items[0], B)\n        assert isinstance(f.items[1], D)\n        assert f.items[0].size == 1\n        assert f.items[1].name == ""item2""\n\n    def test_set(self):\n\n        from allennlp.common.registrable import Registrable\n\n        class A(Registrable):\n            def __init__(self, name: str) -> None:\n                self.name = name\n\n            def __eq__(self, other):\n                return self.name == other.name\n\n            def __hash__(self):\n                return hash(self.name)\n\n        @A.register(""b"")\n        class B(A):\n            pass\n\n        class C(Registrable):\n            pass\n\n        @C.register(""d"")\n        class D(C):\n            def __init__(self, items: Set[A]) -> None:\n                self.items = items\n\n        params = Params(\n            {\n                ""type"": ""d"",\n                ""items"": [\n                    {""type"": ""b"", ""name"": ""item1""},\n                    {""type"": ""b"", ""name"": ""item2""},\n                    {""type"": ""b"", ""name"": ""item2""},\n                ],\n            }\n        )\n        d = C.from_params(params)\n\n        assert isinstance(d.items, set)\n        assert len(d.items) == 2\n        assert all(isinstance(item, B) for item in d.items)\n        assert any(item.name == ""item1"" for item in d.items)\n        assert any(item.name == ""item2"" for item in d.items)\n\n    def test_transferring_of_modules(self):\n\n        model_archive = str(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        trained_model = load_archive(model_archive).model\n\n        config_file = str(self.FIXTURES_ROOT / ""basic_classifier"" / ""experiment_seq2seq.jsonnet"")\n        model_params = Params.from_file(config_file).pop(""model"").as_dict(quiet=True)\n\n        # Override only text_field_embedder (freeze) and seq2seq_encoder params (tunable)\n        model_params[""text_field_embedder""] = {\n            ""_pretrained"": {\n                ""archive_file"": model_archive,\n                ""module_path"": ""_text_field_embedder"",\n                ""freeze"": True,\n            }\n        }\n        model_params[""seq2seq_encoder""] = {\n            ""_pretrained"": {\n                ""archive_file"": model_archive,\n                ""module_path"": ""_seq2seq_encoder"",\n                ""freeze"": False,\n            }\n        }\n\n        transfer_model = Model.from_params(vocab=trained_model.vocab, params=Params(model_params))\n\n        # TextFieldEmbedder and Seq2SeqEncoder parameters should be transferred\n        for trained_parameter, transfer_parameter in zip(\n            trained_model._text_field_embedder.parameters(),\n            transfer_model._text_field_embedder.parameters(),\n        ):\n            assert torch.all(trained_parameter == transfer_parameter)\n        for trained_parameter, transfer_parameter in zip(\n            trained_model._seq2seq_encoder.parameters(),\n            transfer_model._seq2seq_encoder.parameters(),\n        ):\n            assert torch.all(trained_parameter == transfer_parameter)\n        # Any other module\'s parameters shouldn\'t be same (eg. _feedforward)\n        for trained_parameter, transfer_parameter in zip(\n            trained_model._feedforward.parameters(), transfer_model._feedforward.parameters(),\n        ):\n            assert torch.all(trained_parameter != transfer_parameter)\n\n        # TextFieldEmbedder should have requires_grad Off\n        for parameter in transfer_model._text_field_embedder.parameters():\n            assert not parameter.requires_grad\n\n        # # Seq2SeqEncoder should have requires_grad On\n        for parameter in transfer_model._seq2seq_encoder.parameters():\n            assert parameter.requires_grad\n\n    def test_transferring_of_modules_ensures_type_consistency(self):\n\n        model_archive = str(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        trained_model = load_archive(model_archive).model\n\n        config_file = str(self.FIXTURES_ROOT / ""basic_classifier"" / ""experiment_seq2seq.jsonnet"")\n        model_params = Params.from_file(config_file).pop(""model"").as_dict(quiet=True)\n\n        # Override only text_field_embedder and make it load Seq2SeqEncoder\n        model_params[""text_field_embedder""] = {\n            ""_pretrained"": {\n                ""archive_file"": model_archive,\n                ""module_path"": ""_seq2seq_encoder._module"",\n            }\n        }\n        with pytest.raises(ConfigurationError):\n            Model.from_params(vocab=trained_model.vocab, params=Params(model_params))\n\n    def test_bare_string_params(self):\n        dataset = [1]\n\n        class TestLoader(Registrable):\n            @classmethod\n            def from_partial_objects(cls, data_loader: Lazy[DataLoader]) -> DataLoader:\n                return data_loader.construct(dataset=dataset)\n\n        TestLoader.register(""test"", constructor=""from_partial_objects"")(TestLoader)\n\n        data_loader = TestLoader.from_params(\n            Params(\n                {\n                    ""type"": ""test"",\n                    ""data_loader"": {\n                        ""batch_sampler"": {\n                            ""type"": ""basic"",\n                            ""batch_size"": 2,\n                            ""drop_last"": True,\n                            ""sampler"": ""random"",\n                        }\n                    },\n                }\n            )\n        )\n        assert data_loader.batch_sampler.sampler.__class__.__name__ == ""RandomSampler""\n        assert data_loader.batch_sampler.sampler.data_source is dataset\n\n    def test_kwargs_are_passed_to_superclass(self):\n        params = Params(\n            {""type"": ""text_classification_json"", ""lazy"": True, ""cache_directory"": ""tmp""}\n        )\n        reader = DatasetReader.from_params(params)\n        assert reader.lazy is True\n        assert str(reader._cache_directory) == ""tmp""\n\n    def test_kwargs_with_multiple_inheritance(self):\n        # Basic idea: have two identical classes, differing only in the order of their multiple\n        # inheritance, and make sure that passing kwargs up to the super class works in both cases.\n        class A(Registrable):\n            def __init__(self, a: int):\n                self.a = a\n\n        from numbers import Number\n\n        @A.register(""b1"")\n        class B1(A, Number):\n            def __init__(self, b: float, **kwargs):\n                super().__init__(**kwargs)\n                self.b = b\n\n        @A.register(""b2"")\n        class B2(Number, A):\n            def __init__(self, b: float, **kwargs):\n                super().__init__(**kwargs)\n                self.b = b\n\n        b = B1.from_params(params=Params({""a"": 4, ""b"": 5}))\n        assert b.b == 5\n        assert b.a == 4\n\n        b = B2.from_params(params=Params({""a"": 4, ""b"": 5}))\n        assert b.b == 5\n        assert b.a == 4\n\n    def test_only_infer_superclass_params_if_unknown(self):\n\n        from allennlp.common.registrable import Registrable\n\n        class BaseClass(Registrable):\n            def __init__(self):\n                self.x = None\n                self.a = None\n                self.rest = None\n\n        @BaseClass.register(""a"")\n        class A(BaseClass):\n            def __init__(self, a: int, x: int, **kwargs):\n                super().__init__()\n                self.x = x\n                self.a = a\n                self.rest = kwargs\n\n        @BaseClass.register(""b"")\n        class B(A):\n            def __init__(self, a: str, x: int = 42, **kwargs):\n                super().__init__(x=x, a=-1, raw_a=a, **kwargs)\n\n        params = Params({""type"": ""b"", ""a"": ""123""})\n        # The param `x` should not be required as it has default value in `B`\n        # The correct type of the param `a` should be inferred from `B` as well.\n        instance = BaseClass.from_params(params)\n        assert instance.x == 42\n        assert instance.a == -1\n        assert len(instance.rest) == 1\n        assert type(instance.rest[""raw_a""]) == str\n        assert instance.rest[""raw_a""] == ""123""\n\n    def test_kwargs_are_passed_to_deeper_superclasses(self):\n\n        from allennlp.common.registrable import Registrable\n\n        class BaseClass(Registrable):\n            def __init__(self):\n                self.a = None\n                self.b = None\n                self.c = None\n\n        @BaseClass.register(""a"")\n        class A(BaseClass):\n            def __init__(self, a: str):\n                super().__init__()\n                self.a = a\n\n        @BaseClass.register(""b"")\n        class B(A):\n            def __init__(self, b: str, **kwargs):\n                super().__init__(**kwargs)\n                self.b = b\n\n        @BaseClass.register(""c"")\n        class C(B):\n            def __init__(self, c, **kwargs):\n                super().__init__(**kwargs)\n                self.c = c\n\n        params = Params({""type"": ""c"", ""a"": ""a_value"", ""b"": ""b_value"", ""c"": ""c_value""})\n\n        instance = BaseClass.from_params(params)\n        assert instance.a == ""a_value""\n        assert instance.b == ""b_value""\n        assert instance.c == ""c_value""\n\n    def test_lazy_construction_can_happen_multiple_times(self):\n        test_string = ""this is a test""\n        extra_string = ""extra string""\n\n        class ConstructedObject(FromParams):\n            def __init__(self, string: str, extra: str):\n                self.string = string\n                self.extra = extra\n\n        class Testing(FromParams):\n            def __init__(self, lazy_object: Lazy[ConstructedObject]):\n                first_time = lazy_object.construct(extra=extra_string)\n                second_time = lazy_object.construct(extra=extra_string)\n                assert first_time.string == test_string\n                assert first_time.extra == extra_string\n                assert second_time.string == test_string\n                assert second_time.extra == extra_string\n\n        Testing.from_params(Params({""lazy_object"": {""string"": test_string}}))\n\n    def test_iterable(self):\n        from allennlp.common.registrable import Registrable\n\n        class A(Registrable):\n            pass\n\n        @A.register(""b"")\n        class B(A):\n            def __init__(self, size: int) -> None:\n                self.size = size\n\n        class C(Registrable):\n            pass\n\n        @C.register(""d"")\n        class D(C):\n            def __init__(self, items: Iterable[A]) -> None:\n                self.items = items\n\n        params = Params(\n            {""type"": ""d"", ""items"": [{""type"": ""b"", ""size"": 1}, {""type"": ""b"", ""size"": 2}]}\n        )\n        d = C.from_params(params)\n\n        assert isinstance(d.items, Iterable)\n        items = list(d.items)\n        assert len(items) == 2\n        assert all(isinstance(item, B) for item in items)\n        assert items[0].size == 1\n        assert items[1].size == 2\n\n    def test_mapping(self):\n        from allennlp.common.registrable import Registrable\n\n        class A(Registrable):\n            pass\n\n        @A.register(""b"")\n        class B(A):\n            def __init__(self, size: int) -> None:\n                self.size = size\n\n        class C(Registrable):\n            pass\n\n        @C.register(""d"")\n        class D(C):\n            def __init__(self, items: Mapping[str, A]) -> None:\n                self.items = items\n\n        params = Params(\n            {\n                ""type"": ""d"",\n                ""items"": {""first"": {""type"": ""b"", ""size"": 1}, ""second"": {""type"": ""b"", ""size"": 2}},\n            }\n        )\n        d = C.from_params(params)\n\n        assert isinstance(d.items, Mapping)\n        assert len(d.items) == 2\n        assert all(isinstance(key, str) for key in d.items.keys())\n        assert all(isinstance(value, B) for value in d.items.values())\n        assert d.items[""first""].size == 1\n        assert d.items[""second""].size == 2\n\n    def test_extra_parameters_are_not_allowed_when_there_is_no_constructor(self):\n        class A(FromParams):\n            pass\n\n        with pytest.raises(ConfigurationError, match=""Extra parameters""):\n            A.from_params(Params({""some_spurious"": ""key"", ""value"": ""pairs""}))\n\n    def test_raises_when_there_are_no_implementations(self):\n        class A(Registrable):\n            pass\n\n        with pytest.raises(ConfigurationError, match=""no registered concrete types""):\n            A.from_params(""nonexistent_class"")\n\n        with pytest.raises(ConfigurationError, match=""no registered concrete types""):\n            A.from_params(Params({""some_spurious"": ""key"", ""value"": ""pairs""}))\n\n        with pytest.raises(ConfigurationError, match=""no registered concrete types""):\n            A.from_params(Params({}))\n\n        # Some paths through the code are different if there is a constructor here versus not.  We\n        # don\'t actually go through this logic anymore, but it\'s here as a regression test.\n        class B(Registrable):\n            def __init__(self):\n                pass\n\n        with pytest.raises(ConfigurationError, match=""no registered concrete types""):\n            B.from_params(""nonexistent_class"")\n\n        with pytest.raises(ConfigurationError, match=""no registered concrete types""):\n            B.from_params(Params({""some_spurious"": ""key"", ""value"": ""pairs""}))\n\n        with pytest.raises(ConfigurationError, match=""no registered concrete types""):\n            B.from_params(Params({}))\n'"
tests/common/logging_test.py,0,"b'import os\nimport logging\nimport random\n\nfrom allennlp.common.logging import AllenNlpLogger\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestLogging(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        logger = logging.getLogger(str(random.random()))\n        self.test_log_file = os.path.join(self.TEST_DIR, ""test.log"")\n        logger.addHandler(logging.FileHandler(self.test_log_file))\n        logger.setLevel(logging.DEBUG)\n        self.logger = logger\n        self._msg = ""test message""\n\n    def test_debug_once(self):\n        self.logger.debug_once(self._msg)\n        self.logger.debug_once(self._msg)\n\n        with open(self.test_log_file, ""r"") as f:\n            assert len(f.readlines()) == 1\n\n    def test_info_once(self):\n        self.logger.info_once(self._msg)\n        self.logger.info_once(self._msg)\n\n        with open(self.test_log_file, ""r"") as f:\n            assert len(f.readlines()) == 1\n\n    def test_warning_once(self):\n        self.logger.warning_once(self._msg)\n        self.logger.warning_once(self._msg)\n\n        with open(self.test_log_file, ""r"") as f:\n            assert len(f.readlines()) == 1\n\n    def test_error_once(self):\n        self.logger.error_once(self._msg)\n        self.logger.error_once(self._msg)\n\n        with open(self.test_log_file, ""r"") as f:\n            assert len(f.readlines()) == 1\n\n    def test_critical_once(self):\n        self.logger.critical_once(self._msg)\n        self.logger.critical_once(self._msg)\n\n        with open(self.test_log_file, ""r"") as f:\n            assert len(f.readlines()) == 1\n\n    def test_debug_once_different_args(self):\n        self.logger.debug_once(""There are %d lights."", 4)\n        self.logger.debug_once(""There are %d lights."", 5)\n\n        with open(self.test_log_file, ""r"") as f:\n            assert len(f.readlines()) == 1\n\n        assert len(self.logger._seen_msgs) == 1\n\n    def test_getLogger(self):\n        logger = logging.getLogger(""test_logger"")\n\n        assert isinstance(logger, AllenNlpLogger)\n'"
tests/common/params_test.py,0,"b'import json\nimport os\nimport re\nfrom collections import OrderedDict\n\nimport pytest\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.params import infer_and_cast, Params, parse_overrides, unflatten, with_fallback\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestParams(AllenNlpTestCase):\n    def test_load_from_file(self):\n        filename = self.FIXTURES_ROOT / ""simple_tagger"" / ""experiment.json""\n        params = Params.from_file(filename)\n\n        assert ""dataset_reader"" in params\n        assert ""trainer"" in params\n\n        model_params = params.pop(""model"")\n        assert model_params.pop(""type"") == ""simple_tagger""\n\n    def test_replace_none(self):\n        params = Params({""a"": ""None"", ""b"": [1.0, ""None"", 2], ""c"": {""d"": ""None""}})\n        assert params[""a""] is None\n        assert params[""b""][1] is None\n        assert params[""c""][""d""] is None\n\n    def test_bad_unicode_environment_variables(self):\n        filename = self.FIXTURES_ROOT / ""simple_tagger"" / ""experiment.json""\n        os.environ[""BAD_ENVIRONMENT_VARIABLE""] = ""\\udce2""\n        Params.from_file(filename)\n        del os.environ[""BAD_ENVIRONMENT_VARIABLE""]\n\n    def test_overrides(self):\n        filename = self.FIXTURES_ROOT / ""simple_tagger"" / ""experiment.json""\n        overrides = (\n            \'{ ""train_data_path"": ""FOO"", ""model"": { ""type"": ""BAR"" },\'\n            \'""model.text_field_embedder.tokens.type"": ""BAZ"",\'\n            \'""data_loader.batch_sampler.sorting_keys.0"": ""question""}\'\n        )\n        params = Params.from_file(filename, overrides)\n\n        assert ""dataset_reader"" in params\n        assert ""trainer"" in params\n        assert params[""train_data_path""] == ""FOO""\n        assert params[""data_loader""][""batch_sampler""][""sorting_keys""][0] == ""question""\n\n        model_params = params.pop(""model"")\n        assert model_params.pop(""type"") == ""BAR""\n        assert model_params[""text_field_embedder""][""tokens""][""type""] == ""BAZ""\n\n    def test_unflatten(self):\n        flattened = {""a.b.c"": 1, ""a.b.d"": 0, ""a.e.f.g.h"": 2, ""b"": 3}\n        unflattened = unflatten(flattened)\n        assert unflattened == {""a"": {""b"": {""c"": 1, ""d"": 0}, ""e"": {""f"": {""g"": {""h"": 2}}}}, ""b"": 3}\n\n        # should do nothing to a non-flat dictionary\n        assert unflatten(unflattened) == unflattened\n\n    def test_with_fallback(self):\n        preferred = {""a"": 1}\n        fallback = {""a"": 0, ""b"": 2}\n\n        merged = with_fallback(preferred=preferred, fallback=fallback)\n        assert merged == {""a"": 1, ""b"": 2}\n\n        # incompatibility is ok\n        preferred = {""a"": {""c"": 3}}\n        fallback = {""a"": 0, ""b"": 2}\n        merged = with_fallback(preferred=preferred, fallback=fallback)\n        assert merged == {""a"": {""c"": 3}, ""b"": 2}\n\n        # goes deep\n        preferred = {""deep"": {""a"": 1}}\n        fallback = {""deep"": {""a"": 0, ""b"": 2}}\n\n        merged = with_fallback(preferred=preferred, fallback=fallback)\n        assert merged == {""deep"": {""a"": 1, ""b"": 2}}\n\n    def test_parse_overrides(self):\n        assert parse_overrides("""") == {}\n        assert parse_overrides(""{}"") == {}\n\n        override_dict = parse_overrides(\'{""train_data"": ""/train"", ""trainer.num_epochs"": 10}\')\n        assert override_dict == {""train_data"": ""/train"", ""trainer"": {""num_epochs"": 10}}\n\n        params = with_fallback(\n            preferred=override_dict,\n            fallback={\n                ""train_data"": ""/test"",\n                ""model"": ""simple_tagger"",\n                ""trainer"": {""num_epochs"": 100, ""optimizer"": ""sgd""},\n            },\n        )\n\n        assert params == {\n            ""train_data"": ""/train"",\n            ""model"": ""simple_tagger"",\n            ""trainer"": {""num_epochs"": 10, ""optimizer"": ""sgd""},\n        }\n\n    def test_as_flat_dict(self):\n        params = Params({""a"": 10, ""b"": {""c"": 20, ""d"": ""stuff""}}).as_flat_dict()\n\n        assert params == {""a"": 10, ""b.c"": 20, ""b.d"": ""stuff""}\n\n    def test_jsonnet_features(self):\n        config_file = self.TEST_DIR / ""config.jsonnet""\n        with open(config_file, ""w"") as f:\n            f.write(\n                """"""{\n                            // This example is copied straight from the jsonnet docs\n                            person1: {\n                                name: ""Alice"",\n                                welcome: ""Hello "" + self.name + ""!"",\n                            },\n                            person2: self.person1 { name: ""Bob"" },\n                        }""""""\n            )\n\n        params = Params.from_file(config_file)\n\n        alice = params.pop(""person1"")\n        bob = params.pop(""person2"")\n\n        assert alice.as_dict() == {""name"": ""Alice"", ""welcome"": ""Hello Alice!""}\n        assert bob.as_dict() == {""name"": ""Bob"", ""welcome"": ""Hello Bob!""}\n\n        params.assert_empty(""TestParams"")\n\n    def test_regexes_with_backslashes(self):\n        bad_regex = self.TEST_DIR / ""bad_regex.jsonnet""\n        good_regex = self.TEST_DIR / ""good_regex.jsonnet""\n\n        with open(bad_regex, ""w"") as f:\n            f.write(r\'{""myRegex"": ""a\\.b""}\')\n\n        with open(good_regex, ""w"") as f:\n            f.write(r\'{""myRegex"": ""a\\\\.b""}\')\n\n        with pytest.raises(RuntimeError):\n            Params.from_file(bad_regex)\n\n        params = Params.from_file(good_regex)\n        regex = params[""myRegex""]\n\n        assert re.match(regex, ""a.b"")\n        assert not re.match(regex, ""a-b"")\n\n        # Check roundtripping\n        good_regex2 = self.TEST_DIR / ""good_regex2.jsonnet""\n        with open(good_regex2, ""w"") as f:\n            f.write(json.dumps(params.as_dict()))\n        params2 = Params.from_file(good_regex2)\n\n        assert params.as_dict() == params2.as_dict()\n\n    def test_env_var_substitution(self):\n        substitutor = self.TEST_DIR / ""substitutor.jsonnet""\n        key = ""TEST_ENV_VAR_SUBSTITUTION""\n\n        assert os.environ.get(key) is None\n\n        with open(substitutor, ""w"") as f:\n            f.write(f\'{{""path"": std.extVar(""{key}"")}}\')\n\n        # raises without environment variable set\n        with pytest.raises(RuntimeError):\n            Params.from_file(substitutor)\n\n        os.environ[key] = ""PERFECT""\n\n        params = Params.from_file(substitutor)\n        assert params[""path""] == ""PERFECT""\n\n        del os.environ[key]\n\n    @pytest.mark.xfail(\n        not os.path.exists(AllenNlpTestCase.PROJECT_ROOT / ""training_config""),\n        reason=""Training configs not installed with pip"",\n    )\n    def test_known_configs(self):\n        configs = os.listdir(self.PROJECT_ROOT / ""training_config"")\n\n        # Our configs use environment variable substitution, and the _jsonnet parser\n        # will fail if we don\'t pass it correct environment variables.\n        forced_variables = [\n            # constituency parser\n            ""PTB_TRAIN_PATH"",\n            ""PTB_DEV_PATH"",\n            ""PTB_TEST_PATH"",\n            # dependency parser\n            ""PTB_DEPENDENCIES_TRAIN"",\n            ""PTB_DEPENDENCIES_VAL"",\n            # multilingual dependency parser\n            ""TRAIN_PATHNAME"",\n            ""DEV_PATHNAME"",\n            ""TEST_PATHNAME"",\n            # srl_elmo_5.5B\n            ""SRL_TRAIN_DATA_PATH"",\n            ""SRL_VALIDATION_DATA_PATH"",\n            # coref\n            ""COREF_TRAIN_DATA_PATH"",\n            ""COREF_DEV_DATA_PATH"",\n            ""COREF_TEST_DATA_PATH"",\n            # ner\n            ""NER_TRAIN_DATA_PATH"",\n            ""NER_TEST_A_PATH"",\n            ""NER_TEST_B_PATH"",\n            # bidirectional lm\n            ""BIDIRECTIONAL_LM_TRAIN_PATH"",\n            ""BIDIRECTIONAL_LM_VOCAB_PATH"",\n            ""BIDIRECTIONAL_LM_ARCHIVE_PATH"",\n        ]\n\n        for var in forced_variables:\n            os.environ[var] = os.environ.get(var) or str(self.TEST_DIR)\n\n        for config in configs:\n            try:\n                Params.from_file(self.PROJECT_ROOT / ""training_config"" / config)\n            except Exception as e:\n                raise AssertionError(f""unable to load params for {config}, because {e}"")\n\n        for var in forced_variables:\n            if os.environ[var] == str(self.TEST_DIR):\n                del os.environ[var]\n\n    def test_as_ordered_dict(self):\n        # keyD > keyC > keyE; keyDA > keyDB; Next all other keys alphabetically\n        preference_orders = [[""keyD"", ""keyC"", ""keyE""], [""keyDA"", ""keyDB""]]\n        params = Params(\n            {\n                ""keyC"": ""valC"",\n                ""keyB"": ""valB"",\n                ""keyA"": ""valA"",\n                ""keyE"": ""valE"",\n                ""keyD"": {""keyDB"": ""valDB"", ""keyDA"": ""valDA""},\n            }\n        )\n        ordered_params_dict = params.as_ordered_dict(preference_orders)\n        expected_ordered_params_dict = OrderedDict(\n            {\n                ""keyD"": {""keyDA"": ""valDA"", ""keyDB"": ""valDB""},\n                ""keyC"": ""valC"",\n                ""keyE"": ""valE"",\n                ""keyA"": ""valA"",\n                ""keyB"": ""valB"",\n            }\n        )\n        assert json.dumps(ordered_params_dict) == json.dumps(expected_ordered_params_dict)\n\n    def test_to_file(self):\n        # Test to_file works with or without preference orders\n        params_dict = {""keyA"": ""valA"", ""keyB"": ""valB""}\n        expected_ordered_params_dict = OrderedDict({""keyB"": ""valB"", ""keyA"": ""valA""})\n        params = Params(params_dict)\n        file_path = self.TEST_DIR / ""config.jsonnet""\n        # check with preference orders\n        params.to_file(file_path, [[""keyB"", ""keyA""]])\n        with open(file_path, ""r"") as handle:\n            ordered_params_dict = OrderedDict(json.load(handle))\n        assert json.dumps(expected_ordered_params_dict) == json.dumps(ordered_params_dict)\n        # check without preference orders doesn\'t give error\n        params.to_file(file_path)\n\n    def test_infer_and_cast(self):\n        lots_of_strings = {\n            ""a"": [""10"", ""1.3"", ""true""],\n            ""b"": {""x"": 10, ""y"": ""20.1"", ""z"": ""other things""},\n            ""c"": ""just a string"",\n        }\n\n        casted = {\n            ""a"": [10, 1.3, True],\n            ""b"": {""x"": 10, ""y"": 20.1, ""z"": ""other things""},\n            ""c"": ""just a string"",\n        }\n\n        assert infer_and_cast(lots_of_strings) == casted\n\n        contains_bad_data = {""x"": 10, ""y"": int}\n        with pytest.raises(ValueError, match=""cannot infer type""):\n            infer_and_cast(contains_bad_data)\n\n        params = Params(lots_of_strings)\n\n        assert params.as_dict() == lots_of_strings\n        assert params.as_dict(infer_type_and_cast=True) == casted\n\n    def test_pop_choice(self):\n        choices = [""my_model"", ""other_model""]\n        params = Params({""model"": ""my_model""})\n        assert params.pop_choice(""model"", choices) == ""my_model""\n\n        params = Params({""model"": ""non_existent_model""})\n        with pytest.raises(ConfigurationError):\n            params.pop_choice(""model"", choices)\n\n        params = Params({""model"": ""module.submodule.ModelName""})\n        assert params.pop_choice(""model"", ""choices"") == ""module.submodule.ModelName""\n\n        params = Params({""model"": ""module.submodule.ModelName""})\n        with pytest.raises(ConfigurationError):\n            params.pop_choice(""model"", choices, allow_class_names=False)\n'"
tests/common/plugins_test.py,0,"b'from overrides import overrides\n\nfrom allennlp.commands import Subcommand\nfrom allennlp.common.plugins import (\n    discover_plugins,\n    import_plugins,\n)\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.util import pushd\n\n\nclass TestPlugins(AllenNlpTestCase):\n    @overrides\n    def setup_method(self):\n        super().setup_method()\n        self.plugins_root = self.FIXTURES_ROOT / ""plugins""\n\n    def test_no_plugins(self):\n        available_plugins = set(discover_plugins())\n        assert available_plugins == set()\n\n    def test_file_plugin(self):\n        available_plugins = set(discover_plugins())\n        assert available_plugins == set()\n\n        with pushd(self.plugins_root):\n            available_plugins = set(discover_plugins())\n            assert available_plugins == {""d""}\n\n            import_plugins()\n            subcommands_available = Subcommand.list_available()\n            assert ""d"" in subcommands_available\n'"
tests/common/registrable_test.py,0,"b'import inspect\nimport os\n\nimport pytest\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.registrable import Registrable\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.util import push_python_path\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.samplers import Sampler, BatchSampler\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer\nfrom allennlp.data.tokenizers.tokenizer import Tokenizer\nfrom allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder\nfrom allennlp.modules.token_embedders.token_embedder import TokenEmbedder\nfrom allennlp.nn.regularizers.regularizer import Regularizer\n\n\nclass TestRegistrable(AllenNlpTestCase):\n    def test_registrable_functionality_works(self):\n        # This function tests the basic `Registrable` functionality:\n        #\n        #   1. The decorator should add things to the list.\n        #   2. The decorator should crash when adding a duplicate (unless exist_ok=True).\n        #   3. If a default is given, it should show up first in the list.\n        #\n        # What we don\'t test here is that built-in items are registered correctly.  Those are\n        # tested in the other tests below.\n        #\n        # We\'ll test this with the Tokenizer class, just to have a concrete class to use, and one\n        # that has a default.\n        base_class = Tokenizer\n        assert ""fake"" not in base_class.list_available()\n\n        @base_class.register(""fake"")\n        class Fake(base_class):\n\n            pass\n\n        assert base_class.by_name(""fake"") == Fake\n\n        default = base_class.default_implementation\n        if default is not None:\n            assert base_class.list_available()[0] == default\n            base_class.default_implementation = ""fake""\n            assert base_class.list_available()[0] == ""fake""\n\n            with pytest.raises(ConfigurationError):\n                base_class.default_implementation = ""not present""\n                base_class.list_available()\n            base_class.default_implementation = default\n\n        # Verify that registering under a name that already exists\n        # causes a ConfigurationError.\n        with pytest.raises(ConfigurationError):\n\n            @base_class.register(""fake"")\n            class FakeAlternate(base_class):\n\n                pass\n\n        # Registering under a name that already exists should overwrite\n        # if exist_ok=True.\n        @base_class.register(""fake"", exist_ok=True)  # noqa\n        class FakeAlternate(base_class):\n\n            pass\n\n        assert base_class.by_name(""fake"") == FakeAlternate\n\n        del Registrable._registry[base_class][""fake""]\n\n    # TODO(mattg): maybe move all of these into tests for the base class?\n\n    def test_registry_has_builtin_samplers(self):\n        assert Sampler.by_name(""random"").__name__ == ""RandomSampler""\n        assert Sampler.by_name(""sequential"").__name__ == ""SequentialSampler""\n        assert BatchSampler.by_name(""bucket"").__name__ == ""BucketBatchSampler""\n\n    def test_registry_has_builtin_tokenizers(self):\n        assert Tokenizer.by_name(""spacy"").__name__ == ""SpacyTokenizer""\n        assert Tokenizer.by_name(""character"").__name__ == ""CharacterTokenizer""\n\n    def test_registry_has_builtin_token_indexers(self):\n        assert TokenIndexer.by_name(""single_id"").__name__ == ""SingleIdTokenIndexer""\n        assert TokenIndexer.by_name(""characters"").__name__ == ""TokenCharactersIndexer""\n\n    def test_registry_has_builtin_regularizers(self):\n        assert Regularizer.by_name(""l1"").__name__ == ""L1Regularizer""\n        assert Regularizer.by_name(""l2"").__name__ == ""L2Regularizer""\n\n    def test_registry_has_builtin_token_embedders(self):\n        assert TokenEmbedder.by_name(""embedding"").__name__ == ""Embedding""\n        assert TokenEmbedder.by_name(""character_encoding"").__name__ == ""TokenCharactersEncoder""\n\n    def test_registry_has_builtin_text_field_embedders(self):\n        assert TextFieldEmbedder.by_name(""basic"").__name__ == ""BasicTextFieldEmbedder""\n\n    def test_implicit_include_package(self):\n        # Create a new package in a temporary dir\n        packagedir = self.TEST_DIR / ""testpackage""\n        packagedir.mkdir()\n        (packagedir / ""__init__.py"").touch()\n\n        # And add that directory to the path\n        with push_python_path(self.TEST_DIR):\n            # Write out a duplicate dataset reader there, but registered under a different name.\n            reader = DatasetReader.by_name(""text_classification_json"")\n\n            with open(inspect.getabsfile(reader)) as f:\n                code = f.read().replace(\n                    """"""@DatasetReader.register(""text_classification_json"")"""""",\n                    """"""@DatasetReader.register(""text_classification_json-fake"")"""""",\n                )\n\n            with open(os.path.join(packagedir, ""reader.py""), ""w"") as f:\n                f.write(code)\n\n            # Fails to import by registered name\n            with pytest.raises(ConfigurationError) as exc:\n                DatasetReader.by_name(""text_classification_json-fake"")\n                assert ""is not a registered name"" in str(exc.value)\n\n            # Fails to import with wrong module name\n            with pytest.raises(ConfigurationError) as exc:\n                DatasetReader.by_name(\n                    ""testpackage.text_classification_json.TextClassificationJsonReader""\n                )\n                assert ""unable to import module"" in str(exc.value)\n\n            # Fails to import with wrong class name\n            with pytest.raises(ConfigurationError):\n                DatasetReader.by_name(""testpackage.reader.FakeReader"")\n                assert ""unable to find class"" in str(exc.value)\n\n            # Imports successfully with right fully qualified name\n            duplicate_reader = DatasetReader.by_name(\n                ""testpackage.reader.TextClassificationJsonReader""\n            )\n            assert duplicate_reader.__name__ == ""TextClassificationJsonReader""\n'"
tests/common/testing.py,1,"b'import torch\n\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\n\nactual_devices = set()\n\n\nclass TestTesting(AllenNlpTestCase):\n    @multi_device\n    def test_multi_device(self, device: str):\n        actual_devices.add(device)\n\n    def test_devices_accounted_for(self):\n        expected_devices = {""cpu"", ""cuda""} if torch.cuda.is_available() else {""cpu""}\n        assert expected_devices == actual_devices\n'"
tests/common/util_test.py,4,"b'import sys\nfrom collections import OrderedDict\n\nimport pytest\nimport torch\n\nfrom allennlp.common import util\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.util import push_python_path\n\n\nclass Unsanitizable:\n    pass\n\n\nclass Sanitizable:\n    def to_json(self):\n        return {""sanitizable"": True}\n\n\nclass TestCommonUtils(AllenNlpTestCase):\n    def test_group_by_count(self):\n        assert util.group_by_count([1, 2, 3, 4, 5, 6, 7], 3, 20) == [\n            [1, 2, 3],\n            [4, 5, 6],\n            [7, 20, 20],\n        ]\n\n    def test_lazy_groups_of(self):\n        xs = [1, 2, 3, 4, 5, 6, 7]\n        groups = util.lazy_groups_of(iter(xs), group_size=3)\n        assert next(groups) == [1, 2, 3]\n        assert next(groups) == [4, 5, 6]\n        assert next(groups) == [7]\n        with pytest.raises(StopIteration):\n            _ = next(groups)\n\n    def test_pad_sequence_to_length(self):\n        assert util.pad_sequence_to_length([1, 2, 3], 5) == [1, 2, 3, 0, 0]\n        assert util.pad_sequence_to_length([1, 2, 3], 5, default_value=lambda: 2) == [1, 2, 3, 2, 2]\n        assert util.pad_sequence_to_length([1, 2, 3], 5, padding_on_right=False) == [0, 0, 1, 2, 3]\n\n    def test_namespace_match(self):\n        assert util.namespace_match(""*tags"", ""tags"")\n        assert util.namespace_match(""*tags"", ""passage_tags"")\n        assert util.namespace_match(""*tags"", ""question_tags"")\n        assert util.namespace_match(""tokens"", ""tokens"")\n        assert not util.namespace_match(""tokens"", ""stemmed_tokens"")\n\n    def test_sanitize(self):\n        assert util.sanitize(torch.Tensor([1, 2])) == [1, 2]\n        assert util.sanitize(torch.LongTensor([1, 2])) == [1, 2]\n\n        with pytest.raises(ValueError):\n            util.sanitize(Unsanitizable())\n\n        assert util.sanitize(Sanitizable()) == {""sanitizable"": True}\n\n    def test_import_submodules(self):\n        (self.TEST_DIR / ""mymodule"").mkdir()\n        (self.TEST_DIR / ""mymodule"" / ""__init__.py"").touch()\n        (self.TEST_DIR / ""mymodule"" / ""submodule"").mkdir()\n        (self.TEST_DIR / ""mymodule"" / ""submodule"" / ""__init__.py"").touch()\n        (self.TEST_DIR / ""mymodule"" / ""submodule"" / ""subsubmodule.py"").touch()\n\n        with push_python_path(self.TEST_DIR):\n            assert ""mymodule"" not in sys.modules\n            assert ""mymodule.submodule"" not in sys.modules\n\n            util.import_module_and_submodules(""mymodule"")\n\n            assert ""mymodule"" in sys.modules\n            assert ""mymodule.submodule"" in sys.modules\n            assert ""mymodule.submodule.subsubmodule"" in sys.modules\n\n    def test_get_frozen_and_tunable_parameter_names(self):\n        model = torch.nn.Sequential(\n            OrderedDict([(""conv"", torch.nn.Conv1d(5, 5, 5)), (""linear"", torch.nn.Linear(5, 10))])\n        )\n        named_parameters = dict(model.named_parameters())\n        named_parameters[""linear.weight""].requires_grad_(False)\n        named_parameters[""linear.bias""].requires_grad_(False)\n        (\n            frozen_parameter_names,\n            tunable_parameter_names,\n        ) = util.get_frozen_and_tunable_parameter_names(model)\n        assert set(frozen_parameter_names) == {""linear.weight"", ""linear.bias""}\n        assert set(tunable_parameter_names) == {""conv.weight"", ""conv.bias""}\n\n    def test_sanitize_ptb_tokenized_string(self):\n        def create_surrounding_test_case(start_ptb_token, end_ptb_token, start_token, end_token):\n            return (\n                ""a {} b c {} d"".format(start_ptb_token, end_ptb_token),\n                ""a {}b c{} d"".format(start_token, end_token),\n            )\n\n        def create_fwd_token_test_case(fwd_token):\n            return ""a {} b"".format(fwd_token), ""a {}b"".format(fwd_token)\n\n        def create_backward_token_test_case(backward_token):\n            return ""a {} b"".format(backward_token), ""a{} b"".format(backward_token)\n\n        punct_forward = {""`"", ""$"", ""#""}\n        punct_backward = {""."", "","", ""!"", ""?"", "":"", "";"", ""%"", ""\'""}\n\n        test_cases = [\n            # Parentheses\n            create_surrounding_test_case(""-lrb-"", ""-rrb-"", ""("", "")""),\n            create_surrounding_test_case(""-lsb-"", ""-rsb-"", ""["", ""]""),\n            create_surrounding_test_case(""-lcb-"", ""-rcb-"", ""{"", ""}""),\n            # Parentheses don\'t have to match\n            create_surrounding_test_case(""-lsb-"", ""-rcb-"", ""["", ""}""),\n            # Also check that casing doesn\'t matter\n            create_surrounding_test_case(""-LsB-"", ""-rcB-"", ""["", ""}""),\n            # Quotes\n            create_surrounding_test_case(""``"", ""\'\'"", \'""\', \'""\'),\n            # Start/end tokens\n            create_surrounding_test_case(""<s>"", ""</s>"", """", """"),\n            # Tokens that merge forward\n            *[create_fwd_token_test_case(t) for t in punct_forward],\n            # Tokens that merge backward\n            *[create_backward_token_test_case(t) for t in punct_backward],\n            # Merge tokens starting with \' backwards\n            (""I \'m"", ""I\'m""),\n            # Merge tokens backwards when matching (n\'t or na) (special cases, parentheses behave in the same way)\n            (""I do n\'t"", ""I don\'t""),\n            (""gon na"", ""gonna""),\n            # Also make sure casing is preserved\n            (""gon NA"", ""gonNA""),\n            # This is a no op\n            (""A b C d"", ""A b C d""),\n        ]\n\n        for ptb_string, expected in test_cases:\n            actual = util.sanitize_ptb_tokenized_string(ptb_string)\n            assert actual == expected\n'"
tests/data/__init__.py,0,b''
tests/data/dataset_test.py,0,"b'import pytest\nimport numpy\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Instance, Token, Vocabulary\nfrom allennlp.data.batch import Batch\nfrom allennlp.data.fields import TextField, LabelField\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\n\n\nclass TestDataset(AllenNlpTestCase):\n    def setup_method(self):\n        self.vocab = Vocabulary()\n        self.vocab.add_token_to_namespace(""this"")\n        self.vocab.add_token_to_namespace(""is"")\n        self.vocab.add_token_to_namespace(""a"")\n        self.vocab.add_token_to_namespace(""sentence"")\n        self.vocab.add_token_to_namespace(""."")\n        self.token_indexer = {""tokens"": SingleIdTokenIndexer()}\n        self.instances = self.get_instances()\n        super().setup_method()\n\n    def test_instances_must_have_homogeneous_fields(self):\n        instance1 = Instance({""tag"": (LabelField(1, skip_indexing=True))})\n        instance2 = Instance({""words"": TextField([Token(""hello"")], {})})\n        with pytest.raises(ConfigurationError):\n            _ = Batch([instance1, instance2])\n\n    def test_padding_lengths_uses_max_instance_lengths(self):\n        dataset = Batch(self.instances)\n        dataset.index_instances(self.vocab)\n        padding_lengths = dataset.get_padding_lengths()\n        assert padding_lengths == {""text1"": {""tokens___tokens"": 5}, ""text2"": {""tokens___tokens"": 6}}\n\n    def test_as_tensor_dict(self):\n        dataset = Batch(self.instances)\n        dataset.index_instances(self.vocab)\n        padding_lengths = dataset.get_padding_lengths()\n        tensors = dataset.as_tensor_dict(padding_lengths)\n        text1 = tensors[""text1""][""tokens""][""tokens""].detach().cpu().numpy()\n        text2 = tensors[""text2""][""tokens""][""tokens""].detach().cpu().numpy()\n\n        numpy.testing.assert_array_almost_equal(\n            text1, numpy.array([[2, 3, 4, 5, 6], [1, 3, 4, 5, 6]])\n        )\n        numpy.testing.assert_array_almost_equal(\n            text2, numpy.array([[2, 3, 4, 1, 5, 6], [2, 3, 1, 0, 0, 0]])\n        )\n\n    def get_instances(self):\n        field1 = TextField(\n            [Token(t) for t in [""this"", ""is"", ""a"", ""sentence"", "".""]], self.token_indexer\n        )\n        field2 = TextField(\n            [Token(t) for t in [""this"", ""is"", ""a"", ""different"", ""sentence"", "".""]],\n            self.token_indexer,\n        )\n        field3 = TextField(\n            [Token(t) for t in [""here"", ""is"", ""a"", ""sentence"", "".""]], self.token_indexer\n        )\n        field4 = TextField([Token(t) for t in [""this"", ""is"", ""short""]], self.token_indexer)\n        instances = [\n            Instance({""text1"": field1, ""text2"": field2}),\n            Instance({""text1"": field3, ""text2"": field4}),\n        ]\n        return instances\n'"
tests/data/instance_test.py,0,"b'from allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Instance\nfrom allennlp.data.fields import TextField, LabelField\nfrom allennlp.data.token_indexers import PretrainedTransformerIndexer\nfrom allennlp.data.tokenizers import Token\n\n\nclass TestInstance(AllenNlpTestCase):\n    def test_instance_implements_mutable_mapping(self):\n        words_field = TextField([Token(""hello"")], {})\n        label_field = LabelField(1, skip_indexing=True)\n        instance = Instance({""words"": words_field, ""labels"": label_field})\n\n        assert instance[""words""] == words_field\n        assert instance[""labels""] == label_field\n        assert len(instance) == 2\n\n        keys = {k for k, v in instance.items()}\n        assert keys == {""words"", ""labels""}\n\n        values = [v for k, v in instance.items()]\n        assert words_field in values\n        assert label_field in values\n\n    def test_duplicate(self):\n        # Verify the `duplicate()` method works with a `PretrainedTransformerIndexer` in\n        # a `TextField`. See https://github.com/allenai/allennlp/issues/4270.\n        instance = Instance(\n            {\n                ""words"": TextField(\n                    [Token(""hello"")], {""tokens"": PretrainedTransformerIndexer(""bert-base-uncased"")}\n                )\n            }\n        )\n\n        other = instance.duplicate()\n        assert other == instance\n\n        # Adding new fields to the original instance should not effect the duplicate.\n        instance.add_field(""labels"", LabelField(""some_label""))\n        assert ""labels"" not in other.fields\n        assert other != instance  # sanity check on the \'__eq__\' method.\n'"
tests/data/vocabulary_test.py,0,"b'import codecs\nimport gzip\nimport pickle\nimport shutil\nimport zipfile\nfrom copy import deepcopy\n\nimport pytest\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.params import Params\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Instance, Token\nfrom allennlp.data.batch import Batch\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer, TokenCharactersIndexer\nfrom allennlp.data.tokenizers import CharacterTokenizer\nfrom allennlp.data.vocabulary import (\n    _NamespaceDependentDefaultDict,\n    _read_pretrained_tokens,\n    DEFAULT_OOV_TOKEN,\n    Vocabulary,\n)\nfrom allennlp.modules.token_embedders.embedding import format_embeddings_file_uri\n\n\nclass TestVocabulary(AllenNlpTestCase):\n    def setup_method(self):\n        token_indexer = SingleIdTokenIndexer(""tokens"")\n        text_field = TextField(\n            [Token(t) for t in [""a"", ""a"", ""a"", ""a"", ""b"", ""b"", ""c"", ""c"", ""c""]],\n            {""tokens"": token_indexer},\n        )\n        self.instance = Instance({""text"": text_field})\n        self.dataset = Batch([self.instance])\n        super().setup_method()\n\n    def test_pickling(self):\n        vocab = Vocabulary.from_instances(self.dataset)\n\n        pickled = pickle.dumps(vocab)\n        unpickled = pickle.loads(pickled)\n\n        assert dict(unpickled._index_to_token) == dict(vocab._index_to_token)\n        assert dict(unpickled._token_to_index) == dict(vocab._token_to_index)\n        assert unpickled._non_padded_namespaces == vocab._non_padded_namespaces\n        assert unpickled._oov_token == vocab._oov_token\n        assert unpickled._padding_token == vocab._padding_token\n        assert unpickled._retained_counter == vocab._retained_counter\n\n    def test_from_dataset_respects_max_vocab_size_single_int(self):\n        max_vocab_size = 1\n        vocab = Vocabulary.from_instances(self.dataset, max_vocab_size=max_vocab_size)\n        words = vocab.get_index_to_token_vocabulary().values()\n        # Additional 2 tokens are \'@@PADDING@@\' and \'@@UNKNOWN@@\' by default\n        assert len(words) == max_vocab_size + 2\n\n        vocab = Vocabulary.from_instances(self.dataset, min_count=None)\n        words = vocab.get_index_to_token_vocabulary().values()\n        assert len(words) == 5\n\n    def test_from_dataset_respects_min_count(self):\n        vocab = Vocabulary.from_instances(self.dataset, min_count={""tokens"": 4})\n        words = vocab.get_index_to_token_vocabulary().values()\n        assert ""a"" in words\n        assert ""b"" not in words\n        assert ""c"" not in words\n\n        vocab = Vocabulary.from_instances(self.dataset, min_count=None)\n        words = vocab.get_index_to_token_vocabulary().values()\n        assert ""a"" in words\n        assert ""b"" in words\n        assert ""c"" in words\n\n    def test_from_dataset_respects_exclusive_embedding_file(self):\n        embeddings_filename = str(self.TEST_DIR / ""embeddings.gz"")\n        with gzip.open(embeddings_filename, ""wb"") as embeddings_file:\n            embeddings_file.write(""a 1.0 2.3 -1.0\\n"".encode(""utf-8""))\n            embeddings_file.write(""b 0.1 0.4 -4.0\\n"".encode(""utf-8""))\n\n        vocab = Vocabulary.from_instances(\n            self.dataset,\n            min_count={""tokens"": 4},\n            pretrained_files={""tokens"": embeddings_filename},\n            only_include_pretrained_words=True,\n        )\n        words = vocab.get_index_to_token_vocabulary().values()\n        assert ""a"" in words\n        assert ""b"" not in words\n        assert ""c"" not in words\n\n        vocab = Vocabulary.from_instances(\n            self.dataset,\n            pretrained_files={""tokens"": embeddings_filename},\n            only_include_pretrained_words=True,\n        )\n        words = vocab.get_index_to_token_vocabulary().values()\n        assert ""a"" in words\n        assert ""b"" in words\n        assert ""c"" not in words\n\n    def test_from_dataset_respects_inclusive_embedding_file(self):\n        embeddings_filename = str(self.TEST_DIR / ""embeddings.gz"")\n        with gzip.open(embeddings_filename, ""wb"") as embeddings_file:\n            embeddings_file.write(""a 1.0 2.3 -1.0\\n"".encode(""utf-8""))\n            embeddings_file.write(""b 0.1 0.4 -4.0\\n"".encode(""utf-8""))\n\n        vocab = Vocabulary.from_instances(\n            self.dataset,\n            min_count={""tokens"": 4},\n            pretrained_files={""tokens"": embeddings_filename},\n            only_include_pretrained_words=False,\n        )\n        words = vocab.get_index_to_token_vocabulary().values()\n        assert ""a"" in words\n        assert ""b"" in words\n        assert ""c"" not in words\n\n        vocab = Vocabulary.from_instances(\n            self.dataset,\n            pretrained_files={""tokens"": embeddings_filename},\n            only_include_pretrained_words=False,\n        )\n        words = vocab.get_index_to_token_vocabulary().values()\n        assert ""a"" in words\n        assert ""b"" in words\n        assert ""c"" in words\n\n    def test_add_word_to_index_gives_consistent_results(self):\n        vocab = Vocabulary()\n        initial_vocab_size = vocab.get_vocab_size()\n        word_index = vocab.add_token_to_namespace(""word"")\n        assert ""word"" in vocab.get_index_to_token_vocabulary().values()\n        assert vocab.get_token_index(""word"") == word_index\n        assert vocab.get_token_from_index(word_index) == ""word""\n        assert vocab.get_vocab_size() == initial_vocab_size + 1\n\n        # Now add it again, and make sure nothing changes.\n        vocab.add_token_to_namespace(""word"")\n        assert ""word"" in vocab.get_index_to_token_vocabulary().values()\n        assert vocab.get_token_index(""word"") == word_index\n        assert vocab.get_token_from_index(word_index) == ""word""\n        assert vocab.get_vocab_size() == initial_vocab_size + 1\n\n    def test_namespaces(self):\n        vocab = Vocabulary()\n        initial_vocab_size = vocab.get_vocab_size()\n        word_index = vocab.add_token_to_namespace(""word"", namespace=""1"")\n        assert ""word"" in vocab.get_index_to_token_vocabulary(namespace=""1"").values()\n        assert vocab.get_token_index(""word"", namespace=""1"") == word_index\n        assert vocab.get_token_from_index(word_index, namespace=""1"") == ""word""\n        assert vocab.get_vocab_size(namespace=""1"") == initial_vocab_size + 1\n\n        # Now add it again, in a different namespace and a different word, and make sure it\'s like\n        # new.\n        word2_index = vocab.add_token_to_namespace(""word2"", namespace=""2"")\n        word_index = vocab.add_token_to_namespace(""word"", namespace=""2"")\n        assert ""word"" in vocab.get_index_to_token_vocabulary(namespace=""2"").values()\n        assert ""word2"" in vocab.get_index_to_token_vocabulary(namespace=""2"").values()\n        assert vocab.get_token_index(""word"", namespace=""2"") == word_index\n        assert vocab.get_token_index(""word2"", namespace=""2"") == word2_index\n        assert vocab.get_token_from_index(word_index, namespace=""2"") == ""word""\n        assert vocab.get_token_from_index(word2_index, namespace=""2"") == ""word2""\n        assert vocab.get_vocab_size(namespace=""2"") == initial_vocab_size + 2\n\n    def test_namespace_dependent_default_dict(self):\n        default_dict = _NamespaceDependentDefaultDict([""bar"", ""*baz""], lambda: 7, lambda: 3)\n        # \'foo\' is not a padded namespace\n        assert default_dict[""foo""] == 7\n        # ""baz"" is a direct match with a padded namespace\n        assert default_dict[""baz""] == 3\n        # the following match the wildcard ""*baz""\n        assert default_dict[""bar""] == 3\n        assert default_dict[""foobaz""] == 3\n\n    def test_unknown_token(self):\n        # We\'re putting this behavior in a test so that the behavior is documented.  There is\n        # solver code that depends in a small way on how we treat the unknown token, so any\n        # breaking change to this behavior should break a test, so you know you\'ve done something\n        # that needs more consideration.\n        vocab = Vocabulary()\n        oov_token = vocab._oov_token\n        oov_index = vocab.get_token_index(oov_token)\n        assert oov_index == 1\n        assert vocab.get_token_index(""unseen word"") == oov_index\n\n    def test_get_token_index(self):\n        # The behavior of get_token_index depends on whether or not the namespace has an OOV token.\n        vocab = Vocabulary(\n            counter={""labels"": {""foo"": 3, ""bar"": 2}, ""tokens"": {""foo"": 3, ""bar"": 2}},\n            non_padded_namespaces=[""labels""],\n        )\n\n        # Quick sanity check, this is what the token to index mappings should look like.\n        expected_token_to_index_dicts = {\n            ""tokens"": {vocab._padding_token: 0, vocab._oov_token: 1, ""foo"": 2, ""bar"": 3},\n            ""labels"": {""foo"": 0, ""bar"": 1},\n        }\n        assert vocab._token_to_index[""tokens""] == expected_token_to_index_dicts[""tokens""]\n        assert vocab._token_to_index[""labels""] == expected_token_to_index_dicts[""labels""]\n\n        # get_token_index should return the OOV token index for OOV tokens when it can.\n        assert vocab.get_token_index(""baz"", ""tokens"") == 1\n\n        # get_token_index should raise helpful error message when token is OOV and there\n        # is no default OOV token in the namespace.\n        with pytest.raises(\n            KeyError,\n            match=r""\'baz\' not found .* and namespace does not contain the default OOV token .*"",\n        ):\n            vocab.get_token_index(""baz"", ""labels"")\n\n        # same should happen for the default OOV token itself, if not in namespace.\n        with pytest.raises(KeyError, match=rf""\'{vocab._oov_token}\' not found .*""):\n            vocab.get_token_index(vocab._oov_token, ""labels"")\n\n        # Now just make sure the token_to_index mappings haven\'t been modified\n        # (since we\'re defaultdicts we need to be a little careful here).\n        assert vocab._token_to_index[""tokens""] == expected_token_to_index_dicts[""tokens""]\n        assert vocab._token_to_index[""labels""] == expected_token_to_index_dicts[""labels""]\n\n    def test_set_from_file_reads_padded_files(self):\n\n        vocab_filename = self.TEST_DIR / ""vocab_file""\n        with codecs.open(vocab_filename, ""w"", ""utf-8"") as vocab_file:\n            vocab_file.write(""<S>\\n"")\n            vocab_file.write(""</S>\\n"")\n            vocab_file.write(""<UNK>\\n"")\n            vocab_file.write(""a\\n"")\n            vocab_file.write(""tricky\\x0bchar\\n"")\n            vocab_file.write(""word\\n"")\n            vocab_file.write(""another\\n"")\n\n        vocab = Vocabulary()\n        vocab.set_from_file(vocab_filename, is_padded=True, oov_token=""<UNK>"")\n\n        assert vocab._oov_token == DEFAULT_OOV_TOKEN\n        assert vocab.get_token_index(""random string"") == 3\n        assert vocab.get_token_index(""<S>"") == 1\n        assert vocab.get_token_index(""</S>"") == 2\n        assert vocab.get_token_index(DEFAULT_OOV_TOKEN) == 3\n        assert vocab.get_token_index(""a"") == 4\n        assert vocab.get_token_index(""tricky\\x0bchar"") == 5\n        assert vocab.get_token_index(""word"") == 6\n        assert vocab.get_token_index(""another"") == 7\n        assert vocab.get_token_from_index(0) == vocab._padding_token\n        assert vocab.get_token_from_index(1) == ""<S>""\n        assert vocab.get_token_from_index(2) == ""</S>""\n        assert vocab.get_token_from_index(3) == DEFAULT_OOV_TOKEN\n        assert vocab.get_token_from_index(4) == ""a""\n        assert vocab.get_token_from_index(5) == ""tricky\\x0bchar""\n        assert vocab.get_token_from_index(6) == ""word""\n        assert vocab.get_token_from_index(7) == ""another""\n\n    def test_set_from_file_reads_non_padded_files(self):\n\n        vocab_filename = self.TEST_DIR / ""vocab_file""\n        with codecs.open(vocab_filename, ""w"", ""utf-8"") as vocab_file:\n            vocab_file.write(""B-PERS\\n"")\n            vocab_file.write(""I-PERS\\n"")\n            vocab_file.write(""O\\n"")\n            vocab_file.write(""B-ORG\\n"")\n            vocab_file.write(""I-ORG\\n"")\n\n        vocab = Vocabulary()\n        vocab.set_from_file(vocab_filename, is_padded=False, namespace=""tags"")\n        assert vocab.get_token_index(""B-PERS"", namespace=""tags"") == 0\n        assert vocab.get_token_index(""I-PERS"", namespace=""tags"") == 1\n        assert vocab.get_token_index(""O"", namespace=""tags"") == 2\n        assert vocab.get_token_index(""B-ORG"", namespace=""tags"") == 3\n        assert vocab.get_token_index(""I-ORG"", namespace=""tags"") == 4\n        assert vocab.get_token_from_index(0, namespace=""tags"") == ""B-PERS""\n        assert vocab.get_token_from_index(1, namespace=""tags"") == ""I-PERS""\n        assert vocab.get_token_from_index(2, namespace=""tags"") == ""O""\n        assert vocab.get_token_from_index(3, namespace=""tags"") == ""B-ORG""\n        assert vocab.get_token_from_index(4, namespace=""tags"") == ""I-ORG""\n\n    def test_saving_and_loading(self):\n\n        vocab_dir = self.TEST_DIR / ""vocab_save""\n\n        vocab = Vocabulary(non_padded_namespaces=[""a"", ""c""])\n        vocab.add_tokens_to_namespace(\n            [""a0"", ""a1"", ""a2""], namespace=""a""\n        )  # non-padded, should start at 0\n        vocab.add_tokens_to_namespace([""b2"", ""b3""], namespace=""b"")  # padded, should start at 2\n\n        vocab.save_to_files(vocab_dir)\n        vocab2 = Vocabulary.from_files(vocab_dir)\n\n        assert vocab2._non_padded_namespaces == {""a"", ""c""}\n\n        # Check namespace a.\n        assert vocab2.get_vocab_size(namespace=""a"") == 3\n        assert vocab2.get_token_from_index(0, namespace=""a"") == ""a0""\n        assert vocab2.get_token_from_index(1, namespace=""a"") == ""a1""\n        assert vocab2.get_token_from_index(2, namespace=""a"") == ""a2""\n        assert vocab2.get_token_index(""a0"", namespace=""a"") == 0\n        assert vocab2.get_token_index(""a1"", namespace=""a"") == 1\n        assert vocab2.get_token_index(""a2"", namespace=""a"") == 2\n\n        # Check namespace b.\n        assert vocab2.get_vocab_size(namespace=""b"") == 4  # (unk + padding + two tokens)\n        assert vocab2.get_token_from_index(0, namespace=""b"") == vocab._padding_token\n        assert vocab2.get_token_from_index(1, namespace=""b"") == vocab._oov_token\n        assert vocab2.get_token_from_index(2, namespace=""b"") == ""b2""\n        assert vocab2.get_token_from_index(3, namespace=""b"") == ""b3""\n        assert vocab2.get_token_index(vocab._padding_token, namespace=""b"") == 0\n        assert vocab2.get_token_index(vocab._oov_token, namespace=""b"") == 1\n        assert vocab2.get_token_index(""b2"", namespace=""b"") == 2\n        assert vocab2.get_token_index(""b3"", namespace=""b"") == 3\n\n        # Check the dictionaries containing the reverse mapping are identical.\n        assert vocab.get_index_to_token_vocabulary(""a"") == vocab2.get_index_to_token_vocabulary(""a"")\n        assert vocab.get_index_to_token_vocabulary(""b"") == vocab2.get_index_to_token_vocabulary(""b"")\n\n    def test_saving_and_loading_works_with_byte_encoding(self):\n        # We\'re going to set a vocabulary from a TextField using byte encoding, index it, save the\n        # vocab, load the vocab, then index the text field again, and make sure we get the same\n        # result.\n        tokenizer = CharacterTokenizer(byte_encoding=""utf-8"")\n        token_indexer = TokenCharactersIndexer(character_tokenizer=tokenizer, min_padding_length=2)\n        tokens = [Token(t) for t in [""\xc3\x98yvind"", ""f\xc3\xbcr"", ""\xe6\xb1\x89\xe5\xad\x97""]]\n        text_field = TextField(tokens, {""characters"": token_indexer})\n        dataset = Batch([Instance({""sentence"": text_field})])\n        vocab = Vocabulary.from_instances(dataset)\n        text_field.index(vocab)\n        indexed_tokens = deepcopy(text_field._indexed_tokens)\n\n        vocab_dir = self.TEST_DIR / ""vocab_save""\n        vocab.save_to_files(vocab_dir)\n        vocab2 = Vocabulary.from_files(vocab_dir)\n        text_field2 = TextField(tokens, {""characters"": token_indexer})\n        text_field2.index(vocab2)\n        indexed_tokens2 = deepcopy(text_field2._indexed_tokens)\n        assert indexed_tokens == indexed_tokens2\n\n    def test_from_params(self):\n        # Save a vocab to check we can load it from_params.\n        vocab_dir = self.TEST_DIR / ""vocab_save""\n        vocab = Vocabulary(non_padded_namespaces=[""a"", ""c""])\n        vocab.add_tokens_to_namespace(\n            [""a0"", ""a1"", ""a2""], namespace=""a""\n        )  # non-padded, should start at 0\n        vocab.add_tokens_to_namespace([""b2"", ""b3""], namespace=""b"")  # padded, should start at 2\n        vocab.save_to_files(vocab_dir)\n\n        params = Params({""type"": ""from_files"", ""directory"": vocab_dir})\n        vocab2 = Vocabulary.from_params(params)\n        assert vocab.get_index_to_token_vocabulary(""a"") == vocab2.get_index_to_token_vocabulary(""a"")\n        assert vocab.get_index_to_token_vocabulary(""b"") == vocab2.get_index_to_token_vocabulary(""b"")\n\n        # Test case where we build a vocab from a dataset.\n        vocab2 = Vocabulary.from_params(Params({}), instances=self.dataset)\n        assert vocab2.get_index_to_token_vocabulary(""tokens"") == {\n            0: ""@@PADDING@@"",\n            1: ""@@UNKNOWN@@"",\n            2: ""a"",\n            3: ""c"",\n            4: ""b"",\n        }\n        # Test from_params raises when we have neither a dataset and a vocab_directory.\n        with pytest.raises(ConfigurationError):\n            _ = Vocabulary.from_params(Params({}))\n\n        # Test from_params raises when there are any other dict keys\n        # present apart from \'directory\' and we aren\'t calling from_dataset.\n        with pytest.raises(ConfigurationError):\n            _ = Vocabulary.from_params(\n                Params({""type"": ""from_files"", ""directory"": vocab_dir, ""min_count"": {""tokens"": 2}})\n            )\n\n    def test_from_params_adds_tokens_to_vocab(self):\n        vocab = Vocabulary.from_params(\n            Params({""tokens_to_add"": {""tokens"": [""q"", ""x"", ""z""]}}), instances=self.dataset\n        )\n        assert vocab.get_index_to_token_vocabulary(""tokens"") == {\n            0: ""@@PADDING@@"",\n            1: ""@@UNKNOWN@@"",\n            2: ""a"",\n            3: ""c"",\n            4: ""b"",\n            5: ""q"",\n            6: ""x"",\n            7: ""z"",\n        }\n\n    def test_valid_vocab_extension(self):\n        vocab_dir = self.TEST_DIR / ""vocab_save""\n        # Test: padded/non-padded common namespaces are extending appropriately\n        non_padded_namespaces_list = [[], [""tokens""]]\n        for non_padded_namespaces in non_padded_namespaces_list:\n            original_vocab = Vocabulary(non_padded_namespaces=non_padded_namespaces)\n            original_vocab.add_tokens_to_namespace([""d"", ""a"", ""b""], namespace=""tokens"")\n            text_field = TextField(\n                [Token(t) for t in [""a"", ""d"", ""c"", ""e""]], {""tokens"": SingleIdTokenIndexer(""tokens"")}\n            )\n            vocab_dir = self.TEST_DIR / ""vocab_save""\n            shutil.rmtree(vocab_dir, ignore_errors=True)\n            original_vocab.save_to_files(vocab_dir)\n            instances = Batch([Instance({""text"": text_field})])\n            params = Params(\n                {\n                    ""type"": ""extend"",\n                    ""directory"": vocab_dir,\n                    ""non_padded_namespaces"": non_padded_namespaces,\n                }\n            )\n            extended_vocab = Vocabulary.from_params(params, instances=instances)\n\n            extra_count = 2 if extended_vocab.is_padded(""tokens"") else 0\n            assert extended_vocab.get_token_index(""d"", ""tokens"") == 0 + extra_count\n            assert extended_vocab.get_token_index(""a"", ""tokens"") == 1 + extra_count\n            assert extended_vocab.get_token_index(""b"", ""tokens"") == 2 + extra_count\n\n            assert extended_vocab.get_token_index(""c"", ""tokens"")  # should be present\n            assert extended_vocab.get_token_index(""e"", ""tokens"")  # should be present\n\n            assert extended_vocab.get_vocab_size(""tokens"") == 5 + extra_count\n\n        # Test: padded/non-padded non-common namespaces are extending appropriately\n        non_padded_namespaces_list = [[], [""tokens1""], [""tokens1"", ""tokens2""]]\n        for non_padded_namespaces in non_padded_namespaces_list:\n            original_vocab = Vocabulary(non_padded_namespaces=non_padded_namespaces)\n            original_vocab.add_token_to_namespace(""a"", namespace=""tokens1"")  # index2\n            text_field = TextField(\n                [Token(t) for t in [""b""]], {""tokens2"": SingleIdTokenIndexer(""tokens2"")}\n            )\n            instances = Batch([Instance({""text"": text_field})])\n            vocab_dir = self.TEST_DIR / ""vocab_save""\n            shutil.rmtree(vocab_dir, ignore_errors=True)\n            original_vocab.save_to_files(vocab_dir)\n\n            params = Params(\n                {\n                    ""type"": ""extend"",\n                    ""directory"": vocab_dir,\n                    ""non_padded_namespaces"": non_padded_namespaces,\n                }\n            )\n            extended_vocab = Vocabulary.from_params(params, instances=instances)\n\n            # Should have two namespaces\n            assert len(extended_vocab._token_to_index) == 2\n\n            extra_count = 2 if extended_vocab.is_padded(""tokens1"") else 0\n            assert extended_vocab.get_vocab_size(""tokens1"") == 1 + extra_count\n\n            extra_count = 2 if extended_vocab.is_padded(""tokens2"") else 0\n            assert extended_vocab.get_vocab_size(""tokens2"") == 1 + extra_count\n\n    def test_invalid_vocab_extension(self):\n        vocab_dir = self.TEST_DIR / ""vocab_save""\n        original_vocab = Vocabulary(non_padded_namespaces=[""tokens1""])\n        original_vocab.add_tokens_to_namespace([""a"", ""b""], namespace=""tokens1"")\n        original_vocab.add_token_to_namespace(""p"", namespace=""tokens2"")\n        original_vocab.save_to_files(vocab_dir)\n        text_field1 = TextField(\n            [Token(t) for t in [""a"", ""c""]], {""tokens1"": SingleIdTokenIndexer(""tokens1"")}\n        )\n        text_field2 = TextField(\n            [Token(t) for t in [""p"", ""q"", ""r""]], {""tokens2"": SingleIdTokenIndexer(""tokens2"")}\n        )\n        instances = Batch([Instance({""text1"": text_field1, ""text2"": text_field2})])\n\n        # Following 2 should give error: tokens1 is non-padded in original_vocab but not in instances\n        params = Params(\n            {\n                ""type"": ""extend"",\n                ""directory"": vocab_dir,\n                ""non_padded_namespaces"": [],\n                ""tokens_to_add"": {""tokens1"": [""a""], ""tokens2"": [""p""]},\n            }\n        )\n        with pytest.raises(ConfigurationError):\n            _ = Vocabulary.from_params(params, instances=instances)\n\n        # Following 2 should not give error: overlapping namespaces have same padding setting\n        params = Params(\n            {\n                ""type"": ""extend"",\n                ""directory"": vocab_dir,\n                ""non_padded_namespaces"": [""tokens1""],\n                ""tokens_to_add"": {""tokens1"": [""a""], ""tokens2"": [""p""]},\n            }\n        )\n        Vocabulary.from_params(params, instances=instances)\n\n        # Following 2 should give error: tokens2 is padded in instances but not in original_vocab\n        params = Params(\n            {\n                ""type"": ""extend"",\n                ""directory"": vocab_dir,\n                ""non_padded_namespaces"": [""tokens1"", ""tokens2""],\n                ""tokens_to_add"": {""tokens1"": [""a""], ""tokens2"": [""p""]},\n            }\n        )\n        with pytest.raises(ConfigurationError):\n            _ = Vocabulary.from_params(params, instances=instances)\n\n    def test_from_params_extend_config(self):\n\n        vocab_dir = self.TEST_DIR / ""vocab_save""\n        original_vocab = Vocabulary(non_padded_namespaces=[""tokens""])\n        original_vocab.add_token_to_namespace(""a"", namespace=""tokens"")\n        original_vocab.save_to_files(vocab_dir)\n\n        text_field = TextField(\n            [Token(t) for t in [""a"", ""b""]], {""tokens"": SingleIdTokenIndexer(""tokens"")}\n        )\n        instances = Batch([Instance({""text"": text_field})])\n\n        # If you ask to extend vocab from `directory`, instances must be passed\n        # in Vocabulary constructor, or else there is nothing to extend to.\n        params = Params({""type"": ""extend"", ""directory"": vocab_dir})\n        with pytest.raises(ConfigurationError):\n            _ = Vocabulary.from_params(params)\n\n        # If you ask to extend vocab, `directory` key must be present in params,\n        # or else there is nothing to extend from.\n        params = Params({""type"": ""extend""})\n        with pytest.raises(ConfigurationError):\n            _ = Vocabulary.from_params(params, instances=instances)\n\n    def test_from_params_valid_vocab_extension_thoroughly(self):\n        """"""\n        Tests for Valid Vocab Extension thoroughly: Vocab extension is valid\n        when overlapping namespaces have same padding behaviour (padded/non-padded)\n        Summary of namespace paddings in this test:\n        original_vocab namespaces\n            tokens0     padded\n            tokens1     non-padded\n            tokens2     padded\n            tokens3     non-padded\n        instances namespaces\n            tokens0     padded\n            tokens1     non-padded\n            tokens4     padded\n            tokens5     non-padded\n        TypicalExtention example: (of tokens1 namespace)\n        -> original_vocab index2token\n           apple          #0->apple\n           bat            #1->bat\n           cat            #2->cat\n        -> Token to be extended with: cat, an, apple, banana, atom, bat\n        -> extended_vocab: index2token\n           apple           #0->apple\n           bat             #1->bat\n           cat             #2->cat\n           an              #3->an\n           atom            #4->atom\n           banana          #5->banana\n        """"""\n\n        vocab_dir = self.TEST_DIR / ""vocab_save""\n        original_vocab = Vocabulary(non_padded_namespaces=[""tokens1"", ""tokens3""])\n        original_vocab.add_token_to_namespace(""apple"", namespace=""tokens0"")  # index:2\n        original_vocab.add_token_to_namespace(""bat"", namespace=""tokens0"")  # index:3\n        original_vocab.add_token_to_namespace(""cat"", namespace=""tokens0"")  # index:4\n\n        original_vocab.add_token_to_namespace(""apple"", namespace=""tokens1"")  # index:0\n        original_vocab.add_token_to_namespace(""bat"", namespace=""tokens1"")  # index:1\n        original_vocab.add_token_to_namespace(""cat"", namespace=""tokens1"")  # index:2\n\n        original_vocab.add_token_to_namespace(""a"", namespace=""tokens2"")  # index:0\n        original_vocab.add_token_to_namespace(""b"", namespace=""tokens2"")  # index:1\n        original_vocab.add_token_to_namespace(""c"", namespace=""tokens2"")  # index:2\n\n        original_vocab.add_token_to_namespace(""p"", namespace=""tokens3"")  # index:0\n        original_vocab.add_token_to_namespace(""q"", namespace=""tokens3"")  # index:1\n\n        original_vocab.save_to_files(vocab_dir)\n\n        text_field0 = TextField(\n            [Token(t) for t in [""cat"", ""an"", ""apple"", ""banana"", ""atom"", ""bat""]],\n            {""tokens0"": SingleIdTokenIndexer(""tokens0"")},\n        )\n        text_field1 = TextField(\n            [Token(t) for t in [""cat"", ""an"", ""apple"", ""banana"", ""atom"", ""bat""]],\n            {""tokens1"": SingleIdTokenIndexer(""tokens1"")},\n        )\n        text_field4 = TextField(\n            [Token(t) for t in [""l"", ""m"", ""n"", ""o""]], {""tokens4"": SingleIdTokenIndexer(""tokens4"")}\n        )\n        text_field5 = TextField(\n            [Token(t) for t in [""x"", ""y"", ""z""]], {""tokens5"": SingleIdTokenIndexer(""tokens5"")}\n        )\n        instances = Batch(\n            [\n                Instance(\n                    {\n                        ""text0"": text_field0,\n                        ""text1"": text_field1,\n                        ""text4"": text_field4,\n                        ""text5"": text_field5,\n                    }\n                )\n            ]\n        )\n\n        params = Params(\n            {\n                ""type"": ""extend"",\n                ""directory"": vocab_dir,\n                ""non_padded_namespaces"": [""tokens1"", ""tokens5""],\n            }\n        )\n        extended_vocab = Vocabulary.from_params(params, instances=instances)\n\n        # namespaces: tokens0, tokens1 is common.\n        # tokens2, tokens3 only vocab has. tokens4, tokens5 only instances\n        extended_namespaces = {*extended_vocab._token_to_index}\n        assert extended_namespaces == {""tokens{}"".format(i) for i in range(6)}\n\n        # # Check that _non_padded_namespaces list is consistent after extension\n        assert extended_vocab._non_padded_namespaces == {""tokens1"", ""tokens3"", ""tokens5""}\n\n        # # original_vocab[""tokens1""] has 3 tokens, instances of ""tokens1"" ns has 5 tokens. 2 overlapping\n        assert extended_vocab.get_vocab_size(""tokens1"") == 6\n        assert extended_vocab.get_vocab_size(""tokens0"") == 8  # 2 extra overlapping because padded\n\n        # namespace tokens3, tokens4 was only in original_vocab,\n        # and its token count should be same in extended_vocab\n        assert extended_vocab.get_vocab_size(""tokens2"") == original_vocab.get_vocab_size(""tokens2"")\n        assert extended_vocab.get_vocab_size(""tokens3"") == original_vocab.get_vocab_size(""tokens3"")\n\n        # namespace tokens2 was only in instances,\n        # and its token count should be same in extended_vocab\n        assert extended_vocab.get_vocab_size(""tokens4"") == 6  # l,m,n,o + oov + padding\n        assert extended_vocab.get_vocab_size(""tokens5"") == 3  # x,y,z\n\n        # Word2index mapping of all words in all namespaces of original_vocab\n        # should be maintained in extended_vocab\n        for namespace, token2index in original_vocab._token_to_index.items():\n            for token, _ in token2index.items():\n                vocab_index = original_vocab.get_token_index(token, namespace)\n                extended_vocab_index = extended_vocab.get_token_index(token, namespace)\n                assert vocab_index == extended_vocab_index\n        # And same for Index2Word mapping\n        for namespace, index2token in original_vocab._index_to_token.items():\n            for index, _ in index2token.items():\n                vocab_token = original_vocab.get_token_from_index(index, namespace)\n                extended_vocab_token = extended_vocab.get_token_from_index(index, namespace)\n                assert vocab_token == extended_vocab_token\n\n        # Manual Print Check\n        # original_vocab._token_to_index :>\n        # {\n        #   ""tokens0"": {""@@PADDING@@"":0,""@@UNKNOWN@@"":1,""apple"":2,""bat"":3,""cat"":4},\n        #   ""tokens1"": {""apple"": 0,""bat"":1,""cat"":2},\n        #   ""tokens2"": {""@@PADDING@@"":0,""@@UNKNOWN@@"":1,""a"":2,""b"":3,""c"": 4},\n        #   ""tokens3"": {""p"":0,""q"":1}\n        # }\n        # extended_vocab._token_to_index :>\n        # {\n        #   ""tokens0"": {""@@PADDING@@"": 0,""@@UNKNOWN@@"": 1,\n        #               ""apple"": 2,""bat"": 3,""cat"": 4,""an"": 5,""banana"": 6,""atom"": 7},\n        #   ""tokens1"": {""apple"": 0,""bat"": 1,""cat"": 2,""an"": 3,""banana"": 4,""atom"": 5},\n        #   ""tokens2"": {""@@PADDING@@"": 0,""@@UNKNOWN@@"": 1,""a"": 2,""b"": 3,""c"": 4},\n        #   ""tokens3"": {""p"": 0,""q"": 1},\n        #   ""tokens4"": {""@@PADDING@@"": 0,""@@UNKNOWN@@"": 1,""l"": 2,""m"": 3,""n"": 4,""o"": 5},\n        #   ""tokens5"": {""x"": 0,""y"": 1,""z"": 2}\n        # }\n\n    def test_vocab_can_print(self):\n        vocab = Vocabulary(non_padded_namespaces=[""a"", ""c""])\n        vocab.add_tokens_to_namespace([""a0"", ""a1"", ""a2""], namespace=""a"")\n        vocab.add_tokens_to_namespace([""b2"", ""b3""], namespace=""b"")\n        print(vocab)\n\n    def test_read_pretrained_words(self):\n        # The fixture ""fake_embeddings.5d.txt"" was generated using the words in this random quote\n        words = set(\n            ""If you think you are too small to make a difference ""\n            ""try to sleeping with a mosquito \xc3\xa0\xc3\xa8\xc3\xac\xc3\xb2\xc3\xb9"".split("" "")\n        )\n\n        # Reading from a single (compressed) file or a single-file archive\n        base_path = str(self.FIXTURES_ROOT / ""embeddings/fake_embeddings.5d.txt"")\n        for ext in ["""", "".gz"", "".lzma"", "".bz2"", "".zip"", "".tar.gz""]:\n            file_path = base_path + ext\n            words_read = set(_read_pretrained_tokens(file_path))\n            assert words_read == words, (\n                f""Wrong words for file {file_path}\\n""\n                f""   Read: {sorted(words_read)}\\n""\n                f""Correct: {sorted(words)}""\n            )\n\n        # Reading from a multi-file archive\n        base_path = str(self.FIXTURES_ROOT / ""embeddings/multi-file-archive"")\n        file_path = ""folder/fake_embeddings.5d.txt""\n        for ext in ["".zip"", "".tar.gz""]:\n            archive_path = base_path + ext\n            embeddings_file_uri = format_embeddings_file_uri(archive_path, file_path)\n            words_read = set(_read_pretrained_tokens(embeddings_file_uri))\n            assert words_read == words, (\n                f""Wrong words for file {archive_path}\\n""\n                f""   Read: {sorted(words_read)}\\n""\n                f""Correct: {sorted(words)}""\n            )\n\n    def test_from_instances_exclusive_embeddings_file_inside_archive(self):\n        """""" Just for ensuring there are no problems when reading pretrained tokens from an archive """"""\n        # Read embeddings file from archive\n        archive_path = str(self.TEST_DIR / ""embeddings-archive.zip"")\n\n        with zipfile.ZipFile(archive_path, ""w"") as archive:\n            file_path = ""embedding.3d.vec""\n            with archive.open(file_path, ""w"") as embeddings_file:\n                embeddings_file.write(""a 1.0 2.3 -1.0\\n"".encode(""utf-8""))\n                embeddings_file.write(""b 0.1 0.4 -4.0\\n"".encode(""utf-8""))\n\n            with archive.open(""dummy.vec"", ""w"") as dummy_file:\n                dummy_file.write(""c 1.0 2.3 -1.0 3.0\\n"".encode(""utf-8""))\n\n        embeddings_file_uri = format_embeddings_file_uri(archive_path, file_path)\n        vocab = Vocabulary.from_instances(\n            self.dataset,\n            min_count={""tokens"": 4},\n            pretrained_files={""tokens"": embeddings_file_uri},\n            only_include_pretrained_words=True,\n        )\n\n        words = set(vocab.get_index_to_token_vocabulary().values())\n        assert ""a"" in words\n        assert ""b"" not in words\n        assert ""c"" not in words\n\n        vocab = Vocabulary.from_instances(\n            self.dataset,\n            pretrained_files={""tokens"": embeddings_file_uri},\n            only_include_pretrained_words=True,\n        )\n        words = set(vocab.get_index_to_token_vocabulary().values())\n        assert ""a"" in words\n        assert ""b"" in words\n        assert ""c"" not in words\n\n    def test_registrability(self):\n        @Vocabulary.register(""my-vocabulary"", constructor=""constructor"")\n        class MyVocabulary(Vocabulary):\n            @classmethod\n            def constructor(cls):\n                return MyVocabulary()\n\n        params = Params({""type"": ""my-vocabulary""})\n\n        instance = Instance(fields={})\n\n        vocab = Vocabulary.from_params(params=params, instances=[instance])\n\n        assert isinstance(vocab, MyVocabulary)\n\n    def test_max_vocab_size_dict(self):\n        params = Params({""max_vocab_size"": {""tokens"": 1, ""characters"": 20}})\n\n        vocab = Vocabulary.from_params(params=params, instances=self.dataset)\n        words = vocab.get_index_to_token_vocabulary().values()\n        # Additional 2 tokens are \'@@PADDING@@\' and \'@@UNKNOWN@@\' by default\n        assert len(words) == 3\n\n    def test_max_vocab_size_partial_dict(self):\n        indexers = {\n            ""tokens"": SingleIdTokenIndexer(),\n            ""token_characters"": TokenCharactersIndexer(min_padding_length=3),\n        }\n        instance = Instance(\n            {\n                ""text"": TextField(\n                    [Token(w) for w in ""Abc def ghi jkl mno pqr stu vwx yz"".split("" "")], indexers\n                )\n            }\n        )\n        dataset = Batch([instance])\n        params = Params({""max_vocab_size"": {""tokens"": 1}})\n\n        vocab = Vocabulary.from_params(params=params, instances=dataset)\n        assert len(vocab.get_index_to_token_vocabulary(""tokens"").values()) == 3  # 1 + 2\n        assert len(vocab.get_index_to_token_vocabulary(""token_characters"").values()) == 28  # 26 + 2\n\n    def test_min_pretrained_embeddings(self):\n        params = Params(\n            {\n                ""pretrained_files"": {\n                    ""tokens"": str(self.FIXTURES_ROOT / ""embeddings/glove.6B.100d.sample.txt.gz"")\n                },\n                ""min_pretrained_embeddings"": {""tokens"": 50},\n            }\n        )\n\n        vocab = Vocabulary.from_params(params=params, instances=self.dataset)\n        assert vocab.get_vocab_size() >= 50\n        assert vocab.get_token_index(""his"") > 1  # not @@UNKNOWN@@\n\n    def test_custom_padding_oov_tokens(self):\n        vocab = Vocabulary(oov_token=""[UNK]"")\n        assert vocab._oov_token == ""[UNK]""\n        assert vocab._padding_token == ""@@PADDING@@""\n\n        vocab = Vocabulary(padding_token=""[PAD]"")\n        assert vocab._oov_token == ""@@UNKNOWN@@""\n        assert vocab._padding_token == ""[PAD]""\n\n        vocab_dir = self.TEST_DIR / ""vocab_save""\n        vocab = Vocabulary(oov_token=""<UNK>"")\n        vocab.add_tokens_to_namespace([""a0"", ""a1"", ""a2""], namespace=""a"")\n        vocab.save_to_files(vocab_dir)\n\n        params = Params({""type"": ""from_files"", ""directory"": vocab_dir, ""oov_token"": ""<UNK>""})\n        vocab = Vocabulary.from_params(params)\n\n        with pytest.raises(AssertionError) as excinfo:\n            vocab = Vocabulary.from_params(Params({""type"": ""from_files"", ""directory"": vocab_dir}))\n\n        assert ""OOV token not found!"" in str(excinfo.value)\n\n    def test_extend_from_vocab(self):\n        vocab1 = Vocabulary(non_padded_namespaces={""1"", ""2""})\n        vocab2 = Vocabulary(non_padded_namespaces={""3""})\n\n        vocab1.add_tokens_to_namespace([""a"", ""b"", ""c""], namespace=""1"")\n        vocab1.add_tokens_to_namespace([""d"", ""e"", ""f""], namespace=""2"")\n\n        vocab2.add_tokens_to_namespace([""c"", ""d"", ""e""], namespace=""1"")\n        vocab2.add_tokens_to_namespace([""g"", ""h"", ""i""], namespace=""3"")\n\n        vocab1.extend_from_vocab(vocab2)\n        assert vocab1.get_namespaces() == {""1"", ""2"", ""3""}\n        assert vocab1._non_padded_namespaces == {""1"", ""2"", ""3""}\n        assert vocab1.get_token_to_index_vocabulary(""1"") == {\n            ""a"": 0,\n            ""b"": 1,\n            ""c"": 2,\n            ""@@PADDING@@"": 3,\n            ""@@UNKNOWN@@"": 4,\n            ""d"": 5,\n            ""e"": 6,\n        }\n        assert vocab1.get_token_to_index_vocabulary(""2"") == {""d"": 0, ""e"": 1, ""f"": 2}\n        assert vocab1.get_token_to_index_vocabulary(""3"") == {""g"": 0, ""h"": 1, ""i"": 2}\n'"
tests/interpret/__init__.py,0,b''
tests/interpret/hotflip_test.py,0,"b'from allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.token_indexers import TokenCharactersIndexer\nfrom allennlp.interpret.attackers import Hotflip\nfrom allennlp.models.archival import load_archive\nfrom allennlp.modules.token_embedders import EmptyEmbedder\nfrom allennlp.predictors import Predictor\n\n\nclass TestHotflip(AllenNlpTestCase):\n    def test_hotflip(self):\n        inputs = {""sentence"": ""I always write unit tests for my code.""}\n\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive)\n\n        hotflipper = Hotflip(predictor)\n        hotflipper.initialize()\n        attack = hotflipper.attack_from_json(inputs, ""tokens"", ""grad_input_1"")\n        assert attack is not None\n        assert ""final"" in attack\n        assert ""original"" in attack\n        assert ""outputs"" in attack\n        assert len(attack[""final""][0]) == len(\n            attack[""original""]\n        )  # hotflip replaces words without removing\n\n    def test_with_token_characters_indexer(self):\n\n        inputs = {""sentence"": ""I always write unit tests for my code.""}\n\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive)\n        predictor._dataset_reader._token_indexers[""chars""] = TokenCharactersIndexer(\n            min_padding_length=1\n        )\n        predictor._model._text_field_embedder._token_embedders[""chars""] = EmptyEmbedder()\n\n        hotflipper = Hotflip(predictor)\n        hotflipper.initialize()\n        attack = hotflipper.attack_from_json(inputs, ""tokens"", ""grad_input_1"")\n        assert attack is not None\n        assert ""final"" in attack\n        assert ""original"" in attack\n        assert ""outputs"" in attack\n        assert len(attack[""final""][0]) == len(\n            attack[""original""]\n        )  # hotflip replaces words without removing\n'"
tests/interpret/input_reduction_test.py,0,"b'from allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors import Predictor\nfrom allennlp.interpret.attackers import InputReduction\n\n\nclass TestInputReduction(AllenNlpTestCase):\n    def test_input_reduction(self):\n        # test using classification model\n        inputs = {""sentence"": ""I always write unit tests for my code.""}\n\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive)\n\n        reducer = InputReduction(predictor)\n        reduced = reducer.attack_from_json(inputs, ""tokens"", ""grad_input_1"")\n        assert reduced is not None\n        assert ""final"" in reduced\n        assert ""original"" in reduced\n        assert reduced[""final""][0]  # always at least one token\n        assert len(reduced[""final""][0]) <= len(\n            reduced[""original""]\n        )  # input reduction removes tokens\n        for word in reduced[""final""][0]:  # no new words entered\n            assert word in reduced[""original""]\n\n        # test using NER model (tests different underlying logic)\n        inputs = {""sentence"": ""Eric Wallace was an intern at AI2""}\n\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""simple_tagger"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive, ""sentence_tagger"")\n\n        reducer = InputReduction(predictor)\n        reduced = reducer.attack_from_json(inputs, ""tokens"", ""grad_input_1"")\n        assert reduced is not None\n        assert ""final"" in reduced\n        assert ""original"" in reduced\n        for reduced_input in reduced[""final""]:\n            assert reduced_input  # always at least one token\n            assert len(reduced_input) <= len(reduced[""original""])  # input reduction removes tokens\n            for word in reduced_input:  # no new words entered\n                assert word in reduced[""original""]\n'"
tests/interpret/integrated_gradient_test.py,0,"b'from pytest import approx\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors import Predictor\nfrom allennlp.interpret.saliency_interpreters import IntegratedGradient\n\n\nclass TestIntegratedGradient(AllenNlpTestCase):\n    def test_integrated_gradient(self):\n        inputs = {""sentence"": ""It was the ending that I hated""}\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive, ""text_classifier"")\n\n        interpreter = IntegratedGradient(predictor)\n        interpretation = interpreter.saliency_interpret_from_json(inputs)\n        assert interpretation is not None\n        assert ""instance_1"" in interpretation\n        assert ""grad_input_1"" in interpretation[""instance_1""]\n        grad_input_1 = interpretation[""instance_1""][""grad_input_1""]\n        assert len(grad_input_1) == 7  # 7 words in input\n\n        # two interpretations should be identical for integrated gradients\n        repeat_interpretation = interpreter.saliency_interpret_from_json(inputs)\n        repeat_grad_input_1 = repeat_interpretation[""instance_1""][""grad_input_1""]\n        for grad, repeat_grad in zip(grad_input_1, repeat_grad_input_1):\n            assert grad == approx(repeat_grad)\n'"
tests/interpret/simple_gradient_test.py,0,"b'from pytest import approx\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.interpret.saliency_interpreters import SimpleGradient\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors import Predictor\n\n\nclass TestSimpleGradient(AllenNlpTestCase):\n    def test_simple_gradient_basic_text(self):\n        inputs = {""sentence"": ""It was the ending that I hated""}\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive, ""text_classifier"")\n\n        interpreter = SimpleGradient(predictor)\n        interpretation = interpreter.saliency_interpret_from_json(inputs)\n        assert interpretation is not None\n        assert ""instance_1"" in interpretation\n        assert ""grad_input_1"" in interpretation[""instance_1""]\n        grad_input_1 = interpretation[""instance_1""][""grad_input_1""]\n        assert len(grad_input_1) == 7  # 7 words in input\n\n        # two interpretations should be identical for gradient\n        repeat_interpretation = interpreter.saliency_interpret_from_json(inputs)\n        repeat_grad_input_1 = repeat_interpretation[""instance_1""][""grad_input_1""]\n        for grad, repeat_grad in zip(grad_input_1, repeat_grad_input_1):\n            assert grad == approx(repeat_grad)\n'"
tests/interpret/smooth_gradient_test.py,0,"b'from allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors import Predictor\nfrom allennlp.interpret.saliency_interpreters import SmoothGradient\n\n\nclass TestSmoothGradient(AllenNlpTestCase):\n    def test_smooth_gradient(self):\n        inputs = {""sentence"": ""It was the ending that I hated""}\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive, ""text_classifier"")\n\n        interpreter = SmoothGradient(predictor)\n        interpretation = interpreter.saliency_interpret_from_json(inputs)\n        assert interpretation is not None\n        assert ""instance_1"" in interpretation\n        assert ""grad_input_1"" in interpretation[""instance_1""]\n        assert len(interpretation[""instance_1""][""grad_input_1""]) == 7  # 7 words in input\n'"
tests/models/__init__.py,0,b''
tests/models/archival_test.py,1,"b'import copy\n\nimport torch\n\nfrom allennlp.commands.train import train_model\nfrom allennlp.common import Params\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.models.archival import archive_model, load_archive\n\n\ndef assert_models_equal(model, model2):\n    # check that model weights are the same\n    keys = set(model.state_dict().keys())\n    keys2 = set(model2.state_dict().keys())\n\n    assert keys == keys2\n\n    for key in keys:\n        assert torch.equal(model.state_dict()[key], model2.state_dict()[key])\n\n    # check that vocabularies are the same\n    vocab = model.vocab\n    vocab2 = model2.vocab\n\n    assert vocab._token_to_index == vocab2._token_to_index\n    assert vocab._index_to_token == vocab2._index_to_token\n\n\nclass ArchivalTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n\n        self.params = Params(\n            {\n                ""model"": {\n                    ""type"": ""simple_tagger"",\n                    ""text_field_embedder"": {\n                        ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                    },\n                    ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n                },\n                ""dataset_reader"": {""type"": ""sequence_tagging""},\n                ""train_data_path"": str(self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""),\n                ""validation_data_path"": str(self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""),\n                ""data_loader"": {""batch_size"": 2},\n                ""trainer"": {""num_epochs"": 2, ""optimizer"": ""adam""},\n            }\n        )\n\n    def test_archiving(self):\n        # copy params, since they\'ll get consumed during training\n        params_copy = copy.deepcopy(self.params.as_dict())\n\n        # `train_model` should create an archive\n        serialization_dir = self.TEST_DIR / ""archive_test""\n        model = train_model(self.params, serialization_dir=serialization_dir)\n\n        archive_path = serialization_dir / ""model.tar.gz""\n\n        # load from the archive\n        archive = load_archive(archive_path)\n        model2 = archive.model\n\n        assert_models_equal(model, model2)\n\n        # check that params are the same\n        params2 = archive.config\n        assert params2.as_dict() == params_copy\n\n    def test_archive_model_uses_archive_path(self):\n\n        serialization_dir = self.TEST_DIR / ""serialization""\n        # Train a model\n        train_model(self.params, serialization_dir=serialization_dir)\n        # Use a new path.\n        archive_model(\n            serialization_dir=serialization_dir, archive_path=serialization_dir / ""new_path.tar.gz""\n        )\n        archive = load_archive(serialization_dir / ""new_path.tar.gz"")\n        assert archive\n\n    def test_loading_serialization_directory(self):\n        # copy params, since they\'ll get consumed during training\n        params_copy = copy.deepcopy(self.params.as_dict())\n\n        # `train_model` should create an archive\n        serialization_dir = self.TEST_DIR / ""serialization""\n        model = train_model(self.params, serialization_dir=serialization_dir)\n\n        # load from the serialization directory itself\n        archive = load_archive(serialization_dir)\n        model2 = archive.model\n\n        assert_models_equal(model, model2)\n\n        # check that params are the same\n        params2 = archive.config\n        assert params2.as_dict() == params_copy\n\n    def test_can_load_from_archive_model(self):\n        serialization_dir = self.FIXTURES_ROOT / ""basic_classifier"" / ""from_archive_serialization""\n        archive_path = serialization_dir / ""model.tar.gz""\n        model = load_archive(archive_path).model\n\n        # We want to be sure that we don\'t just not crash, but also be sure that we loaded the right\n        # weights for the model.  We\'ll do that by making sure that we didn\'t just load the model\n        # that\'s in the `archive_path` of the config file, which is this one.\n        base_model_path = self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        base_model = load_archive(base_model_path).model\n        base_model_params = dict(base_model.named_parameters())\n        for name, parameters in model.named_parameters():\n            if parameters.size() == base_model_params[name].size():\n                assert not (parameters == base_model_params[name]).all()\n            else:\n                # In this case, the parameters are definitely different, no need for the above\n                # check.\n                pass\n'"
tests/models/basic_classifier_test.py,0,"b'import numpy\n\nfrom allennlp.common.testing import ModelTestCase\n\n\nclass TestBasicClassifier(ModelTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.set_up_model(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""experiment_seq2vec.jsonnet"",\n            self.FIXTURES_ROOT / ""data"" / ""text_classification_json"" / ""imdb_corpus.jsonl"",\n        )\n\n    def test_forward_pass_runs_correctly(self):\n        training_tensors = self.dataset.as_tensor_dict()\n        output_dict = self.model(**training_tensors)\n        output_dict = self.model.make_output_human_readable(output_dict)\n        assert ""label"" in output_dict.keys()\n        probs = output_dict[""probs""][0].data.numpy()\n        numpy.testing.assert_almost_equal(numpy.sum(probs, -1), numpy.array([1]))\n\n    def test_seq2vec_clf_can_train_save_and_load(self):\n        self.set_up_model(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""experiment_seq2vec.jsonnet"",\n            self.FIXTURES_ROOT / ""data"" / ""text_classification_json"" / ""imdb_corpus.jsonl"",\n        )\n        self.ensure_model_can_train_save_and_load(self.param_file)\n\n    def test_seq2seq_clf_can_train_save_and_load(self):\n        self.set_up_model(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""experiment_seq2seq.jsonnet"",\n            self.FIXTURES_ROOT / ""data"" / ""text_classification_json"" / ""imdb_corpus.jsonl"",\n        )\n        self.ensure_model_can_train_save_and_load(self.param_file)\n'"
tests/models/model_test.py,1,"b'import torch\n\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.models import load_archive\n\n\nclass TestModel(AllenNlpTestCase):\n    def test_extend_embedder_vocab(self):\n        model_archive = str(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        trained_model = load_archive(model_archive).model\n\n        original_weight = trained_model._text_field_embedder.token_embedder_tokens.weight\n        assert tuple(original_weight.shape) == (213, 10)\n\n        counter = {""tokens"": {""unawarded"": 1}}\n        trained_model.vocab._extend(counter)\n        trained_model.extend_embedder_vocab()\n\n        extended_weight = trained_model._text_field_embedder.token_embedder_tokens.weight\n        assert tuple(extended_weight.shape) == (214, 10)\n\n        assert torch.all(original_weight == extended_weight[:213, :])\n'"
tests/models/simple_tagger_test.py,2,"b'from flaky import flaky\nimport numpy\nimport pytest\nimport torch\n\nfrom allennlp.common.testing import ModelTestCase\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.params import Params\nfrom allennlp.data.dataset_readers import DatasetReader\nfrom allennlp.data import DataLoader\nfrom allennlp.models import Model\nfrom allennlp.training import GradientDescentTrainer, Trainer\n\n\nclass SimpleTaggerTest(ModelTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.set_up_model(\n            self.FIXTURES_ROOT / ""simple_tagger"" / ""experiment.json"",\n            self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv"",\n        )\n\n    def test_simple_tagger_can_train_save_and_load(self):\n        self.ensure_model_can_train_save_and_load(self.param_file)\n\n    @flaky\n    def test_batch_predictions_are_consistent(self):\n        self.ensure_batch_predictions_are_consistent()\n\n    def test_forward_pass_runs_correctly(self):\n        training_tensors = self.dataset.as_tensor_dict()\n        output_dict = self.model(**training_tensors)\n        output_dict = self.model.make_output_human_readable(output_dict)\n        class_probs = output_dict[""class_probabilities""][0].data.numpy()\n        numpy.testing.assert_almost_equal(numpy.sum(class_probs, -1), numpy.array([1, 1, 1, 1]))\n\n    def test_forward_on_instances_ignores_loss_key_when_batched(self):\n        batch_outputs = self.model.forward_on_instances(self.dataset.instances)\n        for output in batch_outputs:\n            assert ""loss"" not in output.keys()\n\n        # It should be in the single batch case, because we special case it.\n        single_output = self.model.forward_on_instance(self.dataset.instances[0])\n        assert ""loss"" in single_output.keys()\n\n    def test_mismatching_dimensions_throws_configuration_error(self):\n        params = Params.from_file(self.param_file)\n        # Make the encoder wrong - it should be 2 to match\n        # the embedding dimension from the text_field_embedder.\n        params[""model""][""encoder""][""input_size""] = 10\n        with pytest.raises(ConfigurationError):\n            Model.from_params(vocab=self.vocab, params=params.pop(""model""))\n\n    def test_regularization(self):\n        penalty = self.model.get_regularization_penalty()\n        assert penalty == 0\n\n        data_loader = DataLoader(self.instances, batch_size=32)\n        trainer = GradientDescentTrainer(self.model, None, data_loader)  # optimizer,\n\n        # You get a RuntimeError if you call `model.forward` twice on the same inputs.\n        # The data and config are such that the whole dataset is one batch.\n        training_batch = next(iter(data_loader))\n        validation_batch = next(iter(data_loader))\n\n        training_loss = trainer.batch_outputs(training_batch, for_training=True)[""loss""].item()\n        validation_loss = trainer.batch_outputs(validation_batch, for_training=False)[""loss""].item()\n\n        # Training loss should have the regularization penalty, but validation loss should not.\n        numpy.testing.assert_almost_equal(training_loss, validation_loss)\n\n\nclass SimpleTaggerSpanF1Test(ModelTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.set_up_model(\n            self.FIXTURES_ROOT / ""simple_tagger_with_span_f1"" / ""experiment.json"",\n            self.FIXTURES_ROOT / ""data"" / ""conll2003.txt"",\n        )\n\n    def test_simple_tagger_can_train_save_and_load(self):\n        self.ensure_model_can_train_save_and_load(self.param_file)\n\n    @flaky\n    def test_batch_predictions_are_consistent(self):\n        self.ensure_batch_predictions_are_consistent()\n\n    def test_simple_tagger_can_enable_span_f1(self):\n        assert self.model.calculate_span_f1 and self.model._f1_metric is not None\n\n\nclass SimpleTaggerRegularizationTest(ModelTestCase):\n    def setup_method(self):\n        super().setup_method()\n        param_file = self.FIXTURES_ROOT / ""simple_tagger"" / ""experiment_with_regularization.json""\n        self.set_up_model(param_file, self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv"")\n        params = Params.from_file(param_file)\n        self.reader = DatasetReader.from_params(params[""dataset_reader""])\n        self.data_loader = DataLoader.from_params(\n            dataset=self.instances, params=params[""data_loader""]\n        )\n        self.trainer = Trainer.from_params(\n            model=self.model,\n            data_loader=self.data_loader,\n            serialization_dir=self.TEST_DIR,\n            params=params.get(""trainer""),\n        )\n\n    def test_regularization(self):\n        penalty = self.model.get_regularization_penalty().data\n        assert (penalty > 0).all()\n\n        penalty2 = 0\n\n        # Config specifies penalty as\n        #   ""regularizer"": [\n        #     [""weight$"", {""type"": ""l2"", ""alpha"": 10}],\n        #     [""bias$"", {""type"": ""l1"", ""alpha"": 5}]\n        #   ]\n        for name, parameter in self.model.named_parameters():\n            if name.endswith(""weight""):\n                weight_penalty = 10 * torch.sum(torch.pow(parameter, 2))\n                penalty2 += weight_penalty\n            elif name.endswith(""bias""):\n                bias_penalty = 5 * torch.sum(torch.abs(parameter))\n                penalty2 += bias_penalty\n\n        assert (penalty == penalty2.data).all()\n\n        # You get a RuntimeError if you call `model.forward` twice on the same inputs.\n        # The data and config are such that the whole dataset is one batch.\n        training_batch = next(iter(self.data_loader))\n        validation_batch = next(iter(self.data_loader))\n\n        training_batch_outputs = self.trainer.batch_outputs(training_batch, for_training=True)\n        training_loss = training_batch_outputs[""loss""].data\n\n        assert (penalty == training_batch_outputs[""reg_loss""]).all()\n\n        validation_loss = self.trainer.batch_outputs(validation_batch, for_training=False)[\n            ""loss""\n        ].data\n\n        # Training loss should have the regularization penalty, but validation loss should not.\n        assert (training_loss != validation_loss).all()\n\n        # Training loss should equal the validation loss plus the penalty.\n        penalized = validation_loss + penalty\n        assert (training_loss == penalized).all()\n'"
tests/models/test_model_test_case.py,0,"b'import json\nimport pytest\n\nfrom allennlp.common.testing import ModelTestCase\n\n\nclass ModelWithIncorrectValidationMetricTest(ModelTestCase):\n    """"""\n    This test case checks some validating functionality that is implemented\n    in `ensure_model_can_train_save_and_load`\n    """"""\n\n    def setup_method(self):\n        super().setup_method()\n        self.set_up_model(\n            self.FIXTURES_ROOT / ""simple_tagger"" / ""model_test_case.jsonnet"",\n            self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv"",\n        )\n\n    def test_01_test_validation_metric_does_not_exist(self):\n        overrides = {""trainer.num_epochs"": 2}\n        pytest.raises(\n            AssertionError,\n            self.ensure_model_can_train_save_and_load,\n            self.param_file,\n            metric_to_check=""non_existent_metric"",\n            metric_terminal_value=0.0,\n            overrides=json.dumps(overrides),\n        )\n\n    def test_02a_test_validation_metric_terminal_value_not_set(self):\n        pytest.raises(\n            AssertionError,\n            self.ensure_model_can_train_save_and_load,\n            self.param_file,\n            metric_to_check=""accuracy"",\n            metric_terminal_value=None,\n        )\n\n    def test_02b_test_validation_metric_terminal_value_not_met(self):\n        pytest.raises(\n            AssertionError,\n            self.ensure_model_can_train_save_and_load,\n            self.param_file,\n            metric_to_check=""accuracy"",\n            metric_terminal_value=0.0,\n        )\n\n    def test_03_test_validation_metric_exists_and_its_terminal_value_is_met(self):\n        self.ensure_model_can_train_save_and_load(\n            self.param_file, metric_to_check=""accuracy"", metric_terminal_value=1.0,\n        )\n'"
tests/modules/augmented_lstm_test.py,11,"b'import pytest\nimport numpy\nimport torch\nimport torch.nn.init\nfrom torch.nn.modules.rnn import LSTM\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.params import Params\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules.augmented_lstm import AugmentedLstm, AugmentedLSTMCell, BiAugmentedLstm\nfrom allennlp.nn import InitializerApplicator, Initializer\nfrom allennlp.nn.util import sort_batch_by_length\n\n\nclass TestAugmentedLSTM(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        tensor = torch.rand([5, 7, 10])\n        tensor[0, 3:, :] = 0\n        tensor[1, 4:, :] = 0\n        tensor[2, 2:, :] = 0\n        tensor[3, 6:, :] = 0\n        sequence_lengths = torch.LongTensor([3, 4, 2, 6, 7])\n        self.random_tensor = tensor\n        self.sequence_lengths = sequence_lengths\n\n    def test_variable_length_sequences_return_correctly_padded_outputs(self):\n        sorted_tensor, sorted_sequence, _, _ = sort_batch_by_length(\n            self.random_tensor, self.sequence_lengths\n        )\n        tensor = pack_padded_sequence(\n            sorted_tensor, sorted_sequence.data.tolist(), batch_first=True\n        )\n        lstm = AugmentedLstm(10, 11)\n        output, _ = lstm(tensor)\n        output_sequence, _ = pad_packed_sequence(output, batch_first=True)\n\n        numpy.testing.assert_array_equal(output_sequence.data[1, 6:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[2, 4:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[3, 3:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[4, 2:, :].numpy(), 0.0)\n\n    def test_variable_length_sequences_run_backward_return_correctly_padded_outputs(self):\n        sorted_tensor, sorted_sequence, _, _ = sort_batch_by_length(\n            self.random_tensor, self.sequence_lengths\n        )\n        tensor = pack_padded_sequence(\n            sorted_tensor, sorted_sequence.data.tolist(), batch_first=True\n        )\n        lstm = AugmentedLstm(10, 11, go_forward=False)\n        output, _ = lstm(tensor)\n        output_sequence, _ = pad_packed_sequence(output, batch_first=True)\n\n        numpy.testing.assert_array_equal(output_sequence.data[1, 6:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[2, 4:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[3, 3:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[4, 2:, :].numpy(), 0.0)\n\n    def test_augmented_lstm_computes_same_function_as_pytorch_lstm(self):\n        augmented_lstm = AugmentedLstm(10, 11)\n        pytorch_lstm = LSTM(10, 11, num_layers=1, batch_first=True)\n        # Initialize all weights to be == 1.\n        constant_init = Initializer.from_params(Params({""type"": ""constant"", ""val"": 1.0}))\n        initializer = InitializerApplicator([("".*"", constant_init)])\n        initializer(augmented_lstm)\n        initializer(pytorch_lstm)\n\n        initial_state = torch.zeros([1, 5, 11])\n        initial_memory = torch.zeros([1, 5, 11])\n\n        # Use bigger numbers to avoid floating point instability.\n        sorted_tensor, sorted_sequence, _, _ = sort_batch_by_length(\n            self.random_tensor * 5.0, self.sequence_lengths\n        )\n        lstm_input = pack_padded_sequence(\n            sorted_tensor, sorted_sequence.data.tolist(), batch_first=True\n        )\n\n        augmented_output, augmented_state = augmented_lstm(\n            lstm_input, (initial_state, initial_memory)\n        )\n        pytorch_output, pytorch_state = pytorch_lstm(lstm_input, (initial_state, initial_memory))\n        pytorch_output_sequence, _ = pad_packed_sequence(pytorch_output, batch_first=True)\n        augmented_output_sequence, _ = pad_packed_sequence(augmented_output, batch_first=True)\n\n        numpy.testing.assert_array_almost_equal(\n            pytorch_output_sequence.data.numpy(), augmented_output_sequence.data.numpy(), decimal=4\n        )\n        numpy.testing.assert_array_almost_equal(\n            pytorch_state[0].data.numpy(), augmented_state[0].data.numpy(), decimal=4\n        )\n        numpy.testing.assert_array_almost_equal(\n            pytorch_state[1].data.numpy(), augmented_state[1].data.numpy(), decimal=4\n        )\n\n    def test_augmented_lstm_works_with_highway_connections(self):\n        augmented_lstm = AugmentedLstm(10, 11, use_highway=True)\n        sorted_tensor, sorted_sequence, _, _ = sort_batch_by_length(\n            self.random_tensor, self.sequence_lengths\n        )\n        lstm_input = pack_padded_sequence(\n            sorted_tensor, sorted_sequence.data.tolist(), batch_first=True\n        )\n        augmented_lstm(lstm_input)\n\n    def test_augmented_lstm_throws_error_on_non_packed_sequence_input(self):\n        lstm = AugmentedLstm(3, 5)\n        tensor = torch.rand([5, 7, 9])\n        with pytest.raises(ConfigurationError):\n            lstm(tensor)\n\n    def test_augmented_lstm_is_initialized_with_correct_biases(self):\n        lstm = AugmentedLSTMCell(2, 3)\n        true_state_bias = numpy.array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n        numpy.testing.assert_array_equal(lstm.state_linearity.bias.data.numpy(), true_state_bias)\n\n        # Non-highway case.\n        lstm = AugmentedLSTMCell(2, 3, use_highway=False)\n        true_state_bias = numpy.array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n        numpy.testing.assert_array_equal(lstm.state_linearity.bias.data.numpy(), true_state_bias)\n\n    def test_dropout_is_not_applied_to_output_or_returned_hidden_states(self):\n        sorted_tensor, sorted_sequence, _, _ = sort_batch_by_length(\n            self.random_tensor, self.sequence_lengths\n        )\n        tensor = pack_padded_sequence(\n            sorted_tensor, sorted_sequence.data.tolist(), batch_first=True\n        )\n        lstm = AugmentedLstm(10, 11, recurrent_dropout_probability=0.5)\n\n        output, (hidden_state, _) = lstm(tensor)\n        output_sequence, _ = pad_packed_sequence(output, batch_first=True)\n        # Test returned output sequence\n        num_hidden_dims_zero_across_timesteps = ((output_sequence.sum(1) == 0).sum()).item()\n        # If this is not True then dropout has been applied to the output of the LSTM\n        assert not num_hidden_dims_zero_across_timesteps\n        # Should not have dropout applied to the last hidden state as this is not used\n        # within the LSTM and makes it more consistent with the `torch.nn.LSTM` where\n        # dropout is not applied to any of it\'s output. This would also make it more\n        # consistent with the Keras LSTM implementation as well.\n        hidden_state = hidden_state.squeeze()\n        num_hidden_dims_zero_across_timesteps = ((hidden_state == 0).sum()).item()\n        assert not num_hidden_dims_zero_across_timesteps\n\n    def test_dropout_version_is_different_to_no_dropout(self):\n        augmented_lstm = AugmentedLstm(10, 11)\n        dropped_augmented_lstm = AugmentedLstm(10, 11, recurrent_dropout_probability=0.9)\n        # Initialize all weights to be == 1.\n        constant_init = Initializer.from_params(Params({""type"": ""constant"", ""val"": 0.5}))\n        initializer = InitializerApplicator([("".*"", constant_init)])\n        initializer(augmented_lstm)\n        initializer(dropped_augmented_lstm)\n\n        initial_state = torch.randn([1, 5, 11])\n        initial_memory = torch.randn([1, 5, 11])\n\n        # If we use too bigger number like in the PyTorch test the dropout has no affect\n        sorted_tensor, sorted_sequence, _, _ = sort_batch_by_length(\n            self.random_tensor, self.sequence_lengths\n        )\n        lstm_input = pack_padded_sequence(\n            sorted_tensor, sorted_sequence.data.tolist(), batch_first=True\n        )\n\n        augmented_output, augmented_state = augmented_lstm(\n            lstm_input, (initial_state, initial_memory)\n        )\n        dropped_output, dropped_state = dropped_augmented_lstm(\n            lstm_input, (initial_state, initial_memory)\n        )\n        dropped_output_sequence, _ = pad_packed_sequence(dropped_output, batch_first=True)\n        augmented_output_sequence, _ = pad_packed_sequence(augmented_output, batch_first=True)\n        with pytest.raises(AssertionError):\n            numpy.testing.assert_array_almost_equal(\n                dropped_output_sequence.data.numpy(),\n                augmented_output_sequence.data.numpy(),\n                decimal=4,\n            )\n        with pytest.raises(AssertionError):\n            numpy.testing.assert_array_almost_equal(\n                dropped_state[0].data.numpy(), augmented_state[0].data.numpy(), decimal=4\n            )\n        with pytest.raises(AssertionError):\n            numpy.testing.assert_array_almost_equal(\n                dropped_state[1].data.numpy(), augmented_state[1].data.numpy(), decimal=4\n            )\n\n    def test_biaugmented_lstm(self):\n        for bidirectional in [True, False]:\n            bi_augmented_lstm = BiAugmentedLstm(\n                10, 11, 3, recurrent_dropout_probability=0.1, bidirectional=bidirectional\n            )\n            sorted_tensor, sorted_sequence, _, _ = sort_batch_by_length(\n                self.random_tensor, self.sequence_lengths\n            )\n            lstm_input = pack_padded_sequence(\n                sorted_tensor, sorted_sequence.data.tolist(), batch_first=True\n            )\n            bi_augmented_lstm(lstm_input)\n'"
tests/modules/bimpm_matching_test.py,12,"b'import torch\n\nfrom allennlp.common import Params\nfrom allennlp.modules import BiMpmMatching\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestBiMPMMatching(AllenNlpTestCase):\n    def test_forward(self):\n        batch = 16\n        len1, len2 = 21, 24\n        seq_len1 = torch.randint(low=len1 - 10, high=len1 + 1, size=(batch,)).long()\n        seq_len2 = torch.randint(low=len2 - 10, high=len2 + 1, size=(batch,)).long()\n\n        mask1 = []\n        for w in seq_len1:\n            mask1.append([1] * w.item() + [0] * (len1 - w.item()))\n        mask1 = torch.tensor(mask1, dtype=torch.bool)\n        mask2 = []\n        for w in seq_len2:\n            mask2.append([1] * w.item() + [0] * (len2 - w.item()))\n        mask2 = torch.tensor(mask2, dtype=torch.bool)\n\n        d = 200  # hidden dimension\n        n = 20  # number of perspective\n        test1 = torch.randn(batch, len1, d)\n        test2 = torch.randn(batch, len2, d)\n        test1 = test1 * mask1.view(-1, len1, 1).expand(-1, len1, d)\n        test2 = test2 * mask2.view(-1, len2, 1).expand(-1, len2, d)\n\n        test1_fw, test1_bw = torch.split(test1, d // 2, dim=-1)\n        test2_fw, test2_bw = torch.split(test2, d // 2, dim=-1)\n\n        ml_fw = BiMpmMatching.from_params(Params({""is_forward"": True, ""num_perspectives"": n}))\n        ml_bw = BiMpmMatching.from_params(Params({""is_forward"": False, ""num_perspectives"": n}))\n\n        vecs_p_fw, vecs_h_fw = ml_fw(test1_fw, mask1, test2_fw, mask2)\n        vecs_p_bw, vecs_h_bw = ml_bw(test1_bw, mask1, test2_bw, mask2)\n        vecs_p, vecs_h = (\n            torch.cat(vecs_p_fw + vecs_p_bw, dim=2),\n            torch.cat(vecs_h_fw + vecs_h_bw, dim=2),\n        )\n\n        assert vecs_p.size() == torch.Size([batch, len1, 10 + 10 * n])\n        assert vecs_h.size() == torch.Size([batch, len2, 10 + 10 * n])\n        assert (\n            ml_fw.get_output_dim()\n            == ml_bw.get_output_dim()\n            == vecs_p.size(2) // 2\n            == vecs_h.size(2) // 2\n        )\n'"
tests/modules/conditional_random_field_test.py,18,"b'import itertools\nimport math\n\nfrom pytest import approx, raises\nimport torch\nfrom numpy.testing import assert_allclose\n\nfrom allennlp.modules import ConditionalRandomField\nfrom allennlp.modules.conditional_random_field import allowed_transitions\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestConditionalRandomField(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.logits = torch.Tensor(\n            [\n                [[0, 0, 0.5, 0.5, 0.2], [0, 0, 0.3, 0.3, 0.1], [0, 0, 0.9, 10, 1]],\n                [[0, 0, 0.2, 0.5, 0.2], [0, 0, 3, 0.3, 0.1], [0, 0, 0.9, 1, 1]],\n            ]\n        )\n        self.tags = torch.LongTensor([[2, 3, 4], [3, 2, 2]])\n\n        self.transitions = torch.Tensor(\n            [\n                [0.1, 0.2, 0.3, 0.4, 0.5],\n                [0.8, 0.3, 0.1, 0.7, 0.9],\n                [-0.3, 2.1, -5.6, 3.4, 4.0],\n                [0.2, 0.4, 0.6, -0.3, -0.4],\n                [1.0, 1.0, 1.0, 1.0, 1.0],\n            ]\n        )\n\n        self.transitions_from_start = torch.Tensor([0.1, 0.2, 0.3, 0.4, 0.6])\n        self.transitions_to_end = torch.Tensor([-0.1, -0.2, 0.3, -0.4, -0.4])\n\n        # Use the CRF Module with fixed transitions to compute the log_likelihood\n        self.crf = ConditionalRandomField(5)\n        self.crf.transitions = torch.nn.Parameter(self.transitions)\n        self.crf.start_transitions = torch.nn.Parameter(self.transitions_from_start)\n        self.crf.end_transitions = torch.nn.Parameter(self.transitions_to_end)\n\n    def score(self, logits, tags):\n        """"""\n        Computes the likelihood score for the given sequence of tags,\n        given the provided logits (and the transition weights in the CRF model)\n        """"""\n        # Start with transitions from START and to END\n        total = self.transitions_from_start[tags[0]] + self.transitions_to_end[tags[-1]]\n        # Add in all the intermediate transitions\n        for tag, next_tag in zip(tags, tags[1:]):\n            total += self.transitions[tag, next_tag]\n        # Add in the logits for the observed tags\n        for logit, tag in zip(logits, tags):\n            total += logit[tag]\n        return total\n\n    def naive_most_likely_sequence(self, logits, mask):\n        # We iterate over all possible tag sequences and use self.score\n        # to check the likelihood of each. The most likely sequence should be the\n        # same as what we get from viterbi_tags.\n        most_likely_tags = []\n        best_scores = []\n\n        for logit, mas in zip(logits, mask):\n            mask_indices = mas.nonzero().squeeze()\n            logit = torch.index_select(logit, 0, mask_indices)\n            sequence_length = logit.shape[0]\n            most_likely, most_likelihood = None, -float(""inf"")\n            for tags in itertools.product(range(5), repeat=sequence_length):\n                score = self.score(logit.data, tags)\n                if score > most_likelihood:\n                    most_likely, most_likelihood = tags, score\n            # Convert tuple to list; otherwise == complains.\n            most_likely_tags.append(list(most_likely))\n            best_scores.append(most_likelihood)\n\n        return most_likely_tags, best_scores\n\n    def test_forward_works_without_mask(self):\n        log_likelihood = self.crf(self.logits, self.tags).item()\n\n        # Now compute the log-likelihood manually\n        manual_log_likelihood = 0.0\n\n        # For each instance, manually compute the numerator\n        # (which is just the score for the logits and actual tags)\n        # and the denominator\n        # (which is the log-sum-exp of the scores for the logits across all possible tags)\n        for logits_i, tags_i in zip(self.logits, self.tags):\n            numerator = self.score(logits_i.detach(), tags_i.detach())\n            all_scores = [\n                self.score(logits_i.detach(), tags_j)\n                for tags_j in itertools.product(range(5), repeat=3)\n            ]\n            denominator = math.log(sum(math.exp(score) for score in all_scores))\n            # And include them in the manual calculation.\n            manual_log_likelihood += numerator - denominator\n\n        # The manually computed log likelihood should equal the result of crf.forward.\n        assert manual_log_likelihood.item() == approx(log_likelihood)\n\n    def test_forward_works_with_mask(self):\n        # Use a non-trivial mask\n        mask = torch.tensor([[True, True, True], [True, True, False]])\n\n        log_likelihood = self.crf(self.logits, self.tags, mask).item()\n\n        # Now compute the log-likelihood manually\n        manual_log_likelihood = 0.0\n\n        # For each instance, manually compute the numerator\n        #   (which is just the score for the logits and actual tags)\n        # and the denominator\n        #   (which is the log-sum-exp of the scores for the logits across all possible tags)\n        for logits_i, tags_i, mask_i in zip(self.logits, self.tags, mask):\n            # Find the sequence length for this input and only look at that much of each sequence.\n            sequence_length = torch.sum(mask_i.detach())\n            logits_i = logits_i.data[:sequence_length]\n            tags_i = tags_i.data[:sequence_length]\n\n            numerator = self.score(logits_i, tags_i)\n            all_scores = [\n                self.score(logits_i, tags_j)\n                for tags_j in itertools.product(range(5), repeat=sequence_length)\n            ]\n            denominator = math.log(sum(math.exp(score) for score in all_scores))\n            # And include them in the manual calculation.\n            manual_log_likelihood += numerator - denominator\n\n        # The manually computed log likelihood should equal the result of crf.forward.\n        assert manual_log_likelihood.item() == approx(log_likelihood)\n\n    def test_viterbi_tags(self):\n        mask = torch.tensor([[True, True, True], [True, False, True]])\n\n        viterbi_path = self.crf.viterbi_tags(self.logits, mask)\n\n        # Separate the tags and scores.\n        viterbi_tags = [x for x, y in viterbi_path]\n        viterbi_scores = [y for x, y in viterbi_path]\n\n        most_likely_tags, best_scores = self.naive_most_likely_sequence(self.logits, mask)\n\n        assert viterbi_tags == most_likely_tags\n        assert_allclose(viterbi_scores, best_scores, rtol=1e-5)\n\n    def test_viterbi_tags_no_mask(self):\n        viterbi_path = self.crf.viterbi_tags(self.logits)\n\n        # Separate the tags and scores.\n        viterbi_tags = [x for x, y in viterbi_path]\n        viterbi_scores = [y for x, y in viterbi_path]\n\n        mask = torch.tensor([[True, True, True], [True, True, True]])\n        most_likely_tags, best_scores = self.naive_most_likely_sequence(self.logits, mask)\n\n        assert viterbi_tags == most_likely_tags\n        assert_allclose(viterbi_scores, best_scores, rtol=1e-5)\n\n    def test_viterbi_tags_top_k(self):\n        mask = torch.tensor([[True, True, True], [True, True, False]])\n\n        best_paths = self.crf.viterbi_tags(self.logits, mask, top_k=2)\n\n        # Ensure the top path matches not passing top_k\n        top_path_and_score = [top_k_paths[0] for top_k_paths in best_paths]\n        assert top_path_and_score == self.crf.viterbi_tags(self.logits, mask)\n\n        next_path_and_score = [top_k_paths[1] for top_k_paths in best_paths]\n        next_viterbi_tags = [x for x, _ in next_path_and_score]\n\n        # Check that the next best viterbi tags are what I think they should be.\n        assert next_viterbi_tags == [[4, 2, 3], [3, 2]]\n\n    def test_constrained_viterbi_tags(self):\n        constraints = {\n            (0, 0),\n            (0, 1),\n            (1, 1),\n            (1, 2),\n            (2, 2),\n            (2, 3),\n            (3, 3),\n            (3, 4),\n            (4, 4),\n            (4, 0),\n        }\n\n        # Add the transitions to the end tag\n        # and from the start tag.\n        for i in range(5):\n            constraints.add((5, i))\n            constraints.add((i, 6))\n\n        crf = ConditionalRandomField(num_tags=5, constraints=constraints)\n        crf.transitions = torch.nn.Parameter(self.transitions)\n        crf.start_transitions = torch.nn.Parameter(self.transitions_from_start)\n        crf.end_transitions = torch.nn.Parameter(self.transitions_to_end)\n\n        mask = torch.tensor([[True, True, True], [True, True, False]])\n\n        viterbi_path = crf.viterbi_tags(self.logits, mask)\n\n        # Get just the tags from each tuple of (tags, score).\n        viterbi_tags = [x for x, y in viterbi_path]\n\n        # Now the tags should respect the constraints\n        assert viterbi_tags == [[2, 3, 3], [2, 3]]\n\n    def test_allowed_transitions(self):\n\n        bio_labels = [""O"", ""B-X"", ""I-X"", ""B-Y"", ""I-Y""]  # start tag, end tag\n        #              0     1      2      3      4         5          6\n        allowed = allowed_transitions(""BIO"", dict(enumerate(bio_labels)))\n\n        # The empty spaces in this matrix indicate disallowed transitions.\n        assert set(allowed) == {  # Extra column for end tag.\n            (0, 0),\n            (0, 1),\n            (0, 3),\n            (0, 6),\n            (1, 0),\n            (1, 1),\n            (1, 2),\n            (1, 3),\n            (1, 6),\n            (2, 0),\n            (2, 1),\n            (2, 2),\n            (2, 3),\n            (2, 6),\n            (3, 0),\n            (3, 1),\n            (3, 3),\n            (3, 4),\n            (3, 6),\n            (4, 0),\n            (4, 1),\n            (4, 3),\n            (4, 4),\n            (4, 6),\n            (5, 0),\n            (5, 1),\n            (5, 3),  # Extra row for start tag\n        }\n\n        bioul_labels = [\n            ""O"",\n            ""B-X"",\n            ""I-X"",\n            ""L-X"",\n            ""U-X"",\n            ""B-Y"",\n            ""I-Y"",\n            ""L-Y"",\n            ""U-Y"",\n        ]  # start tag, end tag\n        #                0     1      2      3      4      5      6      7      8          9        10\n        allowed = allowed_transitions(""BIOUL"", dict(enumerate(bioul_labels)))\n\n        # The empty spaces in this matrix indicate disallowed transitions.\n        assert set(allowed) == {  # Extra column for end tag.\n            (0, 0),\n            (0, 1),\n            (0, 4),\n            (0, 5),\n            (0, 8),\n            (0, 10),\n            (1, 2),\n            (1, 3),  # noqa\n            (2, 2),\n            (2, 3),\n            (3, 0),\n            (3, 1),\n            (3, 4),\n            (3, 5),\n            (3, 8),\n            (3, 10),\n            (4, 0),\n            (4, 1),\n            (4, 4),\n            (4, 5),\n            (4, 8),\n            (4, 10),\n            (5, 6),\n            (5, 7),\n            (6, 6),\n            (6, 7),\n            (7, 0),\n            (7, 1),\n            (7, 4),\n            (7, 5),\n            (7, 8),\n            (7, 10),\n            (8, 0),\n            (8, 1),\n            (8, 4),\n            (8, 5),\n            (8, 8),\n            (8, 10),\n            # Extra row for start tag.\n            (9, 0),\n            (9, 1),\n            (9, 4),\n            (9, 5),\n            (9, 8),\n        }\n\n        iob1_labels = [""O"", ""B-X"", ""I-X"", ""B-Y"", ""I-Y""]  # start tag, end tag\n        #              0     1      2      3      4         5          6\n        allowed = allowed_transitions(""IOB1"", dict(enumerate(iob1_labels)))\n\n        # The empty spaces in this matrix indicate disallowed transitions.\n        assert set(allowed) == {  # Extra column for end tag.\n            (0, 0),\n            (0, 2),\n            (0, 4),\n            (0, 6),\n            (1, 0),\n            (1, 1),\n            (1, 2),\n            (1, 4),\n            (1, 6),\n            (2, 0),\n            (2, 1),\n            (2, 2),\n            (2, 4),\n            (2, 6),\n            (3, 0),\n            (3, 2),\n            (3, 3),\n            (3, 4),\n            (3, 6),\n            (4, 0),\n            (4, 2),\n            (4, 3),\n            (4, 4),\n            (4, 6),\n            (5, 0),\n            (5, 2),\n            (5, 4),  # Extra row for start tag\n        }\n        with raises(ConfigurationError):\n            allowed_transitions(""allennlp"", {})\n\n        bmes_labels = [""B-X"", ""M-X"", ""E-X"", ""S-X"", ""B-Y"", ""M-Y"", ""E-Y"", ""S-Y""]  # start tag, end tag\n        #               0      1      2      3      4      5      6      7       8          9\n        allowed = allowed_transitions(""BMES"", dict(enumerate(bmes_labels)))\n        assert set(allowed) == {\n            (0, 1),\n            (0, 2),\n            (1, 1),\n            (1, 2),  # Extra column for end tag.\n            (2, 0),\n            (2, 3),\n            (2, 4),\n            (2, 7),\n            (2, 9),  # noqa\n            (3, 0),\n            (3, 3),\n            (3, 4),\n            (3, 7),\n            (3, 9),\n            (4, 5),\n            (4, 6),\n            (5, 5),\n            (5, 6),\n            (6, 0),\n            (6, 3),\n            (6, 4),\n            (6, 7),\n            (6, 9),\n            (7, 0),\n            (7, 3),\n            (7, 4),\n            (7, 7),\n            (7, 9),\n            (8, 0),\n            (8, 3),\n            (8, 4),\n            (8, 7),  # Extra row for start tag\n        }\n'"
tests/modules/elmo_test.py,7,"b'import json\nimport os\nimport warnings\nfrom typing import List\n\nimport numpy\nimport torch\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Instance, Token, Vocabulary\nfrom allennlp.data.batch import Batch\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\nfrom allennlp.data.token_indexers.single_id_token_indexer import SingleIdTokenIndexer\nfrom allennlp.data.dataset_readers.dataset_reader import AllennlpDataset\nfrom allennlp.data.dataloader import DataLoader\nfrom allennlp.modules.elmo import _ElmoBiLm, _ElmoCharacterEncoder, Elmo\nfrom allennlp.modules.token_embedders import ElmoTokenEmbedder\nfrom allennlp.nn.util import remove_sentence_boundaries\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(""ignore"", category=FutureWarning)\n    import h5py\n\n\nclass ElmoTestCase(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.elmo_fixtures_path = self.FIXTURES_ROOT / ""elmo""\n        self.options_file = str(self.elmo_fixtures_path / ""options.json"")\n        self.weight_file = str(self.elmo_fixtures_path / ""lm_weights.hdf5"")\n        self.sentences_json_file = str(self.elmo_fixtures_path / ""sentences.json"")\n        self.sentences_txt_file = str(self.elmo_fixtures_path / ""sentences.txt"")\n\n    def _load_sentences_embeddings(self):\n        """"""\n        Load the test sentences and the expected LM embeddings.\n\n        These files loaded in this method were created with a batch-size of 3.\n        Due to idiosyncrasies with TensorFlow, the 30 sentences in sentences.json are split into 3 files in which\n        the k-th sentence in each is from batch k.\n\n        This method returns a (sentences, embeddings) pair where each is a list of length batch_size.\n        Each list contains a sublist with total_sentence_count / batch_size elements.  As with the original files,\n        the k-th element in the sublist is in batch k.\n        """"""\n        with open(self.sentences_json_file) as fin:\n            sentences = json.load(fin)\n\n        # the expected embeddings\n        expected_lm_embeddings = []\n        for k in range(len(sentences)):\n            embed_fname = os.path.join(self.elmo_fixtures_path, ""lm_embeddings_{}.hdf5"".format(k))\n            expected_lm_embeddings.append([])\n            with h5py.File(embed_fname, ""r"") as fin:\n                for i in range(10):\n                    sent_embeds = fin[""%s"" % i][...]\n                    sent_embeds_concat = numpy.concatenate(\n                        (sent_embeds[0, :, :], sent_embeds[1, :, :]), axis=-1\n                    )\n                    expected_lm_embeddings[-1].append(sent_embeds_concat)\n\n        return sentences, expected_lm_embeddings\n\n    @staticmethod\n    def get_vocab_and_both_elmo_indexed_ids(batch: List[List[str]]):\n        instances = []\n        indexer = ELMoTokenCharactersIndexer()\n        indexer2 = SingleIdTokenIndexer()\n        for sentence in batch:\n            tokens = [Token(token) for token in sentence]\n            field = TextField(tokens, {""character_ids"": indexer, ""tokens"": indexer2})\n            instance = Instance({""elmo"": field})\n            instances.append(instance)\n\n        dataset = Batch(instances)\n        vocab = Vocabulary.from_instances(instances)\n        dataset.index_instances(vocab)\n        return vocab, dataset.as_tensor_dict()[""elmo""]\n\n\nclass TestElmoBiLm(ElmoTestCase):\n    def test_elmo_bilm(self):\n        # get the raw data\n        sentences, expected_lm_embeddings = self._load_sentences_embeddings()\n\n        # load the test model\n        elmo_bilm = _ElmoBiLm(self.options_file, self.weight_file)\n\n        # Deal with the data.\n        indexer = ELMoTokenCharactersIndexer()\n\n        # For each sentence, first create a TextField, then create an instance\n        instances = []\n        for batch in zip(*sentences):\n            for sentence in batch:\n                tokens = [Token(token) for token in sentence.split()]\n                field = TextField(tokens, {""character_ids"": indexer})\n                instance = Instance({""elmo"": field})\n                instances.append(instance)\n\n        vocab = Vocabulary()\n        dataset = AllennlpDataset(instances, vocab)\n        # Now finally we can iterate through batches.\n        loader = DataLoader(dataset, 3)\n        for i, batch in enumerate(loader):\n            lm_embeddings = elmo_bilm(batch[""elmo""][""character_ids""][""elmo_tokens""])\n            top_layer_embeddings, mask = remove_sentence_boundaries(\n                lm_embeddings[""activations""][2], lm_embeddings[""mask""]\n            )\n\n            # check the mask lengths\n            lengths = mask.data.numpy().sum(axis=1)\n            batch_sentences = [sentences[k][i] for k in range(3)]\n            expected_lengths = [len(sentence.split()) for sentence in batch_sentences]\n            assert lengths.tolist() == expected_lengths\n\n            # get the expected embeddings and compare!\n            expected_top_layer = [expected_lm_embeddings[k][i] for k in range(3)]\n            for k in range(3):\n                assert numpy.allclose(\n                    top_layer_embeddings[k, : lengths[k], :].data.numpy(),\n                    expected_top_layer[k],\n                    atol=1.0e-6,\n                )\n\n    def test_elmo_char_cnn_cache_does_not_raise_error_for_uncached_words(self):\n        sentences = [[""This"", ""is"", ""OOV""], [""so"", ""is"", ""this""]]\n        in_vocab_sentences = [[""here"", ""is""], [""a"", ""vocab""]]\n        oov_tensor = self.get_vocab_and_both_elmo_indexed_ids(sentences)[1]\n        vocab, in_vocab_tensor = self.get_vocab_and_both_elmo_indexed_ids(in_vocab_sentences)\n        words_to_cache = list(vocab.get_token_to_index_vocabulary(""tokens"").keys())\n        elmo_bilm = _ElmoBiLm(self.options_file, self.weight_file, vocab_to_cache=words_to_cache)\n\n        elmo_bilm(\n            in_vocab_tensor[""character_ids""][""elmo_tokens""], in_vocab_tensor[""tokens""][""tokens""]\n        )\n        elmo_bilm(oov_tensor[""character_ids""][""elmo_tokens""], oov_tensor[""tokens""][""tokens""])\n\n    def test_elmo_bilm_can_cache_char_cnn_embeddings(self):\n        sentences = [[""This"", ""is"", ""a"", ""sentence""], [""Here"", ""\'s"", ""one""], [""Another"", ""one""]]\n        vocab, tensor = self.get_vocab_and_both_elmo_indexed_ids(sentences)\n        words_to_cache = list(vocab.get_token_to_index_vocabulary(""tokens"").keys())\n        elmo_bilm = _ElmoBiLm(self.options_file, self.weight_file)\n        elmo_bilm.eval()\n        no_cache = elmo_bilm(\n            tensor[""character_ids""][""elmo_tokens""], tensor[""character_ids""][""elmo_tokens""]\n        )\n\n        # ELMo is stateful, so we need to actually re-initialise it for this comparison to work.\n        elmo_bilm = _ElmoBiLm(self.options_file, self.weight_file, vocab_to_cache=words_to_cache)\n        elmo_bilm.eval()\n        cached = elmo_bilm(tensor[""character_ids""][""elmo_tokens""], tensor[""tokens""][""tokens""])\n\n        numpy.testing.assert_array_almost_equal(\n            no_cache[""mask""].data.cpu().numpy(), cached[""mask""].data.cpu().numpy()\n        )\n        for activation_cached, activation in zip(cached[""activations""], no_cache[""activations""]):\n            numpy.testing.assert_array_almost_equal(\n                activation_cached.data.cpu().numpy(), activation.data.cpu().numpy(), decimal=6\n            )\n\n\nclass TestElmo(ElmoTestCase):\n    def setup_method(self):\n        super().setup_method()\n\n        self.elmo = Elmo(self.options_file, self.weight_file, 2, dropout=0.0)\n\n    def _sentences_to_ids(self, sentences):\n        indexer = ELMoTokenCharactersIndexer()\n\n        # For each sentence, first create a TextField, then create an instance\n        instances = []\n        for sentence in sentences:\n            tokens = [Token(token) for token in sentence]\n            field = TextField(tokens, {""character_ids"": indexer})\n            instance = Instance({""elmo"": field})\n            instances.append(instance)\n\n        dataset = Batch(instances)\n        vocab = Vocabulary()\n        dataset.index_instances(vocab)\n        return dataset.as_tensor_dict()[""elmo""][""character_ids""][""elmo_tokens""]\n\n    def test_elmo(self):\n        # Correctness checks are in ElmoBiLm and ScalarMix, here we just add a shallow test\n        # to ensure things execute.\n        sentences = [\n            [""The"", ""sentence"", "".""],\n            [""ELMo"", ""helps"", ""disambiguate"", ""ELMo"", ""from"", ""Elmo"", "".""],\n        ]\n\n        character_ids = self._sentences_to_ids(sentences)\n        output = self.elmo(character_ids)\n        elmo_representations = output[""elmo_representations""]\n        mask = output[""mask""]\n\n        assert len(elmo_representations) == 2\n        assert list(elmo_representations[0].size()) == [2, 7, 32]\n        assert list(elmo_representations[1].size()) == [2, 7, 32]\n        assert list(mask.size()) == [2, 7]\n\n    def test_elmo_keep_sentence_boundaries(self):\n        sentences = [\n            [""The"", ""sentence"", "".""],\n            [""ELMo"", ""helps"", ""disambiguate"", ""ELMo"", ""from"", ""Elmo"", "".""],\n        ]\n        elmo = Elmo(\n            self.options_file, self.weight_file, 2, dropout=0.0, keep_sentence_boundaries=True\n        )\n        character_ids = self._sentences_to_ids(sentences)\n        output = elmo(character_ids)\n        elmo_representations = output[""elmo_representations""]\n        mask = output[""mask""]\n\n        assert len(elmo_representations) == 2\n        # Add 2 to the lengths because we\'re keeping the start and end of sentence tokens.\n        assert list(elmo_representations[0].size()) == [2, 7 + 2, 32]\n        assert list(elmo_representations[1].size()) == [2, 7 + 2, 32]\n        assert list(mask.size()) == [2, 7 + 2]\n\n    def test_elmo_4D_input(self):\n        sentences = [\n            [\n                [""The"", ""sentence"", "".""],\n                [""ELMo"", ""helps"", ""disambiguate"", ""ELMo"", ""from"", ""Elmo"", "".""],\n            ],\n            [[""1"", ""2""], [""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7""]],\n            [[""1"", ""2"", ""3"", ""4"", ""50"", ""60"", ""70""], [""The""]],\n        ]\n\n        all_character_ids = []\n        for batch_sentences in sentences:\n            all_character_ids.append(self._sentences_to_ids(batch_sentences))\n\n        # (2, 3, 7, 50)\n        character_ids = torch.cat([ids.unsqueeze(1) for ids in all_character_ids], dim=1)\n        embeddings_4d = self.elmo(character_ids)\n\n        # Run the individual batches.\n        embeddings_3d = []\n        for char_ids in all_character_ids:\n            self.elmo._elmo_lstm._elmo_lstm.reset_states()\n            embeddings_3d.append(self.elmo(char_ids))\n\n        for k in range(3):\n            numpy.testing.assert_array_almost_equal(\n                embeddings_4d[""elmo_representations""][0][:, k, :, :].data.numpy(),\n                embeddings_3d[k][""elmo_representations""][0].data.numpy(),\n            )\n\n    def test_elmo_with_module(self):\n        # We will create the _ElmoBilm class and pass it in as a module.\n        sentences = [\n            [""The"", ""sentence"", "".""],\n            [""ELMo"", ""helps"", ""disambiguate"", ""ELMo"", ""from"", ""Elmo"", "".""],\n        ]\n\n        character_ids = self._sentences_to_ids(sentences)\n        elmo_bilm = _ElmoBiLm(self.options_file, self.weight_file)\n        elmo = Elmo(None, None, 2, dropout=0.0, module=elmo_bilm)\n        output = elmo(character_ids)\n        elmo_representations = output[""elmo_representations""]\n\n        assert len(elmo_representations) == 2\n        for k in range(2):\n            assert list(elmo_representations[k].size()) == [2, 7, 32]\n\n    def test_elmo_bilm_can_handle_higher_dimensional_input_with_cache(self):\n        sentences = [[""This"", ""is"", ""a"", ""sentence""], [""Here"", ""\'s"", ""one""], [""Another"", ""one""]]\n        vocab, tensor = self.get_vocab_and_both_elmo_indexed_ids(sentences)\n        words_to_cache = list(vocab.get_token_to_index_vocabulary(""tokens"").keys())\n        elmo_bilm = Elmo(self.options_file, self.weight_file, 1, vocab_to_cache=words_to_cache)\n        elmo_bilm.eval()\n\n        individual_dim = elmo_bilm(\n            tensor[""character_ids""][""elmo_tokens""], tensor[""tokens""][""tokens""]\n        )\n        elmo_bilm = Elmo(self.options_file, self.weight_file, 1, vocab_to_cache=words_to_cache)\n        elmo_bilm.eval()\n\n        expanded_word_ids = torch.stack([tensor[""tokens""][""tokens""] for _ in range(4)], dim=1)\n        expanded_char_ids = torch.stack(\n            [tensor[""character_ids""][""elmo_tokens""] for _ in range(4)], dim=1\n        )\n        expanded_result = elmo_bilm(expanded_char_ids, expanded_word_ids)\n        split_result = [\n            x.squeeze(1) for x in torch.split(expanded_result[""elmo_representations""][0], 1, dim=1)\n        ]\n        for expanded in split_result:\n            numpy.testing.assert_array_almost_equal(\n                expanded.data.cpu().numpy(),\n                individual_dim[""elmo_representations""][0].data.cpu().numpy(),\n            )\n\n\nclass TestElmoRequiresGrad(ElmoTestCase):\n    def _run_test(self, requires_grad):\n        embedder = ElmoTokenEmbedder(\n            self.options_file, self.weight_file, requires_grad=requires_grad\n        )\n        batch_size = 3\n        seq_len = 4\n        char_ids = torch.from_numpy(numpy.random.randint(0, 262, (batch_size, seq_len, 50)))\n        embeddings = embedder(char_ids)\n        loss = embeddings.sum()\n        loss.backward()\n\n        elmo_grads = [\n            param.grad for name, param in embedder.named_parameters() if ""_elmo_lstm"" in name\n        ]\n        if requires_grad:\n            # None of the elmo grads should be None.\n            assert all(grad is not None for grad in elmo_grads)\n        else:\n            # All of the elmo grads should be None.\n            assert all(grad is None for grad in elmo_grads)\n\n    def test_elmo_requires_grad(self):\n        self._run_test(True)\n\n    def test_elmo_does_not_require_grad(self):\n        self._run_test(False)\n\n\nclass TestElmoTokenRepresentation(ElmoTestCase):\n    def test_elmo_token_representation(self):\n        # Load the test words and convert to char ids\n        with open(os.path.join(self.elmo_fixtures_path, ""vocab_test.txt""), ""r"") as fin:\n            words = fin.read().strip().split(""\\n"")\n\n        vocab = Vocabulary()\n        indexer = ELMoTokenCharactersIndexer()\n        tokens = [Token(word) for word in words]\n\n        indices = indexer.tokens_to_indices(tokens, vocab)\n        # There are 457 tokens. Reshape into 10 batches of 50 tokens.\n        sentences = []\n        for k in range(10):\n            char_indices = indices[""elmo_tokens""][(k * 50) : ((k + 1) * 50)]\n            sentences.append(\n                indexer.as_padded_tensor_dict(\n                    {""elmo_tokens"": char_indices}, padding_lengths={""elmo_tokens"": 50}\n                )[""elmo_tokens""]\n            )\n        batch = torch.stack(sentences)\n\n        elmo_token_embedder = _ElmoCharacterEncoder(self.options_file, self.weight_file)\n        elmo_token_embedder_output = elmo_token_embedder(batch)\n\n        # Reshape back to a list of words and compare with ground truth.  Need to also\n        # remove <S>, </S>\n        actual_embeddings = remove_sentence_boundaries(\n            elmo_token_embedder_output[""token_embedding""], elmo_token_embedder_output[""mask""]\n        )[0].data.numpy()\n        actual_embeddings = actual_embeddings.reshape(-1, actual_embeddings.shape[-1])\n\n        embedding_file = os.path.join(self.elmo_fixtures_path, ""elmo_token_embeddings.hdf5"")\n        with h5py.File(embedding_file, ""r"") as fin:\n            expected_embeddings = fin[""embedding""][...]\n\n        assert numpy.allclose(actual_embeddings[: len(tokens)], expected_embeddings, atol=1e-6)\n\n    def test_elmo_token_representation_bos_eos(self):\n        # The additional <S> and </S> embeddings added by the embedder should be as expected.\n        indexer = ELMoTokenCharactersIndexer()\n\n        elmo_token_embedder = _ElmoCharacterEncoder(self.options_file, self.weight_file)\n\n        for correct_index, token in [[0, ""<S>""], [2, ""</S>""]]:\n            indices = indexer.tokens_to_indices([Token(token)], Vocabulary())\n            indices = torch.from_numpy(numpy.array(indices[""elmo_tokens""])).view(1, 1, -1)\n            embeddings = elmo_token_embedder(indices)[""token_embedding""]\n            assert numpy.allclose(\n                embeddings[0, correct_index, :].data.numpy(), embeddings[0, 1, :].data.numpy()\n            )\n'"
tests/modules/encoder_base_test.py,26,"b'import numpy\nimport pytest\nimport torch\nfrom torch.nn import LSTM, RNN\n\nfrom allennlp.modules.encoder_base import _EncoderBase\nfrom allennlp.common.testing import AllenNlpTestCase, requires_gpu\nfrom allennlp.nn.util import sort_batch_by_length, get_lengths_from_binary_sequence_mask\n\n\nclass TestEncoderBase(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.lstm = LSTM(\n            bidirectional=True, num_layers=3, input_size=3, hidden_size=7, batch_first=True\n        )\n        self.rnn = RNN(\n            bidirectional=True, num_layers=3, input_size=3, hidden_size=7, batch_first=True\n        )\n        self.encoder_base = _EncoderBase(stateful=True)\n\n        tensor = torch.rand([5, 7, 3])\n        tensor[1, 6:, :] = 0\n        tensor[3, 2:, :] = 0\n        self.tensor = tensor\n        mask = torch.ones(5, 7).bool()\n        mask[1, 6:] = False\n        mask[2, :] = False  # <= completely masked\n        mask[3, 2:] = False\n        mask[4, :] = False  # <= completely masked\n        self.mask = mask\n\n        self.batch_size = 5\n        self.num_valid = 3\n        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n        _, _, restoration_indices, sorting_indices = sort_batch_by_length(tensor, sequence_lengths)\n        self.sorting_indices = sorting_indices\n        self.restoration_indices = restoration_indices\n\n    def test_non_stateful_states_are_sorted_correctly(self):\n        encoder_base = _EncoderBase(stateful=False)\n        initial_states = (torch.randn(6, 5, 7), torch.randn(6, 5, 7))\n        # Check that we sort the state for non-stateful encoders. To test\n        # we\'ll just use a ""pass through"" encoder, as we aren\'t actually testing\n        # the functionality of the encoder here anyway.\n        _, states, restoration_indices = encoder_base.sort_and_run_forward(\n            lambda *x: x, self.tensor, self.mask, initial_states\n        )\n        # Our input tensor had 2 zero length sequences, so we need\n        # to concat a tensor of shape\n        # (num_layers * num_directions, batch_size - num_valid, hidden_dim),\n        # to the output before unsorting it.\n        zeros = torch.zeros([6, 2, 7])\n\n        # sort_and_run_forward strips fully-padded instances from the batch;\n        # in order to use the restoration_indices we need to add back the two\n        #  that got stripped. What we get back should match what we started with.\n        for state, original in zip(states, initial_states):\n            assert list(state.size()) == [6, 3, 7]\n            state_with_zeros = torch.cat([state, zeros], 1)\n            unsorted_state = state_with_zeros.index_select(1, restoration_indices)\n            for index in [0, 1, 3]:\n                numpy.testing.assert_array_equal(\n                    unsorted_state[:, index, :].data.numpy(), original[:, index, :].data.numpy()\n                )\n\n    def test_get_initial_states(self):\n        # First time we call it, there should be no state, so we should return None.\n        assert (\n            self.encoder_base._get_initial_states(\n                self.batch_size, self.num_valid, self.sorting_indices\n            )\n            is None\n        )\n\n        # First test the case that the previous state is _smaller_ than the current state input.\n        initial_states = (torch.randn([1, 3, 7]), torch.randn([1, 3, 7]))\n        self.encoder_base._states = initial_states\n        # sorting indices are: [0, 1, 3, 2, 4]\n        returned_states = self.encoder_base._get_initial_states(\n            self.batch_size, self.num_valid, self.sorting_indices\n        )\n\n        correct_expanded_states = [\n            torch.cat([state, torch.zeros([1, 2, 7])], 1) for state in initial_states\n        ]\n        # State should have been expanded with zeros to have shape (1, batch_size, hidden_size).\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[0].data.numpy(), correct_expanded_states[0].data.numpy()\n        )\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[1].data.numpy(), correct_expanded_states[1].data.numpy()\n        )\n\n        # The returned states should be of shape (1, num_valid, hidden_size) and\n        # they also should have been sorted with respect to the indices.\n        # sorting indices are: [0, 1, 3, 2, 4]\n\n        correct_returned_states = [\n            state.index_select(1, self.sorting_indices)[:, : self.num_valid, :]\n            for state in correct_expanded_states\n        ]\n\n        numpy.testing.assert_array_equal(\n            returned_states[0].data.numpy(), correct_returned_states[0].data.numpy()\n        )\n        numpy.testing.assert_array_equal(\n            returned_states[1].data.numpy(), correct_returned_states[1].data.numpy()\n        )\n\n        # Now test the case that the previous state is larger:\n        original_states = (torch.randn([1, 10, 7]), torch.randn([1, 10, 7]))\n        self.encoder_base._states = original_states\n        # sorting indices are: [0, 1, 3, 2, 4]\n        returned_states = self.encoder_base._get_initial_states(\n            self.batch_size, self.num_valid, self.sorting_indices\n        )\n        # State should not have changed, as they were larger\n        # than the batch size of the requested states.\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[0].data.numpy(), original_states[0].data.numpy()\n        )\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[1].data.numpy(), original_states[1].data.numpy()\n        )\n\n        # The returned states should be of shape (1, num_valid, hidden_size) and they\n        # also should have been sorted with respect to the indices.\n        correct_returned_state = [\n            x.index_select(1, self.sorting_indices)[:, : self.num_valid, :] for x in original_states\n        ]\n        numpy.testing.assert_array_equal(\n            returned_states[0].data.numpy(), correct_returned_state[0].data.numpy()\n        )\n        numpy.testing.assert_array_equal(\n            returned_states[1].data.numpy(), correct_returned_state[1].data.numpy()\n        )\n\n    def test_update_states(self):\n        assert self.encoder_base._states is None\n        initial_states = torch.randn([1, 5, 7]), torch.randn([1, 5, 7])\n\n        index_selected_initial_states = (\n            initial_states[0].index_select(1, self.restoration_indices),\n            initial_states[1].index_select(1, self.restoration_indices),\n        )\n\n        self.encoder_base._update_states(initial_states, self.restoration_indices)\n        # State was None, so the updated state should just be the sorted given state.\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[0].data.numpy(), index_selected_initial_states[0].data.numpy()\n        )\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[1].data.numpy(), index_selected_initial_states[1].data.numpy()\n        )\n\n        new_states = torch.randn([1, 5, 7]), torch.randn([1, 5, 7])\n        # tensor has 2 completely masked rows, so the last 2 rows of the _sorted_ states\n        # will be completely zero, having been appended after calling the respective encoder.\n        new_states[0][:, -2:, :] = 0\n        new_states[1][:, -2:, :] = 0\n\n        index_selected_new_states = (\n            new_states[0].index_select(1, self.restoration_indices),\n            new_states[1].index_select(1, self.restoration_indices),\n        )\n\n        self.encoder_base._update_states(new_states, self.restoration_indices)\n        # Check that the update _preserved_ the state for the rows which were\n        # completely masked (2 and 4):\n        for index in [2, 4]:\n            numpy.testing.assert_array_equal(\n                self.encoder_base._states[0][:, index, :].data.numpy(),\n                index_selected_initial_states[0][:, index, :].data.numpy(),\n            )\n            numpy.testing.assert_array_equal(\n                self.encoder_base._states[1][:, index, :].data.numpy(),\n                index_selected_initial_states[1][:, index, :].data.numpy(),\n            )\n        # Now the states which were updated:\n        for index in [0, 1, 3]:\n            numpy.testing.assert_array_equal(\n                self.encoder_base._states[0][:, index, :].data.numpy(),\n                index_selected_new_states[0][:, index, :].data.numpy(),\n            )\n            numpy.testing.assert_array_equal(\n                self.encoder_base._states[1][:, index, :].data.numpy(),\n                index_selected_new_states[1][:, index, :].data.numpy(),\n            )\n\n        # Now test the case that the new state is smaller:\n        small_new_states = torch.randn([1, 3, 7]), torch.randn([1, 3, 7])\n        # pretend the 2nd sequence in the batch was fully masked.\n        small_restoration_indices = torch.LongTensor([2, 0, 1])\n        small_new_states[0][:, 0, :] = 0\n        small_new_states[1][:, 0, :] = 0\n\n        index_selected_small_states = (\n            small_new_states[0].index_select(1, small_restoration_indices),\n            small_new_states[1].index_select(1, small_restoration_indices),\n        )\n        self.encoder_base._update_states(small_new_states, small_restoration_indices)\n\n        # Check the index for the row we didn\'t update is the same as the previous step:\n        for index in [1, 3]:\n            numpy.testing.assert_array_equal(\n                self.encoder_base._states[0][:, index, :].data.numpy(),\n                index_selected_new_states[0][:, index, :].data.numpy(),\n            )\n            numpy.testing.assert_array_equal(\n                self.encoder_base._states[1][:, index, :].data.numpy(),\n                index_selected_new_states[1][:, index, :].data.numpy(),\n            )\n        # Indices we did update:\n        for index in [0, 2]:\n            numpy.testing.assert_array_equal(\n                self.encoder_base._states[0][:, index, :].data.numpy(),\n                index_selected_small_states[0][:, index, :].data.numpy(),\n            )\n            numpy.testing.assert_array_equal(\n                self.encoder_base._states[1][:, index, :].data.numpy(),\n                index_selected_small_states[1][:, index, :].data.numpy(),\n            )\n\n        # We didn\'t update index 4 in the previous step either, so it should be equal to the\n        # 4th index of initial states.\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[0][:, 4, :].data.numpy(),\n            index_selected_initial_states[0][:, 4, :].data.numpy(),\n        )\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[1][:, 4, :].data.numpy(),\n            index_selected_initial_states[1][:, 4, :].data.numpy(),\n        )\n\n    def test_reset_states(self):\n        # Initialize the encoder states.\n        assert self.encoder_base._states is None\n        initial_states = torch.randn([1, 5, 7]), torch.randn([1, 5, 7])\n        index_selected_initial_states = (\n            initial_states[0].index_select(1, self.restoration_indices),\n            initial_states[1].index_select(1, self.restoration_indices),\n        )\n        self.encoder_base._update_states(initial_states, self.restoration_indices)\n\n        # Check that only some of the states are reset when a mask is provided.\n        mask = torch.tensor([True, True, False, False, False])\n        self.encoder_base.reset_states(mask)\n        # First two states should be zeros\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[0][:, :2, :].data.numpy(),\n            torch.zeros_like(initial_states[0])[:, :2, :].data.numpy(),\n        )\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[1][:, :2, :].data.numpy(),\n            torch.zeros_like(initial_states[1])[:, :2, :].data.numpy(),\n        )\n        # Remaining states should be the same\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[0][:, 2:, :].data.numpy(),\n            index_selected_initial_states[0][:, 2:, :].data.numpy(),\n        )\n        numpy.testing.assert_array_equal(\n            self.encoder_base._states[1][:, 2:, :].data.numpy(),\n            index_selected_initial_states[1][:, 2:, :].data.numpy(),\n        )\n\n        # Check that error is raised if mask has wrong batch size.\n        bad_mask = torch.tensor([True, True, False])\n        with pytest.raises(ValueError):\n            self.encoder_base.reset_states(bad_mask)\n\n        # Check that states are reset to None if no mask is provided.\n        self.encoder_base.reset_states()\n        assert self.encoder_base._states is None\n\n    def test_non_contiguous_initial_states_handled(self):\n        # Check that the encoder is robust to non-contiguous initial states.\n\n        # Case 1: Encoder is not stateful\n\n        # A transposition will make the tensors non-contiguous, start them off at the wrong shape\n        # and transpose them into the right shape.\n        encoder_base = _EncoderBase(stateful=False)\n        initial_states = (\n            torch.randn(5, 6, 7).permute(1, 0, 2),\n            torch.randn(5, 6, 7).permute(1, 0, 2),\n        )\n        assert not initial_states[0].is_contiguous() and not initial_states[1].is_contiguous()\n        assert initial_states[0].size() == torch.Size([6, 5, 7])\n        assert initial_states[1].size() == torch.Size([6, 5, 7])\n\n        # We\'ll pass them through an LSTM encoder and a vanilla RNN encoder to make sure it works\n        # whether the initial states are a tuple of tensors or just a single tensor.\n        encoder_base.sort_and_run_forward(self.lstm, self.tensor, self.mask, initial_states)\n        encoder_base.sort_and_run_forward(self.rnn, self.tensor, self.mask, initial_states[0])\n\n        # Case 2: Encoder is stateful\n\n        # For stateful encoders, the initial state may be non-contiguous if its state was\n        # previously updated with non-contiguous tensors. As in the non-stateful tests, we check\n        # that the encoder still works on initial states for RNNs and LSTMs.\n        final_states = initial_states\n        # Check LSTM\n        encoder_base = _EncoderBase(stateful=True)\n        encoder_base._update_states(final_states, self.restoration_indices)\n        encoder_base.sort_and_run_forward(self.lstm, self.tensor, self.mask)\n        # Check RNN\n        encoder_base.reset_states()\n        encoder_base._update_states([final_states[0]], self.restoration_indices)\n        encoder_base.sort_and_run_forward(self.rnn, self.tensor, self.mask)\n\n    @requires_gpu\n    def test_non_contiguous_initial_states_handled_on_gpu(self):\n        # Some PyTorch operations which produce contiguous tensors on the CPU produce\n        # non-contiguous tensors on the GPU (e.g. forward pass of an RNN when batch_first=True).\n        # Accordingly, we perform the same checks from previous test on the GPU to ensure the\n        # encoder is not affected by which device it is on.\n\n        # Case 1: Encoder is not stateful\n\n        # A transposition will make the tensors non-contiguous, start them off at the wrong shape\n        # and transpose them into the right shape.\n        encoder_base = _EncoderBase(stateful=False).cuda()\n        initial_states = (\n            torch.randn(5, 6, 7).cuda().permute(1, 0, 2),\n            torch.randn(5, 6, 7).cuda().permute(1, 0, 2),\n        )\n        assert not initial_states[0].is_contiguous() and not initial_states[1].is_contiguous()\n        assert initial_states[0].size() == torch.Size([6, 5, 7])\n        assert initial_states[1].size() == torch.Size([6, 5, 7])\n\n        # We\'ll pass them through an LSTM encoder and a vanilla RNN encoder to make sure it works\n        # whether the initial states are a tuple of tensors or just a single tensor.\n        encoder_base.sort_and_run_forward(\n            self.lstm.cuda(), self.tensor.cuda(), self.mask.cuda(), initial_states\n        )\n        encoder_base.sort_and_run_forward(\n            self.rnn.cuda(), self.tensor.cuda(), self.mask.cuda(), initial_states[0]\n        )\n\n        # Case 2: Encoder is stateful\n\n        # For stateful encoders, the initial state may be non-contiguous if its state was\n        # previously updated with non-contiguous tensors. As in the non-stateful tests, we check\n        # that the encoder still works on initial states for RNNs and LSTMs.\n        final_states = initial_states\n        # Check LSTM\n        encoder_base = _EncoderBase(stateful=True).cuda()\n        encoder_base._update_states(final_states, self.restoration_indices.cuda())\n        encoder_base.sort_and_run_forward(self.lstm.cuda(), self.tensor.cuda(), self.mask.cuda())\n        # Check RNN\n        encoder_base.reset_states()\n        encoder_base._update_states([final_states[0]], self.restoration_indices.cuda())\n        encoder_base.sort_and_run_forward(self.rnn.cuda(), self.tensor.cuda(), self.mask.cuda())\n'"
tests/modules/feedforward_test.py,5,"b'from numpy.testing import assert_almost_equal\nimport inspect\nimport pytest\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.modules import FeedForward\nfrom allennlp.nn import InitializerApplicator, Initializer, Activation\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestFeedForward(AllenNlpTestCase):\n    def test_can_construct_from_params(self):\n        params = Params({""input_dim"": 2, ""hidden_dims"": 3, ""activations"": ""relu"", ""num_layers"": 2})\n        feedforward = FeedForward.from_params(params)\n        assert len(feedforward._activations) == 2\n        assert [isinstance(a, torch.nn.ReLU) for a in feedforward._activations]\n        assert len(feedforward._linear_layers) == 2\n        assert [layer.weight.size(-1) == 3 for layer in feedforward._linear_layers]\n\n        params = Params(\n            {\n                ""input_dim"": 2,\n                ""hidden_dims"": [3, 4, 5],\n                ""activations"": [""relu"", ""relu"", ""linear""],\n                ""dropout"": 0.2,\n                ""num_layers"": 3,\n            }\n        )\n        feedforward = FeedForward.from_params(params)\n        assert len(feedforward._activations) == 3\n        assert isinstance(feedforward._activations[0], torch.nn.ReLU)\n        assert isinstance(feedforward._activations[1], torch.nn.ReLU)\n        # It\'s hard to check that the last activation is the lambda function we use for `linear`,\n        # so this is good enough.\n        assert not isinstance(feedforward._activations[2], torch.nn.ReLU)\n\n        assert len(feedforward._linear_layers) == 3\n        assert feedforward._linear_layers[0].weight.size(0) == 3\n        assert feedforward._linear_layers[1].weight.size(0) == 4\n        assert feedforward._linear_layers[2].weight.size(0) == 5\n\n        assert len(feedforward._dropout) == 3\n        assert [d.p == 0.2 for d in feedforward._dropout]\n\n    def test_init_checks_hidden_dim_consistency(self):\n        with pytest.raises(ConfigurationError):\n            FeedForward(2, 4, [5, 5], Activation.by_name(""relu"")())\n\n    def test_init_checks_activation_consistency(self):\n        with pytest.raises(ConfigurationError):\n            FeedForward(2, 4, 5, [Activation.by_name(""relu"")(), Activation.by_name(""relu"")()])\n\n    def test_forward_gives_correct_output(self):\n        params = Params({""input_dim"": 2, ""hidden_dims"": 3, ""activations"": ""relu"", ""num_layers"": 2})\n        feedforward = FeedForward.from_params(params)\n\n        constant_init = Initializer.from_params(Params({""type"": ""constant"", ""val"": 1.0}))\n        initializer = InitializerApplicator([("".*"", constant_init)])\n        initializer(feedforward)\n\n        input_tensor = torch.FloatTensor([[-3, 1]])\n        output = feedforward(input_tensor).data.numpy()\n        assert output.shape == (1, 3)\n        # This output was checked by hand - ReLU makes output after first hidden layer [0, 0, 0],\n        # which then gets a bias added in the second layer to be [1, 1, 1].\n        assert_almost_equal(output, [[1, 1, 1]])\n\n    def test_textual_representation_contains_activations(self):\n        params = Params(\n            {\n                ""input_dim"": 2,\n                ""hidden_dims"": 3,\n                ""activations"": [""linear"", ""relu"", ""swish""],\n                ""num_layers"": 3,\n            }\n        )\n        feedforward = FeedForward.from_params(params)\n        expected_text_representation = inspect.cleandoc(\n            """"""\n            FeedForward(\n              (_activations): ModuleList(\n                (0): Linear()\n                (1): ReLU()\n                (2): Swish()\n              )\n              (_linear_layers): ModuleList(\n                (0): Linear(in_features=2, out_features=3, bias=True)\n                (1): Linear(in_features=3, out_features=3, bias=True)\n                (2): Linear(in_features=3, out_features=3, bias=True)\n              )\n              (_dropout): ModuleList(\n                (0): Dropout(p=0.0, inplace=False)\n                (1): Dropout(p=0.0, inplace=False)\n                (2): Dropout(p=0.0, inplace=False)\n              )\n            )\n            """"""\n        )\n        actual_text_representation = str(feedforward)\n\n        assert actual_text_representation == expected_text_representation\n'"
tests/modules/gated_sum_test.py,3,"b'import pytest\nimport torch\n\nimport numpy\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules import GatedSum\n\n\nclass TestGatedSum(AllenNlpTestCase):\n    def test_gated_sum_can_run_forward(self):\n        a = torch.FloatTensor([1, 2, 3, 4, 5])\n        b = -a + 0.1\n        weight_value = 2\n        gate_value = torch.sigmoid(torch.FloatTensor([1]))\n        expected = gate_value * a + (1 - gate_value) * b\n\n        with torch.no_grad():  # because we want to change the weight\n            gated_sum = GatedSum(a.size(-1))\n            gated_sum._gate.weight *= 0\n            gated_sum._gate.weight += weight_value\n            gated_sum._gate.bias *= 0\n\n            out = gated_sum(a, b)\n            numpy.testing.assert_almost_equal(expected.data.numpy(), out.data.numpy(), decimal=5)\n\n        with pytest.raises(ValueError):\n            GatedSum(a.size(-1))(a, b.unsqueeze(0))\n\n        with pytest.raises(ValueError):\n            GatedSum(100)(a, b)\n\n    def test_input_output_dim(self):\n        dim = 77\n        gated_sum = GatedSum(dim)\n        numpy.testing.assert_equal(gated_sum.get_input_dim(), dim)\n        numpy.testing.assert_equal(gated_sum.get_output_dim(), dim)\n'"
tests/modules/highway_test.py,2,"b'from numpy.testing import assert_almost_equal\nimport torch\n\nfrom allennlp.modules import Highway\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestHighway(AllenNlpTestCase):\n    def test_forward_works_on_simple_input(self):\n        highway = Highway(2, 2)\n\n        highway._layers[0].weight.data.fill_(1)\n        highway._layers[0].bias.data.fill_(0)\n        highway._layers[1].weight.data.fill_(2)\n        highway._layers[1].bias.data.fill_(-2)\n        input_tensor = torch.FloatTensor([[-2, 1], [3, -2]])\n        result = highway(input_tensor).data.numpy()\n        assert result.shape == (2, 2)\n        # This was checked by hand.\n        assert_almost_equal(result, [[-0.0394, 0.0197], [1.7527, -0.5550]], decimal=4)\n\n    def test_forward_works_on_nd_input(self):\n        highway = Highway(2, 2)\n        input_tensor = torch.ones(2, 2, 2)\n        output = highway(input_tensor)\n        assert output.size() == (2, 2, 2)\n'"
tests/modules/lstm_cell_with_projection_test.py,3,"b'import numpy\nimport torch\n\nfrom allennlp.modules.lstm_cell_with_projection import LstmCellWithProjection\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestLstmCellWithProjection(AllenNlpTestCase):\n    def test_elmo_lstm_cell_completes_forward_pass(self):\n        input_tensor = torch.rand(4, 5, 3)\n        input_tensor[1, 4:, :] = 0.0\n        input_tensor[2, 2:, :] = 0.0\n        input_tensor[3, 1:, :] = 0.0\n\n        initial_hidden_state = torch.ones([1, 4, 5])\n        initial_memory_state = torch.ones([1, 4, 7])\n\n        lstm = LstmCellWithProjection(\n            input_size=3,\n            hidden_size=5,\n            cell_size=7,\n            memory_cell_clip_value=2,\n            state_projection_clip_value=1,\n        )\n        output_sequence, lstm_state = lstm(\n            input_tensor, [5, 4, 2, 1], (initial_hidden_state, initial_memory_state)\n        )\n        numpy.testing.assert_array_equal(output_sequence.data[1, 4:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[2, 2:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[3, 1:, :].numpy(), 0.0)\n\n        # Test the state clipping.\n        numpy.testing.assert_array_less(output_sequence.data.numpy(), 1.0)\n        numpy.testing.assert_array_less(-output_sequence.data.numpy(), 1.0)\n\n        # LSTM state should be (num_layers, batch_size, hidden_size)\n        assert list(lstm_state[0].size()) == [1, 4, 5]\n        # LSTM memory cell should be (num_layers, batch_size, cell_size)\n        assert list((lstm_state[1].size())) == [1, 4, 7]\n\n        # Test the cell clipping.\n        numpy.testing.assert_array_less(lstm_state[0].data.numpy(), 2.0)\n        numpy.testing.assert_array_less(-lstm_state[0].data.numpy(), 2.0)\n'"
tests/modules/masked_layer_norm_test.py,4,"b'import numpy as np\nimport torch\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules.masked_layer_norm import MaskedLayerNorm\nfrom allennlp.nn import util\n\n\nclass TestMaskedLayerNorm(AllenNlpTestCase):\n    def test_masked_layer_norm(self):\n        x_n = np.random.rand(2, 3, 7)\n        mask_n = np.array([[1, 1, 0], [1, 1, 1]])\n\n        x = torch.from_numpy(x_n).float()\n        mask = torch.from_numpy(mask_n).bool()\n\n        layer_norm = MaskedLayerNorm(7, gamma0=0.2)\n        normed_x = layer_norm(x, mask)\n\n        N = 7 * 5\n        mean = (x_n * np.expand_dims(mask_n, axis=-1)).sum() / N\n        std = np.sqrt(\n            (((x_n - mean) * np.expand_dims(mask_n, axis=-1)) ** 2).sum() / N\n            + util.tiny_value_of_dtype(torch.float)\n        )\n        expected = 0.2 * (x_n - mean) / (std + util.tiny_value_of_dtype(torch.float))\n\n        assert np.allclose(normed_x.data.numpy(), expected)\n'"
tests/modules/maxout_test.py,1,"b'from numpy.testing import assert_almost_equal\nimport pytest\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.modules import Maxout\nfrom allennlp.nn import InitializerApplicator, Initializer\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestMaxout(AllenNlpTestCase):\n    def test_init_checks_output_dims_consistency(self):\n        with pytest.raises(ConfigurationError):\n            Maxout(input_dim=2, num_layers=2, output_dims=[5, 4, 3], pool_sizes=4, dropout=0.0)\n\n    def test_init_checks_pool_sizes_consistency(self):\n        with pytest.raises(ConfigurationError):\n            Maxout(input_dim=2, num_layers=2, output_dims=5, pool_sizes=[4, 5, 2], dropout=0.0)\n\n    def test_init_checks_dropout_consistency(self):\n        with pytest.raises(ConfigurationError):\n            Maxout(input_dim=2, num_layers=3, output_dims=5, pool_sizes=4, dropout=[0.2, 0.3])\n\n    def test_forward_gives_correct_output(self):\n        params = Params(\n            {""input_dim"": 2, ""output_dims"": 3, ""pool_sizes"": 4, ""dropout"": 0.0, ""num_layers"": 2}\n        )\n        maxout = Maxout.from_params(params)\n\n        constant_init = Initializer.from_params(Params({""type"": ""constant"", ""val"": 1.0}))\n        initializer = InitializerApplicator([("".*"", constant_init)])\n        initializer(maxout)\n\n        input_tensor = torch.FloatTensor([[-3, 1]])\n        output = maxout(input_tensor).data.numpy()\n        assert output.shape == (1, 3)\n        # This output was checked by hand\n        # The output of the first maxout layer is [-1, -1, -1], since the\n        # matrix multiply gives us [-2]*12. Reshaping and maxing\n        # produces [-2, -2, -2] and the bias increments these values.\n        # The second layer output is [-2, -2, -2], since the matrix\n        # matrix multiply gives us [-3]*12. Reshaping and maxing\n        # produces [-3, -3, -3] and the bias increments these values.\n        assert_almost_equal(output, [[-2, -2, -2]])\n'"
tests/modules/residual_with_layer_dropout_test.py,4,"b'from numpy.testing import assert_almost_equal\nimport torch\n\nfrom allennlp.modules import ResidualWithLayerDropout\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestResidualWithLayerDropout(AllenNlpTestCase):\n    def test_dropout_works_for_training(self):\n        layer_input_tensor = torch.FloatTensor([[2, 1], [-3, -2]])\n        layer_output_tensor = torch.FloatTensor([[1, 3], [2, -1]])\n\n        # The layer output should be dropped\n        residual_with_layer_dropout = ResidualWithLayerDropout(1)\n        residual_with_layer_dropout.train()\n        result = residual_with_layer_dropout(layer_input_tensor, layer_output_tensor).data.numpy()\n        assert result.shape == (2, 2)\n        assert_almost_equal(result, [[2, 1], [-3, -2]])\n\n        result = residual_with_layer_dropout(\n            layer_input_tensor, layer_output_tensor, 1, 1\n        ).data.numpy()\n        assert result.shape == (2, 2)\n        assert_almost_equal(result, [[2, 1], [-3, -2]])\n\n        # The layer output should not be dropped\n        residual_with_layer_dropout = ResidualWithLayerDropout(0.0)\n        residual_with_layer_dropout.train()\n        result = residual_with_layer_dropout(layer_input_tensor, layer_output_tensor).data.numpy()\n        assert result.shape == (2, 2)\n        assert_almost_equal(result, [[2 + 1, 1 + 3], [-3 + 2, -2 - 1]])\n\n    def test_dropout_works_for_testing(self):\n        layer_input_tensor = torch.FloatTensor([[2, 1], [-3, -2]])\n        layer_output_tensor = torch.FloatTensor([[1, 3], [2, -1]])\n\n        # During testing, the layer output is re-calibrated according to the survival probability,\n        # and then added to the input.\n        residual_with_layer_dropout = ResidualWithLayerDropout(0.2)\n        residual_with_layer_dropout.eval()\n        result = residual_with_layer_dropout(layer_input_tensor, layer_output_tensor).data.numpy()\n        assert result.shape == (2, 2)\n        assert_almost_equal(result, [[2 + 1 * 0.8, 1 + 3 * 0.8], [-3 + 2 * 0.8, -2 - 1 * 0.8]])\n'"
tests/modules/sampled_softmax_loss_test.py,8,"b'from typing import Tuple\n\nimport torch\nimport numpy as np\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules.sampled_softmax_loss import _choice, SampledSoftmaxLoss\nfrom allennlp.modules import SoftmaxLoss\n\n\nclass TestSampledSoftmaxLoss(AllenNlpTestCase):\n    def test_choice(self):\n        sample, num_tries = _choice(num_words=1000, num_samples=50)\n        assert len(set(sample)) == 50\n        assert all(0 <= x < 1000 for x in sample)\n        assert num_tries >= 50\n\n    def test_sampled_softmax_can_run(self):\n        softmax = SampledSoftmaxLoss(num_words=1000, embedding_dim=12, num_samples=50)\n\n        # sequence_length, embedding_dim\n        embedding = torch.rand(100, 12)\n        targets = torch.randint(0, 1000, (100,)).long()\n\n        _ = softmax(embedding, targets)\n\n    def test_sampled_equals_unsampled_during_eval(self):\n        sampled_softmax = SampledSoftmaxLoss(num_words=10000, embedding_dim=12, num_samples=40)\n        unsampled_softmax = SoftmaxLoss(num_words=10000, embedding_dim=12)\n\n        sampled_softmax.eval()\n        unsampled_softmax.eval()\n\n        # set weights equal, use transpose because opposite shapes\n        sampled_softmax.softmax_w.data = unsampled_softmax.softmax_w.t()\n        sampled_softmax.softmax_b.data = unsampled_softmax.softmax_b\n\n        # sequence_length, embedding_dim\n        embedding = torch.rand(100, 12)\n        targets = torch.randint(0, 1000, (100,)).long()\n\n        full_loss = unsampled_softmax(embedding, targets).item()\n        sampled_loss = sampled_softmax(embedding, targets).item()\n\n        # Should be really close\n        np.testing.assert_almost_equal(sampled_loss, full_loss)\n\n    def test_sampled_softmax_has_greater_loss_in_train_mode(self):\n        sampled_softmax = SampledSoftmaxLoss(num_words=10000, embedding_dim=12, num_samples=10)\n\n        # sequence_length, embedding_dim\n        embedding = torch.rand(100, 12)\n        targets = torch.randint(0, 1000, (100,)).long()\n\n        sampled_softmax.train()\n        train_loss = sampled_softmax(embedding, targets).item()\n\n        sampled_softmax.eval()\n        eval_loss = sampled_softmax(embedding, targets).item()\n\n        assert eval_loss > train_loss\n\n    def test_sampled_equals_unsampled_when_biased_against_non_sampled_positions(self):\n        sampled_softmax = SampledSoftmaxLoss(num_words=10000, embedding_dim=12, num_samples=10)\n        unsampled_softmax = SoftmaxLoss(num_words=10000, embedding_dim=12)\n\n        # fake out choice function\n        FAKE_SAMPLES = [100, 200, 300, 400, 500, 600, 700, 800, 900, 9999]\n\n        def fake_choice(num_words: int, num_samples: int) -> Tuple[np.ndarray, int]:\n            assert (num_words, num_samples) == (10000, 10)\n            return np.array(FAKE_SAMPLES), 12\n\n        sampled_softmax.choice_func = fake_choice\n\n        # bias out the unsampled terms:\n        for i in range(10000):\n            if i not in FAKE_SAMPLES:\n                unsampled_softmax.softmax_b[i] = -10000\n\n        # set weights equal, use transpose because opposite shapes\n        sampled_softmax.softmax_w.data = unsampled_softmax.softmax_w.t()\n        sampled_softmax.softmax_b.data = unsampled_softmax.softmax_b\n\n        sampled_softmax.train()\n        unsampled_softmax.train()\n\n        # sequence_length, embedding_dim\n        embedding = torch.rand(100, 12)\n        targets = torch.randint(0, 1000, (100,)).long()\n\n        full_loss = unsampled_softmax(embedding, targets).item()\n        sampled_loss = sampled_softmax(embedding, targets).item()\n\n        # Should be close\n\n        pct_error = (sampled_loss - full_loss) / full_loss\n        assert abs(pct_error) < 0.001\n'"
tests/modules/scalar_mix_test.py,5,"b'import torch\n\nimport pytest\nimport numpy\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.modules import ScalarMix\nfrom allennlp.nn import util\n\n\nclass TestScalarMix(AllenNlpTestCase):\n    def test_scalar_mix_can_run_forward(self):\n        mixture = ScalarMix(3)\n        tensors = [torch.randn([3, 4, 5]) for _ in range(3)]\n        for k in range(3):\n            mixture.scalar_parameters[k].data[0] = 0.1 * (k + 1)\n        mixture.gamma.data[0] = 0.5\n        result = mixture(tensors)\n\n        weights = [0.1, 0.2, 0.3]\n        normed_weights = numpy.exp(weights) / numpy.sum(numpy.exp(weights))\n        expected_result = sum(normed_weights[k] * tensors[k].data.numpy() for k in range(3))\n        expected_result *= 0.5\n        numpy.testing.assert_almost_equal(expected_result, result.data.numpy())\n\n    def test_scalar_mix_throws_error_on_incorrect_number_of_inputs(self):\n        mixture = ScalarMix(3)\n        tensors = [torch.randn([3, 4, 5]) for _ in range(5)]\n        with pytest.raises(ConfigurationError):\n            _ = mixture(tensors)\n\n    def test_scalar_mix_throws_error_on_incorrect_initial_scalar_parameters_length(self):\n        with pytest.raises(ConfigurationError):\n            ScalarMix(3, initial_scalar_parameters=[0.0, 0.0])\n\n    def test_scalar_mix_trainable_with_initial_scalar_parameters(self):\n        initial_scalar_parameters = [1.0, 2.0, 3.0]\n        mixture = ScalarMix(3, initial_scalar_parameters=initial_scalar_parameters, trainable=False)\n        for i, scalar_mix_parameter in enumerate(mixture.scalar_parameters):\n            assert scalar_mix_parameter.requires_grad is False\n            assert scalar_mix_parameter.item() == initial_scalar_parameters[i]\n\n    def test_scalar_mix_layer_norm(self):\n        mixture = ScalarMix(3, do_layer_norm=""scalar_norm_reg"")\n\n        tensors = [torch.randn([3, 4, 5]) for _ in range(3)]\n        numpy_mask = numpy.ones((3, 4), dtype=""int32"")\n        numpy_mask[1, 2:] = 0\n        mask = torch.from_numpy(numpy_mask).bool()\n\n        weights = [0.1, 0.2, 0.3]\n        for k in range(3):\n            mixture.scalar_parameters[k].data[0] = weights[k]\n        mixture.gamma.data[0] = 0.5\n        result = mixture(tensors, mask)\n\n        normed_weights = numpy.exp(weights) / numpy.sum(numpy.exp(weights))\n        expected_result = numpy.zeros((3, 4, 5))\n        for k in range(3):\n            mean = numpy.mean(tensors[k].data.numpy()[numpy_mask == 1])\n            std = numpy.std(tensors[k].data.numpy()[numpy_mask == 1])\n            normed_tensor = (tensors[k].data.numpy() - mean) / (\n                std + util.tiny_value_of_dtype(torch.float)\n            )\n            expected_result += normed_tensor * normed_weights[k]\n        expected_result *= 0.5\n\n        numpy.testing.assert_almost_equal(expected_result, result.data.numpy(), decimal=6)\n'"
tests/modules/seq2seq_encoder_test.py,0,"b'import pytest\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.modules import Seq2SeqEncoder\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestSeq2SeqEncoder(AllenNlpTestCase):\n    def test_from_params_builders_encoder_correctly(self):\n        # We\'re just making sure parameters get passed through correctly here, and that the basic\n        # API works.\n        params = Params(\n            {\n                ""type"": ""lstm"",\n                ""bidirectional"": True,\n                ""num_layers"": 3,\n                ""input_size"": 5,\n                ""hidden_size"": 7,\n                ""stateful"": True,\n            }\n        )\n        encoder = Seq2SeqEncoder.from_params(params)\n\n        assert encoder.__class__.__name__ == ""LstmSeq2SeqEncoder""\n        assert encoder._module.__class__.__name__ == ""LSTM""\n        assert encoder._module.num_layers == 3\n        assert encoder._module.input_size == 5\n        assert encoder._module.hidden_size == 7\n        assert encoder._module.bidirectional is True\n        assert encoder._module.batch_first is True\n        assert encoder.stateful is True\n\n    def test_from_params_requires_batch_first(self):\n        params = Params({""type"": ""lstm"", ""batch_first"": False})\n        with pytest.raises(ConfigurationError):\n            Seq2SeqEncoder.from_params(params)\n'"
tests/modules/seq2vec_encoder_test.py,0,"b'import pytest\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.modules import Seq2VecEncoder\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestSeq2VecEncoder(AllenNlpTestCase):\n    def test_from_params_builders_encoder_correctly(self):\n        # We\'re just making sure parameters get passed through correctly here, and that the basic\n        # API works.\n        params = Params(\n            {\n                ""type"": ""lstm"",\n                ""bidirectional"": True,\n                ""num_layers"": 3,\n                ""input_size"": 5,\n                ""hidden_size"": 7,\n            }\n        )\n        encoder = Seq2VecEncoder.from_params(params)\n\n        assert encoder.__class__.__name__ == ""LstmSeq2VecEncoder""\n        assert encoder._module.__class__.__name__ == ""LSTM""\n        assert encoder._module.num_layers == 3\n        assert encoder._module.input_size == 5\n        assert encoder._module.hidden_size == 7\n        assert encoder._module.bidirectional is True\n        assert encoder._module.batch_first is True\n\n    def test_from_params_requires_batch_first(self):\n        params = Params({""type"": ""lstm"", ""batch_first"": False})\n        with pytest.raises(ConfigurationError):\n            Seq2VecEncoder.from_params(params)\n'"
tests/modules/stacked_alternating_lstm_test.py,2,"b'import numpy\nimport torch\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom allennlp.modules.stacked_alternating_lstm import StackedAlternatingLstm\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestStackedAlternatingLstm(AllenNlpTestCase):\n    def test_stacked_alternating_lstm_completes_forward_pass(self):\n        input_tensor = torch.rand(4, 5, 3)\n        input_tensor[1, 4:, :] = 0.0\n        input_tensor[2, 2:, :] = 0.0\n        input_tensor[3, 1:, :] = 0.0\n        input_tensor = pack_padded_sequence(input_tensor, [5, 4, 2, 1], batch_first=True)\n        lstm = StackedAlternatingLstm(3, 7, 3)\n        output, _ = lstm(input_tensor)\n        output_sequence, _ = pad_packed_sequence(output, batch_first=True)\n        numpy.testing.assert_array_equal(output_sequence.data[1, 4:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[2, 2:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[3, 1:, :].numpy(), 0.0)\n\n    def test_lstms_are_interleaved(self):\n        lstm = StackedAlternatingLstm(3, 7, 8)\n        for i, layer in enumerate(lstm.lstm_layers):\n            if i % 2 == 0:\n                assert layer.go_forward\n            else:\n                assert not layer.go_forward\n'"
tests/modules/stacked_bidirectional_lstm_test.py,8,"b'import numpy\nimport pytest\nimport torch\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\nfrom allennlp.modules.seq2seq_encoders import Seq2SeqEncoder\nfrom allennlp.modules.seq2vec_encoders import Seq2VecEncoder\nfrom allennlp.common.params import Params\nfrom allennlp.nn import InitializerApplicator, Initializer\nfrom allennlp.nn.util import sort_batch_by_length\n\n\nclass TestStackedBidirectionalLstm:\n    def test_stacked_bidirectional_lstm_completes_forward_pass(self):\n        input_tensor = torch.rand(4, 5, 3)\n        input_tensor[1, 4:, :] = 0.0\n        input_tensor[2, 2:, :] = 0.0\n        input_tensor[3, 1:, :] = 0.0\n        input_tensor = pack_padded_sequence(input_tensor, [5, 4, 2, 1], batch_first=True)\n        lstm = StackedBidirectionalLstm(3, 7, 3)\n        output, _ = lstm(input_tensor)\n        output_sequence, _ = pad_packed_sequence(output, batch_first=True)\n        numpy.testing.assert_array_equal(output_sequence.data[1, 4:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[2, 2:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[3, 1:, :].numpy(), 0.0)\n\n    def test_stacked_bidirectional_lstm_can_build_from_params(self):\n        params = Params(\n            {\n                ""type"": ""stacked_bidirectional_lstm"",\n                ""input_size"": 5,\n                ""hidden_size"": 9,\n                ""num_layers"": 3,\n            }\n        )\n        encoder = Seq2SeqEncoder.from_params(params)\n\n        assert encoder.get_input_dim() == 5\n        assert encoder.get_output_dim() == 18\n        assert encoder.is_bidirectional\n\n    def test_stacked_bidirectional_lstm_can_build_from_params_seq2vec(self):\n        params = Params(\n            {\n                ""type"": ""stacked_bidirectional_lstm"",\n                ""input_size"": 5,\n                ""hidden_size"": 9,\n                ""num_layers"": 3,\n            }\n        )\n        encoder = Seq2VecEncoder.from_params(params)\n\n        assert encoder.get_input_dim() == 5\n        assert encoder.get_output_dim() == 18\n\n    def test_stacked_bidirectional_lstm_can_complete_forward_pass_seq2vec(self):\n        params = Params(\n            {\n                ""type"": ""stacked_bidirectional_lstm"",\n                ""input_size"": 3,\n                ""hidden_size"": 9,\n                ""num_layers"": 3,\n            }\n        )\n        encoder = Seq2VecEncoder.from_params(params)\n        input_tensor = torch.rand(4, 5, 3)\n        mask = torch.ones(4, 5).bool()\n        output = encoder(input_tensor, mask)\n        assert output.detach().numpy().shape == (4, 18)\n\n    @pytest.mark.parametrize(\n        ""dropout_name"", (""layer_dropout_probability"", ""recurrent_dropout_probability"")\n    )\n    def test_stacked_bidirectional_lstm_dropout_version_is_different(self, dropout_name: str):\n        stacked_lstm = StackedBidirectionalLstm(input_size=10, hidden_size=11, num_layers=3)\n        if dropout_name == ""layer_dropout_probability"":\n            dropped_stacked_lstm = StackedBidirectionalLstm(\n                input_size=10, hidden_size=11, num_layers=3, layer_dropout_probability=0.9\n            )\n        elif dropout_name == ""recurrent_dropout_probability"":\n            dropped_stacked_lstm = StackedBidirectionalLstm(\n                input_size=10, hidden_size=11, num_layers=3, recurrent_dropout_probability=0.9\n            )\n        else:\n            raise ValueError(""Do not recognise the following dropout name "" f""{dropout_name}"")\n        # Initialize all weights to be == 1.\n        constant_init = Initializer.from_params(Params({""type"": ""constant"", ""val"": 0.5}))\n        initializer = InitializerApplicator([("".*"", constant_init)])\n        initializer(stacked_lstm)\n        initializer(dropped_stacked_lstm)\n\n        initial_state = torch.randn([3, 5, 11])\n        initial_memory = torch.randn([3, 5, 11])\n\n        tensor = torch.rand([5, 7, 10])\n        sequence_lengths = torch.LongTensor([7, 7, 7, 7, 7])\n\n        sorted_tensor, sorted_sequence, _, _ = sort_batch_by_length(tensor, sequence_lengths)\n        lstm_input = pack_padded_sequence(\n            sorted_tensor, sorted_sequence.data.tolist(), batch_first=True\n        )\n\n        stacked_output, stacked_state = stacked_lstm(lstm_input, (initial_state, initial_memory))\n        dropped_output, dropped_state = dropped_stacked_lstm(\n            lstm_input, (initial_state, initial_memory)\n        )\n        dropped_output_sequence, _ = pad_packed_sequence(dropped_output, batch_first=True)\n        stacked_output_sequence, _ = pad_packed_sequence(stacked_output, batch_first=True)\n        if dropout_name == ""layer_dropout_probability"":\n            with pytest.raises(AssertionError):\n                numpy.testing.assert_array_almost_equal(\n                    dropped_output_sequence.data.numpy(),\n                    stacked_output_sequence.data.numpy(),\n                    decimal=4,\n                )\n        if dropout_name == ""recurrent_dropout_probability"":\n            with pytest.raises(AssertionError):\n                numpy.testing.assert_array_almost_equal(\n                    dropped_state[0].data.numpy(), stacked_state[0].data.numpy(), decimal=4\n                )\n            with pytest.raises(AssertionError):\n                numpy.testing.assert_array_almost_equal(\n                    dropped_state[1].data.numpy(), stacked_state[1].data.numpy(), decimal=4\n                )\n'"
tests/modules/stacked_elmo_lstm_test.py,2,"b'import numpy\nimport torch\n\nfrom allennlp.modules.elmo_lstm import ElmoLstm\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestElmoLstmCell(AllenNlpTestCase):\n    def test_elmo_lstm(self):\n        input_tensor = torch.rand(4, 5, 3)\n        input_tensor[1, 4:, :] = 0.0\n        input_tensor[2, 2:, :] = 0.0\n        input_tensor[3, 1:, :] = 0.0\n        mask = torch.ones([4, 5]).bool()\n        mask[1, 4:] = False\n        mask[2, 2:] = False\n        mask[3, 1:] = False\n\n        lstm = ElmoLstm(\n            num_layers=2,\n            input_size=3,\n            hidden_size=5,\n            cell_size=7,\n            memory_cell_clip_value=2,\n            state_projection_clip_value=1,\n        )\n        output_sequence = lstm(input_tensor, mask)\n\n        # Check all the layer outputs are masked properly.\n        numpy.testing.assert_array_equal(output_sequence.data[:, 1, 4:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[:, 2, 2:, :].numpy(), 0.0)\n        numpy.testing.assert_array_equal(output_sequence.data[:, 3, 1:, :].numpy(), 0.0)\n\n        # LSTM state should be (num_layers, batch_size, hidden_size)\n        assert list(lstm._states[0].size()) == [2, 4, 10]\n        # LSTM memory cell should be (num_layers, batch_size, cell_size)\n        assert list((lstm._states[1].size())) == [2, 4, 14]\n'"
tests/modules/time_distributed_test.py,12,"b'from numpy.testing import assert_almost_equal\nfrom overrides import overrides\nimport torch\nfrom torch.nn import Embedding, Module, Parameter\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules import TimeDistributed\n\n\nclass TestTimeDistributed(AllenNlpTestCase):\n    def test_time_distributed_reshapes_named_arg_correctly(self):\n        char_embedding = Embedding(2, 2)\n        char_embedding.weight = Parameter(torch.FloatTensor([[0.4, 0.4], [0.5, 0.5]]))\n        distributed_embedding = TimeDistributed(char_embedding)\n        char_input = torch.LongTensor([[[1, 0], [1, 1]]])\n        output = distributed_embedding(char_input)\n        assert_almost_equal(\n            output.data.numpy(), [[[[0.5, 0.5], [0.4, 0.4]], [[0.5, 0.5], [0.5, 0.5]]]]\n        )\n\n    def test_time_distributed_reshapes_positional_kwarg_correctly(self):\n        char_embedding = Embedding(2, 2)\n        char_embedding.weight = Parameter(torch.FloatTensor([[0.4, 0.4], [0.5, 0.5]]))\n        distributed_embedding = TimeDistributed(char_embedding)\n        char_input = torch.LongTensor([[[1, 0], [1, 1]]])\n        output = distributed_embedding(input=char_input)\n        assert_almost_equal(\n            output.data.numpy(), [[[[0.5, 0.5], [0.4, 0.4]], [[0.5, 0.5], [0.5, 0.5]]]]\n        )\n\n    def test_time_distributed_works_with_multiple_inputs(self):\n        module = lambda x, y: x + y\n        distributed = TimeDistributed(module)\n        x_input = torch.LongTensor([[[1, 2], [3, 4]]])\n        y_input = torch.LongTensor([[[4, 2], [9, 1]]])\n        output = distributed(x_input, y_input)\n        assert_almost_equal(output.data.numpy(), [[[5, 4], [12, 5]]])\n\n    def test_time_distributed_reshapes_multiple_inputs_with_pass_through_tensor_correctly(self):\n        class FakeModule(Module):\n            @overrides\n            def forward(self, input_tensor, tensor_to_pass_through=None, another_tensor=None):\n\n                return input_tensor + tensor_to_pass_through + another_tensor\n\n        module = FakeModule()\n        distributed_module = TimeDistributed(module)\n\n        input_tensor1 = torch.LongTensor([[[1, 2], [3, 4]]])\n        input_to_pass_through = torch.LongTensor([3, 7])\n        input_tensor2 = torch.LongTensor([[[4, 2], [9, 1]]])\n\n        output = distributed_module(\n            input_tensor1,\n            tensor_to_pass_through=input_to_pass_through,\n            another_tensor=input_tensor2,\n            pass_through=[""tensor_to_pass_through""],\n        )\n        assert_almost_equal(output.data.numpy(), [[[8, 11], [15, 12]]])\n\n    def test_time_distributed_reshapes_multiple_inputs_with_pass_through_non_tensor_correctly(self):\n        class FakeModule(Module):\n            @overrides\n            def forward(self, input_tensor, number=0, another_tensor=None):\n\n                return input_tensor + number + another_tensor\n\n        module = FakeModule()\n        distributed_module = TimeDistributed(module)\n\n        input_tensor1 = torch.LongTensor([[[1, 2], [3, 4]]])\n        input_number = 5\n        input_tensor2 = torch.LongTensor([[[4, 2], [9, 1]]])\n\n        output = distributed_module(\n            input_tensor1,\n            number=input_number,\n            another_tensor=input_tensor2,\n            pass_through=[""number""],\n        )\n        assert_almost_equal(output.data.numpy(), [[[10, 9], [17, 10]]])\n'"
tests/nn/__init__.py,0,b''
tests/nn/beam_search_test.py,10,"b'from typing import Dict, Tuple\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.nn.beam_search import BeamSearch\n\n\ntransition_probabilities = torch.tensor(\n    [\n        [0.0, 0.4, 0.3, 0.2, 0.1, 0.0],  # start token -> jth token\n        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],  # 1st token -> jth token\n        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],  # 2nd token -> jth token\n        [0.0, 0.0, 0.0, 0.0, 1.0, 0.0],  # ...\n        [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # ...\n        [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n    ]  # end token -> jth token\n)\n\n\ndef take_step(\n    last_predictions: torch.Tensor, state: Dict[str, torch.Tensor], step: int\n) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    """"""\n    Take decoding step.\n\n    This is a simple function that defines how probabilities are computed for the\n    next time step during the beam search.\n\n    We use a simple target vocabulary of size 6. In this vocabulary, index 0 represents\n    the start token, and index 5 represents the end token. The transition probability\n    from a state where the last predicted token was token `j` to new token `i` is\n    given by the `(i, j)` element of the matrix `transition_probabilities`.\n    """"""\n    log_probs_list = []\n    for last_token in last_predictions:\n        log_probs = torch.log(transition_probabilities[last_token.item()])\n        log_probs_list.append(log_probs)\n\n    return torch.stack(log_probs_list), state\n\n\nclass BeamSearchTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.end_index = transition_probabilities.size()[0] - 1\n        self.beam_search = BeamSearch(self.end_index, max_steps=10, beam_size=3)\n\n        # This is what the top k should look like for each item in the batch.\n        self.expected_top_k = np.array([[1, 2, 3, 4, 5], [2, 3, 4, 5, 5], [3, 4, 5, 5, 5]])\n\n        # This is what the log probs should look like for each item in the batch.\n        self.expected_log_probs = np.log(np.array([0.4, 0.3, 0.2]))\n\n    def _check_results(\n        self,\n        batch_size: int = 5,\n        expected_top_k: np.array = None,\n        expected_log_probs: np.array = None,\n        beam_search: BeamSearch = None,\n        state: Dict[str, torch.Tensor] = None,\n    ) -> None:\n        expected_top_k = expected_top_k if expected_top_k is not None else self.expected_top_k\n        expected_log_probs = (\n            expected_log_probs if expected_log_probs is not None else self.expected_log_probs\n        )\n        state = state or {}\n\n        beam_search = beam_search or self.beam_search\n        beam_size = beam_search.beam_size\n\n        initial_predictions = torch.tensor([0] * batch_size)\n        top_k, log_probs = beam_search.search(initial_predictions, state, take_step)  # type: ignore\n\n        # top_k should be shape `(batch_size, beam_size, max_predicted_length)`.\n        assert list(top_k.size())[:-1] == [batch_size, beam_size]\n        np.testing.assert_array_equal(top_k[0].numpy(), expected_top_k)\n\n        # log_probs should be shape `(batch_size, beam_size, max_predicted_length)`.\n        assert list(log_probs.size()) == [batch_size, beam_size]\n        np.testing.assert_allclose(log_probs[0].numpy(), expected_log_probs)\n\n    def test_search(self):\n        self._check_results()\n\n    def test_finished_state(self):\n        state = {}\n        state[""foo""] = torch.tensor([[1, 0, 1], [2, 0, 1], [0, 0, 1], [1, 1, 1], [0, 0, 0]])\n        # shape: (batch_size, 3)\n\n        expected_finished_state = {}\n        expected_finished_state[""foo""] = np.array(\n            [\n                [1, 0, 1],\n                [1, 0, 1],\n                [1, 0, 1],\n                [2, 0, 1],\n                [2, 0, 1],\n                [2, 0, 1],\n                [0, 0, 1],\n                [0, 0, 1],\n                [0, 0, 1],\n                [1, 1, 1],\n                [1, 1, 1],\n                [1, 1, 1],\n                [0, 0, 0],\n                [0, 0, 0],\n                [0, 0, 0],\n            ]\n        )\n        # shape: (batch_size x beam_size, 3)\n\n        self._check_results(state=state)\n\n        # check finished state.\n        for key, array in expected_finished_state.items():\n            np.testing.assert_allclose(state[key].numpy(), array)\n\n    def test_batch_size_of_one(self):\n        self._check_results(batch_size=1)\n\n    def test_greedy_search(self):\n        beam_search = BeamSearch(self.end_index, beam_size=1)\n        expected_top_k = np.array([[1, 2, 3, 4, 5]])\n        expected_log_probs = np.log(np.array([0.4]))\n        self._check_results(\n            expected_top_k=expected_top_k,\n            expected_log_probs=expected_log_probs,\n            beam_search=beam_search,\n        )\n\n    def test_early_stopping(self):\n        """"""\n        Checks case where beam search will reach `max_steps` before finding end tokens.\n        """"""\n        beam_search = BeamSearch(self.end_index, beam_size=3, max_steps=3)\n        expected_top_k = np.array([[1, 2, 3], [2, 3, 4], [3, 4, 5]])\n        expected_log_probs = np.log(np.array([0.4, 0.3, 0.2]))\n        self._check_results(\n            expected_top_k=expected_top_k,\n            expected_log_probs=expected_log_probs,\n            beam_search=beam_search,\n        )\n\n    def test_different_per_node_beam_size(self):\n        # per_node_beam_size = 1\n        beam_search = BeamSearch(self.end_index, beam_size=3, per_node_beam_size=1)\n        self._check_results(beam_search=beam_search)\n\n        # per_node_beam_size = 2\n        beam_search = BeamSearch(self.end_index, beam_size=3, per_node_beam_size=2)\n        self._check_results(beam_search=beam_search)\n\n    def test_catch_bad_config(self):\n        """"""\n        If `per_node_beam_size` (which defaults to `beam_size`) is larger than\n        the size of the target vocabulary, `BeamSearch.search` should raise\n        a ConfigurationError.\n        """"""\n        beam_search = BeamSearch(self.end_index, beam_size=20)\n        with pytest.raises(ConfigurationError):\n            self._check_results(beam_search=beam_search)\n\n    def test_warn_for_bad_log_probs(self):\n        # The only valid next step from the initial predictions is the end index.\n        # But with a beam size of 3, the call to `topk` to find the 3 most likely\n        # next beams will result in 2 new beams that are invalid, in that have probability of 0.\n        # The beam search should warn us of this.\n        initial_predictions = torch.LongTensor([self.end_index - 1, self.end_index - 1])\n        with pytest.warns(RuntimeWarning, match=""Infinite log probabilities""):\n            self.beam_search.search(initial_predictions, {}, take_step)\n\n    def test_empty_sequences(self):\n        initial_predictions = torch.LongTensor([self.end_index - 1, self.end_index - 1])\n        beam_search = BeamSearch(self.end_index, beam_size=1)\n        with pytest.warns(RuntimeWarning, match=""Empty sequences predicted""):\n            predictions, log_probs = beam_search.search(initial_predictions, {}, take_step)\n        # predictions hould have shape `(batch_size, beam_size, max_predicted_length)`.\n        assert list(predictions.size()) == [2, 1, 1]\n        # log probs hould have shape `(batch_size, beam_size)`.\n        assert list(log_probs.size()) == [2, 1]\n        assert (predictions == self.end_index).all()\n        assert (log_probs == 0).all()\n'"
tests/nn/chu_liu_edmonds_test.py,1,"b""import numpy\nimport pytest\nimport torch\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.nn.chu_liu_edmonds import _find_cycle, decode_mst\n\n\nclass ChuLiuEdmondsTest(AllenNlpTestCase):\n    def test_find_cycle(self):\n        # No cycle\n        parents = [0, 2, 3, 0, 3]\n        current_nodes = [True for _ in range(5)]\n        has_cycle, cycle = _find_cycle(parents, 5, current_nodes)\n        assert not has_cycle\n        assert not cycle\n\n        # Cycle\n        parents = [0, 2, 3, 1, 3]\n        has_cycle, cycle = _find_cycle(parents, 5, current_nodes)\n        assert has_cycle\n        assert cycle == [1, 2, 3]\n\n        # No cycle if ignored nodes are correctly ignored.\n        parents = [-1, 0, 1, 4, 3]\n        current_nodes = [True for _ in range(5)]\n        current_nodes[4] = False\n        current_nodes[3] = False\n        has_cycle, cycle = _find_cycle(parents, 5, current_nodes)\n        assert not has_cycle\n        assert cycle == []\n\n        # Cycle, but excluding ignored nodes which form their own cycle.\n        parents = [-1, 2, 1, 4, 3]\n        current_nodes = [True for _ in range(5)]\n        current_nodes[1] = False\n        current_nodes[2] = False\n        has_cycle, cycle = _find_cycle(parents, 5, current_nodes)\n        assert has_cycle\n        assert cycle == [3, 4]\n\n    def test_mst(self):\n        # First, test some random cases as sanity checks.\n        # No label case\n        energy = numpy.random.rand(5, 5)\n        heads, types = decode_mst(energy, 5, has_labels=False)\n        assert not _find_cycle(heads, 5, [True] * 5)[0]\n\n        # Labeled case\n        energy = numpy.random.rand(3, 5, 5)\n        heads, types = decode_mst(energy, 5)\n\n        assert not _find_cycle(heads, 5, [True] * 5)[0]\n        label_id_matrix = energy.argmax(axis=0)\n\n        # Check that the labels correspond to the\n        # argmax of the labels for the arcs.\n        for child, parent in enumerate(heads):\n            # The first index corresponds to the symbolic\n            # head token, which won't necessarily have an\n            # argmax type.\n            if child == 0:\n                continue\n            assert types[child] == label_id_matrix[parent, child]\n\n        # Check wrong dimensions throw errors\n        with pytest.raises(ConfigurationError):\n            energy = numpy.random.rand(5, 5)\n            decode_mst(energy, 5, has_labels=True)\n\n        with pytest.raises(ConfigurationError):\n            energy = numpy.random.rand(3, 5, 5)\n            decode_mst(energy, 5, has_labels=False)\n\n    def test_mst_finds_maximum_spanning_tree(self):\n        energy = torch.arange(1, 10).view(1, 3, 3)\n        heads, _ = decode_mst(energy.numpy(), 3)\n        assert list(heads) == [-1, 2, 0]\n"""
tests/nn/initializers_test.py,17,"b'import json\nimport logging\nimport math\n\nimport numpy\nimport pytest\nimport torch\nimport _jsonnet\n\nfrom allennlp.nn import InitializerApplicator, Initializer\nfrom allennlp.nn.initializers import block_orthogonal, uniform_unit_scaling\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.params import Params\n\n\nclass TestInitializers(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        logging.getLogger(""allennlp.nn.initializers"").disabled = False\n\n    def tearDown(self):\n        super().tearDown()\n        logging.getLogger(""allennlp.nn.initializers"").disabled = True\n\n    def test_from_params_string(self):\n        Initializer.from_params(params=""eye"")\n\n    def test_from_params_none(self):\n        Initializer.from_params(params=None)\n\n    def test_regex_matches_are_initialized_correctly(self):\n        class Net(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear_1_with_funky_name = torch.nn.Linear(5, 10)\n                self.linear_2 = torch.nn.Linear(10, 5)\n                self.conv = torch.nn.Conv1d(5, 5, 5)\n\n            def forward(self, inputs):\n                pass\n\n        # Make sure we handle regexes properly\n        json_params = """"""{""initializer"": {""regexes"": [\n        [""conv"", {""type"": ""constant"", ""val"": 5}],\n        [""funky_na.*bi"", {""type"": ""constant"", ""val"": 7}]\n        ]}}\n        """"""\n        params = Params(json.loads(_jsonnet.evaluate_snippet("""", json_params)))\n        initializers = InitializerApplicator.from_params(params=params[""initializer""])\n        model = Net()\n        initializers(model)\n\n        for parameter in model.conv.parameters():\n            assert torch.equal(parameter.data, torch.ones(parameter.size()) * 5)\n\n        parameter = model.linear_1_with_funky_name.bias\n        assert torch.equal(parameter.data, torch.ones(parameter.size()) * 7)\n\n    def test_block_orthogonal_can_initialize(self):\n        tensor = torch.zeros([10, 6])\n        block_orthogonal(tensor, [5, 3])\n        tensor = tensor.data.numpy()\n\n        def test_block_is_orthogonal(block) -> None:\n            matrix_product = block.T @ block\n            numpy.testing.assert_array_almost_equal(\n                matrix_product, numpy.eye(matrix_product.shape[-1]), 6\n            )\n\n        test_block_is_orthogonal(tensor[:5, :3])\n        test_block_is_orthogonal(tensor[:5, 3:])\n        test_block_is_orthogonal(tensor[5:, 3:])\n        test_block_is_orthogonal(tensor[5:, :3])\n\n    def test_block_orthogonal_raises_on_mismatching_dimensions(self):\n        tensor = torch.zeros([10, 6, 8])\n        with pytest.raises(ConfigurationError):\n            block_orthogonal(tensor, [7, 2, 1])\n\n    def test_uniform_unit_scaling_can_initialize(self):\n        tensor = torch.zeros([10, 6])\n        uniform_unit_scaling(tensor, ""linear"")\n\n        assert tensor.data.max() < math.sqrt(3 / 10)\n        assert tensor.data.min() > -math.sqrt(3 / 10)\n\n        # Check that it gets the scaling correct for relu (1.43).\n        uniform_unit_scaling(tensor, ""relu"")\n        assert tensor.data.max() < math.sqrt(3 / 10) * 1.43\n        assert tensor.data.min() > -math.sqrt(3 / 10) * 1.43\n\n    def test_regex_match_prevention_prevents_and_overrides(self):\n        class Net(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear_1 = torch.nn.Linear(5, 10)\n                self.linear_2 = torch.nn.Linear(10, 5)\n                # typical actual usage: modules loaded from allenlp.model.load(..)\n                self.linear_3_transfer = torch.nn.Linear(5, 10)\n                self.linear_4_transfer = torch.nn.Linear(10, 5)\n                self.pretrained_conv = torch.nn.Conv1d(5, 5, 5)\n\n            def forward(self, inputs):\n                pass\n\n        json_params = """"""{""initializer"": {\n        ""regexes"": [\n            ["".*linear.*"", {""type"": ""constant"", ""val"": 10}],\n            ["".*conv.*"", {""type"": ""constant"", ""val"": 10}]\n            ],\n        ""prevent_regexes"": ["".*_transfer.*"", "".*pretrained.*""]\n        }}\n        """"""\n        params = Params(json.loads(_jsonnet.evaluate_snippet("""", json_params)))\n        initializers = InitializerApplicator.from_params(params=params[""initializer""])\n        model = Net()\n        initializers(model)\n\n        for module in [model.linear_1, model.linear_2]:\n            for parameter in module.parameters():\n                assert torch.equal(parameter.data, torch.ones(parameter.size()) * 10)\n\n        transfered_modules = [\n            model.linear_3_transfer,\n            model.linear_4_transfer,\n            model.pretrained_conv,\n        ]\n\n        for module in transfered_modules:\n            for parameter in module.parameters():\n                assert not torch.equal(parameter.data, torch.ones(parameter.size()) * 10)\n'"
tests/nn/pretrained_model_initializer_test.py,14,"b'from typing import Dict, Optional\n\nimport pytest\nimport torch\n\nfrom allennlp.nn import InitializerApplicator, Initializer\nfrom allennlp.nn.initializers import PretrainedModelInitializer\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.params import Params\n\n\nclass _Net1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(5, 10)\n        self.linear_2 = torch.nn.Linear(10, 5)\n\n    def forward(self, inputs):\n        pass\n\n\nclass _Net2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(5, 10)\n        self.linear_3 = torch.nn.Linear(10, 5)\n\n    def forward(self, inputs):\n        pass\n\n\nclass TestPretrainedModelInitializer(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.net1 = _Net1()\n        self.net2 = _Net2()\n        self.temp_file = self.TEST_DIR / ""weights.th""\n        torch.save(self.net2.state_dict(), self.temp_file)\n\n    def _are_equal(self, linear1: torch.nn.Linear, linear2: torch.nn.Linear) -> bool:\n        return torch.equal(linear1.weight, linear2.weight) and torch.equal(\n            linear1.bias, linear2.bias\n        )\n\n    def _get_applicator(\n        self,\n        regex: str,\n        weights_file_path: str,\n        parameter_name_overrides: Optional[Dict[str, str]] = None,\n    ) -> InitializerApplicator:\n        initializer = PretrainedModelInitializer(weights_file_path, parameter_name_overrides)\n        return InitializerApplicator([(regex, initializer)])\n\n    def test_random_initialization(self):\n        # The tests in the class rely on the fact that the parameters for\n        # `self.net1` and `self.net2` are randomly initialized and not\n        # equal at the beginning. This test makes sure that\'s true\n        assert not self._are_equal(self.net1.linear_1, self.net2.linear_1)\n        assert not self._are_equal(self.net1.linear_2, self.net2.linear_3)\n\n    def test_from_params(self):\n        params = Params({""type"": ""pretrained"", ""weights_file_path"": self.temp_file})\n        initializer = Initializer.from_params(params)\n        assert initializer.weights\n        assert initializer.parameter_name_overrides == {}\n\n        name_overrides = {""a"": ""b"", ""c"": ""d""}\n        params = Params(\n            {\n                ""type"": ""pretrained"",\n                ""weights_file_path"": self.temp_file,\n                ""parameter_name_overrides"": name_overrides,\n            }\n        )\n        initializer = Initializer.from_params(params)\n        assert initializer.weights\n        assert initializer.parameter_name_overrides == name_overrides\n\n    def test_default_parameter_names(self):\n        # This test initializes net1 to net2\'s parameters. It doesn\'t use\n        # the parameter name overrides, so it will verify the initialization\n        # works if the two parameters\' names are the same.\n        applicator = self._get_applicator(""linear_1.weight|linear_1.bias"", self.temp_file)\n        applicator(self.net1)\n        assert self._are_equal(self.net1.linear_1, self.net2.linear_1)\n        assert not self._are_equal(self.net1.linear_2, self.net2.linear_3)\n\n    def test_parameter_name_overrides(self):\n        # This test will use the parameter name overrides to initialize all\n        # of net1\'s weights to net2\'s.\n        name_overrides = {""linear_2.weight"": ""linear_3.weight"", ""linear_2.bias"": ""linear_3.bias""}\n        applicator = self._get_applicator(""linear_*"", self.temp_file, name_overrides)\n        applicator(self.net1)\n        assert self._are_equal(self.net1.linear_1, self.net2.linear_1)\n        assert self._are_equal(self.net1.linear_2, self.net2.linear_3)\n\n    def test_size_mismatch(self):\n        # This test will verify that an exception is raised when you try\n        # to initialize a parameter to a pretrained parameter and they have\n        # different sizes\n        name_overrides = {""linear_1.weight"": ""linear_3.weight""}\n        applicator = self._get_applicator(""linear_1.*"", self.temp_file, name_overrides)\n        with pytest.raises(ConfigurationError):\n            applicator(self.net1)\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""No CUDA device registered."")\n    def test_load_to_gpu_from_gpu(self):\n        # This test will make sure that the initializer works on the GPU\n        self.net1.cuda(device=0)\n        self.net2.cuda(device=0)\n\n        # Verify the parameters are on the GPU\n        assert self.net1.linear_1.weight.is_cuda is True\n        assert self.net1.linear_1.bias.is_cuda is True\n        assert self.net2.linear_1.weight.is_cuda is True\n        assert self.net2.linear_1.bias.is_cuda is True\n\n        # We need to manually save the parameters to a file because setup_method()\n        # only does it for the CPU\n        temp_file = self.TEST_DIR / ""gpu_weights.th""\n        torch.save(self.net2.state_dict(), temp_file)\n\n        applicator = self._get_applicator(""linear_1.*"", temp_file)\n        applicator(self.net1)\n\n        # Verify the parameters are still on the GPU\n        assert self.net1.linear_1.weight.is_cuda is True\n        assert self.net1.linear_1.bias.is_cuda is True\n        assert self.net2.linear_1.weight.is_cuda is True\n        assert self.net2.linear_1.bias.is_cuda is True\n\n        # Make sure the weights are identical\n        assert self._are_equal(self.net1.linear_1, self.net2.linear_1)\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""No CUDA device registered."")\n    def test_load_to_cpu_from_gpu(self):\n        # This test will load net2\'s parameters onto the GPU, then use them to\n        # initialize net1 on the CPU\n        self.net2.cuda(device=0)\n\n        # Verify the parameters are on the GPU\n        assert self.net2.linear_1.weight.is_cuda is True\n        assert self.net2.linear_1.bias.is_cuda is True\n\n        temp_file = self.TEST_DIR / ""gpu_weights.th""\n        torch.save(self.net2.state_dict(), temp_file)\n\n        applicator = self._get_applicator(""linear_1.*"", temp_file)\n        applicator(self.net1)\n\n        # Verify the parameters are on the CPU\n        assert self.net1.linear_1.weight.is_cuda is False\n        assert self.net1.linear_1.bias.is_cuda is False\n\n        # Make sure the weights are identical\n        assert self._are_equal(self.net1.linear_1, self.net2.linear_1.cpu())\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""No CUDA device registered."")\n    def test_load_to_gpu_from_cpu(self):\n        # This test will load net1\'s parameters onto the GPU, then use net2\'s\n        # on the CPU to initialize net1\'s parameters.\n        self.net1.cuda(device=0)\n\n        # Verify the parameters are on the GPU\n        assert self.net1.linear_1.weight.is_cuda is True\n        assert self.net1.linear_1.bias.is_cuda is True\n\n        # net2\'s parameters are already saved to CPU from setup_method()\n        applicator = self._get_applicator(""linear_1.*"", self.temp_file)\n        applicator(self.net1)\n\n        # Verify the parameters are on the GPU\n        assert self.net1.linear_1.weight.is_cuda is True\n        assert self.net1.linear_1.bias.is_cuda is True\n\n        # Make sure the weights are identical\n        assert self._are_equal(self.net1.linear_1.cpu(), self.net2.linear_1)\n'"
tests/nn/regularizers_test.py,4,"b'import re\nimport torch\n\nfrom allennlp.common.params import Params\nfrom allennlp.nn import InitializerApplicator, Initializer\nfrom allennlp.nn.regularizers import L1Regularizer, L2Regularizer, RegularizerApplicator\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestRegularizers(AllenNlpTestCase):\n    def test_l1_regularization(self):\n        model = torch.nn.Sequential(torch.nn.Linear(5, 10), torch.nn.Linear(10, 5))\n        constant_init = Initializer.from_params(Params({""type"": ""constant"", ""val"": -1}))\n        initializer = InitializerApplicator([("".*"", constant_init)])\n        initializer(model)\n        value = RegularizerApplicator([("""", L1Regularizer(1.0))])(model)\n        # 115 because of biases.\n        assert value.data.numpy() == 115.0\n\n    def test_l2_regularization(self):\n        model = torch.nn.Sequential(torch.nn.Linear(5, 10), torch.nn.Linear(10, 5))\n        constant_init = Initializer.from_params(Params({""type"": ""constant"", ""val"": 0.5}))\n        initializer = InitializerApplicator([("".*"", constant_init)])\n        initializer(model)\n        value = RegularizerApplicator([("""", L2Regularizer(1.0))])(model)\n        assert value.data.numpy() == 28.75\n\n    def test_regularizer_applicator_respects_regex_matching(self):\n        model = torch.nn.Sequential(torch.nn.Linear(5, 10), torch.nn.Linear(10, 5))\n        constant_init = Initializer.from_params(Params({""type"": ""constant"", ""val"": 1.0}))\n        initializer = InitializerApplicator([("".*"", constant_init)])\n        initializer(model)\n        value = RegularizerApplicator(\n            [(""weight"", L2Regularizer(0.5)), (""bias"", L1Regularizer(1.0))]\n        )(model)\n        assert value.data.numpy() == 65.0\n\n    def test_from_params(self):\n        params = Params({""regexes"": [(""conv"", ""l1""), (""linear"", {""type"": ""l2"", ""alpha"": 10})]})\n        regularizer_applicator = RegularizerApplicator.from_params(params)\n        regularizers = regularizer_applicator._regularizers\n\n        conv = linear = None\n        for regex, regularizer in regularizers:\n            if regex == ""conv"":\n                conv = regularizer\n            elif regex == ""linear"":\n                linear = regularizer\n\n        assert isinstance(conv, L1Regularizer)\n        assert isinstance(linear, L2Regularizer)\n        assert linear.alpha == 10\n\n    def test_frozen_params(self):\n        model = torch.nn.Sequential(torch.nn.Linear(5, 10), torch.nn.Linear(10, 5))\n        constant_init = Initializer.from_params(Params({""type"": ""constant"", ""val"": -1}))\n        initializer = InitializerApplicator([("".*"", constant_init)])\n        initializer(model)\n        # freeze the parameters of the first linear\n        for name, param in model.named_parameters():\n            if re.search(r""0.*$"", name):\n                param.requires_grad = False\n        value = RegularizerApplicator([("""", L1Regularizer(1.0))])(model)\n        # 55 because of bias (5*10 + 5)\n        assert value.data.numpy() == 55\n'"
tests/nn/util_test.py,248,"b'import json\nimport random\nfrom typing import NamedTuple, Any\n\nimport numpy\nfrom numpy.testing import assert_array_almost_equal, assert_almost_equal\nimport torch\nimport pytest\nfrom flaky import flaky\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.util import sanitize\nfrom allennlp.data import Token, Vocabulary\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.token_indexers import (\n    ELMoTokenCharactersIndexer,\n    TokenCharactersIndexer,\n    SingleIdTokenIndexer,\n)\nfrom allennlp.nn import util\nfrom allennlp.models import load_archive\n\n\nclass TestNnUtil(AllenNlpTestCase):\n    def test_get_sequence_lengths_from_binary_mask(self):\n        binary_mask = torch.tensor(\n            [\n                [True, True, True, False, False, False],\n                [True, True, False, False, False, False],\n                [True, True, True, True, True, True],\n                [True, False, False, False, False, False],\n            ]\n        )\n        lengths = util.get_lengths_from_binary_sequence_mask(binary_mask)\n        numpy.testing.assert_array_equal(lengths.numpy(), numpy.array([3, 2, 6, 1]))\n\n    def test_get_mask_from_sequence_lengths(self):\n        sequence_lengths = torch.LongTensor([4, 3, 1, 4, 2])\n        mask = util.get_mask_from_sequence_lengths(sequence_lengths, 5).data.numpy()\n        assert_almost_equal(\n            mask,\n            [[1, 1, 1, 1, 0], [1, 1, 1, 0, 0], [1, 0, 0, 0, 0], [1, 1, 1, 1, 0], [1, 1, 0, 0, 0]],\n        )\n\n    def test_get_sequence_lengths_converts_to_long_tensor_and_avoids_variable_overflow(self):\n        # Tests the following weird behaviour in Pytorch 0.1.12\n        # doesn\'t happen for our sequence masks:\n        #\n        # mask = torch.ones([260]).bool()\n        # mask.sum() # equals 260.\n        # var_mask = t.a.V(mask)\n        # var_mask.sum() # equals 4, due to 8 bit precision - the sum overflows.\n        binary_mask = torch.ones(2, 260).bool()\n        lengths = util.get_lengths_from_binary_sequence_mask(binary_mask)\n        numpy.testing.assert_array_equal(lengths.data.numpy(), numpy.array([260, 260]))\n\n    def test_clamp_tensor(self):\n        # Test on uncoalesced sparse tensor\n        i = torch.LongTensor([[0, 1, 1, 0], [2, 0, 2, 2]])\n        v = torch.FloatTensor([3, 4, -5, 3])\n        tensor = torch.sparse.FloatTensor(i, v, torch.Size([2, 3]))\n        clamped_tensor = util.clamp_tensor(tensor, minimum=-3, maximum=3).to_dense()\n        assert_almost_equal(clamped_tensor, [[0, 0, 3], [3, 0, -3]])\n\n        # Test on coalesced sparse tensor\n        i = torch.LongTensor([[0, 1, 1], [2, 0, 2]])\n        v = torch.FloatTensor([3, 4, -5])\n        tensor = torch.sparse.FloatTensor(i, v, torch.Size([2, 3]))\n        clamped_tensor = util.clamp_tensor(tensor, minimum=-3, maximum=3).to_dense()\n        assert_almost_equal(clamped_tensor, [[0, 0, 3], [3, 0, -3]])\n\n        # Test on dense tensor\n        tensor = torch.tensor([[5, -4, 3], [-3, 0, -30]])\n        clamped_tensor = util.clamp_tensor(tensor, minimum=-3, maximum=3)\n        assert_almost_equal(clamped_tensor, [[3, -3, 3], [-3, 0, -3]])\n\n    def test_sort_tensor_by_length(self):\n        tensor = torch.rand([5, 7, 9])\n        tensor[0, 3:, :] = 0\n        tensor[1, 4:, :] = 0\n        tensor[2, 1:, :] = 0\n        tensor[3, 5:, :] = 0\n\n        sequence_lengths = torch.LongTensor([3, 4, 1, 5, 7])\n        sorted_tensor, sorted_lengths, reverse_indices, _ = util.sort_batch_by_length(\n            tensor, sequence_lengths\n        )\n\n        # Test sorted indices are padded correctly.\n        numpy.testing.assert_array_equal(sorted_tensor[1, 5:, :].data.numpy(), 0.0)\n        numpy.testing.assert_array_equal(sorted_tensor[2, 4:, :].data.numpy(), 0.0)\n        numpy.testing.assert_array_equal(sorted_tensor[3, 3:, :].data.numpy(), 0.0)\n        numpy.testing.assert_array_equal(sorted_tensor[4, 1:, :].data.numpy(), 0.0)\n\n        assert sorted_lengths.data.equal(torch.LongTensor([7, 5, 4, 3, 1]))\n\n        # Test restoration indices correctly recover the original tensor.\n        assert sorted_tensor.index_select(0, reverse_indices).data.equal(tensor.data)\n\n    def test_get_final_encoder_states(self):\n        encoder_outputs = torch.Tensor(\n            [\n                [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n                [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]],\n            ]\n        )\n        mask = torch.tensor([[True, True, True], [True, True, False]])\n        final_states = util.get_final_encoder_states(encoder_outputs, mask, bidirectional=False)\n        assert_almost_equal(final_states.data.numpy(), [[9, 10, 11, 12], [17, 18, 19, 20]])\n        final_states = util.get_final_encoder_states(encoder_outputs, mask, bidirectional=True)\n        assert_almost_equal(final_states.data.numpy(), [[9, 10, 3, 4], [17, 18, 15, 16]])\n\n    def test_masked_softmax_no_mask(self):\n        # Testing the general unmasked 1D case.\n        vector_1d = torch.FloatTensor([[1.0, 2.0, 3.0]])\n        vector_1d_softmaxed = util.masked_softmax(vector_1d, None).data.numpy()\n        assert_array_almost_equal(\n            vector_1d_softmaxed, numpy.array([[0.090031, 0.244728, 0.665241]])\n        )\n        assert_almost_equal(1.0, numpy.sum(vector_1d_softmaxed), decimal=6)\n\n        vector_1d = torch.FloatTensor([[1.0, 2.0, 5.0]])\n        vector_1d_softmaxed = util.masked_softmax(vector_1d, None).data.numpy()\n        assert_array_almost_equal(vector_1d_softmaxed, numpy.array([[0.017148, 0.046613, 0.93624]]))\n\n        # Testing the unmasked 1D case where the input is all 0s.\n        vector_zero = torch.FloatTensor([[0.0, 0.0, 0.0]])\n        vector_zero_softmaxed = util.masked_softmax(vector_zero, None).data.numpy()\n        assert_array_almost_equal(\n            vector_zero_softmaxed, numpy.array([[0.33333334, 0.33333334, 0.33333334]])\n        )\n\n        # Testing the general unmasked batched case.\n        matrix = torch.FloatTensor([[1.0, 2.0, 5.0], [1.0, 2.0, 3.0]])\n        masked_matrix_softmaxed = util.masked_softmax(matrix, None).data.numpy()\n        assert_array_almost_equal(\n            masked_matrix_softmaxed,\n            numpy.array(\n                [[0.01714783, 0.04661262, 0.93623955], [0.09003057, 0.24472847, 0.66524096]]\n            ),\n        )\n\n        # Testing the unmasked batched case where one of the inputs are all 0s.\n        matrix = torch.FloatTensor([[1.0, 2.0, 5.0], [0.0, 0.0, 0.0]])\n        masked_matrix_softmaxed = util.masked_softmax(matrix, None).data.numpy()\n        assert_array_almost_equal(\n            masked_matrix_softmaxed,\n            numpy.array(\n                [[0.01714783, 0.04661262, 0.93623955], [0.33333334, 0.33333334, 0.33333334]]\n            ),\n        )\n\n    def test_masked_softmax_masked(self):\n        # Testing the general masked 1D case.\n        vector_1d = torch.FloatTensor([[1.0, 2.0, 5.0]])\n        mask_1d = torch.tensor([[True, False, True]])\n        vector_1d_softmaxed = util.masked_softmax(vector_1d, mask_1d).data.numpy()\n        assert_array_almost_equal(vector_1d_softmaxed, numpy.array([[0.01798621, 0.0, 0.98201382]]))\n\n        vector_1d = torch.FloatTensor([[0.0, 2.0, 3.0, 4.0]])\n        mask_1d = torch.tensor([[True, False, True, True]])\n        vector_1d_softmaxed = util.masked_softmax(vector_1d, mask_1d).data.numpy()\n        assert_array_almost_equal(\n            vector_1d_softmaxed, numpy.array([[0.01321289, 0.0, 0.26538793, 0.72139918]])\n        )\n\n        # Testing the masked 1D case where the input is all 0s and the mask\n        # is not all 0s.\n        vector_1d = torch.FloatTensor([[0.0, 0.0, 0.0, 0.0]])\n        mask_1d = torch.tensor([[False, False, False, True]])\n        vector_1d_softmaxed = util.masked_softmax(vector_1d, mask_1d).data.numpy()\n        assert_array_almost_equal(vector_1d_softmaxed, numpy.array([[0, 0, 0, 1]]))\n\n        # Testing the masked 1D case where the input is not all 0s\n        # and the mask is all 0s.\n        vector_1d = torch.FloatTensor([[0.0, 2.0, 3.0, 4.0]])\n        mask_1d = torch.tensor([[False, False, False, False]])\n        vector_1d_softmaxed = util.masked_softmax(vector_1d, mask_1d).data.numpy()\n        assert_array_almost_equal(vector_1d_softmaxed, numpy.array([[0.0, 0.0, 0.0, 0.0]]))\n\n        # Testing the masked 1D case where the input is all 0s and\n        # the mask is all 0s.\n        vector_1d = torch.FloatTensor([[0.0, 0.0, 0.0, 0.0]])\n        mask_1d = torch.tensor([[False, False, False, False]])\n        vector_1d_softmaxed = util.masked_softmax(vector_1d, mask_1d).data.numpy()\n        assert_array_almost_equal(vector_1d_softmaxed, numpy.array([[0.0, 0.0, 0.0, 0.0]]))\n\n        # Testing the masked 1D case where there are large elements in the\n        # padding.\n        vector_1d = torch.FloatTensor([[1.0, 1.0, 1e5]])\n        mask_1d = torch.tensor([[True, True, False]])\n        vector_1d_softmaxed = util.masked_softmax(vector_1d, mask_1d).data.numpy()\n        assert_array_almost_equal(vector_1d_softmaxed, numpy.array([[0.5, 0.5, 0]]))\n\n        # Testing the general masked batched case.\n        matrix = torch.FloatTensor([[1.0, 2.0, 5.0], [1.0, 2.0, 3.0]])\n        mask = torch.tensor([[True, False, True], [True, True, True]])\n        masked_matrix_softmaxed = util.masked_softmax(matrix, mask).data.numpy()\n        assert_array_almost_equal(\n            masked_matrix_softmaxed,\n            numpy.array([[0.01798621, 0.0, 0.98201382], [0.090031, 0.244728, 0.665241]]),\n        )\n\n        # Testing the masked batch case where one of the inputs is all 0s but\n        # none of the masks are all 0.\n        matrix = torch.FloatTensor([[0.0, 0.0, 0.0], [1.0, 2.0, 3.0]])\n        mask = torch.tensor([[True, False, True], [True, True, True]])\n        masked_matrix_softmaxed = util.masked_softmax(matrix, mask).data.numpy()\n        assert_array_almost_equal(\n            masked_matrix_softmaxed, numpy.array([[0.5, 0.0, 0.5], [0.090031, 0.244728, 0.665241]])\n        )\n\n        # Testing the masked batch case where one of the inputs is all 0s and\n        # one of the masks are all 0.\n        matrix = torch.FloatTensor([[0.0, 0.0, 0.0], [1.0, 2.0, 3.0]])\n        mask = torch.tensor([[True, False, True], [False, False, False]])\n        masked_matrix_softmaxed = util.masked_softmax(matrix, mask).data.numpy()\n        assert_array_almost_equal(\n            masked_matrix_softmaxed, numpy.array([[0.5, 0.0, 0.5], [0.0, 0.0, 0.0]])\n        )\n\n        matrix = torch.FloatTensor([[0.0, 0.0, 0.0], [1.0, 2.0, 3.0]])\n        mask = torch.tensor([[False, False, False], [True, False, True]])\n        masked_matrix_softmaxed = util.masked_softmax(matrix, mask).data.numpy()\n        assert_array_almost_equal(\n            masked_matrix_softmaxed, numpy.array([[0.0, 0.0, 0.0], [0.11920292, 0.0, 0.88079708]])\n        )\n\n    def test_masked_softmax_memory_efficient_masked(self):\n        # Testing the general masked 1D case.\n        vector_1d = torch.FloatTensor([[1.0, 2.0, 5.0]])\n        mask_1d = torch.tensor([[True, False, True]])\n        vector_1d_softmaxed = util.masked_softmax(\n            vector_1d, mask_1d, memory_efficient=True\n        ).data.numpy()\n        assert_array_almost_equal(vector_1d_softmaxed, numpy.array([[0.01798621, 0.0, 0.98201382]]))\n\n        vector_1d = torch.FloatTensor([[0.0, 2.0, 3.0, 4.0]])\n        mask_1d = torch.tensor([[True, False, True, True]])\n        vector_1d_softmaxed = util.masked_softmax(\n            vector_1d, mask_1d, memory_efficient=True\n        ).data.numpy()\n        assert_array_almost_equal(\n            vector_1d_softmaxed, numpy.array([[0.01321289, 0.0, 0.26538793, 0.72139918]])\n        )\n\n        # Testing the masked 1D case where the input is all 0s and the mask\n        # is not all 0s.\n        vector_1d = torch.FloatTensor([[0.0, 0.0, 0.0, 0.0]])\n        mask_1d = torch.tensor([[False, False, False, True]])\n        vector_1d_softmaxed = util.masked_softmax(\n            vector_1d, mask_1d, memory_efficient=True\n        ).data.numpy()\n        assert_array_almost_equal(vector_1d_softmaxed, numpy.array([[0, 0, 0, 1]]))\n\n        # Testing the masked 1D case where the input is not all 0s\n        # and the mask is all 0s.\n        vector_1d = torch.FloatTensor([[0.0, 2.0, 3.0, 4.0]])\n        mask_1d = torch.tensor([[False, False, False, False]])\n        vector_1d_softmaxed = util.masked_softmax(\n            vector_1d, mask_1d, memory_efficient=True\n        ).data.numpy()\n        assert_array_almost_equal(vector_1d_softmaxed, numpy.array([[0.25, 0.25, 0.25, 0.25]]))\n\n        # Testing the masked 1D case where the input is all 0s and\n        # the mask is all 0s.\n        vector_1d = torch.FloatTensor([[0.0, 0.0, 0.0, 0.0]])\n        mask_1d = torch.tensor([[False, False, False, False]])\n        vector_1d_softmaxed = util.masked_softmax(\n            vector_1d, mask_1d, memory_efficient=True\n        ).data.numpy()\n        assert_array_almost_equal(vector_1d_softmaxed, numpy.array([[0.25, 0.25, 0.25, 0.25]]))\n\n        # Testing the masked 1D case where there are large elements in the\n        # padding.\n        vector_1d = torch.FloatTensor([[1.0, 1.0, 1e5]])\n        mask_1d = torch.tensor([[True, True, False]])\n        vector_1d_softmaxed = util.masked_softmax(\n            vector_1d, mask_1d, memory_efficient=True\n        ).data.numpy()\n        assert_array_almost_equal(vector_1d_softmaxed, numpy.array([[0.5, 0.5, 0]]))\n\n        # Testing the general masked batched case.\n        matrix = torch.FloatTensor([[1.0, 2.0, 5.0], [1.0, 2.0, 3.0]])\n        mask = torch.tensor([[True, False, True], [True, True, True]])\n        masked_matrix_softmaxed = util.masked_softmax(\n            matrix, mask, memory_efficient=True\n        ).data.numpy()\n        assert_array_almost_equal(\n            masked_matrix_softmaxed,\n            numpy.array([[0.01798621, 0.0, 0.98201382], [0.090031, 0.244728, 0.665241]]),\n        )\n\n        # Testing the masked batch case where one of the inputs is all 0s but\n        # none of the masks are all 0.\n        matrix = torch.FloatTensor([[0.0, 0.0, 0.0], [1.0, 2.0, 3.0]])\n        mask = torch.tensor([[True, False, True], [True, True, True]])\n        masked_matrix_softmaxed = util.masked_softmax(\n            matrix, mask, memory_efficient=True\n        ).data.numpy()\n        assert_array_almost_equal(\n            masked_matrix_softmaxed, numpy.array([[0.5, 0.0, 0.5], [0.090031, 0.244728, 0.665241]])\n        )\n\n        # Testing the masked batch case where one of the inputs is all 0s and\n        # one of the masks are all 0.\n        matrix = torch.FloatTensor([[0.0, 0.0, 0.0], [1.0, 2.0, 3.0]])\n        mask = torch.tensor([[True, False, True], [False, False, False]])\n        masked_matrix_softmaxed = util.masked_softmax(\n            matrix, mask, memory_efficient=True\n        ).data.numpy()\n        assert_array_almost_equal(\n            masked_matrix_softmaxed,\n            numpy.array([[0.5, 0.0, 0.5], [0.33333333, 0.33333333, 0.33333333]]),\n        )\n\n        matrix = torch.FloatTensor([[0.0, 0.0, 0.0], [1.0, 2.0, 3.0]])\n        mask = torch.tensor([[False, False, False], [True, False, True]])\n        masked_matrix_softmaxed = util.masked_softmax(\n            matrix, mask, memory_efficient=True\n        ).data.numpy()\n        assert_array_almost_equal(\n            masked_matrix_softmaxed,\n            numpy.array([[0.33333333, 0.33333333, 0.33333333], [0.11920292, 0.0, 0.88079708]]),\n        )\n\n    def test_masked_log_softmax_masked(self):\n        # Tests replicated from test_softmax_masked - we test that exponentiated,\n        # the log softmax contains the correct elements (masked elements should be == 1).\n\n        # Testing the general masked 1D case.\n        vector_1d = torch.FloatTensor([[1.0, 2.0, 5.0]])\n        mask_1d = torch.tensor([[True, False, True]])\n        vector_1d_softmaxed = util.masked_log_softmax(vector_1d, mask_1d).data.numpy()\n        assert_array_almost_equal(\n            numpy.exp(vector_1d_softmaxed), numpy.array([[0.01798621, 0.0, 0.98201382]])\n        )\n\n        vector_1d = torch.FloatTensor([[0.0, 2.0, 3.0, 4.0]])\n        mask_1d = torch.tensor([[True, False, True, True]])\n        vector_1d_softmaxed = util.masked_log_softmax(vector_1d, mask_1d).data.numpy()\n        assert_array_almost_equal(\n            numpy.exp(vector_1d_softmaxed), numpy.array([[0.01321289, 0.0, 0.26538793, 0.72139918]])\n        )\n\n        # Testing the masked 1D case where the input is all 0s and the mask\n        # is not all 0s.\n        vector_1d = torch.FloatTensor([[0.0, 0.0, 0.0, 0.0]])\n        mask_1d = torch.tensor([[False, False, False, True]])\n        vector_1d_softmaxed = util.masked_log_softmax(vector_1d, mask_1d).data.numpy()\n        assert_array_almost_equal(\n            numpy.exp(vector_1d_softmaxed), numpy.array([[0.0, 0.0, 0.0, 1.0]])\n        )\n\n        # Testing the masked 1D case where the input is not all 0s\n        # and the mask is all 0s.  The output here will be arbitrary, but it should not be nan.\n        vector_1d = torch.FloatTensor([[0.0, 2.0, 3.0, 4.0]])\n        mask_1d = torch.tensor([[False, False, False, False]])\n        vector_1d_softmaxed = util.masked_log_softmax(vector_1d, mask_1d).data.numpy()\n        assert not numpy.isnan(vector_1d_softmaxed).any()\n\n    def test_masked_max(self):\n        # Testing the general masked 1D case.\n        vector_1d = torch.FloatTensor([1.0, 12.0, 5.0])\n        mask_1d = torch.tensor([True, False, True])\n        vector_1d_maxed = util.masked_max(vector_1d, mask_1d, dim=0).data.numpy()\n        assert_array_almost_equal(vector_1d_maxed, 5.0)\n\n        # Testing if all masks are zero, the output will be arbitrary, but it should not be nan.\n        vector_1d = torch.FloatTensor([1.0, 12.0, 5.0])\n        mask_1d = torch.tensor([False, False, False])\n        vector_1d_maxed = util.masked_max(vector_1d, mask_1d, dim=0).data.numpy()\n        assert not numpy.isnan(vector_1d_maxed).any()\n\n        # Testing batch value and batch masks\n        matrix = torch.FloatTensor([[1.0, 12.0, 5.0], [-1.0, -2.0, 3.0]])\n        mask = torch.tensor([[True, False, True], [True, True, False]])\n        matrix_maxed = util.masked_max(matrix, mask, dim=-1).data.numpy()\n        assert_array_almost_equal(matrix_maxed, numpy.array([5.0, -1.0]))\n\n        # Testing keepdim for batch value and batch masks\n        matrix = torch.FloatTensor([[1.0, 12.0, 5.0], [-1.0, -2.0, 3.0]])\n        mask = torch.tensor([[True, False, True], [True, True, False]])\n        matrix_maxed = util.masked_max(matrix, mask, dim=-1, keepdim=True).data.numpy()\n        assert_array_almost_equal(matrix_maxed, numpy.array([[5.0], [-1.0]]))\n\n        # Testing broadcast\n        matrix = torch.FloatTensor(\n            [[[1.0, 2.0], [12.0, 3.0], [5.0, -1.0]], [[-1.0, -3.0], [-2.0, -0.5], [3.0, 8.0]]]\n        )\n        mask = torch.tensor([[True, False, True], [True, True, False]]).unsqueeze(-1)\n        matrix_maxed = util.masked_max(matrix, mask, dim=1).data.numpy()\n        assert_array_almost_equal(matrix_maxed, numpy.array([[5.0, 2.0], [-1.0, -0.5]]))\n\n    def test_masked_mean(self):\n        # Testing the general masked 1D case.\n        vector_1d = torch.FloatTensor([1.0, 12.0, 5.0])\n        mask_1d = torch.tensor([True, False, True])\n        vector_1d_mean = util.masked_mean(vector_1d, mask_1d, dim=0).data.numpy()\n        assert_array_almost_equal(vector_1d_mean, 3.0)\n\n        # Testing if all masks are zero, the output will be arbitrary, but it should not be nan.\n        vector_1d = torch.FloatTensor([1.0, 12.0, 5.0])\n        mask_1d = torch.tensor([False, False, False])\n        vector_1d_mean = util.masked_mean(vector_1d, mask_1d, dim=0).data.numpy()\n        assert not numpy.isnan(vector_1d_mean).any()\n\n        # Testing batch value and batch masks\n        matrix = torch.FloatTensor([[1.0, 12.0, 5.0], [-1.0, -2.0, 3.0]])\n        mask = torch.tensor([[True, False, True], [True, True, False]])\n        matrix_mean = util.masked_mean(matrix, mask, dim=-1).data.numpy()\n        assert_array_almost_equal(matrix_mean, numpy.array([3.0, -1.5]))\n\n        # Testing keepdim for batch value and batch masks\n        matrix = torch.FloatTensor([[1.0, 12.0, 5.0], [-1.0, -2.0, 3.0]])\n        mask = torch.tensor([[True, False, True], [True, True, False]])\n        matrix_mean = util.masked_mean(matrix, mask, dim=-1, keepdim=True).data.numpy()\n        assert_array_almost_equal(matrix_mean, numpy.array([[3.0], [-1.5]]))\n\n        # Testing broadcast\n        matrix = torch.FloatTensor(\n            [[[1.0, 2.0], [12.0, 3.0], [5.0, -1.0]], [[-1.0, -3.0], [-2.0, -0.5], [3.0, 8.0]]]\n        )\n        mask = torch.tensor([[True, False, True], [True, True, False]]).unsqueeze(-1)\n        matrix_mean = util.masked_mean(matrix, mask, dim=1).data.numpy()\n        assert_array_almost_equal(matrix_mean, numpy.array([[3.0, 0.5], [-1.5, -1.75]]))\n\n    def test_masked_flip(self):\n        tensor = torch.FloatTensor(\n            [[[6, 6, 6], [1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4], [5, 5, 5]]]\n        )\n        solution = [[[6, 6, 6], [0, 0, 0]], [[4, 4, 4], [3, 3, 3]]]\n        response = util.masked_flip(tensor, [1, 2])\n        assert_almost_equal(response, solution)\n\n        tensor = torch.FloatTensor(\n            [\n                [[6, 6, 6], [1, 1, 1], [2, 2, 2], [0, 0, 0]],\n                [[3, 3, 3], [4, 4, 4], [5, 5, 5], [1, 2, 3]],\n            ]\n        )\n        solution = [\n            [[2, 2, 2], [1, 1, 1], [6, 6, 6], [0, 0, 0]],\n            [[1, 2, 3], [5, 5, 5], [4, 4, 4], [3, 3, 3]],\n        ]\n        response = util.masked_flip(tensor, [3, 4])\n        assert_almost_equal(response, solution)\n\n        tensor = torch.FloatTensor(\n            [\n                [[6, 6, 6], [1, 1, 1], [2, 2, 2], [0, 0, 0]],\n                [[3, 3, 3], [4, 4, 4], [5, 5, 5], [1, 2, 3]],\n                [[1, 1, 1], [2, 2, 2], [0, 0, 0], [0, 0, 0]],\n            ]\n        )\n        solution = [\n            [[2, 2, 2], [1, 1, 1], [6, 6, 6], [0, 0, 0]],\n            [[1, 2, 3], [5, 5, 5], [4, 4, 4], [3, 3, 3]],\n            [[2, 2, 2], [1, 1, 1], [0, 0, 0], [0, 0, 0]],\n        ]\n        response = util.masked_flip(tensor, [3, 4, 2])\n        assert_almost_equal(response, solution)\n\n    def test_get_text_field_mask_returns_a_correct_mask(self):\n        text_field_tensors = {\n            ""indexer_name"": {\n                ""tokens"": torch.LongTensor([[3, 4, 5, 0, 0], [1, 2, 0, 0, 0]]),\n                ""token_characters"": torch.LongTensor(\n                    [\n                        [[1, 2], [3, 0], [2, 0], [0, 0], [0, 0]],\n                        [[5, 0], [4, 6], [0, 0], [0, 0], [0, 0]],\n                    ]\n                ),\n            }\n        }\n        assert_almost_equal(\n            util.get_text_field_mask(text_field_tensors).long().numpy(),\n            [[1, 1, 1, 0, 0], [1, 1, 0, 0, 0]],\n        )\n\n    def test_get_text_field_mask_returns_a_correct_mask_custom_padding_id(self):\n        text_field_tensors = {\n            ""indexer_name"": {\n                ""tokens"": torch.LongTensor([[3, 4, 5, 9, 9], [1, 2, 9, 9, 9]]),\n                ""token_characters"": torch.LongTensor(\n                    [\n                        [[1, 2], [3, 9], [2, 9], [9, 9], [9, 9]],\n                        [[5, 9], [4, 6], [9, 9], [9, 9], [9, 9]],\n                    ]\n                ),\n            }\n        }\n        assert_almost_equal(\n            util.get_text_field_mask(text_field_tensors, padding_id=9).long().numpy(),\n            [[1, 1, 1, 0, 0], [1, 1, 0, 0, 0]],\n        )\n\n    def test_get_text_field_mask_returns_a_correct_mask_character_only_input(self):\n        text_field_tensors = {\n            ""indexer_name"": {\n                ""token_characters"": torch.LongTensor(\n                    [\n                        [[1, 2, 3], [3, 0, 1], [2, 1, 0], [0, 0, 0]],\n                        [[5, 5, 5], [4, 6, 0], [0, 0, 0], [0, 0, 0]],\n                    ]\n                )\n            }\n        }\n        assert_almost_equal(\n            util.get_text_field_mask(text_field_tensors).long().numpy(),\n            [[1, 1, 1, 0], [1, 1, 0, 0]],\n        )\n\n    def test_get_text_field_mask_returns_a_correct_mask_character_only_input_custom_padding_id(\n        self,\n    ):\n        text_field_tensors = {\n            ""indexer_name"": {\n                ""token_characters"": torch.LongTensor(\n                    [\n                        [[1, 2, 3], [3, 9, 1], [2, 1, 9], [9, 9, 9]],\n                        [[5, 5, 5], [4, 6, 9], [9, 9, 9], [9, 9, 9]],\n                    ]\n                )\n            }\n        }\n        assert_almost_equal(\n            util.get_text_field_mask(text_field_tensors, padding_id=9).long().numpy(),\n            [[1, 1, 1, 0], [1, 1, 0, 0]],\n        )\n\n    def test_get_text_field_mask_returns_a_correct_mask_list_field(self):\n        text_field_tensors = {\n            ""indexer_name"": {\n                ""list_tokens"": torch.LongTensor(\n                    [\n                        [[1, 2], [3, 0], [2, 0], [0, 0], [0, 0]],\n                        [[5, 0], [4, 6], [0, 0], [0, 0], [0, 0]],\n                    ]\n                )\n            }\n        }\n        actual_mask = (\n            util.get_text_field_mask(text_field_tensors, num_wrapping_dims=1).long().numpy()\n        )\n        expected_mask = (text_field_tensors[""indexer_name""][""list_tokens""].numpy() > 0).astype(\n            ""int32""\n        )\n        assert_almost_equal(actual_mask, expected_mask)\n\n    def test_get_text_field_mask_returns_mask_key(self):\n        text_field_tensors = {\n            ""indexer_name"": {\n                ""tokens"": torch.LongTensor([[3, 4, 5, 0, 0], [1, 2, 0, 0, 0]]),\n                ""mask"": torch.tensor([[False, False, True]]),\n            }\n        }\n        assert_almost_equal(\n            util.get_text_field_mask(text_field_tensors).long().numpy(), [[0, 0, 1]]\n        )\n\n    def test_weighted_sum_works_on_simple_input(self):\n        batch_size = 1\n        sentence_length = 5\n        embedding_dim = 4\n        sentence_array = numpy.random.rand(batch_size, sentence_length, embedding_dim)\n        sentence_tensor = torch.from_numpy(sentence_array).float()\n        attention_tensor = torch.FloatTensor([[0.3, 0.4, 0.1, 0, 1.2]])\n        aggregated_array = util.weighted_sum(sentence_tensor, attention_tensor).data.numpy()\n        assert aggregated_array.shape == (batch_size, embedding_dim)\n        expected_array = (\n            0.3 * sentence_array[0, 0]\n            + 0.4 * sentence_array[0, 1]\n            + 0.1 * sentence_array[0, 2]\n            + 0.0 * sentence_array[0, 3]\n            + 1.2 * sentence_array[0, 4]\n        )\n        numpy.testing.assert_almost_equal(aggregated_array, [expected_array], decimal=5)\n\n    def test_weighted_sum_handles_higher_order_input(self):\n        batch_size = 1\n        length_1 = 5\n        length_2 = 6\n        length_3 = 2\n        embedding_dim = 4\n        sentence_array = numpy.random.rand(batch_size, length_1, length_2, length_3, embedding_dim)\n        attention_array = numpy.random.rand(batch_size, length_1, length_2, length_3)\n        sentence_tensor = torch.from_numpy(sentence_array).float()\n        attention_tensor = torch.from_numpy(attention_array).float()\n        aggregated_array = util.weighted_sum(sentence_tensor, attention_tensor).data.numpy()\n        assert aggregated_array.shape == (batch_size, length_1, length_2, embedding_dim)\n        expected_array = (\n            attention_array[0, 3, 2, 0] * sentence_array[0, 3, 2, 0]\n            + attention_array[0, 3, 2, 1] * sentence_array[0, 3, 2, 1]\n        )\n        numpy.testing.assert_almost_equal(aggregated_array[0, 3, 2], expected_array, decimal=5)\n\n    def test_weighted_sum_handles_uneven_higher_order_input(self):\n        batch_size = 1\n        length_1 = 5\n        length_2 = 6\n        length_3 = 2\n        embedding_dim = 4\n        sentence_array = numpy.random.rand(batch_size, length_3, embedding_dim)\n        attention_array = numpy.random.rand(batch_size, length_1, length_2, length_3)\n        sentence_tensor = torch.from_numpy(sentence_array).float()\n        attention_tensor = torch.from_numpy(attention_array).float()\n        aggregated_array = util.weighted_sum(sentence_tensor, attention_tensor).data.numpy()\n        assert aggregated_array.shape == (batch_size, length_1, length_2, embedding_dim)\n        for i in range(length_1):\n            for j in range(length_2):\n                expected_array = (\n                    attention_array[0, i, j, 0] * sentence_array[0, 0]\n                    + attention_array[0, i, j, 1] * sentence_array[0, 1]\n                )\n                numpy.testing.assert_almost_equal(\n                    aggregated_array[0, i, j], expected_array, decimal=5\n                )\n\n    def test_weighted_sum_handles_3d_attention_with_3d_matrix(self):\n        batch_size = 1\n        length_1 = 5\n        length_2 = 2\n        embedding_dim = 4\n        sentence_array = numpy.random.rand(batch_size, length_2, embedding_dim)\n        attention_array = numpy.random.rand(batch_size, length_1, length_2)\n        sentence_tensor = torch.from_numpy(sentence_array).float()\n        attention_tensor = torch.from_numpy(attention_array).float()\n        aggregated_array = util.weighted_sum(sentence_tensor, attention_tensor).data.numpy()\n        assert aggregated_array.shape == (batch_size, length_1, embedding_dim)\n        for i in range(length_1):\n            expected_array = (\n                attention_array[0, i, 0] * sentence_array[0, 0]\n                + attention_array[0, i, 1] * sentence_array[0, 1]\n            )\n            numpy.testing.assert_almost_equal(aggregated_array[0, i], expected_array, decimal=5)\n\n    def test_viterbi_decode(self):\n        # Test Viterbi decoding is equal to greedy decoding with no pairwise potentials.\n        sequence_logits = torch.nn.functional.softmax(torch.rand([5, 9]), dim=-1)\n        transition_matrix = torch.zeros([9, 9])\n        indices, _ = util.viterbi_decode(sequence_logits.data, transition_matrix)\n        _, argmax_indices = torch.max(sequence_logits, 1)\n        assert indices == argmax_indices.data.squeeze().tolist()\n\n        # Test Viterbi decoding works with start and end transitions\n        sequence_logits = torch.nn.functional.softmax(torch.rand([5, 9]), dim=-1)\n        transition_matrix = torch.zeros([9, 9])\n        allowed_start_transitions = torch.zeros([9])\n        # Force start tag to be an 8\n        allowed_start_transitions[:8] = float(""-inf"")\n        allowed_end_transitions = torch.zeros([9])\n        # Force end tag to be a 0\n        allowed_end_transitions[1:] = float(""-inf"")\n        indices, _ = util.viterbi_decode(\n            sequence_logits.data,\n            transition_matrix,\n            allowed_end_transitions=allowed_end_transitions,\n            allowed_start_transitions=allowed_start_transitions,\n        )\n        assert indices[0] == 8\n        assert indices[-1] == 0\n\n        # Test that pairwise potentials affect the sequence correctly and that\n        # viterbi_decode can handle -inf values.\n        sequence_logits = torch.FloatTensor(\n            [\n                [0, 0, 0, 3, 5],\n                [0, 0, 0, 3, 4],\n                [0, 0, 0, 3, 4],\n                [0, 0, 0, 3, 4],\n                [0, 0, 0, 3, 4],\n                [0, 0, 0, 3, 4],\n            ]\n        )\n        # The same tags shouldn\'t appear sequentially.\n        transition_matrix = torch.zeros([5, 5])\n        for i in range(5):\n            transition_matrix[i, i] = float(""-inf"")\n        indices, _ = util.viterbi_decode(sequence_logits, transition_matrix)\n        assert indices == [4, 3, 4, 3, 4, 3]\n\n        # Test that unbalanced pairwise potentials break ties\n        # between paths with equal unary potentials.\n        sequence_logits = torch.FloatTensor(\n            [\n                [0, 0, 0, 4, 4],\n                [0, 0, 0, 4, 4],\n                [0, 0, 0, 4, 4],\n                [0, 0, 0, 4, 4],\n                [0, 0, 0, 4, 4],\n                [0, 0, 0, 4, 4],\n            ]\n        )\n        # The 5th tag has a penalty for appearing sequentially\n        # or for transitioning to the 4th tag, making the best\n        # path uniquely to take the 4th tag only.\n        transition_matrix = torch.zeros([5, 5])\n        transition_matrix[4, 4] = -10\n        transition_matrix[4, 3] = -10\n        transition_matrix[3, 4] = -10\n        indices, _ = util.viterbi_decode(sequence_logits, transition_matrix)\n        assert indices == [3, 3, 3, 3, 3, 3]\n\n        sequence_logits = torch.FloatTensor([[1, 0, 0, 4], [1, 0, 6, 2], [0, 3, 0, 4]])\n        # Best path would normally be [3, 2, 3] but we add a\n        # potential from 2 -> 1, making [3, 2, 1] the best path.\n        transition_matrix = torch.zeros([4, 4])\n        transition_matrix[0, 0] = 1\n        transition_matrix[2, 1] = 5\n        indices, value = util.viterbi_decode(sequence_logits, transition_matrix)\n        assert indices == [3, 2, 1]\n        assert value.numpy() == 18\n\n        # Test that providing evidence results in paths containing specified tags.\n        sequence_logits = torch.FloatTensor(\n            [\n                [0, 0, 0, 7, 7],\n                [0, 0, 0, 7, 7],\n                [0, 0, 0, 7, 7],\n                [0, 0, 0, 7, 7],\n                [0, 0, 0, 7, 7],\n                [0, 0, 0, 7, 7],\n            ]\n        )\n        # The 5th tag has a penalty for appearing sequentially\n        # or for transitioning to the 4th tag, making the best\n        # path to take the 4th tag for every label.\n        transition_matrix = torch.zeros([5, 5])\n        transition_matrix[4, 4] = -10\n        transition_matrix[4, 3] = -2\n        transition_matrix[3, 4] = -2\n        # The 1st, 4th and 5th sequence elements are observed - they should be\n        # equal to 2, 0 and 4. The last tag should be equal to 3, because although\n        # the penalty for transitioning to the 4th tag is -2, the unary potential\n        # is 7, which is greater than the combination for any of the other labels.\n        observations = [2, -1, -1, 0, 4, -1]\n        indices, _ = util.viterbi_decode(sequence_logits, transition_matrix, observations)\n        assert indices == [2, 3, 3, 0, 4, 3]\n\n    def test_viterbi_decode_top_k(self):\n        # Test cases taken from: https://gist.github.com/PetrochukM/afaa3613a99a8e7213d2efdd02ae4762\n\n        # Test Viterbi decoding is equal to greedy decoding with no pairwise potentials.\n        sequence_logits = torch.autograd.Variable(torch.rand([5, 9]))\n        transition_matrix = torch.zeros([9, 9])\n\n        indices, _ = util.viterbi_decode(sequence_logits.data, transition_matrix, top_k=5)\n\n        _, argmax_indices = torch.max(sequence_logits, 1)\n        assert indices[0] == argmax_indices.data.squeeze().tolist()\n\n        # Test that pairwise potentials effect the sequence correctly and that\n        # viterbi_decode can handle -inf values.\n        sequence_logits = torch.FloatTensor(\n            [\n                [0, 0, 0, 3, 4],\n                [0, 0, 0, 3, 4],\n                [0, 0, 0, 3, 4],\n                [0, 0, 0, 3, 4],\n                [0, 0, 0, 3, 4],\n                [0, 0, 0, 3, 4],\n            ]\n        )\n        # The same tags shouldn\'t appear sequentially.\n        transition_matrix = torch.zeros([5, 5])\n        for i in range(5):\n            transition_matrix[i, i] = float(""-inf"")\n        indices, _ = util.viterbi_decode(sequence_logits, transition_matrix, top_k=5)\n        assert indices[0] == [3, 4, 3, 4, 3, 4]\n\n        # Test that unbalanced pairwise potentials break ties\n        # between paths with equal unary potentials.\n        sequence_logits = torch.FloatTensor(\n            [\n                [0, 0, 0, 4, 4],\n                [0, 0, 0, 4, 4],\n                [0, 0, 0, 4, 4],\n                [0, 0, 0, 4, 4],\n                [0, 0, 0, 4, 4],\n                [0, 0, 0, 4, 0],\n            ]\n        )\n        # The 5th tag has a penalty for appearing sequentially\n        # or for transitioning to the 4th tag, making the best\n        # path uniquely to take the 4th tag only.\n        transition_matrix = torch.zeros([5, 5])\n        transition_matrix[4, 4] = -10\n        transition_matrix[4, 3] = -10\n        indices, _ = util.viterbi_decode(sequence_logits, transition_matrix, top_k=5)\n        assert indices[0] == [3, 3, 3, 3, 3, 3]\n\n        sequence_logits = torch.FloatTensor([[1, 0, 0, 4], [1, 0, 6, 2], [0, 3, 0, 4]])\n        # Best path would normally be [3, 2, 3] but we add a\n        # potential from 2 -> 1, making [3, 2, 1] the best path.\n        transition_matrix = torch.zeros([4, 4])\n        transition_matrix[0, 0] = 1\n        transition_matrix[2, 1] = 5\n        indices, value = util.viterbi_decode(sequence_logits, transition_matrix, top_k=5)\n        assert indices[0] == [3, 2, 1]\n        assert value[0] == 18\n\n        def _brute_decode(\n            tag_sequence: torch.Tensor, transition_matrix: torch.Tensor, top_k: int = 5\n        ) -> Any:\n            """"""\n            Top-k decoder that uses brute search instead of the Viterbi Decode dynamic programing algorithm\n            """"""\n            # Create all possible sequences\n            sequences = [[]]  # type: ignore\n\n            for i in range(len(tag_sequence)):\n                new_sequences = []  # type: ignore\n                for j in range(len(tag_sequence[i])):\n                    for sequence in sequences:\n                        new_sequences.append(sequence[:] + [j])\n                sequences = new_sequences\n\n            # Score\n            scored_sequences = []  # type: ignore\n            for sequence in sequences:\n                emission_score = sum(tag_sequence[i, j] for i, j in enumerate(sequence))\n                transition_score = sum(\n                    transition_matrix[sequence[i - 1], sequence[i]] for i in range(1, len(sequence))\n                )\n                score = emission_score + transition_score\n                scored_sequences.append((score, sequence))\n\n            # Get the top k scores / paths\n            top_k_sequences = sorted(scored_sequences, key=lambda r: r[0], reverse=True)[:top_k]\n            scores, paths = zip(*top_k_sequences)\n\n            return paths, scores  # type: ignore\n\n        # Run 100 randomly generated parameters and compare the outputs.\n        for i in range(100):\n            num_tags = random.randint(1, 5)\n            seq_len = random.randint(1, 5)\n            k = random.randint(1, 5)\n            sequence_logits = torch.rand([seq_len, num_tags])\n            transition_matrix = torch.rand([num_tags, num_tags])\n            viterbi_paths_v1, viterbi_scores_v1 = util.viterbi_decode(\n                sequence_logits, transition_matrix, top_k=k\n            )\n            viterbi_path_brute, viterbi_score_brute = _brute_decode(\n                sequence_logits, transition_matrix, top_k=k\n            )\n            numpy.testing.assert_almost_equal(\n                list(viterbi_score_brute), viterbi_scores_v1.tolist(), decimal=3\n            )\n            numpy.testing.assert_equal(sanitize(viterbi_paths_v1), viterbi_path_brute)\n\n    def test_sequence_cross_entropy_with_logits_masks_loss_correctly(self):\n\n        # test weight masking by checking that a tensor with non-zero values in\n        # masked positions returns the same loss as a tensor with zeros in those\n        # positions.\n        tensor = torch.rand([5, 7, 4])\n        tensor[0, 3:, :] = 0\n        tensor[1, 4:, :] = 0\n        tensor[2, 2:, :] = 0\n        tensor[3, :, :] = 0\n        weights = (tensor != 0.0)[:, :, 0].long().squeeze(-1)\n        tensor2 = tensor.clone()\n        tensor2[0, 3:, :] = 2\n        tensor2[1, 4:, :] = 13\n        tensor2[2, 2:, :] = 234\n        tensor2[3, :, :] = 65\n        targets = torch.LongTensor(numpy.random.randint(0, 3, [5, 7]))\n        targets *= weights\n\n        loss = util.sequence_cross_entropy_with_logits(tensor, targets, weights)\n        loss2 = util.sequence_cross_entropy_with_logits(tensor2, targets, weights)\n        assert loss.data.numpy() == loss2.data.numpy()\n\n    def test_sequence_cross_entropy_with_logits_smooths_labels_correctly(self):\n        tensor = torch.rand([1, 3, 4])\n        targets = torch.LongTensor(numpy.random.randint(0, 3, [1, 3]))\n\n        weights = torch.ones([2, 3])\n        loss = util.sequence_cross_entropy_with_logits(\n            tensor, targets, weights, label_smoothing=0.1\n        )\n\n        correct_loss = 0.0\n        for prediction, label in zip(tensor.squeeze(0), targets.squeeze(0)):\n            prediction = torch.nn.functional.log_softmax(prediction, dim=-1)\n            correct_loss += prediction[label] * 0.9\n            # incorrect elements\n            correct_loss += prediction.sum() * 0.1 / 4\n        # Average over sequence.\n        correct_loss = -correct_loss / 3\n        numpy.testing.assert_array_almost_equal(loss.data.numpy(), correct_loss.data.numpy())\n\n    def test_sequence_cross_entropy_with_logits_averages_batch_correctly(self):\n        # test batch average is the same as dividing the batch averaged\n        # loss by the number of batches containing any non-padded tokens.\n        tensor = torch.rand([5, 7, 4])\n        tensor[0, 3:, :] = 0\n        tensor[1, 4:, :] = 0\n        tensor[2, 2:, :] = 0\n        tensor[3, :, :] = 0\n        weights = (tensor != 0.0)[:, :, 0].long().squeeze(-1)\n        targets = torch.LongTensor(numpy.random.randint(0, 3, [5, 7]))\n        targets *= weights\n\n        loss = util.sequence_cross_entropy_with_logits(tensor, targets, weights)\n\n        vector_loss = util.sequence_cross_entropy_with_logits(\n            tensor, targets, weights, average=None\n        )\n        # Batch has one completely padded row, so divide by 4.\n        assert loss.data.numpy() == vector_loss.sum().item() / 4\n\n    @flaky(max_runs=3, min_passes=1)\n    def test_sequence_cross_entropy_with_logits_averages_token_correctly(self):\n        # test token average is the same as multiplying the per-batch loss\n        # with the per-batch weights and dividing by the total weight\n        tensor = torch.rand([5, 7, 4])\n        tensor[0, 3:, :] = 0\n        tensor[1, 4:, :] = 0\n        tensor[2, 2:, :] = 0\n        tensor[3, :, :] = 0\n        weights = (tensor != 0.0)[:, :, 0].long().squeeze(-1)\n        targets = torch.LongTensor(numpy.random.randint(0, 3, [5, 7]))\n        targets *= weights\n\n        loss = util.sequence_cross_entropy_with_logits(tensor, targets, weights, average=""token"")\n\n        vector_loss = util.sequence_cross_entropy_with_logits(\n            tensor, targets, weights, average=None\n        )\n        total_token_loss = (vector_loss * weights.float().sum(dim=-1)).sum()\n        average_token_loss = (total_token_loss / weights.float().sum()).detach()\n        assert_almost_equal(loss.detach().item(), average_token_loss.item(), decimal=5)\n\n    def test_sequence_cross_entropy_with_logits_gamma_correctly(self):\n        batch = 1\n        length = 3\n        classes = 4\n        gamma = abs(numpy.random.randn())  # [0, +inf)\n\n        tensor = torch.rand([batch, length, classes])\n        targets = torch.LongTensor(numpy.random.randint(0, classes, [batch, length]))\n        weights = torch.ones([batch, length])\n\n        loss = util.sequence_cross_entropy_with_logits(tensor, targets, weights, gamma=gamma)\n\n        correct_loss = 0.0\n        for logit, label in zip(tensor.squeeze(0), targets.squeeze(0)):\n            p = torch.nn.functional.softmax(logit, dim=-1)\n            pt = p[label]\n            ft = (1 - pt) ** gamma\n            correct_loss += -pt.log() * ft\n        # Average over sequence.\n        correct_loss = correct_loss / length\n        numpy.testing.assert_array_almost_equal(loss.data.numpy(), correct_loss.data.numpy())\n\n    def test_sequence_cross_entropy_with_logits_alpha_float_correctly(self):\n        batch = 1\n        length = 3\n        classes = 2  # alpha float for binary class only\n        alpha = (\n            numpy.random.rand() if numpy.random.rand() > 0.5 else (1.0 - numpy.random.rand())\n        )  # [0, 1]\n\n        tensor = torch.rand([batch, length, classes])\n        targets = torch.LongTensor(numpy.random.randint(0, classes, [batch, length]))\n        weights = torch.ones([batch, length])\n\n        loss = util.sequence_cross_entropy_with_logits(tensor, targets, weights, alpha=alpha)\n\n        correct_loss = 0.0\n        for logit, label in zip(tensor.squeeze(0), targets.squeeze(0)):\n            logp = torch.nn.functional.log_softmax(logit, dim=-1)\n            logpt = logp[label]\n            if label:\n                at = alpha\n            else:\n                at = 1 - alpha\n            correct_loss += -logpt * at\n        # Average over sequence.\n        correct_loss = correct_loss / length\n        numpy.testing.assert_array_almost_equal(loss.data.numpy(), correct_loss.data.numpy())\n\n    def test_sequence_cross_entropy_with_logits_alpha_single_float_correctly(self):\n        batch = 1\n        length = 3\n        classes = 2  # alpha float for binary class only\n        alpha = (\n            numpy.random.rand() if numpy.random.rand() > 0.5 else (1.0 - numpy.random.rand())\n        )  # [0, 1]\n        alpha = torch.tensor(alpha)\n\n        tensor = torch.rand([batch, length, classes])\n        targets = torch.LongTensor(numpy.random.randint(0, classes, [batch, length]))\n        weights = torch.ones([batch, length])\n\n        loss = util.sequence_cross_entropy_with_logits(tensor, targets, weights, alpha=alpha)\n\n        correct_loss = 0.0\n        for logit, label in zip(tensor.squeeze(0), targets.squeeze(0)):\n            logp = torch.nn.functional.log_softmax(logit, dim=-1)\n            logpt = logp[label]\n            if label:\n                at = alpha\n            else:\n                at = 1 - alpha\n            correct_loss += -logpt * at\n        # Average over sequence.\n        correct_loss = correct_loss / length\n        numpy.testing.assert_array_almost_equal(loss.data.numpy(), correct_loss.data.numpy())\n\n    def test_sequence_cross_entropy_with_logits_alpha_list_correctly(self):\n        batch = 1\n        length = 3\n        classes = 4  # alpha float for binary class only\n        alpha = abs(numpy.random.randn(classes))  # [0, +inf)\n\n        tensor = torch.rand([batch, length, classes])\n        targets = torch.LongTensor(numpy.random.randint(0, classes, [batch, length]))\n        weights = torch.ones([batch, length])\n\n        loss = util.sequence_cross_entropy_with_logits(tensor, targets, weights, alpha=alpha)\n\n        correct_loss = 0.0\n        for logit, label in zip(tensor.squeeze(0), targets.squeeze(0)):\n            logp = torch.nn.functional.log_softmax(logit, dim=-1)\n            logpt = logp[label]\n            at = alpha[label]\n            correct_loss += -logpt * at\n        # Average over sequence.\n        correct_loss = correct_loss / length\n        numpy.testing.assert_array_almost_equal(loss.data.numpy(), correct_loss.data.numpy())\n\n    def test_replace_masked_values_replaces_masked_values_with_finite_value(self):\n        tensor = torch.FloatTensor([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]])\n        mask = torch.tensor([[True, True, False]])\n        replaced = util.replace_masked_values(tensor, mask.unsqueeze(-1), 2).data.numpy()\n        assert_almost_equal(replaced, [[[1, 2, 3, 4], [5, 6, 7, 8], [2, 2, 2, 2]]])\n\n    def test_logsumexp(self):\n        # First a simple example where we add probabilities in log space.\n        tensor = torch.FloatTensor([[0.4, 0.1, 0.2]])\n        log_tensor = tensor.log()\n        log_summed = util.logsumexp(log_tensor, dim=-1, keepdim=False)\n        assert_almost_equal(log_summed.exp().data.numpy(), [0.7])\n        log_summed = util.logsumexp(log_tensor, dim=-1, keepdim=True)\n        assert_almost_equal(log_summed.exp().data.numpy(), [[0.7]])\n\n        # Then some more atypical examples, and making sure this will work with how we handle\n        # log masks.\n        tensor = torch.FloatTensor([[float(""-inf""), 20.0]])\n        assert_almost_equal(util.logsumexp(tensor).data.numpy(), [20.0])\n        tensor = torch.FloatTensor([[-200.0, 20.0]])\n        assert_almost_equal(util.logsumexp(tensor).data.numpy(), [20.0])\n        tensor = torch.FloatTensor([[20.0, 20.0], [-200.0, 200.0]])\n        assert_almost_equal(util.logsumexp(tensor, dim=0).data.numpy(), [20.0, 200.0])\n\n    def test_flatten_and_batch_shift_indices(self):\n        indices = numpy.array(\n            [[[1, 2, 3, 4], [5, 6, 7, 8], [9, 9, 9, 9]], [[2, 1, 0, 7], [7, 7, 2, 3], [0, 0, 4, 2]]]\n        )\n        indices = torch.tensor(indices, dtype=torch.long)\n        shifted_indices = util.flatten_and_batch_shift_indices(indices, 10)\n        numpy.testing.assert_array_equal(\n            shifted_indices.data.numpy(),\n            numpy.array(\n                [1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 9, 9, 12, 11, 10, 17, 17, 17, 12, 13, 10, 10, 14, 12]\n            ),\n        )\n\n    def test_batched_index_select(self):\n        indices = numpy.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n        # Each element is a vector of its index.\n        targets = torch.ones([2, 10, 3]).cumsum(1) - 1\n        # Make the second batch double its index so they\'re different.\n        targets[1, :, :] *= 2\n        indices = torch.tensor(indices, dtype=torch.long)\n        selected = util.batched_index_select(targets, indices)\n\n        assert list(selected.size()) == [2, 2, 2, 3]\n        ones = numpy.ones([3])\n        numpy.testing.assert_array_equal(selected[0, 0, 0, :].data.numpy(), ones)\n        numpy.testing.assert_array_equal(selected[0, 0, 1, :].data.numpy(), ones * 2)\n        numpy.testing.assert_array_equal(selected[0, 1, 0, :].data.numpy(), ones * 3)\n        numpy.testing.assert_array_equal(selected[0, 1, 1, :].data.numpy(), ones * 4)\n\n        numpy.testing.assert_array_equal(selected[1, 0, 0, :].data.numpy(), ones * 10)\n        numpy.testing.assert_array_equal(selected[1, 0, 1, :].data.numpy(), ones * 12)\n        numpy.testing.assert_array_equal(selected[1, 1, 0, :].data.numpy(), ones * 14)\n        numpy.testing.assert_array_equal(selected[1, 1, 1, :].data.numpy(), ones * 16)\n\n        indices = numpy.array([[[1, 11], [3, 4]], [[5, 6], [7, 8]]])\n        indices = torch.tensor(indices, dtype=torch.long)\n        with pytest.raises(ConfigurationError):\n            util.batched_index_select(targets, indices)\n\n        indices = numpy.array([[[1, -1], [3, 4]], [[5, 6], [7, 8]]])\n        indices = torch.tensor(indices, dtype=torch.long)\n        with pytest.raises(ConfigurationError):\n            util.batched_index_select(targets, indices)\n\n    def test_batched_span_select(self):\n        # Each element is a vector of its index.\n        targets = torch.ones([3, 12, 2]).cumsum(1) - 1\n        spans = torch.LongTensor(\n            [\n                [[0, 0], [1, 2], [5, 8], [10, 10]],\n                [[i, i] for i in range(3, -1, -1)],\n                [[0, 3], [1, 4], [2, 5], [10, 11]],\n            ]\n        )\n        selected, mask = util.batched_span_select(targets, spans)\n\n        selected = torch.where(mask.unsqueeze(-1), selected, torch.empty_like(selected).fill_(-1))\n\n        numpy.testing.assert_array_equal(\n            selected,\n            [\n                [\n                    [[0, 0], [-1, -1], [-1, -1], [-1, -1]],\n                    [[2, 2], [1, 1], [-1, -1], [-1, -1]],\n                    [[8, 8], [7, 7], [6, 6], [5, 5]],\n                    [[10, 10], [-1, -1], [-1, -1], [-1, -1]],\n                ],\n                [[[i, i], [-1, -1], [-1, -1], [-1, -1]] for i in range(3, -1, -1)],\n                [\n                    [[3, 3], [2, 2], [1, 1], [0, 0]],\n                    [[4, 4], [3, 3], [2, 2], [1, 1]],\n                    [[5, 5], [4, 4], [3, 3], [2, 2]],\n                    [[11, 11], [10, 10], [-1, -1], [-1, -1]],\n                ],\n            ],\n        )\n\n    def test_flattened_index_select(self):\n        indices = numpy.array([[1, 2], [3, 4]])\n        targets = torch.ones([2, 6, 3]).cumsum(1) - 1\n        # Make the second batch double its index so they\'re different.\n        targets[1, :, :] *= 2\n        indices = torch.tensor(indices, dtype=torch.long)\n\n        selected = util.flattened_index_select(targets, indices)\n\n        assert list(selected.size()) == [2, 2, 2, 3]\n\n        ones = numpy.ones([3])\n        numpy.testing.assert_array_equal(selected[0, 0, 0, :].data.numpy(), ones)\n        numpy.testing.assert_array_equal(selected[0, 0, 1, :].data.numpy(), ones * 2)\n        numpy.testing.assert_array_equal(selected[0, 1, 0, :].data.numpy(), ones * 3)\n        numpy.testing.assert_array_equal(selected[0, 1, 1, :].data.numpy(), ones * 4)\n\n        numpy.testing.assert_array_equal(selected[1, 0, 0, :].data.numpy(), ones * 2)\n        numpy.testing.assert_array_equal(selected[1, 0, 1, :].data.numpy(), ones * 4)\n        numpy.testing.assert_array_equal(selected[1, 1, 0, :].data.numpy(), ones * 6)\n        numpy.testing.assert_array_equal(selected[1, 1, 1, :].data.numpy(), ones * 8)\n\n        # Check we only accept 2D indices.\n        with pytest.raises(ConfigurationError):\n            util.flattened_index_select(targets, torch.ones([3, 4, 5]))\n\n    def test_bucket_values(self):\n        indices = torch.LongTensor([1, 2, 7, 1, 56, 900])\n        bucketed_distances = util.bucket_values(indices)\n        numpy.testing.assert_array_equal(\n            bucketed_distances.numpy(), numpy.array([1, 2, 5, 1, 8, 9])\n        )\n\n    def test_add_sentence_boundary_token_ids_handles_2D_input(self):\n        tensor = torch.from_numpy(numpy.array([[1, 2, 3], [4, 5, 0]]))\n        mask = tensor > 0\n        bos = 9\n        eos = 10\n        new_tensor, new_mask = util.add_sentence_boundary_token_ids(tensor, mask, bos, eos)\n        expected_new_tensor = numpy.array([[9, 1, 2, 3, 10], [9, 4, 5, 10, 0]])\n        assert (new_tensor.data.numpy() == expected_new_tensor).all()\n        assert (new_mask.data.numpy() == (expected_new_tensor > 0)).all()\n\n    def test_add_sentence_boundary_token_ids_handles_3D_input(self):\n        tensor = torch.from_numpy(\n            numpy.array(\n                [\n                    [[1, 2, 3, 4], [5, 5, 5, 5], [6, 8, 1, 2]],\n                    [[4, 3, 2, 1], [8, 7, 6, 5], [0, 0, 0, 0]],\n                ]\n            )\n        )\n        mask = (tensor > 0).sum(dim=-1) > 0\n        bos = torch.from_numpy(numpy.array([9, 9, 9, 9]))\n        eos = torch.from_numpy(numpy.array([10, 10, 10, 10]))\n        new_tensor, new_mask = util.add_sentence_boundary_token_ids(tensor, mask, bos, eos)\n        expected_new_tensor = numpy.array(\n            [\n                [[9, 9, 9, 9], [1, 2, 3, 4], [5, 5, 5, 5], [6, 8, 1, 2], [10, 10, 10, 10]],\n                [[9, 9, 9, 9], [4, 3, 2, 1], [8, 7, 6, 5], [10, 10, 10, 10], [0, 0, 0, 0]],\n            ]\n        )\n        assert (new_tensor.data.numpy() == expected_new_tensor).all()\n        assert (new_mask.data.numpy() == ((expected_new_tensor > 0).sum(axis=-1) > 0)).all()\n\n    def test_remove_sentence_boundaries(self):\n        tensor = torch.from_numpy(numpy.random.rand(3, 5, 7))\n        mask = torch.from_numpy(\n            # The mask with two elements is to test the corner case\n            # of an empty sequence, so here we are removing boundaries\n            # from  ""<S> </S>""\n            numpy.array([[1, 1, 0, 0, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 0]])\n        ).bool()\n        new_tensor, new_mask = util.remove_sentence_boundaries(tensor, mask)\n\n        expected_new_tensor = torch.zeros(3, 3, 7)\n        expected_new_tensor[1, 0:3, :] = tensor[1, 1:4, :]\n        expected_new_tensor[2, 0:2, :] = tensor[2, 1:3, :]\n        assert_array_almost_equal(new_tensor.data.numpy(), expected_new_tensor.data.numpy())\n\n        expected_new_mask = torch.from_numpy(numpy.array([[0, 0, 0], [1, 1, 1], [1, 1, 0]])).bool()\n        assert (new_mask.data.numpy() == expected_new_mask.data.numpy()).all()\n\n    def test_add_positional_features(self):\n        # This is hard to test, so we check that we get the same result as the\n        # original tensorflow implementation:\n        # https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py#L270\n        tensor2tensor_result = numpy.asarray(\n            [\n                [0.00000000e00, 0.00000000e00, 1.00000000e00, 1.00000000e00],\n                [8.41470957e-01, 9.99999902e-05, 5.40302277e-01, 1.00000000e00],\n                [9.09297407e-01, 1.99999980e-04, -4.16146845e-01, 1.00000000e00],\n            ]\n        )\n\n        tensor = torch.zeros([2, 3, 4])\n        result = util.add_positional_features(tensor, min_timescale=1.0, max_timescale=1.0e4)\n        numpy.testing.assert_almost_equal(result[0].detach().cpu().numpy(), tensor2tensor_result)\n        numpy.testing.assert_almost_equal(result[1].detach().cpu().numpy(), tensor2tensor_result)\n\n        # Check case with odd number of dimensions.\n        tensor2tensor_result = numpy.asarray(\n            [\n                [\n                    0.00000000e00,\n                    0.00000000e00,\n                    0.00000000e00,\n                    1.00000000e00,\n                    1.00000000e00,\n                    1.00000000e00,\n                    0.00000000e00,\n                ],\n                [\n                    8.41470957e-01,\n                    9.99983307e-03,\n                    9.99999902e-05,\n                    5.40302277e-01,\n                    9.99949992e-01,\n                    1.00000000e00,\n                    0.00000000e00,\n                ],\n                [\n                    9.09297407e-01,\n                    1.99986659e-02,\n                    1.99999980e-04,\n                    -4.16146815e-01,\n                    9.99800026e-01,\n                    1.00000000e00,\n                    0.00000000e00,\n                ],\n            ]\n        )\n\n        tensor = torch.zeros([2, 3, 7])\n        result = util.add_positional_features(tensor, min_timescale=1.0, max_timescale=1.0e4)\n        numpy.testing.assert_almost_equal(result[0].detach().cpu().numpy(), tensor2tensor_result)\n        numpy.testing.assert_almost_equal(result[1].detach().cpu().numpy(), tensor2tensor_result)\n\n    def test_combine_tensors_and_multiply(self):\n        tensors = [torch.Tensor([[[2, 3]]]), torch.Tensor([[[5, 5]]])]\n        weight = torch.Tensor([4, 5])\n\n        combination = ""x""\n        assert_almost_equal(\n            util.combine_tensors_and_multiply(combination, tensors, weight), [[8 + 15]]\n        )\n\n        combination = ""y""\n        assert_almost_equal(\n            util.combine_tensors_and_multiply(combination, tensors, weight), [[20 + 25]]\n        )\n\n        combination = ""x,y""\n        weight2 = torch.Tensor([4, 5, 4, 5])\n        assert_almost_equal(\n            util.combine_tensors_and_multiply(combination, tensors, weight2), [[8 + 20 + 15 + 25]]\n        )\n\n        combination = ""x-y""\n        assert_almost_equal(\n            util.combine_tensors_and_multiply(combination, tensors, weight), [[-3 * 4 + -2 * 5]]\n        )\n\n        combination = ""y-x""\n        assert_almost_equal(\n            util.combine_tensors_and_multiply(combination, tensors, weight), [[3 * 4 + 2 * 5]]\n        )\n\n        combination = ""y+x""\n        assert_almost_equal(\n            util.combine_tensors_and_multiply(combination, tensors, weight), [[7 * 4 + 8 * 5]]\n        )\n\n        combination = ""y*x""\n        assert_almost_equal(\n            util.combine_tensors_and_multiply(combination, tensors, weight), [[10 * 4 + 15 * 5]]\n        )\n\n        combination = ""y/x""\n        assert_almost_equal(\n            util.combine_tensors_and_multiply(combination, tensors, weight),\n            [[(5 / 2) * 4 + (5 / 3) * 5]],\n            decimal=4,\n        )\n\n        combination = ""x/y""\n        assert_almost_equal(\n            util.combine_tensors_and_multiply(combination, tensors, weight),\n            [[(2 / 5) * 4 + (3 / 5) * 5]],\n            decimal=4,\n        )\n\n        with pytest.raises(ConfigurationError):\n            util.combine_tensors_and_multiply(""x+y+y"", tensors, weight)\n\n        with pytest.raises(ConfigurationError):\n            util.combine_tensors_and_multiply(""x%y"", tensors, weight)\n\n    def test_combine_tensors_and_multiply_with_same_batch_size_and_embedding_dim(self):\n        # This test just makes sure we handle some potential edge cases where the lengths of all\n        # dimensions are the same, making sure that the multiplication with the weight vector\n        # happens along the right dimension (it should be the last one).\n        tensors = [torch.Tensor([[[5, 5], [4, 4]], [[2, 3], [1, 1]]])]  # (2, 2, 2)\n        weight = torch.Tensor([4, 5])  # (2,)\n\n        combination = ""x""\n        assert_almost_equal(\n            util.combine_tensors_and_multiply(combination, tensors, weight),\n            [[20 + 25, 16 + 20], [8 + 15, 4 + 5]],\n        )\n\n        tensors = [\n            torch.Tensor([[[5, 5], [2, 2]], [[4, 4], [3, 3]]]),\n            torch.Tensor([[[2, 3]], [[1, 1]]]),\n        ]\n        weight = torch.Tensor([4, 5])\n        combination = ""x*y""\n        assert_almost_equal(\n            util.combine_tensors_and_multiply(combination, tensors, weight),\n            [\n                [5 * 2 * 4 + 5 * 3 * 5, 2 * 2 * 4 + 2 * 3 * 5],\n                [4 * 1 * 4 + 4 * 1 * 5, 3 * 1 * 4 + 3 * 1 * 5],\n            ],\n        )\n\n    def test_combine_tensors_and_multiply_with_batch_size_one(self):\n        seq_len_1 = 10\n        seq_len_2 = 5\n        embedding_dim = 8\n\n        combination = ""x,y,x*y""\n        t1 = torch.randn(1, seq_len_1, embedding_dim)\n        t2 = torch.randn(1, seq_len_2, embedding_dim)\n        combined_dim = util.get_combined_dim(combination, [embedding_dim, embedding_dim])\n        weight = torch.Tensor(combined_dim)\n\n        result = util.combine_tensors_and_multiply(\n            combination, [t1.unsqueeze(2), t2.unsqueeze(1)], weight\n        )\n\n        assert_almost_equal(result.size(), [1, seq_len_1, seq_len_2])\n\n    def test_combine_tensors_and_multiply_with_batch_size_one_and_seq_len_one(self):\n        seq_len_1 = 10\n        seq_len_2 = 1\n        embedding_dim = 8\n\n        combination = ""x,y,x*y""\n        t1 = torch.randn(1, seq_len_1, embedding_dim)\n        t2 = torch.randn(1, seq_len_2, embedding_dim)\n        combined_dim = util.get_combined_dim(combination, [embedding_dim, embedding_dim])\n        weight = torch.Tensor(combined_dim)\n\n        result = util.combine_tensors_and_multiply(\n            combination, [t1.unsqueeze(2), t2.unsqueeze(1)], weight\n        )\n\n        assert_almost_equal(result.size(), [1, seq_len_1, seq_len_2])\n\n    def test_has_tensor(self):\n\n        has_tensor = util.has_tensor\n        tensor = torch.tensor([1, 2, 3])\n\n        assert has_tensor([""a"", 10, tensor])\n        assert not has_tensor([""a"", 10])\n\n        assert has_tensor((""a"", 10, tensor))\n        assert not has_tensor((""a"", 10))\n\n        assert has_tensor({""a"": tensor, ""b"": 1})\n        assert not has_tensor({""a"": 10, ""b"": 1})\n\n        assert has_tensor(tensor)\n        assert not has_tensor(3)\n\n        assert has_tensor({""x"": [0, {""inside"": {""double_inside"": [3, [10, tensor]]}}]})\n\n    def test_combine_initial_dims(self):\n        tensor = torch.randn(4, 10, 20, 17, 5)\n\n        tensor2d = util.combine_initial_dims(tensor)\n        assert list(tensor2d.size()) == [4 * 10 * 20 * 17, 5]\n\n    def test_uncombine_initial_dims(self):\n        embedding2d = torch.randn(4 * 10 * 20 * 17 * 5, 12)\n\n        embedding = util.uncombine_initial_dims(embedding2d, torch.Size((4, 10, 20, 17, 5)))\n        assert list(embedding.size()) == [4, 10, 20, 17, 5, 12]\n\n    def test_inspect_model_parameters(self):\n        model_archive = str(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        parameters_inspection = str(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""parameters_inspection.json""\n        )\n        model = load_archive(model_archive).model\n        with open(parameters_inspection) as file:\n            parameters_inspection_dict = json.load(file)\n        assert parameters_inspection_dict == util.inspect_parameters(model)\n\n    def test_move_to_device(self):\n        # We\'re faking the tensor here so that we can test the calls to .cuda() without actually\n        # needing a GPU.\n        class FakeTensor(torch.Tensor):\n            def __init__(self):\n                self._device = None\n\n            def cuda(self, device):\n                self._device = device\n                return self\n\n        class A(NamedTuple):\n            a: int\n            b: torch.Tensor\n\n        structured_obj = {\n            ""a"": [A(1, FakeTensor()), A(2, FakeTensor())],\n            ""b"": FakeTensor(),\n            ""c"": (1, FakeTensor()),\n        }\n        new_device = torch.device(4)\n        moved_obj = util.move_to_device(structured_obj, new_device)\n        assert moved_obj[""a""][0].a == 1\n        assert moved_obj[""a""][0].b._device == new_device\n        assert moved_obj[""a""][1].b._device == new_device\n        assert moved_obj[""b""]._device == new_device\n        assert moved_obj[""c""][0] == 1\n        assert moved_obj[""c""][1]._device == new_device\n\n    def test_extend_layer(self):\n        lin_layer = torch.nn.Linear(10, 5)\n        new_dim = 8\n\n        old_weights = lin_layer.weight.data.clone()\n        old_bias = lin_layer.bias.data.clone()\n\n        util.extend_layer(lin_layer, new_dim)\n\n        assert lin_layer.weight.data.shape == (8, 10)\n        assert lin_layer.bias.data.shape == (8,)\n\n        assert (lin_layer.weight.data[:5] == old_weights).all()\n        assert (lin_layer.bias.data[:5] == old_bias).all()\n\n        assert lin_layer.out_features == new_dim\n\n    def test_masked_topk_selects_top_scored_items_and_respects_masking(self):\n        items = torch.randn([3, 4, 5]).clamp(min=0.0, max=1.0)\n        items[0, :2, :] = 1\n        items[1, 2:, :] = 1\n        items[2, 2:, :] = 1\n\n        scores = items.sum(-1)\n\n        mask = torch.ones([3, 4]).bool()\n        mask[1, 0] = 0\n        mask[1, 3] = 0\n\n        pruned_scores, pruned_mask, pruned_indices = util.masked_topk(scores, mask, 2)\n\n        # Second element in the batch would have indices 2, 3, but\n        # 3 and 0 are masked, so instead it has 1, 2.\n        numpy.testing.assert_array_equal(\n            pruned_indices.data.numpy(), numpy.array([[0, 1], [1, 2], [2, 3]])\n        )\n        numpy.testing.assert_array_equal(pruned_mask.data.numpy(), numpy.ones([3, 2]))\n\n        # scores should be the result of index_selecting the pruned_indices.\n        correct_scores = util.batched_index_select(scores.unsqueeze(-1), pruned_indices).squeeze(-1)\n        self.assert_array_equal_with_mask(correct_scores, pruned_scores, pruned_mask)\n\n    def test_masked_topk_works_for_completely_masked_rows(self):\n        items = torch.randn([3, 4, 5]).clamp(min=0.0, max=1.0)\n        items[0, :2, :] = 1\n        items[1, 2:, :] = 1\n        items[2, 2:, :] = 1\n\n        scores = items.sum(-1)\n\n        mask = torch.ones([3, 4]).bool()\n        mask[1, 0] = 0\n        mask[1, 3] = 0\n        mask[2, :] = 0  # fully masked last batch element.\n\n        pruned_scores, pruned_mask, pruned_indices = util.masked_topk(scores, mask, 2)\n\n        # We can\'t check the last row here, because it\'s completely masked.\n        # Instead we\'ll check that the scores for these elements are very small.\n        numpy.testing.assert_array_equal(\n            pruned_indices[:2].data.numpy(), numpy.array([[0, 1], [1, 2]])\n        )\n        numpy.testing.assert_array_equal(\n            pruned_mask.data.numpy(), numpy.array([[1, 1], [1, 1], [0, 0]])\n        )\n\n        # scores should be the result of index_selecting the pruned_indices.\n        correct_scores = util.batched_index_select(scores.unsqueeze(-1), pruned_indices).squeeze(-1)\n        self.assert_array_equal_with_mask(correct_scores, pruned_scores, pruned_mask)\n\n    def test_masked_topk_selects_top_scored_items_and_respects_masking_different_num_items(self):\n        items = torch.randn([3, 4, 5]).clamp(min=0.0, max=1.0)\n        items[0, 0, :] = 1.5\n        items[0, 1, :] = 2\n        items[0, 3, :] = 1\n        items[1, 1:3, :] = 1\n        items[2, 0, :] = 1\n        items[2, 1, :] = 2\n        items[2, 2, :] = 1.5\n\n        scores = items.sum(-1)\n\n        mask = torch.ones([3, 4]).bool()\n        mask[1, 3] = 0\n\n        k = torch.tensor([3, 2, 1], dtype=torch.long)\n\n        pruned_scores, pruned_mask, pruned_indices = util.masked_topk(scores, mask, k)\n\n        # Second element in the batch would have indices 2, 3, but\n        # 3 and 0 are masked, so instead it has 1, 2.\n        numpy.testing.assert_array_equal(\n            pruned_indices.data.numpy(), numpy.array([[0, 1, 3], [1, 2, 2], [1, 2, 2]])\n        )\n        numpy.testing.assert_array_equal(\n            pruned_mask.data.numpy(), numpy.array([[1, 1, 1], [1, 1, 0], [1, 0, 0]])\n        )\n\n        # scores should be the result of index_selecting the pruned_indices.\n        correct_scores = util.batched_index_select(scores.unsqueeze(-1), pruned_indices).squeeze(-1)\n        self.assert_array_equal_with_mask(correct_scores, pruned_scores, pruned_mask)\n\n    def test_masked_topk_works_for_row_with_no_items_requested(self):\n        # Case where `num_items_to_keep` is a tensor rather than an int. Make sure it does the right\n        # thing when no items are requested for one of the rows.\n\n        items = torch.randn([3, 4, 5]).clamp(min=0.0, max=1.0)\n        items[0, :3, :] = 1\n        items[1, 2:, :] = 1\n        items[2, 2:, :] = 1\n\n        scores = items.sum(-1)\n\n        mask = torch.ones([3, 4]).bool()\n        mask[1, 0] = 0\n        mask[1, 3] = 0\n\n        k = torch.tensor([3, 2, 0], dtype=torch.long)\n\n        pruned_scores, pruned_mask, pruned_indices = util.masked_topk(scores, mask, k)\n\n        # First element just picks top three entries. Second would pick entries 2 and 3, but 0 and 3\n        # are masked, so it takes 1 and 2 (repeating the second index). The third element is\n        # entirely masked and just repeats the largest index with a top-3 score.\n        numpy.testing.assert_array_equal(\n            pruned_indices.data.numpy(), numpy.array([[0, 1, 2], [1, 2, 2], [3, 3, 3]])\n        )\n        numpy.testing.assert_array_equal(\n            pruned_mask.data.numpy(), numpy.array([[1, 1, 1], [1, 1, 0], [0, 0, 0]])\n        )\n\n        # scores should be the result of index_selecting the pruned_indices.\n        correct_scores = util.batched_index_select(scores.unsqueeze(-1), pruned_indices).squeeze(-1)\n        self.assert_array_equal_with_mask(correct_scores, pruned_scores, pruned_mask)\n\n    def test_masked_topk_works_for_multiple_dimensions(self):\n        # fmt: off\n        items = torch.FloatTensor([  # (3, 2, 5)\n            [[4, 2, 9, 9, 7], [-4, -2, -9, -9, -7]],\n            [[5, 4, 1, 8, 8], [9, 1, 7, 4, 1]],\n            [[9, 8, 9, 6, 0], [2, 2, 2, 2, 2]],\n        ]).unsqueeze(-1).expand(3, 2, 5, 4)\n\n        mask = torch.tensor([\n            [[False, False, False, False, False], [True, True, True, True, True]],\n            [[True, True, True, True, False], [False, True, True, True, True]],\n            [[True, False, True, True, True], [False, True, False, True, True]],\n        ]).unsqueeze(-1).expand(3, 2, 5, 4)\n\n        # This is the same as just specifying a scalar int, but we want to test this behavior\n        k = torch.ones(3, 5, 4, dtype=torch.long)\n        k[1, 3, :] = 2\n\n        target_items = torch.FloatTensor([\n            [[-4, -2, -9, -9, -7], [0, 0, 0, 0, 0]],\n            [[5, 4, 7, 8, 1], [0, 0, 0, 4, 0]],\n            [[9, 2, 9, 6, 2], [0, 0, 0, 0, 0]],\n        ]).unsqueeze(-1).expand(3, 2, 5, 4)\n\n        target_mask = torch.ones(3, 2, 5, 4, dtype=torch.bool)\n        target_mask[:, 1, :, :] = 0\n        target_mask[1, 1, 3, :] = 1\n\n        target_indices = torch.LongTensor([\n            [[1, 1, 1, 1, 1], [0, 0, 0, 0, 0]],\n            [[0, 0, 1, 0, 1], [0, 0, 0, 1, 0]],\n            [[0, 1, 0, 0, 1], [0, 0, 0, 0, 0]],\n        ]).unsqueeze(-1).expand(3, 2, 5, 4)\n        # fmt: on\n\n        pruned_items, pruned_mask, pruned_indices = util.masked_topk(items, mask, k, dim=1)\n\n        numpy.testing.assert_array_equal(pruned_mask.data.numpy(), target_mask.data.numpy())\n        self.assert_array_equal_with_mask(pruned_items, target_items, pruned_mask)\n        self.assert_array_equal_with_mask(pruned_indices, target_indices, pruned_mask)\n\n    def assert_array_equal_with_mask(self, a, b, mask):\n        numpy.testing.assert_array_equal((a * mask).data.numpy(), (b * mask).data.numpy())\n\n    def test_tensors_equal(self):\n        # Basic\n        assert util.tensors_equal(torch.tensor([1]), torch.tensor([1]))\n        assert not util.tensors_equal(torch.tensor([1]), torch.tensor([2]))\n\n        # Bool\n        assert util.tensors_equal(torch.tensor([True]), torch.tensor([True]))\n\n        # Cross dtype\n        assert util.tensors_equal(torch.tensor([1]), torch.tensor([1.0]))\n        assert util.tensors_equal(torch.tensor([1]), torch.tensor([True]))\n\n        # Containers\n        assert util.tensors_equal([torch.tensor([1])], [torch.tensor([1])])\n        assert not util.tensors_equal([torch.tensor([1])], [torch.tensor([2])])\n        assert util.tensors_equal({""key"": torch.tensor([1])}, {""key"": torch.tensor([1])})\n\n    def test_info_value_of_dtype(self):\n        with pytest.raises(TypeError):\n            util.info_value_of_dtype(torch.bool)\n\n        assert util.min_value_of_dtype(torch.half) == -65504.0\n        assert util.max_value_of_dtype(torch.half) == 65504.0\n        assert util.tiny_value_of_dtype(torch.half) == 1e-4\n        assert util.min_value_of_dtype(torch.float) == -3.4028234663852886e38\n        assert util.max_value_of_dtype(torch.float) == 3.4028234663852886e38\n        assert util.tiny_value_of_dtype(torch.float) == 1e-13\n\n        assert util.min_value_of_dtype(torch.uint8) == 0\n        assert util.max_value_of_dtype(torch.uint8) == 255\n        assert util.min_value_of_dtype(torch.long) == -9223372036854775808\n        assert util.max_value_of_dtype(torch.long) == 9223372036854775807\n\n    def test_get_token_ids_from_text_field_tensors(self):\n        # Setting up a number of diffrent indexers, that we can test later.\n        string_tokens = [""This"", ""is"", ""a"", ""test""]\n        tokens = [Token(x) for x in string_tokens]\n        vocab = Vocabulary()\n        vocab.add_tokens_to_namespace(string_tokens, ""tokens"")\n        vocab.add_tokens_to_namespace(\n            set([char for token in string_tokens for char in token]), ""token_characters""\n        )\n        elmo_indexer = ELMoTokenCharactersIndexer()\n        token_chars_indexer = TokenCharactersIndexer()\n        single_id_indexer = SingleIdTokenIndexer()\n        indexers = {""elmo"": elmo_indexer, ""chars"": token_chars_indexer, ""tokens"": single_id_indexer}\n\n        # In all of the tests below, we\'ll want to recover the token ides that were produced by the\n        # single_id indexer, so we grab that output first.\n        text_field = TextField(tokens, {""tokens"": single_id_indexer})\n        text_field.index(vocab)\n        tensors = text_field.as_tensor(text_field.get_padding_lengths())\n        expected_token_ids = tensors[""tokens""][""tokens""]\n\n        # Now the actual tests.\n        text_field = TextField(tokens, indexers)\n        text_field.index(vocab)\n        tensors = text_field.as_tensor(text_field.get_padding_lengths())\n        token_ids = util.get_token_ids_from_text_field_tensors(tensors)\n        assert (token_ids == expected_token_ids).all()\n'"
tests/predictors/__init__.py,0,b''
tests/predictors/predictor_test.py,0,"b'from allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors import Predictor\nfrom allennlp.nn import util\n\n\nclass TestPredictor(AllenNlpTestCase):\n    def test_from_archive_does_not_consume_params(self):\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""simple_tagger"" / ""serialization"" / ""model.tar.gz""\n        )\n        Predictor.from_archive(archive, ""sentence_tagger"")\n\n        # If it consumes the params, this will raise an exception\n        Predictor.from_archive(archive, ""sentence_tagger"")\n\n    def test_loads_correct_dataset_reader(self):\n        # This model has a different dataset reader configuration for train and validation. The\n        # parameter that differs is the token indexer\'s namespace.\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""simple_tagger_with_span_f1"" / ""serialization"" / ""model.tar.gz""\n        )\n\n        predictor = Predictor.from_archive(archive, ""sentence_tagger"")\n        assert predictor._dataset_reader._token_indexers[""tokens""].namespace == ""test_tokens""\n\n        predictor = Predictor.from_archive(\n            archive, ""sentence_tagger"", dataset_reader_to_load=""train""\n        )\n        assert predictor._dataset_reader._token_indexers[""tokens""].namespace == ""tokens""\n\n        predictor = Predictor.from_archive(\n            archive, ""sentence_tagger"", dataset_reader_to_load=""validation""\n        )\n        assert predictor._dataset_reader._token_indexers[""tokens""].namespace == ""test_tokens""\n\n    def test_get_gradients(self):\n        inputs = {\n            ""sentence"": ""I always write unit tests"",\n        }\n\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive)\n\n        instance = predictor._json_to_instance(inputs)\n        outputs = predictor._model.forward_on_instance(instance)\n        labeled_instances = predictor.predictions_to_labeled_instances(instance, outputs)\n        for instance in labeled_instances:\n            grads = predictor.get_gradients([instance])[0]\n            assert ""grad_input_1"" in grads\n            assert grads[""grad_input_1""] is not None\n            assert len(grads[""grad_input_1""][0]) == 5  # 9 words in hypothesis\n\n    def test_get_gradients_when_requires_grad_is_false(self):\n        inputs = {\n            ""sentence"": ""I always write unit tests"",\n        }\n\n        archive = load_archive(\n            self.FIXTURES_ROOT\n            / ""basic_classifier""\n            / ""embedding_with_trainable_is_false""\n            / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive)\n\n        # ensure that requires_grad is initially False on the embedding layer\n        embedding_layer = util.find_embedding_layer(predictor._model)\n        assert not embedding_layer.weight.requires_grad\n        instance = predictor._json_to_instance(inputs)\n        outputs = predictor._model.forward_on_instance(instance)\n        labeled_instances = predictor.predictions_to_labeled_instances(instance, outputs)\n        # ensure that gradients are always present, despite requires_grad being false on the embedding layer\n        for instance in labeled_instances:\n            grads = predictor.get_gradients([instance])[0]\n            assert bool(grads)\n        # ensure that no side effects remain\n        assert not embedding_layer.weight.requires_grad\n'"
tests/predictors/sentence_tagger_test.py,0,"b'from allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors import Predictor\n\n\nclass TestSentenceTaggerPredictor(AllenNlpTestCase):\n    def test_predictions_to_labeled_instances(self):\n        inputs = {""sentence"": ""Eric Wallace was an intern at AI2""}\n\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""simple_tagger"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive, ""sentence_tagger"")\n\n        instance = predictor._json_to_instance(inputs)\n        outputs = predictor._model.forward_on_instance(instance)\n        new_instances = predictor.predictions_to_labeled_instances(instance, outputs)\n        assert len(new_instances) > 1\n        for new_instance in new_instances:\n            assert ""tags"" in new_instance\n            assert len(new_instance[""tags""]) == 7  # 7 words in input\n'"
tests/predictors/text_classifier_test.py,0,"b'import math\n\nfrom pytest import approx\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors import Predictor\n\n\nclass TestTextClassifierPredictor(AllenNlpTestCase):\n    def test_uses_named_inputs(self):\n        inputs = {\n            ""sentence"": ""It was the ending that I hated. I was disappointed that it was so bad.""\n        }\n\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive, ""text_classifier"")\n        result = predictor.predict_json(inputs)\n\n        logits = result.get(""logits"")\n        assert logits is not None\n        assert isinstance(logits, list)\n        assert len(logits) == 2\n        assert all(isinstance(x, float) for x in logits)\n\n        probs = result.get(""probs"")\n        assert probs is not None\n        assert isinstance(probs, list)\n        assert len(probs) == 2\n        assert all(isinstance(x, float) for x in probs)\n        assert all(x >= 0 for x in probs)\n        assert sum(probs) == approx(1.0)\n\n        label = result.get(""label"")\n        assert label is not None\n        assert label in predictor._model.vocab.get_token_to_index_vocabulary(namespace=""labels"")\n\n        exps = [math.exp(x) for x in logits]\n        sum_exps = sum(exps)\n        for e, p in zip(exps, probs):\n            assert e / sum_exps == approx(p)\n\n    def test_batch_prediction(self):\n        batch_inputs = [\n            {""sentence"": ""It was the ending that I hated. I was disappointed that it was so bad.""},\n            {""sentence"": ""This one is honestly the worst movie I\'ve ever watched.""},\n        ]\n\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive, ""text_classifier"")\n        results = predictor.predict_batch_json(batch_inputs)\n        assert len(results) == 2\n\n        for result in results:\n            logits = result.get(""logits"")\n            assert logits is not None\n            assert isinstance(logits, list)\n            assert len(logits) == 2\n            assert all(isinstance(x, float) for x in logits)\n\n            probs = result.get(""probs"")\n            assert probs is not None\n            assert isinstance(probs, list)\n            assert len(probs) == 2\n            assert all(isinstance(x, float) for x in probs)\n            assert all(x >= 0 for x in probs)\n            assert sum(probs) == approx(1.0)\n\n            label = result.get(""label"")\n            assert label is not None\n            assert label in predictor._model.vocab.get_token_to_index_vocabulary(namespace=""labels"")\n\n            exps = [math.exp(x) for x in logits]\n            sum_exps = sum(exps)\n            for e, p in zip(exps, probs):\n                assert e / sum_exps == approx(p)\n\n    def test_predictions_to_labeled_instances(self):\n        inputs = {\n            ""sentence"": ""It was the ending that I hated. I was disappointed that it was so bad.""\n        }\n\n        archive = load_archive(\n            self.FIXTURES_ROOT / ""basic_classifier"" / ""serialization"" / ""model.tar.gz""\n        )\n        predictor = Predictor.from_archive(archive, ""text_classifier"")\n\n        instance = predictor._json_to_instance(inputs)\n        outputs = predictor._model.forward_on_instance(instance)\n        new_instances = predictor.predictions_to_labeled_instances(instance, outputs)\n        assert ""label"" in new_instances[0].fields\n        assert new_instances[0].fields[""label""] is not None\n        assert len(new_instances) == 1\n'"
tests/training/__init__.py,0,b''
tests/training/checkpointer_test.py,0,"b'import os\nimport re\nimport time\nfrom contextlib import contextmanager\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.params import Params\nfrom allennlp.training import Checkpointer, Trainer\n\n\nclass FakeTrainer(Trainer):\n    def __init__(self, model_state, training_states):\n        self._model_state = model_state\n        self._training_states = training_states\n\n    @contextmanager\n    def get_checkpoint_state(self):\n        yield self._model_state, self._training_states\n\n\nclass TestCheckpointer(AllenNlpTestCase):\n    def retrieve_and_delete_saved(self):\n        """"""\n        Helper function for the tests below. Finds the weight and training state files in\n        self.TEST_DIR, parses their names for the epochs that were saved, deletes them,\n        and returns the saved epochs as two lists of integers.\n        """"""\n        serialization_files = os.listdir(self.TEST_DIR)\n        model_checkpoints = [x for x in serialization_files if ""model_state_epoch"" in x]\n        found_model_epochs = [\n            int(re.search(r""model_state_epoch_([0-9\\.\\-]+)\\.th"", x).group(1))\n            for x in model_checkpoints\n        ]\n        for f in model_checkpoints:\n            os.remove(os.path.join(self.TEST_DIR, f))\n        training_checkpoints = [x for x in serialization_files if ""training_state_epoch"" in x]\n        found_training_epochs = [\n            int(re.search(r""training_state_epoch_([0-9\\.\\-]+)\\.th"", x).group(1))\n            for x in training_checkpoints\n        ]\n        for f in training_checkpoints:\n            os.remove(os.path.join(self.TEST_DIR, f))\n        return sorted(found_model_epochs), sorted(found_training_epochs)\n\n    def test_default(self):\n        """"""\n        Tests that the default behavior keeps just the last 2 checkpoints.\n        """"""\n        default_num_to_keep = 2\n        num_epochs = 30\n        target = list(range(num_epochs - default_num_to_keep, num_epochs))\n\n        checkpointer = Checkpointer(serialization_dir=self.TEST_DIR)\n\n        for e in range(num_epochs):\n            checkpointer.save_checkpoint(\n                epoch=e,\n                trainer=FakeTrainer(model_state={""epoch"": e}, training_states={""epoch"": e}),\n                is_best_so_far=False,\n            )\n        models, training = self.retrieve_and_delete_saved()\n        assert models == training == target\n\n    def test_keep_zero(self):\n        checkpointer = Checkpointer(\n            serialization_dir=self.TEST_DIR, num_serialized_models_to_keep=0\n        )\n        for e in range(10):\n            checkpointer.save_checkpoint(\n                epoch=e,\n                trainer=FakeTrainer(model_state={""epoch"": e}, training_states={""epoch"": e}),\n                is_best_so_far=True,\n            )\n        files = os.listdir(self.TEST_DIR)\n        assert ""model_state_epoch_1.th"" not in files\n        assert ""training_state_epoch_1.th"" not in files\n\n    def test_with_time(self):\n        """"""\n        Tests that keep_serialized_model_every_num_seconds parameter causes a checkpoint to be saved\n        after enough time has elapsed between epochs.\n        """"""\n        num_to_keep = 10\n        num_epochs = 30\n        target = list(range(num_epochs - num_to_keep, num_epochs))\n        pauses = [5, 18, 26]\n        target = sorted(set(target + pauses))\n        checkpointer = Checkpointer(\n            serialization_dir=self.TEST_DIR,\n            num_serialized_models_to_keep=num_to_keep,\n            keep_serialized_model_every_num_seconds=1,\n        )\n        for e in range(num_epochs):\n            if e in pauses:\n                time.sleep(2)\n            checkpointer.save_checkpoint(\n                epoch=e,\n                trainer=FakeTrainer(model_state={""epoch"": e}, training_states={""epoch"": e}),\n                is_best_so_far=False,\n            )\n        models, training = self.retrieve_and_delete_saved()\n        assert models == training == target\n\n    def test_registered_subclass(self):\n        """"""\n        Tests that registering Checkpointer subclasses works correctly.\n        """"""\n\n        @Checkpointer.register(""checkpointer_subclass"")\n        class CheckpointerSubclass(Checkpointer):\n            def __init__(self, x: int, y: int) -> None:\n                super().__init__()\n                self.x = x\n                self.y = y\n\n        sub_inst = Checkpointer.from_params(\n            Params({""type"": ""checkpointer_subclass"", ""x"": 1, ""y"": 3})\n        )\n        assert sub_inst.__class__ == CheckpointerSubclass\n        assert sub_inst.x == 1 and sub_inst.y == 3\n\n    def test_base_class_from_params(self):\n        Checkpointer.from_params(Params({}))\n'"
tests/training/moving_average_test.py,9,"b'from typing import Dict\n\nimport torch\nimport numpy as np\n\nfrom allennlp.common.params import Params\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.models.model import Model\nfrom allennlp.training.moving_average import MovingAverage, ExponentialMovingAverage\n\n\nclass MovingAverageTest(AllenNlpTestCase):\n    def test_from_params(self):\n        params = Params({""type"": ""exponential"", ""decay"": 0.99})\n\n        _ = MovingAverage.from_params(params, parameters=[])\n\n    def test_exponential_moving_average_without_steps(self):\n        param1 = torch.ones(5, 3)\n        param2 = torch.ones(2)\n        moving_average = ExponentialMovingAverage(\n            [(""param1"", param1), (""param2"", param2)], decay=0.9999\n        )\n\n        param1.data *= 5  # now all 5s\n        param2.data *= 10  # now all 10s\n        moving_average.apply()\n\n        param1.data *= 5  # now all 25s\n        param2.data *= 10  # now all 100s\n        moving_average.apply()\n\n        # Get shadow variables\n        moving_average.assign_average_value()\n\n        np.testing.assert_array_almost_equal(\n            param1, 1 * 0.9999 ** 2 + 5 * 0.9999 * 0.0001 + 25 * 0.0001\n        )\n        np.testing.assert_array_almost_equal(\n            param2, 1 * 0.9999 ** 2 + 10 * 0.9999 * 0.0001 + 100 * 0.0001\n        )\n\n        # Restore original variables\n        moving_average.restore()\n        np.testing.assert_array_almost_equal(param1, 25)\n        np.testing.assert_array_almost_equal(param2, 100)\n\n    def test_exponential_moving_average_num_updates(self):\n        param1 = torch.ones(5, 3)\n        param2 = torch.ones(2)\n        moving_average = ExponentialMovingAverage(\n            [(""param1"", param1), (""param2"", param2)], decay=0.9999\n        )\n\n        param1.data *= 5  # now all 5s\n        param2.data *= 10  # now all 10s\n        moving_average.apply(num_updates=100)  # 101 / 110 ~ 0.92 < 0.9999\n\n        param1.data *= 5  # now all 25s\n        param2.data *= 10  # now all 100s\n        moving_average.apply(num_updates=1_000_000)  # 1_000_001 / 1_000_010 ~ .999991 > .9999\n\n        # Get shadow variables\n        moving_average.assign_average_value()\n\n        np.testing.assert_array_almost_equal(\n            param1, 1 * (101 / 110) * 0.9999 + 5 * (9 / 110) * 0.9999 + 25 * 0.0001\n        )\n\n        np.testing.assert_array_almost_equal(\n            param2, 1 * (101 / 110) * 0.9999 + 10 * (9 / 110) * 0.9999 + 100 * 0.0001\n        )\n\n        # Restore original variables\n        moving_average.restore()\n        np.testing.assert_array_almost_equal(param1, 25)\n        np.testing.assert_array_almost_equal(param2, 100)\n\n    def test_works_with_model(self):\n        class FakeModel(Model):\n            def __init__(self) -> None:\n                super().__init__(None)\n                self.w = torch.nn.Parameter(torch.randn(1))\n\n            def forward(self, t: torch.Tensor) -> Dict[str, torch.Tensor]:  # type: ignore\n                return {""loss"": (t * self.w).sum()}\n\n        model = FakeModel()\n        moving_average = ExponentialMovingAverage(model.named_parameters())\n\n        optimizer = torch.optim.SGD(list(model.parameters()), lr=0.1)\n\n        for _ in range(10):\n            optimizer.zero_grad()\n            t = torch.randn(10)\n            loss = model.forward(t)[""loss""]\n            loss.backward()\n            optimizer.step()\n            moving_average.apply()\n\n        w_value = model.w.item()\n        shadow_value = moving_average._shadows[""w""].item()\n\n        assert w_value != shadow_value\n\n        moving_average.assign_average_value()\n\n        assert model.w.item() == shadow_value\n\n        moving_average.restore()\n\n        assert model.w.item() == w_value\n\n        # Now keep training:\n\n        for _ in range(10):\n            optimizer.zero_grad()\n            t = torch.randn(10)\n            loss = model.forward(t)[""loss""]\n            loss.backward()\n            optimizer.step()\n            moving_average.apply()\n'"
tests/training/no_op_trainer_test.py,2,"b'import os\nfrom typing import Dict\n\nimport torch\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Vocabulary\nfrom allennlp.data.dataset_readers import SequenceTaggingDatasetReader\nfrom allennlp.models.model import Model\nfrom allennlp.training import NoOpTrainer\n\n\nclass ConstantModel(Model):\n    def forward(self, *inputs) -> Dict[str, torch.Tensor]:\n        return {""class"": torch.tensor(98)}\n\n\nclass TestNoOpTrainer(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.instances = SequenceTaggingDatasetReader().read(\n            self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""\n        )\n        vocab = Vocabulary.from_instances(self.instances)\n        self.vocab = vocab\n        self.model = ConstantModel(vocab)\n\n    def test_trainer_serializes(self):\n        serialization_dir = self.TEST_DIR / ""serialization_dir""\n        trainer = NoOpTrainer(serialization_dir=serialization_dir, model=self.model)\n        metrics = trainer.train()\n        assert metrics == {}\n        assert os.path.exists(serialization_dir / ""best.th"")\n        assert os.path.exists(serialization_dir / ""vocabulary"")\n'"
tests/training/optimizer_test.py,0,"b'from allennlp.common.params import Params\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Vocabulary\nfrom allennlp.data.dataset_readers import SequenceTaggingDatasetReader\nfrom allennlp.data import DataLoader\nfrom allennlp.models.simple_tagger import SimpleTagger\nfrom allennlp.training import GradientDescentTrainer\nfrom allennlp.training.optimizers import Optimizer\n\n\nclass TestOptimizer(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.instances = SequenceTaggingDatasetReader().read(\n            self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""\n        )\n        vocab = Vocabulary.from_instances(self.instances)\n        self.model_params = Params(\n            {\n                ""text_field_embedder"": {\n                    ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                },\n                ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n            }\n        )\n        self.model = SimpleTagger.from_params(vocab=vocab, params=self.model_params)\n\n    def test_optimizer_basic(self):\n        optimizer_params = Params({""type"": ""sgd"", ""lr"": 1})\n        parameters = [[n, p] for n, p in self.model.named_parameters() if p.requires_grad]\n        optimizer = Optimizer.from_params(model_parameters=parameters, params=optimizer_params)\n        param_groups = optimizer.param_groups\n        assert len(param_groups) == 1\n        assert param_groups[0][""lr""] == 1\n\n    def test_optimizer_parameter_groups(self):\n        optimizer_params = Params(\n            {\n                ""type"": ""sgd"",\n                ""lr"": 1,\n                ""momentum"": 5,\n                ""parameter_groups"": [\n                    # the repeated ""bias_"" checks a corner case\n                    # NOT_A_VARIABLE_NAME displays a warning but does not raise an exception\n                    [[""weight_i"", ""bias_"", ""bias_"", ""NOT_A_VARIABLE_NAME""], {""lr"": 2}],\n                    [[""tag_projection_layer""], {""lr"": 3}],\n                ],\n            }\n        )\n        parameters = [[n, p] for n, p in self.model.named_parameters() if p.requires_grad]\n        optimizer = Optimizer.from_params(model_parameters=parameters, params=optimizer_params)\n        param_groups = optimizer.param_groups\n\n        assert len(param_groups) == 3\n        assert param_groups[0][""lr""] == 2\n        assert param_groups[1][""lr""] == 3\n        # base case uses default lr\n        assert param_groups[2][""lr""] == 1\n        for k in range(3):\n            assert param_groups[k][""momentum""] == 5\n\n        # all LSTM parameters except recurrent connections (those with weight_h in name)\n        assert len(param_groups[0][""params""]) == 6\n        # just the projection weight and bias\n        assert len(param_groups[1][""params""]) == 2\n        # the embedding + recurrent connections left in the default group\n        assert len(param_groups[2][""params""]) == 3\n\n\nclass TestDenseSparseAdam(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.instances = SequenceTaggingDatasetReader().read(\n            self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""\n        )\n        self.vocab = Vocabulary.from_instances(self.instances)\n        self.model_params = Params(\n            {\n                ""text_field_embedder"": {\n                    ""token_embedders"": {\n                        ""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5, ""sparse"": True}\n                    }\n                },\n                ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n            }\n        )\n        self.model = SimpleTagger.from_params(vocab=self.vocab, params=self.model_params)\n\n    def test_can_optimise_model_with_dense_and_sparse_params(self):\n        optimizer_params = Params({""type"": ""dense_sparse_adam""})\n        parameters = [[n, p] for n, p in self.model.named_parameters() if p.requires_grad]\n        optimizer = Optimizer.from_params(model_parameters=parameters, params=optimizer_params)\n        self.instances.index_with(self.vocab)\n        GradientDescentTrainer(self.model, optimizer, DataLoader(self.instances, 2)).train()\n'"
tests/training/trainer_test.py,7,"b'import copy\nimport glob\nimport json\nimport os\nimport re\nimport time\nfrom typing import Any, Dict, List\n\nimport math\nimport pytest\n\ntry:\n    from apex import amp\nexcept ImportError:\n    amp = None\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils import clip_grad_norm_\nfrom allennlp.data.dataloader import DataLoader as AllennlpDataLoader\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.params import Params\nfrom allennlp.common.testing import AllenNlpTestCase, requires_gpu, requires_multi_gpu\nfrom allennlp.data import Vocabulary\nfrom allennlp.data.dataloader import TensorDict\nfrom allennlp.data.dataset_readers import SequenceTaggingDatasetReader\nfrom allennlp.models.model import Model\nfrom allennlp.models.simple_tagger import SimpleTagger\nfrom allennlp.training import (\n    GradientDescentTrainer,\n    Checkpointer,\n    TensorboardWriter,\n    BatchCallback,\n    EpochCallback,\n)\nfrom allennlp.training.learning_rate_schedulers import CosineWithRestarts\nfrom allennlp.training.learning_rate_schedulers import ExponentialLearningRateScheduler\nfrom allennlp.training.momentum_schedulers import MomentumScheduler\nfrom allennlp.training.moving_average import ExponentialMovingAverage\nfrom allennlp.data import allennlp_collate\n\n\nclass TrainerTestBase(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.instances = SequenceTaggingDatasetReader().read(\n            self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""\n        )\n        self.instances_lazy = SequenceTaggingDatasetReader(lazy=True).read(\n            self.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""\n        )\n        vocab = Vocabulary.from_instances(self.instances)\n        self.vocab = vocab\n        self.model_params = Params(\n            {\n                ""text_field_embedder"": {\n                    ""token_embedders"": {""tokens"": {""type"": ""embedding"", ""embedding_dim"": 5}}\n                },\n                ""encoder"": {""type"": ""lstm"", ""input_size"": 5, ""hidden_size"": 7, ""num_layers"": 2},\n            }\n        )\n        self.model = SimpleTagger.from_params(vocab=self.vocab, params=self.model_params)\n        self.optimizer = torch.optim.SGD(self.model.parameters(), 0.01, momentum=0.9)\n        self.data_loader = DataLoader(self.instances, batch_size=2, collate_fn=allennlp_collate)\n        self.data_loader_lazy = DataLoader(\n            self.instances_lazy, batch_size=2, collate_fn=allennlp_collate\n        )\n        self.validation_data_loader = DataLoader(\n            self.instances, batch_size=2, collate_fn=allennlp_collate\n        )\n        self.instances.index_with(vocab)\n        self.instances_lazy.index_with(vocab)\n\n\nclass TestTrainer(TrainerTestBase):\n    def test_trainer_can_run(self):\n        trainer = GradientDescentTrainer(\n            model=self.model,\n            optimizer=self.optimizer,\n            data_loader=self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=2,\n        )\n        metrics = trainer.train()\n        assert ""best_validation_loss"" in metrics\n        assert isinstance(metrics[""best_validation_loss""], float)\n        assert ""best_validation_accuracy"" in metrics\n        assert isinstance(metrics[""best_validation_accuracy""], float)\n        assert ""best_validation_accuracy3"" in metrics\n        assert isinstance(metrics[""best_validation_accuracy3""], float)\n        assert ""best_epoch"" in metrics\n        assert isinstance(metrics[""best_epoch""], int)\n\n        # Making sure that both increasing and decreasing validation metrics work.\n        trainer = GradientDescentTrainer(\n            model=self.model,\n            optimizer=self.optimizer,\n            data_loader=self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            validation_metric=""+loss"",\n            num_epochs=2,\n        )\n        metrics = trainer.train()\n        assert ""best_validation_loss"" in metrics\n        assert isinstance(metrics[""best_validation_loss""], float)\n        assert ""best_validation_accuracy"" in metrics\n        assert isinstance(metrics[""best_validation_accuracy""], float)\n        assert ""best_validation_accuracy3"" in metrics\n        assert isinstance(metrics[""best_validation_accuracy3""], float)\n        assert ""best_epoch"" in metrics\n        assert isinstance(metrics[""best_epoch""], int)\n        assert ""peak_worker_0_memory_MB"" in metrics\n        assert isinstance(metrics[""peak_worker_0_memory_MB""], float)\n        assert metrics[""peak_worker_0_memory_MB""] > 0\n\n    def test_trainer_can_run_exponential_moving_average(self):\n        moving_average = ExponentialMovingAverage(self.model.named_parameters(), decay=0.9999)\n        trainer = GradientDescentTrainer(\n            model=self.model,\n            optimizer=self.optimizer,\n            data_loader=self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=2,\n            moving_average=moving_average,\n        )\n        trainer.train()\n\n    @requires_gpu\n    def test_trainer_can_run_cuda(self):\n        self.model.cuda()\n        trainer = GradientDescentTrainer(\n            self.model, self.optimizer, self.data_loader, num_epochs=2, cuda_device=0\n        )\n        metrics = trainer.train()\n        assert ""peak_worker_0_memory_MB"" in metrics\n        assert isinstance(metrics[""peak_worker_0_memory_MB""], float)\n        assert metrics[""peak_worker_0_memory_MB""] > 0\n        assert ""peak_gpu_0_memory_MB"" in metrics\n        assert isinstance(metrics[""peak_gpu_0_memory_MB""], int)\n\n    @requires_multi_gpu\n    def test_passing_trainer_multiple_gpus_raises_error(self):\n        self.model.cuda()\n\n        with pytest.raises(ConfigurationError):\n            GradientDescentTrainer(\n                self.model, self.optimizer, self.data_loader, num_epochs=2, cuda_device=[0, 1],\n            )\n\n    def test_data_loader_lazy_epoch_size_correct(self):\n        num_epochs = 3\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader_lazy,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=self.TEST_DIR,\n        )\n        assert trainer._batch_num_total == 0\n        metrics = trainer.train()\n        epoch = metrics[""epoch""]\n        assert epoch == num_epochs - 1\n        assert trainer._batch_num_total == num_epochs * 2\n\n    def test_data_loader_lazy_epoch_size_correct_custom_epoch_size(self):\n        batches_per_epoch = 3\n        num_epochs = 3\n        data_loader_custom_epoch_lazy = AllennlpDataLoader(\n            self.instances_lazy,\n            batch_size=2,\n            collate_fn=allennlp_collate,\n            batches_per_epoch=batches_per_epoch,\n        )\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            data_loader_custom_epoch_lazy,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=self.TEST_DIR,\n        )\n        assert trainer._batch_num_total == 0\n        metrics = trainer.train()\n        epoch = metrics[""epoch""]\n        assert epoch == num_epochs - 1\n        assert trainer._batch_num_total == num_epochs * batches_per_epoch\n\n    def test_trainer_respects_epoch_size_equals_total(self):\n        batches_per_epoch = 4\n        num_epochs = 3\n        data_loader_equal_epoch = AllennlpDataLoader(\n            self.instances,\n            batch_size=2,\n            collate_fn=allennlp_collate,\n            batches_per_epoch=batches_per_epoch,\n        )\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            data_loader_equal_epoch,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=self.TEST_DIR,\n        )\n        assert trainer._batch_num_total == 0\n        metrics = trainer.train()\n        epoch = metrics[""epoch""]\n        assert epoch == num_epochs - 1\n        assert trainer._batch_num_total == num_epochs * batches_per_epoch\n\n    def test_trainer_respects_epoch_size_larger_tnan_total(self):\n        batches_per_epoch = 7\n        num_epochs = 3\n        data_loader_larger_epoch = AllennlpDataLoader(\n            self.instances,\n            batch_size=2,\n            collate_fn=allennlp_collate,\n            batches_per_epoch=batches_per_epoch,\n        )\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            data_loader_larger_epoch,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=self.TEST_DIR,\n        )\n        assert trainer._batch_num_total == 0\n        metrics = trainer.train()\n        epoch = metrics[""epoch""]\n        assert epoch == num_epochs - 1\n        assert trainer._batch_num_total == num_epochs * batches_per_epoch\n\n    def test_trainer_respects_epoch_size_smaller_tnan_total(self):\n        batches_per_epoch = 1\n        num_epochs = 2\n        data_loader_smaller_epoch = AllennlpDataLoader(\n            self.instances,\n            batch_size=2,\n            collate_fn=allennlp_collate,\n            batches_per_epoch=batches_per_epoch,\n        )\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            data_loader_smaller_epoch,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=self.TEST_DIR,\n        )\n        assert trainer._batch_num_total == 0\n        metrics = trainer.train()\n        epoch = metrics[""epoch""]\n        assert epoch == num_epochs - 1\n        assert trainer._batch_num_total == num_epochs * batches_per_epoch\n\n    def test_trainer_can_resume_training(self):\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=1,\n            serialization_dir=self.TEST_DIR,\n        )\n        trainer.train()\n        new_trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=3,\n            serialization_dir=self.TEST_DIR,\n        )\n\n        epoch = new_trainer._restore_checkpoint()\n        assert epoch == 1\n\n        tracker = trainer._metric_tracker\n        assert tracker.is_best_so_far()\n        assert tracker._best_so_far is not None\n\n        new_trainer.train()\n\n    def test_trainer_can_resume_training_for_exponential_moving_average(self):\n        moving_average = ExponentialMovingAverage(self.model.named_parameters())\n\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=1,\n            serialization_dir=self.TEST_DIR,\n            moving_average=moving_average,\n        )\n        trainer.train()\n\n        new_moving_average = ExponentialMovingAverage(self.model.named_parameters())\n        new_trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=3,\n            serialization_dir=self.TEST_DIR,\n            moving_average=new_moving_average,\n        )\n\n        epoch = new_trainer._restore_checkpoint()\n        assert epoch == 1\n\n        tracker = trainer._metric_tracker\n        assert tracker.is_best_so_far()\n        assert tracker._best_so_far is not None\n\n        new_trainer.train()\n\n    def test_metric_only_considered_best_so_far_when_strictly_better_than_those_before_it_increasing_metric(\n        self,\n    ):\n        new_trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=3,\n            serialization_dir=self.TEST_DIR,\n            patience=5,\n            validation_metric=""+test"",\n        )\n        tracker = new_trainer._metric_tracker\n\n        # when it is the only metric it should be considered the best\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metric(1)\n        assert new_tracker.is_best_so_far()\n\n        # when it is the same as one before it it is not considered the best\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metrics([0.3, 0.3, 0.3, 0.2, 0.5, 0.1, 0.3])\n        assert not new_tracker.is_best_so_far()\n\n        # when it is the best it is considered the best\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metrics([0.3, 0.3, 0.3, 0.2, 0.5, 0.1, 13])\n        assert new_tracker.is_best_so_far()\n\n        # when it is not the the best it is not considered the best\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metrics([0.3, 0.3, 0.3, 0.2, 0.5, 0.1, 0.0013])\n        assert not new_tracker.is_best_so_far()\n\n    def test_metric_only_considered_best_so_far_when_strictly_better_than_those_before_it_decreasing_metric(\n        self,\n    ):\n        new_trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=3,\n            serialization_dir=self.TEST_DIR,\n            patience=5,\n            validation_metric=""-test"",\n        )\n        tracker = new_trainer._metric_tracker\n\n        # when it is the only metric it should be considered the best\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metric(1)\n        assert new_tracker.is_best_so_far()\n\n        # when it is the same as one before it it is not considered the best\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metrics([0.3, 0.3, 0.3, 0.2, 0.5, 0.1, 0.3])\n        assert not new_tracker.is_best_so_far()\n\n        # when it is the best it is considered the best\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metrics([0.3, 0.3, 0.3, 0.2, 0.5, 0.1, 0.0013])\n        assert new_tracker.is_best_so_far()\n\n        # when it is not the the best it is not considered the best\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metrics([0.3, 0.3, 0.3, 0.2, 0.5, 0.1, 13])\n\n    def test_should_stop_early_with_increasing_metric(self):\n        new_trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=3,\n            serialization_dir=self.TEST_DIR,\n            patience=5,\n            validation_metric=""+test"",\n        )\n\n        tracker = new_trainer._metric_tracker\n\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metrics([0.5, 0.3, 0.2, 0.1, 0.4, 0.4])\n        assert new_tracker.should_stop_early()\n\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metrics([0.3, 0.3, 0.3, 0.2, 0.5, 0.1])\n        assert not new_tracker.should_stop_early()\n\n    def test_should_stop_early_with_flat_lining_metric(self):\n        flatline = [0.2] * 6\n        tracker = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=3,\n            serialization_dir=self.TEST_DIR,\n            patience=5,\n            validation_metric=""+test"",\n        )._metric_tracker\n        tracker.add_metrics(flatline)\n        assert tracker.should_stop_early\n\n        tracker = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=3,\n            serialization_dir=self.TEST_DIR,\n            patience=5,\n            validation_metric=""-test"",\n        )._metric_tracker\n        tracker.add_metrics(flatline)\n        assert tracker.should_stop_early\n\n    def test_should_stop_early_with_decreasing_metric(self):\n        new_trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=3,\n            serialization_dir=self.TEST_DIR,\n            patience=5,\n            validation_metric=""-test"",\n        )\n        tracker = new_trainer._metric_tracker\n\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metrics([0.02, 0.3, 0.2, 0.1, 0.4, 0.4])\n        assert new_tracker.should_stop_early()\n\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metrics([0.3, 0.3, 0.2, 0.1, 0.4, 0.5])\n        assert not new_tracker.should_stop_early()\n\n        new_tracker = copy.deepcopy(tracker)\n        new_tracker.add_metrics([0.1, 0.3, 0.2, 0.1, 0.4, 0.5])\n        assert new_tracker.should_stop_early()\n\n    def test_should_stop_early_with_early_stopping_disabled(self):\n        # Increasing metric\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=100,\n            patience=None,\n            validation_metric=""+test"",\n        )\n        tracker = trainer._metric_tracker\n        tracker.add_metrics([float(i) for i in reversed(range(20))])\n        assert not tracker.should_stop_early()\n\n        # Decreasing metric\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=100,\n            patience=None,\n            validation_metric=""-test"",\n        )\n        tracker = trainer._metric_tracker\n        tracker.add_metrics([float(i) for i in range(20)])\n        assert not tracker.should_stop_early()\n\n    def test_should_stop_early_with_invalid_patience(self):\n        for patience in [0, -1, -2, 1.5, ""None""]:\n            with pytest.raises(\n                ConfigurationError,\n                match=\'.* is an invalid value for ""patience"": \'\n                ""it must be a positive integer or None ""\n                ""\\\\(if you want to disable early stopping\\\\)"",\n            ):\n                GradientDescentTrainer(\n                    self.model,\n                    self.optimizer,\n                    self.data_loader,\n                    validation_data_loader=self.validation_data_loader,\n                    num_epochs=100,\n                    patience=patience,\n                    validation_metric=""+test"",\n                )\n\n    def test_trainer_can_run_and_resume_with_momentum_scheduler(self):\n        scheduler = MomentumScheduler.from_params(\n            optimizer=self.optimizer,\n            params=Params({""type"": ""inverted_triangular"", ""cool_down"": 2, ""warm_up"": 2}),\n        )\n        trainer = GradientDescentTrainer(\n            model=self.model,\n            optimizer=self.optimizer,\n            data_loader=self.data_loader,\n            momentum_scheduler=scheduler,\n            validation_metric=""-loss"",\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=4,\n            serialization_dir=self.TEST_DIR,\n        )\n        trainer.train()\n\n        new_scheduler = MomentumScheduler.from_params(\n            optimizer=self.optimizer,\n            params=Params({""type"": ""inverted_triangular"", ""cool_down"": 2, ""warm_up"": 2}),\n        )\n        new_trainer = GradientDescentTrainer(\n            model=self.model,\n            optimizer=self.optimizer,\n            data_loader=self.data_loader,\n            momentum_scheduler=new_scheduler,\n            validation_metric=""-loss"",\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=6,\n            serialization_dir=self.TEST_DIR,\n        )\n        epoch = new_trainer._restore_checkpoint()\n        assert epoch == 4\n        assert new_trainer._momentum_scheduler.last_epoch == 3\n        new_trainer.train()\n\n    def test_trainer_can_run_with_lr_scheduler(self):\n        lr_scheduler = ExponentialLearningRateScheduler(self.optimizer, gamma=0.5)\n        trainer = GradientDescentTrainer(\n            model=self.model,\n            optimizer=self.optimizer,\n            data_loader=self.data_loader,\n            learning_rate_scheduler=lr_scheduler,\n            validation_metric=""-loss"",\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=2,\n        )\n        trainer.train()\n\n    def test_trainer_can_resume_with_lr_scheduler(self):\n        lr_scheduler = CosineWithRestarts(self.optimizer, t_initial=5)\n        trainer = GradientDescentTrainer(\n            model=self.model,\n            optimizer=self.optimizer,\n            data_loader=self.data_loader,\n            learning_rate_scheduler=lr_scheduler,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=2,\n            serialization_dir=self.TEST_DIR,\n        )\n        trainer.train()\n\n        new_lr_scheduler = CosineWithRestarts(self.optimizer, t_initial=5)\n        new_trainer = GradientDescentTrainer(\n            model=self.model,\n            optimizer=self.optimizer,\n            data_loader=self.data_loader,\n            learning_rate_scheduler=new_lr_scheduler,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=4,\n            serialization_dir=self.TEST_DIR,\n        )\n        epoch = new_trainer._restore_checkpoint()\n        assert epoch == 2\n        assert new_trainer._learning_rate_scheduler.last_epoch == 1\n        new_trainer.train()\n\n    def test_trainer_raises_on_model_with_no_loss_key(self):\n        class FakeModel(Model):\n            def forward(self, **kwargs):\n                return {}\n\n        with pytest.raises(RuntimeError):\n            trainer = GradientDescentTrainer(\n                FakeModel(None),\n                self.optimizer,\n                self.data_loader,\n                num_epochs=2,\n                serialization_dir=self.TEST_DIR,\n            )\n            trainer.train()\n\n    def test_trainer_can_log_histograms(self):\n        # enable activation logging\n        for module in self.model.modules():\n            module.should_log_activations = True\n\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            num_epochs=3,\n            serialization_dir=self.TEST_DIR,\n            tensorboard_writer=TensorboardWriter(\n                serialization_dir=self.TEST_DIR, histogram_interval=2\n            ),\n        )\n        trainer.train()\n\n    def test_trainer_respects_num_serialized_models_to_keep(self):\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            num_epochs=5,\n            serialization_dir=self.TEST_DIR,\n            checkpointer=Checkpointer(\n                serialization_dir=self.TEST_DIR, num_serialized_models_to_keep=3\n            ),\n        )\n        trainer.train()\n\n        # Now check the serialized files\n        for prefix in [""model_state_epoch_*"", ""training_state_epoch_*""]:\n            file_names = glob.glob(os.path.join(self.TEST_DIR, prefix))\n            epochs = [int(re.search(r""_([0-9])\\.th"", fname).group(1)) for fname in file_names]\n            assert sorted(epochs) == [2, 3, 4]\n\n    def test_trainer_saves_metrics_every_epoch(self):\n        trainer = GradientDescentTrainer(\n            model=self.model,\n            optimizer=self.optimizer,\n            data_loader=self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=5,\n            serialization_dir=self.TEST_DIR,\n            checkpointer=Checkpointer(\n                serialization_dir=self.TEST_DIR, num_serialized_models_to_keep=3\n            ),\n        )\n        trainer.train()\n\n        for epoch in range(5):\n            epoch_file = self.TEST_DIR / f""metrics_epoch_{epoch}.json""\n            assert epoch_file.exists()\n            metrics = json.load(open(epoch_file))\n            assert ""validation_loss"" in metrics\n            assert ""best_validation_loss"" in metrics\n            assert metrics.get(""epoch"") == epoch\n\n    def test_trainer_respects_keep_serialized_model_every_num_seconds(self):\n        # To test:\n        #   Create an fake data loader that sleeps for 2.5 second per epoch, so the total\n        #   training time for one epoch is slightly greater then 2.5 seconds.\n        #   Run for 6 epochs, keeping the last 2 models, models also kept every 5 seconds.\n        #   Check the resulting checkpoints.  Should then have models at epochs\n        #       2, 4, plus the last two at 5 and 6.\n\n        class SlowDataLoader:\n            data_loader = DataLoader(self.instances, batch_size=2, collate_fn=allennlp_collate)\n\n            def __iter__(self):\n                time.sleep(2.5)\n                return iter(self.data_loader)\n\n            def __len__(self):\n                return len(self.data_loader)\n\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            SlowDataLoader(),\n            num_epochs=6,\n            serialization_dir=self.TEST_DIR,\n            checkpointer=Checkpointer(\n                serialization_dir=self.TEST_DIR,\n                num_serialized_models_to_keep=2,\n                keep_serialized_model_every_num_seconds=5,\n            ),\n        )\n        trainer.train()\n\n        # Now check the serialized files\n        for prefix in [""model_state_epoch_*"", ""training_state_epoch_*""]:\n            file_names = glob.glob(os.path.join(self.TEST_DIR, prefix))\n            epochs = [int(re.search(r""_([0-9])\\.th"", fname).group(1)) for fname in file_names]\n            # epoch N has N-1 in file name\n            assert sorted(epochs) == [1, 3, 4, 5]\n\n    def test_trainer_can_log_learning_rates_tensorboard(self):\n        data_loader = DataLoader(self.instances, batch_size=4, collate_fn=allennlp_collate)\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            data_loader,\n            num_epochs=2,\n            serialization_dir=self.TEST_DIR,\n            tensorboard_writer=TensorboardWriter(\n                serialization_dir=self.TEST_DIR, should_log_learning_rate=True, summary_interval=2,\n            ),\n        )\n\n        trainer.train()\n\n    def test_trainer_saves_models_at_specified_interval(self):\n        data_loader = DataLoader(self.instances, batch_size=4, collate_fn=allennlp_collate)\n\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            data_loader,\n            num_epochs=2,\n            serialization_dir=self.TEST_DIR,\n            checkpointer=Checkpointer(\n                serialization_dir=self.TEST_DIR,\n                model_save_interval=0.0001,\n                num_serialized_models_to_keep=10,\n            ),\n        )\n\n        trainer.train()\n\n        # Now check the serialized files for models saved during the epoch.\n        prefix = ""model_state_epoch_*""\n        file_names = sorted(glob.glob(os.path.join(self.TEST_DIR, prefix)))\n        epochs = [re.search(r""_([0-9\\.\\-]+)\\.th"", fname).group(1) for fname in file_names]\n        # We should have checkpoints at the end of each epoch and during each, e.g.\n        # [0.timestamp, 0, 1.timestamp, 1]\n        assert len(epochs) == 4\n        assert epochs[3] == ""1""\n        assert ""."" in epochs[0]\n\n        # Now make certain we can restore from timestamped checkpoint.\n        # To do so, remove the checkpoint from the end of epoch 1&2, so\n        # that we are forced to restore from the timestamped checkpoints.\n        for k in range(2):\n            os.remove(os.path.join(self.TEST_DIR, ""model_state_epoch_{}.th"".format(k)))\n            os.remove(os.path.join(self.TEST_DIR, ""training_state_epoch_{}.th"".format(k)))\n        os.remove(os.path.join(self.TEST_DIR, ""best.th""))\n\n        restore_trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            num_epochs=2,\n            serialization_dir=self.TEST_DIR,\n            checkpointer=Checkpointer(serialization_dir=self.TEST_DIR, model_save_interval=0.0001),\n        )\n        epoch = restore_trainer._restore_checkpoint()\n        assert epoch == 2\n        # One batch per epoch.\n        assert restore_trainer._batch_num_total == 2\n\n    def test_trainer_saves_and_loads_best_validation_metrics_correctly_1(self):\n        # Use -loss and run 1 epoch of original-training, and one of restored-training\n        # Run 1 epoch of original training.\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            validation_metric=""-loss"",\n            num_epochs=1,\n            serialization_dir=self.TEST_DIR,\n        )\n        trainer.train()\n        _ = trainer._restore_checkpoint()\n        best_epoch_1 = trainer._metric_tracker.best_epoch\n        best_validation_metrics_epoch_1 = trainer._metric_tracker.best_epoch_metrics\n        # best_validation_metrics_epoch_1: {\'accuracy\': 0.75, \'accuracy3\': 1.0, \'loss\': 0.6243013441562653}\n        assert isinstance(best_validation_metrics_epoch_1, dict)\n        assert ""loss"" in best_validation_metrics_epoch_1\n\n        # Run 1 epoch of restored training.\n        restore_trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            validation_metric=""-loss"",\n            num_epochs=2,\n            serialization_dir=self.TEST_DIR,\n        )\n        restore_trainer.train()\n        _ = restore_trainer._restore_checkpoint()\n        best_epoch_2 = restore_trainer._metric_tracker.best_epoch\n        best_validation_metrics_epoch_2 = restore_trainer._metric_tracker.best_epoch_metrics\n\n        # Because of using -loss, 2nd epoch would be better than 1st. So best val metrics should not be same.\n        assert best_epoch_1 == 0 and best_epoch_2 == 1\n        assert best_validation_metrics_epoch_2 != best_validation_metrics_epoch_1\n\n    def test_trainer_saves_and_loads_best_validation_metrics_correctly_2(self):\n        # Use -loss and run 1 epoch of original-training, and one of restored-training\n        # Run 1 epoch of original training.\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            validation_metric=""+loss"",\n            num_epochs=1,\n            serialization_dir=self.TEST_DIR,\n        )\n        trainer.train()\n\n        _ = trainer._restore_checkpoint()\n        best_epoch_1 = trainer._metric_tracker.best_epoch\n        best_validation_metrics_epoch_1 = trainer._metric_tracker.best_epoch_metrics\n        # best_validation_metrics_epoch_1: {\'accuracy\': 0.75, \'accuracy3\': 1.0, \'loss\': 0.6243013441562653}\n        assert isinstance(best_validation_metrics_epoch_1, dict)\n        assert ""loss"" in best_validation_metrics_epoch_1\n\n        # Run 1 more epoch of restored training.\n        restore_trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            validation_metric=""+loss"",\n            num_epochs=2,\n            serialization_dir=self.TEST_DIR,\n        )\n        restore_trainer.train()\n        _ = restore_trainer._restore_checkpoint()\n        best_epoch_2 = restore_trainer._metric_tracker.best_epoch\n        best_validation_metrics_epoch_2 = restore_trainer._metric_tracker.best_epoch_metrics\n\n        # Because of using +loss, 2nd epoch won\'t be better than 1st. So best val metrics should be same.\n        assert best_epoch_1 == best_epoch_2 == 0\n        assert best_validation_metrics_epoch_2 == best_validation_metrics_epoch_1\n\n    def test_restored_training_returns_best_epoch_metrics_even_if_no_better_epoch_is_found_after_restoring(\n        self,\n    ):\n        # Instead of -loss, use +loss to assure 2nd epoch is considered worse.\n        # Run 1 epoch of original training.\n        original_trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            validation_metric=""+loss"",\n            num_epochs=1,\n            serialization_dir=self.TEST_DIR,\n        )\n        training_metrics = original_trainer.train()\n\n        # Run 1 epoch of restored training.\n        restored_trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            validation_metric=""+loss"",\n            num_epochs=2,\n            serialization_dir=self.TEST_DIR,\n        )\n        restored_metrics = restored_trainer.train()\n\n        assert ""best_validation_loss"" in restored_metrics\n        assert ""best_validation_accuracy"" in restored_metrics\n        assert ""best_validation_accuracy3"" in restored_metrics\n        assert ""best_epoch"" in restored_metrics\n\n        # Epoch 2 validation loss should be lesser than that of Epoch 1\n        assert training_metrics[""best_validation_loss""] == restored_metrics[""best_validation_loss""]\n        assert training_metrics[""best_epoch""] == 0\n        assert training_metrics[""validation_loss""] > restored_metrics[""validation_loss""]\n\n    def test_restoring_works_with_older_checkpointing(self):\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=3,\n            serialization_dir=self.TEST_DIR,\n            checkpointer=Checkpointer(\n                serialization_dir=self.TEST_DIR, num_serialized_models_to_keep=4\n            ),\n        )\n        trainer.train()\n\n        for index in range(3):\n            path = str(self.TEST_DIR / ""training_state_epoch_{}.th"".format(index))\n            state = torch.load(path)\n            state.pop(""metric_tracker"")\n            state.pop(""batch_num_total"")\n            state[""val_metric_per_epoch""] = [0.4, 0.1, 0.8]\n            torch.save(state, path)\n\n        next_epoch = trainer._restore_checkpoint()\n        best_epoch = trainer._metric_tracker.best_epoch\n\n        # Loss decreases in 3 epochs, but because we hard fed the val metrics as above:\n        assert next_epoch == 3\n        assert best_epoch == 1\n        assert trainer._metric_tracker._best_so_far == 0.1\n        assert trainer._metric_tracker._epochs_with_no_improvement == 1\n\n    def test_trainer_can_run_gradient_accumulation(self):\n        instances = list(self.instances)\n        steps_to_accumulate = 2\n\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            validation_data_loader=self.validation_data_loader,\n            num_epochs=2,\n            num_gradient_accumulation_steps=steps_to_accumulate,\n        )\n        assert trainer._num_gradient_accumulation_steps == steps_to_accumulate\n\n        metrics = trainer.train()\n\n        num_batches_trained_per_epoch = trainer._batch_num_total // (metrics[""training_epochs""] + 1)\n        num_batches_expected = math.ceil(\n            math.ceil(len(instances) / self.data_loader.batch_size) / steps_to_accumulate\n        )\n\n        assert num_batches_trained_per_epoch == num_batches_expected\n\n    def test_batch_callback_is_called_at_every_batch(self):\n        class FakeBatchCallback(BatchCallback):\n            def __call__(\n                self,\n                trainer: ""GradientDescentTrainer"",\n                batch_inputs: List[List[TensorDict]],\n                batch_outputs: List[Dict[str, Any]],\n                epoch: int,\n                batch_number: int,\n                is_training: bool,\n                is_master: bool,\n            ) -> None:\n                if not hasattr(trainer, ""batch_callback_calls""):\n                    trainer.batch_callback_calls = []  # type: ignore\n                trainer.batch_callback_calls.append((epoch, batch_number, is_training))  # type: ignore\n\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            num_epochs=2,\n            validation_data_loader=self.validation_data_loader,\n            batch_callbacks=[FakeBatchCallback()],\n        )\n        trainer.train()\n        expected_calls = [\n            (epoch, batch_number + 1, is_train)\n            for epoch in range(2)\n            for is_train in (True, False)\n            for batch_number in range(len(self.instances) // 2)\n        ]\n        assert trainer.batch_callback_calls == expected_calls\n\n    def test_epoch_callback_is_called_at_every_epoch(self):\n        class FakeEpochCallback(EpochCallback):\n            def __call__(\n                self,\n                trainer: ""GradientDescentTrainer"",\n                metrics: Dict[str, Any],\n                epoch: int,\n                is_master: bool,\n            ) -> None:\n                if not hasattr(trainer, ""epoch_callback_calls""):\n                    trainer.epoch_callback_calls = []  # type: ignore\n                trainer.epoch_callback_calls.append(epoch)  # type: ignore\n\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            num_epochs=4,\n            validation_data_loader=self.validation_data_loader,\n            epoch_callbacks=[FakeEpochCallback()],\n        )\n        trainer.train()\n        expected_calls = [epoch for epoch in range(-1, 4)]\n        assert trainer.epoch_callback_calls == expected_calls\n\n\nclass TestApexTrainer(TrainerTestBase):\n    @requires_gpu\n    @pytest.mark.skipif(amp is None, reason=""Apex is not installed."")\n    def test_trainer_can_run_amp(self):\n        self.model.cuda()\n        trainer = GradientDescentTrainer(\n            self.model,\n            self.optimizer,\n            self.data_loader,\n            num_epochs=2,\n            cuda_device=0,\n            opt_level=""O1"",\n        )\n        _ = trainer.train()\n\n\nclass TestSparseClipGrad(AllenNlpTestCase):\n    def test_sparse_clip_grad(self):\n        # create a sparse embedding layer, then take gradient\n        embedding = torch.nn.Embedding(100, 16, sparse=True)\n        embedding.zero_grad()\n        ids = (torch.rand(17) * 100).long()\n        # Set some of the ids to the same value so that the sparse gradient\n        # has repeated indices.  This tests some additional logic.\n        ids[:5] = 5\n        loss = embedding(ids).sum()\n        loss.backward()\n        assert embedding.weight.grad.is_sparse\n\n        # Now try to clip the gradients.\n        _ = clip_grad_norm_([embedding.weight], 1.5)\n        # Final norm should be 1.5\n        grad = embedding.weight.grad.coalesce()\n        assert grad._values().norm(2.0).item() == pytest.approx(1.5, rel=1e-4)\n'"
tests/tutorials/__init__.py,0,b''
allennlp/common/testing/__init__.py,3,"b'""""""\nUtilities and helpers for writing tests.\n""""""\nimport torch\nimport pytest\n\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.common.testing.model_test_case import ModelTestCase\n\n\n_available_devices = [""cpu""] + ([""cuda""] if torch.cuda.is_available() else [])\n\n\ndef multi_device(test_method):\n    """"""\n    Decorator that provides an argument `device` of type `str` for each available PyTorch device.\n    """"""\n    return pytest.mark.parametrize(""device"", _available_devices)(pytest.mark.gpu(test_method))\n\n\ndef requires_gpu(test_method):\n    """"""\n    Decorator to indicate that a test requires a GPU device.\n    """"""\n    return pytest.mark.gpu(\n        pytest.mark.skipif(not torch.cuda.is_available(), reason=""No CUDA device registered."")(\n            test_method\n        )\n    )\n\n\ndef requires_multi_gpu(test_method):\n    """"""\n    Decorator to indicate that a test requires multiple GPU devices.\n    """"""\n    return pytest.mark.gpu(\n        pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""2 or more GPUs required."")(\n            test_method\n        )\n    )\n\n\ndef cpu_or_gpu(test_method):\n    """"""\n    Decorator to indicate that a test should run on both CPU and GPU\n    """"""\n    return pytest.mark.gpu(test_method)\n'"
allennlp/common/testing/model_test_case.py,4,"b'import copy\nimport json\nfrom typing import Any, Dict, Iterable, Set, Union\n\nimport torch\nfrom numpy.testing import assert_allclose\n\nfrom allennlp.commands.train import train_model_from_file\nfrom allennlp.common import Params\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.data import DatasetReader, Vocabulary\nfrom allennlp.data import DataLoader\nfrom allennlp.data.batch import Batch\nfrom allennlp.models import load_archive, Model\n\n\nclass ModelTestCase(AllenNlpTestCase):\n    """"""\n    A subclass of [`AllenNlpTestCase`](./allennlp_test_case.md)\n    with added methods for testing [`Model`](../../models/model.md) subclasses.\n    """"""\n\n    def set_up_model(self, param_file, dataset_file):\n\n        self.param_file = param_file\n        params = Params.from_file(self.param_file)\n\n        reader = DatasetReader.from_params(params[""dataset_reader""])\n        # The dataset reader might be lazy, but a lazy list here breaks some of our tests.\n        instances = reader.read(str(dataset_file))\n        # Use parameters for vocabulary if they are present in the config file, so that choices like\n        # ""non_padded_namespaces"", ""min_count"" etc. can be set if needed.\n        if ""vocabulary"" in params:\n            vocab_params = params[""vocabulary""]\n            vocab = Vocabulary.from_params(params=vocab_params, instances=instances)\n        else:\n            vocab = Vocabulary.from_instances(instances)\n        self.vocab = vocab\n        self.instances = instances\n        self.instances.index_with(vocab)\n        self.model = Model.from_params(vocab=self.vocab, params=params[""model""])\n\n        # TODO(joelgrus) get rid of these\n        # (a lot of the model tests use them, so they\'ll have to be changed)\n        self.dataset = Batch(list(self.instances))\n        self.dataset.index_instances(self.vocab)\n\n    def ensure_model_can_train_save_and_load(\n        self,\n        param_file: str,\n        tolerance: float = 1e-4,\n        cuda_device: int = -1,\n        gradients_to_ignore: Set[str] = None,\n        overrides: str = """",\n        metric_to_check: str = None,\n        metric_terminal_value: float = None,\n        metric_tolerance: float = 1e-4,\n        disable_dropout: bool = True,\n    ):\n        """"""\n        # Parameters\n\n        param_file : `str`\n            Path to a training configuration file that we will use to train the model for this\n            test.\n        tolerance : `float`, optional (default=`1e-4`)\n            When comparing model predictions between the originally-trained model and the model\n            after saving and loading, we will use this tolerance value (passed as `rtol` to\n            `numpy.testing.assert_allclose`).\n        cuda_device : `int`, optional (default=`-1`)\n            The device to run the test on.\n        gradients_to_ignore : `Set[str]`, optional (default=`None`)\n            This test runs a gradient check to make sure that we\'re actually computing gradients\n            for all of the parameters in the model.  If you really want to ignore certain\n            parameters when doing that check, you can pass their names here.  This is not\n            recommended unless you\'re `really` sure you don\'t need to have non-zero gradients for\n            those parameters (e.g., some of the beam search / state machine models have\n            infrequently-used parameters that are hard to force the model to use in a small test).\n        overrides : `str`, optional (default = `""""`)\n            A JSON string that we will use to override values in the input parameter file.\n        metric_to_check: `str`, optional (default = `None`)\n            We may want to automatically perform a check that model reaches given metric when\n            training (on validation set, if it is specified). It may be useful in CI, for example.\n            You can pass any metric that is in your model returned metrics.\n        metric_terminal_value: `str`, optional (default = `None`)\n            When you set `metric_to_check`, you need to set the value this metric must converge to\n        metric_tolerance: `float`, optional (default=`1e-4`)\n            Tolerance to check you model metric against metric terminal value. One can expect some\n            variance in model metrics when the training process is highly stochastic.\n        disable_dropout : `bool`, optional (default = `True`)\n            If True we will set all dropout to 0 before checking gradients. (Otherwise, with small\n            datasets, you may get zero gradients because of unlucky dropout.)\n        """"""\n        save_dir = self.TEST_DIR / ""save_and_load_test""\n        archive_file = save_dir / ""model.tar.gz""\n        model = train_model_from_file(param_file, save_dir, overrides=overrides)\n        metrics_file = save_dir / ""metrics.json""\n        if metric_to_check is not None:\n            metrics = json.loads(metrics_file.read_text())\n            metric_value = metrics.get(f""best_validation_{metric_to_check}"") or metrics.get(\n                f""training_{metric_to_check}""\n            )\n            assert metric_value is not None, f""Cannot find {metric_to_check} in metrics.json file""\n            assert metric_terminal_value is not None, ""Please specify metric terminal value""\n            assert abs(metric_value - metric_terminal_value) < metric_tolerance\n        loaded_model = load_archive(archive_file, cuda_device=cuda_device).model\n        state_keys = model.state_dict().keys()\n        loaded_state_keys = loaded_model.state_dict().keys()\n        assert state_keys == loaded_state_keys\n        # First we make sure that the state dict (the parameters) are the same for both models.\n        for key in state_keys:\n            assert_allclose(\n                model.state_dict()[key].cpu().numpy(),\n                loaded_model.state_dict()[key].cpu().numpy(),\n                err_msg=key,\n            )\n        params = Params.from_file(param_file, params_overrides=overrides)\n        reader = DatasetReader.from_params(params[""dataset_reader""])\n\n        print(""Reading with original model"")\n        model_dataset = reader.read(params[""validation_data_path""])\n        model_dataset.index_with(model.vocab)\n\n        print(""Reading with loaded model"")\n        loaded_dataset = reader.read(params[""validation_data_path""])\n        loaded_dataset.index_with(loaded_model.vocab)\n\n        # Need to duplicate params because DataLoader.from_params will consume.\n        data_loader_params = params[""data_loader""]\n        data_loader_params[""shuffle""] = False\n        data_loader_params2 = Params(copy.deepcopy(data_loader_params.as_dict()))\n\n        data_loader = DataLoader.from_params(dataset=model_dataset, params=data_loader_params)\n        data_loader2 = DataLoader.from_params(dataset=loaded_dataset, params=data_loader_params2)\n\n        # We\'ll check that even if we index the dataset with each model separately, we still get\n        # the same result out.\n        model_batch = next(iter(data_loader))\n\n        loaded_batch = next(iter(data_loader2))\n\n        # Check gradients are None for non-trainable parameters and check that\n        # trainable parameters receive some gradient if they are trainable.\n        self.check_model_computes_gradients_correctly(\n            model, model_batch, gradients_to_ignore, disable_dropout\n        )\n\n        # The datasets themselves should be identical.\n        assert model_batch.keys() == loaded_batch.keys()\n        for key in model_batch.keys():\n            self.assert_fields_equal(model_batch[key], loaded_batch[key], key, 1e-6)\n\n        # Set eval mode, to turn off things like dropout, then get predictions.\n        model.eval()\n        loaded_model.eval()\n        # Models with stateful RNNs need their states reset to have consistent\n        # behavior after loading.\n        for model_ in [model, loaded_model]:\n            for module in model_.modules():\n                if hasattr(module, ""stateful"") and module.stateful:\n                    module.reset_states()\n        print(""Predicting with original model"")\n        model_predictions = model(**model_batch)\n        print(""Predicting with loaded model"")\n        loaded_model_predictions = loaded_model(**loaded_batch)\n\n        # Check loaded model\'s loss exists and we can compute gradients, for continuing training.\n        loaded_model_loss = loaded_model_predictions[""loss""]\n        assert loaded_model_loss is not None\n        loaded_model_loss.backward()\n\n        # Both outputs should have the same keys and the values for these keys should be close.\n        for key in model_predictions.keys():\n            self.assert_fields_equal(\n                model_predictions[key], loaded_model_predictions[key], name=key, tolerance=tolerance\n            )\n\n        return model, loaded_model\n\n    def assert_fields_equal(self, field1, field2, name: str, tolerance: float = 1e-6) -> None:\n        if isinstance(field1, torch.Tensor):\n            assert_allclose(\n                field1.detach().cpu().numpy(),\n                field2.detach().cpu().numpy(),\n                rtol=tolerance,\n                err_msg=name,\n            )\n        elif isinstance(field1, dict):\n            assert field1.keys() == field2.keys()\n            for key in field1:\n                self.assert_fields_equal(\n                    field1[key], field2[key], tolerance=tolerance, name=name + ""."" + str(key)\n                )\n        elif isinstance(field1, (list, tuple)):\n            assert len(field1) == len(field2)\n            for i, (subfield1, subfield2) in enumerate(zip(field1, field2)):\n                self.assert_fields_equal(\n                    subfield1, subfield2, tolerance=tolerance, name=name + f""[{i}]""\n                )\n        elif isinstance(field1, (float, int)):\n            assert_allclose([field1], [field2], rtol=tolerance, err_msg=name)\n        else:\n            if field1 != field2:\n                for key in field1.__dict__:\n                    print(key, getattr(field1, key) == getattr(field2, key))\n            assert field1 == field2, f""{name}, {type(field1)}, {type(field2)}""\n\n    @staticmethod\n    def check_model_computes_gradients_correctly(\n        model: Model,\n        model_batch: Dict[str, Union[Any, Dict[str, Any]]],\n        params_to_ignore: Set[str] = None,\n        disable_dropout: bool = True,\n    ):\n        print(""Checking gradients"")\n        model.zero_grad()\n\n        original_dropouts: Dict[str, float] = {}\n\n        if disable_dropout:\n            # Remember original dropouts so we can restore them.\n            for name, module in model.named_modules():\n                if isinstance(module, torch.nn.Dropout):\n                    original_dropouts[name] = getattr(module, ""p"")\n                    setattr(module, ""p"", 0)\n\n        result = model(**model_batch)\n        result[""loss""].backward()\n        has_zero_or_none_grads = {}\n        for name, parameter in model.named_parameters():\n            zeros = torch.zeros(parameter.size())\n            if params_to_ignore and name in params_to_ignore:\n                continue\n            if parameter.requires_grad:\n\n                if parameter.grad is None:\n                    has_zero_or_none_grads[\n                        name\n                    ] = ""No gradient computed (i.e parameter.grad is None)""\n\n                elif parameter.grad.is_sparse or parameter.grad.data.is_sparse:\n                    pass\n\n                # Some parameters will only be partially updated,\n                # like embeddings, so we just check that any gradient is non-zero.\n                elif (parameter.grad.cpu() == zeros).all():\n                    has_zero_or_none_grads[\n                        name\n                    ] = f""zeros with shape ({tuple(parameter.grad.size())})""\n            else:\n                assert parameter.grad is None\n\n        if has_zero_or_none_grads:\n            for name, grad in has_zero_or_none_grads.items():\n                print(f""Parameter: {name} had incorrect gradient: {grad}"")\n            raise Exception(""Incorrect gradients found. See stdout for more info."")\n\n        # Now restore dropouts if we disabled them.\n        if disable_dropout:\n            for name, module in model.named_modules():\n                if name in original_dropouts:\n                    setattr(module, ""p"", original_dropouts[name])\n\n    def ensure_batch_predictions_are_consistent(self, keys_to_ignore: Iterable[str] = ()):\n        """"""\n        Ensures that the model performs the same on a batch of instances as on individual instances.\n        Ignores metrics matching the regexp .*loss.* and those specified explicitly.\n\n        # Parameters\n\n        keys_to_ignore : `Iterable[str]`, optional (default=`()`)\n            Names of metrics that should not be taken into account, e.g. ""batch_weight"".\n        """"""\n        self.model.eval()\n        single_predictions = []\n        for i, instance in enumerate(self.instances):\n            dataset = Batch([instance])\n            tensors = dataset.as_tensor_dict(dataset.get_padding_lengths())\n            result = self.model(**tensors)\n            single_predictions.append(result)\n        full_dataset = Batch(self.instances)\n        batch_tensors = full_dataset.as_tensor_dict(full_dataset.get_padding_lengths())\n        batch_predictions = self.model(**batch_tensors)\n        for i, instance_predictions in enumerate(single_predictions):\n            for key, single_predicted in instance_predictions.items():\n                tolerance = 1e-6\n                if ""loss"" in key:\n                    # Loss is particularly unstable; we\'ll just be satisfied if everything else is\n                    # close.\n                    continue\n                if key in keys_to_ignore:\n                    continue\n                single_predicted = single_predicted[0]\n                batch_predicted = batch_predictions[key][i]\n                if isinstance(single_predicted, torch.Tensor):\n                    if single_predicted.size() != batch_predicted.size():\n                        slices = tuple(slice(0, size) for size in single_predicted.size())\n                        batch_predicted = batch_predicted[slices]\n                    assert_allclose(\n                        single_predicted.data.numpy(),\n                        batch_predicted.data.numpy(),\n                        atol=tolerance,\n                        err_msg=key,\n                    )\n                else:\n                    assert single_predicted == batch_predicted, key\n'"
allennlp/common/testing/test_case.py,0,"b'import logging\nimport os\nimport pathlib\nimport shutil\nimport tempfile\nfrom unittest import mock\n\nfrom allennlp.common.checks import log_pytorch_version_info\n\nTEST_DIR = tempfile.mkdtemp(prefix=""allennlp_tests"")\n\n\nclass AllenNlpTestCase:\n    """"""\n    A custom subclass of `unittest.TestCase` that disables some of the more verbose AllenNLP\n    logging and that creates and destroys a temp directory as a test fixture.\n    """"""\n\n    PROJECT_ROOT = (pathlib.Path(__file__).parent / "".."" / "".."" / "".."").resolve()\n    MODULE_ROOT = PROJECT_ROOT / ""allennlp""\n    TOOLS_ROOT = MODULE_ROOT / ""tools""\n    TESTS_ROOT = PROJECT_ROOT / ""tests""\n    FIXTURES_ROOT = PROJECT_ROOT / ""test_fixtures""\n\n    def setup_method(self):\n        logging.basicConfig(\n            format=""%(asctime)s - %(levelname)s - %(name)s - %(message)s"", level=logging.DEBUG\n        )\n        # Disabling some of the more verbose logging statements that typically aren\'t very helpful\n        # in tests.\n        logging.getLogger(""allennlp.common.params"").disabled = True\n        logging.getLogger(""allennlp.nn.initializers"").disabled = True\n        logging.getLogger(""allennlp.modules.token_embedders.embedding"").setLevel(logging.INFO)\n        logging.getLogger(""urllib3.connectionpool"").disabled = True\n        log_pytorch_version_info()\n\n        self.TEST_DIR = pathlib.Path(TEST_DIR)\n\n        os.makedirs(self.TEST_DIR, exist_ok=True)\n\n        # Due to a bug in pytest we\'ll end up with a bunch of logging errors if we try to\n        # log anything within an \'atexit\' hook.\n        # When https://github.com/pytest-dev/pytest/issues/5502 is fixed we should\n        # be able to remove this work-around.\n        def _cleanup_archive_dir_without_logging(path: str):\n            if os.path.exists(path):\n                shutil.rmtree(path)\n\n        self.patcher = mock.patch(\n            ""allennlp.models.archival._cleanup_archive_dir"", _cleanup_archive_dir_without_logging\n        )\n        self.mock_cleanup_archive_dir = self.patcher.start()\n\n    def teardown_method(self):\n        shutil.rmtree(self.TEST_DIR)\n        self.patcher.stop()\n'"
allennlp/data/dataset_readers/__init__.py,0,"b'""""""\nA :class:`~allennlp.data.dataset_readers.dataset_reader.DatasetReader`\nreads a file and converts it to a collection of\n:class:`~allennlp.data.instance.Instance` s.\nThe various subclasses know how to read specific filetypes\nand produce datasets in the formats required by specific models.\n""""""\n\n\nfrom allennlp.data.dataset_readers.conll2003 import Conll2003DatasetReader\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.dataset_readers.interleaving_dataset_reader import InterleavingDatasetReader\nfrom allennlp.data.dataset_readers.sequence_tagging import SequenceTaggingDatasetReader\nfrom allennlp.data.dataset_readers.sharded_dataset_reader import ShardedDatasetReader\nfrom allennlp.data.dataset_readers.babi import BabiReader\nfrom allennlp.data.dataset_readers.text_classification_json import TextClassificationJsonReader\n'"
allennlp/data/dataset_readers/babi.py,0,"b'import logging\n\nfrom typing import Dict, List\nfrom overrides import overrides\n\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.fields import Field, TextField, ListField, IndexField\nfrom allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\nfrom allennlp.data.tokenizers import Token\n\nlogger = logging.getLogger(__name__)\n\n\n@DatasetReader.register(""babi"")\nclass BabiReader(DatasetReader):\n    """"""\n    Reads one single task in the bAbI tasks format as formulated in\n    Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\n    (https://arxiv.org/abs/1502.05698). Since this class handle a single file,\n    if one wants to load multiple tasks together it has to merge them into a\n    single file and use this reader.\n\n    Registered as a `DatasetReader` with name ""babi"".\n\n    # Parameters\n\n    keep_sentences : `bool`, optional, (default = `False`)\n        Whether to keep each sentence in the context or to concatenate them.\n        Default is `False` that corresponds to concatenation.\n    token_indexers : `Dict[str, TokenIndexer]`, optional (default=`{""tokens"": SingleIdTokenIndexer()}`)\n        We use this to define the input representation for the text.  See :class:`TokenIndexer`.\n    """"""\n\n    def __init__(\n        self,\n        keep_sentences: bool = False,\n        token_indexers: Dict[str, TokenIndexer] = None,\n        **kwargs,\n    ) -> None:\n\n        super().__init__(**kwargs)\n        self._keep_sentences = keep_sentences\n        self._token_indexers = token_indexers or {""tokens"": SingleIdTokenIndexer()}\n\n    @overrides\n    def _read(self, file_path: str):\n        # if `file_path` is a URL, redirect to the cache\n        file_path = cached_path(file_path)\n\n        logger.info(""Reading file at %s"", file_path)\n\n        with open(file_path) as dataset_file:\n            dataset = dataset_file.readlines()\n\n        logger.info(""Reading the dataset"")\n\n        context: List[List[str]] = [[]]\n        for line in dataset:\n            if ""?"" in line:\n                question_str, answer, supports_str = line.replace(""?"", "" ?"").split(""\\t"")\n                question = question_str.split()[1:]\n                supports = [int(support) - 1 for support in supports_str.split()]\n\n                yield self.text_to_instance(context, question, answer, supports)\n            else:\n                new_entry = line.replace(""."", "" ."").split()[1:]\n\n                if line[0] == ""1"":\n                    context = [new_entry]\n                else:\n                    context.append(new_entry)\n\n    @overrides\n    def text_to_instance(\n        self,  # type: ignore\n        context: List[List[str]],\n        question: List[str],\n        answer: str,\n        supports: List[int],\n    ) -> Instance:\n\n        fields: Dict[str, Field] = {}\n\n        if self._keep_sentences:\n            context_field_ks = ListField(\n                [\n                    TextField([Token(word) for word in line], self._token_indexers)\n                    for line in context\n                ]\n            )\n\n            fields[""supports""] = ListField(\n                [IndexField(support, context_field_ks) for support in supports]\n            )\n        else:\n            context_field = TextField(\n                [Token(word) for line in context for word in line], self._token_indexers\n            )\n\n        fields[""context""] = context_field_ks if self._keep_sentences else context_field\n        fields[""question""] = TextField([Token(word) for word in question], self._token_indexers)\n        fields[""answer""] = TextField([Token(answer)], self._token_indexers)\n\n        return Instance(fields)\n'"
allennlp/data/dataset_readers/conll2003.py,0,"b'from typing import Dict, List, Sequence, Iterable\nimport itertools\nimport logging\n\nfrom overrides import overrides\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.dataset_readers.dataset_utils import to_bioul\nfrom allennlp.data.fields import TextField, SequenceLabelField, Field, MetadataField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\nfrom allennlp.data.tokenizers import Token\n\nlogger = logging.getLogger(__name__)\n\n\ndef _is_divider(line: str) -> bool:\n    empty_line = line.strip() == """"\n    if empty_line:\n        return True\n    else:\n        first_token = line.split()[0]\n        if first_token == ""-DOCSTART-"":\n            return True\n        else:\n            return False\n\n\n@DatasetReader.register(""conll2003"")\nclass Conll2003DatasetReader(DatasetReader):\n    """"""\n    Reads instances from a pretokenised file where each line is in the following format:\n\n    ```\n    WORD POS-TAG CHUNK-TAG NER-TAG\n    ```\n\n    with a blank line indicating the end of each sentence\n    and `-DOCSTART- -X- -X- O` indicating the end of each article,\n    and converts it into a `Dataset` suitable for sequence tagging.\n\n    Each `Instance` contains the words in the `""tokens""` `TextField`.\n    The values corresponding to the `tag_label`\n    values will get loaded into the `""tags""` `SequenceLabelField`.\n    And if you specify any `feature_labels` (you probably shouldn\'t),\n    the corresponding values will get loaded into their own `SequenceLabelField` s.\n\n    This dataset reader ignores the ""article"" divisions and simply treats\n    each sentence as an independent `Instance`. (Technically the reader splits sentences\n    on any combination of blank lines and ""DOCSTART"" tags; in particular, it does the right\n    thing on well formed inputs.)\n\n    Registered as a `DatasetReader` with name ""conll2003"".\n\n    # Parameters\n\n    token_indexers : `Dict[str, TokenIndexer]`, optional (default=`{""tokens"": SingleIdTokenIndexer()}`)\n        We use this to define the input representation for the text.  See :class:`TokenIndexer`.\n    tag_label : `str`, optional (default=`ner`)\n        Specify `ner`, `pos`, or `chunk` to have that tag loaded into the instance field `tag`.\n    feature_labels : `Sequence[str]`, optional (default=`()`)\n        These labels will be loaded as features into the corresponding instance fields:\n        `pos` -> `pos_tags`, `chunk` -> `chunk_tags`, `ner` -> `ner_tags`\n        Each will have its own namespace : `pos_tags`, `chunk_tags`, `ner_tags`.\n        If you want to use one of the tags as a `feature` in your model, it should be\n        specified here.\n    coding_scheme : `str`, optional (default=`IOB1`)\n        Specifies the coding scheme for `ner_labels` and `chunk_labels`.\n        Valid options are `IOB1` and `BIOUL`.  The `IOB1` default maintains\n        the original IOB1 scheme in the CoNLL 2003 NER data.\n        In the IOB1 scheme, I is a token inside a span, O is a token outside\n        a span and B is the beginning of span immediately following another\n        span of the same type.\n    label_namespace : `str`, optional (default=`labels`)\n        Specifies the namespace for the chosen `tag_label`.\n    """"""\n\n    _VALID_LABELS = {""ner"", ""pos"", ""chunk""}\n\n    def __init__(\n        self,\n        token_indexers: Dict[str, TokenIndexer] = None,\n        tag_label: str = ""ner"",\n        feature_labels: Sequence[str] = (),\n        coding_scheme: str = ""IOB1"",\n        label_namespace: str = ""labels"",\n        **kwargs,\n    ) -> None:\n        super().__init__(**kwargs)\n        self._token_indexers = token_indexers or {""tokens"": SingleIdTokenIndexer()}\n        if tag_label is not None and tag_label not in self._VALID_LABELS:\n            raise ConfigurationError(""unknown tag label type: {}"".format(tag_label))\n        for label in feature_labels:\n            if label not in self._VALID_LABELS:\n                raise ConfigurationError(""unknown feature label type: {}"".format(label))\n        if coding_scheme not in (""IOB1"", ""BIOUL""):\n            raise ConfigurationError(""unknown coding_scheme: {}"".format(coding_scheme))\n\n        self.tag_label = tag_label\n        self.feature_labels = set(feature_labels)\n        self.coding_scheme = coding_scheme\n        self.label_namespace = label_namespace\n        self._original_coding_scheme = ""IOB1""\n\n    @overrides\n    def _read(self, file_path: str) -> Iterable[Instance]:\n        # if `file_path` is a URL, redirect to the cache\n        file_path = cached_path(file_path)\n\n        with open(file_path, ""r"") as data_file:\n            logger.info(""Reading instances from lines in file at: %s"", file_path)\n\n            # Group into alternative divider / sentence chunks.\n            for is_divider, lines in itertools.groupby(data_file, _is_divider):\n                # Ignore the divider chunks, so that `lines` corresponds to the words\n                # of a single sentence.\n                if not is_divider:\n                    fields = [line.strip().split() for line in lines]\n                    # unzipping trick returns tuples, but our Fields need lists\n                    fields = [list(field) for field in zip(*fields)]\n                    tokens_, pos_tags, chunk_tags, ner_tags = fields\n                    # TextField requires `Token` objects\n                    tokens = [Token(token) for token in tokens_]\n\n                    yield self.text_to_instance(tokens, pos_tags, chunk_tags, ner_tags)\n\n    def text_to_instance(  # type: ignore\n        self,\n        tokens: List[Token],\n        pos_tags: List[str] = None,\n        chunk_tags: List[str] = None,\n        ner_tags: List[str] = None,\n    ) -> Instance:\n        """"""\n        We take `pre-tokenized` input here, because we don\'t have a tokenizer in this class.\n        """"""\n\n        sequence = TextField(tokens, self._token_indexers)\n        instance_fields: Dict[str, Field] = {""tokens"": sequence}\n        instance_fields[""metadata""] = MetadataField({""words"": [x.text for x in tokens]})\n\n        # Recode the labels if necessary.\n        if self.coding_scheme == ""BIOUL"":\n            coded_chunks = (\n                to_bioul(chunk_tags, encoding=self._original_coding_scheme)\n                if chunk_tags is not None\n                else None\n            )\n            coded_ner = (\n                to_bioul(ner_tags, encoding=self._original_coding_scheme)\n                if ner_tags is not None\n                else None\n            )\n        else:\n            # the default IOB1\n            coded_chunks = chunk_tags\n            coded_ner = ner_tags\n\n        # Add ""feature labels"" to instance\n        if ""pos"" in self.feature_labels:\n            if pos_tags is None:\n                raise ConfigurationError(\n                    ""Dataset reader was specified to use pos_tags as ""\n                    ""features. Pass them to text_to_instance.""\n                )\n            instance_fields[""pos_tags""] = SequenceLabelField(pos_tags, sequence, ""pos_tags"")\n        if ""chunk"" in self.feature_labels:\n            if coded_chunks is None:\n                raise ConfigurationError(\n                    ""Dataset reader was specified to use chunk tags as ""\n                    ""features. Pass them to text_to_instance.""\n                )\n            instance_fields[""chunk_tags""] = SequenceLabelField(coded_chunks, sequence, ""chunk_tags"")\n        if ""ner"" in self.feature_labels:\n            if coded_ner is None:\n                raise ConfigurationError(\n                    ""Dataset reader was specified to use NER tags as ""\n                    "" features. Pass them to text_to_instance.""\n                )\n            instance_fields[""ner_tags""] = SequenceLabelField(coded_ner, sequence, ""ner_tags"")\n\n        # Add ""tag label"" to instance\n        if self.tag_label == ""ner"" and coded_ner is not None:\n            instance_fields[""tags""] = SequenceLabelField(coded_ner, sequence, self.label_namespace)\n        elif self.tag_label == ""pos"" and pos_tags is not None:\n            instance_fields[""tags""] = SequenceLabelField(pos_tags, sequence, self.label_namespace)\n        elif self.tag_label == ""chunk"" and coded_chunks is not None:\n            instance_fields[""tags""] = SequenceLabelField(\n                coded_chunks, sequence, self.label_namespace\n            )\n\n        return Instance(instance_fields)\n'"
allennlp/data/dataset_readers/dataset_reader.py,1,"b'import itertools\nfrom typing import Iterable, Iterator, Callable, Optional, List\nimport logging\nimport os\nimport pathlib\n\nimport jsonpickle\nfrom torch.utils.data import Dataset, IterableDataset\n\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.common import Tqdm, util\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.registrable import Registrable\n\nlogger = logging.getLogger(__name__)\n\n\nclass AllennlpDataset(Dataset):\n    def __init__(self, instances: List[Instance], vocab: Vocabulary = None):\n        self.instances = instances\n        self.vocab = vocab\n\n    def __getitem__(self, idx):\n        if self.vocab is not None:\n            self.instances[idx].index_fields(self.vocab)\n        return self.instances[idx]\n\n    def __len__(self):\n        return len(self.instances)\n\n    def index_with(self, vocab: Vocabulary):\n        self.vocab = vocab\n\n\nclass AllennlpLazyDataset(IterableDataset):\n    def __init__(\n        self,\n        deserialize: Callable[[str], Instance] = None,\n        serialize: Callable[[Instance], str] = None,\n        vocab: Vocabulary = None,\n    ) -> None:\n        super().__init__()\n        self.deserialize = deserialize\n        self.serialize = serialize\n        self.vocab = vocab\n\n    def index_with(self, vocab: Vocabulary):\n        self.vocab = vocab\n\n\nclass _LazyInstances(AllennlpLazyDataset):\n    """"""\n    An `Iterable` that just wraps a thunk for generating instances and calls it for\n    each call to `__iter__`.\n    """"""\n\n    def __init__(\n        self,\n        instance_generator: Callable[[str], Iterable[Instance]],\n        file_path: str,\n        cache_file: str = None,\n        deserialize: Callable[[str], Instance] = None,\n        serialize: Callable[[Instance], str] = None,\n        vocab: Vocabulary = None,\n    ) -> None:\n        super().__init__(deserialize, serialize, vocab)\n        self.instance_generator = instance_generator\n        self.file_path = file_path\n        self.cache_file = cache_file\n\n    def __iter__(self) -> Iterator[Instance]:\n        # Case 1: Use cached instances\n        if self.cache_file is not None and os.path.exists(self.cache_file):\n            with open(self.cache_file) as data_file:\n                for line in data_file:\n                    instance = self.deserialize(line)\n                    if self.vocab is not None:\n                        instance.index_fields(self.vocab)\n                    yield instance\n\n        # Case 2: Need to cache instances\n        elif self.cache_file is not None:\n            with open(self.cache_file, ""w"") as data_file:\n                for instance in self.instance_generator(self.file_path):\n                    data_file.write(self.serialize(instance))\n                    data_file.write(""\\n"")\n                    if self.vocab is not None:\n                        instance.index_fields(self.vocab)\n                    yield instance\n        # Case 3: No cache\n        else:\n            instances = self.instance_generator(self.file_path)\n            if isinstance(instances, list):\n                raise ConfigurationError(\n                    ""For a lazy dataset reader, _read() must return a generator""\n                )\n            for instance in instances:\n                if self.vocab is not None:\n                    instance.index_fields(self.vocab)\n                yield instance\n\n\nclass _MaxLazyInstances(AllennlpLazyDataset):\n    def __init__(self, inner: AllennlpLazyDataset, max_instances: int) -> None:\n        super().__init__()\n        self.inner = inner\n        self.max_instances = max_instances\n\n    def __iter__(self) -> Iterator[Instance]:\n        return itertools.islice(iter(self.inner), self.max_instances)\n\n    def index_with(self, vocab: Vocabulary):\n        self.inner.index_with(vocab)\n\n\nclass _DistributedLazyInstances(AllennlpLazyDataset):\n    def __init__(self, inner: AllennlpLazyDataset) -> None:\n        super().__init__()\n        self.inner = inner\n\n    def __iter__(self) -> Iterator[Instance]:\n        from torch import distributed\n\n        logger.info(\n            ""Returning instances i%%%d==%d"", distributed.get_world_size(), distributed.get_rank()\n        )\n        return itertools.islice(\n            iter(self.inner), distributed.get_rank(), None, distributed.get_world_size()\n        )\n\n    def index_with(self, vocab: Vocabulary):\n        self.inner.index_with(vocab)\n\n\nclass DatasetReader(Registrable):\n    """"""\n    A `DatasetReader` knows how to turn a file containing a dataset into a collection\n    of `Instances`.  To implement your own, just override the `_read(file_path)` method\n    to return an `Iterable` of the instances. This could be a list containing the instances\n    or a lazy generator that returns them one at a time.\n\n    All parameters necessary to _read the data apart from the filepath should be passed\n    to the constructor of the `DatasetReader`.\n\n    # Parameters\n\n    lazy : `bool`, optional (default=`False`)\n        If this is true, `instances()` will return an object whose `__iter__` method\n        reloads the dataset each time it\'s called. Otherwise, `instances()` returns a list.\n    cache_directory : `str`, optional (default=`None`)\n        If given, we will use this directory to store a cache of already-processed `Instances` in\n        every file passed to :func:`read`, serialized (by default, though you can override this) as\n        one string-formatted `Instance` per line.  If the cache file for a given `file_path` exists,\n        we read the `Instances` from the cache instead of re-processing the data (using\n        :func:`_instances_from_cache_file`).  If the cache file does _not_ exist, we will _create_\n        it on our first pass through the data (using :func:`_instances_to_cache_file`).\n\n        IMPORTANT CAVEAT: It is the _caller\'s_ responsibility to make sure that this directory is\n        unique for any combination of code and parameters that you use.  That is, if you pass a\n        directory here, we will use any existing cache files in that directory _regardless of the\n        parameters you set for this DatasetReader!_\n    max_instances : `int`, optional (default=`None`)\n        If given, will stop reading after this many instances. This is a useful setting for debugging.\n        Setting this disables caching.\n    manual_distributed_sharding: `bool`, optional (default=`False`)\n        By default, when used in a distributed setting, `DatasetReader` makes sure that each\n        worker process only receives a subset of the data. It does this by reading the whole\n        dataset in each worker, but filtering out the instances that are not needed. If you\n        can implement a faster mechanism that only reads part of the data, set this to True,\n        and do the sharding yourself.\n    """"""\n\n    def __init__(\n        self,\n        lazy: bool = False,\n        cache_directory: Optional[str] = None,\n        max_instances: Optional[int] = None,\n        manual_distributed_sharding: bool = False,\n    ) -> None:\n        self.lazy = lazy\n        self.max_instances = max_instances\n        if cache_directory:\n            self._cache_directory = pathlib.Path(cache_directory)\n            os.makedirs(self._cache_directory, exist_ok=True)\n        else:\n            self._cache_directory = None\n        self.manual_distributed_sharding = manual_distributed_sharding\n\n    def read(self, file_path: str) -> Dataset:\n        """"""\n        Returns an `Iterable` containing all the instances\n        in the specified dataset.\n\n        If `self.lazy` is False, this calls `self._read()`,\n        ensures that the result is a list, then returns the resulting list.\n\n        If `self.lazy` is True, this returns an object whose\n        `__iter__` method calls `self._read()` each iteration.\n        In this case your implementation of `_read()` must also be lazy\n        (that is, not load all instances into memory at once), otherwise\n        you will get a `ConfigurationError`.\n\n        In either case, the returned `Iterable` can be iterated\n        over multiple times. It\'s unlikely you want to override this function,\n        but if you do your result should likewise be repeatedly iterable.\n        """"""\n        lazy = getattr(self, ""lazy"", None)\n\n        if lazy is None:\n            logger.warning(\n                ""DatasetReader.lazy is not set, ""\n                ""did you forget to call the superclass constructor?""\n            )\n\n        if self._cache_directory:\n            cache_file = self._get_cache_location_for_file_path(file_path)\n        else:\n            cache_file = None\n\n        if lazy:\n            lazy_instances: AllennlpLazyDataset = _LazyInstances(\n                self._read,\n                file_path,\n                cache_file,\n                self.deserialize_instance,\n                self.serialize_instance,\n            )\n            if self.max_instances is not None:\n                lazy_instances = _MaxLazyInstances(lazy_instances, self.max_instances)\n            if not self.manual_distributed_sharding and util.is_distributed():\n                lazy_instances = _DistributedLazyInstances(lazy_instances)\n            return lazy_instances\n        else:\n            # First we read the instances, either from a cache or from the original file.\n            if cache_file and os.path.exists(cache_file):\n                instances = self._instances_from_cache_file(cache_file)\n            else:\n                instances = self._read(file_path)\n\n            if self.max_instances is not None:\n                if isinstance(instances, list):\n                    instances = instances[: self.max_instances]\n                else:\n                    instances = itertools.islice(instances, 0, self.max_instances)\n            if not self.manual_distributed_sharding and util.is_distributed():\n                from torch import distributed\n\n                logger.info(\n                    ""Returning instances i%%%d==%d"",\n                    distributed.get_world_size(),\n                    distributed.get_rank(),\n                )\n                instances = itertools.islice(\n                    instances, distributed.get_rank(), None, distributed.get_world_size()\n                )\n\n            # Then some validation.\n            if not isinstance(instances, list):\n                instances = [instance for instance in Tqdm.tqdm(instances)]\n            if not instances:\n                raise ConfigurationError(\n                    ""No instances were read from the given filepath {}. ""\n                    ""Is the path correct?"".format(file_path)\n                )\n\n            # And finally we write to the cache if we need to.\n            if (\n                self.max_instances is None\n                and not util.is_distributed()\n                and cache_file is not None\n                and not os.path.exists(cache_file)\n            ):\n                logger.info(f""Caching instances to {cache_file}"")\n                self._instances_to_cache_file(cache_file, instances)\n\n            return AllennlpDataset(instances)\n\n    def _get_cache_location_for_file_path(self, file_path: str) -> str:\n        return str(self._cache_directory / util.flatten_filename(str(file_path)))\n\n    def _read(self, file_path: str) -> Iterable[Instance]:\n        """"""\n        Reads the instances from the given file_path and returns them as an\n        `Iterable` (which could be a list or could be a generator).\n        You are strongly encouraged to use a generator, so that users can\n        read a dataset in a lazy way, if they so choose.\n        """"""\n        raise NotImplementedError\n\n    def _instances_from_cache_file(self, cache_filename: str) -> Iterable[Instance]:\n        with open(cache_filename, ""r"") as cache_file:\n            for line in cache_file:\n                yield self.deserialize_instance(line.strip())\n\n    def _instances_to_cache_file(self, cache_filename, instances) -> None:\n        with open(cache_filename, ""w"") as cache:\n            for instance in Tqdm.tqdm(instances):\n                cache.write(self.serialize_instance(instance) + ""\\n"")\n\n    def text_to_instance(self, *inputs) -> Instance:\n        """"""\n        Does whatever tokenization or processing is necessary to go from textual input to an\n        `Instance`.  The primary intended use for this is with a\n        :class:`~allennlp.predictors.predictor.Predictor`, which gets text input as a JSON\n        object and needs to process it to be input to a model.\n\n        The intent here is to share code between :func:`_read` and what happens at\n        model serving time, or any other time you want to make a prediction from new data.  We need\n        to process the data in the same way it was done at training time.  Allowing the\n        `DatasetReader` to process new text lets us accomplish this, as we can just call\n        `DatasetReader.text_to_instance` when serving predictions.\n\n        The input type here is rather vaguely specified, unfortunately.  The `Predictor` will\n        have to make some assumptions about the kind of `DatasetReader` that it\'s using, in order\n        to pass it the right information.\n        """"""\n        raise NotImplementedError\n\n    def serialize_instance(self, instance: Instance) -> str:\n        """"""\n        Serializes an `Instance` to a string.  We use this for caching the processed data.\n\n        The default implementation is to use `jsonpickle`.  If you would like some other format\n        for your pre-processed data, override this method.\n        """"""\n\n        return jsonpickle.dumps(instance)\n\n    def deserialize_instance(self, string: str) -> Instance:\n        """"""\n        Deserializes an `Instance` from a string.  We use this when reading processed data from a\n        cache.\n\n        The default implementation is to use `jsonpickle`.  If you would like some other format\n        for your pre-processed data, override this method.\n        """"""\n\n        return jsonpickle.loads(string)  # type: ignore\n'"
allennlp/data/dataset_readers/interleaving_dataset_reader.py,0,"b'from typing import Dict, Iterable\nimport json\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.fields import MetadataField\nfrom allennlp.data.instance import Instance\n\n_VALID_SCHEMES = {""round_robin"", ""all_at_once""}\n\n\n@DatasetReader.register(""interleaving"")\nclass InterleavingDatasetReader(DatasetReader):\n    """"""\n    A `DatasetReader` that wraps multiple other dataset readers,\n    and interleaves their instances, adding a `MetadataField` to\n    indicate the provenance of each instance.\n\n    Unlike most of our other dataset readers, here the `file_path` passed into\n    `read()` should be a JSON-serialized dictionary with one file_path\n    per wrapped dataset reader (and with corresponding keys).\n\n    Registered as a `DatasetReader` with name ""interleaving"".\n\n    # Parameters\n\n    readers : `Dict[str, DatasetReader]`\n        The dataset readers to wrap. The keys of this dictionary will be used\n        as the values in the MetadataField indicating provenance.\n    dataset_field_name : `str`, optional (default = `""dataset""`)\n        The name of the MetadataField indicating which dataset an instance came from.\n    scheme : `str`, optional (default = `""round_robin""`)\n        Indicates how to interleave instances. Currently the two options are ""round_robin"",\n        which repeatedly cycles through the datasets grabbing one instance from each;\n        and ""all_at_once"", which yields all the instances from the first dataset,\n        then all the instances from the second dataset, and so on. You could imagine also\n        implementing some sort of over- or under-sampling, although hasn\'t been done.\n    """"""\n\n    def __init__(\n        self,\n        readers: Dict[str, DatasetReader],\n        dataset_field_name: str = ""dataset"",\n        scheme: str = ""round_robin"",\n        **kwargs,\n    ) -> None:\n        super().__init__(**kwargs)\n        self._readers = readers\n        self._dataset_field_name = dataset_field_name\n\n        if scheme not in _VALID_SCHEMES:\n            raise ConfigurationError(f""invalid scheme: {scheme}"")\n        self._scheme = scheme\n\n    def _read_round_robin(self, datasets: Dict[str, Iterable[Instance]]) -> Iterable[Instance]:\n        remaining = set(datasets)\n        dataset_iterators = {key: iter(dataset) for key, dataset in datasets.items()}\n\n        while remaining:\n            for key, dataset in dataset_iterators.items():\n                if key in remaining:\n                    try:\n                        instance = next(dataset)\n                        instance.fields[self._dataset_field_name] = MetadataField(key)\n                        yield instance\n                    except StopIteration:\n                        remaining.remove(key)\n\n    def _read_all_at_once(self, datasets: Dict[str, Iterable[Instance]]) -> Iterable[Instance]:\n        for key, dataset in datasets.items():\n            for instance in dataset:\n                instance.fields[self._dataset_field_name] = MetadataField(key)\n                yield instance\n\n    def _read(self, file_path: str) -> Iterable[Instance]:\n        try:\n            file_paths = json.loads(file_path)\n        except json.JSONDecodeError:\n            raise ConfigurationError(\n                ""the file_path for the InterleavingDatasetReader ""\n                ""needs to be a JSON-serialized dictionary {reader_name -> file_path}""\n            )\n\n        if file_paths.keys() != self._readers.keys():\n            raise ConfigurationError(""mismatched keys"")\n\n        # Load datasets\n        datasets = {key: reader.read(file_paths[key]) for key, reader in self._readers.items()}\n\n        if self._scheme == ""round_robin"":\n            yield from self._read_round_robin(datasets)\n        elif self._scheme == ""all_at_once"":\n            yield from self._read_all_at_once(datasets)\n        else:\n            raise RuntimeError(""impossible to get here"")\n\n    def text_to_instance(self) -> Instance:  # type: ignore\n\n        raise RuntimeError(""text_to_instance doesn\'t make sense here"")\n'"
allennlp/data/dataset_readers/sequence_tagging.py,0,"b'from typing import Dict, List\nimport logging\n\nfrom overrides import overrides\n\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.fields import TextField, SequenceLabelField, MetadataField, Field\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\nfrom allennlp.data.tokenizers import Token\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_WORD_TAG_DELIMITER = ""###""\n\n\n@DatasetReader.register(""sequence_tagging"")\nclass SequenceTaggingDatasetReader(DatasetReader):\n    """"""\n    Reads instances from a pretokenised file where each line is in the following format:\n\n    ```\n    WORD###TAG [TAB] WORD###TAG [TAB] ..... \\n\n    ```\n\n    and converts it into a `Dataset` suitable for sequence tagging. You can also specify\n    alternative delimiters in the constructor.\n\n    Registered as a `DatasetReader` with name ""sequence_tagging"".\n\n    # Parameters\n\n    word_tag_delimiter: `str`, optional (default=`""###""`)\n        The text that separates each WORD from its TAG.\n    token_delimiter: `str`, optional (default=`None`)\n        The text that separates each WORD-TAG pair from the next pair. If `None`\n        then the line will just be split on whitespace.\n    token_indexers : `Dict[str, TokenIndexer]`, optional (default=`{""tokens"": SingleIdTokenIndexer()}`)\n        We use this to define the input representation for the text.  See :class:`TokenIndexer`.\n        Note that the `output` tags will always correspond to single token IDs based on how they\n        are pre-tokenised in the data file.\n    """"""\n\n    def __init__(\n        self,\n        word_tag_delimiter: str = DEFAULT_WORD_TAG_DELIMITER,\n        token_delimiter: str = None,\n        token_indexers: Dict[str, TokenIndexer] = None,\n        **kwargs,\n    ) -> None:\n        super().__init__(**kwargs)\n        self._token_indexers = token_indexers or {""tokens"": SingleIdTokenIndexer()}\n        self._word_tag_delimiter = word_tag_delimiter\n        self._token_delimiter = token_delimiter\n\n    @overrides\n    def _read(self, file_path):\n        # if `file_path` is a URL, redirect to the cache\n        file_path = cached_path(file_path)\n\n        with open(file_path, ""r"") as data_file:\n\n            logger.info(""Reading instances from lines in file at: %s"", file_path)\n            for line in data_file:\n                line = line.strip(""\\n"")\n\n                # skip blank lines\n                if not line:\n                    continue\n\n                tokens_and_tags = [\n                    pair.rsplit(self._word_tag_delimiter, 1)\n                    for pair in line.split(self._token_delimiter)\n                ]\n                tokens = [Token(token) for token, tag in tokens_and_tags]\n                tags = [tag for token, tag in tokens_and_tags]\n                yield self.text_to_instance(tokens, tags)\n\n    def text_to_instance(  # type: ignore\n        self, tokens: List[Token], tags: List[str] = None\n    ) -> Instance:\n        """"""\n        We take `pre-tokenized` input here, because we don\'t have a tokenizer in this class.\n        """"""\n\n        fields: Dict[str, Field] = {}\n        sequence = TextField(tokens, self._token_indexers)\n        fields[""tokens""] = sequence\n        fields[""metadata""] = MetadataField({""words"": [x.text for x in tokens]})\n        if tags is not None:\n            fields[""tags""] = SequenceLabelField(tags, sequence)\n        return Instance(fields)\n'"
allennlp/data/dataset_readers/sharded_dataset_reader.py,2,"b'import glob\nimport logging\nimport torch\nfrom typing import Iterable\n\nfrom allennlp.common import util\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.instance import Instance\n\n\nlogger = logging.getLogger(__name__)\n\n\n@DatasetReader.register(""sharded"")\nclass ShardedDatasetReader(DatasetReader):\n    """"""\n    Wraps another dataset reader and uses it to read from multiple input files.\n    Note that in this case the `file_path` passed to `read()` should be a glob,\n    and that the dataset reader will return instances from all files matching\n    the glob.\n\n    The order the files are processed in is deterministic to enable the\n    instances to be filtered according to worker rank in the distributed case.\n\n    Registered as a `DatasetReader` with name ""sharded"".\n\n    # Parameters\n\n    base_reader : `DatasetReader`\n        Reader with a read method that accepts a single file.\n    """"""\n\n    def __init__(self, base_reader: DatasetReader, **kwargs,) -> None:\n        super().__init__(**kwargs)\n\n        if util.is_distributed():\n            self._rank = torch.distributed.get_rank()\n            self._world_size = torch.distributed.get_world_size()\n        else:\n            self._rank = 0\n            self._world_size = 1\n\n        self.reader = base_reader\n        self.reader.manual_distributed_sharding = True\n\n    def text_to_instance(self, *args, **kwargs) -> Instance:\n        """"""\n        Just delegate to the base reader text_to_instance.\n        """"""\n        return self.reader.text_to_instance(*args, **kwargs)  # type: ignore\n\n    def _read(self, file_path: str) -> Iterable[Instance]:\n        shards = glob.glob(file_path)\n        # Ensure a consistent order.\n        shards.sort()\n\n        for i, shard in enumerate(shards):\n            if i % self._world_size == self._rank:\n                logger.info(f""reading instances from {shard}"")\n                for instance in self.reader.read(shard):\n                    yield instance\n'"
allennlp/data/dataset_readers/text_classification_json.py,0,"b'from typing import Dict, List, Union\nimport logging\nimport json\nfrom overrides import overrides\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.fields import LabelField, TextField, Field, ListField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\nfrom allennlp.data.tokenizers import Tokenizer, SpacyTokenizer\nfrom allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter\n\nlogger = logging.getLogger(__name__)\n\n\n@DatasetReader.register(""text_classification_json"")\nclass TextClassificationJsonReader(DatasetReader):\n    """"""\n    Reads tokens and their labels from a labeled text classification dataset.\n    Expects a ""text"" field and a ""label"" field in JSON format.\n\n    The output of `read` is a list of `Instance` s with the fields:\n        tokens : `TextField` and\n        label : `LabelField`\n\n    Registered as a `DatasetReader` with name ""text_classification_json"".\n\n    [0]: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n\n    # Parameters\n\n    token_indexers : `Dict[str, TokenIndexer]`, optional\n        optional (default=`{""tokens"": SingleIdTokenIndexer()}`)\n        We use this to define the input representation for the text.\n        See :class:`TokenIndexer`.\n    tokenizer : `Tokenizer`, optional (default = `{""tokens"": SpacyTokenizer()}`)\n        Tokenizer to use to split the input text into words or other kinds of tokens.\n    segment_sentences : `bool`, optional (default = `False`)\n        If True, we will first segment the text into sentences using SpaCy and then tokenize words.\n        Necessary for some models that require pre-segmentation of sentences, like [the Hierarchical\n        Attention Network][0].\n    max_sequence_length : `int`, optional (default = `None`)\n        If specified, will truncate tokens to specified maximum length.\n    skip_label_indexing : `bool`, optional (default = `False`)\n        Whether or not to skip label indexing. You might want to skip label indexing if your\n        labels are numbers, so the dataset reader doesn\'t re-number them starting from 0.\n    """"""\n\n    def __init__(\n        self,\n        token_indexers: Dict[str, TokenIndexer] = None,\n        tokenizer: Tokenizer = None,\n        segment_sentences: bool = False,\n        max_sequence_length: int = None,\n        skip_label_indexing: bool = False,\n        **kwargs,\n    ) -> None:\n        super().__init__(**kwargs)\n        self._tokenizer = tokenizer or SpacyTokenizer()\n        self._segment_sentences = segment_sentences\n        self._max_sequence_length = max_sequence_length\n        self._skip_label_indexing = skip_label_indexing\n        self._token_indexers = token_indexers or {""tokens"": SingleIdTokenIndexer()}\n        if self._segment_sentences:\n            self._sentence_segmenter = SpacySentenceSplitter()\n\n    @overrides\n    def _read(self, file_path):\n        with open(cached_path(file_path), ""r"") as data_file:\n            for line in data_file.readlines():\n                if not line:\n                    continue\n                items = json.loads(line)\n                text = items[""text""]\n                label = items.get(""label"")\n                if label is not None:\n                    if self._skip_label_indexing:\n                        try:\n                            label = int(label)\n                        except ValueError:\n                            raise ValueError(\n                                ""Labels must be integers if skip_label_indexing is True.""\n                            )\n                    else:\n                        label = str(label)\n                instance = self.text_to_instance(text=text, label=label)\n                if instance is not None:\n                    yield instance\n\n    def _truncate(self, tokens):\n        """"""\n        truncate a set of tokens using the provided sequence length\n        """"""\n        if len(tokens) > self._max_sequence_length:\n            tokens = tokens[: self._max_sequence_length]\n        return tokens\n\n    @overrides\n    def text_to_instance(\n        self, text: str, label: Union[str, int] = None\n    ) -> Instance:  # type: ignore\n        """"""\n        # Parameters\n\n        text : `str`, required.\n            The text to classify\n        label : `str`, optional, (default = `None`).\n            The label for this text.\n\n        # Returns\n\n        An `Instance` containing the following fields:\n            - tokens (`TextField`) :\n              The tokens in the sentence or phrase.\n            - label (`LabelField`) :\n              The label label of the sentence or phrase.\n        """"""\n\n        fields: Dict[str, Field] = {}\n        if self._segment_sentences:\n            sentences: List[Field] = []\n            sentence_splits = self._sentence_segmenter.split_sentences(text)\n            for sentence in sentence_splits:\n                word_tokens = self._tokenizer.tokenize(sentence)\n                if self._max_sequence_length is not None:\n                    word_tokens = self._truncate(word_tokens)\n                sentences.append(TextField(word_tokens, self._token_indexers))\n            fields[""tokens""] = ListField(sentences)\n        else:\n            tokens = self._tokenizer.tokenize(text)\n            if self._max_sequence_length is not None:\n                tokens = self._truncate(tokens)\n            fields[""tokens""] = TextField(tokens, self._token_indexers)\n        if label is not None:\n            fields[""label""] = LabelField(label, skip_indexing=self._skip_label_indexing)\n        return Instance(fields)\n'"
allennlp/data/fields/__init__.py,0,"b'""""""\nA :class:`~allennlp.data.fields.field.Field` is some piece of data instance\nthat ends up as an array in a model.\n""""""\n\nfrom allennlp.data.fields.field import Field\nfrom allennlp.data.fields.adjacency_field import AdjacencyField\nfrom allennlp.data.fields.array_field import ArrayField\nfrom allennlp.data.fields.flag_field import FlagField\nfrom allennlp.data.fields.index_field import IndexField\nfrom allennlp.data.fields.label_field import LabelField\nfrom allennlp.data.fields.list_field import ListField\nfrom allennlp.data.fields.metadata_field import MetadataField\nfrom allennlp.data.fields.multilabel_field import MultiLabelField\nfrom allennlp.data.fields.namespace_swapping_field import NamespaceSwappingField\nfrom allennlp.data.fields.sequence_field import SequenceField\nfrom allennlp.data.fields.sequence_label_field import SequenceLabelField\nfrom allennlp.data.fields.span_field import SpanField\nfrom allennlp.data.fields.text_field import TextField\n'"
allennlp/data/fields/adjacency_field.py,3,"b'from typing import Dict, List, Set, Tuple\nimport logging\nimport textwrap\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data.fields.field import Field\nfrom allennlp.data.fields.sequence_field import SequenceField\nfrom allennlp.data.vocabulary import Vocabulary\n\nlogger = logging.getLogger(__name__)\n\n\nclass AdjacencyField(Field[torch.Tensor]):\n    """"""\n    A `AdjacencyField` defines directed adjacency relations between elements\n    in a :class:`~allennlp.data.fields.sequence_field.SequenceField`.\n    Because it\'s a labeling of some other field, we take that field as input here\n    and use it to determine our padding and other things.\n\n    This field will get converted into an array of shape (sequence_field_length, sequence_field_length),\n    where the (i, j)th array element is either a binary flag indicating there is an edge from i to j,\n    or an integer label k, indicating there is a label from i to j of type k.\n\n    # Parameters\n\n    indices : `List[Tuple[int, int]]`\n    sequence_field : `SequenceField`\n        A field containing the sequence that this `AdjacencyField` is labeling.  Most often,\n        this is a `TextField`, for tagging edge relations between tokens in a sentence.\n    labels : `List[str]`, optional, (default = `None`)\n        Optional labels for the edges of the adjacency matrix.\n    label_namespace : `str`, optional (default=`\'labels\'`)\n        The namespace to use for converting tag strings into integers.  We convert tag strings to\n        integers for you, and this parameter tells the `Vocabulary` object which mapping from\n        strings to integers to use (so that ""O"" as a tag doesn\'t get the same id as ""O"" as a word).\n    padding_value : `int`, optional (default = `-1`)\n        The value to use as padding.\n    """"""\n\n    __slots__ = [\n        ""indices"",\n        ""labels"",\n        ""sequence_field"",\n        ""_label_namespace"",\n        ""_padding_value"",\n        ""_indexed_labels"",\n    ]\n\n    # It is possible that users want to use this field with a namespace which uses OOV/PAD tokens.\n    # This warning will be repeated for every instantiation of this class (i.e for every data\n    # instance), spewing a lot of warnings so this class variable is used to only log a single\n    # warning per namespace.\n    _already_warned_namespaces: Set[str] = set()\n\n    def __init__(\n        self,\n        indices: List[Tuple[int, int]],\n        sequence_field: SequenceField,\n        labels: List[str] = None,\n        label_namespace: str = ""labels"",\n        padding_value: int = -1,\n    ) -> None:\n        self.indices = indices\n        self.labels = labels\n        self.sequence_field = sequence_field\n        self._label_namespace = label_namespace\n        self._padding_value = padding_value\n        self._indexed_labels: List[int] = None\n\n        self._maybe_warn_for_namespace(label_namespace)\n        field_length = sequence_field.sequence_length()\n\n        if len(set(indices)) != len(indices):\n            raise ConfigurationError(f""Indices must be unique, but found {indices}"")\n\n        if not all(\n            0 <= index[1] < field_length and 0 <= index[0] < field_length for index in indices\n        ):\n            raise ConfigurationError(\n                f""Label indices and sequence length ""\n                f""are incompatible: {indices} and {field_length}""\n            )\n\n        if labels is not None and len(indices) != len(labels):\n            raise ConfigurationError(\n                f""Labelled indices were passed, but their lengths do not match: ""\n                f"" {labels}, {indices}""\n            )\n\n    def _maybe_warn_for_namespace(self, label_namespace: str) -> None:\n        if not (self._label_namespace.endswith(""labels"") or self._label_namespace.endswith(""tags"")):\n            if label_namespace not in self._already_warned_namespaces:\n                logger.warning(\n                    ""Your label namespace was \'%s\'. We recommend you use a namespace ""\n                    ""ending with \'labels\' or \'tags\', so we don\'t add UNK and PAD tokens by ""\n                    ""default to your vocabulary.  See documentation for ""\n                    ""`non_padded_namespaces` parameter in Vocabulary."",\n                    self._label_namespace,\n                )\n                self._already_warned_namespaces.add(label_namespace)\n\n    @overrides\n    def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        if self._indexed_labels is None and self.labels is not None:\n            for label in self.labels:\n                counter[self._label_namespace][label] += 1  # type: ignore\n\n    @overrides\n    def index(self, vocab: Vocabulary):\n        if self.labels is not None:\n            self._indexed_labels = [\n                vocab.get_token_index(label, self._label_namespace) for label in self.labels\n            ]\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n        return {""num_tokens"": self.sequence_field.sequence_length()}\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:\n        desired_num_tokens = padding_lengths[""num_tokens""]\n        tensor = torch.ones(desired_num_tokens, desired_num_tokens) * self._padding_value\n        labels = self._indexed_labels or [1 for _ in range(len(self.indices))]\n\n        for index, label in zip(self.indices, labels):\n            tensor[index] = label\n        return tensor\n\n    @overrides\n    def empty_field(self) -> ""AdjacencyField"":\n\n        # The empty_list here is needed for mypy\n        empty_list: List[Tuple[int, int]] = []\n        adjacency_field = AdjacencyField(\n            empty_list, self.sequence_field.empty_field(), padding_value=self._padding_value\n        )\n        return adjacency_field\n\n    def __str__(self) -> str:\n        length = self.sequence_field.sequence_length()\n        formatted_labels = """".join(\n            ""\\t\\t"" + labels + ""\\n"" for labels in textwrap.wrap(repr(self.labels), 100)\n        )\n        formatted_indices = """".join(\n            ""\\t\\t"" + index + ""\\n"" for index in textwrap.wrap(repr(self.indices), 100)\n        )\n        return (\n            f""AdjacencyField of length {length}\\n""\n            f""\\t\\twith indices:\\n {formatted_indices}\\n""\n            f""\\t\\tand labels:\\n {formatted_labels} \\t\\tin namespace: \'{self._label_namespace}\'.""\n        )\n\n    def __len__(self):\n        return len(self.sequence_field)\n'"
allennlp/data/fields/array_field.py,2,"b'from typing import Dict\n\nimport numpy\nimport torch\nfrom overrides import overrides\n\nfrom allennlp.data.fields.field import Field\n\n\nclass ArrayField(Field[numpy.ndarray]):\n    """"""\n    A class representing an array, which could have arbitrary dimensions.\n    A batch of these arrays are padded to the max dimension length in the batch\n    for each dimension.\n    """"""\n\n    __slots__ = [""array"", ""padding_value"", ""dtype""]\n\n    def __init__(\n        self, array: numpy.ndarray, padding_value: int = 0, dtype: numpy.dtype = numpy.float32\n    ) -> None:\n        self.array = array\n        self.padding_value = padding_value\n        self.dtype = dtype\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n        return {""dimension_"" + str(i): shape for i, shape in enumerate(self.array.shape)}\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:\n        max_shape = [padding_lengths[""dimension_{}"".format(i)] for i in range(len(padding_lengths))]\n\n        # Convert explicitly to an ndarray just in case it\'s an scalar\n        # (it\'d end up not being an ndarray otherwise).\n        # Also, the explicit dtype declaration for `asarray` is necessary for scalars.\n        return_array = numpy.asarray(\n            numpy.ones(max_shape, dtype=self.dtype) * self.padding_value, dtype=self.dtype\n        )\n\n        # If the tensor has a different shape from the largest tensor, pad dimensions with zeros to\n        # form the right shaped list of slices for insertion into the final tensor.\n        slicing_shape = list(self.array.shape)\n        if len(self.array.shape) < len(max_shape):\n            slicing_shape = slicing_shape + [\n                0 for _ in range(len(max_shape) - len(self.array.shape))\n            ]\n        slices = tuple([slice(0, x) for x in slicing_shape])\n        return_array[slices] = self.array\n        tensor = torch.from_numpy(return_array)\n        return tensor\n\n    @overrides\n    def empty_field(self):\n        # Pass the padding_value, so that any outer field, e.g., `ListField[ArrayField]` uses the\n        # same padding_value in the padded ArrayFields\n        return ArrayField(\n            numpy.array([], dtype=self.dtype), padding_value=self.padding_value, dtype=self.dtype\n        )\n\n    def __str__(self) -> str:\n        return f""ArrayField with shape: {self.array.shape} and dtype: {self.dtype}.""\n\n    def __len__(self):\n        return 1 if self.array.ndim == 0 else self.array.shape[0]\n'"
allennlp/data/fields/field.py,2,"b'from copy import deepcopy\nfrom typing import Dict, Generic, List, TypeVar\n\nimport torch\n\nfrom allennlp.data.vocabulary import Vocabulary\n\nDataArray = TypeVar(\n    ""DataArray"", torch.Tensor, Dict[str, torch.Tensor], Dict[str, Dict[str, torch.Tensor]]\n)\n\n\nclass Field(Generic[DataArray]):\n    """"""\n    A `Field` is some piece of a data instance that ends up as an tensor in a model (either as an\n    input or an output).  Data instances are just collections of fields.\n\n    Fields go through up to two steps of processing: (1) tokenized fields are converted into token\n    ids, (2) fields containing token ids (or any other numeric data) are padded (if necessary) and\n    converted into tensors.  The `Field` API has methods around both of these steps, though they\n    may not be needed for some concrete `Field` classes - if your field doesn\'t have any strings\n    that need indexing, you don\'t need to implement `count_vocab_items` or `index`.  These\n    methods `pass` by default.\n\n    Once a vocabulary is computed and all fields are indexed, we will determine padding lengths,\n    then intelligently batch together instances and pad them into actual tensors.\n    """"""\n\n    __slots__ = []  # type: ignore\n\n    def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        """"""\n        If there are strings in this field that need to be converted into integers through a\n        :class:`Vocabulary`, here is where we count them, to determine which tokens are in or out\n        of the vocabulary.\n\n        If your `Field` does not have any strings that need to be converted into indices, you do\n        not need to implement this method.\n\n        A note on this `counter`: because `Fields` can represent conceptually different things,\n        we separate the vocabulary items by `namespaces`.  This way, we can use a single shared\n        mechanism to handle all mappings from strings to integers in all fields, while keeping\n        words in a `TextField` from sharing the same ids with labels in a `LabelField` (e.g.,\n        ""entailment"" or ""contradiction"" are labels in an entailment task)\n\n        Additionally, a single `Field` might want to use multiple namespaces - `TextFields` can\n        be represented as a combination of word ids and character ids, and you don\'t want words and\n        characters to share the same vocabulary - ""a"" as a word should get a different id from ""a""\n        as a character, and the vocabulary sizes of words and characters are very different.\n\n        Because of this, the first key in the `counter` object is a `namespace`, like ""tokens"",\n        ""token_characters"", ""tags"", or ""labels"", and the second key is the actual vocabulary item.\n        """"""\n        pass\n\n    def index(self, vocab: Vocabulary):\n        """"""\n        Given a :class:`Vocabulary`, converts all strings in this field into (typically) integers.\n        This `modifies` the `Field` object, it does not return anything.\n\n        If your `Field` does not have any strings that need to be converted into indices, you do\n        not need to implement this method.\n        """"""\n        pass\n\n    def get_padding_lengths(self) -> Dict[str, int]:\n        """"""\n        If there are things in this field that need padding, note them here.  In order to pad a\n        batch of instance, we get all of the lengths from the batch, take the max, and pad\n        everything to that length (or use a pre-specified maximum length).  The return value is a\n        dictionary mapping keys to lengths, like `{\'num_tokens\': 13}`.\n\n        This is always called after :func:`index`.\n        """"""\n        raise NotImplementedError\n\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> DataArray:\n        """"""\n        Given a set of specified padding lengths, actually pad the data in this field and return a\n        torch Tensor (or a more complex data structure) of the correct shape.  We also take a\n        couple of parameters that are important when constructing torch Tensors.\n\n        # Parameters\n\n        padding_lengths : `Dict[str, int]`\n            This dictionary will have the same keys that were produced in\n            :func:`get_padding_lengths`.  The values specify the lengths to use when padding each\n            relevant dimension, aggregated across all instances in a batch.\n        """"""\n        raise NotImplementedError\n\n    def empty_field(self) -> ""Field"":\n        """"""\n        So that `ListField` can pad the number of fields in a list (e.g., the number of answer\n        option `TextFields`), we need a representation of an empty field of each type.  This\n        returns that.  This will only ever be called when we\'re to the point of calling\n        :func:`as_tensor`, so you don\'t need to worry about `get_padding_lengths`,\n        `count_vocab_items`, etc., being called on this empty field.\n\n        We make this an instance method instead of a static method so that if there is any state\n        in the Field, we can copy it over (e.g., the token indexers in `TextField`).\n        """"""\n        raise NotImplementedError\n\n    def batch_tensors(self, tensor_list: List[DataArray]) -> DataArray:  # type: ignore\n        """"""\n        Takes the output of `Field.as_tensor()` from a list of `Instances` and merges it into\n        one batched tensor for this `Field`.  The default implementation here in the base class\n        handles cases where `as_tensor` returns a single torch tensor per instance.  If your\n        subclass returns something other than this, you need to override this method.\n\n        This operation does not modify `self`, but in some cases we need the information\n        contained in `self` in order to perform the batching, so this is an instance method, not\n        a class method.\n        """"""\n\n        return torch.stack(tensor_list)\n\n    def __eq__(self, other) -> bool:\n        if isinstance(self, other.__class__):\n            # With the way ""slots"" classes work, self.__slots__ only gives the slots defined\n            # by the current class, but not any of its base classes. Therefore to truly\n            # check for equality we have to check through all of the slots in all of the\n            # base classes as well.\n            for class_ in self.__class__.mro():\n                for attr in getattr(class_, ""__slots__"", []):\n                    if getattr(self, attr) != getattr(other, attr):\n                        return False\n            # It\'s possible that a subclass was not defined as a slots class, in which\n            # case we\'ll need to check __dict__.\n            if hasattr(self, ""__dict__""):\n                return self.__dict__ == other.__dict__\n            return True\n        return NotImplemented\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def duplicate(self):\n        return deepcopy(self)\n'"
allennlp/data/fields/flag_field.py,0,"b'from typing import Any, Dict, List\n\nfrom overrides import overrides\n\nfrom allennlp.data.fields.field import Field\n\n\nclass FlagField(Field[Any]):\n    """"""\n    A class representing a flag, which must be constant across all instances in a batch.\n    This will be passed to a `forward` method as a single value of whatever type you pass in.\n    """"""\n\n    __slots__ = [""flag_value""]\n\n    def __init__(self, flag_value: Any) -> None:\n        self.flag_value = flag_value\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n        return {}\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> Any:\n        return self.flag_value\n\n    @overrides\n    def empty_field(self):\n        # Because this has to be constant across all instances in a batch, we need to keep the same\n        # value.\n        return FlagField(self.flag_value)\n\n    def __str__(self) -> str:\n        return f""FlagField({self.flag_value})""\n\n    def __len__(self) -> int:\n        return 1\n\n    @overrides\n    def batch_tensors(self, tensor_list: List[Any]) -> Any:\n        if len(set(tensor_list)) != 1:\n            raise ValueError(\n                f""Got different values in a FlagField when trying to batch them: {tensor_list}""\n            )\n        return tensor_list[0]\n'"
allennlp/data/fields/index_field.py,3,"b'from typing import Dict\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.data.fields.field import Field\nfrom allennlp.data.fields.sequence_field import SequenceField\nfrom allennlp.common.checks import ConfigurationError\n\n\nclass IndexField(Field[torch.Tensor]):\n    """"""\n    An `IndexField` is an index into a\n    :class:`~allennlp.data.fields.sequence_field.SequenceField`, as might be used for representing\n    a correct answer option in a list, or a span begin and span end position in a passage, for\n    example.  Because it\'s an index into a :class:`SequenceField`, we take one of those as input\n    and use it to compute padding lengths.\n\n    # Parameters\n\n    index : `int`\n        The index of the answer in the :class:`SequenceField`.  This is typically the ""correct\n        answer"" in some classification decision over the sequence, like where an answer span starts\n        in SQuAD, or which answer option is correct in a multiple choice question.  A value of\n        `-1` means there is no label, which can be used for padding or other purposes.\n    sequence_field : `SequenceField`\n        A field containing the sequence that this `IndexField` is a pointer into.\n    """"""\n\n    __slots__ = [""sequence_index"", ""sequence_field""]\n\n    def __init__(self, index: int, sequence_field: SequenceField) -> None:\n        self.sequence_index = index\n        self.sequence_field = sequence_field\n\n        if not isinstance(index, int):\n            raise ConfigurationError(\n                ""IndexFields must be passed integer indices. ""\n                ""Found index: {} with type: {}."".format(index, type(index))\n            )\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n\n        return {}\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:\n\n        tensor = torch.LongTensor([self.sequence_index])\n        return tensor\n\n    @overrides\n    def empty_field(self):\n        return IndexField(-1, self.sequence_field.empty_field())\n\n    def __str__(self) -> str:\n        return f""IndexField with index: {self.sequence_index}.""\n\n    def __eq__(self, other) -> bool:\n        # Allow equality checks to ints that are the sequence index\n        if isinstance(other, int):\n            return self.sequence_index == other\n        return super().__eq__(other)\n\n    def __len__(self):\n        return 1\n'"
allennlp/data/fields/label_field.py,3,"b'from typing import Dict, Union, Set\nimport logging\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.data.fields.field import Field\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.common.checks import ConfigurationError\n\nlogger = logging.getLogger(__name__)\n\n\nclass LabelField(Field[torch.Tensor]):\n    """"""\n    A `LabelField` is a categorical label of some kind, where the labels are either strings of\n    text or 0-indexed integers (if you wish to skip indexing by passing skip_indexing=True).\n    If the labels need indexing, we will use a :class:`Vocabulary` to convert the string labels\n    into integers.\n\n    This field will get converted into an integer index representing the class label.\n\n    # Parameters\n\n    label : `Union[str, int]`\n    label_namespace : `str`, optional (default=`""labels""`)\n        The namespace to use for converting label strings into integers.  We map label strings to\n        integers for you (e.g., ""entailment"" and ""contradiction"" get converted to 0, 1, ...),\n        and this namespace tells the `Vocabulary` object which mapping from strings to integers\n        to use (so ""entailment"" as a label doesn\'t get the same integer id as ""entailment"" as a\n        word).  If you have multiple different label fields in your data, you should make sure you\n        use different namespaces for each one, always using the suffix ""labels"" (e.g.,\n        ""passage_labels"" and ""question_labels"").\n    skip_indexing : `bool`, optional (default=`False`)\n        If your labels are 0-indexed integers, you can pass in this flag, and we\'ll skip the indexing\n        step.  If this is `False` and your labels are not strings, this throws a `ConfigurationError`.\n    """"""\n\n    __slots__ = [""label"", ""_label_namespace"", ""_label_id"", ""_skip_indexing""]\n\n    # Most often, you probably don\'t want to have OOV/PAD tokens with a LabelField, so we warn you\n    # about it when you pick a namespace that will getting these tokens by default.  It is\n    # possible, however, that you _do_ actually want OOV/PAD tokens with this Field.  This class\n    # variable is used to make sure that we only log a single warning for this per namespace, and\n    # not every time you create one of these Field objects.\n    _already_warned_namespaces: Set[str] = set()\n\n    def __init__(\n        self, label: Union[str, int], label_namespace: str = ""labels"", skip_indexing: bool = False\n    ) -> None:\n        self.label = label\n        self._label_namespace = label_namespace\n        self._label_id = None\n        self._maybe_warn_for_namespace(label_namespace)\n        self._skip_indexing = skip_indexing\n\n        if skip_indexing:\n            if not isinstance(label, int):\n                raise ConfigurationError(\n                    ""In order to skip indexing, your labels must be integers. ""\n                    ""Found label = {}"".format(label)\n                )\n            self._label_id = label\n        elif not isinstance(label, str):\n            raise ConfigurationError(\n                ""LabelFields must be passed a string label if skip_indexing=False. ""\n                ""Found label: {} with type: {}."".format(label, type(label))\n            )\n\n    def _maybe_warn_for_namespace(self, label_namespace: str) -> None:\n        if not (self._label_namespace.endswith(""labels"") or self._label_namespace.endswith(""tags"")):\n            if label_namespace not in self._already_warned_namespaces:\n                logger.warning(\n                    ""Your label namespace was \'%s\'. We recommend you use a namespace ""\n                    ""ending with \'labels\' or \'tags\', so we don\'t add UNK and PAD tokens by ""\n                    ""default to your vocabulary.  See documentation for ""\n                    ""`non_padded_namespaces` parameter in Vocabulary."",\n                    self._label_namespace,\n                )\n                self._already_warned_namespaces.add(label_namespace)\n\n    @overrides\n    def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        if self._label_id is None:\n            counter[self._label_namespace][self.label] += 1  # type: ignore\n\n    @overrides\n    def index(self, vocab: Vocabulary):\n        if not self._skip_indexing:\n            self._label_id = vocab.get_token_index(\n                self.label, self._label_namespace  # type: ignore\n            )\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n        return {}\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:\n\n        tensor = torch.tensor(self._label_id, dtype=torch.long)\n        return tensor\n\n    @overrides\n    def empty_field(self):\n        return LabelField(-1, self._label_namespace, skip_indexing=True)\n\n    def __str__(self) -> str:\n        return f""LabelField with label: {self.label} in namespace: \'{self._label_namespace}\'.\'""\n\n    def __len__(self):\n        return 1\n'"
allennlp/data/fields/list_field.py,0,"b'from typing import Dict, List, Iterator\n\nfrom overrides import overrides\n\nfrom allennlp.data.fields.field import DataArray, Field\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.data.fields.sequence_field import SequenceField\nfrom allennlp.common.util import pad_sequence_to_length\n\n\nclass ListField(SequenceField[DataArray]):\n    """"""\n    A `ListField` is a list of other fields.  You would use this to represent, e.g., a list of\n    answer options that are themselves `TextFields`.\n\n    This field will get converted into a tensor that has one more mode than the items in the list.\n    If this is a list of `TextFields` that have shape (num_words, num_characters), this\n    `ListField` will output a tensor of shape (num_sentences, num_words, num_characters).\n\n    # Parameters\n\n    field_list : `List[Field]`\n        A list of `Field` objects to be concatenated into a single input tensor.  All of the\n        contained `Field` objects must be of the same type.\n    """"""\n\n    __slots__ = [""field_list""]\n\n    def __init__(self, field_list: List[Field]) -> None:\n        field_class_set = {field.__class__ for field in field_list}\n        assert (\n            len(field_class_set) == 1\n        ), ""ListFields must contain a single field type, found "" + str(field_class_set)\n        # Not sure why mypy has a hard time with this type...\n        self.field_list: List[Field] = field_list\n\n    # Sequence[Field] methods\n    def __iter__(self) -> Iterator[Field]:\n        return iter(self.field_list)\n\n    def __getitem__(self, idx: int) -> Field:\n        return self.field_list[idx]\n\n    def __len__(self) -> int:\n        return len(self.field_list)\n\n    @overrides\n    def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        for field in self.field_list:\n            field.count_vocab_items(counter)\n\n    @overrides\n    def index(self, vocab: Vocabulary):\n        for field in self.field_list:\n            field.index(vocab)\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n        field_lengths = [field.get_padding_lengths() for field in self.field_list]\n        padding_lengths = {""num_fields"": len(self.field_list)}\n\n        # We take the set of all possible padding keys for all fields, rather\n        # than just a random key, because it is possible for fields to be empty\n        # when we pad ListFields.\n        possible_padding_keys = [\n            key for field_length in field_lengths for key in list(field_length.keys())\n        ]\n\n        for key in set(possible_padding_keys):\n            # In order to be able to nest ListFields, we need to scope the padding length keys\n            # appropriately, so that nested ListFields don\'t all use the same ""num_fields"" key.  So\n            # when we construct the dictionary from the list of fields, we add something to the\n            # name, and we remove it when padding the list of fields.\n            padding_lengths[""list_"" + key] = max(x[key] if key in x else 0 for x in field_lengths)\n\n        # Set minimum padding length to handle empty list fields.\n        for padding_key in padding_lengths:\n            padding_lengths[padding_key] = max(padding_lengths[padding_key], 1)\n\n        return padding_lengths\n\n    @overrides\n    def sequence_length(self) -> int:\n        return len(self.field_list)\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> DataArray:\n        padded_field_list = pad_sequence_to_length(\n            self.field_list, padding_lengths[""num_fields""], self.field_list[0].empty_field\n        )\n        # Here we\'re removing the scoping on the padding length keys that we added in\n        # `get_padding_lengths`; see the note there for more detail.\n        child_padding_lengths = {\n            key.replace(""list_"", """", 1): value\n            for key, value in padding_lengths.items()\n            if key.startswith(""list_"")\n        }\n        padded_fields = [field.as_tensor(child_padding_lengths) for field in padded_field_list]\n        return self.field_list[0].batch_tensors(padded_fields)\n\n    @overrides\n    def empty_field(self):\n        # Our ""empty"" list field will actually have a single field in the list, so that we can\n        # correctly construct nested lists.  For example, if we have a type that is\n        # `ListField[ListField[LabelField]]`, we need the top-level `ListField` to know to\n        # construct a `ListField[LabelField]` when it\'s padding, and the nested `ListField` needs\n        # to know that it\'s empty objects are `LabelFields`.  Having an ""empty"" list actually have\n        # length one makes this all work out, and we\'ll always be padding to at least length 1,\n        # anyway.\n        return ListField([self.field_list[0].empty_field()])\n\n    @overrides\n    def batch_tensors(self, tensor_list: List[DataArray]) -> DataArray:\n        # We defer to the class we\'re wrapping in a list to handle the batching.\n        return self.field_list[0].batch_tensors(tensor_list)\n\n    def __str__(self) -> str:\n        field_class = self.field_list[0].__class__.__name__\n        base_string = f""ListField of {len(self.field_list)} {field_class}s : \\n""\n        return "" "".join([base_string] + [f""\\t {field} \\n"" for field in self.field_list])\n'"
allennlp/data/fields/metadata_field.py,0,"b'from typing import Any, Dict, List, Mapping\n\nfrom overrides import overrides\n\nfrom allennlp.data.fields.field import DataArray, Field\n\n\nclass MetadataField(Field[DataArray], Mapping[str, Any]):\n    """"""\n    A `MetadataField` is a `Field` that does not get converted into tensors.  It just carries\n    side information that might be needed later on, for computing some third-party metric, or\n    outputting debugging information, or whatever else you need.  We use this in the BiDAF model,\n    for instance, to keep track of question IDs and passage token offsets, so we can more easily\n    use the official evaluation script to compute metrics.\n\n    We don\'t try to do any kind of smart combination of this field for batched input - when you use\n    this `Field` in a model, you\'ll get a list of metadata objects, one for each instance in the\n    batch.\n\n    # Parameters\n\n    metadata : `Any`\n        Some object containing the metadata that you want to store.  It\'s likely that you\'ll want\n        this to be a dictionary, but it could be anything you want.\n    """"""\n\n    __slots__ = [""metadata""]\n\n    def __init__(self, metadata: Any) -> None:\n        self.metadata = metadata\n\n    def __getitem__(self, key: str) -> Any:\n        try:\n            return self.metadata[key]  # type: ignore\n        except TypeError:\n            raise TypeError(""your metadata is not a dict"")\n\n    def __iter__(self):\n        try:\n            return iter(self.metadata)\n        except TypeError:\n            raise TypeError(""your metadata is not iterable"")\n\n    def __len__(self):\n        try:\n            return len(self.metadata)\n        except TypeError:\n            raise TypeError(""your metadata has no length"")\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n        return {}\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> DataArray:\n\n        return self.metadata  # type: ignore\n\n    @overrides\n    def empty_field(self) -> ""MetadataField"":\n        return MetadataField(None)\n\n    @overrides\n    def batch_tensors(self, tensor_list: List[DataArray]) -> List[DataArray]:  # type: ignore\n        return tensor_list\n\n    def __str__(self) -> str:\n        return ""MetadataField (print field.metadata to see specific information).""\n'"
allennlp/data/fields/multilabel_field.py,4,"b'from typing import Dict, Union, Sequence, Set, Optional, cast\nimport logging\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.data.fields.field import Field\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.common.checks import ConfigurationError\n\nlogger = logging.getLogger(__name__)\n\n\nclass MultiLabelField(Field[torch.Tensor]):\n    """"""\n    A `MultiLabelField` is an extension of the :class:`LabelField` that allows for multiple labels.\n    It is particularly useful in multi-label classification where more than one label can be correct.\n    As with the :class:`LabelField`, labels are either strings of text or 0-indexed integers (if you wish\n    to skip indexing by passing skip_indexing=True).\n    If the labels need indexing, we will use a :class:`Vocabulary` to convert the string labels\n    into integers.\n\n    This field will get converted into a vector of length equal to the vocabulary size with\n    one hot encoding for the labels (all zeros, and ones for the labels).\n\n    # Parameters\n\n    labels : `Sequence[Union[str, int]]`\n    label_namespace : `str`, optional (default=`""labels""`)\n        The namespace to use for converting label strings into integers.  We map label strings to\n        integers for you (e.g., ""entailment"" and ""contradiction"" get converted to 0, 1, ...),\n        and this namespace tells the `Vocabulary` object which mapping from strings to integers\n        to use (so ""entailment"" as a label doesn\'t get the same integer id as ""entailment"" as a\n        word).  If you have multiple different label fields in your data, you should make sure you\n        use different namespaces for each one, always using the suffix ""labels"" (e.g.,\n        ""passage_labels"" and ""question_labels"").\n    skip_indexing : `bool`, optional (default=`False`)\n        If your labels are 0-indexed integers, you can pass in this flag, and we\'ll skip the indexing\n        step.  If this is `False` and your labels are not strings, this throws a `ConfigurationError`.\n    num_labels : `int`, optional (default=`None`)\n        If `skip_indexing=True`, the total number of possible labels should be provided, which is required\n        to decide the size of the output tensor. `num_labels` should equal largest label id + 1.\n        If `skip_indexing=False`, `num_labels` is not required.\n\n    """"""\n\n    __slots__ = [""labels"", ""_label_namespace"", ""_label_ids"", ""_num_labels""]\n\n    # It is possible that users want to use this field with a namespace which uses OOV/PAD tokens.\n    # This warning will be repeated for every instantiation of this class (i.e for every data\n    # instance), spewing a lot of warnings so this class variable is used to only log a single\n    # warning per namespace.\n    _already_warned_namespaces: Set[str] = set()\n\n    def __init__(\n        self,\n        labels: Sequence[Union[str, int]],\n        label_namespace: str = ""labels"",\n        skip_indexing: bool = False,\n        num_labels: Optional[int] = None,\n    ) -> None:\n        self.labels = labels\n        self._label_namespace = label_namespace\n        self._label_ids = None\n        self._maybe_warn_for_namespace(label_namespace)\n        self._num_labels = num_labels\n\n        if skip_indexing and self.labels:\n            if not all(isinstance(label, int) for label in labels):\n                raise ConfigurationError(\n                    ""In order to skip indexing, your labels must be integers. ""\n                    ""Found labels = {}"".format(labels)\n                )\n            if not num_labels:\n                raise ConfigurationError(""In order to skip indexing, num_labels can\'t be None."")\n\n            if not all(cast(int, label) < num_labels for label in labels):\n                raise ConfigurationError(\n                    ""All labels should be < num_labels. ""\n                    ""Found num_labels = {} and labels = {} "".format(num_labels, labels)\n                )\n\n            self._label_ids = labels\n        else:\n            if not all(isinstance(label, str) for label in labels):\n                raise ConfigurationError(\n                    ""MultiLabelFields expects string labels if skip_indexing=False. ""\n                    ""Found labels: {}"".format(labels)\n                )\n\n    def _maybe_warn_for_namespace(self, label_namespace: str) -> None:\n        if not (label_namespace.endswith(""labels"") or label_namespace.endswith(""tags"")):\n            if label_namespace not in self._already_warned_namespaces:\n                logger.warning(\n                    ""Your label namespace was \'%s\'. We recommend you use a namespace ""\n                    ""ending with \'labels\' or \'tags\', so we don\'t add UNK and PAD tokens by ""\n                    ""default to your vocabulary.  See documentation for ""\n                    ""`non_padded_namespaces` parameter in Vocabulary."",\n                    self._label_namespace,\n                )\n                self._already_warned_namespaces.add(label_namespace)\n\n    @overrides\n    def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        if self._label_ids is None:\n            for label in self.labels:\n                counter[self._label_namespace][label] += 1  # type: ignore\n\n    @overrides\n    def index(self, vocab: Vocabulary):\n        if self._label_ids is None:\n            self._label_ids = [\n                vocab.get_token_index(label, self._label_namespace)  # type: ignore\n                for label in self.labels\n            ]\n        if not self._num_labels:\n            self._num_labels = vocab.get_vocab_size(self._label_namespace)\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n        return {}\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:\n        tensor = torch.zeros(self._num_labels, dtype=torch.long)  # vector of zeros\n        if self._label_ids:\n            tensor.scatter_(0, torch.LongTensor(self._label_ids), 1)\n\n        return tensor\n\n    @overrides\n    def empty_field(self):\n        return MultiLabelField(\n            [], self._label_namespace, skip_indexing=True, num_labels=self._num_labels\n        )\n\n    def __str__(self) -> str:\n        return (\n            f""MultiLabelField with labels: {self.labels} in namespace: \'{self._label_namespace}\'.\'""\n        )\n\n    def __len__(self):\n        return 1\n'"
allennlp/data/fields/namespace_swapping_field.py,3,"b'from typing import Dict, List\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.common.util import pad_sequence_to_length\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.fields.field import Field\n\n\nclass NamespaceSwappingField(Field[torch.Tensor]):\n    """"""\n    A `NamespaceSwappingField` is used to map tokens in one namespace to tokens in another namespace.\n    It is used by seq2seq models with a copy mechanism that copies tokens from the source\n    sentence into the target sentence.\n\n    # Parameters\n\n    source_tokens : `List[Token]`\n        The tokens from the source sentence.\n    target_namespace : `str`\n        The namespace that the tokens from the source sentence will be mapped to.\n    """"""\n\n    __slots__ = [""_source_tokens"", ""_target_namespace"", ""_mapping_array""]\n\n    def __init__(self, source_tokens: List[Token], target_namespace: str) -> None:\n        self._source_tokens = source_tokens\n        self._target_namespace = target_namespace\n        self._mapping_array: List[int] = None\n\n    @overrides\n    def index(self, vocab: Vocabulary):\n        self._mapping_array = [\n            vocab.get_token_index(x.text, self._target_namespace) for x in self._source_tokens\n        ]\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n        return {""num_tokens"": len(self._source_tokens)}\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:\n        desired_length = padding_lengths[""num_tokens""]\n        padded_tokens = pad_sequence_to_length(self._mapping_array, desired_length)\n        tensor = torch.LongTensor(padded_tokens)\n        return tensor\n\n    @overrides\n    def empty_field(self) -> ""NamespaceSwappingField"":\n        return NamespaceSwappingField([], self._target_namespace)\n\n    def __len__(self):\n        return len(self._source_tokens)\n'"
allennlp/data/fields/sequence_field.py,0,"b'from allennlp.data.fields.field import DataArray, Field\n\n\nclass SequenceField(Field[DataArray]):\n    """"""\n    A `SequenceField` represents a sequence of things.  This class just adds a method onto\n    `Field`: :func:`sequence_length`.  It exists so that `SequenceLabelField`, `IndexField` and other\n    similar `Fields` can have a single type to require, with a consistent API, whether they are\n    pointing to words in a `TextField`, items in a `ListField`, or something else.\n    """"""\n\n    __slots__ = []  # type: ignore\n\n    def sequence_length(self) -> int:\n        """"""\n        How many elements are there in this sequence?\n        """"""\n        raise NotImplementedError\n\n    def empty_field(self) -> ""SequenceField"":\n        raise NotImplementedError\n'"
allennlp/data/fields/sequence_label_field.py,3,"b'from typing import Dict, List, Union, Set, Iterator\nimport logging\nimport textwrap\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.util import pad_sequence_to_length\nfrom allennlp.data.fields.field import Field\nfrom allennlp.data.fields.sequence_field import SequenceField\nfrom allennlp.data.vocabulary import Vocabulary\n\nlogger = logging.getLogger(__name__)\n\n\nclass SequenceLabelField(Field[torch.Tensor]):\n    """"""\n    A `SequenceLabelField` assigns a categorical label to each element in a\n    :class:`~allennlp.data.fields.sequence_field.SequenceField`.\n    Because it\'s a labeling of some other field, we take that field as input here, and we use it to\n    determine our padding and other things.\n\n    This field will get converted into a list of integer class ids, representing the correct class\n    for each element in the sequence.\n\n    # Parameters\n\n    labels : `Union[List[str], List[int]]`\n        A sequence of categorical labels, encoded as strings or integers.  These could be POS tags like [NN,\n        JJ, ...], BIO tags like [B-PERS, I-PERS, O, O, ...], or any other categorical tag sequence. If the\n        labels are encoded as integers, they will not be indexed using a vocab.\n    sequence_field : `SequenceField`\n        A field containing the sequence that this `SequenceLabelField` is labeling.  Most often, this is a\n        `TextField`, for tagging individual tokens in a sentence.\n    label_namespace : `str`, optional (default=`\'labels\'`)\n        The namespace to use for converting tag strings into integers.  We convert tag strings to\n        integers for you, and this parameter tells the `Vocabulary` object which mapping from\n        strings to integers to use (so that ""O"" as a tag doesn\'t get the same id as ""O"" as a word).\n    """"""\n\n    __slots__ = [\n        ""labels"",\n        ""sequence_field"",\n        ""_label_namespace"",\n        ""_indexed_labels"",\n        ""_skip_indexing"",\n    ]\n\n    # It is possible that users want to use this field with a namespace which uses OOV/PAD tokens.\n    # This warning will be repeated for every instantiation of this class (i.e for every data\n    # instance), spewing a lot of warnings so this class variable is used to only log a single\n    # warning per namespace.\n    _already_warned_namespaces: Set[str] = set()\n\n    def __init__(\n        self,\n        labels: Union[List[str], List[int]],\n        sequence_field: SequenceField,\n        label_namespace: str = ""labels"",\n    ) -> None:\n        self.labels = labels\n        self.sequence_field = sequence_field\n        self._label_namespace = label_namespace\n        self._indexed_labels = None\n        self._maybe_warn_for_namespace(label_namespace)\n        if len(labels) != sequence_field.sequence_length():\n            raise ConfigurationError(\n                ""Label length and sequence length ""\n                ""don\'t match: %d and %d"" % (len(labels), sequence_field.sequence_length())\n            )\n\n        self._skip_indexing = False\n        if all(isinstance(x, int) for x in labels):\n            self._indexed_labels = labels\n            self._skip_indexing = True\n\n        elif not all(isinstance(x, str) for x in labels):\n            raise ConfigurationError(\n                ""SequenceLabelFields must be passed either all ""\n                ""strings or all ints. Found labels {} with ""\n                ""types: {}."".format(labels, [type(x) for x in labels])\n            )\n\n    def _maybe_warn_for_namespace(self, label_namespace: str) -> None:\n        if not (self._label_namespace.endswith(""labels"") or self._label_namespace.endswith(""tags"")):\n            if label_namespace not in self._already_warned_namespaces:\n                logger.warning(\n                    ""Your label namespace was \'%s\'. We recommend you use a namespace ""\n                    ""ending with \'labels\' or \'tags\', so we don\'t add UNK and PAD tokens by ""\n                    ""default to your vocabulary.  See documentation for ""\n                    ""`non_padded_namespaces` parameter in Vocabulary."",\n                    self._label_namespace,\n                )\n                self._already_warned_namespaces.add(label_namespace)\n\n    # Sequence methods\n    def __iter__(self) -> Iterator[Union[str, int]]:\n        return iter(self.labels)\n\n    def __getitem__(self, idx: int) -> Union[str, int]:\n        return self.labels[idx]\n\n    def __len__(self) -> int:\n        return len(self.labels)\n\n    @overrides\n    def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        if self._indexed_labels is None:\n            for label in self.labels:\n                counter[self._label_namespace][label] += 1  # type: ignore\n\n    @overrides\n    def index(self, vocab: Vocabulary):\n        if not self._skip_indexing:\n            self._indexed_labels = [\n                vocab.get_token_index(label, self._label_namespace)  # type: ignore\n                for label in self.labels\n            ]\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n        return {""num_tokens"": self.sequence_field.sequence_length()}\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:\n        desired_num_tokens = padding_lengths[""num_tokens""]\n        padded_tags = pad_sequence_to_length(self._indexed_labels, desired_num_tokens)\n        tensor = torch.LongTensor(padded_tags)\n        return tensor\n\n    @overrides\n    def empty_field(self) -> ""SequenceLabelField"":\n        # The empty_list here is needed for mypy\n        empty_list: List[str] = []\n        sequence_label_field = SequenceLabelField(empty_list, self.sequence_field.empty_field())\n        sequence_label_field._indexed_labels = empty_list\n        return sequence_label_field\n\n    def __str__(self) -> str:\n        length = self.sequence_field.sequence_length()\n        formatted_labels = """".join(\n            ""\\t\\t"" + labels + ""\\n"" for labels in textwrap.wrap(repr(self.labels), 100)\n        )\n        return (\n            f""SequenceLabelField of length {length} with ""\n            f""labels:\\n {formatted_labels} \\t\\tin namespace: \'{self._label_namespace}\'.""\n        )\n'"
allennlp/data/fields/span_field.py,3,"b'from typing import Dict\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.data.fields.field import Field\nfrom allennlp.data.fields.sequence_field import SequenceField\n\n\nclass SpanField(Field[torch.Tensor]):\n    """"""\n    A `SpanField` is a pair of inclusive, zero-indexed (start, end) indices into a\n    :class:`~allennlp.data.fields.sequence_field.SequenceField`, used to represent a span of text.\n    Because it\'s a pair of indices into a :class:`SequenceField`, we take one of those as input\n    to make the span\'s dependence explicit and to validate that the span is well defined.\n\n    # Parameters\n\n    span_start : `int`, required.\n        The index of the start of the span in the :class:`SequenceField`.\n    span_end : `int`, required.\n        The inclusive index of the end of the span in the :class:`SequenceField`.\n    sequence_field : `SequenceField`, required.\n        A field containing the sequence that this `SpanField` is a span inside.\n    """"""\n\n    __slots__ = [""span_start"", ""span_end"", ""sequence_field""]\n\n    def __init__(self, span_start: int, span_end: int, sequence_field: SequenceField) -> None:\n        self.span_start = span_start\n        self.span_end = span_end\n        self.sequence_field = sequence_field\n\n        if not isinstance(span_start, int) or not isinstance(span_end, int):\n            raise TypeError(\n                f""SpanFields must be passed integer indices. Found span indices: ""\n                f""({span_start}, {span_end}) with types ""\n                f""({type(span_start)} {type(span_end)})""\n            )\n        if span_start > span_end:\n            raise ValueError(\n                f""span_start must be less than span_end, "" f""but found ({span_start}, {span_end}).""\n            )\n\n        if span_end > self.sequence_field.sequence_length() - 1:\n            raise ValueError(\n                f""span_end must be <= len(sequence_length) - 1, but found ""\n                f""{span_end} and {self.sequence_field.sequence_length() - 1} respectively.""\n            )\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n\n        return {}\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:\n\n        tensor = torch.LongTensor([self.span_start, self.span_end])\n        return tensor\n\n    @overrides\n    def empty_field(self):\n        return SpanField(-1, -1, self.sequence_field.empty_field())\n\n    def __str__(self) -> str:\n        return f""SpanField with spans: ({self.span_start}, {self.span_end}).""\n\n    def __eq__(self, other) -> bool:\n        if isinstance(other, tuple) and len(other) == 2:\n            return other == (self.span_start, self.span_end)\n        return super().__eq__(other)\n\n    def __len__(self):\n        return 2\n'"
allennlp/data/fields/text_field.py,3,"b'""""""\nA `TextField` represents a string of text, the kind that you might want to represent with\nstandard word vectors, or pass through an LSTM.\n""""""\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom typing import Dict, List, Optional, Iterator\nimport textwrap\n\nfrom overrides import overrides\nfrom spacy.tokens import Token as SpacyToken\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data.fields.sequence_field import SequenceField\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.nn import util\n\n# There are two levels of dictionaries here: the top level is for the *key*, which aligns\n# TokenIndexers with their corresponding TokenEmbedders.  The bottom level is for the *objects*\n# produced by a given TokenIndexer, which will be input to a particular TokenEmbedder\'s forward()\n# method.  We label these as tensors, because that\'s what they typically are, though they could in\n# reality have arbitrary type.\nTextFieldTensors = Dict[str, Dict[str, torch.Tensor]]\n\n\nclass TextField(SequenceField[TextFieldTensors]):\n    """"""\n    This `Field` represents a list of string tokens.  Before constructing this object, you need\n    to tokenize raw strings using a :class:`~allennlp.data.tokenizers.tokenizer.Tokenizer`.\n\n    Because string tokens can be represented as indexed arrays in a number of ways, we also take a\n    dictionary of :class:`~allennlp.data.token_indexers.token_indexer.TokenIndexer`\n    objects that will be used to convert the tokens into indices.\n    Each `TokenIndexer` could represent each token as a single ID, or a list of character IDs, or\n    something else.\n\n    This field will get converted into a dictionary of arrays, one for each `TokenIndexer`.  A\n    `SingleIdTokenIndexer` produces an array of shape (num_tokens,), while a\n    `TokenCharactersIndexer` produces an array of shape (num_tokens, num_characters).\n    """"""\n\n    __slots__ = [""tokens"", ""_token_indexers"", ""_indexed_tokens""]\n\n    def __init__(self, tokens: List[Token], token_indexers: Dict[str, TokenIndexer]) -> None:\n        self.tokens = tokens\n        self._token_indexers = token_indexers\n        self._indexed_tokens: Optional[Dict[str, IndexedTokenList]] = None\n\n        if not all(isinstance(x, (Token, SpacyToken)) for x in tokens):\n            raise ConfigurationError(\n                ""TextFields must be passed Tokens. ""\n                ""Found: {} with types {}."".format(tokens, [type(x) for x in tokens])\n            )\n\n    @overrides\n    def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        for indexer in self._token_indexers.values():\n            for token in self.tokens:\n                indexer.count_vocab_items(token, counter)\n\n    @overrides\n    def index(self, vocab: Vocabulary):\n        self._indexed_tokens = {}\n        for indexer_name, indexer in self._token_indexers.items():\n            self._indexed_tokens[indexer_name] = indexer.tokens_to_indices(self.tokens, vocab)\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n        """"""\n        The `TextField` has a list of `Tokens`, and each `Token` gets converted into arrays by\n        (potentially) several `TokenIndexers`.  This method gets the max length (over tokens)\n        associated with each of these arrays.\n        """"""\n        if self._indexed_tokens is None:\n            raise ConfigurationError(\n                ""You must call .index(vocabulary) on a field before determining padding lengths.""\n            )\n\n        padding_lengths = {}\n        for indexer_name, indexer in self._token_indexers.items():\n            indexer_lengths = indexer.get_padding_lengths(self._indexed_tokens[indexer_name])\n            for key, length in indexer_lengths.items():\n                padding_lengths[f""{indexer_name}___{key}""] = length\n        return padding_lengths\n\n    @overrides\n    def sequence_length(self) -> int:\n        return len(self.tokens)\n\n    @overrides\n    def as_tensor(self, padding_lengths: Dict[str, int]) -> Dict[str, torch.Tensor]:\n        tensors = {}\n\n        indexer_lengths: Dict[str, Dict[str, int]] = defaultdict(dict)\n        for key, value in padding_lengths.items():\n            # We want this to crash if the split fails. Should never happen, so I\'m not\n            # putting in a check, but if you fail on this line, open a github issue.\n            indexer_name, padding_key = key.split(""___"")\n            indexer_lengths[indexer_name][padding_key] = value\n\n        for indexer_name, indexer in self._token_indexers.items():\n            tensors[indexer_name] = indexer.as_padded_tensor_dict(\n                self._indexed_tokens[indexer_name], indexer_lengths[indexer_name]\n            )\n        return tensors\n\n    @overrides\n    def empty_field(self):\n        text_field = TextField([], self._token_indexers)\n        text_field._indexed_tokens = {}\n        for indexer_name, indexer in self._token_indexers.items():\n            text_field._indexed_tokens[indexer_name] = indexer.get_empty_token_list()\n        return text_field\n\n    @overrides\n    def batch_tensors(self, tensor_list: List[TextFieldTensors]) -> TextFieldTensors:\n        # This is creating a dict of {token_indexer_name: {token_indexer_outputs: batched_tensor}}\n        # for each token indexer used to index this field.\n        indexer_lists: Dict[str, List[Dict[str, torch.Tensor]]] = defaultdict(list)\n        for tensor_dict in tensor_list:\n            for indexer_name, indexer_output in tensor_dict.items():\n                indexer_lists[indexer_name].append(indexer_output)\n        batched_tensors = {\n            # NOTE(mattg): if an indexer has its own nested structure, rather than one tensor per\n            # argument, then this will break.  If that ever happens, we should move this to an\n            # `indexer.batch_tensors` method, with this logic as the default implementation in the\n            # base class.\n            indexer_name: util.batch_tensor_dicts(indexer_outputs)\n            for indexer_name, indexer_outputs in indexer_lists.items()\n        }\n        return batched_tensors\n\n    def __str__(self) -> str:\n        indexers = {\n            name: indexer.__class__.__name__ for name, indexer in self._token_indexers.items()\n        }\n\n        # Double tab to indent under the header.\n        formatted_text = """".join(\n            ""\\t\\t"" + text + ""\\n"" for text in textwrap.wrap(repr(self.tokens), 100)\n        )\n        return (\n            f""TextField of length {self.sequence_length()} with ""\n            f""text: \\n {formatted_text} \\t\\tand TokenIndexers : {indexers}""\n        )\n\n    # Sequence[Token] methods\n    def __iter__(self) -> Iterator[Token]:\n        return iter(self.tokens)\n\n    def __getitem__(self, idx: int) -> Token:\n        return self.tokens[idx]\n\n    def __len__(self) -> int:\n        return len(self.tokens)\n\n    @overrides\n    def duplicate(self):\n        """"""\n        Overrides the behavior of `duplicate` so that `self._token_indexers` won\'t\n        actually be deep-copied.\n\n        Not only would it be extremely inefficient to deep-copy the token indexers,\n        but it also fails in many cases since some tokenizers (like those used in\n        the \'transformers\' lib) cannot actually be deep-copied.\n        """"""\n        new = TextField(deepcopy(self.tokens), {k: v for k, v in self._token_indexers.items()})\n        new._indexed_tokens = deepcopy(self._indexed_tokens)\n        return new\n'"
allennlp/data/samplers/__init__.py,0,"b'from allennlp.data.samplers.samplers import (\n    Sampler,\n    BatchSampler,\n    SequentialSampler,\n    SubsetRandomSampler,\n    WeightedRandomSampler,\n    RandomSampler,\n    BasicBatchSampler,\n)\nfrom allennlp.data.samplers.bucket_batch_sampler import BucketBatchSampler\nfrom allennlp.data.samplers.max_tokens_batch_sampler import MaxTokensBatchSampler\n'"
allennlp/data/samplers/bucket_batch_sampler.py,1,"b'import logging\nfrom typing import List, Iterable, Tuple\nimport random\nimport math\n\nfrom torch.utils import data\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.util import lazy_groups_of\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.samplers import BatchSampler\n\nlogger = logging.getLogger(__name__)\n\n\ndef add_noise_to_value(value: int, noise_param: float):\n    noise_value = value * noise_param\n    noise = random.uniform(-noise_value, noise_value)\n    return value + noise\n\n\n@BatchSampler.register(""bucket"")\nclass BucketBatchSampler(BatchSampler):\n    """"""\n    An sampler which by default, argsorts batches with respect to the maximum input lengths `per\n    batch`. You can provide a list of field names and padding keys (or pass none, in which case they\n    will be inferred) which the dataset will be sorted by before doing this batching, causing inputs\n    with similar length to be batched together, making computation more efficient (as less time is\n    wasted on padded elements of the batch).\n\n    # Parameters\n\n    data_source: `data.Dataset`, required\n        The pytorch `Dataset` of allennlp Instances to bucket.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""batch_sampler"", it gets constructed separately.\n    batch_size : `int`, required\n        The size of each batch of instances yielded when calling the dataloader.\n\n    sorting_keys : `List[str]`, optional\n        To bucket inputs into batches, we want to group the instances by padding length, so that we\n        minimize the amount of padding necessary per batch. In order to do this, we need to know\n        which fields need what type of padding, and in what order.\n\n        Specifying the right keys for this is a bit cryptic, so if this is not given we try to\n        auto-detect the right keys by iterating through a few instances upfront, reading all of the\n        padding keys and seeing which one has the longest length.  We use that one for padding.\n        This should give reasonable results in most cases. Some cases where it might not be the\n        right thing to do are when you have a `ListField[TextField]`, or when you have a really\n        long, constant length `ArrayField`.\n\n        When you need to specify this yourself, you can create an instance from your dataset and\n        call `Instance.get_padding_lengths()` to see a list of all keys used in your data.  You\n        should give one or more of those as the sorting keys here.\n\n    padding_noise : `float`, optional (default=`.1`)\n        When sorting by padding length, we add a bit of noise to the lengths, so that the sorting\n        isn\'t deterministic.  This parameter determines how much noise we add, as a percentage of\n        the actual padding value for each instance.\n\n    drop_last : `bool`, (default = `False`)\n        If `True`, the sampler will drop the last batch if\n        its size would be less than batch_size`.\n\n    """"""\n\n    def __init__(\n        self,\n        data_source: data.Dataset,\n        batch_size: int,\n        sorting_keys: List[str] = None,\n        padding_noise: float = 0.1,\n        drop_last: bool = False,\n    ):\n\n        self.vocab = data_source.vocab\n        self.sorting_keys = sorting_keys\n        self.padding_noise = padding_noise\n        self.batch_size = batch_size\n        self.data_source = data_source\n        self.drop_last = drop_last\n\n    def _argsort_by_padding(\n        self, instances: Iterable[Instance]\n    ) -> Tuple[List[int], List[List[int]]]:\n        """"""\n        Argsorts the instances by their padding lengths, using the keys in\n        `sorting_keys` (in the order in which they are provided). `sorting_keys`\n        is a list of `(field_name, padding_key)` tuples.\n        """"""\n        if not self.sorting_keys:\n            logger.info(""No sorting keys given; trying to guess a good one"")\n            self._guess_sorting_keys(instances)\n            logger.info(f""Using {self.sorting_keys} as the sorting keys"")\n        instances_with_lengths = []\n        for instance in instances:\n            # Make sure instance is indexed before calling .get_padding\n            lengths = []\n            noisy_lengths = []\n            for field_name in self.sorting_keys:\n                if field_name not in instance.fields:\n                    raise ConfigurationError(\n                        f\'Sorting key ""{field_name}"" is not a field in instance. \'\n                        f""Available fields/keys are {list(instance.fields.keys())}.""\n                    )\n                lengths.append(len(instance.fields[field_name]))\n\n                noisy_lengths.append(add_noise_to_value(lengths[-1], self.padding_noise))\n            instances_with_lengths.append((noisy_lengths, lengths, instance))\n        with_indices = [(x, i) for i, x in enumerate(instances_with_lengths)]\n        with_indices.sort(key=lambda x: x[0][0])\n        return (\n            [instance_with_index[-1] for instance_with_index in with_indices],\n            [instance_with_index[0][1] for instance_with_index in with_indices],\n        )\n\n    def __iter__(self) -> Iterable[List[int]]:\n        indices, _ = self._argsort_by_padding(self.data_source)\n        batches = []\n        for group in lazy_groups_of(indices, self.batch_size):\n            batch_indices = list(group)\n            if self.drop_last and len(batch_indices) < self.batch_size:\n                continue\n            batches.append(batch_indices)\n        random.shuffle(batches)\n        for batch in batches:\n            yield batch\n\n    def _guess_sorting_keys(self, instances: Iterable[Instance], num_instances: int = 10) -> None:\n        """"""\n        Use `num_instances` instances from the dataset to infer the keys used\n        for sorting the dataset for bucketing.\n\n        # Parameters\n\n        instances : `Iterable[Instance]`, required.\n            The dataset to guess sorting keys for.\n        num_instances : `int`, optional (default = `10`)\n            The number of instances to use to guess sorting keys. Typically\n            the default value is completely sufficient, but if your instances\n            are not homogeneous, you might need more.\n        """"""\n        max_length = 0.0\n        longest_field: str = None\n        for i, instance in enumerate(instances):\n            instance.index_fields(self.vocab)\n            for field_name, field in instance.fields.items():\n                length = len(field)\n                if length > max_length:\n                    max_length = length\n                    longest_field = field_name\n            if i > num_instances:\n                # Only use num_instances instances to guess the sorting keys.\n                break\n\n        if not longest_field:\n            # This shouldn\'t ever happen (you basically have to have an empty instance list), but\n            # just in case...\n            raise AssertionError(\n                ""Found no field that needed padding; we are surprised you got this error, please ""\n                ""open an issue on github""\n            )\n        self.sorting_keys = [longest_field]\n\n    def __len__(self):\n        batch_count_float = len(self.data_source) / self.batch_size\n        if self.drop_last:\n            return math.floor(batch_count_float)\n        else:\n            return math.ceil(batch_count_float)\n'"
allennlp/data/samplers/max_tokens_batch_sampler.py,1,"b'import logging\nimport random\nfrom typing import List, Iterable, Optional, Iterator, TypeVar\n\nfrom allennlp.data.samplers import BatchSampler, BucketBatchSampler\nfrom torch.utils import data\n\nlogger = logging.getLogger(__name__)\n\n\nA = TypeVar(""A"")\n\n\n@BatchSampler.register(""max_tokens_sampler"")\nclass MaxTokensBatchSampler(BucketBatchSampler):\n    """"""\n    An sampler which by default, argsorts batches with respect to the maximum input lengths `per\n    batch`. Batches are then created such that the number of tokens in a batch does not exceed the given\n    maximum number of tokens. You can provide a list of field names and padding keys (or pass none, in which case\n    they will be inferred) which the dataset will be sorted by before doing this batching, causing inputs\n    with similar length to be batched together, making computation more efficient (as less time is\n    wasted on padded elements of the batch).\n\n    # Parameters\n\n    data_source: `data.Dataset`\n        The pytorch `Dataset` of allennlp Instances to bucket.\n\n    max_tokens : `int`\n        The maximum number of tokens to include in a batch.\n\n    sorting_keys : `List[str]`, optional\n        To bucket inputs into batches, we want to group the instances by padding length, so that we\n        minimize the amount of padding necessary per batch. In order to do this, we need to know\n        which fields need what type of padding, and in what order.\n\n        Specifying the right keys for this is a bit cryptic, so if this is not given we try to\n        auto-detect the right keys by iterating through a few instances upfront, reading all of the\n        padding keys and seeing which one has the longest length.  We use that one for padding.\n        This should give reasonable results in most cases. Some cases where it might not be the\n        right thing to do are when you have a `ListField[TextField]`, or when you have a really\n        long, constant length `ArrayField`.\n\n        When you need to specify this yourself, you can create an instance from your dataset and\n        call `Instance.get_padding_lengths()` to see a list of all keys used in your data.  You\n        should give one or more of those as the sorting keys here.\n\n    padding_noise : `float`, optional (default = `0.1`)\n        When sorting by padding length, we add a bit of noise to the lengths, so that the sorting\n        isn\'t deterministic.  This parameter determines how much noise we add, as a percentage of\n        the actual padding value for each instance.\n    """"""\n\n    def __init__(\n        self,\n        data_source: data.Dataset,\n        max_tokens: Optional[int] = None,\n        sorting_keys: List[str] = None,\n        padding_noise: float = 0.1,\n    ):\n        super().__init__(data_source, -1, sorting_keys, padding_noise, False)\n\n        self.max_tokens = max_tokens\n\n    def _lazy_groups_of_max_size(\n        self, iterable: Iterable[A], sizes: Iterable[int],\n    ) -> Iterator[List[A]]:\n        """"""\n        Takes an `iterable` of data and an iterable `sizes` of the same length which represents the sizes of each\n        corresponding item in `iterable`. The instances from `iterable` are batched such that the total size\n        of the batch as computed from `sizes` does not exceed `max_size`.\n        """"""\n        cur_max_size = 0\n        group: List[A] = []\n\n        iterator = iter(iterable)\n        size_iter = iter(sizes)\n\n        for item, size in zip(iterator, size_iter):\n            if size > self.max_tokens:\n                logger.warning(\n                    ""Found instance of size %d, which is bigger than the expected size for a batch (%d)"",\n                    size,\n                    self.max_tokens,\n                )\n            group_size = max(size, cur_max_size) * (len(group) + 1)\n\n            if group_size > self.max_tokens:\n                yield group\n                cur_max_size = 0\n                group = []\n\n            group.append(item)\n            cur_max_size = max(cur_max_size, size)\n\n        if len(group) != 0:\n            yield group\n\n    def __iter__(self) -> Iterable[List[int]]:\n        indices, lengths = self._argsort_by_padding(self.data_source)\n\n        max_lengths = [max(length) for length in lengths]\n        group_iterator = self._lazy_groups_of_max_size(indices, max_lengths)\n\n        batches = [list(group) for group in group_iterator]\n        random.shuffle(batches)\n        for batch in batches:\n            yield batch\n\n    def __len__(self):\n        # There is no easy way to count the number of batches, so we need to iterate and count.\n        return sum(1 for _ in self)\n'"
allennlp/data/samplers/samplers.py,8,"b'from typing import List, Iterable\nfrom torch.utils import data\n\nfrom allennlp.common.registrable import Registrable\n\n""""""\nDuplicates of the pytorch Sampler classes. Broadly, these only exist\nso that we can add type hints, meaning we can construct them from configuration\nfiles. You can use these directly from Python code, but they are identical to the\npytorch ones.\n""""""\n\n\nclass Sampler(Registrable):\n    """"""\n    A copy of the pytorch [Sampler](https://pytorch.org/docs/stable/_modules/torch/utils/data/sampler.html)\n    which allows us to register it with `Registrable.`\n    """"""\n\n    def __iter__(self) -> Iterable[int]:\n\n        raise NotImplementedError\n\n\nclass BatchSampler(Registrable):\n    """"""\n    A copy of the pytorch\n    [BatchSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.BatchSampler)\n    which allows us to register it with `Registrable.`\n    """"""\n\n    def __iter__(self) -> Iterable[List[int]]:\n\n        raise NotImplementedError\n\n\n@Sampler.register(""sequential"")\nclass SequentialSampler(data.SequentialSampler, Sampler):\n    """"""\n    A registrable version of pytorch\'s\n    [SequentialSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.SequentialSampler).\n\n    Registered as a `Sampler` with name ""sequential"".\n\n    In a typical AllenNLP configuration file, `data_source` parameter does not get an entry under\n    the ""sampler"", it gets constructed separately.\n    """"""\n\n    def __init__(self, data_source: data.Dataset):\n        super().__init__(data_source)\n\n\n@Sampler.register(""random"")\nclass RandomSampler(data.RandomSampler, Sampler):\n    """"""\n    A registrable version of pytorch\'s\n    [RandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler).\n    Samples elements randomly. If without replacement, then sample from a shuffled dataset.\n    If with replacement, then user can specify `num_samples` to draw.\n\n    Registered as a `Sampler` with name ""random"".\n\n    # Parameters\n    data_source: `Dataset`, required\n        The dataset to sample from.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""sampler"", it gets constructed separately.\n    replacement : `bool`, optional (default = `False`)\n        Samples are drawn with replacement if `True`.\n    num_samples: `int` (default = `len(dataset)`)\n        The number of samples to draw. This argument\n        is supposed to be specified only when `replacement` is ``True``.\n    """"""\n\n    def __init__(\n        self, data_source: data.Dataset, replacement: bool = False, num_samples: int = None\n    ):\n        super().__init__(data_source, replacement, num_samples)\n\n\n@Sampler.register(""subset_random"")\nclass SubsetRandomSampler(data.SubsetRandomSampler, Sampler):\n    """"""\n    A registrable version of pytorch\'s\n    [SubsetRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.SubsetRandomSampler).\n    Samples elements randomly from a given list of indices, without replacement.\n\n    Registered as a `Sampler` with name ""subset_random"".\n\n    # Parameters\n    indices: `List[int]`\n        a sequence of indices to sample from.\n    """"""\n\n    def __init__(self, indices: List[int]):\n        super().__init__(indices)\n\n\n@Sampler.register(""weighted_random"")\nclass WeightedRandomSampler(data.WeightedRandomSampler, Sampler):\n    """"""\n    A registrable version of pytorch\'s\n    [WeightedRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler).\n    Samples elements from `[0,...,len(weights)-1]` with given probabilities (weights).\n\n    Registered as a `Sampler` with name ""weighted_random"".\n\n    # Parameters:\n    weights : `List[float]`\n        A sequence of weights, not necessary summing up to one.\n    num_samples : `int`\n        The number of samples to draw.\n    replacement : `bool`\n        If ``True``, samples are drawn with replacement.\n        If not, they are drawn without replacement, which means that when a\n        sample index is drawn for a row, it cannot be drawn again for that row.\n\n    # Examples\n\n    ```python\n    >>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n    [0, 0, 0, 1, 0]\n    >>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n    [0, 1, 4, 3, 2]\n    ```\n    """"""\n\n    def __init__(self, weights: List[float], num_samples: int, replacement: bool = True):\n        super().__init__(weights, num_samples, replacement)\n\n\n@BatchSampler.register(""basic"")\nclass BasicBatchSampler(data.BatchSampler, BatchSampler):\n    """"""\n    A registrable version of pytorch\'s\n    [BatchSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.BatchSampler).\n    Wraps another sampler to yield a mini-batch of indices.\n\n    Registered as a `BatchSampler` with name ""basic"".\n\n    # Parameters\n    sampler: `Sampler`\n        The base sampler.\n    batch_size : `int`\n        The size of the batch.\n    drop_last : `bool`\n        If `True`, the sampler will drop the last batch if\n        its size would be less than batch_size`.\n\n    # Examples\n\n    ```python\n    >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n    [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n    >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n    [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    ```\n    """"""\n\n    def __init__(self, sampler: Sampler, batch_size: int, drop_last: bool):\n        super().__init__(sampler, batch_size, drop_last)\n'"
allennlp/data/token_indexers/__init__.py,0,"b'""""""\nA `TokenIndexer` determines how string tokens get represented as arrays of indices in a model.\n""""""\n\nfrom allennlp.data.token_indexers.single_id_token_indexer import SingleIdTokenIndexer\nfrom allennlp.data.token_indexers.token_characters_indexer import TokenCharactersIndexer\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer\nfrom allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\nfrom allennlp.data.token_indexers.spacy_indexer import SpacyTokenIndexer\nfrom allennlp.data.token_indexers.pretrained_transformer_indexer import PretrainedTransformerIndexer\nfrom allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer import (\n    PretrainedTransformerMismatchedIndexer,\n)\n'"
allennlp/data/token_indexers/elmo_indexer.py,2,"b'from typing import Dict, List\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.util import pad_sequence_to_length\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList\nfrom allennlp.data.vocabulary import Vocabulary\n\n\ndef _make_bos_eos(\n    character: int,\n    padding_character: int,\n    beginning_of_word_character: int,\n    end_of_word_character: int,\n    max_word_length: int,\n):\n    char_ids = [padding_character] * max_word_length\n    char_ids[0] = beginning_of_word_character\n    char_ids[1] = character\n    char_ids[2] = end_of_word_character\n    return char_ids\n\n\nclass ELMoCharacterMapper:\n    """"""\n    Maps individual tokens to sequences of character ids, compatible with ELMo.\n    To be consistent with previously trained models, we include it here as special of existing\n    character indexers.\n\n    We allow to add optional additional special tokens with designated\n    character ids with `tokens_to_add`.\n    """"""\n\n    max_word_length = 50\n\n    # char ids 0-255 come from utf-8 encoding bytes\n    # assign 256-300 to special chars\n    beginning_of_sentence_character = 256  # <begin sentence>\n    end_of_sentence_character = 257  # <end sentence>\n    beginning_of_word_character = 258  # <begin word>\n    end_of_word_character = 259  # <end word>\n    padding_character = 260  # <padding>\n\n    beginning_of_sentence_characters = _make_bos_eos(\n        beginning_of_sentence_character,\n        padding_character,\n        beginning_of_word_character,\n        end_of_word_character,\n        max_word_length,\n    )\n    end_of_sentence_characters = _make_bos_eos(\n        end_of_sentence_character,\n        padding_character,\n        beginning_of_word_character,\n        end_of_word_character,\n        max_word_length,\n    )\n\n    bos_token = ""<S>""\n    eos_token = ""</S>""\n\n    def __init__(self, tokens_to_add: Dict[str, int] = None) -> None:\n        self.tokens_to_add = tokens_to_add or {}\n\n    def convert_word_to_char_ids(self, word: str) -> List[int]:\n        if word in self.tokens_to_add:\n            char_ids = [ELMoCharacterMapper.padding_character] * ELMoCharacterMapper.max_word_length\n            char_ids[0] = ELMoCharacterMapper.beginning_of_word_character\n            char_ids[1] = self.tokens_to_add[word]\n            char_ids[2] = ELMoCharacterMapper.end_of_word_character\n        elif word == ELMoCharacterMapper.bos_token:\n            char_ids = ELMoCharacterMapper.beginning_of_sentence_characters\n        elif word == ELMoCharacterMapper.eos_token:\n            char_ids = ELMoCharacterMapper.end_of_sentence_characters\n        else:\n            word_encoded = word.encode(""utf-8"", ""ignore"")[\n                : (ELMoCharacterMapper.max_word_length - 2)\n            ]\n            char_ids = [ELMoCharacterMapper.padding_character] * ELMoCharacterMapper.max_word_length\n            char_ids[0] = ELMoCharacterMapper.beginning_of_word_character\n            for k, chr_id in enumerate(word_encoded, start=1):\n                char_ids[k] = chr_id\n            char_ids[len(word_encoded) + 1] = ELMoCharacterMapper.end_of_word_character\n\n        # +1 one for masking\n        return [c + 1 for c in char_ids]\n\n    def __eq__(self, other) -> bool:\n        if isinstance(self, other.__class__):\n            return self.__dict__ == other.__dict__\n        return NotImplemented\n\n\n@TokenIndexer.register(""elmo_characters"")\nclass ELMoTokenCharactersIndexer(TokenIndexer):\n    """"""\n    Convert a token to an array of character ids to compute ELMo representations.\n\n    Registered as a `TokenIndexer` with name ""elmo_characters"".\n\n    # Parameters\n\n    namespace : `str`, optional (default=`elmo_characters`)\n    tokens_to_add : `Dict[str, int]`, optional (default=`None`)\n        If not None, then provides a mapping of special tokens to character\n        ids. When using pre-trained models, then the character id must be\n        less then 261, and we recommend using un-used ids (e.g. 1-32).\n    token_min_padding_length : `int`, optional (default=`0`)\n        See :class:`TokenIndexer`.\n    """"""\n\n    def __init__(\n        self,\n        namespace: str = ""elmo_characters"",\n        tokens_to_add: Dict[str, int] = None,\n        token_min_padding_length: int = 0,\n    ) -> None:\n        super().__init__(token_min_padding_length)\n        self._namespace = namespace\n        self._mapper = ELMoCharacterMapper(tokens_to_add)\n\n    @overrides\n    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):\n        pass\n\n    @overrides\n    def get_empty_token_list(self) -> IndexedTokenList:\n        return {""elmo_tokens"": []}\n\n    @overrides\n    def tokens_to_indices(\n        self, tokens: List[Token], vocabulary: Vocabulary\n    ) -> Dict[str, List[List[int]]]:\n        # TODO(brendanr): Retain the token to index mappings in the vocabulary and remove this\n\n        # https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/wordpiece_indexer.py#L113\n\n        texts = [token.text for token in tokens]\n\n        if any(text is None for text in texts):\n            raise ConfigurationError(\n                ""ELMoTokenCharactersIndexer needs a tokenizer that retains text""\n            )\n        return {""elmo_tokens"": [self._mapper.convert_word_to_char_ids(text) for text in texts]}\n\n    @overrides\n    def as_padded_tensor_dict(\n        self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]\n    ) -> Dict[str, torch.Tensor]:\n        # Overriding this method only because we need a different padding token than the default.\n        tensor_dict = {}\n\n        def padding_token():\n            return [0] * ELMoCharacterMapper.max_word_length\n\n        tensor_dict[""elmo_tokens""] = torch.LongTensor(\n            pad_sequence_to_length(\n                tokens[""elmo_tokens""], padding_lengths[""elmo_tokens""], default_value=padding_token\n            )\n        )\n        return tensor_dict\n'"
allennlp/data/token_indexers/pretrained_transformer_indexer.py,5,"b'from typing import Dict, List, Optional, Tuple\nimport logging\nimport torch\nfrom allennlp.common.util import pad_sequence_to_length\n\nfrom overrides import overrides\n\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.data.tokenizers import PretrainedTransformerTokenizer\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList\n\nlogger = logging.getLogger(__name__)\n\n\n@TokenIndexer.register(""pretrained_transformer"")\nclass PretrainedTransformerIndexer(TokenIndexer):\n    """"""\n    This `TokenIndexer` assumes that Tokens already have their indexes in them (see `text_id` field).\n    We still require `model_name` because we want to form allennlp vocabulary from pretrained one.\n    This `Indexer` is only really appropriate to use if you\'ve also used a\n    corresponding :class:`PretrainedTransformerTokenizer` to tokenize your input.  Otherwise you\'ll\n    have a mismatch between your tokens and your vocabulary, and you\'ll get a lot of UNK tokens.\n\n    Registered as a `TokenIndexer` with name ""pretrained_transformer"".\n\n    # Parameters\n\n    model_name : `str`\n        The name of the `transformers` model to use.\n    namespace : `str`, optional (default=`tags`)\n        We will add the tokens in the pytorch_transformer vocabulary to this vocabulary namespace.\n        We use a somewhat confusing default value of `tags` so that we do not add padding or UNK\n        tokens to this namespace, which would break on loading because we wouldn\'t find our default\n        OOV token.\n    max_length : `int`, optional (default = `None`)\n        If not None, split the document into segments of this many tokens (including special tokens)\n        before feeding into the embedder. The embedder embeds these segments independently and\n        concatenate the results to get the original document representation. Should be set to\n        the same value as the `max_length` option on the `PretrainedTransformerEmbedder`.\n    """"""\n\n    def __init__(\n        self, model_name: str, namespace: str = ""tags"", max_length: int = None, **kwargs\n    ) -> None:\n        super().__init__(**kwargs)\n        self._namespace = namespace\n        self._allennlp_tokenizer = PretrainedTransformerTokenizer(model_name)\n        self._tokenizer = self._allennlp_tokenizer.tokenizer\n        self._added_to_vocabulary = False\n\n        self._num_added_start_tokens = len(self._allennlp_tokenizer.single_sequence_start_tokens)\n        self._num_added_end_tokens = len(self._allennlp_tokenizer.single_sequence_end_tokens)\n\n        self._max_length = max_length\n        if self._max_length is not None:\n            num_added_tokens = len(self._allennlp_tokenizer.tokenize(""a"")) - 1\n            self._effective_max_length = (  # we need to take into account special tokens\n                self._max_length - num_added_tokens\n            )\n            if self._effective_max_length <= 0:\n                raise ValueError(\n                    ""max_length needs to be greater than the number of special tokens inserted.""\n                )\n\n    def _add_encoding_to_vocabulary_if_needed(self, vocab: Vocabulary) -> None:\n        """"""\n        Copies tokens from ```transformers``` model\'s vocab to the specified namespace.\n        """"""\n        if self._added_to_vocabulary:\n            return\n\n        try:\n            vocab_items = self._tokenizer.get_vocab().items()\n        except NotImplementedError:\n            vocab_items = (\n                (self._tokenizer.convert_ids_to_tokens(idx), idx)\n                for idx in range(self._tokenizer.vocab_size)\n            )\n        for word, idx in vocab_items:\n            vocab._token_to_index[self._namespace][word] = idx\n            vocab._index_to_token[self._namespace][idx] = word\n\n        self._added_to_vocabulary = True\n\n    @overrides\n    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):\n        # If we only use pretrained models, we don\'t need to do anything here.\n        pass\n\n    @overrides\n    def tokens_to_indices(self, tokens: List[Token], vocabulary: Vocabulary) -> IndexedTokenList:\n        self._add_encoding_to_vocabulary_if_needed(vocabulary)\n\n        indices, type_ids = self._extract_token_and_type_ids(tokens)\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n        output: IndexedTokenList = {\n            ""token_ids"": indices,\n            ""mask"": [True] * len(indices),\n            ""type_ids"": type_ids,\n        }\n\n        return self._postprocess_output(output)\n\n    @overrides\n    def indices_to_tokens(\n        self, indexed_tokens: IndexedTokenList, vocabulary: Vocabulary\n    ) -> List[Token]:\n        token_ids = indexed_tokens[""token_ids""]\n        type_ids = indexed_tokens.get(""type_ids"")\n\n        return [\n            Token(\n                text=vocabulary.get_token_from_index(token_ids[i], self._namespace),\n                text_id=token_ids[i],\n                type_id=type_ids[i] if type_ids is not None else None,\n            )\n            for i in range(len(token_ids))\n        ]\n\n    def _extract_token_and_type_ids(\n        self, tokens: List[Token]\n    ) -> Tuple[List[int], Optional[List[int]]]:\n        """"""\n        Roughly equivalent to `zip(*[(token.text_id, token.type_id) for token in tokens])`,\n        with some checks.\n        """"""\n        indices: List[int] = []\n        type_ids: List[int] = []\n        for token in tokens:\n            if getattr(token, ""text_id"", None) is not None:\n                # `text_id` being set on the token means that we aren\'t using the vocab, we just use\n                # this id instead. Id comes from the pretrained vocab.\n                # It is computed in PretrainedTransformerTokenizer.\n                indices.append(token.text_id)\n            else:\n                raise KeyError(\n                    ""Using PretrainedTransformerIndexer but field text_id is not set""\n                    f"" for the following token: {token.text}""\n                )\n\n            if type_ids is not None and getattr(token, ""type_id"", None) is not None:\n                type_ids.append(token.type_id)\n            else:\n                type_ids.append(0)\n\n        return indices, type_ids\n\n    def _postprocess_output(self, output: IndexedTokenList) -> IndexedTokenList:\n        """"""\n        Takes an IndexedTokenList about to be returned by `tokens_to_indices()` and adds any\n        necessary postprocessing, e.g. long sequence splitting.\n\n        The input should have a `""token_ids""` key corresponding to the token indices. They should\n        have special tokens already inserted.\n        """"""\n        if self._max_length is not None:\n            # We prepare long indices by converting them to (assuming max_length == 5)\n            # [CLS] A B C [SEP] [CLS] D E F [SEP] ...\n            # Embedder is responsible for folding this 1-d sequence to 2-d and feed to the\n            # transformer model.\n            # TODO(zhaofengw): we aren\'t respecting word boundaries when segmenting wordpieces.\n\n            indices = output[""token_ids""]\n            # Strips original special tokens\n            indices = indices[self._num_added_start_tokens : -self._num_added_end_tokens]\n            # Folds indices\n            folded_indices = [\n                indices[i : i + self._effective_max_length]\n                for i in range(0, len(indices), self._effective_max_length)\n            ]\n            # Adds special tokens to each segment\n            folded_indices = [\n                self._tokenizer.build_inputs_with_special_tokens(segment)\n                for segment in folded_indices\n            ]\n            # Flattens\n            indices = [i for segment in folded_indices for i in segment]\n\n            output[""token_ids""] = indices\n            output[""type_ids""] = [0] * len(indices)\n            output[""segment_concat_mask""] = [True] * len(indices)\n\n        return output\n\n    @overrides\n    def get_empty_token_list(self) -> IndexedTokenList:\n        output: IndexedTokenList = {""token_ids"": [], ""mask"": [], ""type_ids"": []}\n        if self._max_length is not None:\n            output[""segment_concat_mask""] = []\n        return output\n\n    @overrides\n    def as_padded_tensor_dict(\n        self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]\n    ) -> Dict[str, torch.Tensor]:\n        tensor_dict = {}\n        for key, val in tokens.items():\n            if key == ""type_ids"":\n                padding_value = 0\n                mktensor = torch.LongTensor\n            elif key == ""mask"" or key == ""wordpiece_mask"":\n                padding_value = False\n                mktensor = torch.BoolTensor\n            elif len(val) > 0 and isinstance(val[0], bool):\n                padding_value = False\n                mktensor = torch.BoolTensor\n            else:\n                padding_value = self._tokenizer.pad_token_id\n                if padding_value is None:\n                    padding_value = (\n                        0  # Some tokenizers don\'t have padding tokens and rely on the mask only.\n                    )\n                mktensor = torch.LongTensor\n\n            tensor = mktensor(\n                pad_sequence_to_length(\n                    val, padding_lengths[key], default_value=lambda: padding_value\n                )\n            )\n\n            tensor_dict[key] = tensor\n        return tensor_dict\n\n    def __eq__(self, other):\n        if isinstance(other, PretrainedTransformerIndexer):\n            for key in self.__dict__:\n                if key == ""_tokenizer"":\n                    # This is a reference to a function in the huggingface code, which we can\'t\n                    # really modify to make this clean.  So we special-case it.\n                    continue\n                if self.__dict__[key] != other.__dict__[key]:\n                    return False\n            return True\n        return NotImplemented\n'"
allennlp/data/token_indexers/pretrained_transformer_mismatched_indexer.py,2,"b'from typing import Dict, List\nimport logging\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.common.util import pad_sequence_to_length\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.token_indexers import PretrainedTransformerIndexer, TokenIndexer\nfrom allennlp.data.token_indexers.token_indexer import IndexedTokenList\n\nlogger = logging.getLogger(__name__)\n\n\n@TokenIndexer.register(""pretrained_transformer_mismatched"")\nclass PretrainedTransformerMismatchedIndexer(TokenIndexer):\n    """"""\n    Use this indexer when (for whatever reason) you are not using a corresponding\n    `PretrainedTransformerTokenizer` on your input. We assume that you used a tokenizer that splits\n    strings into words, while the transformer expects wordpieces as input. This indexer splits the\n    words into wordpieces and flattens them out. You should use the corresponding\n    `PretrainedTransformerMismatchedEmbedder` to embed these wordpieces and then pull out a single\n    vector for each original word.\n\n    Registered as a `TokenIndexer` with name ""pretrained_transformer_mismatched"".\n\n    # Parameters\n\n    model_name : `str`\n        The name of the `transformers` model to use.\n    namespace : `str`, optional (default=`tags`)\n        We will add the tokens in the pytorch_transformer vocabulary to this vocabulary namespace.\n        We use a somewhat confusing default value of `tags` so that we do not add padding or UNK\n        tokens to this namespace, which would break on loading because we wouldn\'t find our default\n        OOV token.\n    max_length : `int`, optional (default = `None`)\n        If positive, split the document into segments of this many tokens (including special tokens)\n        before feeding into the embedder. The embedder embeds these segments independently and\n        concatenate the results to get the original document representation. Should be set to\n        the same value as the `max_length` option on the `PretrainedTransformerMismatchedEmbedder`.\n    """"""\n\n    def __init__(\n        self, model_name: str, namespace: str = ""tags"", max_length: int = None, **kwargs\n    ) -> None:\n        super().__init__(**kwargs)\n        # The matched version v.s. mismatched\n        self._matched_indexer = PretrainedTransformerIndexer(\n            model_name, namespace, max_length, **kwargs\n        )\n        self._allennlp_tokenizer = self._matched_indexer._allennlp_tokenizer\n        self._tokenizer = self._matched_indexer._tokenizer\n        self._num_added_start_tokens = self._matched_indexer._num_added_start_tokens\n        self._num_added_end_tokens = self._matched_indexer._num_added_end_tokens\n\n    @overrides\n    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):\n        return self._matched_indexer.count_vocab_items(token, counter)\n\n    @overrides\n    def tokens_to_indices(self, tokens: List[Token], vocabulary: Vocabulary) -> IndexedTokenList:\n        self._matched_indexer._add_encoding_to_vocabulary_if_needed(vocabulary)\n\n        wordpieces, offsets = self._allennlp_tokenizer.intra_word_tokenize([t.text for t in tokens])\n\n        # For tokens that don\'t correspond to any word pieces, we put (-1, -1) into the offsets.\n        # That results in the embedding for the token to be all zeros.\n        offsets = [x if x is not None else (-1, -1) for x in offsets]\n\n        output: IndexedTokenList = {\n            ""token_ids"": [t.text_id for t in wordpieces],\n            ""mask"": [True] * len(tokens),  # for original tokens (i.e. word-level)\n            ""type_ids"": [t.type_id for t in wordpieces],\n            ""offsets"": offsets,\n            ""wordpiece_mask"": [True] * len(wordpieces),  # for wordpieces (i.e. subword-level)\n        }\n\n        return self._matched_indexer._postprocess_output(output)\n\n    @overrides\n    def get_empty_token_list(self) -> IndexedTokenList:\n        output = self._matched_indexer.get_empty_token_list()\n        output[""offsets""] = []\n        output[""wordpiece_mask""] = []\n        return output\n\n    @overrides\n    def as_padded_tensor_dict(\n        self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]\n    ) -> Dict[str, torch.Tensor]:\n        tokens = tokens.copy()\n        padding_lengths = padding_lengths.copy()\n\n        offsets_tokens = tokens.pop(""offsets"")\n        offsets_padding_lengths = padding_lengths.pop(""offsets"")\n\n        tensor_dict = self._matched_indexer.as_padded_tensor_dict(tokens, padding_lengths)\n        tensor_dict[""offsets""] = torch.LongTensor(\n            pad_sequence_to_length(\n                offsets_tokens, offsets_padding_lengths, default_value=lambda: (0, 0)\n            )\n        )\n        return tensor_dict\n\n    def __eq__(self, other):\n        if isinstance(other, PretrainedTransformerMismatchedIndexer):\n            for key in self.__dict__:\n                if key == ""_tokenizer"":\n                    # This is a reference to a function in the huggingface code, which we can\'t\n                    # really modify to make this clean.  So we special-case it.\n                    continue\n                if self.__dict__[key] != other.__dict__[key]:\n                    return False\n            return True\n        return NotImplemented\n'"
allennlp/data/token_indexers/single_id_token_indexer.py,0,"b'from typing import Dict, List, Optional\nimport itertools\n\nfrom overrides import overrides\n\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList\n\n\n_DEFAULT_VALUE = ""THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING""\n\n\n@TokenIndexer.register(""single_id"")\nclass SingleIdTokenIndexer(TokenIndexer):\n    """"""\n    This :class:`TokenIndexer` represents tokens as single integers.\n\n    Registered as a `TokenIndexer` with name ""single_id"".\n\n    # Parameters\n\n    namespace : `Optional[str]`, optional (default=`""tokens""`)\n        We will use this namespace in the :class:`Vocabulary` to map strings to indices.  If you\n        explicitly pass in `None` here, we will skip indexing and vocabulary lookups.  This means\n        that the `feature_name` you use must correspond to an integer value (like `text_id`, for\n        instance, which gets set by some tokenizers, such as when using byte encoding).\n    lowercase_tokens : `bool`, optional (default=`False`)\n        If `True`, we will call `token.lower()` before getting an index for the token from the\n        vocabulary.\n    start_tokens : `List[str]`, optional (default=`None`)\n        These are prepended to the tokens provided to `tokens_to_indices`.\n    end_tokens : `List[str]`, optional (default=`None`)\n        These are appended to the tokens provided to `tokens_to_indices`.\n    feature_name : `str`, optional (default=`""text""`)\n        We will use the :class:`Token` attribute with this name as input.  This is potentially\n        useful, e.g., for using NER tags instead of (or in addition to) surface forms as your inputs\n        (passing `ent_type_` here would do that).  If you use a non-default value here, you almost\n        certainly want to also change the `namespace` parameter, and you might want to give a\n        `default_value`.\n    default_value : `str`, optional\n        When you want to use a non-default `feature_name`, you sometimes want to have a default\n        value to go with it, e.g., in case you don\'t have an NER tag for a particular token, for\n        some reason.  This value will get used if we don\'t find a value in `feature_name`.  If this\n        is not given, we will crash if a token doesn\'t have a value for the given `feature_name`, so\n        that you don\'t get weird, silent errors by default.\n    token_min_padding_length : `int`, optional (default=`0`)\n        See :class:`TokenIndexer`.\n    """"""\n\n    def __init__(\n        self,\n        namespace: Optional[str] = ""tokens"",\n        lowercase_tokens: bool = False,\n        start_tokens: List[str] = None,\n        end_tokens: List[str] = None,\n        feature_name: str = ""text"",\n        default_value: str = _DEFAULT_VALUE,\n        token_min_padding_length: int = 0,\n    ) -> None:\n        super().__init__(token_min_padding_length)\n        self.namespace = namespace\n        self.lowercase_tokens = lowercase_tokens\n\n        self._start_tokens = [Token(st) for st in (start_tokens or [])]\n        self._end_tokens = [Token(et) for et in (end_tokens or [])]\n        self._feature_name = feature_name\n        self._default_value = default_value\n\n    @overrides\n    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):\n        if self.namespace is not None:\n            text = self._get_feature_value(token)\n            if self.lowercase_tokens:\n                text = text.lower()\n            counter[self.namespace][text] += 1\n\n    @overrides\n    def tokens_to_indices(\n        self, tokens: List[Token], vocabulary: Vocabulary\n    ) -> Dict[str, List[int]]:\n        indices: List[int] = []\n\n        for token in itertools.chain(self._start_tokens, tokens, self._end_tokens):\n            text = self._get_feature_value(token)\n            if self.namespace is None:\n                # We could have a check here that `text` is an int; not sure it\'s worth it.\n                indices.append(text)  # type: ignore\n            else:\n                if self.lowercase_tokens:\n                    text = text.lower()\n                indices.append(vocabulary.get_token_index(text, self.namespace))\n\n        return {""tokens"": indices}\n\n    @overrides\n    def get_empty_token_list(self) -> IndexedTokenList:\n        return {""tokens"": []}\n\n    def _get_feature_value(self, token: Token) -> str:\n        text = getattr(token, self._feature_name)\n        if text is None:\n            if self._default_value is not _DEFAULT_VALUE:\n                text = self._default_value\n            else:\n                raise ValueError(\n                    f""{token} did not have attribute {self._feature_name}. If you ""\n                    ""want to ignore this kind of error, give a default value in the ""\n                    ""constructor of this indexer.""\n                )\n        return text\n'"
allennlp/data/token_indexers/spacy_indexer.py,2,"b'from typing import Dict, List\n\nfrom overrides import overrides\nfrom spacy.tokens import Token as SpacyToken\nimport torch\nimport numpy\n\nfrom allennlp.common.util import pad_sequence_to_length\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList\n\n\n@TokenIndexer.register(""spacy"")\nclass SpacyTokenIndexer(TokenIndexer):\n    """"""\n    This :class:`SpacyTokenIndexer` represents tokens as word vectors\n    from a spacy model. You might want to do this for two main reasons;\n    easier integration with a spacy pipeline and no out of vocabulary\n    tokens.\n\n    Registered as a `TokenIndexer` with name ""spacy"".\n\n    # Parameters\n\n    hidden_dim : `int`, optional (default=`96`)\n        The dimension of the vectors that spacy generates for\n        representing words.\n    token_min_padding_length : `int`, optional (default=`0`)\n        See :class:`TokenIndexer`.\n    """"""\n\n    def __init__(self, hidden_dim: int = 96, token_min_padding_length: int = 0) -> None:\n        self._hidden_dim = hidden_dim\n        super().__init__(token_min_padding_length)\n\n    @overrides\n    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):\n        # We are using spacy to generate embeddings directly for our model,\n        # so we don\'t need to capture the vocab - it is defined by the spacy\n        # model we are using instead.\n        pass\n\n    @overrides\n    def tokens_to_indices(\n        self, tokens: List[SpacyToken], vocabulary: Vocabulary\n    ) -> Dict[str, List[numpy.ndarray]]:\n        if not all(isinstance(x, SpacyToken) for x in tokens):\n            raise ValueError(\n                ""The spacy indexer requires you to use a Tokenizer which produces SpacyTokens.""\n            )\n        indices: List[numpy.ndarray] = [token.vector for token in tokens]\n        return {""tokens"": indices}\n\n    @overrides\n    def as_padded_tensor_dict(\n        self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]\n    ) -> Dict[str, torch.Tensor]:\n        def padding_token():\n            return numpy.zeros(self._hidden_dim, dtype=numpy.float32)\n\n        tensor = torch.FloatTensor(\n            pad_sequence_to_length(\n                tokens[""tokens""], padding_lengths[""tokens""], default_value=padding_token\n            )\n        )\n        return {""tokens"": tensor}\n'"
allennlp/data/token_indexers/token_characters_indexer.py,2,"b'from typing import Dict, List\nimport itertools\nimport warnings\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.util import pad_sequence_to_length\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList\nfrom allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.vocabulary import Vocabulary\n\n\n@TokenIndexer.register(""characters"")\nclass TokenCharactersIndexer(TokenIndexer):\n    """"""\n    This :class:`TokenIndexer` represents tokens as lists of character indices.\n\n    Registered as a `TokenIndexer` with name ""characters"".\n\n    # Parameters\n\n    namespace : `str`, optional (default=`token_characters`)\n        We will use this namespace in the :class:`Vocabulary` to map the characters in each token\n        to indices.\n    character_tokenizer : `CharacterTokenizer`, optional (default=`CharacterTokenizer()`)\n        We use a :class:`CharacterTokenizer` to handle splitting tokens into characters, as it has\n        options for byte encoding and other things.  The default here is to instantiate a\n        `CharacterTokenizer` with its default parameters, which uses unicode characters and\n        retains casing.\n    start_tokens : `List[str]`, optional (default=`None`)\n        These are prepended to the tokens provided to `tokens_to_indices`.\n    end_tokens : `List[str]`, optional (default=`None`)\n        These are appended to the tokens provided to `tokens_to_indices`.\n    min_padding_length : `int`, optional (default=`0`)\n        We use this value as the minimum length of padding. Usually used with :class:`CnnEncoder`, its\n        value should be set to the maximum value of `ngram_filter_sizes` correspondingly.\n    token_min_padding_length : `int`, optional (default=`0`)\n        See :class:`TokenIndexer`.\n    """"""\n\n    def __init__(\n        self,\n        namespace: str = ""token_characters"",\n        character_tokenizer: CharacterTokenizer = CharacterTokenizer(),\n        start_tokens: List[str] = None,\n        end_tokens: List[str] = None,\n        min_padding_length: int = 0,\n        token_min_padding_length: int = 0,\n    ) -> None:\n        super().__init__(token_min_padding_length)\n        if min_padding_length == 0:\n            url = ""https://github.com/allenai/allennlp/issues/1954""\n            warnings.warn(\n                ""You are using the default value (0) of `min_padding_length`, ""\n                f""which can cause some subtle bugs (more info see {url}). ""\n                ""Strongly recommend to set a value, usually the maximum size ""\n                ""of the convolutional layer size when using CnnEncoder."",\n                UserWarning,\n            )\n        self._min_padding_length = min_padding_length\n        self._namespace = namespace\n        self._character_tokenizer = character_tokenizer\n\n        self._start_tokens = [Token(st) for st in (start_tokens or [])]\n        self._end_tokens = [Token(et) for et in (end_tokens or [])]\n\n    @overrides\n    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):\n        if token.text is None:\n            raise ConfigurationError(""TokenCharactersIndexer needs a tokenizer that retains text"")\n        for character in self._character_tokenizer.tokenize(token.text):\n            # If `text_id` is set on the character token (e.g., if we\'re using byte encoding), we\n            # will not be using the vocab for this character.\n            if getattr(character, ""text_id"", None) is None:\n                counter[self._namespace][character.text] += 1\n\n    @overrides\n    def tokens_to_indices(\n        self, tokens: List[Token], vocabulary: Vocabulary\n    ) -> Dict[str, List[List[int]]]:\n        indices: List[List[int]] = []\n        for token in itertools.chain(self._start_tokens, tokens, self._end_tokens):\n            token_indices: List[int] = []\n            if token.text is None:\n                raise ConfigurationError(\n                    ""TokenCharactersIndexer needs a tokenizer that retains text""\n                )\n            for character in self._character_tokenizer.tokenize(token.text):\n                if getattr(character, ""text_id"", None) is not None:\n                    # `text_id` being set on the token means that we aren\'t using the vocab, we just\n                    # use this id instead.\n                    index = character.text_id\n                else:\n                    index = vocabulary.get_token_index(character.text, self._namespace)\n                token_indices.append(index)\n            indices.append(token_indices)\n        return {""token_characters"": indices}\n\n    @overrides\n    def get_padding_lengths(self, indexed_tokens: IndexedTokenList) -> Dict[str, int]:\n        padding_lengths = {}\n        padding_lengths[""token_characters""] = max(\n            len(indexed_tokens[""token_characters""]), self._token_min_padding_length\n        )\n        max_num_characters = self._min_padding_length\n        for token in indexed_tokens[""token_characters""]:\n            max_num_characters = max(len(token), max_num_characters)  # type: ignore\n        padding_lengths[""num_token_characters""] = max_num_characters\n        return padding_lengths\n\n    @overrides\n    def as_padded_tensor_dict(\n        self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]\n    ) -> Dict[str, torch.Tensor]:\n        # Pad the tokens.\n        padded_tokens = pad_sequence_to_length(\n            tokens[""token_characters""],\n            padding_lengths[""token_characters""],\n            default_value=lambda: [],\n        )\n\n        # Pad the characters within the tokens.\n        desired_token_length = padding_lengths[""num_token_characters""]\n        longest_token: List[int] = max(tokens[""token_characters""], key=len, default=[])  # type: ignore\n        padding_value = 0\n        if desired_token_length > len(longest_token):\n            # Since we want to pad to greater than the longest token, we add a\n            # ""dummy token"" so we can take advantage of the fast implementation of itertools.zip_longest.\n            padded_tokens.append([padding_value] * desired_token_length)\n        # pad the list of lists to the longest sublist, appending 0\'s\n        padded_tokens = list(zip(*itertools.zip_longest(*padded_tokens, fillvalue=padding_value)))\n        if desired_token_length > len(longest_token):\n            # Removes the ""dummy token"".\n            padded_tokens.pop()\n        # Truncates all the tokens to the desired length, and return the result.\n        return {\n            ""token_characters"": torch.LongTensor(\n                [list(token[:desired_token_length]) for token in padded_tokens]\n            )\n        }\n\n    @overrides\n    def get_empty_token_list(self) -> IndexedTokenList:\n        return {""token_characters"": []}\n'"
allennlp/data/token_indexers/token_indexer.py,4,"b'from typing import Any, Dict, List\n\nimport torch\n\nfrom allennlp.common import Registrable\nfrom allennlp.common.util import pad_sequence_to_length\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.vocabulary import Vocabulary\n\n# An indexed token list represents the arguments that will be passed to a TokenEmbedder\n# corresponding to this TokenIndexer.  Each argument that the TokenEmbedder needs will have one\n# entry in the IndexedTokenList dictionary, and that argument will typically be a list of integers\n# (for single ID word embeddings) or a nested list of integers (for character ID word embeddings),\n# though it could also be a mask, or any other data that you want to pass.\nIndexedTokenList = Dict[str, List[Any]]\n\n\nclass TokenIndexer(Registrable):\n    """"""\n    A `TokenIndexer` determines how string tokens get represented as arrays of indices in a model.\n    This class both converts strings into numerical values, with the help of a\n    :class:`~allennlp.data.vocabulary.Vocabulary`, and it produces actual arrays.\n\n    Tokens can be represented as single IDs (e.g., the word ""cat"" gets represented by the number\n    34), or as lists of character IDs (e.g., ""cat"" gets represented by the numbers [23, 10, 18]),\n    or in some other way that you can come up with (e.g., if you have some structured input you\n    want to represent in a special way in your data arrays, you can do that here).\n\n    # Parameters\n\n    token_min_padding_length : `int`, optional (default=`0`)\n        The minimum padding length required for the :class:`TokenIndexer`. For example,\n        the minimum padding length of :class:`SingleIdTokenIndexer` is the largest size of\n        filter when using :class:`CnnEncoder`.\n        Note that if you set this for one TokenIndexer, you likely have to set it for all\n        :class:`TokenIndexer` for the same field, otherwise you\'ll get mismatched tensor sizes.\n    """"""\n\n    default_implementation = ""single_id""\n    has_warned_for_as_padded_tensor = False\n\n    def __init__(self, token_min_padding_length: int = 0) -> None:\n        self._token_min_padding_length: int = token_min_padding_length\n\n    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):\n        """"""\n        The :class:`Vocabulary` needs to assign indices to whatever strings we see in the training\n        data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,\n        token).  This method takes a token and a dictionary of counts and increments counts for\n        whatever vocabulary items are present in the token.  If this is a single token ID\n        representation, the vocabulary item is likely the token itself.  If this is a token\n        characters representation, the vocabulary items are all of the characters in the token.\n        """"""\n        raise NotImplementedError\n\n    def tokens_to_indices(self, tokens: List[Token], vocabulary: Vocabulary) -> IndexedTokenList:\n        """"""\n        Takes a list of tokens and converts them to an `IndexedTokenList`.\n        This could be just an ID for each token from the vocabulary.\n        Or it could split each token into characters and return one ID per character.\n        Or (for instance, in the case of byte-pair encoding) there might not be a clean\n        mapping from individual tokens to indices, and the `IndexedTokenList` could be a complex\n        data structure.\n        """"""\n        raise NotImplementedError\n\n    def indices_to_tokens(\n        self, indexed_tokens: IndexedTokenList, vocabulary: Vocabulary\n    ) -> List[Token]:\n        """"""\n        Inverse operations of tokens_to_indices. Takes an `IndexedTokenList` and converts it back\n        into a list of tokens.\n        """"""\n        raise NotImplementedError\n\n    def get_empty_token_list(self) -> IndexedTokenList:\n        """"""\n        Returns an `already indexed` version of an empty token list.  This is typically just an\n        empty list for whatever keys are used in the indexer.\n        """"""\n        raise NotImplementedError\n\n    def get_padding_lengths(self, indexed_tokens: IndexedTokenList) -> Dict[str, int]:\n        """"""\n        This method returns a padding dictionary for the given `indexed_tokens` specifying all\n        lengths that need padding.  If all you have is a list of single ID tokens, this is just the\n        length of the list, and that\'s what the default implementation will give you.  If you have\n        something more complicated, like a list of character ids for token, you\'ll need to override\n        this.\n        """"""\n        padding_lengths = {}\n        for key, token_list in indexed_tokens.items():\n            padding_lengths[key] = max(len(token_list), self._token_min_padding_length)\n        return padding_lengths\n\n    def as_padded_tensor_dict(\n        self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]\n    ) -> Dict[str, torch.Tensor]:\n        """"""\n        This method pads a list of tokens given the input padding lengths (which could actually\n        truncate things, depending on settings) and returns that padded list of input tokens as a\n        `Dict[str, torch.Tensor]`.  This is a dictionary because there should be one key per\n        argument that the `TokenEmbedder` corresponding to this class expects in its `forward()`\n        method (where the argument name in the `TokenEmbedder` needs to make the key in this\n        dictionary).\n\n        The base class implements the case when all you want to do is create a padded `LongTensor`\n        for every list in the `tokens` dictionary.  If your `TokenIndexer` needs more complex\n        logic than that, you need to override this method.\n        """"""\n        tensor_dict = {}\n        for key, val in tokens.items():\n            if val and isinstance(val[0], bool):\n                tensor = torch.BoolTensor(\n                    pad_sequence_to_length(val, padding_lengths[key], default_value=lambda: False)\n                )\n            else:\n                tensor = torch.LongTensor(pad_sequence_to_length(val, padding_lengths[key]))\n            tensor_dict[key] = tensor\n        return tensor_dict\n\n    def __eq__(self, other) -> bool:\n        if isinstance(self, other.__class__):\n            return self.__dict__ == other.__dict__\n        return NotImplemented\n'"
allennlp/data/tokenizers/__init__.py,0,"b'""""""\nThis module contains various classes for performing\ntokenization.\n""""""\n\nfrom allennlp.data.tokenizers.tokenizer import Token, Tokenizer\nfrom allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\nfrom allennlp.data.tokenizers.letters_digits_tokenizer import LettersDigitsTokenizer\nfrom allennlp.data.tokenizers.pretrained_transformer_tokenizer import PretrainedTransformerTokenizer\nfrom allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\nfrom allennlp.data.tokenizers.sentence_splitter import SentenceSplitter\nfrom allennlp.data.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n'"
allennlp/data/tokenizers/character_tokenizer.py,0,"b'from typing import List, Union\n\nfrom overrides import overrides\n\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.tokenizers.tokenizer import Tokenizer\n\n\n@Tokenizer.register(""character"")\nclass CharacterTokenizer(Tokenizer):\n    """"""\n    A `CharacterTokenizer` splits strings into character tokens.\n\n    Registered as a `Tokenizer` with name ""character"".\n\n    # Parameters\n\n    byte_encoding : `str`, optional (default=`None`)\n        If not `None`, we will use this encoding to encode the string as bytes, and use the byte\n        sequence as characters, instead of the unicode characters in the python string.  E.g., the\n        character \'\xc3\xa1\' would be a single token if this option is `None`, but it would be two\n        tokens if this option is set to `""utf-8""`.\n\n        If this is not `None`, `tokenize` will return a `List[int]` instead of a\n        `List[str]`, and we will bypass the vocabulary in the `TokenIndexer`.\n\n    lowercase_characters : `bool`, optional (default=`False`)\n        If `True`, we will lowercase all of the characters in the text before doing any other\n        operation.  You probably do not want to do this, as character vocabularies are generally\n        not very large to begin with, but it\'s an option if you really want it.\n\n    start_tokens : `List[str]`, optional\n        If given, these tokens will be added to the beginning of every string we tokenize.  If\n        using byte encoding, this should actually be a `List[int]`, not a `List[str]`.\n\n    end_tokens : `List[str]`, optional\n        If given, these tokens will be added to the end of every string we tokenize.  If using byte\n        encoding, this should actually be a `List[int]`, not a `List[str]`.\n\n    """"""\n\n    def __init__(\n        self,\n        byte_encoding: str = None,\n        lowercase_characters: bool = False,\n        start_tokens: List[Union[str, int]] = None,\n        end_tokens: List[Union[str, int]] = None,\n    ) -> None:\n        # TODO(brendanr): Add length truncation.\n        self._byte_encoding = byte_encoding\n        self._lowercase_characters = lowercase_characters\n        self._start_tokens = start_tokens or []\n        # We reverse the tokens here because we\'re going to insert them with `insert(0)` later;\n        # this makes sure they show up in the right order.\n        self._start_tokens.reverse()\n        self._end_tokens = end_tokens or []\n\n    @overrides\n    def tokenize(self, text: str) -> List[Token]:\n        if self._lowercase_characters:\n            text = text.lower()\n        if self._byte_encoding is not None:\n            # We add 1 here so that we can still use 0 for masking, no matter what bytes we get out\n            # of this.\n            tokens = [Token(text_id=c + 1) for c in text.encode(self._byte_encoding)]\n        else:\n            tokens = [Token(t) for t in list(text)]\n        for start_token in self._start_tokens:\n            if isinstance(start_token, int):\n                token = Token(text_id=start_token, idx=0)\n            else:\n                token = Token(text=start_token, idx=0)\n            tokens.insert(0, token)\n        for end_token in self._end_tokens:\n            if isinstance(end_token, int):\n                token = Token(text_id=end_token, idx=0)\n            else:\n                token = Token(text=end_token, idx=0)\n            tokens.append(token)\n        return tokens\n\n    def __eq__(self, other) -> bool:\n        if isinstance(self, other.__class__):\n            return self.__dict__ == other.__dict__\n        return NotImplemented\n'"
allennlp/data/tokenizers/letters_digits_tokenizer.py,0,"b'import re\nfrom typing import List\n\nfrom overrides import overrides\n\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.tokenizers.tokenizer import Tokenizer\n\n\n@Tokenizer.register(""letters_digits"")\nclass LettersDigitsTokenizer(Tokenizer):\n    """"""\n    A `Tokenizer` which keeps runs of (unicode) letters and runs of digits together, while\n    every other non-whitespace character becomes a separate word.\n\n    Registered as a `Tokenizer` with name ""letters_digits"".\n    """"""\n\n    @overrides\n    def tokenize(self, text: str) -> List[Token]:\n        # We use the [^\\W\\d_] pattern as a trick to match unicode letters\n        tokens = [Token(m.group(), idx=m.start()) for m in re.finditer(r""[^\\W\\d_]+|\\d+|\\S"", text)]\n        return tokens\n'"
allennlp/data/tokenizers/pretrained_transformer_tokenizer.py,0,"b'import logging\nfrom typing import Any, Dict, List, Optional, Tuple, Iterable\n\nfrom overrides import overrides\nfrom transformers import PreTrainedTokenizer\nfrom transformers.tokenization_auto import AutoTokenizer\n\nfrom allennlp.common.util import sanitize_wordpiece\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.tokenizers.tokenizer import Tokenizer\n\nlogger = logging.getLogger(__name__)\n\n\n@Tokenizer.register(""pretrained_transformer"")\nclass PretrainedTransformerTokenizer(Tokenizer):\n    """"""\n    A `PretrainedTransformerTokenizer` uses a model from HuggingFace\'s\n    `transformers` library to tokenize some input text.  This often means wordpieces\n    (where `\'AllenNLP is awesome\'` might get split into `[\'Allen\', \'##NL\', \'##P\', \'is\',\n    \'awesome\']`), but it could also use byte-pair encoding, or some other tokenization, depending\n    on the pretrained model that you\'re using.\n\n    We take a model name as an input parameter, which we will pass to\n    `AutoTokenizer.from_pretrained`.\n\n    We also add special tokens relative to the pretrained model and truncate the sequences.\n\n    This tokenizer also indexes tokens and adds the indexes to the `Token` fields so that\n    they can be picked up by `PretrainedTransformerIndexer`.\n\n    Registered as a `Tokenizer` with name ""pretrained_transformer"".\n\n    # Parameters\n\n    model_name : `str`\n        The name of the pretrained wordpiece tokenizer to use.\n    add_special_tokens : `bool`, optional, (default=`True`)\n        If set to `True`, the sequences will be encoded with the special tokens relative\n        to their model.\n    max_length : `int`, optional (default=`None`)\n        If set to a number, will limit the total sequence returned so that it has a maximum length.\n        If there are overflowing tokens, those will be added to the returned dictionary\n    stride : `int`, optional (default=`0`)\n        If set to a number along with max_length, the overflowing tokens returned will contain some tokens\n        from the main sequence returned. The value of this argument defines the number of additional tokens.\n    truncation_strategy : `str`, optional (default=`\'longest_first\'`)\n        String selected in the following options:\n        - \'longest_first\' (default) Iteratively reduce the inputs sequence until the input is under max_length\n        starting from the longest one at each token (when there is a pair of input sequences)\n        - \'only_first\': Only truncate the first sequence\n        - \'only_second\': Only truncate the second sequence\n        - \'do_not_truncate\': Do not truncate (raise an error if the input sequence is longer than max_length)\n    tokenizer_kwargs: `Dict[str, Any]`\n        Dictionary with\n        [additional arguments](https://github.com/huggingface/transformers/blob/155c782a2ccd103cf63ad48a2becd7c76a7d2115/transformers/tokenization_utils.py#L691)\n        for `AutoTokenizer.from_pretrained`.\n\n    """"""  # noqa: E501\n\n    def __init__(\n        self,\n        model_name: str,\n        add_special_tokens: bool = True,\n        max_length: Optional[int] = None,\n        stride: int = 0,\n        truncation_strategy: str = ""longest_first"",\n        tokenizer_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        if tokenizer_kwargs is None:\n            tokenizer_kwargs = {}\n        else:\n            tokenizer_kwargs = tokenizer_kwargs.copy()\n        tokenizer_kwargs.setdefault(""use_fast"", True)\n        # Note: Just because we request a fast tokenizer doesn\'t mean we get one.\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model_name, add_special_tokens=False, **tokenizer_kwargs\n        )\n\n        self._add_special_tokens = add_special_tokens\n        self._max_length = max_length\n        self._stride = stride\n        self._truncation_strategy = truncation_strategy\n\n        self._tokenizer_lowercases = self.tokenizer_lowercases(self.tokenizer)\n\n        try:\n            self._reverse_engineer_special_tokens(""a"", ""b"", model_name, tokenizer_kwargs)\n        except AssertionError:\n            # For most transformer models, ""a"" and ""b"" work just fine as dummy tokens.  For a few,\n            # they don\'t, and so we use ""1"" and ""2"" instead.\n            self._reverse_engineer_special_tokens(""1"", ""2"", model_name, tokenizer_kwargs)\n\n    def _reverse_engineer_special_tokens(\n        self,\n        token_a: str,\n        token_b: str,\n        model_name: str,\n        tokenizer_kwargs: Optional[Dict[str, Any]],\n    ):\n        # storing the special tokens\n        self.sequence_pair_start_tokens = []\n        self.sequence_pair_mid_tokens = []\n        self.sequence_pair_end_tokens = []\n        # storing token type ids for the sequences\n        self.sequence_pair_first_token_type_id = None\n        self.sequence_pair_second_token_type_id = None\n\n        # storing the special tokens\n        self.single_sequence_start_tokens = []\n        self.single_sequence_end_tokens = []\n        # storing token type id for the sequence\n        self.single_sequence_token_type_id = None\n\n        # Reverse-engineer the tokenizer for two sequences\n        tokenizer_with_special_tokens = AutoTokenizer.from_pretrained(\n            model_name, add_special_tokens=True, **tokenizer_kwargs\n        )\n        dummy_output = tokenizer_with_special_tokens.encode_plus(\n            token_a,\n            token_b,\n            add_special_tokens=True,\n            return_token_type_ids=True,\n            return_attention_mask=False,\n        )\n        dummy_a = self.tokenizer.encode(token_a, add_special_tokens=False, add_prefix_space=True)[0]\n        assert dummy_a in dummy_output[""input_ids""]\n        dummy_b = self.tokenizer.encode(token_b, add_special_tokens=False, add_prefix_space=True)[0]\n        assert dummy_b in dummy_output[""input_ids""]\n        assert dummy_a != dummy_b\n\n        seen_dummy_a = False\n        seen_dummy_b = False\n        for token_id, token_type_id in zip(\n            dummy_output[""input_ids""], dummy_output[""token_type_ids""]\n        ):\n            if token_id == dummy_a:\n                if seen_dummy_a or seen_dummy_b:  # seeing a twice or b before a\n                    raise ValueError(""Cannot auto-determine the number of special tokens added."")\n                seen_dummy_a = True\n                assert (\n                    self.sequence_pair_first_token_type_id is None\n                    or self.sequence_pair_first_token_type_id == token_type_id\n                ), ""multiple different token type ids found for the first sequence""\n                self.sequence_pair_first_token_type_id = token_type_id\n                continue\n\n            if token_id == dummy_b:\n                if seen_dummy_b:  # seeing b twice\n                    raise ValueError(""Cannot auto-determine the number of special tokens added."")\n                seen_dummy_b = True\n                assert (\n                    self.sequence_pair_second_token_type_id is None\n                    or self.sequence_pair_second_token_type_id == token_type_id\n                ), ""multiple different token type ids found for the second sequence""\n                self.sequence_pair_second_token_type_id = token_type_id\n                continue\n\n            token = Token(\n                tokenizer_with_special_tokens.convert_ids_to_tokens(token_id),\n                text_id=token_id,\n                type_id=token_type_id,\n            )\n            if not seen_dummy_a:\n                self.sequence_pair_start_tokens.append(token)\n            elif not seen_dummy_b:\n                self.sequence_pair_mid_tokens.append(token)\n            else:\n                self.sequence_pair_end_tokens.append(token)\n\n        assert (\n            len(self.sequence_pair_start_tokens)\n            + len(self.sequence_pair_mid_tokens)\n            + len(self.sequence_pair_end_tokens)\n        ) == self.tokenizer.num_special_tokens_to_add(pair=True)\n\n        # Reverse-engineer the tokenizer for one sequence\n        dummy_output = tokenizer_with_special_tokens.encode_plus(\n            token_a,\n            add_special_tokens=True,\n            return_token_type_ids=True,\n            return_attention_mask=False,\n            add_prefix_space=True,\n        )\n\n        seen_dummy_a = False\n        for token_id, token_type_id in zip(\n            dummy_output[""input_ids""], dummy_output[""token_type_ids""]\n        ):\n            if token_id == dummy_a:\n                if seen_dummy_a:\n                    raise ValueError(""Cannot auto-determine the number of special tokens added."")\n                seen_dummy_a = True\n                assert (\n                    self.single_sequence_token_type_id is None\n                    or self.single_sequence_token_type_id == token_type_id\n                ), ""multiple different token type ids found for the sequence""\n                self.single_sequence_token_type_id = token_type_id\n                continue\n\n            token = Token(\n                tokenizer_with_special_tokens.convert_ids_to_tokens(token_id),\n                text_id=token_id,\n                type_id=token_type_id,\n            )\n            if not seen_dummy_a:\n                self.single_sequence_start_tokens.append(token)\n            else:\n                self.single_sequence_end_tokens.append(token)\n\n        assert (\n            len(self.single_sequence_start_tokens) + len(self.single_sequence_end_tokens)\n        ) == self.tokenizer.num_special_tokens_to_add(pair=False)\n\n    @staticmethod\n    def tokenizer_lowercases(tokenizer: PreTrainedTokenizer) -> bool:\n        # Huggingface tokenizers have different ways of remembering whether they lowercase or not. Detecting it\n        # this way seems like the least brittle way to do it.\n        tokenized = tokenizer.tokenize(\n            ""A""\n        )  # Use a single character that won\'t be cut into word pieces.\n        detokenized = "" "".join(tokenized)\n        return ""a"" in detokenized\n\n    @overrides\n    def tokenize(self, text: str) -> List[Token]:\n        """"""\n        This method only handles a single sentence (or sequence) of text.\n        """"""\n        encoded_tokens = self.tokenizer.encode_plus(\n            text=text,\n            add_special_tokens=False,\n            max_length=self._max_length,\n            stride=self._stride,\n            truncation_strategy=self._truncation_strategy,\n            return_tensors=None,\n            return_offsets_mapping=self.tokenizer.is_fast,\n            return_attention_mask=False,\n            return_token_type_ids=True,\n        )\n        # token_ids contains a final list with ids for both regular and special tokens\n        token_ids, token_type_ids, token_offsets = (\n            encoded_tokens[""input_ids""],\n            encoded_tokens[""token_type_ids""],\n            encoded_tokens.get(""offset_mapping""),\n        )\n\n        # If we don\'t have token offsets, try to calculate them ourselves.\n        if token_offsets is None:\n            token_offsets = self._estimate_character_indices(text, token_ids)\n\n        tokens = []\n        for token_id, token_type_id, offsets in zip(token_ids, token_type_ids, token_offsets):\n            if offsets is None or offsets[0] >= offsets[1]:\n                start = None\n                end = None\n            else:\n                start, end = offsets\n\n            tokens.append(\n                Token(\n                    text=self.tokenizer.convert_ids_to_tokens(token_id, skip_special_tokens=False),\n                    text_id=token_id,\n                    type_id=token_type_id,\n                    idx=start,\n                    idx_end=end,\n                )\n            )\n\n        if self._add_special_tokens:\n            tokens = self.add_special_tokens(tokens)\n\n        return tokens\n\n    def _estimate_character_indices(\n        self, text: str, token_ids: List[int]\n    ) -> List[Optional[Tuple[int, int]]]:\n        """"""\n        The huggingface tokenizers produce tokens that may or may not be slices from the\n        original text.  Differences arise from lowercasing, Unicode normalization, and other\n        kinds of normalization, as well as special characters that are included to denote\n        various situations, such as ""##"" in BERT for word pieces from the middle of a word, or\n        ""\xc4\xa0"" in RoBERTa for the beginning of words not at the start of a sentence.\n\n        This code attempts to calculate character offsets while being tolerant to these\n        differences. It scans through the text and the tokens in parallel, trying to match up\n        positions in both. If it gets out of sync, it backs off to not adding any token\n        indices, and attempts to catch back up afterwards. This procedure is approximate.\n        Don\'t rely on precise results, especially in non-English languages that are far more\n        affected by Unicode normalization.\n        """"""\n\n        token_texts = [\n            sanitize_wordpiece(t) for t in self.tokenizer.convert_ids_to_tokens(token_ids)\n        ]\n        token_offsets: List[Optional[Tuple[int, int]]] = [None] * len(token_ids)\n        if self._tokenizer_lowercases:\n            text = text.lower()\n            token_texts = [t.lower() for t in token_texts]\n\n        min_allowed_skipped_whitespace = 3\n        allowed_skipped_whitespace = min_allowed_skipped_whitespace\n\n        text_index = 0\n        token_index = 0\n        while text_index < len(text) and token_index < len(token_ids):\n            token_text = token_texts[token_index]\n            token_start_index = text.find(token_text, text_index)\n\n            # Did we not find it at all?\n            if token_start_index < 0:\n                token_index += 1\n                # When we skip a token, we increase our tolerance, so we have a chance of catching back up.\n                allowed_skipped_whitespace += 1 + min_allowed_skipped_whitespace\n                continue\n\n            # Did we jump too far?\n            non_whitespace_chars_skipped = sum(\n                1 for c in text[text_index:token_start_index] if not c.isspace()\n            )\n            if non_whitespace_chars_skipped > allowed_skipped_whitespace:\n                # Too many skipped characters. Something is wrong. Ignore this token.\n                token_index += 1\n                # When we skip a token, we increase our tolerance, so we have a chance of catching back up.\n                allowed_skipped_whitespace += 1 + min_allowed_skipped_whitespace\n                continue\n            allowed_skipped_whitespace = min_allowed_skipped_whitespace\n\n            token_offsets[token_index] = (\n                token_start_index,\n                token_start_index + len(token_text),\n            )\n            text_index = token_start_index + len(token_text)\n            token_index += 1\n        return token_offsets\n\n    def _intra_word_tokenize(\n        self, string_tokens: List[str]\n    ) -> Tuple[List[Token], List[Optional[Tuple[int, int]]]]:\n        tokens: List[Token] = []\n        offsets: List[Optional[Tuple[int, int]]] = []\n        for token_string in string_tokens:\n            wordpieces = self.tokenizer.encode_plus(\n                token_string,\n                add_special_tokens=False,\n                return_tensors=None,\n                return_offsets_mapping=False,\n                return_attention_mask=False,\n                return_token_type_ids=False,\n            )\n            wp_ids = wordpieces[""input_ids""]\n\n            if len(wp_ids) > 0:\n                offsets.append((len(tokens), len(tokens) + len(wp_ids) - 1))\n                tokens.extend(\n                    Token(text=wp_text, text_id=wp_id)\n                    for wp_id, wp_text in zip(wp_ids, self.tokenizer.convert_ids_to_tokens(wp_ids))\n                )\n            else:\n                offsets.append(None)\n        return tokens, offsets\n\n    @staticmethod\n    def _increment_offsets(\n        offsets: Iterable[Optional[Tuple[int, int]]], increment: int\n    ) -> List[Optional[Tuple[int, int]]]:\n        return [\n            None if offset is None else (offset[0] + increment, offset[1] + increment)\n            for offset in offsets\n        ]\n\n    def intra_word_tokenize(\n        self, string_tokens: List[str]\n    ) -> Tuple[List[Token], List[Optional[Tuple[int, int]]]]:\n        """"""\n        Tokenizes each word into wordpieces separately and returns the wordpiece IDs.\n        Also calculates offsets such that tokens[offsets[i][0]:offsets[i][1] + 1]\n        corresponds to the original i-th token.\n\n        This function inserts special tokens.\n        """"""\n        tokens, offsets = self._intra_word_tokenize(string_tokens)\n        tokens = self.add_special_tokens(tokens)\n        offsets = self._increment_offsets(offsets, len(self.single_sequence_start_tokens))\n        return tokens, offsets\n\n    def intra_word_tokenize_sentence_pair(\n        self, string_tokens_a: List[str], string_tokens_b: List[str]\n    ) -> Tuple[List[Token], List[Tuple[int, int]], List[Tuple[int, int]]]:\n        """"""\n        Tokenizes each word into wordpieces separately and returns the wordpiece IDs.\n        Also calculates offsets such that wordpieces[offsets[i][0]:offsets[i][1] + 1]\n        corresponds to the original i-th token.\n\n        This function inserts special tokens.\n        """"""\n        tokens_a, offsets_a = self._intra_word_tokenize(string_tokens_a)\n        tokens_b, offsets_b = self._intra_word_tokenize(string_tokens_b)\n        offsets_b = self._increment_offsets(\n            offsets_b,\n            (\n                len(self.sequence_pair_start_tokens)\n                + len(tokens_a)\n                + len(self.sequence_pair_mid_tokens)\n            ),\n        )\n        tokens_a = self.add_special_tokens(tokens_a, tokens_b)\n        offsets_a = self._increment_offsets(offsets_a, len(self.sequence_pair_start_tokens))\n\n        return tokens_a, offsets_a, offsets_b\n\n    def add_special_tokens(\n        self, tokens1: List[Token], tokens2: Optional[List[Token]] = None\n    ) -> List[Token]:\n        # Make sure we don\'t change the input parameters\n        import copy\n\n        tokens1 = copy.deepcopy(tokens1)\n        tokens2 = copy.deepcopy(tokens2)\n\n        # We add special tokens and also set token type ids.\n        if tokens2 is None:\n            import copy\n\n            tokens1 = copy.deepcopy(tokens1)\n            for token in tokens1:\n                token.type_id = self.single_sequence_token_type_id\n            return self.single_sequence_start_tokens + tokens1 + self.single_sequence_end_tokens\n        else:\n            for token in tokens1:\n                token.type_id = self.sequence_pair_first_token_type_id\n            for token in tokens2:\n                token.type_id = self.sequence_pair_second_token_type_id\n            return (\n                self.sequence_pair_start_tokens\n                + tokens1\n                + self.sequence_pair_mid_tokens\n                + tokens2\n                + self.sequence_pair_end_tokens\n            )\n\n    def num_special_tokens_for_sequence(self) -> int:\n        return len(self.single_sequence_start_tokens) + len(self.single_sequence_end_tokens)\n\n    def num_special_tokens_for_pair(self) -> int:\n        return (\n            len(self.sequence_pair_start_tokens)\n            + len(self.sequence_pair_mid_tokens)\n            + len(self.sequence_pair_end_tokens)\n        )\n'"
allennlp/data/tokenizers/sentence_splitter.py,0,"b'from typing import List\nfrom overrides import overrides\n\nimport spacy\n\nfrom allennlp.common import Registrable\nfrom allennlp.common.util import get_spacy_model\n\n\nclass SentenceSplitter(Registrable):\n    """"""\n    A `SentenceSplitter` splits strings into sentences.\n    """"""\n\n    default_implementation = ""spacy""\n\n    def split_sentences(self, text: str) -> List[str]:\n        """"""\n        Splits a `text` :class:`str` paragraph into a list of :class:`str`, where each is a sentence.\n        """"""\n        raise NotImplementedError\n\n    def batch_split_sentences(self, texts: List[str]) -> List[List[str]]:\n        """"""\n        Default implementation is to just iterate over the texts and call `split_sentences`.\n        """"""\n        return [self.split_sentences(text) for text in texts]\n\n\n@SentenceSplitter.register(""spacy"")\nclass SpacySentenceSplitter(SentenceSplitter):\n    """"""\n    A `SentenceSplitter` that uses spaCy\'s built-in sentence boundary detection.\n\n    Spacy\'s default sentence splitter uses a dependency parse to detect sentence boundaries, so\n    it is slow, but accurate.\n\n    Another option is to use rule-based sentence boundary detection. It\'s fast and has a small memory footprint,\n    since it uses punctuation to detect sentence boundaries. This can be activated with the `rule_based` flag.\n\n    By default, `SpacySentenceSplitter` calls the default spacy boundary detector.\n\n    Registered as a `SentenceSplitter` with name ""spacy"".\n    """"""\n\n    def __init__(self, language: str = ""en_core_web_sm"", rule_based: bool = False) -> None:\n        # we need spacy\'s dependency parser if we\'re not using rule-based sentence boundary detection.\n        self.spacy = get_spacy_model(language, parse=not rule_based, ner=False, pos_tags=False)\n        if rule_based:\n            # we use `sentencizer`, a built-in spacy module for rule-based sentence boundary detection.\n            # depending on the spacy version, it could be called \'sentencizer\' or \'sbd\'\n            sbd_name = ""sbd"" if spacy.__version__ < ""2.1"" else ""sentencizer""\n            if not self.spacy.has_pipe(sbd_name):\n                sbd = self.spacy.create_pipe(sbd_name)\n                self.spacy.add_pipe(sbd)\n\n    @overrides\n    def split_sentences(self, text: str) -> List[str]:\n        return [sent.string.strip() for sent in self.spacy(text).sents]\n\n    @overrides\n    def batch_split_sentences(self, texts: List[str]) -> List[List[str]]:\n        """"""\n        This method lets you take advantage of spacy\'s batch processing.\n        """"""\n        return [\n            [sentence.string.strip() for sentence in doc.sents] for doc in self.spacy.pipe(texts)\n        ]\n'"
allennlp/data/tokenizers/spacy_tokenizer.py,0,"b'from typing import List, Optional\n\nfrom overrides import overrides\nimport spacy\nfrom spacy.tokens import Doc\n\nfrom allennlp.common.util import get_spacy_model\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.tokenizers.tokenizer import Tokenizer\n\n\n@Tokenizer.register(""spacy"")\nclass SpacyTokenizer(Tokenizer):\n    """"""\n    A `Tokenizer` that uses spaCy\'s tokenizer.  It\'s fast and reasonable - this is the\n    recommended `Tokenizer`. By default it will return allennlp Tokens,\n    which are small, efficient NamedTuples (and are serializable). If you want\n    to keep the original spaCy tokens, pass keep_spacy_tokens=True.  Note that we leave one particular piece of\n    post-processing for later: the decision of whether or not to lowercase the token.  This is for\n    two reasons: (1) if you want to make two different casing decisions for whatever reason, you\n    won\'t have to run the tokenizer twice, and more importantly (2) if you want to lowercase words\n    for your word embedding, but retain capitalization in a character-level representation, we need\n    to retain the capitalization here.\n\n    Registered as a `Tokenizer` with name ""spacy"", which is currently the default.\n\n    # Parameters\n\n    language : `str`, optional, (default=`""en_core_web_sm""`)\n        Spacy model name.\n    pos_tags : `bool`, optional, (default=`False`)\n        If `True`, performs POS tagging with spacy model on the tokens.\n        Generally used in conjunction with :class:`~allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer`.\n    parse : `bool`, optional, (default=`False`)\n        If `True`, performs dependency parsing with spacy model on the tokens.\n        Generally used in conjunction with :class:`~allennlp.data.token_indexers.pos_tag_indexer.DepLabelIndexer`.\n    ner : `bool`, optional, (default=`False`)\n        If `True`, performs dependency parsing with spacy model on the tokens.\n        Generally used in conjunction with :class:`~allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer`.\n    keep_spacy_tokens : `bool`, optional, (default=`False`)\n        If `True`, will preserve spacy token objects, We copy spacy tokens into our own class by default instead\n        because spacy Cython Tokens can\'t be pickled.\n    split_on_spaces : `bool`, optional, (default=`False`)\n        If `True`, will split by spaces without performing tokenization.\n        Used when your data is already tokenized, but you want to perform pos, ner or parsing on the tokens.\n    start_tokens : `Optional[List[str]]`, optional, (default=`None`)\n        If given, these tokens will be added to the beginning of every string we tokenize.\n    end_tokens : `Optional[List[str]]`, optional, (default=`None`)\n        If given, these tokens will be added to the end of every string we tokenize.\n    """"""\n\n    def __init__(\n        self,\n        language: str = ""en_core_web_sm"",\n        pos_tags: bool = False,\n        parse: bool = False,\n        ner: bool = False,\n        keep_spacy_tokens: bool = False,\n        split_on_spaces: bool = False,\n        start_tokens: Optional[List[str]] = None,\n        end_tokens: Optional[List[str]] = None,\n    ) -> None:\n        self.spacy = get_spacy_model(language, pos_tags, parse, ner)\n        if split_on_spaces:\n            self.spacy.tokenizer = _WhitespaceSpacyTokenizer(self.spacy.vocab)\n\n        self._keep_spacy_tokens = keep_spacy_tokens\n        self._start_tokens = start_tokens or []\n        # We reverse the tokens here because we\'re going to insert them with `insert(0)` later;\n        # this makes sure they show up in the right order.\n        self._start_tokens.reverse()\n        self._end_tokens = end_tokens or []\n\n    def _sanitize(self, tokens: List[spacy.tokens.Token]) -> List[Token]:\n        """"""\n        Converts spaCy tokens to allennlp tokens. Is a no-op if\n        keep_spacy_tokens is True\n        """"""\n        if not self._keep_spacy_tokens:\n            tokens = [\n                Token(\n                    token.text,\n                    token.idx,\n                    token.idx + len(token.text),\n                    token.lemma_,\n                    token.pos_,\n                    token.tag_,\n                    token.dep_,\n                    token.ent_type_,\n                )\n                for token in tokens\n            ]\n        for start_token in self._start_tokens:\n            tokens.insert(0, Token(start_token, 0))\n        for end_token in self._end_tokens:\n            tokens.append(Token(end_token, -1))\n        return tokens\n\n    @overrides\n    def batch_tokenize(self, texts: List[str]) -> List[List[Token]]:\n        return [\n            self._sanitize(_remove_spaces(tokens))\n            for tokens in self.spacy.pipe(texts, n_threads=-1)\n        ]\n\n    @overrides\n    def tokenize(self, text: str) -> List[Token]:\n        # This works because our Token class matches spacy\'s.\n        return self._sanitize(_remove_spaces(self.spacy(text)))\n\n\nclass _WhitespaceSpacyTokenizer:\n    """"""\n    Spacy doesn\'t assume that text is tokenised. Sometimes this\n    is annoying, like when you have gold data which is pre-tokenised,\n    but Spacy\'s tokenisation doesn\'t match the gold. This can be used\n    as follows:\n    nlp = spacy.load(""en_core_web_md"")\n    # hack to replace tokenizer with a whitespace tokenizer\n    nlp.tokenizer = _WhitespaceSpacyTokenizer(nlp.vocab)\n    ... use nlp(""here is some text"") as normal.\n    """"""\n\n    def __init__(self, vocab):\n        self.vocab = vocab\n\n    def __call__(self, text):\n        words = text.split("" "")\n        spaces = [True] * len(words)\n        return Doc(self.vocab, words=words, spaces=spaces)\n\n\ndef _remove_spaces(tokens: List[spacy.tokens.Token]) -> List[spacy.tokens.Token]:\n    return [token for token in tokens if not token.is_space]\n'"
allennlp/data/tokenizers/token.py,0,"b'from dataclasses import dataclass\nfrom typing import Optional\n\n\n@dataclass(init=False, repr=False)\nclass Token:\n    """"""\n    A simple token representation, keeping track of the token\'s text, offset in the passage it was\n    taken from, POS tag, dependency relation, and similar information.  These fields match spacy\'s\n    exactly, so we can just use a spacy token for this.\n\n    # Parameters\n\n    text : `str`, optional\n        The original text represented by this token.\n    idx : `int`, optional\n        The character offset of this token into the tokenized passage.\n    idx_end : `int`, optional\n        The character offset one past the last character in the tokenized passage.\n    lemma_ : `str`, optional\n        The lemma of this token.\n    pos_ : `str`, optional\n        The coarse-grained part of speech of this token.\n    tag_ : `str`, optional\n        The fine-grained part of speech of this token.\n    dep_ : `str`, optional\n        The dependency relation for this token.\n    ent_type_ : `str`, optional\n        The entity type (i.e., the NER tag) for this token.\n    text_id : `int`, optional\n        If your tokenizer returns integers instead of strings (e.g., because you\'re doing byte\n        encoding, or some hash-based embedding), set this with the integer.  If this is set, we\n        will bypass the vocabulary when indexing this token, regardless of whether `text` is also\n        set.  You can `also` set `text` with the original text, if you want, so that you can\n        still use a character-level representation in addition to a hash-based word embedding.\n    type_id : `int`, optional\n        Token type id used by some pretrained language models like original BERT\n\n        The other fields on `Token` follow the fields on spacy\'s `Token` object; this is one we\n        added, similar to spacy\'s `lex_id`.\n    """"""\n\n    __slots__ = [\n        ""text"",\n        ""idx"",\n        ""idx_end"",\n        ""lemma_"",\n        ""pos_"",\n        ""tag_"",\n        ""dep_"",\n        ""ent_type_"",\n        ""text_id"",\n        ""type_id"",\n    ]\n    # Defining the `__slots__` of this class is an optimization that dramatically reduces\n    # the size in memory of a `Token` instance. The downside of using `__slots__`\n    # with a dataclass is that you can\'t assign default values at the class level,\n    # which is why we need a custom `__init__` function that provides the default values.\n\n    text: Optional[str]\n    idx: Optional[int]\n    idx_end: Optional[int]\n    lemma_: Optional[str]\n    pos_: Optional[str]\n    tag_: Optional[str]\n    dep_: Optional[str]\n    ent_type_: Optional[str]\n    text_id: Optional[int]\n    type_id: Optional[int]\n\n    def __init__(\n        self,\n        text: str = None,\n        idx: int = None,\n        idx_end: int = None,\n        lemma_: str = None,\n        pos_: str = None,\n        tag_: str = None,\n        dep_: str = None,\n        ent_type_: str = None,\n        text_id: int = None,\n        type_id: int = None,\n    ) -> None:\n        self.text = text\n        self.idx = idx\n        self.idx_end = idx_end\n        self.lemma_ = lemma_\n        self.pos_ = pos_\n        self.tag_ = tag_\n        self.dep_ = dep_\n        self.ent_type_ = ent_type_\n        self.text_id = text_id\n        self.type_id = type_id\n\n    def __str__(self):\n        return self.text\n\n    def __repr__(self):\n        return self.__str__()\n\n\ndef show_token(token: Token) -> str:\n    return (\n        f""{token.text} ""\n        f""(idx: {token.idx}) ""\n        f""(idx_end: {token.idx_end}) ""\n        f""(lemma: {token.lemma_}) ""\n        f""(pos: {token.pos_}) ""\n        f""(tag: {token.tag_}) ""\n        f""(dep: {token.dep_}) ""\n        f""(ent_type: {token.ent_type_}) ""\n        f""(text_id: {token.text_id}) ""\n        f""(type_id: {token.type_id}) ""\n    )\n'"
allennlp/data/tokenizers/tokenizer.py,0,"b'from typing import List, Optional\nimport logging\n\nfrom allennlp.common import Registrable\nfrom allennlp.data.tokenizers.token import Token\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Tokenizer(Registrable):\n    """"""\n    A `Tokenizer` splits strings of text into tokens.  Typically, this either splits text into\n    word tokens or character tokens, and those are the two tokenizer subclasses we have implemented\n    here, though you could imagine wanting to do other kinds of tokenization for structured or\n    other inputs.\n\n    See the parameters to, e.g., :class:`~.SpacyTokenizer`, or whichever tokenizer\n    you want to use.\n\n    If the base input to your model is words, you should use a :class:`~.SpacyTokenizer`, even if\n    you also want to have a character-level encoder to get an additional vector for each word\n    token.  Splitting word tokens into character arrays is handled separately, in the\n    :class:`..token_representations.TokenRepresentation` class.\n    """"""\n\n    default_implementation = ""spacy""\n\n    def batch_tokenize(self, texts: List[str]) -> List[List[Token]]:\n        """"""\n        Batches together tokenization of several texts, in case that is faster for particular\n        tokenizers.\n\n        By default we just do this without batching.  Override this in your tokenizer if you have a\n        good way of doing batched computation.\n        """"""\n        return [self.tokenize(text) for text in texts]\n\n    def tokenize(self, text: str) -> List[Token]:\n        """"""\n        Actually implements splitting words into tokens.\n\n        # Returns\n\n        tokens : `List[Token]`\n        """"""\n        raise NotImplementedError\n\n    def add_special_tokens(\n        self, tokens1: List[Token], tokens2: Optional[List[Token]] = None\n    ) -> List[Token]:\n        """"""\n        Adds special tokens to tokenized text. These are tokens like [CLS] or [SEP].\n\n        Not all tokenizers do this. The default is to just return the tokens unchanged.\n\n        # Parameters\n\n        tokens1 : `List[Token]`\n            The list of tokens to add special tokens to.\n        tokens2 : `Optional[List[Token]]`\n            An optional second list of tokens. This will be concatenated with `tokens1`. Special tokens will be\n            added as appropriate.\n\n        # Returns\n        tokens : `List[Token]`\n            The combined list of tokens, with special tokens added.\n        """"""\n        return tokens1 + (tokens2 or [])\n\n    def num_special_tokens_for_sequence(self) -> int:\n        """"""\n        Returns the number of special tokens added for a single sequence.\n        """"""\n        return 0\n\n    def num_special_tokens_for_pair(self) -> int:\n        """"""\n        Returns the number of special tokens added for a pair of sequences.\n        """"""\n        return 0\n'"
allennlp/data/tokenizers/whitespace_tokenizer.py,0,"b'from typing import List\n\nfrom overrides import overrides\n\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.tokenizers.tokenizer import Tokenizer\n\n\n@Tokenizer.register(""whitespace"")\n@Tokenizer.register(""just_spaces"")\nclass WhitespaceTokenizer(Tokenizer):\n    """"""\n    A `Tokenizer` that assumes you\'ve already done your own tokenization somehow and have\n    separated the tokens by spaces.  We just split the input string on whitespace and return the\n    resulting list.\n\n    Note that we use `text.split()`, which means that the amount of whitespace between the\n    tokens does not matter.  This will never result in spaces being included as tokens.\n\n    Registered as a `Tokenizer` with name ""whitespace"" and ""just_spaces"".\n    """"""\n\n    @overrides\n    def tokenize(self, text: str) -> List[Token]:\n        return [Token(t) for t in text.split()]\n'"
allennlp/interpret/attackers/__init__.py,0,b'from allennlp.interpret.attackers.attacker import Attacker\nfrom allennlp.interpret.attackers.input_reduction import InputReduction\nfrom allennlp.interpret.attackers.hotflip import Hotflip\n'
allennlp/interpret/attackers/attacker.py,0,"b'from typing import List\n\nfrom allennlp.common import Registrable\nfrom allennlp.common.util import JsonDict\nfrom allennlp.predictors import Predictor\n\n\nclass Attacker(Registrable):\n    """"""\n    An `Attacker` will modify an input (e.g., add or delete tokens) to try to change an AllenNLP\n    Predictor\'s output in a desired manner (e.g., make it incorrect).\n    """"""\n\n    def __init__(self, predictor: Predictor) -> None:\n        self.predictor = predictor\n\n    def initialize(self):\n        """"""\n        Initializes any components of the Attacker that are expensive to compute, so that they are\n        not created on __init__().  Default implementation is `pass`.\n        """"""\n        pass\n\n    def attack_from_json(\n        self,\n        inputs: JsonDict,\n        input_field_to_attack: str,\n        grad_input_field: str,\n        ignore_tokens: List[str],\n        target: JsonDict,\n    ) -> JsonDict:\n        """"""\n        This function finds a modification to the input text that would change the model\'s\n        prediction in some desired manner (e.g., an adversarial attack).\n\n        # Parameters\n\n        inputs : `JsonDict`\n            The input you want to attack (the same as the argument to a Predictor, e.g.,\n            predict_json()).\n        input_field_to_attack : `str`\n            The key in the inputs JsonDict you want to attack, e.g., `tokens`.\n        grad_input_field : `str`\n            The field in the gradients dictionary that contains the input gradients.  For example,\n            `grad_input_1` will be the field for single input tasks. See get_gradients() in\n            `Predictor` for more information on field names.\n        target : `JsonDict`\n            If given, this is a `targeted` attack, trying to change the prediction to a particular\n            value, instead of just changing it from its original prediction.  Subclasses are not\n            required to accept this argument, as not all attacks make sense as targeted attacks.\n            Perhaps that means we should make the API more crisp, but adding another class is not\n            worth it.\n\n        # Returns\n\n        reduced_input : `JsonDict`\n            Contains the final, sanitized input after adversarial modification.\n        """"""\n        raise NotImplementedError()\n'"
allennlp/interpret/attackers/hotflip.py,14,"b'from copy import deepcopy\nfrom typing import Dict, List, Tuple\n\nimport numpy\nimport torch\n\nfrom allennlp.common.util import JsonDict, sanitize\nfrom allennlp.data import Instance, Token\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.token_indexers import (\n    ELMoTokenCharactersIndexer,\n    TokenCharactersIndexer,\n    SingleIdTokenIndexer,\n)\nfrom allennlp.interpret.attackers import utils\nfrom allennlp.interpret.attackers.attacker import Attacker\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.nn import util\nfrom allennlp.predictors.predictor import Predictor\n\nDEFAULT_IGNORE_TOKENS = [""@@NULL@@"", ""."", "","", "";"", ""!"", ""?"", ""[MASK]"", ""[SEP]"", ""[CLS]""]\n\n\n@Attacker.register(""hotflip"")\nclass Hotflip(Attacker):\n    """"""\n    Runs the HotFlip style attack at the word-level https://arxiv.org/abs/1712.06751.  We use the\n    first-order taylor approximation described in https://arxiv.org/abs/1903.06620, in the function\n    `_first_order_taylor()`.\n\n    We try to re-use the embedding matrix from the model when deciding what other words to flip a\n    token to.  For a large class of models, this is straightforward.  When there is a\n    character-level encoder, however (e.g., with ELMo, any char-CNN, etc.), or a combination of\n    encoders (e.g., ELMo + glove), we need to construct a fake embedding matrix that we can use in\n    `_first_order_taylor()`.  We do this by getting a list of words from the model\'s vocabulary\n    and embedding them using the encoder.  This can be expensive, both in terms of time and memory\n    usage, so we take a `max_tokens` parameter to limit the size of this fake embedding matrix.\n    This also requires a model to `have` a token vocabulary in the first place, which can be\n    problematic for models that only have character vocabularies.\n\n    Registered as an `Attacker` with name ""hotflip"".\n\n    # Parameters\n\n    predictor : `Predictor`\n        The model (inside a Predictor) that we\'re attacking.  We use this to get gradients and\n        predictions.\n    vocab_namespace : `str`, optional (default=`\'tokens\'`)\n        We use this to know three things: (1) which tokens we should ignore when producing flips\n        (we don\'t consider non-alphanumeric tokens); (2) what the string value is of the token that\n        we produced, so we can show something human-readable to the user; and (3) if we need to\n        construct a fake embedding matrix, we use the tokens in the vocabulary as flip candidates.\n    max_tokens : `int`, optional (default=`5000`)\n        This is only used when we need to construct a fake embedding matrix.  That matrix can take\n        a lot of memory when the vocab size is large.  This parameter puts a cap on the number of\n        tokens to use, so the fake embedding matrix doesn\'t take as much memory.\n    """"""\n\n    def __init__(\n        self, predictor: Predictor, vocab_namespace: str = ""tokens"", max_tokens: int = 5000\n    ) -> None:\n        super().__init__(predictor)\n        self.vocab = self.predictor._model.vocab\n        self.namespace = vocab_namespace\n        # Force new tokens to be alphanumeric\n        self.max_tokens = max_tokens\n        self.invalid_replacement_indices: List[int] = []\n        for i in self.vocab._index_to_token[self.namespace]:\n            if not self.vocab._index_to_token[self.namespace][i].isalnum():\n                self.invalid_replacement_indices.append(i)\n        self.embedding_matrix: torch.Tensor = None\n        self.embedding_layer: torch.nn.Module = None\n        # get device number\n        self.cuda_device = predictor.cuda_device\n\n    def initialize(self):\n        """"""\n        Call this function before running attack_from_json(). We put the call to\n        `_construct_embedding_matrix()` in this function to prevent a large amount of compute\n        being done when __init__() is called.\n        """"""\n        if self.embedding_matrix is None:\n            self.embedding_matrix = self._construct_embedding_matrix()\n\n    def _construct_embedding_matrix(self) -> Embedding:\n        """"""\n        For HotFlip, we need a word embedding matrix to search over. The below is necessary for\n        models such as ELMo, character-level models, or for models that use a projection layer\n        after their word embeddings.\n\n        We run all of the tokens from the vocabulary through the TextFieldEmbedder, and save the\n        final output embedding. We then group all of those output embeddings into an ""embedding\n        matrix"".\n        """"""\n        embedding_layer = util.find_embedding_layer(self.predictor._model)\n        self.embedding_layer = embedding_layer\n        if isinstance(embedding_layer, (Embedding, torch.nn.modules.sparse.Embedding)):\n            # If we\'re using something that already has an only embedding matrix, we can just use\n            # that and bypass this method.\n            return embedding_layer.weight\n\n        # We take the top `self.max_tokens` as candidates for hotflip.  Because we have to\n        # construct a new vector for each of these, we can\'t always afford to use the whole vocab,\n        # for both runtime and memory considerations.\n        all_tokens = list(self.vocab._token_to_index[self.namespace])[: self.max_tokens]\n        max_index = self.vocab.get_token_index(all_tokens[-1], self.namespace)\n        self.invalid_replacement_indices = [\n            i for i in self.invalid_replacement_indices if i < max_index\n        ]\n\n        inputs = self._make_embedder_input(all_tokens)\n\n        # pass all tokens through the fake matrix and create an embedding out of it.\n        embedding_matrix = embedding_layer(inputs).squeeze()\n\n        return embedding_matrix\n\n    def _make_embedder_input(self, all_tokens: List[str]) -> Dict[str, torch.Tensor]:\n        inputs = {}\n        # A bit of a hack; this will only work with some dataset readers, but it\'ll do for now.\n        indexers = self.predictor._dataset_reader._token_indexers  # type: ignore\n        for indexer_name, token_indexer in indexers.items():\n            if isinstance(token_indexer, SingleIdTokenIndexer):\n                all_indices = [\n                    self.vocab._token_to_index[self.namespace][token] for token in all_tokens\n                ]\n                inputs[indexer_name] = {""tokens"": torch.LongTensor(all_indices).unsqueeze(0)}\n            elif isinstance(token_indexer, TokenCharactersIndexer):\n                tokens = [Token(x) for x in all_tokens]\n                max_token_length = max(len(x) for x in all_tokens)\n                # sometime max_token_length is too short for cnn encoder\n                max_token_length = max(max_token_length, token_indexer._min_padding_length)\n                indexed_tokens = token_indexer.tokens_to_indices(tokens, self.vocab)\n                padding_lengths = token_indexer.get_padding_lengths(indexed_tokens)\n                padded_tokens = token_indexer.as_padded_tensor_dict(indexed_tokens, padding_lengths)\n                inputs[indexer_name] = {\n                    ""token_characters"": torch.LongTensor(\n                        padded_tokens[""token_characters""]\n                    ).unsqueeze(0)\n                }\n            elif isinstance(token_indexer, ELMoTokenCharactersIndexer):\n                elmo_tokens = []\n                for token in all_tokens:\n                    elmo_indexed_token = token_indexer.tokens_to_indices(\n                        [Token(text=token)], self.vocab\n                    )[""elmo_tokens""]\n                    elmo_tokens.append(elmo_indexed_token[0])\n                inputs[indexer_name] = {""elmo_tokens"": torch.LongTensor(elmo_tokens).unsqueeze(0)}\n            else:\n                raise RuntimeError(""Unsupported token indexer:"", token_indexer)\n\n        return util.move_to_device(inputs, self.cuda_device)\n\n    def attack_from_json(\n        self,\n        inputs: JsonDict,\n        input_field_to_attack: str = ""tokens"",\n        grad_input_field: str = ""grad_input_1"",\n        ignore_tokens: List[str] = None,\n        target: JsonDict = None,\n    ) -> JsonDict:\n        """"""\n        Replaces one token at a time from the input until the model\'s prediction changes.\n        `input_field_to_attack` is for example `tokens`, it says what the input field is\n        called.  `grad_input_field` is for example `grad_input_1`, which is a key into a grads\n        dictionary.\n\n        The method computes the gradient w.r.t. the tokens, finds the token with the maximum\n        gradient (by L2 norm), and replaces it with another token based on the first-order Taylor\n        approximation of the loss.  This process is iteratively repeated until the prediction\n        changes.  Once a token is replaced, it is not flipped again.\n\n        # Parameters\n\n        inputs : `JsonDict`\n            The model inputs, the same as what is passed to a `Predictor`.\n        input_field_to_attack : `str`, optional (default=`\'tokens\'`)\n            The field that has the tokens that we\'re going to be flipping.  This must be a\n            `TextField`.\n        grad_input_field : `str`, optional (default=`\'grad_input_1\'`)\n            If there is more than one field that gets embedded in your model (e.g., a question and\n            a passage, or a premise and a hypothesis), this tells us the key to use to get the\n            correct gradients.  This selects from the output of :func:`Predictor.get_gradients`.\n        ignore_tokens : `List[str]`, optional (default=`DEFAULT_IGNORE_TOKENS`)\n            These tokens will not be flipped.  The default list includes some simple punctuation,\n            OOV and padding tokens, and common control tokens for BERT, etc.\n        target : `JsonDict`, optional (default=`None`)\n            If given, this will be a `targeted` hotflip attack, where instead of just trying to\n            change a model\'s prediction from what it current is predicting, we try to change it to\n            a `specific` target value.  This is a `JsonDict` because it needs to specify the\n            field name and target value.  For example, for a masked LM, this would be something\n            like `{""words"": [""she""]}`, because `""words""` is the field name, there is one mask\n            token (hence the list of length one), and we want to change the prediction from\n            whatever it was to `""she""`.\n        """"""\n        instance = self.predictor._json_to_instance(inputs)\n        if target is None:\n            output_dict = self.predictor._model.forward_on_instance(instance)\n        else:\n            output_dict = target\n\n        # This now holds the predictions that we want to change (either away from or towards,\n        # depending on whether `target` was passed).  We\'ll use this in the loop below to check for\n        # when we\'ve met our stopping criterion.\n        original_instances = self.predictor.predictions_to_labeled_instances(instance, output_dict)\n\n        # This is just for ease of access in the UI, so we know the original tokens.  It\'s not used\n        # in the logic below.\n        original_text_field: TextField = original_instances[0][  # type: ignore\n            input_field_to_attack\n        ]\n        original_tokens = deepcopy(original_text_field.tokens)\n\n        final_tokens = []\n        final_outputs = []\n        # `original_instances` is a list because there might be several different predictions that\n        # we\'re trying to attack (e.g., all of the NER tags for an input sentence).  We attack them\n        # one at a time.\n        for instance in original_instances:\n            tokens, outputs = self.attack_instance(\n                instance=instance,\n                inputs=inputs,\n                input_field_to_attack=input_field_to_attack,\n                grad_input_field=grad_input_field,\n                ignore_tokens=ignore_tokens,\n                target=target,\n            )\n            final_tokens.append(tokens)\n            final_outputs.append(outputs)\n\n        return sanitize(\n            {""final"": final_tokens, ""original"": original_tokens, ""outputs"": final_outputs}\n        )\n\n    def attack_instance(\n        self,\n        instance: Instance,\n        inputs: JsonDict,\n        input_field_to_attack: str = ""tokens"",\n        grad_input_field: str = ""grad_input_1"",\n        ignore_tokens: List[str] = None,\n        target: JsonDict = None,\n    ) -> Tuple[List[Token], JsonDict]:\n        if self.embedding_matrix is None:\n            self.initialize()\n\n        ignore_tokens = DEFAULT_IGNORE_TOKENS if ignore_tokens is None else ignore_tokens\n\n        # If `target` is `None`, we move away from the current prediction, otherwise we move\n        # _towards_ the target.\n        sign = -1 if target is None else 1\n\n        # Gets a list of the fields that we want to check to see if they change.\n        fields_to_compare = utils.get_fields_to_compare(inputs, instance, input_field_to_attack)\n\n        # We\'ll be modifying the tokens in this text field below, and grabbing the modified\n        # list after the `while` loop.\n        text_field: TextField = instance[input_field_to_attack]  # type: ignore\n\n        # Because we can save computation by getting grads and outputs at the same time, we do\n        # them together at the end of the loop, even though we use grads at the beginning and\n        # outputs at the end.  This is our initial gradient for the beginning of the loop.  The\n        # output can be ignored here.\n        grads, outputs = self.predictor.get_gradients([instance])\n\n        # Ignore any token that is in the ignore_tokens list by setting the token to already\n        # flipped.\n        flipped: List[int] = []\n        for index, token in enumerate(text_field.tokens):\n            if token.text in ignore_tokens:\n                flipped.append(index)\n        if ""clusters"" in outputs:\n            # Coref unfortunately needs a special case here.  We don\'t want to flip words in\n            # the same predicted coref cluster, but we can\'t really specify a list of tokens,\n            # because, e.g., ""he"" could show up in several different clusters.\n            # TODO(mattg): perhaps there\'s a way to get `predictions_to_labeled_instances` to\n            # return the set of tokens that shouldn\'t be changed for each instance?  E.g., you\n            # could imagine setting a field on the `Token` object, that we could then read\n            # here...\n            for cluster in outputs[""clusters""]:\n                for mention in cluster:\n                    for index in range(mention[0], mention[1] + 1):\n                        flipped.append(index)\n\n        while True:\n            # Compute L2 norm of all grads.\n            grad = grads[grad_input_field][0]\n            grads_magnitude = [g.dot(g) for g in grad]\n\n            # only flip a token once\n            for index in flipped:\n                grads_magnitude[index] = -1\n\n            # We flip the token with highest gradient norm.\n            index_of_token_to_flip = numpy.argmax(grads_magnitude)\n            if grads_magnitude[index_of_token_to_flip] == -1:\n                # If we\'ve already flipped all of the tokens, we give up.\n                break\n            flipped.append(index_of_token_to_flip)\n\n            text_field_tensors = text_field.as_tensor(text_field.get_padding_lengths())\n            input_tokens = util.get_token_ids_from_text_field_tensors(text_field_tensors)\n            original_id_of_token_to_flip = input_tokens[index_of_token_to_flip]\n\n            # Get new token using taylor approximation.\n            new_id = self._first_order_taylor(\n                grad[index_of_token_to_flip], original_id_of_token_to_flip, sign\n            )\n\n            # Flip token.  We need to tell the instance to re-index itself, so the text field\n            # will actually update.\n            new_token = Token(self.vocab._index_to_token[self.namespace][new_id])  # type: ignore\n            text_field.tokens[index_of_token_to_flip] = new_token\n            instance.indexed = False\n\n            # Get model predictions on instance, and then label the instances\n            grads, outputs = self.predictor.get_gradients([instance])  # predictions\n            for key, output in outputs.items():\n                if isinstance(output, torch.Tensor):\n                    outputs[key] = output.detach().cpu().numpy().squeeze()\n                elif isinstance(output, list):\n                    outputs[key] = output[0]\n\n            # TODO(mattg): taking the first result here seems brittle, if we\'re in a case where\n            # there are multiple predictions.\n            labeled_instance = self.predictor.predictions_to_labeled_instances(instance, outputs)[0]\n\n            # If we\'ve met our stopping criterion, we stop.\n            has_changed = utils.instance_has_changed(labeled_instance, fields_to_compare)\n            if target is None and has_changed:\n                # With no target, we just want to change the prediction.\n                break\n            if target is not None and not has_changed:\n                # With a given target, we want to *match* the target, which we check by\n                # `not has_changed`.\n                break\n        return text_field.tokens, outputs\n\n    def _first_order_taylor(self, grad: numpy.ndarray, token_idx: torch.Tensor, sign: int) -> int:\n        """"""\n        The below code is based on\n        https://github.com/pmichel31415/translate/blob/paul/pytorch_translate/\n        research/adversarial/adversaries/brute_force_adversary.py\n\n        Replaces the current token_idx with another token_idx to increase the loss. In particular, this\n        function uses the grad, alongside the embedding_matrix to select the token that maximizes the\n        first-order taylor approximation of the loss.\n        """"""\n        grad = util.move_to_device(torch.from_numpy(grad), self.cuda_device)\n        if token_idx.size() != ():\n            # We\'ve got an encoder that only has character ids as input.  We don\'t curently handle\n            # this case, and it\'s not clear it\'s worth it to implement it.  We\'ll at least give a\n            # nicer error than some pytorch dimension mismatch.\n            raise NotImplementedError(\n                ""You are using a character-level indexer with no other indexers. This case is not ""\n                ""currently supported for hotflip. If you would really like to see us support ""\n                ""this, please open an issue on github.""\n            )\n        if token_idx >= self.embedding_matrix.size(0):\n            # This happens when we\'ve truncated our fake embedding matrix.  We need to do a dot\n            # product with the word vector of the current token; if that token is out of\n            # vocabulary for our truncated matrix, we need to run it through the embedding layer.\n            inputs = self._make_embedder_input([self.vocab.get_token_from_index(token_idx)])\n            word_embedding = self.embedding_layer(inputs)[0]\n        else:\n            word_embedding = torch.nn.functional.embedding(\n                util.move_to_device(torch.LongTensor([token_idx]), self.cuda_device),\n                self.embedding_matrix,\n            )\n        word_embedding = word_embedding.detach().unsqueeze(0)\n        grad = grad.unsqueeze(0).unsqueeze(0)\n        # solves equation (3) here https://arxiv.org/abs/1903.06620\n        new_embed_dot_grad = torch.einsum(""bij,kj->bik"", (grad, self.embedding_matrix))\n        prev_embed_dot_grad = torch.einsum(""bij,bij->bi"", (grad, word_embedding)).unsqueeze(-1)\n        neg_dir_dot_grad = sign * (prev_embed_dot_grad - new_embed_dot_grad)\n        neg_dir_dot_grad = neg_dir_dot_grad.detach().cpu().numpy()\n        # Do not replace with non-alphanumeric tokens\n        neg_dir_dot_grad[:, :, self.invalid_replacement_indices] = -numpy.inf\n        best_at_each_step = neg_dir_dot_grad.argmax(2)\n        return best_at_each_step[0].data[0]\n'"
allennlp/interpret/attackers/input_reduction.py,1,"b'from copy import deepcopy\nfrom typing import List, Tuple\nimport heapq\n\nimport numpy as np\nimport torch\n\nfrom allennlp.common.util import JsonDict, sanitize\nfrom allennlp.data import Instance\nfrom allennlp.data.fields import TextField, SequenceLabelField\nfrom allennlp.interpret.attackers import utils\nfrom allennlp.interpret.attackers.attacker import Attacker\nfrom allennlp.predictors import Predictor\n\n\n@Attacker.register(""input-reduction"")\nclass InputReduction(Attacker):\n    """"""\n    Runs the input reduction method from [Pathologies of Neural Models Make Interpretations\n    Difficult](https://arxiv.org/abs/1804.07781), which removes as many words as possible from\n    the input without changing the model\'s prediction.\n\n    The functions on this class handle a special case for NER by looking for a field called ""tags""\n    This check is brittle, i.e., the code could break if the name of this field has changed, or if\n    a non-NER model has a field called ""tags"".\n\n    Registered as an `Attacker` with name ""input-reduction"".\n    """"""\n\n    def __init__(self, predictor: Predictor, beam_size: int = 3) -> None:\n        super().__init__(predictor)\n        self.beam_size = beam_size\n\n    def attack_from_json(\n        self,\n        inputs: JsonDict = None,\n        input_field_to_attack: str = ""tokens"",\n        grad_input_field: str = ""grad_input_1"",\n        ignore_tokens: List[str] = None,\n        target: JsonDict = None,\n    ):\n        if target is not None:\n            raise ValueError(""Input reduction does not implement targeted attacks"")\n        ignore_tokens = [""@@NULL@@""] if ignore_tokens is None else ignore_tokens\n        original_instances = self.predictor.json_to_labeled_instances(inputs)\n        original_text_field: TextField = original_instances[0][  # type: ignore\n            input_field_to_attack\n        ]\n        original_tokens = deepcopy(original_text_field.tokens)\n        final_tokens = []\n        for instance in original_instances:\n            final_tokens.append(\n                self._attack_instance(\n                    inputs, instance, input_field_to_attack, grad_input_field, ignore_tokens\n                )\n            )\n        return sanitize({""final"": final_tokens, ""original"": original_tokens})\n\n    def _attack_instance(\n        self,\n        inputs: JsonDict,\n        instance: Instance,\n        input_field_to_attack: str,\n        grad_input_field: str,\n        ignore_tokens: List[str],\n    ):\n        # Save fields that must be checked for equality\n        fields_to_compare = utils.get_fields_to_compare(inputs, instance, input_field_to_attack)\n\n        # Set num_ignore_tokens, which tells input reduction when to stop\n        # We keep at least one token for input reduction on classification/entailment/etc.\n        if ""tags"" not in instance:\n            num_ignore_tokens = 1\n            tag_mask = None\n\n        # Set num_ignore_tokens for NER and build token mask\n        else:\n            num_ignore_tokens, tag_mask, original_tags = _get_ner_tags_and_mask(\n                instance, input_field_to_attack, ignore_tokens\n            )\n\n        text_field: TextField = instance[input_field_to_attack]  # type: ignore\n        current_tokens = deepcopy(text_field.tokens)\n        candidates = [(instance, -1, tag_mask)]\n        # keep removing tokens until prediction is about to change\n        while len(current_tokens) > num_ignore_tokens and candidates:\n            # sort current candidates by smallest length (we want to remove as many tokens as possible)\n            def get_length(input_instance: Instance):\n                input_text_field: TextField = input_instance[input_field_to_attack]  # type: ignore\n                return len(input_text_field.tokens)\n\n            candidates = heapq.nsmallest(self.beam_size, candidates, key=lambda x: get_length(x[0]))\n\n            beam_candidates = deepcopy(candidates)\n            candidates = []\n            for beam_instance, smallest_idx, tag_mask in beam_candidates:\n                # get gradients and predictions\n                beam_tag_mask = deepcopy(tag_mask)\n                grads, outputs = self.predictor.get_gradients([beam_instance])\n\n                for output in outputs:\n                    if isinstance(outputs[output], torch.Tensor):\n                        outputs[output] = outputs[output].detach().cpu().numpy().squeeze().squeeze()\n                    elif isinstance(outputs[output], list):\n                        outputs[output] = outputs[output][0]\n\n                # Check if any fields have changed, if so, next beam\n                if ""tags"" not in instance:\n                    # relabel beam_instance since last iteration removed an input token\n                    beam_instance = self.predictor.predictions_to_labeled_instances(\n                        beam_instance, outputs\n                    )[0]\n                    if utils.instance_has_changed(beam_instance, fields_to_compare):\n                        continue\n\n                # special case for sentence tagging (we have tested NER)\n                else:\n                    # remove the mask where you remove the input token from.\n                    if smallest_idx != -1:  # Don\'t delete on the very first iteration\n                        del beam_tag_mask[smallest_idx]\n                    cur_tags = [\n                        outputs[""tags""][x] for x in range(len(outputs[""tags""])) if beam_tag_mask[x]\n                    ]\n                    if cur_tags != original_tags:\n                        continue\n\n                # remove a token from the input\n                text_field: TextField = beam_instance[input_field_to_attack]  # type: ignore\n                current_tokens = deepcopy(text_field.tokens)\n                reduced_instances_and_smallest = _remove_one_token(\n                    beam_instance,\n                    input_field_to_attack,\n                    grads[grad_input_field][0],\n                    ignore_tokens,\n                    self.beam_size,\n                    beam_tag_mask,\n                )\n                candidates.extend(reduced_instances_and_smallest)\n        return current_tokens\n\n\ndef _remove_one_token(\n    instance: Instance,\n    input_field_to_attack: str,\n    grads: np.ndarray,\n    ignore_tokens: List[str],\n    beam_size: int,\n    tag_mask: List[int],\n) -> List[Tuple[Instance, int, List[int]]]:\n    """"""\n    Finds the token with the smallest gradient and removes it.\n    """"""\n    # Compute L2 norm of all grads.\n    grads_mag = [np.sqrt(grad.dot(grad)) for grad in grads]\n\n    # Skip all ignore_tokens by setting grad to infinity\n    text_field: TextField = instance[input_field_to_attack]  # type: ignore\n    for token_idx, token in enumerate(text_field.tokens):\n        if token in ignore_tokens:\n            grads_mag[token_idx] = float(""inf"")\n\n    # For NER, skip all tokens that are not in outside\n    if ""tags"" in instance:\n        tag_field: SequenceLabelField = instance[""tags""]  # type: ignore\n        labels: List[str] = tag_field.labels  # type: ignore\n        for idx, label in enumerate(labels):\n            if label != ""O"":\n                grads_mag[idx] = float(""inf"")\n    reduced_instances_and_smallest: List[Tuple[Instance, int, List[int]]] = []\n    for _ in range(beam_size):\n        # copy instance and edit later\n        copied_instance = deepcopy(instance)\n        copied_text_field: TextField = copied_instance[input_field_to_attack]  # type: ignore\n\n        # find smallest\n        smallest = np.argmin(grads_mag)\n        if grads_mag[smallest] == float(""inf""):  # if all are ignored tokens, return.\n            break\n        grads_mag[smallest] = float(""inf"")  # so the other beams don\'t use this token\n\n        # remove smallest\n        inputs_before_smallest = copied_text_field.tokens[0:smallest]\n        inputs_after_smallest = copied_text_field.tokens[smallest + 1 :]\n        copied_text_field.tokens = inputs_before_smallest + inputs_after_smallest\n\n        if ""tags"" in instance:\n            tag_field: SequenceLabelField = copied_instance[""tags""]  # type: ignore\n            tag_field_before_smallest = tag_field.labels[0:smallest]\n            tag_field_after_smallest = tag_field.labels[smallest + 1 :]\n            tag_field.labels = tag_field_before_smallest + tag_field_after_smallest  # type: ignore\n            tag_field.sequence_field = copied_text_field\n\n        copied_instance.indexed = False\n        reduced_instances_and_smallest.append((copied_instance, smallest, tag_mask))\n\n    return reduced_instances_and_smallest\n\n\ndef _get_ner_tags_and_mask(\n    instance: Instance, input_field_to_attack: str, ignore_tokens: List[str]\n):\n    """"""\n    Used for the NER task. Sets the num_ignore tokens, saves the original predicted tag and a 0/1\n    mask in the position of the tags\n    """"""\n    # Set num_ignore_tokens\n    num_ignore_tokens = 0\n    input_field: TextField = instance[input_field_to_attack]  # type: ignore\n    for token in input_field.tokens:\n        if str(token) in ignore_tokens:\n            num_ignore_tokens += 1\n\n    # save the original tags and a 0/1 mask where the tags are\n    tag_mask = []\n    original_tags = []\n    tag_field: SequenceLabelField = instance[""tags""]  # type: ignore\n    for label in tag_field.labels:\n        if label != ""O"":\n            tag_mask.append(1)\n            original_tags.append(label)\n            num_ignore_tokens += 1\n        else:\n            tag_mask.append(0)\n    return num_ignore_tokens, tag_mask, original_tags\n'"
allennlp/interpret/attackers/utils.py,0,"b'from allennlp.common.util import JsonDict\nfrom allennlp.data import Instance\n\n\ndef get_fields_to_compare(\n    inputs: JsonDict, instance: Instance, input_field_to_attack: str\n) -> JsonDict:\n    """"""\n    Gets a list of the fields that should be checked for equality after an attack is performed.\n\n    # Parameters\n\n    inputs : `JsonDict`\n        The input you want to attack, similar to the argument to a Predictor, e.g., predict_json().\n    instance : `Instance`\n        A labeled instance that is output from json_to_labeled_instances().\n    input_field_to_attack : `str`\n        The key in the inputs JsonDict you want to attack, e.g., tokens.\n\n    # Returns\n\n    fields : `JsonDict`\n        The fields that must be compared for equality.\n    """"""\n    # TODO(mattg): this really should live on the Predictor.  We have some messy stuff for, e.g.,\n    # reading comprehension models, and the interpret code can\'t really know about the internals of\n    # that (or at least it shouldn\'t now, and once we split out the reading comprehension repo, it\n    # really *can\'t*).\n    fields_to_compare = {\n        key: instance[key]\n        for key in instance.fields\n        if key not in inputs\n        and key != input_field_to_attack\n        and key != ""metadata""\n        and key != ""output""\n    }\n    return fields_to_compare\n\n\ndef instance_has_changed(instance: Instance, fields_to_compare: JsonDict):\n    if ""clusters"" in fields_to_compare:\n        # Coref needs a special case here, apparently.  I (mattg) am not sure why the check below\n        # doesn\'t catch this case; TODO: look into this.\n        original_clusters = set(tuple(x) for x in fields_to_compare[""clusters""])\n        new_clusters = set(tuple(x) for x in instance[""clusters""])  # type: ignore\n        return original_clusters != new_clusters\n    if any(instance[field] != fields_to_compare[field] for field in fields_to_compare):\n        return True\n    return False\n'"
allennlp/interpret/saliency_interpreters/__init__.py,0,b'from allennlp.interpret.saliency_interpreters.saliency_interpreter import SaliencyInterpreter\nfrom allennlp.interpret.saliency_interpreters.simple_gradient import SimpleGradient\nfrom allennlp.interpret.saliency_interpreters.integrated_gradient import IntegratedGradient\nfrom allennlp.interpret.saliency_interpreters.smooth_gradient import SmoothGradient\n'
allennlp/interpret/saliency_interpreters/integrated_gradient.py,0,"b'import math\nfrom typing import List, Dict, Any\n\nimport numpy\n\nfrom allennlp.common.util import JsonDict, sanitize\nfrom allennlp.data import Instance\nfrom allennlp.interpret.saliency_interpreters.saliency_interpreter import SaliencyInterpreter\nfrom allennlp.nn import util\n\n\n@SaliencyInterpreter.register(""integrated-gradient"")\nclass IntegratedGradient(SaliencyInterpreter):\n    """"""\n    Interprets the prediction using Integrated Gradients (https://arxiv.org/abs/1703.01365)\n\n    Registered as a `SaliencyInterpreter` with name ""integrated-gradient"".\n    """"""\n\n    def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:\n        # Convert inputs to labeled instances\n        labeled_instances = self.predictor.json_to_labeled_instances(inputs)\n\n        instances_with_grads = dict()\n        for idx, instance in enumerate(labeled_instances):\n            # Run integrated gradients\n            grads = self._integrate_gradients(instance)\n\n            # Normalize results\n            for key, grad in grads.items():\n                # The [0] here is undo-ing the batching that happens in get_gradients.\n                embedding_grad = numpy.sum(grad[0], axis=1)\n                norm = numpy.linalg.norm(embedding_grad, ord=1)\n                normalized_grad = [math.fabs(e) / norm for e in embedding_grad]\n                grads[key] = normalized_grad\n\n            instances_with_grads[""instance_"" + str(idx + 1)] = grads\n\n        return sanitize(instances_with_grads)\n\n    def _register_forward_hook(self, alpha: int, embeddings_list: List):\n        """"""\n        Register a forward hook on the embedding layer which scales the embeddings by alpha. Used\n        for one term in the Integrated Gradients sum.\n\n        We store the embedding output into the embeddings_list when alpha is zero.  This is used\n        later to element-wise multiply the input by the averaged gradients.\n        """"""\n\n        def forward_hook(module, inputs, output):\n            # Save the input for later use. Only do so on first call.\n            if alpha == 0:\n                embeddings_list.append(output.squeeze(0).clone().detach().numpy())\n\n            # Scale the embedding by alpha\n            output.mul_(alpha)\n\n        # Register the hook\n        embedding_layer = util.find_embedding_layer(self.predictor._model)\n        handle = embedding_layer.register_forward_hook(forward_hook)\n        return handle\n\n    def _integrate_gradients(self, instance: Instance) -> Dict[str, numpy.ndarray]:\n        """"""\n        Returns integrated gradients for the given [`Instance`](../../data/instance.md)\n        """"""\n        ig_grads: Dict[str, Any] = {}\n\n        # List of Embedding inputs\n        embeddings_list: List[numpy.ndarray] = []\n\n        # Use 10 terms in the summation approximation of the integral in integrated grad\n        steps = 10\n\n        # Exclude the endpoint because we do a left point integral approximation\n        for alpha in numpy.linspace(0, 1.0, num=steps, endpoint=False):\n            # Hook for modifying embedding value\n            handle = self._register_forward_hook(alpha, embeddings_list)\n\n            grads = self.predictor.get_gradients([instance])[0]\n            handle.remove()\n\n            # Running sum of gradients\n            if ig_grads == {}:\n                ig_grads = grads\n            else:\n                for key in grads.keys():\n                    ig_grads[key] += grads[key]\n\n        # Average of each gradient term\n        for key in ig_grads.keys():\n            ig_grads[key] /= steps\n\n        # Gradients come back in the reverse order that they were sent into the network\n        embeddings_list.reverse()\n\n        # Element-wise multiply average gradient by the input\n        for idx, input_embedding in enumerate(embeddings_list):\n            key = ""grad_input_"" + str(idx + 1)\n            ig_grads[key] *= input_embedding\n\n        return ig_grads\n'"
allennlp/interpret/saliency_interpreters/saliency_interpreter.py,0,"b'from allennlp.common import Registrable\nfrom allennlp.common.util import JsonDict\nfrom allennlp.predictors import Predictor\n\n\nclass SaliencyInterpreter(Registrable):\n    """"""\n    A `SaliencyInterpreter` interprets an AllenNLP Predictor\'s outputs by assigning a saliency\n    score to each input token.\n    """"""\n\n    def __init__(self, predictor: Predictor) -> None:\n        self.predictor = predictor\n\n    def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:\n        """"""\n        This function finds saliency values for each input token.\n\n        # Parameters\n\n        inputs : `JsonDict`\n            The input you want to interpret (the same as the argument to a Predictor, e.g., predict_json()).\n\n        # Returns\n\n        interpretation : `JsonDict`\n            Contains the normalized saliency values for each input token. The dict has entries for\n            each instance in the inputs JsonDict, e.g., `{instance_1: ..., instance_2:, ... }`.\n            Each one of those entries has entries for the saliency of the inputs, e.g.,\n            `{grad_input_1: ..., grad_input_2: ... }`.\n        """"""\n        raise NotImplementedError(""Implement this for saliency interpretations"")\n'"
allennlp/interpret/saliency_interpreters/simple_gradient.py,0,"b'import math\n\nfrom typing import List\nimport numpy\n\nfrom allennlp.common.util import JsonDict, sanitize\nfrom allennlp.interpret.saliency_interpreters.saliency_interpreter import SaliencyInterpreter\nfrom allennlp.nn import util\n\n\n@SaliencyInterpreter.register(""simple-gradient"")\nclass SimpleGradient(SaliencyInterpreter):\n    """"""\n    Registered as a `SaliencyInterpreter` with name ""simple-gradient"".\n    """"""\n\n    def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:\n        """"""\n        Interprets the model\'s prediction for inputs.  Gets the gradients of the loss with respect\n        to the input and returns those gradients normalized and sanitized.\n        """"""\n        labeled_instances = self.predictor.json_to_labeled_instances(inputs)\n\n        # List of embedding inputs, used for multiplying gradient by the input for normalization\n        embeddings_list: List[numpy.ndarray] = []\n\n        instances_with_grads = dict()\n        for idx, instance in enumerate(labeled_instances):\n            # Hook used for saving embeddings\n            handle = self._register_forward_hook(embeddings_list)\n            grads = self.predictor.get_gradients([instance])[0]\n            handle.remove()\n\n            # Gradients come back in the reverse order that they were sent into the network\n            embeddings_list.reverse()\n            for key, grad in grads.items():\n                # Get number at the end of every gradient key (they look like grad_input_[int],\n                # we\'re getting this [int] part and subtracting 1 for zero-based indexing).\n                # This is then used as an index into the reversed input array to match up the\n                # gradient and its respective embedding.\n                input_idx = int(key[-1]) - 1\n                # The [0] here is undo-ing the batching that happens in get_gradients.\n                emb_grad = numpy.sum(grad[0] * embeddings_list[input_idx], axis=1)\n                norm = numpy.linalg.norm(emb_grad, ord=1)\n                normalized_grad = [math.fabs(e) / norm for e in emb_grad]\n                grads[key] = normalized_grad\n\n            instances_with_grads[""instance_"" + str(idx + 1)] = grads\n        return sanitize(instances_with_grads)\n\n    def _register_forward_hook(self, embeddings_list: List):\n        """"""\n        Finds all of the TextFieldEmbedders, and registers a forward hook onto them. When forward()\n        is called, embeddings_list is filled with the embedding values. This is necessary because\n        our normalization scheme multiplies the gradient by the embedding value.\n        """"""\n\n        def forward_hook(module, inputs, output):\n            embeddings_list.append(output.squeeze(0).clone().detach().numpy())\n\n        embedding_layer = util.find_embedding_layer(self.predictor._model)\n        handle = embedding_layer.register_forward_hook(forward_hook)\n\n        return handle\n'"
allennlp/interpret/saliency_interpreters/smooth_gradient.py,1,"b'import math\nfrom typing import Dict, Any\n\nimport numpy\nimport torch\n\nfrom allennlp.common.util import JsonDict, sanitize\nfrom allennlp.data import Instance\nfrom allennlp.interpret.saliency_interpreters.saliency_interpreter import SaliencyInterpreter\nfrom allennlp.predictors import Predictor\nfrom allennlp.nn import util\n\n\n@SaliencyInterpreter.register(""smooth-gradient"")\nclass SmoothGradient(SaliencyInterpreter):\n    """"""\n    Interprets the prediction using SmoothGrad (https://arxiv.org/abs/1706.03825)\n\n    Registered as a `SaliencyInterpreter` with name ""smooth-gradient"".\n    """"""\n\n    def __init__(self, predictor: Predictor) -> None:\n        super().__init__(predictor)\n        # Hyperparameters\n        self.stdev = 0.01\n        self.num_samples = 10\n\n    def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:\n        # Convert inputs to labeled instances\n        labeled_instances = self.predictor.json_to_labeled_instances(inputs)\n\n        instances_with_grads = dict()\n        for idx, instance in enumerate(labeled_instances):\n            # Run smoothgrad\n            grads = self._smooth_grads(instance)\n\n            # Normalize results\n            for key, grad in grads.items():\n                # TODO (@Eric-Wallace), SmoothGrad is not using times input normalization.\n                # Fine for now, but should fix for consistency.\n\n                # The [0] here is undo-ing the batching that happens in get_gradients.\n                embedding_grad = numpy.sum(grad[0], axis=1)\n                norm = numpy.linalg.norm(embedding_grad, ord=1)\n                normalized_grad = [math.fabs(e) / norm for e in embedding_grad]\n                grads[key] = normalized_grad\n\n            instances_with_grads[""instance_"" + str(idx + 1)] = grads\n\n        return sanitize(instances_with_grads)\n\n    def _register_forward_hook(self, stdev: float):\n        """"""\n        Register a forward hook on the embedding layer which adds random noise to every embedding.\n        Used for one term in the SmoothGrad sum.\n        """"""\n\n        def forward_hook(module, inputs, output):\n            # Random noise = N(0, stdev * (max-min))\n            scale = output.detach().max() - output.detach().min()\n            noise = torch.randn(output.shape).to(output.device) * stdev * scale\n\n            # Add the random noise\n            output.add_(noise)\n\n        # Register the hook\n        embedding_layer = util.find_embedding_layer(self.predictor._model)\n        handle = embedding_layer.register_forward_hook(forward_hook)\n        return handle\n\n    def _smooth_grads(self, instance: Instance) -> Dict[str, numpy.ndarray]:\n        total_gradients: Dict[str, Any] = {}\n        for _ in range(self.num_samples):\n            handle = self._register_forward_hook(self.stdev)\n            grads = self.predictor.get_gradients([instance])[0]\n            handle.remove()\n\n            # Sum gradients\n            if total_gradients == {}:\n                total_gradients = grads\n            else:\n                for key in grads.keys():\n                    total_gradients[key] += grads[key]\n\n        # Average the gradients\n        for key in total_gradients.keys():\n            total_gradients[key] /= self.num_samples\n\n        return total_gradients\n'"
allennlp/modules/attention/__init__.py,0,b'from allennlp.modules.attention.attention import Attention\nfrom allennlp.modules.attention.bilinear_attention import BilinearAttention\nfrom allennlp.modules.attention.additive_attention import AdditiveAttention\nfrom allennlp.modules.attention.cosine_attention import CosineAttention\nfrom allennlp.modules.attention.dot_product_attention import DotProductAttention\nfrom allennlp.modules.attention.linear_attention import LinearAttention\n'
allennlp/modules/attention/additive_attention.py,9,"b'from overrides import overrides\nimport torch\nfrom torch.nn.parameter import Parameter\n\nfrom allennlp.modules.attention.attention import Attention\n\n\n@Attention.register(""additive"")\nclass AdditiveAttention(Attention):\n    """"""\n    Computes attention between a vector and a matrix using an additive attention function.  This\n    function has two matrices `W`, `U` and a vector `V`. The similarity between the vector\n    `x` and the matrix `y` is computed as `V tanh(Wx + Uy)`.\n\n    This attention is often referred as concat or additive attention. It was introduced in\n    <https://arxiv.org/abs/1409.0473> by Bahdanau et al.\n\n    Registered as an `Attention` with name ""additive"".\n\n    # Parameters\n\n    vector_dim : `int`, required\n        The dimension of the vector, `x`, described above.  This is `x.size()[-1]` - the length\n        of the vector that will go into the similarity computation.  We need this so we can build\n        the weight matrix correctly.\n    matrix_dim : `int`, required\n        The dimension of the matrix, `y`, described above.  This is `y.size()[-1]` - the length\n        of the vector that will go into the similarity computation.  We need this so we can build\n        the weight matrix correctly.\n    normalize : `bool`, optional (default = `True`)\n        If true, we normalize the computed similarities with a softmax, to return a probability\n        distribution for your attention.  If false, this is just computing a similarity score.\n    """"""\n\n    def __init__(self, vector_dim: int, matrix_dim: int, normalize: bool = True) -> None:\n        super().__init__(normalize)\n        self._w_matrix = Parameter(torch.Tensor(vector_dim, vector_dim))\n        self._u_matrix = Parameter(torch.Tensor(matrix_dim, vector_dim))\n        self._v_vector = Parameter(torch.Tensor(vector_dim, 1))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.xavier_uniform_(self._w_matrix)\n        torch.nn.init.xavier_uniform_(self._u_matrix)\n        torch.nn.init.xavier_uniform_(self._v_vector)\n\n    @overrides\n    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:\n        intermediate = vector.matmul(self._w_matrix).unsqueeze(1) + matrix.matmul(self._u_matrix)\n        intermediate = torch.tanh(intermediate)\n        return intermediate.matmul(self._v_vector).squeeze(2)\n'"
allennlp/modules/attention/attention.py,4,"b'""""""\nAn *attention* module that computes the similarity between\nan input vector and the rows of a matrix.\n""""""\n\nimport torch\n\nfrom overrides import overrides\nfrom allennlp.common.registrable import Registrable\nfrom allennlp.nn.util import masked_softmax\n\n\nclass Attention(torch.nn.Module, Registrable):\n    """"""\n    An `Attention` takes two inputs: a (batched) vector and a matrix, plus an optional mask on the\n    rows of the matrix.  We compute the similarity between the vector and each row in the matrix,\n    and then (optionally) perform a softmax over rows using those computed similarities.\n\n\n    Inputs:\n\n    - vector: shape `(batch_size, embedding_dim)`\n    - matrix: shape `(batch_size, num_rows, embedding_dim)`\n    - matrix_mask: shape `(batch_size, num_rows)`, specifying which rows are just padding.\n\n    Output:\n\n    - attention: shape `(batch_size, num_rows)`.\n\n    # Parameters\n\n    normalize : `bool`, optional (default = `True`)\n        If true, we normalize the computed similarities with a softmax, to return a probability\n        distribution for your attention.  If false, this is just computing a similarity score.\n    """"""\n\n    def __init__(self, normalize: bool = True) -> None:\n        super().__init__()\n        self._normalize = normalize\n\n    @overrides\n    def forward(\n        self, vector: torch.Tensor, matrix: torch.Tensor, matrix_mask: torch.BoolTensor = None\n    ) -> torch.Tensor:\n        similarities = self._forward_internal(vector, matrix)\n        if self._normalize:\n            return masked_softmax(similarities, matrix_mask)\n        else:\n            return similarities\n\n    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:\n        raise NotImplementedError\n'"
allennlp/modules/attention/bilinear_attention.py,5,"b'from overrides import overrides\nimport torch\nfrom torch.nn.parameter import Parameter\n\nfrom allennlp.modules.attention.attention import Attention\nfrom allennlp.nn import Activation\n\n\n@Attention.register(""bilinear"")\nclass BilinearAttention(Attention):\n    """"""\n    Computes attention between a vector and a matrix using a bilinear attention function.  This\n    function has a matrix of weights `W` and a bias `b`, and the similarity between the vector\n    `x` and the matrix `y` is computed as `x^T W y + b`.\n\n    Registered as an `Attention` with name ""bilinear"".\n\n    # Parameters\n\n    vector_dim : `int`, required\n        The dimension of the vector, `x`, described above.  This is `x.size()[-1]` - the length\n        of the vector that will go into the similarity computation.  We need this so we can build\n        the weight matrix correctly.\n    matrix_dim : `int`, required\n        The dimension of the matrix, `y`, described above.  This is `y.size()[-1]` - the length\n        of the vector that will go into the similarity computation.  We need this so we can build\n        the weight matrix correctly.\n    activation : `Activation`, optional (default=`linear`)\n        An activation function applied after the `x^T W y + b` calculation.  Default is\n        linear, i.e. no activation.\n    normalize : `bool`, optional (default=`True`)\n        If true, we normalize the computed similarities with a softmax, to return a probability\n        distribution for your attention.  If false, this is just computing a similarity score.\n    """"""\n\n    def __init__(\n        self,\n        vector_dim: int,\n        matrix_dim: int,\n        activation: Activation = None,\n        normalize: bool = True,\n    ) -> None:\n        super().__init__(normalize)\n        self._weight_matrix = Parameter(torch.Tensor(vector_dim, matrix_dim))\n        self._bias = Parameter(torch.Tensor(1))\n        self._activation = activation or Activation.by_name(""linear"")()\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.xavier_uniform_(self._weight_matrix)\n        self._bias.data.fill_(0)\n\n    @overrides\n    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:\n        intermediate = vector.mm(self._weight_matrix).unsqueeze(1)\n        return self._activation(intermediate.bmm(matrix.transpose(1, 2)).squeeze(1) + self._bias)\n'"
allennlp/modules/attention/cosine_attention.py,2,"b'import torch\nfrom overrides import overrides\nfrom allennlp.modules.attention.attention import Attention\nfrom allennlp.nn import util\n\n\n@Attention.register(""cosine"")\nclass CosineAttention(Attention):\n    """"""\n    Computes attention between a vector and a matrix using cosine similarity.\n\n    Registered as an `Attention` with name ""cosine"".\n    """"""\n\n    @overrides\n    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:\n        a_norm = vector / (\n            vector.norm(p=2, dim=-1, keepdim=True) + util.tiny_value_of_dtype(vector.dtype)\n        )\n        b_norm = matrix / (\n            matrix.norm(p=2, dim=-1, keepdim=True) + util.tiny_value_of_dtype(matrix.dtype)\n        )\n        return torch.bmm(a_norm.unsqueeze(dim=1), b_norm.transpose(-1, -2)).squeeze(1)\n'"
allennlp/modules/attention/dot_product_attention.py,1,"b'import torch\nfrom overrides import overrides\nfrom allennlp.modules.attention.attention import Attention\n\n\n@Attention.register(""dot_product"")\nclass DotProductAttention(Attention):\n    """"""\n    Computes attention between a vector and a matrix using dot product.\n\n    Registered as an `Attention` with name ""dot_product"".\n    """"""\n\n    @overrides\n    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:\n        return matrix.bmm(vector.unsqueeze(-1)).squeeze(-1)\n'"
allennlp/modules/attention/linear_attention.py,4,"b'import math\n\nimport torch\nfrom torch.nn import Parameter\nfrom overrides import overrides\nfrom allennlp.modules.attention.attention import Attention\nfrom allennlp.nn import util\nfrom allennlp.nn.activations import Activation\n\n\n@Attention.register(""linear"")\nclass LinearAttention(Attention):\n    """"""\n    This `Attention` module performs a dot product between a vector of weights and some\n    combination of the two input vectors, followed by an (optional) activation function.  The\n    combination used is configurable.\n\n    If the two vectors are `x` and `y`, we allow the following kinds of combinations : `x`,\n    `y`, `x*y`, `x+y`, `x-y`, `x/y`, where each of those binary operations is performed\n    elementwise.  You can list as many combinations as you want, comma separated.  For example, you\n    might give `x,y,x*y` as the `combination` parameter to this class.  The computed similarity\n    function would then be `w^T [x; y; x*y] + b`, where `w` is a vector of weights, `b` is a\n    bias parameter, and `[;]` is vector concatenation.\n\n    Note that if you want a bilinear similarity function with a diagonal weight matrix W, where the\n    similarity function is computed as `x * w * y + b` (with `w` the diagonal of `W`), you can\n    accomplish that with this class by using ""x*y"" for `combination`.\n\n    Registered as an `Attention` with name ""linear"".\n\n    # Parameters\n\n    tensor_1_dim : `int`, required\n        The dimension of the first tensor, `x`, described above.  This is `x.size()[-1]` - the\n        length of the vector that will go into the similarity computation.  We need this so we can\n        build weight vectors correctly.\n    tensor_2_dim : `int`, required\n        The dimension of the second tensor, `y`, described above.  This is `y.size()[-1]` - the\n        length of the vector that will go into the similarity computation.  We need this so we can\n        build weight vectors correctly.\n    combination : `str`, optional (default=`""x,y""`)\n        Described above.\n    activation : `Activation`, optional (default=`linear`)\n        An activation function applied after the `w^T * [x;y] + b` calculation.  Default is\n        linear, i.e. no activation.\n    normalize : `bool`, optional (default=`True`)\n    """"""\n\n    def __init__(\n        self,\n        tensor_1_dim: int,\n        tensor_2_dim: int,\n        combination: str = ""x,y"",\n        activation: Activation = None,\n        normalize: bool = True,\n    ) -> None:\n        super().__init__(normalize)\n        self._combination = combination\n        combined_dim = util.get_combined_dim(combination, [tensor_1_dim, tensor_2_dim])\n        self._weight_vector = Parameter(torch.Tensor(combined_dim))\n        self._bias = Parameter(torch.Tensor(1))\n        self._activation = activation or Activation.by_name(""linear"")()\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        std = math.sqrt(6 / (self._weight_vector.size(0) + 1))\n        self._weight_vector.data.uniform_(-std, std)\n        self._bias.data.fill_(0)\n\n    @overrides\n    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:\n        combined_tensors = util.combine_tensors_and_multiply(\n            self._combination, [vector.unsqueeze(1), matrix], self._weight_vector\n        )\n        return self._activation(combined_tensors.squeeze(1) + self._bias)\n'"
allennlp/modules/matrix_attention/__init__.py,0,b'from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\nfrom allennlp.modules.matrix_attention.bilinear_matrix_attention import BilinearMatrixAttention\nfrom allennlp.modules.matrix_attention.cosine_matrix_attention import CosineMatrixAttention\nfrom allennlp.modules.matrix_attention.dot_product_matrix_attention import DotProductMatrixAttention\nfrom allennlp.modules.matrix_attention.linear_matrix_attention import LinearMatrixAttention\n'
allennlp/modules/matrix_attention/bilinear_matrix_attention.py,11,"b'from overrides import overrides\nimport torch\nfrom torch.nn.parameter import Parameter\n\nfrom allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\nfrom allennlp.nn import Activation\n\n\n@MatrixAttention.register(""bilinear"")\nclass BilinearMatrixAttention(MatrixAttention):\n    """"""\n    Computes attention between two matrices using a bilinear attention function.  This function has\n    a matrix of weights `W` and a bias `b`, and the similarity between the two matrices `X`\n    and `Y` is computed as `X W Y^T + b`.\n\n    Registered as a `MatrixAttention` with name ""bilinear"".\n\n    # Parameters\n\n    matrix_1_dim : `int`, required\n        The dimension of the matrix `X`, described above.  This is `X.size()[-1]` - the length\n        of the vector that will go into the similarity computation.  We need this so we can build\n        the weight matrix correctly.\n    matrix_2_dim : `int`, required\n        The dimension of the matrix `Y`, described above.  This is `Y.size()[-1]` - the length\n        of the vector that will go into the similarity computation.  We need this so we can build\n        the weight matrix correctly.\n    activation : `Activation`, optional (default=`linear`)\n        An activation function applied after the `X W Y^T + b` calculation.  Default is\n        linear, i.e. no activation.\n    use_input_biases : `bool`, optional (default = `False`)\n        If True, we add biases to the inputs such that the final computation\n        is equivalent to the original bilinear matrix multiplication plus a\n        projection of both inputs.\n    label_dim : `int`, optional (default = `1`)\n        The number of output classes. Typically in an attention setting this will be one,\n        but this parameter allows this class to function as an equivalent to `torch.nn.Bilinear`\n        for matrices, rather than vectors.\n    """"""\n\n    def __init__(\n        self,\n        matrix_1_dim: int,\n        matrix_2_dim: int,\n        activation: Activation = None,\n        use_input_biases: bool = False,\n        label_dim: int = 1,\n    ) -> None:\n        super().__init__()\n        if use_input_biases:\n            matrix_1_dim += 1\n            matrix_2_dim += 1\n\n        if label_dim == 1:\n            self._weight_matrix = Parameter(torch.Tensor(matrix_1_dim, matrix_2_dim))\n        else:\n            self._weight_matrix = Parameter(torch.Tensor(label_dim, matrix_1_dim, matrix_2_dim))\n\n        self._bias = Parameter(torch.Tensor(1))\n        self._activation = activation or Activation.by_name(""linear"")()\n        self._use_input_biases = use_input_biases\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.xavier_uniform_(self._weight_matrix)\n        self._bias.data.fill_(0)\n\n    @overrides\n    def forward(self, matrix_1: torch.Tensor, matrix_2: torch.Tensor) -> torch.Tensor:\n\n        if self._use_input_biases:\n            bias1 = matrix_1.new_ones(matrix_1.size()[:-1] + (1,))\n            bias2 = matrix_2.new_ones(matrix_2.size()[:-1] + (1,))\n\n            matrix_1 = torch.cat([matrix_1, bias1], -1)\n            matrix_2 = torch.cat([matrix_2, bias2], -1)\n\n        weight = self._weight_matrix\n        if weight.dim() == 2:\n            weight = weight.unsqueeze(0)\n        intermediate = torch.matmul(matrix_1.unsqueeze(1), weight)\n        final = torch.matmul(intermediate, matrix_2.unsqueeze(1).transpose(2, 3))\n        return self._activation(final.squeeze(1) + self._bias)\n'"
allennlp/modules/matrix_attention/cosine_matrix_attention.py,2,"b'import torch\nfrom overrides import overrides\n\nfrom allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\nfrom allennlp.nn import util\n\n\n@MatrixAttention.register(""cosine"")\nclass CosineMatrixAttention(MatrixAttention):\n    """"""\n    Computes attention between every entry in matrix_1 with every entry in matrix_2 using cosine\n    similarity.\n\n    Registered as a `MatrixAttention` with name ""cosine"".\n    """"""\n\n    @overrides\n    def forward(self, matrix_1: torch.Tensor, matrix_2: torch.Tensor) -> torch.Tensor:\n        a_norm = matrix_1 / (\n            matrix_1.norm(p=2, dim=-1, keepdim=True) + util.tiny_value_of_dtype(matrix_1.dtype)\n        )\n        b_norm = matrix_2 / (\n            matrix_2.norm(p=2, dim=-1, keepdim=True) + util.tiny_value_of_dtype(matrix_2.dtype)\n        )\n        return torch.bmm(a_norm, b_norm.transpose(-1, -2))\n'"
allennlp/modules/matrix_attention/dot_product_matrix_attention.py,1,"b'import torch\nfrom overrides import overrides\n\nfrom allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\n\n\n@MatrixAttention.register(""dot_product"")\nclass DotProductMatrixAttention(MatrixAttention):\n    """"""\n    Computes attention between every entry in matrix_1 with every entry in matrix_2 using a dot\n    product.\n\n    Registered as a `MatrixAttention` with name ""dot_product"".\n    """"""\n\n    @overrides\n    def forward(self, matrix_1: torch.Tensor, matrix_2: torch.Tensor) -> torch.Tensor:\n        return matrix_1.bmm(matrix_2.transpose(2, 1))\n'"
allennlp/modules/matrix_attention/linear_matrix_attention.py,4,"b'import math\n\nimport torch\nfrom torch.nn import Parameter\nfrom overrides import overrides\n\nfrom allennlp.nn import util\nfrom allennlp.nn.activations import Activation\nfrom allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\n\n\n@MatrixAttention.register(""linear"")\nclass LinearMatrixAttention(MatrixAttention):\n    """"""\n    This `MatrixAttention` takes two matrices as input and returns a matrix of attentions\n    by performing a dot product between a vector of weights and some\n    combination of the two input matrices, followed by an (optional) activation function.  The\n    combination used is configurable.\n\n    If the two vectors are `x` and `y`, we allow the following kinds of combinations : `x`,\n    `y`, `x*y`, `x+y`, `x-y`, `x/y`, where each of those binary operations is performed\n    elementwise.  You can list as many combinations as you want, comma separated.  For example, you\n    might give `x,y,x*y` as the `combination` parameter to this class.  The computed similarity\n    function would then be `w^T [x; y; x*y] + b`, where `w` is a vector of weights, `b` is a\n    bias parameter, and `[;]` is vector concatenation.\n\n    Note that if you want a bilinear similarity function with a diagonal weight matrix W, where the\n    similarity function is computed as `x * w * y + b` (with `w` the diagonal of `W`), you can\n    accomplish that with this class by using ""x*y"" for `combination`.\n\n    Registered as a `MatrixAttention` with name ""linear"".\n\n    # Parameters\n\n    tensor_1_dim : `int`, required\n        The dimension of the first tensor, `x`, described above.  This is `x.size()[-1]` - the\n        length of the vector that will go into the similarity computation.  We need this so we can\n        build weight vectors correctly.\n    tensor_2_dim : `int`, required\n        The dimension of the second tensor, `y`, described above.  This is `y.size()[-1]` - the\n        length of the vector that will go into the similarity computation.  We need this so we can\n        build weight vectors correctly.\n    combination : `str`, optional (default=`""x,y""`)\n        Described above.\n    activation : `Activation`, optional (default=`linear`)\n        An activation function applied after the `w^T * [x;y] + b` calculation.  Default is\n        linear, i.e. no activation.\n    """"""\n\n    def __init__(\n        self,\n        tensor_1_dim: int,\n        tensor_2_dim: int,\n        combination: str = ""x,y"",\n        activation: Activation = None,\n    ) -> None:\n        super().__init__()\n        self._combination = combination\n        combined_dim = util.get_combined_dim(combination, [tensor_1_dim, tensor_2_dim])\n        self._weight_vector = Parameter(torch.Tensor(combined_dim))\n        self._bias = Parameter(torch.Tensor(1))\n        self._activation = activation or Activation.by_name(""linear"")()\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        std = math.sqrt(6 / (self._weight_vector.size(0) + 1))\n        self._weight_vector.data.uniform_(-std, std)\n        self._bias.data.fill_(0)\n\n    @overrides\n    def forward(self, matrix_1: torch.Tensor, matrix_2: torch.Tensor) -> torch.Tensor:\n        combined_tensors = util.combine_tensors_and_multiply(\n            self._combination, [matrix_1.unsqueeze(2), matrix_2.unsqueeze(1)], self._weight_vector\n        )\n        return self._activation(combined_tensors + self._bias)\n'"
allennlp/modules/matrix_attention/matrix_attention.py,2,"b'import torch\n\nfrom allennlp.common.registrable import Registrable\n\n\nclass MatrixAttention(torch.nn.Module, Registrable):\n    """"""\n    `MatrixAttention` takes two matrices as input and returns a matrix of attentions.\n\n    We compute the similarity between each row in each matrix and return unnormalized similarity\n    scores.  Because these scores are unnormalized, we don\'t take a mask as input; it\'s up to the\n    caller to deal with masking properly when this output is used.\n\n    Input:\n        - matrix_1 : `(batch_size, num_rows_1, embedding_dim_1)`\n        - matrix_2 : `(batch_size, num_rows_2, embedding_dim_2)`\n\n    Output:\n        - `(batch_size, num_rows_1, num_rows_2)`\n    """"""\n\n    def forward(self, matrix_1: torch.Tensor, matrix_2: torch.Tensor) -> torch.Tensor:\n        raise NotImplementedError\n'"
allennlp/modules/seq2seq_encoders/__init__.py,3,"b'""""""\nModules that transform a sequence of input vectors\ninto a sequence of output vectors.\nSome are just basic wrappers around existing PyTorch modules,\nothers are AllenNLP modules.\n\nThe available Seq2Seq encoders are\n\n- `""gru""` : https://pytorch.org/docs/master/nn.html#torch.nn.GRU\n- `""lstm""` : https://pytorch.org/docs/master/nn.html#torch.nn.LSTM\n- `""rnn""` : https://pytorch.org/docs/master/nn.html#torch.nn.RNN\n- `""augmented_lstm""` : allennlp.modules.augmented_lstm.AugmentedLstm\n- `""alternating_lstm""` : allennlp.modules.stacked_alternating_lstm.StackedAlternatingLstm\n- `""alternating_highway_lstm""` : allennlp.modules.stacked_alternating_lstm.StackedAlternatingLstm (GPU only)\n- `""stacked_self_attention""` : allennlp.modules.stacked_self_attention.StackedSelfAttentionEncoder\n- `""multi_head_self_attention""` : allennlp.modules.multi_head_self_attention.MultiHeadSelfAttention\n- `""pass_through""` : allennlp.modules.pass_through_encoder.PassThroughEncoder\n- `""feedforward""` : allennlp.modules.feedforward_encoder.FeedforwardEncoder\n- `""pytorch_transformer""` : allennlp.modules.seq2seq_encoders.PytorchTransformer\n""""""\n\nfrom allennlp.modules.seq2seq_encoders.compose_encoder import ComposeEncoder\nfrom allennlp.modules.seq2seq_encoders.feedforward_encoder import FeedForwardEncoder\nfrom allennlp.modules.seq2seq_encoders.gated_cnn_encoder import GatedCnnEncoder\nfrom allennlp.modules.seq2seq_encoders.pass_through_encoder import PassThroughEncoder\nfrom allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper import (\n    AugmentedLstmSeq2SeqEncoder,\n    GruSeq2SeqEncoder,\n    LstmSeq2SeqEncoder,\n    PytorchSeq2SeqWrapper,\n    RnnSeq2SeqEncoder,\n    StackedAlternatingLstmSeq2SeqEncoder,\n    StackedBidirectionalLstmSeq2SeqEncoder,\n)\nfrom allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\nfrom allennlp.modules.seq2seq_encoders.pytorch_transformer_wrapper import PytorchTransformer\n'"
allennlp/modules/seq2seq_encoders/compose_encoder.py,3,"b'from overrides import overrides\nimport torch\nfrom typing import List\n\nfrom allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\n\n\n@Seq2SeqEncoder.register(""compose"")\nclass ComposeEncoder(Seq2SeqEncoder):\n\n    """"""This class can be used to compose several encoders in sequence.\n\n    Among other things, this can be used to add a ""pre-contextualizer"" before a Seq2SeqEncoder.\n\n    Registered as a `Seq2SeqEncoder` with name ""compose"".\n\n    # Parameters\n\n    encoders : `List[Seq2SeqEncoder]`, required.\n        A non-empty list of encoders to compose. The encoders must match in bidirectionality.\n    """"""\n\n    def __init__(self, encoders: List[Seq2SeqEncoder]):\n        super().__init__()\n        self.encoders = encoders\n        for idx, encoder in enumerate(encoders):\n            self.add_module(""encoder%d"" % idx, encoder)\n\n        # Compute bidirectionality.\n        all_bidirectional = all(encoder.is_bidirectional() for encoder in encoders)\n        any_bidirectional = any(encoder.is_bidirectional() for encoder in encoders)\n        self.bidirectional = all_bidirectional\n\n        if all_bidirectional != any_bidirectional:\n            raise ValueError(""All encoders need to match in bidirectionality."")\n\n        if len(self.encoders) < 1:\n            raise ValueError(""Need at least one encoder."")\n\n        last_enc = None\n        for enc in encoders:\n            if last_enc is not None and last_enc.get_output_dim() != enc.get_input_dim():\n                raise ValueError(""Encoder input and output dimensions don\'t match."")\n            last_enc = enc\n\n    @overrides\n    def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor = None) -> torch.Tensor:\n        """"""\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, timesteps, input_dim)\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A tensor of shape (batch_size, timesteps).\n\n        # Returns\n\n        A tensor computed by composing the sequence of encoders.\n        """"""\n        for encoder in self.encoders:\n            inputs = encoder(inputs, mask)\n        return inputs\n\n    @overrides\n    def get_input_dim(self) -> int:\n        return self.encoders[0].get_input_dim()\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self.encoders[-1].get_output_dim()\n\n    @overrides\n    def is_bidirectional(self) -> bool:\n        return self.bidirectional\n'"
allennlp/modules/seq2seq_encoders/feedforward_encoder.py,3,"b'import torch\nfrom overrides import overrides\n\nfrom allennlp.modules.feedforward import FeedForward\nfrom allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\n\n\n@Seq2SeqEncoder.register(""feedforward"")\nclass FeedForwardEncoder(Seq2SeqEncoder):\n    """"""\n    This class applies the `FeedForward` to each item in sequences.\n\n    Registered as a `Seq2SeqEncoder` with name ""feedforward"".\n    """"""\n\n    def __init__(self, feedforward: FeedForward) -> None:\n        super().__init__()\n        self._feedforward = feedforward\n\n    @overrides\n    def get_input_dim(self) -> int:\n        return self._feedforward.get_input_dim()\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self._feedforward.get_output_dim()\n\n    @overrides\n    def is_bidirectional(self) -> bool:\n        return False\n\n    @overrides\n    def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor = None) -> torch.Tensor:\n        """"""\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, timesteps, input_dim)\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A tensor of shape (batch_size, timesteps).\n\n        # Returns\n\n        A tensor of shape (batch_size, timesteps, output_dim).\n        """"""\n        if mask is None:\n            return self._feedforward(inputs)\n        else:\n            outputs = self._feedforward(inputs)\n            return outputs * mask.unsqueeze(dim=-1)\n'"
allennlp/modules/seq2seq_encoders/gated_cnn_encoder.py,16,"b'from typing import Sequence, List\nimport math\n\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\n\n\nclass ResidualBlock(torch.nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        layers: Sequence[Sequence[int]],\n        direction: str,\n        do_weight_norm: bool = True,\n        dropout: float = 0.0,\n    ) -> None:\n        super().__init__()\n\n        self.dropout = dropout\n        self._convolutions = torch.nn.ModuleList()\n        last_dim = input_dim\n        for k, layer in enumerate(layers):\n            # We run two convolutions for each block -- one for the\n            # output and one for the gates -- do them at once, and\n            # we\'ll worry about slicing them in forward\n            if len(layer) == 2:\n                # no dilation\n                conv = torch.nn.Conv1d(\n                    last_dim, layer[1] * 2, layer[0], stride=1, padding=layer[0] - 1, bias=True\n                )\n            elif len(layer) == 3:\n                # a dilation\n                assert layer[0] == 2, ""only support kernel = 2 for now""\n                conv = torch.nn.Conv1d(\n                    last_dim,\n                    layer[1] * 2,\n                    layer[0],\n                    stride=1,\n                    padding=layer[2],\n                    dilation=layer[2],\n                    bias=True,\n                )\n            else:\n                raise ValueError(""each layer must have length 2 or 3"")\n\n            # from Convolutional Sequence to Sequence Learning\n            if k == 0:\n                conv_dropout = dropout\n            else:\n                # no dropout\n                conv_dropout = 0.0\n            std = math.sqrt((4 * (1.0 - conv_dropout)) / (layer[0] * last_dim))\n\n            conv.weight.data.normal_(0, std=std)\n            conv.bias.data.zero_()\n\n            if do_weight_norm:\n                # conv.weight.shape == (out_channels, in_channels, kernel width)\n                # in fairseq, conv.weight.shape == ([width, in, out])\n                #   for ConvTBC.  In ConvTBC, weight norm is applied as\n                #   nn.utils.weight_norm(m, dim=2) over the output dimension.\n                # so for regular 1D convs we need to apply over dimension=0\n                conv = torch.nn.utils.weight_norm(conv, name=""weight"", dim=0)\n\n            self._convolutions.append(conv)\n            last_dim = layer[1]\n\n        assert last_dim == input_dim\n\n        if direction not in (""forward"", ""backward""):\n            raise ConfigurationError(f""invalid direction: {direction}"")\n        self._direction = direction\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        # x = (batch_size, dim, timesteps)\n        # outputs: (batch_size, dim, timesteps) = f(x) + x\n        out = x\n        timesteps = x.size(2)\n        for k, convolution in enumerate(self._convolutions):\n            if k == 0 and self.dropout > 0:\n                # apply dropout to the input\n                out = torch.nn.functional.dropout(out, self.dropout, self.training)\n\n            conv_out = convolution(out)\n\n            # remove the padding indices\n            # x is padded by convolution width - 1 in each direction\n            dims_to_remove = conv_out.size(2) - timesteps\n            if dims_to_remove > 0:\n                if self._direction == ""forward"":\n                    # remove from the end of the sequence\n                    conv_out = conv_out.narrow(2, 0, timesteps)\n                else:\n                    # remove from the beginning of the sequence\n                    conv_out = conv_out.narrow(2, dims_to_remove, timesteps)\n\n            out = torch.nn.functional.glu(conv_out, dim=1)\n\n        # see Convolutional Sequence to Sequence Learning\n        return (out + x) * math.sqrt(0.5)\n\n\n@Seq2SeqEncoder.register(""gated-cnn-encoder"")\nclass GatedCnnEncoder(Seq2SeqEncoder):\n    """"""\n    **This is work-in-progress and has not been fully tested yet. Use at your own risk!**\n\n    A `Seq2SeqEncoder` that uses a Gated CNN.\n\n    see\n\n    Language Modeling with Gated Convolutional Networks,  Yann N. Dauphin et al, ICML 2017\n    https://arxiv.org/abs/1612.08083\n\n    Convolutional Sequence to Sequence Learning, Jonas Gehring et al, ICML 2017\n    https://arxiv.org/abs/1705.03122\n\n    Some possibilities:\n\n    Each element of the list is wrapped in a residual block:\n    input_dim = 512\n    layers = [ [[4, 512]], [[4, 512], [4, 512]], [[4, 512], [4, 512]], [[4, 512], [4, 512]]\n    dropout = 0.05\n\n    A ""bottleneck architecture""\n    input_dim = 512\n    layers = [ [[4, 512]], [[1, 128], [5, 128], [1, 512]], ... ]\n\n    An architecture with dilated convolutions\n    input_dim = 512\n    layers = [\n    [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]],   # receptive field == 16\n    [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]],   # receptive field == 31\n    [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]],   # receptive field == 46\n    [[2, 512, 1]], [[2, 512, 2]], [[2, 512, 4]], [[2, 512, 8]],   # receptive field == 57\n    ]\n\n    Registered as a `Seq2SeqEncoder` with name ""gated-cnn-encoder"".\n\n    # Parameters\n\n    input_dim : `int`, required\n        The dimension of the inputs.\n    layers : `Sequence[Sequence[Sequence[int]]]`, required\n        The layer dimensions for each `ResidualBlock`.\n    dropout : `float`, optional (default = `0.0`)\n        The dropout for each `ResidualBlock`.\n    return_all_layers : `bool`, optional (default = `False`)\n        Whether to return all layers or just the last layer.\n    """"""\n\n    def __init__(\n        self,\n        input_dim: int,\n        layers: Sequence[Sequence[Sequence[int]]],\n        dropout: float = 0.0,\n        return_all_layers: bool = False,\n    ) -> None:\n        super().__init__()\n\n        self._forward_residual_blocks = torch.nn.ModuleList()\n        self._backward_residual_blocks = torch.nn.ModuleList()\n        self._input_dim = input_dim\n        self._output_dim = input_dim * 2\n\n        for layer in layers:\n            self._forward_residual_blocks.append(\n                ResidualBlock(input_dim, layer, ""forward"", dropout=dropout)\n            )\n            self._backward_residual_blocks.append(\n                ResidualBlock(input_dim, layer, ""backward"", dropout=dropout)\n            )\n\n        self._return_all_layers = return_all_layers\n\n    def forward(self, token_embeddings: torch.Tensor, mask: torch.BoolTensor):\n\n        # Convolutions need transposed input\n        transposed_embeddings = torch.transpose(token_embeddings, 1, 2)\n\n        # We need to broadcast the mask to feature dimension,\n        # and to use masked_fill_ we need the inverse of the mask.\n        mask_for_fill = ~mask.unsqueeze(1)\n\n        if self._return_all_layers:\n            # outputs will be [[all forward layers], [all backward layers]]\n            layer_outputs: List[List[torch.Tensor]] = [[], []]\n        else:\n            # outputs will be [forward final layer, backward final layer]\n            outputs: List[torch.Tensor] = []\n\n        for k, blocks in enumerate([self._forward_residual_blocks, self._backward_residual_blocks]):\n            out = transposed_embeddings\n            # Due to zero padding for backward sequences, we need\n            # to ensure that the input has zeros everywhere where\n            # there isn\'t a mask.\n            for block in blocks:\n                out = block(out.masked_fill(mask_for_fill, 0.0))\n                if self._return_all_layers:\n                    layer_outputs[k].append(out)\n            if not self._return_all_layers:\n                outputs.append(out)\n\n        if self._return_all_layers:\n            return [\n                torch.cat([fwd, bwd], dim=1).transpose(1, 2) for fwd, bwd in zip(*layer_outputs)\n            ]\n        else:\n            # Concatenate forward and backward, then transpose back\n            return torch.cat(outputs, dim=1).transpose(1, 2)\n\n    def get_input_dim(self) -> int:\n        return self._input_dim\n\n    def get_output_dim(self) -> int:\n        return self._output_dim\n\n    def is_bidirectional(self) -> bool:\n        return True\n'"
allennlp/modules/seq2seq_encoders/pass_through_encoder.py,3,"b'from overrides import overrides\nimport torch\n\nfrom allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\n\n\n@Seq2SeqEncoder.register(""pass_through"")\nclass PassThroughEncoder(Seq2SeqEncoder):\n    """"""\n    This class allows you to specify skipping a `Seq2SeqEncoder` just\n    by changing a configuration file. This is useful for ablations and\n    measuring the impact of different elements of your model.\n\n    Registered as a `Seq2SeqEncoder` with name ""pass_through"".\n    """"""\n\n    def __init__(self, input_dim: int) -> None:\n        super().__init__()\n        self._input_dim = input_dim\n\n    @overrides\n    def get_input_dim(self) -> int:\n        return self._input_dim\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self._input_dim\n\n    @overrides\n    def is_bidirectional(self):\n        return False\n\n    @overrides\n    def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor = None) -> torch.Tensor:\n        """"""\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, timesteps, input_dim)\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A tensor of shape (batch_size, timesteps).\n\n        # Returns\n\n        A tensor of shape (batch_size, timesteps, output_dim),\n        where output_dim = input_dim.\n        """"""\n        if mask is None:\n            return inputs\n        else:\n            # We should mask out the output instead of the input.\n            # But here, output = input, so we directly mask out the input.\n            return inputs * mask.unsqueeze(dim=-1)\n'"
allennlp/modules/seq2seq_encoders/pytorch_seq2seq_wrapper.py,12,"b'from overrides import overrides\nimport torch\nfrom torch.nn.utils.rnn import pad_packed_sequence\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.modules.augmented_lstm import AugmentedLstm\nfrom allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\nfrom allennlp.modules.stacked_alternating_lstm import StackedAlternatingLstm\nfrom allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n\n\nclass PytorchSeq2SeqWrapper(Seq2SeqEncoder):\n    """"""\n    Pytorch\'s RNNs have two outputs: the hidden state for every time step, and the hidden state at\n    the last time step for every layer.  We just want the first one as a single output.  This\n    wrapper pulls out that output, and adds a `get_output_dim` method, which is useful if you\n    want to, e.g., define a linear + softmax layer on top of this to get some distribution over a\n    set of labels.  The linear layer needs to know its input dimension before it is called, and you\n    can get that from `get_output_dim`.\n\n    In order to be wrapped with this wrapper, a class must have the following members:\n\n        - `self.input_size: int`\n        - `self.hidden_size: int`\n        - `def forward(inputs: PackedSequence, hidden_state: torch.Tensor) ->\n          Tuple[PackedSequence, torch.Tensor]`.\n        - `self.bidirectional: bool` (optional)\n\n    This is what pytorch\'s RNN\'s look like - just make sure your class looks like those, and it\n    should work.\n\n    Note that we *require* you to pass a binary mask of shape (batch_size, sequence_length)\n    when you call this module, to avoid subtle bugs around masking.  If you already have a\n    `PackedSequence` you can pass `None` as the second parameter.\n\n    We support stateful RNNs where the final state from each batch is used as the initial\n    state for the subsequent batch by passing `stateful=True` to the constructor.\n    """"""\n\n    def __init__(self, module: torch.nn.Module, stateful: bool = False) -> None:\n        super().__init__(stateful)\n        self._module = module\n        try:\n            if not self._module.batch_first:\n                raise ConfigurationError(""Our encoder semantics assumes batch is always first!"")\n        except AttributeError:\n            pass\n\n        try:\n            self._is_bidirectional = self._module.bidirectional\n        except AttributeError:\n            self._is_bidirectional = False\n        if self._is_bidirectional:\n            self._num_directions = 2\n        else:\n            self._num_directions = 1\n\n    @overrides\n    def get_input_dim(self) -> int:\n        return self._module.input_size\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self._module.hidden_size * self._num_directions\n\n    @overrides\n    def is_bidirectional(self) -> bool:\n        return self._is_bidirectional\n\n    @overrides\n    def forward(\n        self, inputs: torch.Tensor, mask: torch.BoolTensor, hidden_state: torch.Tensor = None\n    ) -> torch.Tensor:\n\n        if self.stateful and mask is None:\n            raise ValueError(""Always pass a mask with stateful RNNs."")\n        if self.stateful and hidden_state is not None:\n            raise ValueError(""Stateful RNNs provide their own initial hidden_state."")\n\n        if mask is None:\n            return self._module(inputs, hidden_state)[0]\n\n        batch_size, total_sequence_length = mask.size()\n\n        packed_sequence_output, final_states, restoration_indices = self.sort_and_run_forward(\n            self._module, inputs, mask, hidden_state\n        )\n\n        unpacked_sequence_tensor, _ = pad_packed_sequence(packed_sequence_output, batch_first=True)\n\n        num_valid = unpacked_sequence_tensor.size(0)\n        # Some RNNs (GRUs) only return one state as a Tensor.  Others (LSTMs) return two.\n        # If one state, use a single element list to handle in a consistent manner below.\n        if not isinstance(final_states, (list, tuple)) and self.stateful:\n            final_states = [final_states]\n\n        # Add back invalid rows.\n        if num_valid < batch_size:\n            _, length, output_dim = unpacked_sequence_tensor.size()\n            zeros = unpacked_sequence_tensor.new_zeros(batch_size - num_valid, length, output_dim)\n            unpacked_sequence_tensor = torch.cat([unpacked_sequence_tensor, zeros], 0)\n\n            # The states also need to have invalid rows added back.\n            if self.stateful:\n                new_states = []\n                for state in final_states:\n                    num_layers, _, state_dim = state.size()\n                    zeros = state.new_zeros(num_layers, batch_size - num_valid, state_dim)\n                    new_states.append(torch.cat([state, zeros], 1))\n                final_states = new_states\n\n        # It\'s possible to need to pass sequences which are padded to longer than the\n        # max length of the sequence to a Seq2SeqEncoder. However, packing and unpacking\n        # the sequences mean that the returned tensor won\'t include these dimensions, because\n        # the RNN did not need to process them. We add them back on in the form of zeros here.\n        sequence_length_difference = total_sequence_length - unpacked_sequence_tensor.size(1)\n        if sequence_length_difference > 0:\n            zeros = unpacked_sequence_tensor.new_zeros(\n                batch_size, sequence_length_difference, unpacked_sequence_tensor.size(-1)\n            )\n            unpacked_sequence_tensor = torch.cat([unpacked_sequence_tensor, zeros], 1)\n\n        if self.stateful:\n            self._update_states(final_states, restoration_indices)\n\n        # Restore the original indices and return the sequence.\n        return unpacked_sequence_tensor.index_select(0, restoration_indices)\n\n\n@Seq2SeqEncoder.register(""gru"")\nclass GruSeq2SeqEncoder(PytorchSeq2SeqWrapper):\n    """"""\n    Registered as a `Seq2SeqEncoder` with name ""gru"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int = 1,\n        bias: bool = True,\n        dropout: float = 0.0,\n        bidirectional: bool = False,\n        stateful: bool = False,\n    ):\n        module = torch.nn.GRU(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            bias=bias,\n            batch_first=True,\n            dropout=dropout,\n            bidirectional=bidirectional,\n        )\n        super().__init__(module=module, stateful=stateful)\n\n\n@Seq2SeqEncoder.register(""lstm"")\nclass LstmSeq2SeqEncoder(PytorchSeq2SeqWrapper):\n    """"""\n    Registered as a `Seq2SeqEncoder` with name ""lstm"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int = 1,\n        bias: bool = True,\n        dropout: float = 0.0,\n        bidirectional: bool = False,\n        stateful: bool = False,\n    ):\n        module = torch.nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            bias=bias,\n            batch_first=True,\n            dropout=dropout,\n            bidirectional=bidirectional,\n        )\n        super().__init__(module=module, stateful=stateful)\n\n\n@Seq2SeqEncoder.register(""rnn"")\nclass RnnSeq2SeqEncoder(PytorchSeq2SeqWrapper):\n    """"""\n    Registered as a `Seq2SeqEncoder` with name ""rnn"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int = 1,\n        nonlinearity: str = ""tanh"",\n        bias: bool = True,\n        dropout: float = 0.0,\n        bidirectional: bool = False,\n        stateful: bool = False,\n    ):\n        module = torch.nn.RNN(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            nonlinearity=nonlinearity,\n            bias=bias,\n            batch_first=True,\n            dropout=dropout,\n            bidirectional=bidirectional,\n        )\n        super().__init__(module=module, stateful=stateful)\n\n\n@Seq2SeqEncoder.register(""augmented_lstm"")\nclass AugmentedLstmSeq2SeqEncoder(PytorchSeq2SeqWrapper):\n    """"""\n    Registered as a `Seq2SeqEncoder` with name ""augmented_lstm"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        go_forward: bool = True,\n        recurrent_dropout_probability: float = 0.0,\n        use_highway: bool = True,\n        use_input_projection_bias: bool = True,\n        stateful: bool = False,\n    ) -> None:\n        module = AugmentedLstm(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            go_forward=go_forward,\n            recurrent_dropout_probability=recurrent_dropout_probability,\n            use_highway=use_highway,\n            use_input_projection_bias=use_input_projection_bias,\n        )\n        super().__init__(module=module, stateful=stateful)\n\n\n@Seq2SeqEncoder.register(""alternating_lstm"")\nclass StackedAlternatingLstmSeq2SeqEncoder(PytorchSeq2SeqWrapper):\n    """"""\n    Registered as a `Seq2SeqEncoder` with name ""alternating_lstm"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int,\n        recurrent_dropout_probability: float = 0.0,\n        use_highway: bool = True,\n        use_input_projection_bias: bool = True,\n        stateful: bool = False,\n    ) -> None:\n        module = StackedAlternatingLstm(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            recurrent_dropout_probability=recurrent_dropout_probability,\n            use_highway=use_highway,\n            use_input_projection_bias=use_input_projection_bias,\n        )\n        super().__init__(module=module, stateful=stateful)\n\n\n@Seq2SeqEncoder.register(""stacked_bidirectional_lstm"")\nclass StackedBidirectionalLstmSeq2SeqEncoder(PytorchSeq2SeqWrapper):\n    """"""\n    Registered as a `Seq2SeqEncoder` with name ""stacked_bidirectional_lstm"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int,\n        recurrent_dropout_probability: float = 0.0,\n        layer_dropout_probability: float = 0.0,\n        use_highway: bool = True,\n        stateful: bool = False,\n    ) -> None:\n        module = StackedBidirectionalLstm(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            recurrent_dropout_probability=recurrent_dropout_probability,\n            layer_dropout_probability=layer_dropout_probability,\n            use_highway=use_highway,\n        )\n        super().__init__(module=module, stateful=stateful)\n'"
allennlp/modules/seq2seq_encoders/pytorch_transformer_wrapper.py,3,"b'from typing import Optional\n\nfrom overrides import overrides\nimport torch\nfrom torch import nn\n\nfrom allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\nfrom allennlp.nn.util import add_positional_features\n\n\n@Seq2SeqEncoder.register(""pytorch_transformer"")\nclass PytorchTransformer(Seq2SeqEncoder):\n    """"""\n    Implements a stacked self-attention encoder similar to the Transformer\n    architecture in [Attention is all you Need]\n    (https://www.semanticscholar.org/paper/Attention-Is-All-You-Need-Vaswani-Shazeer/0737da0767d77606169cbf4187b83e1ab62f6077).\n\n    This class adapts the Transformer from torch.nn for use in AllenNLP. Optionally, it adds positional encodings.\n\n    Registered as a `Seq2SeqEncoder` with name ""pytorch_transformer"".\n\n    # Parameters\n\n    input_dim : `int`, required.\n        The input dimension of the encoder.\n    feedforward_hidden_dim : `int`, required.\n        The middle dimension of the FeedForward network. The input and output\n        dimensions are fixed to ensure sizes match up for the self attention layers.\n    num_layers : `int`, required.\n        The number of stacked self attention -> feedforward -> layer normalisation blocks.\n    num_attention_heads : `int`, required.\n        The number of attention heads to use per layer.\n    use_positional_encoding : `bool`, optional, (default = `True`)\n        Whether to add sinusoidal frequencies to the input tensor. This is strongly recommended,\n        as without this feature, the self attention layers have no idea of absolute or relative\n        position (as they are just computing pairwise similarity between vectors of elements),\n        which can be important features for many tasks.\n    dropout_prob : `float`, optional, (default = `0.1`)\n        The dropout probability for the feedforward network.\n    """"""  # noqa\n\n    def __init__(\n        self,\n        input_dim: int,\n        num_layers: int,\n        feedforward_hidden_dim: int = 2048,\n        num_attention_heads: int = 8,\n        positional_encoding: Optional[str] = None,\n        positional_embedding_size: int = 512,\n        dropout_prob: float = 0.1,\n        activation: str = ""relu"",\n    ) -> None:\n        super().__init__()\n\n        layer = nn.TransformerEncoderLayer(\n            d_model=input_dim,\n            nhead=num_attention_heads,\n            dim_feedforward=feedforward_hidden_dim,\n            dropout=dropout_prob,\n            activation=activation,\n        )\n        self._transformer = nn.TransformerEncoder(layer, num_layers)\n        self._input_dim = input_dim\n\n        # initialize parameters\n        # We do this before the embeddings are initialized so we get the default initialization for the embeddings.\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n        if positional_encoding is None:\n            self._sinusoidal_positional_encoding = False\n            self._positional_embedding = None\n        elif positional_encoding == ""sinusoidal"":\n            self._sinusoidal_positional_encoding = True\n            self._positional_embedding = None\n        elif positional_encoding == ""embedding"":\n            self._sinusoidal_positional_encoding = False\n            self._positional_embedding = nn.Embedding(positional_embedding_size, input_dim)\n        else:\n            raise ValueError(\n                ""positional_encoding must be one of None, \'sinusoidal\', or \'embedding\'""\n            )\n\n    @overrides\n    def get_input_dim(self) -> int:\n        return self._input_dim\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self._input_dim\n\n    @overrides\n    def is_bidirectional(self):\n        return False\n\n    @overrides\n    def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor):\n        output = inputs\n        if self._sinusoidal_positional_encoding:\n            output = add_positional_features(output)\n        if self._positional_embedding is not None:\n            position_ids = torch.arange(inputs.size(1), dtype=torch.long, device=output.device)\n            position_ids = position_ids.unsqueeze(0).expand(inputs.shape[:-1])\n            output = output + self._positional_embedding(position_ids)\n\n        # For some reason the torch transformer expects the shape (sequence, batch, features), not the more\n        # familiar (batch, sequence, features), so we have to fix it.\n        output = output.permute(1, 0, 2)\n        # For some other reason, the torch transformer takes the mask backwards.\n        mask = ~mask\n        output = self._transformer(output, src_key_padding_mask=mask)\n        output = output.permute(1, 0, 2)\n\n        return output\n'"
allennlp/modules/seq2seq_encoders/seq2seq_encoder.py,0,"b'from allennlp.modules.encoder_base import _EncoderBase\nfrom allennlp.common import Registrable\n\n\nclass Seq2SeqEncoder(_EncoderBase, Registrable):\n    """"""\n    A `Seq2SeqEncoder` is a `Module` that takes as input a sequence of vectors and returns a\n    modified sequence of vectors.  Input shape : `(batch_size, sequence_length, input_dim)`; output\n    shape : `(batch_size, sequence_length, output_dim)`.\n\n    We add two methods to the basic `Module` API: `get_input_dim()` and `get_output_dim()`.\n    You might need this if you want to construct a `Linear` layer using the output of this encoder,\n    or to raise sensible errors for mis-matching input dimensions.\n    """"""\n\n    def get_input_dim(self) -> int:\n        """"""\n        Returns the dimension of the vector input for each element in the sequence input\n        to a `Seq2SeqEncoder`. This is `not` the shape of the input tensor, but the\n        last element of that shape.\n        """"""\n        raise NotImplementedError\n\n    def get_output_dim(self) -> int:\n        """"""\n        Returns the dimension of each vector in the sequence output by this `Seq2SeqEncoder`.\n        This is `not` the shape of the returned tensor, but the last element of that shape.\n        """"""\n        raise NotImplementedError\n\n    def is_bidirectional(self) -> bool:\n        """"""\n        Returns `True` if this encoder is bidirectional.  If so, we assume the forward direction\n        of the encoder is the first half of the final dimension, and the backward direction is the\n        second half.\n        """"""\n        raise NotImplementedError\n'"
allennlp/modules/seq2vec_encoders/__init__.py,3,"b'""""""\nModules that transform a sequence of input vectors\ninto a single output vector.\nSome are just basic wrappers around existing PyTorch modules,\nothers are AllenNLP modules.\n\nThe available Seq2Vec encoders are\n\n* `""gru""` https://pytorch.org/docs/master/nn.html#torch.nn.GRU\n* `""lstm""` https://pytorch.org/docs/master/nn.html#torch.nn.LSTM\n* `""rnn""` https://pytorch.org/docs/master/nn.html#torch.nn.RNN\n* `""cnn""` allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder\n* `""augmented_lstm""` allennlp.modules.augmented_lstm.AugmentedLstm\n* `""alternating_lstm""` allennlp.modules.stacked_alternating_lstm.StackedAlternatingLstm\n* `""stacked_bidirectional_lstm""` allennlp.modules.stacked_bidirectional_lstm.StackedBidirectionalLstm\n""""""\n\nfrom allennlp.modules.seq2vec_encoders.bert_pooler import BertPooler\nfrom allennlp.modules.seq2vec_encoders.boe_encoder import BagOfEmbeddingsEncoder\nfrom allennlp.modules.seq2vec_encoders.cls_pooler import ClsPooler\nfrom allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder\nfrom allennlp.modules.seq2vec_encoders.cnn_highway_encoder import CnnHighwayEncoder\nfrom allennlp.modules.seq2vec_encoders.pytorch_seq2vec_wrapper import (\n    AugmentedLstmSeq2VecEncoder,\n    GruSeq2VecEncoder,\n    LstmSeq2VecEncoder,\n    PytorchSeq2VecWrapper,\n    RnnSeq2VecEncoder,\n    StackedAlternatingLstmSeq2VecEncoder,\n    StackedBidirectionalLstmSeq2VecEncoder,\n)\nfrom allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder\n'"
allennlp/modules/seq2vec_encoders/bert_pooler.py,3,"b'from overrides import overrides\n\nimport torch\nimport torch.nn\nfrom transformers.modeling_auto import AutoModel\n\nfrom allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder\n\n\n@Seq2VecEncoder.register(""bert_pooler"")\nclass BertPooler(Seq2VecEncoder):\n    """"""\n    The pooling layer at the end of the BERT model. This returns an embedding for the\n    [CLS] token, after passing it through a non-linear tanh activation; the non-linear layer\n    is also part of the BERT model. If you want to use the pretrained BERT model\n    to build a classifier and you want to use the AllenNLP token-indexer ->\n    token-embedder -> seq2vec encoder setup, this is the Seq2VecEncoder to use.\n    (For example, if you want to experiment with other embedding / encoding combinations.)\n\n    Registered as a `Seq2VecEncoder` with name ""bert_pooler"".\n\n    # Parameters\n\n    pretrained_model : `Union[str, BertModel]`, required\n        The pretrained BERT model to use. If this is a string,\n        we will call `BertModel.from_pretrained(pretrained_model)`\n        and use that.\n    requires_grad : `bool`, optional, (default = `True`)\n        If True, the weights of the pooler will be updated during training.\n        Otherwise they will not.\n    dropout : `float`, optional, (default = `0.0`)\n        Amount of dropout to apply after pooling\n    """"""\n\n    def __init__(\n        self, pretrained_model: str, requires_grad: bool = True, dropout: float = 0.0\n    ) -> None:\n        super().__init__()\n\n        model = AutoModel.from_pretrained(pretrained_model)\n\n        self._dropout = torch.nn.Dropout(p=dropout)\n\n        self.pooler = model.pooler\n        for param in self.pooler.parameters():\n            param.requires_grad = requires_grad\n        self._embedding_dim = model.config.hidden_size\n\n    @overrides\n    def get_input_dim(self) -> int:\n        return self._embedding_dim\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self._embedding_dim\n\n    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor = None):\n        pooled = self.pooler(tokens)\n        pooled = self._dropout(pooled)\n        return pooled\n'"
allennlp/modules/seq2vec_encoders/boe_encoder.py,2,"b'from overrides import overrides\n\nimport torch\n\nfrom allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder\nfrom allennlp.nn.util import get_lengths_from_binary_sequence_mask\n\n\n@Seq2VecEncoder.register(""boe"")\n@Seq2VecEncoder.register(""bag_of_embeddings"")\nclass BagOfEmbeddingsEncoder(Seq2VecEncoder):\n    """"""\n    A `BagOfEmbeddingsEncoder` is a simple [`Seq2VecEncoder`](./seq2vec_encoder.md) which simply sums\n    the embeddings of a sequence across the time dimension. The input to this module is of shape\n    `(batch_size, num_tokens, embedding_dim)`, and the output is of shape `(batch_size, embedding_dim)`.\n\n    Registered as a `Seq2VecEncoder` with name ""bag_of_embeddings"" and ""boe"".\n\n    # Parameters\n\n    embedding_dim : `int`, required\n        This is the input dimension to the encoder.\n    averaged : `bool`, optional (default=`False`)\n        If `True`, this module will average the embeddings across time, rather than simply summing\n        (ie. we will divide the summed embeddings by the length of the sentence).\n    """"""\n\n    def __init__(self, embedding_dim: int, averaged: bool = False) -> None:\n        super().__init__()\n        self._embedding_dim = embedding_dim\n        self._averaged = averaged\n\n    @overrides\n    def get_input_dim(self) -> int:\n        return self._embedding_dim\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self._embedding_dim\n\n    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor = None):\n        if mask is not None:\n            tokens = tokens * mask.unsqueeze(-1)\n\n        # Our input has shape `(batch_size, num_tokens, embedding_dim)`, so we sum out the `num_tokens`\n        # dimension.\n        summed = tokens.sum(1)\n\n        if self._averaged:\n            if mask is not None:\n                lengths = get_lengths_from_binary_sequence_mask(mask)\n                length_mask = lengths > 0\n\n                # Set any length 0 to 1, to avoid dividing by zero.\n                lengths = torch.max(lengths, lengths.new_ones(1))\n            else:\n                lengths = tokens.new_full((1,), fill_value=tokens.size(1))\n                length_mask = None\n\n            summed = summed / lengths.unsqueeze(-1).float()\n\n            if length_mask is not None:\n                summed = summed * (length_mask > 0).unsqueeze(-1)\n\n        return summed\n'"
allennlp/modules/seq2vec_encoders/cls_pooler.py,2,"b'from overrides import overrides\n\nimport torch.nn\n\nfrom allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder\nfrom allennlp.nn.util import get_final_encoder_states\n\n\n@Seq2VecEncoder.register(""cls_pooler"")\nclass ClsPooler(Seq2VecEncoder):\n    """"""\n    Just takes the first vector from a list of vectors (which in a transformer is typically the\n    [CLS] token) and returns it.  For BERT, it\'s recommended to use `BertPooler` instead.\n\n    Registered as a `Seq2VecEncoder` with name ""cls_pooler"".\n\n    # Parameters\n\n    embedding_dim: `int`, optional\n        This isn\'t needed for any computation that we do, but we sometimes rely on `get_input_dim`\n        and `get_output_dim` to check parameter settings, or to instantiate final linear layers.  In\n        order to give the right values there, we need to know the embedding dimension.  If you\'re\n        using this with a transformer from the `transformers` library, this can often be found with\n        `model.config.hidden_size`, if you\'re not sure.\n    cls_is_last_token: `bool`, optional\n        The [CLS] token is the first token for most of the pretrained transformer models.\n        For some models such as XLNet, however, it is the last token, and we therefore need to\n        select at the end.\n    """"""\n\n    def __init__(self, embedding_dim: int = None, cls_is_last_token: bool = False):\n        super().__init__()\n        self._embedding_dim = embedding_dim\n        self._cls_is_last_token = cls_is_last_token\n\n    @overrides\n    def get_input_dim(self) -> int:\n        return self._embedding_dim\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self._embedding_dim\n\n    @overrides\n    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor = None):\n        # tokens is assumed to have shape (batch_size, sequence_length, embedding_dim).\n        # mask is assumed to have shape (batch_size, sequence_length) with all 1s preceding all 0s.\n        if not self._cls_is_last_token:\n            return tokens[:, 0, :]\n        else:  # [CLS] at the end\n            if mask is None:\n                raise ValueError(""Must provide mask for transformer models with [CLS] at the end."")\n            return get_final_encoder_states(tokens, mask)\n'"
allennlp/modules/seq2vec_encoders/cnn_encoder.py,6,"b'from typing import Optional, Tuple\n\nfrom overrides import overrides\nimport torch\nfrom torch.nn import Conv1d, Linear\n\nfrom allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder\nfrom allennlp.nn import Activation\n\n\n@Seq2VecEncoder.register(""cnn"")\nclass CnnEncoder(Seq2VecEncoder):\n    """"""\n    A `CnnEncoder` is a combination of multiple convolution layers and max pooling layers.  As a\n    [`Seq2VecEncoder`](./seq2vec_encoder.md), the input to this module is of shape `(batch_size, num_tokens,\n    input_dim)`, and the output is of shape `(batch_size, output_dim)`.\n\n    The CNN has one convolution layer for each ngram filter size. Each convolution operation gives\n    out a vector of size num_filters. The number of times a convolution layer will be used\n    is `num_tokens - ngram_size + 1`. The corresponding maxpooling layer aggregates all these\n    outputs from the convolution layer and outputs the max.\n\n    This operation is repeated for every ngram size passed, and consequently the dimensionality of\n    the output after maxpooling is `len(ngram_filter_sizes) * num_filters`.  This then gets\n    (optionally) projected down to a lower dimensional output, specified by `output_dim`.\n\n    We then use a fully connected layer to project in back to the desired output_dim.  For more\n    details, refer to ""A Sensitivity Analysis of (and Practitioners\xe2\x80\x99 Guide to) Convolutional Neural\n    Networks for Sentence Classification"", Zhang and Wallace 2016, particularly Figure 1.\n\n    Registered as a `Seq2VecEncoder` with name ""cnn"".\n\n    # Parameters\n\n    embedding_dim : `int`, required\n        This is the input dimension to the encoder.  We need this because we can\'t do shape\n        inference in pytorch, and we need to know what size filters to construct in the CNN.\n    num_filters : `int`, required\n        This is the output dim for each convolutional layer, which is the number of ""filters""\n        learned by that layer.\n    ngram_filter_sizes : `Tuple[int]`, optional (default=`(2, 3, 4, 5)`)\n        This specifies both the number of convolutional layers we will create and their sizes.  The\n        default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding\n        ngrams of size 2 to 5 with some number of filters.\n    conv_layer_activation : `Activation`, optional (default=`torch.nn.ReLU`)\n        Activation to use after the convolution layers.\n    output_dim : `Optional[int]`, optional (default=`None`)\n        After doing convolutions and pooling, we\'ll project the collected features into a vector of\n        this size.  If this value is `None`, we will just return the result of the max pooling,\n        giving an output of shape `len(ngram_filter_sizes) * num_filters`.\n    """"""\n\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_filters: int,\n        ngram_filter_sizes: Tuple[int, ...] = (2, 3, 4, 5),\n        conv_layer_activation: Activation = None,\n        output_dim: Optional[int] = None,\n    ) -> None:\n        super().__init__()\n        self._embedding_dim = embedding_dim\n        self._num_filters = num_filters\n        self._ngram_filter_sizes = ngram_filter_sizes\n        self._activation = conv_layer_activation or Activation.by_name(""relu"")()\n        self._output_dim = output_dim\n\n        self._convolution_layers = [\n            Conv1d(\n                in_channels=self._embedding_dim,\n                out_channels=self._num_filters,\n                kernel_size=ngram_size,\n            )\n            for ngram_size in self._ngram_filter_sizes\n        ]\n        for i, conv_layer in enumerate(self._convolution_layers):\n            self.add_module(""conv_layer_%d"" % i, conv_layer)\n\n        maxpool_output_dim = self._num_filters * len(self._ngram_filter_sizes)\n        if self._output_dim:\n            self.projection_layer = Linear(maxpool_output_dim, self._output_dim)\n        else:\n            self.projection_layer = None\n            self._output_dim = maxpool_output_dim\n\n    @overrides\n    def get_input_dim(self) -> int:\n        return self._embedding_dim\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self._output_dim\n\n    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor):\n        if mask is not None:\n            tokens = tokens * mask.unsqueeze(-1)\n\n        # Our input is expected to have shape `(batch_size, num_tokens, embedding_dim)`.  The\n        # convolution layers expect input of shape `(batch_size, in_channels, sequence_length)`,\n        # where the conv layer `in_channels` is our `embedding_dim`.  We thus need to transpose the\n        # tensor first.\n        tokens = torch.transpose(tokens, 1, 2)\n        # Each convolution layer returns output of size `(batch_size, num_filters, pool_length)`,\n        # where `pool_length = num_tokens - ngram_size + 1`.  We then do an activation function,\n        # then do max pooling over each filter for the whole input sequence.  Because our max\n        # pooling is simple, we just use `torch.max`.  The resultant tensor of has shape\n        # `(batch_size, num_conv_layers * num_filters)`, which then gets projected using the\n        # projection layer, if requested.\n\n        filter_outputs = []\n        for i in range(len(self._convolution_layers)):\n            convolution_layer = getattr(self, ""conv_layer_{}"".format(i))\n            filter_outputs.append(self._activation(convolution_layer(tokens)).max(dim=2)[0])\n\n        # Now we have a list of `num_conv_layers` tensors of shape `(batch_size, num_filters)`.\n        # Concatenating them gives us a tensor of shape `(batch_size, num_filters * num_conv_layers)`.\n        maxpool_output = (\n            torch.cat(filter_outputs, dim=1) if len(filter_outputs) > 1 else filter_outputs[0]\n        )\n\n        if self.projection_layer:\n            result = self.projection_layer(maxpool_output)\n        else:\n            result = maxpool_output\n        return result\n'"
allennlp/modules/seq2vec_encoders/cnn_highway_encoder.py,11,"b'from typing import Sequence, Dict, List, Callable\n\nimport torch\nimport numpy as np\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.modules.layer_norm import LayerNorm\nfrom allennlp.modules.highway import Highway\nfrom allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder\n\n_VALID_PROJECTION_LOCATIONS = {""after_cnn"", ""after_highway"", None}\n\n\n@Seq2VecEncoder.register(""cnn-highway"")\nclass CnnHighwayEncoder(Seq2VecEncoder):\n    """"""\n    The character CNN + highway encoder from\n    [Kim et al ""Character aware neural language models""](https://arxiv.org/abs/1508.06615)\n    with an optional projection.\n\n    Registered as a `Seq2VecEncoder` with name ""cnn-highway"".\n\n    # Parameters\n\n    embedding_dim : `int`, required\n        The dimension of the initial character embedding.\n    filters : `Sequence[Sequence[int]]`, required\n        A sequence of pairs (filter_width, num_filters).\n    num_highway : `int`, required\n        The number of highway layers.\n    projection_dim : `int`, required\n        The output dimension of the projection layer.\n    activation : `str`, optional (default = `\'relu\'`)\n        The activation function for the convolutional layers.\n    projection_location : `str`, optional (default = `\'after_highway\'`)\n        Where to apply the projection layer. Valid values are\n        \'after_highway\', \'after_cnn\', and None.\n    """"""\n\n    def __init__(\n        self,\n        embedding_dim: int,\n        filters: Sequence[Sequence[int]],\n        num_highway: int,\n        projection_dim: int,\n        activation: str = ""relu"",\n        projection_location: str = ""after_highway"",\n        do_layer_norm: bool = False,\n    ) -> None:\n        super().__init__()\n\n        if projection_location not in _VALID_PROJECTION_LOCATIONS:\n            raise ConfigurationError(f""unknown projection location: {projection_location}"")\n\n        self.input_dim = embedding_dim\n        self.output_dim = projection_dim\n        self._projection_location = projection_location\n\n        if activation == ""tanh"":\n            self._activation = torch.nn.functional.tanh\n        elif activation == ""relu"":\n            self._activation = torch.nn.functional.relu\n        else:\n            raise ConfigurationError(f""unknown activation {activation}"")\n\n        # Create the convolutions\n        self._convolutions: List[torch.nn.Module] = []\n        for i, (width, num) in enumerate(filters):\n            conv = torch.nn.Conv1d(\n                in_channels=embedding_dim, out_channels=num, kernel_size=width, bias=True\n            )\n            conv.weight.data.uniform_(-0.05, 0.05)\n            conv.bias.data.fill_(0.0)\n            self.add_module(f""char_conv_{i}"", conv)  # needs to match the old ELMo name\n            self._convolutions.append(conv)\n\n        # Create the highway layers\n        num_filters = sum(num for _, num in filters)\n        if projection_location == ""after_cnn"":\n            highway_dim = projection_dim\n        else:\n            # highway_dim is the number of cnn filters\n            highway_dim = num_filters\n        self._highways = Highway(highway_dim, num_highway, activation=torch.nn.functional.relu)\n        for highway_layer in self._highways._layers:\n            # highway is a linear layer for each highway layer\n            # with fused W and b weights\n            highway_layer.weight.data.normal_(mean=0.0, std=np.sqrt(1.0 / highway_dim))\n            highway_layer.bias[:highway_dim].data.fill_(0.0)\n            highway_layer.bias[highway_dim:].data.fill_(2.0)\n\n        # Projection layer: always num_filters -> projection_dim\n        self._projection = torch.nn.Linear(num_filters, projection_dim, bias=True)\n        self._projection.weight.data.normal_(mean=0.0, std=np.sqrt(1.0 / num_filters))\n        self._projection.bias.data.fill_(0.0)\n\n        # And add a layer norm\n        if do_layer_norm:\n            self._layer_norm: Callable = LayerNorm(self.output_dim)\n        else:\n            self._layer_norm = lambda tensor: tensor\n\n    def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor) -> Dict[str, torch.Tensor]:\n        """"""\n        Compute context insensitive token embeddings for ELMo representations.\n\n        # Parameters\n\n        inputs: `torch.Tensor`\n            Shape `(batch_size, num_characters, embedding_dim)`\n            Character embeddings representing the current batch.\n        mask: `torch.BoolTensor`\n            Shape `(batch_size, num_characters)`\n            Currently unused. The mask for characters is implicit. See TokenCharactersEncoder.forward.\n\n        # Returns\n\n        `encoding`:\n            Shape `(batch_size, projection_dim)` tensor with context-insensitive token representations.\n        """"""\n        # convolutions want (batch_size, embedding_dim, num_characters)\n        inputs = inputs.transpose(1, 2)\n\n        convolutions = []\n        for i in range(len(self._convolutions)):\n            char_conv_i = getattr(self, f""char_conv_{i}"")\n            convolved = char_conv_i(inputs)\n\n            # (batch_size, n_filters for this width)\n            convolved, _ = torch.max(convolved, dim=-1)\n            convolved = self._activation(convolved)\n            convolutions.append(convolved)\n\n        # (batch_size, n_filters)\n        token_embedding = torch.cat(convolutions, dim=-1)\n\n        if self._projection_location == ""after_cnn"":\n            token_embedding = self._projection(token_embedding)\n\n        # apply the highway layers (batch_size, highway_dim)\n        token_embedding = self._highways(token_embedding)\n\n        if self._projection_location == ""after_highway"":\n            # final projection  (batch_size, projection_dim)\n            token_embedding = self._projection(token_embedding)\n\n        # Apply layer norm if appropriate\n        token_embedding = self._layer_norm(token_embedding)\n\n        return token_embedding\n\n    def get_input_dim(self) -> int:\n        return self.input_dim\n\n    def get_output_dim(self) -> int:\n        return self.output_dim\n'"
allennlp/modules/seq2vec_encoders/pytorch_seq2vec_wrapper.py,9,"b'import torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.modules.augmented_lstm import AugmentedLstm\nfrom allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder\nfrom allennlp.modules.stacked_alternating_lstm import StackedAlternatingLstm\nfrom allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n\n\nclass PytorchSeq2VecWrapper(Seq2VecEncoder):\n    """"""\n    Pytorch\'s RNNs have two outputs: the hidden state for every time step, and the hidden state at\n    the last time step for every layer.  We just want the second one as a single output.  This\n    wrapper pulls out that output, and adds a `get_output_dim` method, which is useful if you\n    want to, e.g., define a linear + softmax layer on top of this to get some distribution over a\n    set of labels.  The linear layer needs to know its input dimension before it is called, and you\n    can get that from `get_output_dim`.\n\n    Also, there are lots of ways you could imagine going from an RNN hidden state at every\n    timestep to a single vector - you could take the last vector at all layers in the stack, do\n    some kind of pooling, take the last vector of the top layer in a stack, or many other  options.\n    We just take the final hidden state vector, or in the case of a bidirectional RNN cell, we\n    concatenate the forward and backward final states together. TODO(mattg): allow for other ways\n    of wrapping RNNs.\n\n    In order to be wrapped with this wrapper, a class must have the following members:\n\n        - `self.input_size: int`\n        - `self.hidden_size: int`\n        - `def forward(inputs: PackedSequence, hidden_state: torch.tensor) ->\n          Tuple[PackedSequence, torch.Tensor]`.\n        - `self.bidirectional: bool` (optional)\n\n    This is what pytorch\'s RNN\'s look like - just make sure your class looks like those, and it\n    should work.\n\n    Note that we *require* you to pass sequence lengths when you call this module, to avoid subtle\n    bugs around masking.  If you already have a `PackedSequence` you can pass `None` as the\n    second parameter.\n    """"""\n\n    def __init__(self, module: torch.nn.modules.RNNBase) -> None:\n        # Seq2VecEncoders cannot be stateful.\n        super().__init__(stateful=False)\n        self._module = module\n        try:\n            if not self._module.batch_first:\n                raise ConfigurationError(""Our encoder semantics assumes batch is always first!"")\n        except AttributeError:\n            pass\n\n    def get_input_dim(self) -> int:\n        return self._module.input_size\n\n    def get_output_dim(self) -> int:\n        try:\n            is_bidirectional = self._module.bidirectional\n        except AttributeError:\n            is_bidirectional = False\n        return self._module.hidden_size * (2 if is_bidirectional else 1)\n\n    def forward(\n        self, inputs: torch.Tensor, mask: torch.BoolTensor, hidden_state: torch.Tensor = None\n    ) -> torch.Tensor:\n\n        if mask is None:\n            # If a mask isn\'t passed, there is no padding in the batch of instances, so we can just\n            # return the last sequence output as the state.  This doesn\'t work in the case of\n            # variable length sequences, as the last state for each element of the batch won\'t be\n            # at the end of the max sequence length, so we have to use the state of the RNN below.\n            return self._module(inputs, hidden_state)[0][:, -1, :]\n\n        batch_size = mask.size(0)\n\n        _, state, restoration_indices, = self.sort_and_run_forward(\n            self._module, inputs, mask, hidden_state\n        )\n\n        # Deal with the fact the LSTM state is a tuple of (state, memory).\n        if isinstance(state, tuple):\n            state = state[0]\n\n        num_layers_times_directions, num_valid, encoding_dim = state.size()\n        # Add back invalid rows.\n        if num_valid < batch_size:\n            # batch size is the second dimension here, because pytorch\n            # returns RNN state as a tensor of shape (num_layers * num_directions,\n            # batch_size, hidden_size)\n            zeros = state.new_zeros(\n                num_layers_times_directions, batch_size - num_valid, encoding_dim\n            )\n            state = torch.cat([state, zeros], 1)\n\n        # Restore the original indices and return the final state of the\n        # top layer. Pytorch\'s recurrent layers return state in the form\n        # (num_layers * num_directions, batch_size, hidden_size) regardless\n        # of the \'batch_first\' flag, so we transpose, extract the relevant\n        # layer state (both forward and backward if using bidirectional layers)\n        # and return them as a single (batch_size, self.get_output_dim()) tensor.\n\n        # now of shape: (batch_size, num_layers * num_directions, hidden_size).\n        unsorted_state = state.transpose(0, 1).index_select(0, restoration_indices)\n\n        # Extract the last hidden vector, including both forward and backward states\n        # if the cell is bidirectional. Then reshape by concatenation (in the case\n        # we have bidirectional states) or just squash the 1st dimension in the non-\n        # bidirectional case. Return tensor has shape (batch_size, hidden_size * num_directions).\n        try:\n            last_state_index = 2 if self._module.bidirectional else 1\n        except AttributeError:\n            last_state_index = 1\n        last_layer_state = unsorted_state[:, -last_state_index:, :]\n        return last_layer_state.contiguous().view([-1, self.get_output_dim()])\n\n\n@Seq2VecEncoder.register(""gru"")\nclass GruSeq2VecEncoder(PytorchSeq2VecWrapper):\n    """"""\n    Registered as a `Seq2VecEncoder` with name ""gru"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int = 1,\n        bias: bool = True,\n        dropout: float = 0.0,\n        bidirectional: bool = False,\n    ):\n        module = torch.nn.GRU(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            bias=bias,\n            batch_first=True,\n            dropout=dropout,\n            bidirectional=bidirectional,\n        )\n        super().__init__(module=module)\n\n\n@Seq2VecEncoder.register(""lstm"")\nclass LstmSeq2VecEncoder(PytorchSeq2VecWrapper):\n    """"""\n    Registered as a `Seq2VecEncoder` with name ""lstm"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int = 1,\n        bias: bool = True,\n        dropout: float = 0.0,\n        bidirectional: bool = False,\n    ):\n        module = torch.nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            bias=bias,\n            batch_first=True,\n            dropout=dropout,\n            bidirectional=bidirectional,\n        )\n        super().__init__(module=module)\n\n\n@Seq2VecEncoder.register(""rnn"")\nclass RnnSeq2VecEncoder(PytorchSeq2VecWrapper):\n    """"""\n    Registered as a `Seq2VecEncoder` with name ""rnn"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int = 1,\n        nonlinearity: str = ""tanh"",\n        bias: bool = True,\n        dropout: float = 0.0,\n        bidirectional: bool = False,\n    ):\n        module = torch.nn.RNN(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            nonlinearity=nonlinearity,\n            bias=bias,\n            batch_first=True,\n            dropout=dropout,\n            bidirectional=bidirectional,\n        )\n        super().__init__(module=module)\n\n\n@Seq2VecEncoder.register(""augmented_lstm"")\nclass AugmentedLstmSeq2VecEncoder(PytorchSeq2VecWrapper):\n    """"""\n    Registered as a `Seq2VecEncoder` with name ""augmented_lstm"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        go_forward: bool = True,\n        recurrent_dropout_probability: float = 0.0,\n        use_highway: bool = True,\n        use_input_projection_bias: bool = True,\n    ) -> None:\n        module = AugmentedLstm(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            go_forward=go_forward,\n            recurrent_dropout_probability=recurrent_dropout_probability,\n            use_highway=use_highway,\n            use_input_projection_bias=use_input_projection_bias,\n        )\n        super().__init__(module=module)\n\n\n@Seq2VecEncoder.register(""alternating_lstm"")\nclass StackedAlternatingLstmSeq2VecEncoder(PytorchSeq2VecWrapper):\n    """"""\n    Registered as a `Seq2VecEncoder` with name ""alternating_lstm"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int,\n        recurrent_dropout_probability: float = 0.0,\n        use_highway: bool = True,\n        use_input_projection_bias: bool = True,\n    ) -> None:\n        module = StackedAlternatingLstm(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            recurrent_dropout_probability=recurrent_dropout_probability,\n            use_highway=use_highway,\n            use_input_projection_bias=use_input_projection_bias,\n        )\n        super().__init__(module=module)\n\n\n@Seq2VecEncoder.register(""stacked_bidirectional_lstm"")\nclass StackedBidirectionalLstmSeq2VecEncoder(PytorchSeq2VecWrapper):\n    """"""\n    Registered as a `Seq2VecEncoder` with name ""stacked_bidirectional_lstm"".\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int,\n        recurrent_dropout_probability: float = 0.0,\n        layer_dropout_probability: float = 0.0,\n        use_highway: bool = True,\n    ) -> None:\n        module = StackedBidirectionalLstm(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            recurrent_dropout_probability=recurrent_dropout_probability,\n            layer_dropout_probability=layer_dropout_probability,\n            use_highway=use_highway,\n        )\n        super().__init__(module=module)\n'"
allennlp/modules/seq2vec_encoders/seq2vec_encoder.py,0,"b'from allennlp.modules.encoder_base import _EncoderBase\nfrom allennlp.common import Registrable\n\n\nclass Seq2VecEncoder(_EncoderBase, Registrable):\n    """"""\n    A `Seq2VecEncoder` is a `Module` that takes as input a sequence of vectors and returns a\n    single vector.  Input shape : `(batch_size, sequence_length, input_dim)`; output shape:\n    `(batch_size, output_dim)`.\n\n    We add two methods to the basic `Module` API: `get_input_dim()` and `get_output_dim()`.\n    You might need this if you want to construct a `Linear` layer using the output of this encoder,\n    or to raise sensible errors for mis-matching input dimensions.\n    """"""\n\n    def get_input_dim(self) -> int:\n        """"""\n        Returns the dimension of the vector input for each element in the sequence input\n        to a `Seq2VecEncoder`. This is `not` the shape of the input tensor, but the\n        last element of that shape.\n        """"""\n        raise NotImplementedError\n\n    def get_output_dim(self) -> int:\n        """"""\n        Returns the dimension of the final vector output by this `Seq2VecEncoder`.  This is `not`\n        the shape of the returned tensor, but the last element of that shape.\n        """"""\n        raise NotImplementedError\n'"
allennlp/modules/span_extractors/__init__.py,0,"b'from allennlp.modules.span_extractors.span_extractor import SpanExtractor\nfrom allennlp.modules.span_extractors.endpoint_span_extractor import EndpointSpanExtractor\nfrom allennlp.modules.span_extractors.self_attentive_span_extractor import (\n    SelfAttentiveSpanExtractor,\n)\nfrom allennlp.modules.span_extractors.bidirectional_endpoint_span_extractor import (\n    BidirectionalEndpointSpanExtractor,\n)\n'"
allennlp/modules/span_extractors/bidirectional_endpoint_span_extractor.py,12,"b'import torch\nfrom overrides import overrides\nfrom torch.nn.parameter import Parameter\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.modules.span_extractors.span_extractor import SpanExtractor\nfrom allennlp.modules.token_embedders.embedding import Embedding\nfrom allennlp.nn import util\n\n\n@SpanExtractor.register(""bidirectional_endpoint"")\nclass BidirectionalEndpointSpanExtractor(SpanExtractor):\n    """"""\n    Represents spans from a bidirectional encoder as a concatenation of two different\n    representations of the span endpoints, one for the forward direction of the encoder\n    and one from the backward direction. This type of representation encodes some subtlety,\n    because when you consider the forward and backward directions separately, the end index\n    of the span for the backward direction\'s representation is actually the start index.\n\n    By default, this `SpanExtractor` represents spans as\n    `sequence_tensor[inclusive_span_end] - sequence_tensor[exclusive_span_start]`\n    meaning that the representation is the difference between the the last word in the span\n    and the word `before` the span started. Note that the start and end indices are with\n    respect to the direction that the RNN is going in, so for the backward direction, the\n    start/end indices are reversed.\n\n    Additionally, the width of the spans can be embedded and concatenated on to the\n    final combination.\n\n    The following other types of representation are supported for both the forward and backward\n    directions, assuming that `x = span_start_embeddings` and `y = span_end_embeddings`.\n\n    `x`, `y`, `x*y`, `x+y`, `x-y`, `x/y`, where each of those binary operations\n    is performed elementwise.  You can list as many combinations as you want, comma separated.\n    For example, you might give `x,y,x*y` as the `combination` parameter to this class.\n    The computed similarity function would then be `[x; y; x*y]`, which can then be optionally\n    concatenated with an embedded representation of the width of the span.\n\n    Registered as a `SpanExtractor` with name ""bidirectional_endpoint"".\n\n    # Parameters\n\n    input_dim : `int`, required\n        The final dimension of the `sequence_tensor`.\n    forward_combination : `str`, optional (default = `""y-x""`).\n        The method used to combine the `forward_start_embeddings` and `forward_end_embeddings`\n        for the forward direction of the bidirectional representation.\n        See above for a full description.\n    backward_combination : `str`, optional (default = `""x-y""`).\n        The method used to combine the `backward_start_embeddings` and `backward_end_embeddings`\n        for the backward direction of the bidirectional representation.\n        See above for a full description.\n    num_width_embeddings : `int`, optional (default = `None`).\n        Specifies the number of buckets to use when representing\n        span width features.\n    span_width_embedding_dim : `int`, optional (default = `None`).\n        The embedding size for the span_width features.\n    bucket_widths : `bool`, optional (default = `False`).\n        Whether to bucket the span widths into log-space buckets. If `False`,\n        the raw span widths are used.\n    use_sentinels : `bool`, optional (default = `True`).\n        If `True`, sentinels are used to represent exclusive span indices for the elements\n        in the first and last positions in the sequence (as the exclusive indices for these\n        elements are outside of the the sequence boundary). This is not strictly necessary,\n        as you may know that your exclusive start and end indices are always within your sequence\n        representation, such as if you have appended/prepended <START> and <END> tokens to your\n        sequence.\n    """"""\n\n    def __init__(\n        self,\n        input_dim: int,\n        forward_combination: str = ""y-x"",\n        backward_combination: str = ""x-y"",\n        num_width_embeddings: int = None,\n        span_width_embedding_dim: int = None,\n        bucket_widths: bool = False,\n        use_sentinels: bool = True,\n    ) -> None:\n        super().__init__()\n        self._input_dim = input_dim\n        self._forward_combination = forward_combination\n        self._backward_combination = backward_combination\n        self._num_width_embeddings = num_width_embeddings\n        self._bucket_widths = bucket_widths\n\n        if self._input_dim % 2 != 0:\n            raise ConfigurationError(\n                ""The input dimension is not divisible by 2, but the ""\n                ""BidirectionalEndpointSpanExtractor assumes the embedded representation ""\n                ""is bidirectional (and hence divisible by 2).""\n            )\n        if num_width_embeddings is not None and span_width_embedding_dim is not None:\n            self._span_width_embedding = Embedding(\n                num_embeddings=num_width_embeddings, embedding_dim=span_width_embedding_dim\n            )\n        elif num_width_embeddings is not None or span_width_embedding_dim is not None:\n            raise ConfigurationError(\n                ""To use a span width embedding representation, you must""\n                ""specify both num_width_buckets and span_width_embedding_dim.""\n            )\n        else:\n            self._span_width_embedding = None\n\n        self._use_sentinels = use_sentinels\n        if use_sentinels:\n            self._start_sentinel = Parameter(torch.randn([1, 1, int(input_dim / 2)]))\n            self._end_sentinel = Parameter(torch.randn([1, 1, int(input_dim / 2)]))\n\n    def get_input_dim(self) -> int:\n        return self._input_dim\n\n    def get_output_dim(self) -> int:\n        unidirectional_dim = int(self._input_dim / 2)\n        forward_combined_dim = util.get_combined_dim(\n            self._forward_combination, [unidirectional_dim, unidirectional_dim]\n        )\n        backward_combined_dim = util.get_combined_dim(\n            self._backward_combination, [unidirectional_dim, unidirectional_dim]\n        )\n        if self._span_width_embedding is not None:\n            return (\n                forward_combined_dim\n                + backward_combined_dim\n                + self._span_width_embedding.get_output_dim()\n            )\n        return forward_combined_dim + backward_combined_dim\n\n    @overrides\n    def forward(\n        self,\n        sequence_tensor: torch.FloatTensor,\n        span_indices: torch.LongTensor,\n        sequence_mask: torch.BoolTensor = None,\n        span_indices_mask: torch.BoolTensor = None,\n    ) -> torch.FloatTensor:\n\n        # Both of shape (batch_size, sequence_length, embedding_size / 2)\n        forward_sequence, backward_sequence = sequence_tensor.split(\n            int(self._input_dim / 2), dim=-1\n        )\n        forward_sequence = forward_sequence.contiguous()\n        backward_sequence = backward_sequence.contiguous()\n\n        # shape (batch_size, num_spans)\n        span_starts, span_ends = [index.squeeze(-1) for index in span_indices.split(1, dim=-1)]\n\n        if span_indices_mask is not None:\n            span_starts = span_starts * span_indices_mask\n            span_ends = span_ends * span_indices_mask\n        # We want `exclusive` span starts, so we remove 1 from the forward span starts\n        # as the AllenNLP `SpanField` is inclusive.\n        # shape (batch_size, num_spans)\n        exclusive_span_starts = span_starts - 1\n        # shape (batch_size, num_spans, 1)\n        start_sentinel_mask = (exclusive_span_starts == -1).unsqueeze(-1)\n\n        # We want `exclusive` span ends for the backward direction\n        # (so that the `start` of the span in that direction is exlusive), so\n        # we add 1 to the span ends as the AllenNLP `SpanField` is inclusive.\n        exclusive_span_ends = span_ends + 1\n\n        if sequence_mask is not None:\n            # shape (batch_size)\n            sequence_lengths = util.get_lengths_from_binary_sequence_mask(sequence_mask)\n        else:\n            # shape (batch_size), filled with the sequence length size of the sequence_tensor.\n            sequence_lengths = torch.ones_like(\n                sequence_tensor[:, 0, 0], dtype=torch.long\n            ) * sequence_tensor.size(1)\n\n        # shape (batch_size, num_spans, 1)\n        end_sentinel_mask = (exclusive_span_ends >= sequence_lengths.unsqueeze(-1)).unsqueeze(-1)\n\n        # As we added 1 to the span_ends to make them exclusive, which might have caused indices\n        # equal to the sequence_length to become out of bounds, we multiply by the inverse of the\n        # end_sentinel mask to erase these indices (as we will replace them anyway in the block below).\n        # The same argument follows for the exclusive span start indices.\n        exclusive_span_ends = exclusive_span_ends * ~end_sentinel_mask.squeeze(-1)\n        exclusive_span_starts = exclusive_span_starts * ~start_sentinel_mask.squeeze(-1)\n\n        # We\'ll check the indices here at runtime, because it\'s difficult to debug\n        # if this goes wrong and it\'s tricky to get right.\n        if (exclusive_span_starts < 0).any() or (\n            exclusive_span_ends > sequence_lengths.unsqueeze(-1)\n        ).any():\n            raise ValueError(\n                f""Adjusted span indices must lie inside the length of the sequence tensor, ""\n                f""but found: exclusive_span_starts: {exclusive_span_starts}, ""\n                f""exclusive_span_ends: {exclusive_span_ends} for a sequence tensor with lengths ""\n                f""{sequence_lengths}.""\n            )\n\n        # Forward Direction: start indices are exclusive. Shape (batch_size, num_spans, input_size / 2)\n        forward_start_embeddings = util.batched_index_select(\n            forward_sequence, exclusive_span_starts\n        )\n        # Forward Direction: end indices are inclusive, so we can just use span_ends.\n        # Shape (batch_size, num_spans, input_size / 2)\n        forward_end_embeddings = util.batched_index_select(forward_sequence, span_ends)\n\n        # Backward Direction: The backward start embeddings use the `forward` end\n        # indices, because we are going backwards.\n        # Shape (batch_size, num_spans, input_size / 2)\n        backward_start_embeddings = util.batched_index_select(\n            backward_sequence, exclusive_span_ends\n        )\n        # Backward Direction: The backward end embeddings use the `forward` start\n        # indices, because we are going backwards.\n        # Shape (batch_size, num_spans, input_size / 2)\n        backward_end_embeddings = util.batched_index_select(backward_sequence, span_starts)\n\n        if self._use_sentinels:\n            # If we\'re using sentinels, we need to replace all the elements which were\n            # outside the dimensions of the sequence_tensor with either the start sentinel,\n            # or the end sentinel.\n            forward_start_embeddings = (\n                forward_start_embeddings * ~start_sentinel_mask\n                + start_sentinel_mask * self._start_sentinel\n            )\n            backward_start_embeddings = (\n                backward_start_embeddings * ~end_sentinel_mask\n                + end_sentinel_mask * self._end_sentinel\n            )\n\n        # Now we combine the forward and backward spans in the manner specified by the\n        # respective combinations and concatenate these representations.\n        # Shape (batch_size, num_spans, forward_combination_dim)\n        forward_spans = util.combine_tensors(\n            self._forward_combination, [forward_start_embeddings, forward_end_embeddings]\n        )\n        # Shape (batch_size, num_spans, backward_combination_dim)\n        backward_spans = util.combine_tensors(\n            self._backward_combination, [backward_start_embeddings, backward_end_embeddings]\n        )\n        # Shape (batch_size, num_spans, forward_combination_dim + backward_combination_dim)\n        span_embeddings = torch.cat([forward_spans, backward_spans], -1)\n\n        if self._span_width_embedding is not None:\n            # Embed the span widths and concatenate to the rest of the representations.\n            if self._bucket_widths:\n                span_widths = util.bucket_values(\n                    span_ends - span_starts, num_total_buckets=self._num_width_embeddings\n                )\n            else:\n                span_widths = span_ends - span_starts\n\n            span_width_embeddings = self._span_width_embedding(span_widths)\n            return torch.cat([span_embeddings, span_width_embeddings], -1)\n\n        if span_indices_mask is not None:\n            return span_embeddings * span_indices_mask.unsqueeze(-1)\n        return span_embeddings\n'"
allennlp/modules/span_extractors/endpoint_span_extractor.py,7,"b'import torch\nfrom torch.nn.parameter import Parameter\nfrom overrides import overrides\n\nfrom allennlp.modules.span_extractors.span_extractor import SpanExtractor\nfrom allennlp.modules.token_embedders.embedding import Embedding\nfrom allennlp.nn import util\n\nfrom allennlp.common.checks import ConfigurationError\n\n\n@SpanExtractor.register(""endpoint"")\nclass EndpointSpanExtractor(SpanExtractor):\n    """"""\n    Represents spans as a combination of the embeddings of their endpoints. Additionally,\n    the width of the spans can be embedded and concatenated on to the final combination.\n\n    The following types of representation are supported, assuming that\n    `x = span_start_embeddings` and `y = span_end_embeddings`.\n\n    `x`, `y`, `x*y`, `x+y`, `x-y`, `x/y`, where each of those binary operations\n    is performed elementwise.  You can list as many combinations as you want, comma separated.\n    For example, you might give `x,y,x*y` as the `combination` parameter to this class.\n    The computed similarity function would then be `[x; y; x*y]`, which can then be optionally\n    concatenated with an embedded representation of the width of the span.\n\n    Registered as a `SpanExtractor` with name ""endpoint"".\n\n    # Parameters\n\n    input_dim : `int`, required.\n        The final dimension of the `sequence_tensor`.\n    combination : `str`, optional (default = `""x,y""`).\n        The method used to combine the `start_embedding` and `end_embedding`\n        representations. See above for a full description.\n    num_width_embeddings : `int`, optional (default = `None`).\n        Specifies the number of buckets to use when representing\n        span width features.\n    span_width_embedding_dim : `int`, optional (default = `None`).\n        The embedding size for the span_width features.\n    bucket_widths : `bool`, optional (default = `False`).\n        Whether to bucket the span widths into log-space buckets. If `False`,\n        the raw span widths are used.\n    use_exclusive_start_indices : `bool`, optional (default = `False`).\n        If `True`, the start indices extracted are converted to exclusive indices. Sentinels\n        are used to represent exclusive span indices for the elements in the first\n        position in the sequence (as the exclusive indices for these elements are outside\n        of the the sequence boundary) so that start indices can be exclusive.\n        NOTE: This option can be helpful to avoid the pathological case in which you\n        want span differences for length 1 spans - if you use inclusive indices, you\n        will end up with an `x - x` operation for length 1 spans, which is not good.\n    """"""\n\n    def __init__(\n        self,\n        input_dim: int,\n        combination: str = ""x,y"",\n        num_width_embeddings: int = None,\n        span_width_embedding_dim: int = None,\n        bucket_widths: bool = False,\n        use_exclusive_start_indices: bool = False,\n    ) -> None:\n        super().__init__()\n        self._input_dim = input_dim\n        self._combination = combination\n        self._num_width_embeddings = num_width_embeddings\n        self._bucket_widths = bucket_widths\n\n        self._use_exclusive_start_indices = use_exclusive_start_indices\n        if use_exclusive_start_indices:\n            self._start_sentinel = Parameter(torch.randn([1, 1, int(input_dim)]))\n\n        if num_width_embeddings is not None and span_width_embedding_dim is not None:\n            self._span_width_embedding = Embedding(\n                num_embeddings=num_width_embeddings, embedding_dim=span_width_embedding_dim\n            )\n        elif num_width_embeddings is not None or span_width_embedding_dim is not None:\n            raise ConfigurationError(\n                ""To use a span width embedding representation, you must""\n                ""specify both num_width_buckets and span_width_embedding_dim.""\n            )\n        else:\n            self._span_width_embedding = None\n\n    def get_input_dim(self) -> int:\n        return self._input_dim\n\n    def get_output_dim(self) -> int:\n        combined_dim = util.get_combined_dim(self._combination, [self._input_dim, self._input_dim])\n        if self._span_width_embedding is not None:\n            return combined_dim + self._span_width_embedding.get_output_dim()\n        return combined_dim\n\n    @overrides\n    def forward(\n        self,\n        sequence_tensor: torch.FloatTensor,\n        span_indices: torch.LongTensor,\n        sequence_mask: torch.BoolTensor = None,\n        span_indices_mask: torch.BoolTensor = None,\n    ) -> None:\n        # shape (batch_size, num_spans)\n        span_starts, span_ends = [index.squeeze(-1) for index in span_indices.split(1, dim=-1)]\n\n        if span_indices_mask is not None:\n            # It\'s not strictly necessary to multiply the span indices by the mask here,\n            # but it\'s possible that the span representation was padded with something other\n            # than 0 (such as -1, which would be an invalid index), so we do so anyway to\n            # be safe.\n            span_starts = span_starts * span_indices_mask\n            span_ends = span_ends * span_indices_mask\n\n        if not self._use_exclusive_start_indices:\n            if sequence_tensor.size(-1) != self._input_dim:\n                raise ValueError(\n                    f""Dimension mismatch expected ({sequence_tensor.size(-1)}) ""\n                    f""received ({self._input_dim}).""\n                )\n            start_embeddings = util.batched_index_select(sequence_tensor, span_starts)\n            end_embeddings = util.batched_index_select(sequence_tensor, span_ends)\n\n        else:\n            # We want `exclusive` span starts, so we remove 1 from the forward span starts\n            # as the AllenNLP `SpanField` is inclusive.\n            # shape (batch_size, num_spans)\n            exclusive_span_starts = span_starts - 1\n            # shape (batch_size, num_spans, 1)\n            start_sentinel_mask = (exclusive_span_starts == -1).unsqueeze(-1)\n            exclusive_span_starts = exclusive_span_starts * ~start_sentinel_mask.squeeze(-1)\n\n            # We\'ll check the indices here at runtime, because it\'s difficult to debug\n            # if this goes wrong and it\'s tricky to get right.\n            if (exclusive_span_starts < 0).any():\n                raise ValueError(\n                    f""Adjusted span indices must lie inside the the sequence tensor, ""\n                    f""but found: exclusive_span_starts: {exclusive_span_starts}.""\n                )\n\n            start_embeddings = util.batched_index_select(sequence_tensor, exclusive_span_starts)\n            end_embeddings = util.batched_index_select(sequence_tensor, span_ends)\n\n            # We\'re using sentinels, so we need to replace all the elements which were\n            # outside the dimensions of the sequence_tensor with the start sentinel.\n            start_embeddings = (\n                start_embeddings * ~start_sentinel_mask + start_sentinel_mask * self._start_sentinel\n            )\n\n        combined_tensors = util.combine_tensors(\n            self._combination, [start_embeddings, end_embeddings]\n        )\n        if self._span_width_embedding is not None:\n            # Embed the span widths and concatenate to the rest of the representations.\n            if self._bucket_widths:\n                span_widths = util.bucket_values(\n                    span_ends - span_starts, num_total_buckets=self._num_width_embeddings\n                )\n            else:\n                span_widths = span_ends - span_starts\n\n            span_width_embeddings = self._span_width_embedding(span_widths)\n            combined_tensors = torch.cat([combined_tensors, span_width_embeddings], -1)\n\n        if span_indices_mask is not None:\n            return combined_tensors * span_indices_mask.unsqueeze(-1)\n\n        return combined_tensors\n'"
allennlp/modules/span_extractors/self_attentive_span_extractor.py,7,"b'import torch\nfrom overrides import overrides\n\nfrom allennlp.modules.span_extractors.span_extractor import SpanExtractor\nfrom allennlp.modules.time_distributed import TimeDistributed\nfrom allennlp.nn import util\n\n\n@SpanExtractor.register(""self_attentive"")\nclass SelfAttentiveSpanExtractor(SpanExtractor):\n    """"""\n    Computes span representations by generating an unnormalized attention score for each\n    word in the document. Spans representations are computed with respect to these\n    scores by normalising the attention scores for words inside the span.\n\n    Given these attention distributions over every span, this module weights the\n    corresponding vector representations of the words in the span by this distribution,\n    returning a weighted representation of each span.\n\n    Registered as a `SpanExtractor` with name ""self_attentive"".\n\n    # Parameters\n\n    input_dim : `int`, required.\n        The final dimension of the `sequence_tensor`.\n\n    # Returns\n\n    attended_text_embeddings : `torch.FloatTensor`.\n        A tensor of shape (batch_size, num_spans, input_dim), which each span representation\n        is formed by locally normalising a global attention over the sequence. The only way\n        in which the attention distribution differs over different spans is in the set of words\n        over which they are normalized.\n    """"""\n\n    def __init__(self, input_dim: int) -> None:\n        super().__init__()\n        self._input_dim = input_dim\n        self._global_attention = TimeDistributed(torch.nn.Linear(input_dim, 1))\n\n    def get_input_dim(self) -> int:\n        return self._input_dim\n\n    def get_output_dim(self) -> int:\n        return self._input_dim\n\n    @overrides\n    def forward(\n        self,\n        sequence_tensor: torch.FloatTensor,\n        span_indices: torch.LongTensor,\n        span_indices_mask: torch.BoolTensor = None,\n    ) -> torch.FloatTensor:\n        # shape (batch_size, sequence_length, 1)\n        global_attention_logits = self._global_attention(sequence_tensor)\n\n        # shape (batch_size, sequence_length, embedding_dim + 1)\n        concat_tensor = torch.cat([sequence_tensor, global_attention_logits], -1)\n\n        concat_output, span_mask = util.batched_span_select(concat_tensor, span_indices)\n\n        # Shape: (batch_size, num_spans, max_batch_span_width, embedding_dim)\n        span_embeddings = concat_output[:, :, :, :-1]\n        # Shape: (batch_size, num_spans, max_batch_span_width)\n        span_attention_logits = concat_output[:, :, :, -1]\n\n        # Shape: (batch_size, num_spans, max_batch_span_width)\n        span_attention_weights = util.masked_softmax(span_attention_logits, span_mask)\n\n        # Do a weighted sum of the embedded spans with\n        # respect to the normalised attention distributions.\n        # Shape: (batch_size, num_spans, embedding_dim)\n        attended_text_embeddings = util.weighted_sum(span_embeddings, span_attention_weights)\n\n        if span_indices_mask is not None:\n            # Above we were masking the widths of spans with respect to the max\n            # span width in the batch. Here we are masking the spans which were\n            # originally passed in as padding.\n            return attended_text_embeddings * span_indices_mask.unsqueeze(-1)\n\n        return attended_text_embeddings\n'"
allennlp/modules/span_extractors/span_extractor.py,9,"b'import torch\nfrom overrides import overrides\n\nfrom allennlp.common.registrable import Registrable\n\n\nclass SpanExtractor(torch.nn.Module, Registrable):\n    """"""\n    Many NLP models deal with representations of spans inside a sentence.\n    SpanExtractors define methods for extracting and representing spans\n    from a sentence.\n\n    SpanExtractors take a sequence tensor of shape (batch_size, timesteps, embedding_dim)\n    and indices of shape (batch_size, num_spans, 2) and return a tensor of\n    shape (batch_size, num_spans, ...), forming some representation of the\n    spans.\n    """"""\n\n    @overrides\n    def forward(\n        self,\n        sequence_tensor: torch.FloatTensor,\n        span_indices: torch.LongTensor,\n        sequence_mask: torch.BoolTensor = None,\n        span_indices_mask: torch.BoolTensor = None,\n    ):\n        """"""\n        Given a sequence tensor, extract spans and return representations of\n        them. Span representation can be computed in many different ways,\n        such as concatenation of the start and end spans, attention over the\n        vectors contained inside the span, etc.\n\n        # Parameters\n\n        sequence_tensor : `torch.FloatTensor`, required.\n            A tensor of shape (batch_size, sequence_length, embedding_size)\n            representing an embedded sequence of words.\n        span_indices : `torch.LongTensor`, required.\n            A tensor of shape `(batch_size, num_spans, 2)`, where the last\n            dimension represents the inclusive start and end indices of the\n            span to be extracted from the `sequence_tensor`.\n        sequence_mask : `torch.BoolTensor`, optional (default = `None`).\n            A tensor of shape (batch_size, sequence_length) representing padded\n            elements of the sequence.\n        span_indices_mask : `torch.BoolTensor`, optional (default = `None`).\n            A tensor of shape (batch_size, num_spans) representing the valid\n            spans in the `indices` tensor. This mask is optional because\n            sometimes it\'s easier to worry about masking after calling this\n            function, rather than passing a mask directly.\n\n        # Returns\n\n        A tensor of shape `(batch_size, num_spans, embedded_span_size)`,\n        where `embedded_span_size` depends on the way spans are represented.\n        """"""\n        raise NotImplementedError\n\n    def get_input_dim(self) -> int:\n        """"""\n        Returns the expected final dimension of the `sequence_tensor`.\n        """"""\n        raise NotImplementedError\n\n    def get_output_dim(self) -> int:\n        """"""\n        Returns the expected final dimension of the returned span representation.\n        """"""\n        raise NotImplementedError\n'"
allennlp/modules/text_field_embedders/__init__.py,0,"b'""""""\nA `TextFieldEmbedder` is a `Module` that takes as input the `dict` of NumPy arrays\nproduced by a `TextField` and returns as output an embedded representation of the tokens in that field.\n""""""\n\nfrom allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder\nfrom allennlp.modules.text_field_embedders.basic_text_field_embedder import BasicTextFieldEmbedder\n'"
allennlp/modules/text_field_embedders/basic_text_field_embedder.py,3,"b'from typing import Dict\nimport inspect\n\nimport torch\nfrom overrides import overrides\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data import TextFieldTensors\nfrom allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder\nfrom allennlp.modules.time_distributed import TimeDistributed\nfrom allennlp.modules.token_embedders.token_embedder import TokenEmbedder\n\n\n@TextFieldEmbedder.register(""basic"")\nclass BasicTextFieldEmbedder(TextFieldEmbedder):\n    """"""\n    This is a `TextFieldEmbedder` that wraps a collection of\n    [`TokenEmbedder`](../token_embedders/token_embedder.md) objects.  Each\n    `TokenEmbedder` embeds or encodes the representation output from one\n    [`allennlp.data.TokenIndexer`](../../data/token_indexers/token_indexer.md). As the data produced by a\n    [`allennlp.data.fields.TextField`](../../data/fields/text_field.md) is a dictionary mapping names to these\n    representations, we take `TokenEmbedders` with corresponding names.  Each `TokenEmbedders`\n    embeds its input, and the result is concatenated in an arbitrary (but consistent) order.\n\n    Registered as a `TextFieldEmbedder` with name ""basic"", which is also the default.\n\n    # Parameters\n\n    token_embedders : `Dict[str, TokenEmbedder]`, required.\n        A dictionary mapping token embedder names to implementations.\n        These names should match the corresponding indexer used to generate\n        the tensor passed to the TokenEmbedder.\n    """"""\n\n    def __init__(self, token_embedders: Dict[str, TokenEmbedder]) -> None:\n        super().__init__()\n        # NOTE(mattg): I\'d prefer to just use ModuleDict(token_embedders) here, but that changes\n        # weight locations in torch state dictionaries and invalidates all prior models, just for a\n        # cosmetic change in the code.\n        self._token_embedders = token_embedders\n        for key, embedder in token_embedders.items():\n            name = ""token_embedder_%s"" % key\n            self.add_module(name, embedder)\n        self._ordered_embedder_keys = sorted(self._token_embedders.keys())\n\n    @overrides\n    def get_output_dim(self) -> int:\n        output_dim = 0\n        for embedder in self._token_embedders.values():\n            output_dim += embedder.get_output_dim()\n        return output_dim\n\n    def forward(\n        self, text_field_input: TextFieldTensors, num_wrapping_dims: int = 0, **kwargs\n    ) -> torch.Tensor:\n        if self._token_embedders.keys() != text_field_input.keys():\n            message = ""Mismatched token keys: %s and %s"" % (\n                str(self._token_embedders.keys()),\n                str(text_field_input.keys()),\n            )\n            raise ConfigurationError(message)\n\n        embedded_representations = []\n        for key in self._ordered_embedder_keys:\n            # Note: need to use getattr here so that the pytorch voodoo\n            # with submodules works with multiple GPUs.\n            embedder = getattr(self, ""token_embedder_{}"".format(key))\n            forward_params = inspect.signature(embedder.forward).parameters\n            forward_params_values = {}\n            missing_tensor_args = set()\n            for param in forward_params.keys():\n                if param in kwargs:\n                    forward_params_values[param] = kwargs[param]\n                else:\n                    missing_tensor_args.add(param)\n\n            for _ in range(num_wrapping_dims):\n                embedder = TimeDistributed(embedder)\n\n            tensors: Dict[str, torch.Tensor] = text_field_input[key]\n            if len(tensors) == 1 and len(missing_tensor_args) == 1:\n                # If there\'s only one tensor argument to the embedder, and we just have one tensor to\n                # embed, we can just pass in that tensor, without requiring a name match.\n                token_vectors = embedder(list(tensors.values())[0], **forward_params_values)\n            else:\n                # If there are multiple tensor arguments, we have to require matching names from the\n                # TokenIndexer.  I don\'t think there\'s an easy way around that.\n                token_vectors = embedder(**tensors, **forward_params_values)\n            if token_vectors is not None:\n                # To handle some very rare use cases, we allow the return value of the embedder to\n                # be None; we just skip it in that case.\n                embedded_representations.append(token_vectors)\n        return torch.cat(embedded_representations, dim=-1)\n'"
allennlp/modules/text_field_embedders/text_field_embedder.py,2,"b'import torch\n\nfrom allennlp.common import Registrable\nfrom allennlp.data import TextFieldTensors\n\n\nclass TextFieldEmbedder(torch.nn.Module, Registrable):\n    """"""\n    A `TextFieldEmbedder` is a `Module` that takes as input the\n    [`DataArray`](../../data/fields/text_field.md) produced by a [`TextField`](../../data/fields/text_field.md) and\n    returns as output an embedded representation of the tokens in that field.\n\n    The `DataArrays` produced by `TextFields` are _dictionaries_ with named representations, like\n    ""words"" and ""characters"".  When you create a `TextField`, you pass in a dictionary of\n    [`TokenIndexer`](../../data/token_indexers/token_indexer.md) objects, telling the field how exactly the\n    tokens in the field should be represented.  This class changes the type signature of `Module.forward`,\n    restricting `TextFieldEmbedders` to take inputs corresponding to a single `TextField`, which is\n    a dictionary of tensors with the same names as were passed to the `TextField`.\n\n    We also add a method to the basic `Module` API: `get_output_dim()`.  You might need this\n    if you want to construct a `Linear` layer using the output of this embedder, for instance.\n    """"""\n\n    default_implementation = ""basic""\n\n    def forward(\n        self, text_field_input: TextFieldTensors, num_wrapping_dims: int = 0, **kwargs\n    ) -> torch.Tensor:\n        """"""\n        # Parameters\n\n        text_field_input : `TextFieldTensors`\n            A dictionary that was the output of a call to `TextField.as_tensor`.  Each tensor in\n            here is assumed to have a shape roughly similar to `(batch_size, sequence_length)`\n            (perhaps with an extra trailing dimension for the characters in each token).\n        num_wrapping_dims : `int`, optional (default=`0`)\n            If you have a `ListField[TextField]` that created the `text_field_input`, you\'ll\n            end up with tensors of shape `(batch_size, wrapping_dim1, wrapping_dim2, ...,\n            sequence_length)`.  This parameter tells us how many wrapping dimensions there are, so\n            that we can correctly `TimeDistribute` the embedding of each named representation.\n        """"""\n        raise NotImplementedError\n\n    def get_output_dim(self) -> int:\n        """"""\n        Returns the dimension of the vector representing each token in the output of this\n        `TextFieldEmbedder`.  This is _not_ the shape of the returned tensor, but the last element\n        of that shape.\n        """"""\n        raise NotImplementedError\n'"
allennlp/modules/token_embedders/__init__.py,0,"b'""""""\nA `TokenEmbedder` is a `Module` that\nembeds one-hot-encoded tokens as vectors.\n""""""\n\nfrom allennlp.modules.token_embedders.token_embedder import TokenEmbedder\nfrom allennlp.modules.token_embedders.embedding import Embedding\nfrom allennlp.modules.token_embedders.token_characters_encoder import TokenCharactersEncoder\nfrom allennlp.modules.token_embedders.elmo_token_embedder import ElmoTokenEmbedder\nfrom allennlp.modules.token_embedders.empty_embedder import EmptyEmbedder\nfrom allennlp.modules.token_embedders.bag_of_word_counts_token_embedder import (\n    BagOfWordCountsTokenEmbedder,\n)\nfrom allennlp.modules.token_embedders.pass_through_token_embedder import PassThroughTokenEmbedder\nfrom allennlp.modules.token_embedders.pretrained_transformer_embedder import (\n    PretrainedTransformerEmbedder,\n)\nfrom allennlp.modules.token_embedders.pretrained_transformer_mismatched_embedder import (\n    PretrainedTransformerMismatchedEmbedder,\n)\n'"
allennlp/modules/token_embedders/bag_of_word_counts_token_embedder.py,7,"b'import torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data import Vocabulary\nfrom allennlp.modules.token_embedders.token_embedder import TokenEmbedder\nfrom allennlp.nn.util import get_text_field_mask\n\n\n@TokenEmbedder.register(""bag_of_word_counts"")\nclass BagOfWordCountsTokenEmbedder(TokenEmbedder):\n    """"""\n    Represents a sequence of tokens as a bag of (discrete) word ids, as it was done\n    in the pre-neural days.\n\n    Each sequence gets a vector of length vocabulary size, where the i\'th entry in the vector\n    corresponds to number of times the i\'th token in the vocabulary appears in the sequence.\n\n    By default, we ignore padding tokens.\n\n    Registered as a `TokenEmbedder` with name ""bag_of_word_counts"".\n\n    # Parameters\n\n    vocab : `Vocabulary`\n    vocab_namespace : `str`, optional (default = `""tokens""`)\n        namespace of vocabulary to embed\n    projection_dim : `int`, optional (default = `None`)\n        if specified, will project the resulting bag of words representation\n        to specified dimension.\n    ignore_oov : `bool`, optional (default = `False`)\n        If true, we ignore the OOV token.\n    """"""\n\n    def __init__(\n        self,\n        vocab: Vocabulary,\n        vocab_namespace: str = ""tokens"",\n        projection_dim: int = None,\n        ignore_oov: bool = False,\n    ) -> None:\n        super().__init__()\n        self.vocab = vocab\n        self.vocab_size = vocab.get_vocab_size(vocab_namespace)\n        if projection_dim:\n            self._projection = torch.nn.Linear(self.vocab_size, projection_dim)\n        else:\n            self._projection = None\n        self._ignore_oov = ignore_oov\n        oov_token = vocab._oov_token\n        self._oov_idx = vocab.get_token_to_index_vocabulary(vocab_namespace).get(oov_token)\n        if self._oov_idx is None:\n            raise ConfigurationError(\n                ""OOV token does not exist in vocabulary namespace {}"".format(vocab_namespace)\n            )\n        self.output_dim = projection_dim or self.vocab_size\n\n    def get_output_dim(self):\n        return self.output_dim\n\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n        """"""\n        # Parameters\n\n        inputs : `torch.Tensor`\n            Shape `(batch_size, timesteps, sequence_length)` of word ids\n            representing the current batch.\n\n        # Returns\n\n        `torch.Tensor`\n            The bag-of-words representations for the input sequence, shape\n            `(batch_size, vocab_size)`\n        """"""\n        bag_of_words_vectors = []\n\n        mask = get_text_field_mask({""tokens"": {""tokens"": inputs}})\n        if self._ignore_oov:\n            # also mask out positions corresponding to oov\n            mask &= inputs != self._oov_idx\n        for document, doc_mask in zip(inputs, mask):\n            document = torch.masked_select(document, doc_mask)\n            vec = torch.bincount(document, minlength=self.vocab_size).float()\n            vec = vec.view(1, -1)\n            bag_of_words_vectors.append(vec)\n        bag_of_words_output = torch.cat(bag_of_words_vectors, 0)\n\n        if self._projection:\n            projection = self._projection\n            bag_of_words_output = projection(bag_of_words_output)\n        return bag_of_words_output\n'"
allennlp/modules/token_embedders/elmo_token_embedder.py,5,"b'from typing import List\nimport torch\n\nfrom allennlp.modules.token_embedders.token_embedder import TokenEmbedder\nfrom allennlp.modules.elmo import Elmo\nfrom allennlp.modules.time_distributed import TimeDistributed\n\n\n@TokenEmbedder.register(""elmo_token_embedder"")\nclass ElmoTokenEmbedder(TokenEmbedder):\n    """"""\n    Compute a single layer of ELMo representations.\n\n    This class serves as a convenience when you only want to use one layer of\n    ELMo representations at the input of your network.  It\'s essentially a wrapper\n    around Elmo(num_output_representations=1, ...)\n\n    Registered as a `TokenEmbedder` with name ""elmo_token_embedder"".\n\n    # Parameters\n\n    options_file : `str`, required.\n        An ELMo JSON options file.\n    weight_file : `str`, required.\n        An ELMo hdf5 weight file.\n    do_layer_norm : `bool`, optional.\n        Should we apply layer normalization (passed to `ScalarMix`)?\n    dropout : `float`, optional, (default = `0.5`).\n        The dropout value to be applied to the ELMo representations.\n    requires_grad : `bool`, optional\n        If True, compute gradient of ELMo parameters for fine tuning.\n    projection_dim : `int`, optional\n        If given, we will project the ELMo embedding down to this dimension.  We recommend that you\n        try using ELMo with a lot of dropout and no projection first, but we have found a few cases\n        where projection helps (particularly where there is very limited training data).\n    vocab_to_cache : `List[str]`, optional.\n        A list of words to pre-compute and cache character convolutions\n        for. If you use this option, the ElmoTokenEmbedder expects that you pass word\n        indices of shape (batch_size, timesteps) to forward, instead\n        of character indices. If you use this option and pass a word which\n        wasn\'t pre-cached, this will break.\n    scalar_mix_parameters : `List[int]`, optional, (default=`None`)\n        If not `None`, use these scalar mix parameters to weight the representations\n        produced by different layers. These mixing weights are not updated during\n        training. The mixing weights here should be the unnormalized (i.e., pre-softmax)\n        weights. So, if you wanted to use only the 1st layer of a 2-layer ELMo,\n        you can set this to [-9e10, 1, -9e10 ].\n    """"""\n\n    def __init__(\n        self,\n        options_file: str = ""https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/""\n        + ""elmo_2x4096_512_2048cnn_2xhighway_options.json"",\n        weight_file: str = ""https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/""\n        + ""elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5"",\n        do_layer_norm: bool = False,\n        dropout: float = 0.5,\n        requires_grad: bool = False,\n        projection_dim: int = None,\n        vocab_to_cache: List[str] = None,\n        scalar_mix_parameters: List[float] = None,\n    ) -> None:\n        super().__init__()\n\n        self._elmo = Elmo(\n            options_file,\n            weight_file,\n            1,\n            do_layer_norm=do_layer_norm,\n            dropout=dropout,\n            requires_grad=requires_grad,\n            vocab_to_cache=vocab_to_cache,\n            scalar_mix_parameters=scalar_mix_parameters,\n        )\n        if projection_dim:\n            self._projection = torch.nn.Linear(self._elmo.get_output_dim(), projection_dim)\n            self.output_dim = projection_dim\n        else:\n            self._projection = None\n            self.output_dim = self._elmo.get_output_dim()\n\n    def get_output_dim(self) -> int:\n        return self.output_dim\n\n    def forward(self, elmo_tokens: torch.Tensor, word_inputs: torch.Tensor = None) -> torch.Tensor:\n        """"""\n        # Parameters\n\n        elmo_tokens : `torch.Tensor`\n            Shape `(batch_size, timesteps, 50)` of character ids representing the current batch.\n        word_inputs : `torch.Tensor`, optional.\n            If you passed a cached vocab, you can in addition pass a tensor of shape\n            `(batch_size, timesteps)`, which represent word ids which have been pre-cached.\n\n        # Returns\n\n        `torch.Tensor`\n            The ELMo representations for the input sequence, shape\n            `(batch_size, timesteps, embedding_dim)`\n        """"""\n        elmo_output = self._elmo(elmo_tokens, word_inputs)\n        elmo_representations = elmo_output[""elmo_representations""][0]\n        if self._projection:\n            projection = self._projection\n            for _ in range(elmo_representations.dim() - 2):\n                projection = TimeDistributed(projection)\n            elmo_representations = projection(elmo_representations)\n        return elmo_representations\n'"
allennlp/modules/token_embedders/embedding.py,23,"b'import io\nimport itertools\nimport logging\nimport re\nimport tarfile\nimport warnings\nimport zipfile\nfrom typing import Any, cast, Iterator, NamedTuple, Optional, Sequence, Tuple, BinaryIO\n\nimport numpy\nimport torch\nfrom overrides import overrides\nfrom torch.nn.functional import embedding\n\nfrom allennlp.common import Tqdm\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.file_utils import cached_path, get_file_extension, is_url_or_existing_file\nfrom allennlp.data import Vocabulary\nfrom allennlp.modules.time_distributed import TimeDistributed\nfrom allennlp.modules.token_embedders.token_embedder import TokenEmbedder\nfrom allennlp.nn import util\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(""ignore"", category=FutureWarning)\n    import h5py\n\nlogger = logging.getLogger(__name__)\n\n\n@TokenEmbedder.register(""embedding"")\nclass Embedding(TokenEmbedder):\n    """"""\n    A more featureful embedding module than the default in Pytorch.  Adds the ability to:\n\n        1. embed higher-order inputs\n        2. pre-specify the weight matrix\n        3. use a non-trainable embedding\n        4. project the resultant embeddings to some other dimension (which only makes sense with\n           non-trainable embeddings).\n\n    Note that if you are using our data API and are trying to embed a\n    [`TextField`](../../data/fields/text_field.md), you should use a\n    [`TextFieldEmbedder`](../text_field_embedders/text_field_embedder.md) instead of using this directly.\n\n    Registered as a `TokenEmbedder` with name ""embedding"".\n\n    # Parameters\n\n    num_embeddings : `int`\n        Size of the dictionary of embeddings (vocabulary size).\n    embedding_dim : `int`\n        The size of each embedding vector.\n    projection_dim : `int`, optional (default=`None`)\n        If given, we add a projection layer after the embedding layer.  This really only makes\n        sense if `trainable` is `False`.\n    weight : `torch.FloatTensor`, optional (default=`None`)\n        A pre-initialised weight matrix for the embedding lookup, allowing the use of\n        pretrained vectors.\n    padding_index : `int`, optional (default=`None`)\n        If given, pads the output with zeros whenever it encounters the index.\n    trainable : `bool`, optional (default=`True`)\n        Whether or not to optimize the embedding parameters.\n    max_norm : `float`, optional (default=`None`)\n        If given, will renormalize the embeddings to always have a norm lesser than this\n    norm_type : `float`, optional (default=`2`)\n        The p of the p-norm to compute for the max_norm option\n    scale_grad_by_freq : `bool`, optional (default=`False`)\n        If given, this will scale gradients by the frequency of the words in the mini-batch.\n    sparse : `bool`, optional (default=`False`)\n        Whether or not the Pytorch backend should use a sparse representation of the embedding weight.\n    vocab_namespace : `str`, optional (default=`None`)\n        In case of fine-tuning/transfer learning, the model\'s embedding matrix needs to be\n        extended according to the size of extended-vocabulary. To be able to know how much to\n        extend the embedding-matrix, it\'s necessary to know which vocab_namspace was used to\n        construct it in the original training. We store vocab_namespace used during the original\n        training as an attribute, so that it can be retrieved during fine-tuning.\n    pretrained_file : `str`, optional (default=`None`)\n        Path to a file of word vectors to initialize the embedding matrix. It can be the\n        path to a local file or a URL of a (cached) remote file. Two formats are supported:\n            * hdf5 file - containing an embedding matrix in the form of a torch.Tensor;\n            * text file - an utf-8 encoded text file with space separated fields.\n    vocab : `Vocabulary`, optional (default = `None`)\n        Used to construct an embedding from a pretrained file.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        ""embedding"", it gets specified as a top-level parameter, then is passed in to this module\n        separately.\n\n    # Returns\n\n    An Embedding module.\n    """"""\n\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_embeddings: int = None,\n        projection_dim: int = None,\n        weight: torch.FloatTensor = None,\n        padding_index: int = None,\n        trainable: bool = True,\n        max_norm: float = None,\n        norm_type: float = 2.0,\n        scale_grad_by_freq: bool = False,\n        sparse: bool = False,\n        vocab_namespace: str = ""tokens"",\n        pretrained_file: str = None,\n        vocab: Vocabulary = None,\n    ) -> None:\n        super().__init__()\n\n        if num_embeddings is None and vocab is None:\n            raise ConfigurationError(\n                ""Embedding must be constructed with either num_embeddings or a vocabulary.""\n            )\n\n        if num_embeddings is None:\n            num_embeddings = vocab.get_vocab_size(vocab_namespace)\n        else:\n            # If num_embeddings is present, set default namespace to None so that extend_vocab\n            # call doesn\'t misinterpret that some namespace was originally used.\n            vocab_namespace = None\n\n        self.num_embeddings = num_embeddings\n        self.padding_index = padding_index\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        self.sparse = sparse\n        self._vocab_namespace = vocab_namespace\n        self._pretrained_file = pretrained_file\n\n        self.output_dim = projection_dim or embedding_dim\n\n        if weight is not None and pretrained_file:\n            raise ConfigurationError(\n                ""Embedding was constructed with both a weight and a pretrained file.""\n            )\n\n        elif pretrained_file is not None:\n\n            if vocab is None:\n                raise ConfigurationError(\n                    ""To construct an Embedding from a pretrained file, you must also pass a vocabulary.""\n                )\n\n            # If we\'re loading a saved model, we don\'t want to actually read a pre-trained\n            # embedding file - the embeddings will just be in our saved weights, and we might not\n            # have the original embedding file anymore, anyway.\n\n            # TODO: having to pass tokens here is SUPER gross, but otherwise this breaks the\n            # extend_vocab method, which relies on the value of vocab_namespace being None\n            # to infer at what stage the embedding has been constructed. Phew.\n            weight = _read_pretrained_embeddings_file(\n                pretrained_file, embedding_dim, vocab, vocab_namespace or ""tokens""\n            )\n            self.weight = torch.nn.Parameter(weight, requires_grad=trainable)\n\n        elif weight is not None:\n            self.weight = torch.nn.Parameter(weight, requires_grad=trainable)\n\n        else:\n            weight = torch.FloatTensor(num_embeddings, embedding_dim)\n            self.weight = torch.nn.Parameter(weight, requires_grad=trainable)\n            torch.nn.init.xavier_uniform_(self.weight)\n\n        # Whatever way we have constructed the embedding, it should be consistent with\n        # num_embeddings and embedding_dim.\n        if self.weight.size() != (num_embeddings, embedding_dim):\n            raise ConfigurationError(\n                ""A weight matrix was passed with contradictory embedding shapes.""\n            )\n\n        if self.padding_index is not None:\n            self.weight.data[self.padding_index].fill_(0)\n\n        if projection_dim:\n            self._projection = torch.nn.Linear(embedding_dim, projection_dim)\n        else:\n            self._projection = None\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self.output_dim\n\n    @overrides\n    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n        # tokens may have extra dimensions (batch_size, d1, ..., dn, sequence_length),\n        # but embedding expects (batch_size, sequence_length), so pass tokens to\n        # util.combine_initial_dims (which is a no-op if there are no extra dimensions).\n        # Remember the original size.\n        original_size = tokens.size()\n        tokens = util.combine_initial_dims(tokens)\n\n        embedded = embedding(\n            tokens,\n            self.weight,\n            padding_idx=self.padding_index,\n            max_norm=self.max_norm,\n            norm_type=self.norm_type,\n            scale_grad_by_freq=self.scale_grad_by_freq,\n            sparse=self.sparse,\n        )\n\n        # Now (if necessary) add back in the extra dimensions.\n        embedded = util.uncombine_initial_dims(embedded, original_size)\n\n        if self._projection:\n            projection = self._projection\n            for _ in range(embedded.dim() - 2):\n                projection = TimeDistributed(projection)\n            embedded = projection(embedded)\n        return embedded\n\n    def extend_vocab(\n        self,\n        extended_vocab: Vocabulary,\n        vocab_namespace: str = None,\n        extension_pretrained_file: str = None,\n        model_path: str = None,\n    ):\n        """"""\n        Extends the embedding matrix according to the extended vocabulary.\n        If extension_pretrained_file is available, it will be used for initializing the new words\n        embeddings in the extended vocabulary; otherwise we will check if _pretrained_file attribute\n        is already available. If none is available, they will be initialized with xavier uniform.\n\n        # Parameters\n\n        extended_vocab : `Vocabulary`\n            Vocabulary extended from original vocabulary used to construct\n            this `Embedding`.\n        vocab_namespace : `str`, (optional, default=`None`)\n            In case you know what vocab_namespace should be used for extension, you\n            can pass it. If not passed, it will check if vocab_namespace used at the\n            time of `Embedding` construction is available. If so, this namespace\n            will be used or else extend_vocab will be a no-op.\n        extension_pretrained_file : `str`, (optional, default=`None`)\n            A file containing pretrained embeddings can be specified here. It can be\n            the path to a local file or an URL of a (cached) remote file. Check format\n            details in `from_params` of `Embedding` class.\n        model_path : `str`, (optional, default=`None`)\n            Path traversing the model attributes upto this embedding module.\n            Eg. ""_text_field_embedder.token_embedder_tokens"". This is only useful\n            to give a helpful error message when extend_vocab is implicitly called\n            by train or any other command.\n        """"""\n        # Caveat: For allennlp v0.8.1 and below, we weren\'t storing vocab_namespace as an attribute,\n        # knowing which is necessary at time of embedding vocab extension. So old archive models are\n        # currently unextendable.\n\n        vocab_namespace = vocab_namespace or self._vocab_namespace\n        if not vocab_namespace:\n            # It\'s not safe to default to ""tokens"" or any other namespace.\n            logging.info(\n                ""Loading a model trained before embedding extension was implemented; ""\n                ""pass an explicit vocab namespace if you want to extend the vocabulary.""\n            )\n            return\n\n        extended_num_embeddings = extended_vocab.get_vocab_size(vocab_namespace)\n        if extended_num_embeddings == self.num_embeddings:\n            # It\'s already been extended. No need to initialize / read pretrained file in first place (no-op)\n            return\n\n        if extended_num_embeddings < self.num_embeddings:\n            raise ConfigurationError(\n                f""Size of namespace, {vocab_namespace} for extended_vocab is smaller than ""\n                f""embedding. You likely passed incorrect vocab or namespace for extension.""\n            )\n\n        # Case 1: user passed extension_pretrained_file and it\'s available.\n        if extension_pretrained_file and is_url_or_existing_file(extension_pretrained_file):\n            # Don\'t have to do anything here, this is the happy case.\n            pass\n        # Case 2: user passed extension_pretrained_file and it\'s not available\n        elif extension_pretrained_file:\n            raise ConfigurationError(\n                f""You passed pretrained embedding file {extension_pretrained_file} ""\n                f""for model_path {model_path} but it\'s not available.""\n            )\n        # Case 3: user didn\'t pass extension_pretrained_file, but pretrained_file attribute was\n        # saved during training and is available.\n        elif is_url_or_existing_file(self._pretrained_file):\n            extension_pretrained_file = self._pretrained_file\n        # Case 4: no file is available, hope that pretrained embeddings weren\'t used in the first place and warn\n        else:\n            extra_info = (\n                f""Originally pretrained_file was at "" f""{self._pretrained_file}. ""\n                if self._pretrained_file\n                else """"\n            )\n            # It\'s better to warn here and not give error because there is no way to distinguish between\n            # whether pretrained-file wasn\'t used during training or user forgot to pass / passed incorrect\n            # mapping. Raising an error would prevent fine-tuning in the former case.\n            logging.warning(\n                f""Embedding at model_path, {model_path} cannot locate the pretrained_file. ""\n                f""{extra_info} If you are fine-tuning and want to use using pretrained_file for ""\n                f""embedding extension, please pass the mapping by --embedding-sources argument.""\n            )\n\n        embedding_dim = self.weight.data.shape[-1]\n        if not extension_pretrained_file:\n            extra_num_embeddings = extended_num_embeddings - self.num_embeddings\n            extra_weight = torch.FloatTensor(extra_num_embeddings, embedding_dim)\n            torch.nn.init.xavier_uniform_(extra_weight)\n        else:\n            # It\'s easiest to just reload the embeddings for the entire vocab,\n            # then only keep the ones we need.\n            whole_weight = _read_pretrained_embeddings_file(\n                extension_pretrained_file, embedding_dim, extended_vocab, vocab_namespace\n            )\n            extra_weight = whole_weight[self.num_embeddings :, :]\n\n        device = self.weight.data.device\n        extended_weight = torch.cat([self.weight.data, extra_weight.to(device)], dim=0)\n        self.weight = torch.nn.Parameter(extended_weight, requires_grad=self.weight.requires_grad)\n        self.num_embeddings = extended_num_embeddings\n\n\ndef _read_pretrained_embeddings_file(\n    file_uri: str, embedding_dim: int, vocab: Vocabulary, namespace: str = ""tokens""\n) -> torch.FloatTensor:\n    """"""\n    Returns and embedding matrix for the given vocabulary using the pretrained embeddings\n    contained in the given file. Embeddings for tokens not found in the pretrained embedding file\n    are randomly initialized using a normal distribution with mean and standard deviation equal to\n    those of the pretrained embeddings.\n\n    We support two file formats:\n\n        * text format - utf-8 encoded text file with space separated fields: [word] [dim 1] [dim 2] ...\n          The text file can eventually be compressed, and even resides in an archive with multiple files.\n          If the file resides in an archive with other files, then `embeddings_filename` must\n          be a URI ""(archive_uri)#file_path_inside_the_archive""\n\n        * hdf5 format - hdf5 file containing an embedding matrix in the form of a torch.Tensor.\n\n    If the filename ends with \'.hdf5\' or \'.h5\' then we load from hdf5, otherwise we assume\n    text format.\n\n    # Parameters\n\n    file_uri : `str`, required.\n        It can be:\n\n        * a file system path or a URL of an eventually compressed text file or a zip/tar archive\n          containing a single file.\n\n        * URI of the type `(archive_path_or_url)#file_path_inside_archive` if the text file\n          is contained in a multi-file archive.\n\n    vocab : `Vocabulary`, required.\n        A Vocabulary object.\n    namespace : `str`, (optional, default=`""tokens""`)\n        The namespace of the vocabulary to find pretrained embeddings for.\n    trainable : `bool`, (optional, default=`True`)\n        Whether or not the embedding parameters should be optimized.\n\n    # Returns\n\n    A weight matrix with embeddings initialized from the read file.  The matrix has shape\n    `(vocab.get_vocab_size(namespace), embedding_dim)`, where the indices of words appearing in\n    the pretrained embedding file are initialized to the pretrained embedding value.\n    """"""\n    file_ext = get_file_extension(file_uri)\n    if file_ext in ["".h5"", "".hdf5""]:\n        return _read_embeddings_from_hdf5(file_uri, embedding_dim, vocab, namespace)\n\n    return _read_embeddings_from_text_file(file_uri, embedding_dim, vocab, namespace)\n\n\ndef _read_embeddings_from_text_file(\n    file_uri: str, embedding_dim: int, vocab: Vocabulary, namespace: str = ""tokens""\n) -> torch.FloatTensor:\n    """"""\n    Read pre-trained word vectors from an eventually compressed text file, possibly contained\n    inside an archive with multiple files. The text file is assumed to be utf-8 encoded with\n    space-separated fields: [word] [dim 1] [dim 2] ...\n\n    Lines that contain more numerical tokens than `embedding_dim` raise a warning and are skipped.\n\n    The remainder of the docstring is identical to `_read_pretrained_embeddings_file`.\n    """"""\n    tokens_to_keep = set(vocab.get_index_to_token_vocabulary(namespace).values())\n    vocab_size = vocab.get_vocab_size(namespace)\n    embeddings = {}\n\n    # First we read the embeddings from the file, only keeping vectors for the words we need.\n    logger.info(""Reading pretrained embeddings from file"")\n\n    with EmbeddingsTextFile(file_uri) as embeddings_file:\n        for line in Tqdm.tqdm(embeddings_file):\n            token = line.split("" "", 1)[0]\n            if token in tokens_to_keep:\n                fields = line.rstrip().split("" "")\n                if len(fields) - 1 != embedding_dim:\n                    # Sometimes there are funny unicode parsing problems that lead to different\n                    # fields lengths (e.g., a word with a unicode space character that splits\n                    # into more than one column).  We skip those lines.  Note that if you have\n                    # some kind of long header, this could result in all of your lines getting\n                    # skipped.  It\'s hard to check for that here; you just have to look in the\n                    # embedding_misses_file and at the model summary to make sure things look\n                    # like they are supposed to.\n                    logger.warning(\n                        ""Found line with wrong number of dimensions (expected: %d; actual: %d): %s"",\n                        embedding_dim,\n                        len(fields) - 1,\n                        line,\n                    )\n                    continue\n\n                vector = numpy.asarray(fields[1:], dtype=""float32"")\n                embeddings[token] = vector\n\n    if not embeddings:\n        raise ConfigurationError(\n            ""No embeddings of correct dimension found; you probably ""\n            ""misspecified your embedding_dim parameter, or didn\'t ""\n            ""pre-populate your Vocabulary""\n        )\n\n    all_embeddings = numpy.asarray(list(embeddings.values()))\n    embeddings_mean = float(numpy.mean(all_embeddings))\n    embeddings_std = float(numpy.std(all_embeddings))\n    # Now we initialize the weight matrix for an embedding layer, starting with random vectors,\n    # then filling in the word vectors we just read.\n    logger.info(""Initializing pre-trained embedding layer"")\n    embedding_matrix = torch.FloatTensor(vocab_size, embedding_dim).normal_(\n        embeddings_mean, embeddings_std\n    )\n    num_tokens_found = 0\n    index_to_token = vocab.get_index_to_token_vocabulary(namespace)\n    for i in range(vocab_size):\n        token = index_to_token[i]\n\n        # If we don\'t have a pre-trained vector for this word, we\'ll just leave this row alone,\n        # so the word has a random initialization.\n        if token in embeddings:\n            embedding_matrix[i] = torch.FloatTensor(embeddings[token])\n            num_tokens_found += 1\n        else:\n            logger.debug(\n                ""Token %s was not found in the embedding file. Initialising randomly."", token\n            )\n\n    logger.info(\n        ""Pretrained embeddings were found for %d out of %d tokens"", num_tokens_found, vocab_size\n    )\n\n    return embedding_matrix\n\n\ndef _read_embeddings_from_hdf5(\n    embeddings_filename: str, embedding_dim: int, vocab: Vocabulary, namespace: str = ""tokens""\n) -> torch.FloatTensor:\n    """"""\n    Reads from a hdf5 formatted file. The embedding matrix is assumed to\n    be keyed by \'embedding\' and of size `(num_tokens, embedding_dim)`.\n    """"""\n    with h5py.File(embeddings_filename, ""r"") as fin:\n        embeddings = fin[""embedding""][...]\n\n    if list(embeddings.shape) != [vocab.get_vocab_size(namespace), embedding_dim]:\n        raise ConfigurationError(\n            ""Read shape {0} embeddings from the file, but expected {1}"".format(\n                list(embeddings.shape), [vocab.get_vocab_size(namespace), embedding_dim]\n            )\n        )\n\n    return torch.FloatTensor(embeddings)\n\n\ndef format_embeddings_file_uri(\n    main_file_path_or_url: str, path_inside_archive: Optional[str] = None\n) -> str:\n    if path_inside_archive:\n        return ""({})#{}"".format(main_file_path_or_url, path_inside_archive)\n    return main_file_path_or_url\n\n\nclass EmbeddingsFileURI(NamedTuple):\n    main_file_uri: str\n    path_inside_archive: Optional[str] = None\n\n\ndef parse_embeddings_file_uri(uri: str) -> ""EmbeddingsFileURI"":\n    match = re.fullmatch(r""\\((.*)\\)#(.*)"", uri)\n    if match:\n        fields = cast(Tuple[str, str], match.groups())\n        return EmbeddingsFileURI(*fields)\n    else:\n        return EmbeddingsFileURI(uri, None)\n\n\nclass EmbeddingsTextFile(Iterator[str]):\n    """"""\n    Utility class for opening embeddings text files. Handles various compression formats,\n    as well as context management.\n\n    # Parameters\n\n    file_uri : `str`\n        It can be:\n\n        * a file system path or a URL of an eventually compressed text file or a zip/tar archive\n          containing a single file.\n        * URI of the type `(archive_path_or_url)#file_path_inside_archive` if the text file\n          is contained in a multi-file archive.\n\n    encoding : `str`\n    cache_dir : `str`\n    """"""\n\n    DEFAULT_ENCODING = ""utf-8""\n\n    def __init__(\n        self, file_uri: str, encoding: str = DEFAULT_ENCODING, cache_dir: str = None\n    ) -> None:\n\n        self.uri = file_uri\n        self._encoding = encoding\n        self._cache_dir = cache_dir\n        self._archive_handle: Any = None  # only if the file is inside an archive\n\n        main_file_uri, path_inside_archive = parse_embeddings_file_uri(file_uri)\n        main_file_local_path = cached_path(main_file_uri, cache_dir=cache_dir)\n\n        if zipfile.is_zipfile(main_file_local_path):  # ZIP archive\n            self._open_inside_zip(main_file_uri, path_inside_archive)\n\n        elif tarfile.is_tarfile(main_file_local_path):  # TAR archive\n            self._open_inside_tar(main_file_uri, path_inside_archive)\n\n        else:  # all the other supported formats, including uncompressed files\n            if path_inside_archive:\n                raise ValueError(""Unsupported archive format: %s"" + main_file_uri)\n\n            # All the python packages for compressed files share the same interface of io.open\n            extension = get_file_extension(main_file_uri)\n\n            # Some systems don\'t have support for all of these libraries, so we import them only\n            # when necessary.\n            package = None\n            if extension in ["".txt"", "".vec""]:\n                package = io\n            elif extension == "".gz"":\n                import gzip\n\n                package = gzip\n            elif extension == "".bz2"":\n                import bz2\n\n                package = bz2\n            elif extension == "".lzma"":\n                import lzma\n\n                package = lzma\n\n            if package is None:\n                logger.warning(\n                    \'The embeddings file has an unknown file extension ""%s"". \'\n                    ""We will assume the file is an (uncompressed) text file"",\n                    extension,\n                )\n                package = io\n\n            self._handle = package.open(  # type: ignore\n                main_file_local_path, ""rt"", encoding=encoding\n            )\n\n        # To use this with tqdm we\'d like to know the number of tokens. It\'s possible that the\n        # first line of the embeddings file contains this: if it does, we want to start iteration\n        # from the 2nd line, otherwise we want to start from the 1st.\n        # Unfortunately, once we read the first line, we cannot move back the file iterator\n        # because the underlying file may be ""not seekable""; we use itertools.chain instead.\n        first_line = next(self._handle)  # this moves the iterator forward\n        self.num_tokens = EmbeddingsTextFile._get_num_tokens_from_first_line(first_line)\n        if self.num_tokens:\n            # the first line is a header line: start iterating from the 2nd line\n            self._iterator = self._handle\n        else:\n            # the first line is not a header line: start iterating from the 1st line\n            self._iterator = itertools.chain([first_line], self._handle)\n\n    def _open_inside_zip(self, archive_path: str, member_path: Optional[str] = None) -> None:\n        cached_archive_path = cached_path(archive_path, cache_dir=self._cache_dir)\n        archive = zipfile.ZipFile(cached_archive_path, ""r"")\n        if member_path is None:\n            members_list = archive.namelist()\n            member_path = self._get_the_only_file_in_the_archive(members_list, archive_path)\n        member_path = cast(str, member_path)\n        member_file = cast(BinaryIO, archive.open(member_path, ""r""))\n        self._handle = io.TextIOWrapper(member_file, encoding=self._encoding)\n        self._archive_handle = archive\n\n    def _open_inside_tar(self, archive_path: str, member_path: Optional[str] = None) -> None:\n        cached_archive_path = cached_path(archive_path, cache_dir=self._cache_dir)\n        archive = tarfile.open(cached_archive_path, ""r"")\n        if member_path is None:\n            members_list = archive.getnames()\n            member_path = self._get_the_only_file_in_the_archive(members_list, archive_path)\n        member_path = cast(str, member_path)\n        member = archive.getmember(member_path)  # raises exception if not present\n        member_file = cast(BinaryIO, archive.extractfile(member))\n        self._handle = io.TextIOWrapper(member_file, encoding=self._encoding)\n        self._archive_handle = archive\n\n    def read(self) -> str:\n        return """".join(self._iterator)\n\n    def readline(self) -> str:\n        return next(self._iterator)\n\n    def close(self) -> None:\n        self._handle.close()\n        if self._archive_handle:\n            self._archive_handle.close()\n\n    def __enter__(self) -> ""EmbeddingsTextFile"":\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n        self.close()\n\n    def __iter__(self) -> ""EmbeddingsTextFile"":\n        return self\n\n    def __next__(self) -> str:\n        return next(self._iterator)\n\n    def __len__(self) -> Optional[int]:\n        if self.num_tokens:\n            return self.num_tokens\n        raise AttributeError(\n            ""an object of type EmbeddingsTextFile implements `__len__` only if the underlying ""\n            ""text file declares the number of tokens (i.e. the number of lines following)""\n            ""in the first line. That is not the case of this particular instance.""\n        )\n\n    @staticmethod\n    def _get_the_only_file_in_the_archive(members_list: Sequence[str], archive_path: str) -> str:\n        if len(members_list) > 1:\n            raise ValueError(\n                ""The archive %s contains multiple files, so you must select ""\n                ""one of the files inside providing a uri of the type: %s.""\n                % (\n                    archive_path,\n                    format_embeddings_file_uri(""path_or_url_to_archive"", ""path_inside_archive""),\n                )\n            )\n        return members_list[0]\n\n    @staticmethod\n    def _get_num_tokens_from_first_line(line: str) -> Optional[int]:\n        """""" This function takes in input a string and if it contains 1 or 2 integers, it assumes the\n        largest one it the number of tokens. Returns None if the line doesn\'t match that pattern. """"""\n        fields = line.split("" "")\n        if 1 <= len(fields) <= 2:\n            try:\n                int_fields = [int(x) for x in fields]\n            except ValueError:\n                return None\n            else:\n                num_tokens = max(int_fields)\n                logger.info(\n                    ""Recognized a header line in the embedding file with number of tokens: %d"",\n                    num_tokens,\n                )\n                return num_tokens\n        return None\n'"
allennlp/modules/token_embedders/empty_embedder.py,1,"b'import torch\nfrom allennlp.modules.token_embedders.token_embedder import TokenEmbedder\n\n\n@TokenEmbedder.register(""empty"")\nclass EmptyEmbedder(TokenEmbedder):\n    """"""\n    Assumes you want to completely ignore the output of a `TokenIndexer` for some reason, and does\n    not return anything when asked to embed it.\n\n    You should almost never need to use this; normally you would just not use a particular\n    `TokenIndexer`. It\'s only in very rare cases, like simplicity in data processing for language\n    modeling (where we use just one `TextField` to handle input embedding and computing target ids),\n    where you might want to use this.\n\n    Registered as a `TokenEmbedder` with name ""empty"".\n    """"""\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def get_output_dim(self):\n        return 0\n\n    def forward(self, *inputs, **kwargs) -> torch.Tensor:\n        return None\n'"
allennlp/modules/token_embedders/pass_through_token_embedder.py,1,"b'import torch\nfrom allennlp.modules.token_embedders.token_embedder import TokenEmbedder\n\n\n@TokenEmbedder.register(""pass_through"")\nclass PassThroughTokenEmbedder(TokenEmbedder):\n    """"""\n    Assumes that the input is already vectorized in some way,\n    and just returns it.\n\n    Registered as a `TokenEmbedder` with name ""pass_through"".\n\n    # Parameters\n\n    hidden_dim : `int`, required.\n\n    """"""\n\n    def __init__(self, hidden_dim: int) -> None:\n        self.hidden_dim = hidden_dim\n        super().__init__()\n\n    def get_output_dim(self):\n        return self.hidden_dim\n\n    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n        return tokens\n'"
allennlp/modules/token_embedders/pretrained_transformer_embedder.py,31,"b'import math\nfrom typing import Optional, Tuple\n\nfrom overrides import overrides\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import XLNetConfig\nfrom transformers.modeling_auto import AutoModel\n\nfrom allennlp.data.tokenizers import PretrainedTransformerTokenizer\nfrom allennlp.modules.token_embedders.token_embedder import TokenEmbedder\nfrom allennlp.nn.util import batched_index_select\n\n\n@TokenEmbedder.register(""pretrained_transformer"")\nclass PretrainedTransformerEmbedder(TokenEmbedder):\n    """"""\n    Uses a pretrained model from `transformers` as a `TokenEmbedder`.\n\n    Registered as a `TokenEmbedder` with name ""pretrained_transformer"".\n\n    # Parameters\n\n    model_name : `str`\n        The name of the `transformers` model to use. Should be the same as the corresponding\n        `PretrainedTransformerIndexer`.\n    max_length : `int`, optional (default = `None`)\n        If positive, folds input token IDs into multiple segments of this length, pass them\n        through the transformer model independently, and concatenate the final representations.\n        Should be set to the same value as the `max_length` option on the\n        `PretrainedTransformerIndexer`.\n    sub_module: `str`, optional (default = `None`)\n        The name of a submodule of the transformer to be used as the embedder. Some transformers naturally act\n        as embedders such as BERT. However, other models consist of encoder and decoder, in which case we just\n        want to use the encoder.\n    train_parameters: `bool`, optional (default = `True`)\n        If this is `True`, the transformer weights get updated during training.\n    """"""\n\n    def __init__(\n        self,\n        model_name: str,\n        max_length: int = None,\n        sub_module: str = None,\n        train_parameters: bool = True,\n    ) -> None:\n        super().__init__()\n        self.transformer_model = AutoModel.from_pretrained(model_name)\n        self.config = self.transformer_model.config\n        if sub_module:\n            assert hasattr(self.transformer_model, sub_module)\n            self.transformer_model = getattr(self.transformer_model, sub_module)\n        self._max_length = max_length\n        # I\'m not sure if this works for all models; open an issue on github if you find a case\n        # where it doesn\'t work.\n        self.output_dim = self.config.hidden_size\n        self._train_parameters = train_parameters\n\n        tokenizer = PretrainedTransformerTokenizer(model_name)\n        self._num_added_start_tokens = len(tokenizer.single_sequence_start_tokens)\n        self._num_added_end_tokens = len(tokenizer.single_sequence_end_tokens)\n        self._num_added_tokens = self._num_added_start_tokens + self._num_added_end_tokens\n\n    @overrides\n    def get_output_dim(self):\n        return self.output_dim\n\n    def _number_of_token_type_embeddings(self):\n        if isinstance(self.config, XLNetConfig):\n            return 3  # XLNet has 3 type ids\n        elif hasattr(self.config, ""type_vocab_size""):\n            return self.config.type_vocab_size\n        else:\n            return 0\n\n    @overrides\n    def forward(\n        self,\n        token_ids: torch.LongTensor,\n        mask: torch.BoolTensor,\n        type_ids: Optional[torch.LongTensor] = None,\n        segment_concat_mask: Optional[torch.BoolTensor] = None,\n    ) -> torch.Tensor:  # type: ignore\n        """"""\n        # Parameters\n\n        token_ids: `torch.LongTensor`\n            Shape: `[batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces]`.\n            num_segment_concat_wordpieces is num_wordpieces plus special tokens inserted in the\n            middle, e.g. the length of: ""[CLS] A B C [SEP] [CLS] D E F [SEP]"" (see indexer logic).\n        mask: `torch.BoolTensor`\n            Shape: [batch_size, num_wordpieces].\n        type_ids: `Optional[torch.LongTensor]`\n            Shape: `[batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces]`.\n        segment_concat_mask: `Optional[torch.BoolTensor]`\n            Shape: `[batch_size, num_segment_concat_wordpieces]`.\n\n        # Returns\n\n        `torch.Tensor`\n            Shape: `[batch_size, num_wordpieces, embedding_size]`.\n\n        """"""\n\n        with torch.set_grad_enabled(self._train_parameters):\n            # Some of the huggingface transformers don\'t support type ids at all and crash when you supply\n            # them. For others, you can supply a tensor of zeros, and if you don\'t, they act as if you did.\n            # There is no practical difference to the caller, so here we pretend that one case is the same\n            # as another case.\n            if type_ids is not None:\n                max_type_id = type_ids.max()\n                if max_type_id == 0:\n                    type_ids = None\n                else:\n                    if max_type_id >= self._number_of_token_type_embeddings():\n                        raise ValueError(\n                            ""Found type ids too large for the chosen transformer model.""\n                        )\n                    assert token_ids.shape == type_ids.shape\n\n            fold_long_sequences = (\n                self._max_length is not None and token_ids.size(1) > self._max_length\n            )\n            if fold_long_sequences:\n                batch_size, num_segment_concat_wordpieces = token_ids.size()\n                token_ids, segment_concat_mask, type_ids = self._fold_long_sequences(\n                    token_ids, segment_concat_mask, type_ids\n                )\n\n            transformer_mask = segment_concat_mask if self._max_length is not None else mask\n            # Shape: [batch_size, num_wordpieces, embedding_size],\n            # or if self._max_length is not None:\n            # [batch_size * num_segments, self._max_length, embedding_size]\n\n            # We call this with kwargs because some of the huggingface models don\'t have the\n            # token_type_ids parameter and fail even when it\'s given as None.\n            # Also, as of transformers v2.5.1, they are taking FloatTensor masks.\n            parameters = {""input_ids"": token_ids, ""attention_mask"": transformer_mask.float()}\n            if type_ids is not None:\n                parameters[""token_type_ids""] = type_ids\n            embeddings = self.transformer_model(**parameters)[0]\n\n            if fold_long_sequences:\n                embeddings = self._unfold_long_sequences(\n                    embeddings, segment_concat_mask, batch_size, num_segment_concat_wordpieces\n                )\n\n            return embeddings\n\n    def _fold_long_sequences(\n        self,\n        token_ids: torch.LongTensor,\n        mask: torch.BoolTensor,\n        type_ids: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.LongTensor, torch.LongTensor, Optional[torch.LongTensor]]:\n        """"""\n        We fold 1D sequences (for each element in batch), returned by `PretrainedTransformerIndexer`\n        that are in reality multiple segments concatenated together, to 2D tensors, e.g.\n\n        [ [CLS] A B C [SEP] [CLS] D E [SEP] ]\n        -> [ [ [CLS] A B C [SEP] ], [ [CLS] D E [SEP] [PAD] ] ]\n        The [PAD] positions can be found in the returned `mask`.\n\n        # Parameters\n\n        token_ids: `torch.LongTensor`\n            Shape: `[batch_size, num_segment_concat_wordpieces]`.\n            num_segment_concat_wordpieces is num_wordpieces plus special tokens inserted in the\n            middle, i.e. the length of: ""[CLS] A B C [SEP] [CLS] D E F [SEP]"" (see indexer logic).\n        mask: `torch.BoolTensor`\n            Shape: `[batch_size, num_segment_concat_wordpieces]`.\n            The mask for the concatenated segments of wordpieces. The same as `segment_concat_mask`\n            in `forward()`.\n        type_ids: `Optional[torch.LongTensor]`\n            Shape: [batch_size, num_segment_concat_wordpieces].\n\n        # Returns:\n\n        token_ids: `torch.LongTensor`\n            Shape: [batch_size * num_segments, self._max_length].\n        mask: `torch.BoolTensor`\n            Shape: [batch_size * num_segments, self._max_length].\n        """"""\n        num_segment_concat_wordpieces = token_ids.size(1)\n        num_segments = math.ceil(num_segment_concat_wordpieces / self._max_length)\n        padded_length = num_segments * self._max_length\n        length_to_pad = padded_length - num_segment_concat_wordpieces\n\n        def fold(tensor):  # Shape: [batch_size, num_segment_concat_wordpieces]\n            # Shape: [batch_size, num_segments * self._max_length]\n            tensor = F.pad(tensor, [0, length_to_pad], value=0)\n            # Shape: [batch_size * num_segments, self._max_length]\n            return tensor.reshape(-1, self._max_length)\n\n        return fold(token_ids), fold(mask), fold(type_ids) if type_ids is not None else None\n\n    def _unfold_long_sequences(\n        self,\n        embeddings: torch.FloatTensor,\n        mask: torch.BoolTensor,\n        batch_size: int,\n        num_segment_concat_wordpieces: int,\n    ) -> torch.FloatTensor:\n        """"""\n        We take 2D segments of a long sequence and flatten them out to get the whole sequence\n        representation while remove unnecessary special tokens.\n\n        [ [ [CLS]_emb A_emb B_emb C_emb [SEP]_emb ], [ [CLS]_emb D_emb E_emb [SEP]_emb [PAD]_emb ] ]\n        -> [ [CLS]_emb A_emb B_emb C_emb D_emb E_emb [SEP]_emb ]\n\n        We truncate the start and end tokens for all segments, recombine the segments,\n        and manually add back the start and end tokens.\n\n        # Parameters\n\n        embeddings: `torch.FloatTensor`\n            Shape: [batch_size * num_segments, self._max_length, embedding_size].\n        mask: `torch.BoolTensor`\n            Shape: [batch_size * num_segments, self._max_length].\n            The mask for the concatenated segments of wordpieces. The same as `segment_concat_mask`\n            in `forward()`.\n        batch_size: `int`\n        num_segment_concat_wordpieces: `int`\n            The length of the original ""[ [CLS] A B C [SEP] [CLS] D E F [SEP] ]"", i.e.\n            the original `token_ids.size(1)`.\n\n        # Returns:\n\n        embeddings: `torch.FloatTensor`\n            Shape: [batch_size, self._num_wordpieces, embedding_size].\n        """"""\n\n        def lengths_to_mask(lengths, max_len, device):\n            return torch.arange(max_len, device=device).expand(\n                lengths.size(0), max_len\n            ) < lengths.unsqueeze(1)\n\n        device = embeddings.device\n        num_segments = int(embeddings.size(0) / batch_size)\n        embedding_size = embeddings.size(2)\n\n        # We want to remove all segment-level special tokens but maintain sequence-level ones\n        num_wordpieces = num_segment_concat_wordpieces - (num_segments - 1) * self._num_added_tokens\n\n        embeddings = embeddings.reshape(batch_size, num_segments * self._max_length, embedding_size)\n        mask = mask.reshape(batch_size, num_segments * self._max_length)\n        # We assume that all 1s in the mask precede all 0s, and add an assert for that.\n        # Open an issue on GitHub if this breaks for you.\n        # Shape: (batch_size,)\n        seq_lengths = mask.sum(-1)\n        if not (lengths_to_mask(seq_lengths, mask.size(1), device) == mask).all():\n            raise ValueError(\n                ""Long sequence splitting only supports masks with all 1s preceding all 0s.""\n            )\n        # Shape: (batch_size, self._num_added_end_tokens); this is a broadcast op\n        end_token_indices = (\n            seq_lengths.unsqueeze(-1) - torch.arange(self._num_added_end_tokens, device=device) - 1\n        )\n\n        # Shape: (batch_size, self._num_added_start_tokens, embedding_size)\n        start_token_embeddings = embeddings[:, : self._num_added_start_tokens, :]\n        # Shape: (batch_size, self._num_added_end_tokens, embedding_size)\n        end_token_embeddings = batched_index_select(embeddings, end_token_indices)\n\n        embeddings = embeddings.reshape(batch_size, num_segments, self._max_length, embedding_size)\n        embeddings = embeddings[\n            :, :, self._num_added_start_tokens : -self._num_added_end_tokens, :\n        ]  # truncate segment-level start/end tokens\n        embeddings = embeddings.reshape(batch_size, -1, embedding_size)  # flatten\n\n        # Now try to put end token embeddings back which is a little tricky.\n\n        # The number of segment each sequence spans, excluding padding. Mimicking ceiling operation.\n        # Shape: (batch_size,)\n        num_effective_segments = (seq_lengths + self._max_length - 1) / self._max_length\n        # The number of indices that end tokens should shift back.\n        num_removed_non_end_tokens = (\n            num_effective_segments * self._num_added_tokens - self._num_added_end_tokens\n        )\n        # Shape: (batch_size, self._num_added_end_tokens)\n        end_token_indices -= num_removed_non_end_tokens.unsqueeze(-1)\n        assert (end_token_indices >= self._num_added_start_tokens).all()\n        # Add space for end embeddings\n        embeddings = torch.cat([embeddings, torch.zeros_like(end_token_embeddings)], 1)\n        # Add end token embeddings back\n        embeddings.scatter_(\n            1, end_token_indices.unsqueeze(-1).expand_as(end_token_embeddings), end_token_embeddings\n        )\n\n        # Now put back start tokens. We can do this before putting back end tokens, but then\n        # we need to change `num_removed_non_end_tokens` a little.\n        embeddings = torch.cat([start_token_embeddings, embeddings], 1)\n\n        # Truncate to original length\n        embeddings = embeddings[:, :num_wordpieces, :]\n        return embeddings\n'"
allennlp/modules/token_embedders/pretrained_transformer_mismatched_embedder.py,14,"b'from typing import Optional\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.modules.token_embedders import PretrainedTransformerEmbedder, TokenEmbedder\nfrom allennlp.nn import util\n\n\n@TokenEmbedder.register(""pretrained_transformer_mismatched"")\nclass PretrainedTransformerMismatchedEmbedder(TokenEmbedder):\n    """"""\n    Use this embedder to embed wordpieces given by `PretrainedTransformerMismatchedIndexer`\n    and to pool the resulting vectors to get word-level representations.\n\n    Registered as a `TokenEmbedder` with name ""pretrained_transformer_mismatchd"".\n\n    # Parameters\n\n    model_name : `str`\n        The name of the `transformers` model to use. Should be the same as the corresponding\n        `PretrainedTransformerMismatchedIndexer`.\n    max_length : `int`, optional (default = `None`)\n        If positive, folds input token IDs into multiple segments of this length, pass them\n        through the transformer model independently, and concatenate the final representations.\n        Should be set to the same value as the `max_length` option on the\n        `PretrainedTransformerMismatchedIndexer`.\n    train_parameters: `bool`, optional (default = `True`)\n        If this is `True`, the transformer weights get updated during training.\n    """"""\n\n    def __init__(\n        self, model_name: str, max_length: int = None, train_parameters: bool = True\n    ) -> None:\n        super().__init__()\n        # The matched version v.s. mismatched\n        self._matched_embedder = PretrainedTransformerEmbedder(\n            model_name, max_length, train_parameters=train_parameters\n        )\n\n    @overrides\n    def get_output_dim(self):\n        return self._matched_embedder.get_output_dim()\n\n    @overrides\n    def forward(\n        self,\n        token_ids: torch.LongTensor,\n        mask: torch.BoolTensor,\n        offsets: torch.LongTensor,\n        wordpiece_mask: torch.BoolTensor,\n        type_ids: Optional[torch.LongTensor] = None,\n        segment_concat_mask: Optional[torch.BoolTensor] = None,\n    ) -> torch.Tensor:  # type: ignore\n        """"""\n        # Parameters\n\n        token_ids: `torch.LongTensor`\n            Shape: [batch_size, num_wordpieces] (for exception see `PretrainedTransformerEmbedder`).\n        mask: `torch.BoolTensor`\n            Shape: [batch_size, num_orig_tokens].\n        offsets: `torch.LongTensor`\n            Shape: [batch_size, num_orig_tokens, 2].\n            Maps indices for the original tokens, i.e. those given as input to the indexer,\n            to a span in token_ids. `token_ids[i][offsets[i][j][0]:offsets[i][j][1] + 1]`\n            corresponds to the original j-th token from the i-th batch.\n        wordpiece_mask: `torch.BoolTensor`\n            Shape: [batch_size, num_wordpieces].\n        type_ids: `Optional[torch.LongTensor]`\n            Shape: [batch_size, num_wordpieces].\n        segment_concat_mask: `Optional[torch.BoolTensor]`\n            See `PretrainedTransformerEmbedder`.\n\n        # Returns\n\n        `torch.Tensor`\n            Shape: [batch_size, num_orig_tokens, embedding_size].\n        """"""\n        # Shape: [batch_size, num_wordpieces, embedding_size].\n        embeddings = self._matched_embedder(\n            token_ids, wordpiece_mask, type_ids=type_ids, segment_concat_mask=segment_concat_mask\n        )\n\n        # span_embeddings: (batch_size, num_orig_tokens, max_span_length, embedding_size)\n        # span_mask: (batch_size, num_orig_tokens, max_span_length)\n        span_embeddings, span_mask = util.batched_span_select(embeddings.contiguous(), offsets)\n        span_mask = span_mask.unsqueeze(-1)\n        span_embeddings *= span_mask  # zero out paddings\n\n        span_embeddings_sum = span_embeddings.sum(2)\n        span_embeddings_len = span_mask.sum(2)\n        # Shape: (batch_size, num_orig_tokens, embedding_size)\n        orig_embeddings = span_embeddings_sum / span_embeddings_len\n\n        # All the places where the span length is zero, write in zeros.\n        orig_embeddings[(span_embeddings_len == 0).expand(orig_embeddings.shape)] = 0\n\n        return orig_embeddings\n'"
allennlp/modules/token_embedders/token_characters_encoder.py,2,"b'import torch\n\nfrom allennlp.modules.token_embedders.embedding import Embedding\nfrom allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder\nfrom allennlp.modules.time_distributed import TimeDistributed\nfrom allennlp.modules.token_embedders.token_embedder import TokenEmbedder\n\n\n@TokenEmbedder.register(""character_encoding"")\nclass TokenCharactersEncoder(TokenEmbedder):\n    """"""\n    A `TokenCharactersEncoder` takes the output of a\n    [`TokenCharactersIndexer`](../../data/token_indexers/token_characters_indexer.md), which is a tensor of shape\n    (batch_size, num_tokens, num_characters), embeds the characters, runs a token-level encoder, and\n    returns the result, which is a tensor of shape (batch_size, num_tokens, encoding_dim).  We also\n    optionally apply dropout after the token-level encoder.\n\n    We take the embedding and encoding modules as input, so this class is itself quite simple.\n\n    Registered as a `TokenEmbedder` with name ""character_encoding"".\n    """"""\n\n    def __init__(self, embedding: Embedding, encoder: Seq2VecEncoder, dropout: float = 0.0) -> None:\n        super().__init__()\n        self._embedding = TimeDistributed(embedding)\n        self._encoder = TimeDistributed(encoder)\n        if dropout > 0:\n            self._dropout = torch.nn.Dropout(p=dropout)\n        else:\n            self._dropout = lambda x: x\n\n    def get_output_dim(self) -> int:\n        return self._encoder._module.get_output_dim()\n\n    def forward(self, token_characters: torch.Tensor) -> torch.Tensor:\n        mask = (token_characters != 0).long()\n        return self._dropout(self._encoder(self._embedding(token_characters), mask))\n'"
allennlp/modules/token_embedders/token_embedder.py,1,"b'import torch\n\nfrom allennlp.common import Registrable\n\n\nclass TokenEmbedder(torch.nn.Module, Registrable):\n    """"""\n    A `TokenEmbedder` is a `Module` that takes as input a tensor with integer ids that have\n    been output from a [`TokenIndexer`](../../data/token_indexers/token_indexer.md) and outputs\n    a vector per token in the input.  The input typically has shape `(batch_size, num_tokens)`\n    or `(batch_size, num_tokens, num_characters)`, and the output is of shape `(batch_size, num_tokens,\n    output_dim)`.  The simplest `TokenEmbedder` is just an embedding layer, but for\n    character-level input, it could also be some kind of character encoder.\n\n    We add a single method to the basic `Module` API: `get_output_dim()`.  This lets us\n    more easily compute output dimensions for the\n    [`TextFieldEmbedder`](../../text_field_embedders/text_field_embedder.md),\n    which we might need when defining model parameters such as LSTMs or linear layers, which need\n    to know their input dimension before the layers are called.\n    """"""\n\n    default_implementation = ""embedding""\n\n    def get_output_dim(self) -> int:\n        """"""\n        Returns the final output dimension that this `TokenEmbedder` uses to represent each\n        token.  This is `not` the shape of the returned tensor, but the last element of that shape.\n        """"""\n        raise NotImplementedError\n'"
allennlp/nn/regularizers/__init__.py,0,"b'""""""\nThis module contains classes representing regularization schemes\nas well as a class for applying regularization to parameters.\n""""""\n\nfrom allennlp.nn.regularizers.regularizer import Regularizer\nfrom allennlp.nn.regularizers.regularizers import L1Regularizer\nfrom allennlp.nn.regularizers.regularizers import L2Regularizer\nfrom allennlp.nn.regularizers.regularizer_applicator import RegularizerApplicator\n'"
allennlp/nn/regularizers/regularizer.py,1,"b'import torch\n\nfrom allennlp.common import Registrable\n\n\nclass Regularizer(Registrable):\n    """"""\n    An abstract class representing a regularizer. It must implement\n    call, returning a scalar tensor.\n    """"""\n\n    default_implementation = ""l2""\n\n    def __call__(self, parameter: torch.Tensor) -> torch.Tensor:\n        raise NotImplementedError\n'"
allennlp/nn/regularizers/regularizer_applicator.py,2,"b'import re\nfrom typing import List, Tuple\n\nimport torch\n\nfrom allennlp.common import FromParams\nfrom allennlp.nn.regularizers.regularizer import Regularizer\n\n\nclass RegularizerApplicator(FromParams):\n    """"""\n    Applies regularizers to the parameters of a Module based on regex matches.\n    """"""\n\n    def __init__(self, regexes: List[Tuple[str, Regularizer]] = None) -> None:\n        """"""\n        # Parameters\n\n        regexes : `List[Tuple[str, Regularizer]]`, optional (default = `None`)\n            A sequence of pairs (regex, Regularizer), where each Regularizer\n            applies to the parameters its regex matches (and that haven\'t previously\n            been matched).\n        """"""\n        self._regularizers = regexes or []\n\n    def __call__(self, module: torch.nn.Module) -> torch.Tensor:\n        """"""\n        # Parameters\n\n        module : `torch.nn.Module`, required\n            The module to regularize.\n        """"""\n        accumulator = 0.0\n        for name, parameter in module.named_parameters():\n            # We first check if the parameter needs gradient updates or not\n            if parameter.requires_grad:\n                # For each parameter find the first matching regex.\n                for regex, regularizer in self._regularizers:\n                    if re.search(regex, name):\n                        penalty = regularizer(parameter)\n                        accumulator = accumulator + penalty\n                        break\n        return accumulator\n'"
allennlp/nn/regularizers/regularizers.py,4,"b'import torch\n\nfrom allennlp.nn.regularizers.regularizer import Regularizer\n\n\n@Regularizer.register(""l1"")\nclass L1Regularizer(Regularizer):\n    """"""\n    Represents a penalty proportional to the sum of the absolute values of the parameters\n\n    Registered as a `Regularizer` with name ""l1"".\n    """"""\n\n    def __init__(self, alpha: float = 0.01) -> None:\n        self.alpha = alpha\n\n    def __call__(self, parameter: torch.Tensor) -> torch.Tensor:\n        return self.alpha * torch.sum(torch.abs(parameter))\n\n\n@Regularizer.register(""l2"")\nclass L2Regularizer(Regularizer):\n    """"""\n    Represents a penalty proportional to the sum of squared values of the parameters\n\n    Registered as a `Regularizer` with name ""l2"".\n    """"""\n\n    def __init__(self, alpha: float = 0.01) -> None:\n        self.alpha = alpha\n\n    def __call__(self, parameter: torch.Tensor) -> torch.Tensor:\n        return self.alpha * torch.sum(torch.pow(parameter, 2))\n'"
allennlp/training/learning_rate_schedulers/__init__.py,5,"b'""""""\nAllenNLP uses most\n`PyTorch learning rate schedulers <https://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate>`_,\nwith a thin wrapper to allow registering them and instantiating them `from_params`.\n\nThe available learning rate schedulers from PyTorch are\n\n* `""step"" <https://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.StepLR>`_\n* `""multi_step"" <https://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.MultiStepLR>`_\n* `""exponential"" <https://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.ExponentialLR>`_\n* `""reduce_on_plateau"" <https://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau>`_\n\nIn addition, AllenNLP also provides `cosine with restarts <https://arxiv.org/abs/1608.03983>`_,\na Noam schedule, and a slanted triangular schedule, which are registered as\n""cosine"", ""noam"", and ""slanted_triangular"", respectively.\n""""""\n\nfrom allennlp.training.learning_rate_schedulers.learning_rate_scheduler import (\n    LearningRateScheduler,\n    StepLearningRateScheduler,\n    MultiStepLearningRateScheduler,\n    ExponentialLearningRateScheduler,\n    ReduceOnPlateauLearningRateScheduler,\n)\nfrom allennlp.training.learning_rate_schedulers.cosine import CosineWithRestarts\nfrom allennlp.training.learning_rate_schedulers.noam import NoamLR\nfrom allennlp.training.learning_rate_schedulers.slanted_triangular import SlantedTriangular\nfrom allennlp.training.learning_rate_schedulers.polynomial_decay import PolynomialDecay\n'"
allennlp/training/learning_rate_schedulers/cosine.py,2,"b'import logging\n\nfrom overrides import overrides\nimport numpy as np\nimport torch\n\nfrom allennlp.training.learning_rate_schedulers.learning_rate_scheduler import LearningRateScheduler\n\n\nlogger = logging.getLogger(__name__)\n\n\n@LearningRateScheduler.register(""cosine"")\nclass CosineWithRestarts(LearningRateScheduler):\n    """"""\n    Cosine annealing with restarts.\n\n    This is described in the paper https://arxiv.org/abs/1608.03983. Note that early\n    stopping should typically be avoided when using this schedule.\n\n    Registered as a `LearningRateScheduler` with name ""cosine"".\n\n    # Parameters\n\n    optimizer : `torch.optim.Optimizer`\n        This argument does not get an entry in a configuration file for the object.\n    t_initial : `int`\n        The number of iterations (epochs) within the first cycle.\n    t_mul : `float`, optional (default=`1`)\n        Determines the number of iterations (epochs) in the i-th decay cycle,\n        which is the length of the last cycle multiplied by `t_mul`.\n    eta_min : `float`, optional (default=`0`)\n        The minimum learning rate.\n    eta_mul : `float`, optional (default=`1`)\n        Determines the initial learning rate for the i-th decay cycle, which is the\n        last initial learning rate multiplied by `m_mul`.\n    last_epoch : `int`, optional (default=`-1`)\n        The index of the last epoch. This is used when restarting.\n    """"""\n\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        t_initial: int,\n        t_mul: float = 1.0,\n        eta_min: float = 0.0,\n        eta_mul: float = 1.0,\n        last_epoch: int = -1,\n    ) -> None:\n        assert t_initial > 0\n        assert eta_min >= 0\n        if t_initial == 1 and t_mul == 1 and eta_mul == 1:\n            logger.warning(\n                ""Cosine annealing scheduler will have no effect on the learning ""\n                ""rate since t_initial = t_mul = eta_mul = 1.""\n            )\n        self.t_initial = t_initial\n        self.t_mul = t_mul\n        self.eta_min = eta_min\n        self.eta_mul = eta_mul\n        self._last_restart: int = 0\n        self._cycle_counter: int = 0\n        self._cycle_len: int = t_initial\n        self._n_restarts: int = 0\n        super().__init__(optimizer, last_epoch)\n\n    @overrides\n    def get_values(self):\n        """"""Get updated learning rate.""""""\n        if self.last_epoch == -1:\n            return self.base_values\n\n        step = self.last_epoch + 1\n        self._cycle_counter = step - self._last_restart\n\n        if self._cycle_counter % self._cycle_len == 0:\n            self._n_restarts += 1\n            self._cycle_counter = 0\n            self._last_restart = step\n\n        base_lrs = [lr * self.eta_mul ** self._n_restarts for lr in self.base_values]\n        self._cycle_len = int(self.t_initial * self.t_mul ** self._n_restarts)\n\n        lrs = [\n            self.eta_min\n            + ((lr - self.eta_min) / 2)\n            * (np.cos(np.pi * (self._cycle_counter % self._cycle_len) / self._cycle_len) + 1)\n            for lr in base_lrs\n        ]\n\n        return lrs\n'"
allennlp/training/learning_rate_schedulers/learning_rate_scheduler.py,6,"b'from typing import Any, Dict, List, Union\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.registrable import Registrable\nfrom allennlp.training.scheduler import Scheduler\nfrom allennlp.training.optimizers import Optimizer\n\n\nclass LearningRateScheduler(Scheduler, Registrable):\n    def __init__(self, optimizer: torch.optim.Optimizer, last_epoch: int = -1) -> None:\n        super().__init__(optimizer, ""lr"", last_epoch)\n\n    @overrides\n    def get_values(self):\n        raise NotImplementedError\n\n\nclass _PyTorchLearningRateSchedulerWrapper(LearningRateScheduler):\n    def __init__(self, lr_scheduler: torch.optim.lr_scheduler._LRScheduler) -> None:\n        self.lr_scheduler = lr_scheduler\n\n    def get_values(self):\n        return self.lr_scheduler.get_last_lr()\n\n    @overrides\n    def step(self, metric: float = None) -> None:\n        self.lr_scheduler.step()\n\n    @overrides\n    def state_dict(self) -> Dict[str, Any]:\n        return self.lr_scheduler.state_dict()\n\n    @overrides\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        self.lr_scheduler.load_state_dict(state_dict)\n\n\nclass _PyTorchLearningRateSchedulerWithMetricsWrapper(_PyTorchLearningRateSchedulerWrapper):\n    @overrides\n    def step(self, metric: float = None) -> None:\n        if metric is None:\n            raise ConfigurationError(\n                ""This learning rate scheduler requires ""\n                ""a validation metric to compute the schedule and therefore ""\n                ""must be used with a validation dataset.""\n            )\n        self.lr_scheduler.step(metric)\n\n\n@LearningRateScheduler.register(""step"")\nclass StepLearningRateScheduler(_PyTorchLearningRateSchedulerWrapper):\n    """"""\n    Registered as a `LearningRateScheduler` with name ""step"".  The ""optimizer"" argument does not get\n    an entry in a configuration file for the object.\n    """"""\n\n    def __init__(\n        self, optimizer: Optimizer, step_size: int, gamma: float = 0.1, last_epoch: int = -1\n    ) -> None:\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(\n            optimizer=optimizer, step_size=step_size, gamma=gamma, last_epoch=last_epoch\n        )\n        super().__init__(lr_scheduler)\n\n\n@LearningRateScheduler.register(""multi_step"")\nclass MultiStepLearningRateScheduler(_PyTorchLearningRateSchedulerWrapper):\n    """"""\n    Registered as a `LearningRateScheduler` with name ""multi_step"".  The ""optimizer"" argument does\n    not get an entry in a configuration file for the object.\n    """"""\n\n    def __init__(\n        self, optimizer: Optimizer, milestones: List[int], gamma: float = 0.1, last_epoch: int = -1\n    ) -> None:\n        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n            optimizer=optimizer, milestones=milestones, gamma=gamma, last_epoch=last_epoch\n        )\n        super().__init__(lr_scheduler)\n\n\n@LearningRateScheduler.register(""exponential"")\nclass ExponentialLearningRateScheduler(_PyTorchLearningRateSchedulerWrapper):\n    """"""\n    Registered as a `LearningRateScheduler` with name ""exponential"".  The ""optimizer"" argument does\n    not get an entry in a configuration file for the object.\n    """"""\n\n    def __init__(self, optimizer: Optimizer, gamma: float = 0.1, last_epoch: int = -1) -> None:\n        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n            optimizer=optimizer, gamma=gamma, last_epoch=last_epoch\n        )\n        super().__init__(lr_scheduler)\n\n\n@LearningRateScheduler.register(""reduce_on_plateau"")\nclass ReduceOnPlateauLearningRateScheduler(_PyTorchLearningRateSchedulerWithMetricsWrapper):\n    """"""\n    Registered as a `LearningRateScheduler` with name ""reduce_on_plateau"".  The ""optimizer"" argument\n    does not get an entry in a configuration file for the object.\n    """"""\n\n    def __init__(\n        self,\n        optimizer: Optimizer,\n        mode: str = ""min"",\n        factor: float = 0.1,\n        patience: int = 10,\n        verbose: bool = False,\n        threshold_mode: str = ""rel"",\n        threshold: float = 1e-4,\n        cooldown: int = 0,\n        min_lr: Union[float, List[float]] = 0,\n        eps: float = 1e-8,\n    ) -> None:\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer=optimizer,\n            mode=mode,\n            factor=factor,\n            patience=patience,\n            verbose=verbose,\n            threshold_mode=threshold_mode,\n            threshold=threshold,\n            cooldown=cooldown,\n            min_lr=min_lr,\n            eps=eps,\n        )\n        super().__init__(lr_scheduler)\n'"
allennlp/training/learning_rate_schedulers/noam.py,2,"b'from overrides import overrides\nimport torch\n\nfrom allennlp.training.learning_rate_schedulers.learning_rate_scheduler import LearningRateScheduler\n\n\n@LearningRateScheduler.register(""noam"")\nclass NoamLR(LearningRateScheduler):\n    """"""\n    Implements the Noam Learning rate schedule. This corresponds to increasing the learning rate\n    linearly for the first `warmup_steps` training steps, and decreasing it thereafter proportionally\n    to the inverse square root of the step number, scaled by the inverse square root of the\n    dimensionality of the model. Time will tell if this is just madness or it\'s actually important.\n\n    Registered as a `LearningRateScheduler` with name ""noam"".\n\n    # Parameters\n\n    optimizer : `torch.optim.Optimizer`\n        This argument does not get an entry in a configuration file for the object.\n    model_size : `int`, required.\n        The hidden size parameter which dominates the number of parameters in your model.\n    warmup_steps : `int`, required.\n        The number of steps to linearly increase the learning rate.\n    factor : `float`, optional (default = `1.0`).\n        The overall scale factor for the learning rate decay.\n    """"""\n\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        model_size: int,\n        warmup_steps: int,\n        factor: float = 1.0,\n        last_epoch: int = -1,\n    ) -> None:\n        self.warmup_steps = warmup_steps\n        self.factor = factor\n        self.model_size = model_size\n        super().__init__(optimizer, last_epoch=last_epoch)\n\n    @overrides\n    def step(self, metric: float = None) -> None:\n        pass\n\n    def step_batch(self, batch_num_total: int = None) -> None:\n        if batch_num_total is None:\n            self.last_epoch += 1  # type: ignore\n        else:\n            self.last_epoch = batch_num_total\n        for param_group, learning_rate in zip(self.optimizer.param_groups, self.get_values()):\n            param_group[""lr""] = learning_rate\n\n    def get_values(self):\n        step = max(self.last_epoch, 1)\n        scale = self.factor * (\n            self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup_steps ** (-1.5))\n        )\n\n        return [scale for _ in range(len(self.base_values))]\n'"
allennlp/training/learning_rate_schedulers/polynomial_decay.py,1,"b'from overrides import overrides\nimport torch\n\nfrom allennlp.training.learning_rate_schedulers.learning_rate_scheduler import LearningRateScheduler\n\n\n@LearningRateScheduler.register(""polynomial_decay"")\nclass PolynomialDecay(LearningRateScheduler):\n    """"""\n    Implements polynomial decay Learning rate scheduling. The learning rate is first\n    linearly increased for the first `warmup_steps` training steps. Then it is decayed for\n    `total_steps` - `warmup_steps` from the initial learning rate to `end_learning_rate` using a polynomial\n    of degree `power`.\n\n    Formally,\n\n    `lr` = (`initial_lr` - `end_learning_rate`) *\n           ((`total_steps` - `steps`)/(`total_steps` - `warmup_steps`)) ** `power`\n\n    # Parameters\n\n    total_steps: `int`, required\n        The total number of steps to adjust the learning rate for.\n    warmup_steps : `int`, required\n        The number of steps to linearly increase the learning rate.\n    power : `float`, optional (default = `1.0`)\n        The power of the polynomial used for decaying.\n    end_learning_rate : `float`, optional (default = `0.0`)\n        Final learning rate to decay towards.\n    """"""\n\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        total_steps,\n        power=1.0,\n        warmup_steps=0,\n        end_learning_rate=0.0,\n        last_epoch: int = -1,\n    ):\n        super().__init__(optimizer, last_epoch)\n\n        self.power = power\n        self.warmup_steps = warmup_steps\n        self.total_steps = total_steps\n        self.end_learning_rate = end_learning_rate\n\n        self.steps = 0\n\n        self.step_batch(0)\n\n    @overrides\n    def get_values(self):\n        if self.warmup_steps > 0 and self.steps < self.warmup_steps:\n            f = self.steps / self.warmup_steps\n            return [f * lr for lr in self.base_values]\n\n        if self.steps >= self.total_steps:\n            return [self.end_learning_rate for _ in self.base_values]\n\n        current_decay_steps = self.total_steps - self.steps\n        total_decay_steps = self.total_steps - self.warmup_steps\n        f = (current_decay_steps / total_decay_steps) ** self.power\n        return [\n            f * (lr - self.end_learning_rate) + self.end_learning_rate for lr in self.base_values\n        ]\n\n    @overrides\n    def step(self, metric: float = None) -> None:\n        pass\n\n    @overrides\n    def step_batch(self, batch_num_total: int = None) -> None:\n        if batch_num_total is None:\n            self.steps += 1\n        else:\n            self.steps = batch_num_total\n\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_values()):\n            param_group[self.param_group_field] = lr\n'"
allennlp/training/learning_rate_schedulers/slanted_triangular.py,2,"b'import logging\nfrom typing import List, Optional\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.training.learning_rate_schedulers.learning_rate_scheduler import LearningRateScheduler\n\n\nlogger = logging.getLogger(__name__)\n\n\n@LearningRateScheduler.register(""slanted_triangular"")\nclass SlantedTriangular(LearningRateScheduler):\n    """"""\n    Implements the Slanted Triangular Learning Rate schedule with optional gradual\n    unfreezing. The schedule corresponds to first linearly increasing the learning\n    rate and annealing the learning based on a fixed ratio.\n\n    If we gradually unfreeze, then in the first epoch of training, only the top\n    layer is trained; in the second epoch, the top two layers are trained, etc.\n    During freezing, the learning rate is increased and annealed over one epoch.\n    After freezing finished, the learning rate is increased and annealed over\n    the remaining training iterations.\n\n    Note that with this schedule, early stopping should typically be avoided.\n\n    Registered as a `LearningRateScheduler` with name ""slanted_triangular"".\n\n    # Parameters\n\n    optimizer : `torch.optim.Optimizer`\n        This argument does not get an entry in a configuration file for the object.\n    num_epochs : `int`, required.\n        The total number of epochs for which the model should be trained.\n    num_steps_per_epoch : `Optional[int]`, optional (default = `None`)\n        The number of steps (updates, batches) per training epoch.\n    cut_frac : `float`, optional (default = `0.1`).\n        The fraction of the steps to increase the learning rate.\n    ratio : `float`, optional (default = `32`).\n        The ratio of the smallest to the (largest) base learning rate.\n    gradual_unfreezing : `bool`, optional (default = `False`).\n        Whether gradual unfreezing should be used.\n    discriminative_fine_tuning : `bool`, optional (default = `False`).\n        Whether discriminative fine-tuning (different learning rates per layer)\n        are used.\n    decay_factor : `float`, optional (default = `0.38`).\n        The decay factor by which the learning rate is reduced with\n        discriminative fine-tuning when going a layer deeper.\n    """"""\n\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        num_epochs: int,\n        num_steps_per_epoch: Optional[int] = None,\n        cut_frac: float = 0.1,\n        ratio: int = 32,\n        last_epoch: int = -1,\n        gradual_unfreezing: bool = False,\n        discriminative_fine_tuning: bool = False,\n        decay_factor: float = 0.38,\n    ) -> None:\n        self.num_epochs = num_epochs\n        self.num_steps_per_epoch = num_steps_per_epoch\n        self.cut_frac = cut_frac\n        self.ratio = ratio\n        self.gradual_unfreezing = gradual_unfreezing\n        self.freezing_current = self.gradual_unfreezing\n        self.is_first_epoch = True\n        # track the actual number of steps for each epoch\n        self.batch_num_total_epoch_end: List[int] = []\n        if self.gradual_unfreezing:\n            assert not optimizer.param_groups[-1][""params""], ""The default group should be empty.""\n        if self.gradual_unfreezing or discriminative_fine_tuning:\n            assert len(optimizer.param_groups) > 2, (\n                ""There should be at least 3 param_groups (2 + empty default group)""\n                "" for gradual unfreezing / discriminative fine-tuning to make sense.""\n            )\n        super().__init__(optimizer, last_epoch)\n        self.step()\n        if discriminative_fine_tuning:\n            # skip the last param_group if it is has no parameters\n            exponent = 0\n            for i in range(len(self.base_values) - 1, -1, -1):\n                param_group = optimizer.param_groups[i]\n                if param_group[""params""]:\n                    param_group[""lr""] = self.base_values[i] * decay_factor ** exponent\n                    self.base_values[i] = param_group[""lr""]\n                    exponent += 1\n        # set up for the first batch\n        self.last_batch_num_total = -1\n        self.step_batch(0)\n\n    @overrides\n    def step(self, metric: float = None) -> None:\n        self.last_epoch += 1\n        if len(self.batch_num_total_epoch_end) == 0:\n            self.batch_num_total_epoch_end.append(0)\n        else:\n            self.batch_num_total_epoch_end.append(self.last_batch_num_total)\n\n        if self.gradual_unfreezing:\n            # the method is called once when initialising before the\n            # first epoch (epoch -1) and then always at the end of each\n            # epoch; so the first time, with epoch id -1, we want to set\n            # up for epoch #1; the second time, with epoch id 0,\n            # we want to set up for epoch #2, etc.\n            if self.is_first_epoch:\n                num_layers_to_unfreeze = 1\n                self.is_first_epoch = False\n            else:\n                num_layers_to_unfreeze = self.last_epoch + 2\n            if num_layers_to_unfreeze >= len(self.optimizer.param_groups) - 1:\n                logger.info(""Gradual unfreezing finished. Training all layers."")\n                self.freezing_current = False\n            else:\n                logger.info(\n                    f""Gradual unfreezing. Training only the top {num_layers_to_unfreeze} layers.""\n                )\n            for i, param_group in enumerate(reversed(self.optimizer.param_groups)):\n                for param in param_group[""params""]:\n                    # i = 0 is the default group; we care about i > 0\n                    param.requires_grad = bool(i <= num_layers_to_unfreeze)\n\n    def step_batch(self, batch_num_total: int = None):\n        if batch_num_total is None:\n            batch_num_total = self.last_batch_num_total + 1\n        self.last_batch_num_total = batch_num_total\n        for param_group, learning_rate in zip(self.optimizer.param_groups, self.get_values()):\n            param_group[""lr""] = learning_rate\n\n    def get_values(self):\n        # get the actual number of batches per epoch seen in training\n        if len(self.batch_num_total_epoch_end) > 1:\n            # have finished an epoch\n            actual_num_steps_per_epoch = int(\n                self.batch_num_total_epoch_end[-1] / (len(self.batch_num_total_epoch_end) - 1)\n            )\n        else:\n            actual_num_steps_per_epoch = max(\n                self.num_steps_per_epoch or 1, self.last_batch_num_total\n            )\n\n        if self.freezing_current:\n            # if we still freeze, we restrict the schedule to the current epoch\n            num_steps = actual_num_steps_per_epoch\n            step = min(self.last_batch_num_total - self.batch_num_total_epoch_end[-1], num_steps)\n        else:\n            # otherwise we use the schedule for the rest of training\n            if not self.gradual_unfreezing:\n                frozen_steps = 0\n            else:\n                num_frozen_epochs = len(self.optimizer.param_groups) - 2\n                frozen_steps = self.batch_num_total_epoch_end[num_frozen_epochs]\n            num_steps = self.num_epochs * actual_num_steps_per_epoch - frozen_steps\n            step = min(self.last_batch_num_total - frozen_steps, num_steps)\n        cut = int(num_steps * self.cut_frac)\n        prop = step / cut if step < cut else 1 - (step - cut) / (num_steps - cut)\n        return [lr * (1 + prop * (self.ratio - 1)) / self.ratio for lr in self.base_values]\n'"
allennlp/training/metrics/__init__.py,0,"b'""""""\nA `~allennlp.training.metrics.metric.Metric` is some quantity or quantities\nthat can be accumulated during training or evaluation; for example,\naccuracy or F1 score.\n""""""\n\nfrom allennlp.training.metrics.attachment_scores import AttachmentScores\nfrom allennlp.training.metrics.average import Average\nfrom allennlp.training.metrics.boolean_accuracy import BooleanAccuracy\nfrom allennlp.training.metrics.bleu import BLEU\nfrom allennlp.training.metrics.rouge import ROUGE\nfrom allennlp.training.metrics.categorical_accuracy import CategoricalAccuracy\nfrom allennlp.training.metrics.covariance import Covariance\nfrom allennlp.training.metrics.entropy import Entropy\nfrom allennlp.training.metrics.evalb_bracketing_scorer import (\n    EvalbBracketingScorer,\n    DEFAULT_EVALB_DIR,\n)\nfrom allennlp.training.metrics.fbeta_measure import FBetaMeasure\nfrom allennlp.training.metrics.f1_measure import F1Measure\nfrom allennlp.training.metrics.mean_absolute_error import MeanAbsoluteError\nfrom allennlp.training.metrics.metric import Metric\nfrom allennlp.training.metrics.pearson_correlation import PearsonCorrelation\nfrom allennlp.training.metrics.spearman_correlation import SpearmanCorrelation\nfrom allennlp.training.metrics.perplexity import Perplexity\nfrom allennlp.training.metrics.sequence_accuracy import SequenceAccuracy\nfrom allennlp.training.metrics.span_based_f1_measure import SpanBasedF1Measure\nfrom allennlp.training.metrics.unigram_recall import UnigramRecall\nfrom allennlp.training.metrics.auc import Auc\n'"
allennlp/training/metrics/attachment_scores.py,11,"b'from typing import Optional, List\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""attachment_scores"")\nclass AttachmentScores(Metric):\n    """"""\n    Computes labeled and unlabeled attachment scores for a\n    dependency parse, as well as sentence level exact match\n    for both labeled and unlabeled trees. Note that the input\n    to this metric is the sampled predictions, not the distribution\n    itself.\n\n    # Parameters\n\n    ignore_classes : `List[int]`, optional (default = `None`)\n        A list of label ids to ignore when computing metrics.\n    """"""\n\n    def __init__(self, ignore_classes: List[int] = None) -> None:\n        self._labeled_correct = 0.0\n        self._unlabeled_correct = 0.0\n        self._exact_labeled_correct = 0.0\n        self._exact_unlabeled_correct = 0.0\n        self._total_words = 0.0\n        self._total_sentences = 0.0\n\n        self._ignore_classes: List[int] = ignore_classes or []\n\n    def __call__(  # type: ignore\n        self,\n        predicted_indices: torch.Tensor,\n        predicted_labels: torch.Tensor,\n        gold_indices: torch.Tensor,\n        gold_labels: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n    ):\n        """"""\n        # Parameters\n\n        predicted_indices : `torch.Tensor`, required.\n            A tensor of head index predictions of shape (batch_size, timesteps).\n        predicted_labels : `torch.Tensor`, required.\n            A tensor of arc label predictions of shape (batch_size, timesteps).\n        gold_indices : `torch.Tensor`, required.\n            A tensor of the same shape as `predicted_indices`.\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predicted_labels`.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A tensor of the same shape as `predicted_indices`.\n        """"""\n        detached = self.detach_tensors(\n            predicted_indices, predicted_labels, gold_indices, gold_labels, mask\n        )\n        predicted_indices, predicted_labels, gold_indices, gold_labels, mask = detached\n\n        if mask is None:\n            mask = torch.ones_like(predicted_indices).bool()\n\n        predicted_indices = predicted_indices.long()\n        predicted_labels = predicted_labels.long()\n        gold_indices = gold_indices.long()\n        gold_labels = gold_labels.long()\n\n        # Multiply by a mask denoting locations of\n        # gold labels which we should ignore.\n        for label in self._ignore_classes:\n            label_mask = gold_labels.eq(label)\n            mask = mask & ~label_mask\n\n        correct_indices = predicted_indices.eq(gold_indices).long() * mask\n        unlabeled_exact_match = (correct_indices + ~mask).prod(dim=-1)\n        correct_labels = predicted_labels.eq(gold_labels).long() * mask\n        correct_labels_and_indices = correct_indices * correct_labels\n        labeled_exact_match = (correct_labels_and_indices + ~mask).prod(dim=-1)\n\n        self._unlabeled_correct += correct_indices.sum()\n        self._exact_unlabeled_correct += unlabeled_exact_match.sum()\n        self._labeled_correct += correct_labels_and_indices.sum()\n        self._exact_labeled_correct += labeled_exact_match.sum()\n        self._total_sentences += correct_indices.size(0)\n        self._total_words += correct_indices.numel() - (~mask).sum()\n\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        The accumulated metrics as a dictionary.\n        """"""\n        unlabeled_attachment_score = 0.0\n        labeled_attachment_score = 0.0\n        unlabeled_exact_match = 0.0\n        labeled_exact_match = 0.0\n        if self._total_words > 0.0:\n            unlabeled_attachment_score = float(self._unlabeled_correct) / float(self._total_words)\n            labeled_attachment_score = float(self._labeled_correct) / float(self._total_words)\n        if self._total_sentences > 0:\n            unlabeled_exact_match = float(self._exact_unlabeled_correct) / float(\n                self._total_sentences\n            )\n            labeled_exact_match = float(self._exact_labeled_correct) / float(self._total_sentences)\n        if reset:\n            self.reset()\n        return {\n            ""UAS"": unlabeled_attachment_score,\n            ""LAS"": labeled_attachment_score,\n            ""UEM"": unlabeled_exact_match,\n            ""LEM"": labeled_exact_match,\n        }\n\n    @overrides\n    def reset(self):\n        self._labeled_correct = 0.0\n        self._unlabeled_correct = 0.0\n        self._exact_labeled_correct = 0.0\n        self._exact_unlabeled_correct = 0.0\n        self._total_words = 0.0\n        self._total_sentences = 0.0\n'"
allennlp/training/metrics/auc.py,16,"b'from typing import Optional\n\nfrom overrides import overrides\nimport torch\nfrom sklearn import metrics\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""auc"")\nclass Auc(Metric):\n    """"""\n    The AUC Metric measures the area under the receiver-operating characteristic\n    (ROC) curve for binary classification problems.\n    """"""\n\n    def __init__(self, positive_label=1):\n        super().__init__()\n        self._positive_label = positive_label\n        self._all_predictions = torch.FloatTensor()\n        self._all_gold_labels = torch.LongTensor()\n\n    def __call__(\n        self,\n        predictions: torch.Tensor,\n        gold_labels: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n    ):\n        """"""\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A one-dimensional tensor of prediction scores of shape (batch_size).\n        gold_labels : `torch.Tensor`, required.\n            A one-dimensional label tensor of shape (batch_size), with {1, 0}\n            entries for positive and negative class. If it\'s not binary,\n            `positive_label` should be passed in the initialization.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A one-dimensional label tensor of shape (batch_size).\n        """"""\n\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n\n        # Sanity checks.\n        if gold_labels.dim() != 1:\n            raise ConfigurationError(\n                ""gold_labels must be one-dimensional, ""\n                ""but found tensor of shape: {}"".format(gold_labels.size())\n            )\n        if predictions.dim() != 1:\n            raise ConfigurationError(\n                ""predictions must be one-dimensional, ""\n                ""but found tensor of shape: {}"".format(predictions.size())\n            )\n\n        unique_gold_labels = torch.unique(gold_labels)\n        if unique_gold_labels.numel() > 2:\n            raise ConfigurationError(\n                ""AUC can be used for binary tasks only. gold_labels has {} unique labels, ""\n                ""expected at maximum 2."".format(unique_gold_labels.numel())\n            )\n\n        gold_labels_is_binary = set(unique_gold_labels.tolist()) <= {0, 1}\n        if not gold_labels_is_binary and self._positive_label not in unique_gold_labels:\n            raise ConfigurationError(\n                ""gold_labels should be binary with 0 and 1 or initialized positive_label ""\n                ""{} should be present in gold_labels"".format(self._positive_label)\n            )\n\n        if mask is None:\n            batch_size = gold_labels.shape[0]\n            mask = torch.ones(batch_size, device=gold_labels.device).bool()\n\n        self._all_predictions = self._all_predictions.to(predictions.device)\n        self._all_gold_labels = self._all_gold_labels.to(gold_labels.device)\n\n        self._all_predictions = torch.cat(\n            [self._all_predictions, torch.masked_select(predictions, mask).float()], dim=0\n        )\n        self._all_gold_labels = torch.cat(\n            [self._all_gold_labels, torch.masked_select(gold_labels, mask).long()], dim=0\n        )\n\n    def get_metric(self, reset: bool = False):\n        if self._all_gold_labels.shape[0] == 0:\n            return 0.5\n        false_positive_rates, true_positive_rates, _ = metrics.roc_curve(\n            self._all_gold_labels.cpu().numpy(),\n            self._all_predictions.cpu().numpy(),\n            pos_label=self._positive_label,\n        )\n        auc = metrics.auc(false_positive_rates, true_positive_rates)\n        if reset:\n            self.reset()\n        return auc\n\n    @overrides\n    def reset(self):\n        self._all_predictions = torch.FloatTensor()\n        self._all_gold_labels = torch.LongTensor()\n'"
allennlp/training/metrics/average.py,0,"b'from overrides import overrides\n\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""average"")\nclass Average(Metric):\n    """"""\n    This [`Metric`](./metric.md) breaks with the typical `Metric` API and just stores values that were\n    computed in some fashion outside of a `Metric`.  If you have some external code that computes\n    the metric for you, for instance, you can use this to report the average result using our\n    `Metric` API.\n    """"""\n\n    def __init__(self) -> None:\n        self._total_value = 0.0\n        self._count = 0\n\n    @overrides\n    def __call__(self, value):\n        """"""\n        # Parameters\n\n        value : `float`\n            The value to average.\n        """"""\n        self._total_value += list(self.detach_tensors(value))[0]\n        self._count += 1\n\n    @overrides\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        The average of all values that were passed to `__call__`.\n        """"""\n        average_value = self._total_value / self._count if self._count > 0 else 0\n        if reset:\n            self.reset()\n        return average_value\n\n    @overrides\n    def reset(self):\n        self._total_value = 0.0\n        self._count = 0\n'"
allennlp/training/metrics/bleu.py,6,"b'from collections import Counter\nimport math\nfrom typing import Iterable, Tuple, Dict, Set\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""bleu"")\nclass BLEU(Metric):\n    """"""\n    Bilingual Evaluation Understudy (BLEU).\n\n    BLEU is a common metric used for evaluating the quality of machine translations\n    against a set of reference translations. See\n    [Papineni et. al., ""BLEU: a method for automatic evaluation of machine translation"", 2002][1].\n\n    # Parameters\n\n    ngram_weights : `Iterable[float]`, optional (default = `(0.25, 0.25, 0.25, 0.25)`)\n        Weights to assign to scores for each ngram size.\n    exclude_indices : `Set[int]`, optional (default = `None`)\n        Indices to exclude when calculating ngrams. This should usually include\n        the indices of the start, end, and pad tokens.\n\n    # Notes\n\n    We chose to implement this from scratch instead of wrapping an existing implementation\n    (such as `nltk.translate.bleu_score`) for a two reasons. First, so that we could\n    pass tensors directly to this metric instead of first converting the tensors to lists of strings.\n    And second, because functions like `nltk.translate.bleu_score.corpus_bleu()` are\n    meant to be called once over the entire corpus, whereas it is more efficient\n    in our use case to update the running precision counts every batch.\n\n    This implementation only considers a reference set of size 1, i.e. a single\n    gold target sequence for each predicted sequence.\n\n\n    [1]: https://www.semanticscholar.org/paper/8ff93cfd37dced279134c9d642337a2085b31f59/\n    """"""\n\n    def __init__(\n        self,\n        ngram_weights: Iterable[float] = (0.25, 0.25, 0.25, 0.25),\n        exclude_indices: Set[int] = None,\n    ) -> None:\n        self._ngram_weights = ngram_weights\n        self._exclude_indices = exclude_indices or set()\n        self._precision_matches: Dict[int, int] = Counter()\n        self._precision_totals: Dict[int, int] = Counter()\n        self._prediction_lengths = 0\n        self._reference_lengths = 0\n\n    @overrides\n    def reset(self) -> None:\n        self._precision_matches = Counter()\n        self._precision_totals = Counter()\n        self._prediction_lengths = 0\n        self._reference_lengths = 0\n\n    def _get_modified_precision_counts(\n        self,\n        predicted_tokens: torch.LongTensor,\n        reference_tokens: torch.LongTensor,\n        ngram_size: int,\n    ) -> Tuple[int, int]:\n        """"""\n        Compare the predicted tokens to the reference (gold) tokens at the desired\n        ngram size and calculate the numerator and denominator for a modified\n        form of precision.\n\n        The numerator is the number of ngrams in the predicted sentences that match\n        with an ngram in the corresponding reference sentence, clipped by the total\n        count of that ngram in the reference sentence. The denominator is just\n        the total count of predicted ngrams.\n        """"""\n        clipped_matches = 0\n        total_predicted = 0\n        from allennlp.training.util import ngrams\n\n        for predicted_row, reference_row in zip(predicted_tokens, reference_tokens):\n            predicted_ngram_counts = ngrams(predicted_row, ngram_size, self._exclude_indices)\n            reference_ngram_counts = ngrams(reference_row, ngram_size, self._exclude_indices)\n            for ngram, count in predicted_ngram_counts.items():\n                clipped_matches += min(count, reference_ngram_counts[ngram])\n                total_predicted += count\n        return clipped_matches, total_predicted\n\n    def _get_brevity_penalty(self) -> float:\n        if self._prediction_lengths > self._reference_lengths:\n            return 1.0\n        if self._reference_lengths == 0 or self._prediction_lengths == 0:\n            return 0.0\n        return math.exp(1.0 - self._reference_lengths / self._prediction_lengths)\n\n    @overrides\n    def __call__(\n        self,  # type: ignore\n        predictions: torch.LongTensor,\n        gold_targets: torch.LongTensor,\n    ) -> None:\n        """"""\n        Update precision counts.\n\n        # Parameters\n\n        predictions : `torch.LongTensor`, required\n            Batched predicted tokens of shape `(batch_size, max_sequence_length)`.\n        references : `torch.LongTensor`, required\n            Batched reference (gold) translations with shape `(batch_size, max_gold_sequence_length)`.\n\n        # Returns\n\n        None\n        """"""\n        predictions, gold_targets = self.detach_tensors(predictions, gold_targets)\n        for ngram_size, _ in enumerate(self._ngram_weights, start=1):\n            precision_matches, precision_totals = self._get_modified_precision_counts(\n                predictions, gold_targets, ngram_size\n            )\n            self._precision_matches[ngram_size] += precision_matches\n            self._precision_totals[ngram_size] += precision_totals\n        if not self._exclude_indices:\n            self._prediction_lengths += predictions.size(0) * predictions.size(1)\n            self._reference_lengths += gold_targets.size(0) * gold_targets.size(1)\n        else:\n            from allennlp.training.util import get_valid_tokens_mask\n\n            valid_predictions_mask = get_valid_tokens_mask(predictions, self._exclude_indices)\n            self._prediction_lengths += valid_predictions_mask.sum().item()\n            valid_gold_targets_mask = get_valid_tokens_mask(gold_targets, self._exclude_indices)\n            self._reference_lengths += valid_gold_targets_mask.sum().item()\n\n    @overrides\n    def get_metric(self, reset: bool = False) -> Dict[str, float]:\n        brevity_penalty = self._get_brevity_penalty()\n        ngram_scores = (\n            weight\n            * (\n                math.log(self._precision_matches[n] + 1e-13)\n                - math.log(self._precision_totals[n] + 1e-13)\n            )\n            for n, weight in enumerate(self._ngram_weights, start=1)\n        )\n        bleu = brevity_penalty * math.exp(sum(ngram_scores))\n        if reset:\n            self.reset()\n        return {""BLEU"": bleu}\n'"
allennlp/training/metrics/boolean_accuracy.py,7,"b'from typing import Optional\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""boolean_accuracy"")\nclass BooleanAccuracy(Metric):\n    """"""\n    Just checks batch-equality of two tensors and computes an accuracy metric based on that.\n    That is, if your prediction has shape (batch_size, dim_1, ..., dim_n), this metric considers that\n    as a set of `batch_size` predictions and checks that each is *entirely* correct across the remaining dims.\n    This means the denominator in the accuracy computation is `batch_size`, with the caveat that predictions\n    that are totally masked are ignored (in which case the denominator is the number of predictions that have\n    at least one unmasked element).\n\n    This is similar to [`CategoricalAccuracy`](./categorical_accuracy.md), if you\'ve already done a `.max()`\n    on your predictions.  If you have categorical output, though, you should typically just use\n    `CategoricalAccuracy`.  The reason you might want to use this instead is if you\'ve done\n    some kind of constrained inference and don\'t have a prediction tensor that matches the API of\n    `CategoricalAccuracy`, which assumes a final dimension of size `num_classes`.\n    """"""\n\n    def __init__(self) -> None:\n        self._correct_count = 0.0\n        self._total_count = 0.0\n\n    def __call__(\n        self,\n        predictions: torch.Tensor,\n        gold_labels: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n    ):\n        """"""\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ...).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predictions`.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A tensor of the same shape as `predictions`.\n        """"""\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n\n        # Some sanity checks.\n        if gold_labels.size() != predictions.size():\n            raise ValueError(\n                f""gold_labels must have shape == predictions.size() but ""\n                f""found tensor of shape: {gold_labels.size()}""\n            )\n        if mask is not None and mask.size() != predictions.size():\n            raise ValueError(\n                f""mask must have shape == predictions.size() but ""\n                f""found tensor of shape: {mask.size()}""\n            )\n\n        batch_size = predictions.size(0)\n\n        if mask is not None:\n            # We can multiply by the mask up front, because we\'re just checking equality below, and\n            # this way everything that\'s masked will be equal.\n            predictions = predictions * mask\n            gold_labels = gold_labels * mask\n\n            # We want to skip predictions that are completely masked;\n            # so we\'ll keep predictions that aren\'t.\n            keep = mask.view(batch_size, -1).max(dim=1)[0]\n        else:\n            keep = torch.ones(batch_size, device=predictions.device).bool()\n\n        predictions = predictions.view(batch_size, -1)\n        gold_labels = gold_labels.view(batch_size, -1)\n\n        # At this point, predictions is (batch_size, rest_of_dims_combined),\n        # so .eq -> .prod will be 1 if every element of the instance prediction is correct\n        # and 0 if at least one element of the instance prediction is wrong.\n        # Because of how we\'re handling masking, masked positions are automatically ""correct"".\n        correct = predictions.eq(gold_labels).prod(dim=1).float()\n\n        # Since masked positions are correct, we need to explicitly exclude instance predictions\n        # where the entire prediction is masked (because they look ""correct"").\n        self._correct_count += (correct * keep).sum()\n        self._total_count += keep.sum()\n\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        The accumulated accuracy.\n        """"""\n        if self._total_count > 0:\n            accuracy = float(self._correct_count) / float(self._total_count)\n        else:\n            accuracy = 0.0\n        if reset:\n            self.reset()\n        return accuracy\n\n    @overrides\n    def reset(self):\n        self._correct_count = 0.0\n        self._total_count = 0.0\n'"
allennlp/training/metrics/categorical_accuracy.py,7,"b'from typing import Optional\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""categorical_accuracy"")\nclass CategoricalAccuracy(Metric):\n    """"""\n    Categorical Top-K accuracy. Assumes integer labels, with\n    each item to be classified having a single correct class.\n    Tie break enables equal distribution of scores among the\n    classes with same maximum predicted scores.\n    """"""\n\n    def __init__(self, top_k: int = 1, tie_break: bool = False) -> None:\n        if top_k > 1 and tie_break:\n            raise ConfigurationError(\n                ""Tie break in Categorical Accuracy can be done only for maximum (top_k = 1)""\n            )\n        if top_k <= 0:\n            raise ConfigurationError(""top_k passed to Categorical Accuracy must be > 0"")\n        self._top_k = top_k\n        self._tie_break = tie_break\n        self.correct_count = 0.0\n        self.total_count = 0.0\n\n    def __call__(\n        self,\n        predictions: torch.Tensor,\n        gold_labels: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n    ):\n        """"""\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ..., num_classes).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of integer class label of shape (batch_size, ...). It must be the same\n            shape as the `predictions` tensor without the `num_classes` dimension.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A masking tensor the same size as `gold_labels`.\n        """"""\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n\n        # Some sanity checks.\n        num_classes = predictions.size(-1)\n        if gold_labels.dim() != predictions.dim() - 1:\n            raise ConfigurationError(\n                ""gold_labels must have dimension == predictions.size() - 1 but ""\n                ""found tensor of shape: {}"".format(predictions.size())\n            )\n        if (gold_labels >= num_classes).any():\n            raise ConfigurationError(\n                ""A gold label passed to Categorical Accuracy contains an id >= {}, ""\n                ""the number of classes."".format(num_classes)\n            )\n\n        predictions = predictions.view((-1, num_classes))\n        gold_labels = gold_labels.view(-1).long()\n        if not self._tie_break:\n            # Top K indexes of the predictions (or fewer, if there aren\'t K of them).\n            # Special case topk == 1, because it\'s common and .max() is much faster than .topk().\n            if self._top_k == 1:\n                top_k = predictions.max(-1)[1].unsqueeze(-1)\n            else:\n                top_k = predictions.topk(min(self._top_k, predictions.shape[-1]), -1)[1]\n\n            # This is of shape (batch_size, ..., top_k).\n            correct = top_k.eq(gold_labels.unsqueeze(-1)).float()\n        else:\n            # prediction is correct if gold label falls on any of the max scores. distribute score by tie_counts\n            max_predictions = predictions.max(-1)[0]\n            max_predictions_mask = predictions.eq(max_predictions.unsqueeze(-1))\n            # max_predictions_mask is (rows X num_classes) and gold_labels is (batch_size)\n            # ith entry in gold_labels points to index (0-num_classes) for ith row in max_predictions\n            # For each row check if index pointed by gold_label is was 1 or not (among max scored classes)\n            correct = max_predictions_mask[\n                torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels\n            ].float()\n            tie_counts = max_predictions_mask.sum(-1)\n            correct /= tie_counts.float()\n            correct.unsqueeze_(-1)\n\n        if mask is not None:\n            correct *= mask.view(-1, 1)\n            self.total_count += mask.sum()\n        else:\n            self.total_count += gold_labels.numel()\n        self.correct_count += correct.sum()\n\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        The accumulated accuracy.\n        """"""\n        if self.total_count > 1e-12:\n            accuracy = float(self.correct_count) / float(self.total_count)\n        else:\n            accuracy = 0.0\n        if reset:\n            self.reset()\n        return accuracy\n\n    @overrides\n    def reset(self):\n        self.correct_count = 0.0\n        self.total_count = 0.0\n'"
allennlp/training/metrics/covariance.py,11,"b'from typing import Optional\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""covariance"")\nclass Covariance(Metric):\n    """"""\n    This `Metric` calculates the unbiased sample covariance between two tensors.\n    Each element in the two tensors is assumed to be a different observation of the\n    variable (i.e., the input tensors are implicitly flattened into vectors and the\n    covariance is calculated between the vectors).\n\n    This implementation is mostly modeled after the streaming_covariance function in Tensorflow. See:\n    <https://github.com/tensorflow/tensorflow/blob/v1.10.1/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3127>\n\n    The following is copied from the Tensorflow documentation:\n\n    The algorithm used for this online computation is described in\n    <https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online>.\n    Specifically, the formula used to combine two sample comoments is\n    `C_AB = C_A + C_B + (E[x_A] - E[x_B]) * (E[y_A] - E[y_B]) * n_A * n_B / n_AB`\n    The comoment for a single batch of data is simply `sum((x - E[x]) * (y - E[y]))`, optionally masked.\n    """"""\n\n    def __init__(self) -> None:\n        self._total_prediction_mean = 0.0\n        self._total_label_mean = 0.0\n        self._total_co_moment = 0.0\n        self._total_count = 0.0\n\n    def __call__(\n        self,\n        predictions: torch.Tensor,\n        gold_labels: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n    ):\n        """"""\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ...).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predictions`.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A tensor of the same shape as `predictions`.\n        """"""\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n        # Flatten predictions, gold_labels, and mask. We calculate the covariance between\n        # the vectors, since each element in the predictions and gold_labels tensor is assumed\n        # to be a separate observation.\n        predictions = predictions.view(-1)\n        gold_labels = gold_labels.view(-1)\n\n        if mask is not None:\n            mask = mask.view(-1)\n            predictions = predictions * mask\n            gold_labels = gold_labels * mask\n            num_batch_items = torch.sum(mask).item()\n        else:\n            num_batch_items = gold_labels.numel()\n\n        # Note that self._total_count must be a float or int at all times\n        # If it is a 1-dimension Tensor, the previous count will equal the updated_count.\n        # The sampe applies for previous_total_prediction_mean and\n        # previous_total_label_mean below -- we handle this in the code by\n        # calling .item() judiciously.\n        previous_count = self._total_count\n        updated_count = self._total_count + num_batch_items\n\n        batch_mean_prediction = torch.sum(predictions) / num_batch_items\n        delta_mean_prediction = (\n            (batch_mean_prediction - self._total_prediction_mean) * num_batch_items\n        ) / updated_count\n        previous_total_prediction_mean = self._total_prediction_mean\n        self._total_prediction_mean += delta_mean_prediction.item()\n\n        batch_mean_label = torch.sum(gold_labels) / num_batch_items\n        delta_mean_label = (\n            (batch_mean_label - self._total_label_mean) * num_batch_items\n        ) / updated_count\n        previous_total_label_mean = self._total_label_mean\n        self._total_label_mean += delta_mean_label.item()\n\n        batch_coresiduals = (predictions - batch_mean_prediction) * (gold_labels - batch_mean_label)\n        if mask is not None:\n            batch_co_moment = torch.sum(batch_coresiduals * mask)\n        else:\n            batch_co_moment = torch.sum(batch_coresiduals)\n        delta_co_moment = batch_co_moment + (\n            previous_total_prediction_mean - batch_mean_prediction\n        ) * (previous_total_label_mean - batch_mean_label) * (\n            previous_count * num_batch_items / updated_count\n        )\n        self._total_co_moment += delta_co_moment.item()\n        self._total_count = updated_count\n\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        The accumulated covariance.\n        """"""\n        covariance = self._total_co_moment / (self._total_count - 1)\n        if reset:\n            self.reset()\n        return covariance\n\n    @overrides\n    def reset(self):\n        self._total_prediction_mean = 0.0\n        self._total_label_mean = 0.0\n        self._total_co_moment = 0.0\n        self._total_count = 0.0\n'"
allennlp/training/metrics/entropy.py,7,"b'from typing import Optional\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""entropy"")\nclass Entropy(Metric):\n    def __init__(self) -> None:\n        self._entropy = 0.0\n        self._count = 0\n\n    @overrides\n    def __call__(\n        self,  # type: ignore\n        logits: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n    ):\n        """"""\n        # Parameters\n\n        logits : `torch.Tensor`, required.\n            A tensor of unnormalized log probabilities of shape (batch_size, ..., num_classes).\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A masking tensor of shape (batch_size, ...).\n        """"""\n        logits, mask = self.detach_tensors(logits, mask)\n\n        if mask is None:\n            mask = torch.ones(logits.size()[:-1], device=logits.device).bool()\n\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        probabilities = torch.exp(log_probs) * mask.unsqueeze(-1)\n        weighted_negative_likelihood = -log_probs * probabilities\n        entropy = weighted_negative_likelihood.sum(-1)\n\n        self._entropy += entropy.sum() / mask.sum()\n        self._count += 1\n\n    @overrides\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        The scalar average entropy.\n        """"""\n        average_value = self._entropy / self._count if self._count > 0 else 0\n        if reset:\n            self.reset()\n        return average_value\n\n    @overrides\n    def reset(self):\n        self._entropy = 0.0\n        self._count = 0\n'"
allennlp/training/metrics/evalb_bracketing_scorer.py,0,"b'from typing import List\nimport logging\nimport os\nimport tempfile\nimport subprocess\nimport shutil\n\nfrom overrides import overrides\nfrom nltk import Tree\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.training.metrics.metric import Metric\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_EVALB_DIR = os.path.abspath(\n    os.path.join(\n        os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir, ""tools"", ""EVALB""\n    )\n)\n\n\n@Metric.register(""evalb"")\nclass EvalbBracketingScorer(Metric):\n    """"""\n    This class uses the external EVALB software for computing a broad range of metrics\n    on parse trees. Here, we use it to compute the Precision, Recall and F1 metrics.\n    You can download the source for EVALB from here: <https://nlp.cs.nyu.edu/evalb/>.\n\n    Note that this software is 20 years old. In order to compile it on modern hardware,\n    you may need to remove an `include <malloc.h>` statement in `evalb.c` before it\n    will compile.\n\n    AllenNLP contains the EVALB software, but you will need to compile it yourself\n    before using it because the binary it generates is system dependent. To build it,\n    run `make` inside the `allennlp/tools/EVALB` directory.\n\n    Note that this metric reads and writes from disk quite a bit. You probably don\'t\n    want to include it in your training loop; instead, you should calculate this on\n    a validation set only.\n\n    # Parameters\n\n    evalb_directory_path : `str`, required.\n        The directory containing the EVALB executable.\n    evalb_param_filename : `str`, optional (default = `""COLLINS.prm""`)\n        The relative name of the EVALB configuration file used when scoring the trees.\n        By default, this uses the COLLINS.prm configuration file which comes with EVALB.\n        This configuration ignores POS tags and some punctuation labels.\n    evalb_num_errors_to_kill : `int`, optional (default = `""10""`)\n        The number of errors to tolerate from EVALB before terminating evaluation.\n    """"""\n\n    def __init__(\n        self,\n        evalb_directory_path: str = DEFAULT_EVALB_DIR,\n        evalb_param_filename: str = ""COLLINS.prm"",\n        evalb_num_errors_to_kill: int = 10,\n    ) -> None:\n        self._evalb_directory_path = evalb_directory_path\n        self._evalb_program_path = os.path.join(evalb_directory_path, ""evalb"")\n        self._evalb_param_path = os.path.join(evalb_directory_path, evalb_param_filename)\n        self._evalb_num_errors_to_kill = evalb_num_errors_to_kill\n\n        self._header_line = [\n            ""ID"",\n            ""Len."",\n            ""Stat."",\n            ""Recal"",\n            ""Prec."",\n            ""Bracket"",\n            ""gold"",\n            ""test"",\n            ""Bracket"",\n            ""Words"",\n            ""Tags"",\n            ""Accracy"",\n        ]\n\n        self._correct_predicted_brackets = 0.0\n        self._gold_brackets = 0.0\n        self._predicted_brackets = 0.0\n\n    @overrides\n    def __call__(self, predicted_trees: List[Tree], gold_trees: List[Tree]) -> None:  # type: ignore\n        """"""\n        # Parameters\n\n        predicted_trees : `List[Tree]`\n            A list of predicted NLTK Trees to compute score for.\n        gold_trees : `List[Tree]`\n            A list of gold NLTK Trees to use as a reference.\n        """"""\n        if not os.path.exists(self._evalb_program_path):\n            logger.warning(\n                f""EVALB not found at {self._evalb_program_path}.  Attempting to compile it.""\n            )\n            EvalbBracketingScorer.compile_evalb(self._evalb_directory_path)\n\n            # If EVALB executable still doesn\'t exist, raise an error.\n            if not os.path.exists(self._evalb_program_path):\n                compile_command = (\n                    f""python -c \'from allennlp.training.metrics import EvalbBracketingScorer; ""\n                    f\'EvalbBracketingScorer.compile_evalb(""{self._evalb_directory_path}"")\\\'\'\n                )\n                raise ConfigurationError(\n                    f""EVALB still not found at {self._evalb_program_path}. ""\n                    ""You must compile the EVALB scorer before using it.""\n                    "" Run \'make\' in the \'{}\' directory or run: {}"".format(\n                        self._evalb_program_path, compile_command\n                    )\n                )\n        tempdir = tempfile.mkdtemp()\n        gold_path = os.path.join(tempdir, ""gold.txt"")\n        predicted_path = os.path.join(tempdir, ""predicted.txt"")\n        with open(gold_path, ""w"") as gold_file:\n            for tree in gold_trees:\n                gold_file.write(f""{tree.pformat(margin=1000000)}\\n"")\n\n        with open(predicted_path, ""w"") as predicted_file:\n            for tree in predicted_trees:\n                predicted_file.write(f""{tree.pformat(margin=1000000)}\\n"")\n\n        command = [\n            self._evalb_program_path,\n            ""-p"",\n            self._evalb_param_path,\n            ""-e"",\n            str(self._evalb_num_errors_to_kill),\n            gold_path,\n            predicted_path,\n        ]\n        completed_process = subprocess.run(\n            command, stdout=subprocess.PIPE, universal_newlines=True, check=True\n        )\n\n        for line in completed_process.stdout.split(""\\n""):\n            stripped = line.strip().split()\n            if len(stripped) == 12 and stripped != self._header_line:\n                # This line contains results for a single tree.\n                numeric_line = [float(x) for x in stripped]\n                self._correct_predicted_brackets += numeric_line[5]\n                self._gold_brackets += numeric_line[6]\n                self._predicted_brackets += numeric_line[7]\n\n        shutil.rmtree(tempdir)\n\n    @overrides\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        The average precision, recall and f1.\n        """"""\n        recall = (\n            self._correct_predicted_brackets / self._gold_brackets\n            if self._gold_brackets > 0\n            else 0.0\n        )\n        precision = (\n            self._correct_predicted_brackets / self._predicted_brackets\n            if self._gold_brackets > 0\n            else 0.0\n        )\n        f1_measure = (\n            2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        )\n\n        if reset:\n            self.reset()\n        return {\n            ""evalb_recall"": recall,\n            ""evalb_precision"": precision,\n            ""evalb_f1_measure"": f1_measure,\n        }\n\n    @overrides\n    def reset(self):\n        self._correct_predicted_brackets = 0.0\n        self._gold_brackets = 0.0\n        self._predicted_brackets = 0.0\n\n    @staticmethod\n    def compile_evalb(evalb_directory_path: str = DEFAULT_EVALB_DIR):\n        logger.info(f""Compiling EVALB by running make in {evalb_directory_path}."")\n        os.system(""cd {} && make && cd ../../../"".format(evalb_directory_path))\n\n    @staticmethod\n    def clean_evalb(evalb_directory_path: str = DEFAULT_EVALB_DIR):\n        os.system(""rm {}"".format(os.path.join(evalb_directory_path, ""evalb"")))\n'"
allennlp/training/metrics/f1_measure.py,0,"b'from typing import Tuple\n\nfrom allennlp.training.metrics.metric import Metric\nfrom allennlp.training.metrics.fbeta_measure import FBetaMeasure\n\n\n@Metric.register(""f1"")\nclass F1Measure(FBetaMeasure):\n    """"""\n    Computes Precision, Recall and F1 with respect to a given `positive_label`.\n    For example, for a BIO tagging scheme, you would pass the classification index of\n    the tag you are interested in, resulting in the Precision, Recall and F1 score being\n    calculated for this tag only.\n    """"""\n\n    def __init__(self, positive_label: int) -> None:\n        super().__init__(beta=1, labels=[positive_label])\n        self._positive_label = positive_label\n\n    def get_metric(self, reset: bool = False) -> Tuple[float, float, float]:\n        """"""\n        # Returns\n\n        precision : `float`\n        recall : `float`\n        f1-measure : `float`\n        """"""\n        metric = super().get_metric(reset=reset)\n        # Because we just care about the class `positive_label`\n        # there is just one item in `precision`, `recall`, `fscore`\n        precision = metric[""precision""][0]\n        recall = metric[""recall""][0]\n        fscore = metric[""fscore""][0]\n        return precision, recall, fscore\n\n    @property\n    def _true_positives(self):\n        # When this metric is never called, `self._true_positive_sum` is None,\n        # under which case we return 0.0 for backward compatibility.\n        if self._true_positive_sum is None:\n            return 0.0\n        else:\n            return self._true_positive_sum[self._positive_label]\n\n    @property\n    def _true_negatives(self):\n        # When this metric is never called, `self._true_negative_sum` is None,\n        # under which case we return 0.0 for backward compatibility.\n        if self._true_negative_sum is None:\n            return 0.0\n        else:\n            return self._true_negative_sum[self._positive_label]\n\n    @property\n    def _false_positives(self):\n        # When this metric is never called, `self._pred_sum` is None,\n        # under which case we return 0.0 for backward compatibility.\n        if self._pred_sum is None:\n            return 0.0\n        else:\n            # `self._pred_sum` is the total number of instances under each _predicted_ class,\n            # including true positives and false positives.\n            return self._pred_sum[self._positive_label] - self._true_positives\n\n    @property\n    def _false_negatives(self):\n        # When this metric is never called, `self._true_sum` is None,\n        # under which case we return 0.0 for backward compatibility.\n        if self._true_sum is None:\n            return 0.0\n        else:\n            # `self._true_sum` is the total number of instances under each _true_ class,\n            # including true positives and false negatives.\n            return self._true_sum[self._positive_label] - self._true_positives\n'"
allennlp/training/metrics/fbeta_measure.py,22,"b'from typing import List, Optional, Union\n\nimport torch\nfrom overrides import overrides\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""fbeta"")\nclass FBetaMeasure(Metric):\n    """"""Compute precision, recall, F-measure and support for each class.\n\n    The precision is the ratio `tp / (tp + fp)` where `tp` is the number of\n    true positives and `fp` the number of false positives. The precision is\n    intuitively the ability of the classifier not to label as positive a sample\n    that is negative.\n\n    The recall is the ratio `tp / (tp + fn)` where `tp` is the number of\n    true positives and `fn` the number of false negatives. The recall is\n    intuitively the ability of the classifier to find all the positive samples.\n\n    The F-beta score can be interpreted as a weighted harmonic mean of\n    the precision and recall, where an F-beta score reaches its best\n    value at 1 and worst score at 0.\n\n    If we have precision and recall, the F-beta score is simply:\n    `F-beta = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)`\n\n    The F-beta score weights recall more than precision by a factor of\n    `beta`. `beta == 1.0` means recall and precision are equally important.\n\n    The support is the number of occurrences of each class in `y_true`.\n\n    # Parameters\n\n    beta : `float`, optional (default = `1.0`)\n        The strength of recall versus precision in the F-score.\n\n    average : `str`, optional (default = `None`)\n        If `None`, the scores for each class are returned. Otherwise, this\n        determines the type of averaging performed on the data:\n\n        `\'micro\'`:\n            Calculate metrics globally by counting the total true positives,\n            false negatives and false positives.\n        `\'macro\'`:\n            Calculate metrics for each label, and find their unweighted mean.\n            This does not take label imbalance into account.\n        `\'weighted\'`:\n            Calculate metrics for each label, and find their average weighted\n            by support (the number of true instances for each label). This\n            alters \'macro\' to account for label imbalance; it can result in an\n            F-score that is not between precision and recall.\n\n    labels: `list`, optional\n        The set of labels to include and their order if `average is None`.\n        Labels present in the data can be excluded, for example to calculate a\n        multi-class average ignoring a majority negative class. Labels not present\n        in the data will result in 0 components in a macro or weighted average.\n\n    """"""\n\n    def __init__(self, beta: float = 1.0, average: str = None, labels: List[int] = None) -> None:\n        average_options = {None, ""micro"", ""macro"", ""weighted""}\n        if average not in average_options:\n            raise ConfigurationError(f""`average` has to be one of {average_options}."")\n        if beta <= 0:\n            raise ConfigurationError(""`beta` should be >0 in the F-beta score."")\n        if labels is not None and len(labels) == 0:\n            raise ConfigurationError(""`labels` cannot be an empty list."")\n        self._beta = beta\n        self._average = average\n        self._labels = labels\n\n        # statistics\n        # the total number of true positive instances under each class\n        # Shape: (num_classes, )\n        self._true_positive_sum: Union[None, torch.Tensor] = None\n        # the total number of instances\n        # Shape: (num_classes, )\n        self._total_sum: Union[None, torch.Tensor] = None\n        # the total number of instances under each _predicted_ class,\n        # including true positives and false positives\n        # Shape: (num_classes, )\n        self._pred_sum: Union[None, torch.Tensor] = None\n        # the total number of instances under each _true_ class,\n        # including true positives and false negatives\n        # Shape: (num_classes, )\n        self._true_sum: Union[None, torch.Tensor] = None\n\n    @overrides\n    def __call__(\n        self,\n        predictions: torch.Tensor,\n        gold_labels: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n    ):\n        """"""\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ..., num_classes).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of integer class label of shape (batch_size, ...). It must be the same\n            shape as the `predictions` tensor without the `num_classes` dimension.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A masking tensor the same size as `gold_labels`.\n        """"""\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n\n        # Calculate true_positive_sum, true_negative_sum, pred_sum, true_sum\n        num_classes = predictions.size(-1)\n        if (gold_labels >= num_classes).any():\n            raise ConfigurationError(\n                ""A gold label passed to FBetaMeasure contains ""\n                f""an id >= {num_classes}, the number of classes.""\n            )\n\n        # It means we call this metric at the first time\n        # when `self._true_positive_sum` is None.\n        if self._true_positive_sum is None:\n            self._true_positive_sum = torch.zeros(num_classes, device=predictions.device)\n            self._true_sum = torch.zeros(num_classes, device=predictions.device)\n            self._pred_sum = torch.zeros(num_classes, device=predictions.device)\n            self._total_sum = torch.zeros(num_classes, device=predictions.device)\n\n        if mask is None:\n            mask = torch.ones_like(gold_labels).bool()\n        gold_labels = gold_labels.float()\n\n        argmax_predictions = predictions.max(dim=-1)[1].float()\n        true_positives = (gold_labels == argmax_predictions) & mask\n        true_positives_bins = gold_labels[true_positives]\n\n        # Watch it:\n        # The total numbers of true positives under all _predicted_ classes are zeros.\n        if true_positives_bins.shape[0] == 0:\n            true_positive_sum = torch.zeros(num_classes, device=predictions.device)\n        else:\n            true_positive_sum = torch.bincount(\n                true_positives_bins.long(), minlength=num_classes\n            ).float()\n\n        pred_bins = argmax_predictions[mask].long()\n        # Watch it:\n        # When the `mask` is all 0, we will get an _empty_ tensor.\n        if pred_bins.shape[0] != 0:\n            pred_sum = torch.bincount(pred_bins, minlength=num_classes).float()\n        else:\n            pred_sum = torch.zeros(num_classes, device=predictions.device)\n\n        gold_labels_bins = gold_labels[mask].long()\n        if gold_labels.shape[0] != 0:\n            true_sum = torch.bincount(gold_labels_bins, minlength=num_classes).float()\n        else:\n            true_sum = torch.zeros(num_classes, device=predictions.device)\n\n        self._true_positive_sum += true_positive_sum\n        self._pred_sum += pred_sum\n        self._true_sum += true_sum\n        self._total_sum += mask.sum().to(torch.float)\n\n    @overrides\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        precisions : `List[float]`\n        recalls : `List[float]`\n        f1-measures : `List[float]`\n\n        !!! Note\n            If `self.average` is not `None`, you will get `float` instead of `List[float]`.\n        """"""\n        if self._true_positive_sum is None:\n            raise RuntimeError(""You never call this metric before."")\n\n        tp_sum = self._true_positive_sum\n        pred_sum = self._pred_sum\n        true_sum = self._true_sum\n\n        if self._labels is not None:\n            # Retain only selected labels and order them\n            tp_sum = tp_sum[self._labels]\n            pred_sum = pred_sum[self._labels]\n            true_sum = true_sum[self._labels]\n\n        if self._average == ""micro"":\n            tp_sum = tp_sum.sum()\n            pred_sum = pred_sum.sum()\n            true_sum = true_sum.sum()\n\n        beta2 = self._beta ** 2\n        # Finally, we have all our sufficient statistics.\n        precision = _prf_divide(tp_sum, pred_sum)\n        recall = _prf_divide(tp_sum, true_sum)\n        fscore = (1 + beta2) * precision * recall / (beta2 * precision + recall)\n        fscore[tp_sum == 0] = 0.0\n\n        if self._average == ""macro"":\n            precision = precision.mean()\n            recall = recall.mean()\n            fscore = fscore.mean()\n        elif self._average == ""weighted"":\n            weights = true_sum\n            weights_sum = true_sum.sum()\n            precision = _prf_divide((weights * precision).sum(), weights_sum)\n            recall = _prf_divide((weights * recall).sum(), weights_sum)\n            fscore = _prf_divide((weights * fscore).sum(), weights_sum)\n\n        if reset:\n            self.reset()\n\n        if self._average is None:\n            return {\n                ""precision"": precision.tolist(),\n                ""recall"": recall.tolist(),\n                ""fscore"": fscore.tolist(),\n            }\n        else:\n            return {""precision"": precision.item(), ""recall"": recall.item(), ""fscore"": fscore.item()}\n\n    @overrides\n    def reset(self) -> None:\n        self._true_positive_sum = None\n        self._pred_sum = None\n        self._true_sum = None\n        self._total_sum = None\n\n    @property\n    def _true_negative_sum(self):\n        if self._total_sum is None:\n            return None\n        else:\n            true_negative_sum = (\n                self._total_sum - self._pred_sum - self._true_sum + self._true_positive_sum\n            )\n            return true_negative_sum\n\n\ndef _prf_divide(numerator, denominator):\n    """"""Performs division and handles divide-by-zero.\n\n    On zero-division, sets the corresponding result elements to zero.\n    """"""\n    result = numerator / denominator\n    mask = denominator == 0.0\n    if not mask.any():\n        return result\n\n    # remove nan\n    result[mask] = 0.0\n    return result\n'"
allennlp/training/metrics/mean_absolute_error.py,9,"b'from typing import Optional\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""mean_absolute_error"")\nclass MeanAbsoluteError(Metric):\n    """"""\n    This `Metric` calculates the mean absolute error (MAE) between two tensors.\n    """"""\n\n    def __init__(self) -> None:\n        self._absolute_error = 0.0\n        self._total_count = 0.0\n\n    def __call__(\n        self,\n        predictions: torch.Tensor,\n        gold_labels: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n    ):\n        """"""\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ...).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predictions`.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A tensor of the same shape as `predictions`.\n        """"""\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n\n        absolute_errors = torch.abs(predictions - gold_labels)\n        if mask is not None:\n            absolute_errors *= mask\n            self._total_count += torch.sum(mask)\n        else:\n            self._total_count += gold_labels.numel()\n        self._absolute_error += torch.sum(absolute_errors)\n\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        The accumulated mean absolute error.\n        """"""\n        mean_absolute_error = self._absolute_error / self._total_count\n        if reset:\n            self.reset()\n        return mean_absolute_error\n\n    @overrides\n    def reset(self):\n        self._absolute_error = 0.0\n        self._total_count = 0.0\n'"
allennlp/training/metrics/metric.py,6,"b'from typing import Dict, Iterable, List, Optional, Tuple, Union\n\nimport torch\n\nfrom allennlp.common.registrable import Registrable\n\n\nclass Metric(Registrable):\n    """"""\n    A very general abstract class representing a metric which can be\n    accumulated.\n    """"""\n\n    def __call__(\n        self, predictions: torch.Tensor, gold_labels: torch.Tensor, mask: Optional[torch.BoolTensor]\n    ):\n        """"""\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions.\n        gold_labels : `torch.Tensor`, required.\n            A tensor corresponding to some gold label to evaluate against.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A mask can be passed, in order to deal with metrics which are\n            computed over potentially padded elements, such as sequence labels.\n        """"""\n        raise NotImplementedError\n\n    def get_metric(\n        self, reset: bool\n    ) -> Union[float, Tuple[float, ...], Dict[str, float], Dict[str, List[float]]]:\n        """"""\n        Compute and return the metric. Optionally also call `self.reset`.\n        """"""\n        raise NotImplementedError\n\n    def reset(self) -> None:\n        """"""\n        Reset any accumulators or internal state.\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def detach_tensors(*tensors: torch.Tensor) -> Iterable[torch.Tensor]:\n        """"""\n        If you actually passed gradient-tracking Tensors to a Metric, there will be\n        a huge memory leak, because it will prevent garbage collection for the computation\n        graph. This method ensures the tensors are detached.\n        """"""\n        # Check if it\'s actually a tensor in case something else was passed.\n        return (x.detach() if isinstance(x, torch.Tensor) else x for x in tensors)\n'"
allennlp/training/metrics/pearson_correlation.py,6,"b'from typing import Optional\nimport math\nimport numpy as np\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.training.metrics.covariance import Covariance\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""pearson_correlation"")\nclass PearsonCorrelation(Metric):\n    """"""\n    This `Metric` calculates the sample Pearson correlation coefficient (r)\n    between two tensors. Each element in the two tensors is assumed to be\n    a different observation of the variable (i.e., the input tensors are\n    implicitly flattened into vectors and the correlation is calculated\n    between the vectors).\n\n    This implementation is mostly modeled after the streaming_pearson_correlation function in Tensorflow. See\n    <https://github.com/tensorflow/tensorflow/blob/v1.10.1/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3267>.\n\n    This metric delegates to the Covariance metric the tracking of three [co]variances:\n\n    - `covariance(predictions, labels)`, i.e. covariance\n    - `covariance(predictions, predictions)`, i.e. variance of `predictions`\n    - `covariance(labels, labels)`, i.e. variance of `labels`\n\n    If we have these values, the sample Pearson correlation coefficient is simply:\n\n    r = covariance / (sqrt(predictions_variance) * sqrt(labels_variance))\n\n    if predictions_variance or labels_variance is 0, r is 0\n    """"""\n\n    def __init__(self) -> None:\n        self._predictions_labels_covariance = Covariance()\n        self._predictions_variance = Covariance()\n        self._labels_variance = Covariance()\n\n    def __call__(\n        self,\n        predictions: torch.Tensor,\n        gold_labels: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n    ):\n        """"""\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ...).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predictions`.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A tensor of the same shape as `predictions`.\n        """"""\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n        self._predictions_labels_covariance(predictions, gold_labels, mask)\n        self._predictions_variance(predictions, predictions, mask)\n        self._labels_variance(gold_labels, gold_labels, mask)\n\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        The accumulated sample Pearson correlation.\n        """"""\n        covariance = self._predictions_labels_covariance.get_metric(reset=reset)\n        predictions_variance = self._predictions_variance.get_metric(reset=reset)\n        labels_variance = self._labels_variance.get_metric(reset=reset)\n        if reset:\n            self.reset()\n        denominator = math.sqrt(predictions_variance) * math.sqrt(labels_variance)\n        if np.around(denominator, decimals=5) == 0:\n            pearson_r = 0\n        else:\n            pearson_r = covariance / denominator\n        return pearson_r\n\n    @overrides\n    def reset(self):\n        self._predictions_labels_covariance.reset()\n        self._predictions_variance.reset()\n        self._labels_variance.reset()\n'"
allennlp/training/metrics/perplexity.py,1,"b'from overrides import overrides\nimport torch\n\nfrom allennlp.training.metrics.average import Average\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""perplexity"")\nclass Perplexity(Average):\n    """"""\n    Perplexity is a common metric used for evaluating how well a language model\n    predicts a sample.\n\n    Notes\n    -----\n    Assumes negative log likelihood loss of each batch (base e). Provides the\n    average perplexity of the batches.\n    """"""\n\n    @overrides\n    def get_metric(self, reset: bool = False) -> float:\n        """"""\n        # Returns\n\n        The accumulated perplexity.\n        """"""\n        average_loss = super().get_metric(reset)\n        if average_loss == 0:\n            return 0.0\n\n        # Exponentiate the loss to compute perplexity\n        return float(torch.exp(average_loss))\n'"
allennlp/training/metrics/rouge.py,10,"b'from collections import defaultdict\nfrom typing import Tuple, Dict, Set\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""rogue"")\nclass ROUGE(Metric):\n    """"""\n    Recall-Oriented Understudy for Gisting Evaluation (ROUGE)\n\n    ROUGE is a metric for measuring the quality of summaries. It is based on calculating the recall\n    between ngrams in the predicted summary and a set of reference summaries. See [Lin,\n    ""ROUGE: A Package For Automatic Evaluation Of Summaries"", 2004]\n    (https://api.semanticscholar.org/CorpusID:964287).\n\n    # Parameters\n\n    ngram_size : `int`, optional (default = `2`)\n        ROUGE scores are calculate for ROUGE-1 .. ROUGE-`ngram_size`\n    exclude_indices : `Set[int]`, optional (default = `None`)\n        Indices to exclude when calculating ngrams. This should usually include\n        the indices of the start, end, and pad tokens.\n    """"""\n\n    def __init__(self, ngram_size: int = 2, exclude_indices: Set[int] = None,) -> None:\n        self._ngram_size = ngram_size\n        self._exclude_indices = exclude_indices or set()\n\n        self._total_rouge_n_recalls: Dict[int, float] = defaultdict(lambda: 0.0)\n        self._total_rouge_n_precisions: Dict[int, float] = defaultdict(lambda: 0.0)\n        self._total_rouge_n_f1s: Dict[int, float] = defaultdict(lambda: 0.0)\n\n        self._total_rouge_l_f1 = 0.0\n\n        self._total_sequence_count = 0\n\n    @overrides\n    def reset(self) -> None:\n        self._total_rouge_n_recalls = defaultdict(lambda: 0.0)\n        self._total_rouge_n_precisions = defaultdict(lambda: 0.0)\n        self._total_rouge_n_f1s = defaultdict(lambda: 0.0)\n\n        self._total_rouge_l_f1 = 0.0\n\n        self._total_sequence_count = 0\n\n    def _longest_common_subsequence(self, seq_1: torch.LongTensor, seq_2: torch.LongTensor):\n        """"""\n        Computes the longest common subsequences between `seq_1` and `seq_2`, ignoring `self._exclude_indices`.\n        """"""\n        m = len(seq_1)\n        n = len(seq_2)\n\n        # Slightly lower memory usage by iterating over the longer sequence in outer loop\n        # and storing previous lcs for the shorter sequence\n        if m < n:\n            seq_1, seq_2 = seq_2, seq_1\n            m, n = n, m\n\n        prev_lcs = torch.zeros(n + 1, dtype=torch.long)\n\n        for i in range(m - 1, -1, -1):\n            # Make sure we don\'t count special tokens as part of the subsequences\n            if seq_1[i].item() in self._exclude_indices:\n                continue\n\n            cur_lcs = torch.zeros_like(prev_lcs)\n            for j in range(n - 1, -1, -1):\n                if seq_1[i] == seq_2[j]:\n                    cur_lcs[j] = 1 + prev_lcs[j + 1]\n                else:\n                    cur_lcs[j] = max(cur_lcs[j + 1], prev_lcs[j])\n            prev_lcs = cur_lcs\n\n        return prev_lcs[0].item()\n\n    def _get_rouge_l_score(\n        self, predicted_tokens: torch.LongTensor, reference_tokens: torch.LongTensor\n    ) -> float:\n        """"""\n        Compute sum of F1 scores given batch of predictions and references.\n        """"""\n        total_f1 = 0.0\n\n        for predicted_seq, reference_seq in zip(predicted_tokens, reference_tokens):\n            from allennlp.training.util import get_valid_tokens_mask\n\n            m = get_valid_tokens_mask(reference_seq, self._exclude_indices).sum().item()\n            n = get_valid_tokens_mask(predicted_seq, self._exclude_indices).sum().item()\n\n            lcs = self._longest_common_subsequence(reference_seq, predicted_seq)\n\n            # This also rules out the case that m or n are 0, so we don\'t worry about it later\n            if lcs == 0:\n                continue\n\n            recall_lcs = lcs / m\n            precision_lcs = lcs / n\n\n            f1 = 2 * recall_lcs * precision_lcs / (recall_lcs + precision_lcs)\n\n            total_f1 += f1\n\n        return total_f1\n\n    def _get_rouge_n_stats(\n        self,\n        predicted_tokens: torch.LongTensor,\n        reference_tokens: torch.LongTensor,\n        ngram_size: int,\n    ) -> Tuple[float, float, float]:\n        """"""\n        Compare the predicted tokens to the reference (gold) tokens at the desired\n        ngram size and compute recall, precision and f1 sums\n        """"""\n        total_recall = 0.0\n        total_precision = 0.0\n        total_f1 = 0.0\n\n        for predicted_seq, reference_seq in zip(predicted_tokens, reference_tokens):\n            from allennlp.training.util import ngrams\n\n            predicted_ngram_counts = ngrams(predicted_seq, ngram_size, self._exclude_indices)\n            reference_ngram_counts = ngrams(reference_seq, ngram_size, self._exclude_indices)\n\n            matches = 0\n            total_reference_ngrams = 0\n            for ngram, count in reference_ngram_counts.items():\n                matches += min(predicted_ngram_counts[ngram], count)\n                total_reference_ngrams += count\n\n            total_predicted_ngrams = sum(predicted_ngram_counts.values())\n\n            if total_reference_ngrams == 0 or total_predicted_ngrams == 0 or matches == 0:\n                continue\n\n            recall = matches / total_reference_ngrams\n            precision = matches / total_predicted_ngrams\n\n            f1 = 2.0 * recall * precision / (recall + precision)\n\n            # Accumulate stats\n            total_recall += recall\n            total_precision += precision\n            total_f1 += f1\n\n        return total_recall, total_precision, total_f1\n\n    @overrides\n    def __call__(\n        self,  # type: ignore\n        predictions: torch.LongTensor,\n        gold_targets: torch.LongTensor,\n    ) -> None:\n        """"""\n        Update recall counts.\n\n        # Parameters\n\n        predictions : `torch.LongTensor`\n            Batched predicted tokens of shape `(batch_size, max_sequence_length)`.\n        references : `torch.LongTensor`\n            Batched reference (gold) sequences with shape `(batch_size, max_gold_sequence_length)`.\n\n        # Returns\n\n        None\n        """"""\n        # ROUGE-N\n        predictions, gold_targets = self.detach_tensors(predictions, gold_targets)\n        for n in range(1, self._ngram_size + 1):\n\n            recall, precision, f1 = self._get_rouge_n_stats(predictions, gold_targets, n)\n            self._total_rouge_n_recalls[n] += recall\n            self._total_rouge_n_precisions[n] += precision\n            self._total_rouge_n_f1s[n] += f1\n\n        # ROUGE-L\n        self._total_rouge_l_f1 += self._get_rouge_l_score(predictions, gold_targets)\n\n        self._total_sequence_count += len(predictions)\n\n    def _metric_mean(self, metric_sum):\n        if self._total_sequence_count == 0:\n            return 0.0\n        return metric_sum / self._total_sequence_count\n\n    @overrides\n    def get_metric(self, reset: bool = False) -> Dict[str, float]:\n        """"""\n        # Parameters\n\n        reset : `bool`, optional (default = `False`)\n            Reset any accumulators or internal state.\n\n        # Returns\n\n        Dict[str, float]:\n            A dictionary containing `ROUGE-1` .. `ROUGE-ngram_size` scores.\n        """"""\n\n        metrics = {}\n\n        # ROUGE-N\n        # Recall\n        metrics.update(\n            {\n                f""ROUGE-{i}_R"": self._metric_mean(self._total_rouge_n_recalls[i])\n                for i in range(1, self._ngram_size + 1)\n            }\n        )\n\n        # Precision\n        metrics.update(\n            {\n                f""ROUGE-{i}_P"": self._metric_mean(self._total_rouge_n_precisions[i])\n                for i in range(1, self._ngram_size + 1)\n            }\n        )\n\n        # F1\n        metrics.update(\n            {\n                f""ROUGE-{i}_F1"": self._metric_mean(self._total_rouge_n_f1s[i])\n                for i in range(1, self._ngram_size + 1)\n            }\n        )\n\n        # ROUGE-L\n        # F1\n        metrics[""ROUGE-L""] = self._metric_mean(self._total_rouge_l_f1)\n\n        if reset:\n            self.reset()\n\n        return metrics\n'"
allennlp/training/metrics/sequence_accuracy.py,6,"b'from typing import Optional\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""sequence_accuracy"")\nclass SequenceAccuracy(Metric):\n    """"""\n    Sequence Top-K accuracy. Assumes integer labels, with\n    each item to be classified having a single correct class.\n    """"""\n\n    def __init__(self) -> None:\n        self.correct_count = 0.0\n        self.total_count = 0.0\n\n    def __call__(\n        self,\n        predictions: torch.Tensor,\n        gold_labels: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n    ):\n        """"""\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, k, sequence_length).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of integer class label of shape (batch_size, sequence_length).\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A masking tensor the same size as `gold_labels`.\n        """"""\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n\n        # Some sanity checks.\n        if gold_labels.dim() != predictions.dim() - 1:\n            raise ConfigurationError(\n                ""gold_labels must have dimension == predictions.dim() - 1 but ""\n                ""found tensor of shape: {}"".format(gold_labels.size())\n            )\n        if mask is not None and mask.size() != gold_labels.size():\n            raise ConfigurationError(\n                ""mask must have the same size as predictions but ""\n                ""found tensor of shape: {}"".format(mask.size())\n            )\n\n        k = predictions.size()[1]\n        expanded_size = list(gold_labels.size())\n        expanded_size.insert(1, k)\n        expanded_gold = gold_labels.unsqueeze(1).expand(expanded_size)\n\n        if mask is not None:\n            expanded_mask = mask.unsqueeze(1).expand(expanded_size)\n            masked_gold = expanded_mask * expanded_gold\n            masked_predictions = expanded_mask * predictions\n        else:\n            masked_gold = expanded_gold\n            masked_predictions = predictions\n\n        eqs = masked_gold.eq(masked_predictions)\n        matches_per_question = eqs.min(dim=2)[0]\n        some_match = matches_per_question.max(dim=1)[0]\n        correct = some_match.sum().item()\n\n        self.total_count += predictions.size()[0]\n        self.correct_count += correct\n\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        The accumulated accuracy.\n        """"""\n        if self.total_count > 0:\n            accuracy = self.correct_count / self.total_count\n        else:\n            accuracy = 0\n\n        if reset:\n            self.reset()\n        return accuracy\n\n    @overrides\n    def reset(self):\n        self.correct_count = 0.0\n        self.total_count = 0.0\n'"
allennlp/training/metrics/span_based_f1_measure.py,11,"b'from typing import Dict, List, Optional, Set, Callable\nfrom collections import defaultdict\n\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.nn.util import get_lengths_from_binary_sequence_mask\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.training.metrics.metric import Metric\nfrom allennlp.data.dataset_readers.dataset_utils.span_utils import (\n    bio_tags_to_spans,\n    bioul_tags_to_spans,\n    iob1_tags_to_spans,\n    bmes_tags_to_spans,\n    TypedStringSpan,\n)\n\n\nTAGS_TO_SPANS_FUNCTION_TYPE = Callable[[List[str], Optional[List[str]]], List[TypedStringSpan]]\n\n\n@Metric.register(""span_f1"")\nclass SpanBasedF1Measure(Metric):\n    """"""\n    The Conll SRL metrics are based on exact span matching. This metric\n    implements span-based precision and recall metrics for a BIO tagging\n    scheme. It will produce precision, recall and F1 measures per tag, as\n    well as overall statistics. Note that the implementation of this metric\n    is not exactly the same as the perl script used to evaluate the CONLL 2005\n    data - particularly, it does not consider continuations or reference spans\n    as constituents of the original span. However, it is a close proxy, which\n    can be helpful for judging model performance during training. This metric\n    works properly when the spans are unlabeled (i.e., your labels are\n    simply ""B"", ""I"", ""O"" if using the ""BIO"" label encoding).\n\n    """"""\n\n    def __init__(\n        self,\n        vocabulary: Vocabulary,\n        tag_namespace: str = ""tags"",\n        ignore_classes: List[str] = None,\n        label_encoding: Optional[str] = ""BIO"",\n        tags_to_spans_function: Optional[TAGS_TO_SPANS_FUNCTION_TYPE] = None,\n    ) -> None:\n        """"""\n        # Parameters\n\n        vocabulary : `Vocabulary`, required.\n            A vocabulary containing the tag namespace.\n\n        tag_namespace : `str`, required.\n            This metric assumes that a BIO format is used in which the\n            labels are of the format: [""B-LABEL"", ""I-LABEL""].\n\n        ignore_classes : `List[str]`, optional.\n            Span labels which will be ignored when computing span metrics.\n            A ""span label"" is the part that comes after the BIO label, so it\n            would be ""ARG1"" for the tag ""B-ARG1"". For example by passing:\n\n             `ignore_classes=[""V""]`\n            the following sequence would not consider the ""V"" span at index (2, 3)\n            when computing the precision, recall and F1 metrics.\n\n            [""O"", ""O"", ""B-V"", ""I-V"", ""B-ARG1"", ""I-ARG1""]\n\n            This is helpful for instance, to avoid computing metrics for ""V""\n            spans in a BIO tagging scheme which are typically not included.\n\n        label_encoding : `str`, optional (default = `""BIO""`)\n            The encoding used to specify label span endpoints in the sequence.\n            Valid options are ""BIO"", ""IOB1"", ""BIOUL"" or ""BMES"".\n\n        tags_to_spans_function : `Callable`, optional (default = `None`)\n            If `label_encoding` is `None`, `tags_to_spans_function` will be\n            used to generate spans.\n        """"""\n        if label_encoding and tags_to_spans_function:\n            raise ConfigurationError(\n                ""Both label_encoding and tags_to_spans_function are provided. ""\n                \'Set ""label_encoding=None"" explicitly to enable tags_to_spans_function.\'\n            )\n        if label_encoding:\n            if label_encoding not in [""BIO"", ""IOB1"", ""BIOUL"", ""BMES""]:\n                raise ConfigurationError(\n                    ""Unknown label encoding - expected \'BIO\', \'IOB1\', \'BIOUL\', \'BMES\'.""\n                )\n        elif tags_to_spans_function is None:\n            raise ConfigurationError(\n                ""At least one of the (label_encoding, tags_to_spans_function) should be provided.""\n            )\n\n        self._label_encoding = label_encoding\n        self._tags_to_spans_function = tags_to_spans_function\n        self._label_vocabulary = vocabulary.get_index_to_token_vocabulary(tag_namespace)\n        self._ignore_classes: List[str] = ignore_classes or []\n\n        # These will hold per label span counts.\n        self._true_positives: Dict[str, int] = defaultdict(int)\n        self._false_positives: Dict[str, int] = defaultdict(int)\n        self._false_negatives: Dict[str, int] = defaultdict(int)\n\n    def __call__(\n        self,\n        predictions: torch.Tensor,\n        gold_labels: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n        prediction_map: Optional[torch.Tensor] = None,\n    ):\n        """"""\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, sequence_length, num_classes).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of integer class label of shape (batch_size, sequence_length). It must be the same\n            shape as the `predictions` tensor without the `num_classes` dimension.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A masking tensor the same size as `gold_labels`.\n        prediction_map : `torch.Tensor`, optional (default = `None`).\n            A tensor of size (batch_size, num_classes) which provides a mapping from the index of predictions\n            to the indices of the label vocabulary. If provided, the output label at each timestep will be\n            `vocabulary.get_index_to_token_vocabulary(prediction_map[batch, argmax(predictions[batch, t]))`,\n            rather than simply `vocabulary.get_index_to_token_vocabulary(argmax(predictions[batch, t]))`.\n            This is useful in cases where each Instance in the dataset is associated with a different possible\n            subset of labels from a large label-space (IE FrameNet, where each frame has a different set of\n            possible roles associated with it).\n        """"""\n        if mask is None:\n            mask = torch.ones_like(gold_labels).bool()\n\n        predictions, gold_labels, mask, prediction_map = self.detach_tensors(\n            predictions, gold_labels, mask, prediction_map\n        )\n\n        num_classes = predictions.size(-1)\n        if (gold_labels >= num_classes).any():\n            raise ConfigurationError(\n                ""A gold label passed to SpanBasedF1Measure contains an ""\n                ""id >= {}, the number of classes."".format(num_classes)\n            )\n\n        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n        argmax_predictions = predictions.max(-1)[1]\n\n        if prediction_map is not None:\n            argmax_predictions = torch.gather(prediction_map, 1, argmax_predictions)\n            gold_labels = torch.gather(prediction_map, 1, gold_labels.long())\n\n        argmax_predictions = argmax_predictions.float()\n\n        # Iterate over timesteps in batch.\n        batch_size = gold_labels.size(0)\n        for i in range(batch_size):\n            sequence_prediction = argmax_predictions[i, :]\n            sequence_gold_label = gold_labels[i, :]\n            length = sequence_lengths[i]\n\n            if length == 0:\n                # It is possible to call this metric with sequences which are\n                # completely padded. These contribute nothing, so we skip these rows.\n                continue\n\n            predicted_string_labels = [\n                self._label_vocabulary[label_id]\n                for label_id in sequence_prediction[:length].tolist()\n            ]\n            gold_string_labels = [\n                self._label_vocabulary[label_id]\n                for label_id in sequence_gold_label[:length].tolist()\n            ]\n\n            tags_to_spans_function = None\n            # `label_encoding` is empty and `tags_to_spans_function` is provided.\n            if self._label_encoding is None and self._tags_to_spans_function:\n                tags_to_spans_function = self._tags_to_spans_function\n            # Search by `label_encoding`.\n            elif self._label_encoding == ""BIO"":\n                tags_to_spans_function = bio_tags_to_spans\n            elif self._label_encoding == ""IOB1"":\n                tags_to_spans_function = iob1_tags_to_spans\n            elif self._label_encoding == ""BIOUL"":\n                tags_to_spans_function = bioul_tags_to_spans\n            elif self._label_encoding == ""BMES"":\n                tags_to_spans_function = bmes_tags_to_spans\n\n            predicted_spans = tags_to_spans_function(predicted_string_labels, self._ignore_classes)\n            gold_spans = tags_to_spans_function(gold_string_labels, self._ignore_classes)\n\n            predicted_spans = self._handle_continued_spans(predicted_spans)\n            gold_spans = self._handle_continued_spans(gold_spans)\n\n            for span in predicted_spans:\n                if span in gold_spans:\n                    self._true_positives[span[0]] += 1\n                    gold_spans.remove(span)\n                else:\n                    self._false_positives[span[0]] += 1\n            # These spans weren\'t predicted.\n            for span in gold_spans:\n                self._false_negatives[span[0]] += 1\n\n    @staticmethod\n    def _handle_continued_spans(spans: List[TypedStringSpan]) -> List[TypedStringSpan]:\n        """"""\n        The official CONLL 2012 evaluation script for SRL treats continued spans (i.e spans which\n        have a `C-` prepended to another valid tag) as part of the span that they are continuing.\n        This is basically a massive hack to allow SRL models which produce a linear sequence of\n        predictions to do something close to structured prediction. However, this means that to\n        compute the metric, these continuation spans need to be merged into the span to which\n        they refer. The way this is done is to simply consider the span for the continued argument\n        to start at the start index of the first occurrence of the span and end at the end index\n        of the last occurrence of the span. Handling this is important, because predicting continued\n        spans is difficult and typically will effect overall average F1 score by ~ 2 points.\n\n        # Parameters\n\n        spans : `List[TypedStringSpan]`, required.\n            A list of (label, (start, end)) spans.\n\n        # Returns\n\n        A `List[TypedStringSpan]` with continued arguments replaced with a single span.\n        """"""\n        span_set: Set[TypedStringSpan] = set(spans)\n        continued_labels: List[str] = [\n            label[2:] for (label, span) in span_set if label.startswith(""C-"")\n        ]\n        for label in continued_labels:\n            continued_spans = {span for span in span_set if label in span[0]}\n\n            span_start = min(span[1][0] for span in continued_spans)\n            span_end = max(span[1][1] for span in continued_spans)\n            replacement_span: TypedStringSpan = (label, (span_start, span_end))\n\n            span_set.difference_update(continued_spans)\n            span_set.add(replacement_span)\n\n        return list(span_set)\n\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        `Dict[str, float]`\n            A Dict per label containing following the span based metrics:\n            - precision : `float`\n            - recall : `float`\n            - f1-measure : `float`\n\n            Additionally, an `overall` key is included, which provides the precision,\n            recall and f1-measure for all spans.\n        """"""\n        all_tags: Set[str] = set()\n        all_tags.update(self._true_positives.keys())\n        all_tags.update(self._false_positives.keys())\n        all_tags.update(self._false_negatives.keys())\n        all_metrics = {}\n        for tag in all_tags:\n            precision, recall, f1_measure = self._compute_metrics(\n                self._true_positives[tag], self._false_positives[tag], self._false_negatives[tag]\n            )\n            precision_key = ""precision"" + ""-"" + tag\n            recall_key = ""recall"" + ""-"" + tag\n            f1_key = ""f1-measure"" + ""-"" + tag\n            all_metrics[precision_key] = precision\n            all_metrics[recall_key] = recall\n            all_metrics[f1_key] = f1_measure\n\n        # Compute the precision, recall and f1 for all spans jointly.\n        precision, recall, f1_measure = self._compute_metrics(\n            sum(self._true_positives.values()),\n            sum(self._false_positives.values()),\n            sum(self._false_negatives.values()),\n        )\n        all_metrics[""precision-overall""] = precision\n        all_metrics[""recall-overall""] = recall\n        all_metrics[""f1-measure-overall""] = f1_measure\n        if reset:\n            self.reset()\n        return all_metrics\n\n    @staticmethod\n    def _compute_metrics(true_positives: int, false_positives: int, false_negatives: int):\n        precision = true_positives / (true_positives + false_positives + 1e-13)\n        recall = true_positives / (true_positives + false_negatives + 1e-13)\n        f1_measure = 2.0 * (precision * recall) / (precision + recall + 1e-13)\n        return precision, recall, f1_measure\n\n    def reset(self):\n        self._true_positives = defaultdict(int)\n        self._false_positives = defaultdict(int)\n        self._false_negatives = defaultdict(int)\n'"
allennlp/training/metrics/spearman_correlation.py,14,"b'from typing import Optional\r\n\r\nfrom overrides import overrides\r\nimport torch\r\nimport scipy.stats as stats\r\n\r\nfrom allennlp.training.metrics.metric import Metric\r\n\r\n\r\n@Metric.register(""spearman_correlation"")\r\nclass SpearmanCorrelation(Metric):\r\n    """"""\r\n    This `Metric` calculates the sample Spearman correlation coefficient (r)\r\n    between two tensors. Each element in the two tensors is assumed to be\r\n    a different observation of the variable (i.e., the input tensors are\r\n    implicitly flattened into vectors and the correlation is calculated\r\n    between the vectors).\r\n\r\n    <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>\r\n    """"""\r\n\r\n    def __init__(self) -> None:\r\n        super().__init__()\r\n        self.total_predictions = torch.zeros(0)\r\n        self.total_gold_labels = torch.zeros(0)\r\n\r\n    def __call__(\r\n        self,\r\n        predictions: torch.Tensor,\r\n        gold_labels: torch.Tensor,\r\n        mask: Optional[torch.BoolTensor] = None,\r\n    ):\r\n        """"""\r\n        # Parameters\r\n\r\n        predictions : `torch.Tensor`, required.\r\n            A tensor of predictions of shape (batch_size, ...).\r\n        gold_labels : `torch.Tensor`, required.\r\n            A tensor of the same shape as `predictions`.\r\n        mask : `torch.BoolTensor`, optional (default = `None`).\r\n            A tensor of the same shape as `predictions`.\r\n        """"""\r\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\r\n        # Flatten predictions, gold_labels, and mask. We calculate the Spearman correlation between\r\n        # the vectors, since each element in the predictions and gold_labels tensor is assumed\r\n        # to be a separate observation.\r\n        predictions = predictions.reshape(-1)\r\n        gold_labels = gold_labels.reshape(-1)\r\n\r\n        self.total_predictions = self.total_predictions.to(predictions.device)\r\n        self.total_gold_labels = self.total_gold_labels.to(gold_labels.device)\r\n\r\n        if mask is not None:\r\n            mask = mask.reshape(-1)\r\n            self.total_predictions = torch.cat((self.total_predictions, predictions * mask), 0)\r\n            self.total_gold_labels = torch.cat((self.total_gold_labels, gold_labels * mask), 0)\r\n        else:\r\n            self.total_predictions = torch.cat((self.total_predictions, predictions), 0)\r\n            self.total_gold_labels = torch.cat((self.total_gold_labels, gold_labels), 0)\r\n\r\n    @overrides\r\n    def get_metric(self, reset: bool = False):\r\n        """"""\r\n        # Returns\r\n\r\n        The accumulated sample Spearman correlation.\r\n        """"""\r\n        spearman_correlation = stats.spearmanr(\r\n            self.total_predictions.cpu().numpy(), self.total_gold_labels.cpu().numpy()\r\n        )\r\n\r\n        if reset:\r\n            self.reset()\r\n\r\n        return spearman_correlation[0]\r\n\r\n    @overrides\r\n    def reset(self):\r\n        self.total_predictions = torch.zeros(0)\r\n        self.total_gold_labels = torch.zeros(0)\r\n'"
allennlp/training/metrics/unigram_recall.py,6,"b'from typing import Optional\n\nimport sys\n\nfrom overrides import overrides\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.training.metrics.metric import Metric\n\n\n@Metric.register(""unigram_recall"")\nclass UnigramRecall(Metric):\n    """"""\n    Unigram top-K recall. This does not take word order into account. Assumes\n    integer labels, with each item to be classified having a single correct\n    class.\n    """"""\n\n    def __init__(self) -> None:\n        self.correct_count = 0.0\n        self.total_count = 0.0\n\n    def __call__(\n        self,\n        predictions: torch.Tensor,\n        gold_labels: torch.Tensor,\n        mask: Optional[torch.BoolTensor] = None,\n        end_index: int = sys.maxsize,\n    ):\n        """"""\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, k, sequence_length).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of integer class label of shape (batch_size, sequence_length).\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A masking tensor the same size as `gold_labels`.\n        """"""\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n\n        # Some sanity checks.\n        if gold_labels.dim() != predictions.dim() - 1:\n            raise ConfigurationError(\n                ""gold_labels must have dimension == predictions.dim() - 1 but ""\n                ""found tensor of shape: {}"".format(gold_labels.size())\n            )\n        if mask is not None and mask.size() != gold_labels.size():\n            raise ConfigurationError(\n                ""mask must have the same size as predictions but ""\n                ""found tensor of shape: {}"".format(mask.size())\n            )\n\n        batch_size = predictions.size()[0]\n        correct = 0.0\n        for i in range(batch_size):\n            beams = predictions[i]\n            cur_gold = gold_labels[i]\n\n            if mask is not None:\n                masked_gold = cur_gold * mask[i]\n            else:\n                masked_gold = cur_gold\n            cleaned_gold = [x for x in masked_gold if x not in (0, end_index)]\n\n            retval = 0.0\n            for word in cleaned_gold:\n                stillsearch = True\n                for beam in beams:\n                    # word is from cleaned gold which doesn\'t have 0 or\n                    # end_index, so we don\'t need to explicitly remove those\n                    # from beam.\n                    if stillsearch and word in beam:\n                        retval += 1 / len(cleaned_gold)\n                        stillsearch = False\n            correct += retval\n\n        self.correct_count += correct\n        self.total_count += predictions.size()[0]\n\n    def get_metric(self, reset: bool = False):\n        """"""\n        # Returns\n\n        The accumulated recall.\n        """"""\n        recall = self.correct_count / self.total_count if self.total_count > 0 else 0\n        if reset:\n            self.reset()\n        return recall\n\n    @overrides\n    def reset(self):\n        self.correct_count = 0.0\n        self.total_count = 0.0\n'"
allennlp/training/momentum_schedulers/__init__.py,0,b'from allennlp.training.momentum_schedulers.momentum_scheduler import MomentumScheduler\nfrom allennlp.training.momentum_schedulers.inverted_triangular import InvertedTriangular\n'
allennlp/training/momentum_schedulers/inverted_triangular.py,1,"b'import torch\n\nfrom allennlp.training.momentum_schedulers.momentum_scheduler import MomentumScheduler\n\n\n@MomentumScheduler.register(""inverted_triangular"")\nclass InvertedTriangular(MomentumScheduler):\n    """"""\n    Adjust momentum during training according to an inverted triangle-like schedule.\n\n    The momentum starts off high, then decreases linearly for `cool_down` epochs,\n    until reaching `1 / ratio` th of the original value. Then the momentum increases\n    linearly for `warm_up` epochs until reaching its original value again. If there\n    are still more epochs left over to train, the momentum will stay flat at the original\n    value.\n\n    Registered as a `MomentumScheduler` with name ""inverted_triangular"".  The ""optimizer"" argument\n    does not get an entry in a configuration file for the object.\n    """"""\n\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        cool_down: int,\n        warm_up: int,\n        ratio: int = 10,\n        last_epoch: int = -1,\n    ) -> None:\n        self.cool_down = cool_down\n        self.warm_up = warm_up\n        self.ratio = ratio\n        super().__init__(optimizer, last_epoch)\n\n    def get_values(self):\n        step = self.last_epoch + 1\n        if step <= self.cool_down:\n            values = [m - (m - m / self.ratio) * (step / self.cool_down) for m in self.base_values]\n        elif step <= self.cool_down + self.warm_up:\n            values = [\n                (m / self.ratio) + (m - m / self.ratio) * (step - self.cool_down) / self.warm_up\n                for m in self.base_values\n            ]\n        else:\n            values = self.base_values\n\n        return values\n'"
allennlp/training/momentum_schedulers/momentum_scheduler.py,1,"b'import torch\n\nfrom allennlp.common.registrable import Registrable\nfrom allennlp.training.scheduler import Scheduler\n\n\nclass MomentumScheduler(Scheduler, Registrable):\n    def __init__(self, optimizer: torch.optim.Optimizer, last_epoch: int = -1) -> None:\n        super().__init__(optimizer, ""momentum"", last_epoch)\n\n    def get_values(self) -> None:\n        raise NotImplementedError\n'"
benchmarks/data/tokenizers/__init__.py,0,b''
benchmarks/data/tokenizers/character_tokenizer_bench.py,0,"b'from allennlp.data.tokenizers import CharacterTokenizer\n\n\ntokenizer = CharacterTokenizer()\npassage = (\n    ""Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor ""\n    ""incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis ""\n    ""nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. ""\n    ""Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu ""\n    ""fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in ""\n    ""culpa qui officia deserunt mollit anim id est laborum.""\n)\n\n\ndef bench_character_tokenizer(benchmark):\n    benchmark(tokenizer.tokenize, passage)\n'"
scripts/tests/ai2_internal/resume_daemon_test.py,0,"b'import pytest\nimport sqlite3\nfrom unittest.mock import call, Mock\n\nfrom allennlp.common.testing import AllenNlpTestCase\n\nfrom scripts.ai2_internal.resume_daemon import (\n    BeakerStatus,\n    create_table,\n    handler,\n    logger,\n    resume,\n    start_autoresume,\n)\n\n# Don\'t spam the log in tests.\nlogger.removeHandler(handler)\n\n\nclass ResumeDaemonTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.connection = sqlite3.connect("":memory:"")\n        create_table(self.connection)\n\n    def test_create_beaker_status_works(self):\n        status = BeakerStatus(""stopped"")\n        assert status.name == ""stopped""\n\n    def test_create_beaker_status_throws(self):\n        with pytest.raises(ValueError):\n            status = BeakerStatus(""garbage"")\n            assert status.name == ""garbage""\n\n    def test_does_nothing_on_empty_db(self):\n        beaker = Mock()\n        resume(self.connection, beaker)\n        assert not beaker.method_calls\n\n    def test_does_not_resume_a_running_experiment(self):\n        beaker = Mock()\n        experiment_id = ""foo""\n        start_autoresume(self.connection, experiment_id, 5)\n        beaker.get_status.return_value = BeakerStatus.running\n        resume(self.connection, beaker)\n        beaker.get_status.assert_called()\n        assert len(beaker.method_calls) == 1\n\n    def test_does_not_resume_a_finished_experiment(self):\n        beaker = Mock()\n        experiment_id = ""foo""\n        start_autoresume(self.connection, experiment_id, 5)\n        beaker.get_status.return_value = BeakerStatus.succeeded\n        resume(self.connection, beaker)\n        beaker.get_status.assert_called()\n        assert len(beaker.method_calls) == 1\n\n    def test_does_resume_a_preempted_experiment(self):\n        beaker = Mock()\n        experiment_id = ""foo""\n        start_autoresume(self.connection, experiment_id, 5)\n        beaker.get_status.return_value = BeakerStatus.preempted\n        beaker.resume.return_value = ""foo2""\n        resume(self.connection, beaker)\n        beaker.get_status.assert_called()\n        beaker.resume.assert_called()\n        assert len(beaker.method_calls) == 2\n\n    def test_respects_upper_bound_on_resumes(self):\n        beaker = Mock()\n        experiment_id = ""foo""\n        start_autoresume(self.connection, experiment_id, 5)\n        beaker.get_status.return_value = BeakerStatus.preempted\n        for i in range(10):\n            beaker.resume.return_value = f""foo{i}""\n            resume(self.connection, beaker)\n        calls = [\n            call.get_status(""foo""),\n            call.resume(""foo""),\n            call.get_status(""foo0""),\n            call.resume(""foo0""),\n            call.get_status(""foo1""),\n            call.resume(""foo1""),\n            call.get_status(""foo2""),\n            call.resume(""foo2""),\n            call.get_status(""foo3""),\n            call.resume(""foo3""),\n            call.get_status(""foo4""),\n        ]\n        beaker.assert_has_calls(calls)\n\n    def test_handles_a_realistic_scenario(self):\n        beaker = Mock()\n        experiment_id = ""foo""\n        start_autoresume(self.connection, experiment_id, 5)\n        beaker.get_status.return_value = BeakerStatus.preempted\n        for i in range(10):\n            beaker.resume.return_value = f""foo{i}""\n            if i == 2:\n                beaker.get_status.return_value = BeakerStatus.succeeded\n            resume(self.connection, beaker)\n        calls = [\n            call.get_status(""foo""),\n            call.resume(""foo""),\n            call.get_status(""foo0""),\n            call.resume(""foo0""),\n            call.get_status(""foo1""),\n        ]\n        beaker.assert_has_calls(calls)\n'"
scripts/tests/py2md/basic_example.py,0,"b'""""""\nThis is a docstring.\n""""""\n\nSOME_GLOBAL_VAR = ""Ahhhh I\'m a global var!!""\n""""""\nThis is a global var.\n""""""\n\n\ndef func_with_no_args():\n    """"""\n    This function has no args.\n    """"""\n    return None\n\n\ndef func_with_args(a: int, b: int, c: int = 3) -> int:\n    """"""\n    This function has some args.\n\n    # Parameters\n\n    a : `int`\n        A number.\n    b : `int`\n        Another number.\n    c : `int`, optional (default = `3`)\n        Yet another number.\n\n    # Returns\n\n    `int`\n        The result of `a + b * c`.\n    """"""\n    return a + b * c\n\n\nclass SomeClass:\n    """"""\n    I\'m a class!\n\n    # Paramaters\n\n    x : `float`\n        This attribute is called `x`.\n    """"""\n\n    some_class_level_variable = 1\n    """"""\n    This is how you document a class-level variable.\n    """"""\n\n    some_class_level_var_with_type: int = 1\n\n    def __init__(self) -> None:\n        self.x = 1.0\n\n    def _private_method(self) -> None:\n        """"""\n        Private methods should not be included in documentation.\n        """"""\n        pass\n\n    def some_method(self) -> None:\n        """"""\n        I\'m a method!\n\n        But I don\'t do anything.\n\n        # Returns\n\n        `None`\n        """"""\n        return None\n\n    def method_with_alternative_return_section(self) -> int:\n        """"""\n        Another method.\n\n        # Returns\n\n        A completely arbitrary number.\n        """"""\n        return 3\n\n    def method_with_alternative_return_section3(self) -> int:\n        """"""\n        Another method.\n\n        # Returns\n\n        number : `int`\n            A completely arbitrary number.\n        """"""\n        return 3\n\n\nclass AnotherClassWithReallyLongConstructor:\n    def __init__(\n        self,\n        a_really_long_argument_name: int = 0,\n        another_long_name: float = 2,\n        these_variable_names_are_terrible: str = ""yea I know"",\n        **kwargs,\n    ) -> None:\n        self.a = a_really_long_argument_name\n        self.b = another_long_name\n        self.c = these_variable_names_are_terrible\n        self.other = kwargs\n\n\nclass _PrivateClass:\n    def public_method_on_private_class(self):\n        """"""\n        This should not be documented since the class is private.\n        """"""\n        pass\n'"
scripts/tests/py2md/py2md_test.py,0,"b'from typing import Optional\n\nimport pytest\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom scripts.py2md import py2md, Param, DocstringError\n\n\nclass TestPy2md(AllenNlpTestCase):\n    def test_basic_example(self, capsys):\n        py2md(""scripts.tests.py2md.basic_example"")\n        captured = capsys.readouterr()\n\n        with open(\n            self.PROJECT_ROOT / ""scripts"" / ""tests"" / ""py2md"" / ""basic_example_expected_output.md""\n        ) as f:\n            expected = f.read()\n\n        assert captured.out.split(""\\n"") == expected.split(""\\n"")\n\n\n@pytest.mark.parametrize(\n    ""line_in, line_out"",\n    [\n        (\n            ""a : `int`, optional (default = `None`)"",\n            ""- __a__ : `int`, optional (default = `None`) <br>"",\n        ),\n        (\n            ""foo : `Tuple[int, ...]`, optional (default = `()`)"",\n            ""- __foo__ : `Tuple[int, ...]`, optional (default = `()`) <br>"",\n        ),\n        (""a : `int`, required"", ""- __a__ : `int` <br>""),\n        (""a : `int`"", ""- __a__ : `int` <br>""),\n        (""_a : `int`"", ""- __\\\\_a__ : `int` <br>""),\n        (""a_ : `int`"", ""- __a\\\\___ : `int` <br>""),\n    ],\n)\ndef test_param_from_and_to_line(line_in: str, line_out: Optional[str]):\n    param = Param.from_line(line_in)\n    assert param is not None\n    assert param.to_line() == line_out\n\n\n@pytest.mark.parametrize(\n    ""line"",\n    [\n        ""a : `int`, optional (default = None)"",\n        ""a : `int`, optional (default = `None)"",\n        ""a : `int`, optional (default = None`)"",\n        ""a : int"",\n        ""a : `int"",\n        ""a : int`"",\n    ],\n)\ndef test_param_from_bad_line_raises(line: str):\n    with pytest.raises(DocstringError):\n        Param.from_line(line)\n'"
test_fixtures/plugins/d/__init__.py,0,b'from d.d import D\n'
test_fixtures/plugins/d/d.py,0,"b'import argparse\n\nfrom overrides import overrides\n\nfrom allennlp.commands import Subcommand\n\n\ndef do_nothing(_):\n    pass\n\n\n@Subcommand.register(""d"")\nclass D(Subcommand):\n    @overrides\n    def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        subparser = parser.add_parser(self.name, description=""fake"", help=""fake help"")\n        subparser.set_defaults(func=do_nothing)\n        return subparser\n'"
tests/data/dataset_readers/__init__.py,0,b''
tests/data/dataset_readers/babi_reader_test.py,0,"b'import pytest\n\nfrom allennlp.common import Params\nfrom allennlp.common.util import ensure_list\nfrom allennlp.data.dataset_readers import BabiReader\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestBAbIReader:\n    @pytest.mark.parametrize(\n        ""keep_sentences, lazy"", [(False, False), (False, True), (True, False), (True, True)]\n    )\n    def test_read_from_file(self, keep_sentences, lazy):\n        reader = BabiReader(keep_sentences=keep_sentences, lazy=lazy)\n        instances = ensure_list(reader.read(AllenNlpTestCase.FIXTURES_ROOT / ""data"" / ""babi.txt""))\n        assert len(instances) == 8\n\n        if keep_sentences:\n            assert [t.text for t in instances[0].fields[""context""][3].tokens[3:]] == [\n                ""of"",\n                ""wolves"",\n                ""."",\n            ]\n            assert [t.sequence_index for t in instances[0].fields[""supports""]] == [0, 1]\n        else:\n            assert [t.text for t in instances[0].fields[""context""].tokens[7:9]] == [""afraid"", ""of""]\n\n    def test_can_build_from_params(self):\n        reader = BabiReader.from_params(Params({""keep_sentences"": True}))\n\n        assert reader._keep_sentences\n        assert reader._token_indexers[""tokens""].__class__.__name__ == ""SingleIdTokenIndexer""\n'"
tests/data/dataset_readers/dataset_reader_test.py,0,"b'import os\nimport shutil\n\nimport pytest\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.dataset_readers import TextClassificationJsonReader\nfrom allennlp.data.dataset_readers.dataset_reader import _LazyInstances\n\n\nclass TestDatasetReader(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.cache_directory = str(AllenNlpTestCase.FIXTURES_ROOT / ""data_cache"" / ""with_prefix"")\n\n    def teardown_method(self):\n        super().teardown_method()\n        if os.path.exists(self.cache_directory):\n            shutil.rmtree(self.cache_directory)\n\n    def test_read_creates_cache_file_when_not_present(self):\n        data_file = (\n            AllenNlpTestCase.FIXTURES_ROOT\n            / ""data""\n            / ""text_classification_json""\n            / ""imdb_corpus.jsonl""\n        )\n        reader = TextClassificationJsonReader(cache_directory=self.cache_directory)\n        cache_file = reader._get_cache_location_for_file_path(data_file)\n        assert not os.path.exists(cache_file)\n        reader.read(data_file)\n        assert os.path.exists(cache_file)\n\n    def test_read_uses_existing_cache_file_when_present(self):\n        data_file = (\n            AllenNlpTestCase.FIXTURES_ROOT\n            / ""data""\n            / ""text_classification_json""\n            / ""imdb_corpus.jsonl""\n        )\n        snli_copy_file = str(data_file) + "".copy""\n        shutil.copyfile(data_file, snli_copy_file)\n        reader = TextClassificationJsonReader(cache_directory=self.cache_directory)\n\n        # The first read will create the cache.\n        instances = reader.read(snli_copy_file)\n        # Now we _remove_ the data file, to be sure we\'re reading from the cache.\n        os.remove(snli_copy_file)\n        cached_instances = reader.read(snli_copy_file)\n        # We should get the same instances both times.\n        assert len(instances) == len(cached_instances)\n        for instance, cached_instance in zip(instances, cached_instances):\n            assert instance.fields == cached_instance.fields\n\n    def test_read_only_creates_cache_file_once(self):\n        data_file = (\n            AllenNlpTestCase.FIXTURES_ROOT\n            / ""data""\n            / ""text_classification_json""\n            / ""imdb_corpus.jsonl""\n        )\n        reader = TextClassificationJsonReader(cache_directory=self.cache_directory)\n        cache_file = reader._get_cache_location_for_file_path(data_file)\n\n        # The first read will create the cache.\n        reader.read(data_file)\n        assert os.path.exists(cache_file)\n        with open(cache_file, ""r"") as in_file:\n            cache_contents = in_file.read()\n        # The second and all subsequent reads should _use_ the cache, not modify it.  I looked\n        # into checking file modification times, but this test will probably be faster than the\n        # granularity of `os.path.getmtime()` (which only returns values in seconds).\n        reader.read(data_file)\n        reader.read(data_file)\n        reader.read(data_file)\n        reader.read(data_file)\n        with open(cache_file, ""r"") as in_file:\n            final_cache_contents = in_file.read()\n        assert cache_contents == final_cache_contents\n\n    def test_caching_works_with_lazy_reading(self):\n        data_file = (\n            AllenNlpTestCase.FIXTURES_ROOT\n            / ""data""\n            / ""text_classification_json""\n            / ""imdb_corpus.jsonl""\n        )\n        snli_copy_file = str(data_file) + "".copy""\n        shutil.copyfile(data_file, snli_copy_file)\n        reader = TextClassificationJsonReader(lazy=True, cache_directory=self.cache_directory)\n        cache_file = reader._get_cache_location_for_file_path(snli_copy_file)\n\n        # The call to read() will give us an _iterator_.  We\'ll iterate over it multiple times,\n        # and the caching behavior should change as we go.\n        instances = reader.read(snli_copy_file)\n        assert isinstance(instances, _LazyInstances)\n\n        # The first iteration will create the cache\n        assert not os.path.exists(cache_file)\n        first_pass_instances = []\n        for instance in instances:\n            first_pass_instances.append(instance)\n        assert os.path.exists(cache_file)\n\n        # Now we _remove_ the data file, to be sure we\'re reading from the cache.\n        os.remove(snli_copy_file)\n        second_pass_instances = []\n        for instance in instances:\n            second_pass_instances.append(instance)\n\n        # We should get the same instances both times.\n        assert len(first_pass_instances) == len(second_pass_instances)\n        for instance, cached_instance in zip(first_pass_instances, second_pass_instances):\n            assert instance.fields == cached_instance.fields\n\n        # And just to be super paranoid, in case the second pass somehow bypassed the cache\n        # because of a bug in `_CachedLazyInstance` that\'s hard to detect, we\'ll read the\n        # instances from the cache with a non-lazy iterator and make sure they\'re the same.\n        reader = TextClassificationJsonReader(lazy=False, cache_directory=self.cache_directory)\n        cached_instances = reader.read(snli_copy_file)\n        assert len(first_pass_instances) == len(cached_instances)\n        for instance, cached_instance in zip(first_pass_instances, cached_instances):\n            assert instance.fields == cached_instance.fields\n\n    @pytest.mark.parametrize(""lazy"", (True, False))\n    def test_max_instances(self, lazy):\n        data_file = (\n            AllenNlpTestCase.FIXTURES_ROOT\n            / ""data""\n            / ""text_classification_json""\n            / ""imdb_corpus.jsonl""\n        )\n        reader = TextClassificationJsonReader(max_instances=2, lazy=lazy)\n        instances = reader.read(data_file)\n        instance_count = sum(1 for _ in instances)\n        assert instance_count == 2\n\n    @pytest.mark.parametrize(""lazy"", (True, False))\n    def test_cached_max_instances(self, lazy):\n        data_file = (\n            AllenNlpTestCase.FIXTURES_ROOT\n            / ""data""\n            / ""text_classification_json""\n            / ""imdb_corpus.jsonl""\n        )\n\n        # The first read will create the cache if it\'s not there already.\n        reader = TextClassificationJsonReader(cache_directory=self.cache_directory, lazy=lazy)\n        instances = reader.read(data_file)\n        instance_count = sum(1 for _ in instances)\n        assert instance_count > 2\n\n        # The second read should only return two instances, even though it\'s from the cache.\n        reader = TextClassificationJsonReader(\n            cache_directory=self.cache_directory, max_instances=2, lazy=lazy\n        )\n        instances = reader.read(data_file)\n        instance_count = sum(1 for _ in instances)\n        assert instance_count == 2\n'"
tests/data/dataset_readers/interleaving_dataset_reader_test.py,0,"b'from typing import Iterable\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.dataset_readers import DatasetReader, InterleavingDatasetReader\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.tokenizers import SpacyTokenizer\n\n\nclass PlainTextReader(DatasetReader):\n    def __init__(self):\n        super().__init__()\n        self._token_indexers = {""tokens"": SingleIdTokenIndexer()}\n        self._tokenizer = SpacyTokenizer()\n\n    def _read(self, file_path: str) -> Iterable[Instance]:\n        with open(file_path) as input_file:\n            for line in input_file:\n                yield self.text_to_instance(line)\n\n    def text_to_instance(self, line: str) -> Instance:  # type: ignore\n\n        tokens = self._tokenizer.tokenize(line)\n        return Instance({""line"": TextField(tokens, self._token_indexers)})\n\n\nclass TestInterleavingDatasetReader(AllenNlpTestCase):\n    def test_round_robin(self):\n        readers = {""a"": PlainTextReader(), ""b"": PlainTextReader(), ""c"": PlainTextReader()}\n\n        reader = InterleavingDatasetReader(readers)\n        data_dir = self.FIXTURES_ROOT / ""data""\n\n        file_path = f""""""{{\n            ""a"": ""{data_dir / \'babi.txt\'}"",\n            ""b"": ""{data_dir / \'conll2003.txt\'}"",\n            ""c"": ""{data_dir / \'conll2003.txt\'}""\n        }}""""""\n\n        instances = list(reader.read(file_path))\n        first_three_keys = {instance.fields[""dataset""].metadata for instance in instances[:3]}\n        assert first_three_keys == {""a"", ""b"", ""c""}\n\n        next_three_keys = {instance.fields[""dataset""].metadata for instance in instances[3:6]}\n        assert next_three_keys == {""a"", ""b"", ""c""}\n\n    def test_all_at_once(self):\n        readers = {""f"": PlainTextReader(), ""g"": PlainTextReader(), ""h"": PlainTextReader()}\n\n        reader = InterleavingDatasetReader(\n            readers, dataset_field_name=""source"", scheme=""all_at_once""\n        )\n        data_dir = self.FIXTURES_ROOT / ""data""\n\n        file_path = f""""""{{\n            ""f"": ""{data_dir / \'babi.txt\'}"",\n            ""g"": ""{data_dir / \'conll2003.txt\'}"",\n            ""h"": ""{data_dir / \'conll2003.txt\'}""\n        }}""""""\n\n        buckets = []\n        last_source = None\n\n        # Fill up a bucket until the source changes, then start a new one\n        for instance in reader.read(file_path):\n            source = instance.fields[""source""].metadata\n            if source != last_source:\n                buckets.append([])\n                last_source = source\n            buckets[-1].append(instance)\n\n        # should be in 3 buckets\n        assert len(buckets) == 3\n'"
tests/data/dataset_readers/lazy_dataset_reader_test.py,0,"b'from typing import Iterable, List\n\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.dataset_readers import DatasetReader\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.tokenizers import Token\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.common.util import ensure_list\n\n\nclass LazyDatasetReader(DatasetReader):\n    def __init__(self, instances: List[Instance], lazy: bool) -> None:\n        super().__init__()\n        self.lazy = lazy\n        self._instances = instances\n        self.num_reads = 0\n\n    def _read(self, _: str) -> Iterable[Instance]:\n        self.num_reads += 1\n        return (instance for instance in self._instances)\n\n\nclass TestLazyDatasetReader(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        token_indexer = {""tokens"": SingleIdTokenIndexer()}\n\n        field1 = TextField([Token(t) for t in [""this"", ""is"", ""a"", ""sentence"", "".""]], token_indexer)\n        field2 = TextField(\n            [Token(t) for t in [""this"", ""is"", ""a"", ""different"", ""sentence"", "".""]], token_indexer\n        )\n        field3 = TextField([Token(t) for t in [""here"", ""is"", ""a"", ""sentence"", "".""]], token_indexer)\n        field4 = TextField([Token(t) for t in [""this"", ""is"", ""short""]], token_indexer)\n        self.instances = [\n            Instance({""text1"": field1, ""text2"": field2}),\n            Instance({""text1"": field3, ""text2"": field4}),\n        ]\n\n    def test_lazy(self):\n        reader = LazyDatasetReader(self.instances, lazy=True)\n        assert reader.num_reads == 0\n\n        instances = reader.read(""path/to/file"")\n\n        for _ in range(10):\n            _instances = (i for i in instances)\n            assert ensure_list(_instances) == self.instances\n\n        assert reader.num_reads == 10\n\n    def test_non_lazy(self):\n        reader = LazyDatasetReader(self.instances, lazy=False)\n        assert reader.num_reads == 0\n\n        instances = reader.read(""path/to/file"")\n\n        for _ in range(10):\n            _instances = (i for i in instances)\n            assert ensure_list(_instances) == self.instances\n\n        assert reader.num_reads == 1\n'"
tests/data/dataset_readers/sequence_tagging_test.py,0,"b'import pytest\n\nfrom allennlp.data.dataset_readers import SequenceTaggingDatasetReader\nfrom allennlp.common.util import ensure_list\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestSequenceTaggingDatasetReader:\n    @pytest.mark.parametrize(""lazy"", (True, False))\n    def test_default_format(self, lazy):\n        reader = SequenceTaggingDatasetReader(lazy=lazy)\n        instances = reader.read(AllenNlpTestCase.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv"")\n        instances = ensure_list(instances)\n\n        assert len(instances) == 4\n        fields = instances[0].fields\n        assert [t.text for t in fields[""tokens""].tokens] == [""cats"", ""are"", ""animals"", "".""]\n        assert fields[""tags""].labels == [""N"", ""V"", ""N"", ""N""]\n        fields = instances[1].fields\n        assert [t.text for t in fields[""tokens""].tokens] == [""dogs"", ""are"", ""animals"", "".""]\n        assert fields[""tags""].labels == [""N"", ""V"", ""N"", ""N""]\n        fields = instances[2].fields\n        assert [t.text for t in fields[""tokens""].tokens] == [""snakes"", ""are"", ""animals"", "".""]\n        assert fields[""tags""].labels == [""N"", ""V"", ""N"", ""N""]\n        fields = instances[3].fields\n        assert [t.text for t in fields[""tokens""].tokens] == [""birds"", ""are"", ""animals"", "".""]\n        assert fields[""tags""].labels == [""N"", ""V"", ""N"", ""N""]\n\n    def test_brown_corpus_format(self):\n        reader = SequenceTaggingDatasetReader(word_tag_delimiter=""/"")\n        instances = reader.read(AllenNlpTestCase.FIXTURES_ROOT / ""data"" / ""brown_corpus.txt"")\n        instances = ensure_list(instances)\n\n        assert len(instances) == 4\n        fields = instances[0].fields\n        assert [t.text for t in fields[""tokens""].tokens] == [""cats"", ""are"", ""animals"", "".""]\n        assert fields[""tags""].labels == [""N"", ""V"", ""N"", ""N""]\n        fields = instances[1].fields\n        assert [t.text for t in fields[""tokens""].tokens] == [""dogs"", ""are"", ""animals"", "".""]\n        assert fields[""tags""].labels == [""N"", ""V"", ""N"", ""N""]\n        fields = instances[2].fields\n        assert [t.text for t in fields[""tokens""].tokens] == [""snakes"", ""are"", ""animals"", "".""]\n        assert fields[""tags""].labels == [""N"", ""V"", ""N"", ""N""]\n        fields = instances[3].fields\n        assert [t.text for t in fields[""tokens""].tokens] == [""birds"", ""are"", ""animals"", "".""]\n        assert fields[""tags""].labels == [""N"", ""V"", ""N"", ""N""]\n'"
tests/data/dataset_readers/sharded_dataset_reader_test.py,0,"b'from collections import Counter\nfrom typing import Tuple\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.dataset_readers import SequenceTaggingDatasetReader, ShardedDatasetReader\nfrom allennlp.data.instance import Instance\n\n\ndef fingerprint(instance: Instance) -> Tuple[str, ...]:\n    """"""\n    Get a hashable representation of a sequence tagging instance\n    that can be put in a Counter.\n    """"""\n    text_tuple = tuple(t.text for t in instance.fields[""tokens""].tokens)  # type: ignore\n    labels_tuple = tuple(instance.fields[""tags""].labels)  # type: ignore\n    return text_tuple + labels_tuple\n\n\nclass TestShardedDatasetReader(AllenNlpTestCase):\n    def setup_method(self) -> None:\n        super().setup_method()\n\n        # use SequenceTaggingDatasetReader as the base reader\n        self.base_reader = SequenceTaggingDatasetReader(lazy=True)\n        base_file_path = AllenNlpTestCase.FIXTURES_ROOT / ""data"" / ""sequence_tagging.tsv""\n\n        # Make 100 copies of the data\n        raw_data = open(base_file_path).read()\n        for i in range(100):\n            file_path = self.TEST_DIR / f""identical_{i}.tsv""\n            with open(file_path, ""w"") as f:\n                f.write(raw_data)\n\n        self.identical_files_glob = str(self.TEST_DIR / ""identical_*.tsv"")\n\n    def test_sharded_read(self):\n        reader = ShardedDatasetReader(base_reader=self.base_reader)\n\n        all_instances = []\n\n        for instance in reader.read(self.identical_files_glob):\n            all_instances.append(instance)\n\n        # 100 files * 4 sentences / file\n        assert len(all_instances) == 100 * 4\n\n        counts = Counter(fingerprint(instance) for instance in all_instances)\n\n        # should have the exact same data 100 times\n        assert len(counts) == 4\n        assert counts[(""cats"", ""are"", ""animals"", ""."", ""N"", ""V"", ""N"", ""N"")] == 100\n        assert counts[(""dogs"", ""are"", ""animals"", ""."", ""N"", ""V"", ""N"", ""N"")] == 100\n        assert counts[(""snakes"", ""are"", ""animals"", ""."", ""N"", ""V"", ""N"", ""N"")] == 100\n        assert counts[(""birds"", ""are"", ""animals"", ""."", ""N"", ""V"", ""N"", ""N"")] == 100\n'"
tests/data/dataset_readers/text_classification_json_test.py,0,"b'import pytest\nfrom typing import List\n\nfrom allennlp.data.dataset_readers import TextClassificationJsonReader\nfrom allennlp.common.util import ensure_list\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter\nfrom allennlp.common.util import get_spacy_model\n\n\nclass TestTextClassificationJsonReader:\n    @pytest.mark.parametrize(""lazy"", (True, False))\n    def test_set_skip_indexing_true(self, lazy):\n        reader = TextClassificationJsonReader(lazy=lazy, skip_label_indexing=True)\n        ag_path = (\n            AllenNlpTestCase.FIXTURES_ROOT\n            / ""data""\n            / ""text_classification_json""\n            / ""integer_labels.jsonl""\n        )\n        instances = reader.read(ag_path)\n        instances = ensure_list(instances)\n\n        instance1 = {""tokens"": [""This"", ""text"", ""has"", ""label"", ""0""], ""label"": 0}\n        instance2 = {""tokens"": [""This"", ""text"", ""has"", ""label"", ""1""], ""label"": 1}\n\n        assert len(instances) == 2\n        fields = instances[0].fields\n        assert [t.text for t in fields[""tokens""].tokens] == instance1[""tokens""]\n        assert fields[""label""].label == instance1[""label""]\n        fields = instances[1].fields\n        assert [t.text for t in fields[""tokens""].tokens] == instance2[""tokens""]\n        assert fields[""label""].label == instance2[""label""]\n\n        with pytest.raises(ValueError) as exec_info:\n            ag_path = (\n                AllenNlpTestCase.FIXTURES_ROOT\n                / ""data""\n                / ""text_classification_json""\n                / ""imdb_corpus.jsonl""\n            )\n            ensure_list(reader.read(ag_path))\n        assert str(exec_info.value) == ""Labels must be integers if skip_label_indexing is True.""\n\n    @pytest.mark.parametrize(""lazy"", (True, False))\n    def test_read_from_file_ag_news_corpus(self, lazy):\n        reader = TextClassificationJsonReader(lazy=lazy)\n        ag_path = (\n            AllenNlpTestCase.FIXTURES_ROOT\n            / ""data""\n            / ""text_classification_json""\n            / ""ag_news_corpus.jsonl""\n        )\n        instances = reader.read(ag_path)\n        instances = ensure_list(instances)\n\n        instance1 = {\n            ""tokens"": [\n                ""Memphis"",\n                ""Rout"",\n                ""Still"",\n                ""Stings"",\n                ""for"",\n                ""No"",\n                ""."",\n                ""14"",\n                ""Louisville"",\n                "";"",\n                ""Coach"",\n                ""Petrino"",\n                ""Vows"",\n                ""to"",\n                ""Have"",\n                ""Team"",\n                ""Better"",\n                ""Prepared"",\n                ""."",\n                ""NASHVILLE"",\n                "","",\n                ""Tenn."",\n                ""Nov"",\n                ""3"",\n                "","",\n                ""2004"",\n                ""-"",\n                ""Louisville"",\n                ""#"",\n                ""39;s"",\n                ""30-point"",\n                ""loss"",\n                ""at"",\n                ""home"",\n                ""to"",\n                ""Memphis"",\n                ""last"",\n                ""season"",\n                ""is"",\n                ""still"",\n                ""a"",\n                ""painful"",\n                ""memory"",\n                ""for"",\n                ""the"",\n                ""Cardinals"",\n                ""."",\n            ],\n            ""label"": ""2"",\n        }\n        instance2 = {\n            ""tokens"": [\n                ""AP"",\n                ""-"",\n                ""Eli"",\n                ""Manning"",\n                ""has"",\n                ""replaced"",\n                ""Kurt"",\n                ""Warner"",\n                ""as"",\n                ""the"",\n                ""New"",\n                ""York"",\n                ""Giants"",\n                ""\'"",\n                ""starting"",\n                ""quarterback"",\n                ""."",\n            ],\n            ""label"": ""2"",\n        }\n        instance3 = {\n            ""tokens"": [\n                ""A"",\n                ""conference"",\n                ""dedicated"",\n                ""to"",\n                ""online"",\n                ""journalism"",\n                ""explores"",\n                ""the"",\n                ""effect"",\n                ""blogs"",\n                ""have"",\n                ""on"",\n                ""news"",\n                ""reporting"",\n                ""."",\n                ""Some"",\n                ""say"",\n                ""they"",\n                ""draw"",\n                ""attention"",\n                ""to"",\n                ""under"",\n                ""-"",\n                ""reported"",\n                ""stories"",\n                ""."",\n                ""Others"",\n                ""struggle"",\n                ""to"",\n                ""establish"",\n                ""the"",\n                ""credibility"",\n                ""enjoyed"",\n                ""by"",\n                ""professionals"",\n                ""."",\n            ],\n            ""label"": ""4"",\n        }\n\n        assert len(instances) == 3\n        fields = instances[0].fields\n        assert [t.text for t in fields[""tokens""].tokens] == instance1[""tokens""]\n        assert fields[""label""].label == instance1[""label""]\n        fields = instances[1].fields\n        assert [t.text for t in fields[""tokens""].tokens] == instance2[""tokens""]\n        assert fields[""label""].label == instance2[""label""]\n        fields = instances[2].fields\n        assert [t.text for t in fields[""tokens""].tokens] == instance3[""tokens""]\n        assert fields[""label""].label == instance3[""label""]\n\n    @pytest.mark.parametrize(""lazy"", (True, False))\n    def test_read_from_file_ag_news_corpus_and_truncates_properly(self, lazy):\n        reader = TextClassificationJsonReader(lazy=lazy, max_sequence_length=5)\n        ag_path = (\n            AllenNlpTestCase.FIXTURES_ROOT\n            / ""data""\n            / ""text_classification_json""\n            / ""ag_news_corpus.jsonl""\n        )\n        instances = reader.read(ag_path)\n        instances = ensure_list(instances)\n\n        instance1 = {""tokens"": [""Memphis"", ""Rout"", ""Still"", ""Stings"", ""for""], ""label"": ""2""}\n        instance2 = {""tokens"": [""AP"", ""-"", ""Eli"", ""Manning"", ""has""], ""label"": ""2""}\n        instance3 = {""tokens"": [""A"", ""conference"", ""dedicated"", ""to"", ""online""], ""label"": ""4""}\n\n        assert len(instances) == 3\n        fields = instances[0].fields\n        assert [t.text for t in fields[""tokens""].tokens] == instance1[""tokens""]\n        assert fields[""label""].label == instance1[""label""]\n        fields = instances[1].fields\n        assert [t.text for t in fields[""tokens""].tokens] == instance2[""tokens""]\n        assert fields[""label""].label == instance2[""label""]\n        fields = instances[2].fields\n        assert [t.text for t in fields[""tokens""].tokens] == instance3[""tokens""]\n        assert fields[""label""].label == instance3[""label""]\n\n    @pytest.mark.parametrize(""max_sequence_length"", (None, 5))\n    @pytest.mark.parametrize(""lazy"", (True, False))\n    def test_read_from_file_ag_news_corpus_and_segments_sentences_properly(\n        self, lazy, max_sequence_length\n    ):\n        reader = TextClassificationJsonReader(\n            lazy=lazy, segment_sentences=True, max_sequence_length=max_sequence_length\n        )\n        ag_path = (\n            AllenNlpTestCase.FIXTURES_ROOT\n            / ""data""\n            / ""text_classification_json""\n            / ""ag_news_corpus.jsonl""\n        )\n        instances = reader.read(ag_path)\n        instances = ensure_list(instances)\n\n        splitter = SpacySentenceSplitter()\n        spacy_tokenizer = get_spacy_model(""en_core_web_sm"", False, False, False)\n\n        text1 = (\n            ""Memphis Rout Still Stings for No. 14 Louisville; Coach ""\n            ""Petrino Vows to Have Team Better Prepared. NASHVILLE, ""\n            ""Tenn. Nov 3, 2004 - Louisville #39;s 30-point loss ""\n            ""at home to Memphis last season is still a painful memory ""\n            ""for the Cardinals.""\n        )\n        instance1 = {""text"": text1, ""label"": ""2""}\n        text2 = (\n            ""AP - Eli Manning has replaced Kurt Warner as the New York""\n            "" Giants\' starting quarterback.""\n        )\n        instance2 = {""text"": text2, ""label"": ""2""}\n        text3 = (\n            ""A conference dedicated to online journalism explores the ""\n            ""effect blogs have on news reporting. Some say they draw ""\n            ""attention to under-reported stories. Others struggle to ""\n            ""establish the credibility enjoyed by professionals.""\n        )\n        instance3 = {""text"": text3, ""label"": ""4""}\n\n        for instance in [instance1, instance2, instance3]:\n            sentences = splitter.split_sentences(instance[""text""])\n            tokenized_sentences: List[List[str]] = []\n            for sentence in sentences:\n                tokens = [token.text for token in spacy_tokenizer(sentence)]\n                if max_sequence_length:\n                    tokens = tokens[:max_sequence_length]\n                tokenized_sentences.append(tokens)\n            instance[""tokens""] = tokenized_sentences\n\n        assert len(instances) == 3\n        fields = instances[0].fields\n        text = [[token.text for token in sentence.tokens] for sentence in fields[""tokens""]]\n        assert text == instance1[""tokens""]\n        assert fields[""label""].label == instance1[""label""]\n        fields = instances[1].fields\n        text = [[token.text for token in sentence.tokens] for sentence in fields[""tokens""]]\n        assert text == instance2[""tokens""]\n        assert fields[""label""].label == instance2[""label""]\n        fields = instances[2].fields\n        text = [[token.text for token in sentence.tokens] for sentence in fields[""tokens""]]\n        assert text == instance3[""tokens""]\n        assert fields[""label""].label == instance3[""label""]\n'"
tests/data/fields/__init__.py,0,b''
tests/data/fields/adjacency_field_test.py,0,"b'import pytest\nimport numpy\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.fields import AdjacencyField, TextField\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data import Vocabulary, Token\n\n\nclass TestAdjacencyField(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.text = TextField(\n            [Token(t) for t in [""here"", ""is"", ""a"", ""sentence"", "".""]],\n            {""words"": SingleIdTokenIndexer(""words"")},\n        )\n\n    def test_adjacency_field_can_index_with_vocab(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""a"", namespace=""labels"")\n        vocab.add_token_to_namespace(""b"", namespace=""labels"")\n        vocab.add_token_to_namespace(""c"", namespace=""labels"")\n\n        labels = [""a"", ""b""]\n        indices = [(0, 1), (2, 1)]\n        adjacency_field = AdjacencyField(indices, self.text, labels)\n        adjacency_field.index(vocab)\n        tensor = adjacency_field.as_tensor(adjacency_field.get_padding_lengths())\n        numpy.testing.assert_equal(\n            tensor.numpy(),\n            numpy.array(\n                [\n                    [-1, 0, -1, -1, -1],\n                    [-1, -1, -1, -1, -1],\n                    [-1, 1, -1, -1, -1],\n                    [-1, -1, -1, -1, -1],\n                    [-1, -1, -1, -1, -1],\n                ]\n            ),\n        )\n\n    def test_adjacency_field_raises_with_out_of_bounds_indices(self):\n        with pytest.raises(ConfigurationError):\n            _ = AdjacencyField([(0, 24)], self.text)\n\n    def test_adjacency_field_raises_with_mismatching_labels_for_indices(self):\n        with pytest.raises(ConfigurationError):\n            _ = AdjacencyField([(0, 1), (0, 2)], self.text, [""label1""])\n\n    def test_adjacency_field_raises_with_duplicate_indices(self):\n        with pytest.raises(ConfigurationError):\n            _ = AdjacencyField([(0, 1), (0, 1)], self.text, [""label1""])\n\n    def test_adjacency_field_empty_field_works(self):\n        field = AdjacencyField([(0, 1)], self.text)\n        empty_field = field.empty_field()\n        assert empty_field.indices == []\n\n    def test_printing_doesnt_crash(self):\n        adjacency_field = AdjacencyField([(0, 1)], self.text, [""label1""])\n        print(adjacency_field)\n'"
tests/data/fields/array_field_test.py,6,"b'import numpy\nimport torch\n\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.data.fields import ArrayField, ListField\n\n\nclass TestArrayField(AllenNlpTestCase):\n    def test_get_padding_lengths_correctly_returns_ordered_shape(self):\n        shape = [3, 4, 5, 6]\n        array = numpy.zeros(shape)\n        array_field = ArrayField(array)\n        lengths = array_field.get_padding_lengths()\n        for i in range(len(lengths)):\n            assert lengths[""dimension_{}"".format(i)] == shape[i]\n\n    def test_as_tensor_handles_larger_padding_dimensions(self):\n        shape = [3, 4]\n        array = numpy.ones(shape)\n        array_field = ArrayField(array)\n\n        padded_tensor = (\n            array_field.as_tensor({""dimension_0"": 5, ""dimension_1"": 6}).detach().cpu().numpy()\n        )\n        numpy.testing.assert_array_equal(padded_tensor[:3, :4], array)\n        numpy.testing.assert_array_equal(padded_tensor[3:, 4:], 0.0)\n\n    def test_padding_handles_list_fields(self):\n        array1 = ArrayField(numpy.ones([2, 3]))\n        array2 = ArrayField(numpy.ones([1, 5]))\n        empty_array = array1.empty_field()\n        list_field = ListField([array1, array2, empty_array])\n\n        returned_tensor = (\n            list_field.as_tensor(list_field.get_padding_lengths()).detach().cpu().numpy()\n        )\n        correct_tensor = numpy.array(\n            [\n                [[1.0, 1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0, 0.0]],\n                [[1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0]],\n                [[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]],\n            ]\n        )\n        numpy.testing.assert_array_equal(returned_tensor, correct_tensor)\n\n    def test_padding_handles_list_fields_with_padding_values(self):\n        array1 = ArrayField(numpy.ones([2, 3]), padding_value=-1)\n        array2 = ArrayField(numpy.ones([1, 5]), padding_value=-1)\n        empty_array = array1.empty_field()\n        list_field = ListField([array1, array2, empty_array])\n\n        returned_tensor = (\n            list_field.as_tensor(list_field.get_padding_lengths()).detach().cpu().numpy()\n        )\n        correct_tensor = numpy.array(\n            [\n                [[1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0]],\n                [[1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0]],\n                [[-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0]],\n            ]\n        )\n        numpy.testing.assert_array_equal(returned_tensor, correct_tensor)\n\n    def test_printing_doesnt_crash(self):\n        array = ArrayField(numpy.ones([2, 3]), padding_value=-1)\n        print(array)\n\n    def test_as_tensor_works_with_scalar(self):\n        array = ArrayField(numpy.asarray(42))\n        returned_tensor = array.as_tensor(array.get_padding_lengths())\n        current_tensor = numpy.asarray(42)\n        numpy.testing.assert_array_equal(returned_tensor, current_tensor)\n\n    def test_as_tensor_with_scalar_keeps_dtype(self):\n        array = ArrayField(numpy.asarray(42, dtype=numpy.float32))\n        returned_tensor = array.as_tensor(array.get_padding_lengths())\n        assert returned_tensor.dtype == torch.float32\n\n    def test_alternative_dtypes(self):\n        shape = [3, 4, 5, 6]\n        array = numpy.zeros(shape)\n\n        # Setting dtype to numpy.int64 should produce a torch.LongTensor when field is converted to\n        # a tensor\n        array_field1 = ArrayField(array, dtype=numpy.int64)\n        returned_tensor1 = array_field1.as_tensor(array_field1.get_padding_lengths())\n        assert returned_tensor1.dtype == torch.int64\n\n        # Setting dtype to numpy.uint8 should produce a torch.ByteTensor when field is converted to\n        # a tensor\n        array_field2 = ArrayField(array, dtype=numpy.uint8)\n        returned_tensor2 = array_field2.as_tensor(array_field2.get_padding_lengths())\n        assert returned_tensor2.dtype == torch.uint8\n\n        # Padding should not affect dtype\n        padding_lengths = {""dimension_"" + str(i): 10 for i, _ in enumerate(shape)}\n        padded_tensor = array_field2.as_tensor(padding_lengths)\n        assert padded_tensor.dtype == torch.uint8\n\n        # Empty fields should have the same dtype\n        empty_field = array_field2.empty_field()\n        assert empty_field.dtype == array_field2.dtype\n\n    def test_len_works_with_scalar(self):\n        array = ArrayField(numpy.asarray(42))\n        assert len(array) == 1\n'"
tests/data/fields/field_test.py,0,"b'from allennlp.data.fields import Field\n\n\ndef test_eq_with_inheritance():\n    class SubField(Field):\n\n        __slots__ = [""a""]\n\n        def __init__(self, a):\n            self.a = a\n\n    class SubSubField(SubField):\n\n        __slots__ = [""b""]\n\n        def __init__(self, a, b):\n            super().__init__(a)\n            self.b = b\n\n    class SubSubSubField(SubSubField):\n\n        __slots__ = [""c""]\n\n        def __init__(self, a, b, c):\n            super().__init__(a, b)\n            self.c = c\n\n    assert SubField(1) == SubField(1)\n    assert SubField(1) != SubField(2)\n\n    assert SubSubField(1, 2) == SubSubField(1, 2)\n    assert SubSubField(1, 2) != SubSubField(1, 1)\n    assert SubSubField(1, 2) != SubSubField(2, 2)\n\n    assert SubSubSubField(1, 2, 3) == SubSubSubField(1, 2, 3)\n    assert SubSubSubField(1, 2, 3) != SubSubSubField(0, 2, 3)\n\n\ndef test_eq_with_inheritance_for_non_slots_field():\n    class SubField(Field):\n        def __init__(self, a):\n            self.a = a\n\n    assert SubField(1) == SubField(1)\n    assert SubField(1) != SubField(2)\n\n\ndef test_eq_with_inheritance_for_mixed_field():\n    class SubField(Field):\n\n        __slots__ = [""a""]\n\n        def __init__(self, a):\n            self.a = a\n\n    class SubSubField(SubField):\n        def __init__(self, a, b):\n            super().__init__(a)\n            self.b = b\n\n    assert SubField(1) == SubField(1)\n    assert SubField(1) != SubField(2)\n\n    assert SubSubField(1, 2) == SubSubField(1, 2)\n    assert SubSubField(1, 2) != SubSubField(1, 1)\n    assert SubSubField(1, 2) != SubSubField(2, 2)\n'"
tests/data/fields/flag_field_test.py,0,"b'import pytest\n\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.data.fields import FlagField\n\n\nclass TestFlagField(AllenNlpTestCase):\n    def test_get_padding_lengths_returns_nothing(self):\n        flag_field = FlagField(True)\n        assert flag_field.get_padding_lengths() == {}\n\n    def test_as_tensor_just_returns_value(self):\n        for value in [True, 3.234, ""this is a string""]:\n            assert FlagField(value).as_tensor({}) == value\n\n    def test_printing_doesnt_crash(self):\n        flag = FlagField(True)\n        print(flag)\n\n    def test_batch_tensors_returns_single_value(self):\n        value = True\n        fields = [FlagField(value) for _ in range(5)]\n        values = [field.as_tensor({}) for field in fields]\n        batched_value = fields[0].batch_tensors(values)\n        assert batched_value == value\n\n    def test_batch_tensors_crashes_with_non_uniform_values(self):\n        field = FlagField(True)\n        with pytest.raises(ValueError):\n            field.batch_tensors([True, False, True])\n\n        with pytest.raises(ValueError):\n            field.batch_tensors([1, 2, 3, 4])\n\n        with pytest.raises(ValueError):\n            field.batch_tensors([""different"", ""string"", ""flags""])\n'"
tests/data/fields/index_field_test.py,0,"b'import numpy\nimport pytest\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Token\nfrom allennlp.data.fields import TextField, IndexField\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\n\n\nclass TestIndexField(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.text = TextField(\n            [Token(t) for t in [""here"", ""is"", ""a"", ""sentence"", "".""]],\n            {""words"": SingleIdTokenIndexer(""words"")},\n        )\n\n    def test_as_tensor_converts_field_correctly(self):\n        index_field = IndexField(4, self.text)\n        tensor = index_field.as_tensor(index_field.get_padding_lengths()).detach().cpu().numpy()\n        numpy.testing.assert_array_equal(tensor, numpy.array([4]))\n\n    def test_index_field_raises_on_incorrect_label_type(self):\n        with pytest.raises(ConfigurationError):\n            _ = IndexField(""hello"", self.text)\n\n    def test_index_field_empty_field_works(self):\n        index_field = IndexField(4, self.text)\n        empty_index = index_field.empty_field()\n        assert empty_index.sequence_index == -1\n\n    def test_printing_doesnt_crash(self):\n        print(self.text)\n\n    def test_equality(self):\n        index_field1 = IndexField(4, self.text)\n        index_field2 = IndexField(4, self.text)\n        index_field3 = IndexField(\n            4,\n            TextField(\n                [Token(t) for t in [""AllenNLP"", ""is"", ""the"", ""bomb"", ""!""]],\n                {""words"": SingleIdTokenIndexer(""words"")},\n            ),\n        )\n\n        assert index_field1 == 4\n        assert index_field1 == index_field1\n        assert index_field1 == index_field2\n\n        assert index_field1 != index_field3\n        assert index_field2 != index_field3\n        assert index_field3 == index_field3\n'"
tests/data/fields/label_field_test.py,0,"b'import logging\n\nimport pytest\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.fields import LabelField\nfrom allennlp.data import Vocabulary\n\n\nclass TestLabelField(AllenNlpTestCase):\n    def test_as_tensor_returns_integer_tensor(self):\n        label = LabelField(5, skip_indexing=True)\n\n        tensor = label.as_tensor(label.get_padding_lengths())\n        assert tensor.item() == 5\n\n    def test_label_field_can_index_with_vocab(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""entailment"", namespace=""labels"")\n        vocab.add_token_to_namespace(""contradiction"", namespace=""labels"")\n        vocab.add_token_to_namespace(""neutral"", namespace=""labels"")\n\n        label = LabelField(""entailment"")\n        label.index(vocab)\n        tensor = label.as_tensor(label.get_padding_lengths())\n        assert tensor.item() == 0\n\n    def test_label_field_raises_with_non_integer_labels_and_no_indexing(self):\n        with pytest.raises(ConfigurationError):\n            _ = LabelField(""non integer field"", skip_indexing=True)\n\n    def test_label_field_raises_with_incorrect_label_type(self):\n        with pytest.raises(ConfigurationError):\n            _ = LabelField([], skip_indexing=False)\n\n    def test_label_field_empty_field_works(self):\n        label = LabelField(""test"")\n        empty_label = label.empty_field()\n        assert empty_label.label == -1\n\n    def test_class_variables_for_namespace_warnings_work_correctly(self, caplog):\n        with caplog.at_level(logging.WARNING, logger=""allennlp.data.fields.label_field""):\n            assert ""text"" not in LabelField._already_warned_namespaces\n            _ = LabelField(""test"", label_namespace=""text"")\n            assert caplog.records\n\n            # We\'ve warned once, so we should have set the class variable to False.\n            assert ""text"" in LabelField._already_warned_namespaces\n            caplog.clear()\n            _ = LabelField(""test2"", label_namespace=""text"")\n            assert not caplog.records\n\n            # ... but a new namespace should still log a warning.\n            assert ""text2"" not in LabelField._already_warned_namespaces\n            caplog.clear()\n            _ = LabelField(""test"", label_namespace=""text2"")\n            assert caplog.records\n\n    def test_printing_doesnt_crash(self):\n        label = LabelField(""label"", label_namespace=""namespace"")\n        print(label)\n'"
tests/data/fields/list_field_test.py,3,"b'from typing import Dict\n\nimport numpy\nimport torch\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Token, Vocabulary, Instance\nfrom allennlp.data.fields import TextField, LabelField, ListField, IndexField, SequenceLabelField\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer, TokenCharactersIndexer\nfrom allennlp.data.dataloader import DataLoader\nfrom allennlp.data.dataset_readers.dataset_reader import AllennlpDataset\nfrom allennlp.data.tokenizers import SpacyTokenizer\nfrom allennlp.models import Model\nfrom allennlp.modules import Embedding\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n\n\nclass DummyModel(Model):\n    """"""\n    Performs a common operation (embedding) that won\'t work on an empty tensor.\n    Returns an arbitrary loss.\n    """"""\n\n    def __init__(self, vocab: Vocabulary) -> None:\n        super().__init__(vocab)\n        weight = torch.ones(vocab.get_vocab_size(), 10)\n        token_embedding = Embedding(\n            num_embeddings=vocab.get_vocab_size(), embedding_dim=10, weight=weight, trainable=False\n        )\n        self.embedder = BasicTextFieldEmbedder({""words"": token_embedding})\n\n    def forward(  # type: ignore\n        self, list_tensor: Dict[str, torch.LongTensor]\n    ) -> Dict[str, torch.Tensor]:\n        self.embedder(list_tensor)\n        return {""loss"": 1.0}\n\n\nclass TestListField(AllenNlpTestCase):\n    def setup_method(self):\n        self.vocab = Vocabulary()\n        self.vocab.add_token_to_namespace(""this"", ""words"")\n        self.vocab.add_token_to_namespace(""is"", ""words"")\n        self.vocab.add_token_to_namespace(""a"", ""words"")\n        self.vocab.add_token_to_namespace(""sentence"", ""words"")\n        self.vocab.add_token_to_namespace(""s"", ""characters"")\n        self.vocab.add_token_to_namespace(""e"", ""characters"")\n        self.vocab.add_token_to_namespace(""n"", ""characters"")\n        self.vocab.add_token_to_namespace(""t"", ""characters"")\n        self.vocab.add_token_to_namespace(""c"", ""characters"")\n        for label in [""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", ""h"", ""i"", ""j"", ""k""]:\n            self.vocab.add_token_to_namespace(label, ""labels"")\n\n        self.word_indexer = {""words"": SingleIdTokenIndexer(""words"")}\n        self.words_and_characters_indexers = {\n            ""words"": SingleIdTokenIndexer(""words""),\n            ""characters"": TokenCharactersIndexer(""characters"", min_padding_length=1),\n        }\n        self.field1 = TextField(\n            [Token(t) for t in [""this"", ""is"", ""a"", ""sentence""]], self.word_indexer\n        )\n        self.field2 = TextField(\n            [Token(t) for t in [""this"", ""is"", ""a"", ""different"", ""sentence""]], self.word_indexer\n        )\n        self.field3 = TextField(\n            [Token(t) for t in [""this"", ""is"", ""another"", ""sentence""]], self.word_indexer\n        )\n\n        self.empty_text_field = self.field1.empty_field()\n        self.index_field = IndexField(1, self.field1)\n        self.empty_index_field = self.index_field.empty_field()\n        self.sequence_label_field = SequenceLabelField([1, 1, 0, 1], self.field1)\n        self.empty_sequence_label_field = self.sequence_label_field.empty_field()\n\n        tokenizer = SpacyTokenizer()\n        tokens = tokenizer.tokenize(""Foo"")\n        text_field = TextField(tokens, self.word_indexer)\n        empty_list_field = ListField([text_field.empty_field()])\n        empty_fields = {""list_tensor"": empty_list_field}\n        self.empty_instance = Instance(empty_fields)\n\n        non_empty_list_field = ListField([text_field])\n        non_empty_fields = {""list_tensor"": non_empty_list_field}\n        self.non_empty_instance = Instance(non_empty_fields)\n\n        super().setup_method()\n\n    def test_get_padding_lengths(self):\n        list_field = ListField([self.field1, self.field2, self.field3])\n        list_field.index(self.vocab)\n        lengths = list_field.get_padding_lengths()\n        assert lengths == {""num_fields"": 3, ""list_words___tokens"": 5}\n\n    def test_list_field_can_handle_empty_text_fields(self):\n        list_field = ListField([self.field1, self.field2, self.empty_text_field])\n        list_field.index(self.vocab)\n        tensor_dict = list_field.as_tensor(list_field.get_padding_lengths())\n        numpy.testing.assert_array_equal(\n            tensor_dict[""words""][""tokens""].detach().cpu().numpy(),\n            numpy.array([[2, 3, 4, 5, 0], [2, 3, 4, 1, 5], [0, 0, 0, 0, 0]]),\n        )\n\n    def test_list_field_can_handle_empty_index_fields(self):\n        list_field = ListField([self.index_field, self.index_field, self.empty_index_field])\n        list_field.index(self.vocab)\n        tensor = list_field.as_tensor(list_field.get_padding_lengths())\n        numpy.testing.assert_array_equal(\n            tensor.detach().cpu().numpy(), numpy.array([[1], [1], [-1]])\n        )\n\n    def test_list_field_can_handle_empty_sequence_label_fields(self):\n        list_field = ListField(\n            [self.sequence_label_field, self.sequence_label_field, self.empty_sequence_label_field]\n        )\n        list_field.index(self.vocab)\n        tensor = list_field.as_tensor(list_field.get_padding_lengths())\n        numpy.testing.assert_array_equal(\n            tensor.detach().cpu().numpy(), numpy.array([[1, 1, 0, 1], [1, 1, 0, 1], [0, 0, 0, 0]])\n        )\n\n    def test_all_fields_padded_to_max_length(self):\n        list_field = ListField([self.field1, self.field2, self.field3])\n        list_field.index(self.vocab)\n        tensor_dict = list_field.as_tensor(list_field.get_padding_lengths())\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""words""][""tokens""][0].detach().cpu().numpy(), numpy.array([2, 3, 4, 5, 0])\n        )\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""words""][""tokens""][1].detach().cpu().numpy(), numpy.array([2, 3, 4, 1, 5])\n        )\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""words""][""tokens""][2].detach().cpu().numpy(), numpy.array([2, 3, 1, 5, 0])\n        )\n\n    def test_nested_list_fields_are_padded_correctly(self):\n        nested_field1 = ListField([LabelField(c) for c in [""a"", ""b"", ""c"", ""d"", ""e""]])\n        nested_field2 = ListField([LabelField(c) for c in [""f"", ""g"", ""h"", ""i"", ""j"", ""k""]])\n        list_field = ListField([nested_field1.empty_field(), nested_field1, nested_field2])\n        list_field.index(self.vocab)\n        padding_lengths = list_field.get_padding_lengths()\n        assert padding_lengths == {""num_fields"": 3, ""list_num_fields"": 6}\n        tensor = list_field.as_tensor(padding_lengths).detach().cpu().numpy()\n        numpy.testing.assert_almost_equal(\n            tensor, [[-1, -1, -1, -1, -1, -1], [0, 1, 2, 3, 4, -1], [5, 6, 7, 8, 9, 10]]\n        )\n\n    def test_fields_can_pad_to_greater_than_max_length(self):\n        list_field = ListField([self.field1, self.field2, self.field3])\n        list_field.index(self.vocab)\n        padding_lengths = list_field.get_padding_lengths()\n        padding_lengths[""list_words___tokens""] = 7\n        padding_lengths[""num_fields""] = 5\n        tensor_dict = list_field.as_tensor(padding_lengths)\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""words""][""tokens""][0].detach().cpu().numpy(),\n            numpy.array([2, 3, 4, 5, 0, 0, 0]),\n        )\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""words""][""tokens""][1].detach().cpu().numpy(),\n            numpy.array([2, 3, 4, 1, 5, 0, 0]),\n        )\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""words""][""tokens""][2].detach().cpu().numpy(),\n            numpy.array([2, 3, 1, 5, 0, 0, 0]),\n        )\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""words""][""tokens""][3].detach().cpu().numpy(),\n            numpy.array([0, 0, 0, 0, 0, 0, 0]),\n        )\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""words""][""tokens""][4].detach().cpu().numpy(),\n            numpy.array([0, 0, 0, 0, 0, 0, 0]),\n        )\n\n    def test_as_tensor_can_handle_multiple_token_indexers(self):\n\n        self.field1._token_indexers = self.words_and_characters_indexers\n        self.field2._token_indexers = self.words_and_characters_indexers\n        self.field3._token_indexers = self.words_and_characters_indexers\n\n        list_field = ListField([self.field1, self.field2, self.field3])\n        list_field.index(self.vocab)\n        padding_lengths = list_field.get_padding_lengths()\n        tensor_dict = list_field.as_tensor(padding_lengths)\n        words = tensor_dict[""words""][""tokens""].detach().cpu().numpy()\n        characters = tensor_dict[""characters""][""token_characters""].detach().cpu().numpy()\n        numpy.testing.assert_array_almost_equal(\n            words, numpy.array([[2, 3, 4, 5, 0], [2, 3, 4, 1, 5], [2, 3, 1, 5, 0]])\n        )\n\n        numpy.testing.assert_array_almost_equal(\n            characters[0],\n            numpy.array(\n                [\n                    [5, 1, 1, 2, 0, 0, 0, 0, 0],\n                    [1, 2, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [2, 3, 4, 5, 3, 4, 6, 3, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                ]\n            ),\n        )\n\n        numpy.testing.assert_array_almost_equal(\n            characters[1],\n            numpy.array(\n                [\n                    [5, 1, 1, 2, 0, 0, 0, 0, 0],\n                    [1, 2, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 1, 1, 1, 3, 1, 3, 4, 5],\n                    [2, 3, 4, 5, 3, 4, 6, 3, 0],\n                ]\n            ),\n        )\n\n        numpy.testing.assert_array_almost_equal(\n            characters[2],\n            numpy.array(\n                [\n                    [5, 1, 1, 2, 0, 0, 0, 0, 0],\n                    [1, 2, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 4, 1, 5, 1, 3, 1, 0, 0],\n                    [2, 3, 4, 5, 3, 4, 6, 3, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                ]\n            ),\n        )\n\n    def test_as_tensor_can_handle_multiple_token_indexers_and_empty_fields(self):\n\n        self.field1._token_indexers = self.words_and_characters_indexers\n        self.field2._token_indexers = self.words_and_characters_indexers\n        self.field3._token_indexers = self.words_and_characters_indexers\n\n        list_field = ListField([self.field1.empty_field(), self.field1, self.field2])\n        list_field.index(self.vocab)\n        padding_lengths = list_field.get_padding_lengths()\n        tensor_dict = list_field.as_tensor(padding_lengths)\n        words = tensor_dict[""words""][""tokens""].detach().cpu().numpy()\n        characters = tensor_dict[""characters""][""token_characters""].detach().cpu().numpy()\n\n        numpy.testing.assert_array_almost_equal(\n            words, numpy.array([[0, 0, 0, 0, 0], [2, 3, 4, 5, 0], [2, 3, 4, 1, 5]])\n        )\n\n        numpy.testing.assert_array_almost_equal(characters[0], numpy.zeros([5, 9]))\n\n        numpy.testing.assert_array_almost_equal(\n            characters[1],\n            numpy.array(\n                [\n                    [5, 1, 1, 2, 0, 0, 0, 0, 0],\n                    [1, 2, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [2, 3, 4, 5, 3, 4, 6, 3, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                ]\n            ),\n        )\n\n        numpy.testing.assert_array_almost_equal(\n            characters[2],\n            numpy.array(\n                [\n                    [5, 1, 1, 2, 0, 0, 0, 0, 0],\n                    [1, 2, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 1, 1, 1, 3, 1, 3, 4, 5],\n                    [2, 3, 4, 5, 3, 4, 6, 3, 0],\n                ]\n            ),\n        )\n\n    def test_printing_doesnt_crash(self):\n        list_field = ListField([self.field1, self.field2])\n        print(list_field)\n\n    def test_sequence_methods(self):\n        list_field = ListField([self.field1, self.field2, self.field3])\n\n        assert len(list_field) == 3\n        assert list_field[1] == self.field2\n        assert [f for f in list_field] == [self.field1, self.field2, self.field3]\n\n    def test_empty_list_can_be_tensorized(self):\n        tokenizer = SpacyTokenizer()\n        tokens = tokenizer.tokenize(""Foo"")\n        text_field = TextField(tokens, self.word_indexer)\n        list_field = ListField([text_field.empty_field()])\n        fields = {\n            ""list"": list_field,\n            ""bar"": TextField(tokenizer.tokenize(""BAR""), self.word_indexer),\n        }\n        instance = Instance(fields)\n        instance.index_fields(self.vocab)\n        instance.as_tensor_dict()\n\n    def test_batch_with_some_empty_lists_works(self):\n        dataset = AllennlpDataset([self.empty_instance, self.non_empty_instance], self.vocab)\n\n        model = DummyModel(self.vocab)\n        model.eval()\n        loader = DataLoader(dataset, batch_size=2)\n        batch = next(iter(loader))\n        model.forward(**batch)\n\n    # This use case may seem a bit peculiar. It\'s intended for situations where\n    # you have sparse inputs that are used as additional features for some\n    # prediction, and they are sparse enough that they can be empty for some\n    # cases. It would be silly to try to handle these as None in your model; it\n    # makes a whole lot more sense to just have a minimally-sized tensor that\n    # gets entirely masked and has no effect on the rest of the model.\n    def test_batch_of_entirely_empty_lists_works(self):\n        dataset = AllennlpDataset([self.empty_instance, self.empty_instance], self.vocab)\n\n        model = DummyModel(self.vocab)\n        model.eval()\n        loader = DataLoader(dataset, batch_size=2)\n        batch = next(iter(loader))\n        model.forward(**batch)\n\n    def test_list_of_text_padding(self):\n        from allennlp.data.token_indexers import PretrainedTransformerIndexer\n        from allennlp.data.tokenizers import Token\n        from allennlp.data.fields import (\n            TextField,\n            ListField,\n        )\n        from allennlp.data import Vocabulary\n\n        word_indexer = {""tokens"": PretrainedTransformerIndexer(""albert-base-v2"")}\n        text_field = TextField(\n            [\n                Token(t, text_id=2, type_id=1)\n                for t in [""\xe2\x96\x81allen"", ""n"", ""lp"", ""\xe2\x96\x81has"", ""\xe2\x96\x81no"", ""\xe2\x96\x81bugs"", "".""]\n            ],\n            word_indexer,\n        )\n        list_field = ListField([text_field])\n\n        vocab = Vocabulary()\n        list_field.index(vocab)\n\n        padding_lengths = {\n            ""list_tokens___mask"": 10,\n            ""list_tokens___token_ids"": 10,\n            ""list_tokens___type_ids"": 10,\n            ""num_fields"": 2,\n        }\n\n        tensors = list_field.as_tensor(padding_lengths)[""tokens""]\n        assert tensors[""mask""].size() == (2, 10)\n        assert tensors[""mask""][0, 0] == True  # noqa: E712\n        assert tensors[""mask""][0, 9] == False  # noqa: E712\n        assert (tensors[""mask""][1, :] == False).all()  # noqa: E712\n\n        assert tensors[""token_ids""].size() == (2, 10)\n        assert tensors[""token_ids""][0, 0] == 2\n        assert tensors[""token_ids""][0, 9] == 0\n        assert (tensors[""token_ids""][1, :] == 0).all()\n\n        assert tensors[""type_ids""].size() == (2, 10)\n        assert tensors[""type_ids""][0, 0] == 1\n        assert tensors[""type_ids""][0, 9] == 0\n        assert (tensors[""type_ids""][1, :] == 0).all()\n'"
tests/data/fields/metadata_field_test.py,0,"b'import pytest\n\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.data.fields import MetadataField\n\n\nclass TestMetadataField(AllenNlpTestCase):\n    def test_mapping_works_with_dict(self):\n        field = MetadataField({""a"": 1, ""b"": [0]})\n\n        assert ""a"" in field\n        assert field[""a""] == 1\n        assert len(field) == 2\n\n        keys = {k for k in field}\n        assert keys == {""a"", ""b""}\n\n        values = [v for v in field.values()]\n        assert len(values) == 2\n        assert 1 in values\n        assert [0] in values\n\n    def test_mapping_raises_with_non_dict(self):\n        field = MetadataField(0)\n\n        with pytest.raises(TypeError):\n            _ = field[0]\n\n        with pytest.raises(TypeError):\n            _ = len(field)\n\n        with pytest.raises(TypeError):\n            _ = [x for x in field]\n'"
tests/data/fields/multilabel_field_test.py,0,"b'import logging\n\nimport numpy\nimport pytest\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.fields import MultiLabelField\nfrom allennlp.data.vocabulary import Vocabulary\n\n\nclass TestMultiLabelField(AllenNlpTestCase):\n    def test_as_tensor_returns_integer_tensor(self):\n        f = MultiLabelField([2, 3], skip_indexing=True, label_namespace=""test1"", num_labels=5)\n        tensor = f.as_tensor(f.get_padding_lengths()).detach().cpu().tolist()\n        assert tensor == [0, 0, 1, 1, 0]\n        assert {type(item) for item in tensor} == {int}\n\n    def test_multilabel_field_can_index_with_vocab(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""rel0"", namespace=""rel_labels"")\n        vocab.add_token_to_namespace(""rel1"", namespace=""rel_labels"")\n        vocab.add_token_to_namespace(""rel2"", namespace=""rel_labels"")\n\n        f = MultiLabelField([""rel1"", ""rel0""], label_namespace=""rel_labels"")\n        f.index(vocab)\n        tensor = f.as_tensor(f.get_padding_lengths()).detach().cpu().numpy()\n        numpy.testing.assert_array_almost_equal(tensor, numpy.array([1, 1, 0]))\n\n    def test_multilabel_field_raises_with_non_integer_labels_and_no_indexing(self):\n        with pytest.raises(ConfigurationError):\n            _ = MultiLabelField([""non integer field""], skip_indexing=True)\n\n    def test_multilabel_field_raises_with_no_indexing_and_missing_num_labels(self):\n        with pytest.raises(ConfigurationError):\n            _ = MultiLabelField([0, 2], skip_indexing=True, num_labels=None)\n\n    def test_multilabel_field_raises_with_no_indexing_and_wrong_num_labels(self):\n        with pytest.raises(ConfigurationError):\n            _ = MultiLabelField([0, 2, 4], skip_indexing=True, num_labels=3)\n\n    def test_multilabel_field_raises_with_incorrect_label_type(self):\n        with pytest.raises(ConfigurationError):\n            _ = MultiLabelField([1, 2], skip_indexing=False)\n\n    def test_multilabel_field_raises_with_given_num_labels(self):\n        with pytest.raises(ConfigurationError):\n            _ = MultiLabelField([1, 2], skip_indexing=False, num_labels=4)\n\n    def test_multilabel_field_empty_field_works(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""label1"", namespace=""test_empty_labels"")\n        vocab.add_token_to_namespace(""label2"", namespace=""test_empty_labels"")\n\n        f = MultiLabelField([], label_namespace=""test_empty_labels"")\n        f.index(vocab)\n        tensor = f.as_tensor(f.get_padding_lengths()).detach().cpu().numpy()\n        numpy.testing.assert_array_almost_equal(tensor, numpy.array([0, 0]))\n        g = f.empty_field()\n        g.index(vocab)\n        tensor = g.as_tensor(g.get_padding_lengths()).detach().cpu().numpy()\n        numpy.testing.assert_array_almost_equal(tensor, numpy.array([0, 0]))\n\n        h = MultiLabelField(\n            [0, 0, 1], label_namespace=""test_empty_labels"", num_labels=3, skip_indexing=True\n        )\n        tensor = h.empty_field().as_tensor(None).detach().cpu().numpy()\n        numpy.testing.assert_array_almost_equal(tensor, numpy.array([0, 0, 0]))\n\n    def test_class_variables_for_namespace_warnings_work_correctly(self, caplog):\n        with caplog.at_level(logging.WARNING, logger=""allennlp.data.fields.multilabel_field""):\n            assert ""text"" not in MultiLabelField._already_warned_namespaces\n            _ = MultiLabelField([""test""], label_namespace=""text"")\n            assert caplog.records\n\n            # We\'ve warned once, so we should have set the class variable to False.\n            assert ""text"" in MultiLabelField._already_warned_namespaces\n            caplog.clear()\n            _ = MultiLabelField([""test2""], label_namespace=""text"")\n            assert not caplog.records\n\n            # ... but a new namespace should still log a warning.\n            assert ""text2"" not in MultiLabelField._already_warned_namespaces\n            caplog.clear()\n            _ = MultiLabelField([""test""], label_namespace=""text2"")\n            assert caplog\n\n    def test_printing_doesnt_crash(self):\n        field = MultiLabelField([""label""], label_namespace=""namespace"")\n        print(field)\n'"
tests/data/fields/sequence_label_field_test.py,0,"b'from collections import defaultdict\nimport logging\n\nimport pytest\nimport numpy\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Token, Vocabulary\nfrom allennlp.data.fields import TextField, SequenceLabelField\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\n\n\nclass TestSequenceLabelField(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.text = TextField(\n            [Token(t) for t in [""here"", ""are"", ""some"", ""words"", "".""]],\n            {""words"": SingleIdTokenIndexer(""words"")},\n        )\n\n    def test_tag_length_mismatch_raises(self):\n        with pytest.raises(ConfigurationError):\n            wrong_tags = [""B"", ""O"", ""O""]\n            _ = SequenceLabelField(wrong_tags, self.text)\n\n    def test_count_vocab_items_correctly_indexes_tags(self):\n        tags = [""B"", ""I"", ""O"", ""O"", ""O""]\n        sequence_label_field = SequenceLabelField(tags, self.text, label_namespace=""labels"")\n\n        counter = defaultdict(lambda: defaultdict(int))\n        sequence_label_field.count_vocab_items(counter)\n\n        assert counter[""labels""][""B""] == 1\n        assert counter[""labels""][""I""] == 1\n        assert counter[""labels""][""O""] == 3\n        assert set(counter.keys()) == {""labels""}\n\n    def test_index_converts_field_correctly(self):\n        vocab = Vocabulary()\n        b_index = vocab.add_token_to_namespace(""B"", namespace=""*labels"")\n        i_index = vocab.add_token_to_namespace(""I"", namespace=""*labels"")\n        o_index = vocab.add_token_to_namespace(""O"", namespace=""*labels"")\n\n        tags = [""B"", ""I"", ""O"", ""O"", ""O""]\n        sequence_label_field = SequenceLabelField(tags, self.text, label_namespace=""*labels"")\n        sequence_label_field.index(vocab)\n\n        assert sequence_label_field._indexed_labels == [b_index, i_index, o_index, o_index, o_index]\n\n    def test_as_tensor_produces_integer_targets(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""B"", namespace=""*labels"")\n        vocab.add_token_to_namespace(""I"", namespace=""*labels"")\n        vocab.add_token_to_namespace(""O"", namespace=""*labels"")\n\n        tags = [""B"", ""I"", ""O"", ""O"", ""O""]\n        sequence_label_field = SequenceLabelField(tags, self.text, label_namespace=""*labels"")\n        sequence_label_field.index(vocab)\n        padding_lengths = sequence_label_field.get_padding_lengths()\n        tensor = sequence_label_field.as_tensor(padding_lengths).detach().cpu().numpy()\n        numpy.testing.assert_array_almost_equal(tensor, numpy.array([0, 1, 2, 2, 2]))\n\n    def test_sequence_label_field_raises_on_incorrect_type(self):\n\n        with pytest.raises(ConfigurationError):\n            _ = SequenceLabelField([[], [], [], [], []], self.text)\n\n    def test_class_variables_for_namespace_warnings_work_correctly(self, caplog):\n        with caplog.at_level(logging.WARNING, logger=""allennlp.data.fields.sequence_label_field""):\n            tags = [""B"", ""I"", ""O"", ""O"", ""O""]\n            assert ""text"" not in SequenceLabelField._already_warned_namespaces\n\n            _ = SequenceLabelField(tags, self.text, label_namespace=""text"")\n            assert caplog.records\n\n            # We\'ve warned once, so we should have set the class variable to False.\n            assert ""text"" in SequenceLabelField._already_warned_namespaces\n            caplog.clear()\n            _ = SequenceLabelField(tags, self.text, label_namespace=""text"")\n            assert not caplog.records\n\n            # ... but a new namespace should still log a warning.\n            assert ""text2"" not in SequenceLabelField._already_warned_namespaces\n            caplog.clear()\n            _ = SequenceLabelField(tags, self.text, label_namespace=""text2"")\n            assert caplog.records\n\n    def test_printing_doesnt_crash(self):\n        tags = [""B"", ""I"", ""O"", ""O"", ""O""]\n        sequence_label_field = SequenceLabelField(tags, self.text, label_namespace=""labels"")\n        print(sequence_label_field)\n\n    def test_sequence_methods(self):\n        tags = [""B"", ""I"", ""O"", ""O"", ""O""]\n        sequence_label_field = SequenceLabelField(tags, self.text, label_namespace=""labels"")\n\n        assert len(sequence_label_field) == 5\n        assert sequence_label_field[1] == ""I""\n        assert [label for label in sequence_label_field] == tags\n'"
tests/data/fields/span_field_test.py,0,"b'import numpy\nimport pytest\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Token\nfrom allennlp.data.fields import TextField, SpanField\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\n\n\nclass TestSpanField(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.indexers = {""words"": SingleIdTokenIndexer(""words"")}\n        self.text = TextField(\n            [Token(t) for t in [""here"", ""is"", ""a"", ""sentence"", ""for"", ""spans"", "".""]], self.indexers\n        )\n\n    def test_as_tensor_converts_span_field_correctly(self):\n        span_field = SpanField(2, 3, self.text)\n        tensor = span_field.as_tensor(span_field.get_padding_lengths()).detach().cpu().numpy()\n        numpy.testing.assert_array_equal(tensor, numpy.array([2, 3]))\n\n    def test_span_field_raises_on_incorrect_label_type(self):\n        with pytest.raises(TypeError):\n            _ = SpanField(""hello"", 3, self.text)\n\n    def test_span_field_raises_on_ill_defined_span(self):\n        with pytest.raises(ValueError):\n            _ = SpanField(4, 1, self.text)\n\n    def test_span_field_raises_if_span_end_is_greater_than_sentence_length(self):\n        with pytest.raises(ValueError):\n            _ = SpanField(1, 30, self.text)\n\n    def test_empty_span_field_works(self):\n        span_field = SpanField(1, 3, self.text)\n        empty_span = span_field.empty_field()\n        assert empty_span.span_start == -1\n        assert empty_span.span_end == -1\n\n    def test_printing_doesnt_crash(self):\n        span_field = SpanField(2, 3, self.text)\n        print(span_field)\n\n    def test_equality(self):\n        span_field1 = SpanField(2, 3, self.text)\n        span_field2 = SpanField(2, 3, self.text)\n        span_field3 = SpanField(\n            2, 3, TextField([Token(t) for t in [""not"", ""the"", ""same"", ""tokens""]], self.indexers)\n        )\n\n        assert span_field1 == (2, 3)\n        assert span_field1 == span_field1\n        assert span_field1 == span_field2\n        assert span_field1 != span_field3\n        assert span_field2 != span_field3\n'"
tests/data/fields/text_field_test.py,0,"b'from collections import defaultdict\nfrom typing import Dict, List\n\nimport numpy\nimport pytest\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Token, Vocabulary\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer, TokenCharactersIndexer, TokenIndexer\n\n\nclass DictReturningTokenIndexer(TokenIndexer):\n    """"""\n    A stub TokenIndexer that returns multiple arrays of different lengths.\n    """"""\n\n    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):\n        pass\n\n    def tokens_to_indices(\n        self, tokens: List[Token], vocabulary: Vocabulary\n    ) -> Dict[str, List[int]]:\n        return {\n            ""token_ids"": (\n                [10, 15]\n                + [vocabulary.get_token_index(token.text, ""words"") for token in tokens]\n                + [25]\n            ),\n            ""additional_key"": [22, 29],\n        }\n\n\nclass TestTextField(AllenNlpTestCase):\n    def setup_method(self):\n        self.vocab = Vocabulary()\n        self.vocab.add_token_to_namespace(""sentence"", namespace=""words"")\n        self.vocab.add_token_to_namespace(""A"", namespace=""words"")\n        self.vocab.add_token_to_namespace(""A"", namespace=""characters"")\n        self.vocab.add_token_to_namespace(""s"", namespace=""characters"")\n        self.vocab.add_token_to_namespace(""e"", namespace=""characters"")\n        self.vocab.add_token_to_namespace(""n"", namespace=""characters"")\n        self.vocab.add_token_to_namespace(""t"", namespace=""characters"")\n        self.vocab.add_token_to_namespace(""c"", namespace=""characters"")\n        super().setup_method()\n\n    def test_field_counts_vocab_items_correctly(self):\n        field = TextField(\n            [Token(t) for t in [""This"", ""is"", ""a"", ""sentence"", "".""]],\n            token_indexers={""words"": SingleIdTokenIndexer(""words"")},\n        )\n        namespace_token_counts = defaultdict(lambda: defaultdict(int))\n        field.count_vocab_items(namespace_token_counts)\n\n        assert namespace_token_counts[""words""][""This""] == 1\n        assert namespace_token_counts[""words""][""is""] == 1\n        assert namespace_token_counts[""words""][""a""] == 1\n        assert namespace_token_counts[""words""][""sentence""] == 1\n        assert namespace_token_counts[""words""]["".""] == 1\n        assert list(namespace_token_counts.keys()) == [""words""]\n\n        field = TextField(\n            [Token(t) for t in [""This"", ""is"", ""a"", ""sentence"", "".""]],\n            token_indexers={\n                ""characters"": TokenCharactersIndexer(""characters"", min_padding_length=1)\n            },\n        )\n        namespace_token_counts = defaultdict(lambda: defaultdict(int))\n        field.count_vocab_items(namespace_token_counts)\n\n        assert namespace_token_counts[""characters""][""T""] == 1\n        assert namespace_token_counts[""characters""][""h""] == 1\n        assert namespace_token_counts[""characters""][""i""] == 2\n        assert namespace_token_counts[""characters""][""s""] == 3\n        assert namespace_token_counts[""characters""][""a""] == 1\n        assert namespace_token_counts[""characters""][""e""] == 3\n        assert namespace_token_counts[""characters""][""n""] == 2\n        assert namespace_token_counts[""characters""][""t""] == 1\n        assert namespace_token_counts[""characters""][""c""] == 1\n        assert namespace_token_counts[""characters""]["".""] == 1\n        assert list(namespace_token_counts.keys()) == [""characters""]\n\n        field = TextField(\n            [Token(t) for t in [""This"", ""is"", ""a"", ""sentence"", "".""]],\n            token_indexers={\n                ""words"": SingleIdTokenIndexer(""words""),\n                ""characters"": TokenCharactersIndexer(""characters"", min_padding_length=1),\n            },\n        )\n        namespace_token_counts = defaultdict(lambda: defaultdict(int))\n        field.count_vocab_items(namespace_token_counts)\n        assert namespace_token_counts[""characters""][""T""] == 1\n        assert namespace_token_counts[""characters""][""h""] == 1\n        assert namespace_token_counts[""characters""][""i""] == 2\n        assert namespace_token_counts[""characters""][""s""] == 3\n        assert namespace_token_counts[""characters""][""a""] == 1\n        assert namespace_token_counts[""characters""][""e""] == 3\n        assert namespace_token_counts[""characters""][""n""] == 2\n        assert namespace_token_counts[""characters""][""t""] == 1\n        assert namespace_token_counts[""characters""][""c""] == 1\n        assert namespace_token_counts[""characters""]["".""] == 1\n        assert namespace_token_counts[""words""][""This""] == 1\n        assert namespace_token_counts[""words""][""is""] == 1\n        assert namespace_token_counts[""words""][""a""] == 1\n        assert namespace_token_counts[""words""][""sentence""] == 1\n        assert namespace_token_counts[""words""]["".""] == 1\n        assert set(namespace_token_counts.keys()) == {""words"", ""characters""}\n\n    def test_index_converts_field_correctly(self):\n        vocab = Vocabulary()\n        sentence_index = vocab.add_token_to_namespace(""sentence"", namespace=""words"")\n        capital_a_index = vocab.add_token_to_namespace(""A"", namespace=""words"")\n        capital_a_char_index = vocab.add_token_to_namespace(""A"", namespace=""characters"")\n        s_index = vocab.add_token_to_namespace(""s"", namespace=""characters"")\n        e_index = vocab.add_token_to_namespace(""e"", namespace=""characters"")\n        n_index = vocab.add_token_to_namespace(""n"", namespace=""characters"")\n        t_index = vocab.add_token_to_namespace(""t"", namespace=""characters"")\n        c_index = vocab.add_token_to_namespace(""c"", namespace=""characters"")\n\n        field = TextField(\n            [Token(t) for t in [""A"", ""sentence""]],\n            {""words"": SingleIdTokenIndexer(namespace=""words"")},\n        )\n        field.index(vocab)\n\n        assert field._indexed_tokens[""words""][""tokens""] == [capital_a_index, sentence_index]\n\n        field1 = TextField(\n            [Token(t) for t in [""A"", ""sentence""]],\n            {""characters"": TokenCharactersIndexer(namespace=""characters"", min_padding_length=1)},\n        )\n        field1.index(vocab)\n        assert field1._indexed_tokens[""characters""][""token_characters""] == [\n            [capital_a_char_index],\n            [s_index, e_index, n_index, t_index, e_index, n_index, c_index, e_index],\n        ]\n        field2 = TextField(\n            [Token(t) for t in [""A"", ""sentence""]],\n            token_indexers={\n                ""words"": SingleIdTokenIndexer(namespace=""words""),\n                ""characters"": TokenCharactersIndexer(namespace=""characters"", min_padding_length=1),\n            },\n        )\n        field2.index(vocab)\n        assert field2._indexed_tokens[""words""][""tokens""] == [capital_a_index, sentence_index]\n        assert field2._indexed_tokens[""characters""][""token_characters""] == [\n            [capital_a_char_index],\n            [s_index, e_index, n_index, t_index, e_index, n_index, c_index, e_index],\n        ]\n\n    def test_get_padding_lengths_raises_if_no_indexed_tokens(self):\n\n        field = TextField(\n            [Token(t) for t in [""This"", ""is"", ""a"", ""sentence"", "".""]],\n            token_indexers={""words"": SingleIdTokenIndexer(""words"")},\n        )\n        with pytest.raises(ConfigurationError):\n            field.get_padding_lengths()\n\n    def test_padding_lengths_are_computed_correctly(self):\n        field = TextField(\n            [Token(t) for t in [""This"", ""is"", ""a"", ""sentence"", "".""]],\n            token_indexers={""words"": SingleIdTokenIndexer(""words"")},\n        )\n        field.index(self.vocab)\n        padding_lengths = field.get_padding_lengths()\n        assert padding_lengths == {""words___tokens"": 5}\n\n        field = TextField(\n            [Token(t) for t in [""This"", ""is"", ""a"", ""sentence"", "".""]],\n            token_indexers={\n                ""characters"": TokenCharactersIndexer(""characters"", min_padding_length=1)\n            },\n        )\n        field.index(self.vocab)\n        padding_lengths = field.get_padding_lengths()\n        assert padding_lengths == {\n            ""characters___token_characters"": 5,\n            ""characters___num_token_characters"": 8,\n        }\n\n        field = TextField(\n            [Token(t) for t in [""This"", ""is"", ""a"", ""sentence"", "".""]],\n            token_indexers={\n                ""characters"": TokenCharactersIndexer(""characters"", min_padding_length=1),\n                ""words"": SingleIdTokenIndexer(""words""),\n            },\n        )\n        field.index(self.vocab)\n        padding_lengths = field.get_padding_lengths()\n        assert padding_lengths == {\n            ""characters___token_characters"": 5,\n            ""characters___num_token_characters"": 8,\n            ""words___tokens"": 5,\n        }\n\n    def test_as_tensor_handles_words(self):\n        field = TextField(\n            [Token(t) for t in [""This"", ""is"", ""a"", ""sentence"", "".""]],\n            token_indexers={""words"": SingleIdTokenIndexer(""words"")},\n        )\n        field.index(self.vocab)\n        padding_lengths = field.get_padding_lengths()\n        tensor_dict = field.as_tensor(padding_lengths)\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""words""][""tokens""].detach().cpu().numpy(), numpy.array([1, 1, 1, 2, 1])\n        )\n\n    def test_as_tensor_handles_longer_lengths(self):\n        field = TextField(\n            [Token(t) for t in [""This"", ""is"", ""a"", ""sentence"", "".""]],\n            token_indexers={""words"": SingleIdTokenIndexer(""words"")},\n        )\n        field.index(self.vocab)\n        padding_lengths = field.get_padding_lengths()\n        padding_lengths[""words___tokens""] = 10\n        tensor_dict = field.as_tensor(padding_lengths)\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""words""][""tokens""].detach().cpu().numpy(),\n            numpy.array([1, 1, 1, 2, 1, 0, 0, 0, 0, 0]),\n        )\n\n    def test_as_tensor_handles_characters(self):\n        field = TextField(\n            [Token(t) for t in [""This"", ""is"", ""a"", ""sentence"", "".""]],\n            token_indexers={\n                ""characters"": TokenCharactersIndexer(""characters"", min_padding_length=1)\n            },\n        )\n        field.index(self.vocab)\n        padding_lengths = field.get_padding_lengths()\n        tensor_dict = field.as_tensor(padding_lengths)\n        expected_character_array = numpy.array(\n            [\n                [1, 1, 1, 3, 0, 0, 0, 0],\n                [1, 3, 0, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0],\n                [3, 4, 5, 6, 4, 5, 7, 4],\n                [1, 0, 0, 0, 0, 0, 0, 0],\n            ]\n        )\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""characters""][""token_characters""].detach().cpu().numpy(),\n            expected_character_array,\n        )\n\n    def test_as_tensor_handles_characters_if_empty_field(self):\n        field = TextField(\n            [],\n            token_indexers={\n                ""characters"": TokenCharactersIndexer(""characters"", min_padding_length=1)\n            },\n        )\n        field.index(self.vocab)\n        padding_lengths = field.get_padding_lengths()\n        tensor_dict = field.as_tensor(padding_lengths)\n        expected_character_array = numpy.array([])\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""characters""][""token_characters""].detach().cpu().numpy(),\n            expected_character_array,\n        )\n\n    def test_as_tensor_handles_words_and_characters_with_longer_lengths(self):\n        field = TextField(\n            [Token(t) for t in [""a"", ""sentence"", "".""]],\n            token_indexers={\n                ""words"": SingleIdTokenIndexer(""words""),\n                ""characters"": TokenCharactersIndexer(""characters"", min_padding_length=1),\n            },\n        )\n        field.index(self.vocab)\n        padding_lengths = field.get_padding_lengths()\n        padding_lengths[""words___tokens""] = 5\n        padding_lengths[""characters___token_characters""] = 5\n        padding_lengths[""characters___num_token_characters""] = 10\n        tensor_dict = field.as_tensor(padding_lengths)\n\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""words""][""tokens""].detach().cpu().numpy(), numpy.array([1, 2, 1, 0, 0])\n        )\n        numpy.testing.assert_array_almost_equal(\n            tensor_dict[""characters""][""token_characters""].detach().cpu().numpy(),\n            numpy.array(\n                [\n                    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [3, 4, 5, 6, 4, 5, 7, 4, 0, 0],\n                    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                ]\n            ),\n        )\n\n    def test_printing_doesnt_crash(self):\n        field = TextField(\n            [Token(t) for t in [""A"", ""sentence""]],\n            {""words"": SingleIdTokenIndexer(namespace=""words"")},\n        )\n        print(field)\n\n    def test_token_indexer_returns_dict(self):\n        field = TextField(\n            [Token(t) for t in [""A"", ""sentence""]],\n            token_indexers={\n                ""field_with_dict"": DictReturningTokenIndexer(),\n                ""words"": SingleIdTokenIndexer(""words""),\n                ""characters"": TokenCharactersIndexer(""characters"", min_padding_length=1),\n            },\n        )\n        field.index(self.vocab)\n        padding_lengths = field.get_padding_lengths()\n        assert padding_lengths == {\n            ""field_with_dict___token_ids"": 5,\n            ""field_with_dict___additional_key"": 2,\n            ""words___tokens"": 2,\n            ""characters___token_characters"": 2,\n            ""characters___num_token_characters"": 8,\n        }\n        padding_lengths[""field_with_dict___token_ids""] = 7\n        padding_lengths[""field_with_dict___additional_key""] = 3\n        padding_lengths[""words___tokens""] = 4\n        padding_lengths[""characters___token_characters""] = 4\n        tensors = field.as_tensor(padding_lengths)\n        assert list(tensors[""field_with_dict""][""token_ids""].shape) == [7]\n        assert list(tensors[""field_with_dict""][""additional_key""].shape) == [3]\n        assert list(tensors[""words""][""tokens""].shape) == [4]\n        assert list(tensors[""characters""][""token_characters""].shape) == [4, 8]\n\n    def test_token_padding_lengths_are_computed_correctly(self):\n        field = TextField(\n            [Token(t) for t in [""A"", ""sentence""]],\n            token_indexers={\n                ""field_with_dict"": DictReturningTokenIndexer(token_min_padding_length=3),\n                ""words"": SingleIdTokenIndexer(""words"", token_min_padding_length=3),\n                ""characters"": TokenCharactersIndexer(\n                    ""characters"", min_padding_length=1, token_min_padding_length=3\n                ),\n            },\n        )\n        field.index(self.vocab)\n        padding_lengths = field.get_padding_lengths()\n        assert padding_lengths == {\n            ""field_with_dict___token_ids"": 5,\n            ""field_with_dict___additional_key"": 3,\n            ""words___tokens"": 3,\n            ""characters___token_characters"": 3,\n            ""characters___num_token_characters"": 8,\n        }\n        tensors = field.as_tensor(padding_lengths)\n        assert tensors[""field_with_dict""][""additional_key""].tolist()[-1] == 0\n        assert tensors[""words""][""tokens""].tolist()[-1] == 0\n        assert tensors[""characters""][""token_characters""].tolist()[-1] == [0] * 8\n\n    def test_sequence_methods(self):\n        field = TextField([Token(t) for t in [""This"", ""is"", ""a"", ""sentence"", "".""]], {})\n\n        assert len(field) == 5\n        assert field[1].text == ""is""\n        assert [token.text for token in field] == [""This"", ""is"", ""a"", ""sentence"", "".""]\n'"
tests/data/samplers/__init__.py,0,b''
tests/data/samplers/bucket_batch_sampler_test.py,0,"b'from allennlp.common import Params\nfrom allennlp.data import Instance, Token\nfrom allennlp.data.batch import Batch\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.dataset_readers.dataset_reader import AllennlpDataset\nfrom allennlp.data.dataloader import DataLoader\n\nfrom .sampler_test import SamplerTest\n\n\nclass TestBucketSampler(SamplerTest):\n    def test_create_batches_groups_correctly(self):\n        dataset = AllennlpDataset(self.instances, vocab=self.vocab)\n        sampler = BucketBatchSampler(dataset, batch_size=2, padding_noise=0, sorting_keys=[""text""])\n\n        grouped_instances = []\n        for indices in sampler:\n            grouped_instances.append([self.instances[idx] for idx in indices])\n        expected_groups = [\n            [self.instances[4], self.instances[2]],\n            [self.instances[0], self.instances[1]],\n            [self.instances[3]],\n        ]\n        for group in grouped_instances:\n            assert group in expected_groups\n            expected_groups.remove(group)\n        assert expected_groups == []\n\n    def test_guess_sorting_key_picks_the_longest_key(self):\n        dataset = AllennlpDataset(self.instances, vocab=self.vocab)\n        sampler = BucketBatchSampler(dataset, batch_size=2, padding_noise=0)\n        instances = []\n        short_tokens = [Token(t) for t in [""what"", ""is"", ""this"", ""?""]]\n        long_tokens = [Token(t) for t in [""this"", ""is"", ""a"", ""not"", ""very"", ""long"", ""passage""]]\n        instances.append(\n            Instance(\n                {\n                    ""question"": TextField(short_tokens, self.token_indexers),\n                    ""passage"": TextField(long_tokens, self.token_indexers),\n                }\n            )\n        )\n        instances.append(\n            Instance(\n                {\n                    ""question"": TextField(short_tokens, self.token_indexers),\n                    ""passage"": TextField(long_tokens, self.token_indexers),\n                }\n            )\n        )\n        instances.append(\n            Instance(\n                {\n                    ""question"": TextField(short_tokens, self.token_indexers),\n                    ""passage"": TextField(long_tokens, self.token_indexers),\n                }\n            )\n        )\n        assert sampler.sorting_keys is None\n        sampler._guess_sorting_keys(instances)\n        assert sampler.sorting_keys == [""passage""]\n\n    def test_from_params(self):\n        dataset = AllennlpDataset(self.instances, self.vocab)\n        params = Params({})\n\n        sorting_keys = [""s1"", ""s2""]\n        params[""sorting_keys""] = sorting_keys\n        params[""batch_size""] = 32\n        sampler = BucketBatchSampler.from_params(params=params, data_source=dataset)\n\n        assert sampler.sorting_keys == sorting_keys\n        assert sampler.padding_noise == 0.1\n        assert sampler.batch_size == 32\n\n        params = Params(\n            {\n                ""sorting_keys"": sorting_keys,\n                ""padding_noise"": 0.5,\n                ""batch_size"": 100,\n                ""drop_last"": True,\n            }\n        )\n\n        sampler = BucketBatchSampler.from_params(params=params, data_source=dataset)\n        assert sampler.sorting_keys == sorting_keys\n        assert sampler.padding_noise == 0.5\n        assert sampler.batch_size == 100\n        assert sampler.drop_last\n\n    def test_drop_last_works(self):\n        dataset = AllennlpDataset(self.instances, vocab=self.vocab)\n        sampler = BucketBatchSampler(\n            dataset, batch_size=2, padding_noise=0, sorting_keys=[""text""], drop_last=True,\n        )\n        # We use a custom collate_fn for testing, which doesn\'t actually create tensors,\n        # just the allennlp Batches.\n        dataloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=lambda x: Batch(x))\n        batches = [batch for batch in iter(dataloader)]\n        stats = self.get_batches_stats(batches)\n\n        # all batches have length batch_size\n        assert all(batch_len == 2 for batch_len in stats[""batch_lengths""])\n\n        # we should have lost one instance by skipping the last batch\n        assert stats[""total_instances""] == len(self.instances) - 1\n\n    def test_batch_count(self):\n        dataset = AllennlpDataset(self.instances, vocab=self.vocab)\n        sampler = BucketBatchSampler(dataset, batch_size=2, padding_noise=0, sorting_keys=[""text""])\n        # We use a custom collate_fn for testing, which doesn\'t actually create tensors,\n        # just the allennlp Batches.\n        dataloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=lambda x: Batch(x))\n\n        assert len(dataloader) == 3\n\n    def test_batch_count_with_drop_last(self):\n        dataset = AllennlpDataset(self.instances, vocab=self.vocab)\n        sampler = BucketBatchSampler(\n            dataset, batch_size=2, padding_noise=0, sorting_keys=[""text""], drop_last=True,\n        )\n        # We use a custom collate_fn for testing, which doesn\'t actually create tensors,\n        # just the allennlp Batches.\n        dataloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=lambda x: Batch(x))\n\n        assert len(dataloader) == 2\n'"
tests/data/samplers/max_tokens_batch_sampler_test.py,0,"b'from allennlp.common import Params\nfrom allennlp.data import Instance, Token\nfrom allennlp.data.batch import Batch\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.samplers import MaxTokensBatchSampler\nfrom allennlp.data.dataset_readers.dataset_reader import AllennlpDataset\nfrom allennlp.data.dataloader import DataLoader\n\nfrom .sampler_test import SamplerTest\n\n\nclass TestMaxTokensSampler(SamplerTest):\n    def test_create_batches_groups_correctly(self):\n        dataset = AllennlpDataset(self.instances, vocab=self.vocab)\n        sampler = MaxTokensBatchSampler(\n            dataset, max_tokens=8, padding_noise=0, sorting_keys=[""text""]\n        )\n\n        grouped_instances = []\n        for indices in sampler:\n            grouped_instances.append([self.instances[idx] for idx in indices])\n        expected_groups = [\n            [self.instances[4], self.instances[2]],\n            [self.instances[0], self.instances[1]],\n            [self.instances[3]],\n        ]\n        for group in grouped_instances:\n            assert group in expected_groups\n            expected_groups.remove(group)\n        assert expected_groups == []\n\n    def test_guess_sorting_key_picks_the_longest_key(self):\n        dataset = AllennlpDataset(self.instances, vocab=self.vocab)\n        sampler = MaxTokensBatchSampler(dataset, max_tokens=8, padding_noise=0)\n        instances = []\n        short_tokens = [Token(t) for t in [""what"", ""is"", ""this"", ""?""]]\n        long_tokens = [Token(t) for t in [""this"", ""is"", ""a"", ""not"", ""very"", ""long"", ""passage""]]\n        instances.append(\n            Instance(\n                {\n                    ""question"": TextField(short_tokens, self.token_indexers),\n                    ""passage"": TextField(long_tokens, self.token_indexers),\n                }\n            )\n        )\n        instances.append(\n            Instance(\n                {\n                    ""question"": TextField(short_tokens, self.token_indexers),\n                    ""passage"": TextField(long_tokens, self.token_indexers),\n                }\n            )\n        )\n        instances.append(\n            Instance(\n                {\n                    ""question"": TextField(short_tokens, self.token_indexers),\n                    ""passage"": TextField(long_tokens, self.token_indexers),\n                }\n            )\n        )\n        assert sampler.sorting_keys is None\n        sampler._guess_sorting_keys(instances)\n        assert sampler.sorting_keys == [""passage""]\n\n    def test_from_params(self):\n        dataset = AllennlpDataset(self.instances, self.vocab)\n        params = Params({})\n\n        sorting_keys = [""s1"", ""s2""]\n        params[""sorting_keys""] = sorting_keys\n        params[""max_tokens""] = 32\n        sampler = MaxTokensBatchSampler.from_params(params=params, data_source=dataset)\n\n        assert sampler.sorting_keys == sorting_keys\n        assert sampler.padding_noise == 0.1\n        assert sampler.max_tokens == 32\n\n        params = Params({""sorting_keys"": sorting_keys, ""padding_noise"": 0.5, ""max_tokens"": 100})\n\n        sampler = MaxTokensBatchSampler.from_params(params=params, data_source=dataset)\n        assert sampler.sorting_keys == sorting_keys\n        assert sampler.padding_noise == 0.5\n        assert sampler.max_tokens == 100\n\n    def test_batch_count(self):\n        dataset = AllennlpDataset(self.instances, vocab=self.vocab)\n        sampler = MaxTokensBatchSampler(\n            dataset, max_tokens=8, padding_noise=0, sorting_keys=[""text""]\n        )\n        # We use a custom collate_fn for testing, which doesn\'t actually create tensors,\n        # just the allennlp Batches.\n        dataloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=lambda x: Batch(x))\n\n        assert len(dataloader) == 3\n'"
tests/data/samplers/sampler_test.py,0,"b'from typing import List, Iterable, Dict, Union\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Vocabulary, Instance, Token, Batch\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\n\n\nclass LazyIterable:\n    def __init__(self, instances):\n        self._instances = instances\n\n    def __iter__(self):\n        return (instance for instance in self._instances)\n\n\nclass SamplerTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.token_indexers = {""tokens"": SingleIdTokenIndexer()}\n        self.vocab = Vocabulary()\n        self.this_index = self.vocab.add_token_to_namespace(""this"")\n        self.is_index = self.vocab.add_token_to_namespace(""is"")\n        self.a_index = self.vocab.add_token_to_namespace(""a"")\n        self.sentence_index = self.vocab.add_token_to_namespace(""sentence"")\n        self.another_index = self.vocab.add_token_to_namespace(""another"")\n        self.yet_index = self.vocab.add_token_to_namespace(""yet"")\n        self.very_index = self.vocab.add_token_to_namespace(""very"")\n        self.long_index = self.vocab.add_token_to_namespace(""long"")\n        instances = [\n            self.create_instance([""this"", ""is"", ""a"", ""sentence""]),\n            self.create_instance([""this"", ""is"", ""another"", ""sentence""]),\n            self.create_instance([""yet"", ""another"", ""sentence""]),\n            self.create_instance(\n                [""this"", ""is"", ""a"", ""very"", ""very"", ""very"", ""very"", ""long"", ""sentence""]\n            ),\n            self.create_instance([""sentence""]),\n        ]\n\n        self.instances = instances\n        self.lazy_instances = LazyIterable(instances)\n\n    def create_instance(self, str_tokens: List[str]):\n        tokens = [Token(t) for t in str_tokens]\n        instance = Instance({""text"": TextField(tokens, self.token_indexers)})\n        return instance\n\n    def create_instances_from_token_counts(self, token_counts: List[int]) -> List[Instance]:\n        return [self.create_instance([""word""] * count) for count in token_counts]\n\n    def get_batches_stats(self, batches: Iterable[Batch]) -> Dict[str, Union[int, List[int]]]:\n        grouped_instances = [batch.instances for batch in batches]\n        group_lengths = [len(group) for group in grouped_instances]\n\n        sample_sizes = []\n        for batch in batches:\n            batch_sequence_length = max(\n                instance.get_padding_lengths()[""text""][""tokens___tokens""]\n                for instance in batch.instances\n            )\n            sample_sizes.append(batch_sequence_length * len(batch.instances))\n\n        return {\n            ""batch_lengths"": group_lengths,\n            ""total_instances"": sum(group_lengths),\n            ""sample_sizes"": sample_sizes,\n        }\n\n    def assert_instances_are_correct(self, candidate_instances):\n        # First we need to remove padding tokens from the candidates.\n\n        candidate_instances = [\n            tuple(w for w in instance if w != 0) for instance in candidate_instances\n        ]\n        expected_instances = [\n            tuple(instance.fields[""text""]._indexed_tokens[""tokens""][""tokens""])\n            for instance in self.instances\n        ]\n        assert set(candidate_instances) == set(expected_instances)\n'"
tests/data/token_indexers/__init__.py,0,b''
tests/data/token_indexers/character_token_indexer_test.py,0,"b'from collections import defaultdict\n\nimport pytest\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Token, Vocabulary\nfrom allennlp.data.token_indexers import TokenCharactersIndexer\nfrom allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n\n\nclass CharacterTokenIndexerTest(AllenNlpTestCase):\n    def test_count_vocab_items_respects_casing(self):\n        indexer = TokenCharactersIndexer(""characters"", min_padding_length=5)\n        counter = defaultdict(lambda: defaultdict(int))\n        indexer.count_vocab_items(Token(""Hello""), counter)\n        indexer.count_vocab_items(Token(""hello""), counter)\n        assert counter[""characters""] == {""h"": 1, ""H"": 1, ""e"": 2, ""l"": 4, ""o"": 2}\n\n        indexer = TokenCharactersIndexer(\n            ""characters"", CharacterTokenizer(lowercase_characters=True), min_padding_length=5\n        )\n        counter = defaultdict(lambda: defaultdict(int))\n        indexer.count_vocab_items(Token(""Hello""), counter)\n        indexer.count_vocab_items(Token(""hello""), counter)\n        assert counter[""characters""] == {""h"": 2, ""e"": 2, ""l"": 4, ""o"": 2}\n\n    def test_as_array_produces_token_sequence(self):\n        indexer = TokenCharactersIndexer(""characters"", min_padding_length=1)\n        padded_tokens = indexer.as_padded_tensor_dict(\n            {""token_characters"": [[1, 2, 3, 4, 5], [1, 2, 3], [1]]},\n            padding_lengths={""token_characters"": 4, ""num_token_characters"": 10},\n        )\n        assert padded_tokens[""token_characters""].tolist() == [\n            [1, 2, 3, 4, 5, 0, 0, 0, 0, 0],\n            [1, 2, 3, 0, 0, 0, 0, 0, 0, 0],\n            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        ]\n\n    def test_tokens_to_indices_produces_correct_characters(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""A"", namespace=""characters"")\n        vocab.add_token_to_namespace(""s"", namespace=""characters"")\n        vocab.add_token_to_namespace(""e"", namespace=""characters"")\n        vocab.add_token_to_namespace(""n"", namespace=""characters"")\n        vocab.add_token_to_namespace(""t"", namespace=""characters"")\n        vocab.add_token_to_namespace(""c"", namespace=""characters"")\n\n        indexer = TokenCharactersIndexer(""characters"", min_padding_length=1)\n        indices = indexer.tokens_to_indices([Token(""sentential"")], vocab)\n        assert indices == {""token_characters"": [[3, 4, 5, 6, 4, 5, 6, 1, 1, 1]]}\n\n    def test_start_and_end_tokens(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""A"", namespace=""characters"")  # 2\n        vocab.add_token_to_namespace(""s"", namespace=""characters"")  # 3\n        vocab.add_token_to_namespace(""e"", namespace=""characters"")  # 4\n        vocab.add_token_to_namespace(""n"", namespace=""characters"")  # 5\n        vocab.add_token_to_namespace(""t"", namespace=""characters"")  # 6\n        vocab.add_token_to_namespace(""c"", namespace=""characters"")  # 7\n        vocab.add_token_to_namespace(""<"", namespace=""characters"")  # 8\n        vocab.add_token_to_namespace("">"", namespace=""characters"")  # 9\n        vocab.add_token_to_namespace(""/"", namespace=""characters"")  # 10\n\n        indexer = TokenCharactersIndexer(\n            ""characters"", start_tokens=[""<s>""], end_tokens=[""</s>""], min_padding_length=1\n        )\n        indices = indexer.tokens_to_indices([Token(""sentential"")], vocab)\n        assert indices == {\n            ""token_characters"": [[8, 3, 9], [3, 4, 5, 6, 4, 5, 6, 1, 1, 1], [8, 10, 3, 9]]\n        }\n\n    def test_min_padding_length(self):\n        sentence = ""AllenNLP is awesome .""\n        tokens = [Token(token) for token in sentence.split("" "")]\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""A"", namespace=""characters"")  # 2\n        vocab.add_token_to_namespace(""l"", namespace=""characters"")  # 3\n        vocab.add_token_to_namespace(""e"", namespace=""characters"")  # 4\n        vocab.add_token_to_namespace(""n"", namespace=""characters"")  # 5\n        vocab.add_token_to_namespace(""N"", namespace=""characters"")  # 6\n        vocab.add_token_to_namespace(""L"", namespace=""characters"")  # 7\n        vocab.add_token_to_namespace(""P"", namespace=""characters"")  # 8\n        vocab.add_token_to_namespace(""i"", namespace=""characters"")  # 9\n        vocab.add_token_to_namespace(""s"", namespace=""characters"")  # 10\n        vocab.add_token_to_namespace(""a"", namespace=""characters"")  # 11\n        vocab.add_token_to_namespace(""w"", namespace=""characters"")  # 12\n        vocab.add_token_to_namespace(""o"", namespace=""characters"")  # 13\n        vocab.add_token_to_namespace(""m"", namespace=""characters"")  # 14\n        vocab.add_token_to_namespace(""."", namespace=""characters"")  # 15\n\n        indexer = TokenCharactersIndexer(""characters"", min_padding_length=10)\n        indices = indexer.tokens_to_indices(tokens, vocab)\n        padded = indexer.as_padded_tensor_dict(indices, indexer.get_padding_lengths(indices))\n        assert padded[""token_characters""].tolist() == [\n            [2, 3, 3, 4, 5, 6, 7, 8, 0, 0],\n            [9, 10, 0, 0, 0, 0, 0, 0, 0, 0],\n            [11, 12, 4, 10, 13, 14, 4, 0, 0, 0],\n            [15, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        ]\n\n    def test_warn_min_padding_length(self):\n        with pytest.warns(\n            UserWarning, match=r""using the default value \\(0\\) of `min_padding_length`""\n        ):\n            TokenCharactersIndexer(""characters"")\n'"
tests/data/token_indexers/elmo_indexer_test.py,0,"b'import numpy as np\nimport pytest\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Token, Vocabulary, Instance\nfrom allennlp.data.batch import Batch\nfrom allennlp.data.token_indexers import ELMoTokenCharactersIndexer\nfrom allennlp.data.fields import ListField, TextField\n\n\nclass TestELMoTokenCharactersIndexer(AllenNlpTestCase):\n    def test_bos_to_char_ids(self):\n        indexer = ELMoTokenCharactersIndexer()\n        indices = indexer.tokens_to_indices([Token(""<S>"")], Vocabulary())\n        expected_indices = [\n            259,\n            257,\n            260,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n        ]\n        assert indices == {""elmo_tokens"": [expected_indices]}\n\n    def test_eos_to_char_ids(self):\n        indexer = ELMoTokenCharactersIndexer()\n        indices = indexer.tokens_to_indices([Token(""</S>"")], Vocabulary())\n        expected_indices = [\n            259,\n            258,\n            260,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n        ]\n        assert indices == {""elmo_tokens"": [expected_indices]}\n\n    def test_unicode_to_char_ids(self):\n        indexer = ELMoTokenCharactersIndexer()\n        indices = indexer.tokens_to_indices([Token(chr(256) + ""t"")], Vocabulary())\n        expected_indices = [\n            259,\n            197,\n            129,\n            117,\n            260,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n            261,\n        ]\n        assert indices == {""elmo_tokens"": [expected_indices]}\n\n    def test_elmo_as_array_produces_token_sequence(self):\n        indexer = ELMoTokenCharactersIndexer()\n        tokens = [Token(""Second""), Token(""."")]\n        indices = indexer.tokens_to_indices(tokens, Vocabulary())\n        padded_tokens = indexer.as_padded_tensor_dict(indices, padding_lengths={""elmo_tokens"": 3})\n        expected_padded_tokens = [\n            [\n                259,\n                84,\n                102,\n                100,\n                112,\n                111,\n                101,\n                260,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n            ],\n            [\n                259,\n                47,\n                260,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n            ],\n            [\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n                0,\n            ],\n        ]\n\n        assert padded_tokens[""elmo_tokens""].tolist() == expected_padded_tokens\n\n    def test_elmo_indexer_with_additional_tokens(self):\n        indexer = ELMoTokenCharactersIndexer(tokens_to_add={""<first>"": 1})\n        tokens = [Token(""<first>"")]\n        indices = indexer.tokens_to_indices(tokens, Vocabulary())\n        expected_indices = [\n            [\n                259,\n                2,\n                260,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n                261,\n            ]\n        ]\n        assert indices[""elmo_tokens""] == expected_indices\n\n    def test_elmo_empty_token_list(self):\n        # Basic test\n        indexer = ELMoTokenCharactersIndexer()\n        assert {""elmo_tokens"": []} == indexer.get_empty_token_list()\n        # Real world test\n        indexer = {""elmo"": indexer}\n        tokens_1 = TextField([Token(""Apple"")], indexer)\n        targets_1 = ListField([TextField([Token(""Apple"")], indexer)])\n        tokens_2 = TextField([Token(""Screen""), Token(""device"")], indexer)\n        targets_2 = ListField(\n            [TextField([Token(""Screen"")], indexer), TextField([Token(""Device"")], indexer)]\n        )\n        instance_1 = Instance({""tokens"": tokens_1, ""targets"": targets_1})\n        instance_2 = Instance({""tokens"": tokens_2, ""targets"": targets_2})\n        a_batch = Batch([instance_1, instance_2])\n        a_batch.index_instances(Vocabulary())\n        batch_tensor = a_batch.as_tensor_dict()\n        elmo_target_token_indices = batch_tensor[""targets""][""elmo""][""elmo_tokens""]\n        # The TextField that is empty should have been created using the\n        # `get_empty_token_list` and then padded with zeros.\n        empty_target = elmo_target_token_indices[0][1].numpy()\n        np.testing.assert_array_equal(np.zeros((1, 50)), empty_target)\n        non_empty_targets = [\n            elmo_target_token_indices[0][0],\n            elmo_target_token_indices[1][0],\n            elmo_target_token_indices[1][1],\n        ]\n        for non_empty_target in non_empty_targets:\n            with pytest.raises(AssertionError):\n                np.testing.assert_array_equal(np.zeros((1, 50)), non_empty_target)\n'"
tests/data/token_indexers/pretrained_transformer_indexer_test.py,0,"b'from transformers.tokenization_auto import AutoTokenizer\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Vocabulary\nfrom allennlp.data.token_indexers import PretrainedTransformerIndexer\nfrom allennlp.data.tokenizers import PretrainedTransformerTokenizer\n\n\nclass TestPretrainedTransformerIndexer(AllenNlpTestCase):\n    def test_as_array_produces_token_sequence_bert_uncased(self):\n        tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")\n        allennlp_tokenizer = PretrainedTransformerTokenizer(""bert-base-uncased"")\n        indexer = PretrainedTransformerIndexer(model_name=""bert-base-uncased"")\n        string_specials = ""[CLS] AllenNLP is great [SEP]""\n        string_no_specials = ""AllenNLP is great""\n        tokens = tokenizer.tokenize(string_specials)\n        expected_ids = tokenizer.convert_tokens_to_ids(tokens)\n        # tokens tokenized with our pretrained tokenizer have indices in them\n        allennlp_tokens = allennlp_tokenizer.tokenize(string_no_specials)\n        vocab = Vocabulary()\n        indexed = indexer.tokens_to_indices(allennlp_tokens, vocab)\n        assert indexed[""token_ids""] == expected_ids\n\n    def test_as_array_produces_token_sequence_bert_cased(self):\n        tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n        allennlp_tokenizer = PretrainedTransformerTokenizer(""bert-base-cased"")\n        indexer = PretrainedTransformerIndexer(model_name=""bert-base-cased"")\n        string_specials = ""[CLS] AllenNLP is great [SEP]""\n        string_no_specials = ""AllenNLP is great""\n        tokens = tokenizer.tokenize(string_specials)\n        expected_ids = tokenizer.convert_tokens_to_ids(tokens)\n        # tokens tokenized with our pretrained tokenizer have indices in them\n        allennlp_tokens = allennlp_tokenizer.tokenize(string_no_specials)\n        vocab = Vocabulary()\n        indexed = indexer.tokens_to_indices(allennlp_tokens, vocab)\n        assert indexed[""token_ids""] == expected_ids\n\n    def test_as_array_produces_token_sequence_bert_cased_sentence_pair(self):\n        tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n        allennlp_tokenizer = PretrainedTransformerTokenizer(\n            ""bert-base-cased"", add_special_tokens=False\n        )\n        indexer = PretrainedTransformerIndexer(model_name=""bert-base-cased"")\n        default_format = ""[CLS] AllenNLP is great! [SEP] Really it is! [SEP]""\n        tokens = tokenizer.tokenize(default_format)\n        expected_ids = tokenizer.convert_tokens_to_ids(tokens)\n        allennlp_tokens = allennlp_tokenizer.add_special_tokens(\n            allennlp_tokenizer.tokenize(""AllenNLP is great!""),\n            allennlp_tokenizer.tokenize(""Really it is!""),\n        )\n        vocab = Vocabulary()\n        indexed = indexer.tokens_to_indices(allennlp_tokens, vocab)\n        assert indexed[""token_ids""] == expected_ids\n\n    def test_as_array_produces_token_sequence_roberta(self):\n        tokenizer = AutoTokenizer.from_pretrained(""roberta-base"")\n        allennlp_tokenizer = PretrainedTransformerTokenizer(""roberta-base"")\n        indexer = PretrainedTransformerIndexer(model_name=""roberta-base"")\n        string_specials = ""<s> AllenNLP is great </s>""\n        string_no_specials = ""AllenNLP is great""\n        tokens = tokenizer.tokenize(string_specials)\n        expected_ids = tokenizer.convert_tokens_to_ids(tokens)\n        # tokens tokenized with our pretrained tokenizer have indices in them\n        allennlp_tokens = allennlp_tokenizer.tokenize(string_no_specials)\n        vocab = Vocabulary()\n        indexed = indexer.tokens_to_indices(allennlp_tokens, vocab)\n        assert indexed[""token_ids""] == expected_ids\n\n    def test_as_array_produces_token_sequence_roberta_sentence_pair(self):\n        tokenizer = AutoTokenizer.from_pretrained(""roberta-base"")\n        allennlp_tokenizer = PretrainedTransformerTokenizer(\n            ""roberta-base"", add_special_tokens=False\n        )\n        indexer = PretrainedTransformerIndexer(model_name=""roberta-base"")\n        default_format = ""<s> AllenNLP is great! </s> </s> Really it is! </s>""\n        tokens = tokenizer.tokenize(default_format)\n        expected_ids = tokenizer.convert_tokens_to_ids(tokens)\n        allennlp_tokens = allennlp_tokenizer.add_special_tokens(\n            allennlp_tokenizer.tokenize(""AllenNLP is great!""),\n            allennlp_tokenizer.tokenize(""Really it is!""),\n        )\n        vocab = Vocabulary()\n        indexed = indexer.tokens_to_indices(allennlp_tokens, vocab)\n        assert indexed[""token_ids""] == expected_ids\n\n    def test_transformers_vocab_sizes(self):\n        def check_vocab_size(model_name: str):\n            namespace = ""tags""\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n            allennlp_tokenizer = PretrainedTransformerTokenizer(model_name)\n            indexer = PretrainedTransformerIndexer(model_name=model_name, namespace=namespace)\n            allennlp_tokens = allennlp_tokenizer.tokenize(""AllenNLP is great!"")\n            vocab = Vocabulary()\n            # here we copy entire transformers vocab\n            indexed = indexer.tokens_to_indices(allennlp_tokens, vocab)\n            del indexed\n            assert vocab.get_vocab_size(namespace=namespace) == tokenizer.vocab_size\n\n        check_vocab_size(""roberta-base"")\n        check_vocab_size(""bert-base-cased"")\n        check_vocab_size(""xlm-mlm-ende-1024"")\n\n    def test_transformers_vocabs_added_correctly(self):\n        namespace, model_name = ""tags"", ""roberta-base""\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        allennlp_tokenizer = PretrainedTransformerTokenizer(model_name)\n        indexer = PretrainedTransformerIndexer(model_name=model_name, namespace=namespace)\n        allennlp_tokens = allennlp_tokenizer.tokenize(""AllenNLP is great!"")\n        vocab = Vocabulary()\n        # here we copy entire transformers vocab\n        indexed = indexer.tokens_to_indices(allennlp_tokens, vocab)\n        del indexed\n        assert vocab.get_token_to_index_vocabulary(namespace=namespace) == tokenizer.encoder\n\n    def test_mask(self):\n        # We try these models, because\n        #  - BERT pads tokens with 0\n        #  - RoBERTa pads tokens with 1\n        #  - GPT2 has no padding token, so we choose 0\n        for model in [""bert-base-uncased"", ""roberta-base"", ""gpt2""]:\n            allennlp_tokenizer = PretrainedTransformerTokenizer(model)\n            indexer = PretrainedTransformerIndexer(model_name=model)\n            string_no_specials = ""AllenNLP is great""\n            allennlp_tokens = allennlp_tokenizer.tokenize(string_no_specials)\n            vocab = Vocabulary()\n            indexed = indexer.tokens_to_indices(allennlp_tokens, vocab)\n            expected_masks = [True] * len(indexed[""token_ids""])\n            assert indexed[""mask""] == expected_masks\n            max_length = 10\n            padding_lengths = {key: max_length for key in indexed.keys()}\n            padded_tokens = indexer.as_padded_tensor_dict(indexed, padding_lengths)\n            padding_length = max_length - len(indexed[""mask""])\n            expected_masks = expected_masks + ([False] * padding_length)\n            assert len(padded_tokens[""mask""]) == max_length\n            assert padded_tokens[""mask""].tolist() == expected_masks\n\n            assert len(padded_tokens[""token_ids""]) == max_length\n            pad_token_id = allennlp_tokenizer.tokenizer.pad_token_id\n            if pad_token_id is None:\n                pad_token_id = 0\n            padding_suffix = [pad_token_id] * padding_length\n            assert padded_tokens[""token_ids""][-padding_length:].tolist() == padding_suffix\n\n    def test_long_sequence_splitting(self):\n        tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")\n        allennlp_tokenizer = PretrainedTransformerTokenizer(""bert-base-uncased"")\n        indexer = PretrainedTransformerIndexer(model_name=""bert-base-uncased"", max_length=4)\n        string_specials = ""[CLS] AllenNLP is great [SEP]""\n        string_no_specials = ""AllenNLP is great""\n        tokens = tokenizer.tokenize(string_specials)\n        expected_ids = tokenizer.convert_tokens_to_ids(tokens)\n        assert len(expected_ids) == 7  # just to make sure it\'s what we\'re expecting\n        cls_id, sep_id = expected_ids[0], expected_ids[-1]\n        expected_ids = (\n            expected_ids[:3]\n            + [sep_id, cls_id]\n            + expected_ids[3:5]\n            + [sep_id, cls_id]\n            + expected_ids[5:]\n        )\n\n        allennlp_tokens = allennlp_tokenizer.tokenize(string_no_specials)\n        vocab = Vocabulary()\n        indexed = indexer.tokens_to_indices(allennlp_tokens, vocab)\n        assert indexed[""token_ids""] == expected_ids\n        assert indexed[""segment_concat_mask""] == [True] * len(expected_ids)\n        assert indexed[""mask""] == [True] * 7  # original length\n\n    @staticmethod\n    def _assert_tokens_equal(expected_tokens, actual_tokens):\n        for expected, actual in zip(expected_tokens, actual_tokens):\n            assert expected.text == actual.text\n            assert expected.text_id == actual.text_id\n            assert expected.type_id == actual.type_id\n\n    def test_indices_to_tokens(self):\n        allennlp_tokenizer = PretrainedTransformerTokenizer(""bert-base-uncased"")\n        indexer_max_length = PretrainedTransformerIndexer(\n            model_name=""bert-base-uncased"", max_length=4\n        )\n        indexer_no_max_length = PretrainedTransformerIndexer(model_name=""bert-base-uncased"")\n        string_no_specials = ""AllenNLP is great""\n\n        allennlp_tokens = allennlp_tokenizer.tokenize(string_no_specials)\n        vocab = Vocabulary()\n        indexed = indexer_no_max_length.tokens_to_indices(allennlp_tokens, vocab)\n        tokens_from_indices = indexer_no_max_length.indices_to_tokens(indexed, vocab)\n\n        self._assert_tokens_equal(allennlp_tokens, tokens_from_indices)\n\n        indexed = indexer_max_length.tokens_to_indices(allennlp_tokens, vocab)\n        tokens_from_indices = indexer_max_length.indices_to_tokens(indexed, vocab)\n\n        # For now we are not removing special tokens introduced from max_length\n        sep_cls = [allennlp_tokens[-1], allennlp_tokens[0]]\n        expected = (\n            allennlp_tokens[:3] + sep_cls + allennlp_tokens[3:5] + sep_cls + allennlp_tokens[5:]\n        )\n\n        self._assert_tokens_equal(expected, tokens_from_indices)\n'"
tests/data/token_indexers/pretrained_transformer_mismatched_indexer_test.py,0,"b'from transformers.tokenization_auto import AutoTokenizer\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Token, Vocabulary\nfrom allennlp.data.token_indexers import PretrainedTransformerMismatchedIndexer\n\n\nclass TestPretrainedTransformerMismatchedIndexer(AllenNlpTestCase):\n    def test_bert(self):\n        tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n        indexer = PretrainedTransformerMismatchedIndexer(""bert-base-cased"")\n        text = [""AllenNLP"", ""is"", ""great""]\n        tokens = tokenizer.tokenize("" "".join([""[CLS]""] + text + [""[SEP]""]))\n        expected_ids = tokenizer.convert_tokens_to_ids(tokens)\n        vocab = Vocabulary()\n        indexed = indexer.tokens_to_indices([Token(word) for word in text], vocab)\n        assert indexed[""token_ids""] == expected_ids\n        assert indexed[""mask""] == [True] * len(text)\n        # Hardcoding a few things because we know how BERT tokenization works\n        assert indexed[""offsets""] == [(1, 3), (4, 4), (5, 5)]\n        assert indexed[""wordpiece_mask""] == [True] * len(expected_ids)\n\n        keys = indexed.keys()\n        assert indexer.get_empty_token_list() == {key: [] for key in keys}\n\n        max_length = 10\n        padding_lengths = {key: max_length for key in keys}\n        padded_tokens = indexer.as_padded_tensor_dict(indexed, padding_lengths)\n        for key in keys:\n            padding_length = max_length - len(indexed[key])\n            if key == ""offsets"":\n                padding = (0, 0)\n            elif ""mask"" in key:\n                padding = False\n            else:\n                padding = 0\n            expected_value = indexed[key] + ([padding] * padding_length)\n            assert len(padded_tokens[key]) == max_length\n            if key == ""offsets"":\n                expected_value = [list(t) for t in expected_value]\n            assert padded_tokens[key].tolist() == expected_value\n\n    def test_long_sequence_splitting(self):\n        tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")\n        indexer = PretrainedTransformerMismatchedIndexer(""bert-base-uncased"", max_length=4)\n        text = [""AllenNLP"", ""is"", ""great""]\n        tokens = tokenizer.tokenize("" "".join([""[CLS]""] + text + [""[SEP]""]))\n        expected_ids = tokenizer.convert_tokens_to_ids(tokens)\n        assert len(expected_ids) == 7  # just to make sure it\'s what we\'re expecting\n        cls_id, sep_id = expected_ids[0], expected_ids[-1]\n        expected_ids = (\n            expected_ids[:3]\n            + [sep_id, cls_id]\n            + expected_ids[3:5]\n            + [sep_id, cls_id]\n            + expected_ids[5:]\n        )\n\n        vocab = Vocabulary()\n        indexed = indexer.tokens_to_indices([Token(word) for word in text], vocab)\n\n        assert indexed[""token_ids""] == expected_ids\n        # [CLS] allen ##nl [SEP] [CLS] #p is [SEP] [CLS] great [SEP]\n        assert indexed[""segment_concat_mask""] == [True] * len(expected_ids)\n        # allennlp is great\n        assert indexed[""mask""] == [True] * len(text)\n        # [CLS] allen #nl #p is great [SEP]\n        assert indexed[""wordpiece_mask""] == [True] * 7\n'"
tests/data/token_indexers/single_id_token_indexer_test.py,0,"b'from collections import defaultdict\nfrom dataclasses import dataclass\n\nimport pytest\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Token, Vocabulary\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.tokenizers import SpacyTokenizer\n\n\n@dataclass(init=False)\nclass TokenWithStyle(Token):\n    __slots__ = [""is_bold""]\n\n    is_bold: bool\n\n    def __init__(self, text: str = None, is_bold: bool = False):\n        super().__init__(text=text)\n        self.is_bold = is_bold\n\n\nclass TestSingleIdTokenIndexer(AllenNlpTestCase):\n    def test_count_vocab_items_respects_casing(self):\n        indexer = SingleIdTokenIndexer(""words"")\n        counter = defaultdict(lambda: defaultdict(int))\n        indexer.count_vocab_items(Token(""Hello""), counter)\n        indexer.count_vocab_items(Token(""hello""), counter)\n        assert counter[""words""] == {""hello"": 1, ""Hello"": 1}\n\n        indexer = SingleIdTokenIndexer(""words"", lowercase_tokens=True)\n        counter = defaultdict(lambda: defaultdict(int))\n        indexer.count_vocab_items(Token(""Hello""), counter)\n        indexer.count_vocab_items(Token(""hello""), counter)\n        assert counter[""words""] == {""hello"": 2}\n\n    def test_as_array_produces_token_sequence(self):\n        indexer = SingleIdTokenIndexer(""words"")\n        padded_tokens = indexer.as_padded_tensor_dict({""tokens"": [1, 2, 3, 4, 5]}, {""tokens"": 10})\n        assert padded_tokens[""tokens""].tolist() == [1, 2, 3, 4, 5, 0, 0, 0, 0, 0]\n\n    def test_count_other_features(self):\n        indexer = SingleIdTokenIndexer(""other_features"", feature_name=""is_bold"")\n        counter = defaultdict(lambda: defaultdict(int))\n        token = TokenWithStyle(""Header"")\n        token.is_bold = ""True""\n        indexer.count_vocab_items(token, counter)\n        assert counter[""other_features""] == {""True"": 1}\n\n    def test_count_vocab_items_with_non_default_feature_name(self):\n        tokenizer = SpacyTokenizer(parse=True)\n        tokens = tokenizer.tokenize(""This is a sentence."")\n        tokens = [Token(""<S>"")] + [t for t in tokens] + [Token(""</S>"")]\n        indexer = SingleIdTokenIndexer(\n            namespace=""dep_labels"", feature_name=""dep_"", default_value=""NONE""\n        )\n        counter = defaultdict(lambda: defaultdict(int))\n        for token in tokens:\n            indexer.count_vocab_items(token, counter)\n\n        assert counter[""dep_labels""] == {\n            ""ROOT"": 1,\n            ""nsubj"": 1,\n            ""det"": 1,\n            ""NONE"": 2,\n            ""attr"": 1,\n            ""punct"": 1,\n        }\n\n    def test_tokens_to_indices_with_non_default_feature_name(self):\n        tokenizer = SpacyTokenizer(parse=True)\n        tokens = tokenizer.tokenize(""This is a sentence."")\n        tokens = [t for t in tokens] + [Token(""</S>"")]\n        vocab = Vocabulary()\n        root_index = vocab.add_token_to_namespace(""ROOT"", namespace=""dep_labels"")\n        none_index = vocab.add_token_to_namespace(""NONE"", namespace=""dep_labels"")\n        indexer = SingleIdTokenIndexer(\n            namespace=""dep_labels"", feature_name=""dep_"", default_value=""NONE""\n        )\n        assert indexer.tokens_to_indices([tokens[1]], vocab) == {""tokens"": [root_index]}\n        assert indexer.tokens_to_indices([tokens[-1]], vocab) == {""tokens"": [none_index]}\n\n    def test_crashes_with_empty_feature_value_and_no_default(self):\n        tokenizer = SpacyTokenizer(parse=True)\n        tokens = tokenizer.tokenize(""This is a sentence."")\n        tokens = [t for t in tokens] + [Token(""</S>"")]\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""ROOT"", namespace=""dep_labels"")\n        vocab.add_token_to_namespace(""NONE"", namespace=""dep_labels"")\n        indexer = SingleIdTokenIndexer(namespace=""dep_labels"", feature_name=""dep_"")\n        with pytest.raises(ValueError):\n            indexer.tokens_to_indices([tokens[-1]], vocab)\n\n    def test_no_namespace_means_no_counting(self):\n        tokenizer = SpacyTokenizer(parse=True)\n        tokens = tokenizer.tokenize(""This is a sentence."")\n        tokens = [Token(""<S>"")] + [t for t in tokens] + [Token(""</S>"")]\n        indexer = SingleIdTokenIndexer(namespace=None, feature_name=""text_id"")\n\n        def fail():\n            assert False\n\n        counter = defaultdict(fail)\n        for token in tokens:\n            indexer.count_vocab_items(token, counter)\n\n    def test_no_namespace_means_no_indexing(self):\n        indexer = SingleIdTokenIndexer(namespace=None, feature_name=""text_id"")\n        assert indexer.tokens_to_indices([Token(text_id=23)], None) == {""tokens"": [23]}\n'"
tests/data/token_indexers/spacy_indexer_test.py,0,"b'from allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.token_indexers.spacy_indexer import SpacyTokenIndexer\nfrom allennlp.data.fields.text_field import TextField\nfrom allennlp.common.util import get_spacy_model\nfrom allennlp.data.vocabulary import Vocabulary\n\n\nclass TestSpacyTokenIndexer(AllenNlpTestCase):\n    def test_as_array_produces_token_array(self):\n        indexer = SpacyTokenIndexer()\n        nlp = get_spacy_model(""en_core_web_sm"", pos_tags=True, parse=False, ner=False)\n        tokens = [t for t in nlp(""This is a sentence."")]\n        field = TextField(tokens, token_indexers={""spacy"": indexer})\n\n        vocab = Vocabulary()\n        field.index(vocab)\n\n        # Indexer functionality\n        array_dict = indexer.tokens_to_indices(tokens, vocab)\n        assert len(array_dict[""tokens""]) == 5\n        assert len(array_dict[""tokens""][0]) == 96\n\n        # Check it also works with field\n        lengths = field.get_padding_lengths()\n        array_dict = field.as_tensor(lengths)\n\n        assert list(array_dict[""spacy""][""tokens""].shape) == [5, 96]\n'"
tests/data/tokenizers/__init__.py,0,b''
tests/data/tokenizers/character_tokenizer_test.py,0,"b'from allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.tokenizers import CharacterTokenizer\n\n\nclass TestCharacterTokenizer(AllenNlpTestCase):\n    def test_splits_into_characters(self):\n        tokenizer = CharacterTokenizer(start_tokens=[""<S1>"", ""<S2>""], end_tokens=[""</S2>"", ""</S1>""])\n        sentence = ""A, small sentence.""\n        tokens = [t.text for t in tokenizer.tokenize(sentence)]\n        expected_tokens = [\n            ""<S1>"",\n            ""<S2>"",\n            ""A"",\n            "","",\n            "" "",\n            ""s"",\n            ""m"",\n            ""a"",\n            ""l"",\n            ""l"",\n            "" "",\n            ""s"",\n            ""e"",\n            ""n"",\n            ""t"",\n            ""e"",\n            ""n"",\n            ""c"",\n            ""e"",\n            ""."",\n            ""</S2>"",\n            ""</S1>"",\n        ]\n        assert tokens == expected_tokens\n\n    def test_batch_tokenization(self):\n        tokenizer = CharacterTokenizer()\n        sentences = [\n            ""This is a sentence"",\n            ""This isn\'t a sentence."",\n            ""This is the 3rd sentence."" ""Here\'s the \'fourth\' sentence."",\n        ]\n        batch_tokenized = tokenizer.batch_tokenize(sentences)\n        separately_tokenized = [tokenizer.tokenize(sentence) for sentence in sentences]\n        assert len(batch_tokenized) == len(separately_tokenized)\n        for batch_sentence, separate_sentence in zip(batch_tokenized, separately_tokenized):\n            assert len(batch_sentence) == len(separate_sentence)\n            for batch_word, separate_word in zip(batch_sentence, separate_sentence):\n                assert batch_word.text == separate_word.text\n\n    def test_handles_byte_encoding(self):\n        tokenizer = CharacterTokenizer(byte_encoding=""utf-8"", start_tokens=[259], end_tokens=[260])\n        word = ""\xc3\xa5\xc3\xb8\xc3\xa2\xc3\xa1abe""\n        tokens = [t.text_id for t in tokenizer.tokenize(word)]\n        # Note that we\'ve added one to the utf-8 encoded bytes, to account for masking.\n        expected_tokens = [259, 196, 166, 196, 185, 196, 163, 196, 162, 98, 99, 102, 260]\n        assert tokens == expected_tokens\n'"
tests/data/tokenizers/letters_digits_tokenizer_test.py,0,"b'from allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.tokenizers.letters_digits_tokenizer import LettersDigitsTokenizer\nfrom allennlp.data.tokenizers.token import Token\n\n\nclass TestLettersDigitsTokenizer(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.word_tokenizer = LettersDigitsTokenizer()\n\n    def test_tokenize_handles_complex_punctuation(self):\n        sentence = ""this (sentence) has \'crazy\' \\""punctuation\\"".""\n        expected_tokens = [\n            ""this"",\n            ""("",\n            ""sentence"",\n            "")"",\n            ""has"",\n            ""\'"",\n            ""crazy"",\n            ""\'"",\n            \'""\',\n            ""punctuation"",\n            \'""\',\n            ""."",\n        ]\n        tokens = [t.text for t in self.word_tokenizer.tokenize(sentence)]\n        assert tokens == expected_tokens\n\n    def test_tokenize_handles_unicode_letters(self):\n        sentence = ""HAL9000   and    \xc3\x85ngstr\xc3\xb6m""\n        expected_tokens = [\n            Token(""HAL"", 0),\n            Token(""9000"", 3),\n            Token(""and"", 10),\n            Token(""\xc3\x85ngstr\xc3\xb6m"", 17),\n        ]\n        tokens = self.word_tokenizer.tokenize(sentence)\n        assert [t.text for t in tokens] == [t.text for t in expected_tokens]\n        assert [t.idx for t in tokens] == [t.idx for t in expected_tokens]\n\n    def test_tokenize_handles_splits_all_punctuation(self):\n        sentence = ""wouldn\'t.[have] -3.45(m^2)""\n        expected_tokens = [\n            ""wouldn"",\n            ""\'"",\n            ""t"",\n            ""."",\n            ""["",\n            ""have"",\n            ""]"",\n            ""-"",\n            ""3"",\n            ""."",\n            ""45"",\n            ""("",\n            ""m"",\n            ""^"",\n            ""2"",\n            "")"",\n        ]\n        tokens = [t.text for t in self.word_tokenizer.tokenize(sentence)]\n        assert tokens == expected_tokens\n'"
tests/data/tokenizers/pretrained_transformer_tokenizer_test.py,0,"b'from typing import Iterable, List\n\nfrom allennlp.common import Params\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Token\nfrom allennlp.data.tokenizers import PretrainedTransformerTokenizer\n\n\nclass TestPretrainedTransformerTokenizer(AllenNlpTestCase):\n    def test_splits_roberta(self):\n        tokenizer = PretrainedTransformerTokenizer(""roberta-base"")\n\n        sentence = ""A, <mask> AllenNLP sentence.""\n        expected_tokens = [\n            ""<s>"",\n            ""\xc4\xa0A"",\n            "","",\n            ""<mask>"",\n            ""\xc4\xa0Allen"",\n            ""N"",\n            ""LP"",\n            ""\xc4\xa0sentence"",\n            ""."",\n            ""</s>"",\n        ]\n        tokens = [t.text for t in tokenizer.tokenize(sentence)]\n        assert tokens == expected_tokens\n\n    def test_splits_cased_bert(self):\n        tokenizer = PretrainedTransformerTokenizer(""bert-base-cased"")\n\n        sentence = ""A, [MASK] AllenNLP sentence.""\n        expected_tokens = [\n            ""[CLS]"",\n            ""A"",\n            "","",\n            ""[MASK]"",\n            ""Allen"",\n            ""##NL"",\n            ""##P"",\n            ""sentence"",\n            ""."",\n            ""[SEP]"",\n        ]\n        tokens = [t.text for t in tokenizer.tokenize(sentence)]\n        assert tokens == expected_tokens\n\n    def test_splits_uncased_bert(self):\n        sentence = ""A, [MASK] AllenNLP sentence.""\n        expected_tokens = [\n            ""[CLS]"",\n            ""a"",\n            "","",\n            ""[MASK]"",\n            ""allen"",\n            ""##nl"",\n            ""##p"",\n            ""sentence"",\n            ""."",\n            ""[SEP]"",\n        ]\n        tokenizer = PretrainedTransformerTokenizer(""bert-base-uncased"")\n        tokens = [t.text for t in tokenizer.tokenize(sentence)]\n        assert tokens == expected_tokens\n\n    def test_splits_reformer_small(self):\n        sentence = ""A, [MASK] AllenNLP sentence.""\n        expected_tokens = [\n            ""\xe2\x96\x81A"",\n            "","",\n            ""\xe2\x96\x81"",\n            ""<unk>"",\n            ""M"",\n            ""A"",\n            ""S"",\n            ""K"",\n            ""<unk>"",\n            ""\xe2\x96\x81A"",\n            ""ll"",\n            ""en"",\n            ""N"",\n            ""L"",\n            ""P"",\n            ""\xe2\x96\x81s"",\n            ""ent"",\n            ""en"",\n            ""ce"",\n            ""."",\n        ]\n        tokenizer = PretrainedTransformerTokenizer(""google/reformer-crime-and-punishment"")\n        tokens = [t.text for t in tokenizer.tokenize(sentence)]\n        assert tokens == expected_tokens\n\n    def test_token_idx_bert_uncased(self):\n        sentence = ""A, na\xc3\xafve [MASK] AllenNLP sentence.""\n        expected_tokens = [\n            ""[CLS]"",\n            ""a"",\n            "","",\n            ""naive"",  # BERT normalizes this away\n            ""[MASK]"",\n            ""allen"",\n            ""##nl"",\n            ""##p"",\n            ""sentence"",\n            ""."",\n            ""[SEP]"",\n        ]\n        expected_idxs = [None, 0, 1, 3, 9, 16, 21, 23, 25, 33, None]\n        tokenizer = PretrainedTransformerTokenizer(""bert-base-uncased"")\n        tokenized = tokenizer.tokenize(sentence)\n        tokens = [t.text for t in tokenized]\n        assert tokens == expected_tokens\n        idxs = [t.idx for t in tokenized]\n        assert idxs == expected_idxs\n\n    def test_token_idx_bert_cased(self):\n        sentence = ""A, na\xc3\xafve [MASK] AllenNLP sentence.""\n        expected_tokens = [\n            ""[CLS]"",\n            ""A"",\n            "","",\n            ""naive"",  # BERT normalizes this away\n            ""[MASK]"",\n            ""Allen"",\n            ""##NL"",\n            ""##P"",\n            ""sentence"",\n            ""."",\n            ""[SEP]"",\n        ]\n        expected_idxs = [None, 0, 1, 3, 9, 16, 21, 23, 25, 33, None]\n        tokenizer = PretrainedTransformerTokenizer(""bert-base-cased"")\n        tokenized = tokenizer.tokenize(sentence)\n        tokens = [t.text for t in tokenized]\n        assert tokens == expected_tokens\n        idxs = [t.idx for t in tokenized]\n        assert idxs == expected_idxs\n\n    def test_token_idx_roberta(self):\n        sentence = ""A, na\xc3\xafve <mask> AllenNLP sentence.""\n        expected_tokens = [\n            ""<s>"",\n            ""\xc4\xa0A"",\n            "","",\n            ""\xc4\xa0na\xc3\x83\xc2\xafve"",  # RoBERTa mangles this. Or maybe it ""encodes""?\n            ""<mask>"",\n            ""\xc4\xa0Allen"",\n            ""N"",\n            ""LP"",\n            ""\xc4\xa0sentence"",\n            ""."",\n            ""</s>"",\n        ]\n        expected_idxs = [None, 0, 1, 3, 9, 16, 21, 22, 25, 33, None]\n        tokenizer = PretrainedTransformerTokenizer(""roberta-base"")\n        tokenized = tokenizer.tokenize(sentence)\n        tokens = [t.text for t in tokenized]\n        assert tokens == expected_tokens\n        idxs = [t.idx for t in tokenized]\n        assert idxs == expected_idxs\n\n    def test_token_idx_wikipedia(self):\n        sentence = (\n            ""Tokyo (\xe6\x9d\xb1\xe4\xba\xac T\xc5\x8dky\xc5\x8d, English: /\xcb\x88to\xca\x8akio\xca\x8a/,[7] Japanese: [to\xcb\x90k\xca\xb2o\xcb\x90]), officially ""\n            ""Tokyo Metropolis (\xe6\x9d\xb1\xe4\xba\xac\xe9\x83\xbd T\xc5\x8dky\xc5\x8d-to), is one of the 47 prefectures of Japan.""\n        )\n        for tokenizer_name in [""roberta-base"", ""bert-base-uncased"", ""bert-base-cased""]:\n            tokenizer = PretrainedTransformerTokenizer(tokenizer_name)\n            tokenized = tokenizer.tokenize(sentence)\n            assert tokenized[-2].text == "".""\n            assert tokenized[-2].idx == len(sentence) - 1\n\n    def test_intra_word_tokenize(self):\n        tokenizer = PretrainedTransformerTokenizer(""bert-base-cased"")\n\n        sentence = ""A, [MASK] AllenNLP sentence."".split("" "")\n        expected_tokens = [\n            ""[CLS]"",\n            ""A"",\n            "","",\n            ""[MASK]"",\n            ""Allen"",\n            ""##NL"",\n            ""##P"",\n            ""sentence"",\n            ""."",\n            ""[SEP]"",\n        ]\n        expected_offsets = [(1, 2), (3, 3), (4, 6), (7, 8)]\n        tokens, offsets = tokenizer.intra_word_tokenize(sentence)\n        tokens = [t.text for t in tokens]\n        assert tokens == expected_tokens\n        assert offsets == expected_offsets\n\n        # sentence pair\n        sentence_1 = ""A, [MASK] AllenNLP sentence."".split("" "")\n        sentence_2 = ""A sentence."".split("" "")\n        expected_tokens = [\n            ""[CLS]"",\n            ""A"",\n            "","",\n            ""[MASK]"",\n            ""Allen"",\n            ""##NL"",\n            ""##P"",\n            ""sentence"",\n            ""."",\n            ""[SEP]"",\n            ""A"",  # 10\n            ""sentence"",\n            ""."",\n            ""[SEP]"",\n        ]\n        expected_offsets_a = [(1, 2), (3, 3), (4, 6), (7, 8)]\n        expected_offsets_b = [(10, 10), (11, 12)]\n        tokens, offsets_a, offsets_b = tokenizer.intra_word_tokenize_sentence_pair(\n            sentence_1, sentence_2\n        )\n        tokens = [t.text for t in tokens]\n        assert tokens == expected_tokens\n        assert offsets_a == expected_offsets_a\n        assert offsets_b == expected_offsets_b\n\n    def test_intra_word_tokenize_whitespaces(self):\n        tokenizer = PretrainedTransformerTokenizer(""bert-base-cased"")\n\n        sentence = [""A,"", "" "", ""[MASK]"", ""AllenNLP"", ""\\u007f"", ""sentence.""]\n        expected_tokens = [\n            ""[CLS]"",\n            ""A"",\n            "","",\n            ""[MASK]"",\n            ""Allen"",\n            ""##NL"",\n            ""##P"",\n            ""sentence"",\n            ""."",\n            ""[SEP]"",\n        ]\n        expected_offsets = [(1, 2), None, (3, 3), (4, 6), None, (7, 8)]\n        tokens, offsets = tokenizer.intra_word_tokenize(sentence)\n        tokens = [t.text for t in tokens]\n        assert tokens == expected_tokens\n        assert offsets == expected_offsets\n\n    def test_special_tokens_added(self):\n        def get_token_ids(tokens: Iterable[Token]) -> List[int]:\n            return [t.text_id for t in tokens]\n\n        def get_type_ids(tokens: Iterable[Token]) -> List[int]:\n            return [t.type_id for t in tokens]\n\n        tokenizer = PretrainedTransformerTokenizer(""bert-base-cased"")\n        assert get_token_ids(tokenizer.sequence_pair_start_tokens) == [101]\n        assert get_token_ids(tokenizer.sequence_pair_mid_tokens) == [102]\n        assert get_token_ids(tokenizer.sequence_pair_end_tokens) == [102]\n        assert get_token_ids(tokenizer.single_sequence_start_tokens) == [101]\n        assert get_token_ids(tokenizer.single_sequence_end_tokens) == [102]\n\n        assert get_type_ids(tokenizer.sequence_pair_start_tokens) == [0]\n        assert tokenizer.sequence_pair_first_token_type_id == 0\n        assert get_type_ids(tokenizer.sequence_pair_mid_tokens) == [0]\n        assert tokenizer.sequence_pair_second_token_type_id == 1\n        assert get_type_ids(tokenizer.sequence_pair_end_tokens) == [1]\n\n        assert get_type_ids(tokenizer.single_sequence_start_tokens) == [0]\n        assert tokenizer.single_sequence_token_type_id == 0\n        assert get_type_ids(tokenizer.single_sequence_end_tokens) == [0]\n\n        tokenizer = PretrainedTransformerTokenizer(""xlnet-base-cased"")\n        assert get_token_ids(tokenizer.sequence_pair_start_tokens) == []\n        assert get_token_ids(tokenizer.sequence_pair_mid_tokens) == [4]\n        assert get_token_ids(tokenizer.sequence_pair_end_tokens) == [4, 3]\n        assert get_token_ids(tokenizer.single_sequence_start_tokens) == []\n        assert get_token_ids(tokenizer.single_sequence_end_tokens) == [4, 3]\n\n        assert get_type_ids(tokenizer.sequence_pair_start_tokens) == []\n        assert tokenizer.sequence_pair_first_token_type_id == 0\n        assert get_type_ids(tokenizer.sequence_pair_mid_tokens) == [0]\n        assert tokenizer.sequence_pair_second_token_type_id == 1\n        assert get_type_ids(tokenizer.sequence_pair_end_tokens) == [1, 2]\n\n        assert get_type_ids(tokenizer.single_sequence_start_tokens) == []\n        assert tokenizer.single_sequence_token_type_id == 0\n        assert get_type_ids(tokenizer.single_sequence_end_tokens) == [0, 2]\n\n    def test_tokenizer_kwargs_default(self):\n        text = ""Hello there! General Kenobi.""\n        tokenizer = PretrainedTransformerTokenizer(""bert-base-cased"")\n        original_tokens = [\n            ""[CLS]"",\n            ""Hello"",\n            ""there"",\n            ""!"",\n            ""General"",\n            ""Ken"",\n            ""##ob"",\n            ""##i"",\n            ""."",\n            ""[SEP]"",\n        ]\n        tokenized = [token.text for token in tokenizer.tokenize(text)]\n        assert tokenized == original_tokens\n\n    def test_from_params_kwargs(self):\n        PretrainedTransformerTokenizer.from_params(\n            Params({""model_name"": ""bert-base-uncased"", ""tokenizer_kwargs"": {""max_len"": 10}})\n        )\n'"
tests/data/tokenizers/sentence_splitter_test.py,0,"b'from allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter\n\n\nclass TestSentenceSplitter(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.dep_parse_splitter = SpacySentenceSplitter(rule_based=False)\n        self.rule_based_splitter = SpacySentenceSplitter(rule_based=True)\n\n    def test_rule_based_splitter_passes_through_correctly(self):\n        text = ""This is the first sentence. This is the second sentence! ""\n        tokens = self.rule_based_splitter.split_sentences(text)\n        expected_tokens = [""This is the first sentence."", ""This is the second sentence!""]\n        assert tokens == expected_tokens\n\n    def test_dep_parse_splitter_passes_through_correctly(self):\n        text = ""This is the first sentence. This is the second sentence! ""\n        tokens = self.dep_parse_splitter.split_sentences(text)\n        expected_tokens = [""This is the first sentence."", ""This is the second sentence!""]\n        assert tokens == expected_tokens\n\n    def test_batch_rule_based_sentence_splitting(self):\n        text = [\n            ""This is a sentence. This is a second sentence."",\n            ""This isn\'t a sentence. This is a second sentence! This is a third sentence."",\n        ]\n        batch_split = self.rule_based_splitter.batch_split_sentences(text)\n        separately_split = [self.rule_based_splitter.split_sentences(doc) for doc in text]\n        assert len(batch_split) == len(separately_split)\n        for batch_doc, separate_doc in zip(batch_split, separately_split):\n            assert len(batch_doc) == len(separate_doc)\n            for batch_sentence, separate_sentence in zip(batch_doc, separate_doc):\n                assert batch_sentence == separate_sentence\n\n    def test_batch_dep_parse_sentence_splitting(self):\n        text = [\n            ""This is a sentence. This is a second sentence."",\n            ""This isn\'t a sentence. This is a second sentence! This is a third sentence."",\n        ]\n        batch_split = self.dep_parse_splitter.batch_split_sentences(text)\n        separately_split = [self.dep_parse_splitter.split_sentences(doc) for doc in text]\n        assert len(batch_split) == len(separately_split)\n        for batch_doc, separate_doc in zip(batch_split, separately_split):\n            assert len(batch_doc) == len(separate_doc)\n            for batch_sentence, separate_sentence in zip(batch_doc, separate_doc):\n                assert batch_sentence == separate_sentence\n'"
tests/data/tokenizers/spacy_tokenizer_test.py,0,"b'import spacy\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.tokenizers.token import Token\nfrom allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\n\n\nclass TestSpacyTokenizer(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.word_tokenizer = SpacyTokenizer()\n\n    def test_tokenize_handles_complex_punctuation(self):\n        sentence = ""this (sentence) has \'crazy\' \\""punctuation\\"".""\n        expected_tokens = [\n            ""this"",\n            ""("",\n            ""sentence"",\n            "")"",\n            ""has"",\n            ""\'"",\n            ""crazy"",\n            ""\'"",\n            \'""\',\n            ""punctuation"",\n            \'""\',\n            ""."",\n        ]\n        tokens = self.word_tokenizer.tokenize(sentence)\n        token_text = [t.text for t in tokens]\n        assert token_text == expected_tokens\n        for token in tokens:\n            start = token.idx\n            end = start + len(token.text)\n            assert sentence[start:end] == token.text\n\n    def test_tokenize_handles_contraction(self):\n        # note that ""would\'ve"" is kept together, while ""ain\'t"" is not.\n        sentence = ""it ain\'t joe\'s problem; would been yesterday""\n        expected_tokens = [\n            ""it"",\n            ""ai"",\n            ""n\'t"",\n            ""joe"",\n            ""\'s"",\n            ""problem"",\n            "";"",\n            ""would"",\n            ""been"",\n            ""yesterday"",\n        ]\n        tokens = [t.text for t in self.word_tokenizer.tokenize(sentence)]\n        assert tokens == expected_tokens\n\n    def test_tokenize_handles_multiple_contraction(self):\n        sentence = ""wouldn\'t\'ve""\n        expected_tokens = [""would"", ""n\'t"", ""\'ve""]\n        tokens = [t.text for t in self.word_tokenizer.tokenize(sentence)]\n        assert tokens == expected_tokens\n\n    def test_tokenize_handles_final_apostrophe(self):\n        sentence = ""the jones\' house""\n        expected_tokens = [""the"", ""jones"", ""\'"", ""house""]\n        tokens = [t.text for t in self.word_tokenizer.tokenize(sentence)]\n        assert tokens == expected_tokens\n\n    def test_tokenize_removes_whitespace_tokens(self):\n        sentence = ""the\\n jones\'   house  \\x0b  55""\n        expected_tokens = [""the"", ""jones"", ""\'"", ""house"", ""55""]\n        tokens = [t.text for t in self.word_tokenizer.tokenize(sentence)]\n        assert tokens == expected_tokens\n\n    def test_tokenize_handles_special_cases(self):\n        # note that the etc. doesn\'t quite work --- we can special case this if we want.\n        sentence = ""Mr. and Mrs. Jones, etc., went to, e.g., the store""\n        expected_tokens = [\n            ""Mr."",\n            ""and"",\n            ""Mrs."",\n            ""Jones"",\n            "","",\n            ""etc"",\n            ""."",\n            "","",\n            ""went"",\n            ""to"",\n            "","",\n            ""e.g."",\n            "","",\n            ""the"",\n            ""store"",\n        ]\n        tokens = [t.text for t in self.word_tokenizer.tokenize(sentence)]\n        assert tokens == expected_tokens\n\n    def test_batch_tokenization(self):\n        sentences = [\n            ""This is     a sentence"",\n            ""This isn\'t a sentence."",\n            ""This is the 3rd     sentence."" ""Here\'s the \'fourth\' sentence."",\n        ]\n        batch_split = self.word_tokenizer.batch_tokenize(sentences)\n        separately_split = [self.word_tokenizer.tokenize(sentence) for sentence in sentences]\n        assert len(batch_split) == len(separately_split)\n        for batch_sentence, separate_sentence in zip(batch_split, separately_split):\n            assert len(batch_sentence) == len(separate_sentence)\n            for batch_word, separate_word in zip(batch_sentence, separate_sentence):\n                assert batch_word.text == separate_word.text\n\n    def test_keep_spacy_tokens(self):\n        word_tokenizer = SpacyTokenizer()\n        sentence = ""This should be an allennlp Token""\n        tokens = word_tokenizer.tokenize(sentence)\n        assert tokens\n        assert all(isinstance(token, Token) for token in tokens)\n\n        word_tokenizer = SpacyTokenizer(keep_spacy_tokens=True)\n        sentence = ""This should be a spacy Token""\n        tokens = word_tokenizer.tokenize(sentence)\n        assert tokens\n        assert all(isinstance(token, spacy.tokens.Token) for token in tokens)\n'"
tests/modules/attention/__init__.py,0,b''
tests/modules/attention/additive_attention_test.py,6,"b'from numpy.testing import assert_almost_equal\nimport torch\nfrom torch.nn.parameter import Parameter\n\nfrom allennlp.common import Params\nfrom allennlp.modules.attention import AdditiveAttention\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestAdditiveAttention(AllenNlpTestCase):\n    def test_forward_does_an_additive_product(self):\n        params = Params({""vector_dim"": 2, ""matrix_dim"": 3, ""normalize"": False})\n        additive = AdditiveAttention.from_params(params)\n        additive._w_matrix = Parameter(torch.Tensor([[-0.2, 0.3], [-0.5, 0.5]]))\n        additive._u_matrix = Parameter(torch.Tensor([[0.0, 1.0], [1.0, 1.0], [1.0, -1.0]]))\n        additive._v_vector = Parameter(torch.Tensor([[1.0], [-1.0]]))\n        vectors = torch.FloatTensor([[0.7, -0.8], [0.4, 0.9]])\n        matrices = torch.FloatTensor(\n            [\n                [[1.0, -1.0, 3.0], [0.5, -0.3, 0.0], [0.2, -1.0, 1.0], [0.7, 0.8, -1.0]],\n                [[-2.0, 3.0, -3.0], [0.6, 0.2, 2.0], [0.5, -0.4, -1.0], [0.2, 0.2, 0.0]],\n            ]\n        )\n        result = additive(vectors, matrices).detach().numpy()\n        assert result.shape == (2, 4)\n        assert_almost_equal(\n            result,\n            [\n                [1.975072, -0.04997836, 1.2176098, -0.9205586],\n                [-1.4851665, 1.489604, -1.890285, -1.0672251],\n            ],\n        )\n'"
tests/modules/attention/bilinear_attention_test.py,5,"b'from numpy.testing import assert_almost_equal\nimport torch\nfrom torch.nn.parameter import Parameter\n\nfrom allennlp.common import Params\nfrom allennlp.modules.attention import BilinearAttention\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestBilinearAttention(AllenNlpTestCase):\n    def test_forward_does_a_bilinear_product(self):\n        params = Params({""vector_dim"": 2, ""matrix_dim"": 2, ""normalize"": False})\n        bilinear = BilinearAttention.from_params(params)\n        bilinear._weight_matrix = Parameter(torch.FloatTensor([[-0.3, 0.5], [2.0, -1.0]]))\n        bilinear._bias = Parameter(torch.FloatTensor([0.1]))\n        a_vectors = torch.FloatTensor([[1, 1]])\n        b_vectors = torch.FloatTensor([[[1, 0], [0, 1]]])\n        result = bilinear(a_vectors, b_vectors).detach().numpy()\n        assert result.shape == (1, 2)\n        assert_almost_equal(result, [[1.8, -0.4]])\n'"
tests/modules/attention/cosine_attention_test.py,2,"b'import torch\nfrom numpy.testing import assert_almost_equal\nimport numpy\n\nfrom allennlp.common import Params\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.modules.attention.attention import Attention\nfrom allennlp.modules.attention.cosine_attention import CosineAttention\n\n\nclass TestCosineAttention(AllenNlpTestCase):\n    def test_can_init_cosine(self):\n        legacy_attention = Attention.from_params(Params({""type"": ""cosine""}))\n        isinstance(legacy_attention, CosineAttention)\n\n    def test_cosine_similarity(self):\n        linear = CosineAttention(normalize=False)\n        output = linear(\n            torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),\n            torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]),\n        )\n\n        assert_almost_equal(output.numpy(), numpy.array([[0.0, 0.0], [0.9948, 0.9973]]), decimal=2)\n'"
tests/modules/attention/dot_product_attention_test.py,2,"b'import torch\nfrom numpy.testing import assert_almost_equal\nimport numpy\n\nfrom allennlp.common import Params\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.modules.attention.attention import Attention\nfrom allennlp.modules.attention.dot_product_attention import DotProductAttention\n\n\nclass TestDotProductAttention(AllenNlpTestCase):\n    def test_can_init_dot(self):\n        legacy_attention = Attention.from_params(Params({""type"": ""dot_product""}))\n        isinstance(legacy_attention, DotProductAttention)\n\n    def test_dot_product_similarity(self):\n        linear = DotProductAttention(normalize=False)\n        output = linear(\n            torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),\n            torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]),\n        )\n\n        assert_almost_equal(output.numpy(), numpy.array([[0.0, 0.0], [24.0, 33.0]]), decimal=2)\n'"
tests/modules/attention/linear_attention_test.py,9,"b'from numpy.testing import assert_almost_equal\nimport numpy\nimport torch\nfrom torch.autograd import Variable\nfrom torch.nn import Parameter\n\nfrom allennlp.common import Params\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.modules.attention import LinearAttention\nfrom allennlp.modules.attention.attention import Attention\n\n\nclass LinearAttentionTest(AllenNlpTestCase):\n    def test_can_init_linear(self):\n        legacy_attention = Attention.from_params(\n            Params({""type"": ""linear"", ""tensor_1_dim"": 3, ""tensor_2_dim"": 3})\n        )\n        isinstance(legacy_attention, LinearAttention)\n\n    def test_linear_similarity(self):\n        linear = LinearAttention(3, 3, normalize=True)\n        linear._weight_vector = Parameter(torch.FloatTensor([-0.3, 0.5, 2.0, -1.0, 1, 1]))\n        linear._bias = Parameter(torch.FloatTensor([0.1]))\n        output = linear(\n            Variable(torch.FloatTensor([[-7, -8, -9]])),\n            Variable(torch.FloatTensor([[[1, 2, 3], [4, 5, 6]]])),\n        )\n\n        assert_almost_equal(output.data.numpy(), numpy.array([[0.0474, 0.9526]]), decimal=2)\n\n    def test_bidaf_trilinear_similarity(self):\n        linear = LinearAttention(2, 2, combination=""x,y,x*y"", normalize=False)\n        linear._weight_vector = Parameter(torch.FloatTensor([-0.3, 0.5, 2.0, -1.0, 1, 1]))\n        linear._bias = Parameter(torch.FloatTensor([0.0]))\n        output = linear(\n            torch.FloatTensor([[4, 5]]), torch.FloatTensor([[[1, 2], [4, 5], [7, 8], [10, 11]]])\n        )\n\n        assert_almost_equal(\n            output.data.numpy(),\n            numpy.array(\n                [\n                    [\n                        -1.2 + 2.5 + 2 + -2 + 4 + 10,\n                        -1.2 + 2.5 + 8 + -5 + 16 + 25,\n                        -1.2 + 2.5 + 14 + -8 + 28 + 40,\n                        -1.2 + 2.5 + 20 + -11 + 40 + 55,\n                    ]\n                ]\n            ),\n            decimal=2,\n        )\n'"
tests/modules/matrix_attention/__init__.py,0,b''
tests/modules/matrix_attention/bilinear_matrix_attention_test.py,9,"b'from numpy.testing import assert_almost_equal\nimport torch\nfrom torch.nn.parameter import Parameter\n\nfrom allennlp.common import Params\nfrom allennlp.modules.matrix_attention import BilinearMatrixAttention\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestBilinearMatrixAttention(AllenNlpTestCase):\n    def test_forward_does_a_bilinear_product(self):\n        params = Params({""matrix_1_dim"": 2, ""matrix_2_dim"": 2})\n        bilinear = BilinearMatrixAttention.from_params(params)\n        bilinear._weight_matrix = Parameter(torch.FloatTensor([[-0.3, 0.5], [2.0, -1.0]]))\n        bilinear._bias = Parameter(torch.FloatTensor([0.1]))\n        a_vectors = torch.FloatTensor([[[1, 1], [2, 2]]])\n        b_vectors = torch.FloatTensor([[[1, 0], [0, 1]]])\n        result = bilinear(a_vectors, b_vectors).detach().numpy()\n        assert result.shape == (1, 2, 2)\n        assert_almost_equal(result, [[[1.8, -0.4], [3.5, -0.9]]])\n\n    def test_forward_does_a_bilinear_product_when_using_biases(self):\n        params = Params({""matrix_1_dim"": 2, ""matrix_2_dim"": 2, ""use_input_biases"": True})\n        bilinear = BilinearMatrixAttention.from_params(params)\n        bilinear._weight_matrix = Parameter(\n            torch.FloatTensor([[-0.3, 0.5, 1.0], [2.0, -1.0, -1.0], [1.0, 0.5, 1.0]])\n        )\n        bilinear._bias = Parameter(torch.FloatTensor([0.1]))\n        a_vectors = torch.FloatTensor([[[1, 1], [2, 2]]])\n        b_vectors = torch.FloatTensor([[[1, 0], [0, 1]]])\n        result = bilinear(a_vectors, b_vectors).detach().numpy()\n        assert result.shape == (1, 2, 2)\n        assert_almost_equal(result, [[[3.8, 1.1], [5.5, 0.6]]])\n'"
tests/modules/matrix_attention/cosine_matrix_attention_test.py,2,"b'import torch\nfrom numpy.testing import assert_almost_equal\nimport numpy\n\nfrom allennlp.common import Params\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.modules.matrix_attention import CosineMatrixAttention\nfrom allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\n\n\nclass TestCosineMatrixAttention(AllenNlpTestCase):\n    def test_can_init_cosine(self):\n        legacy_attention = MatrixAttention.from_params(Params({""type"": ""cosine""}))\n        isinstance(legacy_attention, CosineMatrixAttention)\n\n    def test_cosine_similarity(self):\n        # example use case: a batch of size 2.\n        # With a time element component (e.g. sentences of length 2) each word is a vector of length 3.\n        # It is comparing this with another input of the same type\n        output = CosineMatrixAttention()(\n            torch.FloatTensor([[[0, 0, 0], [4, 5, 6]], [[-7, -8, -9], [10, 11, 12]]]),\n            torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]),\n        )\n\n        # For the first batch there is\n        #       no correlation between the first words of the input matrix\n        #       but perfect correlation for the second word\n        # For the second batch there is\n        #     negative correlation for the first words\n        #     correlation for the second word\n        assert_almost_equal(\n            output.numpy(), numpy.array([[[0, 0], [0.97, 1]], [[-1, -0.99], [0.99, 1]]]), decimal=2\n        )\n'"
tests/modules/matrix_attention/dot_product_matrix_attention_test.py,2,"b'import torch\nfrom numpy.testing import assert_almost_equal\nimport numpy\n\nfrom allennlp.common import Params\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.modules.matrix_attention import DotProductMatrixAttention\nfrom allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\n\n\nclass TestDotProductMatrixAttention(AllenNlpTestCase):\n    def test_can_init_dot(self):\n        legacy_attention = MatrixAttention.from_params(Params({""type"": ""dot_product""}))\n        isinstance(legacy_attention, DotProductMatrixAttention)\n\n    def test_dot_product_similarity(self):\n        # example use case: a batch of size 2,\n        # with a time element component (e.g. sentences of length 2) each word is a vector of length 3.\n        # it is comparing this with another input of the same type\n        output = DotProductMatrixAttention()(\n            torch.FloatTensor([[[0, 0, 0], [4, 5, 6]], [[-7, -8, -9], [10, 11, 12]]]),\n            torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]),\n        )\n\n        # for the first batch there is\n        #       no correlation between the first words of the input matrix\n        #       but perfect correlation for the second word\n        # for the second batch there is\n        #       negative correlation for the first words\n        #       a correlation for the second word\n        assert_almost_equal(\n            output.numpy(), numpy.array([[[0, 0], [32, 77]], [[-194, -266], [266, 365]]]), decimal=2\n        )\n'"
tests/modules/matrix_attention/linear_matrix_attention_test.py,9,"b'import numpy\nfrom numpy.testing import assert_almost_equal\nimport torch\nfrom torch.nn import Parameter\n\nfrom allennlp.common import Params\nfrom allennlp.common.testing.test_case import AllenNlpTestCase\nfrom allennlp.modules.matrix_attention import LinearMatrixAttention\nfrom allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\n\n\nclass TestLinearMatrixAttention(AllenNlpTestCase):\n    def test_can_init_dot(self):\n        legacy_attention = MatrixAttention.from_params(\n            Params({""type"": ""linear"", ""tensor_1_dim"": 3, ""tensor_2_dim"": 3})\n        )\n        isinstance(legacy_attention, LinearMatrixAttention)\n\n    def test_linear_similarity(self):\n        linear = LinearMatrixAttention(3, 3)\n        linear._weight_vector = Parameter(torch.FloatTensor([-0.3, 0.5, 2.0, -1.0, 1, 1]))\n        linear._bias = Parameter(torch.FloatTensor([0.1]))\n        output = linear(\n            torch.FloatTensor([[[0, 0, 0], [4, 5, 6]], [[-7, -8, -9], [10, 11, 12]]]),\n            torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]),\n        )\n\n        assert_almost_equal(\n            output.data.numpy(),\n            numpy.array(\n                [[[4.1000, 7.1000], [17.4000, 20.4000]], [[-9.8000, -6.8000], [36.6000, 39.6000]]]\n            ),\n            decimal=2,\n        )\n\n    def test_bidaf_trilinear_similarity(self):\n        linear = LinearMatrixAttention(2, 2, combination=""x,y,x*y"")\n        linear._weight_vector = Parameter(torch.FloatTensor([-0.3, 0.5, 2.0, -1.0, 1, 1]))\n        linear._bias = Parameter(torch.FloatTensor([0.0]))\n        output = linear(\n            torch.FloatTensor([[[0, 0], [4, 5]], [[-7, -8], [10, 11]]]),\n            torch.FloatTensor([[[1, 2], [4, 5]], [[7, 8], [10, 11]]]),\n        )\n\n        assert_almost_equal(\n            output.data.numpy(),\n            numpy.array(\n                [\n                    [\n                        [0 + 0 + 2 + -2 + 0 + 0, 0 + 0 + 8 + -5 + 0 + 0],\n                        [-1.2 + 2.5 + 2 + -2 + 4 + 10, -1.2 + 2.5 + 8 + -5 + 16 + 25],\n                    ],\n                    [\n                        [2.1 + -4 + 14 + -8 + -49 + -64, 2.1 + -4 + 20 + -11 + -70 + -88],\n                        [-3 + 5.5 + 14 + -8 + 70 + 88, -3 + 5.5 + 20 + -11 + 100 + 121],\n                    ],\n                ]\n            ),\n            decimal=2,\n        )\n'"
tests/modules/seq2seq_encoders/__init__.py,0,b''
tests/modules/seq2seq_encoders/compose_encoder_test.py,4,"b'import torch\nimport numpy\nfrom overrides import overrides\nimport pytest\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules.seq2seq_encoders import ComposeEncoder, FeedForwardEncoder, Seq2SeqEncoder\nfrom allennlp.modules import FeedForward\n\n\nclass MockSeq2SeqEncoder(Seq2SeqEncoder):\n    def __init__(self, input_dim: int, output_dim: int, bidirectional: bool = False):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.bidirectional = bidirectional\n\n    @overrides\n    def forward(self, inputs, mask):\n        pass\n\n    @overrides\n    def get_input_dim(self) -> int:\n        return self.input_dim\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self.output_dim\n\n    @overrides\n    def is_bidirectional(self) -> bool:\n        return self.bidirectional\n\n\ndef _make_feedforward(input_dim, output_dim):\n    return FeedForwardEncoder(\n        FeedForward(\n            input_dim=input_dim, num_layers=1, activations=torch.nn.ReLU(), hidden_dims=output_dim\n        )\n    )\n\n\nclass TestPassThroughEncoder(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.encoder = ComposeEncoder(\n            [_make_feedforward(9, 5), _make_feedforward(5, 10), _make_feedforward(10, 3)]\n        )\n\n    def test_get_dimension_is_correct(self):\n        assert self.encoder.get_input_dim() == 9\n        assert self.encoder.get_output_dim() == 3\n\n    def test_composes(self):\n        tensor = torch.zeros(2, 10, 9)\n        output = self.encoder(tensor)\n\n        for encoder in self.encoder.encoders:\n            tensor = encoder(tensor)\n\n        numpy.testing.assert_array_almost_equal(\n            output.detach().cpu().numpy(), tensor.detach().cpu().numpy()\n        )\n\n    def test_pass_through_encoder_with_mask(self):\n        tensor = torch.randn([2, 3, 9])\n        mask = torch.tensor([[True, True, True], [True, False, False]])\n        output = self.encoder(tensor, mask)\n\n        for encoder in self.encoder.encoders:\n            tensor = encoder(tensor, mask)\n\n        numpy.testing.assert_array_almost_equal(\n            output.detach().cpu().numpy(), tensor.detach().cpu().numpy()\n        )\n\n    def test_empty(self):\n        with pytest.raises(ValueError):\n            ComposeEncoder([])\n\n    def test_mismatched_size(self):\n        with pytest.raises(ValueError):\n            ComposeEncoder(\n                [\n                    MockSeq2SeqEncoder(input_dim=9, output_dim=5),\n                    MockSeq2SeqEncoder(input_dim=1, output_dim=2),\n                ]\n            )\n\n    def test_mismatched_bidirectionality(self):\n        with pytest.raises(ValueError):\n            ComposeEncoder(\n                [\n                    MockSeq2SeqEncoder(input_dim=9, output_dim=5),\n                    MockSeq2SeqEncoder(input_dim=5, output_dim=2, bidirectional=True),\n                ]\n            )\n\n    def test_all_bidirectional(self):\n        ComposeEncoder(\n            [\n                MockSeq2SeqEncoder(input_dim=9, output_dim=5, bidirectional=True),\n                MockSeq2SeqEncoder(input_dim=5, output_dim=2, bidirectional=True),\n            ]\n        )\n'"
tests/modules/seq2seq_encoders/feedforward_encoder_test.py,2,"b'import torch\nimport numpy\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules import FeedForward\nfrom allennlp.modules.seq2seq_encoders.feedforward_encoder import FeedForwardEncoder\nfrom allennlp.nn import Activation\n\n\nclass TestFeedforwardEncoder(AllenNlpTestCase):\n    def test_get_dimension_is_correct(self):\n        feedforward = FeedForward(\n            input_dim=10, num_layers=1, hidden_dims=10, activations=Activation.by_name(""linear"")()\n        )\n        encoder = FeedForwardEncoder(feedforward)\n        assert encoder.get_input_dim() == feedforward.get_input_dim()\n        assert encoder.get_output_dim() == feedforward.get_output_dim()\n\n    def test_feedforward_encoder_exactly_match_feedforward_each_item(self):\n        feedforward = FeedForward(\n            input_dim=10, num_layers=1, hidden_dims=10, activations=Activation.by_name(""linear"")()\n        )\n        encoder = FeedForwardEncoder(feedforward)\n        tensor = torch.randn([2, 3, 10])\n        output = encoder(tensor)\n        target = feedforward(tensor)\n        numpy.testing.assert_array_almost_equal(\n            target.detach().cpu().numpy(), output.detach().cpu().numpy()\n        )\n\n        # mask should work\n        mask = torch.tensor([[True, True, True], [True, False, False]])\n        output = encoder(tensor, mask)\n        target = feedforward(tensor) * mask.unsqueeze(dim=-1).float()\n        numpy.testing.assert_array_almost_equal(\n            target.detach().cpu().numpy(), output.detach().cpu().numpy()\n        )\n'"
tests/modules/seq2seq_encoders/gated_cnn_encoder_test.py,7,"b'import torch\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules.seq2seq_encoders.gated_cnn_encoder import GatedCnnEncoder\n\n\nclass TestGatedCnnEncoder(AllenNlpTestCase):\n    def test_gated_cnn_encoder(self):\n        cnn_encoder = GatedCnnEncoder(\n            input_dim=32,\n            layers=[[[4, 32]], [[1, 16], [5, 16], [1, 32]], [[1, 64], [5, 64], [1, 32]]],\n        )\n\n        token_embeddings = torch.rand(5, 10, 32)\n        mask = torch.ones(5, 10).bool()\n        mask[0, 7:] = False\n        mask[1, 5:] = False\n\n        output = cnn_encoder(token_embeddings, mask)\n        assert list(output.size()) == [5, 10, 64]\n\n    def test_gated_cnn_encoder_dilations(self):\n        cnn_encoder = GatedCnnEncoder(\n            input_dim=32, layers=[[[2, 32, 1]], [[2, 32, 2]], [[2, 32, 4]], [[2, 32, 8]]]\n        )\n\n        token_embeddings = torch.rand(5, 10, 32)\n        mask = torch.ones(5, 10).bool()\n        mask[0, 7:] = False\n        mask[1, 5:] = False\n\n        output = cnn_encoder(token_embeddings, mask)\n        assert list(output.size()) == [5, 10, 64]\n\n    def test_gated_cnn_encoder_layers(self):\n        cnn_encoder = GatedCnnEncoder(\n            input_dim=32,\n            layers=[[[4, 32]], [[1, 16], [5, 16], [1, 32]], [[1, 64], [5, 64], [1, 32]]],\n            return_all_layers=True,\n        )\n\n        token_embeddings = torch.rand(5, 10, 32)\n        mask = torch.ones(5, 10).bool()\n        mask[0, 7:] = False\n        mask[1, 5:] = False\n\n        output = cnn_encoder(token_embeddings, mask)\n        assert len(output) == 3\n        concat_layers = torch.cat([layer.unsqueeze(1) for layer in output], dim=1)\n        assert list(concat_layers.size()) == [5, 3, 10, 64]\n'"
tests/modules/seq2seq_encoders/pass_through_encoder_test.py,3,"b'import torch\nimport numpy\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules.seq2seq_encoders import PassThroughEncoder\n\n\nclass TestPassThroughEncoder(AllenNlpTestCase):\n    def test_get_dimension_is_correct(self):\n        encoder = PassThroughEncoder(input_dim=9)\n        assert encoder.get_input_dim() == 9\n        assert encoder.get_output_dim() == 9\n\n    def test_pass_through_encoder_passes_through(self):\n        encoder = PassThroughEncoder(input_dim=9)\n        tensor = torch.randn([2, 3, 9])\n        output = encoder(tensor)\n        numpy.testing.assert_array_almost_equal(\n            tensor.detach().cpu().numpy(), output.detach().cpu().numpy()\n        )\n\n    def test_pass_through_encoder_with_mask(self):\n        encoder = PassThroughEncoder(input_dim=9)\n        tensor = torch.randn([2, 3, 9])\n        mask = torch.tensor([[True, True, True], [True, False, False]])\n        output = encoder(tensor, mask)\n\n        target = tensor * mask.unsqueeze(dim=-1).float()\n        numpy.testing.assert_array_almost_equal(\n            output.detach().cpu().numpy(), target.detach().cpu().numpy()\n        )\n'"
tests/modules/seq2seq_encoders/pytorch_seq2seq_wrapper_test.py,20,"b'import numpy\nfrom numpy.testing import assert_almost_equal\nimport pytest\nimport torch\nfrom torch.nn import LSTM, GRU\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\nfrom allennlp.nn.util import sort_batch_by_length, get_lengths_from_binary_sequence_mask\n\n\nclass TestPytorchSeq2SeqWrapper(AllenNlpTestCase):\n    def test_get_dimension_is_correct(self):\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=2, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2SeqWrapper(lstm)\n        assert encoder.get_output_dim() == 14\n        assert encoder.get_input_dim() == 2\n        lstm = LSTM(\n            bidirectional=False, num_layers=3, input_size=2, hidden_size=7, batch_first=True\n        )\n        encoder = PytorchSeq2SeqWrapper(lstm)\n        assert encoder.get_output_dim() == 7\n        assert encoder.get_input_dim() == 2\n\n    def test_forward_works_even_with_empty_sequences(self):\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=3, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2SeqWrapper(lstm)\n\n        tensor = torch.rand([5, 7, 3])\n        tensor[1, 6:, :] = 0\n        tensor[2, :, :] = 0\n        tensor[3, 2:, :] = 0\n        tensor[4, :, :] = 0\n        mask = torch.ones(5, 7).bool()\n        mask[1, 6:] = False\n        mask[2, :] = False\n        mask[3, 2:] = False\n        mask[4, :] = False\n\n        results = encoder(tensor, mask)\n\n        for i in (0, 1, 3):\n            assert not (results[i] == 0.0).data.all()\n        for i in (2, 4):\n            assert (results[i] == 0.0).data.all()\n\n    def test_forward_pulls_out_correct_tensor_without_sequence_lengths(self):\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=2, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2SeqWrapper(lstm)\n        input_tensor = torch.FloatTensor([[[0.7, 0.8], [0.1, 1.5]]])\n        lstm_output = lstm(input_tensor)\n        encoder_output = encoder(input_tensor, None)\n        assert_almost_equal(encoder_output.data.numpy(), lstm_output[0].data.numpy())\n\n    def test_forward_pulls_out_correct_tensor_with_sequence_lengths(self):\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=3, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2SeqWrapper(lstm)\n        input_tensor = torch.rand([5, 7, 3])\n        input_tensor[1, 6:, :] = 0\n        input_tensor[2, 4:, :] = 0\n        input_tensor[3, 2:, :] = 0\n        input_tensor[4, 1:, :] = 0\n        mask = torch.ones(5, 7).bool()\n        mask[1, 6:] = False\n        mask[2, 4:] = False\n        mask[3, 2:] = False\n        mask[4, 1:] = False\n\n        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n        packed_sequence = pack_padded_sequence(\n            input_tensor, sequence_lengths.data.tolist(), batch_first=True\n        )\n        lstm_output, _ = lstm(packed_sequence)\n        encoder_output = encoder(input_tensor, mask)\n        lstm_tensor, _ = pad_packed_sequence(lstm_output, batch_first=True)\n        assert_almost_equal(encoder_output.data.numpy(), lstm_tensor.data.numpy())\n\n    def test_forward_pulls_out_correct_tensor_for_unsorted_batches(self):\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=3, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2SeqWrapper(lstm)\n        input_tensor = torch.rand([5, 7, 3])\n        input_tensor[0, 3:, :] = 0\n        input_tensor[1, 4:, :] = 0\n        input_tensor[2, 2:, :] = 0\n        input_tensor[3, 6:, :] = 0\n        mask = torch.ones(5, 7).bool()\n        mask[0, 3:] = False\n        mask[1, 4:] = False\n        mask[2, 2:] = False\n        mask[3, 6:] = False\n\n        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n        sorted_inputs, sorted_sequence_lengths, restoration_indices, _ = sort_batch_by_length(\n            input_tensor, sequence_lengths\n        )\n        packed_sequence = pack_padded_sequence(\n            sorted_inputs, sorted_sequence_lengths.data.tolist(), batch_first=True\n        )\n        lstm_output, _ = lstm(packed_sequence)\n        encoder_output = encoder(input_tensor, mask)\n        lstm_tensor, _ = pad_packed_sequence(lstm_output, batch_first=True)\n        assert_almost_equal(\n            encoder_output.data.numpy(),\n            lstm_tensor.index_select(0, restoration_indices).data.numpy(),\n        )\n\n    def test_forward_does_not_compress_tensors_padded_to_greater_than_the_max_sequence_length(self):\n\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=3, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2SeqWrapper(lstm)\n        input_tensor = torch.rand([5, 8, 3])\n        input_tensor[:, 7, :] = 0\n        mask = torch.ones(5, 8).bool()\n        mask[:, 7] = False\n\n        encoder_output = encoder(input_tensor, mask)\n        assert encoder_output.size(1) == 8\n\n    def test_wrapper_raises_if_batch_first_is_false(self):\n\n        with pytest.raises(ConfigurationError):\n            lstm = LSTM(bidirectional=True, num_layers=3, input_size=3, hidden_size=7)\n            _ = PytorchSeq2SeqWrapper(lstm)\n\n    def test_wrapper_works_when_passed_state_with_zero_length_sequences(self):\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=3, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2SeqWrapper(lstm)\n        input_tensor = torch.rand([5, 7, 3])\n        mask = torch.ones(5, 7).bool()\n        mask[0, 3:] = False\n        mask[1, 4:] = False\n        mask[2, 0:] = False\n        mask[3, 6:] = False\n\n        # Initial states are of shape (num_layers * num_directions, batch_size, hidden_dim)\n        initial_states = torch.randn(6, 5, 7), torch.randn(6, 5, 7)\n\n        _ = encoder(input_tensor, mask, initial_states)\n\n    def test_wrapper_can_call_backward_with_zero_length_sequences(self):\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=3, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2SeqWrapper(lstm)\n        input_tensor = torch.rand([5, 7, 3])\n        mask = torch.ones(5, 7).bool()\n        mask[0, 3:] = False\n        mask[1, 4:] = False\n        mask[2, 0:] = 0  # zero length False\n        mask[3, 6:] = False\n\n        output = encoder(input_tensor, mask)\n\n        output.sum().backward()\n\n    def test_wrapper_stateful(self):\n        lstm = LSTM(bidirectional=True, num_layers=2, input_size=3, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2SeqWrapper(lstm, stateful=True)\n\n        # To test the stateful functionality we need to call the encoder multiple times.\n        # Different batch sizes further tests some of the logic.\n        batch_sizes = [5, 10, 8]\n        sequence_lengths = [4, 6, 7]\n        states = []\n        for batch_size, sequence_length in zip(batch_sizes, sequence_lengths):\n            tensor = torch.rand([batch_size, sequence_length, 3])\n            mask = torch.ones(batch_size, sequence_length).bool()\n            mask.data[0, 3:] = 0\n            encoder_output = encoder(tensor, mask)\n            states.append(encoder._states)\n\n        # Check that the output is masked properly.\n        assert_almost_equal(encoder_output[0, 3:, :].data.numpy(), numpy.zeros((4, 14)))\n\n        for k in range(2):\n            assert_almost_equal(\n                states[-1][k][:, -2:, :].data.numpy(), states[-2][k][:, -2:, :].data.numpy()\n            )\n\n    def test_wrapper_stateful_single_state_gru(self):\n        gru = GRU(bidirectional=True, num_layers=2, input_size=3, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2SeqWrapper(gru, stateful=True)\n\n        batch_sizes = [10, 5]\n        states = []\n        for batch_size in batch_sizes:\n            tensor = torch.rand([batch_size, 5, 3])\n            mask = torch.ones(batch_size, 5).bool()\n            mask.data[0, 3:] = 0\n            encoder_output = encoder(tensor, mask)\n            states.append(encoder._states)\n\n        assert_almost_equal(encoder_output[0, 3:, :].data.numpy(), numpy.zeros((2, 14)))\n        assert_almost_equal(\n            states[-1][0][:, -5:, :].data.numpy(), states[-2][0][:, -5:, :].data.numpy()\n        )\n'"
tests/modules/seq2seq_encoders/pytorch_transformer_wrapper_test.py,24,"b'from typing import Optional\nimport torch\nimport pytest\n\nfrom allennlp.modules.seq2seq_encoders import PytorchTransformer\n\n\n@pytest.mark.parametrize(""positional_encoding"", [None, ""sinusoidal"", ""embedding""])\ndef test_positional_embeddings(positional_encoding: Optional[str]):\n    # All sizes are prime, making them easy to find during debugging.\n    batch_size = 7\n    max_seq_len = 101\n    n_head = 5\n    dims = 11 * n_head\n    transformer = PytorchTransformer(\n        dims, 3, positional_encoding=positional_encoding, num_attention_heads=n_head\n    )\n    transformer.eval()\n\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b :] = False\n\n        assert not torch.isnan(inputs).any()\n        assert torch.isfinite(inputs).all()\n        outputs = transformer(inputs, mask)\n        assert outputs.size() == inputs.size()\n        assert not torch.isnan(outputs).any()\n        assert torch.isfinite(outputs).all()\n\n\n@pytest.mark.parametrize(""positional_encoding"", [None, ""sinusoidal"", ""embedding""])\ndef test_mask_works(positional_encoding: Optional[str]):\n    # All sizes are prime, making them easy to find during debugging.\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(\n        dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head\n    )\n    transformer.eval()\n\n    with torch.no_grad():\n        # Construct inputs and masks\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        all_ones_mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        mask = all_ones_mask.clone()\n        for b in range(batch_size):\n            mask[b, max_seq_len - b :] = False\n        altered_inputs = inputs + (~mask).unsqueeze(2) * 10.0\n\n        # Make sure there is a difference without the mask\n        assert not torch.allclose(\n            transformer(inputs, all_ones_mask), transformer(altered_inputs, all_ones_mask)\n        )\n\n        # Make sure there is no difference with the mask\n        assert torch.allclose(\n            torch.masked_select(transformer(inputs, mask), mask.unsqueeze(2)),\n            torch.masked_select(transformer(altered_inputs, mask), mask.unsqueeze(2)),\n        )\n\n\n@pytest.mark.parametrize(""positional_encoding"", [None, ""sinusoidal"", ""embedding""])\ndef test_positional_encodings(positional_encoding: Optional[str]):\n    # All sizes are prime, making them easy to find during debugging.\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(\n        dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head\n    )\n    transformer.eval()\n\n    with torch.no_grad():\n        # We test this by running it twice, once with a shuffled sequence. The results should be the same if there\n        # is no positional encoding, and different otherwise.\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b :] = False\n        unshuffled_output = transformer(inputs, mask)\n\n        shuffle = torch.arange(0, max_seq_len).unsqueeze(0).expand_as(mask).clone()\n        for b in range(batch_size):\n            # Take care not to shuffle the masked values\n            perm = torch.randperm(max_seq_len - b)\n            shuffle[b, : max_seq_len - b] = shuffle[b, perm]\n        shuffle = shuffle.unsqueeze(2).expand_as(inputs)\n        shuffled_input = torch.gather(inputs, 1, shuffle)\n        shuffled_output = transformer(shuffled_input, mask)\n\n        if positional_encoding is None:\n            assert torch.allclose(\n                torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-7\n            )\n        else:\n            assert not torch.allclose(\n                torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-7\n            )\n'"
tests/modules/seq2vec_encoders/__init__.py,0,b''
tests/modules/seq2vec_encoders/boe_encoder_test.py,5,"b'import numpy\nfrom numpy.testing import assert_almost_equal\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestBagOfEmbeddingsEncoder(AllenNlpTestCase):\n    def test_get_dimension_is_correct(self):\n        encoder = BagOfEmbeddingsEncoder(embedding_dim=5)\n        assert encoder.get_input_dim() == 5\n        assert encoder.get_output_dim() == 5\n        encoder = BagOfEmbeddingsEncoder(embedding_dim=12)\n        assert encoder.get_input_dim() == 12\n        assert encoder.get_output_dim() == 12\n\n    def test_can_construct_from_params(self):\n        params = Params({""embedding_dim"": 5})\n        encoder = BagOfEmbeddingsEncoder.from_params(params)\n        assert encoder.get_input_dim() == 5\n        assert encoder.get_output_dim() == 5\n        params = Params({""embedding_dim"": 12, ""averaged"": True})\n        encoder = BagOfEmbeddingsEncoder.from_params(params)\n        assert encoder.get_input_dim() == 12\n        assert encoder.get_output_dim() == 12\n\n    def test_forward_does_correct_computation(self):\n        encoder = BagOfEmbeddingsEncoder(embedding_dim=2)\n        input_tensor = torch.FloatTensor(\n            [[[0.7, 0.8], [0.1, 1.5], [0.3, 0.6]], [[0.5, 0.3], [1.4, 1.1], [0.3, 0.9]]]\n        )\n        mask = torch.ByteTensor([[1, 1, 1], [1, 1, 0]])\n        encoder_output = encoder(input_tensor, mask)\n        assert_almost_equal(\n            encoder_output.data.numpy(),\n            numpy.asarray([[0.7 + 0.1 + 0.3, 0.8 + 1.5 + 0.6], [0.5 + 1.4, 0.3 + 1.1]]),\n        )\n\n    def test_forward_does_correct_computation_with_average(self):\n        encoder = BagOfEmbeddingsEncoder(embedding_dim=2, averaged=True)\n        input_tensor = torch.FloatTensor(\n            [\n                [[0.7, 0.8], [0.1, 1.5], [0.3, 0.6]],\n                [[0.5, 0.3], [1.4, 1.1], [0.3, 0.9]],\n                [[0.4, 0.3], [0.4, 0.3], [1.4, 1.7]],\n            ]\n        )\n        mask = torch.ByteTensor([[1, 1, 1], [1, 1, 0], [0, 0, 0]])\n        encoder_output = encoder(input_tensor, mask)\n        assert_almost_equal(\n            encoder_output.data.numpy(),\n            numpy.asarray(\n                [\n                    [(0.7 + 0.1 + 0.3) / 3, (0.8 + 1.5 + 0.6) / 3],\n                    [(0.5 + 1.4) / 2, (0.3 + 1.1) / 2],\n                    [0.0, 0.0],\n                ]\n            ),\n        )\n\n    def test_forward_does_correct_computation_with_average_no_mask(self):\n        encoder = BagOfEmbeddingsEncoder(embedding_dim=2, averaged=True)\n        input_tensor = torch.FloatTensor(\n            [[[0.7, 0.8], [0.1, 1.5], [0.3, 0.6]], [[0.5, 0.3], [1.4, 1.1], [0.3, 0.9]]]\n        )\n        encoder_output = encoder(input_tensor)\n        assert_almost_equal(\n            encoder_output.data.numpy(),\n            numpy.asarray(\n                [\n                    [(0.7 + 0.1 + 0.3) / 3, (0.8 + 1.5 + 0.6) / 3],\n                    [(0.5 + 1.4 + 0.3) / 3, (0.3 + 1.1 + 0.9) / 3],\n                ]\n            ),\n        )\n'"
tests/modules/seq2vec_encoders/cls_pooler_test.py,4,"b'import numpy\nimport torch\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules.seq2vec_encoders.cls_pooler import ClsPooler\n\n\nclass TestClsPooler(AllenNlpTestCase):\n    def test_encoder(self):\n        embedding = torch.rand(5, 50, 7)\n        encoder = ClsPooler(embedding_dim=7)\n        pooled = encoder(embedding, mask=None)\n\n        assert list(pooled.size()) == [5, 7]\n        numpy.testing.assert_array_almost_equal(embedding[:, 0], pooled)\n\n    def test_cls_at_end(self):\n        embedding = torch.arange(20).reshape(5, 4).unsqueeze(-1).expand(5, 4, 7)\n        mask = torch.tensor(\n            [\n                [True, True, True, True],\n                [True, True, True, False],\n                [True, True, True, True],\n                [True, False, False, False],\n                [True, True, False, False],\n            ]\n        )\n        expected = torch.LongTensor([3, 6, 11, 12, 17]).unsqueeze(-1).expand(5, 7)\n\n        encoder = ClsPooler(embedding_dim=7, cls_is_last_token=True)\n        pooled = encoder(embedding, mask=mask)\n\n        assert list(pooled.size()) == [5, 7]\n        numpy.testing.assert_array_almost_equal(expected, pooled)\n'"
tests/modules/seq2vec_encoders/cnn_encoder_test.py,2,"b'import numpy\nfrom numpy.testing import assert_almost_equal\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.modules.seq2vec_encoders import CnnEncoder\nfrom allennlp.nn import InitializerApplicator, Initializer\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestCnnEncoder(AllenNlpTestCase):\n    def test_get_dimension_is_correct(self):\n        encoder = CnnEncoder(embedding_dim=5, num_filters=4, ngram_filter_sizes=(3, 5))\n        assert encoder.get_output_dim() == 8\n        assert encoder.get_input_dim() == 5\n        encoder = CnnEncoder(\n            embedding_dim=5, num_filters=4, ngram_filter_sizes=(3, 5), output_dim=7\n        )\n        assert encoder.get_output_dim() == 7\n        assert encoder.get_input_dim() == 5\n\n    def test_can_construct_from_params(self):\n        params = Params({""embedding_dim"": 5, ""num_filters"": 4, ""ngram_filter_sizes"": [3, 5]})\n        encoder = CnnEncoder.from_params(params)\n        assert encoder.get_output_dim() == 8\n        params = Params(\n            {""embedding_dim"": 5, ""num_filters"": 4, ""ngram_filter_sizes"": [3, 5], ""output_dim"": 7}\n        )\n        encoder = CnnEncoder.from_params(params)\n        assert encoder.get_output_dim() == 7\n\n    def test_forward_does_correct_computation(self):\n        encoder = CnnEncoder(embedding_dim=2, num_filters=1, ngram_filter_sizes=(1, 2))\n        constant_init = Initializer.from_params(Params({""type"": ""constant"", ""val"": 1.0}))\n        initializer = InitializerApplicator([("".*"", constant_init)])\n        initializer(encoder)\n        input_tensor = torch.FloatTensor([[[0.7, 0.8], [0.1, 1.5]]])\n        encoder_output = encoder(input_tensor, None)\n        assert_almost_equal(\n            encoder_output.data.numpy(), numpy.asarray([[1.6 + 1.0, 3.1 + 1.0]]), decimal=6\n        )\n\n    def test_forward_runs_with_larger_input(self):\n        encoder = CnnEncoder(\n            embedding_dim=7, num_filters=13, ngram_filter_sizes=(1, 2, 3, 4, 5), output_dim=30\n        )\n        tensor = torch.rand(4, 8, 7)\n        assert encoder(tensor, None).size() == (4, 30)\n'"
tests/modules/seq2vec_encoders/cnn_highway_encoder_test.py,2,"b'import numpy as np\nimport torch\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules.seq2vec_encoders.cnn_highway_encoder import CnnHighwayEncoder\nfrom allennlp.modules.time_distributed import TimeDistributed\n\n\nclass TestCnnHighwayEncoder(AllenNlpTestCase):\n    def run_encoder_against_random_embeddings(self, do_layer_norm):\n        encoder = CnnHighwayEncoder(\n            activation=""relu"",\n            embedding_dim=4,\n            filters=[[1, 4], [2, 8], [3, 16], [4, 32], [5, 64]],\n            num_highway=2,\n            projection_dim=16,\n            projection_location=""after_cnn"",\n            do_layer_norm=do_layer_norm,\n        )\n        encoder = TimeDistributed(encoder)\n\n        embedding = torch.from_numpy(np.random.randn(5, 6, 50, 4)).float()\n        mask = torch.ones(5, 6, 50).bool()\n        token_embedding = encoder(embedding, mask)\n\n        assert list(token_embedding.size()) == [5, 6, 16]\n\n    def test_cnn_highway_encoder(self):\n        self.run_encoder_against_random_embeddings(do_layer_norm=False)\n\n    def test_cnn_highway_encoder_with_layer_norm(self):\n        self.run_encoder_against_random_embeddings(do_layer_norm=True)\n'"
tests/modules/seq2vec_encoders/pytorch_seq2vec_wrapper_test.py,13,"b'import pytest\nfrom numpy.testing import assert_almost_equal\nimport torch\nfrom torch.nn import LSTM\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\nfrom allennlp.nn.util import sort_batch_by_length, get_lengths_from_binary_sequence_mask\nfrom allennlp.modules.stacked_alternating_lstm import StackedAlternatingLstm\n\n\nclass TestPytorchSeq2VecWrapper(AllenNlpTestCase):\n    def test_get_dimensions_is_correct(self):\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=2, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2VecWrapper(lstm)\n        assert encoder.get_output_dim() == 14\n        assert encoder.get_input_dim() == 2\n        lstm = LSTM(\n            bidirectional=False, num_layers=3, input_size=2, hidden_size=7, batch_first=True\n        )\n        encoder = PytorchSeq2VecWrapper(lstm)\n        assert encoder.get_output_dim() == 7\n        assert encoder.get_input_dim() == 2\n\n    def test_forward_pulls_out_correct_tensor_without_sequence_lengths(self):\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=2, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2VecWrapper(lstm)\n        input_tensor = torch.FloatTensor([[[0.7, 0.8], [0.1, 1.5]]])\n        lstm_output = lstm(input_tensor)\n        encoder_output = encoder(input_tensor, None)\n        assert_almost_equal(encoder_output.data.numpy(), lstm_output[0].data.numpy()[:, -1, :])\n\n    def test_forward_pulls_out_correct_tensor_with_sequence_lengths(self):\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=3, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2VecWrapper(lstm)\n\n        input_tensor = torch.rand([5, 7, 3])\n        input_tensor[1, 6:, :] = 0\n        input_tensor[2, 4:, :] = 0\n        input_tensor[3, 2:, :] = 0\n        input_tensor[4, 1:, :] = 0\n        mask = torch.ones(5, 7).bool()\n        mask[1, 6:] = False\n        mask[2, 4:] = False\n        mask[3, 2:] = False\n        mask[4, 1:] = False\n\n        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n        packed_sequence = pack_padded_sequence(\n            input_tensor, sequence_lengths.tolist(), batch_first=True\n        )\n        _, state = lstm(packed_sequence)\n        # Transpose output state, extract the last forward and backward states and\n        # reshape to be of dimension (batch_size, 2 * hidden_size).\n        reshaped_state = state[0].transpose(0, 1)[:, -2:, :].contiguous()\n        explicitly_concatenated_state = torch.cat(\n            [reshaped_state[:, 0, :].squeeze(1), reshaped_state[:, 1, :].squeeze(1)], -1\n        )\n        encoder_output = encoder(input_tensor, mask)\n        assert_almost_equal(encoder_output.data.numpy(), explicitly_concatenated_state.data.numpy())\n\n    def test_forward_works_even_with_empty_sequences(self):\n        lstm = LSTM(\n            bidirectional=True, num_layers=3, input_size=3, hidden_size=11, batch_first=True\n        )\n        encoder = PytorchSeq2VecWrapper(lstm)\n\n        tensor = torch.rand([5, 7, 3])\n        tensor[1, 6:, :] = 0\n        tensor[2, :, :] = 0\n        tensor[3, 2:, :] = 0\n        tensor[4, :, :] = 0\n        mask = torch.ones(5, 7).bool()\n        mask[1, 6:] = False\n        mask[2, :] = False\n        mask[3, 2:] = False\n        mask[4, :] = False\n\n        results = encoder(tensor, mask)\n\n        for i in (0, 1, 3):\n            assert not (results[i] == 0.0).data.all()\n        for i in (2, 4):\n            assert (results[i] == 0.0).data.all()\n\n    def test_forward_pulls_out_correct_tensor_with_unsorted_batches(self):\n        lstm = LSTM(bidirectional=True, num_layers=3, input_size=3, hidden_size=7, batch_first=True)\n        encoder = PytorchSeq2VecWrapper(lstm)\n\n        input_tensor = torch.rand([5, 7, 3])\n        input_tensor[0, 3:, :] = 0\n        input_tensor[1, 4:, :] = 0\n        input_tensor[2, 2:, :] = 0\n        input_tensor[3, 6:, :] = 0\n        mask = torch.ones(5, 7).bool()\n        mask[0, 3:] = False\n        mask[1, 4:] = False\n        mask[2, 2:] = False\n        mask[3, 6:] = False\n\n        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n        sorted_inputs, sorted_sequence_lengths, restoration_indices, _ = sort_batch_by_length(\n            input_tensor, sequence_lengths\n        )\n        packed_sequence = pack_padded_sequence(\n            sorted_inputs, sorted_sequence_lengths.tolist(), batch_first=True\n        )\n        _, state = lstm(packed_sequence)\n        # Transpose output state, extract the last forward and backward states and\n        # reshape to be of dimension (batch_size, 2 * hidden_size).\n        sorted_transposed_state = state[0].transpose(0, 1).index_select(0, restoration_indices)\n        reshaped_state = sorted_transposed_state[:, -2:, :].contiguous()\n        explicitly_concatenated_state = torch.cat(\n            [reshaped_state[:, 0, :].squeeze(1), reshaped_state[:, 1, :].squeeze(1)], -1\n        )\n        encoder_output = encoder(input_tensor, mask)\n        assert_almost_equal(encoder_output.data.numpy(), explicitly_concatenated_state.data.numpy())\n\n    def test_wrapper_raises_if_batch_first_is_false(self):\n        with pytest.raises(ConfigurationError):\n            lstm = LSTM(bidirectional=True, num_layers=3, input_size=3, hidden_size=7)\n            _ = PytorchSeq2VecWrapper(lstm)\n\n    def test_wrapper_works_with_alternating_lstm(self):\n        model = PytorchSeq2VecWrapper(\n            StackedAlternatingLstm(input_size=4, hidden_size=5, num_layers=3)\n        )\n\n        input_tensor = torch.randn(2, 3, 4)\n        mask = torch.ones(2, 3).bool()\n        output = model(input_tensor, mask)\n        assert tuple(output.size()) == (2, 5)\n'"
tests/modules/span_extractors/__init__.py,0,b''
tests/modules/span_extractors/bidirectional_endpoint_span_extractor_test.py,20,"b'import numpy\nimport pytest\nimport torch\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.params import Params\nfrom allennlp.modules.span_extractors import BidirectionalEndpointSpanExtractor, SpanExtractor\nfrom allennlp.nn.util import batched_index_select\n\n\nclass TestBidirectonalEndpointSpanExtractor:\n    def test_bidirectional_endpoint_span_extractor_can_build_from_params(self):\n        params = Params(\n            {\n                ""type"": ""bidirectional_endpoint"",\n                ""input_dim"": 4,\n                ""num_width_embeddings"": 5,\n                ""span_width_embedding_dim"": 3,\n            }\n        )\n        extractor = SpanExtractor.from_params(params)\n        assert isinstance(extractor, BidirectionalEndpointSpanExtractor)\n        assert extractor.get_output_dim() == 2 + 2 + 3\n\n    def test_raises_on_odd_input_dimension(self):\n        with pytest.raises(ConfigurationError):\n            _ = BidirectionalEndpointSpanExtractor(7)\n\n    def test_correct_sequence_elements_are_embedded(self):\n        sequence_tensor = torch.randn([2, 5, 8])\n        # concatentate start and end points together to form our representation\n        # for both the forward and backward directions.\n        extractor = BidirectionalEndpointSpanExtractor(\n            input_dim=8, forward_combination=""x,y"", backward_combination=""x,y""\n        )\n        indices = torch.LongTensor([[[1, 3], [2, 4]], [[0, 2], [3, 4]]])\n\n        span_representations = extractor(sequence_tensor, indices)\n\n        assert list(span_representations.size()) == [2, 2, 16]\n        assert extractor.get_output_dim() == 16\n        assert extractor.get_input_dim() == 8\n\n        # We just concatenated the start and end embeddings together, so\n        # we can check they match the original indices if we split them apart.\n        (\n            forward_start_embeddings,\n            forward_end_embeddings,\n            backward_start_embeddings,\n            backward_end_embeddings,\n        ) = span_representations.split(4, -1)\n\n        forward_sequence_tensor, backward_sequence_tensor = sequence_tensor.split(4, -1)\n\n        # Forward direction => subtract 1 from start indices to make them exlusive.\n        correct_forward_start_indices = torch.LongTensor([[0, 1], [-1, 2]])\n        # This index should be -1, so it will be replaced with a sentinel. Here,\n        # we\'ll set it to a value other than -1 so we can index select the indices and\n        # replace it later.\n        correct_forward_start_indices[1, 0] = 1\n\n        # Forward direction => end indices are the same.\n        correct_forward_end_indices = torch.LongTensor([[3, 4], [2, 4]])\n\n        # Backward direction => start indices are exclusive, so add 1 to the end indices.\n        correct_backward_start_indices = torch.LongTensor([[4, 5], [3, 5]])\n        # These exclusive end indices are outside the tensor, so will be replaced with the end sentinel.\n        # Here we replace them with ones so we can index select using these indices without torch\n        # complaining.\n        correct_backward_start_indices[0, 1] = 1\n        correct_backward_start_indices[1, 1] = 1\n        # Backward direction => end indices are inclusive and equal to the forward start indices.\n        correct_backward_end_indices = torch.LongTensor([[1, 2], [0, 3]])\n\n        correct_forward_start_embeddings = batched_index_select(\n            forward_sequence_tensor.contiguous(), correct_forward_start_indices\n        )\n        # This element had sequence_tensor index of 0, so it\'s exclusive index is the start sentinel.\n        correct_forward_start_embeddings[1, 0] = extractor._start_sentinel.data\n        numpy.testing.assert_array_equal(\n            forward_start_embeddings.data.numpy(), correct_forward_start_embeddings.data.numpy()\n        )\n\n        correct_forward_end_embeddings = batched_index_select(\n            forward_sequence_tensor.contiguous(), correct_forward_end_indices\n        )\n        numpy.testing.assert_array_equal(\n            forward_end_embeddings.data.numpy(), correct_forward_end_embeddings.data.numpy()\n        )\n\n        correct_backward_end_embeddings = batched_index_select(\n            backward_sequence_tensor.contiguous(), correct_backward_end_indices\n        )\n        numpy.testing.assert_array_equal(\n            backward_end_embeddings.data.numpy(), correct_backward_end_embeddings.data.numpy()\n        )\n\n        correct_backward_start_embeddings = batched_index_select(\n            backward_sequence_tensor.contiguous(), correct_backward_start_indices\n        )\n        # This element had sequence_tensor index == sequence_tensor.size(1),\n        # so it\'s exclusive index is the end sentinel.\n        correct_backward_start_embeddings[0, 1] = extractor._end_sentinel.data\n        correct_backward_start_embeddings[1, 1] = extractor._end_sentinel.data\n        numpy.testing.assert_array_equal(\n            backward_start_embeddings.data.numpy(), correct_backward_start_embeddings.data.numpy()\n        )\n\n    def test_correct_sequence_elements_are_embedded_with_a_masked_sequence(self):\n        sequence_tensor = torch.randn([2, 5, 8])\n        # concatentate start and end points together to form our representation\n        # for both the forward and backward directions.\n        extractor = BidirectionalEndpointSpanExtractor(\n            input_dim=8, forward_combination=""x,y"", backward_combination=""x,y""\n        )\n        indices = torch.LongTensor(\n            [\n                [[1, 3], [2, 4]],\n                # This span has an end index at the\n                # end of the padded sequence.\n                [[0, 2], [0, 1]],\n            ]\n        )\n        sequence_mask = torch.tensor(\n            [[True, True, True, True, True], [True, True, True, False, False]]\n        )\n\n        span_representations = extractor(sequence_tensor, indices, sequence_mask=sequence_mask)\n\n        # We just concatenated the start and end embeddings together, so\n        # we can check they match the original indices if we split them apart.\n        (\n            forward_start_embeddings,\n            forward_end_embeddings,\n            backward_start_embeddings,\n            backward_end_embeddings,\n        ) = span_representations.split(4, -1)\n\n        forward_sequence_tensor, backward_sequence_tensor = sequence_tensor.split(4, -1)\n\n        # Forward direction => subtract 1 from start indices to make them exlusive.\n        correct_forward_start_indices = torch.LongTensor([[0, 1], [-1, -1]])\n        # These indices should be -1, so they\'ll be replaced with a sentinel. Here,\n        # we\'ll set them to a value other than -1 so we can index select the indices and\n        # replace them later.\n        correct_forward_start_indices[1, 0] = 1\n        correct_forward_start_indices[1, 1] = 1\n\n        # Forward direction => end indices are the same.\n        correct_forward_end_indices = torch.LongTensor([[3, 4], [2, 1]])\n\n        # Backward direction => start indices are exclusive, so add 1 to the end indices.\n        correct_backward_start_indices = torch.LongTensor([[4, 5], [3, 2]])\n        # These exclusive backward start indices are outside the tensor, so will be replaced\n        # with the end sentinel. Here we replace them with ones so we can index select using\n        # these indices without torch complaining.\n        correct_backward_start_indices[0, 1] = 1\n\n        # Backward direction => end indices are inclusive and equal to the forward start indices.\n        correct_backward_end_indices = torch.LongTensor([[1, 2], [0, 0]])\n\n        correct_forward_start_embeddings = batched_index_select(\n            forward_sequence_tensor.contiguous(), correct_forward_start_indices\n        )\n        # This element had sequence_tensor index of 0, so it\'s exclusive index is the start sentinel.\n        correct_forward_start_embeddings[1, 0] = extractor._start_sentinel.data\n        correct_forward_start_embeddings[1, 1] = extractor._start_sentinel.data\n        numpy.testing.assert_array_equal(\n            forward_start_embeddings.data.numpy(), correct_forward_start_embeddings.data.numpy()\n        )\n\n        correct_forward_end_embeddings = batched_index_select(\n            forward_sequence_tensor.contiguous(), correct_forward_end_indices\n        )\n        numpy.testing.assert_array_equal(\n            forward_end_embeddings.data.numpy(), correct_forward_end_embeddings.data.numpy()\n        )\n\n        correct_backward_end_embeddings = batched_index_select(\n            backward_sequence_tensor.contiguous(), correct_backward_end_indices\n        )\n        numpy.testing.assert_array_equal(\n            backward_end_embeddings.data.numpy(), correct_backward_end_embeddings.data.numpy()\n        )\n\n        correct_backward_start_embeddings = batched_index_select(\n            backward_sequence_tensor.contiguous(), correct_backward_start_indices\n        )\n        # This element had sequence_tensor index == sequence_tensor.size(1),\n        # so it\'s exclusive index is the end sentinel.\n        correct_backward_start_embeddings[0, 1] = extractor._end_sentinel.data\n        # This element has sequence_tensor index == the masked length of the batch element,\n        # so it should be the end_sentinel even though it isn\'t greater than sequence_tensor.size(1).\n        correct_backward_start_embeddings[1, 0] = extractor._end_sentinel.data\n\n        numpy.testing.assert_array_equal(\n            backward_start_embeddings.data.numpy(), correct_backward_start_embeddings.data.numpy()\n        )\n\n    def test_forward_doesnt_raise_with_empty_sequence(self):\n        # size: (batch_size=1, sequence_length=2, emb_dim=2)\n        sequence_tensor = torch.FloatTensor([[[0.0, 0.0], [0.0, 0.0]]])\n        # size: (batch_size=1, sequence_length=2)\n        sequence_mask = torch.tensor([[False, False]])\n        # size: (batch_size=1, spans_count=1, 2)\n        span_indices = torch.LongTensor([[[-1, -1]]])\n        # size: (batch_size=1, spans_count=1)\n        span_indices_mask = torch.tensor([[False]])\n        extractor = BidirectionalEndpointSpanExtractor(\n            input_dim=2, forward_combination=""x,y"", backward_combination=""x,y""\n        )\n        span_representations = extractor(\n            sequence_tensor,\n            span_indices,\n            sequence_mask=sequence_mask,\n            span_indices_mask=span_indices_mask,\n        )\n        numpy.testing.assert_array_equal(\n            span_representations.detach(), torch.FloatTensor([[[0.0, 0.0, 0.0, 0.0]]])\n        )\n\n    def test_forward_raises_with_invalid_indices(self):\n        sequence_tensor = torch.randn([2, 5, 8])\n        extractor = BidirectionalEndpointSpanExtractor(input_dim=8)\n        indices = torch.LongTensor([[[-1, 3], [7, 4]], [[0, 12], [0, -1]]])\n\n        with pytest.raises(ValueError):\n            _ = extractor(sequence_tensor, indices)\n'"
tests/modules/span_extractors/endpoint_span_extractor_test.py,10,"b'import numpy\nimport torch\n\nfrom allennlp.modules.span_extractors import SpanExtractor, EndpointSpanExtractor\nfrom allennlp.common.params import Params\nfrom allennlp.nn.util import batched_index_select\n\n\nclass TestEndpointSpanExtractor:\n    def test_endpoint_span_extractor_can_build_from_params(self):\n        params = Params(\n            {\n                ""type"": ""endpoint"",\n                ""input_dim"": 7,\n                ""num_width_embeddings"": 5,\n                ""span_width_embedding_dim"": 3,\n            }\n        )\n        extractor = SpanExtractor.from_params(params)\n        assert isinstance(extractor, EndpointSpanExtractor)\n        assert extractor.get_output_dim() == 17  # 2 * input_dim + span_width_embedding_dim\n\n    def test_correct_sequence_elements_are_embedded(self):\n        sequence_tensor = torch.randn([2, 5, 7])\n        # Concatentate start and end points together to form our representation.\n        extractor = EndpointSpanExtractor(7, ""x,y"")\n\n        indices = torch.LongTensor([[[1, 3], [2, 4]], [[0, 2], [3, 4]]])\n        span_representations = extractor(sequence_tensor, indices)\n\n        assert list(span_representations.size()) == [2, 2, 14]\n        assert extractor.get_output_dim() == 14\n        assert extractor.get_input_dim() == 7\n\n        start_indices, end_indices = indices.split(1, -1)\n        # We just concatenated the start and end embeddings together, so\n        # we can check they match the original indices if we split them apart.\n        start_embeddings, end_embeddings = span_representations.split(7, -1)\n\n        correct_start_embeddings = batched_index_select(sequence_tensor, start_indices.squeeze())\n        correct_end_embeddings = batched_index_select(sequence_tensor, end_indices.squeeze())\n        numpy.testing.assert_array_equal(\n            start_embeddings.data.numpy(), correct_start_embeddings.data.numpy()\n        )\n        numpy.testing.assert_array_equal(\n            end_embeddings.data.numpy(), correct_end_embeddings.data.numpy()\n        )\n\n    def test_masked_indices_are_handled_correctly(self):\n        sequence_tensor = torch.randn([2, 5, 7])\n        # concatentate start and end points together to form our representation.\n        extractor = EndpointSpanExtractor(7, ""x,y"")\n\n        indices = torch.LongTensor([[[1, 3], [2, 4]], [[0, 2], [3, 4]]])\n        span_representations = extractor(sequence_tensor, indices)\n\n        # Make a mask with the second batch element completely masked.\n        indices_mask = torch.tensor([[True, True], [False, False]])\n\n        span_representations = extractor(sequence_tensor, indices, span_indices_mask=indices_mask)\n        start_embeddings, end_embeddings = span_representations.split(7, -1)\n        start_indices, end_indices = indices.split(1, -1)\n\n        correct_start_embeddings = batched_index_select(\n            sequence_tensor, start_indices.squeeze()\n        ).data\n        # Completely masked second batch element, so it should all be zero.\n        correct_start_embeddings[1, :, :].fill_(0)\n        correct_end_embeddings = batched_index_select(sequence_tensor, end_indices.squeeze()).data\n        correct_end_embeddings[1, :, :].fill_(0)\n        numpy.testing.assert_array_equal(\n            start_embeddings.data.numpy(), correct_start_embeddings.numpy()\n        )\n        numpy.testing.assert_array_equal(\n            end_embeddings.data.numpy(), correct_end_embeddings.numpy()\n        )\n\n    def test_masked_indices_are_handled_correctly_with_exclusive_indices(self):\n        sequence_tensor = torch.randn([2, 5, 8])\n        # concatentate start and end points together to form our representation\n        # for both the forward and backward directions.\n        extractor = EndpointSpanExtractor(8, ""x,y"", use_exclusive_start_indices=True)\n        indices = torch.LongTensor([[[1, 3], [2, 4]], [[0, 2], [0, 1]]])\n        sequence_mask = torch.tensor(\n            [[True, True, True, True, True], [True, True, True, False, False]]\n        )\n\n        span_representations = extractor(sequence_tensor, indices, sequence_mask=sequence_mask)\n\n        # We just concatenated the start and end embeddings together, so\n        # we can check they match the original indices if we split them apart.\n        start_embeddings, end_embeddings = span_representations.split(8, -1)\n\n        correct_start_indices = torch.LongTensor([[0, 1], [-1, -1]])\n        # These indices should be -1, so they\'ll be replaced with a sentinel. Here,\n        # we\'ll set them to a value other than -1 so we can index select the indices and\n        # replace them later.\n        correct_start_indices[1, 0] = 1\n        correct_start_indices[1, 1] = 1\n\n        correct_end_indices = torch.LongTensor([[3, 4], [2, 1]])\n\n        correct_start_embeddings = batched_index_select(\n            sequence_tensor.contiguous(), correct_start_indices\n        )\n        # This element had sequence_tensor index of 0, so it\'s exclusive index is the start sentinel.\n        correct_start_embeddings[1, 0] = extractor._start_sentinel.data\n        correct_start_embeddings[1, 1] = extractor._start_sentinel.data\n        numpy.testing.assert_array_equal(\n            start_embeddings.data.numpy(), correct_start_embeddings.data.numpy()\n        )\n\n        correct_end_embeddings = batched_index_select(\n            sequence_tensor.contiguous(), correct_end_indices\n        )\n        numpy.testing.assert_array_equal(\n            end_embeddings.data.numpy(), correct_end_embeddings.data.numpy()\n        )\n'"
tests/modules/span_extractors/self_attentive_span_extractor_test.py,3,"b'import numpy\nimport torch\n\nfrom allennlp.modules.span_extractors import SpanExtractor, SelfAttentiveSpanExtractor\nfrom allennlp.common.params import Params\n\n\nclass TestSelfAttentiveSpanExtractor:\n    def test_locally_normalised_span_extractor_can_build_from_params(self):\n        params = Params({""type"": ""self_attentive"", ""input_dim"": 5})\n        extractor = SpanExtractor.from_params(params)\n        assert isinstance(extractor, SelfAttentiveSpanExtractor)\n\n    def test_attention_is_normalised_correctly(self):\n        input_dim = 7\n        sequence_tensor = torch.randn([2, 5, input_dim])\n        extractor = SelfAttentiveSpanExtractor(input_dim=input_dim)\n        assert extractor.get_output_dim() == input_dim\n        assert extractor.get_input_dim() == input_dim\n\n        # In order to test the attention, we\'ll make the weight which computes the logits\n        # zero, so the attention distribution is uniform over the sentence. This lets\n        # us check that the computed spans are just the averages of their representations.\n        extractor._global_attention._module.weight.data.fill_(0.0)\n        extractor._global_attention._module.bias.data.fill_(0.0)\n\n        indices = torch.LongTensor(\n            [[[1, 3], [2, 4]], [[0, 2], [3, 4]]]\n        )  # smaller span tests masking.\n        span_representations = extractor(sequence_tensor, indices)\n        assert list(span_representations.size()) == [2, 2, input_dim]\n\n        # First element in the batch.\n        batch_element = 0\n        spans = span_representations[batch_element]\n        # First span.\n        mean_embeddings = sequence_tensor[batch_element, 1:4, :].mean(0)\n        numpy.testing.assert_array_almost_equal(spans[0].data.numpy(), mean_embeddings.data.numpy())\n        # Second span.\n        mean_embeddings = sequence_tensor[batch_element, 2:5, :].mean(0)\n        numpy.testing.assert_array_almost_equal(spans[1].data.numpy(), mean_embeddings.data.numpy())\n        # Now the second element in the batch.\n        batch_element = 1\n        spans = span_representations[batch_element]\n        # First span.\n        mean_embeddings = sequence_tensor[batch_element, 0:3, :].mean(0)\n        numpy.testing.assert_array_almost_equal(spans[0].data.numpy(), mean_embeddings.data.numpy())\n        # Second span.\n        mean_embeddings = sequence_tensor[batch_element, 3:5, :].mean(0)\n        numpy.testing.assert_array_almost_equal(spans[1].data.numpy(), mean_embeddings.data.numpy())\n\n        # Now test the case in which we have some masked spans in our indices.\n        indices_mask = torch.tensor([[True, True], [True, False]])\n        span_representations = extractor(sequence_tensor, indices, span_indices_mask=indices_mask)\n\n        # First element in the batch.\n        batch_element = 0\n        spans = span_representations[batch_element]\n        # First span.\n        mean_embeddings = sequence_tensor[batch_element, 1:4, :].mean(0)\n        numpy.testing.assert_array_almost_equal(spans[0].data.numpy(), mean_embeddings.data.numpy())\n        # Second span.\n        mean_embeddings = sequence_tensor[batch_element, 2:5, :].mean(0)\n        numpy.testing.assert_array_almost_equal(spans[1].data.numpy(), mean_embeddings.data.numpy())\n        # Now the second element in the batch.\n        batch_element = 1\n        spans = span_representations[batch_element]\n        # First span.\n        mean_embeddings = sequence_tensor[batch_element, 0:3, :].mean(0)\n        numpy.testing.assert_array_almost_equal(spans[0].data.numpy(), mean_embeddings.data.numpy())\n        # Second span was masked, so should be completely zero.\n        numpy.testing.assert_array_almost_equal(spans[1].data.numpy(), numpy.zeros([input_dim]))\n'"
tests/modules/text_field_embedders/__init__.py,0,b''
tests/modules/text_field_embedders/basic_text_field_embedder_test.py,16,"b'import pytest\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data import Vocabulary\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestBasicTextFieldEmbedder(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.vocab = Vocabulary()\n        self.vocab.add_token_to_namespace(""1"")\n        self.vocab.add_token_to_namespace(""2"")\n        self.vocab.add_token_to_namespace(""3"")\n        self.vocab.add_token_to_namespace(""4"")\n        params = Params(\n            {\n                ""token_embedders"": {\n                    ""words1"": {""type"": ""embedding"", ""embedding_dim"": 2},\n                    ""words2"": {""type"": ""embedding"", ""embedding_dim"": 5},\n                    ""words3"": {""type"": ""embedding"", ""embedding_dim"": 3},\n                }\n            }\n        )\n        self.token_embedder = BasicTextFieldEmbedder.from_params(vocab=self.vocab, params=params)\n        self.inputs = {\n            ""words1"": {""tokens"": torch.LongTensor([[0, 2, 3, 5]])},\n            ""words2"": {""tokens"": torch.LongTensor([[1, 4, 3, 2]])},\n            ""words3"": {""tokens"": torch.LongTensor([[1, 5, 1, 2]])},\n        }\n\n    def test_get_output_dim_aggregates_dimension_from_each_embedding(self):\n        assert self.token_embedder.get_output_dim() == 10\n\n    def test_forward_asserts_input_field_match(self):\n        # Total mismatch\n        self.inputs[""words4""] = self.inputs[""words3""]\n        del self.inputs[""words3""]\n        with pytest.raises(ConfigurationError) as exc:\n            self.token_embedder(self.inputs)\n        assert exc.match(""Mismatched token keys"")\n\n        self.inputs[""words3""] = self.inputs[""words4""]\n\n        # Text field has too many inputs\n        with pytest.raises(ConfigurationError) as exc:\n            self.token_embedder(self.inputs)\n        assert exc.match(""Mismatched token keys"")\n\n        del self.inputs[""words4""]\n\n    def test_forward_concats_resultant_embeddings(self):\n        assert self.token_embedder(self.inputs).size() == (1, 4, 10)\n\n    def test_forward_works_on_higher_order_input(self):\n        params = Params(\n            {\n                ""token_embedders"": {\n                    ""words"": {""type"": ""embedding"", ""num_embeddings"": 20, ""embedding_dim"": 2},\n                    ""characters"": {\n                        ""type"": ""character_encoding"",\n                        ""embedding"": {""embedding_dim"": 4, ""num_embeddings"": 15},\n                        ""encoder"": {\n                            ""type"": ""cnn"",\n                            ""embedding_dim"": 4,\n                            ""num_filters"": 10,\n                            ""ngram_filter_sizes"": [3],\n                        },\n                    },\n                }\n            }\n        )\n        token_embedder = BasicTextFieldEmbedder.from_params(vocab=self.vocab, params=params)\n        inputs = {\n            ""words"": {""tokens"": (torch.rand(3, 4, 5, 6) * 20).long()},\n            ""characters"": {""token_characters"": (torch.rand(3, 4, 5, 6, 7) * 15).long()},\n        }\n        assert token_embedder(inputs, num_wrapping_dims=2).size() == (3, 4, 5, 6, 12)\n\n    def test_forward_runs_with_forward_params(self):\n        class FakeEmbedder(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n\n            def forward(self, tokens: torch.Tensor, extra_arg: int = None):\n                assert tokens is not None\n                assert extra_arg is not None\n                return tokens\n\n        token_embedder = BasicTextFieldEmbedder({""elmo"": FakeEmbedder()})\n        inputs = {""elmo"": {""elmo_tokens"": (torch.rand(3, 6, 5) * 2).long()}}\n        kwargs = {""extra_arg"": 1}\n        token_embedder(inputs, **kwargs)\n\n    def test_forward_runs_with_non_bijective_mapping(self):\n        elmo_fixtures_path = self.FIXTURES_ROOT / ""elmo""\n        options_file = str(elmo_fixtures_path / ""options.json"")\n        weight_file = str(elmo_fixtures_path / ""lm_weights.hdf5"")\n        params = Params(\n            {\n                ""token_embedders"": {\n                    ""words"": {""type"": ""embedding"", ""num_embeddings"": 20, ""embedding_dim"": 2},\n                    ""elmo"": {\n                        ""type"": ""elmo_token_embedder"",\n                        ""options_file"": options_file,\n                        ""weight_file"": weight_file,\n                    },\n                }\n            }\n        )\n        token_embedder = BasicTextFieldEmbedder.from_params(vocab=self.vocab, params=params)\n        inputs = {\n            ""words"": {""tokens"": (torch.rand(3, 6) * 20).long()},\n            ""elmo"": {""elmo_tokens"": (torch.rand(3, 6, 50) * 15).long()},\n        }\n        token_embedder(inputs)\n\n    def test_forward_runs_with_non_bijective_mapping_with_null(self):\n        elmo_fixtures_path = self.FIXTURES_ROOT / ""elmo""\n        options_file = str(elmo_fixtures_path / ""options.json"")\n        weight_file = str(elmo_fixtures_path / ""lm_weights.hdf5"")\n        params = Params(\n            {\n                ""token_embedders"": {\n                    ""elmo"": {\n                        ""type"": ""elmo_token_embedder"",\n                        ""options_file"": options_file,\n                        ""weight_file"": weight_file,\n                    }\n                }\n            }\n        )\n        token_embedder = BasicTextFieldEmbedder.from_params(vocab=self.vocab, params=params)\n        inputs = {""elmo"": {""elmo_tokens"": (torch.rand(3, 6, 50) * 15).long()}}\n        token_embedder(inputs)\n\n    def test_forward_runs_with_non_bijective_mapping_with_dict(self):\n        elmo_fixtures_path = self.FIXTURES_ROOT / ""elmo""\n        options_file = str(elmo_fixtures_path / ""options.json"")\n        weight_file = str(elmo_fixtures_path / ""lm_weights.hdf5"")\n        params = Params(\n            {\n                ""token_embedders"": {\n                    ""words"": {""type"": ""embedding"", ""num_embeddings"": 20, ""embedding_dim"": 2},\n                    ""elmo"": {\n                        ""type"": ""elmo_token_embedder"",\n                        ""options_file"": options_file,\n                        ""weight_file"": weight_file,\n                    },\n                }\n            }\n        )\n        token_embedder = BasicTextFieldEmbedder.from_params(vocab=self.vocab, params=params)\n        inputs = {\n            ""words"": {""tokens"": (torch.rand(3, 6) * 20).long()},\n            ""elmo"": {""elmo_tokens"": (torch.rand(3, 6, 50) * 15).long()},\n        }\n        token_embedder(inputs)\n\n    def test_forward_runs_with_bijective_and_non_bijective_mapping(self):\n        params = Params(\n            {\n                ""token_embedders"": {\n                    ""bert"": {""type"": ""pretrained_transformer"", ""model_name"": ""bert-base-uncased""},\n                    ""token_characters"": {\n                        ""type"": ""character_encoding"",\n                        ""embedding"": {""embedding_dim"": 5},\n                        ""encoder"": {\n                            ""type"": ""cnn"",\n                            ""embedding_dim"": 5,\n                            ""num_filters"": 5,\n                            ""ngram_filter_sizes"": [5],\n                        },\n                    },\n                }\n            }\n        )\n        token_embedder = BasicTextFieldEmbedder.from_params(vocab=self.vocab, params=params)\n        inputs = {\n            ""bert"": {\n                ""token_ids"": (torch.rand(3, 5) * 10).long(),\n                ""mask"": (torch.rand(3, 5) * 1).bool(),\n            },\n            ""token_characters"": {""token_characters"": (torch.rand(3, 5, 5) * 1).long()},\n        }\n        token_embedder(inputs)\n'"
tests/modules/token_embedders/__init__.py,0,b''
tests/modules/token_embedders/bag_of_word_counts_token_embedder_test.py,5,"b'import numpy as np\nimport pytest\nimport torch\nfrom numpy.testing import assert_almost_equal\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Vocabulary\nfrom allennlp.modules.token_embedders import BagOfWordCountsTokenEmbedder\n\n\nclass TestBagOfWordCountsTokenEmbedder(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.vocab = Vocabulary()\n        self.vocab.add_token_to_namespace(""1"")\n        self.vocab.add_token_to_namespace(""2"")\n        self.vocab.add_token_to_namespace(""3"")\n        self.vocab.add_token_to_namespace(""4"")\n        self.non_padded_vocab = Vocabulary(non_padded_namespaces=[""tokens""])\n\n    def test_forward_calculates_bow_properly(self):\n        embedder = BagOfWordCountsTokenEmbedder(self.vocab)\n        numpy_tensor = np.array([[2, 0], [3, 0], [4, 4]])\n        inputs = torch.from_numpy(numpy_tensor)\n        embedder_output = embedder(inputs)\n        numpy_tensor = np.array([[0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 2, 0]])\n        manual_output = torch.from_numpy(numpy_tensor).float()\n        assert_almost_equal(embedder_output.data.numpy(), manual_output.data.numpy())\n\n    def test_zeros_out_unknown_tokens(self):\n        embedder = BagOfWordCountsTokenEmbedder(self.vocab, ignore_oov=True)\n        numpy_tensor = np.array([[1, 5], [2, 0], [4, 4]])\n        inputs = torch.from_numpy(numpy_tensor)\n        embedder_output = embedder(inputs)\n        numpy_tensor = np.array([[0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 2, 0]])\n        manual_output = torch.from_numpy(numpy_tensor).float()\n        assert_almost_equal(embedder_output.data.numpy(), manual_output.data.numpy())\n\n    def test_ignore_oov_should_fail_on_non_padded_vocab(self):\n        with pytest.raises(ConfigurationError):\n            BagOfWordCountsTokenEmbedder(self.non_padded_vocab, ignore_oov=True)\n\n    def test_projects_properly(self):\n        embedder = BagOfWordCountsTokenEmbedder(vocab=self.vocab, projection_dim=50)\n        numpy_tensor = np.array([[1, 0], [1, 0], [4, 4]])\n        inputs = torch.from_numpy(numpy_tensor)\n        embedder_output = embedder(inputs)\n        assert embedder_output.shape[1] == 50\n'"
tests/modules/token_embedders/elmo_token_embedder_test.py,2,"b'import torch\n\nfrom allennlp.common import Params\nfrom allennlp.common.testing import ModelTestCase\nfrom allennlp.data.batch import Batch\nfrom allennlp.modules.token_embedders import ElmoTokenEmbedder\n\n\nclass TestElmoTokenEmbedder(ModelTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.set_up_model(\n            self.FIXTURES_ROOT / ""elmo"" / ""config"" / ""characters_token_embedder.json"",\n            self.FIXTURES_ROOT / ""data"" / ""conll2003.txt"",\n        )\n\n    def test_tagger_with_elmo_token_embedder_can_train_save_and_load(self):\n        self.ensure_model_can_train_save_and_load(self.param_file)\n\n    def test_tagger_with_elmo_token_embedder_forward_pass_runs_correctly(self):\n        dataset = Batch(self.instances)\n        dataset.index_instances(self.vocab)\n        training_tensors = dataset.as_tensor_dict()\n        output_dict = self.model(**training_tensors)\n        probs = output_dict[""class_probabilities""]\n        assert probs.size() == (2, 7, self.model.vocab.get_vocab_size(""labels""))\n\n    def test_forward_works_with_projection_layer(self):\n        params = Params(\n            {\n                ""options_file"": self.FIXTURES_ROOT / ""elmo"" / ""options.json"",\n                ""weight_file"": self.FIXTURES_ROOT / ""elmo"" / ""lm_weights.hdf5"",\n                ""projection_dim"": 20,\n            }\n        )\n        word1 = [0] * 50\n        word2 = [0] * 50\n        word1[0] = 6\n        word1[1] = 5\n        word1[2] = 4\n        word1[3] = 3\n        word2[0] = 3\n        word2[1] = 2\n        word2[2] = 1\n        word2[3] = 0\n        embedding_layer = ElmoTokenEmbedder.from_params(vocab=None, params=params)\n        assert embedding_layer.get_output_dim() == 20\n\n        input_tensor = torch.LongTensor([[word1, word2]])\n        embedded = embedding_layer(input_tensor).data.numpy()\n        assert embedded.shape == (1, 2, 20)\n\n        input_tensor = torch.LongTensor([[[word1]]])\n        embedded = embedding_layer(input_tensor).data.numpy()\n        assert embedded.shape == (1, 1, 1, 20)\n'"
tests/modules/token_embedders/embedding_test.py,16,"b'import gzip\nimport warnings\n\nimport numpy\nimport pytest\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Vocabulary\nfrom allennlp.modules.token_embedders.embedding import (\n    _read_pretrained_embeddings_file,\n    Embedding,\n    EmbeddingsTextFile,\n    format_embeddings_file_uri,\n    parse_embeddings_file_uri,\n)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(""ignore"", category=FutureWarning)\n    import h5py\n\n\nclass TestEmbedding(AllenNlpTestCase):\n    def test_get_embedding_layer_uses_correct_embedding_dim(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""word1"")\n        vocab.add_token_to_namespace(""word2"")\n        embeddings_filename = str(self.TEST_DIR / ""embeddings.gz"")\n        with gzip.open(embeddings_filename, ""wb"") as embeddings_file:\n            embeddings_file.write(""word1 1.0 2.3 -1.0\\n"".encode(""utf-8""))\n            embeddings_file.write(""word2 0.1 0.4 -4.0\\n"".encode(""utf-8""))\n        embedding_weights = _read_pretrained_embeddings_file(embeddings_filename, 3, vocab)\n        assert tuple(embedding_weights.size()) == (4, 3)  # 4 because of padding and OOV\n        with pytest.raises(ConfigurationError):\n            _read_pretrained_embeddings_file(embeddings_filename, 4, vocab)\n\n    def test_forward_works_with_projection_layer(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""the"")\n        vocab.add_token_to_namespace(""a"")\n        params = Params(\n            {\n                ""pretrained_file"": str(\n                    self.FIXTURES_ROOT / ""embeddings/glove.6B.300d.sample.txt.gz""\n                ),\n                ""embedding_dim"": 300,\n                ""projection_dim"": 20,\n            }\n        )\n        embedding_layer = Embedding.from_params(params, vocab=vocab)\n        input_tensor = torch.LongTensor([[3, 2, 1, 0]])\n        embedded = embedding_layer(input_tensor).data.numpy()\n        assert embedded.shape == (1, 4, 20)\n\n        input_tensor = torch.LongTensor([[[3, 2, 1, 0]]])\n        embedded = embedding_layer(input_tensor).data.numpy()\n        assert embedded.shape == (1, 1, 4, 20)\n\n    def test_embedding_layer_actually_initializes_word_vectors_correctly(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""word"")\n        vocab.add_token_to_namespace(""word2"")\n        unicode_space = ""\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0""\n        vocab.add_token_to_namespace(unicode_space)\n        embeddings_filename = str(self.TEST_DIR / ""embeddings.gz"")\n        with gzip.open(embeddings_filename, ""wb"") as embeddings_file:\n            embeddings_file.write(""word 1.0 2.3 -1.0\\n"".encode(""utf-8""))\n            embeddings_file.write(f""{unicode_space} 3.4 3.3 5.0\\n"".encode(""utf-8""))\n        params = Params({""pretrained_file"": embeddings_filename, ""embedding_dim"": 3})\n        embedding_layer = Embedding.from_params(params, vocab=vocab)\n        word_vector = embedding_layer.weight.data[vocab.get_token_index(""word"")]\n        assert numpy.allclose(word_vector.numpy(), numpy.array([1.0, 2.3, -1.0]))\n        word_vector = embedding_layer.weight.data[vocab.get_token_index(unicode_space)]\n        assert numpy.allclose(word_vector.numpy(), numpy.array([3.4, 3.3, 5.0]))\n        word_vector = embedding_layer.weight.data[vocab.get_token_index(""word2"")]\n        assert not numpy.allclose(word_vector.numpy(), numpy.array([1.0, 2.3, -1.0]))\n\n    def test_get_embedding_layer_initializes_unseen_words_randomly_not_zero(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""word"")\n        vocab.add_token_to_namespace(""word2"")\n        embeddings_filename = str(self.TEST_DIR / ""embeddings.gz"")\n        with gzip.open(embeddings_filename, ""wb"") as embeddings_file:\n            embeddings_file.write(""word 1.0 2.3 -1.0\\n"".encode(""utf-8""))\n        params = Params({""pretrained_file"": embeddings_filename, ""embedding_dim"": 3})\n        embedding_layer = Embedding.from_params(params, vocab=vocab)\n        word_vector = embedding_layer.weight.data[vocab.get_token_index(""word2"")]\n        assert not numpy.allclose(word_vector.numpy(), numpy.array([0.0, 0.0, 0.0]))\n\n    def test_read_hdf5_format_file(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""word"")\n        vocab.add_token_to_namespace(""word2"")\n        embeddings_filename = str(self.TEST_DIR / ""embeddings.hdf5"")\n        embeddings = numpy.random.rand(vocab.get_vocab_size(), 5)\n        with h5py.File(embeddings_filename, ""w"") as fout:\n            _ = fout.create_dataset(""embedding"", embeddings.shape, dtype=""float32"", data=embeddings)\n\n        params = Params({""pretrained_file"": embeddings_filename, ""embedding_dim"": 5})\n        embedding_layer = Embedding.from_params(params, vocab=vocab)\n        assert numpy.allclose(embedding_layer.weight.data.numpy(), embeddings)\n\n    def test_read_hdf5_raises_on_invalid_shape(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""word"")\n        embeddings_filename = str(self.TEST_DIR / ""embeddings.hdf5"")\n        embeddings = numpy.random.rand(vocab.get_vocab_size(), 10)\n        with h5py.File(embeddings_filename, ""w"") as fout:\n            _ = fout.create_dataset(""embedding"", embeddings.shape, dtype=""float32"", data=embeddings)\n\n        params = Params({""pretrained_file"": embeddings_filename, ""embedding_dim"": 5})\n        with pytest.raises(ConfigurationError):\n            _ = Embedding.from_params(params, vocab=vocab)\n\n    def test_read_embedding_file_inside_archive(self):\n        token2vec = {\n            ""think"": torch.Tensor([0.143, 0.189, 0.555, 0.361, 0.472]),\n            ""make"": torch.Tensor([0.878, 0.651, 0.044, 0.264, 0.872]),\n            ""difference"": torch.Tensor([0.053, 0.162, 0.671, 0.110, 0.259]),\n            ""\xc3\xa0\xc3\xa8\xc3\xac\xc3\xb2\xc3\xb9"": torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0]),\n        }\n        vocab = Vocabulary()\n        for token in token2vec:\n            vocab.add_token_to_namespace(token)\n\n        params = Params(\n            {\n                ""pretrained_file"": str(self.FIXTURES_ROOT / ""embeddings/multi-file-archive.zip""),\n                ""embedding_dim"": 5,\n            }\n        )\n        with pytest.raises(\n            ValueError,\n            match=""The archive .*/embeddings/multi-file-archive.zip contains multiple files, ""\n            ""so you must select one of the files inside ""\n            ""providing a uri of the type: ""\n            ""\\\\(path_or_url_to_archive\\\\)#path_inside_archive\\\\."",\n        ):\n            Embedding.from_params(params, vocab=vocab)\n\n        for ext in ["".zip"", "".tar.gz""]:\n            archive_path = str(self.FIXTURES_ROOT / ""embeddings/multi-file-archive"") + ext\n            file_uri = format_embeddings_file_uri(archive_path, ""folder/fake_embeddings.5d.txt"")\n            params = Params({""pretrained_file"": file_uri, ""embedding_dim"": 5})\n            embeddings = Embedding.from_params(params, vocab=vocab).weight.data\n            for tok, vec in token2vec.items():\n                i = vocab.get_token_index(tok)\n                assert torch.equal(embeddings[i], vec), ""Problem with format "" + archive_path\n\n    def test_embeddings_text_file(self):\n        txt_path = str(self.FIXTURES_ROOT / ""utf-8_sample/utf-8_sample.txt"")\n\n        # This is for sure a correct way to read an utf-8 encoded text file\n        with open(txt_path, ""rt"", encoding=""utf-8"") as f:\n            correct_text = f.read()\n\n        # Check if we get the correct text on plain and compressed versions of the file\n        paths = [txt_path] + [txt_path + ext for ext in ["".gz"", "".zip""]]\n        for path in paths:\n            with EmbeddingsTextFile(path) as f:\n                text = f.read()\n            assert text == correct_text, ""Test failed for file: "" + path\n\n        # Check for a file contained inside an archive with multiple files\n        for ext in ["".zip"", "".tar.gz"", "".tar.bz2"", "".tar.lzma""]:\n            archive_path = str(self.FIXTURES_ROOT / ""utf-8_sample/archives/utf-8"") + ext\n            file_uri = format_embeddings_file_uri(archive_path, ""folder/utf-8_sample.txt"")\n            with EmbeddingsTextFile(file_uri) as f:\n                text = f.read()\n            assert text == correct_text, ""Test failed for file: "" + archive_path\n\n        # Passing a second level path when not reading an archive\n        with pytest.raises(ValueError):\n            with EmbeddingsTextFile(format_embeddings_file_uri(txt_path, ""a/fake/path"")):\n                pass\n\n    def test_embeddings_text_file_num_tokens(self):\n        test_filename = str(self.TEST_DIR / ""temp_embeddings.vec"")\n\n        def check_num_tokens(first_line, expected_num_tokens):\n            with open(test_filename, ""w"") as f:\n                f.write(first_line)\n            with EmbeddingsTextFile(test_filename) as f:\n                assert (\n                    f.num_tokens == expected_num_tokens\n                ), f""Wrong num tokens for line: {first_line}""\n\n        valid_header_lines = [""1000000 300"", ""300 1000000"", ""1000000""]\n        for line in valid_header_lines:\n            check_num_tokens(line, expected_num_tokens=1_000_000)\n\n        not_header_lines = [""hello 1"", ""hello 1 2"", ""111 222 333"", ""111 222 hello""]\n        for line in not_header_lines:\n            check_num_tokens(line, expected_num_tokens=None)\n\n    def test_decode_embeddings_file_uri(self):\n        first_level_paths = [\n            ""path/to/embeddings.gz"",\n            ""unicode/path/\xc3\xb2\xc3\xa0\xc3\xa8+\xc3\xb9.vec"",\n            ""http://www.embeddings.com/path/to/embeddings.gz"",\n            ""http://www.embeddings.com/\xc3\xa0\xc3\xa8\xc3\xac\xc3\xb2\xc3\xb9?query=blabla.zip"",\n        ]\n        second_level_paths = [""path/to/glove.27B.300d.vec"", ""\xc3\xb2\xc3\xa0\xc3\xa8+\xc3\xb9.vec"", ""crawl-300d-2M.vec""]\n\n        for simple_path in first_level_paths:\n            assert parse_embeddings_file_uri(simple_path) == (simple_path, None)\n\n        for path1, path2 in zip(first_level_paths, second_level_paths):\n            uri = format_embeddings_file_uri(path1, path2)\n            decoded = parse_embeddings_file_uri(uri)\n            assert decoded == (path1, path2)\n\n    def test_embedding_vocab_extension_with_specified_namespace(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""word1"", ""tokens_a"")\n        vocab.add_token_to_namespace(""word2"", ""tokens_a"")\n        embedding_params = Params({""vocab_namespace"": ""tokens_a"", ""embedding_dim"": 10})\n        embedder = Embedding.from_params(embedding_params, vocab=vocab)\n        original_weight = embedder.weight\n\n        assert original_weight.shape[0] == 4\n\n        extension_counter = {""tokens_a"": {""word3"": 1}}\n        vocab._extend(extension_counter)\n\n        embedder.extend_vocab(vocab, ""tokens_a"")  # specified namespace\n\n        extended_weight = embedder.weight\n        assert extended_weight.shape[0] == 5\n        assert torch.all(extended_weight[:4, :] == original_weight[:4, :])\n\n    def test_embedding_vocab_extension_with_default_namespace(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""word1"")\n        vocab.add_token_to_namespace(""word2"")\n        embedding_params = Params({""vocab_namespace"": ""tokens"", ""embedding_dim"": 10})\n        embedder = Embedding.from_params(embedding_params, vocab=vocab)\n        original_weight = embedder.weight\n\n        assert original_weight.shape[0] == 4\n\n        extension_counter = {""tokens"": {""word3"": 1}}\n        vocab._extend(extension_counter)\n\n        embedder.extend_vocab(vocab)  # default namespace\n\n        extended_weight = embedder.weight\n        assert extended_weight.shape[0] == 5\n        assert torch.all(extended_weight[:4, :] == original_weight[:4, :])\n\n    def test_embedding_vocab_extension_without_stored_namespace(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""word1"", ""tokens_a"")\n        vocab.add_token_to_namespace(""word2"", ""tokens_a"")\n        embedding_params = Params({""vocab_namespace"": ""tokens_a"", ""embedding_dim"": 10})\n        embedder = Embedding.from_params(embedding_params, vocab=vocab)\n\n        # Previous models won\'t have _vocab_namespace attribute. Force it to be None\n        embedder._vocab_namespace = None\n        original_weight = embedder.weight\n\n        assert original_weight.shape[0] == 4\n\n        extension_counter = {""tokens_a"": {""word3"": 1}}\n        vocab._extend(extension_counter)\n\n        embedder.extend_vocab(vocab, ""tokens_a"")  # specified namespace\n\n        extended_weight = embedder.weight\n        assert extended_weight.shape[0] == 5\n        assert torch.all(extended_weight[:4, :] == original_weight[:4, :])\n\n    def test_embedding_vocab_extension_works_with_pretrained_embedding_file(self):\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""word1"")\n        vocab.add_token_to_namespace(""word2"")\n\n        embeddings_filename = str(self.TEST_DIR / ""embeddings2.gz"")\n        with gzip.open(embeddings_filename, ""wb"") as embeddings_file:\n            embeddings_file.write(""word3 0.5 0.3 -6.0\\n"".encode(""utf-8""))\n            embeddings_file.write(""word4 1.0 2.3 -1.0\\n"".encode(""utf-8""))\n            embeddings_file.write(""word2 0.1 0.4 -4.0\\n"".encode(""utf-8""))\n            embeddings_file.write(""word1 1.0 2.3 -1.0\\n"".encode(""utf-8""))\n\n        embedding_params = Params(\n            {\n                ""vocab_namespace"": ""tokens"",\n                ""embedding_dim"": 3,\n                ""pretrained_file"": embeddings_filename,\n            }\n        )\n        embedder = Embedding.from_params(embedding_params, vocab=vocab)\n\n        # Change weight to simulate embedding training\n        embedder.weight.data += 1\n        assert torch.all(\n            embedder.weight[2:, :] == torch.Tensor([[2.0, 3.3, 0.0], [1.1, 1.4, -3.0]])\n        )\n        original_weight = embedder.weight\n\n        assert tuple(original_weight.size()) == (4, 3)  # 4 because of padding and OOV\n\n        vocab.add_token_to_namespace(""word3"")\n        embedder.extend_vocab(\n            vocab, extension_pretrained_file=embeddings_filename\n        )  # default namespace\n        extended_weight = embedder.weight\n\n        # Make sure extenstion happened for extra token in extended vocab\n        assert tuple(extended_weight.size()) == (5, 3)\n\n        # Make sure extension doesn\'t change original trained weights.\n        assert torch.all(original_weight[:4, :] == extended_weight[:4, :])\n\n        # Make sure extended weight is taken from the embedding file.\n        assert torch.all(extended_weight[4, :] == torch.Tensor([0.5, 0.3, -6.0]))\n\n    def test_embedding_vocab_extension_is_no_op_when_extension_should_not_happen(self):\n        # Case1: When vocab is already in sync with embeddings it should be a no-op.\n        vocab = Vocabulary({""tokens"": {""word1"": 1, ""word2"": 1}})\n        embedding_params = Params({""vocab_namespace"": ""tokens"", ""embedding_dim"": 10})\n        embedder = Embedding.from_params(embedding_params, vocab=vocab)\n        original_weight = embedder.weight\n        embedder.extend_vocab(vocab, ""tokens"")\n        assert torch.all(embedder.weight == original_weight)\n\n        # Case2: Shouldn\'t wrongly assuming ""tokens"" namespace for extension if no\n        # information on vocab_namespece is available. Rather log a warning and be a no-op.\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""word1"", ""tokens"")\n        vocab.add_token_to_namespace(""word2"", ""tokens"")\n        embedding_params = Params({""vocab_namespace"": ""tokens"", ""embedding_dim"": 10})\n        embedder = Embedding.from_params(embedding_params, vocab=vocab)\n        # Previous models won\'t have _vocab_namespace attribute. Force it to be None\n        embedder._vocab_namespace = None\n        embedder.weight = torch.nn.Parameter(embedder.weight[:1, :])\n        assert embedder.weight.shape[0] == 1\n        embedder.extend_vocab(vocab)  # Don\'t specify namespace\n        assert embedder.weight.shape[0] == 1\n\n    def test_embedding_vocab_extension_raises_error_for_incorrect_vocab(self):\n        # When vocab namespace of extension vocab is smaller than embeddings\n        # it should raise configuration error.\n        vocab = Vocabulary({""tokens"": {""word1"": 1, ""word2"": 1}})\n        embedding_params = Params({""vocab_namespace"": ""tokens"", ""embedding_dim"": 10})\n        embedder = Embedding.from_params(embedding_params, vocab=vocab)\n        with pytest.raises(ConfigurationError):\n            embedder.extend_vocab(Vocabulary(), ""tokens"")\n\n    def test_embedding_constructed_directly_with_pretrained_file(self):\n\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""word"")\n        vocab.add_token_to_namespace(""word2"")\n        unicode_space = ""\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0""\n        vocab.add_token_to_namespace(unicode_space)\n        embeddings_filename = str(self.TEST_DIR / ""embeddings.gz"")\n        with gzip.open(embeddings_filename, ""wb"") as embeddings_file:\n            embeddings_file.write(""word 1.0 2.3 -1.0\\n"".encode(""utf-8""))\n            embeddings_file.write(f""{unicode_space} 3.4 3.3 5.0\\n"".encode(""utf-8""))\n\n        num_embeddings = vocab.get_vocab_size()\n        embedding_layer = Embedding(\n            embedding_dim=3,\n            num_embeddings=num_embeddings,\n            pretrained_file=embeddings_filename,\n            vocab=vocab,\n        )\n        word_vector = embedding_layer.weight.data[vocab.get_token_index(""word"")]\n        assert numpy.allclose(word_vector.numpy(), numpy.array([1.0, 2.3, -1.0]))\n        word_vector = embedding_layer.weight.data[vocab.get_token_index(unicode_space)]\n        assert numpy.allclose(word_vector.numpy(), numpy.array([3.4, 3.3, 5.0]))\n        word_vector = embedding_layer.weight.data[vocab.get_token_index(""word2"")]\n        assert not numpy.allclose(word_vector.numpy(), numpy.array([1.0, 2.3, -1.0]))\n'"
tests/modules/token_embedders/pass_through_embedder_test.py,1,"b'import numpy\nimport torch\nfrom allennlp.modules.token_embedders import PassThroughTokenEmbedder\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestBagOfWordCountsTokenEmbedder(AllenNlpTestCase):\n    def test_pass_through_embedder(self):\n        embedder = PassThroughTokenEmbedder(3)\n        tensor = torch.randn([4, 3])\n        numpy.testing.assert_equal(tensor.numpy(), embedder(tensor).numpy())\n        assert embedder.get_output_dim() == 3\n'"
tests/modules/token_embedders/pretrained_transformer_embedder_test.py,14,"b'import math\nimport pytest\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import Vocabulary\nfrom allennlp.data.batch import Batch\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import PretrainedTransformerIndexer\nfrom allennlp.data.tokenizers import PretrainedTransformerTokenizer\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import PretrainedTransformerEmbedder\n\n\nclass TestPretrainedTransformerEmbedder(AllenNlpTestCase):\n    def test_forward_runs_when_initialized_from_params(self):\n        # This code just passes things off to `transformers`, so we only have a very simple\n        # test.\n        params = Params({""model_name"": ""bert-base-uncased""})\n        embedder = PretrainedTransformerEmbedder.from_params(params)\n        token_ids = torch.randint(0, 100, (1, 4))\n        mask = torch.randint(0, 2, (1, 4)).bool()\n        output = embedder(token_ids=token_ids, mask=mask)\n        assert tuple(output.size()) == (1, 4, 768)\n\n    @pytest.mark.parametrize(""train_parameters"", [True, False])\n    def test_end_to_end(self, train_parameters: bool):\n        tokenizer = PretrainedTransformerTokenizer(model_name=""bert-base-uncased"")\n        token_indexer = PretrainedTransformerIndexer(model_name=""bert-base-uncased"")\n\n        sentence1 = ""A, AllenNLP sentence.""\n        tokens1 = tokenizer.tokenize(sentence1)\n        expected_tokens1 = [""[CLS]"", ""a"", "","", ""allen"", ""##nl"", ""##p"", ""sentence"", ""."", ""[SEP]""]\n        assert [t.text for t in tokens1] == expected_tokens1\n\n        sentence2 = ""AllenNLP is great""\n        tokens2 = tokenizer.tokenize(sentence2)\n        expected_tokens2 = [""[CLS]"", ""allen"", ""##nl"", ""##p"", ""is"", ""great"", ""[SEP]""]\n        assert [t.text for t in tokens2] == expected_tokens2\n\n        vocab = Vocabulary()\n\n        params = Params(\n            {\n                ""token_embedders"": {\n                    ""bert"": {\n                        ""type"": ""pretrained_transformer"",\n                        ""model_name"": ""bert-base-uncased"",\n                        ""train_parameters"": train_parameters,\n                    }\n                }\n            }\n        )\n        token_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=params)\n\n        instance1 = Instance({""tokens"": TextField(tokens1, {""bert"": token_indexer})})\n        instance2 = Instance({""tokens"": TextField(tokens2, {""bert"": token_indexer})})\n\n        batch = Batch([instance1, instance2])\n        batch.index_instances(vocab)\n\n        padding_lengths = batch.get_padding_lengths()\n        tensor_dict = batch.as_tensor_dict(padding_lengths)\n        tokens = tensor_dict[""tokens""]\n        max_length = max(len(tokens1), len(tokens2))\n\n        assert tokens[""bert""][""token_ids""].shape == (2, max_length)\n\n        assert tokens[""bert""][""mask""].tolist() == [\n            [True, True, True, True, True, True, True, True, True],\n            [True, True, True, True, True, True, True, False, False],\n        ]\n\n        # Attention mask\n        bert_vectors = token_embedder(tokens)\n        assert bert_vectors.size() == (2, 9, 768)\n        assert bert_vectors.requires_grad == train_parameters\n\n    def test_big_token_type_ids(self):\n        token_embedder = PretrainedTransformerEmbedder(""roberta-base"")\n        token_ids = torch.LongTensor([[1, 2, 3], [2, 3, 4]])\n        mask = torch.ones_like(token_ids).bool()\n        type_ids = torch.zeros_like(token_ids)\n        type_ids[1, 1] = 1\n        with pytest.raises(ValueError):\n            token_embedder(token_ids, mask, type_ids)\n\n    def test_xlnet_token_type_ids(self):\n        token_embedder = PretrainedTransformerEmbedder(""xlnet-base-cased"")\n        token_ids = torch.LongTensor([[1, 2, 3], [2, 3, 4]])\n        mask = torch.ones_like(token_ids).bool()\n        type_ids = torch.zeros_like(token_ids)\n        type_ids[1, 1] = 1\n        token_embedder(token_ids, mask, type_ids)\n\n    def test_long_sequence_splitting_end_to_end(self):\n        # Mostly the same as the end_to_end test (except for adding max_length=4),\n        # because we don\'t want this splitting behavior to change input/output format.\n\n        tokenizer = PretrainedTransformerTokenizer(model_name=""bert-base-uncased"")\n        token_indexer = PretrainedTransformerIndexer(model_name=""bert-base-uncased"", max_length=4)\n\n        sentence1 = ""A, AllenNLP sentence.""\n        tokens1 = tokenizer.tokenize(sentence1)\n        sentence2 = ""AllenNLP is great""\n        tokens2 = tokenizer.tokenize(sentence2)\n\n        vocab = Vocabulary()\n\n        params = Params(\n            {\n                ""token_embedders"": {\n                    ""bert"": {\n                        ""type"": ""pretrained_transformer"",\n                        ""model_name"": ""bert-base-uncased"",\n                        ""max_length"": 4,\n                    }\n                }\n            }\n        )\n        token_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=params)\n\n        instance1 = Instance({""tokens"": TextField(tokens1, {""bert"": token_indexer})})\n        instance2 = Instance({""tokens"": TextField(tokens2, {""bert"": token_indexer})})\n\n        batch = Batch([instance1, instance2])\n        batch.index_instances(vocab)\n\n        padding_lengths = batch.get_padding_lengths()\n        tensor_dict = batch.as_tensor_dict(padding_lengths)\n        tokens = tensor_dict[""tokens""]\n        max_length = max(len(tokens1), len(tokens2))\n\n        # Adds n_segments * 2 special tokens\n        segment_concat_length = int(math.ceil(max_length / 4)) * 2 + max_length\n        assert tokens[""bert""][""token_ids""].shape == (2, segment_concat_length)\n\n        assert tokens[""bert""][""mask""].tolist() == [\n            [True, True, True, True, True, True, True, True, True],\n            [True, True, True, True, True, True, True, False, False],\n        ]\n        assert tokens[""bert""][""segment_concat_mask""].tolist() == [\n            [True] * segment_concat_length,\n            [True] * (segment_concat_length - 4) + [False] * 4,  # 4 is hard-coded length difference\n        ]\n\n        # Attention mask\n        bert_vectors = token_embedder(tokens)\n        assert bert_vectors.size() == (2, 9, 768)\n\n    def test_fold_long_sequences(self):\n        # Let\'s just say [PAD] is 0, [CLS] is 1, and [SEP] is 2\n        token_ids = torch.LongTensor(\n            [\n                [1, 101, 102, 103, 104, 2, 1, 105, 106, 107, 108, 2, 1, 109, 2],\n                [1, 201, 202, 203, 204, 2, 1, 205, 206, 207, 208, 2, 0, 0, 0],\n                [1, 301, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            ]\n        )  # Shape: [3, 15]\n        segment_concat_mask = (token_ids > 0).long()\n\n        folded_token_ids = torch.LongTensor(\n            [\n                [1, 101, 102, 103, 104, 2],\n                [1, 105, 106, 107, 108, 2],\n                [1, 109, 2, 0, 0, 0],\n                [1, 201, 202, 203, 204, 2],\n                [1, 205, 206, 207, 208, 2],\n                [0, 0, 0, 0, 0, 0],\n                [1, 301, 2, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0],\n            ]\n        )\n        folded_segment_concat_mask = (folded_token_ids > 0).long()\n\n        token_embedder = PretrainedTransformerEmbedder(""bert-base-uncased"", max_length=6)\n\n        (\n            folded_token_ids_out,\n            folded_segment_concat_mask_out,\n            _,\n        ) = token_embedder._fold_long_sequences(token_ids, segment_concat_mask)\n        assert (folded_token_ids_out == folded_token_ids).all()\n        assert (folded_segment_concat_mask_out == folded_segment_concat_mask).all()\n\n    def test_unfold_long_sequences(self):\n        # Let\'s just say [PAD] is 0, [CLS] is xxx1, and [SEP] is xxx2\n        # We assume embeddings are 1-dim and are the same as indices\n        embeddings = torch.LongTensor(\n            [\n                [1001, 101, 102, 103, 104, 1002],\n                [1011, 105, 106, 107, 108, 1012],\n                [1021, 109, 1022, 0, 0, 0],\n                [2001, 201, 202, 203, 204, 2002],\n                [2011, 205, 206, 207, 208, 2012],\n                [0, 0, 0, 0, 0, 0],\n                [3001, 301, 3002, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0],\n            ]\n        ).unsqueeze(-1)\n        mask = (embeddings > 0).long()\n\n        unfolded_embeddings = torch.LongTensor(\n            [\n                [1001, 101, 102, 103, 104, 105, 106, 107, 108, 109, 1022],\n                [2001, 201, 202, 203, 204, 205, 206, 207, 208, 2012, 0],\n                [3001, 301, 3002, 0, 0, 0, 0, 0, 0, 0, 0],\n            ]\n        ).unsqueeze(-1)\n\n        token_embedder = PretrainedTransformerEmbedder(""bert-base-uncased"", max_length=6)\n\n        unfolded_embeddings_out = token_embedder._unfold_long_sequences(\n            embeddings, mask, unfolded_embeddings.size(0), 15\n        )\n        assert (unfolded_embeddings_out == unfolded_embeddings).all()\n\n    def test_encoder_decoder_model(self):\n        token_embedder = PretrainedTransformerEmbedder(""facebook/bart-large"", sub_module=""encoder"")\n        token_ids = torch.LongTensor([[1, 2, 3], [2, 3, 4]])\n        mask = torch.ones_like(token_ids).bool()\n        token_embedder(token_ids, mask)\n'"
tests/modules/token_embedders/pretrained_transformer_mismatched_embedder_test.py,3,"b'import pytest\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.data import Token, Vocabulary\nfrom allennlp.data.batch import Batch\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import PretrainedTransformerMismatchedIndexer\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestPretrainedTransformerMismatchedEmbedder(AllenNlpTestCase):\n    @pytest.mark.parametrize(""train_parameters"", [True, False])\n    def test_end_to_end(self, train_parameters: bool):\n        token_indexer = PretrainedTransformerMismatchedIndexer(""bert-base-uncased"")\n\n        sentence1 = [""A"", "","", ""AllenNLP"", ""sentence"", "".""]\n        sentence2 = [""AllenNLP"", ""is"", ""great""]\n        tokens1 = [Token(word) for word in sentence1]\n        tokens2 = [Token(word) for word in sentence2]\n\n        vocab = Vocabulary()\n\n        params = Params(\n            {\n                ""token_embedders"": {\n                    ""bert"": {\n                        ""type"": ""pretrained_transformer_mismatched"",\n                        ""model_name"": ""bert-base-uncased"",\n                        ""train_parameters"": train_parameters,\n                    }\n                }\n            }\n        )\n        token_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=params)\n\n        instance1 = Instance({""tokens"": TextField(tokens1, {""bert"": token_indexer})})\n        instance2 = Instance({""tokens"": TextField(tokens2, {""bert"": token_indexer})})\n\n        batch = Batch([instance1, instance2])\n        batch.index_instances(vocab)\n\n        padding_lengths = batch.get_padding_lengths()\n        tensor_dict = batch.as_tensor_dict(padding_lengths)\n        tokens = tensor_dict[""tokens""]\n\n        assert tokens[""bert""][""offsets""].tolist() == [\n            [[1, 1], [2, 2], [3, 5], [6, 6], [7, 7]],\n            [[1, 3], [4, 4], [5, 5], [0, 0], [0, 0]],\n        ]\n\n        # Attention mask\n        bert_vectors = token_embedder(tokens)\n        assert bert_vectors.size() == (2, max(len(sentence1), len(sentence2)), 768)\n        assert not torch.isnan(bert_vectors).any()\n        assert bert_vectors.requires_grad == train_parameters\n\n    def test_long_sequence_splitting_end_to_end(self):\n        token_indexer = PretrainedTransformerMismatchedIndexer(""bert-base-uncased"", max_length=4)\n\n        sentence1 = [""A"", "","", ""AllenNLP"", ""sentence"", "".""]\n        sentence2 = [""AllenNLP"", ""is"", ""great""]\n        tokens1 = [Token(word) for word in sentence1]\n        tokens2 = [Token(word) for word in sentence2]\n\n        vocab = Vocabulary()\n\n        params = Params(\n            {\n                ""token_embedders"": {\n                    ""bert"": {\n                        ""type"": ""pretrained_transformer_mismatched"",\n                        ""model_name"": ""bert-base-uncased"",\n                        ""max_length"": 4,\n                    }\n                }\n            }\n        )\n        token_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=params)\n\n        instance1 = Instance({""tokens"": TextField(tokens1, {""bert"": token_indexer})})\n        instance2 = Instance({""tokens"": TextField(tokens2, {""bert"": token_indexer})})\n\n        batch = Batch([instance1, instance2])\n        batch.index_instances(vocab)\n\n        padding_lengths = batch.get_padding_lengths()\n        tensor_dict = batch.as_tensor_dict(padding_lengths)\n        tokens = tensor_dict[""tokens""]\n\n        assert tokens[""bert""][""mask""].tolist() == [\n            [True, True, True, True, True],\n            [True, True, True, False, False],\n        ]\n        assert tokens[""bert""][""offsets""].tolist() == [\n            [[1, 1], [2, 2], [3, 5], [6, 6], [7, 7]],\n            [[1, 3], [4, 4], [5, 5], [0, 0], [0, 0]],\n        ]\n\n        bert_vectors = token_embedder(tokens)\n        assert bert_vectors.size() == (2, max(len(sentence1), len(sentence2)), 768)\n        assert not torch.isnan(bert_vectors).any()\n\n    def test_token_without_wordpieces(self):\n        token_indexer = PretrainedTransformerMismatchedIndexer(""bert-base-uncased"")\n\n        sentence1 = [""A"", """", ""AllenNLP"", ""sentence"", "".""]\n        sentence2 = [""AllenNLP"", """", ""great""]\n        tokens1 = [Token(word) for word in sentence1]\n        tokens2 = [Token(word) for word in sentence2]\n        vocab = Vocabulary()\n        params = Params(\n            {\n                ""token_embedders"": {\n                    ""bert"": {\n                        ""type"": ""pretrained_transformer_mismatched"",\n                        ""model_name"": ""bert-base-uncased"",\n                    }\n                }\n            }\n        )\n        token_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=params)\n\n        instance1 = Instance({""tokens"": TextField(tokens1, {""bert"": token_indexer})})\n        instance2 = Instance({""tokens"": TextField(tokens2, {""bert"": token_indexer})})\n\n        batch = Batch([instance1, instance2])\n        batch.index_instances(vocab)\n\n        padding_lengths = batch.get_padding_lengths()\n        tensor_dict = batch.as_tensor_dict(padding_lengths)\n        tokens = tensor_dict[""tokens""]\n\n        assert tokens[""bert""][""offsets""].tolist() == [\n            [[1, 1], [-1, -1], [2, 4], [5, 5], [6, 6]],\n            [[1, 3], [-1, -1], [4, 4], [0, 0], [0, 0]],\n        ]\n\n        bert_vectors = token_embedder(tokens)\n        assert bert_vectors.size() == (2, max(len(sentence1), len(sentence2)), 768)\n        assert not torch.isnan(bert_vectors).any()\n        assert all(bert_vectors[0, 1] == 0)\n        assert all(bert_vectors[1, 1] == 0)\n'"
tests/modules/token_embedders/token_characters_encoder_test.py,1,"b'from copy import deepcopy\n\nimport numpy\nfrom numpy.testing import assert_almost_equal\nimport torch\n\nfrom allennlp.common import Params\nfrom allennlp.data import Vocabulary\nfrom allennlp.modules import Seq2VecEncoder\nfrom allennlp.modules.token_embedders import Embedding, TokenCharactersEncoder\nfrom allennlp.nn import InitializerApplicator, Initializer\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\nclass TestTokenCharactersEncoder(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.vocab = Vocabulary()\n        self.vocab.add_token_to_namespace(""1"", ""token_characters"")\n        self.vocab.add_token_to_namespace(""2"", ""token_characters"")\n        self.vocab.add_token_to_namespace(""3"", ""token_characters"")\n        self.vocab.add_token_to_namespace(""4"", ""token_characters"")\n        params = Params(\n            {\n                ""embedding"": {""embedding_dim"": 2, ""vocab_namespace"": ""token_characters""},\n                ""encoder"": {\n                    ""type"": ""cnn"",\n                    ""embedding_dim"": 2,\n                    ""num_filters"": 4,\n                    ""ngram_filter_sizes"": [1, 2],\n                    ""output_dim"": 3,\n                },\n            }\n        )\n        self.encoder = TokenCharactersEncoder.from_params(vocab=self.vocab, params=deepcopy(params))\n        self.embedding = Embedding.from_params(vocab=self.vocab, params=params[""embedding""])\n        self.inner_encoder = Seq2VecEncoder.from_params(params[""encoder""])\n        constant_init = Initializer.from_params(Params({""type"": ""constant"", ""val"": 1.0}))\n        initializer = InitializerApplicator([("".*"", constant_init)])\n        initializer(self.encoder)\n        initializer(self.embedding)\n        initializer(self.inner_encoder)\n\n    def test_get_output_dim_uses_encoder_output_dim(self):\n        assert self.encoder.get_output_dim() == 3\n\n    def test_forward_applies_embedding_then_encoder(self):\n        numpy_tensor = numpy.random.randint(6, size=(3, 4, 7))\n        inputs = torch.from_numpy(numpy_tensor)\n        encoder_output = self.encoder(inputs)\n        reshaped_input = inputs.view(12, 7)\n        embedded = self.embedding(reshaped_input)\n        mask = (inputs != 0).long().view(12, 7)\n        reshaped_manual_output = self.inner_encoder(embedded, mask)\n        manual_output = reshaped_manual_output.view(3, 4, 3)\n        assert_almost_equal(encoder_output.data.numpy(), manual_output.data.numpy())\n'"
tests/training/learning_rate_schedulers/__init__.py,0,b''
tests/training/learning_rate_schedulers/cosine_test.py,2,"b'from copy import deepcopy\nfrom typing import Dict, Any\n\nimport torch\nimport pytest\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.training.learning_rate_schedulers import LearningRateScheduler\nfrom allennlp.training.optimizers import Optimizer\n\n\nclass CosineWithRestartsTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.model = torch.nn.Sequential(torch.nn.Linear(10, 10))\n\n        # We use these cases to verify that the scheduler works as expected.\n        # Each case consists of 5 parameters:\n        # - epochs: the total # of epochs to run for.\n        # - params: parameters passed to initialize the scheduler.\n        # - learning rate checks: a list of tuples, each of which specifies an epoch\n        #   number and the expected value of the learning rate at that epoch.\n        # - checkpoints: a list of epoch numbers at which to save the scheduler\n        #   state, and then restore from the saved state and resume.\n        self.cosine_schedule_cases = [\n            (\n                30,\n                {""t_initial"": 30, ""t_mul"": 1.0},\n                [(0, 1.0), (15, 0.5000000000000001), (29, 0.0027390523158632996)],\n                [10, 14],\n            ),\n            (10, {""t_initial"": 1, ""t_mul"": 2.0}, [(0, 1.0), (1, 1.0), (2, 0.5), (3, 1.0)], [1, 3]),\n            (30, {""t_initial"": 1, ""t_mul"": 1.0}, [(0, 1.0), (15, 1.0), (29, 1.0)], []),\n            (\n                60,\n                {""t_initial"": 30, ""t_mul"": 1.0},\n                [\n                    (0, 1.0),\n                    (15, 0.5000000000000001),\n                    (29, 0.0027390523158632996),\n                    (30, 1.0),\n                    (45, 0.5000000000000001),\n                    (59, 0.0027390523158632996),\n                ],\n                [30, 35],\n            ),\n            (\n                60,\n                {""t_initial"": 30, ""t_mul"": 1.0, ""eta_mul"": 0.5},\n                [(0, 1.0), (15, 0.5000000000000001), (29, 0.0027390523158632996), (30, 0.5)],\n                [],\n            ),\n            (\n                100,\n                {""t_initial"": 30, ""t_mul"": 1.5},\n                [(0, 1.0), (29, 0.0027390523158632996), (30, 1.0), (74, 0.0012179748700879012)],\n                [],\n            ),\n            (\n                210,\n                {""t_initial"": 30, ""t_mul"": 2},\n                [\n                    (0, 1.0),\n                    (29, 0.0027390523158632996),\n                    (30, 1.0),\n                    (89, 0.0006852326227130834),\n                    (90, 1.0),\n                    (209, 0.00017133751222137006),\n                ],\n                [],\n            ),\n            (\n                210,\n                {""t_initial"": 30, ""t_mul"": 2, ""eta_mul"": 0.5},\n                [(0, 1.0), (30, 0.5), (90, 0.25)],\n                [29, 90],\n            ),\n            (\n                150,\n                {""t_initial"": 30, ""t_mul"": 1},\n                [\n                    (0, 1.0),\n                    (29, 0.0027390523158632996),\n                    (30, 1.0),\n                    (59, 0.0027390523158632996),\n                    (60, 1.0),\n                    (89, 0.0027390523158632996),\n                    (90, 1.0),\n                ],\n                [],\n            ),\n            (10, {""t_initial"": 1, ""t_mul"": 1, ""eta_mul"": 0.5}, [(0, 1.0), (1, 0.5), (2, 0.25)], []),\n        ]\n\n    def _get_optimizer(self, lr: float = 1.0):\n        return Optimizer.from_params(\n            model_parameters=self.model.named_parameters(), params=Params({""type"": ""sgd"", ""lr"": lr})\n        )\n\n    def test_from_params(self):\n        """"""Make sure `from_params` initializes an instance properly.""""""\n        optim = self._get_optimizer()\n        sched = LearningRateScheduler.from_params(\n            optimizer=optim, params=Params({""type"": ""cosine"", ""t_initial"": 5})\n        )\n\n        assert sched.t_initial == 5\n        assert sched.last_epoch == -1\n\n        # Learning should be unchanged after initializing scheduler.\n        assert optim.param_groups[0][""lr""] == 1.0\n\n        with pytest.raises(ConfigurationError):\n            # t_initial is required.\n            LearningRateScheduler.from_params(optimizer=optim, params=Params({""type"": ""cosine""}))\n\n    def test_schedules(self):\n        """"""Make sure the math is correct.""""""\n        for epochs, params, lr_checks, _ in self.cosine_schedule_cases:\n            optimizer = self._get_optimizer()\n            params[""type""] = ""cosine""\n            scheduler = LearningRateScheduler.from_params(\n                optimizer=optimizer, params=Params(params)\n            )\n            lrs = [optimizer.param_groups[0][""lr""]]\n            for _ in range(epochs):\n                scheduler.step()\n                lrs.append(optimizer.param_groups[0][""lr""])\n\n            for it, lr in lr_checks:\n                assert lrs[it] == lr, f""Iteration {it}: {lrs[it]} != {lr}""\n\n    def test_schedules_with_save_and_resume(self):\n        """"""Make sure scheduler will resume with the right state.""""""\n\n        def init_and_restore_scheduler(\n            optimizer: torch.optim.Optimizer,\n            params: Dict[str, Any],\n            state_dict: Dict[str, Any] = None,\n        ):\n            """"""\n            Initialize a new scheduler and optionally restore its state from\n            a checkpoint.\n            """"""\n            params[""type""] = ""cosine""\n            scheduler = LearningRateScheduler.from_params(\n                optimizer=optimizer, params=Params(deepcopy(params))\n            )\n            if state_dict is not None:\n                scheduler.load_state_dict(state_dict)\n            return scheduler\n\n        for epochs, params, lr_checks, checkpoints in self.cosine_schedule_cases:\n            optimizer = self._get_optimizer()\n            scheduler = init_and_restore_scheduler(optimizer, params)\n            state = scheduler.state_dict()\n\n            lrs = [optimizer.param_groups[0][""lr""]]\n            for epoch in range(epochs):\n                if epoch in checkpoints:\n                    # Restore scheduler from state dict.\n                    scheduler = init_and_restore_scheduler(optimizer, params, state_dict=state)\n\n                # Take step and record learning rate.\n                scheduler.step(1)\n                lrs.append(optimizer.param_groups[0][""lr""])\n\n                # Save state again.\n                state = scheduler.state_dict()\n\n            for it, lr in lr_checks:\n                assert lrs[it] == lr, f""Iteration {it}: {lrs[it]} != {lr}""\n'"
tests/training/learning_rate_schedulers/learning_rate_scheduler_test.py,1,"b'import torch\nimport pytest\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.training.optimizers import Optimizer\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.training.learning_rate_schedulers import LearningRateScheduler\nfrom allennlp.common.params import Params\n\n\nclass LearningRateSchedulersTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.model = torch.nn.Sequential(torch.nn.Linear(10, 10))\n\n    def test_reduce_on_plateau_error_throw_when_no_metrics_exist(self):\n        with pytest.raises(\n            ConfigurationError, match=""learning rate scheduler requires a validation metric""\n        ):\n            LearningRateScheduler.from_params(\n                optimizer=Optimizer.from_params(\n                    model_parameters=self.model.named_parameters(), params=Params({""type"": ""adam""})\n                ),\n                params=Params({""type"": ""reduce_on_plateau""}),\n            ).step(None)\n\n    def test_reduce_on_plateau_works_when_metrics_exist(self):\n        LearningRateScheduler.from_params(\n            optimizer=Optimizer.from_params(\n                model_parameters=self.model.named_parameters(), params=Params({""type"": ""adam""})\n            ),\n            params=Params({""type"": ""reduce_on_plateau""}),\n        ).step(10)\n\n    def test_no_metric_wrapper_can_support_none_for_metrics(self):\n        lrs = LearningRateScheduler.from_params(\n            optimizer=Optimizer.from_params(\n                model_parameters=self.model.named_parameters(), params=Params({""type"": ""adam""})\n            ),\n            params=Params({""type"": ""step"", ""step_size"": 1}),\n        )\n        lrs.lr_scheduler.optimizer.step()  # to avoid a pytorch warning\n        lrs.step(None)\n\n    def test_noam_learning_rate_schedule_does_not_crash(self):\n        lrs = LearningRateScheduler.from_params(\n            optimizer=Optimizer.from_params(\n                model_parameters=self.model.named_parameters(), params=Params({""type"": ""adam""})\n            ),\n            params=Params({""type"": ""noam"", ""model_size"": 10, ""warmup_steps"": 2000}),\n        )\n        lrs.step(None)\n        lrs.step_batch(None)\n\n    def test_polynomial_decay_works_properly(self):\n        scheduler = LearningRateScheduler.from_params(\n            optimizer=Optimizer.from_params(\n                model_parameters=self.model.named_parameters(),\n                params=Params({""type"": ""sgd"", ""lr"": 1.0}),\n            ),\n            params=Params(\n                {\n                    ""type"": ""polynomial_decay"",\n                    ""warmup_steps"": 2,\n                    ""total_steps"": 6,\n                    ""end_learning_rate"": 0.1,\n                    ""power"": 2,\n                }\n            ),\n        )\n        optimizer = scheduler.optimizer\n\n        # Linear warmup for 2 steps.\n        scheduler.step_batch()\n        assert optimizer.param_groups[0][""lr""] == 0.5  # 1.0 * 1/2\n        scheduler.step_batch()\n        assert optimizer.param_groups[0][""lr""] == 1.0  # 1.0 * 2/2\n\n        # Polynomial decay for 4 steps.\n        scheduler.step_batch()\n        assert optimizer.param_groups[0][""lr""] == 0.60625  # (1.0 - 0.1) * (3/4) ** 2 + 0.1\n        scheduler.step_batch()\n        assert optimizer.param_groups[0][""lr""] == 0.325  # (1.0 - 0.1) * (2/4) ** 2 + 0.1\n        scheduler.step_batch()\n        assert optimizer.param_groups[0][""lr""] == 0.15625  # (1.0 - 0.1) * (1/4) ** 2 + 0.1\n        scheduler.step_batch()\n        assert optimizer.param_groups[0][""lr""] == 0.1  # (1.0 - 0.1) * (0/4) ** 2 + 0.1\n\n    def test_exponential_works_properly(self):\n        scheduler = LearningRateScheduler.from_params(\n            optimizer=Optimizer.from_params(\n                model_parameters=self.model.named_parameters(),\n                params=Params({""type"": ""sgd"", ""lr"": 1.0}),\n            ),\n            params=Params({""type"": ""exponential"", ""gamma"": 0.5}),\n        )\n        optimizer = scheduler.lr_scheduler.optimizer\n        optimizer.step()  # to avoid a pytorch warning\n        # Initial learning rate should be unchanged for first epoch.\n        assert optimizer.param_groups[0][""lr""] == 1.0\n        scheduler.step()\n        assert optimizer.param_groups[0][""lr""] == 0.5\n        scheduler.step()\n        assert optimizer.param_groups[0][""lr""] == 0.5 ** 2\n        scheduler.step()\n        assert optimizer.param_groups[0][""lr""] == 0.5 ** 3\n'"
tests/training/learning_rate_schedulers/slanted_triangular_test.py,2,"b'from collections import OrderedDict\nfrom copy import deepcopy\nfrom typing import Any, Dict, List, Tuple\n\nimport torch\nimport pytest\n\nfrom allennlp.data.dataset_readers.dataset_reader import AllennlpDataset\nfrom allennlp.common import Lazy, Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data import DataLoader\nfrom allennlp.training import Trainer\nfrom allennlp.training.learning_rate_schedulers import LearningRateScheduler, SlantedTriangular\nfrom allennlp.training.optimizers import Optimizer\n\n\ndef is_hat_shaped(learning_rates: List[float]):\n    """"""\n    Check if the list of learning rates is ""hat"" shaped, i.e.,\n    increases then decreases\n    """"""\n    # sufficient conditions:\n    #   has both an increasing and decreasing segment\n    #   decrease segment occurs after increasing segment\n    #   once start decreasing, can\'t increase again\n    has_increasing_segment = False\n    has_decreasing_segment = False\n    for k in range(1, len(learning_rates)):\n        delta = learning_rates[k] - learning_rates[k - 1]\n        if delta > 1e-8:\n            has_increasing_segment = True\n            if has_decreasing_segment:\n                # can\'t increase again after hitting the max\n                return False\n        elif delta < -1e-8:\n            if not has_increasing_segment:\n                # can\'t decrease without have an increasing segment\n                return False\n            has_decreasing_segment = True\n        else:\n            # no change\n            pass\n\n    return has_increasing_segment and has_decreasing_segment\n\n\nclass SlantedTriangularTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.model = torch.nn.Sequential(\n            OrderedDict([(""lin1"", torch.nn.Linear(10, 10)), (""lin2"", torch.nn.Linear(10, 10))])\n        )\n\n    def _get_optimizer(self, lr: float = 1.0):\n        optimizer_params = Params({""type"": ""sgd"", ""lr"": lr})\n        optimizer_params[""parameter_groups""] = [[[f""^{m}""], {}] for m in self.model._modules]\n        return Optimizer.from_params(\n            model_parameters=self.model.named_parameters(), params=optimizer_params\n        )\n\n    def _run_scheduler_get_lrs(self, params, num_steps_per_epoch):\n        optimizer = self._get_optimizer()\n        params[""type""] = ""slanted_triangular""\n        scheduler = LearningRateScheduler.from_params(\n            optimizer=optimizer, params=Params(deepcopy(params))\n        )\n        lrs = []\n\n        batch_num_total = 0\n        for epoch in range(params[""num_epochs""]):\n            for _ in range(num_steps_per_epoch):\n                batch_num_total += 1\n                # allennlp trainer calls step_batch after updating parameters\n                # so collect lr at time of parameter update\n                lrs.append(\n                    [\n                        param_group[""lr""] * float(param_group[""params""][0].requires_grad)\n                        for param_group in optimizer.param_groups[:2]\n                    ]\n                )\n                scheduler.step_batch(batch_num_total)\n                if params.get(""gradual_unfreezing"") and epoch == 0:\n                    assert scheduler.freezing_current\n            # step() takes two arguments: validation metric and epoch\n            scheduler.step(None)\n\n        return lrs\n\n    def test_is_hat_shaped(self):\n        assert not is_hat_shaped([0.0] * 10)\n        assert not is_hat_shaped([float(k) for k in range(10)])\n        assert not is_hat_shaped([float(10 - k) for k in range(10)])\n        assert is_hat_shaped([float(k) for k in range(10)] + [float(10 - k) for k in range(10)])\n        assert not is_hat_shaped(\n            [float(k) for k in range(10)]\n            + [float(10 - k) for k in range(10)]\n            + [float(k) for k in range(10)]\n        )\n\n    def test_from_params_in_trainer(self):\n        # This is more of an integration test, making sure that a bunch of pieces fit together\n        # correctly, but it matters most for this learning rate scheduler, so we\'re testing it here.\n        params = Params(\n            {\n                ""num_epochs"": 5,\n                ""learning_rate_scheduler"": {\n                    ""type"": ""slanted_triangular"",\n                    ""gradual_unfreezing"": True,\n                    ""discriminative_fine_tuning"": True,\n                    ""decay_factor"": 0.5,\n                },\n            }\n        )\n        # The method called in the logic below only checks the length of this list, not its\n        # contents, so this should be safe.\n        instances = AllennlpDataset([1] * 40)\n        optim = self._get_optimizer()\n        trainer = Trainer.from_params(\n            model=self.model,\n            optimizer=Lazy(lambda **kwargs: optim),\n            serialization_dir=self.TEST_DIR,\n            params=params,\n            data_loader=DataLoader(instances, batch_size=10),\n        )\n        assert isinstance(trainer._learning_rate_scheduler, SlantedTriangular)\n\n        # This is what we wrote this test for: to be sure that num_epochs is passed correctly, and\n        # that num_steps_per_epoch is computed and passed correctly.  This logic happens inside of\n        # `Trainer.from_partial_objects`.\n        assert trainer._learning_rate_scheduler.num_epochs == 5\n        assert trainer._learning_rate_scheduler.num_steps_per_epoch == 4\n\n        # And we\'ll do one more to make sure that we can override num_epochs in the scheduler if we\n        # really want to.  Not sure why you would ever want to in this case; this is just testing\n        # the functionality.\n        params = Params(\n            {\n                ""num_epochs"": 5,\n                ""learning_rate_scheduler"": {\n                    ""type"": ""slanted_triangular"",\n                    ""num_epochs"": 3,\n                    ""gradual_unfreezing"": True,\n                    ""discriminative_fine_tuning"": True,\n                    ""decay_factor"": 0.5,\n                },\n            }\n        )\n        trainer = Trainer.from_params(\n            model=self.model,\n            optimizer=Lazy(lambda **kwargs: optim),\n            serialization_dir=self.TEST_DIR,\n            params=params,\n            data_loader=DataLoader(instances, batch_size=10),\n        )\n        assert trainer._learning_rate_scheduler.num_epochs == 3\n\n    def test_from_params(self):\n        optim = self._get_optimizer()\n        sched = LearningRateScheduler.from_params(\n            optimizer=optim,\n            params=Params(\n                {\n                    ""type"": ""slanted_triangular"",\n                    ""num_epochs"": 5,\n                    ""num_steps_per_epoch"": 10,\n                    ""gradual_unfreezing"": True,\n                    ""discriminative_fine_tuning"": True,\n                    ""decay_factor"": 0.5,\n                }\n            ),\n        )\n\n        assert sched.num_epochs == 5\n        assert sched.num_steps_per_epoch == 10\n        assert sched.gradual_unfreezing is True\n        assert sched.freezing_current is True\n\n        assert len(optim.param_groups) == 3\n        # The default parameter group in the Optimizer is empty\n        assert not optim.param_groups[-1][""params""]\n        assert optim.param_groups[-2][""lr""] == 1.0 / sched.ratio\n        assert optim.param_groups[-3][""lr""] == 0.5 / sched.ratio\n\n        with pytest.raises(ConfigurationError):\n            # num_epochs and num_steps_per_epoch are required\n            LearningRateScheduler.from_params(\n                optimizer=optim, params=Params({""type"": ""slanted_triangular"", ""num_epochs"": 5})\n            )\n            LearningRateScheduler.from_params(\n                optimizer=optim,\n                params=Params({""type"": ""slanted_triangular"", ""num_steps_epochs"": 10}),\n            )\n\n    def test_schedules(self):\n        slanted_triangular_cases: List[Tuple[Dict[str, Any], List[Tuple[int, int, float]]]] = [\n            (\n                {\n                    ""num_epochs"": 5,\n                    ""num_steps_per_epoch"": 10,\n                    ""gradual_unfreezing"": True,\n                },  # parameters\n                [\n                    (0, 1, 0.03125),  # iteration, layer, learning rate\n                    (0, 0, 0.0),\n                    (1, 1, 1.0),\n                    (1, 0, 0.0),\n                    (9, 1, 0.138888),\n                    (9, 0, 0.0),  # end of the first epoch\n                    (10, 1, 0.03125),\n                    (10, 0, 0.03125),\n                    (14, 1, 1.0),\n                    (14, 0, 1.0),\n                    (49, 1, 0.05815972),\n                    (49, 0, 0.05815972),\n                ],\n            ),\n            (\n                {\n                    ""num_epochs"": 5,\n                    ""num_steps_per_epoch"": 10,\n                    ""discriminative_fine_tuning"": True,\n                    ""decay_factor"": 0.5,\n                },  # parameters\n                [\n                    (0, 1, 0.03125),  # iteration, layer, learning rate\n                    (0, 0, 0.015625),\n                    (5, 1, 1.0),\n                    (5, 0, 0.5),\n                    (49, 1, 0.052777),\n                    (49, 0, 0.026388),\n                ],\n            ),\n            (\n                {\n                    ""num_epochs"": 5,\n                    ""num_steps_per_epoch"": 10,\n                    ""gradual_unfreezing"": True,\n                    ""discriminative_fine_tuning"": True,\n                    ""decay_factor"": 0.5,\n                },  # parameters\n                [\n                    (0, 1, 0.03125),  # iteration, layer, learning rate\n                    (0, 0, 0.0),\n                    (1, 1, 1.0),\n                    (1, 0, 0.0),\n                    (9, 1, 0.138888),\n                    (9, 0, 0.0),  # end of the first epoch\n                    (10, 1, 0.03125),\n                    (10, 0, 0.015625),\n                    (14, 1, 1.0),\n                    (14, 0, 0.5),\n                    (49, 1, 0.0581597222),\n                    (49, 0, 0.0290798611),\n                ],\n            ),\n        ]\n        for params, lr_checks in slanted_triangular_cases:\n            lrs = self._run_scheduler_get_lrs(params, params[""num_steps_per_epoch""])\n\n            for it, layer, lr in lr_checks:\n                lr_check = round(lr, 5)\n                lr = round(lrs[it][layer], 5)\n                assert (\n                    lr == lr_check\n                ), f""Learning rate {lr} at iteration {it} at layer {layer} != {lr_check}.""\n\n    def test_schedules_num_steps_per_epoch(self):\n        # ensure the learning rate schedule still maintains hat shape\n        # if number of actual batches differs from parameter provided\n        # in constructor\n        for gradual_unfreezing in [True, False]:\n            for discriminative_fine_tuning in [True, False]:\n                for num_actual_steps_per_epoch in [7, 11]:\n                    params = {\n                        ""num_epochs"": 5,\n                        ""num_steps_per_epoch"": 10,\n                        ""gradual_unfreezing"": gradual_unfreezing,\n                        ""discriminative_fine_tuning"": discriminative_fine_tuning,\n                    }\n                    lrs = self._run_scheduler_get_lrs(params, num_actual_steps_per_epoch)\n                    first_layer_lrs = [rates[0] for rates in lrs]\n                    second_layer_lrs = [rates[1] for rates in lrs]\n\n                    if gradual_unfreezing:\n                        assert max(first_layer_lrs[:num_actual_steps_per_epoch]) < 1e-8\n                        assert min(first_layer_lrs[:num_actual_steps_per_epoch]) > -1e-8\n                        assert is_hat_shaped(first_layer_lrs[num_actual_steps_per_epoch:])\n                        assert is_hat_shaped(second_layer_lrs[:num_actual_steps_per_epoch])\n                        assert is_hat_shaped(second_layer_lrs[num_actual_steps_per_epoch:])\n                    else:\n                        assert is_hat_shaped(first_layer_lrs)\n                        assert is_hat_shaped(second_layer_lrs)\n'"
tests/training/metrics/__init__.py,0,b''
tests/training/metrics/attachment_scores_test.py,5,"b'import torch\n\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import AttachmentScores\n\n\nclass AttachmentScoresTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.scorer = AttachmentScores()\n\n        self.predictions = torch.Tensor([[0, 1, 3, 5, 2, 4], [0, 3, 2, 1, 0, 0]])\n\n        self.gold_indices = torch.Tensor([[0, 1, 3, 5, 2, 4], [0, 3, 2, 1, 0, 0]])\n\n        self.label_predictions = torch.Tensor([[0, 5, 2, 1, 4, 2], [0, 4, 8, 2, 0, 0]])\n\n        self.gold_labels = torch.Tensor([[0, 5, 2, 1, 4, 2], [0, 4, 8, 2, 0, 0]])\n\n        self.mask = torch.tensor(\n            [[True, True, True, True, True, True], [True, True, True, True, False, False]]\n        )\n\n    def _send_tensors_to_device(self, device: str):\n        self.predictions = self.predictions.to(device)\n        self.gold_indices = self.gold_indices.to(device)\n        self.label_predictions = self.label_predictions.to(device)\n        self.gold_labels = self.gold_labels.to(device)\n        self.mask = self.mask.to(device)\n\n    @multi_device\n    def test_perfect_scores(self, device: str):\n        self._send_tensors_to_device(device)\n\n        self.scorer(\n            self.predictions, self.label_predictions, self.gold_indices, self.gold_labels, self.mask\n        )\n\n        for value in self.scorer.get_metric().values():\n            assert value == 1.0\n\n    @multi_device\n    def test_unlabeled_accuracy_ignores_incorrect_labels(self, device: str):\n        self._send_tensors_to_device(device)\n\n        label_predictions = self.label_predictions\n        # Change some stuff so our 4 of our label predictions are wrong.\n        label_predictions[0, 3:] = 3\n        label_predictions[1, 0] = 7\n        self.scorer(\n            self.predictions, label_predictions, self.gold_indices, self.gold_labels, self.mask\n        )\n\n        metrics = self.scorer.get_metric()\n\n        assert metrics[""UAS""] == 1.0\n        assert metrics[""UEM""] == 1.0\n\n        # 4 / 12 labels were wrong and 2 positions\n        # are masked, so 6/10 = 0.6 LAS.\n        assert metrics[""LAS""] == 0.6\n        # Neither should have labeled exact match.\n        assert metrics[""LEM""] == 0.0\n\n    @multi_device\n    def test_labeled_accuracy_is_affected_by_incorrect_heads(self, device: str):\n        self._send_tensors_to_device(device)\n\n        predictions = self.predictions\n        # Change some stuff so our 4 of our predictions are wrong.\n        predictions[0, 3:] = 3\n        predictions[1, 0] = 7\n        # This one is in the padded part, so it shouldn\'t affect anything.\n        predictions[1, 5] = 7\n        self.scorer(\n            predictions, self.label_predictions, self.gold_indices, self.gold_labels, self.mask\n        )\n\n        metrics = self.scorer.get_metric()\n\n        # 4 heads are incorrect, so the unlabeled score should be\n        # 6/10 = 0.6 LAS.\n        assert metrics[""UAS""] == 0.6\n        # All the labels were correct, but some heads\n        # were wrong, so the LAS should equal the UAS.\n        assert metrics[""LAS""] == 0.6\n\n        # Neither batch element had a perfect labeled or unlabeled EM.\n        assert metrics[""LEM""] == 0.0\n        assert metrics[""UEM""] == 0.0\n\n    @multi_device\n    def test_attachment_scores_can_ignore_labels(self, device: str):\n        self._send_tensors_to_device(device)\n\n        scorer = AttachmentScores(ignore_classes=[1])\n\n        label_predictions = self.label_predictions\n        # Change the predictions where the gold label is 1;\n        # as we are ignoring 1, we should still get a perfect score.\n        label_predictions[0, 3] = 2\n        scorer(self.predictions, label_predictions, self.gold_indices, self.gold_labels, self.mask)\n\n        for value in scorer.get_metric().values():\n            assert value == 1.0\n'"
tests/training/metrics/auc_test.py,13,"b'import pytest\nimport torch\nfrom sklearn import metrics\nfrom torch.testing import assert_allclose\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import Auc\n\n\nclass AucTest(AllenNlpTestCase):\n    @multi_device\n    def test_auc_computation(self, device: str):\n        auc = Auc()\n        all_predictions = []\n        all_labels = []\n        for _ in range(5):\n            predictions = torch.randn(8, device=device)\n            labels = torch.randint(0, 2, (8,), dtype=torch.long, device=device)\n\n            auc(predictions, labels)\n\n            all_predictions.append(predictions)\n            all_labels.append(labels)\n\n        computed_auc_value = auc.get_metric(reset=True)\n\n        false_positive_rates, true_positive_rates, _ = metrics.roc_curve(\n            torch.cat(all_labels, dim=0).cpu().numpy(),\n            torch.cat(all_predictions, dim=0).cpu().numpy(),\n        )\n        real_auc_value = metrics.auc(false_positive_rates, true_positive_rates)\n        assert_allclose(real_auc_value, computed_auc_value)\n\n        # One more computation to assure reset works.\n        predictions = torch.randn(8, device=device)\n        labels = torch.randint(0, 2, (8,), dtype=torch.long, device=device)\n\n        auc(predictions, labels)\n        computed_auc_value = auc.get_metric(reset=True)\n\n        false_positive_rates, true_positive_rates, _ = metrics.roc_curve(\n            labels.cpu().numpy(), predictions.cpu().numpy()\n        )\n        real_auc_value = metrics.auc(false_positive_rates, true_positive_rates)\n        assert_allclose(real_auc_value, computed_auc_value)\n\n    @multi_device\n    def test_auc_gold_labels_behaviour(self, device: str):\n        # Check that it works with different pos_label\n        auc = Auc(positive_label=4)\n\n        predictions = torch.randn(8, device=device)\n        labels = torch.randint(3, 5, (8,), dtype=torch.long, device=device)\n        # We make sure that the positive label is always present.\n        labels[0] = 4\n        auc(predictions, labels)\n        computed_auc_value = auc.get_metric(reset=True)\n\n        false_positive_rates, true_positive_rates, _ = metrics.roc_curve(\n            labels.cpu().numpy(), predictions.cpu().numpy(), pos_label=4\n        )\n        real_auc_value = metrics.auc(false_positive_rates, true_positive_rates)\n        assert_allclose(real_auc_value, computed_auc_value)\n\n        # Check that it errs on getting more than 2 labels.\n        with pytest.raises(ConfigurationError) as _:\n            labels = torch.tensor([3, 4, 5, 6, 7, 8, 9, 10], device=device)\n            auc(predictions, labels)\n\n    @multi_device\n    def test_auc_with_mask(self, device: str):\n        auc = Auc()\n\n        predictions = torch.randn(8, device=device)\n        labels = torch.randint(0, 2, (8,), dtype=torch.long, device=device)\n        mask = torch.tensor([True, True, True, True, False, False, False, False], device=device)\n\n        auc(predictions, labels, mask)\n        computed_auc_value = auc.get_metric(reset=True)\n\n        false_positive_rates, true_positive_rates, _ = metrics.roc_curve(\n            labels[:4].cpu().numpy(), predictions[:4].cpu().numpy()\n        )\n        real_auc_value = metrics.auc(false_positive_rates, true_positive_rates)\n        assert_allclose(real_auc_value, computed_auc_value)\n\n    @multi_device\n    def test_auc_works_without_calling_metric_at_all(self, device: str):\n        auc = Auc()\n        auc.get_metric()\n'"
tests/training/metrics/bleu_test.py,6,"b'import math\nfrom collections import Counter\n\nimport torch\nfrom torch.testing import assert_allclose\n\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import BLEU\nfrom allennlp.training.util import ngrams, get_valid_tokens_mask\n\n\nclass BleuTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.metric = BLEU(ngram_weights=(0.5, 0.5), exclude_indices={0})\n\n    @multi_device\n    def test_get_valid_tokens_mask(self, device: str):\n        tensor = torch.tensor([[1, 2, 3, 0], [0, 1, 1, 0]], device=device)\n        result = get_valid_tokens_mask(tensor, self.metric._exclude_indices).long()\n        check = torch.tensor([[1, 1, 1, 0], [0, 1, 1, 0]], device=device)\n        assert_allclose(result, check)\n\n    @multi_device\n    def test_ngrams(self, device: str):\n        tensor = torch.tensor([1, 2, 3, 1, 2, 0], device=device)\n\n        exclude_indices = self.metric._exclude_indices\n\n        # Unigrams.\n        counts: Counter = Counter(ngrams(tensor, 1, exclude_indices))\n        unigram_check = {(1,): 2, (2,): 2, (3,): 1}\n        assert counts == unigram_check\n\n        # Bigrams.\n        counts = Counter(ngrams(tensor, 2, exclude_indices))\n        bigram_check = {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n        assert counts == bigram_check\n\n        # Trigrams.\n        counts = Counter(ngrams(tensor, 3, exclude_indices))\n        trigram_check = {(1, 2, 3): 1, (2, 3, 1): 1, (3, 1, 2): 1}\n        assert counts == trigram_check\n\n        # ngram size too big, no ngrams produced.\n        counts = Counter(ngrams(tensor, 7, exclude_indices))\n        assert counts == {}\n\n    @multi_device\n    def test_bleu_computed_correctly(self, device: str):\n        self.metric.reset()\n\n        # shape: (batch_size, max_sequence_length)\n        predictions = torch.tensor([[1, 0, 0], [1, 1, 0], [1, 1, 1]], device=device)\n\n        # shape: (batch_size, max_gold_sequence_length)\n        gold_targets = torch.tensor([[2, 0, 0], [1, 0, 0], [1, 1, 2]], device=device)\n\n        self.metric(predictions, gold_targets)\n\n        assert self.metric._prediction_lengths == 6\n        assert self.metric._reference_lengths == 5\n\n        # Number of unigrams in predicted sentences that match gold sentences\n        # (but not more than maximum occurrence of gold unigram within batch).\n        assert self.metric._precision_matches[1] == (\n            0\n            + 1  # no matches in first sentence.\n            + 2  # one clipped match in second sentence.  # two clipped matches in third sentence.\n        )\n\n        # Total number of predicted unigrams.\n        assert self.metric._precision_totals[1] == (1 + 2 + 3)\n\n        # Number of bigrams in predicted sentences that match gold sentences\n        # (but not more than maximum occurrence of gold bigram within batch).\n        assert self.metric._precision_matches[2] == (0 + 0 + 1)\n\n        # Total number of predicted bigrams.\n        assert self.metric._precision_totals[2] == (0 + 1 + 2)\n\n        # Brevity penalty should be 1.0\n        assert self.metric._get_brevity_penalty() == 1.0\n\n        bleu = self.metric.get_metric(reset=True)[""BLEU""]\n        check = math.exp(0.5 * (math.log(3) - math.log(6)) + 0.5 * (math.log(1) - math.log(3)))\n        assert_allclose(bleu, check)\n\n    @multi_device\n    def test_bleu_computed_with_zero_counts(self, device: str):\n        self.metric.reset()\n        assert self.metric.get_metric()[""BLEU""] == 0\n'"
tests/training/metrics/boolean_accuracy_test.py,11,"b'import torch\nimport pytest\n\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import BooleanAccuracy\n\n\nclass BooleanAccuracyTest(AllenNlpTestCase):\n    @multi_device\n    def test_accuracy_computation(self, device: str):\n        accuracy = BooleanAccuracy()\n        predictions = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7]], device=device)\n        targets = torch.tensor([[0, 1], [2, 2], [4, 5], [7, 7]], device=device)\n        accuracy(predictions, targets)\n        assert accuracy.get_metric() == 2 / 4\n\n        mask = torch.ones(4, 2, device=device).bool()\n        mask[1, 1] = 0\n        accuracy(predictions, targets, mask)\n        assert accuracy.get_metric() == 5 / 8\n\n        targets[1, 1] = 3\n        accuracy(predictions, targets)\n        assert accuracy.get_metric() == 8 / 12\n\n        accuracy.reset()\n        accuracy(predictions, targets)\n        assert accuracy.get_metric() == 3 / 4\n\n    @multi_device\n    def test_skips_completely_masked_instances(self, device: str):\n        accuracy = BooleanAccuracy()\n        predictions = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7]], device=device)\n        targets = torch.tensor([[0, 1], [2, 2], [4, 5], [7, 7]], device=device)\n\n        mask = torch.tensor(\n            [[False, False], [True, False], [True, True], [True, True]], device=device\n        )\n        accuracy(predictions, targets, mask)\n\n        # First example should be skipped, second is correct with mask, third is correct, fourth is wrong.\n        assert accuracy.get_metric() == 2 / 3\n\n    @multi_device\n    def test_incorrect_gold_labels_shape_catches_exceptions(self, device: str):\n        accuracy = BooleanAccuracy()\n        predictions = torch.rand([5, 7], device=device)\n        incorrect_shape_labels = torch.rand([5, 8], device=device)\n        with pytest.raises(ValueError):\n            accuracy(predictions, incorrect_shape_labels)\n\n    @multi_device\n    def test_incorrect_mask_shape_catches_exceptions(self, device: str):\n        accuracy = BooleanAccuracy()\n        predictions = torch.rand([5, 7], device=device)\n        labels = torch.rand([5, 7], device=device)\n        incorrect_shape_mask = torch.randint(0, 2, [5, 8], device=device).bool()\n        with pytest.raises(ValueError):\n            accuracy(predictions, labels, incorrect_shape_mask)\n\n    @multi_device\n    def test_does_not_divide_by_zero_with_no_count(self, device: str):\n        accuracy = BooleanAccuracy()\n        assert accuracy.get_metric() == pytest.approx(0.0)\n'"
tests/training/metrics/categorical_accuracy_test.py,23,"b'import pytest\nimport torch\nfrom torch.testing import assert_allclose\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import CategoricalAccuracy\n\n\nclass CategoricalAccuracyTest(AllenNlpTestCase):\n    @multi_device\n    def test_categorical_accuracy(self, device: str):\n        accuracy = CategoricalAccuracy()\n        predictions = torch.tensor(\n            [[0.35, 0.25, 0.1, 0.1, 0.2], [0.1, 0.6, 0.1, 0.2, 0.0]], device=device\n        )\n        targets = torch.tensor([0, 3], device=device)\n        accuracy(predictions, targets)\n        actual_accuracy = accuracy.get_metric()\n        assert actual_accuracy == 0.50\n\n    @multi_device\n    def test_top_k_categorical_accuracy(self, device: str):\n        accuracy = CategoricalAccuracy(top_k=2)\n        predictions = torch.tensor(\n            [[0.35, 0.25, 0.1, 0.1, 0.2], [0.1, 0.6, 0.1, 0.2, 0.0]], device=device\n        )\n        targets = torch.tensor([0, 3], device=device)\n        accuracy(predictions, targets)\n        actual_accuracy = accuracy.get_metric()\n        assert actual_accuracy == 1.0\n\n    @multi_device\n    def test_top_k_categorical_accuracy_accumulates_and_resets_correctly(self, device: str):\n        accuracy = CategoricalAccuracy(top_k=2)\n        predictions = torch.tensor(\n            [[0.35, 0.25, 0.1, 0.1, 0.2], [0.1, 0.6, 0.1, 0.2, 0.0]], device=device\n        )\n        targets = torch.tensor([0, 3], device=device)\n        accuracy(predictions, targets)\n        accuracy(predictions, targets)\n        accuracy(predictions, torch.tensor([4, 4], device=device))\n        accuracy(predictions, torch.tensor([4, 4], device=device))\n        actual_accuracy = accuracy.get_metric(reset=True)\n        assert actual_accuracy == 0.50\n        assert accuracy.correct_count == 0.0\n        assert accuracy.total_count == 0.0\n\n    @multi_device\n    def test_top_k_categorical_accuracy_respects_mask(self, device: str):\n        accuracy = CategoricalAccuracy(top_k=2)\n        predictions = torch.tensor(\n            [[0.35, 0.25, 0.1, 0.1, 0.2], [0.1, 0.6, 0.1, 0.2, 0.0], [0.1, 0.2, 0.5, 0.2, 0.0]],\n            device=device,\n        )\n        targets = torch.tensor([0, 3, 0], device=device)\n        mask = torch.tensor([False, True, True], device=device)\n        accuracy(predictions, targets, mask)\n        actual_accuracy = accuracy.get_metric()\n        assert_allclose(actual_accuracy, 0.50)\n\n    @multi_device\n    def test_top_k_categorical_accuracy_works_for_sequences(self, device: str):\n        accuracy = CategoricalAccuracy(top_k=2)\n        predictions = torch.tensor(\n            [\n                [[0.35, 0.25, 0.1, 0.1, 0.2], [0.1, 0.6, 0.1, 0.2, 0.0], [0.1, 0.6, 0.1, 0.2, 0.0]],\n                [[0.35, 0.25, 0.1, 0.1, 0.2], [0.1, 0.6, 0.1, 0.2, 0.0], [0.1, 0.6, 0.1, 0.2, 0.0]],\n            ],\n            device=device,\n        )\n        targets = torch.tensor([[0, 3, 4], [0, 1, 4]], device=device)\n        accuracy(predictions, targets)\n        actual_accuracy = accuracy.get_metric(reset=True)\n        assert_allclose(actual_accuracy, 0.6666666)\n\n        # Test the same thing but with a mask:\n        mask = torch.tensor([[False, True, True], [True, False, True]], device=device)\n        accuracy(predictions, targets, mask)\n        actual_accuracy = accuracy.get_metric(reset=True)\n        assert_allclose(actual_accuracy, 0.50)\n\n    @multi_device\n    def test_top_k_categorical_accuracy_catches_exceptions(self, device: str):\n        accuracy = CategoricalAccuracy()\n        predictions = torch.rand([5, 7], device=device)\n        out_of_range_labels = torch.tensor([10, 3, 4, 0, 1], device=device)\n        with pytest.raises(ConfigurationError):\n            accuracy(predictions, out_of_range_labels)\n\n    @multi_device\n    def test_tie_break_categorical_accuracy(self, device: str):\n        accuracy = CategoricalAccuracy(tie_break=True)\n        predictions = torch.tensor(\n            [[0.35, 0.25, 0.35, 0.35, 0.35], [0.1, 0.6, 0.1, 0.2, 0.2], [0.1, 0.0, 0.1, 0.2, 0.2]],\n            device=device,\n        )\n        # Test without mask:\n        targets = torch.tensor([2, 1, 4], device=device)\n        accuracy(predictions, targets)\n        assert accuracy.get_metric(reset=True) == (0.25 + 1 + 0.5) / 3.0\n\n        # # # Test with mask\n        mask = torch.tensor([True, False, True], device=device)\n        targets = torch.tensor([2, 1, 4], device=device)\n        accuracy(predictions, targets, mask)\n        assert accuracy.get_metric(reset=True) == (0.25 + 0.5) / 2.0\n\n        # # Test tie-break with sequence\n        predictions = torch.tensor(\n            [\n                [\n                    [0.35, 0.25, 0.35, 0.35, 0.35],\n                    [0.1, 0.6, 0.1, 0.2, 0.2],\n                    [0.1, 0.0, 0.1, 0.2, 0.2],\n                ],\n                [\n                    [0.35, 0.25, 0.35, 0.35, 0.35],\n                    [0.1, 0.6, 0.1, 0.2, 0.2],\n                    [0.1, 0.0, 0.1, 0.2, 0.2],\n                ],\n            ],\n            device=device,\n        )\n        targets = torch.tensor(\n            [[0, 1, 3], [0, 3, 4]], device=device  # 0.25 + 1 + 0.5  # 0.25 + 0 + 0.5 = 2.5\n        )\n        accuracy(predictions, targets)\n        actual_accuracy = accuracy.get_metric(reset=True)\n        assert_allclose(actual_accuracy, 2.5 / 6.0)\n\n    @multi_device\n    def test_top_k_and_tie_break_together_catches_exceptions(self, device: str):\n        with pytest.raises(ConfigurationError):\n            CategoricalAccuracy(top_k=2, tie_break=True)\n\n    @multi_device\n    def test_incorrect_top_k_catches_exceptions(self, device: str):\n        with pytest.raises(ConfigurationError):\n            CategoricalAccuracy(top_k=0)\n\n    @multi_device\n    def test_does_not_divide_by_zero_with_no_count(self, device: str):\n        accuracy = CategoricalAccuracy()\n        assert accuracy.get_metric() == pytest.approx(0.0)\n'"
tests/training/metrics/covariance_test.py,6,"b'import numpy as np\nimport torch\nfrom torch.testing import assert_allclose\n\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import Covariance\n\n\nclass CovarianceTest(AllenNlpTestCase):\n    @multi_device\n    def test_covariance_unmasked_computation(self, device: str):\n        covariance = Covariance()\n        batch_size = 100\n        num_labels = 10\n        predictions = torch.randn(batch_size, num_labels, device=device)\n        labels = 0.5 * predictions + torch.randn(batch_size, num_labels, device=device)\n\n        stride = 10\n\n        for i in range(batch_size // stride):\n            timestep_predictions = predictions[stride * i : stride * (i + 1), :]\n            timestep_labels = labels[stride * i : stride * (i + 1), :]\n            # Flatten the predictions and labels thus far, so numpy treats them as\n            # independent observations.\n            expected_covariance = np.cov(\n                predictions[: stride * (i + 1), :].view(-1).cpu().numpy(),\n                labels[: stride * (i + 1), :].view(-1).cpu().numpy(),\n            )[0, 1]\n            covariance(timestep_predictions, timestep_labels)\n            assert_allclose(expected_covariance, covariance.get_metric())\n\n        # Test reset\n        covariance.reset()\n        covariance(predictions, labels)\n        assert_allclose(\n            np.cov(predictions.view(-1).cpu().numpy(), labels.view(-1).cpu().numpy())[0, 1],\n            covariance.get_metric(),\n        )\n\n    @multi_device\n    def test_covariance_masked_computation(self, device: str):\n        covariance = Covariance()\n        batch_size = 100\n        num_labels = 10\n        predictions = torch.randn(batch_size, num_labels, device=device)\n        labels = 0.5 * predictions + torch.randn(batch_size, num_labels, device=device)\n        # Random binary mask\n        mask = torch.randint(0, 2, size=(batch_size, num_labels), device=device).bool()\n        stride = 10\n\n        for i in range(batch_size // stride):\n            timestep_predictions = predictions[stride * i : stride * (i + 1), :]\n            timestep_labels = labels[stride * i : stride * (i + 1), :]\n            timestep_mask = mask[stride * i : stride * (i + 1), :]\n            # Flatten the predictions, labels, and mask thus far, so numpy treats them as\n            # independent observations.\n            expected_covariance = np.cov(\n                predictions[: stride * (i + 1), :].view(-1).cpu().numpy(),\n                labels[: stride * (i + 1), :].view(-1).cpu().numpy(),\n                fweights=mask[: stride * (i + 1), :].view(-1).cpu().numpy(),\n            )[0, 1]\n            covariance(timestep_predictions, timestep_labels, timestep_mask)\n            assert_allclose(expected_covariance, covariance.get_metric())\n\n        # Test reset\n        covariance.reset()\n        covariance(predictions, labels, mask)\n        assert_allclose(\n            np.cov(\n                predictions.view(-1).cpu().numpy(),\n                labels.view(-1).cpu().numpy(),\n                fweights=mask.view(-1).cpu().numpy(),\n            )[0, 1],\n            covariance.get_metric(),\n        )\n'"
tests/training/metrics/entropy_test.py,10,"b""import torch\nfrom torch.testing import assert_allclose\n\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import Entropy\n\n\nclass EntropyTest(AllenNlpTestCase):\n    @multi_device\n    def test_low_entropy_distribution(self, device: str):\n        metric = Entropy()\n        logits = torch.tensor(\n            [[10000, -10000, -10000, -1000], [10000, -10000, -10000, -1000]],\n            dtype=torch.float,\n            device=device,\n        )\n        metric(logits)\n        assert metric.get_metric() == 0.0\n\n    @multi_device\n    def test_entropy_for_uniform_distribution(self, device: str):\n        metric = Entropy()\n        logits = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 1]], dtype=torch.float, device=device)\n        metric(logits)\n        assert_allclose(metric.get_metric(), torch.tensor(1.38629436, device=device))\n        # actual values shouldn't effect uniform distribution:\n        logits = torch.tensor([[2, 2, 2, 2], [2, 2, 2, 2]], dtype=torch.float, device=device)\n        metric(logits)\n        assert_allclose(metric.get_metric(), torch.tensor(1.38629436, device=device))\n\n        metric.reset()\n        assert metric._entropy == 0.0\n        assert metric._count == 0.0\n\n    @multi_device\n    def test_masked_case(self, device: str):\n        metric = Entropy()\n        # This would have non-zero entropy without the mask.\n        logits = torch.tensor(\n            [[1, 1, 1, 1], [10000, -10000, -10000, -1000]], dtype=torch.float, device=device\n        )\n        mask = torch.tensor([False, True], device=device)\n        metric(logits, mask)\n        assert metric.get_metric() == 0.0\n"""
tests/training/metrics/evalb_bracketing_scorer_test.py,0,"b'from nltk import Tree\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.training.metrics import EvalbBracketingScorer\n\n\nclass EvalbBracketingScorerTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        EvalbBracketingScorer.compile_evalb()\n\n    def tearDown(self):\n        EvalbBracketingScorer.clean_evalb()\n        super().tearDown()\n\n    def test_evalb_correctly_scores_identical_trees(self):\n        tree1 = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")\n        tree2 = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")\n        evalb_scorer = EvalbBracketingScorer()\n        evalb_scorer([tree1], [tree2])\n        metrics = evalb_scorer.get_metric()\n        assert metrics[""evalb_recall""] == 1.0\n        assert metrics[""evalb_precision""] == 1.0\n        assert metrics[""evalb_f1_measure""] == 1.0\n\n    def test_evalb_correctly_scores_imperfect_trees(self):\n        # Change to constiutency label (VP ... )should effect scores, but change to POS\n        # tag (NP dog) should have no effect.\n        tree1 = Tree.fromstring(""(S (VP (D the) (NP dog)) (VP (V chased) (NP (D the) (N cat))))"")\n        tree2 = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")\n        evalb_scorer = EvalbBracketingScorer()\n        evalb_scorer([tree1], [tree2])\n        metrics = evalb_scorer.get_metric()\n        assert metrics[""evalb_recall""] == 0.75\n        assert metrics[""evalb_precision""] == 0.75\n        assert metrics[""evalb_f1_measure""] == 0.75\n\n    def test_evalb_correctly_calculates_bracketing_metrics_over_multiple_trees(self):\n        tree1 = Tree.fromstring(""(S (VP (D the) (NP dog)) (VP (V chased) (NP (D the) (N cat))))"")\n        tree2 = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")\n        evalb_scorer = EvalbBracketingScorer()\n        evalb_scorer([tree1, tree2], [tree2, tree2])\n        metrics = evalb_scorer.get_metric()\n        assert metrics[""evalb_recall""] == 0.875\n        assert metrics[""evalb_precision""] == 0.875\n        assert metrics[""evalb_f1_measure""] == 0.875\n\n    def test_evalb_with_terrible_trees_handles_nan_f1(self):\n        # If precision and recall are zero, evalb returns nan f1.\n        # This checks that we handle the zero division.\n        tree1 = Tree.fromstring(\n            ""(PP (VROOT (PP That) (VROOT (PP could) (VROOT (PP cost) (VROOT (PP him))))) (PP .))""\n        )\n        tree2 = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")\n        evalb_scorer = EvalbBracketingScorer()\n        evalb_scorer([tree1], [tree2])\n        metrics = evalb_scorer.get_metric()\n        assert metrics[""evalb_recall""] == 0.0\n        assert metrics[""evalb_precision""] == 0.0\n        assert metrics[""evalb_f1_measure""] == 0.0\n'"
tests/training/metrics/f1_measure_test.py,13,"b'import pytest\nimport torch\nfrom torch.testing import assert_allclose\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import F1Measure\n\n\nclass F1MeasureTest(AllenNlpTestCase):\n    @multi_device\n    def test_f1_measure_catches_exceptions(self, device: str):\n        f1_measure = F1Measure(0)\n        predictions = torch.rand([5, 7], device=device)\n        out_of_range_labels = torch.tensor([10, 3, 4, 0, 1], device=device)\n        with pytest.raises(ConfigurationError):\n            f1_measure(predictions, out_of_range_labels)\n\n    @multi_device\n    def test_f1_measure(self, device: str):\n        f1_measure = F1Measure(positive_label=0)\n        predictions = torch.tensor(\n            [\n                [0.35, 0.25, 0.1, 0.1, 0.2],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n                [0.1, 0.5, 0.1, 0.2, 0.0],\n                [0.1, 0.2, 0.1, 0.7, 0.0],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n            ],\n            device=device,\n        )\n        # [True Positive, True Negative, True Negative,\n        #  False Negative, True Negative, False Negative]\n        targets = torch.tensor([0, 4, 1, 0, 3, 0], device=device)\n        f1_measure(predictions, targets)\n        precision, recall, f1 = f1_measure.get_metric()\n        assert f1_measure._true_positives == 1.0\n        assert f1_measure._true_negatives == 3.0\n        assert f1_measure._false_positives == 0.0\n        assert f1_measure._false_negatives == 2.0\n        f1_measure.reset()\n        # check value\n        assert_allclose(precision, 1.0)\n        assert_allclose(recall, 0.333333333)\n        assert_allclose(f1, 0.499999999)\n        # check type\n        assert isinstance(precision, float)\n        assert isinstance(recall, float)\n        assert isinstance(f1, float)\n\n        # Test the same thing with a mask:\n        mask = torch.tensor([True, False, True, True, True, False], device=device)\n        f1_measure(predictions, targets, mask)\n        precision, recall, f1 = f1_measure.get_metric()\n        assert f1_measure._true_positives == 1.0\n        assert f1_measure._true_negatives == 2.0\n        assert f1_measure._false_positives == 0.0\n        assert f1_measure._false_negatives == 1.0\n        f1_measure.reset()\n        assert_allclose(precision, 1.0)\n        assert_allclose(recall, 0.5)\n        assert_allclose(f1, 0.6666666666)\n\n    @multi_device\n    def test_f1_measure_other_positive_label(self, device: str):\n        f1_measure = F1Measure(positive_label=1)\n        predictions = torch.tensor(\n            [\n                [0.35, 0.25, 0.1, 0.1, 0.2],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n                [0.1, 0.5, 0.1, 0.2, 0.0],\n                [0.1, 0.2, 0.1, 0.7, 0.0],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n            ],\n            device=device,\n        )\n        # [True Negative, False Positive, True Positive,\n        #  False Positive, True Negative, False Positive]\n        targets = torch.tensor([0, 4, 1, 0, 3, 0], device=device)\n        f1_measure(predictions, targets)\n        precision, recall, f1 = f1_measure.get_metric()\n        assert f1_measure._true_positives == 1.0\n        assert f1_measure._true_negatives == 2.0\n        assert f1_measure._false_positives == 3.0\n        assert f1_measure._false_negatives == 0.0\n        f1_measure.reset()\n        # check value\n        assert_allclose(precision, 0.25)\n        assert_allclose(recall, 1.0)\n        assert_allclose(f1, 0.4)\n        # check type\n        assert isinstance(precision, float)\n        assert isinstance(recall, float)\n        assert isinstance(f1, float)\n\n    @multi_device\n    def test_f1_measure_accumulates_and_resets_correctly(self, device: str):\n        f1_measure = F1Measure(positive_label=0)\n        predictions = torch.tensor(\n            [\n                [0.35, 0.25, 0.1, 0.1, 0.2],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n                [0.1, 0.5, 0.1, 0.2, 0.0],\n                [0.1, 0.2, 0.1, 0.7, 0.0],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n            ],\n            device=device,\n        )\n        # [True Positive, True Negative, True Negative,\n        #  False Negative, True Negative, False Negative]\n        targets = torch.tensor([0, 4, 1, 0, 3, 0], device=device)\n        f1_measure(predictions, targets)\n        f1_measure(predictions, targets)\n        precision, recall, f1 = f1_measure.get_metric()\n        assert f1_measure._true_positives == 2.0\n        assert f1_measure._true_negatives == 6.0\n        assert f1_measure._false_positives == 0.0\n        assert f1_measure._false_negatives == 4.0\n        f1_measure.reset()\n        assert_allclose(precision, 1.0)\n        assert_allclose(recall, 0.333333333)\n        assert_allclose(f1, 0.499999999)\n        assert f1_measure._true_positives == 0.0\n        assert f1_measure._true_negatives == 0.0\n        assert f1_measure._false_positives == 0.0\n        assert f1_measure._false_negatives == 0.0\n\n    @multi_device\n    def test_f1_measure_works_for_sequences(self, device: str):\n        f1_measure = F1Measure(positive_label=0)\n        predictions = torch.tensor(\n            [\n                [[0.35, 0.25, 0.1, 0.1, 0.2], [0.1, 0.6, 0.1, 0.2, 0.0], [0.1, 0.6, 0.1, 0.2, 0.0]],\n                [[0.35, 0.25, 0.1, 0.1, 0.2], [0.1, 0.6, 0.1, 0.2, 0.0], [0.1, 0.6, 0.1, 0.2, 0.0]],\n            ],\n            device=device,\n        )\n        # [[True Positive, True Negative, True Negative],\n        #  [True Positive, True Negative, False Negative]]\n        targets = torch.tensor([[0, 3, 4], [0, 1, 0]], device=device)\n        f1_measure(predictions, targets)\n        precision, recall, f1 = f1_measure.get_metric()\n        assert f1_measure._true_positives == 2.0\n        assert f1_measure._true_negatives == 3.0\n        assert f1_measure._false_positives == 0.0\n        assert f1_measure._false_negatives == 1.0\n        f1_measure.reset()\n        assert_allclose(precision, 1.0)\n        assert_allclose(recall, 0.666666666)\n        assert_allclose(f1, 0.8)\n\n        # Test the same thing with a mask:\n        mask = torch.tensor([[False, True, False], [True, True, True]], device=device)\n        f1_measure(predictions, targets, mask)\n        precision, recall, f1 = f1_measure.get_metric()\n        assert f1_measure._true_positives == 1.0\n        assert f1_measure._true_negatives == 2.0\n        assert f1_measure._false_positives == 0.0\n        assert f1_measure._false_negatives == 1.0\n        assert_allclose(precision, 1.0)\n        assert_allclose(recall, 0.5)\n        assert_allclose(f1, 0.66666666666)\n'"
tests/training/metrics/fbeta_measure_test.py,19,"b'from typing import List\n\nimport torch\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom torch.testing import assert_allclose\nimport pytest\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import FBetaMeasure\n\n\nclass FBetaMeasureTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        # [0, 1, 1, 1, 3, 1]\n        self.predictions = torch.tensor(\n            [\n                [0.35, 0.25, 0.1, 0.1, 0.2],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n                [0.1, 0.5, 0.1, 0.2, 0.0],\n                [0.1, 0.2, 0.1, 0.7, 0.0],\n                [0.1, 0.6, 0.1, 0.2, 0.0],\n            ]\n        )\n        self.targets = torch.tensor([0, 4, 1, 0, 3, 0])\n\n        # detailed target state\n        self.pred_sum = [1, 4, 0, 1, 0]\n        self.true_sum = [3, 1, 0, 1, 1]\n        self.true_positive_sum = [1, 1, 0, 1, 0]\n        self.true_negative_sum = [3, 2, 6, 5, 5]\n        self.total_sum = [6, 6, 6, 6, 6]\n\n        desired_precisions = [1.00, 0.25, 0.00, 1.00, 0.00]\n        desired_recalls = [1 / 3, 1.00, 0.00, 1.00, 0.00]\n        desired_fscores = [\n            (2 * p * r) / (p + r) if p + r != 0.0 else 0.0\n            for p, r in zip(desired_precisions, desired_recalls)\n        ]\n        self.desired_precisions = desired_precisions\n        self.desired_recalls = desired_recalls\n        self.desired_fscores = desired_fscores\n\n    @multi_device\n    def test_config_errors(self, device: str):\n        # Bad beta\n        pytest.raises(ConfigurationError, FBetaMeasure, beta=0.0)\n\n        # Bad average option\n        pytest.raises(ConfigurationError, FBetaMeasure, average=""mega"")\n\n        # Empty input labels\n        pytest.raises(ConfigurationError, FBetaMeasure, labels=[])\n\n    @multi_device\n    def test_runtime_errors(self, device: str):\n        fbeta = FBetaMeasure()\n        # Metric was never called.\n        pytest.raises(RuntimeError, fbeta.get_metric)\n\n    @multi_device\n    def test_fbeta_multiclass_state(self, device: str):\n        self.predictions = self.predictions.to(device)\n        self.targets = self.targets.to(device)\n\n        fbeta = FBetaMeasure()\n        fbeta(self.predictions, self.targets)\n\n        # check state\n        assert_allclose(fbeta._pred_sum.tolist(), self.pred_sum)\n        assert_allclose(fbeta._true_sum.tolist(), self.true_sum)\n        assert_allclose(fbeta._true_positive_sum.tolist(), self.true_positive_sum)\n        assert_allclose(fbeta._true_negative_sum.tolist(), self.true_negative_sum)\n        assert_allclose(fbeta._total_sum.tolist(), self.total_sum)\n\n    @multi_device\n    def test_fbeta_multiclass_metric(self, device: str):\n        self.predictions = self.predictions.to(device)\n        self.targets = self.targets.to(device)\n\n        fbeta = FBetaMeasure()\n        fbeta(self.predictions, self.targets)\n        metric = fbeta.get_metric()\n        precisions = metric[""precision""]\n        recalls = metric[""recall""]\n        fscores = metric[""fscore""]\n\n        # check value\n        assert_allclose(precisions, self.desired_precisions)\n        assert_allclose(recalls, self.desired_recalls)\n        assert_allclose(fscores, self.desired_fscores)\n\n        # check type\n        assert isinstance(precisions, List)\n        assert isinstance(recalls, List)\n        assert isinstance(fscores, List)\n\n    @multi_device\n    def test_fbeta_multiclass_with_mask(self, device: str):\n        self.predictions = self.predictions.to(device)\n        self.targets = self.targets.to(device)\n\n        mask = torch.tensor([True, True, True, True, True, False], device=device)\n\n        fbeta = FBetaMeasure()\n        fbeta(self.predictions, self.targets, mask)\n        metric = fbeta.get_metric()\n        precisions = metric[""precision""]\n        recalls = metric[""recall""]\n        fscores = metric[""fscore""]\n\n        assert_allclose(fbeta._pred_sum.tolist(), [1, 3, 0, 1, 0])\n        assert_allclose(fbeta._true_sum.tolist(), [2, 1, 0, 1, 1])\n        assert_allclose(fbeta._true_positive_sum.tolist(), [1, 1, 0, 1, 0])\n\n        desired_precisions = [1.00, 1 / 3, 0.00, 1.00, 0.00]\n        desired_recalls = [0.50, 1.00, 0.00, 1.00, 0.00]\n        desired_fscores = [\n            (2 * p * r) / (p + r) if p + r != 0.0 else 0.0\n            for p, r in zip(desired_precisions, desired_recalls)\n        ]\n        assert_allclose(precisions, desired_precisions)\n        assert_allclose(recalls, desired_recalls)\n        assert_allclose(fscores, desired_fscores)\n\n    @multi_device\n    def test_fbeta_multiclass_macro_average_metric(self, device: str):\n        self.predictions = self.predictions.to(device)\n        self.targets = self.targets.to(device)\n\n        fbeta = FBetaMeasure(average=""macro"")\n        fbeta(self.predictions, self.targets)\n        metric = fbeta.get_metric()\n        precisions = metric[""precision""]\n        recalls = metric[""recall""]\n        fscores = metric[""fscore""]\n\n        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.\n        macro_precision = torch.tensor(self.desired_precisions).mean()\n        macro_recall = torch.tensor(self.desired_recalls).mean()\n        macro_fscore = torch.tensor(self.desired_fscores).mean()\n        # check value\n        assert_allclose(precisions, macro_precision)\n        assert_allclose(recalls, macro_recall)\n        assert_allclose(fscores, macro_fscore)\n\n        # check type\n        assert isinstance(precisions, float)\n        assert isinstance(recalls, float)\n        assert isinstance(fscores, float)\n\n    @multi_device\n    def test_fbeta_multiclass_micro_average_metric(self, device: str):\n        self.predictions = self.predictions.to(device)\n        self.targets = self.targets.to(device)\n\n        fbeta = FBetaMeasure(average=""micro"")\n        fbeta(self.predictions, self.targets)\n        metric = fbeta.get_metric()\n        precisions = metric[""precision""]\n        recalls = metric[""recall""]\n        fscores = metric[""fscore""]\n\n        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.\n        true_positives = torch.tensor([1, 1, 0, 1, 0], dtype=torch.float32)\n        false_positives = torch.tensor([0, 3, 0, 0, 0], dtype=torch.float32)\n        false_negatives = torch.tensor([2, 0, 0, 0, 1], dtype=torch.float32)\n        mean_true_positive = true_positives.mean()\n        mean_false_positive = false_positives.mean()\n        mean_false_negative = false_negatives.mean()\n\n        micro_precision = mean_true_positive / (mean_true_positive + mean_false_positive)\n        micro_recall = mean_true_positive / (mean_true_positive + mean_false_negative)\n        micro_fscore = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)\n        # check value\n        assert_allclose(precisions, micro_precision)\n        assert_allclose(recalls, micro_recall)\n        assert_allclose(fscores, micro_fscore)\n\n    @multi_device\n    def test_fbeta_multiclass_with_explicit_labels(self, device: str):\n        self.predictions = self.predictions.to(device)\n        self.targets = self.targets.to(device)\n\n        # same prediction but with and explicit label ordering\n        fbeta = FBetaMeasure(labels=[4, 3, 2, 1, 0])\n        fbeta(self.predictions, self.targets)\n        metric = fbeta.get_metric()\n        precisions = metric[""precision""]\n        recalls = metric[""recall""]\n        fscores = metric[""fscore""]\n\n        desired_precisions = self.desired_precisions[::-1]\n        desired_recalls = self.desired_recalls[::-1]\n        desired_fscores = self.desired_fscores[::-1]\n        # check value\n        assert_allclose(precisions, desired_precisions)\n        assert_allclose(recalls, desired_recalls)\n        assert_allclose(fscores, desired_fscores)\n\n    @multi_device\n    def test_fbeta_multiclass_with_macro_average(self, device: str):\n        self.predictions = self.predictions.to(device)\n        self.targets = self.targets.to(device)\n\n        labels = [0, 1]\n        fbeta = FBetaMeasure(average=""macro"", labels=labels)\n        fbeta(self.predictions, self.targets)\n        metric = fbeta.get_metric()\n        precisions = metric[""precision""]\n        recalls = metric[""recall""]\n        fscores = metric[""fscore""]\n\n        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.\n        macro_precision = torch.tensor(self.desired_precisions)[labels].mean()\n        macro_recall = torch.tensor(self.desired_recalls)[labels].mean()\n        macro_fscore = torch.tensor(self.desired_fscores)[labels].mean()\n\n        # check value\n        assert_allclose(precisions, macro_precision)\n        assert_allclose(recalls, macro_recall)\n        assert_allclose(fscores, macro_fscore)\n\n    @multi_device\n    def test_fbeta_multiclass_with_micro_average(self, device: str):\n        self.predictions = self.predictions.to(device)\n        self.targets = self.targets.to(device)\n\n        labels = [1, 3]\n        fbeta = FBetaMeasure(average=""micro"", labels=labels)\n        fbeta(self.predictions, self.targets)\n        metric = fbeta.get_metric()\n        precisions = metric[""precision""]\n        recalls = metric[""recall""]\n        fscores = metric[""fscore""]\n\n        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.\n        true_positives = torch.tensor([1, 1], dtype=torch.float32)\n        false_positives = torch.tensor([3, 0], dtype=torch.float32)\n        false_negatives = torch.tensor([0, 0], dtype=torch.float32)\n        mean_true_positive = true_positives.mean()\n        mean_false_positive = false_positives.mean()\n        mean_false_negative = false_negatives.mean()\n\n        micro_precision = mean_true_positive / (mean_true_positive + mean_false_positive)\n        micro_recall = mean_true_positive / (mean_true_positive + mean_false_negative)\n        micro_fscore = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)\n        # check value\n        assert_allclose(precisions, micro_precision)\n        assert_allclose(recalls, micro_recall)\n        assert_allclose(fscores, micro_fscore)\n\n    @multi_device\n    def test_fbeta_multiclass_with_weighted_average(self, device: str):\n        self.predictions = self.predictions.to(device)\n        self.targets = self.targets.to(device)\n\n        labels = [0, 1]\n        fbeta = FBetaMeasure(average=""weighted"", labels=labels)\n        fbeta(self.predictions, self.targets)\n        metric = fbeta.get_metric()\n        precisions = metric[""precision""]\n        recalls = metric[""recall""]\n        fscores = metric[""fscore""]\n\n        weighted_precision, weighted_recall, weighted_fscore, _ = precision_recall_fscore_support(\n            self.targets.cpu().numpy(),\n            self.predictions.argmax(dim=1).cpu().numpy(),\n            labels=labels,\n            average=""weighted"",\n        )\n\n        # check value\n        assert_allclose(precisions, weighted_precision)\n        assert_allclose(recalls, weighted_recall)\n        assert_allclose(fscores, weighted_fscore)\n\n    @multi_device\n    def test_fbeta_handles_batch_size_of_one(self, device: str):\n        predictions = torch.tensor([[0.2862, 0.3479, 0.1627, 0.2033]], device=device)\n        targets = torch.tensor([1], device=device)\n        mask = torch.tensor([True], device=device)\n\n        fbeta = FBetaMeasure()\n        fbeta(predictions, targets, mask)\n        metric = fbeta.get_metric()\n        precisions = metric[""precision""]\n        recalls = metric[""recall""]\n\n        assert_allclose(precisions, [0.0, 1.0, 0.0, 0.0])\n        assert_allclose(recalls, [0.0, 1.0, 0.0, 0.0])\n'"
tests/training/metrics/mean_absolute_error_test.py,4,"b'import torch\n\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import MeanAbsoluteError\n\n\nclass MeanAbsoluteErrorTest(AllenNlpTestCase):\n    @multi_device\n    def test_mean_absolute_error_computation(self, device: str):\n        mae = MeanAbsoluteError()\n        predictions = torch.tensor(\n            [[1.0, 1.5, 1.0], [2.0, 3.0, 3.5], [4.0, 5.0, 5.5], [6.0, 7.0, 7.5]], device=device\n        )\n        targets = torch.tensor(\n            [[0.0, 1.0, 0.0], [2.0, 2.0, 0.0], [4.0, 5.0, 0.0], [7.0, 7.0, 0.0]], device=device\n        )\n        mae(predictions, targets)\n        assert mae.get_metric() == 21.0 / 12.0\n\n        mask = torch.tensor(\n            [[True, True, False], [True, True, False], [True, True, False], [True, True, False]],\n            device=device,\n        )\n        mae(predictions, targets, mask)\n        assert mae.get_metric() == (21.0 + 3.5) / (12.0 + 8.0)\n\n        new_targets = torch.tensor(\n            [[2.0, 2.0, 0.0], [0.0, 1.0, 0.0], [7.0, 7.0, 0.0], [4.0, 5.0, 0.0]], device=device\n        )\n        mae(predictions, new_targets)\n        assert mae.get_metric() == (21.0 + 3.5 + 32.0) / (12.0 + 8.0 + 12.0)\n\n        mae.reset()\n        mae(predictions, new_targets)\n        assert mae.get_metric() == 32.0 / 12.0\n'"
tests/training/metrics/pearson_correlation_test.py,10,"b'from typing import Optional\n\nimport numpy as np\nimport torch\nfrom torch.testing import assert_allclose\n\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import PearsonCorrelation\n\n\ndef pearson_corrcoef(\n    predictions: np.ndarray, labels: np.ndarray, fweights: Optional[np.ndarray] = None\n):\n    covariance_matrices = np.cov(predictions, labels, fweights=fweights)\n    denominator = np.sqrt(covariance_matrices[0, 0] * covariance_matrices[1, 1])\n    if np.around(denominator, decimals=5) == 0:\n        expected_pearson_correlation = 0\n    else:\n        expected_pearson_correlation = covariance_matrices[0, 1] / denominator\n    return expected_pearson_correlation\n\n\nclass PearsonCorrelationTest(AllenNlpTestCase):\n    @multi_device\n    def test_pearson_correlation_unmasked_computation(self, device: str):\n        pearson_correlation = PearsonCorrelation()\n        batch_size = 100\n        num_labels = 10\n        predictions_1 = torch.randn(batch_size, num_labels, device=device)\n        labels_1 = 0.5 * predictions_1 + torch.randn(batch_size, num_labels, device=device)\n\n        predictions_2 = torch.randn(1, device=device).expand(num_labels)\n        predictions_2 = predictions_2.unsqueeze(0).expand(batch_size, -1)\n        labels_2 = torch.randn(1, device=device).expand(num_labels)\n        labels_2 = 0.5 * predictions_2 + labels_2.unsqueeze(0).expand(batch_size, -1)\n\n        # in most cases, the data is constructed like predictions_1, the data of such a batch different.\n        # but in a few cases, for example, predictions_2, the data of such a batch is exactly the same.\n        predictions_labels = [(predictions_1, labels_1), (predictions_2, labels_2)]\n\n        stride = 10\n\n        for predictions, labels in predictions_labels:\n            pearson_correlation.reset()\n            for i in range(batch_size // stride):\n                timestep_predictions = predictions[stride * i : stride * (i + 1), :]\n                timestep_labels = labels[stride * i : stride * (i + 1), :]\n                expected_pearson_correlation = pearson_corrcoef(\n                    predictions[: stride * (i + 1), :].view(-1).cpu().numpy(),\n                    labels[: stride * (i + 1), :].view(-1).cpu().numpy(),\n                )\n                pearson_correlation(timestep_predictions, timestep_labels)\n                assert_allclose(expected_pearson_correlation, pearson_correlation.get_metric())\n            # Test reset\n            pearson_correlation.reset()\n            pearson_correlation(predictions, labels)\n            assert_allclose(\n                pearson_corrcoef(predictions.view(-1).cpu().numpy(), labels.view(-1).cpu().numpy()),\n                pearson_correlation.get_metric(),\n            )\n\n    @multi_device\n    def test_pearson_correlation_masked_computation(self, device: str):\n        pearson_correlation = PearsonCorrelation()\n        batch_size = 100\n        num_labels = 10\n        predictions_1 = torch.randn(batch_size, num_labels, device=device)\n        labels_1 = 0.5 * predictions_1 + torch.randn(batch_size, num_labels, device=device)\n\n        predictions_2 = torch.randn(1, device=device).expand(num_labels)\n        predictions_2 = predictions_2.unsqueeze(0).expand(batch_size, -1)\n        labels_2 = torch.randn(1, device=device).expand(num_labels)\n        labels_2 = 0.5 * predictions_2 + labels_2.unsqueeze(0).expand(batch_size, -1)\n\n        predictions_labels = [(predictions_1, labels_1), (predictions_2, labels_2)]\n\n        # Random binary mask\n        mask = torch.randint(0, 2, size=(batch_size, num_labels), device=device).bool()\n        stride = 10\n\n        for predictions, labels in predictions_labels:\n            pearson_correlation.reset()\n            for i in range(batch_size // stride):\n                timestep_predictions = predictions[stride * i : stride * (i + 1), :]\n                timestep_labels = labels[stride * i : stride * (i + 1), :]\n                timestep_mask = mask[stride * i : stride * (i + 1), :]\n                expected_pearson_correlation = pearson_corrcoef(\n                    predictions[: stride * (i + 1), :].view(-1).cpu().numpy(),\n                    labels[: stride * (i + 1), :].view(-1).cpu().numpy(),\n                    fweights=mask[: stride * (i + 1), :].view(-1).cpu().numpy(),\n                )\n\n                pearson_correlation(timestep_predictions, timestep_labels, timestep_mask)\n                assert_allclose(expected_pearson_correlation, pearson_correlation.get_metric())\n            # Test reset\n            pearson_correlation.reset()\n            pearson_correlation(predictions, labels, mask)\n            expected_pearson_correlation = pearson_corrcoef(\n                predictions.view(-1).cpu().numpy(),\n                labels.view(-1).cpu().numpy(),\n                fweights=mask.view(-1).cpu().numpy(),\n            )\n\n            assert_allclose(expected_pearson_correlation, pearson_correlation.get_metric())\n'"
tests/training/metrics/rouge_test.py,2,"b'import torch\n\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import ROUGE\n\n\nclass RougeTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.metric = ROUGE(exclude_indices={0})\n\n    def f1(self, r, p):\n        if r == p == 0:\n            return 0\n        return 2 * r * p / (r + p)\n\n    @multi_device\n    def test_rouge(self, device: str):\n        self.metric.reset()\n\n        predictions = torch.tensor([[1, 0, 1, 2], [1, 0, 3, 0], [1, 2, 3, 0]], device=device)\n        targets = torch.tensor([[2, 0, 1, 2], [1, 2, 1, 0], [1, 0, 2, 3]], device=device)\n\n        self.metric(predictions, targets)\n        metrics = self.metric.get_metric()\n\n        assert self.metric._total_sequence_count == 3\n\n        # ROUGE-N\n\n        # Unigram\n        unigram_recall = self.metric._total_rouge_n_recalls[1]\n        assert unigram_recall == 2 / 3 + 1 / 3 + 3 / 3\n        unigram_precision = self.metric._total_rouge_n_precisions[1]\n        assert unigram_precision == 2 / 3 + 1 / 2 + 3 / 3\n        unigram_f1 = self.metric._total_rouge_n_f1s[1]\n        assert unigram_f1 == self.f1(2 / 3, 2 / 3) + self.f1(1 / 2, 1 / 3) + self.f1(3 / 3, 3 / 3)\n\n        assert metrics[""ROUGE-1_R""] == unigram_recall / self.metric._total_sequence_count\n        assert metrics[""ROUGE-1_P""] == unigram_precision / self.metric._total_sequence_count\n        assert metrics[""ROUGE-1_F1""] == unigram_f1 / self.metric._total_sequence_count\n\n        # Bigram\n        bigram_recall = self.metric._total_rouge_n_recalls[2]\n        assert bigram_recall == 1 / 1 + 0 / 2 + 1 / 1\n        bigram_precision = self.metric._total_rouge_n_precisions[2]\n        assert bigram_precision == 1 / 1 + 0 + 1 / 2\n        bigram_f1 = self.metric._total_rouge_n_f1s[2]\n        assert bigram_f1 == self.f1(1 / 1, 1 / 1) + self.f1(0, 0 / 2) + self.f1(1 / 2, 1 / 1)\n\n        assert metrics[""ROUGE-2_R""] == bigram_recall / self.metric._total_sequence_count\n        assert metrics[""ROUGE-2_P""] == bigram_precision / self.metric._total_sequence_count\n        assert metrics[""ROUGE-2_F1""] == bigram_f1 / self.metric._total_sequence_count\n\n        # ROUGE-L\n\n        assert self.metric._total_rouge_l_f1 == self.f1(2 / 3, 2 / 3) + self.f1(\n            1 / 3, 1 / 2\n        ) + self.f1(3 / 3, 3 / 3)\n\n        assert (\n            metrics[""ROUGE-L""] == self.metric._total_rouge_l_f1 / self.metric._total_sequence_count\n        )\n\n    def test_rouge_with_zero_counts(self):\n        self.metric.reset()\n        metrics = self.metric.get_metric()\n        for score in metrics.values():\n            assert score == 0.0\n'"
tests/training/metrics/sequence_accuracy_test.py,9,"b'import torch\nfrom torch.testing import assert_allclose\n\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import SequenceAccuracy\n\n\nclass SequenceAccuracyTest(AllenNlpTestCase):\n    @multi_device\n    def test_sequence_accuracy(self, device: str):\n        accuracy = SequenceAccuracy()\n        gold = torch.tensor([[1, 2, 3], [2, 4, 8], [0, 1, 1]], device=device)\n        predictions = torch.tensor(\n            [[[1, 2, 3], [1, 2, -1]], [[2, 4, 8], [2, 5, 9]], [[-1, -1, -1], [0, 1, -1]]],\n            device=device,\n        )\n\n        accuracy(predictions, gold)\n        actual_accuracy = accuracy.get_metric()\n        assert_allclose(actual_accuracy, 2 / 3)\n\n    @multi_device\n    def test_sequence_accuracy_respects_mask(self, device: str):\n        accuracy = SequenceAccuracy()\n        gold = torch.tensor([[1, 2, 3], [2, 4, 8], [0, 1, 1], [11, 13, 17]], device=device)\n        predictions = torch.tensor(\n            [\n                [[1, 2, 3], [1, 2, -1]],\n                [[2, 4, 8], [2, 5, 9]],\n                [[-1, -1, -1], [0, 1, -1]],\n                [[12, 13, 17], [11, 13, 18]],\n            ],\n            device=device,\n        )\n        mask = torch.tensor(\n            [[False, True, True], [True, True, True], [True, True, False], [True, False, True]],\n            device=device,\n        )\n\n        accuracy(predictions, gold, mask)\n        actual_accuracy = accuracy.get_metric()\n        assert_allclose(actual_accuracy, 3 / 4)\n\n    @multi_device\n    def test_sequence_accuracy_accumulates_and_resets_correctly(self, device: str):\n        accuracy = SequenceAccuracy()\n        gold = torch.tensor([[1, 2, 3]], device=device)\n        accuracy(torch.tensor([[[1, 2, 3]]], device=device), gold)\n        accuracy(torch.tensor([[[1, 2, 4]]], device=device), gold)\n\n        actual_accuracy = accuracy.get_metric(reset=True)\n        assert_allclose(actual_accuracy, 1 / 2)\n        assert accuracy.correct_count == 0\n        assert accuracy.total_count == 0\n\n    @multi_device\n    def test_get_metric_on_new_object_works(self, device: str):\n        accuracy = SequenceAccuracy()\n\n        actual_accuracy = accuracy.get_metric(reset=True)\n        assert_allclose(actual_accuracy, 0)\n'"
tests/training/metrics/span_based_f1_measure_test.py,11,"b'import torch\nfrom torch.testing import assert_allclose\nimport pytest\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.params import Params\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.data import Vocabulary\nfrom allennlp.training.metrics import SpanBasedF1Measure, Metric\n\n\nclass SpanBasedF1Test(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        vocab = Vocabulary()\n        vocab.add_token_to_namespace(""O"", ""tags"")\n        vocab.add_token_to_namespace(""B-ARG1"", ""tags"")\n        vocab.add_token_to_namespace(""I-ARG1"", ""tags"")\n        vocab.add_token_to_namespace(""B-ARG2"", ""tags"")\n        vocab.add_token_to_namespace(""I-ARG2"", ""tags"")\n        vocab.add_token_to_namespace(""B-V"", ""tags"")\n        vocab.add_token_to_namespace(""I-V"", ""tags"")\n        vocab.add_token_to_namespace(""U-ARG1"", ""tags"")\n        vocab.add_token_to_namespace(""U-ARG2"", ""tags"")\n        vocab.add_token_to_namespace(""B-C-ARG1"", ""tags"")\n        vocab.add_token_to_namespace(""I-C-ARG1"", ""tags"")\n        vocab.add_token_to_namespace(""B-ARGM-ADJ"", ""tags"")\n        vocab.add_token_to_namespace(""I-ARGM-ADJ"", ""tags"")\n\n        # BMES.\n        vocab.add_token_to_namespace(""B"", ""bmes_tags"")\n        vocab.add_token_to_namespace(""M"", ""bmes_tags"")\n        vocab.add_token_to_namespace(""E"", ""bmes_tags"")\n        vocab.add_token_to_namespace(""S"", ""bmes_tags"")\n\n        self.vocab = vocab\n\n    @multi_device\n    def test_span_metrics_are_computed_correcly_with_prediction_map(self, device: str):\n        # In this example, datapoint1 only has access to ARG1 and V labels,\n        # whereas datapoint2 only has access to ARG2 and V labels.\n\n        # gold_labels = [[""O"", ""B-ARG1"", ""I-ARG1"", ""O"", ""B-V"", ""O""],\n        #               [""B-ARG2"", ""I-ARG2"", ""O"", ""B-V"", ""I-V"", ""O""]]\n        gold_indices = [[0, 1, 2, 0, 3, 0], [1, 2, 0, 3, 4, 0]]\n        prediction_map_indices = [[0, 1, 2, 5, 6], [0, 3, 4, 5, 6]]\n\n        gold_tensor = torch.tensor(gold_indices, device=device)\n        prediction_map_tensor = torch.tensor(prediction_map_indices, device=device)\n\n        prediction_tensor = torch.rand([2, 6, 5], device=device)\n        prediction_tensor[0, 0, 0] = 1\n        prediction_tensor[0, 1, 1] = 1  # (True Positive - ARG1\n        prediction_tensor[0, 2, 2] = 1  # *)\n        prediction_tensor[0, 3, 0] = 1\n        prediction_tensor[0, 4, 3] = 1  # (True Positive - V)\n        prediction_tensor[0, 5, 1] = 1  # (False Positive - ARG1)\n        prediction_tensor[1, 0, 0] = 1  # (False Negative - ARG2\n        prediction_tensor[1, 1, 0] = 1  # *)\n        prediction_tensor[1, 2, 0] = 1\n        prediction_tensor[1, 3, 3] = 1  # (True Positive - V\n        prediction_tensor[1, 4, 4] = 1  # *)\n        prediction_tensor[1, 5, 1] = 1  # (False Positive - ARG2)\n\n        metric = SpanBasedF1Measure(self.vocab, ""tags"")\n        metric(prediction_tensor, gold_tensor, prediction_map=prediction_map_tensor)\n\n        assert metric._true_positives[""ARG1""] == 1\n        assert metric._true_positives[""ARG2""] == 0\n        assert metric._true_positives[""V""] == 2\n        assert ""O"" not in metric._true_positives.keys()\n        assert metric._false_negatives[""ARG1""] == 0\n        assert metric._false_negatives[""ARG2""] == 1\n        assert metric._false_negatives[""V""] == 0\n        assert ""O"" not in metric._false_negatives.keys()\n        assert metric._false_positives[""ARG1""] == 1\n        assert metric._false_positives[""ARG2""] == 1\n        assert metric._false_positives[""V""] == 0\n        assert ""O"" not in metric._false_positives.keys()\n\n        # Check things are accumulating correctly.\n        metric(prediction_tensor, gold_tensor, prediction_map=prediction_map_tensor)\n        assert metric._true_positives[""ARG1""] == 2\n        assert metric._true_positives[""ARG2""] == 0\n        assert metric._true_positives[""V""] == 4\n        assert ""O"" not in metric._true_positives.keys()\n        assert metric._false_negatives[""ARG1""] == 0\n        assert metric._false_negatives[""ARG2""] == 2\n        assert metric._false_negatives[""V""] == 0\n        assert ""O"" not in metric._false_negatives.keys()\n        assert metric._false_positives[""ARG1""] == 2\n        assert metric._false_positives[""ARG2""] == 2\n        assert metric._false_positives[""V""] == 0\n        assert ""O"" not in metric._false_positives.keys()\n\n        metric_dict = metric.get_metric()\n\n        assert_allclose(metric_dict[""recall-ARG2""], 0.0)\n        assert_allclose(metric_dict[""precision-ARG2""], 0.0)\n        assert_allclose(metric_dict[""f1-measure-ARG2""], 0.0)\n        assert_allclose(metric_dict[""recall-ARG1""], 1.0)\n        assert_allclose(metric_dict[""precision-ARG1""], 0.5)\n        assert_allclose(metric_dict[""f1-measure-ARG1""], 0.666666666)\n        assert_allclose(metric_dict[""recall-V""], 1.0)\n        assert_allclose(metric_dict[""precision-V""], 1.0)\n        assert_allclose(metric_dict[""f1-measure-V""], 1.0)\n        assert_allclose(metric_dict[""recall-overall""], 0.75)\n        assert_allclose(metric_dict[""precision-overall""], 0.6)\n        assert_allclose(metric_dict[""f1-measure-overall""], 0.666666666)\n\n    @multi_device\n    def test_span_metrics_are_computed_correctly(self, device: str):\n        gold_labels = [""O"", ""B-ARG1"", ""I-ARG1"", ""O"", ""B-ARG2"", ""I-ARG2"", ""O"", ""O"", ""O""]\n        gold_indices = [self.vocab.get_token_index(x, ""tags"") for x in gold_labels]\n\n        gold_tensor = torch.tensor([gold_indices], device=device)\n\n        prediction_tensor = torch.rand([2, 9, self.vocab.get_vocab_size(""tags"")], device=device)\n\n        # Test that the span measure ignores completely masked sequences by\n        # passing a mask with a fully masked row.\n        mask = torch.tensor(\n            [\n                [True, True, True, True, True, True, True, True, True],\n                [False, False, False, False, False, False, False, False, False],\n            ],\n            device=device,\n        )\n\n        prediction_tensor[:, 0, 0] = 1\n        prediction_tensor[:, 1, 1] = 1  # (True positive - ARG1\n        prediction_tensor[:, 2, 2] = 1  # *)\n        prediction_tensor[:, 3, 0] = 1\n        prediction_tensor[:, 4, 0] = 1  # (False Negative - ARG2\n        prediction_tensor[:, 5, 0] = 1  # *)\n        prediction_tensor[:, 6, 0] = 1\n        prediction_tensor[:, 7, 1] = 1  # (False Positive - ARG1\n        prediction_tensor[:, 8, 2] = 1  # *)\n\n        metric = SpanBasedF1Measure(self.vocab, ""tags"")\n        metric(prediction_tensor, gold_tensor, mask)\n\n        assert metric._true_positives[""ARG1""] == 1\n        assert metric._true_positives[""ARG2""] == 0\n        assert ""O"" not in metric._true_positives.keys()\n        assert metric._false_negatives[""ARG1""] == 0\n        assert metric._false_negatives[""ARG2""] == 1\n        assert ""O"" not in metric._false_negatives.keys()\n        assert metric._false_positives[""ARG1""] == 1\n        assert metric._false_positives[""ARG2""] == 0\n        assert ""O"" not in metric._false_positives.keys()\n\n        # Check things are accumulating correctly.\n        metric(prediction_tensor, gold_tensor, mask)\n        assert metric._true_positives[""ARG1""] == 2\n        assert metric._true_positives[""ARG2""] == 0\n        assert ""O"" not in metric._true_positives.keys()\n        assert metric._false_negatives[""ARG1""] == 0\n        assert metric._false_negatives[""ARG2""] == 2\n        assert ""O"" not in metric._false_negatives.keys()\n        assert metric._false_positives[""ARG1""] == 2\n        assert metric._false_positives[""ARG2""] == 0\n        assert ""O"" not in metric._false_positives.keys()\n\n        metric_dict = metric.get_metric()\n\n        assert_allclose(metric_dict[""recall-ARG2""], 0.0)\n        assert_allclose(metric_dict[""precision-ARG2""], 0.0)\n        assert_allclose(metric_dict[""f1-measure-ARG2""], 0.0)\n        assert_allclose(metric_dict[""recall-ARG1""], 1.0)\n        assert_allclose(metric_dict[""precision-ARG1""], 0.5)\n        assert_allclose(metric_dict[""f1-measure-ARG1""], 0.666666666)\n        assert_allclose(metric_dict[""recall-overall""], 0.5)\n        assert_allclose(metric_dict[""precision-overall""], 0.5)\n        assert_allclose(metric_dict[""f1-measure-overall""], 0.5)\n\n    @multi_device\n    def test_bmes_span_metrics_are_computed_correctly(self, device: str):\n        # (bmes_tags) B:0, M:1, E:2, S:3.\n        # [S, B, M, E, S]\n        # [S, S, S, S, S]\n        gold_indices = [[3, 0, 1, 2, 3], [3, 3, 3, 3, 3]]\n        gold_tensor = torch.tensor(gold_indices, device=device)\n\n        prediction_tensor = torch.rand([2, 5, 4], device=device)\n        # [S, B, E, S, S]\n        # TP: 2, FP: 2, FN: 1.\n        prediction_tensor[0, 0, 3] = 1  # (True positive)\n        prediction_tensor[0, 1, 0] = 1  # (False positive\n        prediction_tensor[0, 2, 2] = 1  # *)\n        prediction_tensor[0, 3, 3] = 1  # (False positive)\n        prediction_tensor[0, 4, 3] = 1  # (True positive)\n        # [B, E, S, B, E]\n        # TP: 1, FP: 2, FN: 4.\n        prediction_tensor[1, 0, 0] = 1  # (False positive\n        prediction_tensor[1, 1, 2] = 1  # *)\n        prediction_tensor[1, 2, 3] = 1  # (True positive)\n        prediction_tensor[1, 3, 0] = 1  # (False positive\n        prediction_tensor[1, 4, 2] = 1  # *)\n\n        metric = SpanBasedF1Measure(self.vocab, ""bmes_tags"", label_encoding=""BMES"")\n        metric(prediction_tensor, gold_tensor)\n\n        # TP: 3, FP: 4, FN: 5.\n        metric_dict = metric.get_metric()\n\n        assert_allclose(metric_dict[""recall-overall""], 0.375, rtol=0.001, atol=1e-03)\n        assert_allclose(metric_dict[""precision-overall""], 0.428, rtol=0.001, atol=1e-03)\n        assert_allclose(metric_dict[""f1-measure-overall""], 0.4, rtol=0.001, atol=1e-03)\n\n    @multi_device\n    def test_span_f1_can_build_from_params(self, device: str):\n        params = Params({""type"": ""span_f1"", ""tag_namespace"": ""tags"", ""ignore_classes"": [""V""]})\n        metric = Metric.from_params(params=params, vocabulary=self.vocab)\n        assert metric._ignore_classes == [""V""]  # type: ignore\n        assert metric._label_vocabulary == self.vocab.get_index_to_token_vocabulary(  # type: ignore\n            ""tags""\n        )\n\n    @multi_device\n    def test_span_f1_accepts_tags_to_spans_function_argument(self, device: str):\n        def mock_tags_to_spans_function(tag_sequence, classes_to_ignore=None):\n            return [(""mock"", (42, 42))]\n\n        # Should be ignore.\n        bio_tags = [""B-ARG1"", ""O"", ""B-C-ARG1"", ""B-V"", ""B-ARGM-ADJ"", ""O""]\n        gold_indices = [self.vocab.get_token_index(x, ""tags"") for x in bio_tags]\n        gold_tensor = torch.tensor([gold_indices], device=device)\n        prediction_tensor = torch.rand([1, 6, self.vocab.get_vocab_size(""tags"")], device=device)\n\n        metric = SpanBasedF1Measure(\n            self.vocab,\n            ""tags"",\n            label_encoding=None,\n            tags_to_spans_function=mock_tags_to_spans_function,\n        )\n\n        metric(prediction_tensor, gold_tensor)\n        metric_dict = metric.get_metric()\n\n        assert_allclose(metric_dict[""recall-overall""], 1.0)\n        assert_allclose(metric_dict[""precision-overall""], 1.0)\n        assert_allclose(metric_dict[""f1-measure-overall""], 1.0)\n\n        with pytest.raises(ConfigurationError):\n            SpanBasedF1Measure(self.vocab, label_encoding=""INVALID"")\n        with pytest.raises(ConfigurationError):\n            SpanBasedF1Measure(self.vocab, tags_to_spans_function=mock_tags_to_spans_function)\n        with pytest.raises(ConfigurationError):\n            SpanBasedF1Measure(self.vocab, label_encoding=None, tags_to_spans_function=None)\n'"
tests/training/metrics/spearman_correlation_test.py,13,"b'import math\r\n\r\nimport torch\r\nfrom torch.testing import assert_allclose\r\n\r\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\r\nfrom allennlp.training.metrics import SpearmanCorrelation\r\n\r\n\r\ndef spearman_formula(predictions, labels, mask=None):\r\n    """"""\r\n    This function is spearman formula from:\r\n        https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\r\n    """"""\r\n    if mask is not None:\r\n        predictions = predictions * mask\r\n        labels = labels * mask\r\n\r\n    # if all number of a set is same, return np.nan\r\n    if len(torch.unique(predictions)) == 1 or len(torch.unique(labels)) == 1:\r\n        return float(""NaN"")\r\n\r\n    len_pre = len(predictions)\r\n\r\n    predictions = [(k, v) for k, v in enumerate(predictions)]\r\n    predictions.sort(key=lambda x: x[1], reverse=True)\r\n    predictions = [(k, v) for k, v in enumerate(predictions)]\r\n    predictions.sort(key=lambda x: x[1][0])\r\n\r\n    labels = [(k, v) for k, v in enumerate(labels)]\r\n    labels.sort(key=lambda x: x[1], reverse=True)\r\n    labels = [(k, v) for k, v in enumerate(labels)]\r\n    labels.sort(key=lambda x: x[1][0])\r\n\r\n    total = 0\r\n    for i in range(len_pre):\r\n        total += (predictions[i][0] - labels[i][0]) ** 2\r\n    expected_spearman_correlation = 1 - 6 * total / (len_pre * (len_pre ** 2 - 1))\r\n\r\n    return expected_spearman_correlation\r\n\r\n\r\nclass SpearmanCorrelationTest(AllenNlpTestCase):\r\n    @multi_device\r\n    def test_unmasked_computation(self, device: str):\r\n        spearman_correlation = SpearmanCorrelation()\r\n        batch_size = 10\r\n        num_labels = 10\r\n        predictions1 = torch.randn(batch_size, num_labels, device=device)\r\n        labels1 = 0.5 * predictions1 + torch.randn(batch_size, num_labels, device=device)\r\n\r\n        predictions2 = torch.randn(1, device=device).repeat(num_labels)\r\n        predictions2 = predictions2.unsqueeze(0).expand(batch_size, -1)\r\n        labels2 = torch.randn(1, device=device).expand(num_labels)\r\n        labels2 = 0.5 * predictions2 + labels2.unsqueeze(0).expand(batch_size, -1)\r\n\r\n        # in most cases, the data is constructed like predictions_1, the data of such a batch different.\r\n        # but in a few cases, for example, predictions_2, the data of such a batch is exactly the same.\r\n        predictions_labels_ = [(predictions1, labels1), (predictions2, labels2)]\r\n\r\n        for predictions, labels in predictions_labels_:\r\n            spearman_correlation.reset()\r\n            spearman_correlation(predictions, labels)\r\n            assert_allclose(\r\n                spearman_formula(predictions.reshape(-1), labels.reshape(-1)),\r\n                spearman_correlation.get_metric(),\r\n            )\r\n\r\n    @multi_device\r\n    def test_masked_computation(self, device: str):\r\n        spearman_correlation = SpearmanCorrelation()\r\n        batch_size = 10\r\n        num_labels = 10\r\n        predictions1 = torch.randn(batch_size, num_labels, device=device)\r\n        labels1 = 0.5 * predictions1 + torch.randn(batch_size, num_labels, device=device)\r\n\r\n        predictions2 = torch.randn(1, device=device).expand(num_labels)\r\n        predictions2 = predictions2.unsqueeze(0).expand(batch_size, -1)\r\n        labels2 = torch.randn(1, device=device).expand(num_labels)\r\n        labels2 = 0.5 * predictions2 + labels2.unsqueeze(0).expand(batch_size, -1)\r\n\r\n        # in most cases, the data is constructed like predictions_1, the data of such a batch different.\r\n        # but in a few cases, for example, predictions_2, the data of such a batch is exactly the same.\r\n        predictions_labels_ = [(predictions1, labels1), (predictions2, labels2)]\r\n\r\n        # Random binary mask\r\n        mask = torch.randint(0, 2, size=(batch_size, num_labels), device=device).bool()\r\n\r\n        for predictions, labels in predictions_labels_:\r\n            spearman_correlation.reset()\r\n            spearman_correlation(predictions, labels, mask)\r\n            expected_spearman_correlation = spearman_formula(\r\n                predictions.view(-1), labels.view(-1), mask=mask.view(-1)\r\n            )\r\n\r\n            # because add mask, a batch of predictions or labels will have many 0,\r\n            # spearman correlation algorithm will dependence the sorting position of a set of numbers,\r\n            # too many identical numbers will result in different calculation results each time\r\n            # but the positive and negative results are the same,\r\n            # so here we only test the positive and negative results of the results.\r\n            assert (expected_spearman_correlation * spearman_correlation.get_metric()) > 0\r\n\r\n    @multi_device\r\n    def test_reset(self, device: str):\r\n        spearman_correlation = SpearmanCorrelation()\r\n        batch_size = 10\r\n        num_labels = 10\r\n        predictions = torch.randn(batch_size, num_labels, device=device)\r\n        labels = 0.5 * predictions + torch.randn(batch_size, num_labels, device=device)\r\n\r\n        # 1.test spearman_correlation.reset()\r\n        spearman_correlation.reset()\r\n        spearman_correlation(predictions, labels)\r\n        temp = spearman_correlation.get_metric()\r\n        spearman_correlation.reset()\r\n        spearman_correlation(predictions, labels)\r\n        assert spearman_correlation.get_metric() == temp\r\n\r\n        # 2.test spearman_correlation.reset()\r\n        spearman_correlation.reset()\r\n        spearman_correlation(predictions, labels)\r\n\r\n        spearman_correlation.get_metric(reset=False)\r\n        assert spearman_correlation.get_metric() != float(""NaN"")\r\n        spearman_correlation.get_metric(reset=True)\r\n        assert math.isnan(spearman_correlation.get_metric())\r\n'"
tests/training/metrics/unigram_recall_test.py,9,"b'import torch\nfrom torch.testing import assert_allclose\n\nfrom allennlp.common.testing import AllenNlpTestCase, multi_device\nfrom allennlp.training.metrics import UnigramRecall\n\n\nclass UnigramRecallTest(AllenNlpTestCase):\n    @multi_device\n    def test_sequence_recall(self, device: str):\n        recall = UnigramRecall()\n        gold = torch.tensor([[1, 2, 3], [2, 4, 8], [7, 1, 1]], device=device)\n        predictions = torch.tensor(\n            [[[1, 2, 3], [1, 2, -1]], [[2, 4, 8], [2, 5, 9]], [[-1, -1, -1], [7, 1, -1]]],\n            device=device,\n        )\n\n        recall(predictions, gold)\n        actual_recall = recall.get_metric()\n        assert_allclose(actual_recall, 1)\n\n    @multi_device\n    def test_sequence_recall_respects_mask(self, device: str):\n        recall = UnigramRecall()\n        gold = torch.tensor([[2, 4, 8], [1, 2, 3], [7, 1, 1], [11, 14, 17]], device=device)\n        predictions = torch.tensor(\n            [\n                [[2, 4, 8], [2, 5, 9]],  # 3/3\n                [[-1, 2, 4], [3, 8, -1]],  # 2/2\n                [[-1, -1, -1], [7, 2, -1]],  # 1/2\n                [[12, 13, 17], [11, 13, 18]],  # 2/2\n            ],\n            device=device,\n        )\n        mask = torch.tensor(\n            [[True, True, True], [False, True, True], [True, True, False], [True, False, True]],\n            device=device,\n        )\n\n        recall(predictions, gold, mask)\n        actual_recall = recall.get_metric()\n        assert_allclose(actual_recall, 7 / 8)\n\n    @multi_device\n    def test_sequence_recall_accumulates_and_resets_correctly(self, device: str):\n        recall = UnigramRecall()\n        gold = torch.tensor([[1, 2, 3]], device=device)\n        recall(torch.tensor([[[1, 2, 3]]], device=device), gold)\n        recall(torch.tensor([[[7, 8, 4]]], device=device), gold)\n\n        actual_recall = recall.get_metric(reset=True)\n        assert_allclose(actual_recall, 1 / 2)\n        assert recall.correct_count == 0\n        assert recall.total_count == 0\n\n    @multi_device\n    def test_get_metric_on_new_object_works(self, device: str):\n        recall = UnigramRecall()\n\n        actual_recall = recall.get_metric(reset=True)\n        assert_allclose(actual_recall, 0)\n'"
tests/training/momentum_schedulers/__init__.py,0,b''
tests/training/momentum_schedulers/inverted_triangular_test.py,1,"b'from math import isclose\nimport torch\n\nfrom allennlp.common.params import Params\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.training.momentum_schedulers import MomentumScheduler\nfrom allennlp.training.optimizers import Optimizer\n\n\nclass InvertedTriangularTest(AllenNlpTestCase):\n    def setup_method(self):\n        super().setup_method()\n        self.model = torch.nn.Sequential(torch.nn.Linear(10, 10))\n        self.base_momentum = 0.9\n\n    def _get_optimizer(self):\n        return Optimizer.from_params(\n            model_parameters=self.model.named_parameters(),\n            params=Params({""type"": ""sgd"", ""lr"": 1.0, ""momentum"": self.base_momentum}),\n        )\n\n    def test_from_params(self):\n        optimizer = self._get_optimizer()\n        scheduler = MomentumScheduler.from_params(\n            optimizer=optimizer,\n            params=Params({""type"": ""inverted_triangular"", ""cool_down"": 10, ""warm_up"": 10}),\n        )\n        assert scheduler.cool_down == 10\n        assert scheduler.warm_up == 10\n        assert scheduler.ratio == 10\n        assert scheduler.last_epoch == -1\n\n    def test_basic_schedule(self):\n        optimizer = self._get_optimizer()\n        scheduler = MomentumScheduler.from_params(\n            optimizer=optimizer,\n            params=Params(\n                {""type"": ""inverted_triangular"", ""cool_down"": 6, ""warm_up"": 10, ""ratio"": 5}\n            ),\n        )\n        # Before first epoch, momentum should be unchanged.\n        assert optimizer.param_groups[0][""momentum""] == self.base_momentum\n        # After first epoch, `step` is called, and momentum should be adjusted for\n        # the next epoch.\n        scheduler.step()\n        assert isclose(\n            optimizer.param_groups[0][""momentum""],\n            self.base_momentum - (self.base_momentum - self.base_momentum / 5) * (1 / 6),\n        )\n        # After second epoch, `step` is called and momentum is updated for 3rd epoch.\n        scheduler.step()\n        assert isclose(\n            optimizer.param_groups[0][""momentum""],\n            self.base_momentum - (self.base_momentum - self.base_momentum / 5) * (2 / 6),\n        )\n        scheduler.last_epoch = 4\n        # ... after the 6th epoch (epoch id 5), momentum should be set to `base_momentum / ratio`.\n        scheduler.step()\n        assert isclose(optimizer.param_groups[0][""momentum""], self.base_momentum / 5)\n        # Then the momentum stars increasing again.\n        scheduler.step()\n        assert isclose(\n            optimizer.param_groups[0][""momentum""],\n            self.base_momentum / 5 + (self.base_momentum - self.base_momentum / 5) * (1 / 10),\n        )\n        # After the 16th epoch (6 + 10) (epoch id 15), momentum should be back to the base level.\n        scheduler.last_epoch = 14\n        scheduler.step()\n        assert isclose(optimizer.param_groups[0][""momentum""], self.base_momentum)\n        scheduler.step()\n        assert isclose(optimizer.param_groups[0][""momentum""], self.base_momentum)\n        scheduler.step()\n        assert isclose(optimizer.param_groups[0][""momentum""], self.base_momentum)\n'"
tests/tutorials/tagger/__init__.py,0,b''
tests/tutorials/tagger/basic_allennlp_test.py,0,"b'import pytest\n\nfrom allennlp.common.testing import AllenNlpTestCase\n\n\n@pytest.mark.skip(""makes test-install fail (and also takes 30 seconds)"")\nclass TestBasicAllenNlp(AllenNlpTestCase):\n    @classmethod\n    def test_run_as_script(cls):\n        # Just ensure the tutorial runs without throwing an exception.\n\n        import tutorials.tagger.basic_allennlp  # noqa\n'"
allennlp/data/dataset_readers/dataset_utils/__init__.py,0,"b'from allennlp.data.dataset_readers.dataset_utils.span_utils import enumerate_spans\nfrom allennlp.data.dataset_readers.dataset_utils.span_utils import bio_tags_to_spans\nfrom allennlp.data.dataset_readers.dataset_utils.span_utils import to_bioul, iob1_to_bioul\nfrom allennlp.data.dataset_readers.dataset_utils.span_utils import bioul_tags_to_spans\n'"
allennlp/data/dataset_readers/dataset_utils/span_utils.py,0,"b'from typing import Callable, List, Set, Tuple, TypeVar, Optional\nimport warnings\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data.tokenizers.token import Token\n\n\nTypedSpan = Tuple[int, Tuple[int, int]]\nTypedStringSpan = Tuple[str, Tuple[int, int]]\n\n\nclass InvalidTagSequence(Exception):\n    def __init__(self, tag_sequence=None):\n        super().__init__()\n        self.tag_sequence = tag_sequence\n\n    def __str__(self):\n        return "" "".join(self.tag_sequence)\n\n\nT = TypeVar(""T"", str, Token)\n\n\ndef enumerate_spans(\n    sentence: List[T],\n    offset: int = 0,\n    max_span_width: int = None,\n    min_span_width: int = 1,\n    filter_function: Callable[[List[T]], bool] = None,\n) -> List[Tuple[int, int]]:\n    """"""\n    Given a sentence, return all token spans within the sentence. Spans are `inclusive`.\n    Additionally, you can provide a maximum and minimum span width, which will be used\n    to exclude spans outside of this range.\n\n    Finally, you can provide a function mapping `List[T] -> bool`, which will\n    be applied to every span to decide whether that span should be included. This\n    allows filtering by length, regex matches, pos tags or any Spacy `Token`\n    attributes, for example.\n\n    # Parameters\n\n    sentence : `List[T]`, required.\n        The sentence to generate spans for. The type is generic, as this function\n        can be used with strings, or Spacy `Tokens` or other sequences.\n    offset : `int`, optional (default = `0`)\n        A numeric offset to add to all span start and end indices. This is helpful\n        if the sentence is part of a larger structure, such as a document, which\n        the indices need to respect.\n    max_span_width : `int`, optional (default = `None`)\n        The maximum length of spans which should be included. Defaults to len(sentence).\n    min_span_width : `int`, optional (default = `1`)\n        The minimum length of spans which should be included. Defaults to 1.\n    filter_function : `Callable[[List[T]], bool]`, optional (default = `None`)\n        A function mapping sequences of the passed type T to a boolean value.\n        If `True`, the span is included in the returned spans from the\n        sentence, otherwise it is excluded..\n    """"""\n    max_span_width = max_span_width or len(sentence)\n    filter_function = filter_function or (lambda x: True)\n    spans: List[Tuple[int, int]] = []\n\n    for start_index in range(len(sentence)):\n        last_end_index = min(start_index + max_span_width, len(sentence))\n        first_end_index = min(start_index + min_span_width - 1, len(sentence))\n        for end_index in range(first_end_index, last_end_index):\n            start = offset + start_index\n            end = offset + end_index\n            # add 1 to end index because span indices are inclusive.\n            if filter_function(sentence[slice(start_index, end_index + 1)]):\n                spans.append((start, end))\n    return spans\n\n\ndef bio_tags_to_spans(\n    tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[TypedStringSpan]:\n    """"""\n    Given a sequence corresponding to BIO tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e those which do not start with a ""B-LABEL""),\n    as otherwise it is possible to get a perfect precision score whilst still predicting\n    ill-formed spans in addition to the correct spans. This function works properly when\n    the spans are unlabeled (i.e., your labels are simply ""B"", ""I"", and ""O"").\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The integer class labels for a sequence.\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    """"""\n    classes_to_ignore = classes_to_ignore or []\n    spans: Set[Tuple[str, Tuple[int, int]]] = set()\n    span_start = 0\n    span_end = 0\n    active_conll_tag = None\n    for index, string_tag in enumerate(tag_sequence):\n        # Actual BIO tag.\n        bio_tag = string_tag[0]\n        if bio_tag not in [""B"", ""I"", ""O""]:\n            raise InvalidTagSequence(tag_sequence)\n        conll_tag = string_tag[2:]\n        if bio_tag == ""O"" or conll_tag in classes_to_ignore:\n            # The span has ended.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = None\n            # We don\'t care about tags we are\n            # told to ignore, so we do nothing.\n            continue\n        elif bio_tag == ""B"":\n            # We are entering a new span; reset indices\n            # and active tag to new span.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = conll_tag\n            span_start = index\n            span_end = index\n        elif bio_tag == ""I"" and conll_tag == active_conll_tag:\n            # We\'re inside a span.\n            span_end += 1\n        else:\n            # This is the case the bio label is an ""I"", but either:\n            # 1) the span hasn\'t started - i.e. an ill formed span.\n            # 2) The span is an I tag for a different conll annotation.\n            # We\'ll process the previous span if it exists, but also\n            # include this span. This is important, because otherwise,\n            # a model may get a perfect F1 score whilst still including\n            # false positive ill-formed spans.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = conll_tag\n            span_start = index\n            span_end = index\n    # Last token might have been a part of a valid span.\n    if active_conll_tag is not None:\n        spans.add((active_conll_tag, (span_start, span_end)))\n    return list(spans)\n\n\ndef iob1_tags_to_spans(\n    tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[TypedStringSpan]:\n    """"""\n    Given a sequence corresponding to IOB1 tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e., those where ""B-LABEL"" is not preceded\n    by ""I-LABEL"" or ""B-LABEL"").\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The integer class labels for a sequence.\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    """"""\n    classes_to_ignore = classes_to_ignore or []\n    spans: Set[Tuple[str, Tuple[int, int]]] = set()\n    span_start = 0\n    span_end = 0\n    active_conll_tag = None\n    prev_bio_tag = None\n    prev_conll_tag = None\n    for index, string_tag in enumerate(tag_sequence):\n        curr_bio_tag = string_tag[0]\n        curr_conll_tag = string_tag[2:]\n\n        if curr_bio_tag not in [""B"", ""I"", ""O""]:\n            raise InvalidTagSequence(tag_sequence)\n        if curr_bio_tag == ""O"" or curr_conll_tag in classes_to_ignore:\n            # The span has ended.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = None\n        elif _iob1_start_of_chunk(prev_bio_tag, prev_conll_tag, curr_bio_tag, curr_conll_tag):\n            # We are entering a new span; reset indices\n            # and active tag to new span.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = curr_conll_tag\n            span_start = index\n            span_end = index\n        else:\n            # bio_tag == ""I"" and curr_conll_tag == active_conll_tag\n            # We\'re continuing a span.\n            span_end += 1\n\n        prev_bio_tag = string_tag[0]\n        prev_conll_tag = string_tag[2:]\n    # Last token might have been a part of a valid span.\n    if active_conll_tag is not None:\n        spans.add((active_conll_tag, (span_start, span_end)))\n    return list(spans)\n\n\ndef _iob1_start_of_chunk(\n    prev_bio_tag: Optional[str],\n    prev_conll_tag: Optional[str],\n    curr_bio_tag: str,\n    curr_conll_tag: str,\n) -> bool:\n    if curr_bio_tag == ""B"":\n        return True\n    if curr_bio_tag == ""I"" and prev_bio_tag == ""O"":\n        return True\n    if curr_bio_tag != ""O"" and prev_conll_tag != curr_conll_tag:\n        return True\n    return False\n\n\ndef bioul_tags_to_spans(\n    tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[TypedStringSpan]:\n    """"""\n    Given a sequence corresponding to BIOUL tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are not allowed and will raise `InvalidTagSequence`.\n    This function works properly when the spans are unlabeled (i.e., your labels are\n    simply ""B"", ""I"", ""O"", ""U"", and ""L"").\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The tag sequence encoded in BIOUL, e.g. [""B-PER"", ""L-PER"", ""O""].\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n    """"""\n    spans = []\n    classes_to_ignore = classes_to_ignore or []\n    index = 0\n    while index < len(tag_sequence):\n        label = tag_sequence[index]\n        if label[0] == ""U"":\n            spans.append((label.partition(""-"")[2], (index, index)))\n        elif label[0] == ""B"":\n            start = index\n            while label[0] != ""L"":\n                index += 1\n                if index >= len(tag_sequence):\n                    raise InvalidTagSequence(tag_sequence)\n                label = tag_sequence[index]\n                if not (label[0] == ""I"" or label[0] == ""L""):\n                    raise InvalidTagSequence(tag_sequence)\n            spans.append((label.partition(""-"")[2], (start, index)))\n        else:\n            if label != ""O"":\n                raise InvalidTagSequence(tag_sequence)\n        index += 1\n    return [span for span in spans if span[0] not in classes_to_ignore]\n\n\ndef iob1_to_bioul(tag_sequence: List[str]) -> List[str]:\n    warnings.warn(\n        ""iob1_to_bioul has been replaced with \'to_bioul\' to allow more encoding options."",\n        FutureWarning,\n    )\n    return to_bioul(tag_sequence)\n\n\ndef to_bioul(tag_sequence: List[str], encoding: str = ""IOB1"") -> List[str]:\n    """"""\n    Given a tag sequence encoded with IOB1 labels, recode to BIOUL.\n\n    In the IOB1 scheme, I is a token inside a span, O is a token outside\n    a span and B is the beginning of span immediately following another\n    span of the same type.\n\n    In the BIO scheme, I is a token inside a span, O is a token outside\n    a span and B is the beginning of a span.\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The tag sequence encoded in IOB1, e.g. [""I-PER"", ""I-PER"", ""O""].\n    encoding : `str`, optional, (default = `""IOB1""`).\n        The encoding type to convert from. Must be either ""IOB1"" or ""BIO"".\n\n    # Returns\n\n    bioul_sequence : `List[str]`\n        The tag sequence encoded in IOB1, e.g. [""B-PER"", ""L-PER"", ""O""].\n    """"""\n    if encoding not in {""IOB1"", ""BIO""}:\n        raise ConfigurationError(f""Invalid encoding {encoding} passed to \'to_bioul\'."")\n\n    def replace_label(full_label, new_label):\n        # example: full_label = \'I-PER\', new_label = \'U\', returns \'U-PER\'\n        parts = list(full_label.partition(""-""))\n        parts[0] = new_label\n        return """".join(parts)\n\n    def pop_replace_append(in_stack, out_stack, new_label):\n        # pop the last element from in_stack, replace the label, append\n        # to out_stack\n        tag = in_stack.pop()\n        new_tag = replace_label(tag, new_label)\n        out_stack.append(new_tag)\n\n    def process_stack(stack, out_stack):\n        # process a stack of labels, add them to out_stack\n        if len(stack) == 1:\n            # just a U token\n            pop_replace_append(stack, out_stack, ""U"")\n        else:\n            # need to code as BIL\n            recoded_stack = []\n            pop_replace_append(stack, recoded_stack, ""L"")\n            while len(stack) >= 2:\n                pop_replace_append(stack, recoded_stack, ""I"")\n            pop_replace_append(stack, recoded_stack, ""B"")\n            recoded_stack.reverse()\n            out_stack.extend(recoded_stack)\n\n    # Process the tag_sequence one tag at a time, adding spans to a stack,\n    # then recode them.\n    bioul_sequence = []\n    stack: List[str] = []\n\n    for label in tag_sequence:\n        # need to make a dict like\n        # token = {\'token\': \'Matt\', ""labels"": {\'conll2003\': ""B-PER""}\n        #                   \'gold\': \'I-PER\'}\n        # where \'gold\' is the raw value from the CoNLL data set\n\n        if label == ""O"" and len(stack) == 0:\n            bioul_sequence.append(label)\n        elif label == ""O"" and len(stack) > 0:\n            # need to process the entries on the stack plus this one\n            process_stack(stack, bioul_sequence)\n            bioul_sequence.append(label)\n        elif label[0] == ""I"":\n            # check if the previous type is the same as this one\n            # if it is then append to stack\n            # otherwise this start a new entity if the type\n            # is different\n            if len(stack) == 0:\n                if encoding == ""BIO"":\n                    raise InvalidTagSequence(tag_sequence)\n                stack.append(label)\n            else:\n                # check if the previous type is the same as this one\n                this_type = label.partition(""-"")[2]\n                prev_type = stack[-1].partition(""-"")[2]\n                if this_type == prev_type:\n                    stack.append(label)\n                else:\n                    if encoding == ""BIO"":\n                        raise InvalidTagSequence(tag_sequence)\n                    # a new entity\n                    process_stack(stack, bioul_sequence)\n                    stack.append(label)\n        elif label[0] == ""B"":\n            if len(stack) > 0:\n                process_stack(stack, bioul_sequence)\n            stack.append(label)\n        else:\n            raise InvalidTagSequence(tag_sequence)\n\n    # process the stack\n    if len(stack) > 0:\n        process_stack(stack, bioul_sequence)\n\n    return bioul_sequence\n\n\ndef bmes_tags_to_spans(\n    tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[TypedStringSpan]:\n    """"""\n    Given a sequence corresponding to BMES tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e those which do not start with a ""B-LABEL""),\n    as otherwise it is possible to get a perfect precision score whilst still predicting\n    ill-formed spans in addition to the correct spans.\n    This function works properly when the spans are unlabeled (i.e., your labels are\n    simply ""B"", ""M"", ""E"" and ""S"").\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The integer class labels for a sequence.\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    """"""\n\n    def extract_bmes_tag_label(text):\n        bmes_tag = text[0]\n        label = text[2:]\n        return bmes_tag, label\n\n    spans: List[Tuple[str, List[int]]] = []\n    prev_bmes_tag: Optional[str] = None\n    for index, tag in enumerate(tag_sequence):\n        bmes_tag, label = extract_bmes_tag_label(tag)\n        if bmes_tag in (""B"", ""S""):\n            # Regardless of tag, we start a new span when reaching B & S.\n            spans.append((label, [index, index]))\n        elif bmes_tag in (""M"", ""E"") and prev_bmes_tag in (""B"", ""M"") and spans[-1][0] == label:\n            # Only expand the span if\n            # 1. Valid transition: B/M -> M/E.\n            # 2. Matched label.\n            spans[-1][1][1] = index\n        else:\n            # Best effort split for invalid span.\n            spans.append((label, [index, index]))\n        # update previous BMES tag.\n        prev_bmes_tag = bmes_tag\n\n    classes_to_ignore = classes_to_ignore or []\n    return [\n        # to tuple.\n        (span[0], (span[1][0], span[1][1]))\n        for span in spans\n        if span[0] not in classes_to_ignore\n    ]\n'"
tests/data/dataset_readers/dataset_utils/span_utils_test.py,0,"b'from typing import List\nimport pytest\n\nfrom allennlp.common.testing import AllenNlpTestCase\nfrom allennlp.data.dataset_readers.dataset_utils import span_utils\nfrom allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\nfrom allennlp.data.tokenizers.token import Token\n\n\nclass SpanUtilsTest(AllenNlpTestCase):\n    def test_bio_tags_to_spans_extracts_correct_spans(self):\n        tag_sequence = [""O"", ""B-ARG1"", ""I-ARG1"", ""O"", ""B-ARG2"", ""I-ARG2"", ""B-ARG1"", ""B-ARG2""]\n        spans = span_utils.bio_tags_to_spans(tag_sequence)\n        assert set(spans) == {\n            (""ARG1"", (1, 2)),\n            (""ARG2"", (4, 5)),\n            (""ARG1"", (6, 6)),\n            (""ARG2"", (7, 7)),\n        }\n\n        # Check that it raises when we use U- tags for single tokens.\n        tag_sequence = [""O"", ""B-ARG1"", ""I-ARG1"", ""O"", ""B-ARG2"", ""I-ARG2"", ""U-ARG1"", ""U-ARG2""]\n        with pytest.raises(span_utils.InvalidTagSequence):\n            spans = span_utils.bio_tags_to_spans(tag_sequence)\n\n        # Check that invalid BIO sequences are also handled as spans.\n        tag_sequence = [\n            ""O"",\n            ""B-ARG1"",\n            ""I-ARG1"",\n            ""O"",\n            ""I-ARG1"",\n            ""B-ARG2"",\n            ""I-ARG2"",\n            ""B-ARG1"",\n            ""I-ARG2"",\n            ""I-ARG2"",\n        ]\n        spans = span_utils.bio_tags_to_spans(tag_sequence)\n        assert set(spans) == {\n            (""ARG1"", (1, 2)),\n            (""ARG2"", (5, 6)),\n            (""ARG1"", (7, 7)),\n            (""ARG1"", (4, 4)),\n            (""ARG2"", (8, 9)),\n        }\n\n    def test_bio_tags_to_spans_extracts_correct_spans_without_labels(self):\n        tag_sequence = [""O"", ""B"", ""I"", ""O"", ""B"", ""I"", ""B"", ""B""]\n        spans = span_utils.bio_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (1, 2)), ("""", (4, 5)), ("""", (6, 6)), ("""", (7, 7))}\n\n        # Check that it raises when we use U- tags for single tokens.\n        tag_sequence = [""O"", ""B"", ""I"", ""O"", ""B"", ""I"", ""U"", ""U""]\n        with pytest.raises(span_utils.InvalidTagSequence):\n            spans = span_utils.bio_tags_to_spans(tag_sequence)\n\n        # Check that invalid BIO sequences are also handled as spans.\n        tag_sequence = [""O"", ""B"", ""I"", ""O"", ""I"", ""B"", ""I"", ""B"", ""I"", ""I""]\n        spans = span_utils.bio_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (1, 2)), ("""", (4, 4)), ("""", (5, 6)), ("""", (7, 9))}\n\n    def test_bio_tags_to_spans_ignores_specified_tags(self):\n        tag_sequence = [\n            ""B-V"",\n            ""I-V"",\n            ""O"",\n            ""B-ARG1"",\n            ""I-ARG1"",\n            ""O"",\n            ""B-ARG2"",\n            ""I-ARG2"",\n            ""B-ARG1"",\n            ""B-ARG2"",\n        ]\n        spans = span_utils.bio_tags_to_spans(tag_sequence, [""ARG1"", ""V""])\n        assert set(spans) == {(""ARG2"", (6, 7)), (""ARG2"", (9, 9))}\n\n    def test_iob1_tags_to_spans_extracts_correct_spans_without_labels(self):\n        tag_sequence = [""I"", ""B"", ""I"", ""O"", ""B"", ""I"", ""B"", ""B""]\n        spans = span_utils.iob1_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (0, 0)), ("""", (1, 2)), ("""", (4, 5)), ("""", (6, 6)), ("""", (7, 7))}\n\n        # Check that it raises when we use U- tags for single tokens.\n        tag_sequence = [""O"", ""B"", ""I"", ""O"", ""B"", ""I"", ""U"", ""U""]\n        with pytest.raises(span_utils.InvalidTagSequence):\n            spans = span_utils.iob1_tags_to_spans(tag_sequence)\n\n        # Check that invalid IOB1 sequences are also handled as spans.\n        tag_sequence = [""O"", ""B"", ""I"", ""O"", ""I"", ""B"", ""I"", ""B"", ""I"", ""I""]\n        spans = span_utils.iob1_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (1, 2)), ("""", (4, 4)), ("""", (5, 6)), ("""", (7, 9))}\n\n    def test_iob1_tags_to_spans_extracts_correct_spans(self):\n        tag_sequence = [""I-ARG2"", ""B-ARG1"", ""I-ARG1"", ""O"", ""B-ARG2"", ""I-ARG2"", ""B-ARG1"", ""B-ARG2""]\n        spans = span_utils.iob1_tags_to_spans(tag_sequence)\n        assert set(spans) == {\n            (""ARG2"", (0, 0)),\n            (""ARG1"", (1, 2)),\n            (""ARG2"", (4, 5)),\n            (""ARG1"", (6, 6)),\n            (""ARG2"", (7, 7)),\n        }\n\n        # Check that it raises when we use U- tags for single tokens.\n        tag_sequence = [""O"", ""B-ARG1"", ""I-ARG1"", ""O"", ""B-ARG2"", ""I-ARG2"", ""U-ARG1"", ""U-ARG2""]\n        with pytest.raises(span_utils.InvalidTagSequence):\n            spans = span_utils.iob1_tags_to_spans(tag_sequence)\n\n        # Check that invalid IOB1 sequences are also handled as spans.\n        tag_sequence = [\n            ""O"",\n            ""B-ARG1"",\n            ""I-ARG1"",\n            ""O"",\n            ""I-ARG1"",\n            ""B-ARG2"",\n            ""I-ARG2"",\n            ""B-ARG1"",\n            ""I-ARG2"",\n            ""I-ARG2"",\n        ]\n        spans = span_utils.iob1_tags_to_spans(tag_sequence)\n        assert set(spans) == {\n            (""ARG1"", (1, 2)),\n            (""ARG1"", (4, 4)),\n            (""ARG2"", (5, 6)),\n            (""ARG1"", (7, 7)),\n            (""ARG2"", (8, 9)),\n        }\n\n    def test_enumerate_spans_enumerates_all_spans(self):\n        tokenizer = SpacyTokenizer(pos_tags=True)\n        sentence = tokenizer.tokenize(""This is a sentence."")\n\n        spans = span_utils.enumerate_spans(sentence)\n        assert spans == [\n            (0, 0),\n            (0, 1),\n            (0, 2),\n            (0, 3),\n            (0, 4),\n            (1, 1),\n            (1, 2),\n            (1, 3),\n            (1, 4),\n            (2, 2),\n            (2, 3),\n            (2, 4),\n            (3, 3),\n            (3, 4),\n            (4, 4),\n        ]\n\n        spans = span_utils.enumerate_spans(sentence, max_span_width=3, min_span_width=2)\n        assert spans == [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (2, 4), (3, 4)]\n\n        spans = span_utils.enumerate_spans(sentence, max_span_width=3, min_span_width=2, offset=20)\n        assert spans == [(20, 21), (20, 22), (21, 22), (21, 23), (22, 23), (22, 24), (23, 24)]\n\n        def no_prefixed_punctuation(tokens: List[Token]):\n            # Only include spans which don\'t start or end with punctuation.\n            return tokens[0].pos_ != ""PUNCT"" and tokens[-1].pos_ != ""PUNCT""\n\n        spans = span_utils.enumerate_spans(\n            sentence, max_span_width=3, min_span_width=2, filter_function=no_prefixed_punctuation\n        )\n\n        # No longer includes (2, 4) or (3, 4) as these include punctuation\n        # as their last element.\n        assert spans == [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3)]\n\n    def test_bioul_tags_to_spans(self):\n        tag_sequence = [""B-PER"", ""I-PER"", ""L-PER"", ""U-PER"", ""U-LOC"", ""O""]\n        spans = span_utils.bioul_tags_to_spans(tag_sequence)\n        assert spans == [(""PER"", (0, 2)), (""PER"", (3, 3)), (""LOC"", (4, 4))]\n\n        tag_sequence = [""B-PER"", ""I-PER"", ""O""]\n        with pytest.raises(span_utils.InvalidTagSequence):\n            spans = span_utils.bioul_tags_to_spans(tag_sequence)\n\n    def test_bioul_tags_to_spans_without_labels(self):\n        tag_sequence = [""B"", ""I"", ""L"", ""U"", ""U"", ""O""]\n        spans = span_utils.bioul_tags_to_spans(tag_sequence)\n        assert spans == [("""", (0, 2)), ("""", (3, 3)), ("""", (4, 4))]\n\n        tag_sequence = [""B"", ""I"", ""O""]\n        with pytest.raises(span_utils.InvalidTagSequence):\n            spans = span_utils.bioul_tags_to_spans(tag_sequence)\n\n    def test_iob1_to_bioul(self):\n        tag_sequence = [""I-ORG"", ""O"", ""I-MISC"", ""O""]\n        bioul_sequence = span_utils.to_bioul(tag_sequence, encoding=""IOB1"")\n        assert bioul_sequence == [""U-ORG"", ""O"", ""U-MISC"", ""O""]\n\n        tag_sequence = [""O"", ""I-PER"", ""B-PER"", ""I-PER"", ""I-PER"", ""B-PER""]\n        bioul_sequence = span_utils.to_bioul(tag_sequence, encoding=""IOB1"")\n        assert bioul_sequence == [""O"", ""U-PER"", ""B-PER"", ""I-PER"", ""L-PER"", ""U-PER""]\n\n    def test_bio_to_bioul(self):\n        tag_sequence = [""B-ORG"", ""O"", ""B-MISC"", ""O"", ""B-MISC"", ""I-MISC"", ""I-MISC""]\n        bioul_sequence = span_utils.to_bioul(tag_sequence, encoding=""BIO"")\n        assert bioul_sequence == [""U-ORG"", ""O"", ""U-MISC"", ""O"", ""B-MISC"", ""I-MISC"", ""L-MISC""]\n\n        # Encoding in IOB format should throw error with incorrect encoding.\n        with pytest.raises(span_utils.InvalidTagSequence):\n            tag_sequence = [""O"", ""I-PER"", ""B-PER"", ""I-PER"", ""I-PER"", ""B-PER""]\n            bioul_sequence = span_utils.to_bioul(tag_sequence, encoding=""BIO"")\n\n    def test_bmes_tags_to_spans_extracts_correct_spans(self):\n        tag_sequence = [""B-ARG1"", ""M-ARG1"", ""E-ARG1"", ""B-ARG2"", ""E-ARG2"", ""S-ARG3""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {(""ARG1"", (0, 2)), (""ARG2"", (3, 4)), (""ARG3"", (5, 5))}\n\n        tag_sequence = [""S-ARG1"", ""B-ARG2"", ""E-ARG2"", ""S-ARG3""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {(""ARG1"", (0, 0)), (""ARG2"", (1, 2)), (""ARG3"", (3, 3))}\n\n        # Invalid labels.\n        tag_sequence = [""B-ARG1"", ""M-ARG2""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {(""ARG1"", (0, 0)), (""ARG2"", (1, 1))}\n\n        tag_sequence = [""B-ARG1"", ""E-ARG2""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {(""ARG1"", (0, 0)), (""ARG2"", (1, 1))}\n\n        tag_sequence = [""B-ARG1"", ""M-ARG1"", ""M-ARG2""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {(""ARG1"", (0, 1)), (""ARG2"", (2, 2))}\n\n        tag_sequence = [""B-ARG1"", ""M-ARG1"", ""E-ARG2""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {(""ARG1"", (0, 1)), (""ARG2"", (2, 2))}\n\n        # Invalid transitions.\n        tag_sequence = [""B-ARG1"", ""B-ARG1""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {(""ARG1"", (0, 0)), (""ARG1"", (1, 1))}\n\n        tag_sequence = [""B-ARG1"", ""S-ARG1""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {(""ARG1"", (0, 0)), (""ARG1"", (1, 1))}\n\n    def test_bmes_tags_to_spans_extracts_correct_spans_without_labels(self):\n        # Good transitions.\n        tag_sequence = [""B"", ""M"", ""E"", ""B"", ""E"", ""S""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (0, 2)), ("""", (3, 4)), ("""", (5, 5))}\n\n        tag_sequence = [""S"", ""B"", ""E"", ""S""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (0, 0)), ("""", (1, 2)), ("""", (3, 3))}\n\n        # Invalid transitions.\n        tag_sequence = [""B"", ""B"", ""E""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (0, 0)), ("""", (1, 2))}\n\n        tag_sequence = [""B"", ""S""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (0, 0)), ("""", (1, 1))}\n\n        tag_sequence = [""M"", ""B"", ""E""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (0, 0)), ("""", (1, 2))}\n\n        tag_sequence = [""B"", ""M"", ""S""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (0, 1)), ("""", (2, 2))}\n\n        tag_sequence = [""B"", ""E"", ""M"", ""E""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (0, 1)), ("""", (2, 3))}\n\n        tag_sequence = [""B"", ""E"", ""E""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (0, 1)), ("""", (2, 2))}\n\n        tag_sequence = [""S"", ""M""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (0, 0)), ("""", (1, 1))}\n\n        tag_sequence = [""S"", ""E""]\n        spans = span_utils.bmes_tags_to_spans(tag_sequence)\n        assert set(spans) == {("""", (0, 0)), ("""", (1, 1))}\n'"
