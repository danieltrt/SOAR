file_path,api_count,code
main.py,11,"b'""""""\nNeural-Backed Decision Trees training script on CIFAR10, CIFAR100, TinyImagenet200\n\nThe original version of this `main.py` was taken from\n\n    https://github.com/kuangliu/pytorch-cifar\n\nand extended in\n\n    https://github.com/alvinwan/pytorch-cifar-plus\n\nThe script has since been heavily modified to support a number of different\nconfigurations and options. See the current repository for a full description\nof its bells and whistles.\n\n    https://github.com/alvinwan/neural-backed-decision-trees\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nfrom nbdt import data, analysis, loss, models\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport os\nimport argparse\nimport numpy as np\n\nfrom nbdt.utils import (\n    progress_bar, generate_fname, generate_kwargs, Colors, maybe_install_wordnet\n)\n\nmaybe_install_wordnet()\n\ndatasets = (\'CIFAR10\', \'CIFAR100\') + data.imagenet.names + data.custom.names\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch CIFAR Training\')\nparser.add_argument(\'--batch-size\', default=512, type=int,\n                    help=\'Batch size used for training\')\nparser.add_argument(\'--epochs\', \'-e\', default=200, type=int,\n                    help=\'By default, lr schedule is scaled accordingly\')\nparser.add_argument(\'--dataset\', default=\'CIFAR10\', choices=datasets)\nparser.add_argument(\'--arch\', default=\'ResNet18\', choices=list(models.get_model_choices()))\nparser.add_argument(\'--lr\', default=0.1, type=float, help=\'learning rate\')\nparser.add_argument(\'--resume\', \'-r\', action=\'store_true\', help=\'resume from checkpoint\')\n\n# extra general options for main script\nparser.add_argument(\'--path-resume\', default=\'\',\n                    help=\'Overrides checkpoint path generation\')\nparser.add_argument(\'--name\', default=\'\',\n                    help=\'Name of experiment. Used for checkpoint filename\')\nparser.add_argument(\'--pretrained\', action=\'store_true\',\n                    help=\'Download pretrained model. Not all models support this.\')\nparser.add_argument(\'--eval\', help=\'eval only\', action=\'store_true\')\n\n# options specific to this project and its dataloaders\nparser.add_argument(\'--loss\', choices=loss.names, default=\'CrossEntropyLoss\')\nparser.add_argument(\'--analysis\', choices=analysis.names, help=\'Run analysis after each epoch\')\nparser.add_argument(\'--input-size\', type=int,\n                    help=\'Set transform train and val. Samples are resized to \'\n                    \'input-size + 32.\')\nparser.add_argument(\'--lr-decay-every\', type=int, default=0)\n\ndata.custom.add_arguments(parser)\nloss.add_arguments(parser)\nanalysis.add_arguments(parser)\n\nargs = parser.parse_args()\n\nloss.set_default_values(args)\n\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nbest_acc = 0  # best test accuracy\nstart_epoch = 0  # start from epoch 0 or last checkpoint epoch\n\n# Data\nprint(\'==> Preparing data..\')\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ndataset = getattr(data, args.dataset)\n\nif args.dataset in (\'TinyImagenet200\', \'Imagenet1000\'):\n    default_input_size = 64 if args.dataset == \'TinyImagenet200\' else 224\n    input_size = args.input_size or default_input_size\n    transform_train = dataset.transform_train(input_size)\n    transform_test = dataset.transform_val(input_size)\nelif args.input_size is not None and args.input_size > 32:\n    transform_train = transforms.Compose([\n        transforms.Resize(args.input_size + 32),\n        transforms.RandomCrop(args.input_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.Resize(args.input_size + 32),\n        transforms.CenterCrop(args.input_size),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\ndataset_kwargs = generate_kwargs(args, dataset,\n    name=f\'Dataset {args.dataset}\',\n    keys=data.custom.keys,\n    globals=globals())\n\ntrainset = dataset(**dataset_kwargs, root=\'./data\', train=True, download=True, transform=transform_train)\ntestset = dataset(**dataset_kwargs, root=\'./data\', train=False, download=True, transform=transform_test)\n\nassert trainset.classes == testset.classes, (trainset.classes, testset.classes)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=2)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n\nColors.cyan(f\'Training with dataset {args.dataset} and {len(trainset.classes)} classes\')\n\n# Model\nprint(\'==> Building model..\')\nmodel = getattr(models, args.arch)\nmodel_kwargs = {\'num_classes\': len(trainset.classes) }\n\nif args.pretrained:\n    print(\'==> Loading pretrained model..\')\n    try:\n        net = model(pretrained=True, dataset=args.dataset, **model_kwargs)\n    except TypeError as e:  # likely because `dataset` not allowed arg\n        print(e)\n        \n        try:\n            net = model(pretrained=True, **model_kwargs)\n        except Exception as e:\n            Colors.red(f\'Fatal error: {e}\')\n            exit()\nelse:\n    net = model(**model_kwargs)\n\nnet = net.to(device)\nif device == \'cuda\':\n    net = torch.nn.DataParallel(net)\n    cudnn.benchmark = True\n\ncheckpoint_fname = generate_fname(**vars(args))\ncheckpoint_path = \'./checkpoint/{}.pth\'.format(checkpoint_fname)\nprint(f\'==> Checkpoints will be saved to: {checkpoint_path}\')\n\n\n# TODO(alvin): fix checkpoint structure so that this isn\'t neededd\ndef load_state_dict(state_dict):\n    try:\n        net.load_state_dict(state_dict)\n    except RuntimeError as e:\n        if \'Missing key(s) in state_dict:\' in str(e):\n            net.load_state_dict({\n                key.replace(\'module.\', \'\', 1): value\n                for key, value in state_dict.items()\n            })\n\n\nresume_path = args.path_resume or checkpoint_path\nif args.resume:\n    # Load checkpoint.\n    print(\'==> Resuming from checkpoint..\')\n    assert os.path.isdir(\'checkpoint\'), \'Error: no checkpoint directory found!\'\n    if not os.path.exists(resume_path):\n        print(\'==> No checkpoint found. Skipping...\')\n    else:\n        checkpoint = torch.load(resume_path, map_location=torch.device(device))\n\n        if \'net\' in checkpoint:\n            load_state_dict(checkpoint[\'net\'])\n            best_acc = checkpoint[\'acc\']\n            start_epoch = checkpoint[\'epoch\']\n            Colors.cyan(f\'==> Checkpoint found for epoch {start_epoch} with accuracy \'\n                  f\'{best_acc} at {resume_path}\')\n        else:\n            load_state_dict(checkpoint)\n            Colors.cyan(f\'==> Checkpoint found at {resume_path}\')\n\n\ncriterion = nn.CrossEntropyLoss()\nclass_criterion = getattr(loss, args.loss)\nloss_kwargs = generate_kwargs(args, class_criterion,\n    name=f\'Loss {args.loss}\',\n    keys=loss.keys,\n    globals=globals())\ncriterion = class_criterion(**loss_kwargs)\n\noptimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n\ndef adjust_learning_rate(epoch, lr):\n    if args.lr_decay_every:\n        steps = epoch // args.lr_decay_every\n        return lr / (10 ** steps)\n    if epoch <= 150 / 350. * args.epochs:  # 32k iterations\n        return lr\n    elif epoch <= 250 / 350. * args.epochs:  # 48k iterations\n        return lr/10\n    else:\n        return lr/100\n\n# Training\ndef train(epoch, analyzer):\n    analyzer.start_train(epoch)\n    lr = adjust_learning_rate(epoch, args.lr)\n    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n\n    print(\'\\nEpoch: %d\' % epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        stat = analyzer.update_batch(outputs, targets)\n        extra = f\'| {stat}\' if stat else \'\'\n\n        progress_bar(batch_idx, len(trainloader), \'Loss: %.3f | Acc: %.3f%% (%d/%d) %s\'\n            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total, extra))\n\n    analyzer.end_train(epoch)\n\ndef test(epoch, analyzer, checkpoint=True):\n    analyzer.start_test(epoch)\n\n    global best_acc\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            if device == \'cuda\':\n                predicted = predicted.cpu()\n                targets = targets.cpu()\n\n            stat = analyzer.update_batch(outputs, targets)\n            extra = f\'| {stat}\' if stat else \'\'\n\n            progress_bar(batch_idx, len(testloader), \'Loss: %.3f | Acc: %.3f%% (%d/%d) %s\'\n                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total, extra))\n\n    # Save checkpoint.\n    acc = 100.*correct/total\n    print(""Accuracy: {}, {}/{}"".format(acc, correct, total))\n    if acc > best_acc and checkpoint:\n        state = {\n            \'net\': net.state_dict(),\n            \'acc\': acc,\n            \'epoch\': epoch,\n        }\n        if not os.path.isdir(\'checkpoint\'):\n            os.mkdir(\'checkpoint\')\n\n        print(f\'Saving to {checkpoint_fname} ({acc})..\')\n        torch.save(state, f\'./checkpoint/{checkpoint_fname}.pth\')\n        best_acc = acc\n\n    analyzer.end_test(epoch)\n\n\nclass_analysis = getattr(analysis, args.analysis or \'Noop\')\nanalyzer_kwargs = generate_kwargs(args, class_analysis,\n    name=f\'Analyzer {args.analysis}\',\n    keys=analysis.keys,\n    globals=globals())\nanalyzer = class_analysis(**analyzer_kwargs)\n\n\nif args.eval:\n    if not args.resume and not args.pretrained:\n        Colors.red(\' * Warning: Model is not loaded from checkpoint. \'\n        \'Use --resume or --pretrained (if supported)\')\n\n    analyzer.start_epoch(0)\n    test(0, analyzer, checkpoint=False)\n    exit()\n\nfor epoch in range(start_epoch, args.epochs):\n    analyzer.start_epoch(epoch)\n    train(epoch, analyzer)\n    test(epoch, analyzer)\n    analyzer.end_epoch(epoch)\n\nif args.epochs == 0:\n    analyzer.start_epoch(0)\n    test(0, analyzer)\n    analyzer.end_epoch(0)\nprint(f\'Best accuracy: {best_acc} // Checkpoint name: {checkpoint_fname}\')\n'"
setup.py,0,"b'import setuptools\n\nVERSION = \'0.0.4\'\n\nwith open(""requirements.txt"", ""r"") as f:\n    install_requires = f.readlines()\n\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\n\nsetuptools.setup(\n    name=""nbdt"",\n    version=VERSION,\n    author=""Alvin Wan"",  # TODO: proper way to list all paper authors?\n    author_email=""hi@alvinwan.com"",\n    description=""Making decision trees competitive with state-of-the-art ""\n                ""neural networks on CIFAR10, CIFAR100, TinyImagenet200, ""\n                ""Imagenet. Transform any image classification neural network ""\n                ""into an interpretable neural-backed decision tree."",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/alvinwan/neural-backed-decision-trees"",\n    packages=setuptools.find_packages(),\n    install_requires=install_requires,\n    download_url=\'https://github.com/alvinwan/neural-backed-decision-trees/archive/%s.zip\' % VERSION,\n    scripts=[\'nbdt/bin/nbdt-hierarchy\', \'nbdt/bin/nbdt-wnids\', \'nbdt/bin/nbdt\'],\n    classifiers=[\n        ""Intended Audience :: Developers"",\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n    python_requires=\'>=3.5\',\n    include_package_data=True\n)\n'"
nbdt/__init__.py,0,b''
nbdt/analysis.py,0,"b'from nbdt.utils import set_np_printoptions\nfrom nbdt.model import (\n    SoftEmbeddedDecisionRules as SoftRules,\n    HardEmbeddedDecisionRules as HardRules\n)\nimport numpy as np\n\n\n__all__ = names = (\n    \'Noop\', \'ConfusionMatrix\', \'ConfusionMatrixJointNodes\',\n    \'IgnoredSamples\', \'HardEmbeddedDecisionRules\', \'SoftEmbeddedDecisionRules\')\nkeys = (\'path_graph\', \'path_wnids\', \'classes\', \'dataset\')\n\n\ndef add_arguments(parser):\n    pass\n\n\nclass Noop:\n\n    accepts_classes = lambda trainset, **kwargs: trainset.classes\n\n    def __init__(self, classes=()):\n        set_np_printoptions()\n\n        self.classes = classes\n        self.num_classes = len(classes)\n        self.epoch = None\n\n    def start_epoch(self, epoch):\n        self.epoch = epoch\n\n    def start_train(self, epoch):\n        assert epoch == self.epoch\n\n    def update_batch(self, outputs, targets):\n        pass\n\n    def end_train(self, epoch):\n        assert epoch == self.epoch\n\n    def start_test(self, epoch):\n        assert epoch == self.epoch\n\n    def end_test(self, epoch):\n        assert epoch == self.epoch\n\n    def end_epoch(self, epoch):\n        assert epoch == self.epoch\n\n\nclass ConfusionMatrix(Noop):\n\n    def __init__(self, classes):\n        super().__init__(classes)\n        self.k = len(classes)\n        self.m = None\n\n    def start_train(self, epoch):\n        super().start_train(epoch)\n        raise NotImplementedError()\n\n    def start_test(self, epoch):\n        super().start_test(epoch)\n        self.m = np.zeros((self.k, self.k))\n\n    def update_batch(self, outputs, targets):\n        super().update_batch(outputs, targets)\n        _, predicted = outputs.max(1)\n        if len(predicted.shape) == 1:\n            predicted = predicted.numpy().ravel()\n            targets = targets.numpy().ravel()\n            ConfusionMatrix.update(self.m, predicted, targets)\n\n    def end_test(self, epoch):\n        super().end_test(epoch)\n        recall = self.recall()\n        for row, cls in zip(recall, self.classes):\n            print(row, cls)\n        print(recall.diagonal(), \'(diagonal)\')\n\n    @staticmethod\n    def update(confusion_matrix, preds, labels):\n        preds = tuple(preds)\n        labels = tuple(labels)\n\n        for pred, label in zip(preds, labels):\n            confusion_matrix[label, pred] += 1\n\n    @staticmethod\n    def normalize(confusion_matrix, axis):\n        total = confusion_matrix.astype(np.float).sum(axis=axis)\n        total = total[:, None] if axis == 1 else total[None]\n        return confusion_matrix / total\n\n    def recall(self):\n        return ConfusionMatrix.normalize(self.m, 1)\n\n    def precision(self):\n        return ConfusionMatrix.normalize(self.m, 0)\n\n\nclass IgnoredSamples(Noop):\n    """""" Counter for number of ignored samples in decision tree """"""\n\n    def __init__(self, classes=()):\n        super().__init__(classes)\n        self.ignored = None\n\n    def start_test(self, epoch):\n        super().start_test(epoch)\n        self.ignored = 0\n\n    def update_batch(self, outputs, targets):\n        super().update_batch(outputs, targets)\n        self.ignored += outputs[:,0].eq(-1).sum().item()\n\n    def end_test(self, epoch):\n        super().end_test(epoch)\n        print(""Ignored Samples: {}"".format(self.ignored))\n\n\nclass HardEmbeddedDecisionRules(Noop):\n    """"""Evaluation is hard.""""""\n\n    accepts_dataset = lambda trainset, **kwargs: trainset.__class__.__name__\n    accepts_path_graph = True\n    accepts_path_wnids = True\n\n    name = \'NBDT-Hard\'\n\n    def __init__(self, *args, Rules=HardRules, **kwargs):\n        self.rules = Rules(*args, **kwargs)\n\n    def update_batch(self, outputs, targets):\n        super().update_batch(outputs, targets)\n        predicted = self.rules.forward(outputs).max(1)[1].to(targets.device)\n\n        n_samples = outputs.size(0)\n        self.total += n_samples\n        self.correct += (predicted == targets).sum().item()\n        accuracy = round(self.correct / float(self.total), 4) * 100\n        return f\'{self.name}: {accuracy}%\'\n\n    def end_test(self, epoch):\n        super().end_test(epoch)\n        accuracy = round(self.correct / self.total * 100., 2)\n        print(f\'{self.name} Accuracy: {accuracy}%, {self.correct}/{self.total}\')\n\n\nclass SoftEmbeddedDecisionRules(HardEmbeddedDecisionRules):\n    """"""Evaluation is soft.""""""\n\n    name = \'NBDT-Soft\'\n\n    def __init__(self, *args, Rules=None, **kwargs):\n        super().__init__(*args, Rules=SoftRules, **kwargs)\n'"
nbdt/graph.py,1,"b'import networkx as nx\nimport json\nimport random\nfrom nbdt.utils import DATASETS, METHODS, fwd\nfrom networkx.readwrite.json_graph import node_link_data, node_link_graph\nfrom sklearn.cluster import AgglomerativeClustering\nfrom pathlib import Path\nimport nbdt.models as models\nimport torch\nimport argparse\nimport os\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--dataset\',\n        help=\'Must be a folder nbdt/wnids/{dataset}.txt containing wnids\',\n        choices=DATASETS,\n        default=\'CIFAR10\')\n    parser.add_argument(\n        \'--extra\',\n        type=int,\n        default=0,\n        help=\'Percent extra nodes to add to the tree. If 100, the number of \'\n        \'nodes in tree are doubled. Note this is an integral percent.\')\n    parser.add_argument(\n        \'--multi-path\',\n        action=\'store_true\',\n        help=\'Allows each leaf multiple paths to the root.\')\n    parser.add_argument(\'--no-prune\', action=\'store_true\', help=\'Do not prune.\')\n    parser.add_argument(\'--fname\', type=str,\n        help=\'Override all settings and just provide a path to a graph\')\n    parser.add_argument(\'--method\', choices=METHODS,\n        help=\'structure_released.xml apparently is missing many CIFAR100 classes. \'\n        \'As a result, pruning does not work for CIFAR100. Random will randomly \'\n        \'join clusters together, iteratively, to make a roughly-binary tree.\',\n        default=\'induced\')\n    parser.add_argument(\'--seed\', type=int, default=0)\n    parser.add_argument(\'--branching-factor\', type=int, default=2)\n    parser.add_argument(\'--checkpoint\', type=str,\n        help=\'(induced hierarchy) Checkpoint to load into model. The fc weights\'\n        \' are used for clustering.\')\n    parser.add_argument(\'--arch\', type=str, default=\'ResNet18\',\n        help=\'(induced hierarchy) Model name to get pretrained fc weights for.\',\n        choices=list(models.get_model_choices()))\n    parser.add_argument(\'--induced-linkage\', type=str, default=\'ward\',\n        help=\'(induced hierarchy) Linkage type used for agglomerative clustering\')\n    parser.add_argument(\'--induced-affinity\', type=str, default=\'euclidean\',\n        help=\'(induced hierarchy) Metric used for computing similarity\')\n    parser.add_argument(\'--vis-zoom\', type=float, default=1.0)\n    parser.add_argument(\'--vis-curved\', action=\'store_true\',\n        help=\'Use curved lines for edges\')\n    parser.add_argument(\'--vis-sublabels\', action=\'store_true\',\n        help=\'Show sublabels\')\n    parser.add_argument(\'--color\', choices=(\'blue\', \'blue-green\'), default=\'blue\',\n        help=\'Color to use, for colored flags. Note this takes NO effect if \'\n        \'nodes are not colored.\')\n    parser.add_argument(\'--vis-no-color-leaves\', action=\'store_true\',\n        help=\'Do NOT highlight leaves with special color.\')\n    parser.add_argument(\'--vis-color-path-to\', type=str,\n        help=\'Vis all nodes on path from leaf to root, as blue. Pass leaf name.\')\n    parser.add_argument(\'--vis-color-nodes\', nargs=\'*\',\n        help=\'Nodes to color. Nodes are identified by label\')\n    parser.add_argument(\'--vis-force-labels-left\', nargs=\'*\',\n        help=\'Labels to force text left of the node.\')\n    parser.add_argument(\'--vis-leaf-images\', action=\'store_true\',\n        help=\'Include sample images for each leaf/class.\')\n    parser.add_argument(\'--vis-image-resize-factor\', type=float, default=1.,\n        help=\'Factor to resize image size by. Default image size is provided \'\n             \'by the original image. e.g., 32 for CIFAR10, 224 for Imagenet\')\n    parser.add_argument(\'--vis-height\', type=int, default=750,\n        help=\'Height of the outputted visualization\')\n    parser.add_argument(\'--vis-dark\', action=\'store_true\', help=\'Dark mode\')\n    return parser\n\n\ndef generate_fname(method, seed=0, branching_factor=2, extra=0,\n                   no_prune=False, fname=\'\', multi_path=False,\n                   induced_linkage=\'ward\', induced_affinity=\'euclidean\',\n                   checkpoint=None, arch=None, **kwargs):\n    if fname:\n        return fname\n\n    fname = f\'graph-{method}\'\n    if method == \'random\':\n        if seed != 0:\n            fname += f\'-seed{seed}\'\n    if method == \'induced\':\n        assert checkpoint or arch, \\\n            \'Induced hierarchy needs either `arch` or `checkpoint`\'\n        if induced_linkage != \'ward\' and induced_linkage is not None:\n            fname += f\'-linkage{induced_linkage}\'\n        if induced_affinity != \'euclidean\' and induced_affinity is not None:\n            fname += f\'-affinity{induced_affinity}\'\n        if checkpoint:\n            checkpoint_stem = Path(checkpoint).stem\n            if checkpoint_stem.startswith(\'ckpt-\') and checkpoint_stem.count(\'-\') >= 2:\n                checkpoint_suffix = \'-\'.join(checkpoint_stem.split(\'-\')[2:])\n                checkpoint_fname = checkpoint_suffix.replace(\'-induced\', \'\')\n            else:\n                checkpoint_fname = checkpoint_stem\n        else:\n            checkpoint_fname = arch\n        fname += f\'-{checkpoint_fname}\'\n    if method in (\'random\', \'induced\'):\n        if branching_factor != 2:\n            fname += f\'-branch{branching_factor}\'\n    if extra > 0:\n        fname += f\'-extra{extra}\'\n    if no_prune:\n        fname += \'-noprune\'\n    if multi_path:\n        fname += \'-multi\'\n    return fname\n\n\ndef get_directory(dataset, root=\'./nbdt/hierarchies\'):\n    return os.path.join(root, dataset)\n\n\ndef get_wnids_from_dataset(dataset, root=\'./nbdt/wnids\'):\n    directory = get_directory(dataset, root)\n    return get_wnids(f\'{directory}.txt\')\n\n\ndef get_wnids(path_wnids):\n    if not os.path.exists(path_wnids):\n        parent = Path(fwd()).parent\n        print(f\'No such file or directory: {path_wnids}. Looking in {str(parent)}\')\n        path_wnids = parent / path_wnids\n    with open(path_wnids) as f:\n        wnids = [wnid.strip() for wnid in f.readlines()]\n    return wnids\n\n\ndef get_graph_path_from_args(\n        dataset, method, seed=0, branching_factor=2, extra=0,\n        no_prune=False, fname=\'\', multi_path=False,\n        induced_linkage=\'ward\', induced_affinity=\'euclidean\',\n        checkpoint=None, arch=None, **kwargs):\n    fname = generate_fname(\n        method=method,\n        seed=seed,\n        branching_factor=branching_factor,\n        extra=extra,\n        no_prune=no_prune,\n        fname=fname,\n        multi_path=multi_path,\n        induced_linkage=induced_linkage,\n        induced_affinity=induced_affinity,\n        checkpoint=checkpoint,\n        arch=arch)\n    directory = get_directory(dataset)\n    path = os.path.join(directory, f\'{fname}.json\')\n    return path\n\n\n##########\n# SYNSET #\n##########\n\n\ndef synset_to_wnid(synset):\n    return f\'{synset.pos()}{synset.offset():08d}\'\n\n\ndef wnid_to_synset(wnid):\n    from nltk.corpus import wordnet as wn  # entire script should not depend on wn\n\n    offset = int(wnid[1:])\n    pos = wnid[0]\n\n    try:\n        return wn.synset_from_pos_and_offset(wnid[0], offset)\n    except:\n        return FakeSynset(wnid)\n\n\ndef wnid_to_name(wnid):\n    return synset_to_name(wnid_to_synset(wnid))\n\n\ndef synset_to_name(synset):\n    return synset.name().split(\'.\')[0]\n\n\n########\n# TREE #\n########\n\n\ndef is_leaf(G, node):\n    return len(G.succ[node]) == 0\n\n\ndef get_leaves(G, root=None):\n    nodes = G.nodes if root is None else nx.descendants(G, root) | {root}\n    for node in nodes:\n        if is_leaf(G, node):\n            yield node\n\n\ndef get_non_leaves(G):\n    for node in G.nodes:\n        if len(G.succ[node]) > 0:\n            yield node\n\n\ndef get_roots(G):\n    for node in G.nodes:\n        if len(G.pred[node]) == 0:\n            yield node\n\n\ndef get_root(G):\n    roots = list(get_roots(G))\n    assert len(roots) == 1, f\'Multiple ({len(roots)}) found\'\n    return roots[0]\n\n\ndef get_depth(G):\n    def _get_depth(node):\n        if not G.succ[node]:\n            return 1\n        return max([_get_depth(child) for child in G.succ[node]]) + 1\n    return max([_get_depth(root) for root in get_roots(G)])\n\n\ndef get_leaf_to_path(G):\n    leaf_to_path = {}\n    for root in get_roots(G):\n        frontier = [(root, [])]\n        while frontier:\n            node, path = frontier.pop(0)\n            path = path + [node]\n            if is_leaf(G, node):\n                leaf_to_path[node] = path\n                continue\n            frontier.extend([(child, path) for child in G.succ[node]])\n    return leaf_to_path\n\n\ndef set_node_label(G, synset):\n    nx.set_node_attributes(G, {\n        synset_to_wnid(synset): synset_to_name(synset)\n    }, \'label\')\n\n\ndef set_random_node_label(G, i):\n    nx.set_node_attributes(G, {i: \'\'}, \'label\')\n\n\n##########\n# GRAPHS #\n##########\n\n\ndef build_minimal_wordnet_graph(wnids, multi_path=False):\n    G = nx.DiGraph()\n\n    for wnid in wnids:\n        G.add_node(wnid)\n        synset = wnid_to_synset(wnid)\n        set_node_label(G, synset)\n\n        if wnid == \'n10129825\':  # hardcode \'girl\' to not be child of \'woman\'\n            if not multi_path:\n                G.add_edge(\'n09624168\', \'n10129825\')  # child of \'male\' (sibling to \'male_child\')\n            else:\n                G.add_edge(\'n09619168\', \'n10129825\')  # child of \'female\'\n            G.add_edge(\'n09619168\', \'n10129825\')  # child of \'female\'\n            continue\n\n        hypernyms = [synset]\n        while hypernyms:\n            current = hypernyms.pop(0)\n            set_node_label(G, current)\n            for hypernym in current.hypernyms():\n                G.add_edge(synset_to_wnid(hypernym), synset_to_wnid(current))\n                hypernyms.append(hypernym)\n\n                if not multi_path:\n                    break\n\n        children = [(key, wnid_to_synset(key).name()) for key in G.succ[wnid]]\n        assert len(children) == 0, \\\n            f\'Node {wnid} ({synset.name()}) is not a leaf. Children: {children}\'\n    return G\n\n\ndef build_random_graph(wnids, seed=0, branching_factor=2):\n    random.seed(seed)\n\n    G = nx.DiGraph()\n\n    if seed >= 0:\n        random.shuffle(wnids)\n    current = None\n    remaining = wnids\n\n    # Build the graph from the leaves up\n    while len(remaining) > 1:\n        current, remaining = remaining, []\n        while current:\n            nodes, current = current[:branching_factor], current[branching_factor:]\n            remaining.append(nodes)\n\n    # Construct networkx graph from root down\n    G.add_node(\'0\')\n    set_random_node_label(G, \'0\')\n    next = [(remaining[0], \'0\')]\n    i = 1\n    while next:\n        group, parent = next.pop(0)\n        if len(group) == 1:\n            if isinstance(group[0], str):\n                G.add_node(group[0])\n                synset = wnid_to_synset(group[0])\n                set_node_label(G, synset)\n                G.add_edge(parent, group[0])\n            else:\n                next.append((group[0], parent))\n            continue\n\n        for candidate in group:\n            is_leaf = not isinstance(candidate, list)\n            wnid = candidate if is_leaf else str(i)\n            G.add_node(wnid)\n            if is_leaf:\n                synset = wnid_to_synset(wnid)\n                set_node_label(G, synset)\n            else:\n                set_random_node_label(G, wnid)\n            G.add_edge(parent, wnid)\n            i += 1\n\n            if not is_leaf:\n                next.append((candidate, wnid))\n    return G\n\n\ndef prune_single_successor_nodes(G):\n    for node in G.nodes:\n        if len(G.succ[node]) == 1:\n            succ = list(G.succ[node])[0]\n            G = nx.contracted_nodes(G, succ, node, self_loops=False)\n    return G\n\n\ndef makeparentdirs(path):\n    dir = Path(path).parent\n    os.makedirs(dir, exist_ok=True)\n\n\ndef write_wnids(wnids, path):\n    makeparentdirs(path)\n    with open(str(path), \'w\') as f:\n        f.write(\'\\n\'.join(wnids))\n\n\ndef write_graph(G, path):\n    makeparentdirs(path)\n    with open(str(path), \'w\') as f:\n        json.dump(node_link_data(G), f)\n\n\ndef read_graph(path):\n    if not os.path.exists(path):\n        parent = Path(fwd()).parent\n        print(f\'No such file or directory: {path}. Looking in {str(parent)}\')\n        path = parent / path\n    with open(path) as f:\n        return node_link_graph(json.load(f))\n\n\n################\n# INDUCED TREE #\n################\n\n\nMODEL_FC_KEYS = (\n    \'fc.weight\', \'linear.weight\', \'module.linear.weight\',\n    \'module.net.linear.weight\', \'output.weight\', \'module.output.weight\',\n    \'output.fc.weight\', \'module.output.fc.weight\', \'classifier.weight\')\n\n\ndef build_induced_graph(wnids, checkpoint, model=None, linkage=\'ward\',\n        affinity=\'euclidean\', branching_factor=2, dataset=\'CIFAR10\',\n        state_dict=None):\n    num_classes = len(wnids)\n    assert checkpoint or model or state_dict, \\\n        \'Need to specify either `checkpoint` or `method` or `state_dict`.\'\n    if state_dict:\n        centers = get_centers_from_state_dict(state_dict)\n    elif checkpoint:\n        centers = get_centers_from_checkpoint(checkpoint)\n    else:\n        centers = get_centers_from_model(model, num_classes, dataset)\n    assert num_classes == centers.size(0), (\n        f\'The model FC supports {centers.size(0)} classes. However, the dataset\'\n        f\' {dataset} features {num_classes} classes. Try passing the \'\n        \'`--dataset` with the right number of classes.\'\n    )\n\n    G = nx.DiGraph()\n\n    # add leaves\n    for wnid in wnids:\n        G.add_node(wnid)\n        set_node_label(G, wnid_to_synset(wnid))\n\n    # add rest of tree\n    clustering = AgglomerativeClustering(\n        linkage=linkage,\n        n_clusters=branching_factor,\n        affinity=affinity,\n    ).fit(centers)\n    children = clustering.children_\n    index_to_wnid = {}\n\n    for index, pair in enumerate(map(tuple, children)):\n        child_wnids = []\n        child_synsets = []\n        for child in pair:\n            if child < num_classes:\n                child_wnid = wnids[child]\n            else:\n                child_wnid = index_to_wnid[child - num_classes]\n            child_wnids.append(child_wnid)\n            child_synsets.append(wnid_to_synset(child_wnid))\n\n        parent = get_wordnet_meaning(G, child_synsets)\n        parent_wnid = synset_to_wnid(parent)\n        G.add_node(parent_wnid)\n        set_node_label(G, parent)\n        index_to_wnid[index] = parent_wnid\n\n        for child_wnid in child_wnids:\n            G.add_edge(parent_wnid, child_wnid)\n\n    assert len(list(get_roots(G))) == 1, list(get_roots(G))\n    return G\n\n\ndef get_centers_from_checkpoint(checkpoint):\n    data = torch.load(checkpoint, map_location=torch.device(\'cpu\'))\n\n    for key in (\'net\', \'state_dict\'):\n        try:\n            state_dict = data[key]\n            break\n        except:\n            state_dict = data\n\n    fc = get_centers_from_state_dict(state_dict)\n    assert fc is not None, (\n        f\'Could not find FC weights in checkpoint {checkpoint} with keys: {net.keys()}\')\n    return fc\n\n\ndef get_centers_from_model(model, num_classes, dataset):\n    net = None\n    try:\n        net = getattr(models, model)(\n            pretrained=True,\n            num_classes=num_classes,\n            dataset=dataset)\n    except TypeError as e:\n        print(f\'Ignoring TypeError. Retrying without `dataset` kwarg: {e}\')\n        try:\n            net = getattr(models, model)(\n                pretrained=True,\n                num_classes=num_classes)\n        except TypeError as e:\n            print(e)\n    assert net is not None, f\'Could not find pretrained model {model}\'\n    fc = get_centers_from_state_dict(net.state_dict())\n    assert fc is not None, (\n        f\'Could not find FC weights in model {model} with keys: {net.keys()}\')\n    return fc\n\n\ndef get_centers_from_state_dict(state_dict):\n    fc = None\n    for key in MODEL_FC_KEYS:\n        if key in state_dict:\n            fc = state_dict[key]\n            break\n    if fc is not None:\n        return fc.detach()\n\n\n####################\n# AUGMENTING GRAPH #\n####################\n\n\nclass FakeSynset:\n\n    def __init__(self, wnid):\n        self.wnid = wnid\n\n        assert isinstance(wnid, str)\n\n    @staticmethod\n    def create_from_offset(offset):\n        return FakeSynset(\'f{:08d}\'.format(offset))\n\n    def offset(self):\n        return int(self.wnid[1:])\n\n    def pos(self):\n        return \'f\'\n\n    def name(self):\n        return \'(generated)\'\n\n    def definition(self):\n        return \'(generated)\'\n\n\ndef augment_graph(G, extra, allow_imaginary=False, seed=0, max_retries=10000):\n    """"""Augment graph G with extra% more nodes.\n\n    e.g., If G has 100 nodes and extra = 0.5, the final graph will have 150\n    nodes.\n    """"""\n    n = len(G.nodes)\n    n_extra = int(extra / 100. * n)\n    random.seed(seed)\n\n    n_imaginary = 0\n    for i in range(n_extra):\n        candidate, is_imaginary_synset, children = get_new_node(G)\n        if not is_imaginary_synset or \\\n                (is_imaginary_synset and allow_imaginary):\n            add_node_to_graph(G, candidate, children)\n            n_imaginary += is_imaginary_synset\n            continue\n\n        # now, must be imaginary synset AND not allowed\n        if n_imaginary > 0:  # hit max retries before, not likely to find real\n            return G, i, n_imaginary\n\n        retries, is_imaginary_synset = 0, True\n        while is_imaginary_synset:\n            candidate, is_imaginary_synset, children = get_new_node(G)\n            if retries > max_retries:\n                print(f\'Exceeded max retries ({max_retries})\')\n                return G, i, n_imaginary\n        add_node_to_graph(G, candidate, children)\n\n    return G, n_extra, n_imaginary\n\n\ndef get_new_node(G):\n    """"""Get new candidate node for the graph""""""\n    root = get_root(G)\n    nodes = list(filter(lambda node: node is not root and not node.startswith(\'f\'), G.nodes))\n\n    children = get_new_adjacency(G, nodes)\n    synsets = [wnid_to_synset(wnid) for wnid in children]\n\n    candidate = get_wordnet_meaning(G, synsets)\n    is_fake = candidate.pos() == \'f\'\n    return candidate, is_fake, children\n\n\ndef get_wordnet_meaning(G, synsets):\n    hypernyms = get_common_hypernyms(synsets)\n    candidate = pick_unseen_hypernym(G, hypernyms) if hypernyms else None\n    if candidate is None:\n        return FakeSynset.create_from_offset(len(G.nodes))\n    return candidate\n\n\ndef add_node_to_graph(G, candidate, children):\n    root = get_root(G)\n\n    wnid = synset_to_wnid(candidate)\n    G.add_node(wnid)\n    set_node_label(G, candidate)\n\n    for child in children:\n        G.add_edge(wnid, child)\n    G.add_edge(root, wnid)\n\n\ndef get_new_adjacency(G, nodes):\n    adjacency = set(tuple(adj) for adj in G.adj.values())\n    children = next(iter(adjacency))\n\n    while children in adjacency:\n        k = random.randint(2, 4)\n        children = tuple(random.sample(nodes, k=k))\n    return children\n\n\ndef get_common_hypernyms(synsets):\n    if any(synset.pos() == \'f\' for synset in synsets):\n        return set()\n    common_hypernyms = set(synsets[0].common_hypernyms(synsets[1]))\n    for synset in synsets[2:]:\n        common_hypernyms &= set(synsets[0].common_hypernyms(synset))\n    return common_hypernyms\n\n\ndef deepest_synset(synsets):\n    return max(synsets, key=lambda synset: synset.max_depth())\n\n\ndef pick_unseen_hypernym(G, common_hypernyms):\n    assert len(common_hypernyms) > 0\n\n    candidate = deepest_synset(common_hypernyms)\n    wnid = synset_to_wnid(candidate)\n\n    while common_hypernyms and wnid in G.nodes:\n        common_hypernyms -= {candidate}\n        if not common_hypernyms:\n            return None\n\n        candidate = deepest_synset(common_hypernyms)\n        wnid = synset_to_wnid(candidate)\n    return candidate\n'"
nbdt/hierarchy.py,0,"b'from nbdt.utils import DATASETS, METHODS, Colors, fwd\nfrom nbdt.graph import build_minimal_wordnet_graph, build_random_graph, \\\n    prune_single_successor_nodes, write_graph, get_wnids, generate_fname, \\\n    get_parser, get_wnids_from_dataset, get_directory, get_graph_path_from_args, \\\n    augment_graph, get_depth, build_induced_graph, read_graph, get_leaves, \\\n    get_roots, synset_to_wnid, wnid_to_name, get_root\nfrom nbdt import data\nfrom networkx.readwrite.json_graph import adjacency_data\nfrom pathlib import Path\nimport os\nimport json\nimport torchvision\nimport base64\nfrom io import BytesIO\n\n\n############\n# GENERATE #\n############\n\n\ndef print_graph_stats(G, name):\n    num_children = [len(succ) for succ in G.succ]\n    print(\'[{}] \\t Nodes: {} \\t Depth: {} \\t Max Children: {}\'.format(\n        name,\n        len(G.nodes),\n        get_depth(G),\n        max(num_children)))\n\n\ndef assert_all_wnids_in_graph(G, wnids):\n    assert all(wnid.strip() in G.nodes for wnid in wnids), [\n        wnid for wnid in wnids if wnid not in G.nodes\n    ]\n\n\ndef generate_hierarchy(\n        dataset, method, seed=0, branching_factor=2, extra=0,\n        no_prune=False, fname=\'\', single_path=False,\n        induced_linkage=\'ward\', induced_affinity=\'euclidean\',\n        checkpoint=None, arch=None, model=None, **kwargs):\n    wnids = get_wnids_from_dataset(dataset)\n\n    if method == \'wordnet\':\n        G = build_minimal_wordnet_graph(wnids, single_path)\n    elif method == \'random\':\n        G = build_random_graph(wnids, seed=seed, branching_factor=branching_factor)\n    elif method == \'induced\':\n        G = build_induced_graph(wnids,\n            dataset=dataset,\n            checkpoint=checkpoint,\n            model=arch,\n            linkage=induced_linkage,\n            affinity=induced_affinity,\n            branching_factor=branching_factor,\n            state_dict=model.state_dict() if model is not None else None)\n    else:\n        raise NotImplementedError(f\'Method ""{method}"" not yet handled.\')\n    print_graph_stats(G, \'matched\')\n    assert_all_wnids_in_graph(G, wnids)\n\n    if not no_prune:\n        G = prune_single_successor_nodes(G)\n        print_graph_stats(G, \'pruned\')\n        assert_all_wnids_in_graph(G, wnids)\n\n    if extra > 0:\n        G, n_extra, n_imaginary = augment_graph(G, extra, True)\n        print(f\'[extra] \\t Extras: {n_extra} \\t Imaginary: {n_imaginary}\')\n        print_graph_stats(G, \'extra\')\n        assert_all_wnids_in_graph(G, wnids)\n\n    path = get_graph_path_from_args(\n        dataset=dataset,\n        method=method,\n        seed=seed,\n        branching_factor=branching_factor,\n        extra=extra,\n        no_prune=no_prune,\n        fname=fname,\n        single_path=single_path,\n        induced_linkage=induced_linkage,\n        induced_affinity=induced_affinity,\n        checkpoint=checkpoint,\n        arch=arch)\n    write_graph(G, path)\n\n    Colors.green(\'==> Wrote tree to {}\'.format(path))\n\n\n########\n# TEST #\n########\n\n\ndef get_seen_wnids(wnid_set, nodes):\n    leaves_seen = set()\n    for leaf in nodes:\n        if leaf in wnid_set:\n            wnid_set.remove(leaf)\n        if leaf in leaves_seen:\n            pass\n        leaves_seen.add(leaf)\n    return leaves_seen\n\n\ndef match_wnid_leaves(wnids, G, tree_name):\n    wnid_set = set()\n    for wnid in wnids:\n        wnid_set.add(wnid.strip())\n\n    leaves_seen = get_seen_wnids(wnid_set, get_leaves(G))\n    return leaves_seen, wnid_set\n\n\ndef match_wnid_nodes(wnids, G, tree_name):\n    wnid_set = {wnid.strip() for wnid in wnids}\n    leaves_seen = get_seen_wnids(wnid_set, G.nodes)\n\n    return leaves_seen, wnid_set\n\n\ndef print_stats(leaves_seen, wnid_set, tree_name, node_type):\n    print(f""[{tree_name}] \\t {node_type}: {len(leaves_seen)} \\t WNIDs missing from {node_type}: {len(wnid_set)}"")\n    if len(wnid_set):\n        Colors.red(f""==> Warning: WNIDs in wnid.txt are missing from {tree_name} {node_type}"")\n\n\ndef test_hierarchy(args):\n    wnids = get_wnids_from_dataset(args.dataset)\n    path = get_graph_path_from_args(**vars(args))\n    print(\'==> Reading from {}\'.format(path))\n\n    G = read_graph(path)\n\n    G_name = Path(path).stem\n\n    leaves_seen, wnid_set1 = match_wnid_leaves(wnids, G, G_name)\n    print_stats(leaves_seen, wnid_set1, G_name, \'leaves\')\n\n    leaves_seen, wnid_set2 = match_wnid_nodes(wnids, G, G_name)\n    print_stats(leaves_seen, wnid_set2, G_name, \'nodes\')\n\n    num_roots = len(list(get_roots(G)))\n    if num_roots == 1:\n        Colors.green(\'Found just 1 root.\')\n    else:\n        Colors.red(f\'Found {num_roots} roots. Should be only 1.\')\n\n    if len(wnid_set1) == len(wnid_set2) == 0 and num_roots == 1:\n        Colors.green(""==> All checks pass!"")\n    else:\n        Colors.red(\'==> Test failed\')\n\n\n#######\n# VIS #\n#######\n\n\ndef build_tree(G, root,\n        parent=\'null\',\n        color_info=(),\n        force_labels_left=(),\n        include_leaf_images=False,\n        dataset=None,\n        image_resize_factor=1):\n    """"""\n    :param color_info dict[str, dict]: mapping from node labels or IDs to color\n                                       information. This is by default just a\n                                       key called \'color\'\n    """"""\n    children = [\n        build_tree(G, child, root,\n            color_info=color_info,\n            force_labels_left=force_labels_left,\n            include_leaf_images=include_leaf_images,\n            dataset=dataset,\n            image_resize_factor=image_resize_factor)\n        for child in G.succ[root]]\n    _node = G.nodes[root]\n    label = _node.get(\'label\', \'\')\n    sublabel = root\n\n    if root.startswith(\'f\'):  # WARNING: hacky, ignores fake wnids -- this will have to be changed lol\n        sublabel = \'\'\n\n    node = {\n        \'sublabel\': sublabel,\n        \'label\': label,\n        \'parent\': parent,\n        \'children\': children,\n    }\n\n    if label in color_info:\n        node.update(color_info[label])\n\n    if root in color_info:\n        node.update(color_info[root])\n\n    if label in force_labels_left:\n        node[\'force_text_on_left\'] = True\n\n    is_leaf = len(children) == 0\n    if include_leaf_images and is_leaf:\n        try:\n            image = get_class_image_from_dataset(dataset, label)\n        except UserWarning as e:\n            print(e)\n            return node\n        base64_encode = image_to_base64_encode(image, format=""jpeg"")\n        image_href = f""data:image/jpeg;base64,{base64_encode.decode(\'utf-8\')}""\n        image_height, image_width = image.size\n        node[\'image\'] = {\n            \'href\': image_href,\n            \'width\': image_width * image_resize_factor,\n            \'height\': image_height *  image_resize_factor\n        }\n    return node\n\n\ndef build_graph(G):\n    return {\n        \'nodes\': [{\n            \'name\': wnid,\n            \'label\': G.nodes[wnid].get(\'label\', \'\'),\n            \'id\': wnid\n        } for wnid in G.nodes],\n        \'links\': [{\n            \'source\': u,\n            \'target\': v\n        } for u, v in G.edges]\n    }\n\n\ndef get_class_image_from_dataset(dataset, candidate):\n    """"""Returns image for given class `candidate`. Image is PIL.""""""\n    if isinstance(candidate, int):\n        candidate = dataset.classes[candidate]\n    for sample, label in dataset:\n        intersection = compare_wnids(dataset.classes[label], candidate)\n        if label == candidate or intersection:\n            return sample\n    raise UserWarning(f\'No samples with label {candidate} found.\')\n\n\ndef compare_wnids(label1, label2):\n    from nltk.corpus import wordnet as wn  # entire script should not depend on wordnet\n    synsets1 = wn.synsets(label1, pos=wn.NOUN)\n    synsets2 = wn.synsets(label2, pos=wn.NOUN)\n    wnids1 = set(map(synset_to_wnid, synsets1))\n    wnids2 = set(map(synset_to_wnid, synsets2))\n    return wnids1.intersection(wnids2)\n\n\ndef image_to_base64_encode(image, format=""jpeg""):\n    """"""Converts PIL image to base64 encoding, ready for use as data uri.""""""\n    buffered = BytesIO()\n    image.save(buffered, format=format)\n    return base64.b64encode(buffered.getvalue())\n\n\ndef generate_vis(path_template, data, name, fname, zoom=2, straight_lines=True,\n        show_sublabels=False, height=750, dark=False):\n    with open(path_template) as f:\n        html = f.read() \\\n        .replace(\n            ""CONFIG_TREE_DATA"",\n            json.dumps([data])) \\\n        .replace(\n            ""CONFIG_ZOOM"",\n            str(zoom)) \\\n        .replace(\n            ""CONFIG_STRAIGHT_LINES"",\n            str(straight_lines).lower()) \\\n        .replace(\n            ""CONFIG_SHOW_SUBLABELS"",\n            str(show_sublabels).lower()) \\\n        .replace(\n            ""CONFIG_TITLE"",\n            fname) \\\n        .replace(\n            ""CONFIG_VIS_HEIGHT"",\n            str(height)) \\\n        .replace(\n            ""CONFIG_BG_COLOR"",\n            ""#111111"" if dark else ""#FFFFFF"") \\\n        .replace(\n            ""CONFIG_TEXT_COLOR"",\n            \'#FFFFFF\' if dark else \'#000000\') \\\n        .replace(\n            ""CONFIG_TEXT_RECT_COLOR"",\n            ""rgba(17,17,17,0.8)"" if dark else ""rgba(255,255,255,0.8)"")\n\n    os.makedirs(\'out\', exist_ok=True)\n    path_html = f\'out/{fname}-{name}.html\'\n    with open(path_html, \'w\') as f:\n        f.write(html)\n\n    Colors.green(\'==> Wrote HTML to {}\'.format(path_html))\n\n\ndef get_color_info(G, color, color_leaves, color_path_to=None, color_nodes=()):\n    """"""Mapping from node to color information.""""""\n    nodes = {}\n    leaves = list(get_leaves(G))\n    if color_leaves:\n        for leaf in leaves:\n            nodes[leaf] = {\'color\': color}\n\n    for (id, node) in G.nodes.items():\n        if node.get(\'label\', \'\') in color_nodes or id in color_nodes:\n            nodes[id] = {\'color\': color}\n\n    root = get_root(G)\n    target = None\n    for leaf in leaves:\n        node = G.nodes[leaf]\n        if node.get(\'label\', \'\') == color_path_to or leaf == color_path_to:\n            target = leaf\n            break\n\n    if target is not None:\n        while target != root:\n            nodes[target] = {\'color\': color, \'color_incident_edge\': True}\n            view = G.pred[target]\n            target = list(view.keys())[0]\n        nodes[root] = {\'color\': color}\n    return nodes\n\n\ndef generate_vis_fname(vis_color_path_to=None, **kwargs):\n    fname = generate_fname(**kwargs).replace(\'graph-\', f\'{kwargs[""dataset""]}-\', 1)\n    if vis_color_path_to is not None:\n        fname += \'-\' + vis_color_path_to\n    return fname\n\n\ndef generate_hierarchy_vis(args):\n    path = get_graph_path_from_args(**vars(args))\n    print(\'==> Reading from {}\'.format(path))\n\n    G = read_graph(path)\n\n    roots = list(get_roots(G))\n    num_roots = len(roots)\n    root = next(get_roots(G))\n\n    dataset = None\n    if args.dataset:\n        cls = getattr(data, args.dataset)\n        dataset = cls(root=\'./data\', train=False, download=True)\n\n    color_info = get_color_info(\n        G,\n        args.color,\n        color_leaves=not args.vis_no_color_leaves,\n        color_path_to=args.vis_color_path_to,\n        color_nodes=args.vis_color_nodes or ())\n\n    tree = build_tree(G, root,\n        color_info=color_info,\n        force_labels_left=args.vis_force_labels_left or [],\n        dataset=dataset,\n        include_leaf_images=args.vis_leaf_images,\n        image_resize_factor=args.vis_image_resize_factor)\n    graph = build_graph(G)\n\n    if num_roots > 1:\n        Colors.red(f\'Found {num_roots} roots! Should be only 1: {roots}\')\n    else:\n        print(f\'Found just {num_roots} root.\')\n\n    fname = generate_vis_fname(**vars(args))\n    parent = Path(fwd()).parent\n    generate_vis(\n        str(parent / \'nbdt/templates/tree-template.html\'), tree, \'tree\', fname,\n        zoom=args.vis_zoom,\n        straight_lines=not args.vis_curved,\n        show_sublabels=args.vis_sublabels,\n        height=args.vis_height,\n        dark=args.vis_dark)\n'"
nbdt/loss.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import defaultdict\nfrom nbdt.data.custom import Node, dataset_to_dummy_classes\nfrom nbdt.model import HardEmbeddedDecisionRules, SoftEmbeddedDecisionRules\nfrom nbdt.utils import (\n    Colors, dataset_to_default_path_graph, dataset_to_default_path_wnids,\n    hierarchy_to_path_graph\n)\n\n__all__ = names = (\'HardTreeSupLoss\', \'SoftTreeSupLoss\', \'CrossEntropyLoss\')\nkeys = (\n    \'path_graph\', \'path_wnids\', \'tree_supervision_weight\',\n    \'classes\', \'dataset\', \'criterion\'\n)\n\ndef add_arguments(parser):\n    parser.add_argument(\'--hierarchy\',\n                        help=\'Hierarchy to use. If supplied, will be used to \'\n                        \'generate --path-graph. --path-graph takes precedence.\')\n    parser.add_argument(\'--path-graph\', help=\'Path to graph-*.json file.\')  # WARNING: hard-coded suffix -build in generate_fname\n    parser.add_argument(\'--path-wnids\', help=\'Path to wnids.txt file.\')\n    parser.add_argument(\'--tree-supervision-weight\', type=float, default=1,\n                        help=\'Weight assigned to tree supervision losses\')\n\n\ndef set_default_values(args):\n    assert not (args.hierarchy and args.path_graph), \\\n        \'Only one, between --hierarchy and --path-graph can be provided.\'\n    if \'TreeSupLoss\' not in args.loss:\n        return\n    if args.hierarchy and not args.path_graph:\n        args.path_graph = hierarchy_to_path_graph(args.dataset, args.hierarchy)\n    if not args.path_graph:\n        args.path_graph = dataset_to_default_path_graph(args.dataset)\n    if not args.path_wnids:\n        args.path_wnids = dataset_to_default_path_wnids(args.dataset)\n\n\nCrossEntropyLoss = nn.CrossEntropyLoss\n\n\nclass TreeSupLoss(nn.Module):\n\n    accepts_criterion = lambda criterion, **kwargs: criterion\n    accepts_dataset = lambda trainset, **kwargs: trainset.__class__.__name__\n    accepts_path_graph = True\n    accepts_path_wnids = True\n    accepts_classes = True\n    accepts_tree_supervision_weight = True\n    accepts_classes = lambda trainset, **kwargs: trainset.classes\n\n    def __init__(self,\n            dataset,\n            criterion,\n            path_graph=None,\n            path_wnids=None,\n            classes=None,\n            hierarchy=None,\n            Rules=HardEmbeddedDecisionRules,\n            **kwargs):\n        super().__init__()\n\n        if dataset and hierarchy and not path_graph:\n            path_graph = hierarchy_to_path_graph(dataset, hierarchy)\n        if dataset and not path_graph:\n            path_graph = dataset_to_default_path_graph(dataset)\n        if dataset and not path_wnids:\n            path_wnids = dataset_to_default_path_wnids(dataset)\n        if dataset and not classes:\n            classes = dataset_to_dummy_classes(dataset)\n\n        self.init(dataset, criterion, path_graph, path_wnids, classes,\n            Rules=Rules, **kwargs)\n\n    def init(self,\n            dataset,\n            criterion,\n            path_graph,\n            path_wnids,\n            classes,\n            Rules,\n            tree_supervision_weight=1.):\n        """"""\n        Extra init method makes clear which arguments are finally necessary for\n        this class to function. The constructor for this class may generate\n        some of these required arguments if initially missing.\n        """"""\n        self.dataset = dataset\n        self.num_classes = len(classes)\n        self.nodes = Node.get_nodes(path_graph, path_wnids, classes)\n        self.rules = Rules(dataset, path_graph, path_wnids, classes)\n        self.tree_supervision_weight = tree_supervision_weight\n        self.criterion = criterion\n\n    @staticmethod\n    def assert_output_not_nbdt(outputs):\n        """"""\n        >>> x = torch.randn(1, 3, 224, 224)\n        >>> TreeSupLoss.assert_output_not_nbdt(x)  # all good!\n        >>> x._nbdt_output_flag = True\n        >>> TreeSupLoss.assert_output_not_nbdt(x)  #doctest: +ELLIPSIS\n        Traceback (most recent call last):\n            ...\n        AssertionError: ...\n        >>> from nbdt.model import NBDT\n        >>> import torchvision.models as models\n        >>> model = models.resnet18()\n        >>> y = model(x)\n        >>> TreeSupLoss.assert_output_not_nbdt(y)  # all good!\n        >>> model = NBDT(\'CIFAR10\', model, arch=\'ResNet18\')\n        >>> y = model(x)\n        >>> TreeSupLoss.assert_output_not_nbdt(y)  #doctest: +ELLIPSIS\n        Traceback (most recent call last):\n            ...\n        AssertionError: ...\n        """"""\n        assert getattr(outputs, \'_nbdt_output_flag\', False) is False, (\n            ""Uh oh! Looks like you passed an NBDT model\'s output to an NBDT ""\n            ""loss. NBDT losses are designed to take in the *original* model\'s ""\n            ""outputs, as input. NBDT models are designed to only be used ""\n            ""during validation and inference, not during training. Confused? ""\n            "" Check out github.com/alvinwan/nbdt#convert-neural-networks-to-decision-trees""\n            "" for examples and instructions."")\n\n\nclass HardTreeSupLoss(TreeSupLoss):\n\n    def forward(self, outputs, targets):\n        """"""\n        The supplementary losses are all uniformly down-weighted so that on\n        average, each sample incurs half of its loss from standard cross entropy\n        and half of its loss from all nodes.\n\n        The code below is structured weirdly to minimize number of tensors\n        constructed and moved from CPU to GPU or vice versa. In short,\n        all outputs and targets for nodes with 2 children are gathered and\n        moved onto GPU at once. Same with those with 3, with 4 etc. On CIFAR10,\n        the max is 2. On CIFAR100, the max is 8.\n        """"""\n        self.assert_output_not_nbdt(outputs)\n\n        loss = self.criterion(outputs, targets)\n        num_losses = outputs.size(0) * len(self.nodes) / 2.\n\n        outputs_subs = defaultdict(lambda: [])\n        targets_subs = defaultdict(lambda: [])\n        targets_ints = [int(target) for target in targets.cpu().long()]\n        for node in self.nodes:\n            _, outputs_sub, targets_sub = \\\n                HardEmbeddedDecisionRules.get_node_logits_filtered(\n                    node, outputs, targets_ints)\n\n            key = node.num_classes\n            assert outputs_sub.size(0) == len(targets_sub)\n            outputs_subs[key].append(outputs_sub)\n            targets_subs[key].extend(targets_sub)\n\n        for key in outputs_subs:\n            outputs_sub = torch.cat(outputs_subs[key], dim=0)\n            targets_sub = torch.Tensor(targets_subs[key]).long().to(outputs_sub.device)\n\n            if not outputs_sub.size(0):\n                continue\n            fraction = outputs_sub.size(0) / float(num_losses) \\\n                * self.tree_supervision_weight\n            loss += self.criterion(outputs_sub, targets_sub) * fraction\n        return loss\n\n\nclass SoftTreeSupLoss(TreeSupLoss):\n\n    def __init__(self, *args, Rules=None, **kwargs):\n        super().__init__(*args, Rules=SoftEmbeddedDecisionRules, **kwargs)\n\n    def forward(self, outputs, targets):\n        self.assert_output_not_nbdt(outputs)\n\n        loss = self.criterion(outputs, targets)\n        bayesian_outputs = self.rules(outputs)\n        loss += self.criterion(bayesian_outputs, targets) * self.tree_supervision_weight\n        return loss\n'"
nbdt/model.py,8,"b'""""""\nFor external use as part of nbdt package. This is a model that\nruns inference as an NBDT. Note these make no assumption about the\nunderlying neural network other than it (1) is a classification model and\n(2) returns logits.\n""""""\n\nimport torch.nn as nn\nfrom nbdt.utils import (\n    dataset_to_default_path_graph,\n    dataset_to_default_path_wnids,\n    hierarchy_to_path_graph)\nfrom nbdt.models.utils import load_state_dict_from_key, coerce_state_dict\nfrom nbdt.data.custom import Node, dataset_to_dummy_classes\nfrom nbdt.graph import get_root, get_wnids, synset_to_name, wnid_to_name\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nmodel_urls = {\n    (\'ResNet18\', \'CIFAR10\'): \'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-CIFAR10-ResNet18-induced-ResNet18-SoftTreeSupLoss.pth\',\n    (\'wrn28_10_cifar10\', \'CIFAR10\'): \'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-CIFAR10-wrn28_10_cifar10-induced-wrn28_10_cifar10-SoftTreeSupLoss.pth\',\n    (\'wrn28_10_cifar10\', \'CIFAR10\', \'wordnet\'): \'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-CIFAR10-wrn28_10_cifar10-wordnet-SoftTreeSupLoss.pth\',\n    (\'ResNet18\', \'CIFAR100\'): \'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-CIFAR100-ResNet18-induced-ResNet18-SoftTreeSupLoss.pth\',\n    (\'wrn28_10_cifar100\', \'CIFAR100\'): \'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-CIFAR100-wrn28_10_cifar100-induced-wrn28_10_cifar100-SoftTreeSupLoss.pth\',\n    (\'ResNet18\', \'TinyImagenet200\'): \'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-TinyImagenet200-ResNet18-induced-ResNet18-SoftTreeSupLoss-tsw10.0.pth\',\n    (\'wrn28_10\', \'TinyImagenet200\'): \'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-TinyImagenet200-wrn28_10-induced-wrn28_10-SoftTreeSupLoss-tsw10.0.pth\',\n}\n\n\n#########\n# RULES #\n#########\n\n\nclass EmbeddedDecisionRules(nn.Module):\n\n    def __init__(self,\n            dataset,\n            path_graph=None,\n            path_wnids=None,\n            classes=()):\n\n        if not path_graph:\n            path_graph = dataset_to_default_path_graph(dataset)\n        if not path_wnids:\n            path_wnids = dataset_to_default_path_wnids(dataset)\n        if not classes:\n            classes = dataset_to_dummy_classes(dataset)\n        super().__init__()\n        assert all([dataset, path_graph, path_wnids, classes])\n\n        self.classes = classes\n\n        self.nodes = Node.get_nodes(path_graph, path_wnids, classes)\n        self.G = self.nodes[0].G\n        self.wnid_to_node = {node.wnid: node for node in self.nodes}\n\n        self.wnids = get_wnids(path_wnids)\n        self.wnid_to_class = {wnid: cls for wnid, cls in zip(self.wnids, self.classes)}\n\n        self.correct = 0\n        self.total = 0\n\n        self.I = torch.eye(len(classes))\n\n    @staticmethod\n    def get_node_logits(outputs, node):\n        """"""Get output for a particular node\n\n        This `outputs` above are the output of the neural network.\n        """"""\n        return torch.stack([\n            outputs.T[node.new_to_old_classes[new_label]].mean(dim=0)\n            for new_label in range(node.num_classes)\n        ]).T\n\n    @classmethod\n    def get_all_node_outputs(cls, outputs, nodes):\n        """"""Run hard embedded decision rules.\n\n        Returns the output for *every single node.\n        """"""\n        wnid_to_outputs = {}\n        for node in nodes:\n            node_logits = cls.get_node_logits(outputs, node)\n            wnid_to_outputs[node.wnid] = {\n                \'logits\': node_logits,\n                \'preds\': torch.max(node_logits, dim=1)[1],\n                \'probs\': F.softmax(node_logits, dim=1)\n            }\n        return wnid_to_outputs\n\n    def forward_nodes(self, outputs):\n        return self.get_all_node_outputs(outputs, self.nodes)\n\n\nclass HardEmbeddedDecisionRules(EmbeddedDecisionRules):\n\n    @classmethod\n    def get_node_logits_filtered(cls, node, outputs, targets):\n        """"""\'Smarter\' inference for a hard node.\n\n        If you have targets for the node, you can selectively perform inference,\n        only for nodes where the label of a sample is well-defined.\n        """"""\n        classes = [node.old_to_new_classes[int(t)] for t in targets]\n        selector = [bool(cls) for cls in classes]\n        targets_sub = [cls[0] for cls in classes if cls]\n\n        outputs = outputs[selector]\n        if outputs.size(0) == 0:\n            return selector, outputs[:, :node.num_classes], targets_sub\n\n        outputs_sub = cls.get_node_logits(outputs, node)\n        return selector, outputs_sub, targets_sub\n\n    @classmethod\n    def traverse_tree(cls, wnid_to_outputs, nodes, wnid_to_class, classes):\n        """"""Convert node outputs to final prediction.\n\n        Note that the prediction output for this function can NOT be trained\n        on. The outputs have been detached from the computation graph.\n        """"""\n        # move all to CPU, detach from computation graph\n        example = wnid_to_outputs[nodes[0].wnid]\n        n_samples = int(example[\'logits\'].size(0))\n\n        for wnid in tuple(wnid_to_outputs.keys()):\n            outputs = wnid_to_outputs[wnid]\n            outputs[\'preds\'] = list(map(int, outputs[\'preds\'].cpu()))\n            outputs[\'probs\'] = outputs[\'probs\'].detach().cpu()\n\n        wnid_to_node = {node.wnid: node for node in nodes}\n        wnid_root = get_root(nodes[0].G)\n        node_root = wnid_to_node[wnid_root]\n\n        decisions = []\n        preds = []\n        for index in range(n_samples):\n            decision = [{\'node\': node_root, \'name\': \'root\', \'prob\': 1}]\n            wnid, node = wnid_root, node_root\n            while node is not None:\n                if node.wnid not in wnid_to_outputs:\n                    wnid = node = None\n                    break\n                outputs = wnid_to_outputs[node.wnid]\n                index_child = outputs[\'preds\'][index]\n                prob_child = float(outputs[\'probs\'][index][index_child])\n                wnid = node.children[index_child]\n                node = wnid_to_node.get(wnid, None)\n                decision.append({\'node\': node, \'name\': wnid_to_name(wnid), \'prob\': prob_child})\n            cls = wnid_to_class.get(wnid, None)\n            pred = -1 if cls is None else classes.index(cls)\n            preds.append(pred)\n            decisions.append(decision)\n        return torch.Tensor(preds).long(), decisions\n\n    def predicted_to_logits(self, predicted):\n        """"""Convert predicted classes to one-hot logits.""""""\n        if self.I.device != predicted.device:\n            self.I = self.I.to(predicted.device)\n        return self.I[predicted]\n\n    def forward_with_decisions(self, outputs):\n        wnid_to_outputs = self.forward_nodes(outputs)\n        predicted, decisions = self.traverse_tree(\n            wnid_to_outputs, self.nodes, self.wnid_to_class, self.classes)\n        logits = self.predicted_to_logits(predicted)\n        logits._nbdt_output_flag = True  # checked in nbdt losses, to prevent mistakes\n        return logits, decisions\n\n    def forward(self, outputs):\n        outputs, _ = self.forward_with_decisions(outputs)\n        return outputs\n\n\nclass SoftEmbeddedDecisionRules(EmbeddedDecisionRules):\n\n    @classmethod\n    def traverse_tree(cls, wnid_to_outputs, nodes):\n        """"""\n        In theory, the loop over children below could be replaced with just a\n        few lines:\n\n            for index_child in range(len(node.children)):\n                old_indexes = node.new_to_old_classes[index_child]\n                class_probs[:,old_indexes] *= output[:,index_child][:,None]\n\n        However, we collect all indices first, so that only one tensor operation\n        is run. The output is a single distribution over all leaves. The\n        ordering is determined by the original ordering of the provided logits.\n        (I think. Need to check nbdt.data.custom.Node)\n        """"""\n        example = wnid_to_outputs[nodes[0].wnid]\n        num_samples = example[\'logits\'].size(0)\n        num_classes = len(nodes[0].original_classes)\n        device = example[\'logits\'].device\n        class_probs = torch.ones((num_samples, num_classes)).to(device)\n\n        for node in nodes:\n            outputs = wnid_to_outputs[node.wnid]\n\n            old_indices, new_indices = [], []\n            for index_child in range(len(node.children)):\n                old = node.new_to_old_classes[index_child]\n                old_indices.extend(old)\n                new_indices.extend([index_child] * len(old))\n\n            assert len(set(old_indices)) == len(old_indices), (\n                \'All old indices must be unique in order for this operation \'\n                \'to be correct.\'\n            )\n            class_probs[:,old_indices] *= outputs[\'probs\'][:,new_indices]\n        return class_probs\n\n    def forward_with_decisions(self, outputs):\n        outputs = self.forward(outputs)\n        _, predicted = outputs.max(1)\n\n        decisions = []\n        node = self.nodes[0]\n        leaf_to_path_nodes = Node.get_leaf_to_path(self.nodes)\n        for index, prediction in enumerate(predicted):\n            leaf = node.wnids[prediction]\n            decision = leaf_to_path_nodes[leaf]\n            for justification in decision:\n                justification[\'prob\'] = -1  # TODO(alvin): fill in prob\n            decisions.append(decision)\n        return outputs, decisions\n\n    def forward(self, outputs):\n        wnid_to_outputs = self.forward_nodes(outputs)\n        logits = self.traverse_tree(wnid_to_outputs, self.nodes)\n        logits._nbdt_output_flag = True  # checked in nbdt losses, to prevent mistakes\n        return logits\n\n\n##########\n# MODELS #\n##########\n\n\nclass NBDT(nn.Module):\n\n    def __init__(self,\n            dataset,\n            model,\n            arch=None,\n            path_graph=None,\n            path_wnids=None,\n            classes=None,\n            hierarchy=None,\n            pretrained=None,\n            **kwargs):\n        super().__init__()\n\n        if dataset and not hierarchy and not path_graph:\n            assert arch, \'Must specify `arch` if no `hierarchy` or `path_graph`\'\n            hierarchy = f\'induced-{arch}\'\n        if dataset and hierarchy and not path_graph:\n            path_graph = hierarchy_to_path_graph(dataset, hierarchy)\n        if dataset and not path_graph:\n            path_graph = dataset_to_default_path_graph(dataset)\n        if dataset and not path_wnids:\n            path_wnids = dataset_to_default_path_wnids(dataset)\n        if dataset and not classes:\n            classes = dataset_to_dummy_classes(dataset)\n        if pretrained and not arch:\n            raise UserWarning(\n                \'To load a pretrained NBDT, you need to specify the `arch`. \'\n                \'`arch` is the name of the architecture. e.g., ResNet18\')\n        if isinstance(model, str):\n            raise NotImplementedError(\'Model must be nn.Module\')\n\n        self.init(dataset, model, path_graph, path_wnids, classes,\n            arch=arch, pretrained=pretrained, hierarchy=hierarchy, **kwargs)\n\n    def init(self,\n            dataset,\n            model,\n            path_graph,\n            path_wnids,\n            classes,\n            arch=None,\n            pretrained=False,\n            hierarchy=None,\n            eval=True,\n            Rules=HardEmbeddedDecisionRules):\n        """"""\n        Extra init method makes clear which arguments are finally necessary for\n        this class to function. The constructor for this class may generate\n        some of these required arguments if initially missing.\n        """"""\n        self.rules = Rules(dataset, path_graph, path_wnids, classes)\n        self.model = model\n\n        if pretrained:\n            assert arch is not None\n            keys = [(arch, dataset), (arch, dataset, hierarchy)]\n            state_dict = load_state_dict_from_key(\n                keys, model_urls, pretrained=True)\n            self.load_state_dict(state_dict)\n\n        if eval:\n            self.eval()\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict = coerce_state_dict(state_dict, self.model.state_dict())\n        return self.model.load_state_dict(state_dict, **kwargs)\n\n    def state_dict(self):\n        return self.model.state_dict()\n\n    def forward(self, x):\n        x = self.model(x)\n        x = self.rules(x)\n        return x\n\n    def forward_with_decisions(self, x):\n        x = self.model(x)\n        x, decisions = self.rules.forward_with_decisions(x)\n        return x, decisions\n\n\nclass HardNBDT(NBDT):\n\n    def __init__(self, *args, **kwargs):\n        kwargs.update({\n            \'Rules\': HardEmbeddedDecisionRules\n        })\n        super().__init__(*args, **kwargs)\n\n\nclass SoftNBDT(NBDT):\n\n    def __init__(self, *args, **kwargs):\n        kwargs.update({\n            \'Rules\': SoftEmbeddedDecisionRules\n        })\n        super().__init__(*args, **kwargs)\n'"
nbdt/utils.py,5,"b'\'\'\'Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n\'\'\'\nimport os\nimport sys\nimport time\nimport math\nimport numpy as np\n\nfrom urllib.request import urlopen, Request\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom pathlib import Path\nimport io\nimport nltk\n\n# tree-generation consntants\nMETHODS = (\'wordnet\', \'random\', \'induced\')\nDATASETS = (\'CIFAR10\', \'CIFAR100\', \'TinyImagenet200\', \'Imagenet1000\')\nDATASET_TO_NUM_CLASSES = {\n    \'CIFAR10\': 10,\n    \'CIFAR100\': 100,\n    \'TinyImagenet200\': 200,\n    \'Imagenet1000\': 1000\n}\nDATASET_TO_CLASSES = {\n    \'CIFAR10\': [\n        \'airplane\', \'automobile\', \'bird\', \'cat\', \'deer\', \'dog\', \'frog\',\n        \'horse\', \'ship\', \'truck\'\n    ]\n}\n\n\ndef maybe_install_wordnet():\n    try:\n        nltk.data.find(\'corpora/wordnet\')\n    except Exception as e:\n        print(e)\n        nltk.download(\'wordnet\')\n\n\ndef fwd():\n    """"""Get file\'s working directory""""""\n    return Path(__file__).parent.absolute()\n\n\ndef dataset_to_default_path_graph(dataset):\n    return hierarchy_to_path_graph(dataset, \'induced\')\n\n\ndef hierarchy_to_path_graph(dataset, hierarchy):\n    return os.path.join(fwd(), f\'hierarchies/{dataset}/graph-{hierarchy}.json\')\n\n\ndef dataset_to_default_path_wnids(dataset):\n    return os.path.join(fwd(), f\'wnids/{dataset}.txt\')\n\n\ndef generate_kwargs(args, object, name=\'Dataset\', keys=(), globals={}, kwargs=None):\n    kwargs = kwargs or {}\n\n    for key in keys:\n        accepts_key = getattr(object, f\'accepts_{key}\', False)\n        if not accepts_key:\n            continue\n        assert key in args or callable(accepts_key)\n\n        value = getattr(args, key, None)\n        if callable(accepts_key):\n            kwargs[key] = accepts_key(**globals)\n            Colors.cyan(f\'{key}:\\t(callable)\')\n        elif accepts_key and value:\n            kwargs[key] = value\n            Colors.cyan(f\'{key}:\\t{value}\')\n        elif value:\n            Colors.red(\n                f\'Warning: {name} does not support custom \'\n                f\'{key}: {value}\')\n    return kwargs\n\n\ndef load_image_from_path(path):\n    """"""Path can be local or a URL""""""\n    headers = {\n      \'User-Agent\': \'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3\'\n    }\n    if \'http\' in path:\n      request = Request(path, headers=headers)\n      file = io.BytesIO(urlopen(request).read())\n    else:\n      file = path\n    return Image.open(file)\n\n\nclass Colors:\n    RED = \'\\x1b[31m\'\n    GREEN = \'\\x1b[32m\'\n    ENDC = \'\\033[0m\'\n    BOLD = \'\\033[1m\'\n    CYAN = \'\\x1b[36m\'\n\n    @classmethod\n    def red(cls, *args):\n        print(cls.RED + args[0], *args[1:], cls.ENDC)\n\n    @classmethod\n    def green(cls, *args):\n        print(cls.GREEN + args[0], *args[1:], cls.ENDC)\n\n    @classmethod\n    def cyan(cls, *args):\n        print(cls.CYAN + args[0], *args[1:], cls.ENDC)\n\n    @classmethod\n    def bold(cls, *args):\n        print(cls.BOLD + args[0], *args[1:], cls.ENDC)\n\n\ndef get_mean_and_std(dataset):\n    \'\'\'Compute the mean and std value of dataset.\'\'\'\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print(\'==> Computing mean and std..\')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:,i,:,:].mean()\n            std[i] += inputs[:,i,:,:].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\ndef init_params(net):\n    \'\'\'Init layer parameters.\'\'\'\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode=\'fan_out\')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\n\ntry:\n    _, term_width = os.popen(\'stty size\', \'r\').read().split()\n    term_width = int(term_width)\nexcept Exception as e:\n    print(e)\n    term_width = 50\n\nTOTAL_BAR_LENGTH = 65.\nlast_time = time.time()\nbegin_time = last_time\ndef progress_bar(current, total, msg=None):\n    global last_time, begin_time\n    if current == 0:\n        begin_time = time.time()  # Reset for new bar.\n\n    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n\n    sys.stdout.write(\' [\')\n    for i in range(cur_len):\n        sys.stdout.write(\'=\')\n    sys.stdout.write(\'>\')\n    for i in range(rest_len):\n        sys.stdout.write(\'.\')\n    sys.stdout.write(\']\')\n\n    cur_time = time.time()\n    step_time = cur_time - last_time\n    last_time = cur_time\n    tot_time = cur_time - begin_time\n\n    L = []\n    L.append(\'  Step: %s\' % format_time(step_time))\n    L.append(\' | Tot: %s\' % format_time(tot_time))\n    if msg:\n        L.append(\' | \' + msg)\n\n    msg = \'\'.join(L)\n    sys.stdout.write(msg)\n    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n        sys.stdout.write(\' \')\n\n    # Go back to the center of the bar.\n    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n        sys.stdout.write(\'\\b\')\n    sys.stdout.write(\' %d/%d \' % (current+1, total))\n\n    if current < total-1:\n        sys.stdout.write(\'\\r\')\n    else:\n        sys.stdout.write(\'\\n\')\n    sys.stdout.flush()\n\ndef format_time(seconds):\n    days = int(seconds / 3600/24)\n    seconds = seconds - days*3600*24\n    hours = int(seconds / 3600)\n    seconds = seconds - hours*3600\n    minutes = int(seconds / 60)\n    seconds = seconds - minutes*60\n    secondsf = int(seconds)\n    seconds = seconds - secondsf\n    millis = int(seconds*1000)\n\n    f = \'\'\n    i = 1\n    if days > 0:\n        f += str(days) + \'D\'\n        i += 1\n    if hours > 0 and i <= 2:\n        f += str(hours) + \'h\'\n        i += 1\n    if minutes > 0 and i <= 2:\n        f += str(minutes) + \'m\'\n        i += 1\n    if secondsf > 0 and i <= 2:\n        f += str(secondsf) + \'s\'\n        i += 1\n    if millis > 0 and i <= 2:\n        f += str(millis) + \'ms\'\n        i += 1\n    if f == \'\':\n        f = \'0ms\'\n    return f\n\n\ndef set_np_printoptions():\n    np.set_printoptions(formatter={\'float\': lambda x: ""{0:0.3f}"".format(x)})\n\n\ndef generate_fname(dataset, arch, path_graph, wnid=None, name=\'\',\n        trainset=None, include_labels=(), exclude_labels=(),\n        include_classes=(), num_samples=0, tree_supervision_weight=0.5,\n        fine_tune=False, loss=\'CrossEntropyLoss\',\n        **kwargs):\n    fname = \'ckpt\'\n    fname += \'-\' + dataset\n    fname += \'-\' + arch\n    if name:\n        fname += \'-\' + name\n    if path_graph:\n        path = Path(path_graph)\n        fname += \'-\' + path.stem.replace(\'graph-\', \'\', 1)\n    if include_labels:\n        labels = "","".join(map(str, include_labels))\n        fname += f\'-incl{labels}\'\n    if exclude_labels:\n        labels = "","".join(map(str, exclude_labels))\n        fname += f\'-excl{labels}\'\n    if include_classes:\n        labels = "","".join(map(str, include_classes))\n        fname += f\'-incc{labels}\'\n    if num_samples != 0 and num_samples is not None:\n        fname += f\'-samples{num_samples}\'\n    if loss != \'CrossEntropyLoss\':\n        fname += f\'-{loss}\'\n        if tree_supervision_weight is not None and tree_supervision_weight != 1:\n            fname += f\'-tsw{tree_supervision_weight}\'\n    return fname\n'"
tests/conftest.py,5,"b'import pytest\nimport torch\nimport torch.nn as nn\nfrom nbdt.models import ResNet18\n\n\ncollect_ignore = [""setup.py"", ""main.py""]\n\n\n@pytest.fixture\ndef label_cifar10():\n    return torch.randint(10, (1,))\n\n\n@pytest.fixture\ndef input_cifar10():\n    return torch.randn(1, 3, 32, 32)\n\n\n@pytest.fixture\ndef input_cifar100():\n    return torch.randn(1, 3, 32, 32)\n\n\n@pytest.fixture\ndef input_tinyimagenet200():\n    return torch.randn(1, 3, 64, 64)\n\n\n@pytest.fixture\ndef criterion():\n    return nn.CrossEntropyLoss()\n\n\n@pytest.fixture\ndef resnet18_cifar10():\n    return ResNet18(num_classes=10)\n\n\n@pytest.fixture\ndef resnet18_cifar100():\n    return ResNet18(num_classes=100)\n\n\n@pytest.fixture\ndef resnet18_tinyimagenet200():\n    return ResNet18(num_classes=200)\n'"
tests/test_inference.py,0,"b'""""""Tests that models work inference-time""""""\n\nfrom nbdt.model import SoftNBDT, HardNBDT\n\n\ndef test_nbdt_soft_cifar10(input_cifar10, resnet18_cifar10):\n    model_soft = SoftNBDT(dataset=\'CIFAR10\', model=resnet18_cifar10, hierarchy=\'induced\')\n    model_soft(input_cifar10)\n\n\ndef test_nbdt_soft_cifar100(input_cifar100, resnet18_cifar100):\n    model_soft = SoftNBDT(dataset=\'CIFAR100\', model=resnet18_cifar100, hierarchy=\'induced\')\n    model_soft(input_cifar100)\n\n\ndef test_nbdt_soft_tinyimagenet200(input_tinyimagenet200, resnet18_tinyimagenet200):\n    model_soft = SoftNBDT(dataset=\'TinyImagenet200\', model=resnet18_tinyimagenet200, hierarchy=\'induced\')\n    model_soft(input_tinyimagenet200)\n\n\ndef test_nbdt_hard_cifar10(input_cifar10, resnet18_cifar10):\n    model_hard = HardNBDT(dataset=\'CIFAR10\', model=resnet18_cifar10, hierarchy=\'induced\')\n    model_hard(input_cifar10)\n\n\ndef test_nbdt_hard_cifar100(input_cifar100, resnet18_cifar100):\n    model_hard = HardNBDT(dataset=\'CIFAR100\', model=resnet18_cifar100, hierarchy=\'induced\')\n    model_hard(input_cifar100)\n\n\ndef test_nbdt_hard_tinyimagenet200(input_tinyimagenet200, resnet18_tinyimagenet200):\n    model_hard = HardNBDT(dataset=\'TinyImagenet200\', model=resnet18_tinyimagenet200, hierarchy=\'induced\')\n    model_hard(input_tinyimagenet200)\n'"
tests/test_train.py,4,"b'""""""Tests that train utilities work as advertised""""""\n\nimport torch\nimport torch.nn as nn\nfrom nbdt.loss import SoftTreeSupLoss, HardTreeSupLoss\nfrom nbdt.model import HardNBDT\n\n\ndef test_criterion_cifar10(criterion, label_cifar10):\n    criterion = SoftTreeSupLoss(dataset=\'CIFAR10\', criterion=criterion, hierarchy=\'induced\')\n    criterion(torch.randn((1, 10)), label_cifar10)\n\n\ndef test_criterion_cifar100(criterion):\n    criterion = SoftTreeSupLoss(dataset=\'CIFAR100\', criterion=criterion, hierarchy=\'induced\')\n    criterion(torch.randn((1, 100)), torch.randint(100, (1,)))\n\n\ndef test_criterion_tinyimagenet200(criterion):\n    criterion = SoftTreeSupLoss(dataset=\'TinyImagenet200\', criterion=criterion, hierarchy=\'induced\')\n    criterion(torch.randn((1, 200)), torch.randint(200, (1,)))\n\n\ndef test_nbdt_gradient_hard(resnet18_cifar10, input_cifar10, label_cifar10, criterion):\n    output_cifar10 = resnet18_cifar10(input_cifar10)\n    assert output_cifar10.requires_grad\n\n    criterion = HardTreeSupLoss(dataset=\'CIFAR10\', criterion=criterion, hierarchy=\'induced\')\n    loss = criterion(output_cifar10, label_cifar10)\n    loss.backward()\n\n\ndef test_nbdt_gradient_soft(resnet18_cifar10, input_cifar10, label_cifar10, criterion):\n    output_cifar10 = resnet18_cifar10(input_cifar10)\n    assert output_cifar10.requires_grad\n\n    criterion = SoftTreeSupLoss(dataset=\'CIFAR10\', criterion=criterion, hierarchy=\'induced\')\n    loss = criterion(output_cifar10, label_cifar10)\n    loss.backward()\n'"
examples/app/api.py,0,"b'""""""Single-file example for serving an NBDT model.\n\nThis functions as a simple single-endpoint API, using flask.\n""""""\n\n\nfrom flask import Flask, flash, request, redirect, url_for, jsonify\nfrom flask_cors import CORS\nfrom nbdt.model import HardNBDT\nfrom nbdt.models import wrn28_10_cifar10\nfrom torchvision import transforms\nfrom nbdt.utils import DATASET_TO_CLASSES, load_image_from_path, maybe_install_wordnet\nfrom werkzeug.utils import secure_filename\nfrom PIL import Image\nimport os\n\nmaybe_install_wordnet()\napp = Flask(__name__)\napp.config[\'SECRET_KEY\'] = os.urandom(24)\n\nCORS(app)\n\n\nALLOWED_EXTENSIONS = {\'png\', \'jpg\', \'jpeg\'}\n\n\ndef inference(im):\n    # load pretrained NBDT\n    model = wrn28_10_cifar10()\n    model = HardNBDT(\n      pretrained=True,\n      dataset=\'CIFAR10\',\n      arch=\'wrn28_10_cifar10\',\n      model=model)\n\n    # load + transform image\n    transform = transforms.Compose([\n      transforms.Resize(32),\n      transforms.CenterCrop(32),\n      transforms.ToTensor(),\n      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n    x = transform(im)[None]\n\n    # run inference\n    outputs, decisions = model.forward_with_decisions(x)  # use `model(x)` to obtain just logits\n    _, predicted = outputs.max(1)\n    return {\n        \'success\': True,\n        \'prediction\': DATASET_TO_CLASSES[\'CIFAR10\'][predicted[0]],\n        \'decisions\': [{\n            \'name\': info[\'name\'],\n            \'prob\': info[\'prob\']\n        } for info in decisions[0]]\n    }\n\n\ndef allowed_file(filename):\n    return \'.\' in filename and \\\n        filename.rsplit(\'.\', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\nimage_urls = {\n    \'cat\': \'https://images.pexels.com/photos/126407/pexels-photo-126407.jpeg?auto=compress&cs=tinysrgb&h=32\',\n    \'bear\': \'https://images.pexels.com/photos/158109/kodiak-brown-bear-adult-portrait-wildlife-158109.jpeg?auto=compress&cs=tinysrgb&h=32\',\n    \'dog\': \'https://images.pexels.com/photos/1490908/pexels-photo-1490908.jpeg?auto=compress&cs=tinysrgb&h=32\',\n}\n\n\n@app.route(\'/\', methods=[\'GET\', \'POST\'])\ndef upload_file():\n    """"""\n    To use this endpoint. You may use ANY of the following:\n\n        1. POST a URL with name ""url"", or\n        2. call this page with a query param ""url"", or\n        3. POST a file with name ""file"" to this URL\n\n    Note that the ordering above is the order of priority. If a URL is posted,\n    the uploaded file and the query param will be ignored.\n    """"""\n    if request.method == \'POST\' or request.args.get(\'url\', None):\n        url = request.form.get(\'url\', request.args.get(\'url\', None))\n        if url:\n            im = load_image_from_path(url)\n            return jsonify(inference(im))\n        # check if the post request has the file part\n        if \'file\' not in request.files:\n            return jsonify({\n                \'success\': False,\n                \'message\': \'No file part\'\n            })\n        file = request.files[\'file\']\n        # if user does not select file, browser also\n        # submit an empty part without filename\n        if file.filename == \'\':\n            return jsonify({\n                \'sucess\': False,\n                \'message\': \'No selected file\'\n            })\n        if file and allowed_file(file.filename):\n            im = Image.open(file.stream)\n            return jsonify(inference(im))\n        return jsonify({\n            \'success\': False,\n            \'message\': f\'nope. Allowed file? ({file.filename}) Got a file? ({bool(file)})\'\n        })\n    return jsonify({\n        \'success\': False,\n        \'message\': \'You might be looking for the main page. Please see <a href=""http://nbdt.alvinwan.com/demo"">nbdt.alvinwan.com/demo</a>. Here are some sample URLs you can use:\',\n        \'image_urls\': image_urls\n    })\n\n\nif __name__ == \'__main__\':\n    app.run()\n'"
nbdt/data/__init__.py,0,b'from .custom import *\nfrom .imagenet import *\nfrom torchvision.datasets import *\n'
nbdt/data/custom.py,3,"b'import torchvision.datasets as datasets\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom collections import defaultdict\nfrom nbdt.utils import DATASET_TO_NUM_CLASSES, DATASETS\nfrom collections import defaultdict\nfrom nbdt.graph import get_wnids, read_graph, get_leaves, get_non_leaves, \\\n    FakeSynset, get_leaf_to_path, wnid_to_synset, wnid_to_name\nfrom . import imagenet\nimport torch.nn as nn\nimport random\n\n\n__all__ = names = (\'CIFAR10IncludeLabels\',\n                   \'CIFAR100IncludeLabels\', \'TinyImagenet200IncludeLabels\',\n                   \'Imagenet1000IncludeLabels\', \'CIFAR10ExcludeLabels\',\n                   \'CIFAR100ExcludeLabels\', \'TinyImagenet200ExcludeLabels\',\n                   \'Imagenet1000ExcludeLabels\', \'CIFAR10ResampleLabels\',\n                   \'CIFAR100ResampleLabels\', \'TinyImagenet200ResampleLabels\',\n                   \'Imagenet1000ResampleLabels\')\nkeys = (\'include_labels\', \'exclude_labels\', \'include_classes\', \'probability_labels\')\n\n\ndef add_arguments(parser):\n    parser.add_argument(\'--probability-labels\', nargs=\'*\', type=float)\n    parser.add_argument(\'--include-labels\', nargs=\'*\', type=int)\n    parser.add_argument(\'--exclude-labels\', nargs=\'*\', type=int)\n    parser.add_argument(\'--include-classes\', nargs=\'*\', type=int)\n\n\ndef dataset_to_dummy_classes(dataset):\n    assert dataset in DATASETS\n    num_classes = DATASET_TO_NUM_CLASSES[dataset]\n    return [FakeSynset.create_from_offset(i).wnid for i in range(num_classes)]\n\n\nclass Node:\n\n    def __init__(self, wnid, classes, path_graph, path_wnids, other_class=False):\n        self.path_graph = path_graph\n        self.path_wnids = path_wnids\n\n        self.wnid = wnid\n        self.wnids = get_wnids(path_wnids)\n        self.G = read_graph(path_graph)\n        self.synset = wnid_to_synset(wnid)\n\n        self.original_classes = classes\n        self.num_original_classes = len(self.wnids)\n\n        assert not self.is_leaf(), \'Cannot build dataset for leaf\'\n        self.has_other = other_class and not (self.is_root() or self.is_leaf())\n        self.num_children = len(self.get_children())\n        self.num_classes = self.num_children + int(self.has_other)\n\n        self.old_to_new_classes, self.new_to_old_classes = \\\n            self.build_class_mappings()\n        self.classes = self.build_classes()\n\n        assert len(self.classes) == self.num_classes, (\n            f\'Number of classes {self.num_classes} does not equal number of \'\n            f\'class names found ({len(self.classes)}): {self.classes}\'\n        )\n\n        self.children = list(self.get_children())\n        self.leaves = list(self.get_leaves())\n        self.num_leaves = len(self.leaves)\n\n        self._probabilities = None\n        self._class_weights = None\n\n    def wnid_to_class_index(self, wnid):\n        return self.wnids.index(wnid)\n\n    def get_parents(self):\n        return self.G.pred[self.wnid]\n\n    def get_children(self):\n        return self.G.succ[self.wnid]\n\n    def get_leaves(self):\n        return get_leaves(self.G, self.wnid)\n\n    def is_leaf(self):\n        return len(self.get_children()) == 0\n\n    def is_root(self):\n        return len(self.get_parents()) == 0\n\n    def build_class_mappings(self):\n        old_to_new = defaultdict(lambda: [])\n        new_to_old = defaultdict(lambda: [])\n        for new_index, child in enumerate(self.get_children()):\n            for leaf in get_leaves(self.G, child):\n                old_index = self.wnid_to_class_index(leaf)\n                old_to_new[old_index].append(new_index)\n                new_to_old[new_index].append(old_index)\n        if not self.has_other:\n            return old_to_new, new_to_old\n\n        new_index = self.num_children\n        for old in range(self.num_original_classes):\n            if old not in old_to_new:\n                old_to_new[old].append(new_index)\n                new_to_old[new_index].append(old)\n        return old_to_new, new_to_old\n\n    def build_classes(self):\n        return [\n            \',\'.join([self.original_classes[old] for old in old_indices])\n            for new_index, old_indices in sorted(\n                self.new_to_old_classes.items(), key=lambda t: t[0])\n        ]\n\n    @property\n    def class_counts(self):\n        """"""Number of old classes in each new class""""""\n        return [len(old_indices) for old_indices in self.new_to_old_classes]\n\n    @property\n    def probabilities(self):\n        """"""Calculates probability of training on the ith class.\n\n        If the class contains more than `resample_threshold` samples, the\n        probability is lower, as it is likely to cause severe class imbalance\n        issues.\n        """"""\n        if self._probabilities is None:\n            reference = min(self.class_counts)\n            self._probabilities = torch.Tensor([\n                min(1, reference / len(old_indices))\n                for old_indices in self.new_to_old_classes\n            ])\n        return self._probabilities\n\n    @probabilities.setter\n    def probabilities(self, probabilities):\n        self._probabilities = probabilities\n\n    @property\n    def class_weights(self):\n        if self._class_weights is None:\n            self._class_weights = self.probabilities\n        return self._class_weights\n\n    @class_weights.setter\n    def class_weights(self, class_weights):\n        self._class_weights = class_weights\n\n    @staticmethod\n    def get_wnid_to_node(path_graph, path_wnids, classes):\n        wnid_to_node = {}\n        G = read_graph(path_graph)\n        for wnid in get_non_leaves(G):\n            wnid_to_node[wnid] = Node(\n                wnid, classes, path_graph=path_graph, path_wnids=path_wnids)\n        return wnid_to_node\n\n    @staticmethod\n    def get_nodes(path_graph, path_wnids, classes):\n        wnid_to_node = Node.get_wnid_to_node(path_graph, path_wnids, classes)\n        wnids = sorted(wnid_to_node)\n        nodes = [wnid_to_node[wnid] for wnid in wnids]\n        return nodes\n\n    @staticmethod\n    def get_leaf_to_path(nodes):\n        node = nodes[0]\n        leaf_to_path = get_leaf_to_path(node.G)\n        wnid_to_node = {node.wnid: node for node in nodes}\n        leaf_to_path_nodes = {}\n        for leaf in leaf_to_path:\n            leaf_to_path_nodes[leaf] = [\n                {\n                    \'node\': wnid_to_node.get(wnid, None),\n                    \'name\': wnid_to_name(wnid)\n                }\n                for wnid in leaf_to_path[leaf]\n            ]\n        return leaf_to_path_nodes\n\n    @staticmethod\n    def get_root_node_wnid(path_graph):\n        raise UserWarning(\'Root node may have wnid now\')\n        tree = ET.parse(path_graph)\n        for node in tree.iter():\n            wnid = node.get(\'wnid\')\n            if wnid is not None:\n                return wnid\n        return None\n\n    @staticmethod\n    def dim(nodes):\n        return sum([node.num_classes for node in nodes])\n\n\nclass ResampleLabelsDataset(Dataset):\n    """"""\n    Dataset that includes only the labels provided, with a limited number of\n    samples. Note that labels are integers in [0, k) for a k-class dataset.\n\n    :drop_classes bool: Modifies the dataset so that it is only a m-way\n                        classification where m of k classes are kept. Otherwise,\n                        the problem is still k-way.\n    """"""\n\n    accepts_probability_labels = True\n\n    def __init__(self, dataset, probability_labels=1, drop_classes=False, seed=0):\n        self.dataset = dataset\n        self.classes = dataset.classes\n        self.labels = list(range(len(self.classes)))\n        self.probability_labels = self.get_probability_labels(dataset, probability_labels)\n\n        self.drop_classes = drop_classes\n        if self.drop_classes:\n            self.classes, self.labels = self.get_classes_after_drop(\n                dataset, probability_labels)\n\n        assert self.labels, \'No labels are included in `include_labels`\'\n\n        self.new_to_old = self.build_index_mapping(seed=seed)\n\n    def get_probability_labels(self, dataset, ps):\n        if not isinstance(ps, (tuple, list)):\n            return [ps] * len(dataset.classes)\n        if len(ps) == 1:\n            return ps * len(dataset.classes)\n        assert len(ps) == len(dataset.classes), (\n            f\'Length of probabilities vector {len(ps)} must equal that of the \'\n            f\'dataset classes {len(dataset.classes)}.\'\n        )\n        return ps\n\n    def apply_drop(self, dataset, ps):\n        classes = [\n            cls for p, cls in zip(ps, dataset.classes)\n            if p > 0\n        ]\n        labels = [i for p, i in zip(ps, range(len(dataset.classes))) if p > 0]\n        return classes, labels\n\n    def build_index_mapping(self, seed=0):\n        """"""Iterates over all samples in dataset.\n\n        Remaps all to-be-included samples to [0, n) where n is the number of\n        samples with a class in the whitelist.\n\n        Additionally, the outputted list is truncated to match the number of\n        desired samples.\n        """"""\n        random.seed(seed)\n\n        new_to_old = []\n        for old, (_, label) in enumerate(self.dataset):\n            if random.random() < self.probability_labels[label]:\n                new_to_old.append(old)\n        return new_to_old\n\n    def __getitem__(self, index_new):\n        index_old = self.new_to_old[index_new]\n        sample, label_old = self.dataset[index_old]\n\n        label_new = label_old\n        if self.drop_classes:\n            label_new = self.include_labels.index(label_old)\n\n        return sample, label_new\n\n    def __len__(self):\n        return len(self.new_to_old)\n\n\nclass IncludeLabelsDataset(ResampleLabelsDataset):\n\n    accepts_include_labels = True\n    accepts_probability_labels = False\n\n    def __init__(self, dataset, include_labels=(0,)):\n        super().__init__(dataset, probability_labels=[\n            int(cls in include_labels) for cls in range(len(dataset.classes))\n        ])\n\n\nclass CIFAR10ResampleLabels(ResampleLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', probability_labels=1, **kwargs):\n        super().__init__(\n            dataset=datasets.CIFAR10(*args, root=root, **kwargs),\n            probability_labels=probability_labels)\n\n\nclass CIFAR100ResampleLabels(ResampleLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', probability_labels=1, **kwargs):\n        super().__init__(\n            dataset=datasets.CIFAR100(*args, root=root, **kwargs),\n            probability_labels=probability_labels)\n\n\nclass TinyImagenet200ResampleLabels(ResampleLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', probability_labels=1, **kwargs):\n        super().__init__(\n            dataset=imagenet.TinyImagenet200(*args, root=root, **kwargs),\n            probability_labels=probability_labels)\n\n\nclass Imagenet1000ResampleLabels(ResampleLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', probability_labels=1, **kwargs):\n        super().__init__(\n            dataset=imagenet.Imagenet1000(*args, root=root, **kwargs),\n            probability_labels=probability_labels)\n\n\nclass IncludeClassesDataset(IncludeLabelsDataset):\n    """"""\n    Dataset that includes only the labels provided, with a limited number of\n    samples. Note that classes are strings, like \'cat\' or \'dog\'.\n    """"""\n\n    accepts_include_labels = False\n    accepts_include_classes = True\n\n    def __init__(self, dataset, include_classes=()):\n        super().__init__(dataset, include_labels=[\n                dataset.classes.index(cls) for cls in include_classes\n            ])\n\n\nclass CIFAR10IncludeLabels(IncludeLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', include_labels=(0,), **kwargs):\n        super().__init__(\n            dataset=datasets.CIFAR10(*args, root=root, **kwargs),\n            include_labels=include_labels)\n\n\nclass CIFAR100IncludeLabels(IncludeLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', include_labels=(0,), **kwargs):\n        super().__init__(\n            dataset=datasets.CIFAR100(*args, root=root, **kwargs),\n            include_labels=include_labels)\n\n\nclass TinyImagenet200IncludeLabels(IncludeLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', include_labels=(0,), **kwargs):\n        super().__init__(\n            dataset=imagenet.TinyImagenet200(*args, root=root, **kwargs),\n            include_labels=include_labels)\n\n\nclass Imagenet1000IncludeLabels(IncludeLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', include_labels=(0,), **kwargs):\n        super().__init__(\n            dataset=imagenet.Imagenet1000(*args, root=root, **kwargs),\n            include_labels=include_labels)\n\n\nclass ExcludeLabelsDataset(IncludeLabelsDataset):\n\n    accepts_include_labels = False\n    accepts_exclude_labels = True\n\n    def __init__(self, dataset, exclude_labels=(0,)):\n        k = len(dataset.classes)\n        include_labels = set(range(k)) - set(exclude_labels)\n        super().__init__(\n            dataset=dataset,\n            include_labels=include_labels)\n\n\nclass CIFAR10ExcludeLabels(ExcludeLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', exclude_labels=(0,), **kwargs):\n        super().__init__(\n            dataset=datasets.CIFAR10(*args, root=root, **kwargs),\n            exclude_labels=exclude_labels)\n\n\nclass CIFAR100ExcludeLabels(ExcludeLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', exclude_labels=(0,), **kwargs):\n        super().__init__(\n            dataset=datasets.CIFAR100(*args, root=root, **kwargs),\n            exclude_labels=exclude_labels)\n\n\nclass TinyImagenet200ExcludeLabels(ExcludeLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', exclude_labels=(0,), **kwargs):\n        super().__init__(\n            dataset=imagenet.TinyImagenet200(*args, root=root, **kwargs),\n            exclude_labels=exclude_labels)\n\n\nclass Imagenet1000ExcludeLabels(ExcludeLabelsDataset):\n\n    def __init__(self, *args, root=\'./data\', exclude_labels=(0,), **kwargs):\n        super().__init__(\n            dataset=imagenet.Imagenet1000(*args, root=root, **kwargs),\n            exclude_labels=exclude_labels)\n'"
nbdt/data/imagenet.py,1,"b'import os\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nfrom pathlib import Path\nimport zipfile\nimport urllib.request\nimport shutil\nimport time\n\n\n\n__all__ = names = (\'TinyImagenet200\', \'Imagenet1000\',)\n\n\nclass TinyImagenet200(Dataset):\n    """"""Tiny imagenet dataloader""""""\n\n    url = \'http://cs231n.stanford.edu/tiny-imagenet-200.zip\'\n\n    dataset = None\n\n    def __init__(self, root=\'./data\',\n            *args, train=True, download=False, **kwargs):\n        super().__init__()\n\n        if download:\n            self.download(root=root)\n        dataset = _TinyImagenet200Train if train else _TinyImagenet200Val\n        self.root = root\n        self.dataset = dataset(root, *args, **kwargs)\n        self.classes = self.dataset.classes\n        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n\n    @staticmethod\n    def transform_train(input_size=64):\n        return transforms.Compose([\n            transforms.RandomCrop(input_size, padding=8),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n        ])\n\n    @staticmethod\n    def transform_val(input_size=-1):\n        return transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n        ])\n\n    def download(self, root=\'./\'):\n        """"""Download and unzip Imagenet200 files in the `root` directory.""""""\n        dir = os.path.join(root, \'tiny-imagenet-200\')\n        dir_train = os.path.join(dir, \'train\')\n        if os.path.exists(dir) and os.path.exists(dir_train):\n            print(\'==> Already downloaded.\')\n            return\n\n        path = Path(os.path.join(root, \'tiny-imagenet-200.zip\'))\n        if not os.path.exists(path):\n            os.makedirs(path.parent, exist_ok=True)\n\n            print(\'==> Downloading TinyImagenet200...\')\n            with urllib.request.urlopen(self.url) as response, \\\n                    open(str(path), \'wb\') as out_file:\n                shutil.copyfileobj(response, out_file)\n\n        print(\'==> Extracting TinyImagenet200...\')\n        with zipfile.ZipFile(str(path)) as zf:\n            zf.extractall(root)\n\n    def __getitem__(self, i):\n        return self.dataset[i]\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nclass _TinyImagenet200Train(datasets.ImageFolder):\n\n    def __init__(self, root=\'./data\', *args, **kwargs):\n        super().__init__(os.path.join(root, \'tiny-imagenet-200/train\'), *args, **kwargs)\n\n\nclass _TinyImagenet200Val(datasets.ImageFolder):\n\n    def __init__(self, root=\'./data\', *args, **kwargs):\n        super().__init__(os.path.join(root, \'tiny-imagenet-200/val\'), *args, **kwargs)\n\n        self.path_to_class = {}\n        with open(os.path.join(self.root, \'val_annotations.txt\')) as f:\n            for line in f.readlines():\n                parts = line.split()\n                path = os.path.join(self.root, \'images\', parts[0])\n                self.path_to_class[path] = parts[1]\n\n        self.classes = list(sorted(set(self.path_to_class.values())))\n        self.class_to_idx = {\n            label: self.classes.index(label) for label in self.classes\n        }\n\n    def __getitem__(self, i):\n        sample, _ = super().__getitem__(i)\n        path, _ = self.samples[i]\n        label = self.path_to_class[path]\n        target = self.class_to_idx[label]\n        return sample, target\n\n    def __len__(self):\n        return super().__len__()\n\n\nclass Imagenet1000(Dataset):\n    """"""ImageNet dataloader""""""\n\n    dataset = None\n\n    def __init__(self, root=\'./data\',\n            *args, train=True, download=False, **kwargs):\n        super().__init__()\n\n        if download:\n            self.download(root=root)\n        dataset = _Imagenet1000Train if train else _Imagenet1000Val\n        self.root = root\n        self.dataset = dataset(root, *args, **kwargs)\n        self.classes = self.dataset.classes\n        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n\n    def download(self, root=\'./\'):\n        dir = os.path.join(root, \'imagenet-1000\')\n        dir_train = os.path.join(dir, \'train\')\n        if os.path.exists(dir) and os.path.exists(dir_train):\n            print(\'==> Already downloaded.\')\n            return\n\n        msg = (""Please symlink existing ImageNet dataset rather than downloading."")\n        raise RuntimeError(msg)\n\n    @staticmethod\n    def transform_train(input_size=224):\n        return transforms.Compose([\n            transforms.RandomResizedCrop(input_size),  # TODO: may need updating\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ])\n\n    @staticmethod\n    def transform_val(input_size=224):\n        return transforms.Compose([\n            transforms.Resize(input_size + 32),\n            transforms.CenterCrop(input_size),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ])\n\n    def __getitem__(self, i):\n        return self.dataset[i]\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nclass _Imagenet1000Train(datasets.ImageFolder):\n\n    def __init__(self, root=\'./data\', *args, **kwargs):\n        super().__init__(os.path.join(root, \'imagenet-1000/train\'), *args, **kwargs)\n\n\nclass _Imagenet1000Val(datasets.ImageFolder):\n\n    def __init__(self, root=\'./data\', *args, **kwargs):\n        super().__init__(os.path.join(root, \'imagenet-1000/val\'), *args, **kwargs)\n'"
nbdt/models/__init__.py,0,"b""from .resnet import *\nfrom .wideresnet import *\nfrom pytorchcv.models.efficientnet import *\nfrom torchvision.models import *\n\n\ndef get_model_choices():\n    from types import ModuleType\n\n    for key, value in globals().items():\n        if not key.startswith('__') and not isinstance(value, ModuleType) and callable(value):\n            yield key\n"""
nbdt/models/resnet.py,3,"b""'''ResNet in PyTorch.\n\nFor Pre-activation ResNet, see 'preact_resnet.py'.\n\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom nbdt.models.utils import get_pretrained_model\n\n\n__all__ = ('ResNet10', 'ResNet18', 'ResNet34', 'ResNet50', 'ResNet101', 'ResNet152')\n\n\nmodel_urls = {\n    ('ResNet10', 'CIFAR10'): 'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-CIFAR10-ResNet10.pth',\n    ('ResNet10', 'CIFAR100'): 'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-CIFAR100-ResNet10.pth',\n    ('ResNet18', 'CIFAR10'): 'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-CIFAR10-ResNet18.pth',\n    ('ResNet18', 'CIFAR100'): 'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-CIFAR100-ResNet18.pth',\n    ('ResNet18', 'TinyImagenet200'): 'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-TinyImagenet200-ResNet18.pth'\n}\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def featurize(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, out.size()[2:])  # global average pooling\n        out = out.view(out.size(0), -1)\n        return out\n\n    def forward(self, x):\n        out = self.featurize(x)\n        out = self.linear(out)\n        return out\n\ndef _ResNet(arch, *args, pretrained=False, progress=True, dataset='CIFAR10', **kwargs):\n    model = ResNet(*args, **kwargs)\n    model = get_pretrained_model(arch, dataset, model, model_urls,\n        pretrained=pretrained, progress=progress)\n    return model\n\ndef ResNet10(pretrained=False, progress=True, **kwargs):\n    return _ResNet('ResNet10', BasicBlock, [1,1,1,1],\n        pretrained=pretrained, progress=progress, **kwargs)\n\ndef ResNet18(pretrained=False, progress=True, **kwargs):\n    return _ResNet('ResNet18', BasicBlock, [2,2,2,2],\n        pretrained=pretrained, progress=progress, **kwargs)\n\ndef ResNet34(pretrained=False, progress=True, **kwargs):\n    return _ResNet('ResNet34', BasicBlock, [3,4,6,3],\n        pretrained=pretrained, progress=progress, **kwargs)\n\ndef ResNet50(pretrained=False, progress=True, **kwargs):\n    return _ResNet('ResNet50', Bottleneck, [3,4,6,3],\n        pretrained=pretrained, progress=progress, **kwargs)\n\ndef ResNet101(pretrained=False, progress=True, **kwargs):\n    return _ResNet('ResNet101', Bottleneck, [3,4,23,3],\n        pretrained=pretrained, progress=progress, **kwargs)\n\ndef ResNet152(pretrained=False, progress=True, **kwargs):\n    return _ResNet('ResNet152', Bottleneck, [3,8,36,3],\n        pretrained=pretrained, progress=progress, **kwargs)\n\n\ndef test():\n    net = ResNet18()\n    y = net(torch.randn(1,3,32,32))\n    print(y.size())\n\n# test()\n"""
nbdt/models/utils.py,2,"b""from torch.hub import load_state_dict_from_url\nfrom pathlib import Path\nimport torch\n\n\ndef get_pretrained_model(\n        arch, dataset, model, model_urls,\n        pretrained=False,\n        progress=True,\n        root='.cache/torch/checkpoints'):\n    if pretrained:\n        state_dict = load_state_dict_from_key(\n            [(arch, dataset)], model_urls, pretrained, progress, root,\n            device=get_model_device(model))\n        state_dict = coerce_state_dict(state_dict, model.state_dict())\n        model.load_state_dict(state_dict)\n    return model\n\ndef coerce_state_dict(state_dict, reference_state_dict):\n    if 'net' in state_dict:\n        state_dict = state_dict['net']\n    has_reference_module = list(reference_state_dict)[0].startswith('module.')\n    has_module = list(state_dict)[0].startswith('module.')\n    if not has_reference_module and has_module:\n        state_dict = {\n            key.replace('module.', '', 1): value\n            for key, value in state_dict.items()\n        }\n    elif has_reference_module and not has_module:\n        state_dict = {\n            'module.' + key: value\n            for key, value in state_dict.items()\n        }\n    return state_dict\n\ndef get_model_device(model):\n    return next(model.parameters()).device\n\ndef load_state_dict_from_key(\n        keys, model_urls,\n        pretrained=False,\n        progress=True,\n        root='.cache/torch/checkpoints',\n        device='cpu'):\n    valid_keys = [key for key in keys if key in model_urls]\n    if not valid_keys:\n        raise UserWarning(\n            f'None of the keys {keys} correspond to a pretrained model.'\n        )\n    return load_state_dict_from_url(\n        model_urls[valid_keys[-1]],\n        Path.home() / root,\n        progress=progress,\n        check_hash=False,\n        map_location=torch.device(device))\n"""
nbdt/models/wideresnet.py,1,"b'from pytorchcv.models.wrn_cifar import wrn28_10_cifar10, wrn28_10_cifar100, get_wrn_cifar\nfrom nbdt.models.utils import get_pretrained_model\nimport torch.nn as nn\n\n\n__all__ = (\'wrn28_10\', \'wrn28_10_cifar10\', \'wrn28_10_cifar100\')\n\n\nmodel_urls = {\n    (\'wrn28_10\', \'TinyImagenet200\'): \'https://github.com/alvinwan/neural-backed-decision-trees/releases/download/0.0.1/ckpt-TinyImagenet200-wrn28_10.pth\'\n}\n\n\ndef _wrn(arch, model, pretrained=False, progress=True, dataset=\'CIFAR10\'):\n    model = get_pretrained_model(arch, dataset, model, model_urls,\n        pretrained=pretrained, progress=progress)\n    return model\n\n\ndef wrn28_10(pretrained=False, progress=True, dataset=\'CIFAR10\', **kwargs):\n    """"""Replace `final_pool` (8x8 average pooling) with a global average pooling.\n\n    If this gets crappy accuracy for TinyImagenet200, it\'s probably because the\n    final pooled feature map is 16x16 instead of 8x8. So needs another stride 2\n    stage, technically.\n    """"""\n    model = get_wrn_cifar(blocks=28, width_factor=10, model_name=""wrn28_10"", **kwargs)\n    model.features.final_pool = nn.AdaptiveAvgPool2d((1, 1))\n    model = _wrn(\'wrn28_10\', model, pretrained=pretrained, progress=progress, dataset=dataset)\n    return model\n'"
