file_path,api_count,code
eval.py,10,"b'""""""Adapted from:\n    @longcw faster_rcnn_pytorch: https://github.com/longcw/faster_rcnn_pytorch\n    @rbgirshick py-faster-rcnn https://github.com/rbgirshick/py-faster-rcnn\n    Licensed under The MIT License [see LICENSE for details]\n""""""\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nfrom data import VOC_ROOT, VOCAnnotationTransform, VOCDetection, BaseTransform\nfrom data import VOC_CLASSES as labelmap\nimport torch.utils.data as data\n\nfrom ssd import build_ssd\n\nimport sys\nimport os\nimport time\nimport argparse\nimport numpy as np\nimport pickle\nimport cv2\n\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\n\ndef str2bool(v):\n    return v.lower() in (""yes"", ""true"", ""t"", ""1"")\n\n\nparser = argparse.ArgumentParser(\n    description=\'Single Shot MultiBox Detector Evaluation\')\nparser.add_argument(\'--trained_model\',\n                    default=\'weights/ssd300_mAP_77.43_v2.pth\', type=str,\n                    help=\'Trained state_dict file path to open\')\nparser.add_argument(\'--save_folder\', default=\'eval/\', type=str,\n                    help=\'File path to save results\')\nparser.add_argument(\'--confidence_threshold\', default=0.01, type=float,\n                    help=\'Detection confidence threshold\')\nparser.add_argument(\'--top_k\', default=5, type=int,\n                    help=\'Further restrict the number of predictions to parse\')\nparser.add_argument(\'--cuda\', default=True, type=str2bool,\n                    help=\'Use cuda to train model\')\nparser.add_argument(\'--voc_root\', default=VOC_ROOT,\n                    help=\'Location of VOC root directory\')\nparser.add_argument(\'--cleanup\', default=True, type=str2bool,\n                    help=\'Cleanup and remove results files following eval\')\n\nargs = parser.parse_args()\n\nif not os.path.exists(args.save_folder):\n    os.mkdir(args.save_folder)\n\nif torch.cuda.is_available():\n    if args.cuda:\n        torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n    if not args.cuda:\n        print(""WARNING: It looks like you have a CUDA device, but aren\'t using \\\n              CUDA.  Run with --cuda for optimal eval speed."")\n        torch.set_default_tensor_type(\'torch.FloatTensor\')\nelse:\n    torch.set_default_tensor_type(\'torch.FloatTensor\')\n\nannopath = os.path.join(args.voc_root, \'VOC2007\', \'Annotations\', \'%s.xml\')\nimgpath = os.path.join(args.voc_root, \'VOC2007\', \'JPEGImages\', \'%s.jpg\')\nimgsetpath = os.path.join(args.voc_root, \'VOC2007\', \'ImageSets\',\n                          \'Main\', \'{:s}.txt\')\nYEAR = \'2007\'\ndevkit_path = args.voc_root + \'VOC\' + YEAR\ndataset_mean = (104, 117, 123)\nset_type = \'test\'\n\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n\n\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text) - 1,\n                              int(bbox.find(\'ymin\').text) - 1,\n                              int(bbox.find(\'xmax\').text) - 1,\n                              int(bbox.find(\'ymax\').text) - 1]\n        objects.append(obj_struct)\n\n    return objects\n\n\ndef get_output_dir(name, phase):\n    """"""Return the directory where experimental artifacts are placed.\n    If the directory does not exist, it is created.\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    """"""\n    filedir = os.path.join(name, phase)\n    if not os.path.exists(filedir):\n        os.makedirs(filedir)\n    return filedir\n\n\ndef get_voc_results_file_template(image_set, cls):\n    # VOCdevkit/VOC2007/results/det_test_aeroplane.txt\n    filename = \'det_\' + image_set + \'_%s.txt\' % (cls)\n    filedir = os.path.join(devkit_path, \'results\')\n    if not os.path.exists(filedir):\n        os.makedirs(filedir)\n    path = os.path.join(filedir, filename)\n    return path\n\n\ndef write_voc_results_file(all_boxes, dataset):\n    for cls_ind, cls in enumerate(labelmap):\n        print(\'Writing {:s} VOC results file\'.format(cls))\n        filename = get_voc_results_file_template(set_type, cls)\n        with open(filename, \'wt\') as f:\n            for im_ind, index in enumerate(dataset.ids):\n                dets = all_boxes[cls_ind+1][im_ind]\n                if dets == []:\n                    continue\n                # the VOCdevkit expects 1-based indices\n                for k in range(dets.shape[0]):\n                    f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                            format(index[1], dets[k, -1],\n                                   dets[k, 0] + 1, dets[k, 1] + 1,\n                                   dets[k, 2] + 1, dets[k, 3] + 1))\n\n\ndef do_python_eval(output_dir=\'output\', use_07=True):\n    cachedir = os.path.join(devkit_path, \'annotations_cache\')\n    aps = []\n    # The PASCAL VOC metric changed in 2010\n    use_07_metric = use_07\n    print(\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n    for i, cls in enumerate(labelmap):\n        filename = get_voc_results_file_template(set_type, cls)\n        rec, prec, ap = voc_eval(\n           filename, annopath, imgsetpath.format(set_type), cls, cachedir,\n           ovthresh=0.5, use_07_metric=use_07_metric)\n        aps += [ap]\n        print(\'AP for {} = {:.4f}\'.format(cls, ap))\n        with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n            pickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n    print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n    print(\'~~~~~~~~\')\n    print(\'Results:\')\n    for ap in aps:\n        print(\'{:.3f}\'.format(ap))\n    print(\'{:.3f}\'.format(np.mean(aps)))\n    print(\'~~~~~~~~\')\n    print(\'\')\n    print(\'--------------------------------------------------------------\')\n    print(\'Results computed with the **unofficial** Python eval code.\')\n    print(\'Results should be very close to the official MATLAB eval code.\')\n    print(\'--------------------------------------------------------------\')\n\n\ndef voc_ap(rec, prec, use_07_metric=True):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:True).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=True):\n    """"""rec, prec, ap = voc_eval(detpath,\n                           annopath,\n                           imagesetfile,\n                           classname,\n                           [ovthresh],\n                           [use_07_metric])\nTop level function that does the PASCAL VOC evaluation.\ndetpath: Path to detections\n   detpath.format(classname) should produce the detection results file.\nannopath: Path to annotations\n   annopath.format(imagename) should be the xml annotations file.\nimagesetfile: Text file containing the list of images, one image per line.\nclassname: Category name (duh)\ncachedir: Directory for caching the annotations\n[ovthresh]: Overlap threshold (default = 0.5)\n[use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n   (default True)\n""""""\n# assumes detections are in detpath.format(classname)\n# assumes annotations are in annopath.format(imagename)\n# assumes imagesetfile is a text file with each line an image name\n# cachedir caches the annotations in a pickle file\n# first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath % (imagename))\n            if i % 100 == 0:\n                print(\'Reading annotation for {:d}/{:d}\'.format(\n                   i + 1, len(imagenames)))\n        # save\n        print(\'Saving cached annotations to {:s}\'.format(cachefile))\n        with open(cachefile, \'wb\') as f:\n            pickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'rb\') as f:\n            recs = pickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n    if any(lines) == 1:\n\n        splitlines = [x.strip().split(\' \') for x in lines]\n        image_ids = [x[0] for x in splitlines]\n        confidence = np.array([float(x[1]) for x in splitlines])\n        BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n        # sort by confidence\n        sorted_ind = np.argsort(-confidence)\n        sorted_scores = np.sort(-confidence)\n        BB = BB[sorted_ind, :]\n        image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n        nd = len(image_ids)\n        tp = np.zeros(nd)\n        fp = np.zeros(nd)\n        for d in range(nd):\n            R = class_recs[image_ids[d]]\n            bb = BB[d, :].astype(float)\n            ovmax = -np.inf\n            BBGT = R[\'bbox\'].astype(float)\n            if BBGT.size > 0:\n                # compute overlaps\n                # intersection\n                ixmin = np.maximum(BBGT[:, 0], bb[0])\n                iymin = np.maximum(BBGT[:, 1], bb[1])\n                ixmax = np.minimum(BBGT[:, 2], bb[2])\n                iymax = np.minimum(BBGT[:, 3], bb[3])\n                iw = np.maximum(ixmax - ixmin, 0.)\n                ih = np.maximum(iymax - iymin, 0.)\n                inters = iw * ih\n                uni = ((bb[2] - bb[0]) * (bb[3] - bb[1]) +\n                       (BBGT[:, 2] - BBGT[:, 0]) *\n                       (BBGT[:, 3] - BBGT[:, 1]) - inters)\n                overlaps = inters / uni\n                ovmax = np.max(overlaps)\n                jmax = np.argmax(overlaps)\n\n            if ovmax > ovthresh:\n                if not R[\'difficult\'][jmax]:\n                    if not R[\'det\'][jmax]:\n                        tp[d] = 1.\n                        R[\'det\'][jmax] = 1\n                    else:\n                        fp[d] = 1.\n            else:\n                fp[d] = 1.\n\n        # compute precision recall\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n        rec = tp / float(npos)\n        # avoid divide by zero in case the first detection matches a difficult\n        # ground truth\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = voc_ap(rec, prec, use_07_metric)\n    else:\n        rec = -1.\n        prec = -1.\n        ap = -1.\n\n    return rec, prec, ap\n\n\ndef test_net(save_folder, net, cuda, dataset, transform, top_k,\n             im_size=300, thresh=0.05):\n    num_images = len(dataset)\n    # all detections are collected into:\n    #    all_boxes[cls][image] = N x 5 array of detections in\n    #    (x1, y1, x2, y2, score)\n    all_boxes = [[[] for _ in range(num_images)]\n                 for _ in range(len(labelmap)+1)]\n\n    # timers\n    _t = {\'im_detect\': Timer(), \'misc\': Timer()}\n    output_dir = get_output_dir(\'ssd300_120000\', set_type)\n    det_file = os.path.join(output_dir, \'detections.pkl\')\n\n    for i in range(num_images):\n        im, gt, h, w = dataset.pull_item(i)\n\n        x = Variable(im.unsqueeze(0))\n        if args.cuda:\n            x = x.cuda()\n        _t[\'im_detect\'].tic()\n        detections = net(x).data\n        detect_time = _t[\'im_detect\'].toc(average=False)\n\n        # skip j = 0, because it\'s the background class\n        for j in range(1, detections.size(1)):\n            dets = detections[0, j, :]\n            mask = dets[:, 0].gt(0.).expand(5, dets.size(0)).t()\n            dets = torch.masked_select(dets, mask).view(-1, 5)\n            if dets.size(0) == 0:\n                continue\n            boxes = dets[:, 1:]\n            boxes[:, 0] *= w\n            boxes[:, 2] *= w\n            boxes[:, 1] *= h\n            boxes[:, 3] *= h\n            scores = dets[:, 0].cpu().numpy()\n            cls_dets = np.hstack((boxes.cpu().numpy(),\n                                  scores[:, np.newaxis])).astype(np.float32,\n                                                                 copy=False)\n            all_boxes[j][i] = cls_dets\n\n        print(\'im_detect: {:d}/{:d} {:.3f}s\'.format(i + 1,\n                                                    num_images, detect_time))\n\n    with open(det_file, \'wb\') as f:\n        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n\n    print(\'Evaluating detections\')\n    evaluate_detections(all_boxes, output_dir, dataset)\n\n\ndef evaluate_detections(box_list, output_dir, dataset):\n    write_voc_results_file(box_list, dataset)\n    do_python_eval(output_dir)\n\n\nif __name__ == \'__main__\':\n    # load net\n    num_classes = len(labelmap) + 1                      # +1 for background\n    net = build_ssd(\'test\', 300, num_classes)            # initialize SSD\n    net.load_state_dict(torch.load(args.trained_model))\n    net.eval()\n    print(\'Finished loading model!\')\n    # load data\n    dataset = VOCDetection(args.voc_root, [(\'2007\', set_type)],\n                           BaseTransform(300, dataset_mean),\n                           VOCAnnotationTransform())\n    if args.cuda:\n        net = net.cuda()\n        cudnn.benchmark = True\n    # evaluation\n    test_net(args.save_folder, net, args.cuda, dataset,\n             BaseTransform(net.size, dataset_mean), args.top_k, 300,\n             thresh=args.confidence_threshold)\n'"
ssd.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom layers import *\nfrom data import voc, coco\nimport os\n\n\nclass SSD(nn.Module):\n    """"""Single Shot Multibox Architecture\n    The network is composed of a base VGG network followed by the\n    added multibox conv layers.  Each multibox layer branches into\n        1) conv2d for class conf scores\n        2) conv2d for localization predictions\n        3) associated priorbox layer to produce default bounding\n           boxes specific to the layer\'s feature map size.\n    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n\n    Args:\n        phase: (string) Can be ""test"" or ""train""\n        size: input image size\n        base: VGG16 layers for input, size of either 300 or 500\n        extras: extra layers that feed to multibox loc and conf layers\n        head: ""multibox head"" consists of loc and conf conv layers\n    """"""\n\n    def __init__(self, phase, size, base, extras, head, num_classes):\n        super(SSD, self).__init__()\n        self.phase = phase\n        self.num_classes = num_classes\n        self.cfg = (coco, voc)[num_classes == 21]\n        self.priorbox = PriorBox(self.cfg)\n        self.priors = Variable(self.priorbox.forward(), volatile=True)\n        self.size = size\n\n        # SSD network\n        self.vgg = nn.ModuleList(base)\n        # Layer learns to scale the l2 normalized features from conv4_3\n        self.L2Norm = L2Norm(512, 20)\n        self.extras = nn.ModuleList(extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n\n        if phase == \'test\':\n            self.softmax = nn.Softmax(dim=-1)\n            self.detect = Detect(num_classes, 0, 200, 0.01, 0.45)\n\n    def forward(self, x):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n        """"""\n        sources = list()\n        loc = list()\n        conf = list()\n\n        # apply vgg up to conv4_3 relu\n        for k in range(23):\n            x = self.vgg[k](x)\n\n        s = self.L2Norm(x)\n        sources.append(s)\n\n        # apply vgg up to fc7\n        for k in range(23, len(self.vgg)):\n            x = self.vgg[k](x)\n        sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = F.relu(v(x), inplace=True)\n            if k % 2 == 1:\n                sources.append(x)\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n        if self.phase == ""test"":\n            output = self.detect(\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(conf.size(0), -1,\n                             self.num_classes)),                # conf preds\n                self.priors.type(type(x.data))                  # default boxes\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n                self.priors\n            )\n        return output\n\n    def load_weights(self, base_file):\n        other, ext = os.path.splitext(base_file)\n        if ext == \'.pkl\' or \'.pth\':\n            print(\'Loading weights into state dict...\')\n            self.load_state_dict(torch.load(base_file,\n                                 map_location=lambda storage, loc: storage))\n            print(\'Finished!\')\n        else:\n            print(\'Sorry only .pth and .pkl files supported.\')\n\n\n# This function is derived from torchvision VGG make_layers()\n# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\ndef vgg(cfg, i, batch_norm=False):\n    layers = []\n    in_channels = i\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == \'C\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    return layers\n\n\ndef add_extras(cfg, i, batch_norm=False):\n    # Extra layers added to VGG for feature scaling\n    layers = []\n    in_channels = i\n    flag = False\n    for k, v in enumerate(cfg):\n        if in_channels != \'S\':\n            if v == \'S\':\n                layers += [nn.Conv2d(in_channels, cfg[k + 1],\n                           kernel_size=(1, 3)[flag], stride=2, padding=1)]\n            else:\n                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n            flag = not flag\n        in_channels = v\n    return layers\n\n\ndef multibox(vgg, extra_layers, cfg, num_classes):\n    loc_layers = []\n    conf_layers = []\n    vgg_source = [21, -2]\n    for k, v in enumerate(vgg_source):\n        loc_layers += [nn.Conv2d(vgg[v].out_channels,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(vgg[v].out_channels,\n                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n    for k, v in enumerate(extra_layers[1::2], 2):\n        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                 * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                  * num_classes, kernel_size=3, padding=1)]\n    return vgg, extra_layers, (loc_layers, conf_layers)\n\n\nbase = {\n    \'300\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512],\n    \'512\': [],\n}\nextras = {\n    \'300\': [256, \'S\', 512, 128, \'S\', 256, 128, 256, 128, 256],\n    \'512\': [],\n}\nmbox = {\n    \'300\': [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n    \'512\': [],\n}\n\n\ndef build_ssd(phase, size=300, num_classes=21):\n    if phase != ""test"" and phase != ""train"":\n        print(""ERROR: Phase: "" + phase + "" not recognized"")\n        return\n    if size != 300:\n        print(""ERROR: You specified size "" + repr(size) + "". However, "" +\n              ""currently only SSD300 (size=300) is supported!"")\n        return\n    base_, extras_, head_ = multibox(vgg(base[str(size)], 3),\n                                     add_extras(extras[str(size)], 1024),\n                                     mbox[str(size)], num_classes)\n    return SSD(phase, size, base_, extras_, head_, num_classes)\n'"
test.py,10,"b'from __future__ import print_function\nimport sys\nimport os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom data import VOC_ROOT, VOC_CLASSES as labelmap\nfrom PIL import Image\nfrom data import VOCAnnotationTransform, VOCDetection, BaseTransform, VOC_CLASSES\nimport torch.utils.data as data\nfrom ssd import build_ssd\n\nparser = argparse.ArgumentParser(description=\'Single Shot MultiBox Detection\')\nparser.add_argument(\'--trained_model\', default=\'weights/ssd_300_VOC0712.pth\',\n                    type=str, help=\'Trained state_dict file path to open\')\nparser.add_argument(\'--save_folder\', default=\'eval/\', type=str,\n                    help=\'Dir to save results\')\nparser.add_argument(\'--visual_threshold\', default=0.6, type=float,\n                    help=\'Final confidence threshold\')\nparser.add_argument(\'--cuda\', default=True, type=bool,\n                    help=\'Use cuda to train model\')\nparser.add_argument(\'--voc_root\', default=VOC_ROOT, help=\'Location of VOC root directory\')\nparser.add_argument(\'-f\', default=None, type=str, help=""Dummy arg so we can load in Jupyter Notebooks"")\nargs = parser.parse_args()\n\nif args.cuda and torch.cuda.is_available():\n    torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\nelse:\n    torch.set_default_tensor_type(\'torch.FloatTensor\')\n\nif not os.path.exists(args.save_folder):\n    os.mkdir(args.save_folder)\n\n\ndef test_net(save_folder, net, cuda, testset, transform, thresh):\n    # dump predictions and assoc. ground truth to text file for now\n    filename = save_folder+\'test1.txt\'\n    num_images = len(testset)\n    for i in range(num_images):\n        print(\'Testing image {:d}/{:d}....\'.format(i+1, num_images))\n        img = testset.pull_image(i)\n        img_id, annotation = testset.pull_anno(i)\n        x = torch.from_numpy(transform(img)[0]).permute(2, 0, 1)\n        x = Variable(x.unsqueeze(0))\n\n        with open(filename, mode=\'a\') as f:\n            f.write(\'\\nGROUND TRUTH FOR: \'+img_id+\'\\n\')\n            for box in annotation:\n                f.write(\'label: \'+\' || \'.join(str(b) for b in box)+\'\\n\')\n        if cuda:\n            x = x.cuda()\n\n        y = net(x)      # forward pass\n        detections = y.data\n        # scale each detection back up to the image\n        scale = torch.Tensor([img.shape[1], img.shape[0],\n                             img.shape[1], img.shape[0]])\n        pred_num = 0\n        for i in range(detections.size(1)):\n            j = 0\n            while detections[0, i, j, 0] >= 0.6:\n                if pred_num == 0:\n                    with open(filename, mode=\'a\') as f:\n                        f.write(\'PREDICTIONS: \'+\'\\n\')\n                score = detections[0, i, j, 0]\n                label_name = labelmap[i-1]\n                pt = (detections[0, i, j, 1:]*scale).cpu().numpy()\n                coords = (pt[0], pt[1], pt[2], pt[3])\n                pred_num += 1\n                with open(filename, mode=\'a\') as f:\n                    f.write(str(pred_num)+\' label: \'+label_name+\' score: \' +\n                            str(score) + \' \'+\' || \'.join(str(c) for c in coords) + \'\\n\')\n                j += 1\n\n\ndef test_voc():\n    # load net\n    num_classes = len(VOC_CLASSES) + 1 # +1 background\n    net = build_ssd(\'test\', 300, num_classes) # initialize SSD\n    net.load_state_dict(torch.load(args.trained_model))\n    net.eval()\n    print(\'Finished loading model!\')\n    # load data\n    testset = VOCDetection(args.voc_root, [(\'2007\', \'test\')], None, VOCAnnotationTransform())\n    if args.cuda:\n        net = net.cuda()\n        cudnn.benchmark = True\n    # evaluation\n    test_net(args.save_folder, net, args.cuda, testset,\n             BaseTransform(net.size, (104, 117, 123)),\n             thresh=args.visual_threshold)\n\nif __name__ == \'__main__\':\n    test_voc()\n'"
train.py,20,"b'from data import *\nfrom utils.augmentations import SSDAugmentation\nfrom layers.modules import MultiBoxLoss\nfrom ssd import build_ssd\nimport os\nimport sys\nimport time\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nimport torch.nn.init as init\nimport torch.utils.data as data\nimport numpy as np\nimport argparse\n\n\ndef str2bool(v):\n    return v.lower() in (""yes"", ""true"", ""t"", ""1"")\n\n\nparser = argparse.ArgumentParser(\n    description=\'Single Shot MultiBox Detector Training With Pytorch\')\ntrain_set = parser.add_mutually_exclusive_group()\nparser.add_argument(\'--dataset\', default=\'VOC\', choices=[\'VOC\', \'COCO\'],\n                    type=str, help=\'VOC or COCO\')\nparser.add_argument(\'--dataset_root\', default=VOC_ROOT,\n                    help=\'Dataset root directory path\')\nparser.add_argument(\'--basenet\', default=\'vgg16_reducedfc.pth\',\n                    help=\'Pretrained base model\')\nparser.add_argument(\'--batch_size\', default=32, type=int,\n                    help=\'Batch size for training\')\nparser.add_argument(\'--resume\', default=None, type=str,\n                    help=\'Checkpoint state_dict file to resume training from\')\nparser.add_argument(\'--start_iter\', default=0, type=int,\n                    help=\'Resume training at this iter\')\nparser.add_argument(\'--num_workers\', default=4, type=int,\n                    help=\'Number of workers used in dataloading\')\nparser.add_argument(\'--cuda\', default=True, type=str2bool,\n                    help=\'Use CUDA to train model\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=1e-3, type=float,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float,\n                    help=\'Momentum value for optim\')\nparser.add_argument(\'--weight_decay\', default=5e-4, type=float,\n                    help=\'Weight decay for SGD\')\nparser.add_argument(\'--gamma\', default=0.1, type=float,\n                    help=\'Gamma update for SGD\')\nparser.add_argument(\'--visdom\', default=False, type=str2bool,\n                    help=\'Use visdom for loss visualization\')\nparser.add_argument(\'--save_folder\', default=\'weights/\',\n                    help=\'Directory for saving checkpoint models\')\nargs = parser.parse_args()\n\n\nif torch.cuda.is_available():\n    if args.cuda:\n        torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n    if not args.cuda:\n        print(""WARNING: It looks like you have a CUDA device, but aren\'t "" +\n              ""using CUDA.\\nRun with --cuda for optimal training speed."")\n        torch.set_default_tensor_type(\'torch.FloatTensor\')\nelse:\n    torch.set_default_tensor_type(\'torch.FloatTensor\')\n\nif not os.path.exists(args.save_folder):\n    os.mkdir(args.save_folder)\n\n\ndef train():\n    if args.dataset == \'COCO\':\n        if args.dataset_root == VOC_ROOT:\n            if not os.path.exists(COCO_ROOT):\n                parser.error(\'Must specify dataset_root if specifying dataset\')\n            print(""WARNING: Using default COCO dataset_root because "" +\n                  ""--dataset_root was not specified."")\n            args.dataset_root = COCO_ROOT\n        cfg = coco\n        dataset = COCODetection(root=args.dataset_root,\n                                transform=SSDAugmentation(cfg[\'min_dim\'],\n                                                          MEANS))\n    elif args.dataset == \'VOC\':\n        if args.dataset_root == COCO_ROOT:\n            parser.error(\'Must specify dataset if specifying dataset_root\')\n        cfg = voc\n        dataset = VOCDetection(root=args.dataset_root,\n                               transform=SSDAugmentation(cfg[\'min_dim\'],\n                                                         MEANS))\n\n    if args.visdom:\n        import visdom\n        viz = visdom.Visdom()\n\n    ssd_net = build_ssd(\'train\', cfg[\'min_dim\'], cfg[\'num_classes\'])\n    net = ssd_net\n\n    if args.cuda:\n        net = torch.nn.DataParallel(ssd_net)\n        cudnn.benchmark = True\n\n    if args.resume:\n        print(\'Resuming training, loading {}...\'.format(args.resume))\n        ssd_net.load_weights(args.resume)\n    else:\n        vgg_weights = torch.load(args.save_folder + args.basenet)\n        print(\'Loading base network...\')\n        ssd_net.vgg.load_state_dict(vgg_weights)\n\n    if args.cuda:\n        net = net.cuda()\n\n    if not args.resume:\n        print(\'Initializing weights...\')\n        # initialize newly added layers\' weights with xavier method\n        ssd_net.extras.apply(weights_init)\n        ssd_net.loc.apply(weights_init)\n        ssd_net.conf.apply(weights_init)\n\n    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum,\n                          weight_decay=args.weight_decay)\n    criterion = MultiBoxLoss(cfg[\'num_classes\'], 0.5, True, 0, True, 3, 0.5,\n                             False, args.cuda)\n\n    net.train()\n    # loss counters\n    loc_loss = 0\n    conf_loss = 0\n    epoch = 0\n    print(\'Loading the dataset...\')\n\n    epoch_size = len(dataset) // args.batch_size\n    print(\'Training SSD on:\', dataset.name)\n    print(\'Using the specified args:\')\n    print(args)\n\n    step_index = 0\n\n    if args.visdom:\n        vis_title = \'SSD.PyTorch on \' + dataset.name\n        vis_legend = [\'Loc Loss\', \'Conf Loss\', \'Total Loss\']\n        iter_plot = create_vis_plot(\'Iteration\', \'Loss\', vis_title, vis_legend)\n        epoch_plot = create_vis_plot(\'Epoch\', \'Loss\', vis_title, vis_legend)\n\n    data_loader = data.DataLoader(dataset, args.batch_size,\n                                  num_workers=args.num_workers,\n                                  shuffle=True, collate_fn=detection_collate,\n                                  pin_memory=True)\n    # create batch iterator\n    batch_iterator = iter(data_loader)\n    for iteration in range(args.start_iter, cfg[\'max_iter\']):\n        if args.visdom and iteration != 0 and (iteration % epoch_size == 0):\n            update_vis_plot(epoch, loc_loss, conf_loss, epoch_plot, None,\n                            \'append\', epoch_size)\n            # reset epoch loss counters\n            loc_loss = 0\n            conf_loss = 0\n            epoch += 1\n\n        if iteration in cfg[\'lr_steps\']:\n            step_index += 1\n            adjust_learning_rate(optimizer, args.gamma, step_index)\n\n        # load train data\n        images, targets = next(batch_iterator)\n\n        if args.cuda:\n            images = Variable(images.cuda())\n            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n        else:\n            images = Variable(images)\n            targets = [Variable(ann, volatile=True) for ann in targets]\n        # forward\n        t0 = time.time()\n        out = net(images)\n        # backprop\n        optimizer.zero_grad()\n        loss_l, loss_c = criterion(out, targets)\n        loss = loss_l + loss_c\n        loss.backward()\n        optimizer.step()\n        t1 = time.time()\n        loc_loss += loss_l.data[0]\n        conf_loss += loss_c.data[0]\n\n        if iteration % 10 == 0:\n            print(\'timer: %.4f sec.\' % (t1 - t0))\n            print(\'iter \' + repr(iteration) + \' || Loss: %.4f ||\' % (loss.data[0]), end=\' \')\n\n        if args.visdom:\n            update_vis_plot(iteration, loss_l.data[0], loss_c.data[0],\n                            iter_plot, epoch_plot, \'append\')\n\n        if iteration != 0 and iteration % 5000 == 0:\n            print(\'Saving state, iter:\', iteration)\n            torch.save(ssd_net.state_dict(), \'weights/ssd300_COCO_\' +\n                       repr(iteration) + \'.pth\')\n    torch.save(ssd_net.state_dict(),\n               args.save_folder + \'\' + args.dataset + \'.pth\')\n\n\ndef adjust_learning_rate(optimizer, gamma, step):\n    """"""Sets the learning rate to the initial LR decayed by 10 at every\n        specified step\n    # Adapted from PyTorch Imagenet example:\n    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n    """"""\n    lr = args.lr * (gamma ** (step))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef xavier(param):\n    init.xavier_uniform(param)\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        xavier(m.weight.data)\n        m.bias.data.zero_()\n\n\ndef create_vis_plot(_xlabel, _ylabel, _title, _legend):\n    return viz.line(\n        X=torch.zeros((1,)).cpu(),\n        Y=torch.zeros((1, 3)).cpu(),\n        opts=dict(\n            xlabel=_xlabel,\n            ylabel=_ylabel,\n            title=_title,\n            legend=_legend\n        )\n    )\n\n\ndef update_vis_plot(iteration, loc, conf, window1, window2, update_type,\n                    epoch_size=1):\n    viz.line(\n        X=torch.ones((1, 3)).cpu() * iteration,\n        Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu() / epoch_size,\n        win=window1,\n        update=update_type\n    )\n    # initialize epoch plot on first iteration\n    if iteration == 0:\n        viz.line(\n            X=torch.zeros((1, 3)).cpu(),\n            Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu(),\n            win=window2,\n            update=True\n        )\n\n\nif __name__ == \'__main__\':\n    train()\n'"
data/__init__.py,2,"b'from .voc0712 import VOCDetection, VOCAnnotationTransform, VOC_CLASSES, VOC_ROOT\n\nfrom .coco import COCODetection, COCOAnnotationTransform, COCO_CLASSES, COCO_ROOT, get_label_map\nfrom .config import *\nimport torch\nimport cv2\nimport numpy as np\n\ndef detection_collate(batch):\n    """"""Custom collate fn for dealing with batches of images that have a different\n    number of associated object annotations (bounding boxes).\n\n    Arguments:\n        batch: (tuple) A tuple of tensor images and lists of annotations\n\n    Return:\n        A tuple containing:\n            1) (tensor) batch of images stacked on their 0 dim\n            2) (list of tensors) annotations for a given image are stacked on\n                                 0 dim\n    """"""\n    targets = []\n    imgs = []\n    for sample in batch:\n        imgs.append(sample[0])\n        targets.append(torch.FloatTensor(sample[1]))\n    return torch.stack(imgs, 0), targets\n\n\ndef base_transform(image, size, mean):\n    x = cv2.resize(image, (size, size)).astype(np.float32)\n    x -= mean\n    x = x.astype(np.float32)\n    return x\n\n\nclass BaseTransform:\n    def __init__(self, size, mean):\n        self.size = size\n        self.mean = np.array(mean, dtype=np.float32)\n\n    def __call__(self, image, boxes=None, labels=None):\n        return base_transform(image, self.size, self.mean), boxes, labels\n'"
data/coco.py,2,"b'from .config import HOME\nimport os\nimport os.path as osp\nimport sys\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport cv2\nimport numpy as np\n\nCOCO_ROOT = osp.join(HOME, \'data/coco/\')\nIMAGES = \'images\'\nANNOTATIONS = \'annotations\'\nCOCO_API = \'PythonAPI\'\nINSTANCES_SET = \'instances_{}.json\'\nCOCO_CLASSES = (\'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\',\n                \'train\', \'truck\', \'boat\', \'traffic light\', \'fire\', \'hydrant\',\n                \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\',\n                \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\',\n                \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\',\n                \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sports ball\',\n                \'kite\', \'baseball bat\', \'baseball glove\', \'skateboard\',\n                \'surfboard\', \'tennis racket\', \'bottle\', \'wine glass\', \'cup\',\n                \'fork\', \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\',\n                \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\',\n                \'donut\', \'cake\', \'chair\', \'couch\', \'potted plant\', \'bed\',\n                \'dining table\', \'toilet\', \'tv\', \'laptop\', \'mouse\', \'remote\',\n                \'keyboard\', \'cell phone\', \'microwave oven\', \'toaster\', \'sink\',\n                \'refrigerator\', \'book\', \'clock\', \'vase\', \'scissors\',\n                \'teddy bear\', \'hair drier\', \'toothbrush\')\n\n\ndef get_label_map(label_file):\n    label_map = {}\n    labels = open(label_file, \'r\')\n    for line in labels:\n        ids = line.split(\',\')\n        label_map[int(ids[0])] = int(ids[1])\n    return label_map\n\n\nclass COCOAnnotationTransform(object):\n    """"""Transforms a COCO annotation into a Tensor of bbox coords and label index\n    Initilized with a dictionary lookup of classnames to indexes\n    """"""\n    def __init__(self):\n        self.label_map = get_label_map(osp.join(COCO_ROOT, \'coco_labels.txt\'))\n\n    def __call__(self, target, width, height):\n        """"""\n        Args:\n            target (dict): COCO target json annotation as a python dict\n            height (int): height\n            width (int): width\n        Returns:\n            a list containing lists of bounding boxes  [bbox coords, class idx]\n        """"""\n        scale = np.array([width, height, width, height])\n        res = []\n        for obj in target:\n            if \'bbox\' in obj:\n                bbox = obj[\'bbox\']\n                bbox[2] += bbox[0]\n                bbox[3] += bbox[1]\n                label_idx = self.label_map[obj[\'category_id\']] - 1\n                final_box = list(np.array(bbox)/scale)\n                final_box.append(label_idx)\n                res += [final_box]  # [xmin, ymin, xmax, ymax, label_idx]\n            else:\n                print(""no bbox problem!"")\n\n        return res  # [[xmin, ymin, xmax, ymax, label_idx], ... ]\n\n\nclass COCODetection(data.Dataset):\n    """"""`MS Coco Detection <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n    Args:\n        root (string): Root directory where images are downloaded to.\n        set_name (string): Name of the specific set of COCO images.\n        transform (callable, optional): A function/transform that augments the\n                                        raw images`\n        target_transform (callable, optional): A function/transform that takes\n        in the target (bbox) and transforms it.\n    """"""\n\n    def __init__(self, root, image_set=\'trainval35k\', transform=None,\n                 target_transform=COCOAnnotationTransform(), dataset_name=\'MS COCO\'):\n        sys.path.append(osp.join(root, COCO_API))\n        from pycocotools.coco import COCO\n        self.root = osp.join(root, IMAGES, image_set)\n        self.coco = COCO(osp.join(root, ANNOTATIONS,\n                                  INSTANCES_SET.format(image_set)))\n        self.ids = list(self.coco.imgToAnns.keys())\n        self.transform = transform\n        self.target_transform = target_transform\n        self.name = dataset_name\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: Tuple (image, target).\n                   target is the object returned by ``coco.loadAnns``.\n        """"""\n        im, gt, h, w = self.pull_item(index)\n        return im, gt\n\n    def __len__(self):\n        return len(self.ids)\n\n    def pull_item(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: Tuple (image, target, height, width).\n                   target is the object returned by ``coco.loadAnns``.\n        """"""\n        img_id = self.ids[index]\n        target = self.coco.imgToAnns[img_id]\n        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n\n        target = self.coco.loadAnns(ann_ids)\n        path = osp.join(self.root, self.coco.loadImgs(img_id)[0][\'file_name\'])\n        assert osp.exists(path), \'Image path does not exist: {}\'.format(path)\n        img = cv2.imread(osp.join(self.root, path))\n        height, width, _ = img.shape\n        if self.target_transform is not None:\n            target = self.target_transform(target, width, height)\n        if self.transform is not None:\n            target = np.array(target)\n            img, boxes, labels = self.transform(img, target[:, :4],\n                                                target[:, 4])\n            # to rgb\n            img = img[:, :, (2, 1, 0)]\n\n            target = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n        return torch.from_numpy(img).permute(2, 0, 1), target, height, width\n\n    def pull_image(self, index):\n        \'\'\'Returns the original image object at index in PIL form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            cv2 img\n        \'\'\'\n        img_id = self.ids[index]\n        path = self.coco.loadImgs(img_id)[0][\'file_name\']\n        return cv2.imread(osp.join(self.root, path), cv2.IMREAD_COLOR)\n\n    def pull_anno(self, index):\n        \'\'\'Returns the original annotation of image at index\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to get annotation of\n        Return:\n            list:  [img_id, [(label, bbox coords),...]]\n                eg: (\'001718\', [(\'dog\', (96, 13, 438, 332))])\n        \'\'\'\n        img_id = self.ids[index]\n        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n        return self.coco.loadAnns(ann_ids)\n\n    def __repr__(self):\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of datapoints: {}\\n\'.format(self.__len__())\n        fmt_str += \'    Root Location: {}\\n\'.format(self.root)\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        tmp = \'    Target Transforms (if any): \'\n        fmt_str += \'{0}{1}\'.format(tmp, self.target_transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str\n'"
data/config.py,0,"b'# config.py\nimport os.path\n\n# gets home dir cross platform\nHOME = os.path.expanduser(""~"")\n\n# for making bounding boxes pretty\nCOLORS = ((255, 0, 0, 128), (0, 255, 0, 128), (0, 0, 255, 128),\n          (0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128))\n\nMEANS = (104, 117, 123)\n\n# SSD300 CONFIGS\nvoc = {\n    \'num_classes\': 21,\n    \'lr_steps\': (80000, 100000, 120000),\n    \'max_iter\': 120000,\n    \'feature_maps\': [38, 19, 10, 5, 3, 1],\n    \'min_dim\': 300,\n    \'steps\': [8, 16, 32, 64, 100, 300],\n    \'min_sizes\': [30, 60, 111, 162, 213, 264],\n    \'max_sizes\': [60, 111, 162, 213, 264, 315],\n    \'aspect_ratios\': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n    \'variance\': [0.1, 0.2],\n    \'clip\': True,\n    \'name\': \'VOC\',\n}\n\ncoco = {\n    \'num_classes\': 201,\n    \'lr_steps\': (280000, 360000, 400000),\n    \'max_iter\': 400000,\n    \'feature_maps\': [38, 19, 10, 5, 3, 1],\n    \'min_dim\': 300,\n    \'steps\': [8, 16, 32, 64, 100, 300],\n    \'min_sizes\': [21, 45, 99, 153, 207, 261],\n    \'max_sizes\': [45, 99, 153, 207, 261, 315],\n    \'aspect_ratios\': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n    \'variance\': [0.1, 0.2],\n    \'clip\': True,\n    \'name\': \'COCO\',\n}\n'"
data/voc0712.py,4,"b'""""""VOC Dataset Classes\n\nOriginal author: Francisco Massa\nhttps://github.com/fmassa/vision/blob/voc_dataset/torchvision/datasets/voc.py\n\nUpdated by: Ellis Brown, Max deGroot\n""""""\nfrom .config import HOME\nimport os.path as osp\nimport sys\nimport torch\nimport torch.utils.data as data\nimport cv2\nimport numpy as np\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\nVOC_CLASSES = (  # always index 0\n    \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n    \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n    \'cow\', \'diningtable\', \'dog\', \'horse\',\n    \'motorbike\', \'person\', \'pottedplant\',\n    \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\n# note: if you used our download scripts, this should be right\nVOC_ROOT = osp.join(HOME, ""data/VOCdevkit/"")\n\n\nclass VOCAnnotationTransform(object):\n    """"""Transforms a VOC annotation into a Tensor of bbox coords and label index\n    Initilized with a dictionary lookup of classnames to indexes\n\n    Arguments:\n        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n            (default: alphabetic indexing of VOC\'s 20 classes)\n        keep_difficult (bool, optional): keep difficult instances or not\n            (default: False)\n        height (int): height\n        width (int): width\n    """"""\n\n    def __init__(self, class_to_ind=None, keep_difficult=False):\n        self.class_to_ind = class_to_ind or dict(\n            zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n        self.keep_difficult = keep_difficult\n\n    def __call__(self, target, width, height):\n        """"""\n        Arguments:\n            target (annotation) : the target annotation to be made usable\n                will be an ET.Element\n        Returns:\n            a list containing lists of bounding boxes  [bbox coords, class name]\n        """"""\n        res = []\n        for obj in target.iter(\'object\'):\n            difficult = int(obj.find(\'difficult\').text) == 1\n            if not self.keep_difficult and difficult:\n                continue\n            name = obj.find(\'name\').text.lower().strip()\n            bbox = obj.find(\'bndbox\')\n\n            pts = [\'xmin\', \'ymin\', \'xmax\', \'ymax\']\n            bndbox = []\n            for i, pt in enumerate(pts):\n                cur_pt = int(bbox.find(pt).text) - 1\n                # scale height or width\n                cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n                bndbox.append(cur_pt)\n            label_idx = self.class_to_ind[name]\n            bndbox.append(label_idx)\n            res += [bndbox]  # [xmin, ymin, xmax, ymax, label_ind]\n            # img_id = target.find(\'filename\').text[:-4]\n\n        return res  # [[xmin, ymin, xmax, ymax, label_ind], ... ]\n\n\nclass VOCDetection(data.Dataset):\n    """"""VOC Detection Dataset Object\n\n    input is image, target is annotation\n\n    Arguments:\n        root (string): filepath to VOCdevkit folder.\n        image_set (string): imageset to use (eg. \'train\', \'val\', \'test\')\n        transform (callable, optional): transformation to perform on the\n            input image\n        target_transform (callable, optional): transformation to perform on the\n            target `annotation`\n            (eg: take in caption string, return tensor of word indices)\n        dataset_name (string, optional): which dataset to load\n            (default: \'VOC2007\')\n    """"""\n\n    def __init__(self, root,\n                 image_sets=[(\'2007\', \'trainval\'), (\'2012\', \'trainval\')],\n                 transform=None, target_transform=VOCAnnotationTransform(),\n                 dataset_name=\'VOC0712\'):\n        self.root = root\n        self.image_set = image_sets\n        self.transform = transform\n        self.target_transform = target_transform\n        self.name = dataset_name\n        self._annopath = osp.join(\'%s\', \'Annotations\', \'%s.xml\')\n        self._imgpath = osp.join(\'%s\', \'JPEGImages\', \'%s.jpg\')\n        self.ids = list()\n        for (year, name) in image_sets:\n            rootpath = osp.join(self.root, \'VOC\' + year)\n            for line in open(osp.join(rootpath, \'ImageSets\', \'Main\', name + \'.txt\')):\n                self.ids.append((rootpath, line.strip()))\n\n    def __getitem__(self, index):\n        im, gt, h, w = self.pull_item(index)\n\n        return im, gt\n\n    def __len__(self):\n        return len(self.ids)\n\n    def pull_item(self, index):\n        img_id = self.ids[index]\n\n        target = ET.parse(self._annopath % img_id).getroot()\n        img = cv2.imread(self._imgpath % img_id)\n        height, width, channels = img.shape\n\n        if self.target_transform is not None:\n            target = self.target_transform(target, width, height)\n\n        if self.transform is not None:\n            target = np.array(target)\n            img, boxes, labels = self.transform(img, target[:, :4], target[:, 4])\n            # to rgb\n            img = img[:, :, (2, 1, 0)]\n            # img = img.transpose(2, 0, 1)\n            target = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n        return torch.from_numpy(img).permute(2, 0, 1), target, height, width\n        # return torch.from_numpy(img), target, height, width\n\n    def pull_image(self, index):\n        \'\'\'Returns the original image object at index in PIL form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            PIL img\n        \'\'\'\n        img_id = self.ids[index]\n        return cv2.imread(self._imgpath % img_id, cv2.IMREAD_COLOR)\n\n    def pull_anno(self, index):\n        \'\'\'Returns the original annotation of image at index\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to get annotation of\n        Return:\n            list:  [img_id, [(label, bbox coords),...]]\n                eg: (\'001718\', [(\'dog\', (96, 13, 438, 332))])\n        \'\'\'\n        img_id = self.ids[index]\n        anno = ET.parse(self._annopath % img_id).getroot()\n        gt = self.target_transform(anno, 1, 1)\n        return img_id[1], gt\n\n    def pull_tensor(self, index):\n        \'\'\'Returns the original image at an index in tensor form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            tensorized version of img, squeezed\n        \'\'\'\n        return torch.Tensor(self.pull_image(index)).unsqueeze_(0)\n'"
demo/__init__.py,0,b''
demo/live.py,4,"b'from __future__ import print_function\nimport torch\nfrom torch.autograd import Variable\nimport cv2\nimport time\nfrom imutils.video import FPS, WebcamVideoStream\nimport argparse\n\nparser = argparse.ArgumentParser(description=\'Single Shot MultiBox Detection\')\nparser.add_argument(\'--weights\', default=\'weights/ssd_300_VOC0712.pth\',\n                    type=str, help=\'Trained state_dict file path\')\nparser.add_argument(\'--cuda\', default=False, type=bool,\n                    help=\'Use cuda in live demo\')\nargs = parser.parse_args()\n\nCOLORS = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\nFONT = cv2.FONT_HERSHEY_SIMPLEX\n\n\ndef cv2_demo(net, transform):\n    def predict(frame):\n        height, width = frame.shape[:2]\n        x = torch.from_numpy(transform(frame)[0]).permute(2, 0, 1)\n        x = Variable(x.unsqueeze(0))\n        y = net(x)  # forward pass\n        detections = y.data\n        # scale each detection back up to the image\n        scale = torch.Tensor([width, height, width, height])\n        for i in range(detections.size(1)):\n            j = 0\n            while detections[0, i, j, 0] >= 0.6:\n                pt = (detections[0, i, j, 1:] * scale).cpu().numpy()\n                cv2.rectangle(frame,\n                              (int(pt[0]), int(pt[1])),\n                              (int(pt[2]), int(pt[3])),\n                              COLORS[i % 3], 2)\n                cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])),\n                            FONT, 2, (255, 255, 255), 2, cv2.LINE_AA)\n                j += 1\n        return frame\n\n    # start video stream thread, allow buffer to fill\n    print(""[INFO] starting threaded video stream..."")\n    stream = WebcamVideoStream(src=0).start()  # default camera\n    time.sleep(1.0)\n    # start fps timer\n    # loop over frames from the video file stream\n    while True:\n        # grab next frame\n        frame = stream.read()\n        key = cv2.waitKey(1) & 0xFF\n\n        # update FPS counter\n        fps.update()\n        frame = predict(frame)\n\n        # keybindings for display\n        if key == ord(\'p\'):  # pause\n            while True:\n                key2 = cv2.waitKey(1) or 0xff\n                cv2.imshow(\'frame\', frame)\n                if key2 == ord(\'p\'):  # resume\n                    break\n        cv2.imshow(\'frame\', frame)\n        if key == 27:  # exit\n            break\n\n\nif __name__ == \'__main__\':\n    import sys\n    from os import path\n    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n\n    from data import BaseTransform, VOC_CLASSES as labelmap\n    from ssd import build_ssd\n\n    net = build_ssd(\'test\', 300, 21)    # initialize SSD\n    net.load_state_dict(torch.load(args.weights))\n    transform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\n\n    fps = FPS().start()\n    cv2_demo(net.eval(), transform)\n    # stop the timer and display FPS information\n    fps.stop()\n\n    print(""[INFO] elasped time: {:.2f}"".format(fps.elapsed()))\n    print(""[INFO] approx. FPS: {:.2f}"".format(fps.fps()))\n\n    # cleanup\n    cv2.destroyAllWindows()\n    stream.stop()\n'"
layers/__init__.py,0,b'from .functions import *\nfrom .modules import *\n'
layers/box_utils.py,23,"b'# -*- coding: utf-8 -*-\nimport torch\n\n\ndef point_form(boxes):\n    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n\n\ndef center_size(boxes):\n    """""" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n    """"""Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when mathing boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n        idx: (int) current batch index\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    """"""\n    # jaccard index\n    overlaps = jaccard(\n        truths,\n        point_form(priors)\n    )\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n    conf = labels[best_truth_idx] + 1         # Shape: [num_priors]\n    conf[best_truth_overlap < threshold] = 0  # label as background\n    loc = encode(matches, priors, variances)\n    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n    conf_t[idx] = conf  # [num_priors] top class label for each prior\n\n\ndef encode(matched, priors, variances):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n\n    # dist b/t match center and prior\'s center\n    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\n# Adapted from https://github.com/Hakuyume/chainer-ssd\ndef decode(loc, priors, variances):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    boxes = torch.cat((\n        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\n\ndef log_sum_exp(x):\n    """"""Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    """"""\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n\n\n# Original author: Francisco Massa:\n# https://github.com/fmassa/object-detection.torch\n# Ported to PyTorch by Max deGroot (02/01/2017)\ndef nms(boxes, scores, overlap=0.5, top_k=200):\n    """"""Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.\n    """"""\n\n    keep = scores.new(scores.size(0)).zero_().long()\n    if boxes.numel() == 0:\n        return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    # I = I[v >= 0.01]\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    # keep = torch.Tensor()\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        # keep.append(i)\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1:\n            break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n'"
utils/__init__.py,0,b'from .augmentations import SSDAugmentation'
utils/augmentations.py,2,"b'import torch\nfrom torchvision import transforms\nimport cv2\nimport numpy as np\nimport types\nfrom numpy import random\n\n\ndef intersect(box_a, box_b):\n    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n    return inter[:, 0] * inter[:, 1]\n\n\ndef jaccard_numpy(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n        box_b: Single bounding box, Shape: [4]\n    Return:\n        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1]))  # [A,B]\n    area_b = ((box_b[2]-box_b[0]) *\n              (box_b[3]-box_b[1]))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\nclass Compose(object):\n    """"""Composes several augmentations together.\n    Args:\n        transforms (List[Transform]): list of transforms to compose.\n    Example:\n        >>> augmentations.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, boxes=None, labels=None):\n        for t in self.transforms:\n            img, boxes, labels = t(img, boxes, labels)\n        return img, boxes, labels\n\n\nclass Lambda(object):\n    """"""Applies a lambda as a transform.""""""\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img, boxes=None, labels=None):\n        return self.lambd(img, boxes, labels)\n\n\nclass ConvertFromInts(object):\n    def __call__(self, image, boxes=None, labels=None):\n        return image.astype(np.float32), boxes, labels\n\n\nclass SubtractMeans(object):\n    def __init__(self, mean):\n        self.mean = np.array(mean, dtype=np.float32)\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = image.astype(np.float32)\n        image -= self.mean\n        return image.astype(np.float32), boxes, labels\n\n\nclass ToAbsoluteCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] *= width\n        boxes[:, 2] *= width\n        boxes[:, 1] *= height\n        boxes[:, 3] *= height\n\n        return image, boxes, labels\n\n\nclass ToPercentCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] /= width\n        boxes[:, 2] /= width\n        boxes[:, 1] /= height\n        boxes[:, 3] /= height\n\n        return image, boxes, labels\n\n\nclass Resize(object):\n    def __init__(self, size=300):\n        self.size = size\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = cv2.resize(image, (self.size,\n                                 self.size))\n        return image, boxes, labels\n\n\nclass RandomSaturation(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n\n        return image, boxes, labels\n\n\nclass RandomHue(object):\n    def __init__(self, delta=18.0):\n        assert delta >= 0.0 and delta <= 360.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n        return image, boxes, labels\n\n\nclass RandomLightingNoise(object):\n    def __init__(self):\n        self.perms = ((0, 1, 2), (0, 2, 1),\n                      (1, 0, 2), (1, 2, 0),\n                      (2, 0, 1), (2, 1, 0))\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            swap = self.perms[random.randint(len(self.perms))]\n            shuffle = SwapChannels(swap)  # shuffle channels\n            image = shuffle(image)\n        return image, boxes, labels\n\n\nclass ConvertColor(object):\n    def __init__(self, current=\'BGR\', transform=\'HSV\'):\n        self.transform = transform\n        self.current = current\n\n    def __call__(self, image, boxes=None, labels=None):\n        if self.current == \'BGR\' and self.transform == \'HSV\':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        elif self.current == \'HSV\' and self.transform == \'BGR\':\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        else:\n            raise NotImplementedError\n        return image, boxes, labels\n\n\nclass RandomContrast(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    # expects float image\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            alpha = random.uniform(self.lower, self.upper)\n            image *= alpha\n        return image, boxes, labels\n\n\nclass RandomBrightness(object):\n    def __init__(self, delta=32):\n        assert delta >= 0.0\n        assert delta <= 255.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            delta = random.uniform(-self.delta, self.delta)\n            image += delta\n        return image, boxes, labels\n\n\nclass ToCV2Image(object):\n    def __call__(self, tensor, boxes=None, labels=None):\n        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), boxes, labels\n\n\nclass ToTensor(object):\n    def __call__(self, cvimage, boxes=None, labels=None):\n        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n\n\nclass RandomSampleCrop(object):\n    """"""Crop\n    Arguments:\n        img (Image): the image being input during training\n        boxes (Tensor): the original bounding boxes in pt form\n        labels (Tensor): the class labels for each bbox\n        mode (float tuple): the min and max jaccard overlaps\n    Return:\n        (img, boxes, classes)\n            img (Image): the cropped image\n            boxes (Tensor): the adjusted bounding boxes in pt form\n            labels (Tensor): the class labels for each bbox\n    """"""\n    def __init__(self):\n        self.sample_options = (\n            # using entire original input image\n            None,\n            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n            (0.1, None),\n            (0.3, None),\n            (0.7, None),\n            (0.9, None),\n            # randomly sample a patch\n            (None, None),\n        )\n\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, _ = image.shape\n        while True:\n            # randomly choose a mode\n            mode = random.choice(self.sample_options)\n            if mode is None:\n                return image, boxes, labels\n\n            min_iou, max_iou = mode\n            if min_iou is None:\n                min_iou = float(\'-inf\')\n            if max_iou is None:\n                max_iou = float(\'inf\')\n\n            # max trails (50)\n            for _ in range(50):\n                current_image = image\n\n                w = random.uniform(0.3 * width, width)\n                h = random.uniform(0.3 * height, height)\n\n                # aspect ratio constraint b/t .5 & 2\n                if h / w < 0.5 or h / w > 2:\n                    continue\n\n                left = random.uniform(width - w)\n                top = random.uniform(height - h)\n\n                # convert to integer rect x1,y1,x2,y2\n                rect = np.array([int(left), int(top), int(left+w), int(top+h)])\n\n                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n                overlap = jaccard_numpy(boxes, rect)\n\n                # is min and max overlap constraint satisfied? if not try again\n                if overlap.min() < min_iou and max_iou < overlap.max():\n                    continue\n\n                # cut the crop from the image\n                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],\n                                              :]\n\n                # keep overlap with gt box IF center in sampled patch\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n                # mask in all gt boxes that above and to the left of centers\n                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\n                # mask in all gt boxes that under and to the right of centers\n                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\n                # mask in that both m1 and m2 are true\n                mask = m1 * m2\n\n                # have any valid boxes? try again if not\n                if not mask.any():\n                    continue\n\n                # take only matching gt boxes\n                current_boxes = boxes[mask, :].copy()\n\n                # take only matching gt labels\n                current_labels = labels[mask]\n\n                # should we use the box left and top corner or the crop\'s\n                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n                                                  rect[:2])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, :2] -= rect[:2]\n\n                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n                                                  rect[2:])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, 2:] -= rect[:2]\n\n                return current_image, current_boxes, current_labels\n\n\nclass Expand(object):\n    def __init__(self, mean):\n        self.mean = mean\n\n    def __call__(self, image, boxes, labels):\n        if random.randint(2):\n            return image, boxes, labels\n\n        height, width, depth = image.shape\n        ratio = random.uniform(1, 4)\n        left = random.uniform(0, width*ratio - width)\n        top = random.uniform(0, height*ratio - height)\n\n        expand_image = np.zeros(\n            (int(height*ratio), int(width*ratio), depth),\n            dtype=image.dtype)\n        expand_image[:, :, :] = self.mean\n        expand_image[int(top):int(top + height),\n                     int(left):int(left + width)] = image\n        image = expand_image\n\n        boxes = boxes.copy()\n        boxes[:, :2] += (int(left), int(top))\n        boxes[:, 2:] += (int(left), int(top))\n\n        return image, boxes, labels\n\n\nclass RandomMirror(object):\n    def __call__(self, image, boxes, classes):\n        _, width, _ = image.shape\n        if random.randint(2):\n            image = image[:, ::-1]\n            boxes = boxes.copy()\n            boxes[:, 0::2] = width - boxes[:, 2::-2]\n        return image, boxes, classes\n\n\nclass SwapChannels(object):\n    """"""Transforms a tensorized image by swapping the channels in the order\n     specified in the swap tuple.\n    Args:\n        swaps (int triple): final order of channels\n            eg: (2, 1, 0)\n    """"""\n\n    def __init__(self, swaps):\n        self.swaps = swaps\n\n    def __call__(self, image):\n        """"""\n        Args:\n            image (Tensor): image tensor to be transformed\n        Return:\n            a tensor with channels swapped according to swap\n        """"""\n        # if torch.is_tensor(image):\n        #     image = image.data.cpu().numpy()\n        # else:\n        #     image = np.array(image)\n        image = image[:, :, self.swaps]\n        return image\n\n\nclass PhotometricDistort(object):\n    def __init__(self):\n        self.pd = [\n            RandomContrast(),\n            ConvertColor(transform=\'HSV\'),\n            RandomSaturation(),\n            RandomHue(),\n            ConvertColor(current=\'HSV\', transform=\'BGR\'),\n            RandomContrast()\n        ]\n        self.rand_brightness = RandomBrightness()\n        self.rand_light_noise = RandomLightingNoise()\n\n    def __call__(self, image, boxes, labels):\n        im = image.copy()\n        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n        if random.randint(2):\n            distort = Compose(self.pd[:-1])\n        else:\n            distort = Compose(self.pd[1:])\n        im, boxes, labels = distort(im, boxes, labels)\n        return self.rand_light_noise(im, boxes, labels)\n\n\nclass SSDAugmentation(object):\n    def __init__(self, size=300, mean=(104, 117, 123)):\n        self.mean = mean\n        self.size = size\n        self.augment = Compose([\n            ConvertFromInts(),\n            ToAbsoluteCoords(),\n            PhotometricDistort(),\n            Expand(self.mean),\n            RandomSampleCrop(),\n            RandomMirror(),\n            ToPercentCoords(),\n            Resize(self.size),\n            SubtractMeans(self.mean)\n        ])\n\n    def __call__(self, img, boxes, labels):\n        return self.augment(img, boxes, labels)\n'"
layers/functions/__init__.py,0,"b""from .detection import Detect\nfrom .prior_box import PriorBox\n\n\n__all__ = ['Detect', 'PriorBox']\n"""
layers/functions/detection.py,3,"b'import torch\nfrom torch.autograd import Function\nfrom ..box_utils import decode, nms\nfrom data import voc as cfg\n\n\nclass Detect(Function):\n    """"""At test time, Detect is the final layer of SSD.  Decode location preds,\n    apply non-maximum suppression to location predictions based on conf\n    scores and threshold to a top_k number of output predictions for both\n    confidence score and locations.\n    """"""\n    def __init__(self, num_classes, bkg_label, top_k, conf_thresh, nms_thresh):\n        self.num_classes = num_classes\n        self.background_label = bkg_label\n        self.top_k = top_k\n        # Parameters used in nms.\n        self.nms_thresh = nms_thresh\n        if nms_thresh <= 0:\n            raise ValueError(\'nms_threshold must be non negative.\')\n        self.conf_thresh = conf_thresh\n        self.variance = cfg[\'variance\']\n\n    def forward(self, loc_data, conf_data, prior_data):\n        """"""\n        Args:\n            loc_data: (tensor) Loc preds from loc layers\n                Shape: [batch,num_priors*4]\n            conf_data: (tensor) Shape: Conf preds from conf layers\n                Shape: [batch*num_priors,num_classes]\n            prior_data: (tensor) Prior boxes and variances from priorbox layers\n                Shape: [1,num_priors,4]\n        """"""\n        num = loc_data.size(0)  # batch size\n        num_priors = prior_data.size(0)\n        output = torch.zeros(num, self.num_classes, self.top_k, 5)\n        conf_preds = conf_data.view(num, num_priors,\n                                    self.num_classes).transpose(2, 1)\n\n        # Decode predictions into bboxes.\n        for i in range(num):\n            decoded_boxes = decode(loc_data[i], prior_data, self.variance)\n            # For each class, perform nms\n            conf_scores = conf_preds[i].clone()\n\n            for cl in range(1, self.num_classes):\n                c_mask = conf_scores[cl].gt(self.conf_thresh)\n                scores = conf_scores[cl][c_mask]\n                if scores.size(0) == 0:\n                    continue\n                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n                boxes = decoded_boxes[l_mask].view(-1, 4)\n                # idx of highest scoring and non-overlapping boxes per class\n                ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n                output[i, cl, :count] = \\\n                    torch.cat((scores[ids[:count]].unsqueeze(1),\n                               boxes[ids[:count]]), 1)\n        flt = output.contiguous().view(num, -1, 5)\n        _, idx = flt[:, :, 0].sort(1, descending=True)\n        _, rank = idx.sort(1)\n        flt[(rank < self.top_k).unsqueeze(-1).expand_as(flt)].fill_(0)\n        return output\n'"
layers/functions/prior_box.py,1,"b'from __future__ import division\nfrom math import sqrt as sqrt\nfrom itertools import product as product\nimport torch\n\n\nclass PriorBox(object):\n    """"""Compute priorbox coordinates in center-offset form for each source\n    feature map.\n    """"""\n    def __init__(self, cfg):\n        super(PriorBox, self).__init__()\n        self.image_size = cfg[\'min_dim\']\n        # number of priors for feature map location (either 4 or 6)\n        self.num_priors = len(cfg[\'aspect_ratios\'])\n        self.variance = cfg[\'variance\'] or [0.1]\n        self.feature_maps = cfg[\'feature_maps\']\n        self.min_sizes = cfg[\'min_sizes\']\n        self.max_sizes = cfg[\'max_sizes\']\n        self.steps = cfg[\'steps\']\n        self.aspect_ratios = cfg[\'aspect_ratios\']\n        self.clip = cfg[\'clip\']\n        self.version = cfg[\'name\']\n        for v in self.variance:\n            if v <= 0:\n                raise ValueError(\'Variances must be greater than 0\')\n\n    def forward(self):\n        mean = []\n        for k, f in enumerate(self.feature_maps):\n            for i, j in product(range(f), repeat=2):\n                f_k = self.image_size / self.steps[k]\n                # unit center x,y\n                cx = (j + 0.5) / f_k\n                cy = (i + 0.5) / f_k\n\n                # aspect_ratio: 1\n                # rel size: min_size\n                s_k = self.min_sizes[k]/self.image_size\n                mean += [cx, cy, s_k, s_k]\n\n                # aspect_ratio: 1\n                # rel size: sqrt(s_k * s_(k+1))\n                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n                mean += [cx, cy, s_k_prime, s_k_prime]\n\n                # rest of aspect ratios\n                for ar in self.aspect_ratios[k]:\n                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n        # back to torch land\n        output = torch.Tensor(mean).view(-1, 4)\n        if self.clip:\n            output.clamp_(max=1, min=0)\n        return output\n'"
layers/modules/__init__.py,0,"b""from .l2norm import L2Norm\nfrom .multibox_loss import MultiBoxLoss\n\n__all__ = ['L2Norm', 'MultiBoxLoss']\n"""
layers/modules/l2norm.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd import Variable\nimport torch.nn.init as init\n\nclass L2Norm(nn.Module):\n    def __init__(self,n_channels, scale):\n        super(L2Norm,self).__init__()\n        self.n_channels = n_channels\n        self.gamma = scale or None\n        self.eps = 1e-10\n        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init.constant_(self.weight,self.gamma)\n\n    def forward(self, x):\n        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n        #x /= norm\n        x = torch.div(x,norm)\n        out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n        return out\n'"
layers/modules/multibox_loss.py,9,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom data import coco as cfg\nfrom ..box_utils import match, log_sum_exp\n\n\nclass MultiBoxLoss(nn.Module):\n    """"""SSD Weighted Loss Function\n    Compute Targets:\n        1) Produce Confidence Target Indices by matching  ground truth boxes\n           with (default) \'priorboxes\' that have jaccard index > threshold parameter\n           (default threshold: 0.5).\n        2) Produce localization target by \'encoding\' variance into offsets of ground\n           truth boxes and their matched  \'priorboxes\'.\n        3) Hard negative mining to filter the excessive number of negative examples\n           that comes with using a large number of default bounding boxes.\n           (default negative:positive ratio 3:1)\n    Objective Loss:\n        L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n        weighted by \xce\xb1 which is set to 1 by cross val.\n        Args:\n            c: class confidences,\n            l: predicted boxes,\n            g: ground truth boxes\n            N: number of matched default boxes\n        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n    """"""\n\n    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n                 use_gpu=True):\n        super(MultiBoxLoss, self).__init__()\n        self.use_gpu = use_gpu\n        self.num_classes = num_classes\n        self.threshold = overlap_thresh\n        self.background_label = bkg_label\n        self.encode_target = encode_target\n        self.use_prior_for_matching = prior_for_matching\n        self.do_neg_mining = neg_mining\n        self.negpos_ratio = neg_pos\n        self.neg_overlap = neg_overlap\n        self.variance = cfg[\'variance\']\n\n    def forward(self, predictions, targets):\n        """"""Multibox Loss\n        Args:\n            predictions (tuple): A tuple containing loc preds, conf preds,\n            and prior boxes from SSD net.\n                conf shape: torch.size(batch_size,num_priors,num_classes)\n                loc shape: torch.size(batch_size,num_priors,4)\n                priors shape: torch.size(num_priors,4)\n\n            targets (tensor): Ground truth boxes and labels for a batch,\n                shape: [batch_size,num_objs,5] (last idx is the label).\n        """"""\n        loc_data, conf_data, priors = predictions\n        num = loc_data.size(0)\n        priors = priors[:loc_data.size(1), :]\n        num_priors = (priors.size(0))\n        num_classes = self.num_classes\n\n        # match priors (default boxes) and ground truth boxes\n        loc_t = torch.Tensor(num, num_priors, 4)\n        conf_t = torch.LongTensor(num, num_priors)\n        for idx in range(num):\n            truths = targets[idx][:, :-1].data\n            labels = targets[idx][:, -1].data\n            defaults = priors.data\n            match(self.threshold, truths, defaults, self.variance, labels,\n                  loc_t, conf_t, idx)\n        if self.use_gpu:\n            loc_t = loc_t.cuda()\n            conf_t = conf_t.cuda()\n        # wrap targets\n        loc_t = Variable(loc_t, requires_grad=False)\n        conf_t = Variable(conf_t, requires_grad=False)\n\n        pos = conf_t > 0\n        num_pos = pos.sum(dim=1, keepdim=True)\n\n        # Localization Loss (Smooth L1)\n        # Shape: [batch,num_priors,4]\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        loc_p = loc_data[pos_idx].view(-1, 4)\n        loc_t = loc_t[pos_idx].view(-1, 4)\n        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n\n        # Compute max conf across batch for hard negative mining\n        batch_conf = conf_data.view(-1, self.num_classes)\n        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n\n        # Hard Negative Mining\n        loss_c[pos] = 0  # filter out pos boxes for now\n        loss_c = loss_c.view(num, -1)\n        _, loss_idx = loss_c.sort(1, descending=True)\n        _, idx_rank = loss_idx.sort(1)\n        num_pos = pos.long().sum(1, keepdim=True)\n        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n        neg = idx_rank < num_neg.expand_as(idx_rank)\n\n        # Confidence Loss Including Positive and Negative Examples\n        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n        targets_weighted = conf_t[(pos+neg).gt(0)]\n        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n\n        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n\n        N = num_pos.data.sum()\n        loss_l /= N\n        loss_c /= N\n        return loss_l, loss_c\n'"
