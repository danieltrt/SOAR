file_path,api_count,code
examples/PyTorch_MNIST.py,9,"b'# Summary: \xe4\xbd\xbf\xe7\x94\xa8PyTorch\xe7\x8e\xa9\xe8\xbd\xacMNIST \n# Author:  Amusi\n# Date:    2018-12-20 \n# github:  https://github.com/amusi/PyTorch-From-Zero-To-One\n# Reference: https://blog.csdn.net/victoriaw/article/details/72354307\n \nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n \n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch MNIST Example\')\nparser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\n                    help=\'input batch size for training (default: 64)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                    help=\'learning rate (default: 0.01)\')\nparser.add_argument(\'--momentum\', type=float, default=0.5, metavar=\'M\',\n                    help=\'SGD momentum (default: 0.5)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n \ntorch.manual_seed(args.seed) #\xe4\xb8\xbaCPU\xe8\xae\xbe\xe7\xbd\xae\xe7\xa7\x8d\xe5\xad\x90\xe7\x94\xa8\xe4\xba\x8e\xe7\x94\x9f\xe6\x88\x90\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbd\xbf\xe5\xbe\x97\xe7\xbb\x93\xe6\x9e\x9c\xe6\x98\xaf\xe7\xa1\xae\xe5\xae\x9a\xe7\x9a\x84\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)#\xe4\xb8\xba\xe5\xbd\x93\xe5\x89\x8dGPU\xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\xef\xbc\x9b\xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xbf\xe7\x94\xa8\xe5\xa4\x9a\xe4\xb8\xaaGPU\xef\xbc\x8c\xe5\xba\x94\xe8\xaf\xa5\xe4\xbd\xbf\xe7\x94\xa8torch.cuda.manual_seed_all()\xe4\xb8\xba\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84GPU\xe8\xae\xbe\xe7\xbd\xae\xe7\xa7\x8d\xe5\xad\x90\xe3\x80\x82\n \n \nkwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\n""""""\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe7\xbb\x84\xe5\x90\x88\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x92\x8c\xe9\x87\x87\xe6\xa0\xb7\xe5\x99\xa8\xef\xbc\x8c\xe6\x8f\x90\xe4\xbe\x9b\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8a\xe7\x9a\x84\xe5\x8d\x95\xe6\x88\x96\xe5\xa4\x9a\xe8\xbf\x9b\xe7\xa8\x8b\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\n\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9a\ndataset\xef\xbc\x9aDataset\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe4\xbb\x8e\xe5\x85\xb6\xe4\xb8\xad\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\nbatch_size\xef\xbc\x9aint\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\xaabatch\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa0\xb7\xe6\x9c\xac\nshuffle\xef\xbc\x9abool\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82\xe4\xb8\xbaTrue\xe6\x97\xb6\xe8\xa1\xa8\xe7\xa4\xba\xe6\xaf\x8f\xe4\xb8\xaaepoch\xe9\x83\xbd\xe5\xaf\xb9\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb4\x97\xe7\x89\x8c\nsampler\xef\xbc\x9aSampler\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82\xe4\xbb\x8e\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe9\x87\x87\xe6\xa0\xb7\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82\nnum_workers\xef\xbc\x9aint\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8\xe5\xa4\x9a\xe5\xb0\x91\xe5\xad\x90\xe8\xbf\x9b\xe7\xa8\x8b\xe3\x80\x82\xe9\xbb\x98\xe8\xae\xa4\xe5\x80\xbc\xe4\xb8\xba0\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe5\x9c\xa8\xe4\xb8\xbb\xe8\xbf\x9b\xe7\xa8\x8b\xe4\xb8\xad\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\ncollate_fn\xef\xbc\x9acallable\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82\npin_memory\xef\xbc\x9abool\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\ndrop_last\xef\xbc\x9abool\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82True\xe8\xa1\xa8\xe7\xa4\xba\xe5\xa6\x82\xe6\x9e\x9c\xe6\x9c\x80\xe5\x90\x8e\xe5\x89\xa9\xe4\xb8\x8b\xe4\xb8\x8d\xe5\xae\x8c\xe5\x85\xa8\xe7\x9a\x84batch,\xe4\xb8\xa2\xe5\xbc\x83\xe3\x80\x82False\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x8d\xe4\xb8\xa2\xe5\xbc\x83\xe3\x80\x82\n""""""\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\'../data\', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\'../data\', train=False, transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\n \n \nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)#\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba1\xe5\x92\x8c10\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)#\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba10\xe5\x92\x8c20\n        self.conv2_drop = nn.Dropout2d()#\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe4\xbf\xa1\xe9\x81\x93\xef\xbc\x8c\xe5\xb0\x86\xe5\x85\xb6\xe8\xae\xbe\xe4\xb8\xba0\n        self.fc1 = nn.Linear(320, 50)#\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba320\xe5\x92\x8c50\n        self.fc2 = nn.Linear(50, 10)\n \n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))#conv->max_pool->relu\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))#conv->dropout->max_pool->relu\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))#fc->relu\n        x = F.dropout(x, training=self.training)#dropout\n        x = self.fc2(x)\n        return F.log_softmax(x)\n \nmodel = Net()\nif args.cuda:\n    model.cuda()#\xe5\xb0\x86\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\xe7\xa7\xbb\xe5\x8a\xa8\xe5\x88\xb0GPU\xe4\xb8\x8a\n \noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n \ndef train(epoch):\n    model.train()#\xe6\x8a\x8amodule\xe8\xae\xbe\xe6\x88\x90training\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe5\xaf\xb9Dropout\xe5\x92\x8cBatchNorm\xe6\x9c\x89\xe5\xbd\xb1\xe5\x93\x8d\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)#Variable\xe7\xb1\xbb\xe5\xaf\xb9Tensor\xe5\xaf\xb9\xe8\xb1\xa1\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xb0\x81\xe8\xa3\x85\xef\xbc\x8c\xe4\xbc\x9a\xe4\xbf\x9d\xe5\xad\x98\xe8\xaf\xa5\xe5\xbc\xa0\xe9\x87\x8f\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x8a\xe5\xaf\xb9\xe7\x94\x9f\xe6\x88\x90\xe8\xaf\xa5\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0grad_fn\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbc\x95\xe7\x94\xa8\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe8\xaf\xa5\xe5\xbc\xa0\xe9\x87\x8f\xe6\x98\xaf\xe7\x94\xa8\xe6\x88\xb7\xe5\x88\x9b\xe5\xbb\xba\xe7\x9a\x84\xef\xbc\x8cgrad_fn\xe6\x98\xafNone\xef\xbc\x8c\xe7\xa7\xb0\xe8\xbf\x99\xe6\xa0\xb7\xe7\x9a\x84Variable\xe4\xb8\xba\xe5\x8f\xb6\xe5\xad\x90Variable\xe3\x80\x82\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)#\xe8\xb4\x9flog\xe4\xbc\xbc\xe7\x84\xb6\xe6\x8d\x9f\xe5\xa4\xb1\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n \ndef test(epoch):\n    model.eval()#\xe6\x8a\x8amodule\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba\xe8\xaf\x84\xe4\xbc\xb0\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe5\x8f\xaa\xe5\xaf\xb9Dropout\xe5\x92\x8cBatchNorm\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9c\x89\xe5\xbd\xb1\xe5\x93\x8d\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += F.nll_loss(output, target).item()#Variable.data\n        pred = output.data.max(1)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data).cpu().sum()\n \n    test_loss = test_loss\n    test_loss /= len(test_loader) # loss function already averages over batch size\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n \n \nif __name__ == \'__main__\':\n    for epoch in range(1, args.epochs + 1):\n        train(epoch)\n        test(epoch)\n'"
examples/PyTorch_Tensors.py,7,"b'# Summary\xef\xbc\x9aPyTorch\xe7\x9a\x84Tensor\xe5\x9f\xba\xe7\xa1\x80\xe7\x9f\xa5\xe8\xaf\x86\n# Author:  Amusi\n# Date:    2018-12-20 \n# github:  https://github.com/amusi/PyTorch-From-Zero-To-One\n# Reference: http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-tensors\n \nimport torch\n \ndtype = torch.FloatTensor\n# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n \n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n \n# Create random input and output data\nx = torch.randn(N, D_in).type(dtype)\ny = torch.randn(N, D_out).type(dtype)\n \n# Randomly initialize weights\nw1 = torch.randn(D_in, H).type(dtype)\nw2 = torch.randn(H, D_out).type(dtype)\n \nlearning_rate = 1e-6\nfor t in range(500):\n    # Forward pass: compute predicted y\n    h = x.mm(w1)\n    h_relu = h.clamp(min=0)\n    y_pred = h_relu.mm(w2)\n \n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum()\n    print(t, loss)\n \n    # Backprop to compute gradients of w1 and w2 with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.t().mm(grad_y_pred)\n    grad_h_relu = grad_y_pred.mm(w2.t())\n    grad_h = grad_h_relu.clone()\n    grad_h[h < 0] = 0\n    grad_w1 = x.t().mm(grad_h)\n \n    # Update weights using gradient descent\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n'"
examples/pytorch_cudn_cudnn_test.py,4,"b'# Summary: \xe6\xa3\x80\xe6\xb5\x8b\xe5\xbd\x93\xe5\x89\x8dPytorch\xe5\x92\x8c\xe8\xae\xbe\xe5\xa4\x87\xe6\x98\xaf\xe5\x90\xa6\xe6\x94\xaf\xe6\x8c\x81CUDA\xe5\x92\x8ccudnn\n# Author:  Amusi\n# Date:    2018-12-20 \n# github:  https://github.com/amusi/PyTorch-From-Zero-To-One\n\nimport torch\n \nif __name__ == \'__main__\':\n\tprint(""Support CUDA ?: "", torch.cuda.is_available())\n\tx = torch.Tensor([1.0])\n\txx = x.cuda()\n\tprint(xx)\n \n\ty = torch.randn(2, 3)\n\tyy = y.cuda()\n\tprint(yy)\n \n\tzz = xx + yy\n\tprint(zz)\n \n\t# CUDNN TEST\n\tfrom torch.backends import cudnn\n\tprint(""Support cudnn ?: "",cudnn.is_acceptable(xx))\n\n'"
examples/pytorch0.3&0.4/PyTorch4_CIFAR10.py,9,"b'# Summary\xef\xbc\x9aPyTorch0.4.0\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe6\x98\xaf\xe5\x9c\xa8Windows\xe4\xb8\x8a\n# Author:  Amusi\n# Date:    2018-03-31\n# Reference: https://zhuanlan.zhihu.com/p/39667289\n\n# \xe5\xaf\xbc\xe5\x85\xa5\xe5\x8c\x85\n# torch: Pytorch\xe6\x9c\x80\xe6\xa0\xb8\xe5\xbf\x83\xe7\x9a\x84\xe5\xba\x93\xef\xbc\x88\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbb\xa3\xe8\xa1\xa8PyTorch\xef\xbc\x89\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe7\x94\xa8\xe4\xba\x8e\xe8\xae\xa1\xe7\xae\x97 \n# torchvision\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x9ePyTorch\xe5\xb8\xb8\xe7\x94\xa8\xe5\x88\xb0\xe7\x9a\x84\xe5\xba\x93\xef\xbc\x8c\xe6\x9c\x89models\xe3\x80\x81datasets\xe5\x92\x8ctransforms\xe4\xb8\x89\xe4\xb8\xaa\xe5\xad\x90\xe5\x8c\x85\xef\xbc\x8c\xe5\x88\x86\xe5\x88\xab\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x81\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x81\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x92\x8c\xe6\xa0\xbc\xe5\xbc\x8f\xe5\x8f\x98\xe6\x8d\xa2\xe7\xad\x89\xe5\x8a\x9f\xe8\x83\xbd\n# torchvision: https://github.com/pytorch/vision/tree/master/torchvision\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\n\n# \xe5\x9c\xa8\xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe6\x8c\x87\xe5\xae\x9atransform\xef\xbc\x8c\xe5\xb0\xb1\xe4\xbc\x9a\xe5\xba\x94\xe7\x94\xa8\xe6\x88\x91\xe4\xbb\xac\xe5\xae\x9a\xe4\xb9\x89\xe5\xa5\xbd\xe7\x9a\x84transform\n# cifar-10\xe5\xae\x98\xe6\x96\xb9\xe6\x8f\x90\xe4\xbe\x9b\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\x98\xaf\xe7\x94\xa8numpy array\xe5\xad\x98\xe5\x82\xa8\xe7\x9a\x84\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe8\xbf\x99\xe4\xb8\xaatransform\xe4\xbc\x9a\xe6\x8a\x8anumpy array\xe5\x8f\x98\xe6\x88\x90torch tensor\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe6\x8a\x8argb\xe5\x80\xbc\xe5\xbd\x92\xe4\xb8\x80\xe5\x88\xb0[0, 1]\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8c\xba\xe9\x97\xb4\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# root\xe6\x98\xaf\xe5\xad\x98\xe5\x82\xa8\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8cdownload=True\xe6\x8c\x87\xe5\xae\x9a\xe5\xa6\x82\xe6\x9e\x9c\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe5\x85\x88\xe4\xb8\x8b\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8CIFAR\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe5\xa4\xa7\xe6\xa6\x82200MB\xef\xbc\x8c\xe7\xbd\x91\xe9\x80\x9f\xe4\xb8\x8d\xe5\xa5\xbd\xe7\x9a\x84\xe5\x90\x8c\xe5\xad\xa6\xef\xbc\x8c\xe8\xaf\xb7\xe8\x80\x90\xe5\xbf\x83\xe7\xad\x89\xe5\xbe\x85\xef\xbc\x88\xe5\xb9\xb6\xe4\xb8\x8d\xe6\x98\xaf\xe7\xa8\x8b\xe5\xba\x8f\xe5\x8d\xa1\xe4\xbd\x8f\xe4\xba\x86\xef\xbc\x89\ncifar_train = torchvision.datasets.CIFAR10(root=\'./data\', train=True,\n                                           download=True, transform=transform)\ncifar_test = torchvision.datasets.CIFAR10(root=\'./data\', train=False,\n                                          transform=transform)\n\n\t\t\t\t\t\t\t\t\t\t \nprint(cifar_train)\n\nprint(cifar_test)\n\ntrainloader = torch.utils.data.DataLoader(cifar_train, batch_size=32, shuffle=True)\ntestloader = torch.utils.data.DataLoader(cifar_test, batch_size=32, shuffle=True)\n\nclass LeNet(nn.Module):\n    # \xe4\xb8\x80\xe8\x88\xac\xe5\x9c\xa8__init__\xe4\xb8\xad\xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe7\xae\x97\xe5\xad\x90\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe3\x80\x81\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\xae\x97\xe5\xad\x90\xe7\xad\x89\xe7\xad\x89\n    def __init__(self):\n        super(LeNet, self).__init__()\n        # Conv2d\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84channel\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84channel\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x89\xe4\xb8\xaa\xe6\x98\xafkernel size\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # \xe7\x94\xb1\xe4\xba\x8e\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe6\x9c\x8916\xe4\xb8\xaachannel\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaafeature map\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba5*5\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf16*5*5\n        self.fc1 = nn.Linear(16*5*5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        # \xe6\x9c\x80\xe7\xbb\x88\xe6\x9c\x8910\xe7\xb1\xbb\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe6\x95\xb0\xe9\x87\x8f\xe6\x98\xaf10\n        self.fc3 = nn.Linear(84, 10)\n        self.pool = nn.MaxPool2d(2, 2)\n    # forward\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe5\xae\x9a\xe4\xb9\x89\xe4\xba\x86\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe8\xbf\x90\xe7\xae\x97\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe5\x83\x8f\xe5\x86\x99\xe6\x99\xae\xe9\x80\x9a\xe7\x9a\x84python\xe7\xae\x97\xe6\x95\xb0\xe8\xbf\x90\xe7\xae\x97\xe9\x82\xa3\xe6\xa0\xb7\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\t\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        # \xe4\xb8\x8b\xe9\x9d\xa2\xe8\xbf\x99\xe6\xad\xa5\xe6\x8a\x8a\xe4\xba\x8c\xe7\xbb\xb4\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe5\x8f\x98\xe4\xb8\xba\xe4\xb8\x80\xe7\xbb\xb4\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe6\x89\x8d\xe8\x83\xbd\xe5\xa4\x84\xe7\x90\x86\n        x = x.view(-1, 16*5*5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\t\t\n# \xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe6\xb2\xa1\xe6\x9c\x89GPU\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbf\xbd\xe7\x95\xa5device\xe7\x9b\xb8\xe5\x85\xb3\xe7\x9a\x84\xe4\xbb\xa3\xe7\xa0\x81\ndevice = torch.device(""cuda:0"")\nnet = LeNet().to(device)\n\n# optim\xe4\xb8\xad\xe5\xae\x9a\xe4\xb9\x89\xe4\xba\x86\xe5\x90\x84\xe7\xa7\x8d\xe5\x90\x84\xe6\xa0\xb7\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe5\x8c\x85\xe6\x8b\xacSGD\nimport torch.optim as optim\n\n# CrossEntropyLoss\xe5\xb0\xb1\xe6\x98\xaf\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\nprint(""Start Training..."")\nfor epoch in range(30):\n    # \xe6\x88\x91\xe4\xbb\xac\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f\xe6\x9d\xa5\xe8\xae\xb0\xe5\xbd\x95\xe6\xaf\x8f100\xe4\xb8\xaabatch\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87loss\n    loss100 = 0.0\n    # \xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84dataloader\xe6\xb4\xbe\xe4\xb8\x8a\xe4\xba\x86\xe7\x94\xa8\xe5\x9c\xba\n    for i, data in enumerate(trainloader):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device) # \xe6\xb3\xa8\xe6\x84\x8f\xe9\x9c\x80\xe8\xa6\x81\xe5\xa4\x8d\xe5\x88\xb6\xe5\x88\xb0GPU\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        loss100 += loss.item()\n        if i % 100 == 99:\n            print(\'[Epoch %d, Batch %5d] loss: %.3f\' %\n                  (epoch + 1, i + 1, loss100 / 100))\n            loss100 = 0.0\n\nprint(""Done Training!"")\n\n# \xe6\x9e\x84\xe9\x80\xa0\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84dataloader\ndataiter = iter(testloader)\n# \xe9\xa2\x84\xe6\xb5\x8b\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xe5\x92\x8c\xe6\x80\xbb\xe6\x95\xb0\xe9\x87\x8f\ncorrect = 0\ntotal = 0\n# \xe4\xbd\xbf\xe7\x94\xa8torch.no_grad\xe7\x9a\x84\xe8\xaf\x9d\xe5\x9c\xa8\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb8\xad\xe4\xb8\x8d\xe8\xae\xb0\xe5\xbd\x95\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        images, labels = images.to(device), labels.to(device)\n        # \xe9\xa2\x84\xe6\xb5\x8b\n        outputs = net(images)\n        # \xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xae\x9e\xe9\x99\x85\xe4\xb8\x8a\xe6\x98\xaf\xe4\xb8\xaa\xe6\xa6\x82\xe7\x8e\x87\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x8e\xbb\xe6\x9c\x80\xe5\xa4\xa7\xe6\xa6\x82\xe7\x8e\x87\xe7\x9a\x84\xe5\x93\xaa\xe4\xb8\x80\xe9\xa1\xb9\xe4\xbd\x9c\xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe5\x88\x86\xe7\xb1\xbb\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(\'Accuracy of the network on the 10000 test images: %d %%\' % (\n    100 * correct / total))'"
examples/pytorch0.3&0.4/PyTorch_CUDA_CUDNN_Test.py,4,"b'# Summary: \xe6\xa3\x80\xe6\xb5\x8b\xe5\xbd\x93\xe5\x89\x8dPytorch\xe5\x92\x8c\xe8\xae\xbe\xe5\xa4\x87\xe6\x98\xaf\xe5\x90\xa6\xe6\x94\xaf\xe6\x8c\x81CUDA\xe5\x92\x8ccudnn\n# Author:  Amusi\n# Date:    2018-04-01\n\nimport torch\n\nif __name__ == \'__main__\':\n\tprint(""Support CUDA ?: "", torch.cuda.is_available())\n\tx = torch.Tensor([1.0])\n\txx = x.cuda()\n\tprint(xx)\n\n\ty = torch.randn(2, 3)\n\tyy = y.cuda()\n\tprint(yy)\n\n\tzz = xx + yy\n\tprint(zz)\n\n\t# CUDNN TEST\n\tfrom torch.backends import cudnn\n\tprint(""Support cudnn ?: "",cudnn.is_acceptable(xx))'"
examples/pytorch0.3&0.4/PyTorch_MNIST.py,9,"b'# Summary:   \n# Author:    Amusi\n# Date:      2018-03-31\n# Reference: https://blog.csdn.net/victoriaw/article/details/72354307\n\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch MNIST Example\')\nparser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\n                    help=\'input batch size for training (default: 64)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                    help=\'learning rate (default: 0.01)\')\nparser.add_argument(\'--momentum\', type=float, default=0.5, metavar=\'M\',\n                    help=\'SGD momentum (default: 0.5)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed) #\xe4\xb8\xbaCPU\xe8\xae\xbe\xe7\xbd\xae\xe7\xa7\x8d\xe5\xad\x90\xe7\x94\xa8\xe4\xba\x8e\xe7\x94\x9f\xe6\x88\x90\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbd\xbf\xe5\xbe\x97\xe7\xbb\x93\xe6\x9e\x9c\xe6\x98\xaf\xe7\xa1\xae\xe5\xae\x9a\xe7\x9a\x84\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)#\xe4\xb8\xba\xe5\xbd\x93\xe5\x89\x8dGPU\xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\xef\xbc\x9b\xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xbf\xe7\x94\xa8\xe5\xa4\x9a\xe4\xb8\xaaGPU\xef\xbc\x8c\xe5\xba\x94\xe8\xaf\xa5\xe4\xbd\xbf\xe7\x94\xa8torch.cuda.manual_seed_all()\xe4\xb8\xba\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84GPU\xe8\xae\xbe\xe7\xbd\xae\xe7\xa7\x8d\xe5\xad\x90\xe3\x80\x82\n\n\nkwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\n""""""\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe7\xbb\x84\xe5\x90\x88\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x92\x8c\xe9\x87\x87\xe6\xa0\xb7\xe5\x99\xa8\xef\xbc\x8c\xe6\x8f\x90\xe4\xbe\x9b\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8a\xe7\x9a\x84\xe5\x8d\x95\xe6\x88\x96\xe5\xa4\x9a\xe8\xbf\x9b\xe7\xa8\x8b\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\n\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9a\ndataset\xef\xbc\x9aDataset\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe4\xbb\x8e\xe5\x85\xb6\xe4\xb8\xad\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\nbatch_size\xef\xbc\x9aint\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\xaabatch\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa0\xb7\xe6\x9c\xac\nshuffle\xef\xbc\x9abool\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82\xe4\xb8\xbaTrue\xe6\x97\xb6\xe8\xa1\xa8\xe7\xa4\xba\xe6\xaf\x8f\xe4\xb8\xaaepoch\xe9\x83\xbd\xe5\xaf\xb9\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb4\x97\xe7\x89\x8c\nsampler\xef\xbc\x9aSampler\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82\xe4\xbb\x8e\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe9\x87\x87\xe6\xa0\xb7\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82\nnum_workers\xef\xbc\x9aint\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8\xe5\xa4\x9a\xe5\xb0\x91\xe5\xad\x90\xe8\xbf\x9b\xe7\xa8\x8b\xe3\x80\x82\xe9\xbb\x98\xe8\xae\xa4\xe5\x80\xbc\xe4\xb8\xba0\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe5\x9c\xa8\xe4\xb8\xbb\xe8\xbf\x9b\xe7\xa8\x8b\xe4\xb8\xad\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\ncollate_fn\xef\xbc\x9acallable\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82\npin_memory\xef\xbc\x9abool\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\ndrop_last\xef\xbc\x9abool\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x89\xe3\x80\x82True\xe8\xa1\xa8\xe7\xa4\xba\xe5\xa6\x82\xe6\x9e\x9c\xe6\x9c\x80\xe5\x90\x8e\xe5\x89\xa9\xe4\xb8\x8b\xe4\xb8\x8d\xe5\xae\x8c\xe5\x85\xa8\xe7\x9a\x84batch,\xe4\xb8\xa2\xe5\xbc\x83\xe3\x80\x82False\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x8d\xe4\xb8\xa2\xe5\xbc\x83\xe3\x80\x82\n""""""\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\'../data\', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\'../data\', train=False, transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)#\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba1\xe5\x92\x8c10\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)#\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba10\xe5\x92\x8c20\n        self.conv2_drop = nn.Dropout2d()#\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe4\xbf\xa1\xe9\x81\x93\xef\xbc\x8c\xe5\xb0\x86\xe5\x85\xb6\xe8\xae\xbe\xe4\xb8\xba0\n        self.fc1 = nn.Linear(320, 50)#\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba320\xe5\x92\x8c50\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))#conv->max_pool->relu\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))#conv->dropout->max_pool->relu\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))#fc->relu\n        x = F.dropout(x, training=self.training)#dropout\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\nmodel = Net()\nif args.cuda:\n    model.cuda()#\xe5\xb0\x86\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\xe7\xa7\xbb\xe5\x8a\xa8\xe5\x88\xb0GPU\xe4\xb8\x8a\n\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\ndef train(epoch):\n    model.train()#\xe6\x8a\x8amodule\xe8\xae\xbe\xe6\x88\x90training\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe5\xaf\xb9Dropout\xe5\x92\x8cBatchNorm\xe6\x9c\x89\xe5\xbd\xb1\xe5\x93\x8d\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)#Variable\xe7\xb1\xbb\xe5\xaf\xb9Tensor\xe5\xaf\xb9\xe8\xb1\xa1\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xb0\x81\xe8\xa3\x85\xef\xbc\x8c\xe4\xbc\x9a\xe4\xbf\x9d\xe5\xad\x98\xe8\xaf\xa5\xe5\xbc\xa0\xe9\x87\x8f\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x8a\xe5\xaf\xb9\xe7\x94\x9f\xe6\x88\x90\xe8\xaf\xa5\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0grad_fn\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbc\x95\xe7\x94\xa8\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe8\xaf\xa5\xe5\xbc\xa0\xe9\x87\x8f\xe6\x98\xaf\xe7\x94\xa8\xe6\x88\xb7\xe5\x88\x9b\xe5\xbb\xba\xe7\x9a\x84\xef\xbc\x8cgrad_fn\xe6\x98\xafNone\xef\xbc\x8c\xe7\xa7\xb0\xe8\xbf\x99\xe6\xa0\xb7\xe7\x9a\x84Variable\xe4\xb8\xba\xe5\x8f\xb6\xe5\xad\x90Variable\xe3\x80\x82\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)#\xe8\xb4\x9flog\xe4\xbc\xbc\xe7\x84\xb6\xe6\x8d\x9f\xe5\xa4\xb1\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\ndef test(epoch):\n    model.eval()#\xe6\x8a\x8amodule\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba\xe8\xaf\x84\xe4\xbc\xb0\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe5\x8f\xaa\xe5\xaf\xb9Dropout\xe5\x92\x8cBatchNorm\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9c\x89\xe5\xbd\xb1\xe5\x93\x8d\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += F.nll_loss(output, target).data[0]#Variable.data\n        pred = output.data.max(1)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data).cpu().sum()\n\n    test_loss = test_loss\n    test_loss /= len(test_loader) # loss function already averages over batch size\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nif __name__ == \'__main__\':\n    for epoch in range(1, args.epochs + 1):\n        train(epoch)\n        test(epoch)'"
examples/pytorch0.3&0.4/PyTorch_Tensors.py,7,"b'# Summary\xef\xbc\x9aAmusi\xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe8\xb7\x91PyTorch\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe6\x98\xaf\xe5\x9c\xa8Windows\xe4\xb8\x8a\n# Author:  Amusi\n# Date:    2018-03-31\n# Reference: http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-tensors\n\nimport torch\n\n\ndtype = torch.FloatTensor\n# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random input and output data\nx = torch.randn(N, D_in).type(dtype)\ny = torch.randn(N, D_out).type(dtype)\n\n# Randomly initialize weights\nw1 = torch.randn(D_in, H).type(dtype)\nw2 = torch.randn(H, D_out).type(dtype)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    # Forward pass: compute predicted y\n    h = x.mm(w1)\n    h_relu = h.clamp(min=0)\n    y_pred = h_relu.mm(w2)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum()\n    print(t, loss)\n\n    # Backprop to compute gradients of w1 and w2 with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.t().mm(grad_y_pred)\n    grad_h_relu = grad_y_pred.mm(w2.t())\n    grad_h = grad_h_relu.clone()\n    grad_h[h < 0] = 0\n    grad_w1 = x.t().mm(grad_h)\n\n    # Update weights using gradient descent\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2'"
