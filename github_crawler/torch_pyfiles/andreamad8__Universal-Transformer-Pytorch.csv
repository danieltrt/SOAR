file_path,api_count,code
main.py,4,"b'import argparse\nfrom torchtext import datasets\nfrom torchtext.datasets.babi import BABI20Field\nfrom models.UTransformer import BabiUTransformer\nfrom models.common_layer import NoamOpt\nimport torch.nn as nn\nimport torch\nimport numpy as np\nfrom copy import deepcopy\n\ndef parse_config():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--cuda"", action=""store_true"")\n    parser.add_argument(""--save_path"", type=str, default=""save/"")\n    parser.add_argument(""--task"", type=int, default=1)\n    parser.add_argument(""--run_avg"", type=int, default=10)\n    parser.add_argument(""--heads"", type=int, default=2)\n    parser.add_argument(""--depth"", type=int, default=128)\n    parser.add_argument(""--filter"", type=int, default=128)\n    parser.add_argument(""--max_hops"", type=int, default=6)\n    parser.add_argument(""--batch_size"", type=int, default=100)\n    parser.add_argument(""--emb"", type=int, default=128)\n    parser.add_argument(""--lr"", type=float, default=0.001)\n    parser.add_argument(""--act"", action=""store_true"")\n    parser.add_argument(""--act_loss_weight"", type=float, default=0.001)\n    parser.add_argument(""--noam"", action=""store_true"")\n    parser.add_argument(""--verbose"", action=""store_true"")\n    return parser.parse_args()\n\n\ndef get_babi_vocab(task):\n    text = BABI20Field(70)\n    train, val, test = datasets.BABI20.splits(text, root=\'.data\', task=task, joint=False,\n                                         tenK=True, only_supporting=False)\n    text.build_vocab(train)\n    vocab_len = len(text.vocab.freqs) \n    # print(""VOCAB LEN:"",vocab_len )\n    return vocab_len + 1\n\ndef evaluate(model, criterion, loader):\n    model.eval()\n    acc = []\n    loss = []\n    for b in loader:\n        story, query, answer = b.story,b.query,b.answer.squeeze()\n        if(config.cuda): story, query, answer = story.cuda(), query.cuda(), answer.cuda()\n        pred_prob = model(story, query)\n        loss.append(criterion(pred_prob[0], answer).item()) \n        pred = pred_prob[1].data.max(1)[1] # max func return (max, argmax)\n        acc.append( pred.eq(answer.data).cpu().numpy() ) \n\n    acc = np.concatenate(acc)\n    acc  = np.mean(acc)\n    loss = np.mean(loss)\n    return acc,loss\n\ndef main(config):\n    vocab_len = get_babi_vocab(config.task)\n    train_iter, val_iter, test_iter = datasets.BABI20.iters(batch_size=config.batch_size, \n                                                            root=\'.data\', \n                                                            memory_size=70, \n                                                            task=config.task, \n                                                            joint=False,\n                                                            tenK=False, \n                                                            only_supporting=False, \n                                                            sort=False, \n                                                            shuffle=True)\n    model = BabiUTransformer(num_vocab=vocab_len, \n                    embedding_size=config.emb, \n                    hidden_size=config.emb, \n                    num_layers=config.max_hops,\n                    num_heads=config.heads, \n                    total_key_depth=config.depth, \n                    total_value_depth=config.depth,\n                    filter_size=config.filter,\n                    act=config.act)\n    if(config.verbose):\n        print(model)\n        print(""ACT"",config.act)\n    if(config.cuda): model.cuda()       \n    \n    criterion = nn.CrossEntropyLoss()\n    if(config.noam):\n        opt = NoamOpt(config.emb, 1, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n    else:\n        opt = torch.optim.Adam(model.parameters(),lr=config.lr)\n\n    if(config.verbose):\n        acc_val, loss_val = evaluate(model, criterion, val_iter)\n        print(""RAND_VAL ACC:{:.4f}\\t RAND_VAL LOSS:{:.4f}"".format(acc_val, loss_val))\n    correct = []\n    loss_nb = []\n    cnt_batch = 0\n    avg_best = 0\n    cnt = 0\n    model.train()\n    for b in train_iter:\n        story, query, answer = b.story,b.query,b.answer.squeeze()\n        if(config.cuda): story, query, answer = story.cuda(), query.cuda(), answer.cuda()\n        if(config.noam):\n            opt.optimizer.zero_grad()\n        else:\n            opt.zero_grad()\n        pred_prob = model(story, query)\n        loss = criterion(pred_prob[0], answer)\n        if(config.act):\n            R_t = pred_prob[2][0] \n            N_t = pred_prob[2][1]\n            p_t = R_t + N_t\n            avg_p_t = torch.sum(torch.sum(p_t,dim=1)/p_t.size(1))/p_t.size(0)\n            loss += config.act_loss_weight * avg_p_t.item()\n\n        loss.backward()\n        opt.step()\n\n        ## LOG\n        loss_nb.append(loss.item())\n        pred = pred_prob[1].data.max(1)[1] # max func return (max, argmax)\n        correct.append(np.mean(pred.eq(answer.data).cpu().numpy()))\n        cnt_batch += 1\n        if(cnt_batch % 10 == 0):\n            acc = np.mean(correct)\n            loss_nb = np.mean(loss_nb)\n            if(config.verbose):\n                print(""TRN ACC:{:.4f}\\tTRN LOSS:{:.4f}"".format(acc, loss_nb))\n\n            acc_val, loss_val = evaluate(model, criterion, val_iter)\n            if(config.verbose):\n                print(""VAL ACC:{:.4f}\\tVAL LOSS:{:.4f}"".format(acc_val, loss_val))\n\n            if(acc_val > avg_best):\n                avg_best = acc_val\n                weights_best = deepcopy(model.state_dict())\n                cnt = 0\n            else:\n                cnt += 1\n            if(cnt == 45): break\n            if(avg_best == 1.0): break \n\n            correct = []\n            loss_nb = []\n            cnt_batch = 0\n\n\n    model.load_state_dict({ name: weights_best[name] for name in weights_best })\n    acc_test, loss_test = evaluate(model, criterion, test_iter)\n    if(config.verbose):\n        print(""TST ACC:{:.4f}\\tTST LOSS:{:.4f}"".format(acc_val, loss_val))  \n    return acc_test\n\nif __name__ == ""__main__"":\n    config = parse_config()\n    for t in range(1,21):\n        config.task = t\n        acc = []\n        for i in range(config.run_avg):\n            acc.append(main(config))\n        print(""Noam"",config.noam,""ACT"",config.act,""Task:"",config.task,""Max:"",max(acc),""Mean:"",np.mean(acc),""Std:"",np.std(acc))\n\n'"
models/UTransformer.py,13,"b'### TAKEN FROM https://github.com/kolloldas/torchnlp\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.nn.init as I\nimport numpy as np\nimport math\nfrom models.common_layer import EncoderLayer ,DecoderLayer ,MultiHeadAttention ,Conv ,PositionwiseFeedForward ,LayerNorm ,_gen_bias_mask ,_gen_timing_signal\n\nclass BabiUTransformer(nn.Module):\n    """"""\n    A Transformer Module For BabI data. \n    Inputs should be in the shape story: [batch_size, memory_size, story_len ]\n                                  query: [batch_size, 1, story_len]\n    Outputs will have the shape [batch_size, ]\n    """"""\n    def __init__(self, num_vocab, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n                 filter_size, max_length=71, input_dropout=0.0, layer_dropout=0.0, \n                 attention_dropout=0.0, relu_dropout=0.0, use_mask=False, act=False ):\n        super(BabiUTransformer, self).__init__()\n        self.embedding_dim = embedding_size\n        self.emb = nn.Embedding(num_vocab, embedding_size, padding_idx=0)\n        self.transformer_enc = Encoder(embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n                                filter_size, max_length=71, input_dropout=input_dropout, layer_dropout=layer_dropout, \n                                attention_dropout=attention_dropout, relu_dropout=relu_dropout, use_mask=False, act=act)\n\n        self.W = nn.Linear(self.embedding_dim,num_vocab)\n\n\n        # Share the weight matrix between target word embedding & the final logit dense layer\n        self.W.weight = self.emb.weight\n        \n        self.softmax = nn.Softmax(dim=1)\n        ## POSITIONAL MASK\n        self.mask = nn.Parameter(I.constant_(torch.empty(11, self.embedding_dim), 1))\n\n    def forward(self,story, query):\n\n        story_size = story.size()\n        ## STORY ENCODER + MUlt Mask\n        embed = self.emb(story.view(story.size(0), -1))\n        embed = embed.view(story_size+(embed.size(-1),))\n        embed_story = torch.sum(embed*self.mask[:story.size(2),:].unsqueeze(0), 2)\n\n        ## QUERY ENCODER + MUlt Mask\n        query_embed = self.emb(query)\n        embed_query = torch.sum(query_embed.unsqueeze(1)*self.mask[:query.size(1),:], 2)\n\n        ## CONCAT STORY AND QUERY\n        embed = torch.cat([embed_story, embed_query],dim=1)\n\n        ## APPLY TRANSFORMER\n        logit, act = self.transformer_enc(embed)\n        \n        a_hat = self.W(torch.sum(logit,dim=1)/logit.size(1)) ## reduce mean\n\n        return a_hat, self.softmax(a_hat), act\n\n\n\nclass Encoder(nn.Module):\n    """"""\n    A Transformer Encoder module. \n    Inputs should be in the shape [batch_size, length, hidden_size]\n    Outputs will have the shape [batch_size, length, hidden_size]\n    Refer Fig.1 in https://arxiv.org/pdf/1706.03762.pdf\n    """"""\n    def __init__(self, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n                 filter_size, max_length=100, input_dropout=0.0, layer_dropout=0.0, \n                 attention_dropout=0.0, relu_dropout=0.0, use_mask=False, act=False):\n        """"""\n        Parameters:\n            embedding_size: Size of embeddings\n            hidden_size: Hidden size\n            num_layers: Total layers in the Encoder\n            num_heads: Number of attention heads\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            filter_size: Hidden size of the middle layer in FFN\n            max_length: Max sequence length (required for timing signal)\n            input_dropout: Dropout just after embedding\n            layer_dropout: Dropout for each layer\n            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n            use_mask: Set to True to turn on future value masking\n        """"""\n        \n        super(Encoder, self).__init__()\n        \n        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n        ## for t\n        self.position_signal = _gen_timing_signal(num_layers, hidden_size)\n\n        self.num_layers = num_layers\n        self.act = act\n        params =(hidden_size, \n                 total_key_depth or hidden_size,\n                 total_value_depth or hidden_size,\n                 filter_size, \n                 num_heads, \n                 _gen_bias_mask(max_length) if use_mask else None,\n                 layer_dropout, \n                 attention_dropout, \n                 relu_dropout)\n\n        self.proj_flag = False\n        if(embedding_size == hidden_size):\n            self.embedding_proj = nn.Linear(embedding_size, hidden_size, bias=False)\n            self.proj_flag = True\n\n        self.enc = EncoderLayer(*params)\n        \n        self.layer_norm = LayerNorm(hidden_size)\n        self.input_dropout = nn.Dropout(input_dropout)\n        if(self.act):\n            self.act_fn = ACT_basic(hidden_size)\n\n    def forward(self, inputs):\n\n        #Add input dropout\n        x = self.input_dropout(inputs)\n\n        if(self.proj_flag):\n            # Project to hidden size\n            x = self.embedding_proj(x)\n\n        if(self.act):\n            x, (remainders,n_updates) = self.act_fn(x, inputs, self.enc, self.timing_signal, self.position_signal, self.num_layers)\n            return x, (remainders,n_updates)\n        else:\n            for l in range(self.num_layers):\n                x += self.timing_signal[:, :inputs.shape[1], :].type_as(inputs.data)\n                x += self.position_signal[:, l, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.data)\n                x = self.enc(x)\n            return x, None\n\ndef get_attn_key_pad_mask(seq_k, seq_q):\n    \'\'\' For masking out the padding part of key sequence. \'\'\'\n    # Expand to fit the shape of key query attention matrix.\n    len_q = seq_q.size(1)\n    PAD = 0\n    padding_mask = seq_k.eq(PAD)\n    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n\n    return padding_mask\n\nclass Decoder(nn.Module):\n    """"""\n    A Transformer Decoder module. \n    Inputs should be in the shape [batch_size, length, hidden_size]\n    Outputs will have the shape [batch_size, length, hidden_size]\n    Refer Fig.1 in https://arxiv.org/pdf/1706.03762.pdf\n    """"""\n    def __init__(self, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n                 filter_size, max_length=100, input_dropout=0.0, layer_dropout=0.0, \n                 attention_dropout=0.0, relu_dropout=0.0, act=False):\n        """"""\n        Parameters:\n            embedding_size: Size of embeddings\n            hidden_size: Hidden size\n            num_layers: Total layers in the Encoder\n            num_heads: Number of attention heads\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            filter_size: Hidden size of the middle layer in FFN\n            max_length: Max sequence length (required for timing signal)\n            input_dropout: Dropout just after embedding\n            layer_dropout: Dropout for each layer\n            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n        """"""\n        \n        super(Decoder, self).__init__()\n        \n        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n        self.position_signal = _gen_timing_signal(num_layers, hidden_size)\n        self.num_layers = num_layers\n        self.act = act\n        params =(hidden_size, \n                 total_key_depth or hidden_size,\n                 total_value_depth or hidden_size,\n                 filter_size, \n                 num_heads, \n                 _gen_bias_mask(max_length), # mandatory\n                 layer_dropout, \n                 attention_dropout, \n                 relu_dropout)\n\n        self.proj_flag = False\n        if(embedding_size == hidden_size):\n            self.embedding_proj = nn.Linear(embedding_size, hidden_size, bias=False)\n            self.proj_flag = True\n        self.dec = DecoderLayer(*params) \n        \n        self.layer_norm = LayerNorm(hidden_size)\n        self.input_dropout = nn.Dropout(input_dropout)\n        if(self.act):\n            self.act_fn = ACT_basic(hidden_size)\n    \n    def forward(self, inputs, encoder_output):\n        #Add input dropout\n        x = self.input_dropout(inputs)\n        \n        if(self.proj_flag):\n            # Project to hidden size\n            x = self.embedding_proj(x)\n        \n        if(self.act):\n            x, (remainders,n_updates) = self.act_fn(x, inputs, self.dec, self.timing_signal, self.position_signal, self.num_layers, encoder_output)\n            return x, (remainders,n_updates)\n        else:\n            for l in range(self.num_layers):\n                x += self.timing_signal[:, :inputs.shape[1], :].type_as(inputs.data)\n                x += self.position_signal[:, l, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.data)\n                x, _ = self.dec((x, encoder_output))\n        return x\n\n\n\n### CONVERTED FROM https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer_util.py#L1062\nclass ACT_basic(nn.Module):\n    def __init__(self,hidden_size):\n        super(ACT_basic, self).__init__()\n        self.sigma = nn.Sigmoid()\n        self.p = nn.Linear(hidden_size,1)  \n        self.p.bias.data.fill_(1) \n        self.threshold = 1 - 0.1\n\n    def forward(self, state, inputs, fn, time_enc, pos_enc, max_hop, encoder_output=None):\n        # init_hdd\n        ## [B, S]\n        halting_probability = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n        ## [B, S\n        remainders = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n        ## [B, S]\n        n_updates = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n        ## [B, S, HDD]\n        previous_state = torch.zeros_like(inputs).cuda()\n        step = 0\n        # for l in range(self.num_layers):\n        while( ((halting_probability<self.threshold) & (n_updates < max_hop)).byte().any()):\n            # Add timing signal\n            state = state + time_enc[:, :inputs.shape[1], :].type_as(inputs.data)\n            state = state + pos_enc[:, step, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.data)\n\n            p = self.sigma(self.p(state)).squeeze(-1)\n            # Mask for inputs which have not halted yet\n            still_running = (halting_probability < 1.0).float()\n\n            # Mask of inputs which halted at this step\n            new_halted = (halting_probability + p * still_running > self.threshold).float() * still_running\n\n            # Mask of inputs which haven\'t halted, and didn\'t halt this step\n            still_running = (halting_probability + p * still_running <= self.threshold).float() * still_running\n\n            # Add the halting probability for this step to the halting\n            # probabilities for those input which haven\'t halted yet\n            halting_probability = halting_probability + p * still_running\n\n            # Compute remainders for the inputs which halted at this step\n            remainders = remainders + new_halted * (1 - halting_probability)\n\n            # Add the remainders to those inputs which halted at this step\n            halting_probability = halting_probability + new_halted * remainders\n\n            # Increment n_updates for all inputs which are still running\n            n_updates = n_updates + still_running + new_halted\n\n            # Compute the weight to be applied to the new state and output\n            # 0 when the input has already halted\n            # p when the input hasn\'t halted yet\n            # the remainders when it halted this step\n            update_weights = p * still_running + new_halted * remainders\n\n            if(encoder_output):\n                state, _ = fn((state,encoder_output))\n            else:\n                # apply transformation on the state\n                state = fn(state)\n\n            # update running part in the weighted state and keep the rest\n            previous_state = ((state * update_weights.unsqueeze(-1)) + (previous_state * (1 - update_weights.unsqueeze(-1))))\n            ## previous_state is actually the new_state at end of hte loop \n            ## to save a line I assigned to previous_state so in the next \n            ## iteration is correct. Notice that indeed we return previous_state\n            step+=1\n        return previous_state, (remainders,n_updates)\n'"
models/common_layer.py,12,"b'### MOSTO OF IT TAKEN FROM https://github.com/kolloldas/torchnlp\n## MINOR CHANGES\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.nn.init as I\nimport numpy as np\nimport math\n\n\nclass EncoderLayer(nn.Module):\n    """"""\n    Represents one Encoder layer of the Transformer Encoder\n    Refer Fig. 1 in https://arxiv.org/pdf/1706.03762.pdf\n    NOTE: The layer normalization step has been moved to the input as per latest version of T2T\n    """"""\n    def __init__(self, hidden_size, total_key_depth, total_value_depth, filter_size, num_heads,\n                 bias_mask=None, layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0):\n        """"""\n        Parameters:\n            hidden_size: Hidden size\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            filter_size: Hidden size of the middle layer in FFN\n            num_heads: Number of attention heads\n            bias_mask: Masking tensor to prevent connections to future elements\n            layer_dropout: Dropout for this layer\n            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n        """"""\n        \n        super(EncoderLayer, self).__init__()\n        \n        self.multi_head_attention = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth, \n                                                       hidden_size, num_heads, bias_mask, attention_dropout)\n        \n        self.positionwise_feed_forward = PositionwiseFeedForward(hidden_size, filter_size, hidden_size,\n                                                                 layer_config=\'cc\', padding = \'both\', \n                                                                 dropout=relu_dropout)\n        self.dropout = nn.Dropout(layer_dropout)\n        self.layer_norm_mha = LayerNorm(hidden_size)\n        self.layer_norm_ffn = LayerNorm(hidden_size)\n        \n    def forward(self, inputs):\n        x = inputs\n        \n        # Layer Normalization\n        x_norm = self.layer_norm_mha(x)\n        \n        # Multi-head attention\n        y = self.multi_head_attention(x_norm, x_norm, x_norm)\n        \n        # Dropout and residual\n        x = self.dropout(x + y)\n        \n        # Layer Normalization\n        x_norm = self.layer_norm_ffn(x)\n        \n        # Positionwise Feedforward\n        y = self.positionwise_feed_forward(x_norm)\n        \n        # Dropout and residual\n        y = self.dropout(x + y)\n        \n        return y\n\nclass DecoderLayer(nn.Module):\n    """"""\n    Represents one Decoder layer of the Transformer Decoder\n    Refer Fig. 1 in https://arxiv.org/pdf/1706.03762.pdf\n    NOTE: The layer normalization step has been moved to the input as per latest version of T2T\n    """"""\n    def __init__(self, hidden_size, total_key_depth, total_value_depth, filter_size, num_heads,\n                 bias_mask, layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0):\n        """"""\n        Parameters:\n            hidden_size: Hidden size\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            filter_size: Hidden size of the middle layer in FFN\n            num_heads: Number of attention heads\n            bias_mask: Masking tensor to prevent connections to future elements\n            layer_dropout: Dropout for this layer\n            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n        """"""\n        \n        super(DecoderLayer, self).__init__()\n        \n        self.multi_head_attention_dec = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth, \n                                                       hidden_size, num_heads, bias_mask, attention_dropout)\n\n        self.multi_head_attention_enc_dec = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth, \n                                                       hidden_size, num_heads, None, attention_dropout)\n        \n        self.positionwise_feed_forward = PositionwiseFeedForward(hidden_size, filter_size, hidden_size,\n                                                                 layer_config=\'cc\', padding = \'left\', \n                                                                 dropout=relu_dropout)\n        self.dropout = nn.Dropout(layer_dropout)\n        self.layer_norm_mha_dec = LayerNorm(hidden_size)\n        self.layer_norm_mha_enc = LayerNorm(hidden_size)\n        self.layer_norm_ffn = LayerNorm(hidden_size)\n\n        \n    def forward(self, inputs):\n        """"""\n        NOTE: Inputs is a tuple consisting of decoder inputs and encoder output\n        """"""\n        x, encoder_outputs = inputs\n        \n        # Layer Normalization before decoder self attention\n        x_norm = self.layer_norm_mha_dec(x)\n        \n        # Masked Multi-head attention\n        y = self.multi_head_attention_dec(x_norm, x_norm, x_norm)\n        \n        # Dropout and residual after self-attention\n        x = self.dropout(x + y)\n\n        # Layer Normalization before encoder-decoder attention\n        x_norm = self.layer_norm_mha_enc(x)\n\n        # Multi-head encoder-decoder attention\n        y = self.multi_head_attention_enc_dec(x_norm, encoder_outputs, encoder_outputs)\n        \n        # Dropout and residual after encoder-decoder attention\n        x = self.dropout(x + y)\n        \n        # Layer Normalization\n        x_norm = self.layer_norm_ffn(x)\n        \n        # Positionwise Feedforward\n        y = self.positionwise_feed_forward(x_norm)\n        \n        # Dropout and residual after positionwise feed forward layer\n        y = self.dropout(x + y)\n        \n        # Return encoder outputs as well to work with nn.Sequential\n        return y, encoder_outputs\n\n\n\nclass MultiHeadAttention(nn.Module):\n    """"""\n    Multi-head attention as per https://arxiv.org/pdf/1706.03762.pdf\n    Refer Figure 2\n    """"""\n    def __init__(self, input_depth, total_key_depth, total_value_depth, output_depth, \n                 num_heads, bias_mask=None, dropout=0.0):\n        """"""\n        Parameters:\n            input_depth: Size of last dimension of input\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            num_heads: Number of attention heads\n            bias_mask: Masking tensor to prevent connections to future elements\n            dropout: Dropout probability (Should be non-zero only during training)\n        """"""\n        super(MultiHeadAttention, self).__init__()\n        # Checks borrowed from \n        # https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n        if total_key_depth % num_heads != 0:\n            raise ValueError(""Key depth (%d) must be divisible by the number of ""\n                             ""attention heads (%d)."" % (total_key_depth, num_heads))\n        if total_value_depth % num_heads != 0:\n            raise ValueError(""Value depth (%d) must be divisible by the number of ""\n                             ""attention heads (%d)."" % (total_value_depth, num_heads))\n            \n        self.num_heads = num_heads\n        self.query_scale = (total_key_depth//num_heads)**-0.5\n        self.bias_mask = bias_mask\n        \n        # Key and query depth will be same\n        self.query_linear = nn.Linear(input_depth, total_key_depth, bias=False)\n        self.key_linear = nn.Linear(input_depth, total_key_depth, bias=False)\n        self.value_linear = nn.Linear(input_depth, total_value_depth, bias=False)\n        self.output_linear = nn.Linear(total_value_depth, output_depth, bias=False)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def _split_heads(self, x):\n        """"""\n        Split x such to add an extra num_heads dimension\n        Input:\n            x: a Tensor with shape [batch_size, seq_length, depth]\n        Returns:\n            A Tensor with shape [batch_size, num_heads, seq_length, depth/num_heads]\n        """"""\n        if len(x.shape) != 3:\n            raise ValueError(""x must have rank 3"")\n        shape = x.shape\n        return x.view(shape[0], shape[1], self.num_heads, shape[2]//self.num_heads).permute(0, 2, 1, 3)\n    \n    def _merge_heads(self, x):\n        """"""\n        Merge the extra num_heads into the last dimension\n        Input:\n            x: a Tensor with shape [batch_size, num_heads, seq_length, depth/num_heads]\n        Returns:\n            A Tensor with shape [batch_size, seq_length, depth]\n        """"""\n        if len(x.shape) != 4:\n            raise ValueError(""x must have rank 4"")\n        shape = x.shape\n        return x.permute(0, 2, 1, 3).contiguous().view(shape[0], shape[2], shape[3]*self.num_heads)\n        \n    def forward(self, queries, keys, values, src_mask=None):\n        \n        # Do a linear for each component\n        queries = self.query_linear(queries)\n        keys = self.key_linear(keys)\n        values = self.value_linear(values)\n        \n        # Split into multiple heads\n        queries = self._split_heads(queries)\n        keys = self._split_heads(keys)\n        values = self._split_heads(values)\n        \n        # Scale queries\n        queries *= self.query_scale\n        \n        # Combine queries and keys\n        logits = torch.matmul(queries, keys.permute(0, 1, 3, 2))\n        \n\n        if src_mask is not None:\n            logits = logits.masked_fill(src_mask, -np.inf)\n            \n        # Add bias to mask future values\n        if self.bias_mask is not None:\n            logits += self.bias_mask[:, :, :logits.shape[-2], :logits.shape[-1]].type_as(logits.data)\n        \n        # Convert to probabilites\n        weights = nn.functional.softmax(logits, dim=-1)\n        \n        # Dropout\n        weights = self.dropout(weights)\n        \n        # Combine with values to get context\n        contexts = torch.matmul(weights, values)\n        \n        # Merge heads\n        contexts = self._merge_heads(contexts)\n        #contexts = torch.tanh(contexts)\n        \n        # Linear to get output\n        outputs = self.output_linear(contexts)\n        \n        return outputs\n\nclass Conv(nn.Module):\n    """"""\n    Convenience class that does padding and convolution for inputs in the format\n    [batch_size, sequence length, hidden size]\n    """"""\n    def __init__(self, input_size, output_size, kernel_size, pad_type):\n        """"""\n        Parameters:\n            input_size: Input feature size\n            output_size: Output feature size\n            kernel_size: Kernel width\n            pad_type: left -> pad on the left side (to mask future data), \n                      both -> pad on both sides\n        """"""\n        super(Conv, self).__init__()\n        padding = (kernel_size - 1, 0) if pad_type == \'left\' else (kernel_size//2, (kernel_size - 1)//2)\n        self.pad = nn.ConstantPad1d(padding, 0)\n        self.conv = nn.Conv1d(input_size, output_size, kernel_size=kernel_size, padding=0)\n\n    def forward(self, inputs):\n        inputs = self.pad(inputs.permute(0, 2, 1))\n        outputs = self.conv(inputs).permute(0, 2, 1)\n\n        return outputs\n\n\nclass PositionwiseFeedForward(nn.Module):\n    """"""\n    Does a Linear + RELU + Linear on each of the timesteps\n    """"""\n    def __init__(self, input_depth, filter_size, output_depth, layer_config=\'ll\', padding=\'left\', dropout=0.0):\n        """"""\n        Parameters:\n            input_depth: Size of last dimension of input\n            filter_size: Hidden size of the middle layer\n            output_depth: Size last dimension of the final output\n            layer_config: ll -> linear + ReLU + linear\n                          cc -> conv + ReLU + conv etc.\n            padding: left -> pad on the left side (to mask future data), \n                     both -> pad on both sides\n            dropout: Dropout probability (Should be non-zero only during training)\n        """"""\n        super(PositionwiseFeedForward, self).__init__()\n        \n        layers = []\n        sizes = ([(input_depth, filter_size)] + \n                 [(filter_size, filter_size)]*(len(layer_config)-2) + \n                 [(filter_size, output_depth)])\n\n        for lc, s in zip(list(layer_config), sizes):\n            if lc == \'l\':\n                layers.append(nn.Linear(*s))\n            elif lc == \'c\':\n                layers.append(Conv(*s, kernel_size=3, pad_type=padding))\n            else:\n                raise ValueError(""Unknown layer type {}"".format(lc))\n\n        self.layers = nn.ModuleList(layers)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, inputs):\n        x = inputs\n        for i, layer in enumerate(self.layers):\n            x = layer(x)\n            if i < len(self.layers):\n                x = self.relu(x)\n                x = self.dropout(x)\n\n        return x\n\n\nclass LayerNorm(nn.Module):\n    # Borrowed from jekbradbury\n    # https://github.com/pytorch/pytorch/issues/1959\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\n\ndef _gen_bias_mask(max_length):\n    """"""\n    Generates bias values (-Inf) to mask future timesteps during attention\n    """"""\n    np_mask = np.triu(np.full([max_length, max_length], -np.inf), 1)\n    torch_mask = torch.from_numpy(np_mask).type(torch.FloatTensor)\n    \n    return torch_mask.unsqueeze(0).unsqueeze(1)\n\ndef _gen_timing_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n    """"""\n    Generates a [1, length, channels] timing signal consisting of sinusoids\n    Adapted from:\n    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n    """"""\n    position = np.arange(length)\n    num_timescales = channels // 2\n    log_timescale_increment = ( math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales) - 1))\n    inv_timescales = min_timescale * np.exp(np.arange(num_timescales).astype(np.float) * -log_timescale_increment)\n    scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)\n\n    signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)\n    signal = np.pad(signal, [[0, 0], [0, channels % 2]], \n                    \'constant\', constant_values=[0.0, 0.0])\n    signal =  signal.reshape([1, length, channels])\n\n    return torch.from_numpy(signal).type(torch.FloatTensor)\n\n\ndef position_encoding(sentence_size, embedding_dim):\n    encoding = np.ones((embedding_dim, sentence_size), dtype=np.float32)\n    ls = sentence_size + 1\n    le = embedding_dim + 1\n    for i in range(1, le):\n        for j in range(1, ls):\n            encoding[i-1, j-1] = (i - (embedding_dim+1)/2) * (j - (sentence_size+1)/2)\n    encoding = 1 + 4 * encoding / embedding_dim / sentence_size\n    # Make position encoding of time words identity to avoid modifying them\n    # encoding[:, -1] = 1.0\n    return np.transpose(encoding)\n\n\nclass LabelSmoothing(nn.Module):\n    ""Implement label smoothing.""\n    def __init__(self, size, padding_idx, smoothing=0.0):\n        super(LabelSmoothing, self).__init__()\n        self.criterion = nn.KLDivLoss(reduction=\'sum\')\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n        \n    def forward(self, x, target):\n        assert x.size(1) == self.size\n        true_dist = x.data.clone()\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() > 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        return self.criterion(x, true_dist)\n\n\nclass NoamOpt:\n    ""Optim wrapper that implements rate.""\n    def __init__(self, model_size, factor, warmup, optimizer):\n        self.optimizer = optimizer\n        self._step = 0\n        self.warmup = warmup\n        self.factor = factor\n        self.model_size = model_size\n        self._rate = 0\n        \n    def step(self):\n        ""Update parameters and rate""\n        self._step += 1\n        rate = self.rate()\n        for p in self.optimizer.param_groups:\n            p[\'lr\'] = rate\n        self._rate = rate\n        self.optimizer.step()\n        \n    def rate(self, step = None):\n        ""Implement `lrate` above""\n        if step is None:\n            step = self._step\n        return self.factor * \\\n            (self.model_size ** (-0.5) *\n            min(step ** (-0.5), step * self.warmup ** (-1.5)))'"
