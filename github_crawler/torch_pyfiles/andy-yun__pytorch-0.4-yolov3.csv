file_path,api_count,code
cfg.py,10,"b'import torch\nfrom utils import convert2cpu\n\ndef parse_cfg(cfgfile):\n    blocks = []\n    fp = open(cfgfile, \'r\')\n    block =  None\n    line = fp.readline()\n    while line != \'\':\n        line = line.rstrip()\n        if line == \'\' or line[0] == \'#\':\n            line = fp.readline()\n            continue        \n        elif line[0] == \'[\':\n            if block:\n                blocks.append(block)\n            block = dict()\n            block[\'type\'] = line.lstrip(\'[\').rstrip(\']\')\n            # set default value\n            if block[\'type\'] == \'convolutional\':\n                block[\'batch_normalize\'] = 0\n        else:\n            key,value = line.split(\'=\')\n            key = key.strip()\n            if key == \'type\':\n                key = \'_type\'\n            value = value.strip()\n            block[key] = value\n        line = fp.readline()\n\n    if block:\n        blocks.append(block)\n    fp.close()\n    return blocks\n\ndef print_cfg(blocks):\n    print(\'layer     filters    size              input                output\');\n    prev_width = 416\n    prev_height = 416\n    prev_filters = 3\n    out_filters =[]\n    out_widths =[]\n    out_heights =[]\n    ind = -2\n    for block in blocks:\n        ind = ind + 1\n        if block[\'type\'] == \'net\':\n            prev_width = int(block[\'width\'])\n            prev_height = int(block[\'height\'])\n            continue\n        elif block[\'type\'] == \'convolutional\':\n            filters = int(block[\'filters\'])\n            kernel_size = int(block[\'size\'])\n            stride = int(block[\'stride\'])\n            is_pad = int(block[\'pad\'])\n            pad = (kernel_size-1)//2 if is_pad else 0\n            width = (prev_width + 2*pad - kernel_size)//stride + 1\n            height = (prev_height + 2*pad - kernel_size)//stride + 1\n            print(\'%5d %-6s %4d  %d x %d / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (ind, \'conv\', filters, kernel_size, kernel_size, stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'maxpool\':\n            pool_size = int(block[\'size\'])\n            stride = int(block[\'stride\'])\n            width = prev_width//stride\n            height = prev_height//stride\n            print(\'%5d %-6s       %d x %d / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (ind, \'max\', pool_size, pool_size, stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'avgpool\':\n            width = 1\n            height = 1\n            print(\'%5d %-6s                   %3d x %3d x%4d   ->  %3d\' % (ind, \'avg\', prev_width, prev_height, prev_filters,  prev_filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'softmax\':\n            print(\'%5d %-6s                                    ->  %3d\' % (ind, \'softmax\', prev_filters))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'cost\':\n            print(\'%5d %-6s                                     ->  %3d\' % (ind, \'cost\', prev_filters))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'reorg\':\n            stride = int(block[\'stride\'])\n            filters = stride * stride * prev_filters\n            width = prev_width//stride\n            height = prev_height//stride\n            print(\'%5d %-6s             / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (ind, \'reorg\', stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'upsample\':\n            stride = int(block[\'stride\'])\n            filters = prev_filters\n            width = prev_width*stride\n            height = prev_height*stride\n            print(\'%5d %-6s           * %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (ind, \'upsample\', stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)           \n        elif block[\'type\'] == \'route\':\n            layers = block[\'layers\'].split(\',\')\n            layers = [int(i) if int(i) > 0 else int(i)+ind for i in layers]\n            if len(layers) == 1:\n                print(\'%5d %-6s %d\' % (ind, \'route\', layers[0]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                prev_filters = out_filters[layers[0]]\n            elif len(layers) == 2:\n                print(\'%5d %-6s %d %d\' % (ind, \'route\', layers[0], layers[1]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                assert(prev_width == out_widths[layers[1]])\n                assert(prev_height == out_heights[layers[1]])\n                prev_filters = out_filters[layers[0]] + out_filters[layers[1]]\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] in [\'region\', \'yolo\']:\n            print(\'%5d %-6s\' % (ind, \'detection\'))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'shortcut\':\n            from_id = int(block[\'from\'])\n            from_id = from_id if from_id > 0 else from_id+ind\n            print(\'%5d %-6s %d\' % (ind, \'shortcut\', from_id))\n            prev_width = out_widths[from_id]\n            prev_height = out_heights[from_id]\n            prev_filters = out_filters[from_id]\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'connected\':\n            filters = int(block[\'output\'])\n            print(\'%5d %-6s                            %d  ->  %3d\' % (ind, \'connected\', prev_filters,  filters))\n            prev_filters = filters\n            out_widths.append(1)\n            out_heights.append(1)\n            out_filters.append(prev_filters)\n        else:\n            print(\'unknown type %s\' % (block[\'type\']))\n\ndef load_conv(buf, start, conv_model):\n\n    num_w = conv_model.weight.numel()\n    num_b = conv_model.bias.numel()\n    #print(""start: {}, num_w: {}, num_b: {}"".format(start, num_w, num_b))\n    # by ysyun, use .view_as()\n    conv_model.bias.data.copy_(torch.from_numpy(buf[start:start+num_b]).view_as(conv_model.bias.data));   start = start + num_b\n    conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w]).view_as(conv_model.weight.data)); start = start + num_w\n    return start\n\ndef save_conv(fp, conv_model):\n    if conv_model.bias.is_cuda:\n        convert2cpu(conv_model.bias.data).numpy().tofile(fp)\n        convert2cpu(conv_model.weight.data).numpy().tofile(fp)\n    else:\n        conv_model.bias.data.numpy().tofile(fp)\n        conv_model.weight.data.numpy().tofile(fp)\n\ndef load_conv_bn(buf, start, conv_model, bn_model):\n    num_w = conv_model.weight.numel()\n    num_b = bn_model.bias.numel()\n    bn_model.bias.data.copy_(torch.from_numpy(buf[start:start+num_b]));     start = start + num_b\n    bn_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_b]));   start = start + num_b\n    bn_model.running_mean.copy_(torch.from_numpy(buf[start:start+num_b]));  start = start + num_b\n    bn_model.running_var.copy_(torch.from_numpy(buf[start:start+num_b]));   start = start + num_b\n    #conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w])); start = start + num_w\n    conv_model.weight.data.copy_(torch.from_numpy(buf[start:start + num_w]).view_as(conv_model.weight.data)); start = start + num_w\n    return start\n\ndef save_conv_bn(fp, conv_model, bn_model):\n    if bn_model.bias.is_cuda:\n        convert2cpu(bn_model.bias.data).numpy().tofile(fp)\n        convert2cpu(bn_model.weight.data).numpy().tofile(fp)\n        convert2cpu(bn_model.running_mean).numpy().tofile(fp)\n        convert2cpu(bn_model.running_var).numpy().tofile(fp)\n        convert2cpu(conv_model.weight.data).numpy().tofile(fp)\n    else:\n        bn_model.bias.data.numpy().tofile(fp)\n        bn_model.weight.data.numpy().tofile(fp)\n        bn_model.running_mean.numpy().tofile(fp)\n        bn_model.running_var.numpy().tofile(fp)\n        conv_model.weight.data.numpy().tofile(fp)\n\ndef load_fc(buf, start, fc_model):\n    num_w = fc_model.weight.numel()\n    num_b = fc_model.bias.numel()\n    fc_model.bias.data.copy_(torch.from_numpy(buf[start:start+num_b]));     start = start + num_b\n    fc_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w]));   start = start + num_w \n    return start\n\ndef save_fc(fp, fc_model):\n    fc_model.bias.data.numpy().tofile(fp)\n    fc_model.weight.data.numpy().tofile(fp)\n\nif __name__ == \'__main__\':\n    import sys\n    blocks = parse_cfg(\'cfg/yolo.cfg\')\n    if len(sys.argv) == 2:\n        blocks = parse_cfg(sys.argv[1])\n    print_cfg(blocks)\n'"
darknet.py,5,"b""import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom cfg import *\nimport numpy as np\nfrom region_layer import RegionLayer\nfrom yolo_layer import YoloLayer\n#from layers.batchnorm.bn import BN2d\n\nclass MaxPoolStride1(nn.Module):\n    def __init__(self):\n        super(MaxPoolStride1, self).__init__()\n\n    def forward(self, x):\n        x = F.max_pool2d(F.pad(x, (0,1,0,1), mode='replicate'), 2, stride=1)\n        return x\n\nclass Upsample(nn.Module):\n    def __init__(self, stride=2):\n        super(Upsample, self).__init__()\n        self.stride = stride\n    def forward(self, x):\n        stride = self.stride\n        assert(x.data.dim() == 4)\n        B = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        ws = stride\n        hs = stride\n        x = x.view(B, C, H, 1, W, 1).expand(B, C, H, hs, W, ws).contiguous().view(B, C, H*hs, W*ws)\n        return x\n\nclass Reorg(nn.Module):\n    def __init__(self, stride=2):\n        super(Reorg, self).__init__()\n        self.stride = stride\n    def forward(self, x):\n        stride = self.stride\n        assert(x.data.dim() == 4)\n        B = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        assert(H % stride == 0)\n        assert(W % stride == 0)\n        ws = stride\n        hs = stride\n        x = x.view(B, C, H//hs, hs, W//ws, ws).transpose(3,4).contiguous()\n        x = x.view(B, C, (H//hs)*(W//ws), hs*ws).transpose(2,3).contiguous()\n        x = x.view(B, C, hs*ws, H//hs, W//ws).transpose(1,2).contiguous()\n        x = x.view(B, hs*ws*C, H//hs, W//ws)\n        return x\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, x):\n        N = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        x = F.avg_pool2d(x, (H, W))\n        x = x.view(N, C)\n        return x\n\n# for route and shortcut\nclass EmptyModule(nn.Module):\n    def __init__(self):\n        super(EmptyModule, self).__init__()\n\n    def forward(self, x):\n        return x\n\n# support route shortcut and reorg\n\nclass Darknet(nn.Module):\n    def net_name(self):\n        names_list = ('region', 'yolo')\n        name = names_list[0]\n        for m in self.models:\n            if isinstance(m, YoloLayer):\n                name = names_list[1]\n        return name\n\n    def getLossLayers(self):\n        loss_layers = []\n        for m in self.models:\n            if isinstance(m, RegionLayer) or isinstance(m, YoloLayer):\n                loss_layers.append(m)\n        return loss_layers\n\n    def __init__(self, cfgfile, use_cuda=True):\n        super(Darknet, self).__init__()\n        self.use_cuda = use_cuda\n        self.blocks = parse_cfg(cfgfile)\n        self.models = self.create_network(self.blocks) # merge conv, bn,leaky\n        self.loss_layers = self.getLossLayers()\n\n        #self.width = int(self.blocks[0]['width'])\n        #self.height = int(self.blocks[0]['height'])\n\n        if len(self.loss_layers) > 0:\n            last = len(self.loss_layers)-1\n            self.anchors = self.loss_layers[last].anchors\n            self.num_anchors = self.loss_layers[last].num_anchors\n            self.anchor_step = self.loss_layers[last].anchor_step\n            self.num_classes = self.loss_layers[last].num_classes\n\n        # default format : major=0, minor=1\n        self.header = torch.IntTensor([0,1,0,0])\n        self.seen = 0\n\n    def forward(self, x):\n        ind = -2\n        #self.loss_layers = None\n        outputs = dict()\n        out_boxes = dict()\n        outno = 0\n        for block in self.blocks:\n            ind = ind + 1\n\n            if block['type'] == 'net':\n                continue\n            elif block['type'] in ['convolutional', 'maxpool', 'reorg', 'upsample', 'avgpool', 'softmax', 'connected']:\n                x = self.models[ind](x)\n                outputs[ind] = x\n            elif block['type'] == 'route':\n                layers = block['layers'].split(',')\n                layers = [int(i) if int(i) > 0 else int(i)+ind for i in layers]\n                if len(layers) == 1:\n                    x = outputs[layers[0]]\n                elif len(layers) == 2:\n                    x1 = outputs[layers[0]]\n                    x2 = outputs[layers[1]]\n                    x = torch.cat((x1,x2),1)\n                outputs[ind] = x\n            elif block['type'] == 'shortcut':\n                from_layer = int(block['from'])\n                activation = block['activation']\n                from_layer = from_layer if from_layer > 0 else from_layer + ind\n                x1 = outputs[from_layer]\n                x2 = outputs[ind-1]\n                x  = x1 + x2\n                if activation == 'leaky':\n                    x = F.leaky_relu(x, 0.1, inplace=True)\n                elif activation == 'relu':\n                    x = F.relu(x, inplace=True)\n                outputs[ind] = x\n            elif block['type'] in [ 'region', 'yolo']:\n                boxes = self.models[ind].get_mask_boxes(x)\n                out_boxes[outno]= boxes\n                outno += 1\n                outputs[ind] = None\n            elif block['type'] == 'cost':\n                continue\n            else:\n                print('unknown type %s' % (block['type']))\n        return x if outno == 0 else out_boxes\n\n    def print_network(self):\n        print_cfg(self.blocks)\n\n    def create_network(self, blocks):\n        models = nn.ModuleList()\n    \n        prev_filters = 3\n        out_filters =[]\n        prev_stride = 1\n        out_strides = []\n        conv_id = 0\n        ind = -2\n        for block in blocks:\n            ind += 1\n            if block['type'] == 'net':\n                prev_filters = int(block['channels'])\n                self.width = int(block['width'])\n                self.height = int(block['height'])\n                continue\n            elif block['type'] == 'convolutional':\n                conv_id = conv_id + 1\n                batch_normalize = int(block['batch_normalize'])\n                filters = int(block['filters'])\n                kernel_size = int(block['size'])\n                stride = int(block['stride'])\n                is_pad = int(block['pad'])\n                pad = (kernel_size-1)//2 if is_pad else 0\n                activation = block['activation']\n                model = nn.Sequential()\n                if batch_normalize:\n                    model.add_module('conv{0}'.format(conv_id), nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=False))\n                    model.add_module('bn{0}'.format(conv_id), nn.BatchNorm2d(filters))\n                    #model.add_module('bn{0}'.format(conv_id), BN2d(filters))\n                else:\n                    model.add_module('conv{0}'.format(conv_id), nn.Conv2d(prev_filters, filters, kernel_size, stride, pad))\n                if activation == 'leaky':\n                    model.add_module('leaky{0}'.format(conv_id), nn.LeakyReLU(0.1, inplace=True))\n                elif activation == 'relu':\n                    model.add_module('relu{0}'.format(conv_id), nn.ReLU(inplace=True))\n                prev_filters = filters\n                out_filters.append(prev_filters)\n                prev_stride = stride * prev_stride\n                out_strides.append(prev_stride)                \n                models.append(model)\n            elif block['type'] == 'maxpool':\n                pool_size = int(block['size'])\n                stride = int(block['stride'])\n                if stride > 1:\n                    model = nn.MaxPool2d(pool_size, stride)\n                else:\n                    model = MaxPoolStride1()\n                out_filters.append(prev_filters)\n                prev_stride = stride * prev_stride\n                out_strides.append(prev_stride)                \n                models.append(model)\n            elif block['type'] == 'avgpool':\n                model = GlobalAvgPool2d()\n                out_filters.append(prev_filters)\n                models.append(model)\n            elif block['type'] == 'softmax':\n                model = nn.Softmax()\n                out_strides.append(prev_stride)\n                out_filters.append(prev_filters)\n                models.append(model)\n            elif block['type'] == 'cost':\n                if block['_type'] == 'sse':\n                    model = nn.MSELoss(size_average=True)\n                elif block['_type'] == 'L1':\n                    model = nn.L1Loss(size_average=True)\n                elif block['_type'] == 'smooth':\n                    model = nn.SmoothL1Loss(size_average=True)\n                out_filters.append(1)\n                out_strides.append(prev_stride)\n                models.append(model)\n            elif block['type'] == 'reorg':\n                stride = int(block['stride'])\n                prev_filters = stride * stride * prev_filters\n                out_filters.append(prev_filters)\n                prev_stride = prev_stride * stride\n                out_strides.append(prev_stride)                \n                models.append(Reorg(stride))\n            elif block['type'] == 'upsample':\n                stride = int(block['stride'])\n                out_filters.append(prev_filters)\n                prev_stride = prev_stride / stride\n                out_strides.append(prev_stride)                \n                #models.append(nn.Upsample(scale_factor=stride, mode='nearest'))\n                models.append(Upsample(stride))\n            elif block['type'] == 'route':\n                layers = block['layers'].split(',')\n                ind = len(models)\n                layers = [int(i) if int(i) > 0 else int(i)+ind for i in layers]\n                if len(layers) == 1:\n                    prev_filters = out_filters[layers[0]]\n                    prev_stride = out_strides[layers[0]]\n                elif len(layers) == 2:\n                    assert(layers[0] == ind - 1)\n                    prev_filters = out_filters[layers[0]] + out_filters[layers[1]]\n                    prev_stride = out_strides[layers[0]]\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(EmptyModule())\n            elif block['type'] == 'shortcut':\n                ind = len(models)\n                prev_filters = out_filters[ind-1]\n                out_filters.append(prev_filters)\n                prev_stride = out_strides[ind-1]\n                out_strides.append(prev_stride)\n                models.append(EmptyModule())\n            elif block['type'] == 'connected':\n                filters = int(block['output'])\n                if block['activation'] == 'linear':\n                    model = nn.Linear(prev_filters, filters)\n                elif block['activation'] == 'leaky':\n                    model = nn.Sequential(\n                               nn.Linear(prev_filters, filters),\n                               nn.LeakyReLU(0.1, inplace=True))\n                elif block['activation'] == 'relu':\n                    model = nn.Sequential(\n                               nn.Linear(prev_filters, filters),\n                               nn.ReLU(inplace=True))\n                prev_filters = filters\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(model)\n            elif block['type'] == 'region':\n                region_layer = RegionLayer(use_cuda=self.use_cuda)\n                anchors = block['anchors'].split(',')\n                region_layer.anchors = [float(i) for i in anchors]\n                region_layer.num_classes = int(block['classes'])\n                region_layer.num_anchors = int(block['num'])\n                region_layer.anchor_step = len(region_layer.anchors)//region_layer.num_anchors\n                region_layer.rescore = int(block['rescore'])\n                region_layer.object_scale = float(block['object_scale'])\n                region_layer.noobject_scale = float(block['noobject_scale'])\n                region_layer.class_scale = float(block['class_scale'])\n                region_layer.coord_scale = float(block['coord_scale'])\n                region_layer.thresh = float(block['thresh'])\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(region_layer)\n            elif block['type'] == 'yolo':\n                yolo_layer = YoloLayer(use_cuda=self.use_cuda)\n                anchors = block['anchors'].split(',')\n                anchor_mask = block['mask'].split(',')\n                yolo_layer.anchor_mask = [int(i) for i in anchor_mask]\n                yolo_layer.anchors = [float(i) for i in anchors]\n                yolo_layer.num_classes = int(block['classes'])\n                yolo_layer.num_anchors = int(block['num'])\n                yolo_layer.anchor_step = len(yolo_layer.anchors)//yolo_layer.num_anchors\n                try:\n                    yolo_layer.rescore = int(block['rescore'])\n                except:\n                    pass\n                yolo_layer.ignore_thresh = float(block['ignore_thresh'])\n                yolo_layer.truth_thresh = float(block['truth_thresh'])\n                yolo_layer.stride = prev_stride\n                yolo_layer.nth_layer = ind\n                yolo_layer.net_width = self.width\n                yolo_layer.net_height = self.height\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(yolo_layer)                \n            else:\n                print('unknown type %s' % (block['type']))\n    \n        return models\n\n    def load_binfile(self, weightfile):\n        fp = open(weightfile, 'rb')\n       \n        version = np.fromfile(fp, count=3, dtype=np.int32)\n        version = [int(i) for i in version]\n        if version[0]*10+version[1] >=2 and version[0] < 1000 and version[1] < 1000:\n            seen = np.fromfile(fp, count=1, dtype=np.int64)\n        else:\n            seen = np.fromfile(fp, count=1, dtype=np.int32)\n        self.header = torch.from_numpy(np.concatenate((version, seen), axis=0))\n        self.seen = int(seen)\n        body = np.fromfile(fp, dtype=np.float32)\n        fp.close()\n        return body\n\n    def load_weights(self, weightfile):\n        buf = self.load_binfile(weightfile)\n\n        start = 0\n        ind = -2\n        for block in self.blocks:\n            if start >= buf.size:\n                break\n            ind = ind + 1\n            if block['type'] == 'net':\n                continue\n            elif block['type'] == 'convolutional':\n                model = self.models[ind]\n                batch_normalize = int(block['batch_normalize'])\n                if batch_normalize:\n                    start = load_conv_bn(buf, start, model[0], model[1])\n                else:\n                    start = load_conv(buf, start, model[0])\n            elif block['type'] == 'connected':\n                model = self.models[ind]\n                if block['activation'] != 'linear':\n                    start = load_fc(buf, start, model[0])\n                else:\n                    start = load_fc(buf, start, model)\n            elif block['type'] == 'maxpool':\n                pass\n            elif block['type'] == 'reorg':\n                pass\n            elif block['type'] == 'upsample':\n                pass\n            elif block['type'] == 'route':\n                pass\n            elif block['type'] == 'shortcut':\n                pass\n            elif block['type'] == 'region':\n                pass\n            elif block['type'] == 'yolo':\n                pass                \n            elif block['type'] == 'avgpool':\n                pass\n            elif block['type'] == 'softmax':\n                pass\n            elif block['type'] == 'cost':\n                pass\n            else:\n                print('unknown type %s' % (block['type']))\n\n    def save_weights(self, outfile, cutoff=0):\n        if cutoff <= 0:\n            cutoff = len(self.blocks)-1\n\n        dirname = os.path.dirname(outfile)\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        fp = open(outfile, 'wb')\n        self.header[3] = self.seen\n        header = np.array(self.header[0:3].numpy(), np.int32)\n        header.tofile(fp)\n        if (self.header[0]*10+self.header[1]) >= 2:\n            seen = np.array(self.seen, np.int64)\n        else:\n            seen = np.array(self.seen, np.int32)\n        seen.tofile(fp)\n\n        ind = -1\n        for blockId in range(1, cutoff+1):\n            ind = ind + 1\n            block = self.blocks[blockId]\n            if block['type'] == 'convolutional':\n                model = self.models[ind]\n                batch_normalize = int(block['batch_normalize'])\n                if batch_normalize:\n                    save_conv_bn(fp, model[0], model[1])\n                else:\n                    save_conv(fp, model[0])\n            elif block['type'] == 'connected':\n                model = self.models[ind]\n                if block['activation'] != 'linear':\n                    save_fc(fp, model)\n                else:\n                    save_fc(fp, model[0])\n            elif block['type'] == 'maxpool':\n                pass\n            elif block['type'] == 'reorg':\n                pass\n            elif block['type'] == 'upsample':\n                pass                \n            elif block['type'] == 'route':\n                pass\n            elif block['type'] == 'shortcut':\n                pass\n            elif block['type'] == 'region':\n                pass\n            elif block['type'] == 'yolo':\n                pass\n            elif block['type'] == 'avgpool':\n                pass\n            elif block['type'] == 'softmax':\n                pass\n            elif block['type'] == 'cost':\n                pass\n            else:\n                print('unknown type %s' % (block['type']))\n        fp.close()\n"""
dataset.py,9,"b""#!/usr/bin/python\n# encoding: utf-8\n\nimport os\nimport random\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom utils import read_truths_args, read_truths\nfrom image import *\n\ndef custom_collate(batch):\n    data = torch.stack([item[0] for item in batch], 0)\n    targets = torch.stack([item[1] for item in batch], 0)\n    return data, targets\n\nclass listDataset(Dataset):\n    def __init__(self, root, shape=None, shuffle=True, crop=False, jitter=0.3, hue=0.1, saturation=1.5, exposure=1.5, transform=None, target_transform=None, train=False, seen=0, batch_size=64, num_workers=4):\n       with open(root, 'r') as file:\n           self.lines = file.readlines()\n\n       if shuffle:\n           random.shuffle(self.lines)\n\n       self.nSamples  = len(self.lines)\n       self.transform = transform\n       self.target_transform = target_transform\n       self.train = train\n       self.shape = shape\n       self.seen = seen\n       self.batch_size = batch_size\n       self.num_workers = num_workers\n\n       self.crop = crop\n       self.jitter = jitter\n       self.hue = hue\n       self.saturation = saturation\n       self.exposure = exposure\n\n    def __len__(self):\n        return self.nSamples\n\n    def get_different_scale(self):\n        if self.seen < 4000*self.batch_size:\n            wh = 13*32                          # 416\n        elif self.seen < 8000*self.batch_size:\n            wh = (random.randint(0,3) + 13)*32  # 416, 480\n        elif self.seen < 12000*self.batch_size:\n            wh = (random.randint(0,5) + 12)*32  # 384, ..., 544\n        elif self.seen < 16000*self.batch_size:\n            wh = (random.randint(0,7) + 11)*32  # 352, ..., 576\n        else: # self.seen < 20000*self.batch_size:\n            wh = (random.randint(0,9) + 10)*32  # 320, ..., 608\n        return (wh, wh)\n\n    def __getitem__(self, index):\n        assert index <= len(self), 'index range error'\n        imgpath = self.lines[index].rstrip()\n\n        if self.train:\n            if self.seen % (self.batch_size * 10) == 0: # in paper, every 10 batches, but we did every 64 images\n                self.shape = self.get_different_scale()\n            img, label = load_data_detection(imgpath, self.shape, self.crop, self.jitter, self.hue, self.saturation, self.exposure)\n            label = torch.from_numpy(label)\n        else:\n            img = Image.open(imgpath).convert('RGB')\n            if self.shape:\n                img, org_w, org_h = letterbox_image(img, self.shape[0], self.shape[1]), img.width, img.height\n    \n            labpath = imgpath.replace('images', 'labels').replace('JPEGImages', 'labels').replace('.jpg', '.txt').replace('.png','.txt')\n            label = torch.zeros(50*5)\n            #if os.path.getsize(labpath):\n            #tmp = torch.from_numpy(np.loadtxt(labpath))\n            try:\n                tmp = torch.from_numpy(read_truths_args(labpath, 8.0/img.width).astype('float32'))\n            except Exception:\n                tmp = torch.zeros(1,5)\n            #tmp = torch.from_numpy(read_truths(labpath))\n            tmp = tmp.view(-1)\n            tsz = tmp.numel()\n            #print('labpath = %s , tsz = %d' % (labpath, tsz))\n            if tsz > 50*5:\n                label = tmp[0:50*5]\n            elif tsz > 0:\n                label[0:tsz] = tmp\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n\n        self.seen = self.seen + self.num_workers\n        if self.train:\n            return (img, label)\n        else:\n            return (img, label, org_w, org_h)\n"""
debug.py,6,"b""from __future__ import print_function\nimport torch.optim as optim\nimport os\nimport torch\nimport numpy as np\nfrom darknet import Darknet\nfrom PIL import Image\nfrom utils import image2torch, convert2cpu\n\ncfgfile = 'face4.1re_95.91.cfg'\nweightfile = 'face4.1re_95.91.conv.15'\nimgpath = 'data/train/images/10002.png'\nlabpath = imgpath.replace('images', 'labels').replace('JPEGImages', 'labels').replace('.jpg', '.txt').replace('.png','.txt')\nlabel = torch.zeros(50*5)\nif os.path.getsize(labpath):\n    tmp = torch.from_numpy(np.loadtxt(labpath))\n    #tmp = torch.from_numpy(read_truths_args(labpath, 8.0/img.width))\n    #tmp = torch.from_numpy(read_truths(labpath))\n    tmp = tmp.view(-1)\n    tsz = tmp.numel()\n    #print('labpath = %s , tsz = %d' % (labpath, tsz))\n    if tsz > 50*5:\n        label = tmp[0:50*5]\n    elif tsz > 0:\n        label[0:tsz] = tmp\nlabel = label.view(1, 50*5)\n\nm = Darknet(cfgfile)\nregion_loss = m.loss\nm.load_weights(weightfile)\n\nprint('--- bn weight ---')\nprint(m.models[0][1].weight)\nprint('--- bn bias ---')\nprint(m.models[0][1].bias)\nprint('--- bn running_mean ---')\nprint(m.models[0][1].running_mean)\nprint('--- bn running_var ---')\nprint(m.models[0][1].running_var)\n\nm.train()\nm = m.cuda()\n\noptimizer = optim.SGD(m.parameters(), lr=1e-2, momentum=0.9, weight_decay=0.1)\n\nimg = Image.open(imgpath)\nimg = image2torch(img).cuda()\n\ntarget = label\n\nprint('----- img ---------------------')\nprint(img.data.storage()[0:100])\nprint('----- target  -----------------')\nprint(target.data.storage()[0:100])\n\noptimizer.zero_grad()\noutput = m(img)\nprint('----- output ------------------')\nprint(output.data.storage()[0:100])\nexit()\n\nloss = region_loss(output, target)\nprint('----- loss --------------------')\nprint(loss)\n\nsave_grad = None\ndef extract(grad):\n    global saved_grad\n    saved_grad = convert2cpu(grad.data)\n\noutput.register_hook(extract)\nloss.backward()\n\nsaved_grad = saved_grad.view(-1)\nfor i in xrange(saved_grad.size(0)):\n    if abs(saved_grad[i]) >= 0.001:\n        print('%d : %f' % (i, saved_grad[i]))\n\nprint(m.state_dict().keys())\n#print(m.models[0][0].weight.grad.data.storage()[0:100])\n#print(m.models[14][0].weight.data.storage()[0:100])\nweight = m.models[13][0].weight.data\ngrad = m.models[13][0].weight.grad.data\nmask = torch.abs(grad) >= 0.1\nprint(weight[mask])\nprint(grad[mask])\n\noptimizer.step()\nweight2 = m.models[13][0].weight.data\nprint(weight2[mask])\n"""
demo.py,0,"b'from utils import *\nfrom darknet import Darknet\nimport cv2\n\ndef demo(cfgfile, weightfile):\n    m = Darknet(cfgfile)\n    m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    if m.num_classes == 20:\n        namesfile = \'data/voc.names\'\n    elif m.num_classes == 80:\n        namesfile = \'data/coco.names\'\n    else:\n        namesfile = \'data/names\'\n    print(""{} is used for classification"".format(namesfile))\n    class_names = load_class_names(namesfile)\n \n    use_cuda = True\n    if use_cuda:\n        m.cuda()\n\n    cap = cv2.VideoCapture(1)\n    if not cap.isOpened():\n        print(""Unable to open camera"")\n        exit(-1)\n\n    while True:\n        res, img = cap.read()\n        if res:\n            sized = cv2.resize(img, (m.width, m.height))\n            bboxes = do_detect(m, sized, 0.5, 0.4, use_cuda)\n            print(\'------\')\n            draw_img = plot_boxes_cv2(img, bboxes, None, class_names)\n            cv2.imshow(cfgfile, draw_img)\n            cv2.waitKey(1)\n        else:\n             print(""Unable to read image"")\n             exit(-1) \n\n############################################\nif __name__ == \'__main__\':\n    if len(sys.argv) == 3:\n        cfgfile = sys.argv[1]\n        weightfile = sys.argv[2]\n        demo(cfgfile, weightfile)\n        #demo(\'cfg/tiny-yolo-voc.cfg\', \'tiny-yolo-voc.weights\')\n    else:\n        print(\'Usage:\')\n        print(\'    python demo.py cfgfile weightfile\')\n        print(\'\')\n        print(\'    perform detection on camera\')\n'"
detect.py,1,"b'import sys\nimport time\nfrom PIL import Image, ImageDraw\n#from models.tiny_yolo import TinyYoloNet\nfrom utils import *\nfrom image import letterbox_image, correct_yolo_boxes\nfrom darknet import Darknet\n\nnamesfile=None\ndef detect(cfgfile, weightfile, imgfile):\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    # if m.num_classes == 20:\n    #     namesfile = \'data/voc.names\'\n    # elif m.num_classes == 80:\n    #     namesfile = \'data/coco.names\'\n    # else:\n    #     namesfile = \'data/names\'\n    \n    use_cuda = torch.cuda.is_available()\n    if use_cuda:\n        m.cuda()\n\n    img = Image.open(imgfile).convert(\'RGB\')\n    sized = letterbox_image(img, m.width, m.height)\n\n    start = time.time()\n    boxes = do_detect(m, sized, 0.5, 0.4, use_cuda)\n    correct_yolo_boxes(boxes, img.width, img.height, m.width, m.height)\n\n    finish = time.time()\n    print(\'%s: Predicted in %f seconds.\' % (imgfile, (finish-start)))\n\n    class_names = load_class_names(namesfile)\n    plot_boxes(img, boxes, \'predictions.jpg\', class_names)\n\ndef detect_cv2(cfgfile, weightfile, imgfile):\n    import cv2\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    if m.num_classes == 20:\n        namesfile = \'data/voc.names\'\n    elif m.num_classes == 80:\n        namesfile = \'data/coco.names\'\n    else:\n        namesfile = \'data/names\'\n    \n    use_cuda = True\n    if use_cuda:\n        m.cuda()\n\n    img = cv2.imread(imgfile)\n    sized = cv2.resize(img, (m.width, m.height))\n    sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n    \n    for i in range(2):\n        start = time.time()\n        boxes = do_detect(m, sized, 0.5, 0.4, use_cuda)\n        finish = time.time()\n        if i == 1:\n            print(\'%s: Predicted in %f seconds.\' % (imgfile, (finish-start)))\n\n    class_names = load_class_names(namesfile)\n    plot_boxes_cv2(img, boxes, savename=\'predictions.jpg\', class_names=class_names)\n\ndef detect_skimage(cfgfile, weightfile, imgfile):\n    from skimage import io\n    from skimage.transform import resize\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    if m.num_classes == 20:\n        namesfile = \'data/voc.names\'\n    elif m.num_classes == 80:\n        namesfile = \'data/coco.names\'\n    else:\n        namesfile = \'data/names\'\n    \n    use_cuda = True\n    if use_cuda:\n        m.cuda()\n\n    img = io.imread(imgfile)\n    sized = resize(img, (m.width, m.height)) * 255\n    \n    for i in range(2):\n        start = time.time()\n        boxes = do_detect(m, sized, 0.5, 0.4, use_cuda)\n        finish = time.time()\n        if i == 1:\n            print(\'%s: Predicted in %f seconds.\' % (imgfile, (finish-start)))\n\n    class_names = load_class_names(namesfile)\n    plot_boxes_cv2(img, boxes, savename=\'predictions.jpg\', class_names=class_names)\n\nif __name__ == \'__main__\':\n    if len(sys.argv) == 5:\n        cfgfile = sys.argv[1]\n        weightfile = sys.argv[2]\n        imgfile = sys.argv[3]\n        globals()[""namesfile""] = sys.argv[4]\n        detect(cfgfile, weightfile, imgfile)\n        #detect_cv2(cfgfile, weightfile, imgfile)\n        #detect_skimage(cfgfile, weightfile, imgfile)\n    else:\n        print(\'Usage: \')\n        print(\'  python detect.py cfgfile weightfile imgfile names\')\n        #detect(\'cfg/tiny-yolo-voc.cfg\', \'tiny-yolo-voc.weights\', \'data/person.jpg\', version=1)\n'"
eval.py,11,"b'from __future__ import print_function\nimport sys\nimport time\nimport torch\nfrom torchvision import datasets, transforms\nimport os\nimport dataset\nimport random\nimport math\nimport numpy as np\nfrom utils import get_all_boxes, multi_bbox_ious, nms, read_data_cfg, logging\nfrom cfg import parse_cfg\nfrom darknet import Darknet\nimport argparse\nfrom image import correct_yolo_boxes\n\n# etc parameters\nuse_cuda      = True\nseed          = 22222\neps           = 1e-5\n\n# Test parameters\nconf_thresh   = 0.25\nnms_thresh    = 0.4\niou_thresh    = 0.5\n\nFLAGS = None\n\ndef main():\n    # Training settings\n    datacfg       = FLAGS.data\n    cfgfile       = FLAGS.config\n\n    data_options  = read_data_cfg(datacfg)\n    testlist      = data_options[\'valid\']\n    gpus          = data_options[\'gpus\']  # e.g. 0,1,2,3\n    ngpus         = len(gpus.split(\',\'))\n\n    num_workers   = int(data_options[\'num_workers\'])\n    # for testing, batch_size is setted to 1 (one)\n    batch_size    = 1 # int(net_options[\'batch\'])\n\n    global use_cuda\n    use_cuda = torch.cuda.is_available() and (True if use_cuda is None else use_cuda)\n\n    ###############\n    torch.manual_seed(seed)\n    if use_cuda:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = gpus\n        torch.cuda.manual_seed(seed)\n\n    global model\n    model = Darknet(cfgfile)\n    #model.print_network()\n\n    init_width   = model.width\n    init_height  = model.height\n\n    kwargs = {\'num_workers\': num_workers, \'pin_memory\': True} if use_cuda else {}\n    \n    global test_loader\n    test_loader = torch.utils.data.DataLoader(\n        dataset.listDataset(testlist, shape=(init_width, init_height),\n                    shuffle=False,\n                    transform=transforms.Compose([\n                        transforms.ToTensor(),\n                    ]), train=False),\n        batch_size=batch_size, shuffle=False, **kwargs)\n\n    if use_cuda:\n        if ngpus > 1:\n            model = torch.nn.DataParallel(model)\n            model = model.module\n    model = model.to(torch.device(""cuda"" if use_cuda else ""cpu""))\n    for w in FLAGS.weights:\n        model.load_weights(w)\n        logging(\'evaluating ... %s\' % (w))\n        test()\n\ndef test():\n    def truths_length(truths):\n        for i in range(50):\n            if truths[i][1] == 0:\n                return i\n        return 50\n\n    model.eval()\n    num_classes = model.num_classes\n    total       = 0.0\n    proposals   = 0.0\n    correct     = 0.0\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n    if model.net_name() == \'region\': # region_layer\n        shape=(0,0)\n    else:\n        shape=(model.width, model.height)\n    for data, target, org_w, org_h in test_loader:\n        data = data.to(device)\n        output = model(data)\n        all_boxes = get_all_boxes(output, shape, conf_thresh, num_classes, use_cuda=use_cuda)\n\n        for k in range(len(all_boxes)):\n            boxes = all_boxes[k]\n            correct_yolo_boxes(boxes, org_w[k], org_h[k], model.width, model.height)\n            boxes = np.array(nms(boxes, nms_thresh))\n            truths = target[k].view(-1, 5)\n            num_gts = truths_length(truths)\n            total = total + num_gts\n            num_pred = len(boxes)\n            if num_pred == 0:\n                continue\n\n            proposals += int((boxes[:,4]>conf_thresh).sum())\n            for i in range(num_gts):\n                gt_boxes = torch.FloatTensor([truths[i][1], truths[i][2], truths[i][3], truths[i][4], 1.0, 1.0, truths[i][0]])\n                gt_boxes = gt_boxes.repeat(num_pred,1).t()\n                pred_boxes = torch.FloatTensor(boxes).t()\n                best_iou, best_j = torch.max(multi_bbox_ious(gt_boxes, pred_boxes, x1y1x2y2=False),0)\n                # pred_boxes and gt_boxes are transposed for torch.max\n                if best_iou > iou_thresh and pred_boxes[6][best_j] == gt_boxes[6][0]:\n                    correct += 1\n\n    precision = 1.0*correct/(proposals+eps)\n    recall = 1.0*correct/(total+eps)\n    fscore = 2.0*precision*recall/(precision+recall+eps)\n    logging(""correct: %d, precision: %f, recall: %f, fscore: %f"" % (correct, precision, recall, fscore))\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data\', \'-d\',    type=str, \n        default=\'cfg/sketch.data\', help=\'data definition file\')\n    parser.add_argument(\'--config\', \'-c\',  type=str, \n        default=\'cfg/sketch.cfg\', help=\'network configuration file\')\n    parser.add_argument(\'--weights\', \'-w\', type=str, nargs=\'+\', \n        default=[\'weights/yolov3.weights\'], help=\'initial weights file\')\n    FLAGS, _ = parser.parse_known_args()\n\n    main()\n'"
focal_loss.py,7,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# --------------------------------------------------------\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Chao CHEN (chaochancs@gmail.com)\n# Created On: 2017-08-11\n# --------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    r""""""\n        This criterion is a implemenation of Focal Loss, which is proposed in \n        Focal Loss for Dense Object Detection.\n            \n            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n    \n        The losses are averaged across observations for each minibatch.\n\n        Args:\n            alpha(1D Tensor) : the scalar factor for this criterion\n            gamma(float, double) : gamma > 0; reduces the relative loss for well-classi\xef\xac\x81ed examples (p > .5), \n                                   putting more focus on hard, misclassi\xef\xac\x81ed examples\n            size_average(bool): size_average(bool): By default, the losses are averaged over observations for each minibatch.\n                                However, if the field size_average is set to False, the losses are\n                                instead summed for each minibatch.\n\n    """"""\n    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n        super(FocalLoss, self).__init__()\n        if alpha is None:\n            self.alpha = torch.ones(class_num, 1)\n        else:\n            self.alpha = alpha\n        self.gamma = gamma\n        self.class_num = class_num\n        self.size_average = size_average\n\n    def forward(self, inputs, targets):\n        N = inputs.size(0)\n        print(N)\n        C = inputs.size(1)\n        P = F.softmax(inputs)\n\n        class_mask = inputs.data.new(N, C).fill_(0)\n        ids = targets.view(-1, 1)\n        class_mask.scatter_(1, ids.data, 1.)\n        #print(class_mask)\n        \n\n        if inputs.is_cuda and not self.alpha.is_cuda:\n            self.alpha = self.alpha.cuda()\n        alpha = self.alpha[ids.data.view(-1)]\n        \n        probs = (P*class_mask).sum(1).view(-1,1)\n\n        log_p = probs.log()\n        #print(\'probs size= {}\'.format(probs.size()))\n        #print(probs)\n\n        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p \n        #print(\'-----bacth_loss------\')\n        #print(batch_loss)\n\n        \n        if self.size_average:\n            loss = batch_loss.mean()\n        else:\n            loss = batch_loss.sum()\n        return loss\n\n        \n\nif __name__ == ""__main__"":\n    alpha = torch.rand(21, 1)\n    print(alpha)\n    FL = FocalLoss(class_num=5, gamma=0 )\n    CE = nn.CrossEntropyLoss()\n    N = 4\n    C = 5\n    inputs = torch.rand(N, C, requires_grad=True)\n    targets = torch.LongTensor(N).random_(C)\n    inputs_fl = inputs.clone()\n    targets_fl = targets.clone()\n\n    inputs_ce = inputs.clone()\n    targets_ce = targets.clone()\n    print(\'----inputs----\')\n    print(inputs)\n    print(\'---target-----\')\n    print(targets)\n\n    fl_loss = FL(inputs_fl, targets_fl)\n    ce_loss = CE(inputs_ce, targets_ce)\n    print(\'ce = {}, fl ={}\'.format(ce_loss.data[0], fl_loss.data[0]))\n    fl_loss.backward()\n    ce_loss.backward()\n    #print(inputs_fl.grad.data)\n    print(inputs_ce.grad.data)\n'"
image.py,0,"b'#!/usr/bin/python\n# encoding: utf-8\nimport os\nfrom PIL import Image, ImageFile\nimport numpy as np\n\n# to avoid image file truncation error\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ndef scale_image_channel(im, c, v):\n    cs = list(im.split())\n    cs[c] = cs[c].point(lambda i: i * v)\n    out = Image.merge(im.mode, tuple(cs))\n    return out\n\ndef image_scale_and_shift(img, new_w, new_h, net_w, net_h, dx, dy):\n    scaled = img.resize((new_w, new_h))\n    # find to be cropped area\n    sx, sy = -dx if dx < 0 else 0, -dy if dy < 0 else 0\n    ex, ey = new_w if sx+new_w<=net_w else net_w-sx, new_h if sy+new_h<=net_h else net_h-sy\n    scaled = scaled.crop((sx, sy, ex, ey))\n\n    # find the paste position\n    sx, sy = dx if dx > 0 else 0, dy if dy > 0 else 0\n    assert sx+scaled.width<=net_w and sy+scaled.height<=net_h\n    new_img = Image.new(""RGB"", (net_w, net_h), (127, 127, 127))\n    new_img.paste(scaled, (sx, sy))\n    del scaled\n    return new_img\n\ndef image_scale_and_shift_nosafe(img, new_w, new_h, net_w, net_h, dx, dy):\n    scaled = img.resize((new_w, new_h))\n    new_img = Image.new(""RGB"", (net_w, net_h), (127, 127, 127))\n    new_img.paste(scaled, (dx, dy))\n    del scaled\n    return new_img\n\ndef image_scale_and_shift_slow(img, new_w, new_h, net_w, net_h, dx, dy):\n    scaled = np.array(img.resize((new_w, new_h)))\n    # scaled.size : [height, width, channel]\n    \n    if dx > 0: \n        shifted = np.pad(scaled, ((0,0), (dx,0), (0,0)), mode=\'constant\', constant_values=127)\n    else:\n        shifted = scaled[:,-dx:,:]\n\n    if (new_w + dx) < net_w:\n        shifted = np.pad(shifted, ((0,0), (0, net_w - (new_w+dx)), (0,0)), mode=\'constant\', constant_values=127)\n               \n    if dy > 0: \n        shifted = np.pad(shifted, ((dy,0), (0,0), (0,0)), mode=\'constant\', constant_values=127)\n    else:\n        shifted = shifted[-dy:,:,:]\n        \n    if (new_h + dy) < net_h:\n        shifted = np.pad(shifted, ((0, net_h - (new_h+dy)), (0,0), (0,0)), mode=\'constant\', constant_values=127)\n    #print(""scaled: {} ==> dx {} dy {} for shifted: {}"".format(scaled.shape, dx, dy, shifted.shape))\n    return Image.fromarray(shifted[:net_h, :net_w,:])\n  \ndef distort_image(im, hue, sat, val):\n    im = im.convert(\'HSV\')\n    cs = list(im.split())\n    cs[1] = cs[1].point(lambda i: i * sat)\n    cs[2] = cs[2].point(lambda i: i * val)\n    \n    def change_hue(x):\n        x += hue*255\n        if x > 255:\n            x -= 255\n        if x < 0:\n            x += 255\n        return x\n    cs[0] = cs[0].point(change_hue)\n    im = Image.merge(im.mode, tuple(cs))\n\n    im = im.convert(\'RGB\')\n    #constrain_image(im)\n    return im\n\ndef rand_scale(s):\n    scale = np.random.uniform(1, s)\n    if np.random.randint(2): \n        return scale\n    return 1./scale\n\ndef random_distort_image(im, hue, saturation, exposure):\n    dhue = np.random.uniform(-hue, hue)\n    dsat = rand_scale(saturation)\n    dexp = rand_scale(exposure)\n    res = distort_image(im, dhue, dsat, dexp)\n    return res\n\ndef data_augmentation_crop(img, shape, jitter, hue, saturation, exposure):\n    oh = img.height  \n    ow = img.width\n    \n    dw =int(ow*jitter)\n    dh =int(oh*jitter)\n\n    pleft  = np.random.randint(-dw, dw)\n    pright = np.random.randint(-dw, dw)\n    ptop   = np.random.randint(-dh, dh)\n    pbot   = np.random.randint(-dh, dh)\n\n    swidth =  ow - pleft - pright\n    sheight = oh - ptop - pbot\n\n    sx = ow / float(swidth)\n    sy = oh / float(sheight)\n    \n    flip = np.random.randint(2)\n\n    cropbb = np.array([pleft, ptop, pleft + swidth - 1, ptop + sheight - 1])\n    # following two lines are old method. out of image boundary is filled with black (0,0,0)\n    #cropped = img.crop( cropbb )\n    #sized = cropped.resize(shape)\n\n    nw, nh = cropbb[2]-cropbb[0], cropbb[3]-cropbb[1]\n    # get the real image part\n    cropbb[0] = -min(cropbb[0], 0)\n    cropbb[1] = -min(cropbb[1], 0)\n    cropbb[2] = min(cropbb[2], ow)\n    cropbb[3] = min(cropbb[3], oh)\n    cropped = img.crop( cropbb )\n\n    # calculate the position to paste\n    bb = (pleft if pleft > 0 else 0, ptop if ptop > 0 else 0)\n    new_img = Image.new(""RGB"", (nw, nh), (127,127,127))\n    new_img.paste(cropped, bb)\n\n    sized = new_img.resize(shape)\n    del cropped, new_img\n    \n    dx = (float(pleft)/ow) * sx\n    dy = (float(ptop) /oh) * sy\n\n    if flip: \n        sized = sized.transpose(Image.FLIP_LEFT_RIGHT)\n    img = random_distort_image(sized, hue, saturation, exposure)\n    # for compatibility to nocrop version (like original version)\n    return img, flip, dx, dy, sx, sy \n\ndef data_augmentation_nocrop(img, shape, jitter, hue, sat, exp):\n    net_w, net_h = shape\n    img_w, img_h = img.width, img.height\n        \n    # determine the amount of scaling and cropping\n    dw = jitter * img_w\n    dh = jitter * img_h\n\n    new_ar = (img_w + np.random.uniform(-dw, dw)) / (img_h + np.random.uniform(-dh, dh))\n    # scale = np.random.uniform(0.25, 2)\n    scale = 1.\n\n    if (new_ar < 1):\n        new_h = int(scale * net_h)\n        new_w = int(net_h * new_ar)\n    else:\n        new_w = int(scale * net_w)\n        new_h = int(net_w / new_ar)\n            \n    dx = int(np.random.uniform(0, net_w - new_w))\n    dy = int(np.random.uniform(0, net_h - new_h))\n    sx, sy = new_w / net_w, new_h / net_h\n        \n    # apply scaling and shifting\n    new_img = image_scale_and_shift(img, new_w, new_h, net_w, net_h, dx, dy)\n        \n    # randomly distort hsv space\n    new_img = random_distort_image(new_img, hue, sat, exp)\n        \n    # randomly flip\n    flip = np.random.randint(2)\n    if flip: \n        new_img = new_img.transpose(Image.FLIP_LEFT_RIGHT)\n            \n    dx, dy = dx/net_w, dy/net_h\n    return new_img, flip, dx, dy, sx, sy \n\ndef fill_truth_detection(labpath, crop, flip, dx, dy, sx, sy):\n    max_boxes = 50\n    label = np.zeros((max_boxes,5))\n    if os.path.getsize(labpath):\n        bs = np.loadtxt(labpath)\n        if bs is None:\n            return label\n        bs = np.reshape(bs, (-1, 5))\n        cc = 0\n        for i in range(bs.shape[0]):\n            x1 = bs[i][1] - bs[i][3]/2\n            y1 = bs[i][2] - bs[i][4]/2\n            x2 = bs[i][1] + bs[i][3]/2\n            y2 = bs[i][2] + bs[i][4]/2\n            \n            x1 = min(0.999, max(0, x1 * sx - dx)) \n            y1 = min(0.999, max(0, y1 * sy - dy)) \n            x2 = min(0.999, max(0, x2 * sx - dx))\n            y2 = min(0.999, max(0, y2 * sy - dy))\n            \n            bs[i][1] = (x1 + x2)/2 # center x\n            bs[i][2] = (y1 + y2)/2 # center y\n            bs[i][3] = (x2 - x1)   # width\n            bs[i][4] = (y2 - y1)   # height\n\n            if flip:\n                bs[i][1] =  0.999 - bs[i][1] \n            \n            # when crop is applied, we should check the cropped width/height ratio\n            if bs[i][3] < 0.002 or bs[i][4] < 0.002 or \\\n                (crop and (bs[i][3]/bs[i][4] > 20 or bs[i][4]/bs[i][3] > 20)):\n                continue\n            label[cc] = bs[i]\n            cc += 1\n            if cc >= 50:\n                break\n\n    label = np.reshape(label, (-1))\n    return label\n\ndef letterbox_image(img, net_w, net_h):\n    im_w, im_h = img.size\n    if float(net_w)/float(im_w) < float(net_h)/float(im_h):\n        new_w = net_w\n        new_h = (im_h * net_w)//im_w\n    else:\n        new_w = (im_w * net_h)//im_h\n        new_h = net_h\n    resized = img.resize((new_w, new_h), Image.ANTIALIAS)\n    lbImage = Image.new(""RGB"", (net_w, net_h), (127,127,127))\n    lbImage.paste(resized, \\\n            ((net_w-new_w)//2, (net_h-new_h)//2, \\\n             (net_w+new_w)//2, (net_h+new_h)//2))\n    return lbImage\n\ndef correct_yolo_boxes(boxes, im_w, im_h, net_w, net_h):\n    im_w, im_h = float(im_w), float(im_h)\n    net_w, net_h = float(net_w), float(net_h)\n    if net_w/im_w < net_h/im_h:\n        new_w = net_w\n        new_h = (im_h * net_w)/im_w\n    else:\n        new_w = (im_w * net_h)/im_h\n        new_h = net_h\n\n    xo, xs = (net_w - new_w)/(2*net_w), net_w/new_w\n    yo, ys = (net_h - new_h)/(2*net_h), net_h/new_h\n    for i in range(len(boxes)):\n        b = boxes[i] \n        b[0] = (b[0] - xo) * xs\n        b[1] = (b[1] - yo) * ys\n        b[2] *= xs\n        b[3] *= ys\n    return\n\ndef load_data_detection(imgpath, shape, crop, jitter, hue, saturation, exposure):\n    labpath = imgpath.replace(\'images\', \'labels\').replace(\'JPEGImages\', \'labels\').replace(\'.jpg\', \'.txt\').replace(\'.png\',\'.txt\')\n\n    ## data augmentation\n    img = Image.open(imgpath).convert(\'RGB\')\n    if crop:         # marvis version\n        img,flip,dx,dy,sx,sy = data_augmentation_crop(img, shape, jitter, hue, saturation, exposure)\n    else:            # original version\n        img,flip,dx,dy,sx,sy = data_augmentation_nocrop(img, shape, jitter, hue, saturation, exposure)\n    label = fill_truth_detection(labpath, crop, flip, -dx, -dy, sx, sy)\n    return img, label\n'"
outputs.py,0,"b'import numpy as np\r\nclass Outputs:\r\n    def __init__(self):\r\n        self.num_outputs =0\r\n        self.outputs = []\r\n        self.masks = []\r\n        self.num_masks = []\r\n\r\n    def __iter__(self):\r\n        self.current = 0\r\n        return self\r\n\r\n    def __next__(self):\r\n        if self.current > self.num_outputs:\r\n            raise StopIteration\r\n        else:\r\n            self.current += 1\r\n            return self.get(self.current-1)\r\n\r\n    def num(self):\r\n        return self.num_outputs\r\n\r\n    def size(self):\r\n        if self.num_outputs > 0:\r\n            return self.outputs[0].data.size(0)\r\n        else:\r\n            return 0\r\n\r\n    def get(self, index):\r\n        if index < self.num_outputs:\r\n            return [self.outputs[index].data, self.masks[index], self.num_masks[index]]\r\n        else:\r\n            return [None, None, None]\r\n    \r\n    def get_out(self, index):\r\n        if index < self.num_outputs:\r\n            return self.outputs[index]\r\n        else:\r\n            return None\r\n\r\n    def add(self, outbox):\r\n        if len(outbox) == 3:\r\n            self.outputs.append(outbox[0])\r\n            self.masks.append(outbox[1])\r\n            self.num_masks.append(outbox[2])\r\n            self.num_outputs += 1'"
partial.py,0,"b""from darknet import Darknet\n\ndef partial(cfgfile, weightfile, outfile, cutoff):\n    m = Darknet(cfgfile)\n    m.print_network()\n    m.load_weights(weightfile)\n    m.seen = 0\n    m.save_weights(outfile, cutoff)\n    print('save %s' % (outfile))\n\nif __name__ == '__main__':\n    import sys\n    if len(sys.argv) == 5:\n        cfgfile = sys.argv[1]\n        weightfile = sys.argv[2]\n        outfile = sys.argv[3]\n        cutoff = int(sys.argv[4])\n        partial(cfgfile, weightfile, outfile, cutoff)\n    else:\n        print('Usage:')\n        print('python partial.py cfgfile weightfile output cutoff')\n        #partial('cfg/tiny-yolo-voc.cfg', 'tiny-yolo-voc.weights', 'tiny-yolo-voc.conv.15', 15)\n\n"""
recall.py,0,"b'from PIL import Image, ImageDraw\nfrom utils import *\nfrom darknet import Darknet\n\ndef eval_list(cfgfile, weightfile, imglist):\n    #m = TinyYoloFace14Net()\n    #m.eval()\n    #m.load_darknet_weights(tiny_yolo_weight)\n\n    m = Darknet(cfgfile)\n    m.eval()\n    m.load_weights(weightfile)\n    eval_wid = m.width\n    eval_hei = m.height\n\n    use_cuda = True\n    if use_cuda:\n        m.cuda()\n\n    conf_thresh = 0.25\n    nms_thresh = 0.4\n    iou_thresh = 0.5\n    min_box_scale = 8. / m.width\n\n    with open(imglist) as fp:\n        lines = fp.readlines()\n\n    total = 0.0\n    proposals = 0.0\n    correct = 0.0\n    lineId = 0\n    avg_iou = 0.0\n    for line in lines:\n        img_path = line.rstrip()\n        if img_path[0] == \'#\':\n            continue\n        lineId = lineId + 1\n        lab_path = img_path.replace(\'images\', \'labels\')\n        lab_path = lab_path.replace(\'JPEGImages\', \'labels\')\n        lab_path = lab_path.replace(\'.jpg\', \'.txt\').replace(\'.png\', \'.txt\')\n        #truths = read_truths(lab_path)\n        truths = read_truths_args(lab_path, min_box_scale)\n        #print(truths)\n\n        img = Image.open(img_path).convert(\'RGB\').resize((eval_wid, eval_hei))\n        boxes = do_detect(m, img, conf_thresh, nms_thresh, use_cuda)\n        if False:\n            savename = ""tmp/%06d.jpg"" % (lineId)\n            print(""save %s"" % savename)\n            plot_boxes(img, boxes, savename)\n        \n        total = total + truths.shape[0]\n\n        for i in range(len(boxes)):\n            if boxes[i][4] > conf_thresh:\n                proposals = proposals+1\n\n        for i in range(truths.shape[0]):\n            box_gt = [truths[i][1], truths[i][2], truths[i][3], truths[i][4], 1.0]\n            best_iou = 0\n            for j in range(len(boxes)):\n                iou = bbox_iou(box_gt, boxes[j], x1y1x2y2=False)\n                best_iou = max(iou, best_iou)\n            if best_iou > iou_thresh:\n                avg_iou += best_iou\n                correct = correct+1\n\n    precision = 1.0*correct/proposals\n    recall = 1.0*correct/total\n    fscore = 2.0*precision*recall/(precision+recall)\n    print(""%d IOU: %f, Recal: %f, Precision: %f, Fscore: %f\\n"" % (lineId-1, avg_iou/correct, recall, precision, fscore))\n\nif __name__ == \'__main__\':\n    import sys\n    if len(sys.argv) == 4:\n        cfgfile = sys.argv[1]\n        weightfile = sys.argv[2]\n        imglist = sys.argv[3]\n        eval_list(cfgfile, weightfile, imglist)\n    else:\n        print(\'Usage:\')\n        print(\'python recall.py cfgfile weightfile imglist\')\n        #python recall.py test160.cfg backup/000022.weights face_test.txt\n'"
region_layer.py,31,"b'import math\nimport numpy as np\nimport sys\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils import bbox_iou, multi_bbox_ious, convert2cpu\n\nclass RegionLayer(nn.Module):\n    def __init__(self, num_classes=0, anchors=[1.0], num_anchors=1, use_cuda=None):\n        super(RegionLayer, self).__init__()\n        use_cuda = torch.cuda.is_available() and (True if use_cuda is None else use_cuda)\n        self.device = torch.device(""cuda"" if use_cuda else ""cpu"")\n        self.num_classes = num_classes\n        self.num_anchors = num_anchors\n        self.anchor_step = len(anchors)//num_anchors\n        #self.anchors = torch.stack(torch.FloatTensor(anchors).split(self.anchor_step)).to(self.device)\n        self.anchors = torch.FloatTensor(anchors).view(self.num_anchors, self.anchor_step).to(self.device)\n        self.rescore = 1\n        self.coord_scale = 1\n        self.noobject_scale = 1\n        self.object_scale = 5\n        self.class_scale = 1\n        self.thresh = 0.6\n        self.seen = 0\n\n    def build_targets(self, pred_boxes, target, nH, nW):\n        nB = target.size(0)\n        nA = self.num_anchors\n        noobj_mask = torch.ones (nB, nA, nH, nW)\n        obj_mask   = torch.zeros(nB, nA, nH, nW)\n        coord_mask = torch.zeros(nB, nA, nH, nW)\n        tcoord     = torch.zeros( 4, nB, nA, nH, nW)\n        tconf      = torch.zeros(nB, nA, nH, nW)\n        tcls       = torch.zeros(nB, nA, nH, nW)\n\n        nAnchors = nA*nH*nW\n        nPixels  = nH*nW\n        nGT = 0 # number of ground truth\n        nRecall = 0\n        # it works faster on CPU than on GPU.\n        anchors = self.anchors.to(""cpu"")\n\n        if self.seen < 12800:\n            tcoord[0].fill_(0.5)\n            tcoord[1].fill_(0.5)\n            coord_mask.fill_(0.01)\n            # initial w, h == 0 means log(1)==0, s.t, anchor is equal to ground truth.\n\n        for b in range(nB):\n            cur_pred_boxes = pred_boxes[b*nAnchors:(b+1)*nAnchors].t()\n            cur_ious = torch.zeros(nAnchors)\n            tbox = target[b].view(-1,5).to(""cpu"")\n            for t in range(50):\n                if tbox[t][1] == 0:\n                    break\n                gx, gw = [ i * nW for i in (tbox[t][1], tbox[t][3]) ]\n                gy, gh = [ i * nH for i in (tbox[t][2], tbox[t][4]) ]\n                cur_gt_boxes = torch.FloatTensor([gx, gy, gw, gh]).repeat(nAnchors,1).t()\n                cur_ious = torch.max(cur_ious, multi_bbox_ious(cur_pred_boxes, cur_gt_boxes, x1y1x2y2=False))\n            ignore_ix = (cur_ious>self.thresh).view(nA,nH,nW)\n            noobj_mask[b][ignore_ix] = 0\n\n            for t in range(50):\n                if tbox[t][1] == 0:\n                    break\n                nGT += 1\n                gx, gw = [ i * nW for i in (tbox[t][1], tbox[t][3]) ]\n                gy, gh = [ i * nH for i in (tbox[t][2], tbox[t][4]) ]\n                gw, gh = gw.float(), gh.float()\n                gi, gj = int(gx), int(gy)\n\n                tmp_gt_boxes = torch.FloatTensor([0, 0, gw, gh]).repeat(nA,1).t()\n                anchor_boxes = torch.cat((torch.zeros(nA, 2), anchors),1).t()\n                tmp_ious = multi_bbox_ious(anchor_boxes, tmp_gt_boxes, x1y1x2y2=False)\n                best_iou, best_n = torch.max(tmp_ious, 0)\n\n                if self.anchor_step == 4: # this part is not tested.\n                    tmp_ious_mask = (tmp_ious==best_iou)\n                    if tmp_ious_mask.sum() > 0:\n                        gt_pos = torch.FloatTensor([gi, gj, gx, gy]).repeat(nA,1).t()\n                        an_pos = anchor_boxes[4:6] # anchor_boxes are consisted of [0 0 aw ah ax ay]\n                        dist = pow(((gt_pos[0]+an_pos[0])-gt_pos[2]),2) + pow(((gt_pos[1]+an_pos[1])-gt_pos[3]),2)\n                        dist[1-tmp_ious_mask]=10000 # set the large number for the small ious\n                        _, best_n = torch.min(dist,0)\n\n                gt_box = torch.FloatTensor([gx, gy, gw, gh])\n                pred_box = pred_boxes[b*nAnchors+best_n*nPixels+gj*nW+gi]\n                iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n\n                obj_mask  [b][best_n][gj][gi] = 1\n                noobj_mask[b][best_n][gj][gi] = 0\n                coord_mask[b][best_n][gj][gi] = 2. - tbox[t][3]*tbox[t][4]\n                tcoord [0][b][best_n][gj][gi] = gx - gi\n                tcoord [1][b][best_n][gj][gi] = gy - gj\n                tcoord [2][b][best_n][gj][gi] = math.log(gw/anchors[best_n][0])\n                tcoord [3][b][best_n][gj][gi] = math.log(gh/anchors[best_n][1])\n                tcls      [b][best_n][gj][gi] = tbox[t][0]\n                tconf     [b][best_n][gj][gi] = iou if self.rescore else 1.\n                if iou > 0.5:\n                    nRecall += 1\n\n        return nGT, nRecall, obj_mask, noobj_mask, coord_mask, tcoord, tconf, tcls\n\n    def get_mask_boxes(self, output):\n        if not isinstance(self.anchors, torch.Tensor):\n            self.anchors = torch.FloatTensor(self.anchors).view(self.num_anchors, self.anchor_step).to(self.device)\n        masked_anchors = self.anchors.view(-1)\n        num_anchors = torch.IntTensor([self.num_anchors]).to(self.device)\n        return {\'x\':output, \'a\':masked_anchors, \'n\':num_anchors}\n\n    def forward(self, output, target):\n        #output : BxAs*(4+1+num_classes)*H*W\n        t0 = time.time()\n        nB = output.data.size(0)    # batch size\n        nA = self.num_anchors\n        nC = self.num_classes\n        nH = output.data.size(2)\n        nW = output.data.size(3)\n        cls_anchor_dim = nB*nA*nH*nW\n\n        if not isinstance(self.anchors, torch.Tensor):\n            self.anchors = torch.FloatTensor(self.anchors).view(self.num_anchors, self.anchor_step).to(self.device)\n\n        output = output.view(nB, nA, (5+nC), nH, nW).to(self.device)\n        cls_grid = torch.linspace(5,5+nC-1,nC).long().to(self.device)\n        ix = torch.LongTensor(range(0,5)).to(self.device)\n        pred_boxes = torch.FloatTensor(4, cls_anchor_dim).to(self.device)\n\n        coord = output.index_select(2, ix[0:4]).view(nB*nA, -1, nH*nW).transpose(0,1).contiguous().view(-1,cls_anchor_dim)  # x, y, w, h\n        coord[0:2] = coord[0:2].sigmoid()\n        conf = output.index_select(2, ix[4]).view(cls_anchor_dim).sigmoid()\n\n        cls  = output.index_select(2, cls_grid)\n        cls  = cls.view(nB*nA, nC, nH*nW).transpose(1,2).contiguous().view(cls_anchor_dim, nC)\n\n        t1 = time.time()\n        grid_x = torch.linspace(0, nW-1, nW).repeat(nB*nA, nH, 1).view(cls_anchor_dim).to(self.device)\n        grid_y = torch.linspace(0, nH-1, nH).repeat(nW,1).t().repeat(nB*nA, 1, 1).view(cls_anchor_dim).to(self.device)\n        anchor_w = self.anchors.index_select(1, ix[0]).repeat(nB, nH*nW).view(cls_anchor_dim)\n        anchor_h = self.anchors.index_select(1, ix[1]).repeat(nB, nH*nW).view(cls_anchor_dim)\n\n        pred_boxes[0] = coord[0] + grid_x\n        pred_boxes[1] = coord[1] + grid_y\n        pred_boxes[2] = coord[2].exp() * anchor_w\n        pred_boxes[3] = coord[3].exp() * anchor_h\n        # for build_targets. it works faster on CPU than on GPU\n        pred_boxes = convert2cpu(pred_boxes.transpose(0,1).contiguous().view(-1,4)).detach()\n\n        t2 = time.time()\n        nGT, nRecall, obj_mask, noobj_mask, coord_mask, tcoord, tconf, tcls = \\\n            self.build_targets(pred_boxes, target.detach(), nH, nW)\n\n        cls_mask = (obj_mask == 1)\n        tcls = tcls[cls_mask].long().view(-1).to(self.device)\n        cls_mask = cls_mask.view(-1, 1).repeat(1,nC).to(self.device)\n        cls = cls[cls_mask].view(-1, nC)\n\n        nProposals = int((conf > 0.25).sum())\n\n        tcoord = tcoord.view(4, cls_anchor_dim).to(self.device)\n        tconf = tconf.view(cls_anchor_dim).to(self.device)        \n\n        conf_mask = (self.object_scale * obj_mask + self.noobject_scale * noobj_mask).view(cls_anchor_dim).to(self.device)\n        obj_mask = obj_mask.view(cls_anchor_dim).to(self.device)\n        coord_mask = coord_mask.view(cls_anchor_dim).to(self.device)\n\n        t3 = time.time()\n        loss_coord = self.coord_scale * nn.MSELoss(reduction=\'sum\')(coord*coord_mask, tcoord*coord_mask)/nB\n        loss_conf = nn.MSELoss(reduction=\'sum\')(conf*conf_mask, tconf*conf_mask)/nB\n        loss_cls = self.class_scale * nn.CrossEntropyLoss(reduction=\'sum\')(cls, tcls)/nB\n        loss = loss_coord + loss_conf + loss_cls\n\n        t4 = time.time()\n        if False:\n            print(\'-\'*30)\n            print(\'        activation : %f\' % (t1 - t0))\n            print(\' create pred_boxes : %f\' % (t2 - t1))\n            print(\'     build targets : %f\' % (t3 - t2))\n            print(\'       create loss : %f\' % (t4 - t3))\n            print(\'             total : %f\' % (t4 - t0))\n        print(\'%d: nGT %3d, nRC %3d, nPP %3d, loss: box %6.3f, conf %6.3f, class %6.3f, total %7.3f\' \n            % (self.seen, nGT, nRecall, nProposals, loss_coord, loss_conf, loss_cls, loss))\n        if math.isnan(loss.item()):\n            print(conf, tconf)\n            sys.exit(0)\n        return loss\n'"
train.py,15,"b'from __future__ import print_function\nimport sys\n\nimport time\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nimport gc\n\nimport dataset\nfrom utils import *\nfrom image import correct_yolo_boxes\nfrom cfg import parse_cfg\nfrom darknet import Darknet\nimport argparse\n\nFLAGS = None\nunparsed = None\ndevice = None\n\n# global variables\n# Training settings\n# Train parameters\nuse_cuda      = None\neps           = 1e-5\nkeep_backup   = 5\nsave_interval = 5  # epoches\ntest_interval = 10  # epoches\ndot_interval  = 70  # batches\n\n# Test parameters\nevaluate = False\nconf_thresh   = 0.25\nnms_thresh    = 0.4\niou_thresh    = 0.5\n\n# no test evalulation\nno_eval = False\ninit_eval = False\n\n# Training settings\ndef load_testlist(testlist):\n    init_width = model.width\n    init_height = model.height\n\n    kwargs = {\'num_workers\': num_workers, \'pin_memory\': True} if use_cuda else {}\n    loader = torch.utils.data.DataLoader(\n        dataset.listDataset(testlist, shape=(init_width, init_height),\n                       shuffle=False,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                       ]), train=False),\n        batch_size=batch_size, shuffle=False, **kwargs)\n    return loader\n\ndef main():\n    datacfg    = FLAGS.data\n    cfgfile    = FLAGS.config\n    weightfile = FLAGS.weights\n    no_eval    = FLAGS.no_eval\n    init_eval  = FLAGS.init_eval\n\n    data_options  = read_data_cfg(datacfg)\n    net_options   = parse_cfg(cfgfile)[0]\n\n    global use_cuda\n    use_cuda = torch.cuda.is_available() and (True if use_cuda is None else use_cuda)\n\n    globals()[""trainlist""]     = data_options[\'train\']\n    globals()[""testlist""]      = data_options[\'valid\']\n    globals()[""backupdir""]     = data_options[\'backup\']\n    globals()[""gpus""]          = data_options[\'gpus\']  # e.g. 0,1,2,3\n    globals()[""ngpus""]         = len(gpus.split(\',\'))\n    globals()[""num_workers""]   = int(data_options[\'num_workers\'])\n\n    globals()[""batch_size""]    = int(net_options[\'batch\'])\n    globals()[""max_batches""]   = int(net_options[\'max_batches\'])\n    globals()[""learning_rate""] = float(net_options[\'learning_rate\'])\n    globals()[""momentum""]      = float(net_options[\'momentum\'])\n    globals()[""decay""]         = float(net_options[\'decay\'])\n    globals()[""steps""]         = [float(step) for step in net_options[\'steps\'].split(\',\')]\n    globals()[""scales""]        = [float(scale) for scale in net_options[\'scales\'].split(\',\')]\n\n    #Train parameters\n    global max_epochs\n    try:\n        max_epochs = int(net_options[\'max_epochs\'])\n    except KeyError:\n        nsamples = file_lines(trainlist)\n        max_epochs = (max_batches*batch_size)//nsamples+1\n\n    seed = int(time.time())\n    torch.manual_seed(seed)\n    if use_cuda:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = gpus\n        torch.cuda.manual_seed(seed)\n    global device\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n    global model\n    model = Darknet(cfgfile, use_cuda=use_cuda)\n    if weightfile is not None:\n        model.load_weights(weightfile)\n\n    #model.print_network()\n\n    nsamples = file_lines(trainlist)\n    #initialize the model\n    if FLAGS.reset:\n        model.seen = 0\n        init_epoch = 0\n    else:\n        init_epoch = model.seen//nsamples\n\n    global loss_layers\n    loss_layers = model.loss_layers\n    for l in loss_layers:\n        l.seen = model.seen\n\n    globals()[""test_loader""] = load_testlist(testlist)\n    if use_cuda:\n        if ngpus > 1:\n            model = torch.nn.DataParallel(model).to(device)\n        else:\n            model = model.to(device)\n\n    params_dict = dict(model.named_parameters())\n    params = []\n    for key, value in params_dict.items():\n        if key.find(\'.bn\') >= 0 or key.find(\'.bias\') >= 0:\n            params += [{\'params\': [value], \'weight_decay\': 0.0}]\n        else:\n            params += [{\'params\': [value], \'weight_decay\': decay*batch_size}]\n    global optimizer\n    optimizer = optim.SGD(model.parameters(), \n                        lr=learning_rate/batch_size, momentum=momentum, \n                        dampening=0, weight_decay=decay*batch_size)\n\n    if evaluate:\n        logging(\'evaluating ...\')\n        test(0)\n    else:\n        try:\n            print(""Training for ({:d},{:d})"".format(init_epoch+1, max_epochs))\n            fscore = 0\n            correct = 0\n            if init_eval and not no_eval and init_epoch > test_interval:\n                print(\'>> initial evaluating ...\')\n                mcorrect,mfscore = test(init_epoch)\n                print(\'>> done evaluation.\')\n            else:\n                mfscore = 0.5\n                mcorrect = 0\n            for epoch in range(init_epoch+1, max_epochs+1):\n                nsamples = train(epoch)\n                if epoch % save_interval == 0:\n                    savemodel(epoch, nsamples)\n                if not no_eval and epoch >= test_interval and (epoch%test_interval) == 0:\n                    print(\'>> interim evaluating ...\')\n                    correct, fscore = test(epoch)\n                    print(\'>> done evaluation.\')\n                if FLAGS.localmax and correct > mcorrect:\n                    mfscore = fscore\n                    mcorrect = correct\n                    savemodel(epoch, nsamples, True)\n                print(\'-\'*90)\n        except KeyboardInterrupt:\n            print(\'=\'*80)\n            print(\'Exiting from training by interrupt\')\n                \ndef adjust_learning_rate(optimizer, batch):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = learning_rate\n    for i in range(len(steps)):\n        scale = scales[i] if i < len(scales) else 1\n        if batch >= steps[i]:\n            lr = lr * scale\n            if batch == steps[i]:\n                break\n        else:\n            break\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr/batch_size\n    return lr\n\ndef curmodel():\n    if ngpus > 1:\n        cur_model = model.module\n    else:\n        cur_model = model\n    return cur_model\n\ndef train(epoch):\n    global processed_batches\n    t0 = time.time()\n    cur_model = curmodel()\n    init_width = cur_model.width\n    init_height = cur_model.height\n    kwargs = {\'num_workers\': num_workers, \'pin_memory\': True} if use_cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n                dataset.listDataset(trainlist, \n                                    shape=(init_width, init_height),\n                                    shuffle=True,\n                                    transform=transforms.Compose([\n                                        transforms.ToTensor(),\n                                        ]), \n                                    train=True,\n                                    seen=cur_model.seen,\n                                    batch_size=batch_size,\n                                    num_workers=num_workers),\n                collate_fn=dataset.custom_collate, \n                batch_size=batch_size, shuffle=False, **kwargs)\n\n    processed_batches = cur_model.seen//batch_size\n    lr = adjust_learning_rate(optimizer, processed_batches)\n    logging(\'[%03d] processed %d samples, lr %e\' % (epoch, epoch * len(train_loader.dataset), lr))\n    model.train()\n    t1 = time.time()\n    avg_time = torch.zeros(9)\n    for batch_idx, (data, target) in enumerate(train_loader):\n        t2 = time.time()\n        adjust_learning_rate(optimizer, processed_batches)\n        processed_batches = processed_batches + 1\n        #if (batch_idx+1) % dot_interval == 0:\n        #    sys.stdout.write(\'.\')\n\n        t3 = time.time()\n        data, target = data.to(device), target.to(device)\n\n        t4 = time.time()\n        optimizer.zero_grad()\n\n        t5 = time.time()\n        output = model(data)\n\n        t6 = time.time()\n        org_loss = []\n        for i, l in enumerate(loss_layers):\n            l.seen = l.seen + data.data.size(0)\n            ol=l(output[i][\'x\'], target)\n            org_loss.append(ol)\n\n        t7 = time.time()\n\n        #for i, l in enumerate(reversed(org_loss)):\n        #    l.backward(retain_graph=True if i < len(org_loss)-1 else False)\n        # org_loss.reverse()\n        sum(org_loss).backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), 10000)\n        #for p in model.parameters():\n        #    p.data.add_(-lr, p.grad.data)\n\n        t8 = time.time()\n        optimizer.step()\n        \n        t9 = time.time()\n        if False and batch_idx > 1:\n            avg_time[0] = avg_time[0] + (t2-t1)\n            avg_time[1] = avg_time[1] + (t3-t2)\n            avg_time[2] = avg_time[2] + (t4-t3)\n            avg_time[3] = avg_time[3] + (t5-t4)\n            avg_time[4] = avg_time[4] + (t6-t5)\n            avg_time[5] = avg_time[5] + (t7-t6)\n            avg_time[6] = avg_time[6] + (t8-t7)\n            avg_time[7] = avg_time[7] + (t9-t8)\n            avg_time[8] = avg_time[8] + (t9-t1)\n            print(\'-------------------------------\')\n            print(\'       load data : %f\' % (avg_time[0]/(batch_idx)))\n            print(\'     cpu to cuda : %f\' % (avg_time[1]/(batch_idx)))\n            print(\'cuda to variable : %f\' % (avg_time[2]/(batch_idx)))\n            print(\'       zero_grad : %f\' % (avg_time[3]/(batch_idx)))\n            print(\' forward feature : %f\' % (avg_time[4]/(batch_idx)))\n            print(\'    forward loss : %f\' % (avg_time[5]/(batch_idx)))\n            print(\'        backward : %f\' % (avg_time[6]/(batch_idx)))\n            print(\'            step : %f\' % (avg_time[7]/(batch_idx)))\n            print(\'           total : %f\' % (avg_time[8]/(batch_idx)))\n        t1 = time.time()\n        del data, target\n        org_loss.clear()\n        gc.collect()\n\n    print(\'\')\n    t1 = time.time()\n    nsamples = len(train_loader.dataset)\n    logging(\'[%03d] training with %f samples/s\' % (epoch, nsamples/(t1-t0)))\n    return nsamples\n    \ndef savemodel(epoch, nsamples, curmax=False):\n    cur_model = curmodel()\n    if curmax:\n        logging(\'save local maximum weights to %s/localmax.weights\' % (backupdir))\n    else:\n        logging(\'save weights to %s/%06d.weights\' % (backupdir, epoch))\n    cur_model.seen = epoch * nsamples\n    if curmax: \n        cur_model.save_weights(\'%s/localmax.weights\' % (backupdir))\n    else:\n        cur_model.save_weights(\'%s/%06d.weights\' % (backupdir, epoch))\n        old_wgts = \'%s/%06d.weights\' % (backupdir, epoch-keep_backup*save_interval)\n        try: #  it avoids the unnecessary call to os.path.exists()\n            os.remove(old_wgts)\n        except OSError:\n            pass\n\ndef test(epoch):\n    def truths_length(truths):\n        for i in range(50):\n            if truths[i][1] == 0:\n                return i\n        return 50\n\n    model.eval()\n    cur_model = curmodel()\n    num_classes = cur_model.num_classes\n    total       = 0.0\n    proposals   = 0.0\n    correct     = 0.0\n\n    if cur_model.net_name() == \'region\': # region_layer\n        shape=(0,0)\n    else:\n        shape=(cur_model.width, cur_model.height)\n    with torch.no_grad():\n        for data, target, org_w, org_h in test_loader:\n            data = data.to(device)\n            output = model(data)\n            all_boxes = get_all_boxes(output, shape, conf_thresh, num_classes, use_cuda=use_cuda)\n\n            for k in range(len(all_boxes)):\n                boxes = all_boxes[k]\n                correct_yolo_boxes(boxes, org_w[k], org_h[k], cur_model.width, cur_model.height)\n                boxes = np.array(nms(boxes, nms_thresh))\n\n                truths = target[k].view(-1, 5)\n                num_gts = truths_length(truths)\n                total = total + num_gts\n                num_pred = len(boxes)\n                if num_pred == 0:\n                    continue\n\n                proposals += int((boxes[:,4]>conf_thresh).sum())\n                for i in range(num_gts):\n                    gt_boxes = torch.FloatTensor([truths[i][1], truths[i][2], truths[i][3], truths[i][4], 1.0, 1.0, truths[i][0]])\n                    gt_boxes = gt_boxes.repeat(num_pred,1).t()\n                    pred_boxes = torch.FloatTensor(boxes).t()\n                    best_iou, best_j = torch.max(multi_bbox_ious(gt_boxes, pred_boxes, x1y1x2y2=False),0)\n                    # pred_boxes and gt_boxes are transposed for torch.max\n                    if best_iou > iou_thresh and pred_boxes[6][best_j] == gt_boxes[6][0]:\n                        correct += 1\n                        \n    precision = 1.0*correct/(proposals+eps)\n    recall = 1.0*correct/(total+eps)\n    fscore = 2.0*precision*recall/(precision+recall+eps)\n    savelog(""[%03d] correct: %d, precision: %f, recall: %f, fscore: %f"" % (epoch, correct, precision, recall, fscore))\n    return correct,fscore\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data\', \'-d\',\n        type=str, default=\'cfg/sketch.data\', help=\'data definition file\')\n    parser.add_argument(\'--config\', \'-c\',\n        type=str, default=\'cfg/sketch.cfg\', help=\'network configuration file\')\n    parser.add_argument(\'--weights\', \'-w\',\n        type=str, help=\'initial weights file\')\n    parser.add_argument(\'--initeval\', \'-i\', dest=\'init_eval\', action=\'store_true\',\n        help=\'performs inital evalulation\')\n    parser.add_argument(\'--noeval\', \'-n\', dest=\'no_eval\', action=\'store_true\',\n        help=\'prohibit test evalulation\')\n    parser.add_argument(\'--reset\', \'-r\',\n        action=""store_true"", default=False, help=\'initialize the epoch and model seen value\')\n    parser.add_argument(\'--localmax\', \'-l\',\n        action=""store_true"", default=False, help=\'save net weights for local maximum fscore\')\n\n    FLAGS, _ = parser.parse_known_args()\n    main()\n\n'"
utils.py,23,"b'import sys\nimport os\nimport time\nimport math\nimport torch\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\nimport itertools\nimport struct # get_image_size\nimport imghdr # get_image_size\n\ndef sigmoid(x):\n    return 1.0/(math.exp(-x)+1.)\n\ndef softmax(x):\n    x = torch.exp(x - torch.max(x))\n    x = x/x.sum()\n    return x\n\ndef bbox_iou(box1, box2, x1y1x2y2=True):\n    if x1y1x2y2:\n        x1_min = min(box1[0], box2[0])\n        x2_max = max(box1[2], box2[2])\n        y1_min = min(box1[1], box2[1])\n        y2_max = max(box1[3], box2[3])\n        w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n        w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n    else:\n        w1, h1 = box1[2], box1[3]\n        w2, h2 = box2[2], box2[3]\n        x1_min = min(box1[0]-w1/2.0, box2[0]-w2/2.0)\n        x2_max = max(box1[0]+w1/2.0, box2[0]+w2/2.0)\n        y1_min = min(box1[1]-h1/2.0, box2[1]-h2/2.0)\n        y2_max = max(box1[1]+h1/2.0, box2[1]+h2/2.0)\n\n    w_union = x2_max - x1_min\n    h_union = y2_max - y1_min\n    w_cross = w1 + w2 - w_union\n    h_cross = h1 + h2 - h_union\n    carea = 0\n    if w_cross <= 0 or h_cross <= 0:\n        return 0.0\n\n    area1 = w1 * h1\n    area2 = w2 * h2\n    carea = w_cross * h_cross\n    uarea = area1 + area2 - carea\n    return float(carea/uarea)\n\ndef multi_bbox_ious(boxes1, boxes2, x1y1x2y2=True):\n    if x1y1x2y2:\n        x1_min = torch.min(boxes1[0], boxes2[0])\n        x2_max = torch.max(boxes1[2], boxes2[2])\n        y1_min = torch.min(boxes1[1], boxes2[1])\n        y2_max = torch.max(boxes1[3], boxes2[3])\n        w1, h1 = boxes1[2] - boxes1[0], boxes1[3] - boxes1[1]\n        w2, h2 = boxes2[2] - boxes2[0], boxes2[3] - boxes2[1]\n    else:\n        w1, h1 = boxes1[2], boxes1[3]\n        w2, h2 = boxes2[2], boxes2[3]\n        x1_min = torch.min(boxes1[0]-w1/2.0, boxes2[0]-w2/2.0)\n        x2_max = torch.max(boxes1[0]+w1/2.0, boxes2[0]+w2/2.0)\n        y1_min = torch.min(boxes1[1]-h1/2.0, boxes2[1]-h2/2.0)\n        y2_max = torch.max(boxes1[1]+h1/2.0, boxes2[1]+h2/2.0)\n\n    w_union = x2_max - x1_min\n    h_union = y2_max - y1_min\n    w_cross = w1 + w2 - w_union\n    h_cross = h1 + h2 - h_union\n    mask = (((w_cross <= 0) + (h_cross <= 0)) > 0)\n    area1 = w1 * h1\n    area2 = w2 * h2\n    carea = w_cross * h_cross\n    carea[mask] = 0\n    uarea = area1 + area2 - carea\n    return carea/uarea\n\ndef nms(boxes, nms_thresh):\n    if len(boxes) == 0:\n        return boxes\n\n    res =[]\n    for item in boxes:\n        temp = []\n        for ite in item:\n            if torch.is_tensor(ite):\n                ite = float(ite.numpy())\n            temp.append(ite)\n        res.append(temp)\n    boxes = res\n\n    det_confs = np.zeros(len(boxes))\n    for i in range(len(boxes)):\n        det_confs[i] = 1-boxes[i][4]\n\n    sortIds = np.argsort(det_confs)\n    out_boxes = []\n    for i in range(len(boxes)):\n        box_i = boxes[sortIds[i]]\n        if box_i[4] > 0:\n            out_boxes.append(box_i)\n            for j in range(i+1, len(boxes)):\n                box_j = boxes[sortIds[j]]\n                if bbox_iou(box_i, box_j, x1y1x2y2=False) > nms_thresh:\n                    #print(box_i, box_j, bbox_iou(box_i, box_j, x1y1x2y2=False))\n                    box_j[4] = 0\n    return out_boxes\n\ndef convert2cpu(gpu_matrix):\n    return torch.FloatTensor(gpu_matrix.size()).copy_(gpu_matrix)\n\ndef convert2cpu_long(gpu_matrix):\n    return torch.LongTensor(gpu_matrix.size()).copy_(gpu_matrix)\n\ndef get_all_boxes(output, netshape, conf_thresh, num_classes, only_objectness=1, validation=False, use_cuda=True):\n    # total number of inputs (batch size)\n    # first element (x) for first tuple (x, anchor_mask, num_anchor)\n    tot = output[0][\'x\'].data.size(0)\n    all_boxes = [[] for i in range(tot)]\n    for i in range(len(output)):\n        pred = output[i][\'x\'].data\n\n        # find number of workers (.s.t, number of GPUS) \n        nw = output[i][\'n\'].data.size(0)\n        anchors = output[i][\'a\'].chunk(nw)[0]\n        num_anchors = output[i][\'n\'].data[0].item()\n\n        b = get_region_boxes(pred, netshape, conf_thresh, num_classes, anchors, num_anchors, \\\n                only_objectness=only_objectness, validation=validation, use_cuda=use_cuda)\n        for t in range(tot):\n            all_boxes[t] += b[t]\n    return all_boxes\n\ndef get_region_boxes(output, netshape, conf_thresh, num_classes, anchors, num_anchors, only_objectness=1, validation=False, use_cuda=True):\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n    anchors = anchors.to(device)\n    anchor_step = anchors.size(0)//num_anchors\n    if output.dim() == 3:\n        output = output.unsqueeze(0)\n    batch = output.size(0)\n    assert(output.size(1) == (5+num_classes)*num_anchors)\n    h = output.size(2)\n    w = output.size(3)\n    cls_anchor_dim = batch*num_anchors*h*w\n    if netshape[0] != 0:\n        nw, nh = netshape\n    else:\n        nw, nh = w, h\n\n    t0 = time.time()\n    all_boxes = []\n    output = output.view(batch*num_anchors, 5+num_classes, h*w).transpose(0,1).contiguous().view(5+num_classes, cls_anchor_dim)\n\n    grid_x = torch.linspace(0, w-1, w).repeat(batch*num_anchors, h, 1).view(cls_anchor_dim).to(device)\n    grid_y = torch.linspace(0, h-1, h).repeat(w,1).t().repeat(batch*num_anchors, 1, 1).view(cls_anchor_dim).to(device)\n    ix = torch.LongTensor(range(0,2)).to(device)\n    anchor_w = anchors.view(num_anchors, anchor_step).index_select(1, ix[0]).repeat(batch, h*w).view(cls_anchor_dim)\n    anchor_h = anchors.view(num_anchors, anchor_step).index_select(1, ix[1]).repeat(batch, h*w).view(cls_anchor_dim)\n\n    xs, ys = output[0].sigmoid() + grid_x, output[1].sigmoid() + grid_y\n    ws, hs = output[2].exp() * anchor_w.detach(), output[3].exp() * anchor_h.detach()\n    det_confs = output[4].sigmoid()\n\n    # by ysyun, dim=1 means input is 2D or even dimension else dim=0\n    cls_confs = torch.nn.Softmax(dim=1)(output[5:5+num_classes].transpose(0,1)).detach()\n    cls_max_confs, cls_max_ids = torch.max(cls_confs, 1)\n    cls_max_confs = cls_max_confs.view(-1)\n    cls_max_ids = cls_max_ids.view(-1)\n    t1 = time.time()\n    \n    sz_hw = h*w\n    sz_hwa = sz_hw*num_anchors\n    det_confs = convert2cpu(det_confs)\n    cls_max_confs = convert2cpu(cls_max_confs)\n    cls_max_ids = convert2cpu_long(cls_max_ids)\n    xs, ys = convert2cpu(xs), convert2cpu(ys)\n    ws, hs = convert2cpu(ws), convert2cpu(hs)\n    if validation:\n        cls_confs = convert2cpu(cls_confs.view(-1, num_classes))\n\n    t2 = time.time()\n    for b in range(batch):\n        boxes = []\n        for cy in range(h):\n            for cx in range(w):\n                for i in range(num_anchors):\n                    ind = b*sz_hwa + i*sz_hw + cy*w + cx\n                    det_conf =  det_confs[ind]\n                    conf = det_conf * (cls_max_confs[ind] if not only_objectness else 1.0)\n    \n                    if conf > conf_thresh:\n                        bcx = xs[ind]\n                        bcy = ys[ind]\n                        bw = ws[ind]\n                        bh = hs[ind]\n                        cls_max_conf = cls_max_confs[ind]\n                        cls_max_id = cls_max_ids[ind]\n                        box = [bcx/w, bcy/h, bw/nw, bh/nh, det_conf, cls_max_conf, cls_max_id]\n                        if (not only_objectness) and validation:\n                            for c in range(num_classes):\n                                tmp_conf = cls_confs[ind][c]\n                                if c != cls_max_id and det_confs[ind]*tmp_conf > conf_thresh:\n                                    box.append(tmp_conf)\n                                    box.append(c)\n                        boxes.append(box)\n        all_boxes.append(boxes)\n    t3 = time.time()\n    if False:\n        print(\'---------------------------------\')\n        print(\'matrix computation : %f\' % (t1-t0))\n        print(\'        gpu to cpu : %f\' % (t2-t1))\n        print(\'      boxes filter : %f\' % (t3-t2))\n        print(\'---------------------------------\')\n    return all_boxes\n\ndef plot_boxes_cv2(img, boxes, savename=None, class_names=None, color=None):\n    import cv2\n    colors = torch.FloatTensor([[1,0,1],[0,0,1],[0,1,1],[0,1,0],[1,1,0],[1,0,0]])\n    def get_color(c, x, max_val):\n        ratio = float(x)/max_val * 5\n        i = int(math.floor(ratio))\n        j = int(math.ceil(ratio))\n        ratio = ratio - i\n        r = (1-ratio) * colors[i][c] + ratio*colors[j][c]\n        return int(r*255)\n\n    width = img.shape[1]\n    height = img.shape[0]\n    for i in range(len(boxes)):\n        box = boxes[i]\n        x1 = int(round((box[0] - box[2]/2.0) * width))\n        y1 = int(round((box[1] - box[3]/2.0) * height))\n        x2 = int(round((box[0] + box[2]/2.0) * width))\n        y2 = int(round((box[1] + box[3]/2.0) * height))\n\n        if color:\n            rgb = color\n        else:\n            rgb = (255, 0, 0)\n        if len(box) >= 7 and class_names:\n            cls_conf = box[5]\n            cls_id = box[6]\n            #print(\'%s: %f\' % (class_names[cls_id], cls_conf))\n            classes = len(class_names)\n            offset = cls_id * 123457 % classes\n            red   = get_color(2, offset, classes)\n            green = get_color(1, offset, classes)\n            blue  = get_color(0, offset, classes)\n            if color is None:\n                rgb = (red, green, blue)\n            img = cv2.putText(img, class_names[cls_id], (x1,y1), cv2.FONT_HERSHEY_SIMPLEX, 1.2, rgb, 1)\n        img = cv2.rectangle(img, (x1,y1), (x2,y2), rgb, 1)\n    if savename:\n        print(""save plot results to %s"" % savename)\n        cv2.imwrite(savename, img)\n    return img\n\ndef drawrect(drawcontext, xy, outline=None, width=0):\n    x1, y1, x2, y2 = xy\n    points = (x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)\n    drawcontext.line(points, fill=outline, width=width)\n\ndef drawtext(img, pos, text, bgcolor=(255,255,255), font=None):\n    if font is None:\n        font = ImageFont.load_default().font\n    (tw, th) = font.getsize(text)\n    box_img = Image.new(\'RGB\', (tw+2, th+2), bgcolor)\n    ImageDraw.Draw(box_img).text((0, 0), text, fill=(0,0,0,255), font=font)\n    if img.mode != \'RGB\':\n        img = img.convert(\'RGB\')\n    sx, sy = int(pos[0]),int(pos[1]-th-2)\n    if sx<0:\n        sx=0\n    if sy<0:\n        sy=0\n    img.paste(box_img, (sx, sy))\n\ndef plot_boxes(img, boxes, savename=None, class_names=None):\n    colors = torch.FloatTensor([[1,0,1],[0,0,1],[0,1,1],[0,1,0],[1,1,0],[1,0,0]])\n    def get_color(c, x, max_val):\n        ratio = float(x)/max_val * 5\n        i = int(math.floor(ratio))\n        j = int(math.ceil(ratio))\n        ratio = ratio - i\n        r = (1-ratio) * colors[i][c] + ratio*colors[j][c]\n        return int(r*255)\n\n    width = img.width\n    height = img.height\n    draw = ImageDraw.Draw(img)\n    try:\n        font = ImageFont.truetype(""arialbd"", 14)\n    except:\n        font=None\n    print(""%d box(es) is(are) found"" % len(boxes))\n    for i in range(len(boxes)):\n        box = boxes[i]\n        x1,y1,x2,y2 = (box[0] - box[2]/2.0) * width, (box[1] - box[3]/2.0) * height, \\\n                (box[0] + box[2]/2.0) * width, (box[1] + box[3]/2.0) * height\n\n        rgb = (255, 0, 0)\n        if len(box) >= 7 and class_names:\n            cls_conf = box[5]\n            cls_id = int(box[6])\n            print(\'%s: %f\' % (class_names[cls_id], cls_conf))\n            classes = len(class_names)\n            offset = cls_id * 123457 % classes\n            red   = get_color(2, offset, classes)\n            green = get_color(1, offset, classes)\n            blue  = get_color(0, offset, classes)\n            rgb = (red, green, blue)\n            text = ""{} : {:.3f}"".format(class_names[cls_id],cls_conf)\n            drawtext(img, (x1, y1), text, bgcolor=rgb, font=font)\n        drawrect(draw, [x1, y1, x2, y2], outline=rgb, width=2)\n    if savename:\n        print(""save plot results to %s"" % savename)\n        img.save(savename)\n    return img\n\ndef read_truths(lab_path):\n    if not os.path.exists(lab_path):\n        return np.array([])\n    if os.path.getsize(lab_path):\n        truths = np.loadtxt(lab_path)\n        truths = truths.reshape(truths.size//5, 5) # to avoid single truth problem\n        return truths\n    else:\n        return np.array([])\n\ndef read_truths_args(lab_path, min_box_scale):\n    truths = read_truths(lab_path)\n    new_truths = []\n    for i in range(truths.shape[0]):\n        if truths[i][3] < min_box_scale:\n            continue\n        new_truths.append([truths[i][0], truths[i][1], truths[i][2], truths[i][3], truths[i][4]])\n    return np.array(new_truths)\n\ndef load_class_names(namesfile):\n    class_names = []\n    with open(namesfile, \'r\', encoding=\'utf8\') as fp:\n        lines = fp.readlines()\n    for line in lines:\n        class_names.append(line.strip())\n    return class_names\n\ndef image2torch(img):\n    if isinstance(img, Image.Image):\n        width = img.width\n        height = img.height\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes()))\n        img = img.view(height, width, 3).transpose(0,1).transpose(0,2).contiguous()\n        img = img.view(1, 3, height, width)\n        img = img.float().div(255.0)\n    elif type(img) == np.ndarray: # cv2 image\n        img = torch.from_numpy(img.transpose(2,0,1)).float().div(255.0).unsqueeze(0)\n    else:\n        print(""unknown image type"")\n        exit(-1)\n    return img\n\nimport types\ndef do_detect(model, img, conf_thresh, nms_thresh, use_cuda=True):\n    model.eval()\n    t0 = time.time()\n    img = image2torch(img)\n    t1 = time.time()\n\n    img = img.to(torch.device(""cuda"" if use_cuda else ""cpu""))\n    t2 = time.time()\n\n    out_boxes = model(img)\n    if model.net_name() == \'region\': # region_layer\n        shape=(0,0)\n    else:\n        shape=(model.width, model.height)\n    boxes = get_all_boxes(out_boxes, shape, conf_thresh, model.num_classes, use_cuda=use_cuda)[0]\n    \n    t3 = time.time()\n    boxes = nms(boxes, nms_thresh)\n    t4 = time.time()\n\n    if False:\n        print(\'-----------------------------------\')\n        print(\' image to tensor : %f\' % (t1 - t0))\n        print(\'  tensor to cuda : %f\' % (t2 - t1))\n        print(\'         predict : %f\' % (t3 - t2))\n        print(\'             nms : %f\' % (t4 - t3))\n        print(\'           total : %f\' % (t4 - t0))\n        print(\'-----------------------------------\')\n    return boxes\n\ndef read_data_cfg(datacfg):\n    options = dict()\n    options[\'gpus\'] = \'0,1,2,3\'\n    options[\'num_workers\'] = \'10\'\n    with open(datacfg, \'r\') as fp:\n        lines = fp.readlines()\n\n    for line in lines:\n        line = line.strip()\n        if line == \'\':\n            continue\n        key,value = line.split(\'=\')\n        key = key.strip()\n        value = value.strip()\n        options[key] = value\n    return options\n\ndef scale_bboxes(bboxes, width, height):\n    import copy\n    dets = copy.deepcopy(bboxes)\n    for i in range(len(dets)):\n        dets[i][0] = dets[i][0] * width\n        dets[i][1] = dets[i][1] * height\n        dets[i][2] = dets[i][2] * width\n        dets[i][3] = dets[i][3] * height\n    return dets\n      \ndef file_lines(thefilepath):\n    count = 0\n    thefile = open(thefilepath, \'rb\')\n    while True:\n        buffer = thefile.read(8192*1024)\n        if not buffer:\n            break\n        count += buffer.count(b\'\\n\')\n    thefile.close( )\n    return count\n\ndef get_image_size(fname):\n    \'\'\'Determine the image type of fhandle and return its size.\n    from draco\'\'\'\n    with open(fname, \'rb\') as fhandle:\n        head = fhandle.read(24)\n        if len(head) != 24: \n            return\n        if imghdr.what(fname) == \'png\':\n            check = struct.unpack(\'>i\', head[4:8])[0]\n            if check != 0x0d0a1a0a:\n                return\n            width, height = struct.unpack(\'>ii\', head[16:24])\n        elif imghdr.what(fname) == \'gif\':\n            width, height = struct.unpack(\'<HH\', head[6:10])\n        elif imghdr.what(fname) == \'jpeg\' or imghdr.what(fname) == \'jpg\':\n            try:\n                fhandle.seek(0) # Read 0xff next\n                size = 2 \n                ftype = 0 \n                while not 0xc0 <= ftype <= 0xcf:\n                    fhandle.seek(size, 1)\n                    byte = fhandle.read(1)\n                    while ord(byte) == 0xff:\n                        byte = fhandle.read(1)\n                    ftype = ord(byte)\n                    size = struct.unpack(\'>H\', fhandle.read(2))[0] - 2 \n                # We are at a SOFn block\n                fhandle.seek(1, 1)  # Skip `precision\' byte.\n                height, width = struct.unpack(\'>HH\', fhandle.read(4))\n            except Exception: #IGNORE:W0703\n                return\n        else:\n            return\n        return width, height\n\ndef logging(message):\n    print(\'%s %s\' % (time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime()), message))\n\ndef savelog(message):\n    logging(message)\n    with open(\'savelog.txt\', \'a\') as f:\n        print(\'%s %s\' % (time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime()), message), file=f)\n'"
valid.py,1,"b""import torch\nfrom darknet import Darknet\nimport dataset\nfrom torchvision import datasets, transforms\nfrom utils import get_all_boxes, bbox_iou, nms, read_data_cfg, load_class_names\nfrom image import correct_yolo_boxes\nimport os\n\ndef valid(datacfg, cfgfile, weightfile, outfile):\n    options = read_data_cfg(datacfg)\n    valid_images = options['valid']\n    name_list = options['names']\n    prefix = 'results'\n    names = load_class_names(name_list)\n\n    with open(valid_images) as fp:\n        tmp_files = fp.readlines()\n        valid_files = [item.rstrip() for item in tmp_files]\n    \n    m = Darknet(cfgfile)\n    m.print_network()\n    m.load_weights(weightfile)\n    m.cuda()\n    m.eval()\n\n    valid_dataset = dataset.listDataset(valid_images, shape=(m.width, m.height),\n                       shuffle=False,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                       ]))\n    valid_batchsize = 2\n    assert(valid_batchsize > 1)\n\n    kwargs = {'num_workers': 4, 'pin_memory': True}\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=valid_batchsize, shuffle=False, **kwargs) \n\n    fps = [0]*m.num_classes\n    if not os.path.exists('results'):\n        os.mkdir('results')\n    for i in range(m.num_classes):\n        buf = '%s/%s%s.txt' % (prefix, outfile, names[i])\n        fps[i] = open(buf, 'w')\n   \n    lineId = -1\n    \n    conf_thresh = 0.005\n    nms_thresh = 0.45\n    if m.net_name() == 'region': # region_layer\n        shape=(0,0)\n    else:\n        shape=(m.width, m.height)\n    for _, (data, target, org_w, org_h) in enumerate(valid_loader):\n        data = data.cuda()\n        output = m(data)\n        batch_boxes = get_all_boxes(output, shape, conf_thresh, m.num_classes, only_objectness=0, validation=True)\n        \n        for i in range(len(batch_boxes)):\n            lineId += 1\n            fileId = os.path.basename(valid_files[lineId]).split('.')[0]\n            #width, height = get_image_size(valid_files[lineId])\n            width, height = float(org_w[i]), float(org_h[i])\n            print(valid_files[lineId])\n            boxes = batch_boxes[i]\n            correct_yolo_boxes(boxes, width, height, m.width, m.height)\n            boxes = nms(boxes, nms_thresh)\n            for box in boxes:\n                x1 = (box[0] - box[2]/2.0) * width\n                y1 = (box[1] - box[3]/2.0) * height\n                x2 = (box[0] + box[2]/2.0) * width\n                y2 = (box[1] + box[3]/2.0) * height\n\n                det_conf = box[4]\n                for j in range((len(box)-5)//2):\n                    cls_conf = box[5+2*j]\n                    cls_id = int(box[6+2*j])\n                    prob = det_conf * cls_conf\n                    fps[cls_id].write('%s %f %f %f %f %f\\n' % (fileId, prob, x1, y1, x2, y2))\n\n    for i in range(m.num_classes):\n        fps[i].close()\n\nif __name__ == '__main__':\n    import sys\n    if len(sys.argv) == 4:\n        datacfg = sys.argv[1]\n        cfgfile = sys.argv[2]\n        weightfile = sys.argv[3]\n        outfile = 'comp4_det_test_'\n        valid(datacfg, cfgfile, weightfile, outfile)\n    else:\n        print('Usage:')\n        print(' python valid.py datacfg cfgfile weightfile')\n"""
yolo_layer.py,24,"b'import math\nimport numpy as np\nimport sys\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils import bbox_iou, multi_bbox_ious, convert2cpu\n\nclass YoloLayer(nn.Module):\n    def __init__(self, anchor_mask=[], num_classes=0, anchors=[1.0], num_anchors=1, use_cuda=None):\n        super(YoloLayer, self).__init__()\n        use_cuda = torch.cuda.is_available() and (True if use_cuda is None else use_cuda)\n        self.device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n        self.anchor_mask = anchor_mask\n        self.num_classes = num_classes\n        self.anchors = anchors\n        self.num_anchors = num_anchors\n        self.anchor_step = len(anchors)//num_anchors\n        self.rescore = 1\n        self.ignore_thresh = 0.5\n        self.truth_thresh = 1.\n        self.nth_layer = 0\n        self.seen = 0\n        self.net_width = 0\n        self.net_height = 0\n\n    def get_mask_boxes(self, output):\n        masked_anchors = []\n        for m in self.anchor_mask:\n            masked_anchors += self.anchors[m*self.anchor_step:(m+1)*self.anchor_step]\n\n        masked_anchors = torch.FloatTensor(masked_anchors).to(self.device)\n        num_anchors = torch.IntTensor([len(self.anchor_mask)]).to(self.device)\n        return {\'x\':output, \'a\':masked_anchors, \'n\':num_anchors}\n\n    def build_targets(self, pred_boxes, target, anchors, nA, nH, nW):\n        nB = target.size(0)\n        anchor_step = anchors.size(1) # anchors[nA][anchor_step]\n        noobj_mask = torch.ones (nB, nA, nH, nW)\n        obj_mask   = torch.zeros(nB, nA, nH, nW)\n        coord_mask = torch.zeros(nB, nA, nH, nW)\n        tcoord     = torch.zeros( 4, nB, nA, nH, nW)\n        tconf      = torch.zeros(nB, nA, nH, nW)\n        tcls       = torch.zeros(nB, nA, nH, nW, self.num_classes)\n\n        nAnchors = nA*nH*nW\n        nPixels  = nH*nW\n        nGT = 0\n        nRecall = 0\n        nRecall75 = 0\n\n        # it works faster on CPU than on GPU.\n        anchors = anchors.to(""cpu"")\n\n        for b in range(nB):\n            cur_pred_boxes = pred_boxes[b*nAnchors:(b+1)*nAnchors].t()\n            cur_ious = torch.zeros(nAnchors)\n            tbox = target[b].view(-1,5).to(""cpu"")\n\n            for t in range(50):\n                if tbox[t][1] == 0:\n                    break\n                gx, gy = tbox[t][1] * nW, tbox[t][2] * nH\n                gw, gh = tbox[t][3] * self.net_width, tbox[t][4] * self.net_height\n                cur_gt_boxes = torch.FloatTensor([gx, gy, gw, gh]).repeat(nAnchors,1).t()\n                cur_ious = torch.max(cur_ious, multi_bbox_ious(cur_pred_boxes, cur_gt_boxes, x1y1x2y2=False))\n            ignore_ix = (cur_ious>self.ignore_thresh).view(nA,nH,nW)\n            noobj_mask[b][ignore_ix] = 0\n\n            for t in range(50):\n                if tbox[t][1] == 0:\n                    break\n                nGT += 1\n                gx, gy = tbox[t][1] * nW, tbox[t][2] * nH\n                gw, gh = tbox[t][3] * self.net_width, tbox[t][4] * self.net_height\n                gw, gh = gw.float(), gh.float()\n                gi, gj = int(gx), int(gy)\n\n                tmp_gt_boxes = torch.FloatTensor([0, 0, gw, gh]).repeat(nA,1).t()\n                anchor_boxes = torch.cat((torch.zeros(nA, anchor_step), anchors),1).t()\n                _, best_n = torch.max(multi_bbox_ious(anchor_boxes, tmp_gt_boxes, x1y1x2y2=False), 0)\n\n                gt_box = torch.FloatTensor([gx, gy, gw, gh])\n                pred_box = pred_boxes[b*nAnchors+best_n*nPixels+gj*nW+gi]\n                iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n\n                obj_mask  [b][best_n][gj][gi] = 1\n                noobj_mask[b][best_n][gj][gi] = 0\n                coord_mask[b][best_n][gj][gi] = 2. - tbox[t][3]*tbox[t][4]\n                tcoord [0][b][best_n][gj][gi] = gx - gi\n                tcoord [1][b][best_n][gj][gi] = gy - gj\n                tcoord [2][b][best_n][gj][gi] = math.log(gw/anchors[best_n][0])\n                tcoord [3][b][best_n][gj][gi] = math.log(gh/anchors[best_n][1])\n                tcls      [b][best_n][gj][gi][int(tbox[t][0])] = 1\n                tconf     [b][best_n][gj][gi] = iou if self.rescore else 1.\n\n                if iou > 0.5:\n                    nRecall += 1\n                    if iou > 0.75:\n                        nRecall75 += 1\n\n        return nGT, nRecall, nRecall75, obj_mask, noobj_mask, coord_mask, tcoord, tconf, tcls\n\n    def forward(self, output, target):\n        #output : BxAs*(4+1+num_classes)*H*W\n        mask_tuple = self.get_mask_boxes(output)\n        t0 = time.time()\n        nB = output.data.size(0)    # batch size\n        nA = mask_tuple[\'n\'].item() # num_anchors\n        nC = self.num_classes\n        nH = output.data.size(2)\n        nW = output.data.size(3)\n        anchor_step = mask_tuple[\'a\'].size(0)//nA\n        anchors = mask_tuple[\'a\'].view(nA, anchor_step).to(self.device)\n        cls_anchor_dim = nB*nA*nH*nW\n\n        output  = output.view(nB, nA, (5+nC), nH, nW)\n        cls_grid = torch.linspace(5,5+nC-1,nC).long().to(self.device)\n        ix = torch.LongTensor(range(0,5)).to(self.device)\n        pred_boxes = torch.FloatTensor(4, cls_anchor_dim).to(self.device)\n\n        coord = output.index_select(2, ix[0:4]).view(nB*nA, -1, nH*nW).transpose(0,1).contiguous().view(-1,cls_anchor_dim)  # x, y, w, h\n        coord[0:2] = coord[0:2].sigmoid()\n        conf = output.index_select(2, ix[4]).view(cls_anchor_dim).sigmoid()\n\n        cls  = output.index_select(2, cls_grid)\n        cls  = cls.view(nB*nA, nC, nH*nW).transpose(1,2).contiguous().view(cls_anchor_dim, nC).to(self.device)\n\n        t1 = time.time()\n        grid_x = torch.linspace(0, nW-1, nW).repeat(nB*nA, nH, 1).view(cls_anchor_dim).to(self.device)\n        grid_y = torch.linspace(0, nH-1, nH).repeat(nW,1).t().repeat(nB*nA, 1, 1).view(cls_anchor_dim).to(self.device)\n        anchor_w = anchors.index_select(1, ix[0]).repeat(nB, nH*nW).view(cls_anchor_dim)\n        anchor_h = anchors.index_select(1, ix[1]).repeat(nB, nH*nW).view(cls_anchor_dim)\n\n        pred_boxes[0] = coord[0] + grid_x\n        pred_boxes[1] = coord[1] + grid_y\n        pred_boxes[2] = coord[2].exp() * anchor_w\n        pred_boxes[3] = coord[3].exp() * anchor_h\n        # for build_targets. it works faster on CPU than on GPU\n        pred_boxes = convert2cpu(pred_boxes.transpose(0,1).contiguous().view(-1,4)).detach()\n\n        t2 = time.time()\n        nGT, nRecall, nRecall75, obj_mask, noobj_mask, coord_mask, tcoord, tconf, tcls = \\\n            self.build_targets(pred_boxes, target.detach(), anchors.detach(), nA, nH, nW)\n\n        conf_mask = (obj_mask + noobj_mask).view(cls_anchor_dim).to(self.device)\n        obj_mask  = (obj_mask==1).view(cls_anchor_dim)\n\n        nProposals = int((conf > 0.25).sum())\n\n        coord = coord[:,obj_mask]\n        tcoord = tcoord.view(4, cls_anchor_dim)[:,obj_mask].to(self.device)        \n\n        tconf = tconf.view(cls_anchor_dim).to(self.device)        \n\n        cls = cls[obj_mask,:].to(self.device)\n        tcls = tcls.view(cls_anchor_dim, nC)[obj_mask,:].to(self.device)\n\n        t3 = time.time()\n        loss_coord = nn.BCELoss(reduction=\'sum\')(coord[0:2], tcoord[0:2])/nB + \\\n                     nn.MSELoss(reduction=\'sum\')(coord[2:4], tcoord[2:4])/nB\n        loss_conf  = nn.BCELoss(reduction=\'sum\')(conf*conf_mask, tconf*conf_mask)/nB\n        loss_cls   = nn.BCEWithLogitsLoss(reduction=\'sum\')(cls, tcls)/nB\n\n        loss = loss_coord + loss_conf + loss_cls\n\n        t4 = time.time()\n        if False:\n            print(\'-\'*30)\n            print(\'        activation : %f\' % (t1 - t0))\n            print(\' create pred_boxes : %f\' % (t2 - t1))\n            print(\'     build targets : %f\' % (t3 - t2))\n            print(\'       create loss : %f\' % (t4 - t3))\n            print(\'             total : %f\' % (t4 - t0))\n        print(\'%d: Layer(%03d) nGT %3d, nRC %3d, nRC75 %3d, nPP %3d, loss: box %6.3f, conf %6.3f, class %6.3f, total %7.3f\' \n                % (self.seen, self.nth_layer, nGT, nRecall, nRecall75, nProposals, loss_coord, loss_conf, loss_cls, loss))\n        if math.isnan(loss.item()):\n            print(coord, conf, tconf)\n            sys.exit(0)\n        return loss\n'"
data/voc_label.py,0,"b'import xml.etree.ElementTree as ET\nimport pickle\nimport os\nfrom os import listdir, getcwd\nfrom os.path import join\n\nsets=[(\'2012\', \'train\'), (\'2012\', \'val\'), (\'2007\', \'train\'), (\'2007\', \'val\'), (\'2007\', \'test\')]\n\nclasses = [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"", ""tvmonitor""]\n\n\ndef convert(size, box):\n    dw = 1./size[0]\n    dh = 1./size[1]\n    x = (box[0] + box[1])/2.0\n    y = (box[2] + box[3])/2.0\n    w = box[1] - box[0]\n    h = box[3] - box[2]\n    x = x*dw\n    w = w*dw\n    y = y*dh\n    h = h*dh\n    return (x,y,w,h)\n\ndef convert_annotation(year, image_id):\n    in_file = open(\'VOCdevkit/VOC%s/Annotations/%s.xml\'%(year, image_id))\n    out_file = open(\'VOCdevkit/VOC%s/labels/%s.txt\'%(year, image_id), \'w\')\n    tree=ET.parse(in_file)\n    root = tree.getroot()\n    size = root.find(\'size\')\n    w = int(size.find(\'width\').text)\n    h = int(size.find(\'height\').text)\n\n    for obj in root.iter(\'object\'):\n        difficult = obj.find(\'difficult\').text\n        cls = obj.find(\'name\').text\n        if cls not in classes or int(difficult) == 1:\n            continue\n        cls_id = classes.index(cls)\n        xmlbox = obj.find(\'bndbox\')\n        b = (float(xmlbox.find(\'xmin\').text), float(xmlbox.find(\'xmax\').text), float(xmlbox.find(\'ymin\').text), float(xmlbox.find(\'ymax\').text))\n        bb = convert((w,h), b)\n        out_file.write(str(cls_id) + "" "" + "" "".join([str(a) for a in bb]) + \'\\n\')\n\nwd = getcwd()\n\nfor year, image_set in sets:\n    if not os.path.exists(\'VOCdevkit/VOC%s/labels/\'%(year)):\n        os.makedirs(\'VOCdevkit/VOC%s/labels/\'%(year))\n    image_ids = open(\'VOCdevkit/VOC%s/ImageSets/Main/%s.txt\'%(year, image_set)).read().strip().split()\n    list_file = open(\'%s_%s.txt\'%(year, image_set), \'w\')\n    for image_id in image_ids:\n        list_file.write(\'%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg\\n\'%(wd, year, image_id))\n        convert_annotation(year, image_id)\n    list_file.close()\n\n'"
models/caffe_net.py,10,"b'import numpy as np\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom PIL import Image, ImageDraw\nimport sys\nfrom collections import OrderedDict\nfrom utils import do_detect, plot_boxes, load_class_names\nsys.path.append(\'/home/xiaohang/caffe/python\')\nsys.path.append(\'.\')\nimport caffe\nfrom region_loss import RegionLoss\nclass Scale(nn.Module):\n    def __init__(self):\n        super(Scale, self).__init__()\n    def forward(self, x):\n        return x\n\n\nclass Eltwise(nn.Module):\n    def __init__(self, operation=\'+\'):\n        super(Eltwise, self).__init__()\n        self.operation = operation\n\n    def forward(self, input_feats):\n        if isinstance(input_feats, tuple):\n            print ""error : The input of Eltwise layer must be a tuple""\n        for i, feat in enumerate(input_feats):\n            if x is None:\n                x = feat\n                continue\n            if self.operation == \'+\':\n                x += feat\n            if self.operation == \'*\':\n                x *= feat\n            if self.operation == \'/\':\n                x /= feat\n        return x\n\nclass Concat(nn.Module):\n    def __init__(self):\n        super(Concat, self).__init__()\n\n    def forward(self, input_feats):\n        if not isinstance(input_feats, tuple):\n            print \'The input of Concat layer must be a tuple\'\n        self.length = len(input_feats)\n        x = torch.cat(input_feats, 1)\n        return x\n\n\n\ndef parse_prototxt(protofile):\n    def line_type(line):\n        if line.find(\':\') >= 0:\n            return 0\n        elif line.find(\'{\') >= 0:\n            return 1\n        return -1\n\n    def parse_param_block(fp):\n        block = dict()\n        line = fp.readline().strip()\n        while line != \'}\':\n            ltype = line_type(line)\n            if ltype == 0: # key: value\n                key, value = line.split(\':\')\n                key = key.strip()\n                value = value.strip().strip(\'""\')\n                block[key] = value\n            elif ltype == 1: # blockname {\n                key = line.split(\'{\')[0].strip()\n                sub_block = parse_param_block(fp)\n                block[key] = sub_block\n            line = fp.readline().strip()\n        return block\n\n    def parse_layer_block(fp):\n        block = dict()\n        block[\'top\'] = []\n        block[\'bottom\'] = []\n        line = fp.readline().strip()\n        while line != \'}\':\n            ltype = line_type(line)\n            if ltype == 0: # key: value\n                key, value = line.split(\':\')\n                key = key.strip()\n                value = value.strip().strip(\'""\')\n                if key == \'top\' or key == \'bottom\':\n                    block[key].append(value)\n                else:\n                    block[key] = value\n            elif ltype == 1: # blockname {\n                key = line.split(\'{\')[0].strip()\n                sub_block = parse_param_block(fp)\n                block[key] = sub_block\n            line = fp.readline().strip()\n        return block\n\n    fp = open(protofile, \'r\')\n    props = dict()\n    layers = []\n    line = fp.readline()\n    while line != \'\':\n        ltype = line_type(line)\n        if ltype == 0: # key: value\n            key, value = line.split(\':\')\n            key = key.strip()\n            value = value.strip().strip(\'""\')\n            props[key] = value\n        elif ltype == 1: # blockname {\n            key = line.split(\'{\')[0].strip()\n            assert(key == \'layer\' or key == \'input_shape\')\n            layer = parse_layer_block(fp)\n            layers.append(layer)\n            #print layer\n        line = fp.readline()\n    net_info = dict()\n    net_info[\'props\'] = props\n    net_info[\'layers\'] = layers\n    #print net_info\n\n    return net_info\n\n\nclass CaffeNet(nn.Module):\n    def __init__(self, protofile, caffemodel):\n        super(CaffeNet, self).__init__()\n        self.seen = 0\n        self.num_classes = 1\n        self.is_pretrained = True\n        if not caffemodel is None:\n            self.is_pretrained = True\n        self.anchors = [0.625,0.750,   0.625,0.750,   0.625,0.750, \\\n                0.625,0.750,   0.625,0.750,   1.000,1.200,  \\\n                1.000,1.200,   1.000,1.200,   1.000,1.200,   \\\n                1.600,1.920,   2.560,3.072,   4.096,4.915,  \\\n                6.554,7.864,   10.486,12.583]\n        #self.anchors = [1.3221, 1.73145, 3.19275, 4.00944, 5.05587, 8.09892, 9.47112, 4.84053, 11.2364, 10.0071]\n        self.num_anchors = len(self.anchors)/2\n        self.width = 480\n        self.height = 320\n\n        self.loss = RegionLoss(self.num_classes, self.anchors, self.num_anchors)\n\n        self.net_info = parse_prototxt(protofile)\n        self.models = self.create_network(self.net_info)\n        self.modelList = nn.ModuleList()\n        if self.is_pretrained:\n            self.load_weigths_from_caffe(protofile, caffemodel)\n        for name,model in self.models.items():\n            self.modelList.append(model)\n\n\n    def load_weigths_from_caffe(self, protofile, caffemodel):\n        caffe.set_mode_cpu()\n        net = caffe.Net(protofile, caffemodel, caffe.TEST)\n        for name, layer in self.models.items():\n            if isinstance(layer, nn.Conv2d):\n                caffe_weight = net.params[name][0].data\n                layer.weight.data = torch.from_numpy(caffe_weight)\n                if len(net.params[name]) > 1:\n                    caffe_bias = net.params[name][1].data\n                    layer.bias.data = torch.from_numpy(caffe_bias)\n                continue\n            if isinstance(layer, nn.BatchNorm2d):\n                caffe_means = net.params[name][0].data\n                caffe_var = net.params[name][1].data\n                layer.running_mean = torch.from_numpy(caffe_means)\n                layer.running_var = torch.from_numpy(caffe_var)\n                # find the scale layer\n                top_name_of_bn = self.layer_map_to_top[name][0]\n                scale_name = \'\'\n                for caffe_layer in self.net_info[\'layers\']:\n                    if caffe_layer[\'type\'] == \'Scale\' and caffe_layer[\'bottom\'][0] == top_name_of_bn:\n                        scale_name = caffe_layer[\'name\']\n                        break\n                if scale_name != \'\':\n                    caffe_weight = net.params[scale_name][0].data\n                    layer.weight.data = torch.from_numpy(caffe_weight)\n                    if len(net.params[name]) > 1:\n                        caffe_bias = net.params[scale_name][1].data\n                        layer.bias.data = torch.from_numpy(caffe_bias)\n\n\n\n    def print_network(self):\n        print(self.net_info)\n\n    def create_network(self, net_info):\n        #print net_info\n        models = OrderedDict()\n        top_dim = {\'data\': 3}\n        self.layer_map_to_bottom = dict()\n        self.layer_map_to_top = dict()\n\n        for layer in net_info[\'layers\']:\n            name = layer[\'name\']\n            ltype = layer[\'type\']\n\n            if ltype == \'Data\':\n                continue\n            if ltype == \'ImageData\':\n                continue\n            if layer.has_key(\'top\'):\n                tops = layer[\'top\']\n                self.layer_map_to_top[name] = tops\n            if layer.has_key(\'bottom\'):\n                bottoms = layer[\'bottom\']\n                self.layer_map_to_bottom[name] = bottoms\n            if ltype == \'Convolution\':\n                filters = int(layer[\'convolution_param\'][\'num_output\'])\n                kernel_size = int(layer[\'convolution_param\'][\'kernel_size\'])\n                stride = 1\n                group = 1\n                pad = 0\n                bias = True\n                dilation = 1\n                if layer[\'convolution_param\'].has_key(\'stride\'):\n                    stride = int(layer[\'convolution_param\'][\'stride\'])\n                if layer[\'convolution_param\'].has_key(\'pad\'):\n                    pad = int(layer[\'convolution_param\'][\'pad\'])\n                if layer[\'convolution_param\'].has_key(\'group\'):\n                    group = int(layer[\'convolution_param\'][\'group\'])\n                if layer[\'convolution_param\'].has_key(\'bias_term\'):\n                    bias = True if layer[\'convolution_param\']\\\n                            [\'bias_term\'].lower() == \'false\' else False \n                if layer[\'convolution_param\'].has_key(\'dilation\'):\n                    dilation = int(layer[\'convolution_param\'][\'dilation\'])\n                num_output = int(layer[\'convolution_param\'][\'num_output\'])\n                top_dim[tops[0]]=num_output\n                num_input = top_dim[bottoms[0]]\n                models[name] =  nn.Conv2d(num_input, num_output, kernel_size,\n                    stride,pad,groups=group, bias=bias, dilation=dilation)\n            elif ltype == \'ReLU\':\n                inplace = (bottoms == tops)\n                top_dim[tops[0]] = top_dim[bottoms[0]]\n                models[name] = nn.ReLU(inplace=False)\n            elif ltype == \'Pooling\':\n                kernel_size = int(layer[\'pooling_param\'][\'kernel_size\'])\n                stride = 1\n                if layer[\'pooling_param\'].has_key(\'stride\'):\n                    stride = int(layer[\'pooling_param\'][\'stride\'])\n                top_dim[tops[0]] = top_dim[bottoms[0]]\n                models[name] = nn.MaxPool2d(kernel_size, stride)\n            elif ltype == \'BatchNorm\':\n                if layer[\'batch_norm_param\'].has_key(\'use_global_stats\'):\n                    use_global_stats = True if layer[\'batch_norm_param\']\\\n                            [\'use_global_stats\'].lower() == \'true\' else False\n                top_dim[tops[0]] = top_dim[bottoms[0]]\n                \n                models[name] = nn.BatchNorm2d(top_dim[bottoms[0]])\n\n            elif ltype == \'Scale\':\n                top_dim[tops[0]] = top_dim[bottoms[0]]\n                models[name] = Scale()\n            elif ltype == \'Eltwise\':\n                top_dim[tops[0]] = top_dim[bottoms[0]]\n                models[name] = Eltwise(\'+\')\n            elif ltype == \'Concat\':\n                top_dim[tops[0]] = 0\n                for i, x in enumerate(bottoms):\n                    top_dim[tops[0]] += top_dim[x]\n                models[name] = Concat()\n            elif ltype == \'Dropout\':\n                if layer[\'top\'][0] == layer[\'bottom\'][0]:\n                    inplace = True\n                else: \n                    inplace = False\n                top_dim[tops[0]] = top_dim[bottoms[0]]\n                models[name] = nn.Dropout2d(inplace=inplace)\n            else:\n                print \'%s is not NotImplemented\'%ltype\n\n        return models\n\n    def forward(self, x, target=None):\n        blobs = OrderedDict()\n        for name, layer in self.models.items():\n            output_names = self.layer_map_to_top[name]\n            input_names = self.layer_map_to_bottom[name]\n            print ""-----------------------------------------""\n            print \'input_names: \',input_names\n            print \'output_names:\',output_names\n            print layer\n            # frist layer\n            if input_names[0] == \'data\':\n                top_blobs = layer(x)\n            else:\n                input_blobs = [blobs[i] for i in input_names ]\n                if isinstance(layer, Concat) or isinstance(layer, Eltwise):\n                    top_blobs = layer(input_blobs)\n                else:\n                    top_blobs = layer(input_blobs[0])\n            if not isinstance(top_blobs, tuple):\n                top_blobs = (top_blobs,)\n\n            for k, v in zip(output_names, top_blobs):\n                blobs[k] = v\n        output_name = blobs.keys()[-1]\n        print \'output_name\',output_name\n        return blobs[output_name]\n\n\n\nif __name__ == \'__main__\':\n    prototxt = \'tiny_yolo_nbn_reluface.prototxt\'\n    caffemodel = \'/nfs/xiaohang/for_chenchao/tiny_yolo_nbn_reluface.caffemodel\'\n    imgfile = \'data/face.jpg\'\n    \n    m = CaffeNet(prototxt, caffemodel)\n    use_cuda = 1\n    if use_cuda:\n        m.cuda()\n\n    img = Image.open(imgfile).convert(\'RGB\')\n    sized = img.resize((m.width, m.height))\n    #if m.num_classes == 20:\n    #    namesfile = \'../data/voc.names\'\n    #class_names = load_class_names(namesfile)\n    class_names = [\'face\']\n    for i in range(1):\n        start = time.time()\n        boxes = do_detect(m, sized, 0.5, 0.4, use_cuda)\n        finish = time.time()\n        if i == 1:\n            print(\'%s: Predicted in %f seconds.\' % (imgfile, (finish-start)))\n\n    plot_boxes(img, boxes, \'predictions.jpg\', class_names)\n'"
models/resnet.py,2,"b'import numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nfrom region_loss import RegionLoss\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, \n                     stride=stride, padding=1, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\nclass Resnet101(nn.Module):\n    def __init__(self):\n        super(Resnet, self).__init__()\n        self.seen = 0\n        self.num_classes = 20\n        self.anchors = [1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52]\n        self.num_anchors = len(self.anchors)/2\n        num_output = (5+self.num_classes)*self.num_anchors\n        self.width = 160\n        self.height = 160\n\n        self.loss = RegionLoss(self.num_classes, self.anchors, self.num_anchors)\n        self.model = ResNet(Bottleneck, [3, 4, 6, 3])\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n    def print_network(self):\n        print(self)\n\n'"
models/tiny_yolo.py,2,"b""import numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nfrom cfg import *\nfrom darknet import MaxPoolStride1\nfrom region_loss import RegionLoss\n\nclass TinyYoloNet(nn.Module):\n    def __init__(self):\n        super(TinyYoloNet, self).__init__()\n        self.seen = 0\n        self.num_classes = 20\n        self.anchors = [1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52]\n        self.num_anchors = len(self.anchors)/2\n        num_output = (5+self.num_classes)*self.num_anchors\n        self.width = 160\n        self.height = 160\n\n        self.loss = RegionLoss(self.num_classes, self.anchors, self.num_anchors)\n        self.cnn = nn.Sequential(OrderedDict([\n            # conv1\n            ('conv1', nn.Conv2d( 3, 16, 3, 1, 1, bias=False)),\n            ('bn1', nn.BatchNorm2d(16)),\n            ('leaky1', nn.LeakyReLU(0.1, inplace=True)),\n            ('pool1', nn.MaxPool2d(2, 2)),\n\n            # conv2\n            ('conv2', nn.Conv2d(16, 32, 3, 1, 1, bias=False)),\n            ('bn2', nn.BatchNorm2d(32)),\n            ('leaky2', nn.LeakyReLU(0.1, inplace=True)),\n            ('pool2', nn.MaxPool2d(2, 2)),\n\n            # conv3\n            ('conv3', nn.Conv2d(32, 64, 3, 1, 1, bias=False)),\n            ('bn3', nn.BatchNorm2d(64)),\n            ('leaky3', nn.LeakyReLU(0.1, inplace=True)),\n            ('pool3', nn.MaxPool2d(2, 2)),\n\n            # conv4\n            ('conv4', nn.Conv2d(64, 128, 3, 1, 1, bias=False)),\n            ('bn4', nn.BatchNorm2d(128)),\n            ('leaky4', nn.LeakyReLU(0.1, inplace=True)),\n            ('pool4', nn.MaxPool2d(2, 2)),\n\n            # conv5\n            ('conv5', nn.Conv2d(128, 256, 3, 1, 1, bias=False)),\n            ('bn5', nn.BatchNorm2d(256)),\n            ('leaky5', nn.LeakyReLU(0.1, inplace=True)),\n            ('pool5', nn.MaxPool2d(2, 2)),\n\n            # conv6\n            ('conv6', nn.Conv2d(256, 512, 3, 1, 1, bias=False)),\n            ('bn6', nn.BatchNorm2d(512)),\n            ('leaky6', nn.LeakyReLU(0.1, inplace=True)),\n            ('pool6', MaxPoolStride1()),\n\n            # conv7\n            ('conv7', nn.Conv2d(512, 1024, 3, 1, 1, bias=False)),\n            ('bn7', nn.BatchNorm2d(1024)),\n            ('leaky7', nn.LeakyReLU(0.1, inplace=True)),\n\n            # conv8\n            ('conv8', nn.Conv2d(1024, 1024, 3, 1, 1, bias=False)),\n            ('bn8', nn.BatchNorm2d(1024)),\n            ('leaky8', nn.LeakyReLU(0.1, inplace=True)),\n\n            # output\n            ('output', nn.Conv2d(1024, num_output, 1, 1, 0)),\n        ]))\n\n    def forward(self, x):\n        x = self.cnn(x)\n        return x\n\n    def print_network(self):\n        print(self)\n\n    def load_weights(self, path):\n        #buf = np.fromfile('tiny-yolo-voc.weights', dtype = np.float32)\n        buf = np.fromfile(path, dtype = np.float32)\n        start = 4\n        \n        start = load_conv_bn(buf, start, self.cnn[0], self.cnn[1])\n        start = load_conv_bn(buf, start, self.cnn[4], self.cnn[5])\n        start = load_conv_bn(buf, start, self.cnn[8], self.cnn[9])\n        start = load_conv_bn(buf, start, self.cnn[12], self.cnn[13])\n        start = load_conv_bn(buf, start, self.cnn[16], self.cnn[17])\n        start = load_conv_bn(buf, start, self.cnn[20], self.cnn[21])\n        \n        start = load_conv_bn(buf, start, self.cnn[24], self.cnn[25])\n        start = load_conv_bn(buf, start, self.cnn[27], self.cnn[28])\n        start = load_conv(buf, start, self.cnn[30])\n\nif __name__ == '__main__':\n    from PIL import Image\n    from utils import *\n    m = TinyYoloNet() \n    m.float()\n    m.eval()\n    m.load_darknet_weights('tiny-yolo-voc.weights')\n    print(m)\n    \n    use_cuda = 1\n    if use_cuda:\n        m.cuda()\n\n    img = Image.open('data/person.jpg').convert('RGB')\n    sized = img.resize((416,416))\n    boxes = do_detect(m, sized, 0.5, 0.4, use_cuda)\n\n    class_names = load_class_names('data/voc.names')\n    plot_boxes(img, boxes, 'predict1.jpg', class_names)  \n\n"""
scripts/coco_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport os,sys\n#import cPickle\nimport _pickle as cPickle\nimport numpy as np\nfrom scripts.eval_ap import parse_rec\nfrom scripts.eval_all import get_image_xml_name\nfrom utils import load_class_names\nfrom scripts.my_eval import compute_ap\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n_classes = None\ndef convert_bb2lab(classname, imagepath):\n    info_path = imagepath.replace(\'images\', \'labels\'). \\\n            replace(\'JPEGImages\', \'labels\'). \\\n            replace(\'.jpg\', \'.txt\').replace(\'.png\',\'.txt\')\n    img = Image.open(imagepath)\n    w, h = img.size\n    objs = []\n    try:\n        gt_bbs = np.loadtxt(info_path)\n    except:\n        return objs\n\n    gt_bbs = gt_bbs.reshape(gt_bbs.size//5, 5)\n    for i in range(len(gt_bbs)):\n        obj = {}\n        gt_bb = gt_bbs[i]\n\n        obj[\'name\'] = classname[(int)(gt_bb[0])]\n        obj[\'pose\'] = \'Unspecified\'\n        obj[\'truncated\'] = 0\n        obj[\'difficult\'] = 0\n\n        bb = np.zeros(4);\n        hbw = gt_bb[3]/2.0 # half bounding box width\n        hbh = gt_bb[4]/2.0 # half bounding box height\n        bb[0] = (int)((gt_bb[1] - hbw) * w)    # xmin\n        bb[1] = (int)((gt_bb[2] - hbh) * h)    # ymin\n        bb[2] = (int)((gt_bb[1] + hbw) * w)    # xmax\n        bb[3] = (int)((gt_bb[2] + hbh) * h)    # ymax\n        obj[\'bbox\'] = bb\n        objs.append(obj)\n    return objs\n\ndef coco_eval(detpath, imagesetfile, classname, cachedir,\n             ovthresh=0.5, use_07_metric=False):\n    """"""rec, prec, ap = coco_eval(detpath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """"""\n    # assumes detections are in detpath.format(classname)\n    # assumes annotations are in annopath.format(imagename)\n    # assumes imagesetfile is a text file with each line an image name\n    # cachedir caches the annotations in a pickle file\n\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            imagekey = os.path.basename(imagename).split(\'.\')[0]    \n            lab = convert_bb2lab(_classes, imagename)\n            if len(lab) > 0:\n                recs[imagekey] = lab\n            else:\n                print(""skipped key: {}, path: {}"".format(imagekey, imagename))\n\n            if i % 100 == 0:\n                print (\'Reading annotation for {:d}/{:d}\'.format(\n                    i + 1, len(imagenames)))\n        # save\n        print (\'Saving cached annotations to {:s}\'.format(cachefile))\n        with open(cachefile, \'wb\') as f:\n            cPickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'rb\') as f:\n            recs = cPickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        imagekey = os.path.basename(imagename).split(\'.\')[0]\n        try:\n            R = [obj for obj in recs[imagekey] if obj[\'name\'] == classname]\n        except KeyError:\n            #print(""skipped: %s %s"" % (imagename, imagekey))\n            continue;\n        except Exception as e:\n            print(type(e))\n            print(e.args)\n            print(e)\n            print(""%s %s"" % (imagename, imagekey))\n            exit(0)\n\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagekey] = {\'bbox\': bbox, \'difficult\': difficult, \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n\n    splitlines = [x.strip().split(\' \') for x in lines]\n    image_ids = [x[0] for x in splitlines]\n    confidence = np.array([float(x[1]) for x in splitlines])\n    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n    # go down dets and mark TPs and FPs\n    nd = len(image_ids)\n    tp = np.zeros(nd)\n    fp = np.zeros(nd)\n    for d in range(nd):\n        try:\n            R = class_recs[image_ids[d]]\n        except KeyError:\n            #print(""skipeed: {}"".format(image_ids[d]))\n            continue;\n\n        bb = BB[d, :].astype(float)\n        ovmax = -np.inf\n        BBGT = R[\'bbox\'].astype(float)\n\n        if BBGT.size > 0:\n            # compute overlaps\n            # intersection\n            ixmin = np.maximum(BBGT[:, 0], bb[0])\n            iymin = np.maximum(BBGT[:, 1], bb[1])\n            ixmax = np.minimum(BBGT[:, 2], bb[2])\n            iymax = np.minimum(BBGT[:, 3], bb[3])\n            iw = np.maximum(ixmax - ixmin + 1., 0.)\n            ih = np.maximum(iymax - iymin + 1., 0.)\n            inters = iw * ih\n\n            # union\n            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n            overlaps = inters / uni\n            ovmax = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n\n        if ovmax > ovthresh:\n            if not R[\'difficult\'][jmax]:\n                if not R[\'det\'][jmax]:\n                    tp[d] = 1.\n                    R[\'det\'][jmax] = 1\n                else:\n                    fp[d] = 1.\n        else:\n            fp[d] = 1.\n\n    # compute precision recall\n    fp = np.cumsum(fp)\n    tp = np.cumsum(tp)\n    rec = tp / float(npos)\n    # avoid divide by zero in case the first detection matches a difficult\n    # ground truth\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = compute_ap(rec, prec, use_07_metric)\n\n    #print(\'class: {:<10s} \\t num occurrence: {:4d}\'.format(classname, npos))\n\n    return rec, prec, ap, npos\n    \ndef _do_python_eval(res_prefix, imagesetfile, classesfile, output_dir = \'output\'):\n    \n    filename = res_prefix + \'{:s}.txt\'\n\n    cachedir = os.path.join(output_dir, \'annotations_cache\')\n    aps = []\n    # The PASCAL VOC metric changed in 2010\n    use_07_metric = False\n    #print (\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n        \n    global _classes\n    _classes = load_class_names(classesfile)\n\n    total = 0\n    for i, cls in enumerate(_classes):\n        if cls == \'__background__\':\n            continue\n      \n        rec, prec, ap, noccur = coco_eval(\n            filename, imagesetfile, cls, cachedir, ovthresh=0.5,\n            use_07_metric=use_07_metric)\n        aps += [ap]\n        total += noccur\n        print(\'AP for {:<10s} = {:.4f} with {:4d} views\'.format(cls, ap, noccur))\n        with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n            cPickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n\n    print(\'Mean AP = {:.4f} with total {:4d} views\'.format(np.mean(aps), total))\n    \n    print(\'~\'*30)\n    print(\' \'*10, \'Results:\')\n    print(\'-\'*30)\n    for i, ap in enumerate(aps):\n        print(\'{:<10s}\\t{:.3f}\'.format(_classes[i], ap))\n    print(\'=\'*30)\n    print(\'{:^10s}\\t{:.3f}\'.format(\'Average\', np.mean(aps)))\n    print(\'~\'*30)\n    print(\'\')\n    print(\'--------------------------------------------------------------\')\n    print(\'Results computed with the **unofficial** Python eval code.\')\n    print(\'Results should be very close to the official MATLAB eval code.\')\n    print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n    print(\'-- Thanks, The Management\')\n    print(\'--------------------------------------------------------------\')\n\n\nif __name__ == \'__main__\':\n    #res_prefixc = \'/data/hongji/darknet/results/comp4_det_test_\' \n    #res_prefix = \'results/comp4_det_test_\'    \n    #test_file = \'data/sketch_test.txt\'\n    #class_names = \'data/sketch.names\'\n    res_prefix = sys.argv[1]\n    test_file = sys.argv[2]\n    class_names = sys.argv[3]\n    _do_python_eval(res_prefix, test_file, class_names, output_dir = \'output\')\n\n\n'"
scripts/eval_all.py,1,"b'import os\nimport os.path\nfrom PIL import Image\nimport sys\nfrom torch.autograd import Variable\nfrom darknet import Darknet\nfrom utils import get_all_boxes, do_detect, plot_boxes, load_class_names, image2torch, get_region_boxes, nms\nimport numpy as np\n\nconf_thresh = 0.005\n#conf_thresh = 0.5\nnms_thresh = 0.45\ndef save_boxes(imgfile, img, boxes, savename):\n    fp = open(savename, \'w\')\n    filename = os.path.basename(savename)\n    filename = os.path.splitext(filename)[0]\n    fp.write(\'# imagepath = %s\\n\' % imgfile)\n    fp.write(\'# basename = %s\\n\' % filename)\n    fp.write(\'# nbbs = %d\\n\' % len(boxes))\n    width = img.width\n    height = img.height\n    # box[0], box[1] : center x, center y\n    # box[2], box[3] : width, height\n    # box[4] : confidence\n    # box[5] : max confidence of the class\n    # box[6] : max class id\n    for box in boxes:\n        x1 = (box[0] - box[2]/2.0) * width\n        y1 = (box[1] - box[3]/2.0) * height\n        x2 = (box[0] + box[2]/2.0) * width\n        y2 = (box[1] + box[3]/2.0) * height\n\n        det_conf = box[4]\n        for j in range((len(box)-5)//2):\n            cls_conf = box[5+2*j]\n            cls_id = box[6+2*j]\n            prob = det_conf * cls_conf\n            fp.write(\'%d %f %f %f %f %f\\n\' % (cls_id, prob, x1, y1, x2, y2 ))\n    fp.close()\n\ndef get_det_image_name(imagefile):\n    file, ext = os.path.splitext(imagefile)\n    imgname = file + ""_det"" + ext\n    return imgname\n\ndef get_det_result_name(imagefile):\n    return imagefile.replace(\'images\', \'results\').replace(\'JPEGImages\', \'results\').replace(\'.jpg\', \'.det\').replace(\'.png\',\'.det\')\n\ndef get_image_xml_name(imagefile):\n    return imagefile.replace(\'images\', \'Annotations\').replace(\'JPEGImages\', \'Annotations\').replace(\'.jpg\', \'.xml\').replace(\'.png\',\'.xml\')\n\ndef eval_list(cfgfile, namefile, weightfile, testfile):\n    m = Darknet(cfgfile)\n    m.load_weights(weightfile)\n    use_cuda = 1\n    if use_cuda:\n        m.cuda()\n\n    class_names = load_class_names(namefile)\n\n    file_list = []\n    with open(testfile, ""r"") as fin:\n        for f in fin:\n            file_list.append(f.strip())\n\n    for imgfile in file_list:\n        img = Image.open(imgfile).convert(\'RGB\')\n        sized = img.resize((m.width, m.height))\n        filename = os.path.basename(imgfile)\n        filename = os.path.splitext(filename)[0]\n        #print(filename, img.width, img.height, sized_width, sized_height)\n\n        if m.width * m.height > 1024 * 2560:\n            print(\'omit %s\' % filename)\n            continue\n\n        if False:\n            boxes = do_detect(m, sized, conf_thresh, nms_thresh, use_cuda)\n        else:\n            m.eval()\n            sized = image2torch(sized).cuda();\n            #output = m(Variable(sized, volatile=True)).data\n            output = m(sized)\n            #boxes = get_region_boxes(output, conf_thresh, m.num_classes, m.anchors, m.num_anchors, 0, 1)[0]\n            boxes = get_all_boxes(output, conf_thresh, m.num_classes)[0]\n            boxes = np.array(nms(boxes, nms_thresh))\n\n        if False:\n            savename = get_det_image_name(imgfile)\n            print(\'img: save to %s\' % savename)\n            plot_boxes(img, boxes, savename, class_names)\n\n        if False:\n            savename = get_det_result_name(imgfile)\n            print(\'det: save to %s\' % savename)\n            save_boxes(imgfile, img, boxes, savename)\n\nif __name__ == \'__main__\':\n    savedir = None\n    if len(sys.argv) == 5:\n        cfgfile = sys.argv[1]\n        namefile = sys.argv[2]\n        wgtfile = sys.argv[3]\n        testlist = sys.argv[4]\n\n        eval_list (cfgfile, namefile, wgtfile, testlist)\n    else:\n        print(""Usage: %s cfgfile classname weight testlist"" % sys.argv[0] )\n\n'"
scripts/eval_ap.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport xml.etree.ElementTree as ET\nimport os,sys\n#import cPickle\nimport _pickle as cPickle\nimport numpy as np\nfrom scripts.eval_all import get_det_result_name, get_image_xml_name\nfrom utils import load_class_names\n\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                              int(bbox.find(\'ymin\').text),\n                              int(bbox.find(\'xmax\').text),\n                              int(bbox.find(\'ymax\').text)]\n        objects.append(obj_struct)\n\n    return objects\n\ndef eval_ap(rec, prec):\n    """""" ap = eval_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    """"""\n\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\ndef get_recs_from_cache(imagenames, cachedir, cachename):\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, cachename)\n\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(get_image_xml_name(imagename))\n            #if i % 100 == 0:\n            #    print (\'Reading annotation for {:d}/{:d}\'.format(\n            #        i + 1, len(imagenames)))\n        # save\n        # print (\'Saving cached annotations to {:s}\'.format(cachefile))\n        with open(cachefile, \'wb\') as f:\n            cPickle.dump(recs, f)\n    else:\n        # load\n        # print (\'loaded cached annotations from {:s}\'.format(cachefile))\n        with open(cachefile, \'rb\') as f:\n            recs = cPickle.load(f)\n        try:\n            for imagename in imagenames:\n                recs[imagename]\n        except Exception as e:\n            print(""Exception: {0}"".format(e))\n            print (\'\\t{:s} is corrupted. retry!!\'.format(cachefile))\n            os.remove(cachefile)\n            recs = get_recs_from_cache(imagenames, cachedir, cachename)\n    return recs\n\ndef get_class_det_result(detpath, classname):\n    lines = []\n    cls = classes.index(classname)\n    imagename = None\n    with open(detpath, \'r\') as f:\n        lines = f.readlines()\n    splitlines = [x.strip().split(\' \') for x in lines]\n    lines = []\n    for i, l in enumerate(splitlines):\n            if l[0] == \'#\' and l[2] == \'=\': \n                if l[1] == \'imagepath\':\n                    imagename = l[3]\n            elif l[0] != \'\' and l[0] != \'#\':\n                if int(l[0]) == cls:\n                    lines.append([imagename] +  l[1:])\n    assert(imagename is not None)\n    #print(""{:s} {:s} {:d}"".format(detpath, classname, len(lines)))\n    return lines\n\ndef get_class_detection(imagenames, classname ):\n    # load annots\n    classlines = []\n    for i, imagename in enumerate(imagenames):\n        det = get_det_result_name(imagename)\n        lines = get_class_det_result(det, classname)\n        classlines.extend(lines)\n\n    #print(classlines)\n    ids = [x[0] for x in classlines]\n    conf = np.array([float(x[1])for x in classlines])\n    bb = np.array([[float(z)for z in x[2:]] for x in classlines])\n\n    #print(ids)\n    #print(bb)\n    #print(conf)\n\n    return ids, conf, bb\n\ndef eval(imagelist, classname, cachedir, ovthresh=0.5):\n    """"""rec, prec, ap = eval(imagelist, classname, [ovthresh])\n                                \n    Top level function that does the PASCAL VOC evaluation.\n\n    imagelist: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    """"""\n    # read list of images\n    with open(imagelist, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n\n    # cachedir caches the annotations in a pickle file\n    recs = get_recs_from_cache(imagenames, cachedir, \'annots.pk\')\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    image_ids, confidence, BB = \\\n        get_class_detection(imagenames, classname )\n\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    #print(image_ids)\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n    # go down dets and mark TPs and FPs\n    nd = len(image_ids)\n    tp = np.zeros(nd)\n    fp = np.zeros(nd)\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        #print(""%s (%s) "" % (image_ids[d],classname), end=\'\')\n        #print(R)\n        bb = BB[d, :].astype(float)\n        ovmax = -np.inf\n        BBGT = R[\'bbox\'].astype(float)\n\n        if BBGT.size > 0:\n            # compute overlaps\n            # intersection\n            ixmin = np.maximum(BBGT[:, 0], bb[0])\n            iymin = np.maximum(BBGT[:, 1], bb[1])\n            ixmax = np.minimum(BBGT[:, 2], bb[2])\n            iymax = np.minimum(BBGT[:, 3], bb[3])\n            iw = np.maximum(ixmax - ixmin + 1., 0.)\n            ih = np.maximum(iymax - iymin + 1., 0.)\n            inters = iw * ih\n\n            # union\n            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n            overlaps = inters / uni\n            ovmax = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n\n        if ovmax > ovthresh:\n            if not R[\'difficult\'][jmax]:\n                if not R[\'det\'][jmax]:\n                    tp[d] = 1.\n                    R[\'det\'][jmax] = 1\n                else:\n                    fp[d] = 1.\n        else:\n            fp[d] = 1.\n\n    # compute precision recall\n    fp = np.cumsum(fp)\n    tp = np.cumsum(tp)\n    rec = tp / float(npos)\n    # avoid divide by zero in case the first detection matches a difficult\n    # ground truth\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = eval_ap(rec, prec)\n\n    return rec, prec, ap\n    \n\ndef _do_python_eval(testlist, namelist, output_dir = \'output\'):\n\n    cachedir = os.path.join(output_dir, \'annotations_cache\')\n    aps = []\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n\n    global classes\n    classes = load_class_names(namelist)\n\n    for i, cls in enumerate(classes):\n        rec, prec, ap = eval(testlist, cls, cachedir, ovthresh=0.5)\n        aps += [ap]\n        print(\'AP for {} = {:.4f}\'.format(cls, ap))\n        with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n            cPickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n\n    print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n    print(\'~~~~~~~~~~~~~\')\n    print(\'   Results:\')\n    print(\'-------------\')\n    for i, ap in enumerate(aps):\n        print(\'{:<10s}\\t{:.3f}\'.format(classes[i], ap))\n    print(\'=============\')\n    print(\'{:^10s}\\t{:.3f}\'.format(\'Average\', np.mean(aps)))\n    print(\'~~~~~~~~~~~~~\')\n    print(\'\')\n    print(\'--------------------------------------------------------------\')\n    print(\'Results computed with the **unofficial** Python eval code.\')\n    print(\'Results should be very close to the official MATLAB eval code.\')\n    print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n    print(\'-- Thanks, The Management\')\n    print(\'--------------------------------------------------------------\')\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) == 3:\n        testlist = sys.argv[1]\n        namelist = sys.argv[2]\n        _do_python_eval(testlist, namelist, output_dir = \'output\')\n    else:\n        print(""Usage: %s testlist namelist"" % sys.argv[0] )\n        \n\n'"
scripts/eval_widerface.py,0,"b'import os\nimport os.path\nfrom PIL import Image\nimport sys\nfrom darknet import Darknet\nfrom utils import do_detect, plot_boxes, load_class_names\n\ndef save_boxes(img, boxes, savename):\n    fp = open(savename, \'w\')\n    filename = os.path.basename(savename)\n    filename = os.path.splitext(filename)[0]\n    fp.write(\'%s\\n\' % filename)\n    fp.write(\'%d\\n\' % len(boxes))\n    width = img.width\n    height = img.height\n    for box in boxes:\n        x1 = round((box[0] - box[2]/2.0) * width)\n        y1 = round((box[1] - box[3]/2.0) * height)\n        x2 = round((box[0] + box[2]/2.0) * width)\n        y2 = round((box[1] + box[3]/2.0) * height)\n        w = x2 - x1\n        h = y2 - y1\n        conf = box[4]\n        fp.write(\'%d %d %d %d %f\\n\' % (x1, y1, w, h, conf))\n    fp.close()\n\ndef eval_widerface(cfgfile, weightfile, valdir, savedir):\n    m = Darknet(cfgfile)\n    m.load_weights(weightfile)\n    use_cuda = 1\n    if use_cuda:\n        m.cuda()\n\n    scale_size = 16\n    class_names = load_class_names(\'data/names\')\n    for parent,dirnames,filenames in os.walk(valdir):\n        if parent != valdir:\n            targetdir = os.path.join(savedir, os.path.basename(parent))\n            if not os.path.isdir(targetdir):\n                os.mkdir(targetdir)\n            for filename in filenames:\n                imgfile = os.path.join(parent,filename)\n                img = Image.open(imgfile).convert(\'RGB\')\n                sized_width = int(round(img.width*1.0/scale_size) * 16)\n                sized_height = int(round(img.height*1.0/scale_size) * 16)\n                sized = img.resize((sized_width, sized_height))\n                print(filename, img.width, img.height, sized_width, sized_height)\n                if sized_width * sized_height > 1024 * 2560:\n                    print(\'omit %s\' % filename)\n                    continue\n                boxes = do_detect(m, sized, 0.05, 0.4, use_cuda)\n                if True:\n                    savename = os.path.join(targetdir, filename)\n                    print(\'save to %s\' % savename)\n                    plot_boxes(img, boxes, savename, class_names)\n                if True:\n                    savename = os.path.join(targetdir, os.path.splitext(filename)[0]+"".txt"")\n                    print(\'save to %s\' % savename)\n                    save_boxes(img, boxes, savename)\n\nif __name__ == \'__main__\':\n    #eval_widerface(\'resnet50_test.cfg\', \'resnet50_98000.weights\', \'widerface/WIDER_val/images/\', \'widerface/wider_val_pred/\')\n    #eval_widerface(\'resnet50_test.cfg\', \'resnet50_148000.weights\', \'widerface/WIDER_val/images/\', \'widerface/wider_val_pred/\')\n    eval_widerface(\'resnet50_x32_test.cfg\', \'resnet50_x32_288000.weights\', \'widerface/WIDER_val/images/\', \'widerface/wider_val_pred/\')\n\n'"
scripts/my_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport os,sys\n#import cPickle\nimport _pickle as cPickle\nimport numpy as np\nfrom scripts.eval_ap import parse_rec\nfrom scripts.eval_all import get_image_xml_name\nfrom utils import load_class_names\n\n\ndef compute_ap(rec, prec, use_07_metric=False):\n    """""" ap = compute_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\ndef my_eval(detpath, imagesetfile, classname, cachedir,\n             ovthresh=0.5, use_07_metric=False):\n    """"""rec, prec, ap = my_eval(detpath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """"""\n    # assumes detections are in detpath.format(classname)\n    # assumes annotations are in annopath.format(imagename)\n    # assumes imagesetfile is a text file with each line an image name\n    # cachedir caches the annotations in a pickle file\n\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            imagekey = os.path.basename(imagename).split(\'.\')[0]    \n            recs[imagekey] = parse_rec(get_image_xml_name(imagename))\n            if i % 100 == 0:\n                print (\'Reading annotation for {:d}/{:d}\'.format(\n                    i + 1, len(imagenames)))\n        # save\n        print (\'Saving cached annotations to {:s}\'.format(cachefile))\n        with open(cachefile, \'wb\') as f:\n            cPickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'rb\') as f:\n            recs = cPickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        imagekey = os.path.basename(imagename).split(\'.\')[0]\n        try:\n            R = [obj for obj in recs[imagekey] if obj[\'name\'] == classname]\n        except:\n            print(""%s %s"" % (imagename, imagekey))\n            exit(0)\n\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagekey] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n\n    splitlines = [x.strip().split(\' \') for x in lines]\n    image_ids = [x[0] for x in splitlines]\n    confidence = np.array([float(x[1]) for x in splitlines])\n    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    if len(sorted_ind) > 0:\n        BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n    # go down dets and mark TPs and FPs\n    nd = len(image_ids)\n    tp = np.zeros(nd)\n    fp = np.zeros(nd)\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        bb = BB[d, :].astype(float)\n        ovmax = -np.inf\n        BBGT = R[\'bbox\'].astype(float)\n\n        if BBGT.size > 0:\n            # compute overlaps\n            # intersection\n            ixmin = np.maximum(BBGT[:, 0], bb[0])\n            iymin = np.maximum(BBGT[:, 1], bb[1])\n            ixmax = np.minimum(BBGT[:, 2], bb[2])\n            iymax = np.minimum(BBGT[:, 3], bb[3])\n            iw = np.maximum(ixmax - ixmin + 1., 0.)\n            ih = np.maximum(iymax - iymin + 1., 0.)\n            inters = iw * ih\n\n            # union\n            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n            overlaps = inters / uni\n            ovmax = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n\n        if ovmax > ovthresh:\n            if not R[\'difficult\'][jmax]:\n                if not R[\'det\'][jmax]:\n                    tp[d] = 1.\n                    R[\'det\'][jmax] = 1\n                else:\n                    fp[d] = 1.\n        else:\n            fp[d] = 1.\n\n    # compute precision recall\n    fp = np.cumsum(fp)\n    tp = np.cumsum(tp)\n    rec = tp / float(npos)\n    # avoid divide by zero in case the first detection matches a difficult\n    # ground truth\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = compute_ap(rec, prec, use_07_metric)\n\n    #print(\'class: {:<10s} \\t num occurrence: {:4d}\'.format(classname, npos))\n\n    return rec, prec, ap, npos\n    \n\n\ndef _do_python_eval(res_prefix, imagesetfile, classesfile, output_dir = \'output\'):\n    \n    filename = res_prefix + \'{:s}.txt\'\n\n    cachedir = os.path.join(output_dir, \'annotations_cache\')\n    aps = []\n    # The PASCAL VOC metric changed in 2010\n    use_07_metric = False\n    #print (\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n        \n    _classes = load_class_names(classesfile)\n    total = 0\n    for i, cls in enumerate(_classes):\n        if cls == \'__background__\':\n            continue\n      \n        rec, prec, ap, noccur = my_eval(\n            filename, imagesetfile, cls, cachedir, ovthresh=0.5,\n            use_07_metric=use_07_metric)\n        aps += [ap]\n        total += noccur\n        print(\'AP for {:<10s} = {:.4f} with {:4d} views\'.format(cls, ap, noccur))\n        with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n            cPickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n\n    print(\'Mean AP = {:.4f} with total {:4d} views\'.format(np.mean(aps), total))\n    \n    print(\'~\'*30)\n    print(\' \'*10, \'Results:\')\n    print(\'-\'*30)\n    for i, ap in enumerate(aps):\n        print(\'{:<10s}\\t{:.3f}\'.format(_classes[i], ap))\n    print(\'=\'*30)\n    print(\'{:^10s}\\t{:.3f}\'.format(\'Average\', np.mean(aps)))\n    print(\'~\'*30)\n    print(\'\')\n    print(\'--------------------------------------------------------------\')\n    print(\'Results computed with the **unofficial** Python eval code.\')\n    print(\'Results should be very close to the official MATLAB eval code.\')\n    print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n    print(\'-- Thanks, The Management\')\n    print(\'--------------------------------------------------------------\')\n\n\nif __name__ == \'__main__\':\n    #res_prefixc = \'/data/hongji/darknet/results/comp4_det_test_\' \n    #res_prefix = \'results/comp4_det_test_\'    \n    #test_file = \'data/sketch_test.txt\'\n    #class_names = \'data/sketch.names\'\n    res_prefix = sys.argv[1]\n    test_file = sys.argv[2]\n    class_names = sys.argv[3]\n    _do_python_eval(res_prefix, test_file, class_names, output_dir = \'output\')\n\n\n'"
scripts/voc_label.py,0,"b'import xml.etree.ElementTree as ET\nimport pickle\nimport os\nfrom os import listdir, getcwd\nfrom os.path import join\n\nsets=[(\'2012\', \'train\'), (\'2012\', \'val\'), (\'2007\', \'train\'), (\'2007\', \'val\'), (\'2007\', \'test\')]\n\nclasses = [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"", ""tvmonitor""]\n\n\ndef convert(size, box):\n    dw = 1./size[0]\n    dh = 1./size[1]\n    x = (box[0] + box[1])/2.0\n    y = (box[2] + box[3])/2.0\n    w = box[1] - box[0]\n    h = box[3] - box[2]\n    x = x*dw\n    w = w*dw\n    y = y*dh\n    h = h*dh\n    return (x,y,w,h)\n\ndef convert_annotation(year, image_id):\n    in_file = open(\'VOCdevkit/VOC%s/Annotations/%s.xml\'%(year, image_id))\n    out_file = open(\'VOCdevkit/VOC%s/labels/%s.txt\'%(year, image_id), \'w\')\n    tree=ET.parse(in_file)\n    root = tree.getroot()\n    size = root.find(\'size\')\n    w = int(size.find(\'width\').text)\n    h = int(size.find(\'height\').text)\n\n    for obj in root.iter(\'object\'):\n        difficult = obj.find(\'difficult\').text\n        cls = obj.find(\'name\').text\n        if cls not in classes or int(difficult) == 1:\n            continue\n        cls_id = classes.index(cls)\n        xmlbox = obj.find(\'bndbox\')\n        b = (float(xmlbox.find(\'xmin\').text), float(xmlbox.find(\'xmax\').text), float(xmlbox.find(\'ymin\').text), float(xmlbox.find(\'ymax\').text))\n        bb = convert((w,h), b)\n        out_file.write(str(cls_id) + "" "" + "" "".join([str(a) for a in bb]) + \'\\n\')\n\nwd = getcwd()\n\nfor year, image_set in sets:\n    if not os.path.exists(\'VOCdevkit/VOC%s/labels/\'%(year)):\n        os.makedirs(\'VOCdevkit/VOC%s/labels/\'%(year))\n    image_ids = open(\'VOCdevkit/VOC%s/ImageSets/Main/%s.txt\'%(year, image_set)).read().strip().split()\n    list_file = open(\'%s_%s.txt\'%(year, image_set), \'w\')\n    for image_id in image_ids:\n        list_file.write(\'%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg\\n\'%(wd, year, image_id))\n        convert_annotation(year, image_id)\n    list_file.close()\n\n'"
layers/batchnorm/bn.py,13,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\nfrom torch.autograd import Function\nimport bn_lib\n\nclass BN2dFunc(Function):\n    def __init__(self, running_mean, running_var, training, momentum, eps):\n        self.running_mean = running_mean\n        self.running_var = running_var\n        self.training = training\n        self.momentum = momentum\n        self.eps = eps\n\n    def forward(self, input, weight, bias):\n        nB = input.size(0)\n        nC = input.size(1)\n        nH = input.size(2)\n        nW = input.size(3)\n\n        output = input.new(nB, nC, nH, nW) \n        self.input = input\n        self.weight = weight\n        self.bias = bias\n        self.x = input.new(nB, nC, nH, nW) \n        self.x_norm = input.new(nB, nC, nH, nW) \n        self.mean = input.new(nB, nC) \n        self.var = input.new(nB, nC) \n\n        if input.is_cuda:\n            bn_lib.bn_forward_gpu(input, self.x, self.x_norm, self.mean, self.running_mean, self.var, self.running_var, weight, bias, self.training, output)\n        else:\n            bn_lib.bn_forward(input, self.x, self.x_norm, self.mean, self.running_mean, self.var, self.running_var, weight, bias, self.training, output)\n        return output\n\n    def backward(self, grad_output):\n        nB = grad_output.size(0)\n        nC = grad_output.size(1)\n        nH = grad_output.size(2)\n        nW = grad_output.size(3)\n        grad_input = grad_output.new(nB, nC, nH, nW) \n        grad_mean = grad_output.new(nC) \n        grad_var = grad_output.new(nC) \n        grad_weight = grad_output.new(nC) \n        grad_bias = grad_output.new(nC) \n        \n        if grad_output.is_cuda:\n            bn_lib.bn_backward_gpu(grad_output, self.input, self.x_norm, self.mean, grad_mean, self.var, grad_var, self.weight, grad_weight, self.bias, grad_bias, self.training, grad_input)\n        else:\n            bn_lib.bn_backward(grad_output, self.input, self.x_norm, self.mean, grad_mean, self.var, grad_var, self.weight, grad_weight, self.bias, grad_bias, self.training, grad_input)\n        \n        return grad_input, grad_weight, grad_bias  \n\nclass BN2d(nn.Module):\n    def __init__(self, num_features, momentum=0.01, eps=1e-5):\n        super(BN2d, self).__init__()\n        self.num_features = num_features\n        self.weight = Parameter(torch.Tensor(num_features))\n        self.bias = Parameter(torch.Tensor(num_features))\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.zeros(num_features))\n        self.momentum = momentum\n        self.eps = eps\n\n        self.running_mean.zero_()\n        self.running_var.fill_(1)\n        self.weight.data.uniform_()\n        self.bias.data.zero_()\n\n    def forward(self, input):\n        #print('------------ BN2d input -------------')\n        #print(input.data.storage()[0:10])\n        return BN2dFunc(self.running_mean, self.running_var, self.training, self.momentum, self.eps)(input, self.weight, self.bias)\n\nclass BN2d_slow(nn.Module):\n    def __init__(self, num_features, momentum=0.01):\n        super(BN2d_slow, self).__init__()\n        self.num_features = num_features\n        self.weight = Parameter(torch.Tensor(num_features))\n        self.bias = Parameter(torch.Tensor(num_features))\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.zeros(num_features))\n        self.eps = 1e-5\n        self.momentum = momentum\n\n        self.running_mean.zero_()\n        self.running_var.fill_(1)\n        self.weight.data.uniform_()\n        self.bias.data.zero_()\n    def forward(self, x): \n        nB = x.data.size(0)\n        nC = x.data.size(1)\n        nH = x.data.size(2)\n        nW = x.data.size(3)\n        samples = nB*nH*nW\n        y = x.view(nB, nC, nH*nW).transpose(1,2).contiguous().view(-1,nC)\n        if self.training:\n            print('forward in training mode on autograd')\n            m = Variable(y.mean(0).data, requires_grad=False)\n            v = Variable(y.var(0).data, requires_grad=False)\n            self.running_mean = (1-self.momentum)*self.running_mean + self.momentum * m.data.view(-1)\n            self.running_var = (1-self.momentum)*self.running_var + self.momentum * v.data.view(-1)\n            m = m.repeat(samples, 1)\n            v = v.repeat(samples, 1)*(samples-1.0)/samples\n        else:\n            m = Variable(self.running_mean.repeat(samples, 1), requires_grad=False)\n            v = Variable(self.running_var.repeat(samples, 1), requires_grad=False)\n        w = self.weight.repeat(samples, 1)\n        b = self.bias.repeat(samples, 1)\n        y = (y - m)/(v+self.eps).sqrt() * w + b \n        y = y.view(nB, nH*nW, nC).transpose(1,2).contiguous().view(nB, nC, nH, nW) \n        return y\n\n\nif __name__ == '__main__':\n    nB = 64\n    nC = 3\n    nH = 4\n    nW = 4\n    samples = nB*nH*nW\n    a = torch.rand(nB,nC,nH,nW)\n    a = Variable(a)\n    nn_model  = nn.BatchNorm2d(nC)\n    dkn_model = BN2d(nC)\n    atg_model = BN2d_slow(nC)\n\n    nn_model.weight.data.fill_(1.0)\n    nn_model.bias.data.zero_()\n    dkn_model.weight.data.fill_(1.0)\n    dkn_model.bias.data.zero_()\n    atg_model.weight.data.fill_(1.0)\n    atg_model.bias.data.zero_()\n    nn_out_cpu = nn_model(a)\n    dkn_out_cpu = dkn_model(a)\n    atg_out_cpu = atg_model(a)\n\n\n\n    a = a.cuda()\n    nn_model.cuda()\n    dkn_model.cuda()\n    atg_model.cuda()\n\n    nn_out_gpu = nn_model(a)\n    dkn_out_gpu = dkn_model(a)\n    atg_out_gpu = atg_model(a)\n\n    print('--- nn cpu out ---')\n    print(nn_out_cpu.data.storage()[0:10])\n    print('--- dkn cpu out ---')\n    print(dkn_out_cpu.data.storage()[0:10])\n    print('--- atg cpu out ---')\n    print(atg_out_cpu.data.storage()[0:10])\n\n\n    print('--- nn gpu out ---')\n    print(nn_out_gpu.data.storage()[0:10])\n    print('--- dkn gpu out ---')\n    print(dkn_out_gpu.data.storage()[0:10])\n    print('--- atg gpu out ---')\n    print(atg_out_gpu.data.storage()[0:10])\n"""
layers/batchnorm/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/batchnorm.c']\nheaders = ['src/batchnorm.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    #sources += ['src/cuda.c']\n    #headers += ['src/cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\n#extra_objects=[]\nextra_objects = ['obj/blas_kernels.o', 'obj/cuda.o', 'obj/blas.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    'bn_lib',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
tools/lmdb/create_dataset.py,0,"b'import sys\nimport os\nimport lmdb # install lmdb by ""pip install lmdb""\nimport cv2\nimport numpy as np\n\ndef checkImageIsValid(imageBin):\n    if imageBin is None:\n        return False\n    imageBuf = np.fromstring(imageBin, dtype=np.uint8)\n    img = cv2.imdecode(imageBuf, cv2.IMREAD_COLOR)\n    imgH, imgW = img.shape[0], img.shape[1]\n    if imgH * imgW == 0:\n        return False\n    return True\n\n\ndef writeCache(env, cache):\n    with env.begin(write=True) as txn:\n        for k, v in cache.iteritems():\n            txn.put(k, v)\n\n\ndef createDataset(outputPath, imageListFile, checkValid=True):\n    """"""\n    Create LMDB dataset for CRNN training.\n\n    ARGS:\n        outputPath    : LMDB output path\n        imagePathList : list of image path\n        checkValid    : if true, check the validity of every image\n    """"""\n    with open(imageListFile) as fp:\n        imagePathList = fp.readlines()\n    nSamples = len(imagePathList)\n    env = lmdb.open(outputPath, map_size=1099511627776)\n    cache = {}\n    cnt = 1\n    for i in xrange(nSamples):\n        imagePath = imagePathList[i].rstrip()\n        labelPath = imagePath.replace(\'images\', \'labels\').replace(\'JPEGImages\', \'labels\').replace(\'.jpg\', \'.txt\').replace(\'.png\',\'.txt\')\n        with open(labelPath) as f:\n            label = f.readlines()\n        label = \'\'.join(label)\n\n        if not os.path.exists(imagePath):\n            print(\'%s does not exist\' % imagePath)\n            continue\n        with open(imagePath, \'r\') as f:\n            imageBin = f.read()\n        if checkValid:\n            if not checkImageIsValid(imageBin):\n                print(\'%s is not a valid image\' % imagePath)\n                continue\n\n        imageKey = \'image-%09d\' % cnt\n        labelKey = \'label-%09d\' % cnt\n        cache[imageKey] = imageBin\n        cache[labelKey] = label\n        if cnt % 1000 == 0:\n            writeCache(env, cache)\n            cache = {}\n            print(\'Written %d / %d\' % (cnt, nSamples))\n        cnt += 1\n    nSamples = cnt-1\n    cache[\'num-samples\'] = str(nSamples)\n    writeCache(env, cache)\n    print(\'Created dataset with %d samples\' % nSamples)\n\nif __name__ == \'__main__\':\n    outputPath = sys.argv[1]\n    imageListFile = sys.argv[2]\n    createDataset(outputPath, imageListFile, checkValid=True)\n'"
tools/lmdb/lmdb_utils.py,3,"b""#!/usr/bin/python\n# encoding: utf-8\n\nimport os\nimport random\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom utils import read_truths_args, read_truths\nimport cv2\nimport lmdb\n\nclass lmdbDataset(Dataset):\n\n    def __init__(self, lmdb_root, shape=None, shuffle=True, transform=None, target_transform=None, train=False, seen=0):\n        self.env = lmdb.open(lmdb_root,\n                 max_readers=1,\n                 readonly=True,\n                 lock=False,\n                 readahead=False,\n                 meminit=False)\n        self.txn = self.env.begin(write=False) \n        self.nSamples = int(self.txn.get('num-samples'))\n        self.indices = range(self.nSamples) \n        if shuffle:\n            random.shuffle(self.indices)\n \n        self.transform = transform\n        self.target_transform = target_transform\n        self.train = train\n        self.shape = shape\n        self.seen = seen\n        #if self.train:\n        #    print('init seen to %d' % (self.seen))\n\n    def __len__(self):\n        return self.nSamples\n\n    def __getitem__(self, index):\n        assert index <= len(self), 'index range error'\n        imgkey = 'image-%09d' % (self.indices[index]+1)\n        labkey = 'label-%09d' % (self.indices[index]+1)\n        label = torch.zeros(50*5)\n\n        imageBin = self.txn.get(imgkey)\n        imageBuf = np.fromstring(imageBin, dtype=np.uint8)\n        img = cv2.imdecode(imageBuf, cv2.IMREAD_COLOR)\n        if self.train and index % 64 == 0:\n            if self.seen < 4000*64*4:\n               width = (random.randint(0,2)*2 + 13)*32\n               self.shape = (width, width)\n            elif self.seen < 8000*64*4:\n               width = (random.randint(0,4)*2 + 9)*32\n               self.shape = (width, width)\n            elif self.seen < 12000*64*4:\n               width = (random.randint(0,6)*2 + 5)*32\n               self.shape = (width, width)\n            elif self.seen < 12000*64*4:\n               width = (random.randint(0,12) + 5)*32\n               self.shape = (width, width)\n            else: # self.seen < 20000*64*4:\n               width = (random.randint(0,16) + 3)*32\n               self.shape = (width, width)\n\n        if self.shape:\n            img = cv2.resize(img, self.shape, interpolation = cv2.INTER_CUBIC)\n\n        tid = 0\n        truths = self.txn.get(labkey).rstrip().split('\\n')\n        for truth in truths:\n            truth = truth.split()\n            tmp = [float(t) for t in truth]\n            if tmp[3] > 8.0/img.shape[0]:\n                label[tid*5+0] = tmp[0]\n                label[tid*5+1] = tmp[1]\n                label[tid*5+2] = tmp[2]\n                label[tid*5+3] = tmp[3]\n                label[tid*5+4] = tmp[4]\n                tid = tid + 1\n\n        width = img.shape[0]\n        height = img.shape[1]\n        img = torch.from_numpy(img)\n        img = img.view(height, width, 3).transpose(0,1).transpose(0,2).contiguous()\n        img = img.view(1, 3, height, width)\n        img = img.float().div(255.0)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n\n        self.seen = self.seen + 4\n        return (img, label)\n\ndef lmdb_nsamples(db):\n    env = lmdb.open(db,\n                max_readers=1,\n                readonly=True,\n                lock=False,\n                readahead=False,\n                meminit=False)\n\n    with env.begin(write=False) as txn:\n        nSamples = int(txn.get('num-samples'))\n    return nSamples\n\n"""
tools/lmdb/plot_lmdb.py,0,"b""import lmdb\nimport cv2\nimport numpy as np\n\nenv = lmdb.open('data/face_test_lmdb',\n                max_readers=1,\n                readonly=True,\n                lock=False,\n                readahead=False,\n                meminit=False)\n\nwith env.begin(write=False) as txn:\n    nSamples = int(txn.get('num-samples'))\n    #print nSamples\n    for index in range(nSamples):\n        image_key = 'image-%09d' % (index+1)\n        label_key = 'label-%09d' % (index+1)\n        imageBin = txn.get(image_key)\n        imageBuf = np.fromstring(imageBin, dtype=np.uint8)\n        img = cv2.imdecode(imageBuf, cv2.IMREAD_COLOR)\n        imgH, imgW = img.shape[0], img.shape[1]\n        labels = txn.get(label_key).rstrip().split('\\n')\n        for label in labels:\n            label = label.split()\n            box = [float(i) for i in label]\n            x = box[1]*imgW\n            y = box[2]*imgH\n            w = box[3]*imgW\n            h = box[4]*imgH\n            x1 = int(x - w/2.0)\n            x2 = int(x + w/2.0)\n            y1 = int(y - h/2.0)\n            y2 = int(y + h/2.0)\n            cv2.rectangle(img, (x1,y1), (x2,y2), (255,0,0), 3)\n        savename = 'tmp/%s.png'%(image_key)\n        print('save %s' % (savename))\n        cv2.imwrite(savename, img)\n\n"""
tools/lmdb/train_lmdb.py,10,"b'from __future__ import print_function\nimport sys\nif len(sys.argv) != 4:\n    print(\'Usage:\')\n    print(\'python train.py datacfg cfgfile weightfile\')\n    exit()\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\nimport lmdb_utils\nimport random\nimport math\nfrom utils import *\nfrom cfg import parse_cfg\nfrom region_loss import RegionLoss\nfrom darknet import Darknet\n\n\n# Training settings\ndatacfg       = sys.argv[1]\ncfgfile       = sys.argv[2]\nweightfile    = sys.argv[3]\n\ndata_options  = read_data_cfg(datacfg)\nnet_options   = parse_cfg(cfgfile)[0]\n\ntraindb       = data_options[\'train\']\ntestdb        = data_options[\'valid\']\nbackupdir     = data_options[\'backup\']\nnsamples      = lmdb_utils.lmdb_nsamples(traindb)\n\nbatch_size    = int(net_options[\'batch\'])\nmax_batches   = int(net_options[\'max_batches\'])\nlearning_rate = float(net_options[\'learning_rate\'])\nmomentum      = float(net_options[\'momentum\'])\n\nmax_epochs    = max_batches*batch_size/nsamples+1\nuse_cuda      = True\nseed          = 22222\neps           = 1e-5\n\n###############\ntorch.manual_seed(seed)\nif use_cuda:\n    torch.cuda.manual_seed(seed)\n\nmodel       = Darknet(cfgfile)\nregion_loss = model.loss\n\nmodel.load_weights(weightfile)\nmodel.print_network()\ninit_epoch = model.seen / nsamples \n\nkwargs = {\'num_workers\': 8, \'pin_memory\': True} if use_cuda else {}\ntest_loader = torch.utils.data.DataLoader(\n    lmdb_utils.lmdbDataset(testdb, shape=(160, 160),\n                   shuffle=False,\n                   transform=None,\n                   train=False),\n    batch_size=batch_size, shuffle=False, **kwargs)\n\nif use_cuda:\n    model = torch.nn.DataParallel(model).cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n\ndef adjust_learning_rate(optimizer, epoch):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = learning_rate * (0.1 ** (epoch // 50))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    logging(\'set lr=%f\' % (lr))\n\ndef train(epoch):\n    train_loader = torch.utils.data.DataLoader(\n        lmdb_utils.lmdbDataset(traindb, shape=(model.module.width, model.module.height),\n                       shuffle=True,\n                       train=True, seen=model.module.seen),\n        batch_size=batch_size, shuffle=False, **kwargs)\n\n    logging(\'epoch %d : processed %d samples\' % (epoch, epoch * len(train_loader.dataset)))\n    model.train()\n    adjust_learning_rate(optimizer, epoch)\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if (batch_idx+1) % 70 == 0:\n            sys.stdout.write(\'.\')\n\n        if use_cuda:\n            data = data.cuda()\n            #target= target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = region_loss(output, target)\n        loss.backward()\n        optimizer.step()\n    print(\'\')\n    logging(\'save weights to %s/%06d.weights\' % (backupdir, epoch+1))\n    model.module.seen = (epoch + 1) * len(train_loader.dataset)\n    model.module.save_weights(\'%s/%06d.weights\' % (backupdir, epoch+1))\n\ndef test(epoch):\n    def truths_length(truths):\n        for i in range(50):\n            if truths[i][1] == 0:\n                return i\n\n    model.eval()\n    num_classes = model.module.num_classes\n    anchors     = model.module.anchors\n    num_anchors = model.module.num_anchors\n    conf_thresh = 0.25\n    nms_thresh  = 0.4\n    iou_thresh  = 0.5\n    total       = 0.0\n    proposals   = 0.0\n    correct     = 0.0\n\n    for batch_idx, (data, target) in enumerate(test_loader):\n        if use_cuda:\n            data = data.cuda()\n        data = Variable(data, volatile=True)\n        output = model(data).data\n        all_boxes = get_region_boxes(output, conf_thresh, num_classes, anchors, num_anchors)\n        if output.size(0) == 1:\n            all_boxes = [all_boxes]\n        for i in range(output.size(0)):\n            boxes = all_boxes[i]\n            boxes = nms(boxes, nms_thresh)\n            truths = target[i].view(-1, 5)\n            num_gts = truths_length(truths)\n     \n            total = total + num_gts\n    \n            for i in range(len(boxes)):\n                if boxes[i][4] > conf_thresh:\n                    proposals = proposals+1\n\n            for i in range(num_gts):\n                box_gt = [truths[i][1], truths[i][2], truths[i][3], truths[i][4], 1.0]\n                best_iou = 0\n                for j in range(len(boxes)):\n                    iou = bbox_iou(box_gt, boxes[j], x1y1x2y2=False)\n                    best_iou = max(iou, best_iou)\n                if best_iou > iou_thresh:\n                    correct = correct+1\n\n    precision = 1.0*correct/(proposals+eps)\n    recall = 1.0*correct/(total+eps)\n    fscore = 2.0*precision*recall/(precision+recall+eps)\n    logging(""precision: %f, recall: %f, fscore: %f"" % (precision, recall, fscore))\n\nevaluate = True\nif evaluate:\n    print(\'evaluating ...\')\n    test(0)\nelse:\n    for epoch in range(init_epoch, max_epochs): \n        train(epoch)\n        test(epoch)\n'"
layers/batchnorm/bn_lib/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._bn_lib import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        locals[symbol] = _wrap_function(fn, _ffi)\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
