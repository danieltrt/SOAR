file_path,api_count,code
datasets/augmentations.py,0,"b""# Adapted from https://github.com/ZijunDeng/pytorch-semantic-segmentation/blob/master/utils/joint_transforms.py\n\nimport math\nimport random\nimport numbers\nimport numpy as np\n\nfrom PIL import Image, ImageOps\n\n\nclass Compose(object):\n    def __init__(self, augmentations):\n        self.augmentations = augmentations\n\n    def __call__(self, img, mask):\n        img, mask = Image.fromarray(img, mode='RGB'), Image.fromarray(mask, mode='L')\n        assert img.size == mask.size\n\n        for a in self.augmentations:\n            img, mask = a(img, mask)\n        return np.array(img, dtype=np.uint8), np.array(mask, dtype=np.uint8)\n\n\nclass RandomCrop(object):\n    def __init__(self, size, padding=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, img, mask):\n        if self.padding > 0:\n            img = ImageOps.expand(img, border=self.padding, fill=0)\n            mask = ImageOps.expand(mask, border=self.padding, fill=0)\n\n        assert img.size == mask.size\n\n        w, h = img.size\n        th, tw = self.size\n        if w == tw and h == th:\n            return img, mask\n        if w < tw or h < th:\n            return img.resize((tw, th), Image.BILINEAR), mask.resize((tw, th), Image.NEAREST)\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n        return img.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass CenterCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        return img.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass RandomHorizontallyFlip(object):\n    def __call__(self, img, mask):\n        if random.random() < 0.5:\n            return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT)\n        return img, mask\n\n\nclass FreeScale(object):\n    def __init__(self, size):\n        self.size = tuple(reversed(size))  # size: (h, w)\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        return img.resize(self.size, Image.BILINEAR), mask.resize(self.size, Image.NEAREST)\n\n\nclass Scale(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        if (w >= h and w == self.size[1]) or (h >= w and h == self.size[0]):\n            return img, mask\n\n        oh, ow = self.size\n        return img.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)\n\n\nclass RandomSizedCrop(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        for attempt in range(10):\n            area = img.size[0] * img.size[1]\n            target_area = random.uniform(0.45, 1.0) * area\n            aspect_ratio = random.uniform(0.5, 2)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img.size[0] and h <= img.size[1]:\n                x1 = random.randint(0, img.size[0] - w)\n                y1 = random.randint(0, img.size[1] - h)\n\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                mask = mask.crop((x1, y1, x1 + w, y1 + h))\n                assert (img.size == (w, h))\n\n                return img.resize((self.size, self.size), Image.BILINEAR), mask.resize((self.size, self.size),\n                                                                                       Image.NEAREST)\n\n        # Fallback\n        scale = Scale(self.size)\n        crop = CenterCrop(self.size)\n        return crop(*scale(img, mask))\n\n\nclass RandomRotate(object):\n    def __init__(self, degree):\n        self.degree = degree\n\n    def __call__(self, img, mask):\n        rotate_degree = random.random() * 2 * self.degree - self.degree\n        return img.rotate(rotate_degree, Image.BILINEAR), mask.rotate(rotate_degree, Image.NEAREST)\n\n\nclass RandomSized(object):\n    def __init__(self, limit):\n        self.limit = limit\n\n    def __call__(self, img, mask):\n\n        scale = random.uniform(self.limit[0], self.limit[1])\n        w = int(scale * img.size[0])\n        h = int(scale * img.size[1])\n\n        img, mask = img.resize((w, h), Image.BILINEAR), mask.resize((w, h), Image.NEAREST)\n\n        return img, mask\n"""
datasets/calculate_class_weight.py,0,"b'import numpy as np\nimport os\n\nfrom scripts.utils import recursive_glob\n\n\ndef calc_median_frequency(classes, present_num):\n    """"""\n    Class balancing by median frequency balancing method.\n    Reference: https://arxiv.org/pdf/1411.4734.pdf\n       \'a = median_freq / freq(c) where freq(c) is the number of pixels\n        of class c divided by the total number of pixels in images where\n        c is present, and median_freq is the median of these frequencies.\'\n    """"""\n    class_freq = classes / present_num\n    median_freq = np.median(class_freq)\n    return median_freq / class_freq\n\n\ndef calc_log_frequency(classes, value=1.02):\n    """"""Class balancing by ERFNet method.\n       prob = each_sum_pixel / each_sum_pixel.max()\n       a = 1 / (log(1.02 + prob)).\n    """"""\n    class_freq = classes / classes.sum()  # ERFNet is max, but ERFNet is sum\n    # print(class_freq)\n    # print(np.log(value + class_freq))\n    return 1 / np.log(value + class_freq)\n\n\nif __name__ == \'__main__\':\n    import os\n    import scipy.misc as misc\n    from datasets.cityscapes_loader import CityscapesLoader\n\n    method = ""median""\n    result_path = ""/afs/cg.cs.tu-bs.de/home/zhang/SEDPShuffleNet/datasets""\n\n    traval = ""gtFine""\n    imgs_path = ""/zfs/zhang/Cityscapes/leftImg8bit/train""\n    lbls_path = ""/zfs/zhang/Cityscapes/gtFine/train""\n    images = recursive_glob(rootdir=imgs_path, suffix=\'.png\')\n\n    num_classes = 19\n\n    local_path = ""/zfs/zhang/Cityscapes""\n    dst = CityscapesLoader(local_path, gt=""gtFine"", split=""train"", is_transform=True, augmentations=None)\n\n    classes, present_num = ([0 for i in range(num_classes)] for i in range(2))\n\n    for idx, img_path in enumerate(images):\n        lbl_path = os.path.join(lbls_path, img_path.split(os.sep)[-2],\n                                os.path.basename(img_path)[:-15] + \'{}_labelIds.png\'.format(traval))\n\n        lbl = misc.imread(lbl_path)\n        lbl = dst.encode_segmap(np.array(lbl, dtype=np.uint8))\n\n        for nc in range(num_classes):\n            num_pixel = (lbl == nc).sum()\n            if num_pixel:\n                classes[nc] += num_pixel\n                present_num[nc] += 1\n\n    if 0 in classes:\n        raise Exception(""Some classes are not found"")\n\n    classes = np.array(classes, dtype=""f"")\n    presetn_num = np.array(classes, dtype=""f"")\n    if method == ""median"":\n        class_weight = calc_median_frequency(classes, present_num)\n    elif method == ""log"":\n        class_weight = calc_log_frequency(classes)\n    else:\n        raise Exception(""Please assign method to \'mean\' or \'log\'"")\n\n    print(""class weight"", class_weight)\n    result_path = os.path.join(result_path, ""{}_class_weight.npy"".format(method))\n    np.save(result_path, class_weight)\n    print(""Done!"")\n'"
datasets/cityscapes_loader.py,3,"b'import os\nimport torch\nimport scipy.misc as misc\n\nfrom torch.utils import data\nfrom datasets.augmentations import *\nfrom scripts.utils import recursive_glob\n\n\nclass CityscapesLoader(data.Dataset):\n    """"""\n    CityscapesLoader\n\n    https://www.cityscapes-dataset.com\n\n    Data is derived from CityScapes, and can be downloaded from here:\n    https://www.cityscapes-dataset.com/downloads/\n\n    Many Thanks to @fvisin for the loader repo:\n    https://github.com/fvisin/dataset_loaders/blob/master/dataset_loaders/images/cityscapes.py\n    """"""\n    colors = [  # [  0,   0,   0],\n        [128, 64, 128],\n        [244, 35, 232],\n        [70, 70, 70],\n        [102, 102, 156],\n        [190, 153, 153],\n        [153, 153, 153],\n        [250, 170, 30],\n        [220, 220, 0],\n        [107, 142, 35],\n        [152, 251, 152],\n        [0, 130, 180],\n        [220, 20, 60],\n        [255, 0, 0],\n        [0, 0, 142],\n        [0, 0, 70],\n        [0, 60, 100],\n        [0, 80, 100],\n        [0, 0, 230],\n        [119, 11, 32]]\n\n    label_colours = dict(zip(range(19), colors))\n\n    def __init__(self, root, split=""train"", gt=""gtCoarse"", img_size=(512, 1024),\n                 is_transform=False, augmentations=None):\n        """"""\n        :param root:         (str)  Path to the data sets root\n        :param split:        (str)  Data set split -- \'train\' \'train_extra\' or \'val\'\n        :param gt:           (str)  Type of ground truth label -- \'gtFine\' or \'gtCoarse\'\n        :param img_size:     (tuple or int) The size of the input image\n        :param is_transform: (bool) Transform the image or not\n        :param augmentations (object) Data augmentations used in the image and label\n        """"""\n        self.root = root\n        self.gt = gt\n        self.split = split\n        self.is_transform = is_transform\n        self.augmentations = augmentations\n\n        self.n_classes = 19\n        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n        self.mean = np.array([73.16, 82.91, 72.39])\n        self.files = {}\n\n        self.images_base = os.path.join(self.root, \'leftImg8bit\', self.split)\n        self.annotations_base = os.path.join(self.root, gt, self.split)\n\n        self.files[split] = recursive_glob(rootdir=self.images_base, suffix=\'.png\')\n\n        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n        self.class_names = [\'unlabelled\', \'road\', \'sidewalk\', \'building\', \'wall\', \'fence\',\n                            \'pole\', \'traffic_light\', \'traffic_sign\', \'vegetation\', \'terrain\',\n                            \'sky\', \'person\', \'rider\', \'car\', \'truck\', \'bus\', \'train\',\n                            \'motorcycle\', \'bicycle\']\n\n        self.ignore_index = 250\n        self.class_map = dict(zip(self.valid_classes, range(19)))\n\n        if not self.files[split]:\n            raise Exception(""> No files for split=[%s] found in %s"" % (split, self.images_base))\n\n        print(""> Found %d %s images..."" % (len(self.files[split]), split))\n\n    def __len__(self):\n        """"""__len__""""""\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        """"""__getitem__\n\n        :param index:\n        """"""\n        img_path = self.files[self.split][index].rstrip()\n        lbl_path = os.path.join(self.annotations_base,\n                                img_path.split(os.sep)[-2],\n                                os.path.basename(img_path)[:-15] + \'{}_labelIds.png\'.format(self.gt))\n\n        if not os.path.isfile(img_path) or not os.path.exists(img_path):\n            raise Exception(""{} is not a file, can not open with imread."".format(img_path))\n\n        img = misc.imread(img_path)\n        img = np.array(img, dtype=np.uint8)\n        # img = misc.imresize(img, (self.img_size[0], self.img_size[1], ""bilinear""))\n\n        if not os.path.isfile(lbl_path) or not os.path.exists(lbl_path):\n            raise Exception(""{} is not a file, can not open with imread."".format(lbl_path))\n\n        lbl = misc.imread(lbl_path)\n        # lbl = misc.imresize(lbl, (self.img_size[0], self.img_size[1]), ""nearest"", mode=\'F\')\n        lbl = self.encode_segmap(np.array(lbl, dtype=np.uint8))\n\n        if self.augmentations is not None:\n            img, lbl = self.augmentations(img, lbl)\n\n        if self.is_transform:\n            img, lbl = self.transform(img, lbl)\n\n        img = torch.from_numpy(img).float()\n        lbl = torch.from_numpy(lbl).long()\n        return img, lbl\n\n    def transform(self, img, lbl):\n        """"""transform\n\n        :param img:\n        :param lbl:\n        """"""\n        img = img[:, :, ::-1]         # From RGB to BGR\n        img = img.astype(float)\n        img -= self.mean\n        img /= 255.0\n        img = img.transpose(2, 0, 1)  # From H*W*C to C*H*W\n\n        if not np.all(np.unique(lbl[lbl != self.ignore_index]) < self.n_classes):\n            raise ValueError(""> Segmentation map contained invalid class values."")\n\n        return img, lbl\n\n    def decode_segmap(self, temp):\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = self.label_colours[l][0]\n            g[temp == l] = self.label_colours[l][1]\n            b[temp == l] = self.label_colours[l][2]\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n        rgb[:, :, 0] = r / 255.0\n        rgb[:, :, 1] = g / 255.0\n        rgb[:, :, 2] = b / 255.0\n        return rgb\n\n    def encode_segmap(self, mask):\n        # Put all void classes to zero\n        for _voidc in self.void_classes:\n            mask[mask == _voidc] = self.ignore_index\n        for _validc in self.valid_classes:\n            mask[mask == _validc] = self.class_map[_validc]\n        return mask\n\n\n# +++++++++++++++++++++++++++++++++++++++++++++ #\n# Test the code of \'CityscapesLoader\'\n# +++++++++++++++++++++++++++++++++++++++++++++ #\nif __name__ == \'__main__\':\n    net_h, net_w = 448, 896\n    augment = Compose([RandomHorizontallyFlip(), RandomSized((0.625, 0.75)),\n                       RandomRotate(6), RandomCrop((net_h, net_w))])\n\n    local_path = ""/zfs/zhang/Cityscapes""\n    dst = CityscapesLoader(local_path, split=""train_extra"", is_transform=True, augmentations=augment)\n\n    """"""\n    color_map = dst.label_colours\n    class_names = dst.class_names\n\n    grid_height = int(net_h//dst.n_classes)\n    start_pixel = int((net_h % dst.n_classes) / 2)\n\n    color_bar = np.ones((net_h, 120, 3), dtype=np.uint8)\n    for label_id in np.arange(dst.n_classes):\n        end_pixel = start_pixel + grid_height\n        color_bar[start_pixel:end_pixel, :, :] = color_map[label_id]\n\n        font = cv2.FONT_HERSHEY_TRIPLEX\n        cv2.putText(color_bar, class_names[label_id+1],\n                    (2, start_pixel + 5 + int(grid_height//2)),\n                    font, 0.55, (255, 255, 255), 1, cv2.LINE_AA)\n\n        start_pixel = end_pixel\n\n    cv2.namedWindow(""color bar"", cv2.WINDOW_NORMAL)\n    cv2.imshow(""color bar"", color_bar)\n    cv2.waitKey(0)\n    """"""\n\n    bs = 6\n    trainloader = data.DataLoader(dst, batch_size=bs, num_workers=2, shuffle=True)\n    for i, data in enumerate(trainloader):\n        print(""batch :"", i)\n        """"""\n        imgs, labels = data\n        imgs = imgs.numpy()[:, :, :, ::-1]\n        labels = labels.numpy()\n\n        cv2.namedWindow(""Image"", cv2.WINDOW_NORMAL)\n        cv2.imshow(""Image"", np.squeeze(imgs.astype(np.uint8)))\n\n        cv2.namedWindow(""Mask"", cv2.WINDOW_NORMAL)\n        cv2.imshow(""Mask"", np.squeeze(labels))\n        cv2.waitKey(0)\n        """"""'"
datasets/demo_cityscapes.py,6,"b'import os\nimport time\n\nimport cv2\nimport numpy as np\nimport scipy.misc as misc\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom models.mobilenetv2plus import MobileNetV2Plus\nfrom models.sewrnetv2 import SEWiderResNetV2\nfrom modules import InPlaceABNWrapper\nfrom functools import partial\n\n\ndef test(video_root, output_root, model_path):\n    net_h, net_w, color_bar_w = 896, 1792, 120\n    frame_size = (net_w + color_bar_w, net_h)\n    codec = cv2.VideoWriter_fourcc(*\'MJPG\')\n\n    data_path = ""/zfs/zhang/Cityscapes""\n    loader = CityscapesLoader(data_path, is_transform=True, split=\'val\',\n                              img_size=(net_h, net_w), augmentations=None)\n    n_classes = loader.n_classes\n\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Setup Color Bar\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    color_map = loader.label_colours\n    class_names = loader.class_names\n\n    grid_height = int(net_h // loader.n_classes)\n    start_pixel = int((net_h % loader.n_classes) / 2)\n\n    color_bar = np.ones((net_h, color_bar_w, 3), dtype=np.uint8)*128\n    for label_id in np.arange(loader.n_classes):\n        end_pixel = start_pixel + grid_height\n        color_bar[start_pixel:end_pixel, :, :] = color_map[label_id]\n\n        font = cv2.FONT_HERSHEY_TRIPLEX\n        cv2.putText(color_bar, class_names[label_id + 1],\n                    (2, start_pixel + 5 + int(grid_height // 2)),\n                    font, 0.55, (255, 255, 255), 1, cv2.LINE_AA)\n\n        start_pixel = end_pixel\n    color_bar = color_bar[:, :, ::-1]\n\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Model\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 1. Setting up Model..."")\n    model = MobileNetV2Plus(n_class=n_classes, in_size=(net_h, net_w), width_mult=1.0,\n                            out_sec=256, aspp_sec=(12*2, 24*2, 36*2),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    # state = convert_state_dict(torch.load(""/media/datavolume3/huijun/SEDPShuffleNet/weights/{}"".format(\n    #     args.model_path))[\'model_state\'])\n    pre_weight = torch.load(model_path)[\'model_state\']\n    model.load_state_dict(pre_weight)\n\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Inference Model\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    org_video_sub = os.listdir(video_root)\n    org_video_sub.sort()\n    prd_video_sub = os.listdir(output_root)\n    prd_video_sub.sort()\n\n    my_writer = cv2.VideoWriter(""Cityscapes_Result.avi"", codec, 24.0, frame_size)\n    for v_id in np.arange(len(org_video_sub)):\n        assert org_video_sub[v_id] == prd_video_sub[v_id]\n        print(""> 2. Processing Video # {}..."".format(v_id))\n        curr_video_path = os.path.join(video_path, org_video_sub[v_id])\n        images_name = os.listdir(curr_video_path)\n        images_name.sort()\n\n        for img_id in np.arange(len(images_name)):\n            curr_image = images_name[img_id]\n            print(""> Processing Video #{} Image: {}..."".format(v_id, curr_image))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 2.1 Pre-processing Image\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            curr_img_path = os.path.join(curr_video_path, curr_image)\n            image = misc.imread(curr_img_path)\n            image = np.array(image, dtype=np.uint8)\n\n            start_time = time.time()\n            resized_img = misc.imresize(image, (loader.img_size[0], loader.img_size[1]), interp=\'bilinear\')\n            image = misc.imresize(image, (loader.img_size[0], loader.img_size[1]), interp=\'bilinear\')\n\n            image = image[:, :, ::-1]         # RGB -> BGR\n            image = image.astype(float)\n            image -= loader.mean\n            image /= 255.0\n\n            image = image.transpose(2, 0, 1)  # HWC -> CHW\n            image = np.expand_dims(image, 0)\n            image = torch.from_numpy(image).float()\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 2.2 Prediction/Inference\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            model.eval()\n            images = Variable(image.cuda(), volatile=True)\n\n            outputs = F.softmax(model(images), dim=1)\n            pred = np.squeeze(outputs.data.max(1)[1].cpu().numpy(), axis=0)\n\n            decoded = loader.decode_segmap(pred) * 255\n            decoded = decoded.astype(np.uint8)\n            print(""> Processed Video #{} Image: {}, Time: {}s"".format(v_id, curr_image, (time.time() - start_time)))\n\n            img_msk = cv2.addWeighted(resized_img, 0.55, decoded, 0.45, 0)\n            img_msk = img_msk[:, :, ::-1]  # RGB\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 2.3 Saving prediction result\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            save_path = os.path.join(output_root, prd_video_sub[v_id], curr_image)\n            cv2.imwrite(save_path, img_msk)\n\n            # img_msk_color = np.zeros((net_h, net_w + 120, 3))\n            img_msk_color = np.concatenate((img_msk, color_bar), axis=1)\n\n            # cv2.imshow(""show"", img_msk_color)\n            # cv2.waitKey(0)\n            my_writer.write(img_msk_color)\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++ #"")\n    my_writer.release()\n\n\nif __name__ == \'__main__\':\n    video_path = ""/zfs/zhang/Cityscapes/leftImg8bit/demoVideo""\n    output_path = ""/zfs/zhang/Cityscapes/demo""\n    model_weight = ""/zfs/zhang/TrainLog/weights/{}"".format(""cityscapes_mobilenetv2_best_model.pkl"")\n    test(video_path, output_path, model_weight)\n'"
datasets/demo_mvd.py,0,"b'""""""\r\n    This file shows how to load and use the dataset\r\n""""""\r\n\r\nimport json\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom PIL import Image\r\n\r\n\r\ndef apply_color_map(image_array, labels):\r\n    color_array = np.zeros((image_array.shape[0], image_array.shape[1], 3), dtype=np.uint8)\r\n\r\n    for label_id, label in enumerate(labels):\r\n        # set all pixels with the current label to the color of the current label\r\n        color_array[image_array == label_id] = label[""color""]\r\n\r\n    return color_array\r\n\r\n\r\ndef main(mvd_dir):\r\n    # a nice example\r\n    key = \'M2kh294N9c72sICO990Uew\'\r\n\r\n    # read in config file\r\n    with open(\'config.json\') as config_file:\r\n        config = json.load(config_file)\r\n    # in this example we are only interested in the labels\r\n    labels = config[\'labels\']\r\n\r\n    # print labels\r\n    print(""There are {} labels in the config file"".format(len(labels)))\r\n    for label_id, label in enumerate(labels):\r\n        print(""{:>30} ({:2d}): {:<50} has instances: {}"".format(label[""readable""], label_id, label[""name""], label[""instances""]))\r\n\r\n    # set up paths for every image\r\n    image_path = ""{}training/images/{}.jpg"".format(mvd_dir, key)\r\n    label_path = ""{}training/labels/{}.png"".format(mvd_dir, key)\r\n    instance_path = ""{}training/instances/{}.png"".format(mvd_dir, key)\r\n\r\n    # load images\r\n    base_image = Image.open(image_path)\r\n    label_image = Image.open(label_path)\r\n    instance_image = Image.open(instance_path)\r\n\r\n    # convert labeled data to numpy arrays for better handling\r\n    label_array = np.array(label_image)\r\n    instance_array = np.array(instance_image, dtype=np.uint16)\r\n\r\n    # now we split the instance_array into labels and instance ids\r\n    instance_label_array = np.array(instance_array / 256, dtype=np.uint8)\r\n    instance_ids_array = np.array(instance_array % 256, dtype=np.uint8)\r\n\r\n    # for visualization, we apply the colors stored in the config\r\n    colored_label_array = apply_color_map(label_array, labels)\r\n    colored_instance_label_array = apply_color_map(instance_label_array, labels)\r\n\r\n    # plot the result\r\n    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20,15))\r\n\r\n    ax[0][0].imshow(base_image)\r\n    ax[0][0].get_xaxis().set_visible(False)\r\n    ax[0][0].get_yaxis().set_visible(False)\r\n    ax[0][0].set_title(""Base image"")\r\n    ax[0][1].imshow(colored_label_array)\r\n    ax[0][1].get_xaxis().set_visible(False)\r\n    ax[0][1].get_yaxis().set_visible(False)\r\n    ax[0][1].set_title(""Labels"")\r\n    ax[1][0].imshow(instance_ids_array)\r\n    ax[1][0].get_xaxis().set_visible(False)\r\n    ax[1][0].get_yaxis().set_visible(False)\r\n    ax[1][0].set_title(""Instance IDs"")\r\n    ax[1][1].imshow(colored_instance_label_array)\r\n    ax[1][1].get_xaxis().set_visible(False)\r\n    ax[1][1].get_yaxis().set_visible(False)\r\n    ax[1][1].set_title(""Labels from instance file (identical to labels above)"")\r\n\r\n    fig.savefig(\'MVD_plot.png\')\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    mvd_root = ""/zfs/zhang/MVD/""\r\n    main(mvd_root)\r\n'"
datasets/evaluation_cityscapes.py,7,"b'import time\nimport os\n\nimport torch.nn.functional as F\nimport scipy.misc as misc\nimport numpy as np\nimport torch\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom models.rfmobilenetv2plus import RFMobileNetV2Plus\nfrom models.mobilenetv2plus import MobileNetV2Plus\nfrom models.sewrnetv2 import SEWiderResNetV2\nfrom modules import InPlaceABNWrapper\nfrom torch.autograd import Variable\nfrom functools import partial\n\n\ndef evaluate_valset(data_root, model_path, result_path, traval):\n    train_id2label_id = {0: 7,\n                         1: 8,\n                         2: 11,\n                         3: 12,\n                         4: 13,\n                         5: 17,\n                         6: 19,\n                         7: 20,\n                         8: 21,\n                         9: 22,\n                         10: 23,\n                         11: 24,\n                         12: 25,\n                         13: 26,\n                         14: 27,\n                         15: 28,\n                         16: 31,\n                         17: 32,\n                         18: 33}\n\n    net_h, net_w = 896, 1792\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup DataLoader\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'val\',\n                              img_size=(net_h, net_w), augmentations=None)\n    n_classes = loader.n_classes\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n    model = RFMobileNetV2Plus(n_class=n_classes, in_size=(net_h, net_w), width_mult=1.0,\n                            out_sec=256, aspp_sec=(12*2, 24*2, 36*2),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    # state = convert_state_dict(torch.load(""/media/datavolume3/huijun/SEDPShuffleNet/weights/{}"".format(\n    #     args.model_path))[\'model_state\'])\n    pre_weight = torch.load(model_path)\n    pre_weight = pre_weight[\'model_state\']\n    model.load_state_dict(pre_weight)\n    del pre_weight\n\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Inference Model\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    vali_root = ""/zfs/zhang/Cityscapes/leftImg8bit/{}"".format(traval)\n    org_vali_sub = os.listdir(vali_root)\n    org_vali_sub.sort()\n\n    for v_id in np.arange(len(org_vali_sub)):\n        print(""> 2. Processing City # {}..."".format(org_vali_sub[v_id]))\n        curr_city_path = os.path.join(vali_root, org_vali_sub[v_id])\n        images_name = os.listdir(curr_city_path)\n        images_name.sort()\n\n        for img_id in np.arange(len(images_name)):\n            curr_image = images_name[img_id]\n            print(""> Processing City #{} Image: {}..."".format(org_vali_sub[v_id], curr_image))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 2.1 Pre-processing Image\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            curr_img_path = os.path.join(curr_city_path, curr_image)\n            image = misc.imread(curr_img_path)\n            image = np.array(image, dtype=np.uint8)\n            image = misc.imresize(image, (loader.img_size[0], loader.img_size[1]))\n\n            image = image[:, :, ::-1]         # From RGB to BGR\n            image = image.astype(float)\n            image -= loader.mean\n            image /= 255.0\n            image = image.transpose(2, 0, 1)  # From H*W*C to C*H*W\n\n            image = np.expand_dims(image, 0)\n            image = torch.from_numpy(image).float()\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 2.2 Prediction/Inference\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            model.eval()\n\n            start_time = time.time()\n            images = Variable(image.cuda(), volatile=True)\n            outputs = F.softmax(model(images), dim=1)\n            pred = outputs.data.max(1)[1]\n            print(""> Inference Time: {}s"".format(time.time() - start_time))\n            pred = np.squeeze(pred.cpu().numpy(), axis=0)\n\n            fun_classes = np.unique(pred)\n            print(\'> {} Classes found before resize: {}\'.format(len(fun_classes), fun_classes))\n            pred = pred.astype(np.uint8)\n            pred = misc.imresize(pred, (1024, 2048), ""nearest"")\n\n            fun_classes = np.unique(pred)\n            print(\'> {} Classes found after resize: {}\'.format(len(fun_classes), fun_classes))\n\n            mapper = lambda t: train_id2label_id[t]\n            vfunc = np.vectorize(mapper)\n            pred = vfunc(pred)\n\n            fun_classes = np.unique(pred)\n            print(\'> {} Classes found after resize: {}\'.format(len(fun_classes), fun_classes))\n            print(""> Processed City #{} Image: {}, Time: {}s"".format(org_vali_sub[v_id], curr_image,\n                                                                     (time.time() - start_time)))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 2.3 Saving prediction result\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            save_path = os.path.join(result_path, org_vali_sub[v_id])\n            if not os.path.exists(save_path):\n                os.makedirs(save_path, exist_ok=True)\n\n            # cv2.namedWindow(""Prediction"", cv2.WINDOW_NORMAL)\n            # cv2.imshow(""Prediction"", pred)\n            # cv2.waitKey(0)\n            pred = pred.astype(np.uint8)\n            save_name = os.path.basename(curr_image)[:-15] + \'pred_labelIds.png\'\n            save_path = os.path.join(save_path, save_name)\n            misc.imsave(save_path, pred)\n            # cv2.imwrite(save_path, pred)\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1""\n    traval = ""test""\n\n    data_path = ""/zfs/zhang/Cityscapes""\n    result_path = ""/zfs/zhang/Cityscapes/results/{}"".format(traval)\n    model_weight = ""/zfs/zhang/TrainLog/weights/cityscapes_rfmobilenetv2_best_model.pkl""\n    evaluate_valset(data_path, model_weight, result_path, traval)\n'"
datasets/gan_augmention.py,0,"b'import os\nimport imageio\nfrom shutil import copyfile\n\n\nif __name__ == ""__main__"":\n    root_dir = ""/afs/cg.cs.tu-bs.de/home/zhang/PycharmProjects/pix2pixHD/results/label2city_1024p/test_latest""\n    gan_dir = os.path.join(root_dir, ""images"")\n    gan_label_dir = ""/afs/cg.cs.tu-bs.de/home/zhang/PycharmProjects/pix2pixHD/datasets/cityscapes/test_label""\n\n    image_dir = os.path.join(root_dir, ""leftImg8bit"", ""gan"")\n    label_dir = os.path.join(root_dir, ""gtFine"", ""gan"")\n\n    all_files = os.listdir(gan_dir)\n    all_files.sort()\n\n    synthesized_images = list(filter(lambda x: \'_synthesized_image.jpg\' in x, all_files))\n    labels = os.listdir(gan_label_dir)\n    labels.sort()\n\n    for index, names in enumerate(zip(synthesized_images, labels)):\n        image_name, label_name = names[0], names[1]\n\n        image = imageio.imread(os.path.join(gan_dir, image_name))\n        image_name = image_name.replace(""_gtFine_labelIds_synthesized_image.jpg"", ""_leftImg8bit.png"")\n\n        name_split = image_name.split(""_"")\n        city_name = str(name_split[0])\n        iid1 = str(name_split[1])\n        iid2 = str(name_split[2])\n        new_iid2 = ""{}"".format(str(index).zfill(6))\n\n        image_name = image_name.replace(city_name, ""gan"")\n        image_name = image_name.replace(iid1, ""000000"")\n        image_name = image_name.replace(iid2, new_iid2)\n\n        print(""> Processing Image: {}"".format(image_name))\n        imageio.imwrite(os.path.join(image_dir, image_name), image)\n\n        new_label_name = label_name.replace(city_name, ""gan"")\n        new_label_name = new_label_name.replace(iid1, ""000000"")\n        new_label_name = new_label_name.replace(iid2, new_iid2)\n\n        copyfile(os.path.join(gan_label_dir, label_name), os.path.join(label_dir, new_label_name))\n\n        if (os.path.basename(image_name)[:-15] + \'gtFine_labelIds.png\') != new_label_name:\n            raise Exception(""> Name mismatch !!!"")\n\n    print(""> ++++++++++++++++++++++++++++++++++++++++++++ <"")\n    print(""> Done!!!"")\n    print(""> ++++++++++++++++++++++++++++++++++++++++++++ <"")\n'"
datasets/mapillary_vistas_loader.py,3,"b'import torch\nimport json\nimport os\n\nfrom scripts.utils import recursive_glob\nfrom datasets.augmentations import *\nfrom torch.utils import data\nfrom PIL import Image\n\n\nclass MapillaryVistasLoader(data.Dataset):\n    """"""\n    MapillaryVistasLoader\n    https://www.mapillary.com/dataset/vistas\n    https://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html\n\n    Data is derived from Mapillary, and can be downloaded from here (need request):\n    https://www.mapillary.com/dataset/vistas\n    """"""\n\n    def __init__(self, root, split=""training"", img_size=(640, 1280), is_transform=True, augmentations=None):\n        """"""\n        :param root:         (str)  Path to the data sets root\n        :param split:        (str)  Data set split -- \'training\' or \'validation\'\n        :param img_size:     (tuple or int) The size of the input image\n        :param is_transform: (bool) Transform the image or not\n        :param augmentations (object) Data augmentations used in the image and label\n        """"""\n        self.root = root\n        self.split = split\n        self.is_transform = is_transform\n        self.augmentations = augmentations\n        self.n_classes = 65\n\n        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n        self.mean = np.array([80.5423, 91.3162, 81.4312])\n        self.files = {}\n\n        self.images_base = os.path.join(self.root, self.split, \'images\')\n        self.annotations_base = os.path.join(self.root, self.split, \'labels\')\n\n        self.files[split] = recursive_glob(rootdir=self.images_base, suffix=\'.jpg\')\n\n        self.class_ids, self.class_names, self.class_colors = self._parse_config()\n\n        self.ignore_id = 65\n\n        if not self.files[split]:\n            raise Exception(""> No files for split=[%s] found in %s"" % (split, self.images_base))\n\n        print(""> Found %d %s images..."" % (len(self.files[split]), split))\n\n    @staticmethod\n    def _parse_config():\n        # read in config file\n        with open(\'/afs/cg.cs.tu-bs.de/home/zhang/SEDPShuffleNet/datasets/config.json\') as config_file:\n            config = json.load(config_file)\n\n        labels = config[\'labels\']\n\n        class_names = []\n        class_ids = []\n        class_colors = []\n        print(""> There are {} labels in the config file"".format(len(labels)))\n        for label_id, label in enumerate(labels):\n            class_names.append(label[""readable""])\n            class_ids.append(label_id)\n            class_colors.append(label[""color""])\n\n        return class_names, class_ids, class_colors\n\n    def __len__(self):\n        """"""__len__""""""\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        """"""__getitem__\n\n        :param index:\n        """"""\n        img_path = self.files[self.split][index].rstrip()\n        lbl_path = os.path.join(self.annotations_base, os.path.basename(img_path).replace("".jpg"", "".png""))\n\n        if not os.path.isfile(img_path) or not os.path.exists(img_path):\n            raise Exception(""{} is not a file, can not open with imread."".format(img_path))\n\n        img = Image.open(img_path)\n        img = np.array(img, dtype=np.uint8)\n        # print(""> Image size: {}, {}"".format(img.shape[0], img.shape[1]))\n\n        if not os.path.isfile(lbl_path) or not os.path.exists(lbl_path):\n            raise Exception(""{} is not a file, can not open with imread."".format(lbl_path))\n\n        lbl = Image.open(lbl_path)\n        lbl = np.array(lbl, dtype=np.uint8)\n\n        # classes = np.unique(lbl)\n        # print(""> Number of classes before resize: {}"".format(classes))\n        # lbl = misc.imresize(lbl, (self.img_size[0], self.img_size[1]), ""nearest"", mode=\'F\')\n        # print(""> Number of classes after resize: {}"".format(classes))\n        # if not np.all(classes == np.unique(lbl)):\n        #     print(""> !!!!!!!!!!!!!!!! WARN: resizing labels yielded fewer classes. !!!!!!!!!!!!!!!!!!!!"")\n\n        if self.augmentations is not None:\n            img, lbl = self.augmentations(img, lbl)\n\n        if self.is_transform:\n            img, lbl = self.transform(img, lbl)\n\n        img = torch.from_numpy(img).float()\n        lbl = torch.from_numpy(lbl).long()\n        return img, lbl\n\n    def transform(self, img, lbl):\n        """"""transform\n\n        :param img:\n        :param lbl:\n        """"""\n        img = img[:, :, ::-1]         # From RGB to BGR\n        img = img.astype(float)\n        img -= self.mean\n        img /= 255.0\n        img = img.transpose(2, 0, 1)  # From HWC to CHW\n\n        return img, lbl\n\n    def apply_color_map(self, image_array):\n        color_array = np.zeros((image_array.shape[0], image_array.shape[1], 3), dtype=np.uint8)\n\n        for cls_id, color in enumerate(self.class_colors):\n            # set all pixels with the current label to the color of the current label\n            color_array[image_array == cls_id] = color\n\n        return color_array\n\n\n# +++++++++++++++++++++++++++++++++++++++++++++ #\n# Test the code of \'MapillaryVistasLoader\'\n# +++++++++++++++++++++++++++++++++++++++++++++ #\nif __name__ == \'__main__\':\n    net_h, net_w = 448, 896\n    augment = Compose([RandomHorizontallyFlip(), RandomSized((0.625, 0.75)),\n                       RandomRotate(6), RandomCrop((net_h, net_w))])\n\n    local_path = \'/zfs/zhang/MVD\'\n    dst = MapillaryVistasLoader(local_path, img_size=(net_h, net_w), is_transform=True, augmentations=augment)\n    bs = 8\n    trainloader = data.DataLoader(dst, batch_size=bs, num_workers=4, shuffle=True)\n    for i, data in enumerate(trainloader):\n        print(""batch :"", i)\n        """"""\n        imgs, labels = data\n        imgs = imgs.numpy()[:, :, :, ::-1]\n\n        cv2.namedWindow(""Image"", cv2.WINDOW_NORMAL)\n        cv2.imshow(""Image"", np.squeeze(imgs.astype(np.uint8)))\n\n        # cv2.namedWindow(""Mask"", cv2.WINDOW_NORMAL)\n        # cv2.imshow(""Mask"", labels[0])\n        cv2.waitKey(0)\n        """"""'"
datasets/setup.py,0,"b'#!/usr/bin/python\n#\n# Enable cython support for eval scripts\n# Run as\n# setup.py build_ext --inplace\n#\n# For MacOS X you may have to export the numpy headers in CFLAGS\n# export CFLAGS=""-I /usr/local/lib/python3.6/site-packages/numpy/core/include $CFLAGS""\n\nimport os\nfrom setuptools import setup, find_packages\nhas_cython = True\n\ntry:\n    from Cython.Build import cythonize\nexcept:\n    print(""Unable to find dependency cython. Please install for great speed improvements when evaluating."")\n    print(""sudo pip install cython"")\n    has_cython = False\n\ninclude_dirs = []\ntry:\n    import numpy as np\n    include_dirs = np.get_include()\nexcept:\n    print(""Unable to find numpy, please install."")\n    print(""sudo pip install numpy"")\n\nos.environ[""CC""] = ""g++""\nos.environ[""CXX""] = ""g++""\n\npyxFile = os.path.join(""cityscapesscripts"", ""evaluation"", ""addToConfusionMatrix.pyx"")\n\next_modules = []\nif has_cython:\n    ext_modules = cythonize(pyxFile)\n\nconfig = {\n    \'name\': \'cityscapesscripts\',\n    \'description\': \'The Cityscapes Dataset Scripts Repository\',\n    \'author\': \'Marius Cordts\',\n    \'url\': \'www.cityscapes-dataset.net\',\n    \'download_url\': \'www.cityscapes-dataset.net\',\n    \'author_email\': \'mail@cityscapes-dataset.net\',\n    \'license\': \'https://github.com/mcordts/cityscapesScripts/blob/master/license.txt\',\n    \'version\': \'1.0.0\',\n    \'install_requires\': [\'numpy\', \'matplotlib\', \'cython\', \'pillow\'],\n    \'packages\': find_packages(),\n    \'scripts\': [],\n    \'entry_points\': {\'gui_scripts\': [\'csViewer = cityscapesscripts.viewer.cityscapesViewer:main\',\n                                     \'csLabelTool = cityscapesscripts.annotation.cityscapesLabelTool:main\'],\n                     \'console_scripts\': [\'csEvalPixelLevelSemanticLabeling = cityscapesscripts.evaluation.evalPixelLevelSemanticLabeling:main\',\n                                         \'csEvalInstanceLevelSemanticLabeling = cityscapesscripts.evaluation.evalInstanceLevelSemanticLabeling:main\',\n                                         \'csCreateTrainIdLabelImgs = cityscapesscripts.preparation.createTrainIdLabelImgs:main\',\n                                         \'csCreateTrainIdInstanceImgs = cityscapesscripts.preparation.createTrainIdInstanceImgs:main\']},\n    \'package_data\': {\'\': [\'icons/*.png\']},\n    \'ext_modules\': ext_modules,\n    \'include_dirs\': [include_dirs]\n}\n\nsetup(**config)\n'"
models/__init__.py,0,b''
models/inceptionresnetv2.py,10,"b'""""""\n    InceptionResNetV2 using the architecture of InceptionV4 as the paper\n     ""Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning""\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom functools import partial\nfrom collections import OrderedDict\nfrom modules import ModifiedSCSEBlock, ASPPInPlaceABNBlock, ABN, InPlaceABNWrapper\n\n\nclass BasicConv2d(nn.Module):\n    """"""\n        Define the basic conv-bn-relu block\n    """"""\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0, dilate=1):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, dilation=dilate, bias=False)  # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=0.001,      # value found in tensorflow\n                                 momentum=0.1,  # default pytorch value\n                                 affine=True)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_5b(nn.Module):\n    """"""\n        Define the 35x35 grid modules of Inception V4 network\n        to replace later-half stem modules of InceptionResNet V2\n    """"""\n    def __init__(self):\n        super(Mixed_5b, self).__init__()\n\n        self.branch0 = BasicConv2d(192, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(192, 48, kernel_size=1, stride=1),\n            BasicConv2d(48, 64, kernel_size=5, stride=1, padding=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(192, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(192, 64, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block35(nn.Module):\n    """"""\n        The 35x35 grid modules of InceptionResNet V2\n    """"""\n    def __init__(self, scale=1.0):\n        super(Block35, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(320, 32, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_6a(nn.Module):\n    """"""\n        The 35x35 to 17x17 reduction module\n    """"""\n    def __init__(self):\n        super(Mixed_6a, self).__init__()\n\n        self.branch0 = BasicConv2d(320, 384, kernel_size=3, stride=1, padding=2, dilate=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=2, dilate=2),\n            BasicConv2d(256, 384, kernel_size=3, stride=1, padding=2, dilate=2)\n        )\n\n        self.branch2 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n                                     nn.Upsample(scale_factor=2, mode=""bilinear""))\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Block17(nn.Module):\n    """"""\n        The 17x17 grid modules of InceptionResNet V2\n    """"""\n    def __init__(self, scale=1.0):\n        super(Block17, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(1088, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 128, kernel_size=1, stride=1),\n            BasicConv2d(128, 160, kernel_size=(1, 7), stride=1, padding=(0, 6), dilate=(1, 2)),\n            BasicConv2d(160, 192, kernel_size=(7, 1), stride=1, padding=(6, 0), dilate=(2, 1))\n        )\n\n        self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_7a(nn.Module):\n    """"""\n        The 17x17 to 8x8 reduction module\n    """"""\n    def __init__(self):\n        super(Mixed_7a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=1, padding=4, dilate=4)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=4, dilate=4)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=4, dilate=4),\n            BasicConv2d(288, 320, kernel_size=3, stride=1, padding=4, dilate=4)\n        )\n\n        self.branch3 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n                                     nn.Upsample(scale_factor=2, mode=""bilinear""))\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block8(nn.Module):\n    """"""\n        The 8x8 grid modules of InceptionResNet V2\n    """"""\n    def __init__(self, scale=1.0, noReLU=False):\n        super(Block8, self).__init__()\n\n        self.scale = scale\n        self.noReLU = noReLU\n\n        self.branch0 = BasicConv2d(2080, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(2080, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1, 3), stride=1, padding=(0, 4), dilate=(1, 4)),\n            BasicConv2d(224, 256, kernel_size=(3, 1), stride=1, padding=(4, 0), dilate=(4, 1))\n        )\n\n        self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)\n        if not self.noReLU:\n            self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        if not self.noReLU:\n            out = self.relu(out)\n        return out\n\n\nclass InceptionResNetV2(nn.Module):\n    def __init__(self, num_clases=19, in_size=(448, 896), aspp_out=512, fusion_out=64, aspp_sec=(12, 24, 36), norm_act=ABN):\n        super(InceptionResNetV2, self).__init__()\n        self.num_clases = num_clases\n        # Modules\n        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2, padding=1)\n        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2, padding=1)\n        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1, padding=1)\n        self.maxpool_5a = nn.MaxPool2d(3, stride=2, padding=1)\n\n        self.mixed_5b = Mixed_5b()\n\n        self.repeat = nn.Sequential(\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17)\n        )\n        self.mixed_6a = Mixed_6a()\n\n        self.repeat_1 = nn.Sequential(\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10)\n        )\n        self.mixed_7a = Mixed_7a()\n\n        self.repeat_2 = nn.Sequential(\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20)\n        )\n\n        self.block8 = Block8(noReLU=True)\n        self.conv2d_7b = BasicConv2d(2080, 1536, kernel_size=1, stride=1)\n\n        if num_clases != 0:\n            self.stg3_fusion = nn.Conv2d(192, fusion_out, kernel_size=1, stride=1, padding=0, bias=False)\n\n            self.aspp = nn.Sequential(OrderedDict([(""aspp"", ASPPInPlaceABNBlock(1536, aspp_out,\n                                                    feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                                    up_ratio=2, aspp_sec=aspp_sec))]))\n\n            self.score_se = nn.Sequential(ModifiedSCSEBlock(channel=aspp_out+fusion_out, reduction=16))\n            self.score = nn.Sequential(OrderedDict([(""conv"", nn.Conv2d(aspp_out+fusion_out, num_clases,\n                                                                       kernel_size=3, stride=1,\n                                                                       padding=1, bias=True)),\n                                                    (""up"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n    def forward(self, x):\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        c1 = self.conv2d_1a(x)    # [1, 32, 224, 448]   1 / 2\n        c1 = self.conv2d_2a(c1)   # [1, 32, 224, 448]\n        c1 = self.conv2d_2b(c1)   # [1, 64, 224, 448]\n        c1 = self.maxpool_3a(c1)  # [1, 64, 112, 224]\n        c1 = self.conv2d_3b(c1)   # [1, 80, 112, 224]\n        c1 = self.conv2d_4a(c1)   # [1, 192, 112, 224]  1 / 4\n        c2 = self.maxpool_5a(c1)  # [1, 192, 56, 112]\n        c2 = self.mixed_5b(c2)    # [1, 320, 56, 112]\n        c2 = self.repeat(c2)      # [1, 320, 56, 112]    1 / 8\n        c2 = self.mixed_6a(c2)    # [1, 1088, 56, 112]\n        c2 = self.repeat_1(c2)    # [1, 1088, 56, 112]   1 / 16\n        c2 = self.mixed_7a(c2)    # [1, 2080, 56, 112]\n        c2 = self.repeat_2(c2)    # [1, 2080, 56, 112]\n        c2 = self.block8(c2)      # [1, 2080, 56, 112]\n        c2 = self.conv2d_7b(c2)   # [1, 1536, 56, 112]   1 / 32\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.num_clases != 0:\n            # (N, 4096, H/8, W/8) -> (N, 512, H/4, W/4)\n            c2 = self.score_se(torch.cat([self.aspp(c2)[1], self.stg3_fusion(c1)], dim=1))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n\n            return self.score(c2)\n        else:\n            return c2\n\n\n\'\'\'\nTEST\n\'\'\'\nif __name__ == \'__main__\':\n    import time\n    from torch.autograd import Variable\n    model = InceptionResNetV2().cuda()\n\n    input_model = Variable(torch.randn(1, 3, 448, 896).cuda(), requires_grad=True)\n\n    while True:\n        start_time = time.time()\n        _ = model(input_model)\n        end_time = time.time()\n        print(""InceptionResNetV2 inference time: {}s"".format(end_time - start_time))\n\n    # for name, param in model.named_parameters():\n    #     print(""Name: {}, Size: {}"".format(name, param.size()))\n'"
models/mixscaledensenet.py,8,"b'import torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\nfrom modules import SCSEBlock, InPlaceABN, ASPPInPlaceABNBlock, InPlaceABNWrapper, DenseModule\nfrom collections import OrderedDict\nfrom functools import partial\n\n\nclass MixedScaleDenseNet(nn.Module):\n    """"""\n    Mixed Scale Dense Network\n    """"""\n    def __init__(self, n_class=19, in_size=(448, 896), num_layers=128, in_chns=32, squeeze_ratio=1.0/32, out_chns=1,\n                 dilate_sec=(1, 2, 4, 8, 4, 2), aspp_sec=(24, 48, 72), norm_act=InPlaceABN):\n        """"""\n        MixedScaleDenseNet: Mixed Scale Dense Network\n\n        :param n_class:    (int) Number of classes\n        :param in_size:    (tuple or int) Size of the input image feed to the network\n        :param num_layers: (int) Number of layers used in the mixed scale dense block/stage\n        :param in_chns:    (int) Input channels of the mixed scale dense block/stage\n        :param out_chns:   (int) Output channels of each Conv used in the mixed scale dense block/stage\n        :param dilate_sec: (tuple) Dilation rates used in the mixed scale dense block/stage\n        :param aspp_sec:   (tuple) Dilation rates used in ASPP\n        :param norm_act:   (object) Batch Norm Activation Type\n        """"""\n        super(MixedScaleDenseNet, self).__init__()\n\n        self.n_classes = n_class\n\n        self.conv_in = nn.Sequential(OrderedDict([(""conv"", nn.Conv2d(in_channels=3, out_channels=in_chns,\n                                                                     kernel_size=7, stride=2,\n                                                                     padding=3, bias=False)),\n                                                  (""norm"", norm_act(in_chns)),\n                                                  (""pool"", nn.MaxPool2d(3, stride=2, padding=1))]))\n\n        self.dense = DenseModule(in_chns, squeeze_ratio, out_chns, num_layers,\n                                 dilate_sec=dilate_sec, norm_act=norm_act)\n\n        self.last_channel = self.dense.out_channels  # in_chns + num_layers * out_chns\n\n        # Pooling and predictor\n        self.feat_out = norm_act(self.last_channel)\n        self.out_se = nn.Sequential(SCSEBlock(channel=self.last_channel, reduction=16))\n\n        if self.n_classes != 0:\n            self.aspp = nn.Sequential(ASPPInPlaceABNBlock(self.last_channel, self.last_channel,\n                                                          feat_res=(int(in_size[0] / 4), int(in_size[1] / 4)),\n                                                          aspp_sec=aspp_sec, norm_act=norm_act))\n\n            self.score_se = nn.Sequential(SCSEBlock(channel=self.last_channel, reduction=16))\n            self.score = nn.Sequential(OrderedDict([(""norm.1"", norm_act(self.last_channel)),\n                                                    (""conv.1"", nn.Conv2d(self.last_channel, self.last_channel,\n                                                                         kernel_size=3, stride=1, padding=2,\n                                                                         dilation=2, bias=False)),\n                                                    (""norm.2"", norm_act(self.last_channel)),\n                                                    (""conv.2"", nn.Conv2d(self.last_channel, self.n_classes,\n                                                                         kernel_size=1, stride=1, padding=0,\n                                                                         bias=True)),\n                                                    (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n    def forward(self, x):\n        # [N, 3, H, W] -> [N, 32, H/4, W/4] -> [N, 128+32, H/4, W/4]  1/4\n        x = self.out_se(self.feat_out(self.dense(self.conv_in(x))))\n\n        if self.n_classes != 0:\n            return self.score(self.score_se(self.aspp(x)[1]))\n        else:\n            return x\n\n\n# +++++++++++++++++++++++++++++++++++++++++++++ #\n# Test the code of \'MixedScaleDenseNet\'\n# +++++++++++++++++++++++++++++++++++++++++++++ #\nif __name__ == ""__main__"":\n    import time\n    from scripts.loss import *\n    from torch.autograd import Variable\n\n    net_h, net_w = 448, 896\n    model = MixedScaleDenseNet(n_class=19, in_size=(net_h, net_w), num_layers=96, in_chns=32,\n                               squeeze_ratio=1.0 / 32, out_chns=1,\n                               dilate_sec=(1, 2, 4, 8, 16, 8, 4, 2),\n                               aspp_sec=(24, 48, 72),\n                               norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    model.cuda()\n\n    model_dict = model.state_dict()\n\n    pre_weight = torch.load(""/zfs/zhang/TrainLog/weights/cityscapes_msdensenet_best_model.pkl"")[""model_state""]\n\n    keys = list(pre_weight.keys())\n    keys.sort()\n    for k in keys:\n        if ""aspp"" in k:\n            pre_weight.pop(k)\n        if ""score"" in k:\n            pre_weight.pop(k)\n\n    state = {""model_state"": pre_weight}\n    torch.save(state, ""{}msdensenet_model.pkl"".format(""/zfs/zhang/TrainLog/weights/""))\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.90, weight_decay=5e-4)\n    loss_fn = bootstrapped_cross_entropy2d\n\n    i = 0\n    while True:\n        i += 1\n        print(""iter :"", i)\n        model.train()\n\n        dummy_input = Variable(torch.rand(2, 3, net_h, net_w).cuda(), requires_grad=True)\n        dummy_target = Variable(torch.rand(2, net_h, net_w).cuda(), requires_grad=False).long()\n\n        start_time = time.time()\n        dummy_out = model(dummy_input)\n        print(""> Inference Time: {}"".format(time.time()-start_time))\n\n        optimizer.zero_grad()\n\n        topk = 512 * 256\n        loss = loss_fn(dummy_out, dummy_target, K=topk)\n        print(""> Loss: {}"".format(loss.data[0]))\n\n        loss.backward()\n        optimizer.step()\n\n    # model_dict = model.state_dict()\n    # graph = make_dot(dummy_out, model_dict)\n    # graph.view()\n'"
models/mobilenetv2aspp.py,11,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom modules import ModifiedSCSEBlock, InPlaceABN, SDASPPInPlaceABNBlock, InPlaceABNWrapper\nfrom modules.misc import InvertedResidual, conv_bn\nfrom collections import OrderedDict\nfrom functools import partial\n\n\nclass MobileNetV2ASPP(nn.Module):\n    def __init__(self, n_class=19, in_size=(448, 896), width_mult=1.,\n                 out_sec=256, aspp_sec=(12, 24, 36), norm_act=InPlaceABN):\n        """"""\n        MobileNetV2Plus: MobileNetV2 based Semantic Segmentation\n        :param n_class:    (int)  Number of classes\n        :param in_size:    (tuple or int) Size of the input image feed to the network\n        :param width_mult: (float) Network width multiplier\n        :param out_sec:    (tuple) Number of the output channels of the ASPP Block\n        :param aspp_sec:   (tuple) Dilation rates used in ASPP\n        """"""\n        super(MobileNetV2ASPP, self).__init__()\n\n        self.n_class = n_class\n        # setting of inverted residual blocks\n        self.interverted_residual_setting = [\n            # t, c, n, s, d\n            [1, 16, 1, 1, 1],    # 1/2\n            [6, 24, 2, 2, 1],    # 1/4\n            [6, 32, 3, 2, 1],    # 1/8\n            [6, 64, 4, 1, 2],    # 1/8\n            [6, 96, 3, 1, 4],    # 1/8\n            [6, 160, 3, 1, 8],   # 1/8\n            [6, 320, 1, 1, 16],  # 1/8\n        ]\n\n        # building first layer\n        assert in_size[0] % 8 == 0\n        assert in_size[1] % 8 == 0\n\n        self.input_size = in_size\n\n        input_channel = int(32 * width_mult)\n        self.mod1 = nn.Sequential(OrderedDict([(""conv1"", conv_bn(inp=3, oup=input_channel, stride=2))]))\n\n        # building inverted residual blocks\n        mod_id = 0\n        for t, c, n, s, d in self.interverted_residual_setting:\n            output_channel = int(c * width_mult)\n\n            # Create blocks for module\n            blocks = []\n            for block_id in range(n):\n                if block_id == 0 and s == 2:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=s,\n                                                                                dilate=1,\n                                                                                expand_ratio=t)))\n                else:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=1,\n                                                                                dilate=d,\n                                                                                expand_ratio=t)))\n\n                input_channel = output_channel\n\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n            mod_id += 1\n\n        # building last several layers\n        org_last_chns = (self.interverted_residual_setting[0][1] +\n                         self.interverted_residual_setting[1][1] +\n                         self.interverted_residual_setting[2][1] +\n                         self.interverted_residual_setting[3][1] +\n                         self.interverted_residual_setting[4][1] +\n                         self.interverted_residual_setting[5][1] +\n                         self.interverted_residual_setting[6][1])\n\n        self.last_channel = int(org_last_chns * width_mult) if width_mult > 1.0 else org_last_chns\n        self.out_se = nn.Sequential(ModifiedSCSEBlock(channel=self.last_channel, reduction=16))\n\n        if self.n_class != 0:\n            self.sdaspp = nn.Sequential(SDASPPInPlaceABNBlock(self.last_channel, out_sec,\n                                                              feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                                              aspp_sec=aspp_sec, norm_act=norm_act))\n\n            in_stag2_up_chs = self.interverted_residual_setting[1][1] + self.interverted_residual_setting[0][1]\n            self.score_se = nn.Sequential(ModifiedSCSEBlock(channel=out_sec + in_stag2_up_chs, reduction=16))\n            self.score = nn.Sequential(OrderedDict([(""norm.1"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.1"", nn.Conv2d(out_sec + in_stag2_up_chs,\n                                                                         out_sec + in_stag2_up_chs,\n                                                                         kernel_size=3, stride=1, padding=2,\n                                                                         dilation=2, bias=False)),\n                                                    (""norm.2"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.2"", nn.Conv2d(out_sec + in_stag2_up_chs, self.n_class,\n                                                                         kernel_size=1, stride=1, padding=0,\n                                                                         bias=True)),\n                                                    (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n            """"""\n            self.score = nn.Sequential(OrderedDict([(""norm"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""dconv"", nn.Conv2d(out_sec + in_stag2_up_chs,\n                                                                        out_sec + in_stag2_up_chs,\n                                                                        kernel_size=3, stride=1, padding=2,\n                                                                        groups=out_sec + in_stag2_up_chs,\n                                                                        dilation=2, bias=False)),\n                                                    (""pconv"", nn.Conv2d(out_sec + in_stag2_up_chs, self.n_class,\n                                                                        kernel_size=1, stride=1, padding=0,\n                                                                        bias=True)),\n                                                    (""up"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n            """"""\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n            Channel shuffle operation\n            :param x: input tensor\n            :param groups: split channels into groups\n            :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        stg1 = self.mod1(x)     # (N, 32,   224, 448)  1/2\n        stg1 = self.mod2(stg1)  # (N, 16,   224, 448)  1/2 -> 1/4 -> 1/8\n        stg2 = self.mod3(stg1)  # (N, 24,   112, 224)  1/4 -> 1/8\n        stg3 = self.mod4(stg2)  # (N, 32,   56,  112)  1/8\n        stg4 = self.mod5(stg3)  # (N, 64,   56,  112)  1/8 dilation=2\n        stg5 = self.mod6(stg4)  # (N, 96,   56,  112)  1/8 dilation=4\n        stg6 = self.mod7(stg5)  # (N, 160,  56,  112)  1/8 dilation=8\n        stg7 = self.mod8(stg6)  # (N, 320,  56,  112)  1/8 dilation=16\n\n        stg1_1 = F.max_pool2d(input=stg1, kernel_size=3, stride=2, padding=1)    # 1/4\n        stg1_2 = F.max_pool2d(input=stg1_1, kernel_size=3, stride=2, padding=1)  # 1/8\n        stg2_1 = F.max_pool2d(input=stg2, kernel_size=3, stride=2, padding=1)    # 1/8\n\n        # (N, 712, 56,  112)  1/8  (16+24+32+64+96+160+320)\n        stg8 = self.out_se(torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1))\n        # stg8 = torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.n_class != 0:\n            # (N, 672, H/8, W/8) -> (N, 256, H/4, W/4)\n            de_stg1 = self.sdaspp(stg8)\n\n            # (N, 256+24+16=296, H/4, W/4)\n            de_stg1 = self.score_se(torch.cat([de_stg1, stg2, stg1_1], dim=1))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            net_out = self.score(de_stg1)\n\n            return net_out\n        else:\n            return stg8\n\n\nif __name__ == \'__main__\':\n    import time\n    import torch\n    from torch.autograd import Variable\n\n    dummy_in = Variable(torch.randn(1, 3, 448, 896).cuda(), requires_grad=True)\n\n    model = MobileNetV2ASPP(n_class=19)\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n    dummy_out = model(dummy_in)\n\n    model_dict = model.state_dict()\n\n    pre_weight = torch.load(""/zfs/zhang/TrainLog/weights/cityscapes_mobilenetv2_gtfine_best_model.pkl"")[""model_state""]\n\n    keys = list(pre_weight.keys())\n    keys.sort()\n    for k in keys:\n        if ""aspp"" in k:\n            pre_weight.pop(k)\n        if ""score"" in k:\n            pre_weight.pop(k)\n\n    state = {""model_state"": pre_weight}\n    torch.save(state, ""{}mobilenetv2plus_model.pkl"".format(""/zfs/zhang/TrainLog/weights/""))\n'"
models/mobilenetv2exfuse.py,13,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom modules import SCSEBlock, InPlaceABN, InPlaceABNWrapper, RFBlock\nfrom modules.misc import InvertedResidual, conv_bn\nfrom modules.exfuse import SemanticSupervision\nfrom collections import OrderedDict\nfrom functools import partial\n\n\nclass MobileNetV2ExFuse(nn.Module):\n    def __init__(self, n_class=19, in_size=(448, 896), width_mult=1.,\n                 out_sec=256, norm_act=InPlaceABN, traval=""train""):\n        """"""\n        MobileNetV2Plus: MobileNetV2 based Semantic Segmentation\n        :param n_class:    (int)  Number of classes\n        :param in_size:    (tuple or int) Size of the input image feed to the network\n        :param width_mult: (float) Network width multiplier\n        :param out_sec:    (tuple) Number of the output channels of the ASPP Block\n        :param aspp_sec:   (tuple) Dilation rates used in ASPP\n        """"""\n        super(MobileNetV2ExFuse, self).__init__()\n\n        self.n_class = n_class\n        self.traval = traval\n        # setting of inverted residual blocks\n        self.interverted_residual_setting = [\n            # t, c, n, s, d\n            [1, 16, 1, 1, 1],    # 1/2\n            [6, 24, 2, 2, 1],    # 1/4\n            [6, 32, 3, 2, 1],    # 1/8\n            [6, 64, 4, 1, 2],    # 1/8\n            [6, 96, 3, 1, 4],    # 1/8\n            [6, 160, 3, 1, 8],   # 1/8\n            [6, 320, 1, 1, 16],  # 1/8\n        ]\n\n        # building first layer\n        assert in_size[0] % 8 == 0\n        assert in_size[1] % 8 == 0\n\n        self.input_size = in_size\n\n        input_channel = int(32 * width_mult)\n        self.mod1 = nn.Sequential(OrderedDict([(""conv1"", conv_bn(inp=3, oup=input_channel, stride=2))]))\n\n        # building inverted residual blocks\n        mod_id = 0\n        for t, c, n, s, d in self.interverted_residual_setting:\n            output_channel = int(c * width_mult)\n\n            # Create blocks for module\n            blocks = []\n            for block_id in range(n):\n                if block_id == 0 and s == 2:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=s,\n                                                                                dilate=1,\n                                                                                expand_ratio=t)))\n                else:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=1,\n                                                                                dilate=d,\n                                                                                expand_ratio=t)))\n\n                input_channel = output_channel\n\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n            mod_id += 1\n\n        # building last several layers\n        org_last_chns = (self.interverted_residual_setting[0][1] +\n                         self.interverted_residual_setting[1][1] +\n                         self.interverted_residual_setting[2][1] +\n                         self.interverted_residual_setting[3][1] +\n                         self.interverted_residual_setting[4][1] +\n                         self.interverted_residual_setting[5][1] +\n                         self.interverted_residual_setting[6][1])\n\n        self.last_channel = int(org_last_chns * width_mult) if width_mult > 1.0 else org_last_chns\n        self.out_se = nn.Sequential(SCSEBlock(channel=self.last_channel, reduction=16))\n\n        if self.n_class != 0:\n            self.rfblock = nn.Sequential(RFBlock(in_chs=self.last_channel, out_chs=out_sec,\n                                                 scale=1.0, feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                                 up_ratio=2, norm_act=norm_act))\n\n            in_stag2_up_chs = self.interverted_residual_setting[1][1] + self.interverted_residual_setting[0][1]\n            self.score_se = nn.Sequential(SCSEBlock(channel=out_sec + in_stag2_up_chs, reduction=16))\n            self.score = nn.Sequential(OrderedDict([(""norm.1"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.1"", nn.Conv2d(out_sec + in_stag2_up_chs,\n                                                                         out_sec + in_stag2_up_chs,\n                                                                         kernel_size=3, stride=1, padding=2,\n                                                                         dilation=2, bias=False)),\n                                                    (""norm.2"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.2"", nn.Conv2d(out_sec + in_stag2_up_chs, self.n_class,\n                                                                         kernel_size=1, stride=1, padding=0,\n                                                                         bias=True)),\n                                                    (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n            self.sesuper1 = SemanticSupervision(in_chns=self.interverted_residual_setting[0][1],\n                                                out_chns=self.n_class)\n            self.sesuper2 = SemanticSupervision(in_chns=self.interverted_residual_setting[1][1],\n                                                out_chns=self.n_class)\n            self.sesuper3 = SemanticSupervision(in_chns=self.interverted_residual_setting[2][1],\n                                                out_chns=self.n_class)\n            self.sesuper4 = SemanticSupervision(in_chns=self.interverted_residual_setting[3][1],\n                                                out_chns=self.n_class)\n            self.sesuper5 = SemanticSupervision(in_chns=self.interverted_residual_setting[4][1],\n                                                out_chns=self.n_class)\n            self.sesuper6 = SemanticSupervision(in_chns=self.interverted_residual_setting[5][1],\n                                                out_chns=self.n_class)\n            self.sesuper7 = SemanticSupervision(in_chns=self.interverted_residual_setting[6][1],\n                                                out_chns=self.n_class)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n            Channel shuffle operation\n            :param x: input tensor\n            :param groups: split channels into groups\n            :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        stg1 = self.mod1(x)     # (N, 32,   224, 448)  1/2\n        stg1 = self.mod2(stg1)  # (N, 16,   224, 448)  1/2 -> 1/4 -> 1/8\n        stg2 = self.mod3(stg1)  # (N, 24,   112, 224)  1/4 -> 1/8\n        stg3 = self.mod4(stg2)  # (N, 32,   56,  112)  1/8\n        stg4 = self.mod5(stg3)  # (N, 64,   56,  112)  1/8 dilation=2\n        stg5 = self.mod6(stg4)  # (N, 96,   56,  112)  1/8 dilation=4\n        stg6 = self.mod7(stg5)  # (N, 160,  56,  112)  1/8 dilation=8\n        stg7 = self.mod8(stg6)  # (N, 320,  56,  112)  1/8 dilation=16\n\n        stg1_1 = F.max_pool2d(input=stg1, kernel_size=3, stride=2, ceil_mode=True)    # 1/4\n        stg1_2 = F.max_pool2d(input=stg1_1, kernel_size=3, stride=2, ceil_mode=True)  # 1/8\n        stg2_1 = F.max_pool2d(input=stg2, kernel_size=3, stride=2, ceil_mode=True)    # 1/8\n\n        # (N, 672, 56,  112)  1/8  (16+24+32+64+96+160+320)\n        stg8 = self.out_se(torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1))\n        # stg8 = torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.traval == ""train"" and self.n_class != 0:\n            # (N, 672, H/8, W/8) -> (N, 256, H/4, W/4)\n            de_stg1 = self.rfblock(stg8)\n\n            # (N, 256+24+16=296, H/4, W/4)\n            de_stg1 = self.score_se(torch.cat([de_stg1, stg2, stg1_1], dim=1))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            net_out = self.score(de_stg1)\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 4. Auxiliary supervision: semantic supervision\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            enc1 = self.sesuper1(stg1)\n            enc2 = self.sesuper2(stg2)\n            enc3 = self.sesuper3(stg3)\n            enc4 = self.sesuper4(stg4)\n            enc5 = self.sesuper5(stg5)\n            enc6 = self.sesuper6(stg6)\n            enc7 = self.sesuper7(stg7)\n\n            return enc1, enc2, enc3, enc4, enc5, enc6, enc7, net_out\n        elif self.traval == ""train"" and self.n_class != 0:\n            # (N, 672, H/8, W/8) -> (N, 256, H/4, W/4)\n            de_stg1 = self.rfblock(stg8)\n\n            # (N, 256+24+16=296, H/4, W/4)\n            de_stg1 = self.score_se(torch.cat([de_stg1, stg2, stg1_1], dim=1))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            return self.score(de_stg1)\n        else:\n            return stg8\n\n\nif __name__ == \'__main__\':\n    import os\n    import time\n    from scripts.loss import *\n    from torch.autograd import Variable\n\n    net_h, net_w = 384, 768\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    model = MobileNetV2ExFuse(n_class=19, in_size=(net_h, net_w), width_mult=1.0, out_sec=256,\n                              norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1),\n                              traval=""train"")\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    model_dict = model.state_dict()\n\n    pre_weight = torch.load(""/zfs/zhang/TrainLog/weights/cityscapes_mobilenetv2_gtfine_best_model.pkl"")[""model_state""]\n    pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n    model_dict.update(pretrained_dict)\n    model.load_state_dict(model_dict)\n\n    del pre_weight\n    del model_dict\n    del pretrained_dict\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.90, weight_decay=5e-4)\n    loss_fn = bootstrapped_cross_entropy2d\n\n    i = 0\n    while True:\n        i += 1\n        print(""iter :"", i)\n        model.train()\n\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        dummy_target = Variable(torch.rand(1, net_h, net_w).cuda(), requires_grad=False).long()\n\n        start_time = time.time()\n        dummy_out = model(dummy_input)\n        print(""> Inference Time: {}"".format(time.time() - start_time))\n\n        optimizer.zero_grad()\n\n        topk = 512 * 256\n        loss = loss_fn(dummy_out, dummy_target, K=topk)\n        print(""> Loss: {}"".format(loss.data[0]))\n\n        loss.backward()\n        optimizer.step()\n\n'"
models/mobilenetv2plus.py,11,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom modules import SCSEBlock, InPlaceABN, ASPPInPlaceABNBlock, InPlaceABNWrapper\nfrom modules.misc import InvertedResidual, conv_bn\nfrom collections import OrderedDict\nfrom functools import partial\n\n\nclass MobileNetV2Plus(nn.Module):\n    def __init__(self, n_class=19, in_size=(448, 896), width_mult=1.,\n                 out_sec=256, aspp_sec=(12, 24, 36), norm_act=InPlaceABN):\n        """"""\n        MobileNetV2Plus: MobileNetV2 based Semantic Segmentation\n        :param n_class:    (int)  Number of classes\n        :param in_size:    (tuple or int) Size of the input image feed to the network\n        :param width_mult: (float) Network width multiplier\n        :param out_sec:    (tuple) Number of the output channels of the ASPP Block\n        :param aspp_sec:   (tuple) Dilation rates used in ASPP\n        """"""\n        super(MobileNetV2Plus, self).__init__()\n\n        self.n_class = n_class\n        # setting of inverted residual blocks\n        self.interverted_residual_setting = [\n            # t, c, n, s, d\n            [1, 16, 1, 1, 1],    # 1/2\n            [6, 24, 2, 2, 1],    # 1/4\n            [6, 32, 3, 2, 1],    # 1/8\n            [6, 64, 4, 1, 2],    # 1/8\n            [6, 96, 3, 1, 4],    # 1/8\n            [6, 160, 3, 1, 8],   # 1/8\n            [6, 320, 1, 1, 16],  # 1/8\n        ]\n\n        # building first layer\n        assert in_size[0] % 8 == 0\n        assert in_size[1] % 8 == 0\n\n        self.input_size = in_size\n\n        input_channel = int(32 * width_mult)\n        self.mod1 = nn.Sequential(OrderedDict([(""conv1"", conv_bn(inp=3, oup=input_channel, stride=2))]))\n\n        # building inverted residual blocks\n        mod_id = 0\n        for t, c, n, s, d in self.interverted_residual_setting:\n            output_channel = int(c * width_mult)\n\n            # Create blocks for module\n            blocks = []\n            for block_id in range(n):\n                if block_id == 0 and s == 2:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=s,\n                                                                                dilate=1,\n                                                                                expand_ratio=t)))\n                else:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=1,\n                                                                                dilate=d,\n                                                                                expand_ratio=t)))\n\n                input_channel = output_channel\n\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n            mod_id += 1\n\n        # building last several layers\n        org_last_chns = (self.interverted_residual_setting[0][1] +\n                         self.interverted_residual_setting[1][1] +\n                         self.interverted_residual_setting[2][1] +\n                         self.interverted_residual_setting[3][1] +\n                         self.interverted_residual_setting[4][1] +\n                         self.interverted_residual_setting[5][1] +\n                         self.interverted_residual_setting[6][1])\n\n        self.last_channel = int(org_last_chns * width_mult) if width_mult > 1.0 else org_last_chns\n        self.out_se = nn.Sequential(SCSEBlock(channel=self.last_channel, reduction=16))\n\n        if self.n_class != 0:\n            self.aspp = nn.Sequential(ASPPInPlaceABNBlock(self.last_channel, out_sec,\n                                                          feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                                          aspp_sec=aspp_sec, norm_act=norm_act))\n\n            in_stag2_up_chs = self.interverted_residual_setting[1][1] + self.interverted_residual_setting[0][1]\n            self.score_se = nn.Sequential(SCSEBlock(channel=out_sec + in_stag2_up_chs, reduction=16))\n            self.score = nn.Sequential(OrderedDict([(""norm.1"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.1"", nn.Conv2d(out_sec + in_stag2_up_chs,\n                                                                         out_sec + in_stag2_up_chs,\n                                                                         kernel_size=3, stride=1, padding=2,\n                                                                         dilation=2, bias=False)),\n                                                    (""norm.2"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.2"", nn.Conv2d(out_sec + in_stag2_up_chs, self.n_class,\n                                                                         kernel_size=1, stride=1, padding=0,\n                                                                         bias=True)),\n                                                    (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n            Channel shuffle operation\n            :param x: input tensor\n            :param groups: split channels into groups\n            :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        stg1 = self.mod1(x)     # (N, 32,   224, 448)  1/2\n        stg1 = self.mod2(stg1)  # (N, 16,   224, 448)  1/2 -> 1/4 -> 1/8\n        stg2 = self.mod3(stg1)  # (N, 24,   112, 224)  1/4 -> 1/8\n        stg3 = self.mod4(stg2)  # (N, 32,   56,  112)  1/8\n        stg4 = self.mod5(stg3)  # (N, 64,   56,  112)  1/8 dilation=2\n        stg5 = self.mod6(stg4)  # (N, 96,   56,  112)  1/8 dilation=4\n        stg6 = self.mod7(stg5)  # (N, 160,  56,  112)  1/8 dilation=8\n        stg7 = self.mod8(stg6)  # (N, 320,  56,  112)  1/8 dilation=16\n\n        stg1_1 = F.max_pool2d(input=stg1, kernel_size=3, stride=2, ceil_mode=True)    # 1/4\n        stg1_2 = F.max_pool2d(input=stg1_1, kernel_size=3, stride=2, ceil_mode=True)  # 1/8\n        stg2_1 = F.max_pool2d(input=stg2, kernel_size=3, stride=2, ceil_mode=True)    # 1/8\n\n        # (N, 712, 56,  112)  1/8  (16+24+32+64+96+160+320)\n        stg8 = self.out_se(torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1))\n        # stg8 = torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.n_class != 0:\n            # (N, 672, H/8, W/8) -> (N, 256, H/4, W/4)\n            de_stg1 = self.aspp(stg8)[1]\n\n            # (N, 256+24+16=296, H/4, W/4)\n            de_stg1 = self.score_se(torch.cat([de_stg1, stg2, stg1_1], dim=1))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            net_out = self.score(de_stg1)\n\n            return net_out\n        else:\n            return stg8\n\n\nif __name__ == \'__main__\':\n    import time\n    import torch\n    from torch.autograd import Variable\n\n    dummy_in = Variable(torch.randn(1, 3, 448, 896).cuda(), requires_grad=True)\n\n    model = MobileNetV2Plus(n_class=19)\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n    dummy_out = model(dummy_in)\n\n    model_dict = model.state_dict()\n\n    pre_weight = torch.load(""/zfs/zhang/TrainLog/weights/cityscapes_mobilenetv2_gtfine_best_model.pkl"")[""model_state""]\n\n    keys = list(pre_weight.keys())\n    keys.sort()\n    for k in keys:\n        if ""aspp"" in k:\n            pre_weight.pop(k)\n        if ""score"" in k:\n            pre_weight.pop(k)\n\n    state = {""model_state"": pre_weight}\n    torch.save(state, ""{}mobilenetv2plus_model.pkl"".format(""/zfs/zhang/TrainLog/weights/""))\n'"
models/mobilenetv2share.py,15,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom modules import SCSEBlock, InPlaceABN, ASPPInPlaceABNBlock, InPlaceABNWrapper\nfrom modules.misc import InvertedResidual, conv_bn\nfrom collections import OrderedDict\nfrom functools import partial\n\n\nclass MobileNetV2Share(nn.Module):\n    def __init__(self, n_class=19, in_size=(896, 17), width_mult=1.,\n                 out_sec=256, aspp_sec=(24, 48, 72), norm_act=InPlaceABN):\n        """"""\n        MobileNetV2Plus: MobileNetV2 based Semantic Segmentation\n        :param n_class:    (int)  Number of classes\n        :param in_size:    (tuple or int) Size of the input image feed to the network\n        :param width_mult: (float) Network width multiplier\n        :param out_sec:    (tuple) Number of the output channels of the ASPP Block\n        :param aspp_sec:   (tuple) Dilation rates used in ASPP\n        """"""\n        super(MobileNetV2Share, self).__init__()\n\n        self.n_class = n_class\n        # setting of inverted residual blocks\n        self.interverted_residual_setting = [\n            # t, c, n, s, d\n            [1, 16, 1, 1, 1],    # 1/2\n            [6, 24, 2, 2, 1],    # 1/4\n            [6, 32, 3, 2, 1],    # 1/8\n            [6, 64, 4, 1, 2],    # 1/8\n            [6, 96, 3, 1, 4],    # 1/8\n            [6, 160, 3, 1, 8],   # 1/8\n            [6, 320, 1, 1, 16],  # 1/8\n        ]\n\n        # building first layer\n        assert in_size[0] % 8 == 0\n        assert in_size[1] % 8 == 0\n\n        self.input_size = in_size\n\n        input_channel = int(32 * width_mult)\n        self.mod1 = nn.Sequential(OrderedDict([(""conv1"", conv_bn(inp=3, oup=input_channel, stride=2))]))\n\n        # building inverted residual blocks\n        mod_id = 0\n        for t, c, n, s, d in self.interverted_residual_setting:\n            output_channel = int(c * width_mult)\n\n            # Create blocks for module\n            blocks = []\n            for block_id in range(n):\n                if block_id == 0 and s == 2:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=s,\n                                                                                dilate=1,\n                                                                                expand_ratio=t)))\n                else:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=1,\n                                                                                dilate=d,\n                                                                                expand_ratio=t)))\n\n                input_channel = output_channel\n\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n            mod_id += 1\n\n        # building last several layers\n        org_last_chns = (self.interverted_residual_setting[0][1] +\n                         self.interverted_residual_setting[1][1] +\n                         self.interverted_residual_setting[2][1] +\n                         self.interverted_residual_setting[3][1] +\n                         self.interverted_residual_setting[4][1] +\n                         self.interverted_residual_setting[5][1] +\n                         self.interverted_residual_setting[6][1])\n\n        self.last_channel = int(org_last_chns * width_mult) if width_mult > 1.0 else org_last_chns\n        self.out_se = nn.Sequential(SCSEBlock(channel=self.last_channel, reduction=16))\n\n        if self.n_class != 0:\n            self.aspp = nn.Sequential(ASPPInPlaceABNBlock(self.last_channel, out_sec,\n                                                          feat_res=(int(in_size[0] / 16), int(in_size[1] / 16)),\n                                                          aspp_sec=aspp_sec, norm_act=norm_act))\n\n            in_stag2_up_chs = self.interverted_residual_setting[1][1] + self.interverted_residual_setting[0][1]\n            self.score_se = nn.Sequential(SCSEBlock(channel=out_sec + in_stag2_up_chs, reduction=16))\n            self.score = nn.Sequential(OrderedDict([(""norm.1"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.1"", nn.Conv2d(out_sec + in_stag2_up_chs,\n                                                                         out_sec + in_stag2_up_chs,\n                                                                         kernel_size=3, stride=1, padding=2,\n                                                                         dilation=2, bias=False)),\n                                                    (""norm.2"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.2"", nn.Conv2d(out_sec + in_stag2_up_chs, self.n_class,\n                                                                         kernel_size=1, stride=1, padding=0,\n                                                                         bias=True)),\n                                                    (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n            Channel shuffle operation\n            :param x: input tensor\n            :param groups: split channels into groups\n            :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    @staticmethod\n    def __split_cat(x):\n        h_up, h_down = x.chunk(chunks=2, dim=2)  # [N, C, H, W]\n        w_left_up, w_right_up = h_up.chunk(chunks=2, dim=3)\n        w_left_down, w_right_down = h_down.chunk(chunks=2, dim=3)\n\n        return torch.cat([w_left_up, w_right_up, w_left_down, w_right_down], dim=0)\n\n    @staticmethod\n    def __recat_feat(x, bs=2):\n        batch_size, num_channels, height, width = x.data.size()\n\n        batches = x.chunk(chunks=batch_size // bs, dim=0)  # [N, C, H, W]\n        out = torch.cat([torch.cat([batches[0], batches[1]], dim=3), torch.cat([batches[2], batches[3]], dim=3)], dim=2)\n\n        return out\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = self.__split_cat(x)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        stg1 = self.mod1(x)     # (N, 32,   224, 448)  1/2\n        stg1 = self.mod2(stg1)  # (N, 16,   224, 448)  1/2 -> 1/4 -> 1/8\n        stg2 = self.mod3(stg1)  # (N, 24,   112, 224)  1/4 -> 1/8\n        stg3 = self.mod4(stg2)  # (N, 32,   56,  112)  1/8\n        stg4 = self.mod5(stg3)  # (N, 64,   56,  112)  1/8 dilation=2\n        stg5 = self.mod6(stg4)  # (N, 96,   56,  112)  1/8 dilation=4\n        stg6 = self.mod7(stg5)  # (N, 160,  56,  112)  1/8 dilation=8\n        stg7 = self.mod8(stg6)  # (N, 320,  56,  112)  1/8 dilation=16\n\n        stg1_1 = F.max_pool2d(input=stg1, kernel_size=3, stride=2, ceil_mode=True)    # 1/4\n        stg1_2 = F.max_pool2d(input=stg1_1, kernel_size=3, stride=2, ceil_mode=True)  # 1/8\n        stg2_1 = F.max_pool2d(input=stg2, kernel_size=3, stride=2, ceil_mode=True)    # 1/8\n\n        # (N, 712, 56,  112)  1/8  (16+24+32+64+96+160+320)\n        stg8 = self.out_se(torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1))\n        # stg8 = torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.n_class != 0:\n            # (N, 672, H/8, W/8) -> (N, 256, H/4, W/4)\n            # stg8 = self.__recat_feat(stg8, bs=batch_size)\n            de_stg1 = self.aspp(stg8)[1]\n\n            # (N, 256+24+16=296, H/4, W/4)\n            # de_stg1 = self.score_se(torch.cat([de_stg1,\n            #                                   self.__recat_feat(stg2, bs=batch_size),\n            #                                   self.__recat_feat(stg1_1, bs=batch_size)], dim=1))\n\n            de_stg1 = self.score_se(torch.cat([de_stg1, stg2, stg1_1], dim=1))\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            de_stg1 = self.__recat_feat(de_stg1, bs=batch_size)\n            net_out = self.score(de_stg1)\n\n            return net_out\n        else:\n            return stg8\n\n\nif __name__ == \'__main__\':\n    import time\n    from scripts.loss import *\n    from torch.autograd import Variable\n\n    net_h, net_w = 448, 896\n\n    model = MobileNetV2Share(n_class=19, in_size=(net_h, net_w), width_mult=1.0,\n                             out_sec=256, aspp_sec=(24, 48, 72),\n                             norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()\n    model_dict = model.state_dict()\n\n    pre_weight = torch.load(""/zfs/zhang/TrainLog/weights/cityscapes_mobilenetv2_gtfine_best_model.pkl"")[""model_state""]\n    pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n    model_dict.update(pretrained_dict)\n    model.load_state_dict(model_dict)\n\n    del pre_weight\n    del model_dict\n    del pretrained_dict\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.90, weight_decay=5e-4)\n    loss_fn = bootstrapped_cross_entropy2d\n\n    i = 0\n    while True:\n        i += 1\n        print(""iter :"", i)\n        model.train()\n\n        dummy_input = Variable(torch.rand(2, 3, net_h, net_w).cuda(), requires_grad=True)\n        dummy_target = Variable(torch.rand(2, net_h, net_w).cuda(), requires_grad=False).long()\n\n        start_time = time.time()\n        dummy_out = model(dummy_input)\n        print(""> Inference Time: {}"".format(time.time() - start_time))\n\n        optimizer.zero_grad()\n\n        topk = 512 * 256\n        loss = loss_fn(dummy_out, dummy_target, K=topk)\n        print(""> Loss: {}"".format(loss.data[0]))\n\n        loss.backward()\n        optimizer.step()'"
models/mobilenetv2vortex.py,11,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom modules import ModifiedSCSEBlock, InPlaceABN, VortexPooling, InPlaceABNWrapper\nfrom modules.misc import InvertedResidual, conv_bn\nfrom collections import OrderedDict\nfrom functools import partial\n\n\nclass MobileNetV2Vortex(nn.Module):\n    def __init__(self, n_class=19, in_size=(448, 896), width_mult=1.,\n                 out_sec=256, rate_sec=(3, 9, 27), norm_act=InPlaceABN):\n        """"""\n        MobileNetV2Plus: MobileNetV2 based Semantic Segmentation\n        :param n_class:    (int)  Number of classes\n        :param in_size:    (tuple or int) Size of the input image feed to the network\n        :param width_mult: (float) Network width multiplier\n        :param out_sec:    (tuple) Number of the output channels of the ASPP Block\n        :param aspp_sec:   (tuple) Dilation rates used in ASPP\n        """"""\n        super(MobileNetV2Vortex, self).__init__()\n\n        self.n_class = n_class\n        # setting of inverted residual blocks\n        self.interverted_residual_setting = [\n            # t, c, n, s, d\n            [1, 16, 1, 1, 1],    # 1/2\n            [6, 24, 2, 2, 1],    # 1/4\n            [6, 32, 3, 2, 1],    # 1/8\n            [6, 64, 4, 1, 2],    # 1/8\n            [6, 96, 3, 1, 4],    # 1/8\n            [6, 160, 3, 1, 8],   # 1/8\n            [6, 320, 1, 1, 16],  # 1/8\n        ]\n\n        # building first layer\n        assert in_size[0] % 8 == 0\n        assert in_size[1] % 8 == 0\n\n        self.input_size = in_size\n\n        input_channel = int(32 * width_mult)\n        self.mod1 = nn.Sequential(OrderedDict([(""conv1"", conv_bn(inp=3, oup=input_channel, stride=2))]))\n\n        # building inverted residual blocks\n        mod_id = 0\n        for t, c, n, s, d in self.interverted_residual_setting:\n            output_channel = int(c * width_mult)\n\n            # Create blocks for module\n            blocks = []\n            for block_id in range(n):\n                if block_id == 0 and s == 2:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=s,\n                                                                                dilate=1,\n                                                                                expand_ratio=t)))\n                else:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=1,\n                                                                                dilate=d,\n                                                                                expand_ratio=t)))\n\n                input_channel = output_channel\n\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n            mod_id += 1\n\n        # building last several layers\n        org_last_chns = (self.interverted_residual_setting[0][1] +\n                         self.interverted_residual_setting[1][1] +\n                         self.interverted_residual_setting[2][1] +\n                         self.interverted_residual_setting[3][1] +\n                         self.interverted_residual_setting[4][1] +\n                         self.interverted_residual_setting[5][1] +\n                         self.interverted_residual_setting[6][1])\n\n        self.last_channel = int(org_last_chns * width_mult) if width_mult > 1.0 else org_last_chns\n        self.out_se = nn.Sequential(ModifiedSCSEBlock(channel=self.last_channel, reduction=16))\n\n        if self.n_class != 0:\n            self.vortex = nn.Sequential(VortexPooling(self.last_channel, out_sec,\n                                                      feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                                      rate=rate_sec))\n\n            in_stag2_up_chs = self.interverted_residual_setting[1][1] + self.interverted_residual_setting[0][1]\n            self.score_se = nn.Sequential(ModifiedSCSEBlock(channel=out_sec + in_stag2_up_chs, reduction=16))\n            self.score = nn.Sequential(OrderedDict([(""norm.1"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.1"", nn.Conv2d(out_sec + in_stag2_up_chs,\n                                                                         out_sec + in_stag2_up_chs,\n                                                                         kernel_size=3, stride=1, padding=2,\n                                                                         dilation=2, bias=False)),\n                                                    (""norm.2"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.2"", nn.Conv2d(out_sec + in_stag2_up_chs, self.n_class,\n                                                                         kernel_size=1, stride=1, padding=0,\n                                                                         bias=True)),\n                                                    (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n            """"""\n            self.score = nn.Sequential(OrderedDict([(""norm"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""dconv"", nn.Conv2d(out_sec + in_stag2_up_chs,\n                                                                        out_sec + in_stag2_up_chs,\n                                                                        kernel_size=3, stride=1, padding=2,\n                                                                        groups=out_sec + in_stag2_up_chs,\n                                                                        dilation=2, bias=False)),\n                                                    (""pconv"", nn.Conv2d(out_sec + in_stag2_up_chs, self.n_class,\n                                                                        kernel_size=1, stride=1, padding=0,\n                                                                        bias=True)),\n                                                    (""up"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n            """"""\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n            Channel shuffle operation\n            :param x: input tensor\n            :param groups: split channels into groups\n            :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        stg1 = self.mod1(x)     # (N, 32,   224, 448)  1/2\n        stg1 = self.mod2(stg1)  # (N, 16,   224, 448)  1/2 -> 1/4 -> 1/8\n        stg2 = self.mod3(stg1)  # (N, 24,   112, 224)  1/4 -> 1/8\n        stg3 = self.mod4(stg2)  # (N, 32,   56,  112)  1/8\n        stg4 = self.mod5(stg3)  # (N, 64,   56,  112)  1/8 dilation=2\n        stg5 = self.mod6(stg4)  # (N, 96,   56,  112)  1/8 dilation=4\n        stg6 = self.mod7(stg5)  # (N, 160,  56,  112)  1/8 dilation=8\n        stg7 = self.mod8(stg6)  # (N, 320,  56,  112)  1/8 dilation=16\n\n        stg1_1 = F.max_pool2d(input=stg1, kernel_size=3, stride=2, padding=1)    # 1/4\n        stg1_2 = F.max_pool2d(input=stg1_1, kernel_size=3, stride=2, padding=1)  # 1/8\n        stg2_1 = F.max_pool2d(input=stg2, kernel_size=3, stride=2, padding=1)    # 1/8\n\n        # (N, 712, 56,  112)  1/8  (16+24+32+64+96+160+320)\n        stg7 = self.out_se(torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1))\n        # stg7 = torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.n_class != 0:\n            # (N, 672, H/8, W/8) -> (N, 256, H/4, W/4)\n            de_stg1 = self.vortex(stg7)\n\n            # (N, 256+24+16=296, H/4, W/4)\n            de_stg1 = self.score_se(torch.cat([de_stg1, stg2, stg1_1], dim=1))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            net_out = self.score(de_stg1)\n\n            return net_out\n        else:\n            return stg7\n\n\nif __name__ == \'__main__\':\n    import os\n    import time\n    from scripts.loss import *\n    from torch.autograd import Variable\n\n    net_h, net_w = 384, 768\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    model = MobileNetV2Vortex(n_class=19, in_size=(448, 896), width_mult=1., out_sec=256, rate_sec=(3, 9, 27),\n                              norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.90, weight_decay=5e-4)\n    loss_fn = bootstrapped_cross_entropy2d\n\n    i = 0\n    while True:\n        i += 1\n        print(""iter :"", i)\n        model.train()\n\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        dummy_target = Variable(torch.rand(1, net_h, net_w).cuda(), requires_grad=False).long()\n\n        start_time = time.time()\n        dummy_out = model(dummy_input)\n        print(""> Inference Time: {}"".format(time.time() - start_time))\n\n        optimizer.zero_grad()\n\n        topk = 512 * 256\n        loss = loss_fn(dummy_out, dummy_target, K=topk)\n        print(""> Loss: {}"".format(loss.data[0]))\n\n        loss.backward()\n        optimizer.step()\n'"
models/rfmobilenetv2context.py,6,"b'import math\nimport torch\nimport encoding\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom modules import RFBlock, ContextEncodeDropInplaceABN\nfrom modules import InPlaceABN,  InPlaceABNWrapper\nfrom modules.misc import InvertedResidual, conv_bn\nfrom collections import OrderedDict\nfrom functools import partial\n\n\nclass MobileNetV2Context(nn.Module):\n    def __init__(self, n_class=19, in_size=(448, 896), width_mult=1.,\n                 out_sec=256, context=(32, 4), aspp_sec=(12, 24, 36), norm_act=InPlaceABN):\n        """"""\n        MobileNetV2Plus: MobileNetV2 based Semantic Segmentation\n        :param n_class:    (int)  Number of classes\n        :param in_size:    (tuple or int) Size of the input image feed to the network\n        :param width_mult: (float) Network width multiplier\n        :param out_sec:    (tuple) Number of the output channels of the ASPP Block\n        :param context:   (tuple) K and reduction\n        """"""\n        super(MobileNetV2Context, self).__init__()\n\n        self.n_class = n_class\n        # setting of inverted residual blocks\n        self.interverted_residual_setting = [\n            # t, c, n, s, d\n            [1, 16, 1, 1, 1],    # 1/2\n            [6, 24, 2, 2, 1],    # 1/4\n            [6, 32, 3, 2, 1],    # 1/8\n            [6, 64, 4, 1, 2],    # 1/8\n            [6, 96, 3, 1, 4],    # 1/8\n            [6, 160, 3, 1, 8],   # 1/8\n            [6, 320, 1, 1, 16],  # 1/8\n        ]\n\n        # building first layer\n        assert in_size[0] % 8 == 0\n        assert in_size[1] % 8 == 0\n\n        self.input_size = in_size\n\n        input_channel = int(32 * width_mult)\n        self.mod1 = nn.Sequential(OrderedDict([(""conv1"", conv_bn(inp=3, oup=input_channel, stride=2))]))\n\n        # building inverted residual blocks\n        mod_id = 0\n        for t, c, n, s, d in self.interverted_residual_setting:\n            output_channel = int(c * width_mult)\n\n            # Create blocks for module\n            blocks = []\n            for block_id in range(n):\n                if block_id == 0 and s == 2:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=s,\n                                                                                dilate=1,\n                                                                                expand_ratio=t)))\n                else:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=1,\n                                                                                dilate=d,\n                                                                                expand_ratio=t)))\n\n                input_channel = output_channel\n\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n            mod_id += 1\n\n        # building last several layers\n        org_last_chns = (self.interverted_residual_setting[0][1] +\n                         self.interverted_residual_setting[1][1] +\n                         self.interverted_residual_setting[2][1] +\n                         self.interverted_residual_setting[3][1] +\n                         self.interverted_residual_setting[4][1] +\n                         self.interverted_residual_setting[5][1] +\n                         self.interverted_residual_setting[6][1])\n\n        self.last_channel = int(org_last_chns * width_mult) if width_mult > 1.0 else org_last_chns\n        self.context1 = ContextEncodeDropInplaceABN(channel=self.last_channel, K=context[0],\n                                                    reduction=context[1], norm_act=norm_act)\n        self.se_loss1 = nn.Sequential(OrderedDict([(""linear"", nn.Linear(int(self.last_channel / context[1]) *\n                                                                        context[0], self.n_class))]))\n\n        self.rfblock = nn.Sequential(RFBlock(in_chs=self.last_channel, out_chs=out_sec,\n                                             scale=1.0, feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                             up_ratio=2, aspp_sec=aspp_sec, norm_act=norm_act))\n        if self.n_class != 0:\n            in_stag2_up_chs = self.interverted_residual_setting[1][1] + self.interverted_residual_setting[0][1]\n\n            self.context2 = ContextEncodeDropInplaceABN(channel=(out_sec + in_stag2_up_chs), K=context[0],\n                                                        reduction=context[1], norm_act=norm_act)\n            self.se_loss2 = nn.Sequential(OrderedDict([(""linear"", nn.Linear(int((out_sec + in_stag2_up_chs)\n                                                                                / context[1]) * context[0],\n                                                                            self.n_class))]))\n\n            self.score = nn.Sequential(OrderedDict([(""norm.1"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.1"", nn.Conv2d(out_sec + in_stag2_up_chs, out_sec,\n                                                                         kernel_size=3, stride=1, padding=2,\n                                                                         dilation=2, bias=False)),\n                                                    (""norm.2"", norm_act(out_sec)),\n                                                    (""conv.2"", nn.Conv2d(out_sec, self.n_class,\n                                                                         kernel_size=1, stride=1, padding=0,\n                                                                         bias=True)),\n                                                    (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n            """"""\n\n            self.score = nn.Sequential(OrderedDict([(""norm"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv"", nn.Conv2d(out_sec + in_stag2_up_chs, self.n_class,\n                                                                       kernel_size=1, stride=1, padding=0,\n                                                                       bias=True)),\n                                                    (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n            """"""\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        stg1 = self.mod1(x)     # (N, 32,   224, 448)  1/2\n        stg1 = self.mod2(stg1)  # (N, 16,   224, 448)  1/2 -> 1/4 -> 1/8\n        stg2 = self.mod3(stg1)  # (N, 24,   112, 224)  1/4 -> 1/8\n        stg3 = self.mod4(stg2)  # (N, 32,   56,  112)  1/8\n        stg4 = self.mod5(stg3)  # (N, 64,   56,  112)  1/8 dilation=2\n        stg5 = self.mod6(stg4)  # (N, 96,   56,  112)  1/8 dilation=4\n        stg6 = self.mod7(stg5)  # (N, 160,  56,  112)  1/8 dilation=8\n        stg7 = self.mod8(stg6)  # (N, 320,  56,  112)  1/8 dilation=16\n\n        stg1_1 = F.max_pool2d(input=stg1, kernel_size=3, stride=2, ceil_mode=True)    # 1/4\n        stg1_2 = F.max_pool2d(input=stg1_1, kernel_size=3, stride=2, ceil_mode=True)  # 1/8\n        stg2_1 = F.max_pool2d(input=stg2, kernel_size=3, stride=2, ceil_mode=True)    # 1/8\n\n        # (N, 712, 56,  112)  1/8  (16+24+32+64+96+160+320)\n        enc1, stg8 = self.context1(torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1))\n        stg8 = self.rfblock(stg8)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.n_class != 0:\n            enc2, stg8 = self.context2(torch.cat([stg8, stg2, stg1_1], dim=1))\n            return self.se_loss1(enc1), self.se_loss2(enc2), self.score(stg8)\n        else:\n            return stg8\n\n\nif __name__ == \'__main__\':\n    import os\n    import torch\n    from torch.autograd import Variable\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n\n    dummy_in = Variable(torch.randn(1, 3, 448, 448).cuda(), requires_grad=True)\n\n    model = MobileNetV2Context(n_class=19, in_size=(448, 448), width_mult=1., out_sec=256, context=(32, 4),\n                               norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1)).cuda()\n    enc1, enc2, dummy_out = model(dummy_in)\n    print(""ok!!!"")\n\n'"
models/rfmobilenetv2plus.py,12,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom modules import SCSEBlock, InPlaceABN, InPlaceABNWrapper, RFBlock\nfrom modules.misc import InvertedResidual, conv_bn\nfrom collections import OrderedDict\nfrom functools import partial\n\n\nclass RFMobileNetV2Plus(nn.Module):\n    def __init__(self, n_class=19, in_size=(448, 896), width_mult=1.,\n                 out_sec=256, aspp_sec=(12, 24, 36), norm_act=InPlaceABN):\n        """"""\n        MobileNetV2Plus: MobileNetV2 based Semantic Segmentation\n        :param n_class:    (int)  Number of classes\n        :param in_size:    (tuple or int) Size of the input image feed to the network\n        :param width_mult: (float) Network width multiplier\n        :param out_sec:    (tuple) Number of the output channels of the ASPP Block\n        :param aspp_sec:   (tuple) Dilation rates used in ASPP\n        """"""\n        super(RFMobileNetV2Plus, self).__init__()\n\n        self.n_class = n_class\n        # setting of inverted residual blocks\n        self.interverted_residual_setting = [\n            # t, c, n, s, d\n            [1, 16, 1, 1, 1],    # 1/2\n            [6, 24, 2, 2, 1],    # 1/4\n            [6, 32, 3, 2, 1],    # 1/8\n            [6, 64, 4, 1, 2],    # 1/8\n            [6, 96, 3, 1, 4],    # 1/8\n            [6, 160, 3, 1, 8],   # 1/8\n            [6, 320, 1, 1, 16],  # 1/8\n        ]\n\n        # building first layer\n        assert in_size[0] % 8 == 0\n        assert in_size[1] % 8 == 0\n\n        self.input_size = in_size\n\n        input_channel = int(32 * width_mult)\n        self.mod1 = nn.Sequential(OrderedDict([(""conv1"", conv_bn(inp=3, oup=input_channel, stride=2))]))\n\n        # building inverted residual blocks\n        mod_id = 0\n        for t, c, n, s, d in self.interverted_residual_setting:\n            output_channel = int(c * width_mult)\n\n            # Create blocks for module\n            blocks = []\n            for block_id in range(n):\n                if block_id == 0 and s == 2:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=s,\n                                                                                dilate=1,\n                                                                                expand_ratio=t)))\n                else:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=1,\n                                                                                dilate=d,\n                                                                                expand_ratio=t)))\n\n                input_channel = output_channel\n\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n            mod_id += 1\n\n        # building last several layers\n        org_last_chns = (self.interverted_residual_setting[0][1] +\n                         self.interverted_residual_setting[1][1] +\n                         self.interverted_residual_setting[2][1] +\n                         self.interverted_residual_setting[3][1] +\n                         self.interverted_residual_setting[4][1] +\n                         self.interverted_residual_setting[5][1] +\n                         self.interverted_residual_setting[6][1])\n\n        self.last_channel = int(org_last_chns * width_mult) if width_mult > 1.0 else org_last_chns\n        self.out_se = nn.Sequential(SCSEBlock(channel=self.last_channel, reduction=16))\n\n        if self.n_class != 0:\n            self.rfblock = nn.Sequential(RFBlock(in_chs=self.last_channel, out_chs=out_sec,\n                                                 scale=1.0, feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                                 up_ratio=2, norm_act=norm_act))\n\n            in_stag2_up_chs = self.interverted_residual_setting[1][1] + self.interverted_residual_setting[0][1]\n            self.score_se = nn.Sequential(SCSEBlock(channel=out_sec + in_stag2_up_chs, reduction=16))\n            self.score = nn.Sequential(OrderedDict([(""norm.1"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.1"", nn.Conv2d(out_sec + in_stag2_up_chs,\n                                                                         out_sec + in_stag2_up_chs,\n                                                                         kernel_size=3, stride=1, padding=2,\n                                                                         dilation=2, bias=False)),\n                                                    (""norm.2"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.2"", nn.Conv2d(out_sec + in_stag2_up_chs, self.n_class,\n                                                                         kernel_size=1, stride=1, padding=0,\n                                                                         bias=True)),\n                                                    (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n            Channel shuffle operation\n            :param x: input tensor\n            :param groups: split channels into groups\n            :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        stg1 = self.mod1(x)     # (N, 32,   224, 448)  1/2\n        stg1 = self.mod2(stg1)  # (N, 16,   224, 448)  1/2 -> 1/4 -> 1/8\n        stg2 = self.mod3(stg1)  # (N, 24,   112, 224)  1/4 -> 1/8\n        stg3 = self.mod4(stg2)  # (N, 32,   56,  112)  1/8\n        stg4 = self.mod5(stg3)  # (N, 64,   56,  112)  1/8 dilation=2\n        stg5 = self.mod6(stg4)  # (N, 96,   56,  112)  1/8 dilation=4\n        stg6 = self.mod7(stg5)  # (N, 160,  56,  112)  1/8 dilation=8\n        stg7 = self.mod8(stg6)  # (N, 320,  56,  112)  1/8 dilation=16\n\n        stg1_1 = F.max_pool2d(input=stg1, kernel_size=3, stride=2, ceil_mode=True)    # 1/4\n        stg1_2 = F.max_pool2d(input=stg1_1, kernel_size=3, stride=2, ceil_mode=True)  # 1/8\n        stg2_1 = F.max_pool2d(input=stg2, kernel_size=3, stride=2, ceil_mode=True)    # 1/8\n\n        # (N, 672, 56,  112)  1/8  (16+24+32+64+96+160+320)\n        stg8 = self.out_se(torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1))\n        # stg8 = torch.cat([stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1], dim=1)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.n_class != 0:\n            # (N, 672, H/8, W/8) -> (N, 256, H/4, W/4)\n            de_stg1 = self.rfblock(stg8)\n\n            # (N, 256+24+16=296, H/4, W/4)\n            de_stg1 = self.score_se(torch.cat([de_stg1, stg2, stg1_1], dim=1))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            net_out = self.score(de_stg1)\n\n            return net_out\n        else:\n            return stg8\n\n\nif __name__ == \'__main__\':\n    import os\n    import time\n    from scripts.loss import *\n    from torch.autograd import Variable\n\n    net_h, net_w = 384, 768\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    model = RFMobileNetV2Plus(n_class=19, in_size=(net_h, net_w), width_mult=1.0,\n                              out_sec=256, aspp_sec=(12, 24, 36),\n                              norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    model_dict = model.state_dict()\n\n    pre_weight = torch.load(""/zfs/zhang/TrainLog/weights/cityscapes_mobilenetv2_gtfine_best_model.pkl"")[""model_state""]\n    pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n    model_dict.update(pretrained_dict)\n    model.load_state_dict(model_dict)\n\n    del pre_weight\n    del model_dict\n    del pretrained_dict\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.90, weight_decay=5e-4)\n    loss_fn = bootstrapped_cross_entropy2d\n\n    i = 0\n    while True:\n        i += 1\n        print(""iter :"", i)\n        model.train()\n\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        dummy_target = Variable(torch.rand(1, net_h, net_w).cuda(), requires_grad=False).long()\n\n        start_time = time.time()\n        dummy_out = model(dummy_input)\n        print(""> Inference Time: {}"".format(time.time() - start_time))\n\n        optimizer.zero_grad()\n\n        topk = 512 * 256\n        loss = loss_fn(dummy_out, dummy_target, K=topk)\n        print(""> Loss: {}"".format(loss.data[0]))\n\n        loss.backward()\n        optimizer.step()\n\n'"
models/rfshufflenetv2plus.py,14,"b'import torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\nfrom modules import SCSEBlock, InPlaceABN, ASPPInPlaceABNBlock, InPlaceABNWrapper, RFBlock\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom torch.nn import init\n\n\ndef conv3x3(in_channels, out_channels, stride=1, bias=True, groups=1, dilate=1):\n    """"""3x3 convolution with padding\n    """"""\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n                     padding=dilate, bias=bias, groups=groups, dilation=dilate)\n\n\ndef conv1x1(in_channels, out_channels, groups=1):\n    """"""1x1 convolution with padding\n    - Normal pointwise convolution When groups == 1\n    - Grouped pointwise convolution when groups > 1\n    """"""\n    return nn.Conv2d(in_channels, out_channels, kernel_size=1, groups=groups, stride=1)\n\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n\n    channels_per_group = num_channels // groups\n\n    # reshape\n    x = x.view(batchsize, groups,\n               channels_per_group, height, width)\n\n    # transpose\n    # - contiguous() required if transpose() is used before view().\n    #   See https://github.com/pytorch/pytorch/issues/764\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n\n\nclass ShuffleUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3,\n                 dilate=1, grouped_conv=True, combine=\'add\', up=False):\n\n        super(ShuffleUnit, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.grouped_conv = grouped_conv\n        self.combine = combine\n        self.groups = groups\n        self.bottleneck_channels = self.out_channels // 4\n\n        # define the type of ShuffleUnit\n        if self.combine == \'add\':\n            # ShuffleUnit Figure 2b\n            self.depthwise_stride = 1\n            self.dilate = dilate\n            self.up = False\n            self._combine_func = self._add\n        elif self.combine == \'concat\':\n            # ShuffleUnit Figure 2c\n            self.depthwise_stride = 1 if up is True else 2\n            self.dilate = dilate if up is True else 1\n            self.up = up\n            self._combine_func = self._concat\n\n            # ensure output of concat has the same channels as\n            # original output channels.\n            self.out_channels -= self.in_channels\n        else:\n            raise ValueError(""Cannot combine tensors with \\""{}\\"""" \\\n                             ""Only \\""add\\"" and \\""concat\\"" are"" \\\n                             ""supported"".format(self.combine))\n\n        # Use a 1x1 grouped or non-grouped convolution to reduce input channels\n        # to bottleneck channels, as in a ResNet bottleneck module.\n        # NOTE: Do not use group convolution for the first conv1x1 in Stage 2.\n        self.first_1x1_groups = self.groups if grouped_conv else 1\n\n        self.g_conv_1x1_compress = self._make_grouped_conv1x1(self.in_channels,\n                                                              self.bottleneck_channels,\n                                                              self.first_1x1_groups,\n                                                              batch_norm=True,\n                                                              relu=True)\n\n        # 3x3 depthwise convolution followed by batch normalization\n        self.depthwise_conv3x3 = conv3x3(self.bottleneck_channels,\n                                         self.bottleneck_channels,\n                                         stride=self.depthwise_stride,\n                                         groups=self.bottleneck_channels,\n                                         dilate=self.dilate)\n        self.bn_after_depthwise = nn.BatchNorm2d(self.bottleneck_channels)\n\n        # Use 1x1 grouped convolution to expand from\n        # bottleneck_channels to out_channels\n        self.g_conv_1x1_expand = self._make_grouped_conv1x1(self.bottleneck_channels,\n                                                            self.out_channels,\n                                                            self.groups,\n                                                            batch_norm=True,\n                                                            relu=False)\n\n    @staticmethod\n    def _add(x, out):\n        # residual connection\n        return x + out\n\n    @staticmethod\n    def _concat(x, out):\n        # concatenate along channel axis\n        return torch.cat((x, out), dim=1)\n\n    def _make_grouped_conv1x1(self, in_channels, out_channels, groups,\n                              batch_norm=True, relu=False):\n\n        modules = OrderedDict()\n\n        conv = conv1x1(in_channels, out_channels, groups=groups)\n        modules[\'conv1x1\'] = conv\n\n        if batch_norm:\n            modules[\'batch_norm\'] = nn.BatchNorm2d(out_channels)\n        if relu:\n            modules[\'relu\'] = nn.ReLU()\n        if len(modules) > 1:\n            return nn.Sequential(modules)\n        else:\n            return conv\n\n    def forward(self, x):\n        # save for combining later with output\n        residual = x\n\n        if self.combine == \'concat\':\n            residual = F.avg_pool2d(residual, kernel_size=3, stride=2, padding=1)\n            if self.up is True:\n                residual = F.upsample(residual, scale_factor=2, mode=""bilinear"")\n\n        out = self.g_conv_1x1_compress(x)\n        out = channel_shuffle(out, self.groups)\n\n        out = self.depthwise_conv3x3(out)\n        out = self.bn_after_depthwise(out)\n\n        out = self.g_conv_1x1_expand(out)\n\n        out = self._combine_func(residual, out)\n        return F.relu(out)\n\n\nclass RFShuffleNetV2Plus(nn.Module):\n    """"""ShuffleNet implementation.\n    """"""\n\n    def __init__(self, n_class=19, groups=3, in_channels=3, in_size=(448, 896),\n                 out_sec=256, aspp_sec=(12, 24, 36), norm_act=InPlaceABN):\n        """"""ShuffleNet constructor.\n\n        Arguments:\n            groups (int, optional): number of groups to be used in grouped\n                1x1 convolutions in each ShuffleUnit. Default is 3 for best\n                performance according to original paper.\n            in_channels (int, optional): number of channels in the input tensor.\n                Default is 3 for RGB image inputs.\n            num_classes (int, optional): number of classes to predict. Default\n                is 19 for ImageNet.\n\n        """"""\n        super(RFShuffleNetV2Plus, self).__init__()\n\n        self.groups = groups\n        self.stage_repeats = [3, 7, 3]\n        self.in_channels = in_channels\n        self.n_class = n_class\n\n        # index 0 is invalid and should never be called.\n        # only used for indexing convenience.\n        if groups == 1:\n            self.stage_out_channels = [-1, 24, 144, 288, 567]\n        elif groups == 2:\n            self.stage_out_channels = [-1, 24, 200, 400, 800]\n        elif groups == 3:\n            self.stage_out_channels = [-1, 24, 240, 480, 960]\n        elif groups == 4:\n            self.stage_out_channels = [-1, 24, 272, 544, 1088]\n        elif groups == 8:\n            self.stage_out_channels = [-1, 24, 384, 768, 1536]\n        else:\n            raise ValueError(\n                """"""{} groups is not supported for\n                   1x1 Grouped Convolutions"""""".format(groups))\n\n        # Stage 1 always has 24 output channels\n        self.conv1 = conv3x3(self.in_channels,\n                             self.stage_out_channels[1],  # stage 1\n                             stride=2)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # Stage 2\n        self.stage2 = self._make_stage(2, dilate=2)\n        # Stage 3\n        self.stage3 = self._make_stage(3, dilate=4)\n        # Stage 4\n        self.stage4 = self._make_stage(4, dilate=8)\n\n        # building last several layers\n        self.last_channel = (2 * self.stage_out_channels[1] +\n                             self.stage_out_channels[2] +\n                             self.stage_out_channels[3] +\n                             self.stage_out_channels[4])\n        self.out_se = nn.Sequential(SCSEBlock(channel=self.last_channel, reduction=16))\n\n        if self.n_class != 0:\n            self.rfblock = nn.Sequential(RFBlock(in_chs=self.last_channel, out_chs=out_sec,\n                                                 scale=1.0, feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                                 up_ratio=2, aspp_sec=aspp_sec, norm_act=norm_act))\n\n            in_stag2_up_chs = 2 * self.stage_out_channels[1]\n            self.score_se = nn.Sequential(SCSEBlock(channel=out_sec + in_stag2_up_chs, reduction=16))\n            self.score = nn.Sequential(OrderedDict([(""norm.1"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.1"", nn.Conv2d(out_sec + in_stag2_up_chs,\n                                                                         out_sec + in_stag2_up_chs,\n                                                                         kernel_size=3, stride=1, padding=2,\n                                                                         dilation=2, bias=False)),\n                                                    (""norm.2"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.2"", nn.Conv2d(out_sec + in_stag2_up_chs, self.n_class,\n                                                                         kernel_size=1, stride=1, padding=0,\n                                                                         bias=True)),\n                                                    (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    init.constant(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant(m.weight, 1)\n                init.constant(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant(m.bias, 0)\n\n    def _make_stage(self, stage, dilate=1):\n        modules = OrderedDict()\n        stage_name = ""ShuffleUnit_Stage{}"".format(stage)\n\n        # First ShuffleUnit in the stage\n        # 1. non-grouped 1x1 convolution (i.e. pointwise convolution)\n        #   is used in Stage 2. Group convolutions used everywhere else.\n        grouped_conv = stage > 2\n\n        # 2. concatenation unit is always used.\n        if stage >= 3:\n            first_module = ShuffleUnit(self.stage_out_channels[stage - 1],\n                                       self.stage_out_channels[stage],\n                                       groups=self.groups,\n                                       dilate=dilate,\n                                       grouped_conv=grouped_conv,\n                                       combine=\'concat\',\n                                       up=True)\n        else:\n            first_module = ShuffleUnit(self.stage_out_channels[stage - 1],\n                                       self.stage_out_channels[stage],\n                                       groups=self.groups,\n                                       dilate=1,\n                                       grouped_conv=grouped_conv,\n                                       combine=\'concat\',\n                                       up=False)\n\n        modules[stage_name + ""_0""] = first_module\n\n        # add more ShuffleUnits depending on pre-defined number of repeats\n        for i in range(self.stage_repeats[stage - 2]):\n            name = stage_name + ""_{}"".format(i + 1)\n            module = ShuffleUnit(self.stage_out_channels[stage],\n                                 self.stage_out_channels[stage],\n                                 groups=self.groups,\n                                 dilate=dilate,\n                                 grouped_conv=True,\n                                 combine=\'add\', up=False)\n            modules[name] = module\n\n        return nn.Sequential(modules)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n            Channel shuffle operation\n            :param x: input tensor\n            :param groups: split channels into groups\n            :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        stg0 = self.conv1(x)       # [24, H/2, W/2]\n        stg1 = self.maxpool(stg0)  # [24, H/4, W/4]\n\n        stg2 = self.stage2(stg1)   # [240, H/8, W/8]\n        stg3 = self.stage3(stg2)   # [480, H/8, W/8]\n        stg4 = self.stage4(stg3)   # [960, H/8, W/8]\n\n        stg1_1 = F.avg_pool2d(input=stg0, kernel_size=3, stride=2, padding=1)    # 1/4\n        stg1_2 = F.avg_pool2d(input=stg1_1, kernel_size=3, stride=2, padding=1)  # 1/8\n        stg1_3 = F.max_pool2d(input=stg1, kernel_size=3, stride=2, padding=1)    # 1/8\n\n        # global average pooling layer\n        # (N, 24+24+240+480+960, 56,  112)  1/8\n        stg5 = self.out_se(torch.cat([stg2, stg3, stg4, stg1_2, stg1_3], dim=1))\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.n_class != 0:\n            # (N, 24+240+480+960, H/8, W/8) -> (N, 256, H/4, W/4)\n            de_stg1 = self.rfblock(stg5)\n\n            # (N, 256+24+24, H/4, W/4)\n            de_stg1 = self.score_se(torch.cat([de_stg1, stg1, stg1_1], dim=1))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            net_out = self.score(de_stg1)\n\n            return net_out\n        else:\n            return stg5\n\n\nif __name__ == ""__main__"":\n    import os\n    import time\n    from scripts.loss import *\n    from functools import partial\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n\n    net_h, net_w = 384, 768\n    model = RFShuffleNetV2Plus(n_class=19, groups=3, in_channels=3, in_size=(net_h, net_w),\n                               out_sec=256, aspp_sec=(12, 24, 36),\n                               norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1)).cuda()\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    model_dict = model.state_dict()\n\n    pre_weight = torch.load(""/zfs/zhang/TrainLog/weights/shufflenet.pth.tar"")\n    pre_weight = pre_weight[""state_dict""]\n\n    pretrained_dict = {""module."" + k: v for k, v in pre_weight.items() if ""module."" + k in model_dict}\n    model_dict.update(pretrained_dict)\n    model.load_state_dict(model_dict)\n\n    del pre_weight\n    del model_dict\n    del pretrained_dict\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.90, weight_decay=5e-4)\n    loss_fn = bootstrapped_cross_entropy2d\n\n    i = 0\n    while True:\n        i += 1\n        print(""iter :"", i)\n        model.train()\n\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        dummy_target = Variable(torch.rand(1, net_h, net_w).cuda(), requires_grad=False).long()\n\n        start_time = time.time()\n        dummy_out = model(dummy_input)\n        print(""> Inference Time: {}"".format(time.time() - start_time))\n\n        optimizer.zero_grad()\n\n        topk = 512 * 256\n        loss = loss_fn(dummy_out, dummy_target, K=topk)\n        print(""> Loss: {}"".format(loss.data[0]))\n\n        loss.backward()\n        optimizer.step()\n\n'"
models/sedpshufflenet.py,5,"b'import torch\nimport torch.nn as nn\n\nfrom collections import OrderedDict\nfrom modules import ABN, SEBlock, CatInPlaceABN, ASPPInPlaceABNBlock, DualPathInPlaceABNBlock\n\n\nclass SEDPNShuffleNet(nn.Module):\n    def __init__(self, small=False, classes=19, in_size=(448, 896), num_init_features=64,\n                 k_r=96, groups=4, k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n                 out_sec=(512, 256, 128), dil_sec=(1, 1, 1, 2, 4),\n                 aspp_sec=(7, 14, 21), norm_act=ABN):\n        super(SEDPNShuffleNet, self).__init__()\n        bw_factor = 1 if small else 4\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. conv1 (N, 3, W, H)->(N, 64, W/4, H/4)\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if small:\n            self.encode_in = nn.Sequential(OrderedDict([(""conv_in"", nn.Conv2d(3, num_init_features,\n                                                       kernel_size=3, stride=2,\n                                                       padding=1, bias=False)),\n                                                       (""bn_in"", norm_act(num_init_features)),\n                                                       (""pool_in"", nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))\n        else:\n            self.encode_in = nn.Sequential(OrderedDict([(""conv_in"", nn.Conv2d(3, num_init_features,\n                                                       kernel_size=7, stride=2,\n                                                       padding=3, bias=False)),\n                                                       (""bn_in"", norm_act(num_init_features)),\n                                                       (""pool_in"", nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. conv2 (N, 64, W/4, H/4)->(N, 336, W/4, H/4)\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        conv1x1c_ch = 64 * bw_factor                           # For 1x1c ch=64 OR 256 + inc\n        inc = inc_sec[0]                                       # For Dense ch=16\n        conv1x1a_ch = (k_r * conv1x1c_ch) // (64 * bw_factor)  # For 1x1a ch=96\n        conv3x3b_ch = conv1x1a_ch                              # For 3x3b ch=96\n\n        encode_blocks1 = OrderedDict()\n        encode_blocks1[\'conv2_1\'] = DualPathInPlaceABNBlock(num_init_features, conv1x1a_ch, conv3x3b_ch,\n                                                            conv1x1c_ch, inc, groups, dil_sec[0],\n                                                            \'proj\', norm_act=norm_act)\n\n        in_chs = conv1x1c_ch + 3 * inc                         # 96+3*16=144\n        for i in range(2, k_sec[0] + 1):\n            encode_blocks1[\'conv2_\' + str(i)] = DualPathInPlaceABNBlock(in_chs, conv1x1a_ch, conv3x3b_ch,\n                                                                        conv1x1c_ch, inc, groups, dil_sec[0],\n                                                                        \'normal\', norm_act=norm_act)\n            in_chs += inc\n\n        self.encode_stg1 = nn.Sequential(encode_blocks1)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 3. conv3 (N, 336, W/4, H/4)->(N, 704, W/8, H/8)\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        conv1x1c_ch = 128 * bw_factor                          # For 1x1c ch=128 OR 512 + inc\n        inc = inc_sec[1]                                       # For Dense ch=32\n        conv1x1a_ch = (k_r * conv1x1c_ch) // (64 * bw_factor)  # For 1x1a ch=192\n        conv3x3b_ch = conv1x1a_ch                              # For 3x3b ch=192\n\n        encode_blocks2 = OrderedDict()\n        encode_blocks2[\'conv3_1\'] = DualPathInPlaceABNBlock(in_chs, conv1x1a_ch, conv3x3b_ch,\n                                                            conv1x1c_ch, inc, groups, dil_sec[1],\n                                                            \'down\', norm_act=norm_act)\n\n        in_chs = conv1x1c_ch + 3 * inc\n        for i in range(2, k_sec[1] + 1):\n            encode_blocks2[\'conv3_\' + str(i)] = DualPathInPlaceABNBlock(in_chs, conv1x1a_ch, conv3x3b_ch,\n                                                                        conv1x1c_ch, inc, groups, dil_sec[1],\n                                                                        \'normal\', norm_act=norm_act)\n            in_chs += inc\n\n        self.encode_stg2 = nn.Sequential(encode_blocks2)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 4. conv4 (N, 704, W/8, H/8)->(N, 1552, W/16, H/16)\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        conv1x1c_ch = 256 * bw_factor                          # For 1x1c ch=256 OR 1024 + inc\n        inc = inc_sec[2]                                       # For Dense ch=24\n        conv1x1a_ch = (k_r * conv1x1c_ch) // (64 * bw_factor)  # For 1x1a ch=384\n        conv3x3b_ch = conv1x1a_ch                              # For 3x3b ch=384\n\n        encode_blocks3 = OrderedDict()\n        encode_blocks3[\'conv4_1\'] = DualPathInPlaceABNBlock(in_chs, conv1x1a_ch, conv3x3b_ch,\n                                                            conv1x1c_ch, inc, groups, dil_sec[2],\n                                                            \'down\', norm_act=norm_act)\n\n        in_chs = conv1x1c_ch + 3 * inc\n        for i in range(2, int(k_sec[2]/2) + 1):\n            encode_blocks3[\'conv4_\' + str(i)] = DualPathInPlaceABNBlock(in_chs, conv1x1a_ch, conv3x3b_ch,\n                                                                        conv1x1c_ch, inc, groups, dil_sec[2],\n                                                                        \'normal\', norm_act=norm_act)\n            in_chs += inc\n\n        for i in range(int(k_sec[2]/2) + 1, k_sec[2] + 1):\n            encode_blocks3[\'conv4_\' + str(i)] = DualPathInPlaceABNBlock(in_chs, conv1x1a_ch, conv3x3b_ch,\n                                                                        conv1x1c_ch, inc, groups, dil_sec[3],\n                                                                        \'normal\', norm_act=norm_act)\n            in_chs += inc\n\n        self.encode_stg3 = nn.Sequential(encode_blocks3)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 5. conv5 (N, 1552, W/16, H/16)->(N, 2688, W/16, H/16)\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        conv1x1c_ch = 512 * bw_factor                          # For 1x1c ch=512 OR 2048 + inc\n        inc = inc_sec[3]                                       # For Dense ch=128\n        conv1x1a_ch = (k_r * conv1x1c_ch) // (64 * bw_factor)  # For 1x1a ch=768\n        conv3x3b_ch = conv1x1a_ch                              # For 3x3b ch=768\n\n        encode_blocks4 = OrderedDict()\n        encode_blocks4[\'conv5_1\'] = DualPathInPlaceABNBlock(in_chs, conv1x1a_ch, conv3x3b_ch,\n                                                            conv1x1c_ch, inc, groups, dil_sec[4],\n                                                            \'proj\', norm_act=norm_act)\n\n        in_chs = conv1x1c_ch + 3 * inc\n        for i in range(2, k_sec[3] + 1):\n            encode_blocks4[\'conv5_\' + str(i)] = DualPathInPlaceABNBlock(in_chs, conv1x1a_ch, conv3x3b_ch,\n                                                                        conv1x1c_ch, inc, groups, dil_sec[4],\n                                                                        \'normal\', norm_act=norm_act)\n            in_chs += inc\n\n        encode_blocks4[\'conv5_bn_ac\'] = CatInPlaceABN(in_chs)\n        self.encode_stg4 = nn.Sequential(encode_blocks4)\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 6. ASPP #1 (N, 2688, W/16, H/16)->(N, 512, W/8, H/8)\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        self.aspp1 = nn.Sequential(OrderedDict([(""aspp1"", ASPPInPlaceABNBlock(in_chs, out_sec[0],\n                                                 feat_res=(int(in_size[0] / 16), int(in_size[1] / 16)),\n                                                 aspp_sec=aspp_sec, norm_act=norm_act))]))\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7. ASPP #2 (N, 1216, W/8, H/8)->(N, 256, W/4, H/4)\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        self.aspp2_in = nn.Sequential(OrderedDict([(""aspp2_in"", CatInPlaceABN(704, norm_act=norm_act))]))\n        self.aspp2 = nn.Sequential(OrderedDict([(""aspp2"", ASPPInPlaceABNBlock(out_sec[0]+704, out_sec[1],\n                                                 feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                                 aspp_sec=(aspp_sec[0]*2, aspp_sec[1]*2, aspp_sec[2]*2),\n                                                 norm_act=norm_act))]))\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 8. ASPP #3 (N, 592, W/4, H/4)->(N, 128, W/1, H/1)\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        self.aspp3_in = nn.Sequential(OrderedDict([(""aspp3_in"", CatInPlaceABN(336, norm_act=norm_act))]))\n        self.aspp3 = nn.Sequential(OrderedDict([(""aspp3"", ASPPInPlaceABNBlock(out_sec[1]+336, out_sec[2],\n                                                feat_res=(int(in_size[0] / 4), int(in_size[1] / 4)), up_ratio=4,\n                                                aspp_sec=(aspp_sec[0]*4, aspp_sec[1]*4, aspp_sec[2]*4),\n                                                norm_act=norm_act))]))\n\n        self.score1 = nn.Sequential(OrderedDict([(""score1"", nn.Conv2d(out_sec[0], classes,\n                                                                      kernel_size=1, stride=1, padding=0, bias=True)),\n                                                 (""se1_classes"", SEBlock(classes, 4)),\n                                                 (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n        self.score2 = nn.Sequential(OrderedDict([(""score2"", nn.Conv2d(out_sec[1], classes,\n                                                                      kernel_size=1, stride=1, padding=0, bias=True)),\n                                                (""se2_classes"", SEBlock(classes, 4)),\n                                                (""up2"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n        self.score3 = nn.Sequential(OrderedDict([(""score3"", nn.Conv2d(out_sec[2], classes,\n                                                                      kernel_size=1, stride=1, padding=0, bias=True)),\n                                                (""se3_classes"", SEBlock(classes, 4))]))\n\n        self.score4 = nn.Sequential(OrderedDict([(""score4_norm"", norm_act(classes)),\n                                                (""score4"", nn.Conv2d(classes, classes,\n                                                                     kernel_size=1, stride=1, padding=0, bias=True)),\n                                                (""se4_classes"", SEBlock(classes, 4))]))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n            Channel shuffle operation\n            :param x: input tensor\n            :param groups: split channels into groups\n            :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        en_in = self.encode_in(x)            # (N, 64, W/4, H/4)\n        en_stg1 = self.encode_stg1(en_in)    # (N, 336, W/4, H/4)\n        en_stg2 = self.encode_stg2(en_stg1)  # (N, 704, W/8, H/8)\n        en_stg3 = self.encode_stg3(en_stg2)  # (N, 1552, W/16, H/16)\n        en_stg4 = self.encode_stg4(en_stg3)  # (N, 2688, W/16, H/16)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        out_stg1, de_stg1 = self.aspp1(en_stg4)                                       # (N, 512, W/8, H/8)\n\n        # (N, 256, W/4, H/4)\n        out_stg2, de_stg2 = self.aspp2(self._channel_shuffle(torch.cat([de_stg1, self.aspp2_in(en_stg2)], dim=1), 2))\n\n        # (N, 128, W/1, H/1)\n        de_stg3 = self.aspp3(self._channel_shuffle(torch.cat([de_stg2, self.aspp3_in(en_stg1)], dim=1), 2))[1]\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 3. Classifier: pixel-wise classification-segmentation\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        out_stg1 = self.score1(out_stg1)\n        out_stg2 = self.score2(out_stg2)\n        out_stg3 = self.score3(de_stg3)\n        out_stg4 = self.score4(torch.max(torch.max(out_stg1, out_stg2), out_stg3))\n        return out_stg1, out_stg2, out_stg3, out_stg4\n'"
models/sewrnetv1.py,5,"b'import torch\nimport torch.nn as nn\n\nfrom functools import partial\nfrom collections import OrderedDict\nfrom modules import IdentityResidualBlock, SEBlock, ASPPInPlaceABNBlock, ABN\n\n\nclass SEWiderResNetV1(nn.Module):\n    def __init__(self, structure, norm_act=ABN, classes=0, dilation=False, is_se=True,\n                 in_size=(448, 896), out_sec=(512, 256, 128), aspp_sec=(12, 24, 36)):\n        """"""\n        Wider ResNet with pre-activation (identity mapping) and Squeeze & Excitation(SE) blocks\n\n        :param structure: (list of int) Number of residual blocks in each of the six modules of the network.\n        :param norm_act:  (callable) Function to create normalization / activation Module.\n        :param classes:   (int) Not `0` for segmentation task\n        :param dilation:  (bool) `True` for segmentation task\n        :param is_se:     (bool) Use Squeeze & Excitation (SE) or not\n        :param in_size:   (tuple of int) Size of the input image\n        :param out_sec:   (tuple of int) Number of channels of the ASPP output\n        :param aspp_sec:  (tuple of int) Dilation rate used in ASPP\n        """"""\n        super(SEWiderResNetV1, self).__init__()\n        self.structure = structure\n        self.dilation = dilation\n        self.classes = classes\n\n        if len(structure) != 6:\n            raise ValueError(""Expected a structure with six values"")\n\n        # Initial layers\n        self.mod1 = nn.Sequential(OrderedDict([\n            (""conv1"", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))\n        ]))\n\n        # Groups of residual blocks\n        in_channels = 64\n        channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048), (1024, 2048, 4096)]\n        for mod_id, num in enumerate(structure):\n            # Create blocks for module\n            blocks = []\n            for block_id in range(num):\n                if not dilation:\n                    dil = 1\n                    stride = 2 if block_id == 0 and 2 <= mod_id <= 4 else 1\n                else:\n                    if mod_id == 3:\n                        dil = 2\n                    elif mod_id == 4:\n                        dil = 4\n                    elif mod_id == 5:\n                        dil = 8\n                    else:\n                        dil = 1\n\n                    stride = 2 if block_id == 0 and mod_id == 2 else 1\n\n                if mod_id == 4:\n                    drop = partial(nn.Dropout2d, p=0.2)\n                elif mod_id == 5:\n                    drop = partial(nn.Dropout2d, p=0.3)\n                else:\n                    drop = None\n\n                blocks.append((\n                    ""block%d"" % (block_id + 1),\n                    IdentityResidualBlock(in_channels, channels[mod_id], norm_act=norm_act,\n                                          stride=stride, dilation=dil, dropout=drop, is_se=is_se)\n                ))\n\n                # Update channels and p_keep\n                in_channels = channels[mod_id][-1]\n\n            # Create module\n            if mod_id < 2:\n                self.add_module(""pool%d"" % (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n\n        # Pooling and predictor\n        self.bn_out = norm_act(in_channels)\n\n        if classes != 0:\n            self.aspp1 = nn.Sequential(OrderedDict([(""aspp1"", ASPPInPlaceABNBlock(channels[5][2], out_sec[0],\n                                                     feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                                     aspp_sec=aspp_sec))]))\n\n            self.aspp2 = nn.Sequential(OrderedDict([(""aspp2"", ASPPInPlaceABNBlock(out_sec[0]+256, out_sec[1],\n                                                     feat_res=(int(in_size[0] / 4), int(in_size[1] / 4)),\n                                                      aspp_sec=(aspp_sec[0]*2, aspp_sec[1]*2, aspp_sec[2]*2)))]))\n\n            self.aspp3 = nn.Sequential(OrderedDict([(""aspp3"", ASPPInPlaceABNBlock(out_sec[1]+128, out_sec[2],\n                                                    feat_res=(int(in_size[0] / 2), int(in_size[1] / 2)),\n                                                    aspp_sec=(aspp_sec[0]*4, aspp_sec[1]*4, aspp_sec[2]*4)))]))\n\n            self.score1 = nn.Sequential(OrderedDict([(""score1"", nn.Conv2d(out_sec[0], classes,\n                                                      kernel_size=1, stride=1, padding=0, bias=True)),\n                                                     (""se1_classes"", SEBlock(classes, 4)),\n                                                     (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n            self.score2 = nn.Sequential(OrderedDict([(""score2"", nn.Conv2d(out_sec[1], classes,\n                                                      kernel_size=1, stride=1, padding=0, bias=True)),\n                                                     (""se2_classes"", SEBlock(classes, 4)),\n                                                     (""up2"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n            self.score3 = nn.Sequential(OrderedDict([(""score3"", nn.Conv2d(out_sec[2], classes,\n                                                      kernel_size=1, stride=1, padding=0, bias=True)),\n                                                     (""se3_classes"", SEBlock(classes, 4))]))\n\n            self.score4 = nn.Sequential(OrderedDict([(""score4_norm"", norm_act(classes)),\n                                                     (""score4"", nn.Conv2d(classes, classes,\n                                                      kernel_size=1, stride=1, padding=0, bias=True)),\n                                                     (""se4_classes"", SEBlock(classes, 4))]))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n        Channel shuffle operation\n        :param x: input tensor\n        :param groups: split channels into groups\n        :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, img):\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        stg1 = self.mod1(img)               # (N, 64, 448, 896)   1/1\n        stg2 = self.mod2(self.pool2(stg1))  # (N, 128, 224, 448)  1/2                 3\n        stg3 = self.mod3(self.pool3(stg2))  # (N, 256, 112, 224)  1/4                 3\n        stg4 = self.mod4(stg3)              # (N, 512, 56, 112)   1/8 Stride=2        6\n        stg4 = self.mod5(stg4)              # (N, 1024, 56, 112)  1/8 dilation=2      3\n        stg4 = self.mod6(stg4)              # (N, 2048, 56, 112)  1/8 dilation=4      1\n        stg4 = self.mod7(stg4)              # (N, 4096, 56, 112)  1/8 dilation=8      1\n        stg4 = self.bn_out(stg4)            # (N, 4096, 56, 112)  1/8\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.classes != 0:\n            # (N, 4096, H/8, W/8) -> (N, 512, H/4, W/4)\n            out_stg1, de_stg1 = self.aspp1(stg4)\n\n            # (N, 768, H/4, W/4) -> (N, 256, W/2, H/2)\n            out_stg2, de_stg2 = self.aspp2(self._channel_shuffle(torch.cat([de_stg1, stg3], dim=1), 3))\n\n            # (N, 384, H/2, W/2) -> (N, 128, H/1, W/1)\n            de_stg3 = self.aspp3(self._channel_shuffle(torch.cat([de_stg2, stg2], dim=1), 3))[1]\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            out_stg1 = self.score1(out_stg1)\n            out_stg2 = self.score2(out_stg2)\n            out_stg3 = self.score3(de_stg3)\n            out_stg4 = self.score4(torch.max(torch.max(out_stg1, out_stg2), out_stg3))\n\n            return out_stg1, out_stg2, out_stg3, out_stg4\n        else:\n            return stg4\n\n\n_NETS = {\n    ""16"": {""structure"": [1, 1, 1, 1, 1, 1]},\n    ""20"": {""structure"": [1, 1, 1, 3, 1, 1]},\n    ""38"": {""structure"": [3, 3, 6, 3, 1, 1]},\n}\n'"
models/sewrnetv2.py,7,"b'import torch\nimport torch.nn as nn\n\nfrom functools import partial\nfrom collections import OrderedDict\nfrom modules import IdentityResidualBlock, ASPPInPlaceABNBlock, ABN, InPlaceABNWrapper\n\n\nclass SEWiderResNetV2(nn.Module):\n    def __init__(self, structure, norm_act=ABN, classes=0, dilation=True, is_se=True,\n                 in_size=(448, 896), aspp_out=512, fusion_out=64, aspp_sec=(12, 24, 36)):\n        """"""\n        Wider ResNet with pre-activation (identity mapping) and Squeeze & Excitation(SE) blocks\n\n        :param structure: (list of int) Number of residual blocks in each of the six modules of the network.\n        :param norm_act:  (callable) Function to create normalization / activation Module.\n        :param classes:   (int) Not `0` for segmentation task\n        :param dilation:  (bool) `True` for segmentation task\n        :param is_se:     (bool) Use Squeeze & Excitation (SE) or not\n        :param in_size:   (tuple of int) Size of the input image\n        :param out_sec:   (tuple of int) Number of channels of the ASPP output\n        :param aspp_sec:  (tuple of int) Dilation rate used in ASPP\n        """"""\n        super(SEWiderResNetV2, self).__init__()\n        self.structure = structure\n        self.dilation = dilation\n        self.classes = classes\n\n        if len(structure) != 6:\n            raise ValueError(""Expected a structure with six values"")\n\n        # Initial layers\n        self.mod1 = nn.Sequential(OrderedDict([\n            (""conv1"", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))\n        ]))\n\n        # Groups of residual blocks\n        in_channels = 64\n        channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048), (1024, 2048, 4096)]\n        for mod_id, num in enumerate(structure):\n            # Create blocks for module\n            blocks = []\n            for block_id in range(num):\n                if not dilation:\n                    dil = 1\n                    stride = 2 if block_id == 0 and 2 <= mod_id <= 4 else 1\n                else:\n                    if mod_id == 3:\n                        dil = 2\n                    elif mod_id == 4:\n                        dil = 4\n                    elif mod_id == 5:\n                        dil = 8\n                    else:\n                        dil = 1\n\n                    stride = 2 if block_id == 0 and mod_id == 2 else 1\n\n                if mod_id == 4:\n                    drop = partial(nn.Dropout2d, p=0.2)\n                elif mod_id == 5:\n                    drop = partial(nn.Dropout2d, p=0.3)\n                else:\n                    drop = None\n\n                blocks.append((\n                    ""block%d"" % (block_id + 1),\n                    IdentityResidualBlock(in_channels, channels[mod_id], norm_act=norm_act,\n                                          stride=stride, dilation=dil, dropout=drop, is_se=is_se)\n                ))\n\n                # Update channels and p_keep\n                in_channels = channels[mod_id][-1]\n\n            # Create module\n            if mod_id < 2:\n                self.add_module(""pool%d"" % (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n\n        # Pooling and predictor\n        # self.feat_out = nn.Sequential(OrderedDict([(""out_norm"", norm_act(in_channels)),\n        #                                            (""out_down"", nn.Conv2d(in_channels, 1024,\n        #                                                                   kernel_size=1, stride=1,\n        #                                                                   padding=0, bias=True))]))\n\n        self.bn_out = norm_act(in_channels)\n\n        if classes != 0:\n            self.stg3_fusion = nn.Conv2d(channels[1][1], fusion_out, kernel_size=1, stride=1, padding=0, bias=False)\n\n            self.aspp = nn.Sequential(OrderedDict([(""aspp"", ASPPInPlaceABNBlock(channels[5][2], aspp_out,\n                                                    feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                                    up_ratio=2, aspp_sec=aspp_sec))]))\n\n            self.score = nn.Sequential(OrderedDict([(""conv"", nn.Conv2d(aspp_out+fusion_out, classes,\n                                                                       kernel_size=3, stride=1,\n                                                                       padding=1, bias=True)),\n                                                    (""up"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n        Channel shuffle operation\n        :param x: input tensor\n        :param groups: split channels into groups\n        :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, img):\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        stg1 = self.mod1(img)               # (N, 64, 448, 896)   1/1\n        stg2 = self.mod2(self.pool2(stg1))  # (N, 128, 224, 448)  1/2                 3\n        stg3 = self.mod3(self.pool3(stg2))  # (N, 256, 112, 224)  1/4                 3\n        stg4 = self.mod4(stg3)              # (N, 512, 56, 112)   1/8 Stride=2        6\n        stg4 = self.mod5(stg4)              # (N, 1024, 56, 112)  1/8 dilation=2      3\n        stg4 = self.mod6(stg4)              # (N, 2048, 56, 112)  1/8 dilation=4      1\n        stg4 = self.mod7(stg4)              # (N, 4096, 56, 112)  1/8 dilation=8      1\n        stg4 = self.bn_out(stg4)            # (N, 4096, 56, 112)  1/8\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.classes != 0:\n            # (N, 4096, H/8, W/8) -> (N, 512, H/4, W/4)\n            de_stg1 = self.aspp(stg4)[1]\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            net_out = self.score(torch.cat([de_stg1, self.stg3_fusion(stg3)], dim=1))\n\n            return net_out\n        else:\n            return stg4\n\n\nif __name__ == \'__main__\':\n    import os\n    import time\n    from torch.autograd import Variable\n\n    pre_weight = torch.load(""/zfs/zhang/TrainLog/weights/wrnet_pretrained.pth.tar"")\n\n    dummy_in = Variable(torch.randn(1, 3, 448, 896), requires_grad=True)\n\n    model = SEWiderResNetV2(structure=[3, 3, 6, 3, 1, 1],\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1),\n                            classes=19, dilation=True, is_se=True, in_size=(448, 896),\n                            aspp_out=512, fusion_out=64, aspp_sec=(12, 24, 36))\n\n    model_dict = model.state_dict()\n\n    keys = list(pre_weight.keys())\n    keys.sort()\n    for k in keys:\n        if ""score"" in k:\n            pre_weight.pop(k)\n\n    state = {""model_state"": pre_weight}\n    torch.save(state, ""{}sewrnetv2_model.pkl"".format(""/zfs/zhang/TrainLog/weights/""))\n'"
models/shufflenetv2plus.py,15,"b'import torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\nfrom modules import SCSEBlock, InPlaceABN, ASPPInPlaceABNBlock, InPlaceABNWrapper\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom torch.nn import init\n\n\ndef conv3x3(in_channels, out_channels, stride=1, bias=True, groups=1, dilate=1):\n    """"""3x3 convolution with padding\n    """"""\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n                     padding=dilate, bias=bias, groups=groups, dilation=dilate)\n\n\ndef conv1x1(in_channels, out_channels, groups=1):\n    """"""1x1 convolution with padding\n    - Normal pointwise convolution When groups == 1\n    - Grouped pointwise convolution when groups > 1\n    """"""\n    return nn.Conv2d(in_channels, out_channels, kernel_size=1, groups=groups, stride=1)\n\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n\n    channels_per_group = num_channels // groups\n\n    # reshape\n    x = x.view(batchsize, groups,\n               channels_per_group, height, width)\n\n    # transpose\n    # - contiguous() required if transpose() is used before view().\n    #   See https://github.com/pytorch/pytorch/issues/764\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n\n\nclass ShuffleUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3,\n                 dilate=1, grouped_conv=True, combine=\'add\', up=False):\n\n        super(ShuffleUnit, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.grouped_conv = grouped_conv\n        self.combine = combine\n        self.groups = groups\n        self.bottleneck_channels = self.out_channels // 4\n\n        # define the type of ShuffleUnit\n        if self.combine == \'add\':\n            # ShuffleUnit Figure 2b\n            self.depthwise_stride = 1\n            self.dilate = dilate\n            self.up = False\n            self._combine_func = self._add\n        elif self.combine == \'concat\':\n            # ShuffleUnit Figure 2c\n            self.depthwise_stride = 1 if up is True else 2\n            self.dilate = dilate if up is True else 1\n            self.up = up\n            self._combine_func = self._concat\n\n            # ensure output of concat has the same channels as\n            # original output channels.\n            self.out_channels -= self.in_channels\n        else:\n            raise ValueError(""Cannot combine tensors with \\""{}\\"""" \\\n                             ""Only \\""add\\"" and \\""concat\\"" are"" \\\n                             ""supported"".format(self.combine))\n\n        # Use a 1x1 grouped or non-grouped convolution to reduce input channels\n        # to bottleneck channels, as in a ResNet bottleneck module.\n        # NOTE: Do not use group convolution for the first conv1x1 in Stage 2.\n        self.first_1x1_groups = self.groups if grouped_conv else 1\n\n        self.g_conv_1x1_compress = self._make_grouped_conv1x1(self.in_channels,\n                                                              self.bottleneck_channels,\n                                                              self.first_1x1_groups,\n                                                              batch_norm=True,\n                                                              relu=True)\n\n        # 3x3 depthwise convolution followed by batch normalization\n        self.depthwise_conv3x3 = conv3x3(self.bottleneck_channels,\n                                         self.bottleneck_channels,\n                                         stride=self.depthwise_stride,\n                                         groups=self.bottleneck_channels,\n                                         dilate=self.dilate)\n        self.bn_after_depthwise = nn.BatchNorm2d(self.bottleneck_channels)\n\n        # Use 1x1 grouped convolution to expand from\n        # bottleneck_channels to out_channels\n        self.g_conv_1x1_expand = self._make_grouped_conv1x1(self.bottleneck_channels,\n                                                            self.out_channels,\n                                                            self.groups,\n                                                            batch_norm=True,\n                                                            relu=False)\n\n    @staticmethod\n    def _add(x, out):\n        # residual connection\n        return x + out\n\n    @staticmethod\n    def _concat(x, out):\n        # concatenate along channel axis\n        return torch.cat((x, out), dim=1)\n\n    def _make_grouped_conv1x1(self, in_channels, out_channels, groups,\n                              batch_norm=True, relu=False):\n\n        modules = OrderedDict()\n\n        conv = conv1x1(in_channels, out_channels, groups=groups)\n        modules[\'conv1x1\'] = conv\n\n        if batch_norm:\n            modules[\'batch_norm\'] = nn.BatchNorm2d(out_channels)\n        if relu:\n            modules[\'relu\'] = nn.ReLU()\n        if len(modules) > 1:\n            return nn.Sequential(modules)\n        else:\n            return conv\n\n    def forward(self, x):\n        # save for combining later with output\n        residual = x\n\n        if self.combine == \'concat\':\n            residual = F.avg_pool2d(residual, kernel_size=3, stride=2, padding=1)\n            if self.up is True:\n                residual = F.upsample(residual, scale_factor=2, mode=""bilinear"")\n\n        out = self.g_conv_1x1_compress(x)\n        out = channel_shuffle(out, self.groups)\n\n        out = self.depthwise_conv3x3(out)\n        out = self.bn_after_depthwise(out)\n\n        out = self.g_conv_1x1_expand(out)\n\n        out = self._combine_func(residual, out)\n        return F.relu(out)\n\n\nclass ShuffleNetV2Plus(nn.Module):\n    """"""ShuffleNet implementation.\n    """"""\n\n    def __init__(self, n_class=19, groups=3, in_channels=3, in_size=(448, 896),\n                 out_sec=256, aspp_sec=(12, 24, 36), norm_act=InPlaceABN):\n        """"""ShuffleNet constructor.\n\n        Arguments:\n            groups (int, optional): number of groups to be used in grouped\n                1x1 convolutions in each ShuffleUnit. Default is 3 for best\n                performance according to original paper.\n            in_channels (int, optional): number of channels in the input tensor.\n                Default is 3 for RGB image inputs.\n            num_classes (int, optional): number of classes to predict. Default\n                is 19 for ImageNet.\n\n        """"""\n        super(ShuffleNetV2Plus, self).__init__()\n\n        self.groups = groups\n        self.stage_repeats = [3, 7, 3]\n        self.in_channels = in_channels\n        self.n_class = n_class\n\n        # index 0 is invalid and should never be called.\n        # only used for indexing convenience.\n        if groups == 1:\n            self.stage_out_channels = [-1, 24, 144, 288, 567]\n        elif groups == 2:\n            self.stage_out_channels = [-1, 24, 200, 400, 800]\n        elif groups == 3:\n            self.stage_out_channels = [-1, 24, 240, 480, 960]\n        elif groups == 4:\n            self.stage_out_channels = [-1, 24, 272, 544, 1088]\n        elif groups == 8:\n            self.stage_out_channels = [-1, 24, 384, 768, 1536]\n        else:\n            raise ValueError(\n                """"""{} groups is not supported for\n                   1x1 Grouped Convolutions"""""".format(groups))\n\n        # Stage 1 always has 24 output channels\n        self.conv1 = conv3x3(self.in_channels,\n                             self.stage_out_channels[1],  # stage 1\n                             stride=2)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # Stage 2\n        self.stage2 = self._make_stage(2, dilate=2)\n        # Stage 3\n        self.stage3 = self._make_stage(3, dilate=4)\n        # Stage 4\n        self.stage4 = self._make_stage(4, dilate=8)\n\n        # building last several layers\n        self.last_channel = (2 * self.stage_out_channels[1] +\n                             self.stage_out_channels[2] +\n                             self.stage_out_channels[3] +\n                             self.stage_out_channels[4])\n        self.out_se = nn.Sequential(SCSEBlock(channel=self.last_channel, reduction=16))\n\n        if self.n_class != 0:\n            self.aspp = nn.Sequential(ASPPInPlaceABNBlock(self.last_channel, out_sec,\n                                                          feat_res=(int(in_size[0] / 8), int(in_size[1] / 8)),\n                                                          aspp_sec=aspp_sec, norm_act=norm_act))\n\n            in_stag2_up_chs = 2 * self.stage_out_channels[1]\n            self.score_se = nn.Sequential(SCSEBlock(channel=out_sec + in_stag2_up_chs, reduction=16))\n            self.score = nn.Sequential(OrderedDict([(""norm.1"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.1"", nn.Conv2d(out_sec + in_stag2_up_chs,\n                                                                         out_sec + in_stag2_up_chs,\n                                                                         kernel_size=3, stride=1, padding=2,\n                                                                         dilation=2, bias=False)),\n                                                    (""norm.2"", norm_act(out_sec + in_stag2_up_chs)),\n                                                    (""conv.2"", nn.Conv2d(out_sec + in_stag2_up_chs, self.n_class,\n                                                                         kernel_size=1, stride=1, padding=0,\n                                                                         bias=True)),\n                                                    (""up1"", nn.Upsample(size=in_size, mode=\'bilinear\'))]))\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    init.constant(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant(m.weight, 1)\n                init.constant(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant(m.bias, 0)\n\n    def _make_stage(self, stage, dilate=1):\n        modules = OrderedDict()\n        stage_name = ""ShuffleUnit_Stage{}"".format(stage)\n\n        # First ShuffleUnit in the stage\n        # 1. non-grouped 1x1 convolution (i.e. pointwise convolution)\n        #   is used in Stage 2. Group convolutions used everywhere else.\n        grouped_conv = stage > 2\n\n        # 2. concatenation unit is always used.\n        if stage >= 3:\n            first_module = ShuffleUnit(self.stage_out_channels[stage - 1],\n                                       self.stage_out_channels[stage],\n                                       groups=self.groups,\n                                       dilate=dilate,\n                                       grouped_conv=grouped_conv,\n                                       combine=\'concat\',\n                                       up=True)\n        else:\n            first_module = ShuffleUnit(self.stage_out_channels[stage - 1],\n                                       self.stage_out_channels[stage],\n                                       groups=self.groups,\n                                       dilate=1,\n                                       grouped_conv=grouped_conv,\n                                       combine=\'concat\',\n                                       up=False)\n\n        modules[stage_name + ""_0""] = first_module\n\n        # add more ShuffleUnits depending on pre-defined number of repeats\n        for i in range(self.stage_repeats[stage - 2]):\n            name = stage_name + ""_{}"".format(i + 1)\n            module = ShuffleUnit(self.stage_out_channels[stage],\n                                 self.stage_out_channels[stage],\n                                 groups=self.groups,\n                                 dilate=dilate,\n                                 grouped_conv=True,\n                                 combine=\'add\', up=False)\n            modules[name] = module\n\n        return nn.Sequential(modules)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n            Channel shuffle operation\n            :param x: input tensor\n            :param groups: split channels into groups\n            :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        stg0 = self.conv1(x)       # [24, H/2, W/2]\n        stg1 = self.maxpool(stg0)  # [24, H/4, W/4]\n\n        stg2 = self.stage2(stg1)   # [240, H/8, W/8]\n        stg3 = self.stage3(stg2)   # [480, H/8, W/8]\n        stg4 = self.stage4(stg3)   # [960, H/8, W/8]\n\n        stg1_1 = F.avg_pool2d(input=stg0, kernel_size=3, stride=2, padding=1)    # 1/4\n        stg1_2 = F.avg_pool2d(input=stg1_1, kernel_size=3, stride=2, padding=1)  # 1/8\n        stg1_3 = F.max_pool2d(input=stg1, kernel_size=3, stride=2, padding=1)    # 1/8\n\n        # global average pooling layer\n        # (N, 24+24+240+480+960, 56,  112)  1/8\n        stg5 = self.out_se(torch.cat([stg2, stg3, stg4, stg1_2, stg1_3], dim=1))\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.n_class != 0:\n            # (N, 24+240+480+960, H/8, W/8) -> (N, 256, H/4, W/4)\n            de_stg1 = self.aspp(stg5)[1]\n\n            # (N, 256+24+24, H/4, W/4)\n            de_stg1 = self.score_se(torch.cat([de_stg1, stg1, stg1_1], dim=1))\n\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            # 3. Classifier: pixel-wise classification-segmentation\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n            net_out = self.score(de_stg1)\n\n            return net_out\n        else:\n            return stg5\n\n\nif __name__ == ""__main__"":\n    import os\n    import time\n    from scripts.loss import *\n    from functools import partial\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n\n    net_h, net_w = 384, 768\n    model = ShuffleNetV2Plus(n_class=19, groups=3, in_channels=3, in_size=(net_h, net_w),\n                             out_sec=256, aspp_sec=(12, 24, 36),\n                             norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1)).cuda()\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    model_dict = model.state_dict()\n\n    pre_weight = torch.load(""/zfs/zhang/TrainLog/weights/shufflenet.pth.tar"")\n    pre_weight = pre_weight[""state_dict""]\n\n    pretrained_dict = {""module."" + k: v for k, v in pre_weight.items() if ""module."" + k in model_dict}\n    model_dict.update(pretrained_dict)\n    state = {\'model_state\': model_dict}\n    torch.save(state, ""/zfs/zhang/TrainLog/weights/shufflenetv2plus_model.pkl"")\n    model.load_state_dict(model_dict)\n\n    del pre_weight\n    del model_dict\n    del pretrained_dict\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.90, weight_decay=5e-4)\n    loss_fn = bootstrapped_cross_entropy2d\n\n    i = 0\n    while True:\n        i += 1\n        print(""iter :"", i)\n        model.train()\n\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        dummy_target = Variable(torch.rand(1, net_h, net_w).cuda(), requires_grad=False).long()\n\n        start_time = time.time()\n        dummy_out = model(dummy_input)\n        print(""> Inference Time: {}"".format(time.time() - start_time))\n\n        optimizer.zero_grad()\n\n        topk = 512 * 256\n        loss = loss_fn(dummy_out, dummy_target, K=topk)\n        print(""> Loss: {}"".format(loss.data[0]))\n\n        loss.backward()\n        optimizer.step()\n\n'"
modules/__init__.py,0,"b'from .misc import GlobalAvgPool2d, CatInPlaceABN, ModifiedSCSEBlock, SCSEBlock, SEBlock, LightHeadBlock, VortexPooling\nfrom .misc import ASPPBlock, SDASPPInPlaceABNBlock, ASPPInPlaceABNBlock, InvertedResidual\nfrom .bn import ABN, InPlaceABN, InPlaceABNWrapper, InPlaceABNSync, InPlaceABNSyncWrapper\nfrom . context_encode import ContextEncodeInplaceABN, ContextEncodeDropInplaceABN\nfrom .dualpath import DualPathInPlaceABNBlock\nfrom .residual import IdentityResidualBlock\nfrom .dense import DenseModule, DPDenseModule\nfrom .rfblock import RFBlock'"
modules/bn.py,11,"b'from collections import OrderedDict, Iterable\nfrom itertools import repeat\n\ntry:\n    # python 3\n    from queue import Queue\nexcept ImportError:\n    # python 2\n    from Queue import Queue\n\nimport torch\nimport torch.nn as nn\nimport torch.autograd as autograd\n\nfrom .functions import inplace_abn, inplace_abn_sync\n\n\ndef _pair(x):\n    if isinstance(x, Iterable):\n        return x\n    return tuple(repeat(x, 2))\n\n\nclass ABN(nn.Sequential):\n    """"""Activated Batch Normalization\n\n    This gathers a `BatchNorm2d` and an activation function in a single module\n    """"""\n\n    def __init__(self, num_features, activation=nn.ReLU(inplace=True), **kwargs):\n        """"""Creates an Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        activation : nn.Module\n            Module used as an activation function.\n        kwargs\n            All other arguments are forwarded to the `BatchNorm2d` constructor.\n        """"""\n        super(ABN, self).__init__(OrderedDict([\n            (""bn"", nn.BatchNorm2d(num_features, **kwargs)),\n            (""act"", activation)\n        ]))\n\n\nclass InPlaceABN(nn.Module):\n    """"""InPlace Activated Batch Normalization""""""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an InPlace Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(InPlaceABN, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        self.activation = activation\n        self.slope = slope\n        if self.affine:\n            self.weight = nn.Parameter(torch.Tensor(num_features))\n            self.bias = nn.Parameter(torch.Tensor(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.running_mean.zero_()\n        self.running_var.fill_(1)\n        if self.affine:\n            self.weight.data.fill_(1)\n            self.bias.data.zero_()\n\n    def forward(self, x):\n        return inplace_abn(x, self.weight, self.bias, autograd.Variable(self.running_mean),\n                           autograd.Variable(self.running_var), self.training, self.momentum, self.eps,\n                           self.activation, self.slope)\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \' slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass InPlaceABNSync(nn.Module):\n    """"""InPlace Activated Batch Normalization with cross-GPU synchronization\n\n    This assumes that it will be replicated across GPUs using the same mechanism as in `nn.DataParallel`.\n    """"""\n\n    def __init__(self, num_features, devices=None, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"",\n                 slope=0.01):\n        """"""Creates a synchronized, InPlace Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        devices : list of int or None\n            IDs of the GPUs that will run the replicas of this module.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(InPlaceABNSync, self).__init__()\n        self.num_features = num_features\n        self.devices = devices if devices else list(range(torch.cuda.device_count()))\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        self.activation = activation\n        self.slope = slope\n        if self.affine:\n            self.weight = nn.Parameter(torch.Tensor(num_features))\n            self.bias = nn.Parameter(torch.Tensor(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n        self.reset_parameters()\n\n        # Initialize queues\n        self.worker_ids = self.devices[1:]\n        self.master_queue = Queue(len(self.worker_ids))\n        self.worker_queues = [Queue(1) for _ in self.worker_ids]\n\n    def reset_parameters(self):\n        self.running_mean.zero_()\n        self.running_var.fill_(1)\n        if self.affine:\n            self.weight.data.fill_(1)\n            self.bias.data.zero_()\n\n    def forward(self, x):\n        if x.get_device() == self.devices[0]:\n            # Master mode\n            extra = {\n                ""is_master"": True,\n                ""master_queue"": self.master_queue,\n                ""worker_queues"": self.worker_queues,\n                ""worker_ids"": self.worker_ids\n            }\n        else:\n            # Worker mode\n            extra = {\n                ""is_master"": False,\n                ""master_queue"": self.master_queue,\n                ""worker_queue"": self.worker_queues[self.worker_ids.index(x.get_device())]\n            }\n\n        return inplace_abn_sync(x, self.weight, self.bias, autograd.Variable(self.running_mean),\n                                autograd.Variable(self.running_var), extra, self.training, self.momentum, self.eps,\n                                self.activation, self.slope)\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, devices={devices}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \' slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass InPlaceABNWrapper(nn.Module):\n    """"""Wrapper module to make `InPlaceABN` compatible with `ABN`""""""\n\n    def __init__(self, *args, **kwargs):\n        super(InPlaceABNWrapper, self).__init__()\n        self.bn = InPlaceABN(*args, **kwargs)\n\n    def forward(self, input):\n        return self.bn(input)\n\n\nclass InPlaceABNSyncWrapper(nn.Module):\n    """"""Wrapper module to make `InPlaceABNSync` compatible with `ABN`""""""\n\n    def __init__(self, *args, **kwargs):\n        super(InPlaceABNSyncWrapper, self).__init__()\n        self.bn = InPlaceABNSync(*args, **kwargs)\n\n    def forward(self, input):\n        return self.bn(input)\n'"
modules/build.py,1,"b'import os\n\nfrom torch.utils.ffi import create_extension\n\nsources = [\'src/lib_cffi.cpp\']\nheaders = [\'src/lib_cffi.h\']\nextra_objects = [\'src/bn.o\']\nwith_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    \'_ext\',\n    headers=headers,\n    sources=sources,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects,\n    extra_compile_args=[""-std=c++11""]\n)\n\nif __name__ == \'__main__\':\n    ffi.build()\n'"
modules/context_encode.py,5,"b'from modules import InPlaceABN, ASPPInPlaceABNBlock, InPlaceABNWrapper\n\nimport torch.nn as nn\nimport encoding\nimport torch\n\n\nclass ContextEncodeInplaceABN(nn.Module):\n    def __init__(self, channel, K=16, reduction=4, norm_act=InPlaceABN):\n        super(ContextEncodeInplaceABN, self).__init__()\n        out_channel = int(channel / reduction)\n\n        self.pre_abn = norm_act(channel)\n        self.context_enc = nn.Sequential(norm_act(channel),\n                                         nn.Conv2d(channel, out_channel, kernel_size=1, stride=1, padding=0),\n                                         norm_act(out_channel),\n                                         encoding.nn.Encoding(D=out_channel, K=K),\n                                         encoding.nn.View(-1, out_channel*K),\n                                         encoding.nn.Normalize())\n\n        self.channel_se = nn.Sequential(nn.Linear(out_channel * K, channel), nn.Sigmoid())\n\n        self.spatial_se = nn.Sequential(nn.Conv2d(channel, 1, kernel_size=1,\n                                                  stride=1, padding=0, bias=False),\n                                        nn.Sigmoid())\n\n    def forward(self, x):\n        batch_size, num_channels, _, _ = x.size()\n\n        pre_x = self.pre_abn(x.clone())\n        encode = self.context_enc(pre_x)\n        chn_se = self.channel_se(encode).view(batch_size, num_channels, 1, 1)\n\n        spa_se = self.spatial_se(pre_x)\n\n        return encode, torch.mul(torch.mul(x, spa_se), chn_se)\n\n\nclass ContextEncodeDropInplaceABN(nn.Module):\n    def __init__(self, channel, K=16, reduction=4, norm_act=InPlaceABN):\n        super(ContextEncodeDropInplaceABN, self).__init__()\n        out_channel = int(channel / reduction)\n\n        self.pre_abn = norm_act(channel)\n        self.context_enc = nn.Sequential(nn.Conv2d(channel, out_channel, kernel_size=1,\n                                                   stride=1, padding=0),\n                                         norm_act(out_channel),\n                                         encoding.nn.EncodingDrop(D=out_channel, K=K),\n                                         encoding.nn.View(-1, out_channel*K),\n                                         encoding.nn.Normalize())\n\n        self.channel_se = nn.Sequential(nn.Linear(out_channel*K, channel), nn.Sigmoid())\n\n        self.spatial_se = nn.Sequential(nn.Conv2d(channel, 1, kernel_size=1,\n                                                  stride=1, padding=0, bias=False),\n                                        nn.Sigmoid())\n\n    def forward(self, x):\n        batch_size, num_channels, _, _ = x.size()\n\n        pre_x = self.pre_abn(x.clone())\n        encode = self.context_enc(pre_x)\n        chn_se = self.channel_se(encode).view(batch_size, num_channels, 1, 1)\n\n        spa_se = self.spatial_se(pre_x)\n\n        return encode, torch.mul(torch.mul(x, spa_se), chn_se)\n\n\nif __name__ == ""__main__"":\n    from functools import partial\n    from torch.autograd import Variable\n\n    B, C, H, W, K = 2, 32, 56, 56, 32\n    dummy_in = Variable(torch.randn(B, C, H, W).cuda(), requires_grad=True)\n\n    context = ContextEncodeInplaceABN(channel=C, K=K, reduction=4,\n                                      norm_act=partial(InPlaceABNWrapper,\n                                                       activation=""leaky_relu"",\n                                                       slope=0.1)).cuda()\n\n    enc, scored = context(dummy_in)\n    print(""ok!!!"")\n\n    context_drop = ContextEncodeDropInplaceABN(channel=C, K=K, reduction=4,\n                                               norm_act=partial(InPlaceABNWrapper,\n                                                                activation=""leaky_relu"",\n                                                                slope=0.1)).cuda()\n\n    enc, scored = context(dummy_in)\n    print(""ok!!!"")\n'"
modules/dense.py,5,"b'from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\nfrom .bn import ABN\n\n\nclass DenseModule(nn.Module):\n    def __init__(self, in_chns, squeeze_ratio, out_chns, n_layers, dilate_sec=(1, 2, 4, 8, 16), norm_act=ABN):\n        super(DenseModule, self).__init__()\n        self.n_layers = n_layers\n        self.mid_out = int(in_chns * squeeze_ratio)\n\n        self.convs1 = nn.ModuleList()\n        self.convs3 = nn.ModuleList()\n\n        for idx in range(self.n_layers):\n            dilate = dilate_sec[idx % len(dilate_sec)]\n            self.last_channel = in_chns + idx * out_chns\n\n            """"""\n            self.convs1.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(self.last_channel)),\n                (""conv"", nn.Conv2d(self.last_channel, self.mid_out, 1, bias=False))\n            ])))\n            """"""\n\n            self.convs3.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(self.last_channel)),\n                (""conv"", nn.Conv2d(self.last_channel, out_chns, kernel_size=3, stride=1,\n                                   padding=dilate, dilation=dilate, bias=False))\n            ])))\n\n    @property\n    def out_channels(self):\n        return self.last_channel + 1\n\n    def forward(self, x):\n        inputs = [x]\n        for i in range(self.n_layers):\n            x = torch.cat(inputs, dim=1)\n            # x = self.convs1[i](x)\n            x = self.convs3[i](x)\n            inputs += [x]\n\n        return torch.cat(inputs, dim=1)\n\n\nclass DPDenseModule(nn.Module):\n    def __init__(self, in_chns, squeeze_ratio, out_chns, n_layers, dilate_sec=(1, 2, 4, 8, 16), norm_act=ABN):\n        super(DPDenseModule, self).__init__()\n        self.n_layers = n_layers\n        self.convs3 = nn.ModuleList()\n\n        for idx in range(self.n_layers):\n            dilate = dilate_sec[idx % len(dilate_sec)]\n            self.last_channel = in_chns + idx * out_chns\n            mid_out = int(self.last_channel * squeeze_ratio)\n\n            self.convs3.append(nn.Sequential(OrderedDict([(""bn.1"", norm_act(self.last_channel)),\n                                                          (""conv_up"", nn.Conv2d(self.last_channel, mid_out,\n                                                                                kernel_size=1, stride=1, padding=0,\n                                                                                bias=False)),\n                                                          (""bn.2"", norm_act(mid_out)),\n                                                          (""dconv"", nn.Conv2d(mid_out, mid_out,\n                                                                              kernel_size=3, stride=1, padding=dilate,\n                                                                              groups=mid_out, dilation=dilate,\n                                                                              bias=False)),\n                                                          (""pconv"", nn.Conv2d(mid_out, out_chns,\n                                                                              kernel_size=1, stride=1, padding=0,\n                                                                              bias=False)),\n                                                          (""dropout"", nn.Dropout2d(p=0.2, inplace=True))])))\n            """"""\n            self.convs3.append(nn.Sequential(OrderedDict([(""bn.1"", norm_act(self.last_channel)),\n                                                          (""dconv"", nn.Conv2d(self.last_channel, self.last_channel,\n                                                                              kernel_size=3, stride=1, padding=dilate,\n                                                                              groups=self.last_channel, dilation=dilate,\n                                                                              bias=False)),\n                                                          (""pconv"", nn.Conv2d(self.last_channel, out_chns,\n                                                                              kernel_size=1, stride=1, padding=0,\n                                                                              bias=False)),\n                                                          (""dropout"", nn.Dropout2d(p=0.2, inplace=True))])))\n            """"""\n    @property\n    def out_channels(self):\n        return self.last_channel + 1\n\n    def forward(self, x):\n        inputs = [x]\n        for i in range(self.n_layers):\n            x = torch.cat(inputs, dim=1)\n            x = self.convs3[i](x)\n            inputs += [x]\n\n        return torch.cat(inputs, dim=1)\n\n'"
modules/dualpath.py,7,"b'import torch\nimport torch.nn as nn\n\nfrom .bn import ABN\nfrom .misc import SEBlock\nfrom collections import OrderedDict\n\n\nclass DualPathInPlaceABNBlock(nn.Module):\n    def __init__(self, in_chs, num_1x1_a, num_3x3_b, num_1x1_c, inc,\n                 groups=1, dilation=1, block_type=\'normal\', norm_act=ABN):\n        super(DualPathInPlaceABNBlock, self).__init__()\n\n        self.num_1x1_c = num_1x1_c\n        self.dilation = dilation\n        self.groups = groups\n        self.inc = inc\n\n        if block_type is \'proj\':\n            self.key_stride = 1\n            self.has_proj = True\n        elif block_type is \'down\':\n            self.key_stride = 2\n            self.has_proj = True\n        else:\n            assert block_type is \'normal\'\n            self.key_stride = 1\n            self.has_proj = False\n\n        if self.has_proj:\n            # Using different member names here to allow easier parameter key matching for conversion\n            if self.key_stride == 2:\n                self.c1x1_w_s2 = nn.Sequential(OrderedDict([(""conv1x1_w_s2_bn"", norm_act(in_chs)),\n                                                            (""conv1x1_w_s2"", nn.Conv2d(in_chs, num_1x1_c + 2 * inc,\n                                                             kernel_size=1, stride=2, padding=0, groups=self.groups,\n                                                             dilation=1, bias=False))]))\n            else:\n                self.c1x1_w_s1 = nn.Sequential(OrderedDict([(""conv1x1_w_s1_bn"", norm_act(in_chs)),\n                                                            (""conv1x1_w_s1"", nn.Conv2d(in_chs, num_1x1_c + 2 * inc,\n                                                             kernel_size=1, stride=1, padding=0, groups=self.groups,\n                                                             dilation=1, bias=False))]))\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. 1x1 group point-wise convolution\n        # Use a 1x1 grouped or non-grouped convolution to reduce input channels\n        # to bottleneck channels, as in a ResNet bottleneck module.\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        self.c1x1_a = nn.Sequential(OrderedDict([(""conv1x1_a_bn"", norm_act(in_chs)),\n                                                 (""conv1x1_a"", nn.Conv2d(in_chs, num_1x1_a,\n                                                  kernel_size=1, stride=1, padding=0, groups=self.groups,\n                                                  dilation=1, bias=False))]))\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. 3x3 depth-wise convolution\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        if self.dilation > 1 and self.key_stride == 1:\n            self.c3x3_b = nn.Sequential(OrderedDict([(""conv3x3_b_bn"", norm_act(num_1x1_a)),\n                                                     (""conv3x3_b"", nn.Conv2d(num_1x1_a, num_3x3_b,\n                                                      kernel_size=3, stride=1, padding=dilation,\n                                                      groups=num_1x1_a, dilation=self.dilation, bias=False))]))\n        else:\n            self.c3x3_b = nn.Sequential(OrderedDict([(""conv3x3_b_bn"", norm_act(num_1x1_a)),\n                                                     (""conv3x3_b"", nn.Conv2d(num_1x1_a, num_3x3_b,\n                                                      kernel_size=3, stride=self.key_stride, padding=1,\n                                                      groups=num_1x1_a, dilation=1, bias=False))]))\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 3. 1x1 group point-wise convolution\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        self.c1x1_c = nn.Sequential(OrderedDict([(""conv1x1_c_bn"", norm_act(num_3x3_b)),\n                                                 (""conv1x1_c"", nn.Conv2d(num_3x3_b, num_1x1_c + inc,\n                                                  kernel_size=1, stride=1, padding=0,\n                                                  groups=self.groups, dilation=1, bias=False)),\n                                                 (""se_block"", SEBlock(num_1x1_c + inc, 16)),\n                                                 (""dropout"", nn.Dropout2d(p=0.2, inplace=True))]))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n            Channel shuffle operation\n            :param x: input tensor\n            :param groups: split channels into groups\n            :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        x_in = torch.cat(x, dim=1) if isinstance(x, tuple) else x\n\n        if self.has_proj:\n            if self.key_stride == 2:\n                x_s = self.c1x1_w_s2(x_in.clone())\n            else:\n                x_s = self.c1x1_w_s1(x_in.clone())\n\n            x_s = self._channel_shuffle(x_s, self.groups)          # shuffle channels in group\n            x_s = torch.split(x_s, self.num_1x1_c, dim=1)    # split channels for res and dense\n        else:\n            x_s = x\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. 1x1 group point-wise convolution\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        x_r = self.c1x1_a(x_in)\n        x_r = self._channel_shuffle(x_r, self.groups)  # shuffle channels in group\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. 3x3 depth-wise convolution\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        x_r = self.c3x3_b(x_r)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 3. 1x1 group point-wise convolution\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        x_r = self.c1x1_c(x_r)\n        x_r = self._channel_shuffle(x_r, self.groups)  # shuffle channels in group\n        x_r = torch.split(x_r, self.num_1x1_c, dim=1)  # split channels for res and dense\n\n        resid = torch.add(x_s[0], 1, x_r[0])         # for res-net\n        dense = torch.cat([x_s[1], x_r[1]], dim=1)   # for dense-net\n        return resid, dense\n\n'"
modules/exfuse.py,3,"b'import torch.nn as nn\nimport torch\n\nfrom collections import OrderedDict\n\n\nclass SemanticSupervision(nn.Module):\n    def __init__(self, in_chns, out_chns):\n        super(SemanticSupervision, self).__init__()\n        self.out_chns = out_chns\n\n        self.semantic = nn.Sequential(OrderedDict([(""conv1x7"", nn.Conv2d(in_chns, (in_chns // 2) * 3,\n                                                                         kernel_size=(1, 7), stride=1,\n                                                                         padding=(0, 3), bias=False)),\n                                                   (""norm1"", nn.BatchNorm2d((in_chns // 2) * 3)),\n                                                   (""act1"", nn.LeakyReLU(negative_slope=0.1, inplace=True)),\n                                                   (""conv7x1"", nn.Conv2d((in_chns // 2) * 3, (out_chns // 2) * 3,\n                                                                         kernel_size=(7, 1), stride=1,\n                                                                         padding=(3, 0), bias=False)),\n                                                   (""norm2"", nn.BatchNorm2d((out_chns // 2) * 3)),\n                                                   (""act2"", nn.LeakyReLU(negative_slope=0.1, inplace=True))]))\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear((out_chns // 2) * 3, out_chns)\n\n    def forward(self, x):\n        bahs, chs, _, _ = x.size()\n        se = self.semantic(x)\n\n        se = self.avg_pool(se).view(bahs, (self.out_chns // 2) * 3)\n        se = self.classifier(se)\n        return se\n\n\nif __name__ == ""__main__"":\n    from torch.autograd import Variable\n\n    dummy_input = Variable(torch.rand(1, 16, 224, 448), requires_grad=True)\n\n    sesuper = SemanticSupervision(in_chns=16, out_chns=19)\n\n    se = sesuper(dummy_input)\n\n    print(""ok!!"")'"
modules/functions.py,3,"b'import torch.autograd as autograd\nimport torch.cuda.comm as comm\nfrom torch.autograd.function import once_differentiable\n\nfrom . import _ext\n\n# Activation names\nACT_LEAKY_RELU = ""leaky_relu""\nACT_ELU = ""elu""\nACT_NONE = ""none""\n\n\ndef _broadcast_shape(x):\n    out_size = []\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            out_size.append(1)\n        else:\n            out_size.append(s)\n    return out_size\n\n\ndef _reduce(x):\n    if len(x.size()) == 2:\n        return x.sum(dim=0)\n    else:\n        n, c = x.size()[0:2]\n        return x.contiguous().view((n, c, -1)).sum(2).sum(0)\n\n\ndef _count_samples(x):\n    count = 1\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            count *= s\n    return count\n\n\ndef _act_forward(ctx, x):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _ext.leaky_relu_cuda(x, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _ext.elu_cuda(x)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\ndef _act_backward(ctx, x, dx):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _ext.leaky_relu_backward_cuda(x, dx, ctx.slope)\n        _ext.leaky_relu_cuda(x, 1. / ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _ext.elu_backward_cuda(x, dx)\n        _ext.elu_inv_cuda(x)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\ndef _check_contiguous(*args):\n    if not all([mod is None or mod.is_contiguous() for mod in args]):\n        raise ValueError(""Non-contiguous input"")\n\n\nclass InPlaceABN(autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n\n        n = _count_samples(x)\n\n        if ctx.training:\n            mean = x.new().resize_as_(running_mean)\n            var = x.new().resize_as_(running_var)\n            _check_contiguous(x, mean, var)\n            _ext.bn_mean_var_cuda(x, mean, var)\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * n / (n - 1))\n        else:\n            mean, var = running_mean, running_var\n\n        _check_contiguous(x, mean, var, weight, bias)\n        _ext.bn_forward_cuda(\n            x, mean, var,\n            weight if weight is not None else x.new(),\n            bias if bias is not None else x.new(),\n            x, x, ctx.eps)\n\n        # Activation\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, weight, bias, running_mean, running_var)\n        ctx.mark_dirty(x)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, weight, bias, running_mean, running_var = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.needs_input_grad[0]:\n            dx = dz.new().resize_as_(dz)\n        else:\n            dx = None\n\n        if ctx.needs_input_grad[1]:\n            dweight = dz.new().resize_as_(running_mean).zero_()\n        else:\n            dweight = None\n\n        if ctx.needs_input_grad[2]:\n            dbias = dz.new().resize_as_(running_mean).zero_()\n        else:\n            dbias = None\n\n        if ctx.training:\n            edz = dz.new().resize_as_(running_mean)\n            eydz = dz.new().resize_as_(running_mean)\n            _check_contiguous(z, dz, weight, bias, edz, eydz)\n            _ext.bn_edz_eydz_cuda(\n                z, dz,\n                weight if weight is not None else dz.new(),\n                bias if bias is not None else dz.new(),\n                edz, eydz)\n        else:\n            # TODO: implement CUDA backward for inference mode\n            edz = dz.new().resize_as_(running_mean).zero_()\n            eydz = dz.new().resize_as_(running_mean).zero_()\n\n        _check_contiguous(dz, z, ctx.var, weight, bias, edz, eydz, dx, dweight, dbias)\n        _ext.bn_backard_cuda(\n            dz, z, ctx.var,\n            weight if weight is not None else dz.new(),\n            bias if bias is not None else dz.new(),\n            edz, eydz,\n            dx if dx is not None else dz.new(),\n            dweight if dweight is not None else dz.new(),\n            dbias if dbias is not None else dz.new(),\n            ctx.eps)\n\n        del ctx.var\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\n\nclass InPlaceABNSync(autograd.Function):\n    @classmethod\n    def forward(cls, ctx, x, weight, bias, running_mean, running_var,\n                extra, training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        cls._parse_extra(ctx, extra)\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n\n        n = _count_samples(x) * (ctx.master_queue.maxsize + 1)\n\n        if ctx.training:\n            mean = x.new().resize_(1, running_mean.size(0))\n            var = x.new().resize_(1, running_var.size(0))\n            _check_contiguous(x, mean, var)\n            _ext.bn_mean_var_cuda(x, mean, var)\n\n            if ctx.is_master:\n                means, vars = [mean], [var]\n                for _ in range(ctx.master_queue.maxsize):\n                    mean_w, var_w = ctx.master_queue.get()\n                    ctx.master_queue.task_done()\n                    means.append(mean_w)\n                    vars.append(var_w)\n\n                means = comm.gather(means)\n                vars = comm.gather(vars)\n\n                mean = means.mean(0)\n                var = (vars + (mean - means) ** 2).mean(0)\n\n                tensors = comm.broadcast_coalesced((mean, var), [mean.get_device()] + ctx.worker_ids)\n                for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                    queue.put(ts)\n            else:\n                ctx.master_queue.put((mean, var))\n                mean, var = ctx.worker_queue.get()\n                ctx.worker_queue.task_done()\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * n / (n - 1))\n        else:\n            mean, var = running_mean, running_var\n\n        _check_contiguous(x, mean, var, weight, bias)\n        _ext.bn_forward_cuda(\n            x, mean, var,\n            weight if weight is not None else x.new(),\n            bias if bias is not None else x.new(),\n            x, x, ctx.eps)\n\n        # Activation\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, weight, bias, running_mean, running_var)\n        ctx.mark_dirty(x)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, weight, bias, running_mean, running_var = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.needs_input_grad[0]:\n            dx = dz.new().resize_as_(dz)\n        else:\n            dx = None\n\n        if ctx.needs_input_grad[1]:\n            dweight = dz.new().resize_as_(running_mean).zero_()\n        else:\n            dweight = None\n\n        if ctx.needs_input_grad[2]:\n            dbias = dz.new().resize_as_(running_mean).zero_()\n        else:\n            dbias = None\n\n        if ctx.training:\n            edz = dz.new().resize_as_(running_mean)\n            eydz = dz.new().resize_as_(running_mean)\n            _check_contiguous(z, dz, weight, bias, edz, eydz)\n            _ext.bn_edz_eydz_cuda(\n                z, dz,\n                weight if weight is not None else dz.new(),\n                bias if bias is not None else dz.new(),\n                edz, eydz)\n\n            if ctx.is_master:\n                edzs, eydzs = [edz], [eydz]\n                for _ in range(len(ctx.worker_queues)):\n                    edz_w, eydz_w = ctx.master_queue.get()\n                    ctx.master_queue.task_done()\n                    edzs.append(edz_w)\n                    eydzs.append(eydz_w)\n\n                edz = comm.reduce_add(edzs) / (ctx.master_queue.maxsize + 1)\n                eydz = comm.reduce_add(eydzs) / (ctx.master_queue.maxsize + 1)\n\n                tensors = comm.broadcast_coalesced((edz, eydz), [edz.get_device()] + ctx.worker_ids)\n                for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                    queue.put(ts)\n            else:\n                ctx.master_queue.put((edz, eydz))\n                edz, eydz = ctx.worker_queue.get()\n                ctx.worker_queue.task_done()\n        else:\n            edz = dz.new().resize_as_(running_mean).zero_()\n            eydz = dz.new().resize_as_(running_mean).zero_()\n\n        _check_contiguous(dz, z, ctx.var, weight, bias, edz, eydz, dx, dweight, dbias)\n        _ext.bn_backard_cuda(\n            dz, z, ctx.var,\n            weight if weight is not None else dz.new(),\n            bias if bias is not None else dz.new(),\n            edz, eydz,\n            dx if dx is not None else dz.new(),\n            dweight if dweight is not None else dz.new(),\n            dbias if dbias is not None else dz.new(),\n            ctx.eps)\n\n        del ctx.var\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None, None\n\n    @staticmethod\n    def _parse_extra(ctx, extra):\n        ctx.is_master = extra[""is_master""]\n        if ctx.is_master:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queues = extra[""worker_queues""]\n            ctx.worker_ids = extra[""worker_ids""]\n        else:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queue = extra[""worker_queue""]\n\n\ninplace_abn = InPlaceABN.apply\ninplace_abn_sync = InPlaceABNSync.apply\n\n__all__ = [""inplace_abn"", ""inplace_abn_sync""]\n'"
modules/group_norm.py,5,"b'import torch\nimport torch.nn as nn\nimport tensorflow as tf\nfrom torch.autograd import Variable\n\n\ndef tf_group_norm(x, gamma, beta, groups, eps=1e-5):\n    # x: input features with shape [N,C,H,W]\n    # gamma, beta: scale and offset, with shape [1,C,1,1]\n    # G: number of groups for GN\n    batch_size, num_channels, height, width = x.shape\n    channels_per_group = num_channels // groups\n    x = tf.reshape(x, [batch_size, groups, channels_per_group, height, width])\n    mean, var = tf.nn.moments(x, axes=[2, 3, 4], keep_dims=True)\n    x = (x - mean) / tf.sqrt(var + eps)\n    x = tf.reshape(x, [batch_size, num_channels, height, width])\n    return x * gamma + beta\n\n\nclass GroupNorm2D(nn.Module):\n    """"""\n        Group Normalization\n        Reference: https://128.84.21.199/abs/1803.08494v1\n    """"""\n    def __init__(self, num_features, num_groups=32, eps=1e-5):\n        """"""\n\n        :param num_features:\n        :param num_groups:\n        :param eps:\n        """"""\n        super(GroupNorm2D, self).__init__()\n        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1))\n        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n        self.num_groups = num_groups\n        self.eps = eps\n\n    def forward(self, x):\n        # x: input features with shape [N, C, H, W]\n        # weight, bias: scale and offset, with shape [1, C, 1, 1]\n        # groups: number of groups for GroupNorm\n\n        batch_size, num_channels, height, width = x.size()\n        assert num_channels % self.num_groups == 0\n\n        x = x.view(batch_size, self.num_groups, -1)\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True)\n\n        x = (x-mean) / (var+self.eps).sqrt()\n        x = x.view(batch_size, num_channels, height, width)\n        return x * self.weight + self.bias\n\n\nif __name__ == ""__main__"":\n    dummy_in = Variable(torch.randn(1, 32, 448, 896).cuda(), requires_grad=True)\n\n    group_norm = GroupNorm2D(num_features=32, num_groups=8, eps=1e-5)\n    dummy_out = group_norm(dummy_in)\n\n    dummy_in = tf.random_normal([1, 32, 448, 896], mean=-1, stddev=4)\n    gamma = tf.random_normal([1, 32, 1, 1], mean=-1, stddev=4)\n    beta = tf.random_normal([1, 32, 1, 1], mean=-1, stddev=4)\n\n    dummy_out = tf_group_norm(dummy_in, gamma, beta, 8, eps=1e-5)'"
modules/misc.py,17,"b'import torch\nimport torch.nn as nn\nfrom .bn import ABN\nfrom collections import OrderedDict\n\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        """"""Global average pooling over the input\'s spatial dimensions""""""\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        in_size = inputs.size()\n        inputs = inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n        return inputs.view((in_size[0], in_size[1], 1, 1))\n\n\n# +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# InPlace Activated BatchNorm\n# +++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass CatInPlaceABN(nn.Module):\n    """"""\n    Block for concat the two output tensor of feature net\n    """"""\n    def __init__(self, in_chs, norm_act=ABN):\n\n        super(CatInPlaceABN, self).__init__()\n        self.norm_act = norm_act(in_chs)\n\n    def forward(self, x):\n        x = torch.cat(x, dim=1) if isinstance(x, tuple) else x\n        x = self.norm_act(x)\n        return x\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# InplaceABNConv for Large Separable Convolution Block\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass LightHeadBlock(nn.Module):\n    def __init__(self, in_chs, mid_chs=256, out_chs=256, kernel_size=15, norm_act=ABN):\n        super(LightHeadBlock, self).__init__()\n        pad = int((kernel_size - 1) / 2)\n\n        # kernel size had better be odd number so as to avoid alignment error\n        self.abn = norm_act(in_chs)\n        self.conv_l = nn.Sequential(OrderedDict([(""conv_lu"", nn.Conv2d(in_chs, mid_chs,\n                                                                       kernel_size=(kernel_size, 1),\n                                                                       padding=(pad, 0))),\n                                                 (""conv_ld"", nn.Conv2d(mid_chs, out_chs,\n                                                                       kernel_size=(1, kernel_size),\n                                                                       padding=(0, pad)))]))\n\n        self.conv_r = nn.Sequential(OrderedDict([(""conv_ru"", nn.Conv2d(in_chs, mid_chs,\n                                                                       kernel_size=(1, kernel_size),\n                                                                       padding=(0, pad))),\n                                                 (""conv_rd"", nn.Conv2d(mid_chs, out_chs,\n                                                                       kernel_size=(kernel_size, 1),\n                                                                       padding=(pad, 0)))]))\n\n    def forward(self, x):\n        x = self.abn(x)\n        x_l = self.conv_l(x)\n        x_r = self.conv_r(x)\n        return torch.add(x_l, 1, x_r)\n\n\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.fcs = nn.Sequential(nn.Linear(channel, int(channel/reduction)),\n                                 nn.LeakyReLU(negative_slope=0.1, inplace=True),\n                                 nn.Linear(int(channel/reduction), channel),\n                                 nn.Sigmoid())\n\n    def forward(self, x):\n        bahs, chs, _, _ = x.size()\n\n        # Returns a new tensor with the same data as the self tensor but of a different size.\n        y = self.avg_pool(x).view(bahs, chs)\n        y = self.fcs(y).view(bahs, chs, 1, 1)\n        return torch.mul(x, y)\n\n\nclass SCSEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SCSEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel//reduction)),\n                                                nn.ReLU(inplace=True),\n                                                nn.Linear(int(channel//reduction), channel),\n                                                nn.Sigmoid())\n\n        self.spatial_se = nn.Sequential(nn.Conv2d(channel, 1, kernel_size=1,\n                                                  stride=1, padding=0, bias=False),\n                                        nn.Sigmoid())\n\n    def forward(self, x):\n        bahs, chs, _, _ = x.size()\n\n        # Returns a new tensor with the same data as the self tensor but of a different size.\n        chn_se = self.avg_pool(x).view(bahs, chs)\n        chn_se = self.channel_excitation(chn_se).view(bahs, chs, 1, 1)\n        chn_se = torch.mul(x, chn_se)\n\n        spa_se = self.spatial_se(x)\n        spa_se = torch.mul(x, spa_se)\n        return torch.add(chn_se, 1, spa_se)\n\n\nclass ModifiedSCSEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(ModifiedSCSEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel//reduction)),\n                                                nn.ReLU(inplace=True),\n                                                nn.Linear(int(channel//reduction), channel),\n                                                nn.Sigmoid())\n\n        self.spatial_se = nn.Sequential(nn.Conv2d(channel, 1, kernel_size=1,\n                                                  stride=1, padding=0, bias=False),\n                                        nn.Sigmoid())\n\n    def forward(self, x):\n        bahs, chs, _, _ = x.size()\n\n        # Returns a new tensor with the same data as the self tensor but of a different size.\n        chn_se = self.avg_pool(x).view(bahs, chs)\n        chn_se = self.channel_excitation(chn_se).view(bahs, chs, 1, 1)\n\n        spa_se = self.spatial_se(x)\n        return torch.mul(torch.mul(x, chn_se), spa_se)\n\n\n# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Vortex Pooling: Improving Context Representation in Semantic Segmentation\n# https://arxiv.org/abs/1804.06242v1\n# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass VortexPooling(nn.Module):\n    def __init__(self, in_chs, out_chs, feat_res=(56, 112), up_ratio=2, rate=(3, 9, 27)):\n        super(VortexPooling, self).__init__()\n        self.gave_pool = nn.Sequential(OrderedDict([(""gavg"", nn.AdaptiveAvgPool2d((1, 1))),\n                                                    (""conv1x1"", nn.Conv2d(in_chs, out_chs,\n                                                                          kernel_size=1, stride=1, padding=0,\n                                                                          groups=1, bias=False, dilation=1)),\n                                                    (""up0"", nn.Upsample(size=feat_res, mode=\'bilinear\')),\n                                                    (""bn0"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        self.conv3x3 = nn.Sequential(OrderedDict([(""conv3x3"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                        stride=1, padding=1, bias=False,\n                                                                        groups=1, dilation=1)),\n                                                  (""bn3x3"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        self.vortex_bra1 = nn.Sequential(OrderedDict([(""avg_pool"", nn.AvgPool2d(kernel_size=rate[0], stride=1,\n                                                                                padding=int((rate[0]-1)/2), ceil_mode=False)),\n                                                      (""conv3x3"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                            stride=1, padding=rate[0], bias=False,\n                                                                            groups=1, dilation=rate[0])),\n                                                      (""bn3x3"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        self.vortex_bra2 = nn.Sequential(OrderedDict([(""avg_pool"", nn.AvgPool2d(kernel_size=rate[1], stride=1,\n                                                                                padding=int((rate[1]-1)/2), ceil_mode=False)),\n                                                      (""conv3x3"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                            stride=1, padding=rate[1], bias=False,\n                                                                            groups=1, dilation=rate[1])),\n                                                      (""bn3x3"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        self.vortex_bra3 = nn.Sequential(OrderedDict([(""avg_pool"", nn.AvgPool2d(kernel_size=rate[2], stride=1,\n                                                                                padding=int((rate[2]-1)/2), ceil_mode=False)),\n                                                      (""conv3x3"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                            stride=1, padding=rate[2], bias=False,\n                                                                            groups=1, dilation=rate[2])),\n                                                      (""bn3x3"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        self.vortex_catdown = nn.Sequential(OrderedDict([(""conv_down"", nn.Conv2d(5 * out_chs, out_chs, kernel_size=1,\n                                                                                 stride=1, padding=1, bias=False,\n                                                                                 groups=1, dilation=1)),\n                                                         (""bn_down"", nn.BatchNorm2d(num_features=out_chs)),\n                                                         (""dropout"", nn.Dropout2d(p=0.2, inplace=True))]))\n\n        self.upsampling = nn.Upsample(size=(int(feat_res[0] * up_ratio), int(feat_res[1] * up_ratio)), mode=\'bilinear\')\n\n    def forward(self, x):\n        out = torch.cat([self.gave_pool(x),\n                         self.conv3x3(x),\n                         self.vortex_bra1(x),\n                         self.vortex_bra2(x),\n                         self.vortex_bra3(x)], dim=1)\n\n        out = self.vortex_catdown(out)\n        return self.upsampling(out)\n\n\nclass ASPPBlock(nn.Module):\n    def __init__(self, in_chs, out_chs, feat_res=(56, 112), up_ratio=2, aspp_sec=(12, 24, 36)):\n        super(ASPPBlock, self).__init__()\n\n        self.gave_pool = nn.Sequential(OrderedDict([(""gavg"", nn.AdaptiveAvgPool2d((1, 1))),\n                                                    (""conv1_0"", nn.Conv2d(in_chs, out_chs,\n                                                                          kernel_size=1, stride=1, padding=0,\n                                                                          groups=1, bias=False, dilation=1)),\n                                                    (""up0"", nn.Upsample(size=feat_res, mode=\'bilinear\')),\n                                                    (""bn0"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        self.conv1x1 = nn.Sequential(OrderedDict([(""conv1_1"", nn.Conv2d(in_chs, out_chs, kernel_size=1,\n                                                                        stride=1, padding=0, bias=False,\n                                                                        groups=1, dilation=1)),\n                                                  (""bn1_1"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        self.aspp_bra1 = nn.Sequential(OrderedDict([(""conv2_1"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                          stride=1, padding=aspp_sec[0], bias=False,\n                                                                          groups=1, dilation=aspp_sec[0])),\n                                                    (""bn2_1"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        self.aspp_bra2 = nn.Sequential(OrderedDict([(""conv2_2"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                          stride=1, padding=aspp_sec[1], bias=False,\n                                                                          groups=1, dilation=aspp_sec[1])),\n                                                    (""bn2_2"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        self.aspp_bra3 = nn.Sequential(OrderedDict([(""conv2_3"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                          stride=1, padding=aspp_sec[2], bias=False,\n                                                                          groups=1, dilation=aspp_sec[2])),\n                                                    (""bn2_3"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        self.aspp_catdown = nn.Sequential(OrderedDict([(""conv_down"", nn.Conv2d(5*out_chs, out_chs, kernel_size=1,\n                                                                               stride=1, padding=1, bias=False,\n                                                                               groups=1, dilation=1)),\n                                                       (""bn_down"", nn.BatchNorm2d(num_features=out_chs)),\n                                                       (""dropout"", nn.Dropout2d(p=0.2, inplace=True))]))\n\n        self.upsampling = nn.Upsample(size=(int(feat_res[0]*up_ratio), int(feat_res[1]*up_ratio)), mode=\'bilinear\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n        Channel shuffle operation\n        :param x: input tensor\n        :param groups: split channels into groups\n        :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        out = torch.cat([self.gave_pool(x),\n                         self.conv1x1(x),\n                         self.aspp_bra1(x),\n                         self.aspp_bra2(x),\n                         self.aspp_bra3(x)], dim=1)\n\n        out = self.aspp_catdown(out)\n        return self.upsampling(out)\n\n\nclass ASPPInPlaceABNBlock(nn.Module):\n    def __init__(self, in_chs, out_chs, feat_res=(56, 112),\n                 up_ratio=2, aspp_sec=(12, 24, 36), norm_act=ABN):\n        super(ASPPInPlaceABNBlock, self).__init__()\n\n        self.in_norm = norm_act(in_chs)\n        self.gave_pool = nn.Sequential(OrderedDict([(""gavg"", nn.AdaptiveAvgPool2d((1, 1))),\n                                                    (""conv1_0"", nn.Conv2d(in_chs, out_chs,\n                                                                          kernel_size=1, stride=1, padding=0,\n                                                                          groups=1, bias=False, dilation=1)),\n                                                    (""up0"", nn.Upsample(size=feat_res, mode=\'bilinear\'))]))\n\n        self.conv1x1 = nn.Sequential(OrderedDict([(""conv1_1"", nn.Conv2d(in_chs, out_chs, kernel_size=1,\n                                                                        stride=1, padding=0, bias=False,\n                                                                        groups=1, dilation=1))]))\n\n        self.aspp_bra1 = nn.Sequential(OrderedDict([(""conv2_1"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                          stride=1, padding=aspp_sec[0], bias=False,\n                                                                          groups=1, dilation=aspp_sec[0]))]))\n\n        self.aspp_bra2 = nn.Sequential(OrderedDict([(""conv2_2"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                          stride=1, padding=aspp_sec[1], bias=False,\n                                                                          groups=1, dilation=aspp_sec[1]))]))\n\n        self.aspp_bra3 = nn.Sequential(OrderedDict([(""conv2_3"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                          stride=1, padding=aspp_sec[2], bias=False,\n                                                                          groups=1, dilation=aspp_sec[2]))]))\n\n        self.aspp_catdown = nn.Sequential(OrderedDict([(""norm_act"", norm_act(5*out_chs)),\n                                                       (""conv_down"", nn.Conv2d(5*out_chs, out_chs, kernel_size=1,\n                                                                               stride=1, padding=1, bias=False,\n                                                                               groups=1, dilation=1)),\n                                                       (""dropout"", nn.Dropout2d(p=0.2, inplace=True))]))\n\n        self.upsampling = nn.Upsample(size=(int(feat_res[0]*up_ratio), int(feat_res[1]*up_ratio)), mode=\'bilinear\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n        Channel shuffle operation\n        :param x: input tensor\n        :param groups: split channels into groups\n        :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        x = self.in_norm(x)\n        x = torch.cat([self.gave_pool(x),\n                       self.conv1x1(x),\n                       self.aspp_bra1(x),\n                       self.aspp_bra2(x),\n                       self.aspp_bra3(x)], dim=1)\n\n        out = self.aspp_catdown(x)\n        return out, self.upsampling(out)\n\n\nclass SDASPPInPlaceABNBlock(nn.Module):\n    def __init__(self, in_chs, out_chs, feat_res=(56, 112),\n                 up_ratio=2, aspp_sec=(12, 24, 36), norm_act=ABN):\n        super(SDASPPInPlaceABNBlock, self).__init__()\n\n        self.in_norm = norm_act(in_chs)\n        self.gave_pool = nn.Sequential(OrderedDict([(""gavg"", nn.AdaptiveAvgPool2d((1, 1))),\n                                                    (""conv1_0"", nn.Conv2d(in_chs, out_chs,\n                                                                          kernel_size=1, stride=1, padding=0,\n                                                                          groups=1, bias=False, dilation=1)),\n                                                    (""up0"", nn.Upsample(size=feat_res, mode=\'bilinear\'))]))\n\n        self.conv1x1 = nn.Sequential(OrderedDict([(""conv1_1"", nn.Conv2d(in_chs, out_chs, kernel_size=1,\n                                                                        stride=1, padding=0, bias=False,\n                                                                        groups=1, dilation=1))]))\n\n        self.aspp_bra1 = nn.Sequential(OrderedDict([(""dconv2_1"", nn.Conv2d(in_chs, in_chs, kernel_size=3,\n                                                                           stride=1, padding=aspp_sec[0], bias=False,\n                                                                           groups=in_chs, dilation=aspp_sec[0])),\n                                                    (""pconv2_1"", nn.Conv2d(in_chs, out_chs, kernel_size=1,\n                                                                           stride=1, padding=0, bias=False,\n                                                                           groups=1, dilation=1))]))\n\n        self.aspp_bra2 = nn.Sequential(OrderedDict([(""dconv2_2"", nn.Conv2d(in_chs, in_chs, kernel_size=3,\n                                                                           stride=1, padding=aspp_sec[1], bias=False,\n                                                                           groups=in_chs, dilation=aspp_sec[1])),\n                                                    (""pconv2_2"", nn.Conv2d(in_chs, out_chs, kernel_size=1,\n                                                                           stride=1, padding=0, bias=False,\n                                                                           groups=1, dilation=1))]))\n\n        self.aspp_bra3 = nn.Sequential(OrderedDict([(""dconv2_3"", nn.Conv2d(in_chs, in_chs, kernel_size=3,\n                                                                           stride=1, padding=aspp_sec[2], bias=False,\n                                                                           groups=in_chs, dilation=aspp_sec[2])),\n                                                    (""pconv2_3"", nn.Conv2d(in_chs, out_chs, kernel_size=1,\n                                                                           stride=1, padding=0, bias=False,\n                                                                           groups=1, dilation=1))]))\n\n        self.aspp_catdown = nn.Sequential(OrderedDict([(""norm_act"", norm_act(5*out_chs)),\n                                                       (""conv_down"", nn.Conv2d(5*out_chs, out_chs, kernel_size=1,\n                                                                               stride=1, padding=1, bias=False,\n                                                                               groups=1, dilation=1)),\n                                                       (""dropout"", nn.Dropout2d(p=0.2, inplace=True))]))\n\n        self.upsampling = nn.Upsample(size=(int(feat_res[0]*up_ratio), int(feat_res[1]*up_ratio)), mode=\'bilinear\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # channel_shuffle: shuffle channels in groups\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    @staticmethod\n    def _channel_shuffle(x, groups):\n        """"""\n        Channel shuffle operation\n        :param x: input tensor\n        :param groups: split channels into groups\n        :return: channel shuffled tensor\n        """"""\n        batch_size, num_channels, height, width = x.data.size()\n\n        channels_per_group = num_channels // groups\n\n        # reshape\n        x = x.view(batch_size, groups, channels_per_group, height, width)\n\n        # transpose\n        # - contiguous() required if transpose() is used before view().\n        #   See https://github.com/pytorch/pytorch/issues/764\n        x = torch.transpose(x, 1, 2).contiguous().view(batch_size, -1, height, width)\n\n        return x\n\n    def forward(self, x):\n        x = self.in_norm(x)\n        x = torch.cat([self.gave_pool(x),\n                       self.conv1x1(x),\n                       self.aspp_bra1(x),\n                       self.aspp_bra2(x),\n                       self.aspp_bra3(x)], dim=1)\n\n        return self.upsampling(self.aspp_catdown(x))\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# For MobileNetV2\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(in_channels=inp, out_channels=oup, kernel_size=3, stride=stride, padding=1, bias=False),\n        nn.BatchNorm2d(num_features=oup, eps=1e-05, momentum=0.1, affine=True),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, stride=1, padding=0, bias=False),\n        nn.BatchNorm2d(num_features=oup, eps=1e-05, momentum=0.1, affine=True),\n        nn.ReLU(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, dilate, expand_ratio):\n        """"""\n        InvertedResidual: Core block of the MobileNetV2\n        :param inp:    (int) Number of the input channels\n        :param oup:    (int) Number of the output channels\n        :param stride: (int) Stride used in the Conv3x3\n        :param dilate: (int) Dilation used in the Conv3x3\n        :param expand_ratio: (int) Expand ratio of the Channel Width of the Block\n        """"""\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        self.conv = nn.Sequential(\n            # pw\n            nn.Conv2d(in_channels=inp, out_channels=inp * expand_ratio,\n                      kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False),\n            nn.BatchNorm2d(num_features=inp * expand_ratio, eps=1e-05, momentum=0.1, affine=True),\n            nn.ReLU6(inplace=True),\n\n            # dw\n            nn.Conv2d(in_channels=inp * expand_ratio, out_channels=inp * expand_ratio,\n                      kernel_size=3, stride=stride, padding=dilate, dilation=dilate,\n                      groups=inp * expand_ratio, bias=False),\n            nn.BatchNorm2d(num_features=inp * expand_ratio, eps=1e-05, momentum=0.1, affine=True),\n            nn.ReLU6(inplace=True),\n\n            # pw-linear\n            nn.Conv2d(in_channels=inp * expand_ratio, out_channels=oup,\n                      kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False),\n            nn.BatchNorm2d(num_features=oup, eps=1e-05, momentum=0.1, affine=True),\n        )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return torch.add(x, 1, self.conv(x))\n        else:\n            return self.conv(x)\n\n\nclass SCSEInvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, dilate, expand_ratio):\n        """"""\n        InvertedResidual: Core block of the MobileNetV2\n        :param inp:    (int) Number of the input channels\n        :param oup:    (int) Number of the output channels\n        :param stride: (int) Stride used in the Conv3x3\n        :param dilate: (int) Dilation used in the Conv3x3\n        :param expand_ratio: (int) Expand ratio of the Channel Width of the Block\n        """"""\n        super(SCSEInvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        self.conv = nn.Sequential(\n            # pw\n            nn.Conv2d(in_channels=inp, out_channels=inp * expand_ratio,\n                      kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False),\n            nn.BatchNorm2d(num_features=inp * expand_ratio, eps=1e-05, momentum=0.1, affine=True),\n            nn.ReLU6(inplace=True),\n\n            # dw\n            nn.Conv2d(in_channels=inp * expand_ratio, out_channels=inp * expand_ratio,\n                      kernel_size=3, stride=stride, padding=dilate, dilation=dilate,\n                      groups=inp * expand_ratio, bias=False),\n            nn.BatchNorm2d(num_features=inp * expand_ratio, eps=1e-05, momentum=0.1, affine=True),\n            nn.ReLU6(inplace=True),\n\n            # pw-linear\n            nn.Conv2d(in_channels=inp * expand_ratio, out_channels=oup,\n                      kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False),\n            nn.BatchNorm2d(num_features=oup, eps=1e-05, momentum=0.1, affine=True),\n            SCSEBlock(channel=oup, reduction=2)\n        )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return torch.add(x, 1, self.conv(x))\n        else:\n            return self.conv(x)\n'"
modules/residual.py,1,"b'import torch.nn as nn\nfrom .bn import ABN\nfrom .misc import SEBlock\nfrom collections import OrderedDict\n\n\nclass IdentityResidualBlock(nn.Module):\n    def __init__(self, in_channels, channels, stride=1, dilation=1, groups=1, norm_act=ABN, is_se=False, dropout=None):\n        """"""Configurable identity-mapping residual block\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels.\n        channels : list of int\n            Number of channels in the internal feature maps. Can either have two or three elements: if three construct\n            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then\n            `3 x 3` then `1 x 1` convolutions.\n        stride : int\n            Stride of the first `3 x 3` convolution\n        dilation : int\n            Dilation to apply to the `3 x 3` convolutions.\n        groups : int\n            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with\n            bottleneck blocks.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        dropout: callable\n            Function to create Dropout Module.\n        """"""\n        super(IdentityResidualBlock, self).__init__()\n\n        # Check parameters for inconsistencies\n        if len(channels) != 2 and len(channels) != 3:\n            raise ValueError(""channels must contain either two or three values"")\n        if len(channels) == 2 and groups != 1:\n            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")\n\n        is_bottleneck = len(channels) == 3\n        need_proj_conv = stride != 1 or in_channels != channels[-1]\n\n        self.bn1 = norm_act(in_channels)\n        if not is_bottleneck:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,\n                                    dilation=dilation)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    dilation=dilation))\n            ]\n            if dropout is not None:\n                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]\n        else:\n            if not is_se:\n                layers = [\n                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),\n                    (""bn2"", norm_act(channels[0])),\n                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                        groups=groups, dilation=dilation)),\n                    (""bn3"", norm_act(channels[1])),\n                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))\n                ]\n            else:\n                layers = [\n                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),\n                    (""bn2"", norm_act(channels[0])),\n                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                        groups=groups, dilation=dilation)),\n                    (""bn3"", norm_act(channels[1])),\n                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False)),\n                    (""se_block"", SEBlock(channels[2], 16))\n                ]\n            if dropout is not None:\n                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]\n        self.convs = nn.Sequential(OrderedDict(layers))\n\n        if need_proj_conv:\n            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n\n    def forward(self, x):\n        if hasattr(self, ""proj_conv""):\n            bn1 = self.bn1(x)\n            shortcut = self.proj_conv(bn1)\n        else:\n            shortcut = x.clone()\n            bn1 = self.bn1(x)\n\n        out = self.convs(bn1)\n        out.add_(shortcut)\n\n        return out\n'"
modules/rfblock.py,5,"b'import time\nimport torch\nimport torch.nn as nn\n\nfrom modules import InPlaceABN\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\n\n\nclass RFBlock(nn.Module):\n    def __init__(self, in_chs, out_chs, scale=0.1, feat_res=(56, 112), aspp_sec=(12, 24, 36),\n                 up_ratio=2, norm_act=InPlaceABN):\n        super(RFBlock, self).__init__()\n        self.scale = scale\n\n        self.down_chs = nn.Sequential(OrderedDict([(""norm_act"", norm_act(in_chs)),\n                                                   (""down_conv1x1"", nn.Conv2d(in_chs, out_chs,\n                                                                              kernel_size=1, stride=1,\n                                                                              padding=0, bias=False))]))\n\n        self.gave_pool = nn.Sequential(OrderedDict([(""norm_act"", norm_act(out_chs)),\n                                                    (""gavg"", nn.AdaptiveAvgPool2d((1, 1))),\n                                                    (""conv1_0"", nn.Conv2d(out_chs, out_chs,\n                                                                          kernel_size=1, stride=1, padding=0,\n                                                                          groups=1, bias=False, dilation=1)),\n                                                    (""up0"", nn.Upsample(size=feat_res, mode=\'bilinear\'))]))\n\n        self.branch0 = nn.Sequential(OrderedDict([(""norm_act"", norm_act(out_chs)),\n                                                  (""conv1x1"", nn.Conv2d(out_chs, out_chs,\n                                                                        kernel_size=1, stride=1,\n                                                                        padding=0, bias=False)),\n                                                  (""norm_act"", norm_act(out_chs)),\n                                                  (""aconv1"", nn.Conv2d(out_chs, out_chs,\n                                                                       kernel_size=3, stride=1,\n                                                                       padding=1, dilation=1,\n                                                                       bias=False))]))\n\n        self.branch1 = nn.Sequential(OrderedDict([(""norm_act"", norm_act(out_chs)),\n                                                  (""conv1x3"", nn.Conv2d(out_chs, (out_chs // 2) * 3,\n                                                                        kernel_size=(1, 3), stride=1,\n                                                                        padding=(0, 1), bias=False)),\n                                                  (""norm_act"", norm_act((out_chs // 2) * 3)),\n                                                  (""conv3x1"", nn.Conv2d((out_chs // 2) * 3, out_chs,\n                                                                        kernel_size=(3, 1), stride=1,\n                                                                        padding=(1, 0), bias=False)),\n                                                  (""norm_act"", norm_act(out_chs)),\n                                                  (""aconv3"", nn.Conv2d(out_chs, out_chs,\n                                                                       kernel_size=3, stride=1,\n                                                                       padding=aspp_sec[0],\n                                                                       dilation=aspp_sec[0],\n                                                                       bias=False))]))\n\n        self.branch2 = nn.Sequential(OrderedDict([(""norm_act"", norm_act(out_chs)),\n                                                  (""conv1x5"", nn.Conv2d(out_chs, (out_chs // 2) * 3,\n                                                                        kernel_size=(1, 5), stride=1,\n                                                                        padding=(0, 2), bias=False)),\n                                                  (""norm_act"", norm_act((out_chs // 2) * 3)),\n                                                  (""conv5x1"", nn.Conv2d((out_chs // 2) * 3, out_chs,\n                                                                        kernel_size=(5, 1), stride=1,\n                                                                        padding=(2, 0), bias=False)),\n                                                  (""norm_act"", norm_act(out_chs)),\n                                                  (""aconv5"", nn.Conv2d(out_chs, out_chs,\n                                                                       kernel_size=3, stride=1,\n                                                                       padding=aspp_sec[1],\n                                                                       dilation=aspp_sec[1],\n                                                                       bias=False))]))\n\n        self.branch3 = nn.Sequential(OrderedDict([(""norm_act"", norm_act(out_chs)),\n                                                  (""conv1x7"", nn.Conv2d(out_chs, (out_chs // 2) * 3,\n                                                                        kernel_size=(1, 7), stride=1,\n                                                                        padding=(0, 3), bias=False)),\n                                                  (""norm_act"", norm_act((out_chs // 2) * 3)),\n                                                  (""conv7x1"", nn.Conv2d((out_chs // 2) * 3, out_chs,\n                                                                        kernel_size=(7, 1), stride=1,\n                                                                        padding=(3, 0), bias=False)),\n                                                  (""norm_act"", norm_act(out_chs)),\n                                                  (""aconv7"", nn.Conv2d(out_chs, out_chs,\n                                                                       kernel_size=3, stride=1,\n                                                                       padding=aspp_sec[2],\n                                                                       dilation=aspp_sec[2],\n                                                                       bias=False))]))\n\n        self.conv_linear = nn.Sequential(OrderedDict([(""conv1x1_linear"", nn.Conv2d(out_chs * 5, out_chs,\n                                                                                   kernel_size=1, stride=1,\n                                                                                   padding=0, bias=False))]))\n\n        self.upsampling = nn.Upsample(size=(int(feat_res[0] * up_ratio),\n                                            int(feat_res[1] * up_ratio)),\n                                      mode=\'bilinear\')\n\n    def forward(self, x):\n        down = self.down_chs(x)\n        out = torch.cat([self.gave_pool(down.clone()),\n                         self.branch0(down.clone()),\n                         self.branch1(down.clone()),\n                         self.branch2(down.clone()),\n                         self.branch3(down.clone())], dim=1)\n\n        return self.upsampling(torch.add(self.conv_linear(out), self.scale, down))  # out=input+value\xc3\x97other\n\n\nif __name__ == ""__main__"":\n    from functools import partial\n    from modules import InPlaceABNWrapper\n    input_chs = 712\n    output_chs = 256\n    feat_maps = Variable(torch.randn(1, input_chs, 32, 32).cuda())\n\n    rfblocka = RFBlock(in_chs=input_chs, out_chs=output_chs,\n                       scale=0.1, feat_res=(32, 32),\n                       norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1)).cuda()\n\n    start_time = time.time()\n    _ = rfblocka(feat_maps)\n    end_time = time.time()\n    print(""RFBlock: {}s"".format(end_time - start_time))\n'"
net_viz/__init__.py,0,b''
net_viz/guided_backprop.py,3,"b'""""""\nCreated on Thu Oct 26 11:23:47 2017\n\n@author: Utku Ozbulak - github.com/utkuozbulak\n""""""\nimport torch\nfrom torch.nn import ReLU\n\nfrom net_viz.misc import get_params, convert_to_grayscale, save_gradient_images, get_positive_negative_saliency\n\n\nclass GuidedBackprop():\n    """"""\n       Produces gradients generated with guided back propagation from the given image\n    """"""\n    def __init__(self, model, processed_im, target_class):\n        self.model = model\n        self.input_image = processed_im\n        self.target_class = target_class\n        self.gradients = None\n        # Put model in evaluation mode\n        self.model.eval()\n        self.update_relus()\n        self.hook_layers()\n\n    def hook_layers(self):\n        def hook_function(module, grad_in, grad_out):\n            self.gradients = grad_in[0]\n\n        # Register hook to the first layer\n        first_layer = list(self.model.features._modules.items())[0][1]\n        first_layer.register_backward_hook(hook_function)\n\n    def update_relus(self):\n        """"""\n            Updates relu activation functions so that it only returns positive gradients\n        """"""\n        def relu_hook_function(module, grad_in, grad_out):\n            """"""\n            If there is a negative gradient, changes it to zero\n            """"""\n            if isinstance(module, ReLU):\n                return (torch.clamp(grad_in[0], min=0.0),)\n        # Loop through layers, hook up ReLUs with relu_hook_function\n        for pos, module in self.model.features._modules.items():\n            if isinstance(module, ReLU):\n                module.register_backward_hook(relu_hook_function)\n\n    def generate_gradients(self):\n        # Forward pass\n        model_output = self.model(self.input_image)\n        # Zero gradients\n        self.model.zero_grad()\n        # Target for backprop\n        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n        one_hot_output[0][self.target_class] = 1\n        # Backward pass\n        model_output.backward(gradient=one_hot_output)\n        # Convert Pytorch variable to numpy array\n        # [0] to get rid of the first channel (1,3,224,224)\n        gradients_as_arr = self.gradients.data.numpy()[0]\n        return gradients_as_arr\n\n\nif __name__ == \'__main__\':\n    target_example = 0  # Snake\n    (original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n        get_params(target_example)\n\n    # Guided backprop\n    GBP = GuidedBackprop(pretrained_model, prep_img, target_class)\n    # Get gradients\n    guided_grads = GBP.generate_gradients()\n    # Save colored gradients\n    save_gradient_images(guided_grads, file_name_to_export + \'_Guided_BP_color\')\n    # Convert to grayscale\n    grayscale_guided_grads = convert_to_grayscale(guided_grads)\n    # Save grayscale gradients\n    save_gradient_images(grayscale_guided_grads, file_name_to_export + \'_Guided_BP_gray\')\n    # Positive and negative saliency maps\n    pos_sal, neg_sal = get_positive_negative_saliency(guided_grads)\n    save_gradient_images(pos_sal, file_name_to_export + \'_pos_sal\')\n    save_gradient_images(neg_sal, file_name_to_export + \'_neg_sal\')\n    print(\'Guided backprop completed\')'"
net_viz/layer_viz.py,4,"b'""""""\nCreated on Sat Nov 18 23:12:08 2017\n\n@author: Utku Ozbulak - github.com/utkuozbulak\n""""""\nimport os\nimport cv2\nimport numpy as np\n\nimport torch\nfrom torch.optim import SGD\nfrom torchvision import models\n\nfrom net_viz.misc import preprocess_image, recreate_image\n\n\nclass CNNLayerVisualization(object):\n    """"""\n        Produces an image that minimizes the loss of a convolution\n        operation for a specific layer and filter\n    """"""\n    def __init__(self, model, selected_layer, selected_filter):\n        self.model = model\n        self.model.eval()\n        self.selected_layer = selected_layer\n        self.selected_filter = selected_filter\n        self.conv_output = 0\n\n        # Generate a random image\n        self.created_image = np.uint8(np.random.uniform(150, 180, (448, 896, 3)))\n        # Process image and return variable\n        self.processed_image = None\n        # Create the folder to export images if not exists\n        if not os.path.exists(\'../generated\'):\n            os.makedirs(\'../generated\')\n\n    def hook_layer(self):\n        def hook_function(module, grad_in, grad_out):\n            # Gets the conv output of the selected filter (from selected layer)\n            self.conv_output = grad_out[0, self.selected_filter]\n\n        # Hook the selected layer\n        self.model[self.selected_layer].register_forward_hook(hook_function)\n\n    def visualise_layer_with_hooks(self):\n        # Hook the selected layer\n        self.hook_layer()\n        # Process image and return variable\n        self.processed_image = preprocess_image(self.created_image)\n        # Define optimizer for the image\n        # Earlier layers need higher learning rates to visualize whereas later layers need less\n        optimizer = SGD([self.processed_image], lr=5, weight_decay=1e-6)\n        for i in range(1, 51):\n            optimizer.zero_grad()\n            # Assign create image to a variable to move forward in the model\n            x = self.processed_image\n            for index, layer in enumerate(self.model):\n                # Forward pass layer by layer\n                # x is not used after this point because it is only needed to trigger\n                # the forward hook function\n                x = layer(x)\n                # Only need to forward until the selected layer is reached\n                if index == self.selected_layer:\n                    # (forward hook function triggered)\n                    break\n            # Loss function is the mean of the output of the selected layer/filter\n            # We try to minimize the mean of the output of that specific filter\n            loss = torch.mean(self.conv_output)\n            print(\'Iteration:\', str(i), \'Loss:\', ""{0:.2f}"".format(loss.data.numpy()[0]))\n            # Backward\n            loss.backward()\n            # Update image\n            optimizer.step()\n            # Recreate image\n            self.created_image = recreate_image(self.processed_image)\n            # Save image\n            if i % 5 == 0:\n                cv2.imwrite(\'../generated/layer_vis_l\' + str(self.selected_layer) +\n                            \'_f\' + str(self.selected_filter) + \'_iter\'+str(i)+\'.jpg\',\n                            self.created_image)\n\n    def visualise_layer_without_hooks(self):\n        # Process image and return variable\n        self.processed_image = preprocess_image(self.created_image)\n        # Define optimizer for the image\n        # Earlier layers need higher learning rates to visualize whereas later layers need less\n        optimizer = SGD([self.processed_image], lr=5, weight_decay=1e-6)\n        for i in range(1, 51):\n            optimizer.zero_grad()\n            # Assign create image to a variable to move forward in the model\n            x = self.processed_image\n            for index, layer in enumerate(self.model):\n                # Forward pass layer by layer\n                x = layer(x)\n                if index == self.selected_layer:\n                    # Only need to forward until the selected layer is reached\n                    # Now, x is the output of the selected layer\n                    break\n            # Here, we get the specific filter from the output of the convolution operation\n            # x is a tensor of shape 1x512x28x28.(For layer 17)\n            # So there are 512 unique filter outputs\n            # Following line selects a filter from 512 filters so self.conv_output will become\n            # a tensor of shape 28x28\n            self.conv_output = x[0, self.selected_filter]\n            # Loss function is the mean of the output of the selected layer/filter\n            # We try to minimize the mean of the output of that specific filter\n            loss = torch.mean(self.conv_output)\n            print(\'Iteration:\', str(i), \'Loss:\', ""{0:.2f}"".format(loss.data.numpy()[0]))\n            # Backward\n            loss.backward()\n            # Update image\n            optimizer.step()\n            # Recreate image\n            self.created_image = recreate_image(self.processed_image)\n            # Save image\n            if i % 5 == 0:\n                cv2.imwrite(\'/afs/cg.cs.tu-bs.de/home/zhang/SEDPShuffleNet/net_viz/viz_outs/layer_vis_l\' +\n                            str(self.selected_layer) + \'_f\' + str(self.selected_filter) + \'_iter\'+str(i)+\'.jpg\',\n                            self.created_image)\n\n\nif __name__ == \'__main__\':\n    from models.mobilenetv2plus import MobileNetV2Plus\n    from scripts.utils import convert_state_dict\n    from modules import InPlaceABNWrapper\n    from functools import partial\n\n    net_h, net_w = 448, 896\n    cnn_layer = 17\n    filter_pos = 0\n\n    # Fully connected layer is not needed\n\n    model = MobileNetV2Plus(n_class=19, in_size=(net_h, net_w), width_mult=1.0,\n                            out_sec=256, aspp_sec=(12, 24, 36),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n\n    pre_weight = torch.load(""/zfs/zhang/TrainLog/weights/cityscapes_mobilenetv2_best_model.pkl"")\n    pre_weight = pre_weight[""model_state""]\n\n    model_dict = model.state_dict()\n\n    pretrained_dict = {k[7:]: v for k, v in pre_weight.items() if k[7:] in model_dict}\n    model_dict.update(pretrained_dict)\n    model.load_state_dict(model_dict)\n\n    del pre_weight\n    del model_dict\n    del pretrained_dict\n\n    layer_vis = CNNLayerVisualization(model, cnn_layer, filter_pos)\n\n    # Layer visualization with pytorch hooks\n    # layer_vis.visualise_layer_with_hooks()\n\n    # Layer visualization without pytorch hooks\n    layer_vis.visualise_layer_without_hooks()\n'"
net_viz/misc.py,2,"b'""""""\nCreated on Thu Oct 21 11:09:09 2017\n\n@author: Utku Ozbulak - github.com/utkuozbulak\n""""""\nimport os\nimport copy\nimport cv2\nimport numpy as np\n\nimport torch\nfrom torch.autograd import Variable\nfrom torchvision import models\n\n\ndef convert_to_grayscale(cv2im):\n    """"""\n        Converts 3d image to grayscale\n\n    Args:\n        cv2im (numpy arr): RGB image with shape (D,W,H)\n\n    returns:\n        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n    """"""\n    grayscale_im = np.sum(np.abs(cv2im), axis=0)\n    im_max = np.percentile(grayscale_im, 99)\n    im_min = np.min(grayscale_im)\n    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n    return grayscale_im\n\n\ndef save_gradient_images(gradient, file_name):\n    """"""\n        Exports the original gradient image\n\n    Args:\n        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n        file_name (str): File name to be exported\n    """"""\n    if not os.path.exists(\'../results\'):\n        os.makedirs(\'../results\')\n    gradient = gradient - gradient.min()\n    gradient /= gradient.max()\n    gradient = np.uint8(gradient * 255).transpose(1, 2, 0)\n    path_to_file = os.path.join(\'../results\', file_name + \'.jpg\')\n    # Convert RBG to GBR\n    gradient = gradient[..., ::-1]\n    cv2.imwrite(path_to_file, gradient)\n\n\ndef save_class_activation_on_image(org_img, activation_map, file_name):\n    """"""\n        Saves cam activation map and activation map on the original image\n\n    Args:\n        org_img (PIL img): Original image\n        activation_map (numpy arr): activation map (grayscale) 0-255\n        file_name (str): File name of the exported image\n    """"""\n    if not os.path.exists(\'../results\'):\n        os.makedirs(\'../results\')\n    # Grayscale activation map\n    path_to_file = os.path.join(\'../results\', file_name+\'_Cam_Grayscale.jpg\')\n    cv2.imwrite(path_to_file, activation_map)\n    # Heatmap of activation map\n    activation_heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_HSV)\n    path_to_file = os.path.join(\'../results\', file_name+\'_Cam_Heatmap.jpg\')\n    cv2.imwrite(path_to_file, activation_heatmap)\n    # Heatmap on picture\n    org_img = cv2.resize(org_img, (224, 224))\n    img_with_heatmap = np.float32(activation_heatmap) + np.float32(org_img)\n    img_with_heatmap = img_with_heatmap / np.max(img_with_heatmap)\n    path_to_file = os.path.join(\'../results\', file_name+\'_Cam_On_Image.jpg\')\n    cv2.imwrite(path_to_file, np.uint8(255 * img_with_heatmap))\n\n\ndef preprocess_image(cv2im, resize_im=True):\n    """"""\n        Processes image for CNNs\n\n    Args:\n        PIL_img (PIL_img): Image to process\n        resize_im (bool): Resize to 224 or not\n    returns:\n        im_as_var (Pytorch variable): Variable that contains processed float tensor\n    """"""\n    # mean and std list for channels (Imagenet)\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    # Resize image\n    if resize_im:\n        cv2im = cv2.resize(cv2im, (224, 224))\n    im_as_arr = np.float32(cv2im)\n    im_as_arr = np.ascontiguousarray(im_as_arr[..., ::-1])\n    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n    # Normalize the channels\n    for channel, _ in enumerate(im_as_arr):\n        im_as_arr[channel] /= 255\n        im_as_arr[channel] -= mean[channel]\n        im_as_arr[channel] /= std[channel]\n    # Convert to float tensor\n    im_as_ten = torch.from_numpy(im_as_arr).float()\n    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n    im_as_ten.unsqueeze_(0)\n    # Convert to Pytorch variable\n    im_as_var = Variable(im_as_ten, requires_grad=True)\n    return im_as_var\n\n\ndef recreate_image(im_as_var):\n    """"""\n        Recreates images from a torch variable, sort of reverse preprocessing\n\n    Args:\n        im_as_var (torch variable): Image to recreate\n\n    returns:\n        recreated_im (numpy arr): Recreated image in array\n    """"""\n    reverse_mean = [-0.485, -0.456, -0.406]\n    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n    for c in range(3):\n        recreated_im[c] /= reverse_std[c]\n        recreated_im[c] -= reverse_mean[c]\n    recreated_im[recreated_im > 1] = 1\n    recreated_im[recreated_im < 0] = 0\n    recreated_im = np.round(recreated_im * 255)\n\n    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n    # Convert RBG to GBR\n    recreated_im = recreated_im[..., ::-1]\n    return recreated_im\n\n\ndef get_positive_negative_saliency(gradient):\n    """"""\n        Generates positive and negative saliency maps based on the gradient\n    Args:\n        gradient (numpy arr): Gradient of the operation to visualize\n\n    returns:\n        pos_saliency ( )\n    """"""\n    pos_saliency = (np.maximum(0, gradient) / gradient.max())\n    neg_saliency = (np.maximum(0, -gradient) / -gradient.min())\n    return pos_saliency, neg_saliency\n\n\ndef get_params(example_index):\n    """"""\n        Gets used variables for almost all visualizations, like the image, model etc.\n\n    Args:\n        example_index (int): Image id to use from examples\n\n    returns:\n        original_image (numpy arr): Original image read from the file\n        prep_img (numpy_arr): Processed image\n        target_class (int): Target class for the image\n        file_name_to_export (string): File name to export the visualizations\n        pretrained_model(Pytorch model): Model to use for the operations\n    """"""\n    # Pick one of the examples\n    example_list = [[\'../input_images/snake.jpg\', 56],\n                    [\'../input_images/cat_dog.png\', 243],\n                    [\'../input_images/spider.png\', 72]]\n    selected_example = example_index\n    img_path = example_list[selected_example][0]\n    target_class = example_list[selected_example][1]\n    file_name_to_export = img_path[img_path.rfind(\'/\')+1:img_path.rfind(\'.\')]\n    # Read image\n    original_image = cv2.imread(img_path, 1)\n    # Process image\n    prep_img = preprocess_image(original_image)\n    # Define model\n    pretrained_model = models.alexnet(pretrained=True)\n    return (original_image,\n            prep_img,\n            target_class,\n            file_name_to_export,\n            pretrained_model)'"
net_viz/net_viz_pytorch.py,4,"b'from functools import partial\n\nimport torch\nfrom torch.autograd import Variable\n\nfrom models.sewrnetv1 import SEWiderResNetV1\nfrom modules import InPlaceABNWrapper\nfrom net_viz.visualize import make_dot\n\nif __name__ == ""__main__"":\n    net_h, net_w = 448, 896\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n    model = SEWiderResNetV1(structure=[3, 3, 6, 3, 1, 1],\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1),\n                            classes=19, dilation=True, is_se=True, in_size=(net_h, net_w),\n                            out_sec=(512, 256, 128), aspp_sec=(12, 24, 36))\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    pre_weight = torch.load(""/media/datavolume3/huijun/SEDPShuffleNet/weights/{}"".format(\n        ""cityscapes_sewrnet_best_model.pkl""))[\'model_state\']\n    model.load_state_dict(pre_weight)\n    model_dict = model.state_dict()\n\n    for name, param in model.named_parameters():\n        print(""Name: {}, Size: {}"".format(name, param.size()))\n\n    dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n    output = model(dummy_input)[3]\n\n    graph = make_dot(output, model_dict)\n    graph.view()\n'"
net_viz/visualize.py,3,"b'import torch\nfrom graphviz import Digraph\nfrom torch.autograd import Variable\n\n\ndef make_dot(var, params=None):\n    """""" Produces Graphviz representation of PyTorch autograd graph\n\n    Blue nodes are the Variables that require grad, orange are Tensors\n    saved for backward in torch.autograd.Function\n\n    Args:\n        var: output Variable\n        params: dict of (name, Variable) to add names to node that\n            require grad (TODO: make optional)\n    """"""\n    if params is not None:\n        # assert all(isinstance(p, Variable) for p in params.values())\n        param_map = {id(v): k for k, v in params.items()}\n\n    node_attr = dict(style=\'filled\',\n                     shape=\'box\',\n                     align=\'left\',\n                     fontsize=\'12\',\n                     ranksep=\'0.1\',\n                     height=\'0.2\')\n    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=""12,12""))\n    seen = set()\n\n    def size_to_str(size):\n        return \'(\'+(\', \').join([\'%d\' % v for v in size])+\')\'\n\n    def add_nodes(var):\n        if var not in seen:\n            if torch.is_tensor(var):\n                dot.node(str(id(var)), size_to_str(var.size()), fillcolor=\'orange\')\n            elif isinstance(var, Variable):  # hasattr(var, \'variable\'):\n                # u = var.variable\n                # name = param_map[id(u)] if params is not None else \'\'\n                # node_name = \'%s\\n %s\' % (name, size_to_str(u.size()))\n                # dot.node(str(id(var)), node_name, fillcolor=\'lightblue\')\n                value = \'(\' + (\', \').join([\'%d\' % v for v in var.size()]) + \')\'\n                dot.node(str(id(var)), str(value), fillcolor=\'lightblue\')\n            else:\n                dot.node(str(id(var)), str(type(var).__name__))\n            seen.add(var)\n            if hasattr(var, \'next_functions\'):\n                for u in var.next_functions:\n                    if u[0] is not None:\n                        dot.edge(str(id(u[0])), str(id(var)))\n                        add_nodes(u[0])\n            if hasattr(var, \'saved_tensors\'):\n                for t in var.saved_tensors:\n                    dot.edge(str(id(t)), str(id(var)))\n                    add_nodes(t)\n\n    add_nodes(var.grad_fn)\n    return dot'"
scripts/__init__.py,0,b''
scripts/cyclical_lr.py,4,"b'import numpy as np\nfrom torch.optim import Optimizer\n\n\nclass CyclicLR(object):\n    """"""Sets the learning rate of each parameter group according to\n    cyclical learning rate policy (CLR). The policy cycles the learning\n    rate between two boundaries with a constant frequency, as detailed in\n    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n    The distance between the two boundaries can be scaled on a per-iteration\n    or per-cycle basis.\n\n    Cyclical learning rate policy changes the learning rate after every batch.\n    `batch_step` should be called after a batch has been used for training.\n    To resume training, save `last_batch_iteration` and use it to instantiate `CycleLR`.\n\n    This class has three built-in policies, as put forth in the paper:\n    ""triangular"":\n        A basic triangular cycle w/ no amplitude scaling.\n    ""triangular2"":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    ""exp_range"":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n        cycle iteration.\n\n    This implementation was adapted from the github repo: `bckenstler/CLR`_\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        base_lr (float or list): Initial learning rate which is the\n            lower boundary in the cycle for eachparam groups.\n            Default: 0.001\n        max_lr (float or list): Upper boundaries in the cycle for\n            each parameter group. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore\n            max_lr may not actually be reached depending on\n            scaling function. Default: 0.006\n        step_size (int): Number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch. Default: 2000\n        mode (str): One of {triangular, triangular2, exp_range}.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n            Default: \'triangular\'\n        gamma (float): Constant in \'exp_range\' scaling function:\n            gamma**(cycle iterations)\n            Default: 1.0\n        scale_fn (function): Custom scaling policy defined by a single\n            argument lambda function, where\n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored\n            Default: None\n        scale_mode (str): {\'cycle\', \'iterations\'}.\n            Defines whether scale_fn is evaluated on\n            cycle number or cycle iterations (training\n            iterations since start of cycle).\n            Default: \'cycle\'\n        last_batch_iteration (int): The index of the last batch. Default: -1\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = torch.optim.CyclicLR(optimizer)\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         scheduler.batch_step()\n        >>>         train_batch(...)\n\n    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n    """"""\n\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode=\'triangular\', gamma=1.,\n                 scale_fn=None, scale_mode=\'cycle\', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} base_lr, got {}"".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} max_lr, got {}"".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in [\'triangular\', \'triangular2\', \'exp_range\'] \\\n                and scale_fn is None:\n            raise ValueError(\'mode is invalid and scale_fn is None\')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == \'triangular\':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'triangular2\':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'exp_range\':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = \'iterations\'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n\n    @staticmethod\n    def _triangular_scale_fn(x):\n        return 1.\n\n    @staticmethod\n    def _triangular2_scale_fn(x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == \'cycle\':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs\n'"
scripts/deploy_model.py,7,"b'import argparse\nimport os\n\nimport torch.nn.functional as F\nimport scipy.misc as misc\nimport numpy as np\nimport torch\nimport time\nimport cv2\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom models.rfmobilenetv2plus import RFMobileNetV2Plus\nfrom models.mobilenetv2plus import MobileNetV2Plus\nfrom models.sewrnetv2 import SEWiderResNetV2\nfrom modules import InPlaceABNWrapper\nfrom torch.autograd import Variable\nfrom functools import partial\n\ntry:\n    import pydensecrf.densecrf as dcrf\nexcept:\n    print(""Failed to import pydensecrf, CRF post-processing will not work"")\n\n\ndef test(args):\n    net_h, net_w = 896, 1792   # 512, 1024  768, 1536  896, 1792  1024, 2048\n    # Setup image\n    print(""Read Input Image from : {}"".format(args.img_path))\n    img_path = ""/afs/cg.cs.tu-bs.de/home/zhang/SEDPShuffleNet/deploy/munster_000168_000019_leftImg8bit.png""\n    mask_path = ""/afs/cg.cs.tu-bs.de/home/zhang/SEDPShuffleNet/deploy/munster_000168_000019_gtFine_color.png""\n    img = misc.imread(img_path)\n    msk = cv2.imread(mask_path)\n\n    data_path = ""/zfs/zhang/Cityscapes""\n    loader = CityscapesLoader(data_path, is_transform=True, split=\'val\',\n                              img_size=(net_h, net_w),\n                              augmentations=None)\n    n_classes = loader.n_classes\n\n    # Setup Model\n    print(""> 1. Setting up Model..."")\n    model = MobileNetV2Plus(n_class=n_classes, in_size=(net_h, net_w), width_mult=1.0,\n                            out_sec=256, aspp_sec=(12*2, 24*2, 36*2),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    # state = convert_state_dict(torch.load(""/media/datavolume3/huijun/SEDPShuffleNet/weights/{}"".format(\n    #     args.model_path))[\'model_state\'])\n    pre_weight = torch.load(""/zfs/zhang/TrainLog/weights/{}"".format(\n        ""cityscapes_mobilenetv2_best_model.pkl""))\n    pre_weight = pre_weight[\'model_state\']\n    model.load_state_dict(pre_weight)\n\n    # state = model.state_dict()\n    # torch.save(state, ""cityscapes_sewrnet_best_model.pkl"")\n\n    resized_img = misc.imresize(img, (loader.img_size[0], loader.img_size[1]), interp=\'bilinear\')\n\n    img = img[:, :, ::-1]\n    img = np.array(img, dtype=np.uint8)\n    img = misc.imresize(img, (loader.img_size[0], loader.img_size[1]))\n    img = img.astype(np.float32)\n    img -= loader.mean\n    img = img.astype(np.float32) / 255.0\n\n    # NHWC -> NCWH\n    img = img.transpose(2, 0, 1)\n    img = np.expand_dims(img, 0)\n    img = torch.from_numpy(img).float()\n\n    model.eval()\n\n    images = Variable(img.cuda(), volatile=True)\n\n    start_time = time.time()\n    outputs = F.softmax(model(images), dim=1)\n    pred = np.squeeze(outputs.data.max(1)[1].cpu().numpy(), axis=0)\n    print(""Inference time: {}s"".format(time.time()-start_time))\n\n    decoded = loader.decode_segmap(pred)*255\n    decoded = decoded.astype(np.uint8)\n    img_msk = cv2.addWeighted(resized_img, 0.60, decoded, 0.40, 0)\n    fun_classes = np.unique(pred)\n    print(\'> {} Classes found: {}\'.format(len(fun_classes), fun_classes))\n\n    out_path = ""/afs/cg.cs.tu-bs.de/home/zhang/SEDPShuffleNet/output/{}"".format(""munster_000168_000019_imgmsk.png"")\n    misc.imsave(out_path, img_msk)\n    out_path = ""/afs/cg.cs.tu-bs.de/home/zhang/SEDPShuffleNet/output/{}"".format(""munster_000168_000019_msk.png"")\n    misc.imsave(out_path, decoded)\n    print(""> Segmentation Mask Saved at: {}"".format(out_path))\n\n    msk = misc.imresize(msk, (loader.img_size[0], loader.img_size[1]))\n    cv2.namedWindow(""Org Mask"", cv2.WINDOW_NORMAL)\n    cv2.imshow(""Org Mask"", msk)\n    cv2.namedWindow(""Pre Mask"", cv2.WINDOW_NORMAL)\n    cv2.imshow(""Pre Mask"", decoded[:, :, ::-1])\n    cv2.namedWindow(""Image Mask"", cv2.WINDOW_NORMAL)\n    cv2.imshow(""Image Mask"", img_msk[:, :, ::-1])\n    cv2.waitKey(0)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Params\')\n\n    parser.add_argument(\'--model_path\', nargs=\'?\', type=str, default=\'cityscapes_best_model.pkl\',\n                        help=\'Path to the saved model\')\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'cityscapes\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_path\', nargs=\'?\', type=str, default=None,\n                        help=\'Path of the input image\')\n    parser.add_argument(\'--out_path\', nargs=\'?\', type=str, default=None,\n                        help=\'Path of the output segmap\')\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    deploy_args = parser.parse_args()\n    test(deploy_args)\n'"
scripts/loss.py,14,"b'from torch.autograd import Variable\nimport torch.nn.functional as F\nimport scripts.utils as utils\nimport torch.nn as nn\nimport numpy as np\nimport torch\n\n\nclass CrossEntropy2d(nn.Module):\n    def __init__(self, size_average=True, ignore_label=255):\n        super(CrossEntropy2d, self).__init__()\n        self.size_average = size_average\n        self.ignore_label = ignore_label\n\n    def forward(self, predict, target, weight=None):\n        """"""\n            Args:\n                predict:(n, c, h, w)\n                target:(n, h, w)\n                weight (Tensor, optional): a manual rescaling weight given to each class.\n                                           If given, has to be a Tensor of size ""nclasses""\n        """"""\n        assert not target.requires_grad\n        assert predict.dim() == 4\n        assert target.dim() == 3\n        assert predict.size(0) == target.size(0), ""{0} vs {1} "".format(predict.size(0), target.size(0))\n        assert predict.size(2) == target.size(1), ""{0} vs {1} "".format(predict.size(2), target.size(1))\n        assert predict.size(3) == target.size(2), ""{0} vs {1} "".format(predict.size(3), target.size(3))\n\n        n, c, h, w = predict.size()\n        target_mask = (target >= 0) * (target != self.ignore_label)\n\n        target = target[target_mask]\n        predict = predict.transpose(1, 2).transpose(2, 3).contiguous()\n        predict = predict[target_mask.view(n, h, w, 1).repeat(1, 1, 1, c)].view(-1, c)\n\n        loss = F.cross_entropy(predict, target, weight=weight, size_average=self.size_average)\n        return loss\n\n\ndef cross_entropy2d(input, target, weight=None, size_average=True):\n    # 1. input: (n, c, h, w), target: (n, h, w)\n    n, c, h, w = input.size()\n\n    # 2. log_p: (n, c, h, w)\n    log_p = F.log_softmax(input, dim=1)\n\n    # 3. log_p: (n*h*w, c) - contiguous() required if transpose() is used before view().\n    log_p = log_p.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n    log_p = log_p[target.view(n * h * w, 1).repeat(1, c) >= 0]\n    log_p = log_p.view(-1, c)\n\n    # 4. target: (n*h*w,)\n    mask = target >= 0\n    target = target[mask]\n\n    loss = F.nll_loss(log_p, target, ignore_index=250, weight=weight, size_average=False)\n    if size_average:\n        loss /= mask.data.sum()\n        # loss /= mask.sum().data[0]\n    return loss\n\n\ndef bootstrapped_cross_entropy2d(input, target, K, weight=None, size_average=False):\n    """"""A categorical cross entropy loss for 4D tensors.\n        We assume the following layout: (batch, classes, height, width)\n        Args:\n            input: The outputs.\n            target: The predictions.\n            K: The number of pixels to select in the bootstrapping process.\n               The total number of pixels is determined as 512 * multiplier.\n        Returns:\n            The pixel-bootstrapped cross entropy loss.\n    """"""\n    batch_size = input.size()[0]\n\n    def _bootstrap_xentropy_single(input, target, K, weight=None, size_average=False):\n        n, c, h, w = input.size()\n\n        # 1. The log softmax. log_p: (n, c, h, w)\n        log_p = F.log_softmax(input, dim=1)\n\n        # 2. log_p: (n*h*w, c) - contiguous() required if transpose() is used before view().\n        log_p = log_p.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n        log_p = log_p[target.view(n * h * w, 1).repeat(1, c) >= 0]\n        log_p = log_p.view(-1, c)\n\n        # 3. target: (n*h*w,)\n        mask = target >= 0\n        target = target[mask]\n\n        loss = F.nll_loss(log_p, target, weight=weight, ignore_index=250,\n                          reduce=False, size_average=size_average)\n\n        # For each element in the batch, collect the top K worst predictions\n        topk_loss, _ = loss.topk(K)\n        reduced_topk_loss = topk_loss.sum() / K\n\n        return reduced_topk_loss\n\n    loss = 0.0\n    # Bootstrap from each image not entire batch\n    for i in range(batch_size):\n        loss += _bootstrap_xentropy_single(input=torch.unsqueeze(input[i], 0),\n                                           target=torch.unsqueeze(target[i], 0),\n                                           K=K,\n                                           weight=weight,\n                                           size_average=size_average)\n    return loss / float(batch_size)\n\n\nclass FocalLoss2D(nn.Module):\n    """"""\n    Focal Loss, which is proposed in:\n        ""Focal Loss for Dense Object Detection (https://arxiv.org/abs/1708.02002v2)""\n    """"""\n    def __init__(self, num_classes=19, ignore_label=250, alpha=0.25, gamma=2, size_average=True):\n        """"""\n        Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n\n        :param num_classes:   (int) num of the classes\n        :param ignore_label:  (int) ignore label\n        :param alpha:         (1D Tensor or Variable) the scalar factor\n        :param gamma:         (float) gamma > 0;\n                                      reduces the relative loss for well-classified examples (probabilities > .5),\n                                      putting more focus on hard, mis-classified examples\n        :param size_average:  (bool): By default, the losses are averaged over observations for each mini-batch.\n                                      If the size_average is set to False, the losses are\n                                      instead summed for each mini-batch.\n        """"""\n        super(FocalLoss2D, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.num_classes = num_classes\n        self.ignore_label = ignore_label\n        self.size_average = size_average\n        self.one_hot = Variable(torch.eye(self.num_classes))\n\n    def forward(self, cls_preds, cls_targets):\n        """"""\n\n        :param cls_preds:    (n, c, h, w)\n        :param cls_targets:  (n, h, w)\n        :return:\n        """"""\n        assert not cls_targets.requires_grad\n        assert cls_targets.dim() == 3\n        assert cls_preds.size(0) == cls_targets.size(0), ""{0} vs {1} "".format(cls_preds.size(0), cls_targets.size(0))\n        assert cls_preds.size(2) == cls_targets.size(1), ""{0} vs {1} "".format(cls_preds.size(2), cls_targets.size(1))\n        assert cls_preds.size(3) == cls_targets.size(2), ""{0} vs {1} "".format(cls_preds.size(3), cls_targets.size(3))\n\n        if cls_preds.is_cuda:\n            self.one_hot = self.one_hot.cuda()\n\n        n, c, h, w = cls_preds.size()\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. target reshape and one-hot encode\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1.1. target: (n*h*w,)\n        cls_targets = cls_targets.view(n * h * w, 1)\n        target_mask = (cls_targets >= 0) * (cls_targets != self.ignore_label)\n\n        cls_targets = cls_targets[target_mask]\n        cls_targets = self.one_hot.index_select(dim=0, index=cls_targets)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. compute focal loss for multi-classification\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2.1. The softmax. prob: (n, c, h, w)\n        prob = F.softmax(cls_preds, dim=1)\n        # 2.2. prob: (n*h*w, c) - contiguous() required if transpose() is used before view().\n        prob = prob.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n        prob = prob[target_mask.repeat(1, c)]\n        prob = prob.view(-1, c)  # (n*h*w, c)\n\n        probs = torch.clamp((prob * cls_targets).sum(1).view(-1, 1), min=1e-8, max=1.0)\n        batch_loss = -self.alpha * (torch.pow((1 - probs), self.gamma)) * probs.log()\n\n        if self.size_average:\n            loss = batch_loss.mean()\n        else:\n            loss = batch_loss.sum()\n\n        return loss\n\n\nclass SemanticEncodingLoss(nn.Module):\n    def __init__(self, num_classes=19, ignore_label=250, alpha=0.25):\n        super(SemanticEncodingLoss, self).__init__()\n        self.alpha = alpha\n\n        self.num_classes = num_classes\n        self.ignore_label = ignore_label\n\n    def unique_encode(self, cls_targets):\n        batch_size, _, _ = cls_targets.size()\n        target_mask = (cls_targets >= 0) * (cls_targets != self.ignore_label)\n        cls_targets = [cls_targets[idx].masked_select(target_mask[idx]) for idx in np.arange(batch_size)]\n\n        # unique_cls = [np.unique(label.numpy(), return_counts=True) for label in cls_targets]\n        unique_cls = [np.unique(label.numpy()) for label in cls_targets]\n\n        encode = np.zeros((batch_size, self.num_classes), dtype=np.uint8)\n\n        for idx in np.arange(batch_size):\n            np.put(encode[idx], unique_cls[idx], 1)\n\n        return torch.from_numpy(encode).float()\n\n    def forward(self, predicts, enc_cls_target, size_average=True):\n        se_loss = F.binary_cross_entropy_with_logits(predicts, enc_cls_target, weight=None,\n                                                     size_average=size_average)\n\n        return self.alpha * se_loss\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Lovasz-Softmax\n# Maxim Berman 2018 ESAT-PSI KU Leuven\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n\n    if p > 1:  # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    """"""\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / union\n        ious.append(iou)\n    iou = utils.mean(ious)    # mean accross images if per_image\n    return 100 * iou\n\n\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    """"""\n    Array of IoU for each (non ignored) class\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []\n        for i in range(C):\n            if i != ignore:  # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / union)\n        ious.append(iou)\n    ious = map(utils.mean, zip(*ious))  # mean accross images if per_image\n    return 100 * np.array(ious)\n\n\ndef lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    """"""\n    if per_image:\n        loss = utils.mean(lovasz_softmax_flat(*flatten_probas(prob, lab, ignore), only_present=only_present)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, only_present=False):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n    """"""\n    C = probas.size(1)\n    losses = []\n    for c in range(C):\n        fg = (labels == c).float() # foreground for class c\n        if only_present and fg.sum() == 0:\n            continue\n        errors = (fg - probas[:, c]).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, lovasz_grad(fg_sorted)))\n    return utils.mean(losses)\n\n\ndef flatten_probas(scores, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    B, C, H, W = scores.size()\n    scores = scores.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\nif __name__ == ""__main__"":\n    from torch.autograd import Variable\n\n    while True:\n        dummy_in = Variable(torch.randn(2, 3, 32, 32), requires_grad=True)\n        dummy_gt = Variable(torch.LongTensor(2, 32, 32).random_(0, 3))\n\n        dummy_in = F.softmax(dummy_in, dim=1)\n        loss = lovasz_softmax(dummy_in, dummy_gt, ignore=255)\n        print(loss.data[0])\n'"
scripts/metrics.py,0,"b'import numpy as np\n\n\nclass RunningScore(object):\n    def __init__(self, n_classes):\n        self.n_classes = n_classes\n        self.confusion_matrix = np.zeros((n_classes, n_classes))\n\n    @staticmethod\n    def _fast_hist(label_true, label_pred, n_class):\n        mask = (label_true >= 0) & (label_true < n_class)\n        hist = np.bincount(n_class * label_true[mask].astype(int) + label_pred[mask],\n                           minlength=n_class**2).reshape(n_class, n_class)\n        return hist\n\n    def update(self, label_trues, label_preds):\n        for lt, lp in zip(label_trues, label_preds):\n            self.confusion_matrix += self._fast_hist(lt.flatten(), lp.flatten(), self.n_classes)\n\n    def get_scores(self):\n        """"""Returns accuracy score evaluation result.\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n        """"""\n        hist = self.confusion_matrix\n        tp = np.diag(hist)\n        sum_a1 = hist.sum(axis=1)\n\n        acc = tp.sum() / (hist.sum() + np.finfo(np.float32).eps)\n\n        acc_cls = tp / (sum_a1 + np.finfo(np.float32).eps)\n        acc_cls = np.nanmean(acc_cls)\n\n        iu = tp / (sum_a1 + hist.sum(axis=0) - tp + np.finfo(np.float32).eps)\n        mean_iu = np.nanmean(iu)\n\n        freq = sum_a1 / (hist.sum() + np.finfo(np.float32).eps)\n        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n        cls_iu = dict(zip(range(self.n_classes), iu))\n\n        return {\'Overall_Acc\': acc,\n                \'Mean_Acc\': acc_cls,\n                \'FreqW_Acc\': fwavacc,\n                \'Mean_IoU\': mean_iu}, cls_iu\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n\n\nif __name__ == ""__main__"":\n    n_class = 2\n    score = RunningScore(n_class)\n\n    label_true = np.array([1, 0, 0, 1, 1, 0, 1, 0, 1, 0])\n    label_pred = np.array([1, 1, 0, 1, 0, 0, 1, 1, 0, 0])\n\n    score.update(label_true, label_pred)\n    print(score.confusion_matrix)\n\n'"
scripts/model_measure.py,3,"b'import torch\nimport operator\nimport torch.nn as nn\nfrom functools import reduce\nfrom torch.autograd import Variable\n\n\ncount_ops = 0\ncount_params = 0\n\n\ndef get_num_gen(gen):\n    return sum(1 for x in gen)\n\n\ndef is_pruned(layer):\n    try:\n        layer.mask\n        return True\n    except AttributeError:\n        return False\n\n\ndef is_leaf(model):\n    return get_num_gen(model.children()) == 0\n\n\ndef get_layer_info(layer):\n    layer_str = str(layer)\n    type_name = layer_str[:layer_str.find(\'(\')].strip()\n    return type_name\n\n\ndef get_layer_param(model):\n    return sum([reduce(operator.mul, i.size(), 1) for i in model.parameters()])\n\n\n### The input batch size should be 1 to call this function\ndef measure_layer(layer, x):\n    global count_ops, count_params\n    delta_ops = 0\n    delta_params = 0\n    multi_add = 1\n    type_name = get_layer_info(layer)\n\n    ### ops_conv\n    if type_name in [\'Conv2d\']:\n        out_h = int((x.size()[2] + 2 * layer.padding[0] - layer.kernel_size[0]) /\n                    layer.stride[0] + 1)\n        out_w = int((x.size()[3] + 2 * layer.padding[1] - layer.kernel_size[1]) /\n                    layer.stride[1] + 1)\n        delta_ops = layer.in_channels * layer.out_channels * layer.kernel_size[0] *  \\\n                layer.kernel_size[1] * out_h * out_w / layer.groups * multi_add\n        delta_params = get_layer_param(layer)\n\n    ### ops_nonlinearity\n    elif type_name in [\'ReLU\', \'ReLU6\', \'LeakyReLU\', \'Sigmoid\']:\n        delta_ops = x.numel()\n        delta_params = get_layer_param(layer)\n\n    ### ops_pooling\n    elif type_name in [\'AvgPool2d\']:\n        in_w = x.size()[2]\n        kernel_ops = layer.kernel_size * layer.kernel_size\n        out_w = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n        out_h = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n        delta_ops = x.size()[0] * x.size()[1] * out_w * out_h * kernel_ops\n        delta_params = get_layer_param(layer)\n\n    elif type_name in [\'AdaptiveAvgPool2d\']:\n        delta_ops = x.size()[0] * x.size()[1] * x.size()[2] * x.size()[3]\n        delta_params = get_layer_param(layer)\n\n    ### ops_linear\n    elif type_name in [\'Linear\']:\n        weight_ops = layer.weight.numel() * multi_add\n        bias_ops = layer.bias.numel()\n        delta_ops = x.size()[0] * (weight_ops + bias_ops)\n        delta_params = get_layer_param(layer)\n\n    ### ops_nothing\n    elif type_name in [\'BatchNorm2d\', \'Dropout2d\', \'DropChannel\',\n                       \'Dropout\', \'InPlaceABN\', \'InPlaceABNSync\', \'Upsample\', \'MaxPool2d\']:\n        delta_params = get_layer_param(layer)\n\n    ### unknown layer type\n    else:\n        raise TypeError(\'unknown layer type: %s\' % type_name)\n\n    count_ops += delta_ops\n    count_params += delta_params\n    return\n\n\ndef measure_model(model, H, W):\n    global count_ops, count_params\n    count_ops = 0\n    count_params = 0\n    data = Variable(torch.zeros(1, 3, H, W).cuda())\n\n    def should_measure(x):\n        return is_leaf(x) or is_pruned(x)\n\n    def modify_forward(model):\n        for child in model.children():\n            if should_measure(child):\n                def new_forward(m):\n                    def lambda_forward(x):\n                        measure_layer(m, x)\n                        return m.old_forward(x)\n                    return lambda_forward\n                child.old_forward = child.forward\n                child.forward = new_forward(child)\n            else:\n                modify_forward(child)\n\n    def restore_forward(model):\n        for child in model.children():\n            # leaf node\n            if is_leaf(child) and hasattr(child, \'old_forward\'):\n                child.forward = child.old_forward\n                child.old_forward = None\n            else:\n                restore_forward(child)\n\n    modify_forward(model)\n    model.forward(data)\n    restore_forward(model)\n\n    return count_ops, count_params\n\n\nif __name__ == \'__main__\':\n    from functools import partial\n    from modules import InPlaceABNWrapper\n    from models.sewrnetv2 import SEWiderResNetV2\n    from models.mobilenetv2aspp import MobileNetV2ASPP\n    from models.mobilenetv2plus import MobileNetV2Plus\n    from models.shufflenetv2plus import ShuffleNetV2Plus\n    from models.rfmobilenetv2plus import RFMobileNetV2Plus\n    from models.mixscaledensenet import MixedScaleDenseNet\n\n    net_h, net_w = 448, 896\n\n    """"""\n    model = RFMobileNetV2Plus(n_class=19, in_size=(net_h, net_w), width_mult=1.0,\n                            out_sec=256, aspp_sec=(12, 24, 36),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n\n\n    model = SEWiderResNetV2(structure=[3, 3, 6, 3, 1, 1],\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1),\n                            classes=19, dilation=True, is_se=True, in_size=(net_h, net_w),\n                            aspp_out=512, fusion_out=64, aspp_sec=(12, 24, 36))\n\n    model = ShuffleNetV2Plus(n_class=19, groups=3, in_channels=3, in_size=(net_h, net_w),\n                             out_sec=256, aspp_sec=(12, 24, 36),\n                             norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    \n    model = MixedScaleDenseNet(n_class=19, in_size=(net_h, net_w), num_layers=96, in_chns=32,\n                               squeeze_ratio=1.0 / 32, out_chns=1,\n                               dilate_sec=(1, 2, 4, 8, 16),\n                               aspp_sec=(24, 48, 72),\n                               norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    """"""\n    model = MobileNetV2ASPP(n_class=19, in_size=(net_h, net_w), width_mult=1.0,\n                            out_sec=256, aspp_sec=(12, 24, 36),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    model.cuda()\n\n    count_ops, count_params = measure_model(model, net_h, net_w)\n    print(\'FLOPs: {}, Params: {}\'.format(count_ops, count_params))\n'"
scripts/test_inplace.py,5,"b'import torch\nimport os\n\nfrom models.sedpshufflenet import SEDPNShuffleNet\nfrom scripts.loss import cross_entropy2d\nfrom modules import InPlaceABNWrapper\nfrom torch.autograd import Variable\nfrom functools import partial\n\nif __name__ == ""__main__"":\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    model = SEDPNShuffleNet(small=False, classes=19, in_size=(448, 896), num_init_features=64,\n                            k_r=96, groups=4, k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n                            out_sec=(512, 256, 128), dil_sec=(1, 1, 1, 2, 4), aspp_sec=(6, 12, 18),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    for name, param in model.named_parameters():\n        print(""Name: {}, Size: {}"".format(name, param.size()))\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.90, weight_decay=5e-4)\n    loss_fn = cross_entropy2d\n\n    i = 0\n    while True:\n        i += 1\n        print(""iter :"", i)\n        model.train()\n\n        dummy_input = Variable(torch.rand(2, 3, 448, 896).cuda(), requires_grad=True)\n        dummy_target = Variable(torch.rand(2, 448, 896).cuda(), requires_grad=False).long()\n        output = model(dummy_input)[3]\n\n        optimizer.zero_grad()\n\n        loss = loss_fn(output, dummy_target)\n        loss.backward()\n\n        optimizer.step()\n'"
scripts/train_auxiliary.py,15,"b'import torch.nn.functional as F\nimport argparse\nimport random\nimport torch\nimport time\nimport os\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom scripts.loss import SemanticEncodingLoss, bootstrapped_cross_entropy2d\nfrom scripts.utils import update_aggregated_weight_average\nfrom models.mobilenetv2exfuse import MobileNetV2ExFuse\nfrom scripts.utils import cosine_annealing_lr\nfrom scripts.utils import poly_topk_scheduler\nfrom scripts.utils import set_optimizer_lr\nfrom scripts.cyclical_lr import CyclicLR\nfrom scripts.metrics import RunningScore\nfrom modules import InPlaceABNWrapper\nfrom datasets.augmentations import *\nfrom functools import partial\n\n\ndef train(args, data_root, save_root):\n    weight_dir = ""{}weights/"".format(save_root)\n    log_dir = ""{}logs/MobileNetV2ExFuse-{}"".format(save_root, time.strftime(""%Y-%m-%d-%H-%M-%S"", time.localtime()))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Augmentations\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    net_h, net_w = int(args.img_rows*args.crop_ratio), int(args.img_cols*args.crop_ratio)\n\n    augment_train = Compose([RandomHorizontallyFlip(), RandomSized((0.5, 0.75)),\n                             RandomRotate(5), RandomCrop((net_h, net_w))])\n    augment_valid = Compose([RandomHorizontallyFlip(), Scale((args.img_rows, args.img_cols)),\n                             CenterCrop((net_h, net_w))])\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    train_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'train\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_train)\n    valid_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'val\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_valid)\n\n    n_classes = train_loader.n_classes\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Metrics\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    running_metrics = RunningScore(n_classes)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 4. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n\n    model = MobileNetV2ExFuse(n_class=19, in_size=(net_h, net_w), width_mult=1.0, out_sec=256,\n                              norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1),\n                              traval=""train"")\n\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    # 4.1 Setup Optimizer\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # Check if model has custom optimizer / loss\n    if hasattr(model.module, \'optimizer\'):\n        optimizer = model.module.optimizer\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.90,\n                                    weight_decay=5e-4, nesterov=True)\n\n    # 4.2 Setup Loss\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    class_weight = None\n    se_loss = None\n    ce_loss = None\n    if hasattr(model.module, \'loss\'):\n        print(\'> Using custom loss\')\n        loss_fn = model.module.loss\n    else:\n        # loss_fn = cross_entropy2d\n\n        class_weight = np.array([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n                                 5.58540548, 3.56563995, 0.12704978, 1.,         0.46783719, 1.34551528,\n                                 5.29974114, 0.28342531, 0.9396095,  0.81551811, 0.42679146, 3.6399074,\n                                 2.78376194], dtype=float)\n\n        """"""\n        class_weight = np.array([3.045384,  12.862123,   4.509889,  38.15694,  35.25279,  31.482613,\n                                 45.792305,  39.694073,  6.0639296,  32.16484,  17.109228,   31.563286,\n                                 47.333973,  11.610675,  44.60042,   45.23716,  45.283024,  48.14782,\n                                 41.924667], dtype=float)/10.0\n        """"""\n        class_weight = torch.from_numpy(class_weight).float().cuda()\n        se_loss = SemanticEncodingLoss(num_classes=19, ignore_label=250, alpha=0.20)\n        ce_loss = bootstrapped_cross_entropy2d\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 5. Resume Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    best_iou = -100.0\n    args.start_epoch = 0\n    if args.resume is not None:\n        full_path = ""{}{}"".format(weight_dir, args.resume)\n        if os.path.isfile(full_path):\n            print(""> Loading model and optimizer from checkpoint \'{}\'"".format(args.resume))\n\n            checkpoint = torch.load(full_path)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_iou = checkpoint[\'best_iou\']\n            model.load_state_dict(checkpoint[\'model_state\'])          # weights\n            optimizer.load_state_dict(checkpoint[\'optimizer_state\'])  # gradient state\n\n            # for param_group in optimizer.param_groups:\n            # s    param_group[\'lr\'] = 1e-5\n\n            del checkpoint\n            print(""> Loaded checkpoint \'{}\' (epoch {}, iou {})"".format(args.resume,\n                                                                       args.start_epoch,\n                                                                       best_iou))\n\n        else:\n            print(""> No checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        if args.pre_trained is not None:\n            print(""> Loading weights from pre-trained model \'{}\'"".format(args.pre_trained))\n            full_path = ""{}{}"".format(weight_dir, args.pre_trained)\n\n            pre_weight = torch.load(full_path)\n            pre_weight = pre_weight[""model_state""]\n            # pre_weight = pre_weight[""state_dict""]\n\n            model_dict = model.state_dict()\n\n            pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            model.load_state_dict(model_dict)\n\n            del pre_weight\n            del model_dict\n            del pretrained_dict\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 3. Setup visdom for visualization\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    writer = None\n    if args.visdom:\n        writer = SummaryWriter(log_dir=log_dir, comment=""MobileNetV2ExFuse"")\n\n    if args.visdom:\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        writer.add_graph(model, dummy_input)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 6. Train Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 2. Model Training start..."")\n    train_loader = data.DataLoader(train_loader, batch_size=args.batch_size, num_workers=6, shuffle=True)\n    valid_loader = data.DataLoader(valid_loader, batch_size=args.batch_size, num_workers=6)\n\n    num_batches = int(math.ceil(len(train_loader.dataset.files[train_loader.dataset.split]) /\n                                float(train_loader.batch_size)))\n\n    lr_period = 20 * num_batches\n    swa_weights = model.state_dict()\n\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n    # scheduler = CyclicLR(optimizer, base_lr=1.0e-3, max_lr=6.0e-3, step_size=2*num_batches)\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=32, gamma=0.1)\n    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n\n    topk_init = 512\n    # topk_multipliers = [64, 128, 256, 512]\n    for epoch in np.arange(args.start_epoch, args.n_epoch):\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.1 Mini-Batch Learning\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Training Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.train()\n\n        last_loss = 0.0\n        topk_base = topk_init\n        pbar = tqdm(np.arange(num_batches))\n        for train_i, (images, labels) in enumerate(train_loader):  # One mini-Batch data, One iteration\n            full_iter = (epoch * num_batches) + train_i + 1\n\n            # poly_lr_scheduler(optimizer, init_lr=args.l_rate, iter=full_iter,\n            #                   lr_decay_iter=1, max_iter=args.n_epoch*num_batches, power=0.9)\n\n            batch_lr = args.l_rate * cosine_annealing_lr(lr_period, full_iter)\n            optimizer = set_optimizer_lr(optimizer, batch_lr)\n\n            topk_base = poly_topk_scheduler(init_topk=topk_init, iter=full_iter, topk_decay_iter=1,\n                                            max_iter=args.n_epoch*num_batches, power=0.95)\n\n            images = Variable(images.cuda(), requires_grad=True)   # Image feed into the deep neural network\n            se_labels = se_loss.unique_encode(labels)\n            se_labels = Variable(se_labels.cuda(), requires_grad=False)\n            ce_labels = Variable(labels.cuda(), requires_grad=False)\n\n            optimizer.zero_grad()\n            enc1, enc2, enc3, enc4, enc5, enc6, enc7, net_out = model(images)  # Here we have 3 output for 3 loss\n\n            topk = topk_base * 512\n            if random.random() < 0.20:\n                train_ce_loss = ce_loss(input=net_out, target=ce_labels, K=topk,\n                                        weight=class_weight, size_average=True)\n            else:\n                train_ce_loss = ce_loss(input=net_out, target=ce_labels, K=topk,\n                                        weight=None, size_average=True)\n\n            train_se_loss1 = se_loss(predicts=enc1, enc_cls_target=se_labels, size_average=True)\n            train_se_loss2 = se_loss(predicts=enc2, enc_cls_target=se_labels, size_average=True)\n            train_se_loss3 = se_loss(predicts=enc3, enc_cls_target=se_labels, size_average=True)\n            train_se_loss4 = se_loss(predicts=enc4, enc_cls_target=se_labels, size_average=True)\n            train_se_loss5 = se_loss(predicts=enc5, enc_cls_target=se_labels, size_average=True)\n            train_se_loss6 = se_loss(predicts=enc6, enc_cls_target=se_labels, size_average=True)\n            train_se_loss7 = se_loss(predicts=enc7, enc_cls_target=se_labels, size_average=True)\n\n            train_loss = (train_ce_loss + train_se_loss1 + train_se_loss2 + train_se_loss3 +\n                          train_se_loss4 + train_se_loss5 + train_se_loss6 + train_se_loss7)\n\n            last_loss = train_loss.data[0]\n            last_ce_loss = train_ce_loss.data[0]\n            last_se_loss1 = train_se_loss1.data[0]\n            last_se_loss2 = train_se_loss2.data[0]\n            last_se_loss3 = train_se_loss3.data[0]\n            last_se_loss4 = train_se_loss4.data[0]\n            last_se_loss5 = train_se_loss5.data[0]\n            last_se_loss6 = train_se_loss6.data[0]\n            last_se_loss7 = train_se_loss7.data[0]\n            pbar.update(1)\n            pbar.set_description(""> Epoch [%d/%d]"" % (epoch + 1, args.n_epoch))\n            pbar.set_postfix(Loss=last_loss, CELoss=last_ce_loss,\n                             SELoss1=last_se_loss1, SELoss2=last_se_loss2,\n                             SELoss3=last_se_loss3, SELoss4=last_se_loss4,\n                             SELoss5=last_se_loss5, SELoss6=last_se_loss6,\n                             SELoss7=last_se_loss7, TopK=topk_base, LR=batch_lr)\n\n            train_loss.backward()\n            optimizer.step()\n\n            if full_iter % lr_period == 0:\n                swa_weights = update_aggregated_weight_average(model, swa_weights, full_iter, lr_period)\n                state = {\'model_state\': swa_weights}\n                torch.save(state, ""{}{}_mobilenetv2context_swa_model.pkl"".format(weight_dir, args.dataset))\n\n            if (train_i + 1) % 31 == 0:\n                loss_log = ""Epoch [%d/%d], Iter: %d, Loss: \\t %.4f, CELoss: \\t %.4f, "" \\\n                           ""SELoss1: \\t %.4f, SELoss2: \\t%.4f, SELoss3: \\t%.4f, "" \\\n                           ""SELoss4: \\t%.4f, SELoss5: \\t%.4f, SELoss6: \\t%.4f,"" \\\n                           ""SELoss7: \\t%.4f, "" % (epoch + 1, args.n_epoch, train_i + 1, last_loss,\n                                                  last_ce_loss, last_se_loss1, last_se_loss2, last_se_loss3,\n                                                  last_se_loss4, last_se_loss5, last_se_loss6, last_se_loss7)\n\n                net_out = F.softmax(net_out, dim=1)\n                pred = net_out.data.max(1)[1].cpu().numpy()\n                gt = ce_labels.data.cpu().numpy()\n\n                running_metrics.update(gt, pred)\n                score, class_iou = running_metrics.get_scores()\n\n                metric_log = """"\n                for k, v in score.items():\n                    metric_log += "" {}: \\t %.4f, "".format(k) % v\n                running_metrics.reset()\n\n                logs = loss_log + metric_log\n                # print(logs)\n\n                if args.visdom:\n                    writer.add_scalar(\'Training/Losses\', last_loss, full_iter)\n                    writer.add_scalars(\'Training/Metrics\', score, full_iter)\n                    writer.add_text(\'Training/Text\', logs, full_iter)\n\n                    for name, param in model.named_parameters():\n                        writer.add_histogram(name, param.clone().cpu().data.numpy(), full_iter)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.2 Mini-Batch Validation\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Validation for Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.eval()\n\n        mval_loss = 0.0\n        vali_count = 0\n        for i_val, (images, labels) in enumerate(valid_loader):\n            vali_count += 1\n\n            images = Variable(images.cuda(), volatile=True)\n            ce_labels = Variable(labels.cuda(), requires_grad=False)\n\n            net_out = model(images)[7]  # Here we have 4 output for 4 loss\n\n            topk = topk_base * 512\n            val_loss = ce_loss(input=net_out, target=ce_labels, K=topk,\n                               weight=None, size_average=False)\n\n            mval_loss += val_loss.data[0]\n\n            net_out = F.softmax(net_out, dim=1)\n            pred = net_out.data.max(1)[1].cpu().numpy()\n            gt = ce_labels.data.cpu().numpy()\n            running_metrics.update(gt, pred)\n\n        mval_loss /= vali_count\n\n        loss_log = ""Epoch [%d/%d] Loss: \\t %.4f"" % (epoch + 1, args.n_epoch, mval_loss)\n        metric_log = """"\n        score, class_iou = running_metrics.get_scores()\n        for k, v in score.items():\n            metric_log += "" {} \\t %.4f, "".format(k) % v\n        running_metrics.reset()\n\n        logs = loss_log + metric_log\n        # print(logs)\n        pbar.set_postfix(Train_Loss=last_loss, Vali_Loss=mval_loss, Vali_mIoU=score[\'Mean_IoU\'])\n\n        if args.visdom:\n            writer.add_scalar(\'Validation/Losses\', mval_loss, epoch)\n            writer.add_scalars(\'Validation/Metrics\', score, epoch)\n            writer.add_text(\'Validation/Text\', logs, epoch)\n\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n            # export scalar data to JSON for external processing\n            # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n\n        if score[\'Mean_IoU\'] >= best_iou:\n            best_iou = score[\'Mean_IoU\']\n            state = {\'epoch\': epoch + 1,\n                     ""best_iou"": best_iou,\n                     \'model_state\': model.state_dict(),\n                     \'optimizer_state\': optimizer.state_dict()}\n            torch.save(state, ""{}{}_mobilenetv2exfuse_best_model.pkl"".format(weight_dir, args.dataset))\n\n        # scheduler.step()\n        # scheduler.batch_step()\n        pbar.close()\n\n    if args.visdom:\n        # export scalar data to JSON for external processing\n        # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n        writer.close()\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Training Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Hyper-params\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    parser = argparse.ArgumentParser(description=\'Hyperparams\')\n\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'cityscapes\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_rows\', nargs=\'?\', type=int, default=512,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--img_cols\', nargs=\'?\', type=int, default=1024,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--n_epoch\', nargs=\'?\', type=int, default=200,\n                        help=\'# of the epochs\')\n    parser.add_argument(\'--batch_size\', nargs=\'?\', type=int, default=5,\n                        help=\'Batch Size\')\n    parser.add_argument(\'--l_rate\', nargs=\'?\', type=float, default=2.5e-3,\n                        help=\'Learning Rate\')\n    parser.add_argument(\'--crop_ratio\', nargs=\'?\', type=float, default=0.875,\n                        help=\'The ratio to crop the input image\')\n    parser.add_argument(\'--resume\', nargs=\'?\', type=str, default=None,\n                        help=\'Path to previous saved model to restart from\')\n    parser.add_argument(\'--pre_trained\', nargs=\'?\', type=str, default=""cityscapes_rfmobilenetv2_best_model.pkl"",\n                        help=\'Path to pre-trained  model to init from\')\n    parser.add_argument(\'--visdom\', nargs=\'?\', type=bool, default=True,\n                        help=\'Show visualization(s) on visdom | True by  default\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Train the Deep Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    data_path = ""/zfs/zhang/Cityscapes""\n    save_path = ""/zfs/zhang/TrainLog/""\n    train_args = parser.parse_args()\n    train(train_args, data_path, save_path)\n'"
scripts/train_context.py,15,"b'import torch.nn.functional as F\nimport argparse\nimport random\nimport torch\nimport time\nimport os\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom scripts.loss import SemanticEncodingLoss, bootstrapped_cross_entropy2d\nfrom scripts.utils import update_aggregated_weight_average\nfrom models.rfmobilenetv2context import MobileNetV2Context\nfrom scripts.utils import cosine_annealing_lr\nfrom scripts.utils import poly_topk_scheduler\nfrom scripts.utils import set_optimizer_lr\nfrom scripts.cyclical_lr import CyclicLR\nfrom scripts.metrics import RunningScore\nfrom modules import InPlaceABNWrapper\nfrom datasets.augmentations import *\nfrom functools import partial\n\n\ndef train(args, data_root, save_root):\n    weight_dir = ""{}weights/"".format(save_root)\n    log_dir = ""{}logs/MobileNetV2Context-{}"".format(save_root, time.strftime(""%Y-%m-%d-%H-%M-%S"", time.localtime()))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Augmentations\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    net_h, net_w = int(args.img_rows*args.crop_ratio), int(args.img_cols*args.crop_ratio)\n\n    augment_train = Compose([RandomHorizontallyFlip(), RandomSized((0.5, 0.75)),\n                             RandomRotate(5), RandomCrop((net_h, net_w))])\n    augment_valid = Compose([RandomHorizontallyFlip(), Scale((args.img_rows, args.img_cols)),\n                             CenterCrop((net_h, net_w))])\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    train_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'train\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_train)\n    valid_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'val\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_valid)\n\n    n_classes = train_loader.n_classes\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Metrics\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    running_metrics = RunningScore(n_classes)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 4. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n\n    model = MobileNetV2Context(n_class=19, in_size=(net_h, net_w), width_mult=1., out_sec=256, context=(32, 4),\n                               norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    # 4.1 Setup Optimizer\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # Check if model has custom optimizer / loss\n    if hasattr(model.module, \'optimizer\'):\n        optimizer = model.module.optimizer\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.90,\n                                    weight_decay=5e-4, nesterov=True)\n\n    # 4.2 Setup Loss\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    class_weight = None\n    if hasattr(model.module, \'loss\'):\n        print(\'> Using custom loss\')\n        loss_fn = model.module.loss\n    else:\n        # loss_fn = cross_entropy2d\n\n        class_weight = np.array([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n                                 5.58540548, 3.56563995, 0.12704978, 1.,         0.46783719, 1.34551528,\n                                 5.29974114, 0.28342531, 0.9396095,  0.81551811, 0.42679146, 3.6399074,\n                                 2.78376194], dtype=float)\n\n        """"""\n        class_weight = np.array([3.045384,  12.862123,   4.509889,  38.15694,  35.25279,  31.482613,\n                                 45.792305,  39.694073,  6.0639296,  32.16484,  17.109228,   31.563286,\n                                 47.333973,  11.610675,  44.60042,   45.23716,  45.283024,  48.14782,\n                                 41.924667], dtype=float)/10.0\n        """"""\n        class_weight = torch.from_numpy(class_weight).float().cuda()\n        se_loss = SemanticEncodingLoss(num_classes=19, ignore_label=250, alpha=0.20)\n        ce_loss = bootstrapped_cross_entropy2d\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 5. Resume Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    best_iou = -100.0\n    args.start_epoch = 0\n    if args.resume is not None:\n        full_path = ""{}{}"".format(weight_dir, args.resume)\n        if os.path.isfile(full_path):\n            print(""> Loading model and optimizer from checkpoint \'{}\'"".format(args.resume))\n\n            checkpoint = torch.load(full_path)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_iou = checkpoint[\'best_iou\']\n            model.load_state_dict(checkpoint[\'model_state\'])          # weights\n            optimizer.load_state_dict(checkpoint[\'optimizer_state\'])  # gradient state\n\n            # for param_group in optimizer.param_groups:\n            # s    param_group[\'lr\'] = 1e-5\n\n            del checkpoint\n            print(""> Loaded checkpoint \'{}\' (epoch {}, iou {})"".format(args.resume,\n                                                                       args.start_epoch,\n                                                                       best_iou))\n\n        else:\n            print(""> No checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        if args.pre_trained is not None:\n            print(""> Loading weights from pre-trained model \'{}\'"".format(args.pre_trained))\n            full_path = ""{}{}"".format(weight_dir, args.pre_trained)\n\n            pre_weight = torch.load(full_path)\n            pre_weight = pre_weight[""model_state""]\n            # pre_weight = pre_weight[""state_dict""]\n\n            model_dict = model.state_dict()\n\n            pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            model.load_state_dict(model_dict)\n\n            del pre_weight\n            del model_dict\n            del pretrained_dict\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 3. Setup visdom for visualization\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    writer = None\n    if args.visdom:\n        writer = SummaryWriter(log_dir=log_dir, comment=""MobileNetV2Context"")\n\n    if args.visdom:\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        writer.add_graph(model, dummy_input)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 6. Train Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 2. Model Training start..."")\n    train_loader = data.DataLoader(train_loader, batch_size=args.batch_size, num_workers=6, shuffle=True)\n    valid_loader = data.DataLoader(valid_loader, batch_size=args.batch_size, num_workers=6)\n\n    num_batches = int(math.ceil(len(train_loader.dataset.files[train_loader.dataset.split]) /\n                                float(train_loader.batch_size)))\n\n    lr_period = 20 * num_batches\n    swa_weights = model.state_dict()\n\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n    # scheduler = CyclicLR(optimizer, base_lr=1.0e-3, max_lr=6.0e-3, step_size=2*num_batches)\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=32, gamma=0.1)\n    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n\n    topk_init = 512\n    # topk_multipliers = [64, 128, 256, 512]\n    for epoch in np.arange(args.start_epoch, args.n_epoch):\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.1 Mini-Batch Learning\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Training Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.train()\n\n        last_loss = 0.0\n        topk_base = topk_init\n        pbar = tqdm(np.arange(num_batches))\n        for train_i, (images, labels) in enumerate(train_loader):  # One mini-Batch data, One iteration\n            full_iter = (epoch * num_batches) + train_i + 1\n\n            # poly_lr_scheduler(optimizer, init_lr=args.l_rate, iter=full_iter,\n            #                   lr_decay_iter=1, max_iter=args.n_epoch*num_batches, power=0.9)\n\n            batch_lr = args.l_rate * cosine_annealing_lr(lr_period, full_iter)\n            optimizer = set_optimizer_lr(optimizer, batch_lr)\n\n            topk_base = poly_topk_scheduler(init_topk=topk_init, iter=full_iter, topk_decay_iter=1,\n                                            max_iter=args.n_epoch*num_batches, power=0.95)\n\n            images = Variable(images.cuda(), requires_grad=True)   # Image feed into the deep neural network\n            se_labels = se_loss.unique_encode(labels)\n            se_labels = Variable(se_labels.cuda(), requires_grad=False)\n            ce_labels = Variable(labels.cuda(), requires_grad=False)\n\n            optimizer.zero_grad()\n            enc1, enc2, net_out = model(images)  # Here we have 3 output for 3 loss\n\n            topk = topk_base * 512\n            if random.random() < 0.20:\n                train_ce_loss = ce_loss(input=net_out, target=ce_labels, K=topk,\n                                        weight=class_weight, size_average=True)\n            else:\n                train_ce_loss = ce_loss(input=net_out, target=ce_labels, K=topk,\n                                        weight=None, size_average=True)\n            \n            train_se_loss1 = se_loss(predicts=enc1, enc_cls_target=se_labels, size_average=True)\n            train_se_loss2 = se_loss(predicts=enc2, enc_cls_target=se_labels, size_average=True)\n\n            train_loss = train_ce_loss + train_se_loss1 + train_se_loss2\n\n            last_loss = train_loss.data[0]\n            last_ce_loss = train_ce_loss.data[0]\n            last_se_loss1 = train_se_loss1.data[0]\n            last_se_loss2 = train_se_loss2.data[0]\n            pbar.update(1)\n            pbar.set_description(""> Epoch [%d/%d]"" % (epoch + 1, args.n_epoch))\n            pbar.set_postfix(Loss=last_loss, CELoss=last_ce_loss,\n                             SELoss1=last_se_loss1, SELoss2=last_se_loss2,\n                             TopK=topk_base, LR=batch_lr)\n\n            train_loss.backward()\n            optimizer.step()\n\n            if full_iter % lr_period == 0:\n                swa_weights = update_aggregated_weight_average(model, swa_weights, full_iter, lr_period)\n                state = {\'model_state\': swa_weights}\n                torch.save(state, ""{}{}_mobilenetv2context_swa_model.pkl"".format(weight_dir, args.dataset))\n\n            if (train_i + 1) % 31 == 0:\n                loss_log = ""Epoch [%d/%d], Iter: %d, Loss: \\t %.4f, CELoss: \\t %.4f, "" \\\n                           ""SELoss1: \\t %.4f, SELoss2: \\t%.4f, "" % (epoch + 1, args.n_epoch,\n                                                                    train_i + 1, last_loss,\n                                                                    last_ce_loss,\n                                                                    last_se_loss1,\n                                                                    last_se_loss2)\n\n                net_out = F.softmax(net_out, dim=1)\n                pred = net_out.data.max(1)[1].cpu().numpy()\n                gt = ce_labels.data.cpu().numpy()\n\n                running_metrics.update(gt, pred)\n                score, class_iou = running_metrics.get_scores()\n\n                metric_log = """"\n                for k, v in score.items():\n                    metric_log += "" {}: \\t %.4f, "".format(k) % v\n                running_metrics.reset()\n\n                logs = loss_log + metric_log\n                # print(logs)\n\n                if args.visdom:\n                    writer.add_scalar(\'Training/Losses\', last_loss, full_iter)\n                    writer.add_scalars(\'Training/Metrics\', score, full_iter)\n                    writer.add_text(\'Training/Text\', logs, full_iter)\n\n                    for name, param in model.named_parameters():\n                        writer.add_histogram(name, param.clone().cpu().data.numpy(), full_iter)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.2 Mini-Batch Validation\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Validation for Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.eval()\n\n        mval_loss = 0.0\n        vali_count = 0\n        for i_val, (images, labels) in enumerate(valid_loader):\n            vali_count += 1\n\n            images = Variable(images.cuda(), volatile=True)\n            ce_labels = Variable(labels.cuda(), requires_grad=False)\n\n            enc1, enc2, net_out = model(images)  # Here we have 4 output for 4 loss\n\n            topk = topk_base * 512\n            val_loss = ce_loss(input=net_out, target=ce_labels, K=topk,\n                               weight=None, size_average=False)\n\n            mval_loss += val_loss.data[0]\n\n            net_out = F.softmax(net_out, dim=1)\n            pred = net_out.data.max(1)[1].cpu().numpy()\n            gt = ce_labels.data.cpu().numpy()\n            running_metrics.update(gt, pred)\n\n        mval_loss /= vali_count\n\n        loss_log = ""Epoch [%d/%d] Loss: \\t %.4f"" % (epoch + 1, args.n_epoch, mval_loss)\n        metric_log = """"\n        score, class_iou = running_metrics.get_scores()\n        for k, v in score.items():\n            metric_log += "" {} \\t %.4f, "".format(k) % v\n        running_metrics.reset()\n\n        logs = loss_log + metric_log\n        # print(logs)\n        pbar.set_postfix(Train_Loss=last_loss, Vali_Loss=mval_loss, Vali_mIoU=score[\'Mean_IoU\'])\n\n        if args.visdom:\n            writer.add_scalar(\'Validation/Losses\', mval_loss, epoch)\n            writer.add_scalars(\'Validation/Metrics\', score, epoch)\n            writer.add_text(\'Validation/Text\', logs, epoch)\n\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n            # export scalar data to JSON for external processing\n            # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n\n        if score[\'Mean_IoU\'] >= best_iou:\n            best_iou = score[\'Mean_IoU\']\n            state = {\'epoch\': epoch + 1,\n                     ""best_iou"": best_iou,\n                     \'model_state\': model.state_dict(),\n                     \'optimizer_state\': optimizer.state_dict()}\n            torch.save(state, ""{}{}_mobilenetv2context_best_model.pkl"".format(weight_dir, args.dataset))\n\n        # scheduler.step()\n        # scheduler.batch_step()\n        pbar.close()\n\n    if args.visdom:\n        # export scalar data to JSON for external processing\n        # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n        writer.close()\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Training Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Hyper-params\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    parser = argparse.ArgumentParser(description=\'Hyperparams\')\n\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'cityscapes\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_rows\', nargs=\'?\', type=int, default=512,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--img_cols\', nargs=\'?\', type=int, default=1024,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--n_epoch\', nargs=\'?\', type=int, default=200,\n                        help=\'# of the epochs\')\n    parser.add_argument(\'--batch_size\', nargs=\'?\', type=int, default=7,\n                        help=\'Batch Size\')\n    parser.add_argument(\'--l_rate\', nargs=\'?\', type=float, default=2.5e-3,\n                        help=\'Learning Rate\')\n    parser.add_argument(\'--crop_ratio\', nargs=\'?\', type=float, default=0.875,\n                        help=\'The ratio to crop the input image\')\n    parser.add_argument(\'--resume\', nargs=\'?\', type=str, default=None,\n                        help=\'Path to previous saved model to restart from\')\n    parser.add_argument(\'--pre_trained\', nargs=\'?\', type=str, default=""cityscapes_mobilenetv2context_best_model.pkl"",\n                        help=\'Path to pre-trained  model to init from\')\n    parser.add_argument(\'--visdom\', nargs=\'?\', type=bool, default=True,\n                        help=\'Show visualization(s) on visdom | True by  default\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Train the Deep Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    data_path = ""/zfs/zhang/Cityscapes""\n    save_path = ""/zfs/zhang/TrainLog/""\n    train_args = parser.parse_args()\n    train(train_args, data_path, save_path)\n'"
scripts/train_inplace.py,14,"b'import torch.nn.functional as F\nimport argparse\nimport torch\nimport time\nimport os\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom scripts.utils import poly_lr_scheduler, init_weights\nfrom scripts.loss import bootstrapped_cross_entropy2d\nfrom scripts.utils import cosine_annealing_lr\nfrom scripts.utils import poly_topk_scheduler\nfrom models.sewrnetv2 import SEWiderResNetV2\nfrom scripts.utils import set_optimizer_lr\nfrom scripts.metrics import RunningScore\nfrom modules import InPlaceABNWrapper\nfrom datasets.augmentations import *\nfrom functools import partial\n\n\ndef train(args, data_root, save_root):\n    weight_dir = ""{}weights/"".format(save_root)\n    log_dir = ""{}logs/SE-WRNetV2-{}"".format(save_root, time.strftime(""%Y-%m-%d-%H-%M-%S"", time.localtime()))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Augmentations\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    net_h, net_w = int(args.img_rows*args.crop_ratio), int(args.img_cols*args.crop_ratio)\n\n    augment_train = Compose([RandomHorizontallyFlip(), RandomSized((0.5, 0.75)),\n                             RandomRotate(5), RandomCrop((net_h, net_w))])\n    augment_valid = Compose([RandomHorizontallyFlip(), Scale((args.img_rows, args.img_cols)),\n                             CenterCrop((net_h, net_w))])\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    train_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'train\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_train)\n    valid_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'val\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_valid)\n\n    n_classes = train_loader.n_classes\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Metrics\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    running_metrics = RunningScore(n_classes)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 4. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n\n    model = SEWiderResNetV2(structure=[3, 3, 6, 3, 1, 1],\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1),\n                            classes=n_classes, dilation=True, is_se=True, in_size=(net_h, net_w),\n                            aspp_out=512, fusion_out=64, aspp_sec=(12, 24, 36))\n\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()\n\n    # 4.1 Setup Optimizer\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # Check if model has custom optimizer / loss\n    if hasattr(model.module, \'optimizer\'):\n        optimizer = model.module.optimizer\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.90, weight_decay=5e-4, nesterov=True)\n        # optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999),\n        #                             eps=1e-08, weight_decay=0, amsgrad=True)\n        # optimizer = YFOptimizer(model.parameters(), lr=2.5e-3, mu=0.9, clip_thresh=10000, weight_decay=5e-4)\n\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n\n    # 4.2 Setup Loss\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    class_weight = None\n    if hasattr(model.module, \'loss\'):\n        print(\'> Using custom loss\')\n        loss_fn = model.module.loss\n    else:\n        class_weight = np.array([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n                                 5.58540548, 3.56563995, 0.12704978, 1., 0.46783719, 1.34551528,\n                                 5.29974114, 0.28342531, 0.9396095, 0.81551811, 0.42679146, 3.6399074,\n                                 2.78376194], dtype=float)\n\n        """"""\n        class_weight = np.array([3.045384,  12.862123,   4.509889,  38.15694,  35.25279,  31.482613,\n                                 45.792305,  39.694073,  6.0639296,  32.16484,  17.109228,   31.563286,\n                                 47.333973,  11.610675,  44.60042,   45.23716,  45.283024,  48.14782,\n                                 41.924667], dtype=float)/10.0\n        """"""\n        class_weight = torch.from_numpy(class_weight).float().cuda()\n        loss_fn = bootstrapped_cross_entropy2d\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 5. Resume Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    args.start_epoch = 0\n    best_iou = -100.0\n    if args.resume is not None:\n        full_path = ""{}{}"".format(weight_dir, args.resume)\n        if os.path.isfile(full_path):\n            print(""> Loading model and optimizer from checkpoint \'{}\'"".format(args.resume))\n\n            checkpoint = torch.load(full_path)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_iou = checkpoint[\'best_iou\']\n            model.load_state_dict(checkpoint[\'model_state\'])          # weights\n\n            optimizer.load_state_dict(checkpoint[\'optimizer_state\'])  # gradient state\n\n            # optimizer = YFOptimizer(model.parameters(), lr=2.5e-3, mu=0.9, clip_thresh=10000, weight_decay=5e-4)\n            del checkpoint\n\n            print(""> Loaded checkpoint \'{}\' (epoch {})"".format(args.resume, args.start_epoch))\n\n        else:\n            print(""> No checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        init_weights(model, activation=""leaky_relu"", slope=0.1, init=""kaiming_uniform"")\n        if args.pre_trained is not None:\n            print(""> Loading weights from pre-trained model \'{}\'"".format(args.pre_trained))\n            full_path = ""{}{}"".format(weight_dir, args.pre_trained)\n\n            pre_weight = torch.load(full_path)\n            pre_weight = pre_weight[""model_state""]\n            # pre_weight = pre_weight[""state_dict""]\n\n            model_dict = model.state_dict()\n            pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            model.load_state_dict(model_dict)\n\n            del pre_weight\n            del model_dict\n            del pretrained_dict\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 3. Setup tensor_board for visualization\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    writer = None\n    if args.tensor_board:\n        writer = SummaryWriter(log_dir=log_dir, comment=""SE-WRNet"")\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 6. Train Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 2. Model Training start..."")\n    train_loader = data.DataLoader(train_loader, batch_size=args.batch_size, num_workers=6, shuffle=True)\n    valid_loader = data.DataLoader(valid_loader, batch_size=args.batch_size, num_workers=6)\n\n    num_batches = int(math.ceil(len(train_loader.dataset.files[train_loader.dataset.split]) /\n                                float(train_loader.batch_size)))\n\n    lr_period = 20 * num_batches\n\n    topk_init = 512\n    # topk_multipliers = [64, 128, 256, 512]\n    for epoch in np.arange(args.start_epoch, args.n_epoch):\n        last_loss = 0.0\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.1 Mini-Batch Learning\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Training Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.train()\n        topk_base = topk_init\n        pbar = tqdm(np.arange(num_batches))\n        for train_i, (images, labels) in enumerate(train_loader):  # One mini-Batch data, One iteration\n            full_iter = (epoch * num_batches) + train_i + 1\n\n            # curr_lr = poly_lr_scheduler(optimizer, init_lr=args.l_rate, iter=full_iter,\n            #                             lr_decay_iter=1, max_iter=args.n_epoch*num_batches, power=0.9)\n            batch_lr = args.l_rate * cosine_annealing_lr(lr_period, full_iter)\n            optimizer = set_optimizer_lr(optimizer, batch_lr)\n\n            topk_base = poly_topk_scheduler(init_topk=topk_init, iter=full_iter, topk_decay_iter=1,\n                                            max_iter=args.n_epoch * num_batches, power=0.95)\n\n            images = Variable(images.cuda(), requires_grad=True)   # Image feed into the deep neural network\n            labels = Variable(labels.cuda(), requires_grad=False)\n\n            optimizer.zero_grad()\n            net_out = model(images)  # Here we have 4 output for 4 loss\n\n            topk = topk_base * 512\n            if random.random() < 0.20:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=class_weight)\n            else:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=None)\n\n            pbar.update(1)\n            pbar.set_description(""> Epoch [%d/%d]"" % (epoch + 1, args.n_epoch))\n\n            last_loss = train_loss.data[0]\n            pbar.set_postfix(Loss=last_loss, TopK=topk_base, LR=batch_lr)\n\n            train_loss.backward()\n            optimizer.step()\n\n            if (train_i + 1) % 31 == 0:\n\n                loss_log = ""Epoch [%d/%d], Iter: %d Loss: \\t %.4f"" % (epoch + 1, args.n_epoch, train_i + 1, last_loss)\n\n                net_out = F.softmax(net_out, dim=1)\n                pred = net_out.data.max(1)[1].cpu().numpy()\n                gt = labels.data.cpu().numpy()\n\n                running_metrics.update(gt, pred)\n                score, class_iou = running_metrics.get_scores()\n\n                metric_log = """"\n                for k, v in score.items():\n                    metric_log += "" {}: \\t %.4f, "".format(k) % v\n                running_metrics.reset()\n\n                logs = loss_log + metric_log\n                # print(logs)\n\n                if args.tensor_board:\n                    writer.add_scalar(\'Training/Losses\', last_loss, full_iter)\n                    writer.add_scalars(\'Training/Metrics\', score, full_iter)\n                    # writer.add_scalar(\'Training/Learning Rate\', curr_lr, full_iter)\n\n                    writer.add_text(\'Training/Text\', logs, full_iter)\n\n                    for name, param in model.named_parameters():\n                        writer.add_histogram(name, param.clone().cpu().data.numpy(), full_iter)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.2 Mini-Batch Validation\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Validation for Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.eval()\n        avg_val_loss = 0.0\n        vali_count = 0\n        for i_val, (images_val, labels_val) in enumerate(valid_loader):\n            vali_count += 1\n\n            images_val = Variable(images_val.cuda(), volatile=True)\n            labels_val = Variable(labels_val.cuda(), volatile=True)\n\n            net_out = model(images_val)  # Here we have 4 output for 4 loss\n\n            topk = topk_base * 512\n            val_loss = loss_fn(input=net_out, target=labels_val, K=topk, weight=None)\n            avg_val_loss += val_loss.data[0]\n\n            net_out = F.softmax(net_out, dim=1)\n            pred = net_out.data.max(1)[1].cpu().numpy()\n            gt = labels_val.data.cpu().numpy()\n            running_metrics.update(gt, pred)\n\n        avg_val_loss /= vali_count\n        loss_log = ""Epoch [%d/%d] Loss: \\t %.4f"" % (epoch + 1, args.n_epoch, avg_val_loss)\n\n        metric_log = """"\n        score, class_iou = running_metrics.get_scores()\n        for k, v in score.items():\n            metric_log += "" {} \\t %.4f, "".format(k) % v\n        running_metrics.reset()\n\n        logs = loss_log + metric_log\n        # print(logs)\n        pbar.set_postfix(Train_Loss=last_loss, Vali_Loss=avg_val_loss, Vali_mIoU=score[\'Mean_IoU\'])\n\n        if args.tensor_board:\n            writer.add_scalar(\'Validation/Losses\', avg_val_loss, epoch)\n            writer.add_scalars(\'Validation/Metrics\', score, epoch)\n\n            writer.add_text(\'Validation/Text\', logs, epoch)\n\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n            # export scalar data to JSON for external processing\n            # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n\n        if score[\'Mean_IoU\'] >= best_iou:\n            best_iou = score[\'Mean_IoU\']\n            state = {\'epoch\': epoch + 1,\n                     ""best_iou"": best_iou,\n                     \'model_state\': model.state_dict(),\n                     \'optimizer_state\': optimizer.state_dict()}\n            torch.save(state, ""{}{}_sewrnetv2_gtfine_best_model.pkl"".format(weight_dir, args.dataset))\n\n        # Note that step should be called after validate()\n        # scheduler.step()\n        pbar.close()\n\n    if args.tensor_board:\n        # export scalar data to JSON for external processing\n        # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n        writer.close()\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Training Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Hyper-params\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    parser = argparse.ArgumentParser(description=\'Hyperparams\')\n\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'cityscapes\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_rows\', nargs=\'?\', type=int, default=512,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--img_cols\', nargs=\'?\', type=int, default=1024,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--n_epoch\', nargs=\'?\', type=int, default=128,\n                        help=\'# of the epochs\')\n    parser.add_argument(\'--batch_size\', nargs=\'?\', type=int, default=3,\n                        help=\'Batch Size\')\n    parser.add_argument(\'--l_rate\', nargs=\'?\', type=float, default=2.5e-3,\n                        help=\'Learning Rate\')\n    parser.add_argument(\'--crop_ratio\', nargs=\'?\', type=float, default=0.875,\n                        help=\'The ratio to crop the input image\')\n    parser.add_argument(\'--resume\', nargs=\'?\', type=str, default=None,\n                        help=\'Path to previous saved model to restart from\')\n    parser.add_argument(\'--pre_trained\', nargs=\'?\', type=str, default=""cityscapes_sewrnetv2_gtfine_best_model.pkl"",\n                        help=\'Path to pre-trained  model to init from\')\n    parser.add_argument(\'--tensor_board\', nargs=\'?\', type=bool, default=True,\n                        help=\'Show visualization(s) on tensor_board | True by  default\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Train the Deep Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    data_path = ""/zfs/zhang/Cityscapes""\n    save_path = ""/zfs/zhang/TrainLog/""\n    train_args = parser.parse_args()\n    train(train_args, data_path, save_path)\n'"
scripts/train_lovasz.py,15,"b'import torch.nn.functional as F\nimport argparse\nimport random\nimport torch\nimport time\nimport os\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom scripts.loss import bootstrapped_cross_entropy2d, cross_entropy2d, lovasz_softmax\nfrom scripts.utils import update_aggregated_weight_average\nfrom models.rfmobilenetv2plus import RFMobileNetV2Plus\nfrom models.mobilenetv2plus import MobileNetV2Plus\nfrom scripts.utils import cosine_annealing_lr\nfrom scripts.utils import poly_topk_scheduler\nfrom scripts.utils import set_optimizer_lr\nfrom scripts.cyclical_lr import CyclicLR\nfrom scripts.metrics import RunningScore\nfrom modules import InPlaceABNWrapper\nfrom datasets.augmentations import *\nfrom functools import partial\n\n\ndef train(args, data_root, save_root):\n    weight_dir = ""{}weights/"".format(save_root)\n    log_dir = ""{}logs/RFMobileNetV2PlusLovasz-{}"".format(save_root, time.strftime(""%Y-%m-%d-%H-%M-%S"", time.localtime()))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Augmentations\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    net_h, net_w = int(args.img_rows*args.crop_ratio), int(args.img_cols*args.crop_ratio)\n\n    augment_train = Compose([RandomHorizontallyFlip(), RandomSized((0.5, 0.75)),\n                             RandomRotate(5), RandomCrop((net_h, net_w))])\n    augment_valid = Compose([RandomHorizontallyFlip(), Scale((args.img_rows, args.img_cols)),\n                             CenterCrop((net_h, net_w))])\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    train_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'train\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_train)\n    valid_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'val\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_valid)\n\n    n_classes = train_loader.n_classes\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Metrics\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    running_metrics = RunningScore(n_classes)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 4. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n\n    model = RFMobileNetV2Plus(n_class=n_classes, in_size=(net_h, net_w), width_mult=1.0,\n                              out_sec=256, aspp_sec=(12, 24, 36),\n                              norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    """"""\n\n    model = MobileNetV2Plus(n_class=n_classes, in_size=(net_h, net_w), width_mult=1.0,\n                            out_sec=256, aspp_sec=(12, 24, 36),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    """"""\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()\n\n    # 4.1 Setup Optimizer\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # Check if model has custom optimizer / loss\n    if hasattr(model.module, \'optimizer\'):\n        optimizer = model.module.optimizer\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.90,\n                                    weight_decay=5e-4, nesterov=True)\n\n        # for pg in optimizer.param_groups:\n        #     print(pg[\'lr\'])\n\n        # optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999),\n        #                             eps=1e-08, weight_decay=0, amsgrad=True)\n        # optimizer = YFOptimizer(model.parameters(), lr=2.5e-3, mu=0.9, clip_thresh=10000, weight_decay=5e-4)\n\n    # 4.2 Setup Loss\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    class_weight = None\n    if hasattr(model.module, \'loss\'):\n        print(\'> Using custom loss\')\n        loss_fn = model.module.loss\n    else:\n        loss_fn = lovasz_softmax\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 5. Resume Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    best_iou = -100.0\n    args.start_epoch = 0\n    if args.resume is not None:\n        full_path = ""{}{}"".format(weight_dir, args.resume)\n        if os.path.isfile(full_path):\n            print(""> Loading model and optimizer from checkpoint \'{}\'"".format(args.resume))\n\n            checkpoint = torch.load(full_path)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_iou = checkpoint[\'best_iou\']\n            model.load_state_dict(checkpoint[\'model_state\'])          # weights\n            optimizer.load_state_dict(checkpoint[\'optimizer_state\'])  # gradient state\n\n            # for param_group in optimizer.param_groups:\n            # s    param_group[\'lr\'] = 1e-5\n\n            del checkpoint\n            print(""> Loaded checkpoint \'{}\' (epoch {}, iou {})"".format(args.resume,\n                                                                       args.start_epoch,\n                                                                       best_iou))\n\n        else:\n            print(""> No checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        if args.pre_trained is not None:\n            print(""> Loading weights from pre-trained model \'{}\'"".format(args.pre_trained))\n            full_path = ""{}{}"".format(weight_dir, args.pre_trained)\n\n            pre_weight = torch.load(full_path)\n            pre_weight = pre_weight[""model_state""]\n            # pre_weight = pre_weight[""state_dict""]\n\n            model_dict = model.state_dict()\n\n            pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            model.load_state_dict(model_dict)\n\n            del pre_weight\n            del model_dict\n            del pretrained_dict\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 3. Setup tensor_board for visualization\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    writer = None\n    if args.tensor_board:\n        writer = SummaryWriter(log_dir=log_dir, comment=""RFMobileNetV2PlusLovasz"")\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        writer.add_graph(model, dummy_input)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 6. Train Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 2. Model Training start..."")\n    train_loader = data.DataLoader(train_loader, batch_size=args.batch_size, num_workers=6, shuffle=True)\n    valid_loader = data.DataLoader(valid_loader, batch_size=args.batch_size, num_workers=6)\n\n    num_batches = int(math.ceil(len(train_loader.dataset.files[train_loader.dataset.split]) /\n                                float(train_loader.batch_size)))\n\n    lr_period = 20 * num_batches\n    swa_weights = model.state_dict()\n\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n    # scheduler = CyclicLR(optimizer, base_lr=1.0e-3, max_lr=6.0e-3, step_size=2*num_batches)\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=32, gamma=0.1)\n    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n\n    for epoch in np.arange(args.start_epoch, args.n_epoch):\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.1 Mini-Batch Learning\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Training Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.train()\n\n        last_loss = 0.0\n        pbar = tqdm(np.arange(num_batches))\n        for train_i, (images, labels) in enumerate(train_loader):  # One mini-Batch data, One iteration\n            full_iter = (epoch * num_batches) + train_i + 1\n\n            # poly_lr_scheduler(optimizer, init_lr=args.l_rate, iter=full_iter,\n            #                   lr_decay_iter=1, max_iter=args.n_epoch*num_batches, power=0.9)\n\n            batch_lr = args.l_rate * cosine_annealing_lr(lr_period, full_iter)\n            optimizer = set_optimizer_lr(optimizer, batch_lr)\n\n            images = Variable(images.cuda(), requires_grad=True)   # Image feed into the deep neural network\n            labels = Variable(labels.cuda(), requires_grad=False)\n\n            optimizer.zero_grad()\n            net_out = model(images)  # Here we have 3 output for 3 loss\n            net_out = F.softmax(net_out, dim=1)\n            loss = lovasz_softmax(net_out, labels, ignore=250)\n\n            last_loss = loss.data[0]\n            pbar.update(1)\n            pbar.set_description(""> Epoch [%d/%d]"" % (epoch + 1, args.n_epoch))\n            pbar.set_postfix(Loss=last_loss, LR=batch_lr)\n\n            loss.backward()\n            optimizer.step()\n\n            if full_iter % lr_period == 0:\n                swa_weights = update_aggregated_weight_average(model, swa_weights, full_iter, lr_period)\n                state = {\'model_state\': swa_weights}\n                torch.save(state, ""{}{}_rfmobilenetv2_lovasz_swa_model.pkl"".format(weight_dir, args.dataset))\n\n            if (train_i + 1) % 31 == 0:\n                loss_log = ""Epoch [%d/%d], Iter: %d Loss: \\t %.4f"" % (epoch + 1, args.n_epoch,\n                                                                      train_i + 1, last_loss)\n\n                # net_out = F.softmax(net_out, dim=1)\n                pred = net_out.data.max(1)[1].cpu().numpy()\n                gt = labels.data.cpu().numpy()\n\n                running_metrics.update(gt, pred)\n                score, class_iou = running_metrics.get_scores()\n\n                metric_log = """"\n                for k, v in score.items():\n                    metric_log += "" {}: \\t %.4f, "".format(k) % v\n                running_metrics.reset()\n\n                logs = loss_log + metric_log\n                # print(logs)\n\n                if args.tensor_board:\n                    writer.add_scalar(\'Training/Losses\', last_loss, full_iter)\n                    writer.add_scalars(\'Training/Metrics\', score, full_iter)\n                    writer.add_text(\'Training/Text\', logs, full_iter)\n\n                    for name, param in model.named_parameters():\n                        writer.add_histogram(name, param.clone().cpu().data.numpy(), full_iter)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.2 Mini-Batch Validation\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Validation for Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.eval()\n\n        mval_loss = 0.0\n        vali_count = 0\n        for i_val, (images, labels) in enumerate(valid_loader):\n            vali_count += 1\n\n            images = Variable(images.cuda(), volatile=True)\n            labels = Variable(labels.cuda(), volatile=True)\n\n            net_out = model(images)  # Here we have 4 output for 4 loss\n            net_out = F.softmax(net_out, dim=1)\n            loss = lovasz_softmax(net_out, labels, ignore=250)\n            mval_loss += loss.data[0]\n\n            pred = net_out.data.max(1)[1].cpu().numpy()\n            gt = labels.data.cpu().numpy()\n            running_metrics.update(gt, pred)\n\n        mval_loss /= vali_count\n\n        loss_log = ""Epoch [%d/%d] Loss: \\t %.4f"" % (epoch + 1, args.n_epoch, mval_loss)\n        metric_log = """"\n        score, class_iou = running_metrics.get_scores()\n        for k, v in score.items():\n            metric_log += "" {} \\t %.4f, "".format(k) % v\n        running_metrics.reset()\n\n        logs = loss_log + metric_log\n        # print(logs)\n        pbar.set_postfix(Train_Loss=last_loss, Vali_Loss=mval_loss, Vali_mIoU=score[\'Mean_IoU\'])\n\n        if args.tensor_board:\n            writer.add_scalar(\'Validation/Losses\', mval_loss, epoch)\n            writer.add_scalars(\'Validation/Metrics\', score, epoch)\n            writer.add_text(\'Validation/Text\', logs, epoch)\n\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n        if score[\'Mean_IoU\'] >= best_iou:\n            best_iou = score[\'Mean_IoU\']\n            state = {\'epoch\': epoch + 1,\n                     ""best_iou"": best_iou,\n                     \'model_state\': model.state_dict(),\n                     \'optimizer_state\': optimizer.state_dict()}\n            torch.save(state, ""{}{}_rfmobilenetv2_lovasz_best_model.pkl"".format(weight_dir, args.dataset))\n\n        # scheduler.step()\n        # scheduler.batch_step()\n        pbar.close()\n\n    if args.tensor_board:\n        # export scalar data to JSON for external processing\n        # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n        writer.close()\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Training Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Hyper-params\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    parser = argparse.ArgumentParser(description=\'Hyperparams\')\n\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'cityscapes\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_rows\', nargs=\'?\', type=int, default=512,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--img_cols\', nargs=\'?\', type=int, default=1024,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--n_epoch\', nargs=\'?\', type=int, default=200,\n                        help=\'# of the epochs\')\n    parser.add_argument(\'--batch_size\', nargs=\'?\', type=int, default=10,\n                        help=\'Batch Size\')\n    parser.add_argument(\'--l_rate\', nargs=\'?\', type=float, default=2.5e-3,\n                        help=\'Learning Rate\')\n    parser.add_argument(\'--crop_ratio\', nargs=\'?\', type=float, default=0.875,\n                        help=\'The ratio to crop the input image\')\n    parser.add_argument(\'--resume\', nargs=\'?\', type=str, default=None,\n                        help=\'Path to previous saved model to restart from\')\n    parser.add_argument(\'--pre_trained\', nargs=\'?\', type=str, default=""cityscapes_rfmobilenetv2_best_model.pkl"",\n                        help=\'Path to pre-trained  model to init from\')\n    parser.add_argument(\'--tensor_board\', nargs=\'?\', type=bool, default=True,\n                        help=\'Show visualization(s) on tensor_board | True by  default\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Train the Deep Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    data_path = ""/zfs/zhang/Cityscapes""\n    save_path = ""/zfs/zhang/TrainLog/""\n    train_args = parser.parse_args()\n    train(train_args, data_path, save_path)\n'"
scripts/train_mixscale.py,15,"b'import torch.nn.functional as F\nimport argparse\nimport torch\nimport time\nimport os\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom scripts.utils import update_aggregated_weight_average\nfrom models.mixscaledensenet import MixedScaleDenseNet\nfrom scripts.loss import bootstrapped_cross_entropy2d\nfrom scripts.utils import cosine_annealing_lr\nfrom scripts.utils import poly_topk_scheduler\nfrom scripts.utils import set_optimizer_lr\nfrom scripts.metrics import RunningScore\nfrom scripts.utils import init_weights\nfrom modules import InPlaceABNWrapper\nfrom datasets.augmentations import *\nfrom functools import partial\n\n\ndef train(args, data_root, save_root):\n    weight_dir = ""{}weights/"".format(save_root)\n    log_dir = ""{}logs/MixedScaleDenseNet-{}"".format(save_root, time.strftime(""%Y-%m-%d-%H-%M-%S"", time.localtime()))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Augmentations\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    net_h, net_w = int(args.img_rows*args.crop_ratio), int(args.img_cols*args.crop_ratio)\n\n    augment_train = Compose([RandomHorizontallyFlip(), RandomSized((0.5, 0.75)),\n                             RandomRotate(5), RandomCrop((net_h, net_w))])\n    augment_valid = Compose([RandomHorizontallyFlip(), Scale((args.img_rows, args.img_cols)),\n                             CenterCrop((net_h, net_w))])\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    train_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'train\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_train)\n    valid_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'val\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_valid)\n\n    n_classes = train_loader.n_classes\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Metrics\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    running_metrics = RunningScore(n_classes)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 4. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n\n    model = MixedScaleDenseNet(n_class=n_classes, in_size=(net_h, net_w), num_layers=96, in_chns=32,\n                               squeeze_ratio=1.0/32, out_chns=1,\n                               dilate_sec=(1, 2, 4, 8, 16),\n                               aspp_sec=(24, 48, 72),\n                               norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()\n\n    # 4.1 Setup Optimizer\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # Check if model has custom optimizer / loss\n    if hasattr(model.module, \'optimizer\'):\n        optimizer = model.module.optimizer\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.90,\n                                    weight_decay=5e-4, nesterov=True)\n\n        # for pg in optimizer.param_groups:\n        #     print(pg[\'lr\'])\n\n        # optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999),\n        #                             eps=1e-08, weight_decay=0, amsgrad=True)\n        # optimizer = YFOptimizer(model.parameters(), lr=2.5e-3, mu=0.9, clip_thresh=10000, weight_decay=5e-4)\n\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=16, gamma=0.1)\n    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n\n    # 4.2 Setup Loss\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    class_weight = None\n    if hasattr(model.module, \'loss\'):\n        print(\'> Using custom loss\')\n        loss_fn = model.module.loss\n    else:\n        # loss_fn = cross_entropy2d\n\n        class_weight = np.array([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n                                 5.58540548, 3.56563995, 0.12704978, 1., 0.46783719, 1.34551528,\n                                 5.29974114, 0.28342531, 0.9396095, 0.81551811, 0.42679146, 3.6399074,\n                                 2.78376194], dtype=float)\n\n        """"""\n        class_weight = np.array([3.045384,  12.862123,   4.509889,  38.15694,  35.25279,  31.482613,\n                                 45.792305,  39.694073,  6.0639296,  32.16484,  17.109228,   31.563286,\n                                 47.333973,  11.610675,  44.60042,   45.23716,  45.283024,  48.14782,\n                                 41.924667], dtype=float)/10.0\n        """"""\n        class_weight = torch.from_numpy(class_weight).float().cuda()\n        loss_fn = bootstrapped_cross_entropy2d\n        # loss_fn = cross_entropy2d\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 5. Resume Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    args.start_epoch = 0\n    best_iou = -100.0\n    if args.resume is not None:\n        full_path = ""{}{}"".format(weight_dir, args.resume)\n        if os.path.isfile(full_path):\n            print(""> Loading model and optimizer from checkpoint \'{}\'"".format(args.resume))\n\n            checkpoint = torch.load(full_path)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_iou = checkpoint[\'best_iou\']\n            model.load_state_dict(checkpoint[\'model_state\'])  # weights\n            optimizer.load_state_dict(checkpoint[\'optimizer_state\'])  # gradient state\n\n            # for param_group in optimizer.param_groups:\n            # s    param_group[\'lr\'] = 1e-5\n\n            del checkpoint\n            print(""> Loaded checkpoint \'{}\' (epoch {}, iou {})"".format(args.resume,\n                                                                       args.start_epoch,\n                                                                       best_iou))\n\n        else:\n            print(""> No checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        init_weights(model, activation=""leaky_relu"", slope=0.1, init=""kaiming_normal"")\n        if args.pre_trained is not None:\n            print(""> Loading weights from pre-trained model \'{}\'"".format(args.pre_trained))\n            full_path = ""{}{}"".format(weight_dir, args.pre_trained)\n\n            pre_weight = torch.load(full_path)\n            pre_weight = pre_weight[""model_state""]\n            # pre_weight = pre_weight[""state_dict""]\n\n            model_dict = model.state_dict()\n\n            pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            model.load_state_dict(model_dict)\n\n            del pre_weight\n            del model_dict\n            del pretrained_dict\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 3. Setup tensor_board for visualization\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    writer = None\n    if args.tensor_board:\n        writer = SummaryWriter(log_dir=log_dir, comment=""MixedScaleDenseNet"")\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 6. Train Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 2. Model Training start..."")\n    train_loader = data.DataLoader(train_loader, batch_size=args.batch_size, num_workers=4, shuffle=True)\n    valid_loader = data.DataLoader(valid_loader, batch_size=args.batch_size, num_workers=4)\n\n    num_batches = int(math.ceil(len(train_loader.dataset.files[train_loader.dataset.split]) /\n                                float(train_loader.batch_size)))\n\n    lr_period = 30 * num_batches\n    swa_weights = model.state_dict()\n\n    topk_init = 768\n    # topk_multipliers = [64, 128, 256, 512]\n    for epoch in np.arange(args.start_epoch, args.n_epoch):\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.1 Mini-Batch Learning\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Training Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.train()\n\n        last_loss = 0.0\n        topk_base = topk_init\n        pbar = tqdm(np.arange(num_batches))\n        for train_i, (images, labels) in enumerate(train_loader):  # One mini-Batch data, One iteration\n            full_iter = (epoch * num_batches) + train_i + 1\n\n            # poly_lr_scheduler(optimizer, init_lr=args.l_rate, iter=full_iter,\n            #                   lr_decay_iter=1, max_iter=args.n_epoch*num_batches, power=0.9)\n            batch_lr = args.l_rate * cosine_annealing_lr(lr_period, full_iter)\n            optimizer = set_optimizer_lr(optimizer, batch_lr)\n\n            topk_base = poly_topk_scheduler(init_topk=topk_init, iter=full_iter, topk_decay_iter=1,\n                                            max_iter=args.n_epoch * num_batches, power=0.98)\n\n            images = Variable(images.cuda(), requires_grad=True)  # Image feed into the deep neural network\n            labels = Variable(labels.cuda(), requires_grad=False)\n\n            optimizer.zero_grad()\n            net_out = model(images)\n\n            topk = topk_base * 512\n            if random.random() < 0.10:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=class_weight)\n            else:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=None)\n\n            last_loss = train_loss.data[0]\n\n            pbar.update(1)\n            pbar.set_description(""> Epoch [%d/%d]"" % (epoch + 1, args.n_epoch))\n            pbar.set_postfix(Loss=last_loss, TopK=topk_base, LR=batch_lr)\n\n            train_loss.backward()\n            optimizer.step()\n\n            if full_iter % lr_period == 0:\n                swa_weights = update_aggregated_weight_average(model, swa_weights, full_iter, lr_period)\n                state = {\'model_state\': swa_weights}\n                torch.save(state, ""{}{}_msdensenet_swa_model.pkl"".format(weight_dir, args.dataset))\n\n            if (train_i + 1) % 31 == 0:\n\n                loss_log = ""Epoch [%d/%d], Iter: %d Loss: \\t %.4f"" % (epoch + 1, args.n_epoch,\n                                                                      train_i + 1, last_loss)\n\n                net_out = F.softmax(net_out, dim=1)\n                pred = net_out.data.max(1)[1].cpu().numpy()\n                gt = labels.data.cpu().numpy()\n\n                running_metrics.update(gt, pred)\n\n                score, class_iou = running_metrics.get_scores()\n\n                metric_log = """"\n                for k, v in score.items():\n                    metric_log += "" {}: \\t %.4f, "".format(k) % v\n                running_metrics.reset()\n\n                logs = loss_log + metric_log\n                # print(logs)\n\n                if args.tensor_board:\n                    writer.add_scalar(\'Training/Loss\', last_loss, full_iter)\n                    writer.add_scalars(\'Training/Metrics\', score, full_iter)\n\n                    writer.add_text(\'Training/Text\', logs, full_iter)\n\n                    for name, param in model.named_parameters():\n                        writer.add_histogram(name, param.clone().cpu().data.numpy(), full_iter)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.2 Mini-Batch Validation\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Validation for Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.eval()\n\n        mval_loss = 0.0\n        vali_count = 0\n        for i_val, (images_val, labels_val) in enumerate(valid_loader):\n            vali_count += 1\n\n            images_val = Variable(images_val.cuda(), volatile=True)\n            labels_val = Variable(labels_val.cuda(), volatile=True)\n\n            net_out = model(images_val)  # Here we have 4 output for 4 loss\n\n            topk = topk_base * 512\n            val_loss = loss_fn(input=net_out, target=labels_val, K=topk)\n\n            mval_loss += val_loss.data[0]\n\n            net_out = F.softmax(net_out, dim=1)\n            pred = net_out.data.max(1)[1].cpu().numpy()\n            gt = labels_val.data.cpu().numpy()\n            running_metrics.update(gt, pred)\n\n        mval_loss /= vali_count\n\n        loss_log = ""Epoch [%d/%d] Loss: \\t %.4f"" % (epoch + 1, args.n_epoch, mval_loss)\n        metric_log = """"\n        score, class_iou = running_metrics.get_scores()\n        for k, v in score.items():\n            metric_log += "" {} \\t %.4f, "".format(k) % v\n        running_metrics.reset()\n\n        logs = loss_log + metric_log\n        # print(logs)\n        pbar.set_postfix(Train_Loss=last_loss, Vali_Loss=mval_loss, Vali_mIoU=score[\'Mean_IoU\'])\n\n        if args.tensor_board:\n            writer.add_scalar(\'Validation/Loss\', mval_loss, epoch)\n            writer.add_scalars(\'Validation/Metrics\', score, epoch)\n\n            writer.add_text(\'Validation/Text\', logs, epoch)\n\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n            # export scalar data to JSON for external processing\n            # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n\n        if score[\'Mean_IoU\'] >= best_iou:\n            best_iou = score[\'Mean_IoU\']\n            state = {\'epoch\': epoch + 1,\n                     ""best_iou"": best_iou,\n                     \'model_state\': model.state_dict(),\n                     \'optimizer_state\': optimizer.state_dict()}\n            torch.save(state, ""{}{}_msdensenet_best_model.pkl"".format(weight_dir, args.dataset))\n\n        # Note that step should be called after validate()\n        # scheduler.step()\n        pbar.close()\n\n    if args.tensor_board:\n        # export scalar data to JSON for external processing\n        # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n        writer.close()\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Training Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Hyper-params\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    parser = argparse.ArgumentParser(description=\'Hyperparams\')\n\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'cityscapes\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_rows\', nargs=\'?\', type=int, default=512,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--img_cols\', nargs=\'?\', type=int, default=1024,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--n_epoch\', nargs=\'?\', type=int, default=128,\n                        help=\'# of the epochs\')\n    parser.add_argument(\'--batch_size\', nargs=\'?\', type=int, default=6,\n                        help=\'Batch Size\')\n    parser.add_argument(\'--l_rate\', nargs=\'?\', type=float, default=2.5e-2,\n                        help=\'Learning Rate\')\n    parser.add_argument(\'--crop_ratio\', nargs=\'?\', type=float, default=0.875,\n                        help=\'The ratio to crop the input image\')\n    parser.add_argument(\'--resume\', nargs=\'?\', type=str, default=None,\n                        help=\'Path to previous saved model to restart from\')\n    parser.add_argument(\'--pre_trained\', nargs=\'?\', type=str, default=""msdensenet_model.pkl"",\n                        help=\'Path to pre-trained  model to init from\')\n    parser.add_argument(\'--tensor_board\', nargs=\'?\', type=bool, default=True,\n                        help=\'Show visualization(s) on tensor_board | True by  default\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Train the Deep Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    data_path = ""/zfs/zhang/Cityscapes""\n    save_path = ""/zfs/zhang/TrainLog/""\n    train_args = parser.parse_args()\n    train(train_args, data_path, save_path)\n'"
scripts/train_mobile.py,16,"b'import torch.nn.functional as F\nimport argparse\nimport random\nimport torch\nimport time\nimport os\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom scripts.loss import bootstrapped_cross_entropy2d, cross_entropy2d\nfrom scripts.utils import update_aggregated_weight_average\nfrom models.rfmobilenetv2plus import RFMobileNetV2Plus\nfrom models.mobilenetv2plus import MobileNetV2Plus\nfrom scripts.utils import cosine_annealing_lr\nfrom scripts.utils import poly_topk_scheduler\nfrom scripts.utils import set_optimizer_lr\nfrom scripts.cyclical_lr import CyclicLR\nfrom scripts.metrics import RunningScore\nfrom modules import InPlaceABNWrapper\nfrom datasets.augmentations import *\nfrom functools import partial\n\n\ndef train(args, data_root, save_root):\n    weight_dir = ""{}weights/"".format(save_root)\n    log_dir = ""{}logs/RFMobileNetV2Plus-{}"".format(save_root, time.strftime(""%Y-%m-%d-%H-%M-%S"", time.localtime()))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Augmentations\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    net_h, net_w = int(args.img_rows*args.crop_ratio), int(args.img_cols*args.crop_ratio)\n\n    augment_train = Compose([RandomHorizontallyFlip(), RandomSized((0.5, 0.75)),\n                             RandomRotate(5), RandomCrop((net_h, net_w))])\n    augment_valid = Compose([RandomHorizontallyFlip(), Scale((args.img_rows, args.img_cols)),\n                             CenterCrop((net_h, net_w))])\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    train_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'train\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_train)\n    valid_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'val\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_valid)\n\n    n_classes = train_loader.n_classes\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Metrics\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    running_metrics = RunningScore(n_classes)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 4. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n\n    model = RFMobileNetV2Plus(n_class=n_classes, in_size=(net_h, net_w), width_mult=1.0,\n                              out_sec=256, aspp_sec=(12, 24, 36),\n                              norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    """"""\n\n    model = MobileNetV2Plus(n_class=n_classes, in_size=(net_h, net_w), width_mult=1.0,\n                            out_sec=256, aspp_sec=(12, 24, 36),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    """"""\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()\n\n    # 4.1 Setup Optimizer\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # Check if model has custom optimizer / loss\n    if hasattr(model.module, \'optimizer\'):\n        optimizer = model.module.optimizer\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.90,\n                                    weight_decay=5e-4, nesterov=True)\n\n        # for pg in optimizer.param_groups:\n        #     print(pg[\'lr\'])\n\n        # optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999),\n        #                             eps=1e-08, weight_decay=0, amsgrad=True)\n        # optimizer = YFOptimizer(model.parameters(), lr=2.5e-3, mu=0.9, clip_thresh=10000, weight_decay=5e-4)\n\n    # 4.2 Setup Loss\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    class_weight = None\n    if hasattr(model.module, \'loss\'):\n        print(\'> Using custom loss\')\n        loss_fn = model.module.loss\n    else:\n        # loss_fn = cross_entropy2d\n\n        class_weight = np.array([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n                                 5.58540548, 3.56563995, 0.12704978, 1.,         0.46783719, 1.34551528,\n                                 5.29974114, 0.28342531, 0.9396095,  0.81551811, 0.42679146, 3.6399074,\n                                 2.78376194], dtype=float)\n\n        """"""\n        class_weight = np.array([3.045384,  12.862123,   4.509889,  38.15694,  35.25279,  31.482613,\n                                 45.792305,  39.694073,  6.0639296,  32.16484,  17.109228,   31.563286,\n                                 47.333973,  11.610675,  44.60042,   45.23716,  45.283024,  48.14782,\n                                 41.924667], dtype=float)/10.0\n        """"""\n        class_weight = torch.from_numpy(class_weight).float().cuda()\n        loss_fn = bootstrapped_cross_entropy2d\n        # loss_fn = cross_entropy2d\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 5. Resume Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    best_iou = -100.0\n    args.start_epoch = 0\n    if args.resume is not None:\n        full_path = ""{}{}"".format(weight_dir, args.resume)\n        if os.path.isfile(full_path):\n            print(""> Loading model and optimizer from checkpoint \'{}\'"".format(args.resume))\n\n            checkpoint = torch.load(full_path)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_iou = checkpoint[\'best_iou\']\n            model.load_state_dict(checkpoint[\'model_state\'])          # weights\n            optimizer.load_state_dict(checkpoint[\'optimizer_state\'])  # gradient state\n\n            # for param_group in optimizer.param_groups:\n            # s    param_group[\'lr\'] = 1e-5\n\n            del checkpoint\n            print(""> Loaded checkpoint \'{}\' (epoch {}, iou {})"".format(args.resume,\n                                                                       args.start_epoch,\n                                                                       best_iou))\n\n        else:\n            print(""> No checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        if args.pre_trained is not None:\n            print(""> Loading weights from pre-trained model \'{}\'"".format(args.pre_trained))\n            full_path = ""{}{}"".format(weight_dir, args.pre_trained)\n\n            pre_weight = torch.load(full_path)\n            pre_weight = pre_weight[""model_state""]\n            # pre_weight = pre_weight[""state_dict""]\n\n            model_dict = model.state_dict()\n\n            pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            model.load_state_dict(model_dict)\n\n            del pre_weight\n            del model_dict\n            del pretrained_dict\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 3. Setup tensor_board for visualization\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    writer = None\n    if args.tensor_board:\n        writer = SummaryWriter(log_dir=log_dir, comment=""RFMobileNetV2Plus"")\n\n    if args.tensor_board:\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        writer.add_graph(model, dummy_input)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 6. Train Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 2. Model Training start..."")\n    train_loader = data.DataLoader(train_loader, batch_size=args.batch_size, num_workers=6, shuffle=True)\n    valid_loader = data.DataLoader(valid_loader, batch_size=args.batch_size, num_workers=6)\n\n    num_batches = int(math.ceil(len(train_loader.dataset.files[train_loader.dataset.split]) /\n                                float(train_loader.batch_size)))\n\n    lr_period = 20 * num_batches\n    swa_weights = model.state_dict()\n\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n    # scheduler = CyclicLR(optimizer, base_lr=1.0e-3, max_lr=6.0e-3, step_size=2*num_batches)\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=32, gamma=0.1)\n    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n\n    topk_init = 512\n    # topk_multipliers = [64, 128, 256, 512]\n    for epoch in np.arange(args.start_epoch, args.n_epoch):\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.1 Mini-Batch Learning\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Training Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.train()\n\n        last_loss = 0.0\n        topk_base = topk_init\n        pbar = tqdm(np.arange(num_batches))\n        for train_i, (images, labels) in enumerate(train_loader):  # One mini-Batch data, One iteration\n            full_iter = (epoch * num_batches) + train_i + 1\n\n            # poly_lr_scheduler(optimizer, init_lr=args.l_rate, iter=full_iter,\n            #                   lr_decay_iter=1, max_iter=args.n_epoch*num_batches, power=0.9)\n\n            batch_lr = args.l_rate * cosine_annealing_lr(lr_period, full_iter)\n            optimizer = set_optimizer_lr(optimizer, batch_lr)\n\n            topk_base = poly_topk_scheduler(init_topk=topk_init, iter=full_iter, topk_decay_iter=1,\n                                            max_iter=args.n_epoch*num_batches, power=0.95)\n\n            images = Variable(images.cuda(), requires_grad=True)   # Image feed into the deep neural network\n            labels = Variable(labels.cuda(), requires_grad=False)\n\n            optimizer.zero_grad()\n            net_out = model(images)  # Here we have 3 output for 3 loss\n\n            topk = topk_base * 512\n            if random.random() < 0.20:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=class_weight)\n            else:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=None)\n\n            last_loss = train_loss.data[0]\n            pbar.update(1)\n            pbar.set_description(""> Epoch [%d/%d]"" % (epoch + 1, args.n_epoch))\n            pbar.set_postfix(Loss=last_loss, TopK=topk_base, LR=batch_lr)\n\n            train_loss.backward()\n            optimizer.step()\n\n            if full_iter % lr_period == 0:\n                swa_weights = update_aggregated_weight_average(model, swa_weights, full_iter, lr_period)\n                state = {\'model_state\': swa_weights}\n                torch.save(state, ""{}{}_rfmobilenetv2_swa_model.pkl"".format(weight_dir, args.dataset))\n\n            if (train_i + 1) % 31 == 0:\n                loss_log = ""Epoch [%d/%d], Iter: %d Loss: \\t %.4f"" % (epoch + 1, args.n_epoch,\n                                                                      train_i + 1, last_loss)\n\n                net_out = F.softmax(net_out, dim=1)\n                pred = net_out.data.max(1)[1].cpu().numpy()\n                gt = labels.data.cpu().numpy()\n\n                running_metrics.update(gt, pred)\n                score, class_iou = running_metrics.get_scores()\n\n                metric_log = """"\n                for k, v in score.items():\n                    metric_log += "" {}: \\t %.4f, "".format(k) % v\n                running_metrics.reset()\n\n                logs = loss_log + metric_log\n                # print(logs)\n\n                if args.tensor_board:\n                    writer.add_scalar(\'Training/Losses\', last_loss, full_iter)\n                    writer.add_scalars(\'Training/Metrics\', score, full_iter)\n                    writer.add_text(\'Training/Text\', logs, full_iter)\n\n                    for name, param in model.named_parameters():\n                        writer.add_histogram(name, param.clone().cpu().data.numpy(), full_iter)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.2 Mini-Batch Validation\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Validation for Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.eval()\n\n        mval_loss = 0.0\n        vali_count = 0\n        for i_val, (images_val, labels_val) in enumerate(valid_loader):\n            vali_count += 1\n\n            images_val = Variable(images_val.cuda(), volatile=True)\n            labels_val = Variable(labels_val.cuda(), volatile=True)\n\n            net_out = model(images_val)  # Here we have 4 output for 4 loss\n\n            topk = topk_base * 512\n            val_loss = loss_fn(input=net_out, target=labels_val, K=topk, weight=None)\n\n            mval_loss += val_loss.data[0]\n\n            net_out = F.softmax(net_out, dim=1)\n            pred = net_out.data.max(1)[1].cpu().numpy()\n            gt = labels_val.data.cpu().numpy()\n            running_metrics.update(gt, pred)\n\n        mval_loss /= vali_count\n\n        loss_log = ""Epoch [%d/%d] Loss: \\t %.4f"" % (epoch + 1, args.n_epoch, mval_loss)\n        metric_log = """"\n        score, class_iou = running_metrics.get_scores()\n        for k, v in score.items():\n            metric_log += "" {} \\t %.4f, "".format(k) % v\n        running_metrics.reset()\n\n        logs = loss_log + metric_log\n        # print(logs)\n        pbar.set_postfix(Train_Loss=last_loss, Vali_Loss=mval_loss, Vali_mIoU=score[\'Mean_IoU\'])\n\n        if args.tensor_board:\n            writer.add_scalar(\'Validation/Losses\', mval_loss, epoch)\n            writer.add_scalars(\'Validation/Metrics\', score, epoch)\n            writer.add_text(\'Validation/Text\', logs, epoch)\n\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n            # export scalar data to JSON for external processing\n            # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n\n        if score[\'Mean_IoU\'] >= best_iou:\n            best_iou = score[\'Mean_IoU\']\n            state = {\'epoch\': epoch + 1,\n                     ""best_iou"": best_iou,\n                     \'model_state\': model.state_dict(),\n                     \'optimizer_state\': optimizer.state_dict()}\n            torch.save(state, ""{}{}_rfmobilenetv2_best_model.pkl"".format(weight_dir, args.dataset))\n\n        # scheduler.step()\n        # scheduler.batch_step()\n        pbar.close()\n\n    if args.tensor_board:\n        # export scalar data to JSON for external processing\n        # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n        writer.close()\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Training Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Hyper-params\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    parser = argparse.ArgumentParser(description=\'Hyperparams\')\n\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'cityscapes\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_rows\', nargs=\'?\', type=int, default=512,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--img_cols\', nargs=\'?\', type=int, default=1024,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--n_epoch\', nargs=\'?\', type=int, default=200,\n                        help=\'# of the epochs\')\n    parser.add_argument(\'--batch_size\', nargs=\'?\', type=int, default=10,\n                        help=\'Batch Size\')\n    parser.add_argument(\'--l_rate\', nargs=\'?\', type=float, default=2.5e-3,\n                        help=\'Learning Rate\')\n    parser.add_argument(\'--crop_ratio\', nargs=\'?\', type=float, default=0.875,\n                        help=\'The ratio to crop the input image\')\n    parser.add_argument(\'--resume\', nargs=\'?\', type=str, default=""cityscapes_rfmobilenetv2_best_model.pkl"",\n                        help=\'Path to previous saved model to restart from\')\n    parser.add_argument(\'--pre_trained\', nargs=\'?\', type=str, default=""cityscapes_rfmobilenetv2_fit_best_model.pkl"",\n                        help=\'Path to pre-trained  model to init from\')\n    parser.add_argument(\'--tensor_board\', nargs=\'?\', type=bool, default=True,\n                        help=\'Show visualization(s) on tensor_board | True by  default\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Train the Deep Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    data_path = ""/zfs/zhang/Cityscapes""\n    save_path = ""/zfs/zhang/TrainLog/""\n    train_args = parser.parse_args()\n    train(train_args, data_path, save_path)\n'"
scripts/train_mobile_mvd.py,15,"b'import torch.nn.functional as F\nimport argparse\nimport torch\nimport time\nimport os\n\nfrom datasets.mapillary_vistas_loader import MapillaryVistasLoader\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom scripts.loss import bootstrapped_cross_entropy2d\nfrom models.mobilenetv2plus import MobileNetV2Plus\nfrom scripts.metrics import RunningScore\nfrom datasets.augmentations import *\n\n\ndef train(args, data_root, save_root):\n    weight_dir = ""{}weights/"".format(save_root)\n    log_dir = ""{}logs/MobileNetV2Plus-{}"".format(save_root, time.strftime(""%Y-%m-%d-%H-%M-%S"", time.localtime()))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Augmentations\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    net_h, net_w = int(args.img_rows*args.crop_ratio), int(args.img_cols*args.crop_ratio)\n\n    augment_train = Compose([RandomHorizontallyFlip(), RandomSized((0.625, 0.75)),\n                             RandomRotate(6), RandomCrop((net_h, net_w))])\n    augment_valid = Compose([RandomHorizontallyFlip(), RandomSized((0.625, 0.75)),\n                             CenterCrop((net_h, net_w))])\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    train_loader = MapillaryVistasLoader(data_root, split=""training"", is_transform=True,\n                                         img_size=(args.img_rows, args.img_cols), augmentations=augment_train)\n    valid_loader = MapillaryVistasLoader(data_root, split=""validation"", is_transform=True,\n                                         img_size=(args.img_rows, args.img_cols), augmentations=augment_valid)\n\n    n_classes = train_loader.n_classes\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Metrics\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    running_metrics = RunningScore(n_classes)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 4. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n\n    model = MobileNetV2Plus(n_class=n_classes, in_size=(net_h, net_w), width_mult=1.0,\n                            out_sec=(252, 86), aspp_sec=(12, 24, 36))\n\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()\n\n    # 4.1 Setup Optimizer\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # Check if model has custom optimizer / loss\n    if hasattr(model.module, \'optimizer\'):\n        optimizer = model.module.optimizer\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.90, weight_decay=5e-4, nesterov=True)\n\n        # for pg in optimizer.param_groups:\n        #     print(pg[\'lr\'])\n\n        # optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999),\n        #                             eps=1e-08, weight_decay=0, amsgrad=True)\n        # optimizer = YFOptimizer(model.parameters(), lr=2.5e-3, mu=0.9, clip_thresh=10000, weight_decay=5e-4)\n\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n\n    # 4.2 Setup Loss\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    if hasattr(model.module, \'loss\'):\n        print(\'> Using custom loss\')\n        loss_fn = model.module.loss\n    else:\n        # loss_fn = cross_entropy2d\n        loss_fn = bootstrapped_cross_entropy2d\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 5. Resume Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    args.start_epoch = 0\n    best_iou = -100.0\n    if args.resume is not None:\n        full_path = ""{}{}"".format(weight_dir, args.resume)\n        if os.path.isfile(full_path):\n            print(""> Loading model and optimizer from checkpoint \'{}\'"".format(args.resume))\n\n            checkpoint = torch.load(full_path)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_iou = checkpoint[\'best_iou\']\n            model.load_state_dict(checkpoint[\'model_state\'])          # weights\n\n            optimizer.load_state_dict(checkpoint[\'optimizer_state\'])  # gradient state\n\n            # optimizer = YFOptimizer(model.parameters(), lr=2.5e-3, mu=0.9, clip_thresh=10000, weight_decay=5e-4)\n            del checkpoint\n\n            print(""> Loaded checkpoint \'{}\' (epoch {})"".format(args.resume, args.start_epoch))\n\n        else:\n            print(""> No checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        if args.pre_trained is not None:\n            print(""> Loading weights from pre-trained model \'{}\'"".format(args.pre_trained))\n            full_path = ""{}{}"".format(weight_dir, args.pre_trained)\n\n            pre_weight = torch.load(full_path)\n            pre_weight = pre_weight[""model_state""]\n            # pre_weight = pre_weight[""state_dict""]\n\n            model_dict = model.state_dict()\n\n            pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            model.load_state_dict(model_dict)\n\n            del pre_weight\n            del model_dict\n            del pretrained_dict\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 3. Setup tensor_board for visualization\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    writer = None\n    if args.tensor_board:\n        writer = SummaryWriter(log_dir=log_dir, comment=""MobileNetV2"")\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        writer.add_graph(model, dummy_input)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 6. Train Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 2. Model Training start..."")\n    train_loader = data.DataLoader(train_loader, batch_size=args.batch_size, num_workers=6, shuffle=True)\n    valid_loader = data.DataLoader(valid_loader, batch_size=args.batch_size, num_workers=6)\n\n    num_batches = int(math.ceil(len(train_loader.dataset.files[train_loader.dataset.split]) /\n                                float(train_loader.batch_size)))\n\n    loss_wgt1 = 1.0\n    loss_wgt2 = 1.0\n    loss_wgt3 = 1.0\n\n    topk_base = 512\n    # topk_multipliers = [64, 128, 256, 512]\n    for epoch in np.arange(args.start_epoch, args.n_epoch):\n        pbar = tqdm(np.arange(num_batches))\n        last_loss = [0.0, 0.0, 0.0]\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.1 Mini-Batch Learning\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Training Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.train()\n\n        for train_i, (images, labels) in enumerate(train_loader):  # One mini-Batch data, One iteration\n            full_iter = (epoch * num_batches) + train_i + 1\n\n            # poly_lr_scheduler(optimizer, init_lr=args.l_rate, iter=full_iter,\n            #                   lr_decay_iter=1, max_iter=args.n_epoch*num_batches, power=0.9)\n\n            pbar.update(1)\n            pbar.set_description(""> Epoch [%d/%d]"" % (epoch + 1, args.n_epoch))\n\n            images = Variable(images.cuda(), requires_grad=True)   # Image feed into the deep neural network\n            labels = Variable(labels.cuda(), requires_grad=False)\n\n            optimizer.zero_grad()\n            out_stg1, out_stg2, out_stg3 = model(images)  # Here we have 3 output for 3 loss\n\n            topk = topk_base * 256\n            stg1_loss = loss_wgt1 * loss_fn(input=out_stg1, target=labels, K=topk)\n            stg2_loss = loss_wgt2 * loss_fn(input=out_stg2, target=labels, K=topk)\n            stg3_loss = loss_wgt3 * loss_fn(input=out_stg3, target=labels, K=topk)\n\n            last_loss = [stg1_loss.data[0], stg2_loss.data[0], stg3_loss.data[0]]\n            loss = [stg1_loss, stg2_loss, stg3_loss]\n            torch.autograd.backward(loss)\n            optimizer.step()\n\n            pbar.set_postfix(Loss1=stg1_loss.data[0], Loss2=stg2_loss.data[0], Loss3=stg3_loss.data[0])\n\n            if (train_i + 1) % 62 == 0:\n\n                loss_log = ""Epoch [%d/%d], Iter: %d Loss1: \\t %.4f, Loss2: \\t %.4f, "" \\\n                           ""Loss3: \\t %.4f"" % (epoch + 1, args.n_epoch, train_i + 1,\n                                               last_loss[0], last_loss[1], last_loss[2])\n\n                out_stg3 = F.softmax(out_stg3, dim=1)\n                pred = out_stg3.data.max(1)[1].cpu().numpy()\n                gt = labels.data.cpu().numpy()\n\n                running_metrics.update(gt, pred)\n\n                score, class_iou = running_metrics.get_scores()\n\n                metric_log = """"\n                for k, v in score.items():\n                    metric_log += "" {}: \\t %.4f, "".format(k) % v\n                running_metrics.reset()\n\n                logs = loss_log + metric_log\n                # print(logs)\n\n                if args.tensor_board:\n                    writer.add_scalars(\'Training/Losses\',\n                                       {\'Loss_Stage1\': last_loss[0],\n                                        \'Loss_Stage2\': last_loss[1],\n                                        \'Loss_Stage3\': last_loss[2]},\n                                       full_iter)\n                    writer.add_scalars(\'Training/Metrics\', score, full_iter)\n\n                    writer.add_text(\'Training/Text\', logs, full_iter)\n\n                    for name, param in model.named_parameters():\n                        writer.add_histogram(name, param.clone().cpu().data.numpy(), full_iter)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.2 Mini-Batch Validation\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Validation for Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.eval()\n        val_loss = [0.0, 0.0, 0.0]\n        vali_count = 0\n        for i_val, (images_val, labels_val) in enumerate(valid_loader):\n            vali_count += 1\n\n            images_val = Variable(images_val.cuda(), volatile=True)\n            labels_val = Variable(labels_val.cuda(), volatile=True)\n\n            out_stg1, out_stg2, out_stg3 = model(images_val)  # Here we have 4 output for 4 loss\n\n            topk = topk_base * 256\n            stg1_val_loss = loss_wgt1 * loss_fn(input=out_stg1, target=labels_val, K=topk)\n            stg2_val_loss = loss_wgt2 * loss_fn(input=out_stg2, target=labels_val, K=topk)\n            stg3_val_loss = loss_wgt3 * loss_fn(input=out_stg3, target=labels_val, K=topk)\n\n            val_loss = [val_loss[0] + stg1_val_loss.data[0],\n                        val_loss[1] + stg2_val_loss.data[0],\n                        val_loss[2] + stg3_val_loss.data[0]]\n\n            out_stg3 = F.softmax(out_stg3, dim=1)\n            pred = out_stg3.data.max(1)[1].cpu().numpy()\n            gt = labels_val.data.cpu().numpy()\n            running_metrics.update(gt, pred)\n\n        val_loss = [val_loss[0]/vali_count, val_loss[1]/vali_count, val_loss[2]/vali_count]\n\n        loss_log = ""Epoch [%d/%d] Loss1: \\t %.4f, Loss2: \\t %.4f, "" \\\n                   ""Loss3: \\t %.4f"" % (epoch + 1, args.n_epoch,\n                                       val_loss[0], val_loss[1],  val_loss[2])\n        metric_log = """"\n        score, class_iou = running_metrics.get_scores()\n        for k, v in score.items():\n            metric_log += "" {} \\t %.4f, "".format(k) % v\n        running_metrics.reset()\n\n        logs = loss_log + metric_log\n        # print(logs)\n        pbar.set_postfix(Train_Loss=last_loss[1], Vali_Loss=val_loss[1]/loss_wgt2, Vali_mIoU=score[\'Mean_IoU\'])\n\n        if args.tensor_board:\n            writer.add_scalars(\'Validation/Losses\',\n                               {\'Loss_Stage1\': val_loss[0],\n                                \'Loss_Stage2\': val_loss[1],\n                                \'Loss_Stage3\': val_loss[2]}, epoch)\n            writer.add_scalars(\'Validation/Metrics\', score, epoch)\n\n            writer.add_text(\'Validation/Text\', logs, epoch)\n\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n            # export scalar data to JSON for external processing\n            # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n\n        if score[\'Mean_IoU\'] >= best_iou:\n            best_iou = score[\'Mean_IoU\']\n            state = {\'epoch\': epoch + 1,\n                     ""best_iou"": best_iou,\n                     \'model_state\': model.state_dict(),\n                     \'optimizer_state\': optimizer.state_dict()}\n            torch.save(state, ""{}{}_mobilenetv2_gtfine_best_model.pkl"".format(weight_dir, args.dataset))\n\n        # Note that step should be called after validate()\n        scheduler.step()\n        pbar.close()\n\n    if args.tensor_board:\n        # export scalar data to JSON for external processing\n        # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n        writer.close()\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Training Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Hyper-params\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    parser = argparse.ArgumentParser(description=\'Hyperparams\')\n\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'mvd\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_rows\', nargs=\'?\', type=int, default=512,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--img_cols\', nargs=\'?\', type=int, default=1024,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--n_epoch\', nargs=\'?\', type=int, default=256,\n                        help=\'# of the epochs\')\n    parser.add_argument(\'--batch_size\', nargs=\'?\', type=int, default=2,\n                        help=\'Batch Size\')\n    parser.add_argument(\'--l_rate\', nargs=\'?\', type=float, default=1e-3,\n                        help=\'Learning Rate\')\n    parser.add_argument(\'--crop_ratio\', nargs=\'?\', type=float, default=0.875,\n                        help=\'The ratio to crop the input image\')\n    parser.add_argument(\'--resume\', nargs=\'?\', type=str, default=None,\n                        help=\'Path to previous saved model to restart from\')\n    parser.add_argument(\'--pre_trained\', nargs=\'?\', type=str, default=""mobilenetv2plus_model.pkl"",\n                        help=\'Path to pre-trained  model to init from\')\n    parser.add_argument(\'--tensor_board\', nargs=\'?\', type=bool, default=True,\n                        help=\'Show visualization(s) on tensor_board | True by  default\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Train the Deep Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    data_path = ""/zfs/zhang/MVD""\n    save_path = ""/zfs/zhang/TrainLog/""\n    train_args = parser.parse_args()\n    train(train_args, data_path, save_path)\n'"
scripts/train_sedpnet.py,12,"b'import argparse\nimport torch\nimport time\nimport os\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom scripts.utils import poly_lr_scheduler, init_weights\nfrom scripts.loss import bootstrapped_cross_entropy2d\nfrom models.sedpshufflenet import SEDPNShuffleNet\nfrom scripts.metrics import RunningScore\nfrom modules import InPlaceABNWrapper\nfrom datasets.augmentations import *\nfrom functools import partial\n\n\ndef train(args, data_root, save_root):\n    weight_dir = ""{}weights/"".format(save_root)\n    log_dir = ""{}logs/SE-DPShuffleNet-{}"".format(save_root, time.strftime(""%Y-%m-%d-%H-%M-%S"", time.localtime()))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Augmentations\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    net_h, net_w = int(args.img_rows*args.crop_ratio), int(args.img_cols*args.crop_ratio)\n    augment_train = Compose([RandomHorizontallyFlip(), RandomRotate(6), RandomCrop((net_h, net_w))])\n    augment_valid = Compose([CenterCrop((net_h, net_w))])\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Dataloader\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    train_loader = CityscapesLoader(data_root, is_transform=True, gt=""gtCoarse"", split=\'train_extra\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_train)\n    valid_loader = CityscapesLoader(data_path, is_transform=True, gt=""gtCoarse"", split=\'val\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_valid)\n\n    n_classes = train_loader.n_classes\n    train_loader = data.DataLoader(train_loader, batch_size=args.batch_size, num_workers=8, shuffle=True)\n    valid_loader = data.DataLoader(valid_loader, batch_size=args.batch_size, num_workers=8)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 3. Setup Metrics\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    running_metrics = RunningScore(n_classes)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 4. Setup tensor_board for visualization\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    writer = None\n    if args.tensor_board:\n        writer = SummaryWriter(log_dir=log_dir, comment=""SE-DPShuffleNet"")\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 5. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n    model = SEDPNShuffleNet(small=False, classes=n_classes, in_size=(net_h, net_w), num_init_features=64,\n                            k_r=96, groups=4, k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n                            out_sec=(512, 256, 128), dil_sec=(1, 1, 1, 2, 4), aspp_sec=(6, 12, 18),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    # if args.tensor_board:\n    #     dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda())\n    #     writer.add_graph(model, dummy_input)\n\n    # 5.1 Setup Optimizer\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # Check if model has custom optimizer / loss\n    if hasattr(model.module, \'optimizer\'):\n        optimizer = model.module.optimizer\n    else:\n        # optimizer = torch.optim.Adam(model.parameters(), lr=args.l_rate, weight_decay=5e-4)\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.90, weight_decay=5e-4)\n\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n\n    # 5.2 Setup Loss\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    if hasattr(model.module, \'loss\'):\n        print(\'Using custom loss\')\n        loss_fn = model.module.loss\n    else:\n        loss_fn = bootstrapped_cross_entropy2d\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 6. Resume Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    if args.resume is not None:\n        if os.path.isfile(args.resume):\n            print(""Loading model and optimizer from checkpoint \'{}\'"".format(args.resume))\n            full_path = ""{}{}"".format(weight_dir, args.resume)\n\n            checkpoint = torch.load(full_path)\n            model.load_state_dict(checkpoint[\'model_state\'])          # weights\n            optimizer.load_state_dict(checkpoint[\'optimizer_state\'])  # gradient state\n\n            print(""Loaded checkpoint \'{}\' (epoch {})"".format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(""No checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        init_weights(model)\n        if args.pre_trained is not None:\n            print(""> Loading weights from pre-trained model \'{}\'"".format(args.pre_trained))\n            full_path = ""{}{}"".format(weight_dir, args.pre_trained)\n\n            pre_weight = torch.load(full_path)\n            pre_weight = pre_weight[""model_state""]\n            # pre_weight = pre_weight[""state_dict""]\n\n            model_dict = model.state_dict()\n            pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            model.load_state_dict(model_dict)\n\n            del pre_weight\n            del model_dict\n            del pretrained_dict\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 7. Train Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 2. Model Training start..."")\n    num_batches = int(math.ceil(len(train_loader.dataset.files[train_loader.dataset.split]) /\n                                float(train_loader.batch_size)))\n\n    loss_wgt1 = 1.0\n    loss_wgt2 = 1.0\n    loss_wgt3 = 1.0\n    loss_wgt4 = 1.0\n\n    best_iou = -100.0\n    for epoch in np.arange(args.n_epoch):\n        pbar = tqdm(np.arange(num_batches))\n        last_loss = [0.0, 0.0, 0.0, 0.0]\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.1 Mini-Batch Learning\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Training Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.train()\n\n        for train_i, (images, labels) in enumerate(train_loader):  # One mini-Batch data, One iteration\n            pbar.update(1)\n            pbar.set_description(""> Epoch [%d/%d]"" % (epoch + 1, args.n_epoch))\n\n            images = Variable(images.cuda(), requires_grad=True)   # Image feed into the deep neural network\n            labels = Variable(labels.cuda(async=True), requires_grad=False)\n\n            optimizer.zero_grad()\n            out_stg1, out_stg2, out_stg3, out_stg4 = model(images)  # Here we have 4 output for 4 loss\n\n            stg1_loss = loss_wgt1 * loss_fn(input=out_stg1, target=labels, K=512*256)\n            stg2_loss = loss_wgt2 * loss_fn(input=out_stg2, target=labels, K=512*256)\n            stg3_loss = loss_wgt3 * loss_fn(input=out_stg3, target=labels, K=512*256)\n            stg4_loss = loss_wgt4 * loss_fn(input=out_stg4, target=labels, K=512*256)\n\n            last_loss = [stg1_loss.data[0], stg2_loss.data[0],\n                         stg3_loss.data[0], stg4_loss.data[0]]\n\n            loss = [stg1_loss, stg2_loss, stg3_loss, stg4_loss]\n            torch.autograd.backward(loss)\n            optimizer.step()\n\n            pbar.set_postfix(Loss1=last_loss[0], Loss2=last_loss[1], Loss3=last_loss[2], Loss4=last_loss[3])\n\n            if (train_i + 1) % 31 == 0:\n                full_iter = (epoch*num_batches) + train_i + 1\n                loss_log = ""Epoch [%d/%d], Iter: %d Loss1: \\t %.4f, Loss2: \\t %.4f, "" \\\n                           ""Loss3: \\t %.4f, Loss: \\t %.4f,"" % (epoch + 1, args.n_epoch, train_i + 1,\n                                                               last_loss[0], last_loss[1],\n                                                               last_loss[2], last_loss[3])\n\n                pred = out_stg4.data.max(1)[1].cpu().numpy()\n                gt = labels.data.cpu().numpy()\n                running_metrics.update(gt, pred)\n                score, class_iou = running_metrics.get_scores()\n\n                metric_log = """"\n                for k, v in score.items():\n                    metric_log += "" {}: \\t %.4f, "".format(k) % v\n                running_metrics.reset()\n\n                logs = loss_log + metric_log\n                # print(logs)\n\n                if args.tensor_board:\n                    writer.add_scalars(\'Training/Losses\',\n                                       {\'Loss_Stage1\': last_loss[0], \'Loss_Stage2\': last_loss[1],\n                                        \'Loss_Stage3\': last_loss[2], \'Loss_Stage4\': last_loss[3]},\n                                       full_iter)\n                    writer.add_scalars(\'Training/Metrics\', score, full_iter)\n\n                    writer.add_text(\'Training/Text\', logs, full_iter)\n\n                    for name, param in model.named_parameters():\n                        writer.add_histogram(name, param.clone().cpu().data.numpy(), full_iter)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.2 Mini-Batch Validation\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Validation for Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.eval()\n        val_loss = [0.0, 0.0, 0.0, 0.0]\n\n        vali_count = 0\n        for i_val, (images_val, labels_val) in enumerate(valid_loader):\n            vali_count += 1\n\n            images_val = Variable(images_val.cuda(), volatile=True)\n            labels_val = Variable(labels_val.cuda(), volatile=True)\n\n            out_stg1, out_stg2, out_stg3, out_stg4 = model(images_val)  # Here we have 4 output for 4 loss\n            stg1_val_loss = loss_wgt1 * loss_fn(input=out_stg1, target=labels_val, K=512*256)\n            stg2_val_loss = loss_wgt2 * loss_fn(input=out_stg2, target=labels_val, K=512*256)\n            stg3_val_loss = loss_wgt3 * loss_fn(input=out_stg3, target=labels_val, K=512*256)\n            stg4_val_loss = loss_wgt4 * loss_fn(input=out_stg4, target=labels_val, K=512*256)\n\n            val_loss = [val_loss[0] + stg1_val_loss.data[0], val_loss[1] + stg2_val_loss.data[0],\n                        val_loss[2] + stg3_val_loss.data[0], val_loss[3] + stg4_val_loss.data[0]]\n\n            pred = out_stg4.data.max(1)[1].cpu().numpy()\n            gt = labels_val.data.cpu().numpy()\n            running_metrics.update(gt, pred)\n\n        val_loss = [val_loss[0]/vali_count, val_loss[1]/vali_count,\n                    val_loss[2]/vali_count, val_loss[3]/vali_count]\n\n        loss_log = ""Epoch [%d/%d] Loss1: \\t %.4f, Loss2: \\t %.4f, "" \\\n                   ""Loss3: \\t %.4f, Loss: \\t %.4f,"" % (epoch + 1, args.n_epoch,\n                                                       val_loss[0], val_loss[1],\n                                                       val_loss[2], val_loss[3])\n        metric_log = """"\n        score, class_iou = running_metrics.get_scores()\n        for k, v in score.items():\n            metric_log += "" {} \\t %.4f, "".format(k) % v\n        running_metrics.reset()\n\n        logs = loss_log + metric_log\n        # print(logs)\n        pbar.set_postfix(Train_Loss=last_loss[3], Vali_Loss=val_loss[3]/loss_wgt4, Vali_mIoU=score[\'Mean_IoU\'])\n\n        if args.tensor_board:\n            writer.add_scalars(\'Validation/Losses\',\n                               {\'Loss_Stage1\': val_loss[0], \'Loss_Stage2\': val_loss[1],\n                                \'Loss_Stage3\': val_loss[2], \'Loss_Stage4\': val_loss[3]}, epoch)\n            writer.add_scalars(\'Validation/Metrics\', score, epoch)\n\n            writer.add_text(\'Validation/Text\', logs, epoch)\n\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n            # export scalar data to JSON for external processing\n            # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n\n        if score[\'Mean_IoU\'] >= best_iou:\n            best_iou = score[\'Mean_IoU\']\n            state = {\'epoch\': epoch + 1,\n                     ""best_iou"": best_iou,\n                     \'model_state\': model.state_dict(),\n                     \'optimizer_state\': optimizer.state_dict()}\n            torch.save(state, ""{}{}_sedpshufflenet_best_model.pkl"".format(weight_dir, args.dataset))\n\n        # Note that step should be called after validate()\n        scheduler.step()\n        pbar.close()\n\n    if args.tensor_board:\n        # export scalar data to JSON for external processing\n        # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n        writer.close()\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Training Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Hyper-params\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    parser = argparse.ArgumentParser(description=\'Hyperparams\')\n\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'cityscapes\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_rows\', nargs=\'?\', type=int, default=512,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--img_cols\', nargs=\'?\', type=int, default=1024,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--n_epoch\', nargs=\'?\', type=int, default=120,\n                        help=\'# of the epochs\')\n    parser.add_argument(\'--batch_size\', nargs=\'?\', type=int, default=2,\n                        help=\'Batch Size\')\n    parser.add_argument(\'--l_rate\', nargs=\'?\', type=float, default=2.5e-3,\n                        help=\'Learning Rate\')\n    parser.add_argument(\'--crop_ratio\', nargs=\'?\', type=float, default=0.875,\n                        help=\'The ratio to crop the input image\')\n    parser.add_argument(\'--resume\', nargs=\'?\', type=str, default=None,\n                        help=\'Path to previous saved model to restart from\')\n    parser.add_argument(\'--tensor_board\', nargs=\'?\', type=bool, default=True,\n                        help=\'Show visualization(s) on tensor_board | True by  default\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Train the Deep Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2""\n    data_path = ""/media/datavolume3/huijun/SEDPShuffleNet/datasets/cityscapes""\n    save_path = ""/media/datavolume3/huijun/SEDPShuffleNet/""\n    train_args = parser.parse_args()\n    train(train_args, data_path, save_path)\n'"
scripts/train_share.py,16,"b'import torch.nn.functional as F\nimport argparse\nimport random\nimport torch\nimport time\nimport os\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom scripts.loss import bootstrapped_cross_entropy2d, cross_entropy2d\nfrom scripts.utils import update_aggregated_weight_average\nfrom models.rfmobilenetv2plus import RFMobileNetV2Plus\nfrom models.mobilenetv2share import MobileNetV2Share\nfrom scripts.utils import cosine_annealing_lr\nfrom scripts.utils import poly_topk_scheduler\nfrom scripts.utils import set_optimizer_lr\nfrom modules import InPlaceABNSyncWrapper\nfrom scripts.cyclical_lr import CyclicLR\nfrom scripts.metrics import RunningScore\nfrom datasets.augmentations import *\nfrom functools import partial\n\n\ndef train(args, data_root, save_root):\n    weight_dir = ""{}weights/"".format(save_root)\n    log_dir = ""{}logs/MobileNetV2Share-{}"".format(save_root, time.strftime(""%Y-%m-%d-%H-%M-%S"", time.localtime()))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Augmentations\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    net_h, net_w = int(args.img_rows*args.crop_ratio), int(args.img_cols*args.crop_ratio)\n\n    augment_train = Compose([RandomHorizontallyFlip(), RandomSized((0.875, 1.25)),\n                             RandomCrop((net_h, net_w))])\n    augment_valid = Compose([RandomHorizontallyFlip(), RandomSized((0.875, 1.25)),\n                             CenterCrop((net_h, net_w))])\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    train_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'train\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_train)\n    valid_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'val\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_valid)\n\n    n_classes = train_loader.n_classes\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Metrics\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    running_metrics = RunningScore(n_classes)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 4. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n\n    model = MobileNetV2Share(n_class=19, in_size=(net_h, net_w), width_mult=1.0,\n                             out_sec=256, aspp_sec=(12, 24, 36),\n                             norm_act=partial(InPlaceABNSyncWrapper, activation=""leaky_relu"", slope=0.1))\n    """"""\n\n    model = MobileNetV2Plus(n_class=n_classes, in_size=(net_h, net_w), width_mult=1.0,\n                            out_sec=256, aspp_sec=(12, 24, 36),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    """"""\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()\n\n    # 4.1 Setup Optimizer\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # Check if model has custom optimizer / loss\n    if hasattr(model.module, \'optimizer\'):\n        optimizer = model.module.optimizer\n    else:\n        """"""\n        group_param = [{\'params\': model.module.mod1.parameters(), \'lr\': 0.0},\n                       {\'params\': model.module.mod2.parameters(), \'lr\': 0.0},\n                       {\'params\': model.module.mod3.parameters(), \'lr\': 0.0},\n                       {\'params\': model.module.mod4.parameters(), \'lr\': 0.0},\n                       {\'params\': model.module.mod5.parameters(), \'lr\': 0.0},\n                       {\'params\': model.module.mod6.parameters(), \'lr\': 0.0},\n                       {\'params\': model.module.mod7.parameters(), \'lr\': 0.0},\n                       {\'params\': model.module.mod8.parameters(), \'lr\': 0.0},\n                       {\'params\': model.module.out_se.parameters(), \'lr\': 0.0},\n                       {\'params\': model.module.aspp.parameters()},\n                       {\'params\': model.module.score_se.parameters()},\n                       {\'params\': model.module.score.parameters()}]\n        """"""\n\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.90,\n                                    weight_decay=5e-4, nesterov=True)\n\n        # for pg in optimizer.param_groups:\n        #    print(pg[\'lr\'])\n\n        # optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999),\n        #                             eps=1e-08, weight_decay=0, amsgrad=True)\n        # optimizer = YFOptimizer(model.parameters(), lr=2.5e-3, mu=0.9, clip_thresh=10000, weight_decay=5e-4)\n\n    # 4.2 Setup Loss\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    class_weight = None\n    if hasattr(model.module, \'loss\'):\n        print(\'> Using custom loss\')\n        loss_fn = model.module.loss\n    else:\n        # loss_fn = cross_entropy2d\n\n        class_weight = np.array([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n                                 5.58540548, 3.56563995, 0.12704978, 1.,         0.46783719, 1.34551528,\n                                 5.29974114, 0.28342531, 0.9396095,  0.81551811, 0.42679146, 3.6399074,\n                                 2.78376194], dtype=float)\n\n        """"""\n        class_weight = np.array([3.045384,  12.862123,   4.509889,  38.15694,  35.25279,  31.482613,\n                                 45.792305,  39.694073,  6.0639296,  32.16484,  17.109228,   31.563286,\n                                 47.333973,  11.610675,  44.60042,   45.23716,  45.283024,  48.14782,\n                                 41.924667], dtype=float)/10.0\n        """"""\n        class_weight = torch.from_numpy(class_weight).float().cuda()\n        loss_fn = bootstrapped_cross_entropy2d\n        # loss_fn = cross_entropy2d\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 5. Resume Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    best_iou = -100.0\n    args.start_epoch = 0\n    if args.resume is not None:\n        full_path = ""{}{}"".format(weight_dir, args.resume)\n        if os.path.isfile(full_path):\n            print(""> Loading model and optimizer from checkpoint \'{}\'"".format(args.resume))\n\n            checkpoint = torch.load(full_path)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_iou = checkpoint[\'best_iou\']\n            model.load_state_dict(checkpoint[\'model_state\'])          # weights\n            optimizer.load_state_dict(checkpoint[\'optimizer_state\'])  # gradient state\n\n            # for param_group in optimizer.param_groups:\n            # s    param_group[\'lr\'] = 1e-5\n\n            del checkpoint\n            print(""> Loaded checkpoint \'{}\' (epoch {}, iou {})"".format(args.resume,\n                                                                       args.start_epoch,\n                                                                       best_iou))\n\n        else:\n            print(""> No checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        if args.pre_trained is not None:\n            print(""> Loading weights from pre-trained model \'{}\'"".format(args.pre_trained))\n            full_path = ""{}{}"".format(weight_dir, args.pre_trained)\n\n            pre_weight = torch.load(full_path)\n            pre_weight = pre_weight[""model_state""]\n            # pre_weight = pre_weight[""state_dict""]\n\n            model_dict = model.state_dict()\n\n            pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            model.load_state_dict(model_dict)\n\n            del pre_weight\n            del model_dict\n            del pretrained_dict\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 3. Setup tensor_board for visualization\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    writer = None\n    if args.tensor_board:\n        writer = SummaryWriter(log_dir=log_dir, comment=""MobileNetV2Share"")\n\n    # if args.tensor_board:\n        # dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        # writer.add_graph(model, dummy_input)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 6. Train Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 2. Model Training start..."")\n    train_loader = data.DataLoader(train_loader, batch_size=args.batch_size, num_workers=6, shuffle=True)\n    valid_loader = data.DataLoader(valid_loader, batch_size=args.batch_size, num_workers=6)\n\n    num_batches = int(math.ceil(len(train_loader.dataset.files[train_loader.dataset.split]) /\n                                float(train_loader.batch_size)))\n\n    lr_period = 20 * num_batches\n    swa_weights = model.state_dict()\n\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n    # scheduler = CyclicLR(optimizer, base_lr=1.0e-3, max_lr=6.0e-3, step_size=2*num_batches)\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=32, gamma=0.1)\n    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n\n    topk_init = 896\n    # topk_multipliers = [64, 128, 256, 512]\n    for epoch in np.arange(args.start_epoch, args.n_epoch):\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.1 Mini-Batch Learning\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Training Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.train()\n\n        last_loss = 0.0\n        topk_base = topk_init\n        pbar = tqdm(np.arange(num_batches))\n        for train_i, (images, labels) in enumerate(train_loader):  # One mini-Batch data, One iteration\n            full_iter = (epoch * num_batches) + train_i + 1\n\n            # poly_lr_scheduler(optimizer, init_lr=args.l_rate, iter=full_iter,\n            #                   lr_decay_iter=1, max_iter=args.n_epoch*num_batches, power=0.9)\n\n            batch_lr = args.l_rate * cosine_annealing_lr(lr_period, full_iter)\n            optimizer = set_optimizer_lr(optimizer, batch_lr)\n\n            topk_base = poly_topk_scheduler(init_topk=topk_init, iter=full_iter, topk_decay_iter=1,\n                                            max_iter=args.n_epoch*num_batches, power=0.95)\n\n            images = Variable(images.cuda(), requires_grad=True)   # Image feed into the deep neural network\n            labels = Variable(labels.cuda(), requires_grad=False)\n\n            optimizer.zero_grad()\n            net_out = model(images)  # Here we have 3 output for 3 loss\n\n            topk = topk_base * 896\n            if random.random() < 0.10:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=class_weight)\n            else:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=None)\n\n            last_loss = train_loss.data[0]\n            pbar.update(1)\n            pbar.set_description(""> Epoch [%d/%d]"" % (epoch + 1, args.n_epoch))\n            pbar.set_postfix(Loss=last_loss, TopK=topk_base, LR=batch_lr)\n\n            train_loss.backward()\n            optimizer.step()\n\n            if full_iter % lr_period == 0:\n                swa_weights = update_aggregated_weight_average(model, swa_weights, full_iter, lr_period)\n                state = {\'model_state\': swa_weights}\n                torch.save(state, ""{}{}_mobilenetv2share_swa_model.pkl"".format(weight_dir, args.dataset))\n\n            if (train_i + 1) % 31 == 0:\n                loss_log = ""Epoch [%d/%d], Iter: %d Loss: \\t %.4f"" % (epoch + 1, args.n_epoch,\n                                                                      train_i + 1, last_loss)\n\n                net_out = F.softmax(net_out, dim=1)\n                pred = net_out.data.max(1)[1].cpu().numpy()\n                gt = labels.data.cpu().numpy()\n\n                running_metrics.update(gt, pred)\n                score, class_iou = running_metrics.get_scores()\n\n                metric_log = """"\n                for k, v in score.items():\n                    metric_log += "" {}: \\t %.4f, "".format(k) % v\n                running_metrics.reset()\n\n                logs = loss_log + metric_log\n                # print(logs)\n\n                if args.tensor_board:\n                    writer.add_scalar(\'Training/Losses\', last_loss, full_iter)\n                    writer.add_scalars(\'Training/Metrics\', score, full_iter)\n                    writer.add_text(\'Training/Text\', logs, full_iter)\n\n                    for name, param in model.named_parameters():\n                        writer.add_histogram(name, param.clone().cpu().data.numpy(), full_iter)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.2 Mini-Batch Validation\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Validation for Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.eval()\n\n        mval_loss = 0.0\n        vali_count = 0\n        for i_val, (images_val, labels_val) in enumerate(valid_loader):\n            vali_count += 1\n\n            images_val = Variable(images_val.cuda(), volatile=True)\n            labels_val = Variable(labels_val.cuda(), volatile=True)\n\n            net_out = model(images_val)  # Here we have 4 output for 4 loss\n\n            topk = topk_base * 896\n            val_loss = loss_fn(input=net_out, target=labels_val, K=topk, weight=None)\n\n            mval_loss += val_loss.data[0]\n\n            net_out = F.softmax(net_out, dim=1)\n            pred = net_out.data.max(1)[1].cpu().numpy()\n            gt = labels_val.data.cpu().numpy()\n            running_metrics.update(gt, pred)\n\n        mval_loss /= vali_count\n\n        loss_log = ""Epoch [%d/%d] Loss: \\t %.4f"" % (epoch + 1, args.n_epoch, mval_loss)\n        metric_log = """"\n        score, class_iou = running_metrics.get_scores()\n        for k, v in score.items():\n            metric_log += "" {} \\t %.4f, "".format(k) % v\n        running_metrics.reset()\n\n        logs = loss_log + metric_log\n        # print(logs)\n        pbar.set_postfix(Train_Loss=last_loss, Vali_Loss=mval_loss, Vali_mIoU=score[\'Mean_IoU\'])\n\n        if args.tensor_board:\n            writer.add_scalar(\'Validation/Losses\', mval_loss, epoch)\n            writer.add_scalars(\'Validation/Metrics\', score, epoch)\n            writer.add_text(\'Validation/Text\', logs, epoch)\n\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n            # export scalar data to JSON for external processing\n            # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n\n        if score[\'Mean_IoU\'] >= best_iou:\n            best_iou = score[\'Mean_IoU\']\n            state = {\'epoch\': epoch + 1,\n                     ""best_iou"": best_iou,\n                     \'model_state\': model.state_dict(),\n                     \'optimizer_state\': optimizer.state_dict()}\n            torch.save(state, ""{}{}_mobilenetv2share_best_model.pkl"".format(weight_dir, args.dataset))\n\n        # scheduler.step()\n        # scheduler.batch_step()\n        pbar.close()\n\n    if args.tensor_board:\n        # export scalar data to JSON for external processing\n        # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n        writer.close()\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Training Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Hyper-params\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    parser = argparse.ArgumentParser(description=\'Hyperparams\')\n\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'cityscapes\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_rows\', nargs=\'?\', type=int, default=1024,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--img_cols\', nargs=\'?\', type=int, default=2048,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--n_epoch\', nargs=\'?\', type=int, default=200,\n                        help=\'# of the epochs\')\n    parser.add_argument(\'--batch_size\', nargs=\'?\', type=int, default=2,\n                        help=\'Batch Size\')\n    parser.add_argument(\'--l_rate\', nargs=\'?\', type=float, default=2.5e-3,\n                        help=\'Learning Rate\')\n    parser.add_argument(\'--crop_ratio\', nargs=\'?\', type=float, default=0.875,\n                        help=\'The ratio to crop the input image\')\n    parser.add_argument(\'--resume\', nargs=\'?\', type=str, default=None,\n                        help=\'Path to previous saved model to restart from\')\n    parser.add_argument(\'--pre_trained\', nargs=\'?\', type=str, default=""cityscapes_mobilenetv2_best_model.pkl"",\n                        help=\'Path to pre-trained  model to init from\')\n    parser.add_argument(\'--tensor_board\', nargs=\'?\', type=bool, default=True,\n                        help=\'Show visualization(s) on tensor_board | True by  default\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Train the Deep Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    data_path = ""/zfs/zhang/Cityscapes""\n    save_path = ""/zfs/zhang/TrainLog/""\n    train_args = parser.parse_args()\n    train(train_args, data_path, save_path)\n'"
scripts/train_shuffle.py,16,"b'import torch.nn.functional as F\nimport argparse\nimport random\nimport torch\nimport time\nimport os\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom scripts.loss import bootstrapped_cross_entropy2d, cross_entropy2d\nfrom scripts.utils import update_aggregated_weight_average\nfrom models.shufflenetv2plus import ShuffleNetV2Plus\nfrom models.mobilenetv2plus import MobileNetV2Plus\nfrom scripts.utils import cosine_annealing_lr\nfrom scripts.utils import poly_topk_scheduler\nfrom scripts.utils import set_optimizer_lr\nfrom scripts.cyclical_lr import CyclicLR\nfrom scripts.metrics import RunningScore\nfrom modules import InPlaceABNWrapper\nfrom datasets.augmentations import *\nfrom functools import partial\n\n\ndef train(args, data_root, save_root):\n    weight_dir = ""{}weights/"".format(save_root)\n    log_dir = ""{}logs/ShuffleNetV2Plus-{}"".format(save_root, time.strftime(""%Y-%m-%d-%H-%M-%S"", time.localtime()))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Augmentations\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    net_h, net_w = int(args.img_rows*args.crop_ratio), int(args.img_cols*args.crop_ratio)\n\n    augment_train = Compose([RandomHorizontallyFlip(), RandomSized((0.5, 0.75)),\n                             RandomRotate(5), RandomCrop((net_h, net_w))])\n    augment_valid = Compose([RandomHorizontallyFlip(), Scale((args.img_rows, args.img_cols)),\n                             CenterCrop((net_h, net_w))])\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    train_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'train\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_train)\n    valid_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'val\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_valid)\n\n    n_classes = train_loader.n_classes\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Metrics\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    running_metrics = RunningScore(n_classes)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 4. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n    """"""\n    model = RFShuffleNetV2Plus(n_class=19, groups=3, in_channels=3, in_size=(net_h, net_w),\n                               out_sec=256, aspp_sec=(12, 24, 36),\n                               norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    """"""\n\n    model = ShuffleNetV2Plus(n_class=19, groups=3, in_channels=3, in_size=(net_h, net_w),\n                             out_sec=256, aspp_sec=(12, 24, 36),\n                             norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()\n\n    # 4.1 Setup Optimizer\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # Check if model has custom optimizer / loss\n    if hasattr(model.module, \'optimizer\'):\n        optimizer = model.module.optimizer\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.90,\n                                    weight_decay=5e-4, nesterov=True)\n\n        # for pg in optimizer.param_groups:\n        #     print(pg[\'lr\'])\n\n        # optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999),\n        #                             eps=1e-08, weight_decay=0, amsgrad=True)\n        # optimizer = YFOptimizer(model.parameters(), lr=2.5e-3, mu=0.9, clip_thresh=10000, weight_decay=5e-4)\n\n    # 4.2 Setup Loss\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    class_weight = None\n    if hasattr(model.module, \'loss\'):\n        print(\'> Using custom loss\')\n        loss_fn = model.module.loss\n    else:\n        # loss_fn = cross_entropy2d\n\n        class_weight = np.array([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n                                 5.58540548, 3.56563995, 0.12704978, 1.,         0.46783719, 1.34551528,\n                                 5.29974114, 0.28342531, 0.9396095,  0.81551811, 0.42679146, 3.6399074,\n                                 2.78376194], dtype=float)\n\n        """"""\n        class_weight = np.array([3.045384,  12.862123,   4.509889,  38.15694,  35.25279,  31.482613,\n                                 45.792305,  39.694073,  6.0639296,  32.16484,  17.109228,   31.563286,\n                                 47.333973,  11.610675,  44.60042,   45.23716,  45.283024,  48.14782,\n                                 41.924667], dtype=float)/10.0\n        """"""\n        class_weight = torch.from_numpy(class_weight).float().cuda()\n        loss_fn = bootstrapped_cross_entropy2d\n        # loss_fn = cross_entropy2d\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 5. Resume Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    best_iou = -100.0\n    args.start_epoch = 0\n    if args.resume is not None:\n        full_path = ""{}{}"".format(weight_dir, args.resume)\n        if os.path.isfile(full_path):\n            print(""> Loading model and optimizer from checkpoint \'{}\'"".format(args.resume))\n\n            checkpoint = torch.load(full_path)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_iou = checkpoint[\'best_iou\']\n            model.load_state_dict(checkpoint[\'model_state\'])          # weights\n            optimizer.load_state_dict(checkpoint[\'optimizer_state\'])  # gradient state\n\n            # for param_group in optimizer.param_groups:\n            # s    param_group[\'lr\'] = 1e-5\n\n            del checkpoint\n            print(""> Loaded checkpoint \'{}\' (epoch {}, iou {})"".format(args.resume,\n                                                                       args.start_epoch,\n                                                                       best_iou))\n\n        else:\n            print(""> No checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        if args.pre_trained is not None:\n            print(""> Loading weights from pre-trained model \'{}\'"".format(args.pre_trained))\n            full_path = ""{}{}"".format(weight_dir, args.pre_trained)\n\n            pre_weight = torch.load(full_path)\n            pre_weight = pre_weight[""model_state""]\n            # pre_weight = pre_weight[""state_dict""]\n\n            model_dict = model.state_dict()\n\n            pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            model.load_state_dict(model_dict)\n\n            del pre_weight\n            del model_dict\n            del pretrained_dict\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 3. Setup tensor_board for visualization\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    writer = None\n    if args.tensor_board:\n        writer = SummaryWriter(log_dir=log_dir, comment=""ShuffleNetV2Plus"")\n\n    if args.tensor_board:\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        writer.add_graph(model, dummy_input)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 6. Train Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 2. Model Training start..."")\n    train_loader = data.DataLoader(train_loader, batch_size=args.batch_size, num_workers=6, shuffle=True)\n    valid_loader = data.DataLoader(valid_loader, batch_size=args.batch_size, num_workers=6)\n\n    num_batches = int(math.ceil(len(train_loader.dataset.files[train_loader.dataset.split]) /\n                                float(train_loader.batch_size)))\n\n    lr_period = 20 * num_batches\n    swa_weights = model.state_dict()\n\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n    # scheduler = CyclicLR(optimizer, base_lr=1.0e-3, max_lr=6.0e-3, step_size=2*num_batches)\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=32, gamma=0.1)\n    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n\n    topk_init = 512\n    # topk_multipliers = [64, 128, 256, 512]\n    for epoch in np.arange(args.start_epoch, args.n_epoch):\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.1 Mini-Batch Learning\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Training Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.train()\n\n        last_loss = 0.0\n        topk_base = topk_init\n        pbar = tqdm(np.arange(num_batches))\n        for train_i, (images, labels) in enumerate(train_loader):  # One mini-Batch data, One iteration\n            full_iter = (epoch * num_batches) + train_i + 1\n\n            # poly_lr_scheduler(optimizer, init_lr=args.l_rate, iter=full_iter,\n            #                   lr_decay_iter=1, max_iter=args.n_epoch*num_batches, power=0.9)\n\n            batch_lr = args.l_rate * cosine_annealing_lr(lr_period, full_iter)\n            optimizer = set_optimizer_lr(optimizer, batch_lr)\n\n            topk_base = poly_topk_scheduler(init_topk=topk_init, iter=full_iter, topk_decay_iter=1,\n                                            max_iter=args.n_epoch*num_batches, power=0.99)\n\n            images = Variable(images.cuda(), requires_grad=True)   # Image feed into the deep neural network\n            labels = Variable(labels.cuda(), requires_grad=False)\n\n            optimizer.zero_grad()\n            net_out = model(images)  # Here we have 3 output for 3 loss\n\n            topk = topk_base * 512\n            if random.random() < 0.10:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=class_weight)\n            else:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=None)\n\n            last_loss = train_loss.data[0]\n            pbar.update(1)\n            pbar.set_description(""> Epoch [%d/%d]"" % (epoch + 1, args.n_epoch))\n            pbar.set_postfix(Loss=last_loss, TopK=topk_base, LR=batch_lr)\n\n            train_loss.backward()\n            optimizer.step()\n\n            if full_iter % lr_period == 0:\n                swa_weights = update_aggregated_weight_average(model, swa_weights, full_iter, lr_period)\n                state = {\'model_state\': swa_weights}\n                torch.save(state, ""{}{}_mobilenetv2_swa_model.pkl"".format(weight_dir, args.dataset))\n\n            if (train_i + 1) % 31 == 0:\n                loss_log = ""Epoch [%d/%d], Iter: %d Loss: \\t %.4f"" % (epoch + 1, args.n_epoch,\n                                                                      train_i + 1, last_loss)\n\n                net_out = F.softmax(net_out, dim=1)\n                pred = net_out.data.max(1)[1].cpu().numpy()\n                gt = labels.data.cpu().numpy()\n\n                running_metrics.update(gt, pred)\n                score, class_iou = running_metrics.get_scores()\n\n                metric_log = """"\n                for k, v in score.items():\n                    metric_log += "" {}: \\t %.4f, "".format(k) % v\n                running_metrics.reset()\n\n                logs = loss_log + metric_log\n                # print(logs)\n\n                if args.tensor_board:\n                    writer.add_scalar(\'Training/Losses\', last_loss, full_iter)\n                    writer.add_scalars(\'Training/Metrics\', score, full_iter)\n                    writer.add_text(\'Training/Text\', logs, full_iter)\n\n                    for name, param in model.named_parameters():\n                        writer.add_histogram(name, param.clone().cpu().data.numpy(), full_iter)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.2 Mini-Batch Validation\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Validation for Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.eval()\n\n        mval_loss = 0.0\n        vali_count = 0\n        for i_val, (images_val, labels_val) in enumerate(valid_loader):\n            vali_count += 1\n\n            images_val = Variable(images_val.cuda(), volatile=True)\n            labels_val = Variable(labels_val.cuda(), volatile=True)\n\n            net_out = model(images_val)  # Here we have 4 output for 4 loss\n\n            topk = topk_base * 512\n            val_loss = loss_fn(input=net_out, target=labels_val, K=topk, weight=None)\n\n            mval_loss += val_loss.data[0]\n\n            net_out = F.softmax(net_out, dim=1)\n            pred = net_out.data.max(1)[1].cpu().numpy()\n            gt = labels_val.data.cpu().numpy()\n            running_metrics.update(gt, pred)\n\n        mval_loss /= vali_count\n\n        loss_log = ""Epoch [%d/%d] Loss: \\t %.4f"" % (epoch + 1, args.n_epoch, mval_loss)\n        metric_log = """"\n        score, class_iou = running_metrics.get_scores()\n        for k, v in score.items():\n            metric_log += "" {} \\t %.4f, "".format(k) % v\n        running_metrics.reset()\n\n        logs = loss_log + metric_log\n        # print(logs)\n        pbar.set_postfix(Train_Loss=last_loss, Vali_Loss=mval_loss, Vali_mIoU=score[\'Mean_IoU\'])\n\n        if args.tensor_board:\n            writer.add_scalar(\'Validation/Losses\', mval_loss, epoch)\n            writer.add_scalars(\'Validation/Metrics\', score, epoch)\n            writer.add_text(\'Validation/Text\', logs, epoch)\n\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n            # export scalar data to JSON for external processing\n            # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n\n        if score[\'Mean_IoU\'] >= best_iou:\n            best_iou = score[\'Mean_IoU\']\n            state = {\'epoch\': epoch + 1,\n                     ""best_iou"": best_iou,\n                     \'model_state\': model.state_dict(),\n                     \'optimizer_state\': optimizer.state_dict()}\n            torch.save(state, ""{}{}_shufflenetv2plus_best_model.pkl"".format(weight_dir, args.dataset))\n\n        # scheduler.step()\n        # scheduler.batch_step()\n        pbar.close()\n\n    if args.tensor_board:\n        # export scalar data to JSON for external processing\n        # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n        writer.close()\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Training Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Hyper-params\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    parser = argparse.ArgumentParser(description=\'Hyperparams\')\n\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'cityscapes\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_rows\', nargs=\'?\', type=int, default=512,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--img_cols\', nargs=\'?\', type=int, default=1024,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--n_epoch\', nargs=\'?\', type=int, default=128,\n                        help=\'# of the epochs\')\n    parser.add_argument(\'--batch_size\', nargs=\'?\', type=int, default=6,\n                        help=\'Batch Size\')\n    parser.add_argument(\'--l_rate\', nargs=\'?\', type=float, default=2.5e-3,\n                        help=\'Learning Rate\')\n    parser.add_argument(\'--crop_ratio\', nargs=\'?\', type=float, default=0.875,\n                        help=\'The ratio to crop the input image\')\n    parser.add_argument(\'--resume\', nargs=\'?\', type=str, default=""cityscapes_shufflenetv2plus_best_model.pkl"",\n                        help=\'Path to previous saved model to restart from\')\n    parser.add_argument(\'--pre_trained\', nargs=\'?\', type=str, default=""shufflenetv2plus_model.pkl"",\n                        help=\'Path to pre-trained  model to init from\')\n    parser.add_argument(\'--tensor_board\', nargs=\'?\', type=bool, default=True,\n                        help=\'Show visualization(s) on tensor_board | True by  default\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Train the Deep Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    data_path = ""/zfs/zhang/Cityscapes""\n    save_path = ""/zfs/zhang/TrainLog/""\n    train_args = parser.parse_args()\n    train(train_args, data_path, save_path)\n'"
scripts/train_vortex.py,16,"b'import torch.nn.functional as F\nimport argparse\nimport random\nimport torch\nimport time\nimport os\n\nfrom datasets.cityscapes_loader import CityscapesLoader\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom scripts.loss import bootstrapped_cross_entropy2d, cross_entropy2d\nfrom scripts.utils import update_aggregated_weight_average\nfrom models.mobilenetv2vortex import MobileNetV2Vortex\nfrom scripts.utils import cosine_annealing_lr\nfrom scripts.utils import poly_topk_scheduler\nfrom scripts.utils import set_optimizer_lr\nfrom scripts.cyclical_lr import CyclicLR\nfrom scripts.metrics import RunningScore\nfrom modules import InPlaceABNWrapper\nfrom datasets.augmentations import *\nfrom functools import partial\n\n\ndef train(args, data_root, save_root):\n    weight_dir = ""{}weights/"".format(save_root)\n    log_dir = ""{}logs/MobileNetV2Vortex-{}"".format(save_root, time.strftime(""%Y-%m-%d-%H-%M-%S"", time.localtime()))\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Augmentations\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    net_h, net_w = int(args.img_rows*args.crop_ratio), int(args.img_cols*args.crop_ratio)\n\n    augment_train = Compose([RandomHorizontallyFlip(), RandomSized((0.5, 0.75)),\n                             RandomRotate(5), RandomCrop((net_h, net_w))])\n    augment_valid = Compose([RandomHorizontallyFlip(), Scale((args.img_rows, args.img_cols)),\n                             CenterCrop((net_h, net_w))])\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 0. Setting up DataLoader..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    train_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'train\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_train)\n    valid_loader = CityscapesLoader(data_root, gt=""gtFine"", is_transform=True, split=\'val\',\n                                    img_size=(args.img_rows, args.img_cols),\n                                    augmentations=augment_valid)\n\n    n_classes = train_loader.n_classes\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 2. Setup Metrics\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    running_metrics = RunningScore(n_classes)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 4. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n\n    model = MobileNetV2Vortex(n_class=19, in_size=(net_h, net_w), width_mult=1., out_sec=256, rate_sec=(3, 9, 27),\n                              norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    """"""\n\n    model = MobileNetV2Plus(n_class=n_classes, in_size=(net_h, net_w), width_mult=1.0,\n                            out_sec=256, aspp_sec=(12, 24, 36),\n                            norm_act=partial(InPlaceABNWrapper, activation=""leaky_relu"", slope=0.1))\n    """"""\n    # np.arange(torch.cuda.device_count())\n    model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()\n\n    # 4.1 Setup Optimizer\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # Check if model has custom optimizer / loss\n    if hasattr(model.module, \'optimizer\'):\n        optimizer = model.module.optimizer\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.l_rate, momentum=0.90,\n                                    weight_decay=5e-4, nesterov=True)\n\n        # for pg in optimizer.param_groups:\n        #     print(pg[\'lr\'])\n\n        # optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999),\n        #                             eps=1e-08, weight_decay=0, amsgrad=True)\n        # optimizer = YFOptimizer(model.parameters(), lr=2.5e-3, mu=0.9, clip_thresh=10000, weight_decay=5e-4)\n\n    # 4.2 Setup Loss\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    class_weight = None\n    if hasattr(model.module, \'loss\'):\n        print(\'> Using custom loss\')\n        loss_fn = model.module.loss\n    else:\n        # loss_fn = cross_entropy2d\n\n        class_weight = np.array([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n                                 5.58540548, 3.56563995, 0.12704978, 1.,         0.46783719, 1.34551528,\n                                 5.29974114, 0.28342531, 0.9396095,  0.81551811, 0.42679146, 3.6399074,\n                                 2.78376194], dtype=float)\n\n        """"""\n        class_weight = np.array([3.045384,  12.862123,   4.509889,  38.15694,  35.25279,  31.482613,\n                                 45.792305,  39.694073,  6.0639296,  32.16484,  17.109228,   31.563286,\n                                 47.333973,  11.610675,  44.60042,   45.23716,  45.283024,  48.14782,\n                                 41.924667], dtype=float)/10.0\n        """"""\n        class_weight = torch.from_numpy(class_weight).float().cuda()\n        loss_fn = bootstrapped_cross_entropy2d\n        # loss_fn = cross_entropy2d\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 5. Resume Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    best_iou = -100.0\n    args.start_epoch = 0\n    if args.resume is not None:\n        full_path = ""{}{}"".format(weight_dir, args.resume)\n        if os.path.isfile(full_path):\n            print(""> Loading model and optimizer from checkpoint \'{}\'"".format(args.resume))\n\n            checkpoint = torch.load(full_path)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_iou = checkpoint[\'best_iou\']\n            model.load_state_dict(checkpoint[\'model_state\'])          # weights\n            optimizer.load_state_dict(checkpoint[\'optimizer_state\'])  # gradient state\n\n            # for param_group in optimizer.param_groups:\n            # s    param_group[\'lr\'] = 1e-5\n\n            del checkpoint\n            print(""> Loaded checkpoint \'{}\' (epoch {}, iou {})"".format(args.resume,\n                                                                       args.start_epoch,\n                                                                       best_iou))\n\n        else:\n            print(""> No checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        if args.pre_trained is not None:\n            print(""> Loading weights from pre-trained model \'{}\'"".format(args.pre_trained))\n            full_path = ""{}{}"".format(weight_dir, args.pre_trained)\n\n            pre_weight = torch.load(full_path)\n            pre_weight = pre_weight[""model_state""]\n            # pre_weight = pre_weight[""state_dict""]\n\n            model_dict = model.state_dict()\n\n            pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            model.load_state_dict(model_dict)\n\n            del pre_weight\n            del model_dict\n            del pretrained_dict\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 3. Setup tensor_board for visualization\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    writer = None\n    if args.tensor_board:\n        writer = SummaryWriter(log_dir=log_dir, comment=""MobileNetV2Vortex"")\n\n    if args.tensor_board:\n        dummy_input = Variable(torch.rand(1, 3, net_h, net_w).cuda(), requires_grad=True)\n        writer.add_graph(model, dummy_input)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 6. Train Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> 2. Model Training start..."")\n    train_loader = data.DataLoader(train_loader, batch_size=args.batch_size, num_workers=6, shuffle=True)\n    valid_loader = data.DataLoader(valid_loader, batch_size=args.batch_size, num_workers=6)\n\n    num_batches = int(math.ceil(len(train_loader.dataset.files[train_loader.dataset.split]) /\n                                float(train_loader.batch_size)))\n\n    lr_period = 20 * num_batches\n    swa_weights = model.state_dict()\n\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n    # scheduler = CyclicLR(optimizer, base_lr=1.0e-3, max_lr=6.0e-3, step_size=2*num_batches)\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=32, gamma=0.1)\n    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n\n    topk_init = 512\n    # topk_multipliers = [64, 128, 256, 512]\n    for epoch in np.arange(args.start_epoch, args.n_epoch):\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.1 Mini-Batch Learning\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Training Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.train()\n\n        last_loss = 0.0\n        topk_base = topk_init\n        pbar = tqdm(np.arange(num_batches))\n        for train_i, (images, labels) in enumerate(train_loader):  # One mini-Batch data, One iteration\n            full_iter = (epoch * num_batches) + train_i + 1\n\n            # poly_lr_scheduler(optimizer, init_lr=args.l_rate, iter=full_iter,\n            #                   lr_decay_iter=1, max_iter=args.n_epoch*num_batches, power=0.9)\n\n            batch_lr = args.l_rate * cosine_annealing_lr(lr_period, full_iter)\n            optimizer = set_optimizer_lr(optimizer, batch_lr)\n\n            topk_base = poly_topk_scheduler(init_topk=topk_init, iter=full_iter, topk_decay_iter=1,\n                                            max_iter=args.n_epoch*num_batches, power=0.95)\n\n            images = Variable(images.cuda(), requires_grad=True)   # Image feed into the deep neural network\n            labels = Variable(labels.cuda(), requires_grad=False)\n\n            optimizer.zero_grad()\n            net_out = model(images)  # Here we have 3 output for 3 loss\n\n            topk = topk_base * 512\n            if random.random() < 0.20:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=class_weight)\n            else:\n                train_loss = loss_fn(input=net_out, target=labels, K=topk, weight=None)\n\n            last_loss = train_loss.data[0]\n            pbar.update(1)\n            pbar.set_description(""> Epoch [%d/%d]"" % (epoch + 1, args.n_epoch))\n            pbar.set_postfix(Loss=last_loss, TopK=topk_base, LR=batch_lr)\n\n            train_loss.backward()\n            optimizer.step()\n\n            if full_iter % lr_period == 0:\n                swa_weights = update_aggregated_weight_average(model, swa_weights, full_iter, lr_period)\n                state = {\'model_state\': swa_weights}\n                torch.save(state, ""{}{}_mobilenetv2vortex_swa_model.pkl"".format(weight_dir, args.dataset))\n\n            if (train_i + 1) % 31 == 0:\n                loss_log = ""Epoch [%d/%d], Iter: %d Loss: \\t %.4f"" % (epoch + 1, args.n_epoch,\n                                                                      train_i + 1, last_loss)\n\n                net_out = F.softmax(net_out, dim=1)\n                pred = net_out.data.max(1)[1].cpu().numpy()\n                gt = labels.data.cpu().numpy()\n\n                running_metrics.update(gt, pred)\n                score, class_iou = running_metrics.get_scores()\n\n                metric_log = """"\n                for k, v in score.items():\n                    metric_log += "" {}: \\t %.4f, "".format(k) % v\n                running_metrics.reset()\n\n                logs = loss_log + metric_log\n                # print(logs)\n\n                if args.tensor_board:\n                    writer.add_scalar(\'Training/Losses\', last_loss, full_iter)\n                    writer.add_scalars(\'Training/Metrics\', score, full_iter)\n                    writer.add_text(\'Training/Text\', logs, full_iter)\n\n                    for name, param in model.named_parameters():\n                        writer.add_histogram(name, param.clone().cpu().data.numpy(), full_iter)\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 7.2 Mini-Batch Validation\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # print(""> Validation for Epoch [%d/%d]:"" % (epoch + 1, args.n_epoch))\n        model.eval()\n\n        mval_loss = 0.0\n        vali_count = 0\n        for i_val, (images_val, labels_val) in enumerate(valid_loader):\n            vali_count += 1\n\n            images_val = Variable(images_val.cuda(), volatile=True)\n            labels_val = Variable(labels_val.cuda(), volatile=True)\n\n            net_out = model(images_val)  # Here we have 4 output for 4 loss\n\n            topk = topk_base * 512\n            val_loss = loss_fn(input=net_out, target=labels_val, K=topk, weight=None)\n\n            mval_loss += val_loss.data[0]\n\n            net_out = F.softmax(net_out, dim=1)\n            pred = net_out.data.max(1)[1].cpu().numpy()\n            gt = labels_val.data.cpu().numpy()\n            running_metrics.update(gt, pred)\n\n        mval_loss /= vali_count\n\n        loss_log = ""Epoch [%d/%d] Loss: \\t %.4f"" % (epoch + 1, args.n_epoch, mval_loss)\n        metric_log = """"\n        score, class_iou = running_metrics.get_scores()\n        for k, v in score.items():\n            metric_log += "" {} \\t %.4f, "".format(k) % v\n        running_metrics.reset()\n\n        logs = loss_log + metric_log\n        # print(logs)\n        pbar.set_postfix(Train_Loss=last_loss, Vali_Loss=mval_loss, Vali_mIoU=score[\'Mean_IoU\'])\n\n        if args.tensor_board:\n            writer.add_scalar(\'Validation/Losses\', mval_loss, epoch)\n            writer.add_scalars(\'Validation/Metrics\', score, epoch)\n            writer.add_text(\'Validation/Text\', logs, epoch)\n\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n            # export scalar data to JSON for external processing\n            # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n\n        if score[\'Mean_IoU\'] >= best_iou:\n            best_iou = score[\'Mean_IoU\']\n            state = {\'epoch\': epoch + 1,\n                     ""best_iou"": best_iou,\n                     \'model_state\': model.state_dict(),\n                     \'optimizer_state\': optimizer.state_dict()}\n            torch.save(state, ""{}{}_mobilenetv2vortex_best_model.pkl"".format(weight_dir, args.dataset))\n\n        # scheduler.step()\n        # scheduler.batch_step()\n        pbar.close()\n\n    if args.tensor_board:\n        # export scalar data to JSON for external processing\n        # writer.export_scalars_to_json(""{}/all_scalars.json"".format(log_dir))\n        writer.close()\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Training Done!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == \'__main__\':\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Hyper-params\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    parser = argparse.ArgumentParser(description=\'Hyperparams\')\n\n    parser.add_argument(\'--dataset\', nargs=\'?\', type=str, default=\'cityscapes\',\n                        help=\'Dataset to use [\\\'cityscapes, mvd etc\\\']\')\n    parser.add_argument(\'--img_rows\', nargs=\'?\', type=int, default=512,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--img_cols\', nargs=\'?\', type=int, default=1024,\n                        help=\'Height of the input image\')\n    parser.add_argument(\'--n_epoch\', nargs=\'?\', type=int, default=200,\n                        help=\'# of the epochs\')\n    parser.add_argument(\'--batch_size\', nargs=\'?\', type=int, default=10,\n                        help=\'Batch Size\')\n    parser.add_argument(\'--l_rate\', nargs=\'?\', type=float, default=2.5e-3,\n                        help=\'Learning Rate\')\n    parser.add_argument(\'--crop_ratio\', nargs=\'?\', type=float, default=0.875,\n                        help=\'The ratio to crop the input image\')\n    parser.add_argument(\'--resume\', nargs=\'?\', type=str, default=None,\n                        help=\'Path to previous saved model to restart from\')\n    parser.add_argument(\'--pre_trained\', nargs=\'?\', type=str, default=""mobilenetv2plus_model.pkl"",\n                        help=\'Path to pre-trained  model to init from\')\n    parser.add_argument(\'--tensor_board\', nargs=\'?\', type=bool, default=True,\n                        help=\'Show visualization(s) on tensor_board | True by  default\')\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Train the Deep Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1,0""\n    data_path = ""/zfs/zhang/Cityscapes""\n    save_path = ""/zfs/zhang/TrainLog/""\n    train_args = parser.parse_args()\n    train(train_args, data_path, save_path)\n'"
scripts/utils.py,4,"b'# from modules import InPlaceABN, InPlaceABNSync\nfrom itertools import filterfalse\nimport torch.nn as nn\nimport numpy as np\nimport torch\nimport math\nimport os\n\n\ndef get_mean_and_std(dataset):\n    \'\'\'Compute the mean and std value of dataset.\'\'\'\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print(\'> Computing mean and std of images in the dataset..\')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:, i, :, :].mean()\n            std[i] += inputs[:, i, :, :].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\n""""""\ndef init_weights(model, activation=""leaky_relu"", slope=0.1, init=""kaiming_uniform"", gain_multiplier=1):\n    for name, m in model.named_modules():\n        if isinstance(m, nn.Conv2d):\n            init_fn = getattr(nn.init, init)\n\n            if init.startswith(""xavier"") or init == ""orthogonal"":\n                gain = gain_multiplier\n\n                if activation == ""relu"" or activation == ""elu"":\n                    gain *= nn.init.calculate_gain(""relu"")\n                elif activation == ""leaky_relu"":\n                    gain *= nn.init.calculate_gain(""leaky_relu"", slope)\n\n                init_fn(m.weight, gain)\n            elif init.startswith(""kaiming""):\n                if activation == ""relu"" or activation == ""elu"":\n                    init_fn(m.weight, 0)\n                else:\n                    init_fn(m.weight, slope)\n\n            if hasattr(m, ""bias"") and m.bias is not None:\n                nn.init.constant(m.bias, 0.0)\n\n        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, InPlaceABN) or isinstance(m, InPlaceABNSync):\n            nn.init.constant(m.weight, 1.0)\n            nn.init.constant(m.bias, 0.0)\n        elif isinstance(m, nn.Linear):\n            nn.init.xavier_uniform(m.weight, 0.1)\n            nn.init.constant(m.bias, 0.0)\n""""""\n\ndef recursive_glob(rootdir=\'.\', suffix=\'\'):\n    """"""Performs recursive glob with given suffix and rootdir\n        :param rootdir is the root directory\n        :param suffix is the suffix to be searched\n    """"""\n    return [os.path.join(looproot, filename)\n            for looproot, _, filenames in os.walk(rootdir)\n            for filename in filenames if filename.endswith(suffix)]\n\n\ndef poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1, max_iter=89280, power=0.9):\n    """"""Polynomial decay of learning rate\n        :param init_lr is base learning rate\n        :param iter is a current iteration\n        :param lr_decay_iter how frequently decay occurs, default is 1\n        :param max_iter is number of maximum iterations\n        :param power is a polymomial power\n\n    """"""\n    curr_lr = init_lr\n    if iter % lr_decay_iter or iter > max_iter:\n        return curr_lr\n\n    for param_group in optimizer.param_groups:\n        curr_lr = init_lr * (1 - iter / max_iter) ** power\n        param_group[\'lr\'] = curr_lr\n\n    return curr_lr\n\n\ndef cosine_annealing_lr(period, batch_idx):\n    # returns normalised anytime sgdr schedule given period and batch_idx\n    # best performing settings reported in paper are T_0 = 10, T_mult=2\n    # so always use T_mult=2\n    # \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n    # \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n\n    batch_idx = float(batch_idx)\n    restart_period = period\n    while batch_idx/restart_period > 1.:\n        batch_idx = batch_idx - restart_period\n        restart_period = restart_period * 2.\n\n    radians = math.pi*(batch_idx/restart_period)\n    return 0.5*(1.0 + math.cos(radians))\n\n\ndef set_optimizer_lr(optimizer, lr):\n    # callback to set the learning rate in an optimizer, without rebuilding the whole optimizer\n    for param_group in optimizer.param_groups:\n        if param_group[\'lr\'] > 0.0:\n            param_group[\'lr\'] = lr\n    return optimizer\n\n\ndef poly_topk_scheduler(init_topk, iter, topk_decay_iter=1, max_iter=89280, power=0.9):\n    curr_topk = init_topk\n    if iter % topk_decay_iter or iter > max_iter:\n        return curr_topk\n\n    curr_topk = int(init_topk * (1 - iter / max_iter) ** power)\n    if curr_topk <= 128:\n        curr_topk = 128\n\n    return curr_topk\n\n\ndef alpha_blend(input_image, segmentation_mask, alpha=0.5):\n    """"""Alpha Blending utility to overlay RGB masks on RBG images\n        :param input_image is a np.ndarray with 3 channels\n        :param segmentation_mask is a np.ndarray with 3 channels\n        :param alpha is a float value\n\n    """"""\n    blended = np.zeros(input_image.size, dtype=np.float32)\n    blended = input_image * alpha + segmentation_mask * (1 - alpha)\n    return blended\n\n\ndef convert_state_dict(state_dict):\n    """"""Converts a state dict saved from a dataParallel module to normal\n       module state_dict inplace\n       :param state_dict is the loaded DataParallel model_state\n\n    """"""\n\n    for k, v in state_dict.items():\n        name = k[7:]  # remove `module.`\n        state_dict[name] = v\n        del state_dict[k]\n    return state_dict\n\n\ndef update_aggregated_weight_average(model, weight_aws, full_iter, cycle_length):\n    for name, param in model.named_parameters():\n        n_model = full_iter/cycle_length\n        weight_aws[name] = (weight_aws[name]*n_model + param.data)/(n_model + 1)\n\n    return weight_aws\n\n\ndef mean(l, ignore_nan=False, empty=0):\n    """"""\n    nanmean compatible with generators.\n    """"""\n    l = iter(l)\n    if ignore_nan:\n        l = filterfalse(np.isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == \'raise\':\n            raise ValueError(\'Empty mean\')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n\n'"
scripts/yellowfin.py,13,"b'import math\nimport numpy as np\nimport torch\nimport copy\nimport logging\nimport os\nimport pickle as cp\n\n# eps for numerical stability\neps = 1e-6\n\n\nclass YFOptimizer(object):\n    def __init__(self, var_list, lr=0.001, mu=0.0, clip_thresh=None, weight_decay=0.0,\n                 beta=0.999, curv_win_width=20, zero_debias=True, sparsity_debias=False, delta_mu=0.0,\n                 auto_clip_fac=None, force_non_inc_step=False, h_max_log_smooth=True, h_min_log_smooth=True,\n                 checkpoint_interval=1000, verbose=False, adapt_clip=True, stat_protect_fac=100.0,\n                 catastrophic_move_thresh=100.0,\n                 use_disk_checkpoint=False, checkpoint_dir=\'./YF_workspace\'):\n        \'\'\'\n        clip thresh is the threshold value on ||lr * gradient||\n        delta_mu can be place holder/variable/python scalar. They are used for additional\n        momentum in situations such as asynchronous-parallel training. The default is 0.0\n        for basic usage of the optimizer.\n        Args:\n          lr: python scalar. The initial value of learning rate, we use 1.0 in our paper.\n          mu: python scalar. The initial value of momentum, we use 0.0 in our paper.\n          clip_thresh: python scalar. The manaully-set clipping threshold for tf.clip_by_global_norm.\n            if None, the automatic clipping can be carried out. The automatic clipping\n            feature is parameterized by argument auto_clip_fac. The auto clip feature\n            can be switched off with auto_clip_fac = None\n          beta: python scalar. The smoothing parameter for estimations.\n          sparsity_debias: gradient norm and curvature are biased to larger values when\n          calculated with sparse gradient. This is useful when the model is very sparse,\n          e.g. LSTM with word embedding. For non-sparse CNN, turning it off could slightly\n          accelerate the speed.\n          delta_mu: for extensions. Not necessary in the basic use.\n          force_non_inc_step: in some very rare cases, it is necessary to force ||lr * gradient||\n          to be not increasing dramatically for stableness after some iterations.\n          In practice, if turned on, we enforce lr * sqrt(smoothed ||grad||^2)\n          to be less than 2x of the minimal value of historical value on smoothed || lr * grad ||.\n          This feature is turned off by default.\n          checkpoint_interval: interval to do checkpointing. For potential recovery from crashing.\n          stat_protect_fac: a loose hard adaptive threshold over ||grad||^2. It is to protect stat\n          from being destropied by exploding gradient.\n        Other features:\n          If you want to manually control the learning rates, self.lr_factor is\n          an interface to the outside, it is an multiplier for the internal learning rate\n          in YellowFin. It is helpful when you want to do additional hand tuning\n          or some decaying scheme to the tuned learning rate in YellowFin.\n          Example on using lr_factor can be found here:\n          https://github.com/JianGoForIt/YellowFin_Pytorch/blob/master/pytorch-cifar/main.py#L109\n        \'\'\'\n        self._lr = lr\n        self._mu = mu\n        self._lr_t = lr\n        self._mu_t = mu\n        # we convert var_list from generator to list so that\n        # it can be used for multiple times\n        self._var_list = list(var_list)\n        self._clip_thresh = clip_thresh\n        self._auto_clip_fac = auto_clip_fac\n        self._beta = beta\n        self._curv_win_width = curv_win_width\n        self._zero_debias = zero_debias\n        self._sparsity_debias = sparsity_debias\n        self._force_non_inc_step = force_non_inc_step\n        self._optimizer = torch.optim.SGD(self._var_list, lr=self._lr,\n                                          momentum=self._mu, weight_decay=weight_decay)\n        self._iter = 0\n        # global states are the statistics\n        self._global_state = {}\n\n        # for decaying learning rate and etc.\n        self._lr_factor = 1.0\n\n        # smoothing options\n        self._h_max_log_smooth = h_max_log_smooth\n        self._h_min_log_smooth = h_min_log_smooth\n\n        # checkpoint interval\n        self._checkpoint_interval = checkpoint_interval\n\n        self._verbose = verbose\n        if self._verbose:\n            logging.debug(\'Verbose mode with debugging info logged.\')\n\n        # clip exploding gradient\n        self._adapt_clip = adapt_clip\n        self._exploding_grad_clip_thresh = 1e3\n        self._exploding_grad_clip_target_value = 1e3\n        self._stat_protect_fac = stat_protect_fac\n        self._catastrophic_move_thresh = catastrophic_move_thresh\n        self._exploding_grad_detected = False\n\n        # workspace creation\n        self._use_disk_checkpoint = use_disk_checkpoint\n        self._checkpoint_dir = checkpoint_dir\n        if use_disk_checkpoint:\n            if not os.path.exists(self._checkpoint_dir):\n                os.makedirs(self._checkpoint_dir)\n            self._checkpoint_file = ""checkpoint_pid_"" + str(os.getpid())\n\n    def state_dict(self):\n        # for checkpoint saving\n        sgd_state_dict = self._optimizer.state_dict()\n        # for recover model internally in case of numerical issue\n        model_state_list = [p.data \\\n                            for group in self._optimizer.param_groups for p in group[\'params\']]\n        global_state = self._global_state\n        lr_factor = self._lr_factor\n        iter = self._iter\n        lr = self._lr\n        mu = self._mu\n        clip_thresh = self._clip_thresh\n        beta = self._beta\n        curv_win_width = self._curv_win_width\n        zero_debias = self._zero_debias\n        h_min = self._h_min\n        h_max = self._h_max\n\n        return {\n            ""sgd_state_dict"": sgd_state_dict,\n            ""model_state_list"": model_state_list,\n            ""global_state"": global_state,\n            ""lr_factor"": lr_factor,\n            ""iter"": iter,\n            ""lr"": lr,\n            ""mu"": mu,\n            ""clip_thresh"": clip_thresh,\n            ""beta"": beta,\n            ""curv_win_width"": curv_win_width,\n            ""zero_debias"": zero_debias,\n            ""h_min"": h_min,\n            ""h_max"": h_max\n        }\n\n    def load_state_dict(self, state_dict):\n        # for checkpoint saving\n        self._optimizer.load_state_dict(state_dict[\'sgd_state_dict\'])\n        # for recover model internally if any numerical issue happens\n        param_id = 0\n        for group in self._optimizer.param_groups:\n            for p in group[""params""]:\n                p.data.copy_(state_dict[""model_state_list""][param_id])\n                param_id += 1\n        self._global_state = state_dict[\'global_state\']\n        self._lr_factor = state_dict[\'lr_factor\']\n        self._iter = state_dict[\'iter\']\n        self._lr = state_dict[\'lr\']\n        self._mu = state_dict[\'mu\']\n        self._clip_thresh = state_dict[\'clip_thresh\']\n        self._beta = state_dict[\'beta\']\n        self._curv_win_width = state_dict[\'curv_win_width\']\n        self._zero_debias = state_dict[\'zero_debias\']\n        self._h_min = state_dict[""h_min""]\n        self._h_max = state_dict[""h_max""]\n        return\n\n    def load_state_dict_perturb(self, state_dict):\n        # for checkpoint saving\n        self._optimizer.load_state_dict(state_dict[\'sgd_state_dict\'])\n        # for recover model internally if any numerical issue happens\n        param_id = 0\n        for group in self._optimizer.param_groups:\n            for p in group[""params""]:\n                p.data.copy_(state_dict[""model_state_list""][param_id])\n                p.data += 1e-8\n                param_id += 1\n        self._global_state = state_dict[\'global_state\']\n        self._lr_factor = state_dict[\'lr_factor\']\n        self._iter = state_dict[\'iter\']\n        self._lr = state_dict[\'lr\']\n        self._mu = state_dict[\'mu\']\n        self._clip_thresh = state_dict[\'clip_thresh\']\n        self._beta = state_dict[\'beta\']\n        self._curv_win_width = state_dict[\'curv_win_width\']\n        self._zero_debias = state_dict[\'zero_debias\']\n        self._h_min = state_dict[""h_min""]\n        self._h_max = state_dict[""h_max""]\n        return\n\n    def set_lr_factor(self, factor):\n        self._lr_factor = factor\n        return\n\n    def get_lr_factor(self):\n        return self._lr_factor\n\n    def zero_grad(self):\n        self._optimizer.zero_grad()\n        return\n\n    def zero_debias_factor(self):\n        return 1.0 - self._beta ** (self._iter + 1)\n\n    def zero_debias_factor_delay(self, delay):\n        # for exponentially averaged stat which starts at non-zero iter\n        return 1.0 - self._beta ** (self._iter - delay + 1)\n\n    def curvature_range(self):\n        global_state = self._global_state\n        if self._iter == 0:\n            global_state[""curv_win""] = torch.FloatTensor(self._curv_win_width, 1).zero_()\n        curv_win = global_state[""curv_win""]\n        grad_norm_squared = self._global_state[""grad_norm_squared""]\n        # curv_win[self._iter % self._curv_win_width] = np.log(grad_norm_squared + eps)\n        curv_win[self._iter % self._curv_win_width] = grad_norm_squared\n        valid_end = min(self._curv_win_width, self._iter + 1)\n        # we use running average over log scale, accelerating\n        # h_max / min in the begining to follow the varying trend of curvature.\n        beta = self._beta\n        if self._iter == 0:\n            global_state[""h_min_avg""] = 0.0\n            global_state[""h_max_avg""] = 0.0\n            self._h_min = 0.0\n            self._h_max = 0.0\n        if self._h_min_log_smooth:\n            global_state[""h_min_avg""] = \\\n                global_state[""h_min_avg""] * beta + (1 - beta) * torch.min(np.log(curv_win[:valid_end] + eps))\n        else:\n            global_state[""h_min_avg""] = \\\n                global_state[""h_min_avg""] * beta + (1 - beta) * torch.min(curv_win[:valid_end])\n        if self._h_max_log_smooth:\n            global_state[""h_max_avg""] = \\\n                global_state[""h_max_avg""] * beta + (1 - beta) * torch.max(np.log(curv_win[:valid_end] + eps))\n        else:\n            global_state[""h_max_avg""] = \\\n                global_state[""h_max_avg""] * beta + (1 - beta) * torch.max(curv_win[:valid_end])\n        if self._zero_debias:\n            debias_factor = self.zero_debias_factor()\n            if self._h_min_log_smooth:\n                self._h_min = np.exp(global_state[""h_min_avg""] / debias_factor)\n            else:\n                self._h_min = global_state[""h_min_avg""] / debias_factor\n            if self._h_max_log_smooth:\n                self._h_max = np.exp(global_state[""h_max_avg""] / debias_factor)\n            else:\n                self._h_max = global_state[""h_max_avg""] / debias_factor\n        else:\n            if self._h_min_log_smooth:\n                self._h_min = np.exp(global_state[""h_min_avg""])\n            else:\n                self._h_min = global_state[""h_min_avg""]\n            if self._h_max_log_smooth:\n                self._h_max = np.exp(global_state[""h_max_avg""])\n            else:\n                self._h_max = global_state[""h_max_avg""]\n        if self._sparsity_debias:\n            self._h_min *= self._sparsity_avg\n            self._h_max *= self._sparsity_avg\n        return\n\n    def grad_variance(self):\n        global_state = self._global_state\n        beta = self._beta\n        self._grad_var = np.array(0.0, dtype=np.float32)\n        for group_id, group in enumerate(self._optimizer.param_groups):\n            for p_id, p in enumerate(group[\'params\']):\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self._optimizer.state[p]\n                if self._iter == 0:\n                    state[""grad_avg""] = grad.new().resize_as_(grad).zero_()\n                    state[""grad_avg_squared""] = 0.0\n                state[""grad_avg""].mul_(beta).add_(1 - beta, grad)\n                self._grad_var += torch.sum(state[""grad_avg""] * state[""grad_avg""])\n\n        if self._zero_debias:\n            debias_factor = self.zero_debias_factor()\n        else:\n            debias_factor = 1.0\n\n        self._grad_var /= -(debias_factor ** 2)\n        self._grad_var += global_state[\'grad_norm_squared_avg\'] / debias_factor\n        # in case of negative variance: the two term are using different debias factors\n        self._grad_var = max(self._grad_var, eps)\n        if self._sparsity_debias:\n            self._grad_var *= self._sparsity_avg\n        return\n\n    def dist_to_opt(self):\n        global_state = self._global_state\n        beta = self._beta\n        if self._iter == 0:\n            global_state[""grad_norm_avg""] = 0.0\n            global_state[""dist_to_opt_avg""] = 0.0\n        global_state[""grad_norm_avg""] = \\\n            global_state[""grad_norm_avg""] * beta + (1 - beta) * math.sqrt(global_state[""grad_norm_squared""])\n        global_state[""dist_to_opt_avg""] = \\\n            global_state[""dist_to_opt_avg""] * beta \\\n            + (1 - beta) * global_state[""grad_norm_avg""] / (global_state[\'grad_norm_squared_avg\'] + eps)\n        if self._zero_debias:\n            debias_factor = self.zero_debias_factor()\n            self._dist_to_opt = global_state[""dist_to_opt_avg""] / debias_factor\n        else:\n            self._dist_to_opt = global_state[""dist_to_opt_avg""]\n        if self._sparsity_debias:\n            self._dist_to_opt /= (np.sqrt(self._sparsity_avg) + eps)\n        return\n\n    def grad_sparsity(self):\n        global_state = self._global_state\n        if self._iter == 0:\n            global_state[""sparsity_avg""] = 0.0\n        non_zero_cnt = 0.0\n        all_entry_cnt = 0.0\n        for group in self._optimizer.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                grad_non_zero = grad.nonzero()\n                if grad_non_zero.dim() > 0:\n                    non_zero_cnt += grad_non_zero.size()[0]\n                all_entry_cnt += torch.numel(grad)\n        beta = self._beta\n        global_state[""sparsity_avg""] = beta * global_state[""sparsity_avg""] \\\n                                       + (1 - beta) * non_zero_cnt / float(all_entry_cnt)\n        self._sparsity_avg = \\\n            global_state[""sparsity_avg""] / self.zero_debias_factor()\n\n        if self._verbose:\n            logging.debug(""sparsity %f, sparsity avg %f"", non_zero_cnt / float(all_entry_cnt), self._sparsity_avg)\n\n        return\n\n    def lr_grad_norm_avg(self):\n        # this is for enforcing lr * grad_norm not\n        # increasing dramatically in case of instability.\n        #  Not necessary for basic use.\n        global_state = self._global_state\n        beta = self._beta\n        if ""lr_grad_norm_avg"" not in global_state:\n            global_state[\'grad_norm_squared_avg_log\'] = 0.0\n        global_state[\'grad_norm_squared_avg_log\'] = \\\n            global_state[\'grad_norm_squared_avg_log\'] * beta \\\n            + (1 - beta) * np.log(global_state[\'grad_norm_squared\'] + eps)\n        if ""lr_grad_norm_avg"" not in global_state:\n            global_state[""lr_grad_norm_avg""] = \\\n                0.0 * beta + (1 - beta) * np.log(self._lr * np.sqrt(global_state[\'grad_norm_squared\']) + eps)\n            # we monitor the minimal smoothed ||lr * grad||\n            global_state[""lr_grad_norm_avg_min""] = \\\n                np.exp(global_state[""lr_grad_norm_avg""] / self.zero_debias_factor())\n        else:\n            global_state[""lr_grad_norm_avg""] = global_state[""lr_grad_norm_avg""] * beta \\\n                                               + (1 - beta) * np.log(\n                self._lr * np.sqrt(global_state[\'grad_norm_squared\']) + eps)\n            global_state[""lr_grad_norm_avg_min""] = \\\n                min(global_state[""lr_grad_norm_avg_min""],\n                    np.exp(global_state[""lr_grad_norm_avg""] / self.zero_debias_factor()))\n\n    def before_apply(self):\n        # compute running average of gradient and norm of gradient\n        beta = self._beta\n        global_state = self._global_state\n        if self._iter == 0:\n            global_state[""grad_norm_squared_avg""] = 0.0\n\n        global_state[""grad_norm_squared""] = 0.0\n        for group_id, group in enumerate(self._optimizer.param_groups):\n            for p_id, p in enumerate(group[\'params\']):\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                param_grad_norm_squared = torch.sum(grad * grad)\n                global_state[\'grad_norm_squared\'] += param_grad_norm_squared\n\n                if self._verbose:\n                    logging.debug(""Iteration  %f"", self._iter)\n                    logging.debug(""param grad squared gid %d, pid %d, %f, log scale: %f"", group_id, p_id,\n                                  param_grad_norm_squared,\n                                  np.log(param_grad_norm_squared + 1e-10) / np.log(10))\n\n        if self._iter >= 1:\n            self._exploding_grad_clip_thresh = self._h_max\n            self._exploding_grad_clip_target_value = np.sqrt(self._h_max)\n            if global_state[\'grad_norm_squared\'] >= self._exploding_grad_clip_thresh:\n                self._exploding_grad_detected = True\n            else:\n                self._exploding_grad_detected = False\n\n        global_state[\'grad_norm_squared_avg\'] = \\\n            global_state[\'grad_norm_squared_avg\'] * beta + (1 - beta) * global_state[\'grad_norm_squared\']\n\n        if self._verbose:\n            logging.debug(""overall grad norm squared %f, log scale: %f"",\n                          global_state[\'grad_norm_squared\'],\n                          np.log(global_state[\'grad_norm_squared\'] + 1e-10) / np.log(10))\n\n        if self._sparsity_debias:\n            self.grad_sparsity()\n\n        self.curvature_range()\n        self.grad_variance()\n        self.dist_to_opt()\n\n        if self._verbose:\n            logging.debug(""h_max %f "", self._h_max)\n            logging.debug(""h_min %f "", self._h_min)\n            logging.debug(""dist %f "", self._dist_to_opt)\n            logging.debug(""var %f "", self._grad_var)\n\n        if self._iter > 0:\n            self.get_mu()\n            self.get_lr()\n\n            self._lr = beta * self._lr + (1 - beta) * self._lr_t\n            self._mu = beta * self._mu + (1 - beta) * self._mu_t\n\n            if self._verbose:\n                logging.debug(""lr_t %f"", self._lr_t)\n                logging.debug(""mu_t %f"", self._mu_t)\n                logging.debug(""lr %f"", self._lr)\n                logging.debug(""mu %f"", self._mu)\n        return\n\n    def get_lr(self):\n        self._lr_t = (1.0 - math.sqrt(self._mu_t)) ** 2 / (self._h_min + eps)\n        # slow start of lr to prevent huge lr when there is only a few iteration finished\n        self._lr_t = min(self._lr_t, self._lr_t * (self._iter + 1) / float(10.0 * self._curv_win_width))\n        return\n\n    def get_cubic_root(self):\n        # We have the equation x^2 D^2 + (1-x)^4 * C / h_min^2\n        # where x = sqrt(mu).\n        # We substitute x, which is sqrt(mu), with x = y + 1.\n        # It gives y^3 + py = q\n        # where p = (D^2 h_min^2)/(2*C) and q = -p.\n        # We use the Vieta\'s substution to compute the root.\n        # There is only one real solution y (which is in [0, 1] ).\n        # http://mathworld.wolfram.com/VietasSubstitution.html\n        # eps in the numerator is to prevent momentum = 1 in case of zero gradient\n        if np.isnan(self._dist_to_opt) or np.isnan(self._h_min) or np.isnan(self._grad_var) \\\n                or np.isinf(self._dist_to_opt) or np.isinf(self._h_min) or np.isinf(self._grad_var):\n            logging.warning(""Input to cubic solver has invalid nan/inf value!"")\n            raise Exception(""Input to cubic solver has invalid nan/inf value!"")\n\n        p = (self._dist_to_opt + eps) ** 2 * (self._h_min + eps) ** 2 / 2 / (self._grad_var + eps)\n        w3 = (-math.sqrt(p ** 2 + 4.0 / 27.0 * p ** 3) - p) / 2.0\n        w = math.copysign(1.0, w3) * math.pow(math.fabs(w3), 1.0 / 3.0)\n        y = w - p / 3.0 / (w + eps)\n        x = y + 1\n\n        if self._verbose:\n            logging.debug(""p %f, denominator %f"", p, self._grad_var + eps)\n            logging.debug(""w3 %f "", w3)\n            logging.debug(""y %f, denominator %f"", y, w + eps)\n\n        if np.isnan(x) or np.isinf(x):\n            logging.warning(""Output from cubic is invalid nan/inf value!"")\n            raise Exception(""Output from cubic is invalid nan/inf value!"")\n\n        return x\n\n    def get_mu(self):\n        root = self.get_cubic_root()\n        dr = max((self._h_max + eps) / (self._h_min + eps), 1.0 + eps)\n        self._mu_t = max(root ** 2, ((np.sqrt(dr) - 1) / (np.sqrt(dr) + 1)) ** 2)\n        return\n\n    def update_hyper_param(self):\n        for group in self._optimizer.param_groups:\n            group[\'momentum\'] = self._mu_t\n            # group[\'momentum\'] = max(self._mu, self._mu_t)\n            if self._force_non_inc_step == False:\n                group[\'lr\'] = self._lr_t * self._lr_factor\n                # a loose clamping to prevent catastrophically large move. If the move\n                # is too large, we set lr to 0 and only use the momentum to move\n                if self._adapt_clip and (group[\'lr\'] * np.sqrt(\n                        self._global_state[\'grad_norm_squared\']) >= self._catastrophic_move_thresh):\n                    group[\'lr\'] = self._catastrophic_move_thresh / np.sqrt(\n                        self._global_state[\'grad_norm_squared\'] + eps)\n                    if self._verbose:\n                        logging.warning(""clip catastropic move!"")\n            elif self._iter > self._curv_win_width:\n                # force to guarantee lr * grad_norm not increasing dramatically.\n                # Not necessary for basic use. Please refer to the comments\n                # in YFOptimizer.__init__ for more details\n                self.lr_grad_norm_avg()\n                debias_factor = self.zero_debias_factor()\n                group[\'lr\'] = min(self._lr * self._lr_factor,\n                                  2.0 * self._global_state[""lr_grad_norm_avg_min""] \\\n                                  / (np.sqrt(\n                                      np.exp(self._global_state[\'grad_norm_squared_avg_log\'] / debias_factor)) + eps))\n        return\n\n    def auto_clip_thresh(self):\n        # Heuristic to automatically prevent sudden exploding gradient\n        # Not necessary for basic use.\n        return math.sqrt(self._h_max) * self._auto_clip_fac\n\n    def step(self):\n        # add weight decay\n        for group in self._optimizer.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n        if self._clip_thresh != None:\n            torch.nn.utils.clip_grad_norm(self._var_list, self._clip_thresh)\n        elif self._iter != 0 and self._auto_clip_fac != None:\n            # do not clip the first iteration\n            torch.nn.utils.clip_grad_norm(self._var_list, self.auto_clip_thresh())\n\n        # loose threshold for preventing exploding gradients from destroying statistics\n        if self._adapt_clip and (self._iter > 1):\n            torch.nn.utils.clip_grad_norm(self._var_list, np.sqrt(self._stat_protect_fac * self._h_max) + eps)\n\n        try:\n            # before appply\n            self.before_apply()\n\n            # update learning rate and momentum\n            self.update_hyper_param()\n\n            # periodically save model and states\n            if self._iter % self._checkpoint_interval == 0:\n                if self._use_disk_checkpoint and os.path.exists(self._checkpoint_dir):\n                    checkpoint_path = self._checkpoint_dir + ""/"" + self._checkpoint_file\n                    with open(checkpoint_path, ""wb"") as f:\n                        cp.dump(self.state_dict(), f, protocol=2)\n                else:\n                    self._state_checkpoint = copy.deepcopy(self.state_dict())\n\n            # protection from exploding gradient\n            if self._exploding_grad_detected and self._verbose:\n                logging.warning(\n                    ""exploding gradient detected: grad norm detection thresh %f , grad norm %f, grad norm after clip%f"",\n                    np.sqrt(self._exploding_grad_clip_thresh),\n                    np.sqrt(self._global_state[\'grad_norm_squared\']),\n                    self._exploding_grad_clip_target_value)\n            if self._adapt_clip and self._exploding_grad_detected:\n                # print(""exploding gradient detected: grad norm detection thresh "",\n                # np.sqrt(self._exploding_grad_clip_thresh),\n                #   ""grad norm"", np.sqrt(self._global_state[\'grad_norm_squared\'] ),\n                #   ""grad norm after clip "", self._exploding_grad_clip_target_value)\n                torch.nn.utils.clip_grad_norm(self._var_list, self._exploding_grad_clip_target_value + eps)\n\n            self._optimizer.step()\n\n            self._iter += 1\n        except:\n            # load the last checkpoint\n            logging.warning(""Numerical issue triggered restore with backup. Resuming from last checkpoint."")\n            if self._use_disk_checkpoint and os.path.exists(self._checkpoint_dir):\n                checkpoint_path = self._checkpoint_dir + ""/"" + self._checkpoint_file\n                with open(checkpoint_path, ""rb"") as f:\n                    self.load_state_dict_perturb(cp.load(f))\n            else:\n                self.load_state_dict_perturb(copy.deepcopy(self._state_checkpoint))\n\n        return\n'"
datasets/cityscapesscripts/__init__.py,0,b''
modules/_ext/__init__.py,1,"b'from torch.utils.ffi import _wrap_function\nfrom .__ext import lib as _lib, ffi as _ffi\n\n\n__all__ = []\n\n\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
datasets/cityscapesscripts/annotation/__init__.py,0,b''
datasets/cityscapesscripts/annotation/cityscapesLabelTool.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\n#################\n## Import modules\n#################\n\n# pyqt for everything graphical\nfrom PyQt4 import QtGui, QtCore\n# get command line parameters\nimport sys\n# walk directories\nimport glob\n# access to OS functionality\nimport os\n# (de)serialize config file\nimport json\n# call processes\nimport subprocess\n# get the user name\nimport getpass\n# xml parsing\nimport xml.etree.ElementTree as ET\n# copy text to clipboard\ntry:\n    from Tkinter import Tk\nexcept:\n    from tkinter import Tk\n# copy stuff\nimport copy\n\n#################\n## Helper classes\n#################\n\n# annotation helper\nsys.path.append( os.path.normpath( os.path.join( os.path.dirname( __file__ ) , \'..\' , \'helpers\' ) ) )\nfrom annotation import Point, Annotation, CsObject\nfrom labels     import name2label, assureSingleInstanceName\n\n# Helper class that contains the current configuration of the Gui\n# This config is loaded when started and saved when leaving\nclass configuration:\n    # Constructor\n    def __init__(self):\n        # The filename of the image we currently working on\n        self.currentFile = """"\n        # The filename of the labels we currently working on\n        self.currentLabelFile = """"\n        # The filename of the corrections we currently working on\n        self.currentCorrectionFile = """"\n        # The path where the Cityscapes dataset is located\n        self.csPath = """"\n        # The path of the images of the currently loaded city\n        self.city = """"\n        # The name of the currently loaded city\n        self.cityName = """"\n        # The type of the current annotations\n        self.gtType = """"\n        # The split, where the currently loaded city belongs to\n        self.split = """"\n        # The path of the labels. In this folder we expect a folder for each city\n        # Within these city folders we expect the label with a filename matching\n        # the images, except for the extension\n        self.labelPath = """"\n        # The path to store correction markings\n        self.correctionPath = """"\n        # The transparency of the labels over the image\n        self.transp = 0.5\n        # The zoom toggle\n        self.zoom = False\n        # The zoom factor\n        self.zoomFactor = 1.0\n        # The size of the zoom window. Currently there is no setter or getter for that\n        self.zoomSize = 400 #px\n        # The highlight toggle\n        self.highlight = False\n        # The highlight label\n        self.highlightLabelSelection = """"\n        # Screenshot file\n        self.screenshotFilename = ""%i""\n        # Correction mode\n        self.correctionMode = False\n        # Warn before saving that you are overwriting files\n        self.showSaveWarning = True\n\n    # Load from given filename\n    def load(self, filename):\n        if os.path.isfile(filename):\n            with open(filename, \'r\') as f:\n                jsonText = f.read()\n                jsonDict = json.loads(jsonText)\n                for key in jsonDict:\n                    if key in self.__dict__:\n                        self.__dict__[key] = jsonDict[key]\n        self.fixConsistency()\n\n    # Make sure the config is consistent.\n    # Automatically called after loading\n    def fixConsistency(self):\n        if self.currentFile:\n            self.currentFile      = os.path.normpath(self.currentFile)\n        if self.currentLabelFile:\n            self.currentLabelFile = os.path.normpath(self.currentLabelFile)\n        if self.currentCorrectionFile:\n            self.currentCorrectionFile = os.path.normpath(self.currentCorrectionFile)\n        if self.csPath:\n            self.csPath = os.path.normpath(self.csPath)\n            if not os.path.isdir(self.csPath):\n                self.csPath = """"\n        if self.city:\n            self.city = os.path.normpath(self.city)\n            if not os.path.isdir(self.city):\n                self.city = """"\n        if self.labelPath:\n            self.labelPath = os.path.normpath(self.labelPath)\n\n        if self.correctionPath:\n            self.correctionPath = os.path.normpath(self.correctionPath)\n\n        if self.city:\n            self.cityName == os.path.basename(self.city)\n\n        if not os.path.isfile(self.currentFile) or os.path.dirname(self.currentFile) != self.city:\n            self.currentFile = """"\n\n        if not os.path.isfile(self.currentLabelFile)                       or \\\n           not os.path.isdir( os.path.join(self.labelPath,self.cityName) ) or \\\n           os.path.dirname(self.currentLabelFile) != os.path.join(self.labelPath,self.cityName):\n            self.currentLabelFile = """"\n\n        if not os.path.isfile(self.currentCorrectionFile)                       or \\\n           not os.path.isdir( os.path.join(self.correctionPath,self.cityName) ) or \\\n           os.path.dirname(self.currentCorrectionFile) != os.path.join(self.correctionPath,self.cityName):\n            self.currentCorrectionFile = """"\n\n\n    # Save to given filename (using pickle)\n    def save(self, filename):\n        with open(filename, \'w\') as f:\n            f.write(json.dumps(self.__dict__, default=lambda o: o.__dict__, sort_keys=True, indent=4))\n\n\ndef enum(**enums):\n    return type(\'Enum\', (), enums)\n\n\nclass CorrectionBox:\n\n    types = enum(TO_CORRECT=1, TO_REVIEW=2, RESOLVED=3, QUESTION=4)\n\n    def __init__(self, rect=None, annotation=""""):\n        self.type = CorrectionBox.types.TO_CORRECT\n        self.bbox = rect\n        self.annotation = annotation\n        self.selected = False\n\n        return\n\n    def get_colour(self):\n        if self.type == CorrectionBox.types.TO_CORRECT:\n            return QtGui.QColor(255,0,0)\n        elif self.type == CorrectionBox.types.TO_REVIEW:\n            return QtGui.QColor(255,255,0)\n        elif self.type == CorrectionBox.types.RESOLVED:\n            return QtGui.QColor(0,255,0)\n        elif self.type == CorrectionBox.types.QUESTION:\n            return QtGui.QColor(0,0,255)\n\n    def select(self):\n        if not self.selected:\n            self.selected = True\n        return\n\n    def unselect(self):\n        if self.selected:\n            self.selected = False\n        return\n    # Read the information from the given object node in an XML file\n    # The node must have the tag object and contain all expected fields\n    def readFromXMLNode(self, correctionNode):\n        if not correctionNode.tag == \'correction\':\n            return\n\n        typeNode        = correctionNode.find(\'type\')\n        self.type       = int(typeNode.text)\n        annotationNode  = correctionNode.find(\'annotation\')\n        self.annotation = annotationNode.text\n        bboxNode        = correctionNode.find(\'bbox\')\n        x = float(bboxNode.find(\'x\').text)\n        y = float(bboxNode.find(\'y\').text)\n        width  = float(bboxNode.find(\'width\').text)\n        height = float(bboxNode.find(\'height\').text)\n        self.bbox       = QtCore.QRectF(x,y,width,height)\n\n    # Append the information to a node of an XML file\n    # Creates an object node with all children and appends to the given node\n    # Usually the given node is the root\n    def appendToXMLNode(self, node):\n\n        # New object node\n        correctionNode = ET.SubElement(node,\'correction\')\n        correctionNode.tail = ""\\n""\n        correctionNode.text = ""\\n""\n\n        # Name node\n        typeNode = ET.SubElement(correctionNode,\'type\')\n        typeNode.tail = ""\\n""\n        typeNode.text = str(int(self.type))\n\n        # Deleted node\n        annotationNode = ET.SubElement(correctionNode,\'annotation\')\n        annotationNode.tail = ""\\n""\n        annotationNode.text = str(self.annotation)\n\n        # Polygon node\n        bboxNode = ET.SubElement(correctionNode,\'bbox\')\n        bboxNode.text = ""\\n""\n        bboxNode.tail = ""\\n""\n\n        xNode = ET.SubElement(bboxNode,\'x\')\n        xNode.tail = ""\\n""\n        yNode = ET.SubElement(bboxNode,\'y\')\n        yNode.tail = ""\\n""\n        xNode.text = str(int(round(self.bbox.x())))\n        yNode.text = str(int(round(self.bbox.y())))\n        wNode = ET.SubElement(bboxNode,\'width\')\n        wNode.tail = ""\\n""\n        hNode = ET.SubElement(bboxNode,\'height\')\n        hNode.tail = ""\\n""\n        wNode.text = str(int(round(self.bbox.width())))\n        hNode.text = str(int(round(self.bbox.height())))\n\n\n#################\n## Main GUI class\n#################\n\n# The main class which is a QtGui -> Main Window\nclass CityscapesLabelTool(QtGui.QMainWindow):\n\n    #############################\n    ## Construction / Destruction\n    #############################\n\n    # Constructor\n    def __init__(self):\n        # Construct base class\n        super(CityscapesLabelTool, self).__init__()\n\n        # The filename of where the config is saved and loaded\n        configDir = os.path.dirname( __file__ )\n        self.configFile = os.path.join( configDir , ""cityscapesLabelTool.conf"" )\n\n        # This is the configuration.\n        self.config = configuration()\n        self.config.load(self.configFile)\n\n        # for copying text to clipboard\n        self.tk = Tk()\n\n        # Other member variables\n\n        # The width that we actually use to show the image\n        self.w                = 0\n        # The height that we actually use to show the image\n        self.h                = 0\n        # The horizontal offset where we start drawing within the widget\n        self.xoff             = 0\n        # The vertical offset where we start drawing withing the widget\n        self.yoff             = 0\n        # A gap that we  leave around the image as little border\n        self.bordergap        = 20\n        # The scale that was used, ie\n        # self.w = self.scale * self.image.width()\n        # self.h = self.scale * self.image.height()\n        self.scale            = 1.0\n        # Filenames of all images in current city\n        self.images           = []\n        # Image extension\n        self.imageExt          = ""_leftImg8bit.png""\n        # Ground truth extension\n        self.gtExt             = ""{}_polygons.json""\n        # Current image as QImage\n        self.image            = QtGui.QImage()\n        # Index of the current image within the city folder\n        self.idx              = 0\n        # All annotated objects in current image\n        self.annotation       = None\n        # The XML ElementTree representing the corrections for the current image\n        self.correctionXML         = None\n        # A list of changes that we did on the current annotation\n        # Each change is simply a descriptive string\n        self.changes          = []\n        # The current object the mouse points to. It\'s index in self.annotation.objects\n        self.mouseObj         = -1\n        # The currently selected objects. Their index in self.annotation.objects\n        self.selObjs          = []\n        # The objects that are highlighted. List of object instances\n        self.highlightObjs    = []\n        # A label that is selected for highlighting\n        self.highlightObjLabel = None\n        # Texture for highlighting\n        self.highlightTexture = None\n        # The position of the mouse\n        self.mousePos         = None\n        # TODO: NEEDS BETTER EXPLANATION/ORGANISATION\n        self.mousePosOrig     = None\n        # The position of the mouse scaled to label coordinates\n        self.mousePosScaled   = None\n        # If the mouse is outside of the image\n        self.mouseOutsideImage = True\n        # The position of the mouse upon enabling the zoom window\n        self.mousePosOnZoom = None\n        # The button state of the mouse\n        self.mouseButtons     = 0\n        # A list of objects with changed layer\n        self.changedLayer     = []\n        # A list of objects with changed polygon\n        self.changedPolygon   = []\n        # A polygon that is drawn by the user\n        self.drawPoly         = QtGui.QPolygonF()\n        # Treat the polygon as being closed\n        self.drawPolyClosed   = False\n        # A point of this poly that is dragged\n        self.draggedPt        = -1\n        # A list of toolbar actions that need an image\n        self.actImage         = []\n        # A list of toolbar actions that need an image that is not the first\n        self.actImageNotFirst = []\n        # A list of toolbar actions that need an image that is not the last\n        self.actImageNotLast  = []\n        # A list of toolbar actions that need changes\n        self.actChanges       = []\n        # A list of toolbar actions that need a drawn polygon or selected objects\n        self.actPolyOrSelObj  = []\n        # A list of toolbar actions that need a closed drawn polygon\n        self.actClosedPoly    = []\n        # A list of toolbar actions that need selected objects\n        self.actSelObj        = []\n        # A list of toolbar actions that need a single active selected object\n        self.singleActSelObj  = []\n        # Toggle status of auto-doing screenshots\n        self.screenshotToggleState = False\n        # Toggle status of the play icon\n        self.playState        = False\n        # Temporary zero transparency\n        self.transpTempZero   = False\n\n        # Toggle correction mode on and off\n        self.correctAction   = []\n        self.corrections     = []\n        self.selected_correction = -1\n\n        self.in_progress_bbox = None\n        self.in_progress_correction = None\n        self.mousePressEvent = []\n\n        # Default label\n        self.defaultLabel = \'static\'\n        if not self.defaultLabel in name2label:\n            print( \'The {0} label is missing in the internal label definitions.\'.format(self.defaultLabel) )\n            return\n        # Last selected label\n        self.lastLabel = self.defaultLabel\n\n        # Setup the GUI\n        self.initUI()\n\n        # Initially clear stuff\n        self.deselectAllObjects()\n        self.clearPolygon()\n        self.clearChanges()\n\n        # If we already know a city from the saved config -> load it\n        self.loadCity()\n        self.imageChanged()\n\n    # Destructor\n    def __del__(self):\n        self.config.save(self.configFile)\n\n    # Construct everything GUI related. Called by constructor\n    def initUI(self):\n        # Create a toolbar\n        self.toolbar = self.addToolBar(\'Tools\')\n\n        # Add the tool buttons\n        iconDir = os.path.join( os.path.dirname(__file__) , \'icons\' )\n\n        # Loading a new city\n        loadAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'open.png\' )), \'&Tools\', self)\n        loadAction.setShortcuts([\'o\'])\n        self.setTip( loadAction, \'Open city\' )\n        loadAction.triggered.connect( self.selectCity )\n        self.toolbar.addAction(loadAction)\n\n        # Open previous image\n        backAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'back.png\')), \'&Tools\', self)\n        backAction.setShortcut(\'left\')\n        backAction.setStatusTip(\'Previous image\')\n        backAction.triggered.connect( self.prevImage )\n        self.toolbar.addAction(backAction)\n        self.actImageNotFirst.append(backAction)\n\n        # Open next image\n        nextAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'next.png\')), \'&Tools\', self)\n        nextAction.setShortcut(\'right\')\n        self.setTip( nextAction, \'Next image\' )\n        nextAction.triggered.connect( self.nextImage )\n        self.toolbar.addAction(nextAction)\n        self.actImageNotLast.append(nextAction)\n\n        # Play\n        playAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'play.png\')), \'&Tools\', self)\n        playAction.setShortcut(\' \')\n        playAction.setCheckable(True)\n        playAction.setChecked(False)\n        self.setTip( playAction, \'Play all images\' )\n        playAction.triggered.connect( self.playImages )\n        self.toolbar.addAction(playAction)\n        self.actImageNotLast.append(playAction)\n        self.playAction = playAction\n\n        # Select image\n        selImageAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'shuffle.png\' )), \'&Tools\', self)\n        selImageAction.setShortcut(\'i\')\n        self.setTip( selImageAction, \'Select image\' )\n        selImageAction.triggered.connect( self.selectImage )\n        self.toolbar.addAction(selImageAction)\n        self.actImage.append(selImageAction)\n\n        # Save the current image\n        saveAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'save.png\' )), \'&Tools\', self)\n        saveAction.setShortcuts(\'s\')\n        self.setTip( saveAction,\'Save changes\' )\n        saveAction.triggered.connect( self.save )\n        self.toolbar.addAction(saveAction)\n        self.actChanges.append(saveAction)\n\n        # Clear the currently edited polygon\n        clearPolAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'clearpolygon.png\' )), \'&Tools\', self)\n        clearPolAction.setShortcuts([\'q\',\'Esc\'])\n        self.setTip( clearPolAction, \'Clear polygon\' )\n        clearPolAction.triggered.connect( self.clearPolygonAction )\n        self.toolbar.addAction(clearPolAction)\n        self.actPolyOrSelObj.append(clearPolAction)\n\n        # Create new object from drawn polygon\n        newObjAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'newobject.png\' )), \'&Tools\', self)\n        newObjAction.setShortcuts([\'n\'])\n        self.setTip( newObjAction, \'New object\' )\n        newObjAction.triggered.connect( self.newObject )\n        self.toolbar.addAction(newObjAction)\n        self.actClosedPoly.append(newObjAction)\n\n        # Delete the currently selected object\n        deleteObjectAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'deleteobject.png\' )), \'&Tools\', self)\n        deleteObjectAction.setShortcuts([\'d\',\'delete\'])\n        self.setTip( deleteObjectAction, \'Delete object\' )\n        deleteObjectAction.triggered.connect( self.deleteObject )\n        self.toolbar.addAction(deleteObjectAction)\n        self.actSelObj.append(deleteObjectAction)\n\n        # Undo changes in current image, ie. reload labels from file\n        undoAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'undo.png\' )), \'&Tools\', self)\n        undoAction.setShortcuts(\'u\')\n        self.setTip( undoAction,\'Undo all unsaved changes\' )\n        undoAction.triggered.connect( self.undo )\n        self.toolbar.addAction(undoAction)\n        self.actChanges.append(undoAction)\n\n        # Modify the label of a selected object\n        labelAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'modify.png\' )), \'&Tools\', self)\n        labelAction.setShortcuts([\'m\',\'l\'])\n        self.setTip( labelAction, \'Modify label\' )\n        labelAction.triggered.connect( self.modifyLabel )\n        self.toolbar.addAction(labelAction)\n        self.actSelObj.append(labelAction)\n\n        # Move selected object a layer up\n        layerUpAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'layerup.png\' )), \'&Tools\', self)\n        layerUpAction.setShortcuts([\'Up\'])\n        self.setTip( layerUpAction, \'Move object a layer up\' )\n        layerUpAction.triggered.connect( self.layerUp )\n        self.toolbar.addAction(layerUpAction)\n        self.singleActSelObj.append(layerUpAction)\n\n        # Move selected object a layer down\n        layerDownAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'layerdown.png\' )), \'&Tools\', self)\n        layerDownAction.setShortcuts([\'Down\'])\n        self.setTip( layerDownAction, \'Move object a layer down\' )\n        layerDownAction.triggered.connect( self.layerDown )\n        self.toolbar.addAction(layerDownAction)\n        self.singleActSelObj.append(layerDownAction)\n\n        # Enable/disable zoom. Toggle button\n        zoomAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'zoom.png\' )), \'&Tools\', self)\n        zoomAction.setShortcuts([\'z\'])\n        zoomAction.setCheckable(True)\n        zoomAction.setChecked(self.config.zoom)\n        self.setTip( zoomAction, \'Enable/disable permanent zoom\' )\n        zoomAction.toggled.connect( self.zoomToggle )\n        self.toolbar.addAction(zoomAction)\n        self.actImage.append(zoomAction)\n\n        # Highlight objects of a certain class\n        highlightAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'highlight.png\' )), \'&Tools\', self)\n        highlightAction.setShortcuts([\'g\'])\n        highlightAction.setCheckable(True)\n        highlightAction.setChecked(self.config.highlight)\n        self.setTip( highlightAction, \'Enable/disable highlight of certain object class\' )\n        highlightAction.toggled.connect( self.highlightClassToggle )\n        self.toolbar.addAction(highlightAction)\n        self.actImage.append(highlightAction)\n\n        # Decrease transparency\n        minusAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'minus.png\' )), \'&Tools\', self)\n        minusAction.setShortcut(\'-\')\n        self.setTip( minusAction, \'Decrease transparency\' )\n        minusAction.triggered.connect( self.minus )\n        self.toolbar.addAction(minusAction)\n\n        # Increase transparency\n        plusAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'plus.png\' )), \'&Tools\', self)\n        plusAction.setShortcut(\'+\')\n        self.setTip( plusAction, \'Increase transparency\' )\n        plusAction.triggered.connect( self.plus )\n        self.toolbar.addAction(plusAction)\n\n        # Take a screenshot\n        screenshotAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'screenshot.png\' )), \'&Tools\', self)\n        screenshotAction.setShortcut(\'t\')\n        self.setTip( screenshotAction, \'Take a screenshot\' )\n        screenshotAction.triggered.connect( self.screenshot )\n        self.toolbar.addAction(screenshotAction)\n        self.actImage.append(screenshotAction)\n\n        # Take a screenshot in each loaded frame\n        screenshotToggleAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'screenshotToggle.png\' )), \'&Tools\', self)\n        screenshotToggleAction.setShortcut(\'Ctrl+t\')\n        screenshotToggleAction.setCheckable(True)\n        screenshotToggleAction.setChecked(False)\n        self.setTip( screenshotToggleAction, \'Take a screenshot in each loaded frame\' )\n        screenshotToggleAction.toggled.connect( self.screenshotToggle )\n        self.toolbar.addAction(screenshotToggleAction)\n        self.actImage.append(screenshotToggleAction)\n\n        # Display path to current image in message bar\n        displayFilepathAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'filepath.png\' )), \'&Tools\', self)\n        displayFilepathAction.setShortcut(\'f\')\n        self.setTip( displayFilepathAction, \'Show path to current image\' )\n        displayFilepathAction.triggered.connect( self.displayFilepath )\n        self.toolbar.addAction(displayFilepathAction)\n\n        # Open correction mode\n        self.correctAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'checked6.png\' )), \'&Tools\', self)\n        self.correctAction.setShortcut(\'c\')\n        self.correctAction.setCheckable(True)\n        self.correctAction.setChecked(self.config.correctionMode)\n        if self.config.correctionMode:\n            self.correctAction.setIcon(QtGui.QIcon(os.path.join( iconDir , \'checked6_red.png\' )))\n        self.setTip( self.correctAction, \'Toggle correction mode\' )\n        self.correctAction.triggered.connect( self.toggleCorrectionMode )\n        self.toolbar.addAction(self.correctAction)\n\n        # Display help message\n        helpAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'help19.png\' )), \'&Tools\', self)\n        helpAction.setShortcut(\'h\')\n        self.setTip( helpAction, \'Help\' )\n        helpAction.triggered.connect( self.displayHelpMessage )\n        self.toolbar.addAction(helpAction)\n\n        # Close the application\n        exitAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'exit.png\' )), \'&Tools\', self)\n        #exitAction.setShortcuts([\'Esc\'])\n        self.setTip( exitAction, \'Exit\' )\n        exitAction.triggered.connect( self.close )\n        self.toolbar.addAction(exitAction)\n\n        # The default text for the status bar\n        self.defaultStatusbar = \'Ready\'\n        # Create a statusbar. Init with default\n        self.statusBar().showMessage( self.defaultStatusbar )\n\n        # Enable mouse move events\n        self.setMouseTracking(True)\n        self.toolbar.setMouseTracking(True)\n        # Open in full screen\n        screenShape = QtGui.QDesktopWidget().screenGeometry()\n        self.resize(screenShape.width(), screenShape.height())\n        # Set a title\n        self.applicationTitle = \'Cityscapes Label Tool v1.0\'\n        self.setWindowTitle(self.applicationTitle)\n        # And show the application\n        self.show()\n\n    #############################\n    ## Toolbar call-backs\n    #############################\n\n    # The user pressed ""select city""\n    # The purpose of this method is to set these configuration attributes:\n    #   - self.config.city           : path to the folder containing the images to annotate\n    #   - self.config.cityName       : name of this folder, i.e. the city\n    #   - self.config.labelPath      : path to the folder to store the polygons\n    #   - self.config.correctionPath : path to store the correction boxes in\n    #   - self.config.gtType         : type of ground truth, e.g. gtFine or gtCoarse\n    #   - self.config.split          : type of split, e.g. train, val, test\n    # The current implementation uses the environment variable \'CITYSCAPES_DATASET\'\n    # to determine the dataset root folder and search available data within.\n    # Annotation types are required to start with \'gt\', e.g. gtFine or gtCoarse.\n    # To add your own annotations you could create a folder gtCustom with similar structure.\n    #\n    # However, this implementation could be easily changed to a completely different folder structure.\n    # Just make sure to specify all three paths and a descriptive name as \'cityName\'.\n    # The gtType and split can be left empty.\n    def selectCity(self):\n        # Reset the status bar to this message when leaving\n        restoreMessage = self.statusBar().currentMessage()\n\n        csPath = self.config.csPath\n        if not csPath or not os.path.isdir(csPath):\n            if \'CITYSCAPES_DATASET\' in os.environ:\n                csPath = os.environ[\'CITYSCAPES_DATASET\']\n            else:\n                csPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),\'..\',\'..\')\n\n        availableCities = []\n        annotations = sorted( glob.glob( os.path.join( csPath , \'gt*\' ) ) )\n        annotations = [ os.path.basename(a) for a in annotations ]\n        splits      = [ ""train_extra"" , ""train""  , ""val"" , ""test"" ]\n        for gt in annotations:\n            for split in splits:\n                cities = glob.glob(os.path.join(csPath, gt, split, \'*\'))\n                cities.sort()\n                availableCities.extend( [ (split,gt,os.path.basename(c)) for c in cities if os.path.isdir(c)] )\n\n        # List of possible labels\n        items = [split + "", "" + gt + "", "" + city for (split,gt,city) in availableCities]\n        # default\n        previousItem = self.config.split + "", "" + self.config.gtType + "", "" + self.config.cityName\n        default = 0\n        if previousItem in items:\n            default = items.index(previousItem)\n\n        # Specify title\n        dlgTitle = ""Select city""\n        message  = dlgTitle\n        question = dlgTitle\n        message  = ""Select city for editing""\n        question = ""Which city would you like to edit?""\n        self.statusBar().showMessage(message)\n\n        if items:\n\n            # Create and wait for dialog\n            (item, ok) = QtGui.QInputDialog.getItem(self, dlgTitle, question, items, default, False)\n\n            # Restore message\n            self.statusBar().showMessage( restoreMessage )\n\n            if ok and item:\n                (split,gt,city) = [ str(i) for i in item.split(\', \') ]\n                self.config.city     = os.path.normpath( os.path.join( csPath, ""leftImg8bit"" , split , city ) )\n                self.config.cityName = city\n\n                self.config.labelPath      = os.path.normpath( os.path.join( csPath, gt               , split , city ) )\n                self.config.correctionPath = os.path.normpath( os.path.join( csPath, gt+\'_corrections\', split , city ) )\n\n                self.config.gtType = gt\n                self.config.split  = split\n\n                self.deselectAllObjects()\n                self.clearPolygon()\n                self.loadCity()\n                self.imageChanged()\n\n        else:\n\n            warning = """"\n            warning += ""The data was not found. Please:\\n\\n""\n            warning += "" - make sure the scripts folder is in the Cityscapes root folder\\n""\n            warning += ""or\\n""\n            warning += "" - set CITYSCAPES_DATASET to the Cityscapes root folder\\n""\n            warning += ""       e.g. \'export CITYSCAPES_DATASET=<root_path>\'\\n""\n\n            reply = QtGui.QMessageBox.information(self, ""ERROR!"", warning, QtGui.QMessageBox.Ok)\n            if reply == QtGui.QMessageBox.Ok:\n                sys.exit()\n\n        return\n\n    # Switch to previous image in file list\n    # Load the image\n    # Load its labels\n    # Update the mouse selection\n    # View\n    def prevImage(self):\n        if not self.images:\n            return\n        if self.idx > 0:\n            if self.checkAndSave():\n                self.idx -= 1\n                self.imageChanged()\n        return\n\n    # Switch to next image in file list\n    # Load the image\n    # Load its labels\n    # Update the mouse selection\n    # View\n    def nextImage(self):\n        if not self.images:\n            return\n        if self.idx < len(self.images)-1:\n            if self.checkAndSave():\n                self.idx += 1\n                self.imageChanged()\n        elif self.playState:\n            self.playState = False\n            self.playAction.setChecked(False)\n\n        if self.playState:\n            QtCore.QTimer.singleShot(0, self.nextImage)\n        return\n\n    # Play images, i.e. auto-switch to next image\n    def playImages(self, status):\n        self.playState = status\n        if self.playState:\n            QtCore.QTimer.singleShot(0, self.nextImage)\n\n\n    # switch correction mode on and off\n    def toggleCorrectionMode(self):\n        if not self.config.correctionMode:\n            self.config.correctionMode = True\n            iconDir = os.path.join( os.path.dirname(sys.argv[0]) , \'icons\' )\n            self.correctAction.setIcon(QtGui.QIcon(os.path.join( iconDir , \'checked6_red.png\' )))\n        else:\n            self.config.correctionMode = False\n            iconDir = os.path.join( os.path.dirname(sys.argv[0]) , \'icons\' )\n            self.correctAction.setIcon(QtGui.QIcon(os.path.join( iconDir , \'checked6.png\' )))\n        self.update()\n        return\n\n\n    # Switch to a selected image of the file list\n    # Ask the user for an image\n    # Load the image\n    # Load its labels\n    # Update the mouse selection\n    # View\n    def selectImage(self):\n        if not self.images:\n            return\n\n        dlgTitle = ""Select image to load""\n        self.statusBar().showMessage(dlgTitle)\n        items = QtCore.QStringList( [ ""{}: {}"".format(num,os.path.basename(i)) for (num,i) in enumerate(self.images) ] )\n        (item, ok) = QtGui.QInputDialog.getItem(self, dlgTitle, ""Image"", items, self.idx, False)\n        if (ok and item):\n            idx = items.indexOf(item)\n            if idx != self.idx and self.checkAndSave():\n                self.idx = idx\n                self.imageChanged()\n        else:\n            # Restore the message\n            self.statusBar().showMessage( self.defaultStatusbar )\n\n\n    # Save labels\n    def save(self):\n        # Status\n        saved = False\n        # Message to show at the status bar when done\n        message = """"\n        # Only save if there are changes, labels, an image filename and an image\n        if self.changes and (self.annotation or self.corrections) and self.config.currentFile and self.image:\n            if self.annotation:\n                # set image dimensions\n                self.annotation.imgWidth = self.image.width()\n                self.annotation.imgHeight = self.image.height()\n\n                # Determine the filename\n                # If we have a loaded label file, then this is also the filename\n                filename = self.config.currentLabelFile\n                # If not, then generate one\n                if not filename:\n                    filename = self.getLabelFilename(True)\n\n                if filename:\n                    proceed = True\n                    # warn user that he is overwriting an old file\n                    if os.path.isfile(filename) and self.config.showSaveWarning:\n                        msgBox = QtGui.QMessageBox(self)\n                        msgBox.setWindowTitle(""Overwriting"")\n                        msgBox.setText(""Saving overwrites the original file and it cannot be reversed. Do you want to continue?"")\n                        msgBox.addButton(QtGui.QMessageBox.Cancel)\n                        okAndNeverAgainButton = msgBox.addButton(\'OK and never ask again\',QtGui.QMessageBox.AcceptRole)\n                        okButton = msgBox.addButton(QtGui.QMessageBox.Ok)\n                        msgBox.setDefaultButton(QtGui.QMessageBox.Ok)\n                        msgBox.setIcon(QtGui.QMessageBox.Warning)\n                        msgBox.exec_()\n\n                        # User clicked on ""OK""\n                        if msgBox.clickedButton() == okButton:\n                            pass\n                        # User clicked on ""OK and never ask again""\n                        elif msgBox.clickedButton() == okAndNeverAgainButton:\n                            self.config.showSaveWarning = False\n                        else:\n                            # Do nothing\n                            message += ""Nothing saved, no harm has been done. ""\n                            proceed = False\n\n                    # Save JSON file\n                    if proceed:\n                        try:\n                            self.annotation.toJsonFile(filename)\n                            saved = True\n                            message += ""Saved labels to {0} "".format(filename)\n                        except IOError as e:\n                            message += ""Error writing labels to {0}. Message: {1} "".format( filename, e.strerror )\n\n                else:\n                    message += ""Error writing labels. Cannot generate a valid filename. ""\n            if self.corrections or self.config.currentCorrectionFile:\n                # Determine the filename\n                # If we have a loaded label file, then this is also the filename\n                filename = self.config.currentCorrectionFile\n                # If not, then generate one\n                if not filename:\n                    filename = self.getCorrectionFilename(True)\n\n                if filename:\n                    # Prepare the root\n                    root = ET.Element(\'correction\')\n                    root.text = ""\\n""\n                    root.tail = ""\\n""\n                    # Add the filename of the image that is annotated\n                    filenameNode = ET.SubElement(root,\'filename\')\n                    filenameNode.text = os.path.basename(self.config.currentFile)\n                    filenameNode.tail = ""\\n""\n                    # Add the folder where this image is located in\n                    # For compatibility with the LabelMe Tool, we need to use the folder\n                    # StereoDataset/cityName\n                    folderNode = ET.SubElement(root,\'folder\')\n                    folderNode.text = ""StereoDataset/"" + self.config.cityName\n                    folderNode.tail = ""\\n""\n                    # The name of the tool. Here, we do not follow the output of the LabelMe tool,\n                    # since this is crap anyway\n                    sourceNode = ET.SubElement(root,\'source\')\n                    sourceNode.text = ""\\n""\n                    sourceNode.tail = ""\\n""\n                    sourceImageNode = ET.SubElement(sourceNode,\'sourceImage\')\n                    sourceImageNode.text = ""Label Cities""\n                    sourceImageNode.tail = ""\\n""\n                    sourceAnnotationNode = ET.SubElement(sourceNode,\'sourceAnnotation\')\n                    sourceAnnotationNode.text = ""mcLabelTool""\n                    sourceAnnotationNode.tail = ""\\n""\n                    # The image size\n                    imagesizeNode = ET.SubElement(root,\'imagesize\')\n                    imagesizeNode.text = ""\\n""\n                    imagesizeNode.tail = ""\\n""\n                    nrowsNode = ET.SubElement(imagesizeNode,\'nrows\')\n                    nrowsNode.text = str(self.image.height())\n                    nrowsNode.tail = ""\\n""\n                    ncolsNode = ET.SubElement(imagesizeNode,\'ncols\')\n                    ncolsNode.text = str(self.image.height())\n                    ncolsNode.tail = ""\\n""\n                    # Add all objects\n                    for correction in self.corrections:\n                        correction.appendToXMLNode(root)\n\n                    # Create the actual XML tree\n                    self.correctionXML = ET.ElementTree(root)\n\n                    # Save XML file\n                    try:\n                        self.correctionXML.write(filename)\n                        saved = True\n                        message += ""Saved corrections to {0} "".format(filename)\n                    except IOError as e:\n                        message += ""Error writing corrections to {0}. Message: {1} "".format( filename, e.strerror )\n                else:\n                    message += ""Error writing corrections. Cannot generate a valid filename. ""\n            # Clear changes\n            if saved:\n                self.clearChanges()\n        else:\n            message += ""Nothing to save ""\n            saved   = True\n\n        # Show the status message\n        self.statusBar().showMessage(message)\n\n        return saved\n\n    # Undo changes, ie. reload labels\n    def undo(self):\n        # check if we really want to do this in case there are multiple changes\n        if len( self.changes ) > 1:\n            # Backup of status message\n            restoreMessage = self.statusBar().currentMessage()\n            # Create the dialog\n            dlgTitle = ""Undo changes?""\n            self.statusBar().showMessage(dlgTitle)\n            text = ""Do you want to undo the following changes?\\n""\n            for c in self.changes:\n                text += ""- "" + c + \'\\n\'\n            buttons = QtGui.QMessageBox.Ok | QtGui.QMessageBox.Cancel\n            ret = QtGui.QMessageBox.question(self, dlgTitle, text, buttons, QtGui.QMessageBox.Ok )\n            proceed = False\n            # If the user selected yes -> undo\n            if ret == QtGui.QMessageBox.Ok:\n                proceed = True\n            self.statusBar().showMessage( restoreMessage )\n\n            # If we do not proceed -> return\n            if not proceed:\n                return\n\n        # Clear labels to force a reload\n        self.annotation = None\n        # Reload\n        self.imageChanged()\n\n    # Clear the drawn polygon and update\n    def clearPolygonAction(self):\n        self.deselectAllObjects()\n        self.clearPolygon()\n        self.update()\n\n    # Create a new object from the current polygon\n    def newObject(self):\n        # Default label\n        label = self.lastLabel\n\n        # Ask the user for a label\n        (label, ok) = self.getLabelFromUser( label )\n\n        if ok and label:\n            # Append and create the new object\n            self.appendObject( label , self.drawPoly )\n\n            # Clear the drawn polygon\n            self.deselectAllObjects()\n            self.clearPolygon()\n\n            # Default message\n            self.statusBar().showMessage( self.defaultStatusbar )\n\n            # Set as default label for next time\n            self.lastLabel = label\n\n        # Redraw\n        self.update()\n\n    # Delete the currently selected object\n    def deleteObject(self):\n        # Cannot do anything without a selected object\n        if not self.selObjs:\n            return\n        # Cannot do anything without labels\n        if not self.annotation:\n            return\n\n        for selObj in self.selObjs:\n            # The selected object that is deleted\n            obj = self.annotation.objects[selObj]\n\n        # Delete\n        obj.delete()\n\n        # Save changes\n        self.addChange( ""Deleted object {0} with label {1}"".format( obj.id, obj.label ) )\n\n        # Clear polygon\n        self.deselectAllObjects()\n        self.clearPolygon()\n\n        # Redraw\n        self.update()\n\n    # Modify the label of a selected object\n    def modifyLabel(self):\n        # Cannot do anything without labels\n        if not self.annotation:\n            return\n        # Cannot do anything without a selected object\n        if not self.selObjs:\n            return\n\n        # The last selected object\n        obj = self.annotation.objects[self.selObjs[-1]]\n        # default label\n        defaultLabel = obj.label\n        defaultId    = -1\n        # If there is only one object the dialog text can be improved\n        if len(self.selObjs) == 1:\n            defaultId = obj.id\n\n        (label, ok) = self.getLabelFromUser( defaultLabel , defaultId )\n\n        if ok and label:\n            for selObj in self.selObjs:\n                # The selected object that is modified\n                obj = self.annotation.objects[selObj]\n\n                # Save changes\n                if obj.label != label:\n                    self.addChange( ""Set label {0} for object {1} with previous label {2}"".format( label, obj.id, obj.label ) )\n                    obj.label = label\n                    obj.updateDate()\n\n        # Update\n        self.update()\n\n    # Move object a layer up\n    def layerUp(self):\n        # Change layer\n        self.modifyLayer(+1)\n        # Update\n        self.update()\n\n    # Move object a layer down\n    def layerDown(self):\n        # Change layer\n        self.modifyLayer(-1)\n        # Update\n        self.update()\n\n    # Toggle zoom\n    def zoomToggle(self, status):\n        self.config.zoom = status\n        if status :\n            self.mousePosOnZoom = self.mousePos\n        self.update()\n\n    # Toggle highlight\n    def highlightClassToggle(self, status):\n        if status :\n            defaultLabel = """"\n            if self.config.highlightLabelSelection and self.config.highlightLabelSelection in name2label:\n                defaultLabel = self.config.highlightLabelSelection\n            (label, ok) = self.getLabelFromUser( defaultLabel )\n\n            if ok and label:\n                self.config.highlightLabelSelection = label\n            else:\n                status = False\n\n        self.config.highlight = status\n        self.update()\n\n    # Increase label transparency\n    def minus(self):\n        self.config.transp = max(self.config.transp-0.1,0.0)\n        self.update()\n\n\n    def displayFilepath(self):\n        self.statusBar().showMessage(""Current image: {0}"".format( self.config.currentFile ))\n        self.update()\n\n\n    # Decrease label transparency\n    def plus(self):\n        self.config.transp = min(self.config.transp+0.1,1.0)\n        self.update()\n\n    # Take a screenshot\n    def screenshot(self):\n        # Get a filename for saving\n        dlgTitle = ""Get screenshot filename""\n        filter = ""Images (*.png *.xpm *.jpg)""\n        answer = QtGui.QFileDialog.getSaveFileName(self, dlgTitle, self.config.screenshotFilename,filter, options=QtGui.QFileDialog.DontUseNativeDialog)\n        if answer:\n            self.config.screenshotFilename = str(answer)\n        else:\n            return\n\n        # Actually make the screenshot\n        self.doScreenshot()\n\n    # Toggle auto-making of screenshots\n    def screenshotToggle(self, status):\n        self.screenshotToggleState = status\n        if status:\n            self.screenshot()\n\n    def displayHelpMessage(self):\n\n        message = self.applicationTitle + ""\\n\\n""\n        message += ""INSTRUCTIONS\\n""\n        message += "" - press open (left button) to select a city from drop-down menu\\n""\n        message += "" - browse images and edit labels using\\n""\n        message += ""   the toolbar buttons (check tooltips) and the controls below\\n""\n        message += "" - note that the editing happens in-place;\\n""\n        message += ""   if you want to annotate your own images or edit a custom\\n""\n        message += ""   set of labels, check (and modify) the code of the method \'loadCity\'\\n""\n        message += "" - note that this tool modifys the JSON polygon files, but\\n""\n        message += ""   does not create or update the pngs; for the latter use\\n""\n        message += ""   the preparation tools that come with this tool box.\\n""\n        message += ""\\n""\n        message += ""CONTROLS\\n""\n        message += "" - highlight objects [move mouse]\\n""\n        message += "" - draw new polygon\\n""\n        message += ""     - start drawing a polygon [left click]\\n""\n        message += ""     - add point to open polygon [left click]\\n""\n        message += ""     - delete last added point [Backspace]\\n""\n        message += ""     - close polygon [left click on first point]\\n""\n        message += "" - select closed polygon, existing object [Ctrl + left click]\\n""\n        message += ""     - move point [left click and hold on point, move mouse]\\n""\n        message += ""     - add point [click on edge]\\n""\n        message += ""     - delete point from polygon [Shift + left click on point]\\n""\n        message += ""     - deselect polygon [Q]\\n""\n        message += ""     - select multiple polygons [Ctrl + left click]\\n""\n        message += "" - intersect/merge two polygons: draw new polygon, then\\n""\n        message += ""     - intersect [Shift + left click on existing polygon]\\n""\n        message += ""     - merge [Alt + left click on existing polygon]\\n""\n        message += "" - open zoom window [Z or hold down right mouse button]\\n""\n        message += ""     - zoom in/out [mousewheel]\\n""\n        message += ""     - enlarge/shrink zoom window [shift+mousewheel]\\n""\n        message += "" - start correction mode [C]\\n""\n        message += ""     - draw a correction box [left click and hold, move, release]\\n""\n        message += ""     - set box type [1,2,3,4]\\n""\n        message += ""     - previous/next box [E,R]\\n""\n        message += ""     - delete box [D]\\n""\n        message += ""     - modify text, use ascii only [M]\\n""\n\n        QtGui.QMessageBox.about(self, ""HELP!"", message)\n        self.update()\n\n    # Close the application\n    def closeEvent(self,event):\n        if self.checkAndSave():\n            event.accept()\n        else:\n            event.ignore()\n\n\n    #############################\n    ## Custom events\n    #############################\n\n    def imageChanged(self):\n        # Clear corrections\n        self.corrections = []\n        self.selected_correction = -1\n        # Clear the polygon\n        self.deselectAllObjects()\n        self.clearPolygon()\n        # Load the first image\n        self.loadImage()\n        # Load its labels if available\n        self.loadLabels()\n        # Load its corrections if available\n        self.loadCorrections()\n        # Update the object the mouse points to\n        self.updateMouseObject()\n        # Update the GUI\n        self.update()\n        # Save screenshot if set\n        if self.screenshotToggleState:\n            self.doScreenshot()\n\n    #############################\n    ## File I/O\n    #############################\n\n    # Load the currently selected city if possible\n    def loadCity(self):\n        # Search for all *.pngs to get the image list\n        self.images = []\n        if os.path.isdir(self.config.city):\n            self.images = glob.glob( os.path.join( self.config.city , \'*\' + self.imageExt ) )\n            self.images.sort()\n            if self.config.currentFile in self.images:\n                self.idx = self.images.index(self.config.currentFile)\n            else:\n                self.idx = 0\n\n    # Load the currently selected image\n    # Does only load if not previously loaded\n    # Does not refresh the GUI\n    def loadImage(self):\n        success = False\n        message = self.defaultStatusbar\n        if self.images:\n            filename = self.images[self.idx]\n            filename = os.path.normpath( filename )\n            if not self.image.isNull() and filename == self.config.currentFile:\n                success = True\n            else:\n                self.image = QtGui.QImage(filename)\n                if self.image.isNull():\n                    message = ""Failed to read image: {0}"".format( filename )\n                else:\n                    message = ""Read image: {0}"".format( filename )\n                    self.config.currentFile = filename\n                    success = True\n\n        # Update toolbar actions that need an image\n        for act in self.actImage:\n            act.setEnabled(success)\n        for act in self.actImageNotFirst:\n            act.setEnabled(success and self.idx > 0)\n        for act in self.actImageNotLast:\n            act.setEnabled(success and self.idx < len(self.images)-1)\n\n        self.statusBar().showMessage(message)\n\n    # Load the labels from file\n    # Only loads if they exist\n    # Otherwise the filename is stored and that\'s it\n    def loadLabels(self):\n        filename = self.getLabelFilename()\n        if not filename or not os.path.isfile(filename):\n            self.clearAnnotation()\n            return\n\n        # If we have everything and the filename did not change, then we are good\n        if self.annotation and filename == self.currentLabelFile:\n            return\n\n        # Clear the current labels first\n        self.clearAnnotation()\n\n        try:\n            self.annotation = Annotation()\n            self.annotation.fromJsonFile(filename)\n        except IOError as e:\n            # This is the error if the file does not exist\n            message = ""Error parsing labels in {0}. Message: {1}"".format( filename, e.strerror )\n            self.statusBar().showMessage(message)\n\n        # Remember the filename loaded\n        self.currentLabelFile = filename\n\n        # Remeber the status bar message to restore it later\n        restoreMessage = self.statusBar().currentMessage()\n\n        # Restore the message\n        self.statusBar().showMessage( restoreMessage )\n\n    # Load the labels from file\n    # Only loads if they exist\n    # Otherwise the filename is stored and that\'s it\n    def loadCorrections(self): #TODO\n        filename = self.getCorrectionFilename()\n        if not filename:\n            self.clearCorrections()\n            return\n\n        # If we have everything and the filename did not change, then we are good\n        if self.correctionXML and self.corrections and filename == self.config.currentCorrectionFile:\n            return\n\n        # Clear the current labels first\n        self.clearCorrections()\n\n        # We do not always expect to have corrections, therefore prevent a failure due to missing file\n        if not os.path.isfile(filename):\n            return\n\n        try:\n            # Try to parse the XML file\n            self.correctionXML = ET.parse( filename )\n        except IOError as e:\n            # This is the error if the file does not exist\n            message = ""Error parsing corrections in {0}. Message: {1}"".format( filename, e.strerror )\n            self.statusBar().showMessage(message)\n            self.correctionXML = []\n            return\n        except ET.ParseError as e:\n            # This is the error if the content is no valid XML\n            message = ""Error parsing corrections in {0}. Message: {1}"".format( filename, e )\n            self.statusBar().showMessage(message)\n            self.correctionXML = []\n            return\n\n        # Remember the filename loaded\n        self.config.currentCorrectionFile = filename\n\n        # Remeber the status bar message to restore it later\n        restoreMessage = self.statusBar().currentMessage()\n\n        # Iterate through all objects in the XML\n        root = self.correctionXML.getroot()\n        for i, objNode in enumerate(root.findall(\'correction\')):\n            # Instantate a new object and read the XML node\n            obj = CorrectionBox()\n            obj.readFromXMLNode( objNode )\n            if i == 0:\n                self.selected_correction = 0\n                obj.select()\n\n            # Append the object to our list of labels\n            self.corrections.append(obj)\n\n        # Restore the message\n        self.statusBar().showMessage( restoreMessage )\n\n\n    def modify_correction_type(self, correction_type):\n        if self.selected_correction >= 0:\n           self.corrections[self.selected_correction].type = correction_type\n           self.addChange( ""Modified correction type."")\n           self.update()\n        return\n\n\n    def delete_selected_annotation(self):\n        if self.selected_correction >= 0 and self.config.correctionMode:\n           del self.corrections[self.selected_correction]\n           if self.selected_correction == len(self.corrections):\n               self.selected_correction = self.selected_correction - 1\n           if self.selected_correction >= 0:\n               self.corrections[self.selected_correction].select()\n           self.addChange( ""Deleted correction."")\n           self.update()\n        return\n\n    def modify_correction_description(self):\n        if self.selected_correction >= 0 and self.config.correctionMode:\n            description = QtGui.QInputDialog.getText(self, ""Modify Error Description"", ""Please describe the labeling error briefly."",\n                                                     text = self.corrections[self.selected_correction].annotation)\n            if description[1]:\n                self.corrections[self.selected_correction].annotation = description[0]\n                self.addChange( ""Changed correction description."")\n                self.update()\n        return\n\n    def select_next_correction(self):\n\n        if self.selected_correction >= 0:\n            self.corrections[self.selected_correction].unselect()\n            if self.selected_correction == (len(self.corrections) - 1) :\n                self.selected_correction = 0\n            else :\n                self.selected_correction = self.selected_correction + 1\n            self.corrections[self.selected_correction].select()\n            self.update()\n\n        return\n\n    def select_previous_correction(self):\n\n        if self.selected_correction >= 0 :\n            self.corrections[self.selected_correction].unselect()\n            if self.selected_correction == 0 :\n                self.selected_correction = (len(self.corrections) - 1)\n            else :\n                self.selected_correction = self.selected_correction - 1\n            self.corrections[self.selected_correction].select()\n            self.update()\n\n        return\n\n\n    #############################\n    ## Drawing\n    #############################\n\n    # This method is called when redrawing everything\n    # Can be manually triggered by self.update()\n    # Note that there must not be any other self.update within this method\n    # or any methods that are called within\n    def paintEvent(self, event):\n        # Create a QPainter that can perform draw actions within a widget or image\n        qp = QtGui.QPainter()\n        # Begin drawing in the application widget\n        qp.begin(self)\n        # Update scale\n        self.updateScale(qp)\n        # Determine the object ID to highlight\n        self.getHighlightedObject(qp)\n        # Draw the image first\n        self.drawImage(qp)\n        # Draw the labels on top\n        overlay = self.drawLabels(qp)\n        # Draw the user drawn polygon\n        self.drawDrawPoly(qp)\n        self.drawDrawRect(qp)\n        # Draw the label name next to the mouse\n        self.drawLabelAtMouse(qp)\n        # Draw the zoom\n        # self.drawZoom(qp, overlay)\n        self.drawZoom(qp,None)\n\n        # Thats all drawing\n        qp.end()\n\n        # Forward the paint event\n        QtGui.QMainWindow.paintEvent(self,event)\n\n    # Update the scaling\n    def updateScale(self, qp):\n        if not self.image.width() or not self.image.height():\n            return\n        # Horizontal offset\n        self.xoff  = self.bordergap\n        # Vertical offset\n        self.yoff  = self.toolbar.height()+self.bordergap\n        # We want to make sure to keep the image aspect ratio and to make it fit within the widget\n        # Without keeping the aspect ratio, each side of the image is scaled (multiplied) with\n        sx = float(qp.device().width()  - 2*self.xoff) / self.image.width()\n        sy = float(qp.device().height() - 2*self.yoff) / self.image.height()\n        # To keep the aspect ratio while making sure it fits, we use the minimum of both scales\n        # Remember the scale for later\n        self.scale = min( sx , sy )\n        # These are then the actual dimensions used\n        self.w     = self.scale * self.image.width()\n        self.h     = self.scale * self.image.height()\n\n    # Determine the highlighted object for drawing\n    def getHighlightedObject(self, qp):\n        # These variables we want to fill\n        self.highlightObjs = []\n        self.highlightObjLabel = None\n\n        # Without labels we cannot do so\n        if not self.annotation:\n            return\n\n        # If available set the selected objects\n        highlightObjIds = self.selObjs\n        # If not available but the polygon is empty or closed, its the mouse object\n        if not highlightObjIds and (self.drawPoly.isEmpty() or self.drawPolyClosed) and self.mouseObj>=0 and not self.mouseOutsideImage:\n            highlightObjIds = [self.mouseObj]\n        # Get the actual object that is highlighted\n        if highlightObjIds:\n            self.highlightObjs = [ self.annotation.objects[i] for i in highlightObjIds ]\n        # Set the highlight object label if appropriate\n        if self.config.highlight:\n            self.highlightObjLabel = self.config.highlightLabelSelection\n        elif len(highlightObjIds) == 1 and self.config.correctionMode:\n            self.highlightObjLabel = self.annotation.objects[highlightObjIds[-1]].label\n\n    # Draw the image in the given QPainter qp\n    def drawImage(self, qp):\n        # Return if no image available\n        if self.image.isNull():\n            return\n\n        # Save the painters current setting to a stack\n        qp.save()\n        # Draw the image\n        qp.drawImage(QtCore.QRect( self.xoff, self.yoff, self.w, self.h ), self.image)\n        # Restore the saved setting from the stack\n        qp.restore()\n\n    def getPolygon(self, obj):\n        poly = QtGui.QPolygonF()\n        for pt in obj.polygon:\n            point = QtCore.QPointF(pt.x,pt.y)\n            poly.append( point )\n        return poly\n\n    # Draw the labels in the given QPainter qp\n    # optionally provide a list of labels to ignore\n    def drawLabels(self, qp, ignore = []):\n        if self.image.isNull() or self.w <= 0 or self.h <= 0:\n            return\n        if not self.annotation:\n            return\n        if self.transpTempZero:\n            return\n\n        # The overlay is created in the viewing coordinates\n        # This way, the drawing is more dense and the polygon edges are nicer\n        # We create an image that is the overlay\n        # Within this image we draw using another QPainter\n        # Finally we use the real QPainter to overlay the overlay-image on what is drawn so far\n\n        # The image that is used to draw the overlays\n        overlay = QtGui.QImage( self.w, self.h, QtGui.QImage.Format_ARGB32_Premultiplied )\n        # Fill the image with the default color\n        defaultLabel = name2label[self.defaultLabel]\n        col = QtGui.QColor( *defaultLabel.color )\n        overlay.fill( col )\n        # Create a new QPainter that draws in the overlay image\n        qp2 = QtGui.QPainter()\n        qp2.begin(overlay)\n\n        # The color of the outlines\n        qp2.setPen(QtGui.QColor(\'white\'))\n        # Draw all objects\n        for obj in self.annotation.objects:\n            # Some are flagged to not be drawn. Skip them\n            if not obj.draw:\n                continue\n\n            # The label of the object\n            name      = assureSingleInstanceName( obj.label )\n            # If we do not know a color for this label, warn the user\n            if not name in name2label:\n                print( ""The annotations contain unkown labels. This should not happen. Please inform the datasets authors. Thank you!"" )\n                print( ""Details: label \'{}\', file \'{}\'"".format(name,self.currentLabelFile) )\n                continue\n\n            # If we ignore this label, skip\n            if name in ignore:\n                continue\n\n            poly = self.getPolygon(obj)\n\n            # Scale the polygon properly\n            polyToDraw = poly * QtGui.QTransform.fromScale(self.scale,self.scale)\n\n            # Default drawing\n            # Color from color table, solid brush\n            col   = QtGui.QColor( *name2label[name].color     )\n            brush = QtGui.QBrush( col, QtCore.Qt.SolidPattern )\n            qp2.setBrush(brush)\n            # Overwrite drawing if this is the highlighted object\n            if ( obj in self.highlightObjs or name == self.highlightObjLabel ):\n                # First clear everything below of the polygon\n                qp2.setCompositionMode( QtGui.QPainter.CompositionMode_Clear )\n                qp2.drawPolygon( polyToDraw )\n                qp2.setCompositionMode( QtGui.QPainter.CompositionMode_SourceOver )\n                # Set the drawing to a special pattern\n                brush = QtGui.QBrush(col,QtCore.Qt.DiagCrossPattern)\n                qp2.setBrush(brush)\n\n            qp2.drawPolygon( polyToDraw )\n\n        # Draw outline of selected object dotted\n        for obj in self.highlightObjs:\n            brush = QtGui.QBrush(QtCore.Qt.NoBrush)\n            qp2.setBrush(brush)\n            qp2.setPen(QtCore.Qt.DashLine)\n            polyToDraw = self.getPolygon(obj) * QtGui.QTransform.fromScale(self.scale,self.scale)\n            qp2.drawPolygon( polyToDraw )\n\n        # End the drawing of the overlay\n        qp2.end()\n        # Save QPainter settings to stack\n        qp.save()\n        # Define transparency\n        qp.setOpacity(self.config.transp)\n        # Draw the overlay image\n        qp.drawImage(self.xoff,self.yoff,overlay)\n        # Restore settings\n        qp.restore()\n\n        return overlay\n\n    def drawDrawRect(self, qp):\n\n        qp.save()\n        qp.setBrush(QtGui.QBrush(QtCore.Qt.NoBrush))\n        qp.setFont(QtGui.QFont(\'QFont::AnyStyle\', 14))\n        thickPen = QtGui.QPen()\n        qp.setPen(thickPen)\n\n        for c in self.corrections:\n            rect = copy.deepcopy(c.bbox)\n\n            width = rect.width()\n            height = rect.height()\n            rect.setX(c.bbox.x() * self.scale + self.xoff)\n            rect.setY(c.bbox.y() * self.scale + self.yoff)\n\n            rect.setWidth(width * self.scale)\n            rect.setHeight(height * self.scale)\n\n            if c.selected:\n                thickPen.setColor(QtGui.QColor(0,0,0))\n                if c.type == CorrectionBox.types.QUESTION:\n                    descr = ""QUESTION""\n                elif c.type == CorrectionBox.types.RESOLVED:\n                    descr = ""FIXED""\n                else:\n                    descr = ""ERROR""\n                qp.setPen(thickPen)\n                qp.drawText(QtCore.QPoint( self.xoff, self.yoff + self.h + 20 ),\n                                           ""(%s: %s)"" % (descr, c.annotation))\n                pen_width = 6\n            else:\n                pen_width = 3\n\n            colour = c.get_colour()\n            thickPen.setColor(colour)\n            thickPen.setWidth(pen_width)\n            qp.setPen(thickPen)\n            qp.drawRect(rect)\n\n        if self.in_progress_bbox is not None:\n            rect = copy.deepcopy(self.in_progress_bbox)\n            width = rect.width()\n            height = rect.height()\n            rect.setX(self.in_progress_bbox.x() * self.scale + self.xoff)\n            rect.setY(self.in_progress_bbox.y() * self.scale + self.yoff)\n\n            rect.setWidth(width * self.scale)\n            rect.setHeight(height * self.scale)\n\n            thickPen.setColor(QtGui.QColor(255,0,0))\n            thickPen.setWidth(3)\n            qp.setPen(thickPen)\n            qp.drawRect(rect)\n\n\n        qp.restore()\n\n    # Draw the polygon that is drawn and edited by the user\n    # Usually the polygon must be rescaled properly. However when drawing\n    # The polygon within the zoom, this is not needed. Therefore the option transform.\n    def drawDrawPoly(self, qp, transform=None):\n        # Nothing to do?\n        if self.drawPoly.isEmpty():\n            return\n        if not self.image:\n            return\n\n        # Save QPainter settings to stack\n        qp.save()\n\n        # The polygon - make a copy\n        poly = QtGui.QPolygonF(self.drawPoly)\n\n        # Append the current mouse position\n        if not self.drawPolyClosed and (self.mousePosScaled is not None):\n            poly.append( self.mousePosScaled )\n\n        # Transform\n        if not transform:\n            poly = poly * QtGui.QTransform.fromScale(self.scale,self.scale)\n            poly.translate(self.xoff,self.yoff)\n        else:\n            poly = poly * transform\n\n        # Do not fill the polygon\n        qp.setBrush(QtGui.QBrush(QtCore.Qt.NoBrush))\n\n        # Draw the polygon edges\n        polyColor = QtGui.QColor(255,0,0)\n        qp.setPen(polyColor)\n        if not self.drawPolyClosed:\n            qp.drawPolyline( poly )\n        else:\n            qp.drawPolygon( poly )\n\n        # Get the ID of the closest point to the mouse\n        if self.mousePosScaled is not None:\n            closestPt = self.getClosestPoint( self.drawPoly, self.mousePosScaled )\n        else:\n            closestPt = (-1,-1)\n\n        # If a polygon edge is selected, draw in bold\n        if closestPt[0] != closestPt[1]:\n            thickPen = QtGui.QPen(polyColor)\n            thickPen.setWidth(3)\n            qp.setPen(thickPen)\n            qp.drawLine( poly[closestPt[0]], poly[closestPt[1]] )\n\n        # Draw the polygon points\n        qp.setPen(polyColor)\n        startDrawingPts = 0\n\n        # A bit different if not closed\n        if not self.drawPolyClosed:\n            # Draw\n            self.drawPoint( qp, poly.first(), True, closestPt==(0,0) and self.drawPoly.size()>1 )\n            # Do not draw again\n            startDrawingPts = 1\n\n        # The next in red\n        for pt in range(startDrawingPts,poly.size()):\n            self.drawPoint( qp, poly[pt], False, self.drawPolyClosed and closestPt==(pt,pt) )\n\n        # Restore QPainter settings from stack\n        qp.restore()\n\n    # Draw the label name next to the mouse\n    def drawLabelAtMouse(self, qp):\n        # Nothing to do without a highlighted object\n        if not self.highlightObjs:\n            return\n        # Also we do not want to draw the label, if we have a drawn polygon\n        if not self.drawPoly.isEmpty():\n            return\n        # Nothing to without a mouse position\n        if not self.mousePos:\n            return\n\n        # Save QPainter settings to stack\n        qp.save()\n\n        # That is the mouse positiong\n        mouse = self.mousePos\n\n        # Will show zoom\n        showZoom = self.config.zoom and not self.image.isNull() and self.w and self.h\n\n        # The text that is written next to the mouse\n        mouseText = self.highlightObjs[-1].label\n\n        # Where to write the text\n        # Depends on the zoom (additional offset to mouse to make space for zoom?)\n        # The location in the image (if we are at the top we want to write below of the mouse)\n        off = 36\n        if showZoom:\n            off += self.config.zoomSize/2\n        if mouse.y()-off > self.toolbar.height():\n            top = mouse.y()-off\n            btm = mouse.y()\n            vAlign = QtCore.Qt.AlignTop\n        else:\n            # The height of the cursor\n            if not showZoom:\n                off += 20\n            top = mouse.y()\n            btm = mouse.y()+off\n            vAlign = QtCore.Qt.AlignBottom\n\n        # Here we can draw\n        rect = QtCore.QRect()\n        rect.setTopLeft(QtCore.QPoint(mouse.x()-100,top))\n        rect.setBottomRight(QtCore.QPoint(mouse.x()+100,btm))\n\n        # The color\n        qp.setPen(QtGui.QColor(\'white\'))\n        # The font to use\n        font = QtGui.QFont(""Helvetica"",20,QtGui.QFont.Bold)\n        qp.setFont(font)\n        # Non-transparent\n        qp.setOpacity(1)\n        # Draw the text, horizontally centered\n        qp.drawText(rect,QtCore.Qt.AlignHCenter|vAlign,mouseText)\n        # Restore settings\n        qp.restore()\n\n    # Draw the zoom\n    def drawZoom(self,qp,overlay):\n        # Zoom disabled?\n        if not self.config.zoom:\n            return\n        # No image\n        if self.image.isNull() or not self.w or not self.h:\n            return\n        # No mouse\n        if not self.mousePos:\n            return\n\n        # Abbrevation for the zoom window size\n        zoomSize = self.config.zoomSize\n        # Abbrevation for the mouse position\n        mouse = self.mousePos\n\n        # The pixel that is the zoom center\n        pix = self.mousePosScaled\n        # The size of the part of the image that is drawn in the zoom window\n        selSize = zoomSize / ( self.config.zoomFactor * self.config.zoomFactor )\n        # The selection window for the image\n        sel  = QtCore.QRectF(pix.x()  -selSize/2 ,pix.y()  -selSize/2 ,selSize,selSize  )\n        # The selection window for the widget\n        view = QtCore.QRectF(mouse.x()-zoomSize/2,mouse.y()-zoomSize/2,zoomSize,zoomSize)\n\n        # Show the zoom image\n        qp.drawImage(view,self.image,sel)\n\n        # If we are currently drawing the polygon, we need to draw again in the zoom\n        if not self.drawPoly.isEmpty():\n            transform = QtGui.QTransform()\n            quadFrom = QtGui.QPolygonF()\n            quadFrom.append( sel.topLeft() )\n            quadFrom.append( sel.topRight() )\n            quadFrom.append( sel.bottomRight() )\n            quadFrom.append( sel.bottomLeft() )\n            quadTo = QtGui.QPolygonF()\n            quadTo.append( view.topLeft() )\n            quadTo.append( view.topRight() )\n            quadTo.append( view.bottomRight() )\n            quadTo.append( view.bottomLeft() )\n            if QtGui.QTransform.quadToQuad( quadFrom , quadTo , transform ):\n                qp.setClipRect(view)\n                #transform.translate(self.xoff,self.yoff)\n                self.drawDrawPoly(qp,transform)\n            else:\n                print( ""not possible"" )\n\n\n    #############################\n    ## Mouse/keyboard events\n    #############################\n\n    # Mouse moved\n    # Need to save the mouse position\n    # Need to drag a polygon point\n    # Need to update the mouse selected object\n    def mouseMoveEvent(self,event):\n        if self.image.isNull() or self.w == 0 or self.h == 0:\n            return\n\n        self.updateMousePos( event.posF() )\n\n        if not self.config.correctionMode:\n            # If we are dragging a point, update\n            if self.draggedPt >= 0:\n                # Update the dragged point\n                self.drawPoly.replace( self.draggedPt , self.mousePosScaled )\n                # If the polygon is the polygon of the selected object,\n                # update the object polygon and\n                # keep track of the changes we do\n                if self.selObjs:\n                    obj = self.annotation.objects[self.selObjs[-1]]\n                    obj.polygon[self.draggedPt] = Point(self.mousePosScaled.x(),self.mousePosScaled.y())\n                    # Check if we changed the object\'s polygon the first time\n                    if not obj.id in self.changedPolygon:\n                        self.changedPolygon.append(obj.id)\n                        self.addChange( ""Changed polygon of object {0} with label {1}"".format( obj.id, obj.label ) )\n        else:\n            if self.in_progress_bbox is not None:\n                p0 = (self.mousePosScaled.x(), self.mousePosScaled.y())\n                p1 = (self.mousePressEvent.x(), self.mousePressEvent.y())\n                xy = min(p0[0], p1[0]), min(p0[1], p1[1])\n                w, h = abs(p0[0] - p1[0]), abs(p0[1] - p1[1])\n                self.in_progress_bbox = QtCore.QRectF(xy[0], xy[1], w, h)\n            #p.set_x(xy[0])\n            #p.set_y(xy[1])\n            #p.set_width(w)\n            #p.set_height(h)\n\n        # Update the object selected by the mouse\n        self.updateMouseObject()\n\n        # Redraw\n        self.update()\n\n    # Mouse left the widget\n    def leaveEvent(self, event):\n        self.mousePos = None\n        self.mousePosScaled = None\n        self.mouseOutsideImage = True\n\n    # Mouse button pressed\n    # Start dragging of polygon point\n    # Enable temporary toggling of zoom\n    def mousePressEvent(self,event):\n\n        self.mouseButtons = event.buttons()\n        shiftPressed = QtGui.QApplication.keyboardModifiers() == QtCore.Qt.ShiftModifier\n        self.updateMousePos( event.posF() )\n        self.mousePressEvent = self.mousePosScaled\n        # Handle left click\n        if event.button() == QtCore.Qt.LeftButton:\n\n            # If the drawn polygon is closed and the mouse clicks a point,\n            # Then this one is dragged around\n            if not self.config.correctionMode:\n                if self.drawPolyClosed and (self.mousePosScaled is not None):\n                    closestPt = self.getClosestPoint( self.drawPoly, self.mousePosScaled )\n                    if shiftPressed :\n                        if closestPt[0] == closestPt[1]:\n                            del self.drawPoly[closestPt[0]]\n\n                            # If the polygon is the polygon of the selected object,\n                            # update the object\n                            # and keep track of the changes we do\n                            if self.selObjs:\n                                obj = self.annotation.objects[self.selObjs[-1]]\n                                del obj.polygon[closestPt[0]]\n                                # Check if we changed the object\'s polygon the first time\n                                if not obj.id in self.changedPolygon:\n                                    self.changedPolygon.append(obj.id)\n                                    self.addChange( ""Changed polygon of object {0} with label {1}"".format( obj.id, obj.label ) )\n\n                            self.update()\n                    else :\n                        # If we got a point (or nothing), we make it dragged\n                        if closestPt[0] == closestPt[1]:\n                            self.draggedPt = closestPt[0]\n                        # If we got an edge, we insert a point and make it dragged\n                        else:\n                            self.drawPoly.insert( closestPt[1] , self.mousePosScaled )\n                            self.draggedPt = closestPt[1]\n                            # If the polygon is the polygon of the selected object,\n                            # update the object\n                            # and keep track of the changes we do\n                            if self.selObjs:\n                                obj = self.annotation.objects[self.selObjs[-1]]\n                                obj.polygon.insert( closestPt[1] , Point( self.mousePosScaled.x() , self.mousePosScaled.y() ) )\n                                # Check if we changed the object\'s polygon the first time\n                                if not obj.id in self.changedPolygon:\n                                    self.changedPolygon.append(obj.id)\n                                    self.addChange( ""Changed polygon of object {0} with label {1}"".format( obj.id, obj.label ) )\n            else:\n                assert self.in_progress_bbox == None\n                self.in_progress_bbox = QtCore.QRectF(self.mousePosScaled.x(), self.mousePosScaled.y(), 0, 0)\n\n        # Handle right click\n        elif event.button() == QtCore.Qt.RightButton:\n            self.toggleZoom( event.posF() )\n\n        # Redraw\n        self.update()\n\n\n    # Mouse button released\n    # End dragging of polygon\n    # Select an object\n    # Add a point to the polygon\n    # Disable temporary toggling of zoom\n    def mouseReleaseEvent(self,event):\n        self.mouseButtons = event.buttons()\n        ctrlPressed  = event.modifiers() & QtCore.Qt.ControlModifier\n        shiftPressed = event.modifiers() & QtCore.Qt.ShiftModifier\n        altPressed   = event.modifiers() & QtCore.Qt.AltModifier\n\n        # Handle left click\n        if event.button() == QtCore.Qt.LeftButton:\n            if not self.config.correctionMode:\n                # Check if Ctrl is pressed\n                if ctrlPressed:\n                    # If also Shift is pressed and we have a closed polygon, then we intersect\n                    # the polygon with the mouse object\n                    if shiftPressed and self.drawPolyClosed:\n                        self.intersectPolygon()\n                    # If also Alt is pressed and we have a closed polygon, then we merge\n                    # the polygon with the mouse object\n                    if altPressed and self.drawPolyClosed:\n                        self.mergePolygon()\n                    # Make the current mouse object the selected\n                    # and process the selection\n                    else:\n                        self.selectObject()\n                # Add the point to the drawn polygon if not already closed\n                elif not self.drawPolyClosed:\n                    # If the mouse would close the poly make sure to do so\n                    if self.ptClosesPoly( ):\n                        self.closePolygon()\n                    elif self.mousePosScaled is not None:\n                        if not self.drawPolyClosed and self.drawPoly.isEmpty() :\n                            self.mousePosOnZoom = self.mousePos\n                        self.addPtToPoly( self.mousePosScaled )\n                # Otherwise end a possible dragging\n                elif self.drawPolyClosed:\n                    self.draggedPt = -1\n            else:\n                if self.in_progress_bbox is not None:\n                    if self.in_progress_bbox.width() > 20:\n                        description = QtGui.QInputDialog.getText(self, ""Error Description"", ""Please describe the labeling error briefly."")\n                        if description[1] and description[0]:\n                            self.corrections.append(CorrectionBox(self.in_progress_bbox, annotation=description[0]))\n                            #last_annotation = self.in_progress_annotation  #TODO: self?\n                            self.corrections[self.selected_correction].unselect()\n                            self.selected_correction = len(self.corrections)-1\n                            self.corrections[self.selected_correction].select()\n                            self.addChange( ""Added correction."")\n                    self.in_progress_annotation = None\n                    self.in_progress_bbox = None\n\n        # Handle right click\n        elif event.button() == QtCore.Qt.RightButton:\n            self.toggleZoom( event.posF() )\n\n        # Redraw\n        self.update()\n\n    # Mouse wheel scrolled\n    def wheelEvent(self, event):\n        deltaDegree = event.delta() / 8 # Rotation in degree\n        deltaSteps  = deltaDegree / 15 # Usually one step on the mouse is 15 degrees\n\n        if self.config.zoom:\n            # If shift is pressed, change zoom window size\n            if event.modifiers() and QtCore.Qt.Key_Shift:\n                self.config.zoomSize += deltaSteps * 10\n                self.config.zoomSize = max( self.config.zoomSize, 10   )\n                self.config.zoomSize = min( self.config.zoomSize, 1000 )\n            # Change zoom factor\n            else:\n                self.config.zoomFactor += deltaSteps * 0.05\n                self.config.zoomFactor = max( self.config.zoomFactor, 0.1 )\n                self.config.zoomFactor = min( self.config.zoomFactor, 10 )\n            self.update()\n\n    # Key pressed\n    def keyPressEvent(self,e):\n        # Ctrl key changes mouse cursor\n        if e.key() == QtCore.Qt.Key_Control:\n            QtGui.QApplication.setOverrideCursor(QtGui.QCursor(QtCore.Qt.PointingHandCursor))\n        # Backspace deletes last point from polygon\n        elif e.key() == QtCore.Qt.Key_Backspace:\n            if not self.drawPolyClosed:\n                del self.drawPoly[-1]\n                self.update()\n        # set alpha to temporary zero\n        elif e.key() == QtCore.Qt.Key_0:\n            self.transpTempZero = True\n            self.update()\n        elif e.key() == QtCore.Qt.Key_E:\n            self.select_next_correction()\n        elif e.key() == QtCore.Qt.Key_R:\n            self.select_previous_correction()\n        elif e.key() == QtCore.Qt.Key_1:\n            self.modify_correction_type(CorrectionBox.types.TO_CORRECT)\n        elif e.key() == QtCore.Qt.Key_2:\n            self.modify_correction_type(CorrectionBox.types.TO_REVIEW)\n        elif e.key() == QtCore.Qt.Key_3:\n            self.modify_correction_type(CorrectionBox.types.RESOLVED)\n        elif e.key() == QtCore.Qt.Key_4:\n            self.modify_correction_type(CorrectionBox.types.QUESTION)\n        elif e.key() == QtCore.Qt.Key_D and self.config.correctionMode:\n            self.delete_selected_annotation()\n        elif e.key() == QtCore.Qt.Key_M and self.config.correctionMode:\n            self.modify_correction_description()\n\n    # Key released\n    def keyReleaseEvent(self,e):\n        # Ctrl key changes mouse cursor\n        if e.key() == QtCore.Qt.Key_Control:\n            QtGui.QApplication.restoreOverrideCursor()\n        # check for zero to release temporary zero\n        # somehow, for the numpad key in some machines, a check on Insert is needed aswell\n        elif e.key() == QtCore.Qt.Key_0 or e.key() == QtCore.Qt.Key_Insert:\n            self.transpTempZero = False\n            self.update()\n\n    #############################\n    ## Little helper methods\n    #############################\n\n    # Helper method that sets tooltip and statustip\n    # Provide an QAction and the tip text\n    # This text is appended with a hotkeys and then assigned\n    def setTip( self, action, tip ):\n        tip += "" (Hotkeys: \'"" + ""\', \'"".join([str(s.toString()) for s in action.shortcuts()]) + ""\')""\n        action.setStatusTip(tip)\n        action.setToolTip(tip)\n\n    # Set the mouse positions\n    # There are the original positions refering to the screen\n    # Scaled refering to the image\n    # And a zoom version, where the mouse movement is artificially slowed down\n    def updateMousePos( self, mousePosOrig ):\n        if self.config.zoomFactor <= 1 or (self.drawPolyClosed or self.drawPoly.isEmpty()):\n            sens = 1.0\n        else :\n            sens = 1.0/pow(self.config.zoomFactor, 3);\n\n        if self.config.zoom and self.mousePosOnZoom is not None:\n            mousePos       = QtCore.QPointF(round((1-sens)*self.mousePosOnZoom.x() + (sens)*mousePosOrig.x()), round((1-sens)*self.mousePosOnZoom.y() + sens*mousePosOrig.y()))\n        else :\n            mousePos       = mousePosOrig\n        mousePosScaled = QtCore.QPointF( float(mousePos.x() - self.xoff) / self.scale , float(mousePos.y() - self.yoff) / self.scale )\n        mouseOutsideImage = not self.image.rect().contains( mousePosScaled.toPoint() )\n\n        mousePosScaled.setX( max( mousePosScaled.x() , 0. ) )\n        mousePosScaled.setY( max( mousePosScaled.y() , 0. ) )\n        mousePosScaled.setX( min( mousePosScaled.x() , self.image.rect().right() ) )\n        mousePosScaled.setY( min( mousePosScaled.y() , self.image.rect().bottom() ) )\n\n        if not self.image.rect().contains( mousePosScaled.toPoint() ):\n            self.mousePos = None\n            self.mousePosScaled = None\n            self.mousePosOrig = None\n            self.updateMouseObject()\n            self.update()\n            return\n\n        self.mousePos          = mousePos\n        self.mousePosScaled    = mousePosScaled\n        self.mousePosOrig      = mousePosOrig\n        self.mouseOutsideImage = mouseOutsideImage\n\n    # Toggle the zoom and update all mouse positions\n    def toggleZoom(self, mousePosOrig):\n        self.config.zoom = not self.config.zoom\n\n        if self.config.zoom:\n            self.mousePosOnZoom = self.mousePos\n            # Update the mouse position afterwards\n            self.updateMousePos( mousePosOrig )\n        else:\n            # Update the mouse position first\n            self.updateMousePos( mousePosOrig )\n            # Update the dragged point to the non-zoom point\n            if not self.config.correctionMode and self.draggedPt >= 0:\n                self.drawPoly.replace( self.draggedPt , self.mousePosScaled )\n\n    # Get the point/edge index within the given polygon that is close to the given point\n    # Returns (-1,-1) if none is close enough\n    # Returns (i,i) if the point with index i is closed\n    # Returns (i,i+1) if the edge from points i to i+1 is closest\n    def getClosestPoint( self, poly, pt ):\n        closest = (-1,-1)\n        distTh    = 4.0\n        dist      = 1e9 # should be enough\n        for i in range(poly.size()):\n            curDist = self.ptDist(poly[i],pt)\n            if curDist < dist:\n                closest = (i,i)\n                dist = curDist\n        # Close enough?\n        if dist <= distTh:\n            return closest\n\n        # Otherwise see if the polygon is closed, but a line is close enough\n        if self.drawPolyClosed and poly.size() >= 2:\n            for i in range(poly.size()):\n                pt1 = poly[i]\n                j = i+1\n                if j == poly.size():\n                    j = 0\n                pt2 = poly[j]\n                edge = QtCore.QLineF(pt1,pt2)\n                normal = edge.normalVector()\n                normalThroughMouse = QtCore.QLineF( pt.x(),pt.y(),pt.x()+normal.dx(),pt.y()+normal.dy() )\n                intersectionPt = QtCore.QPointF()\n                intersectionType = edge.intersect( normalThroughMouse , intersectionPt )\n                if intersectionType == QtCore.QLineF.BoundedIntersection:\n                    curDist = self.ptDist(intersectionPt,pt)\n                    if curDist < dist:\n                        closest = (i,j)\n                        dist = curDist\n\n        # Close enough?\n        if dist <= distTh:\n            return closest\n\n        # If we didnt return yet, we didnt find anything\n        return (-1,-1)\n\n    # Get distance between two points\n    def ptDist( self, pt1, pt2 ):\n        # A line between both\n        line = QtCore.QLineF( pt1 , pt2 )\n        # Length\n        lineLength = line.length()\n        return lineLength\n\n    # Determine if the given point closes the drawn polygon (snapping)\n    def ptClosesPoly(self):\n        if self.drawPoly.isEmpty():\n            return False\n        if self.mousePosScaled is None:\n            return False\n        closestPt = self.getClosestPoint( self.drawPoly, self.mousePosScaled )\n        return closestPt==(0,0)\n\n    # Draw a point using the given QPainter qp\n    # If its the first point in a polygon its drawn in green\n    # if not in red\n    # Also the radius might be increased\n    def drawPoint(self, qp, pt, isFirst, increaseRadius):\n        # The first in green\n        if isFirst:\n            qp.setBrush(QtGui.QBrush(QtGui.QColor(0,255,0),QtCore.Qt.SolidPattern))\n        # Other in red\n        else:\n            qp.setBrush(QtGui.QBrush(QtGui.QColor(255,0,0),QtCore.Qt.SolidPattern))\n\n        # Standard radius\n        r = 3.0\n        # Increase maybe\n        if increaseRadius:\n            r *= 2.5\n        # Draw\n        qp.drawEllipse( pt, r, r )\n\n    # Determine if the given candidate for a label path makes sense\n    def isLabelPathValid(self,labelPath):\n        return os.path.isdir(labelPath)\n\n    # Ask the user to select a label\n    # If you like, you can give an object ID for a better dialog texting\n    # Note that giving an object ID assumes that its current label is the default label\n    # If you dont, the message ""Select new label"" is used\n    # Return is (label, ok). \'ok\' is false if the user pressed Cancel\n    def getLabelFromUser(self, defaultLabel = """", objID = -1):\n        # Reset the status bar to this message when leaving\n        restoreMessage = self.statusBar().currentMessage()\n\n        # Update defaultLabel\n        if not defaultLabel:\n            defaultLabel = self.defaultLabel\n\n        # List of possible labels\n        items = QtCore.QStringList(name2label.keys())\n        items.sort()\n        default = items.indexOf(defaultLabel)\n        if default < 0:\n            self.statusBar().showMessage( \'The selected label is missing in the internal color map.\' )\n            return\n\n        # Specify title\n        dlgTitle = ""Select label""\n        message  = dlgTitle\n        question = dlgTitle\n        if objID >= 0:\n            message  = ""Select new label for object {0} with current label {1}"".format( objID, defaultLabel )\n            question = ""Label for object {0}"".format(objID)\n        self.statusBar().showMessage(message)\n\n        # Create and wait for dialog\n        (item, ok) = QtGui.QInputDialog.getItem(self, dlgTitle, question, items, default, False)\n\n        # Process the answer a bit\n        item = str(item)\n\n        # Restore message\n        self.statusBar().showMessage( restoreMessage )\n\n        # Return\n        return (item, ok)\n\n    # Add a point to the drawn polygon\n    def addPtToPoly(self, pt):\n        self.drawPoly.append( pt )\n        # Enable actions that need a polygon\n        for act in self.actPolyOrSelObj:\n            act.setEnabled(True)\n\n    # Clear the drawn polygon\n    def clearPolygon(self):\n        # We do not clear, since the drawPoly might be a reference on an object one\n        self.drawPoly = QtGui.QPolygonF()\n        self.drawPolyClosed = False\n        # Disable actions that need a polygon\n        for act in self.actPolyOrSelObj:\n            act.setEnabled(bool(self.selObjs))\n        for act in self.actClosedPoly:\n            act.setEnabled(False)\n\n    # We just closed the polygon and need to deal with this situation\n    def closePolygon(self):\n        self.drawPolyClosed = True\n        for act in self.actClosedPoly:\n            act.setEnabled(True)\n        message = ""What should I do with the polygon? Press n to create a new object, press Ctrl + Left Click to intersect with another object""\n        self.statusBar().showMessage(message)\n\n    # Intersect the drawn polygon with the mouse object\n    # and create a new object with same label and so on\n    def intersectPolygon(self):\n        # Cannot do anything without labels\n        if not self.annotation:\n            return\n        # Cannot do anything without a single selected object\n        if self.mouseObj < 0:\n            return\n        # The selected object that is modified\n        obj = self.annotation.objects[self.mouseObj]\n\n        # The intersection of the polygons\n        intersection = self.drawPoly.intersected( self.getPolygon(obj) )\n\n        if not intersection.isEmpty():\n            # Ask the user for a label\n            self.drawPoly = intersection\n            (label, ok) = self.getLabelFromUser( obj.label )\n\n            if ok and label:\n                # Append and create the new object\n                self.appendObject( label , intersection )\n\n                # Clear the drawn polygon\n                self.clearPolygon()\n\n                # Default message\n                self.statusBar().showMessage( self.defaultStatusbar )\n\n        # Deselect\n        self.deselectAllObjects()\n        # Redraw\n        self.update()\n\n    # Merge the drawn polygon with the mouse object\n    # and create a new object with same label and so on\n    def mergePolygon(self):\n        # Cannot do anything without labels\n        if not self.annotation:\n            return\n        # Cannot do anything without a single selected object\n        if self.mouseObj < 0:\n            return\n        # The selected object that is modified\n        obj = self.annotation.objects[self.mouseObj]\n\n        # The union of the polygons\n        union = self.drawPoly.united( self.getPolygon(obj) )\n\n        if not union.isEmpty():\n            # Ask the user for a label\n            self.drawPoly = union\n            (label, ok) = self.getLabelFromUser( obj.label )\n\n            if ok and label:\n                # Append and create the new object\n                self.appendObject( label , union )\n\n                # Clear the drawn polygon\n                self.clearPolygon()\n\n                # Default message\n                self.statusBar().showMessage( self.defaultStatusbar )\n\n        # Deselect\n        self.deselectAllObjects()\n        # Redraw\n        self.update()\n\n    # Edit an object\'s polygon or clear the polygon if multiple objects are selected\n    def initPolygonFromObject(self):\n        # Cannot do anything without labels\n        if not self.annotation:\n            return\n        # Cannot do anything without any selected object\n        if not self.selObjs:\n            return\n        # If there are multiple objects selected, we clear the polygon\n        if len(self.selObjs) > 1:\n            self.clearPolygon()\n            self.update()\n            return\n\n        # The selected object that is used for init\n        obj = self.annotation.objects[self.selObjs[-1]]\n\n        # Make a reference to the polygon\n        self.drawPoly = self.getPolygon(obj)\n\n        # Make sure its closed\n        self.drawPolyClosed = True\n\n        # Update toolbar icons\n        # Enable actions that need a polygon\n        for act in self.actPolyOrSelObj:\n            act.setEnabled(True)\n        # Enable actions that need a closed polygon\n        for act in self.actClosedPoly:\n            act.setEnabled(True)\n\n        # Redraw\n        self.update()\n\n    # Create new object\n    def appendObject(self, label, polygon):\n        # Create empty annotation object\n        # if first object\n        if not self.annotation:\n            self.annotation = Annotation()\n\n        # Search the highest ID\n        newID = 0\n        for obj in self.annotation.objects:\n            if obj.id >= newID:\n                newID = obj.id + 1\n\n        # New object\n        # Insert the object in the labels list\n        obj          = CsObject()\n        obj.label    = label\n\n        obj.polygon = [ Point(p.x(),p.y()) for p in polygon ]\n\n        obj.id       = newID\n        obj.deleted  = 0\n        obj.verified = 0\n        obj.user     = getpass.getuser()\n        obj.updateDate()\n\n        self.annotation.objects.append(obj)\n\n        # Append to changes\n        self.addChange( ""Created object {0} with label {1}"".format( newID, label ) )\n\n        # Clear the drawn polygon\n        self.deselectAllObjects()\n        self.clearPolygon()\n\n        # select the new object\n        self.mouseObj = 0\n        self.selectObject()\n\n    # Helper for leaving an image\n    # Returns true if the image can be left, false if not\n    # Checks for possible changes and asks the user if they should be saved\n    # If the user says yes, then they are saved and true is returned\n    def checkAndSave(self):\n        # Without changes it\'s ok to leave the image\n        if not self.changes:\n            return True\n\n        # Backup of status message\n        restoreMessage = self.statusBar().currentMessage()\n        # Create the dialog\n        dlgTitle = ""Save changes?""\n        self.statusBar().showMessage(dlgTitle)\n        text = ""Do you want to save the following changes?\\n""\n        for c in self.changes:\n            text += ""- "" + c + \'\\n\'\n        buttons = QtGui.QMessageBox.Save | QtGui.QMessageBox.Discard | QtGui.QMessageBox.Cancel\n        ret = QtGui.QMessageBox.question(self, dlgTitle, text, buttons, QtGui.QMessageBox.Save )\n        proceed = False\n        # If the user selected yes -> save\n        if ret == QtGui.QMessageBox.Save:\n            proceed = self.save()\n        # If the user selected to discard the changes, clear them\n        elif ret == QtGui.QMessageBox.Discard:\n            self.clearChanges( )\n            proceed = True\n        # Otherwise prevent leaving the image\n        else:\n            proceed = False\n        self.statusBar().showMessage( restoreMessage )\n        return proceed\n\n    # Actually save a screenshot\n    def doScreenshot(self):\n        # For creating the screenshot we re-use the label drawing function\n        # However, we draw in an image using a QPainter\n\n        # Create such an image\n        img = QtGui.QImage( self.image )\n        # Create a QPainter that can perform draw actions within a widget or image\n        qp = QtGui.QPainter()\n        # Begin drawing in the image\n        qp.begin(img)\n\n        # Remember some settings\n        xoff = self.xoff\n        yoff = self.yoff\n        scale = self.scale\n        w = self.w\n        h = self.h\n        # Update scale\n        self.xoff = 0\n        self.yoff = 0\n        self.scale = 1\n        self.w = self.image.width()\n        self.h = self.image.height()\n        # Detactivate the highlighted object\n        self.highlightObjs = []\n\n        # Blur the license plates\n        # make this variabel a member and use as option if desired\n        blurLicensePlates = True\n        if blurLicensePlates:\n            self.blurLicensePlates(qp)\n\n        # Draw the labels on top\n        ignore = []\n        if blurLicensePlates:\n            ignore.append( \'numberplate\' )\n        self.drawLabels(qp,ignore)\n\n        # Finish drawing\n        qp.end()\n        # Reset scale and stuff\n        self.xoff = xoff\n        self.yoff = yoff\n        self.scale = scale\n        self.w = w\n        self.h = h\n\n        # Generate the real filename for saving\n        file = self.config.screenshotFilename\n        # Replace occurance of %c with the city name (as directory)\n        # Generate the directory if necessary\n        cityIdx = file.find(\'%c\')\n        if cityIdx >= 0:\n            if self.config.cityName:\n                dir = os.path.join( file[:cityIdx] , self.config.cityName )\n                if not os.path.exists(dir):\n                    os.makedirs(dir)\n                file = file.replace( \'%c\',self.config.cityName + \'/\', 1 )\n\n                if file.find(\'%c\') > 0:\n                    message = ""Found multiple \'%c\' in screenshot filename. Not allowed""\n                    file = None\n            else:\n                message = ""Do not have a city name. Cannot replace \'%c\' in screenshot filename.""\n                file = None\n        # Replace occurances of %i with the image filename (without extension)\n        if file:\n            file = file.replace( \'%i\',os.path.splitext(os.path.basename(self.config.currentFile))[0] )\n        # Add extension .png if no extension given\n        if file:\n            if not os.path.splitext(file)[1]:\n                file += \'.png\'\n        # Save\n        if file:\n            success = img.save(file)\n            if success:\n                message = ""Saved screenshot to "" + file\n            else:\n                message = ""Failed to save screenshot""\n\n        self.statusBar().showMessage(message)\n        # Update to reset everything to the correct state\n        self.update()\n\n    # Blur the license plates\n    # Argument is a qPainter\n    # Thus, only use this method for screenshots.\n    def blurLicensePlates(self,qp):\n        # license plate name\n        searchedNames = [ \'license plate\' ]\n\n        # the image\n        img = self.image\n\n        # Draw all objects\n        for obj in self.annotation.objects:\n            # Some are flagged to not be drawn. Skip them\n            if not obj.draw:\n                continue\n\n            # The label of the object\n            name      = obj.label\n            # If we do not know a color for this label, skip\n            if not name2label.has_key( name ):\n                continue\n            # If we do not blur this label, skip\n            if not name in searchedNames:\n                continue\n\n            # Scale the polygon properly\n            polyToDraw = self.getPolygon(obj) * QtGui.QTransform.fromScale(self.scale,self.scale)\n            bb = polyToDraw.boundingRect()\n\n            # Get the mean color within the polygon\n            meanR = 0\n            meanG = 0\n            meanB = 0\n            num   = 0\n            for y in range( max(int(bb.top()),0) , min(int(bb.bottom()+1.5),img.height()) ):\n                for x in range( max(int(bb.left()),0) , min(int(bb.right()+1.5),img.width()) ):\n                    col = img.pixel(x,y)\n                    meanR += QtGui.QColor(col).red()\n                    meanG += QtGui.QColor(col).green()\n                    meanB += QtGui.QColor(col).blue()\n                    num   += 1\n            meanR /= float(num)\n            meanG /= float(num)\n            meanB /= float(num)\n            col = QtGui.QColor( meanR , meanG , meanB )\n            qp.setPen(col)\n            brush = QtGui.QBrush( col, QtCore.Qt.SolidPattern )\n            qp.setBrush(brush)\n\n            # Default drawing\n            qp.drawPolygon( polyToDraw )\n\n\n    # Update the object that is selected by the current mouse curser\n    def updateMouseObject(self):\n        self.mouseObj   = -1\n        if self.mousePosScaled is None:\n            return\n        if not self.annotation or not self.annotation.objects:\n            return\n        for idx in reversed(range(len(self.annotation.objects))):\n            obj = self.annotation.objects[idx]\n            if obj.draw and self.getPolygon(obj).containsPoint(self.mousePosScaled, QtCore.Qt.OddEvenFill):\n                self.mouseObj = idx\n                break\n\n    # Print info about the currently selected object at the status bar\n    def infoOnSelectedObject(self):\n        if not self.selObjs:\n            return\n        objID = self.selObjs[-1]\n        if self.annotation and objID >= 0:\n            obj = self.annotation.objects[objID]\n            self.statusBar().showMessage(""Label of object {0}: {1}"".format(obj.id,obj.label))\n        #else:\n        #    self.statusBar().showMessage(self.defaultStatusbar)\n\n    # Make the object selected by the mouse the real selected object\n    def selectObject(self):\n        # If there is no mouse selection, we are good\n        if self.mouseObj < 0:\n            self.deselectObject()\n            return\n\n        # Append the object to selection if it\'s not in there\n        if not self.mouseObj in self.selObjs:\n            self.selObjs.append( self.mouseObj )\n        # Otherwise remove the object\n        else:\n            self.deselectObject()\n\n        # update polygon\n        self.initPolygonFromObject()\n\n        # If we have selected objects make the toolbar actions active\n        if self.selObjs:\n            for act in self.actSelObj + self.actPolyOrSelObj:\n                act.setEnabled(True)\n        # If we have a single selected object make their toolbar actions active\n        for act in self.singleActSelObj:\n            act.setEnabled(len(self.selObjs) == 1)\n\n        self.infoOnSelectedObject()\n\n    # Deselect object\n    def deselectObject(self):\n        # If there is no object to deselect, we are good\n        if not self.selObjs:\n            return\n        # If the mouse does not select and object, remove the last one\n        if self.mouseObj < 0:\n            del self.selObjs[-1]\n        # Otherwise try to find the mouse obj in the list\n        if self.mouseObj in self.selObjs:\n            self.selObjs.remove(self.mouseObj)\n\n        # No object left?\n        if not self.selObjs:\n            for act in self.actSelObj:\n                act.setEnabled(False)\n            for act in self.actPolyOrSelObj:\n                act.setEnabled(bool(self.drawPoly))\n        # If we have a single selected object make their toolbar actions active\n        for act in self.singleActSelObj:\n            act.setEnabled(len(self.selObjs) == 1)\n        self.infoOnSelectedObject()\n\n    # Deselect all objects\n    def deselectAllObjects(self):\n        # If there is no object to deselect, we are good\n        self.selObjs = []\n        self.mouseObj = -1\n        for act in self.actSelObj:\n            act.setEnabled(False)\n        # If we have a single selected object make their toolbar actions active\n        for act in self.singleActSelObj:\n            act.setEnabled(len(self.selObjs) == 1)\n        self.infoOnSelectedObject()\n\n    # Modify the layer of the selected object\n    # Move the layer up (negative offset) or down (postive offset)\n    def modifyLayer(self, offset):\n        # Cannot do anything without labels\n        if not self.annotation:\n            return\n        # Cannot do anything without a single selected object\n        if len(self.selObjs) != 1:\n            return\n\n        # The selected object that is modified\n        obj = self.annotation.objects[self.selObjs[-1]]\n        # The index in the label list we are right now\n        oldidx = self.selObjs[-1]\n        # The index we want to move to\n        newidx = oldidx + offset\n\n        # Make sure not not exceed zero and the list\n        newidx = max(newidx,0)\n        newidx = min(newidx,len(self.annotation.objects)-1)\n\n        # If new and old idx are equal, there is nothing to do\n        if oldidx == newidx:\n            return\n\n        # Move the entry in the labels list\n        self.annotation.objects.insert(newidx, self.annotation.objects.pop(oldidx))\n\n        # Update the selected object to the new index\n        self.selObjs[-1] = newidx\n        self.statusBar().showMessage(""Moved object {0} with label {1} to layer {2}"".format(obj.id,obj.label,newidx))\n\n        # Check if we moved the object the first time\n        if not obj.id in self.changedLayer:\n            self.changedLayer.append(obj.id)\n            self.addChange( ""Changed layer for object {0} with label {1}"".format( obj.id, obj.label ) )\n\n    # Add a new change\n    def addChange(self, text):\n        if not text:\n            return\n\n        self.changes.append( text )\n        for act in self.actChanges:\n            act.setEnabled(True)\n\n    # Clear list of changes\n    def clearChanges(self):\n        self.changes        = []\n        self.changedLayer   = []\n        self.changedPolygon = []\n        for act in self.actChanges:\n            act.setEnabled(False)\n\n    # Clear the current labels\n    def clearAnnotation(self):\n        self.annotation = None\n        self.clearChanges()\n        self.deselectAllObjects()\n        self.clearPolygon()\n        self.config.currentLabelFile = """"\n\n    def clearCorrections(self):\n        self.correctionXML = None\n        self.corrections  = []\n        #self.clearChanges() #TODO perhaps?\n        #self.clearPolygon()\n        self.config.currentCorrectionFile = """"\n\n    # Get the filename where to load/save labels\n    # Returns empty string if not possible\n    # Set the createDirs to true, if you want to create needed directories\n    def getLabelFilename( self , createDirs = False ):\n        # We need the name of the current city\n        if not self.config.cityName:\n            return """"\n        # And we need to have a directory where labels should be searched\n        if not self.config.labelPath:\n            return """"\n        # Without the name of the current images, there is also nothing we can do\n        if not self.config.currentFile:\n            return """"\n        # Check if the label directory is valid. This folder is selected by the user\n        # and thus expected to exist\n        if not self.isLabelPathValid(self.config.labelPath):\n            return """"\n        # Dirs are not automatically created in this version of the tool\n        if not os.path.isdir( self.config.labelPath ):\n            return """"\n\n        labelDir = self.config.labelPath\n\n        # extension of ground truth files\n        if self.config.gtType:\n            ext = self.gtExt.format(\'_\'+self.config.gtType)\n        else:\n            ext = self.gtExt.format(\'\')\n        # Generate the filename of the label file\n        filename = os.path.basename( self.config.currentFile )\n        filename = filename.replace( self.imageExt , ext )\n        filename = os.path.join( labelDir , filename )\n        filename = os.path.normpath(filename)\n        return filename\n\n    # Get the filename where to load/save labels\n    # Returns empty string if not possible\n    # Set the createDirs to true, if you want to create needed directories\n    def getCorrectionFilename( self , createDirs = False ):\n        # And we need to have a directory where corrections are stored\n        if not self.config.correctionPath:\n            return """"\n        # Without the name of the current images, there is also nothing we can do\n        if not self.config.currentFile:\n            return """"\n\n        # Folder where to store the labels\n        correctionDir = self.config.correctionPath\n\n        # If the folder does not exist, create it if allowed\n        if not os.path.isdir( correctionDir ):\n            if createDirs:\n                os.makedirs( correctionDir )\n                if not os.path.isdir( correctionDir ):\n                    return """"\n            else:\n                return """"\n\n        # Generate the filename of the label file\n        filename = os.path.basename( self.config.currentFile )\n        filename = filename.replace( self.imageExt ,\'.xml\')\n        filename = os.path.join( correctionDir , filename )\n        filename = os.path.normpath(filename)\n        return filename\n\n    # Disable the popup menu on right click\n    def createPopupMenu(self):\n        pass\n\n\ndef main():\n\n    app = QtGui.QApplication(sys.argv)\n\n    tool = CityscapesLabelTool()\n\n    sys.exit(app.exec_())\n\n\nif __name__ == \'__main__\':\n    main()\n'"
datasets/cityscapesscripts/evaluation/__init__.py,0,b''
datasets/cityscapesscripts/evaluation/evalInstanceLevelSemanticLabeling.py,0,"b'#!/usr/bin/python\n#\n# The evaluation script for instance-level semantic labeling.\n# We use this script to evaluate your approach on the test set.\n# You can use the script to evaluate on the validation set.\n#\n# Please check the description of the ""getPrediction"" method below\n# and set the required environment variables as needed, such that\n# this script can locate your results.\n# If the default implementation of the method works, then it\'s most likely\n# that our evaluation server will be able to process your results as well.\n#\n# To run this script, make sure that your results contain text files\n# (one for each test set image) with the content:\n#   relPathPrediction1 labelIDPrediction1 confidencePrediction1\n#   relPathPrediction2 labelIDPrediction2 confidencePrediction2\n#   relPathPrediction3 labelIDPrediction3 confidencePrediction3\n#   ...\n#\n# - The given paths ""relPathPrediction"" point to images that contain\n# binary masks for the described predictions, where any non-zero is\n# part of the predicted instance. The paths must not contain spaces,\n# must be relative to the root directory and must point to locations\n# within the root directory.\n# - The label IDs ""labelIDPrediction"" specify the class of that mask,\n# encoded as defined in labels.py. Note that the regular ID is used,\n# not the train ID.\n# - The field ""confidencePrediction"" is a float value that assigns a\n# confidence score to the mask.\n#\n# Note that this tool creates a file named ""gtInstances.json"" during its\n# first run. This file helps to speed up computation and should be deleted\n# whenever anything changes in the ground truth annotations or anything\n# goes wrong.\n\n# python imports\nfrom __future__ import print_function\nimport os, sys\nimport fnmatch\nfrom copy import deepcopy\n\n# Cityscapes imports\nsys.path.append( os.path.normpath( os.path.join( os.path.dirname( __file__ ) , \'..\' , \'helpers\' ) ) )\nfrom csHelpers      import *\nfrom instances2dict import instances2dict\n\n\n###################################\n# PLEASE READ THESE INSTRUCTIONS!!!\n###################################\n# Provide the prediction file for the given ground truth file.\n# Please read the instructions above for a description of\n# the result file.\n#\n# The current implementation expects the results to be in a certain root folder.\n# This folder is one of the following with decreasing priority:\n#   - environment variable CITYSCAPES_RESULTS\n#   - environment variable CITYSCAPES_DATASET/results\n#   - ../../results/""\n# (Remember to set the variables using ""export CITYSCAPES_<VARIABLE>=<path>"".)\n#\n# Within the root folder, a matching prediction file is recursively searched.\n# A file matches, if the filename follows the pattern\n# <city>_123456_123456*.txt\n# for a ground truth filename\n# <city>_123456_123456_gtFine_instanceIds.png\ndef getPrediction( groundTruthFile , args ):\n    # determine the prediction path, if the method is first called\n    if not args.predictionPath:\n        rootPath = None\n        if \'CITYSCAPES_RESULTS\' in os.environ:\n            rootPath = os.environ[\'CITYSCAPES_RESULTS\']\n        elif \'CITYSCAPES_DATASET\' in os.environ:\n            rootPath = os.path.join( os.environ[\'CITYSCAPES_DATASET\'] , ""results"" )\n        else:\n            rootPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),\'..\',\'..\',\'results\')\n\n        if not os.path.isdir(rootPath):\n            printError(""Could not find a result root folder. Please read the instructions of this method."")\n\n        args.predictionPath = os.path.abspath(rootPath)\n\n    # walk the prediction path, if not happened yet\n    if not args.predictionWalk:\n        walk = []\n        for root, dirnames, filenames in os.walk(args.predictionPath):\n            walk.append( (root,filenames) )\n        args.predictionWalk = walk\n\n    csFile = getCsFileInfo(groundTruthFile)\n    filePattern = ""{}_{}_{}*.txt"".format( csFile.city , csFile.sequenceNb , csFile.frameNb )\n\n    predictionFile = None\n    for root, filenames in args.predictionWalk:\n        for filename in fnmatch.filter(filenames, filePattern):\n            if not predictionFile:\n                predictionFile = os.path.join(root, filename)\n            else:\n                printError(""Found multiple predictions for ground truth {}"".format(groundTruthFile))\n\n    if not predictionFile:\n        printError(""Found no prediction for ground truth {}"".format(groundTruthFile))\n\n    return predictionFile\n\n\n######################\n# Parameters\n######################\n\n\n# A dummy class to collect all bunch of data\nclass CArgs(object):\n    pass\n# And a global object of that class\nargs = CArgs()\n\n# Where to look for Cityscapes\nif \'CITYSCAPES_DATASET\' in os.environ:\n    args.cityscapesPath = os.environ[\'CITYSCAPES_DATASET\']\nelse:\n    args.cityscapesPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),\'..\',\'..\')\n\n# Parameters that should be modified by user\nargs.exportFile         = os.path.join( args.cityscapesPath , ""evaluationResults"" , ""resultInstanceLevelSemanticLabeling.json"" )\nargs.groundTruthSearch  = os.path.join( args.cityscapesPath , ""gtFine"" , ""val"" , ""*"", ""*_gtFine_instanceIds.png"" )\n\n# overlaps for evaluation\nargs.overlaps           = np.arange(0.5,1.,0.05)\n# minimum region size for evaluation [pixels]\nargs.minRegionSizes     = np.array( [ 100 , 1000 , 1000 ] )\n# distance thresholds [m]\nargs.distanceThs        = np.array( [  float(\'inf\') , 100 , 50 ] )\n# distance confidences\nargs.distanceConfs      = np.array( [ -float(\'inf\') , 0.5 , 0.5 ] )\n\nargs.gtInstancesFile    = os.path.join(os.path.dirname(os.path.realpath(__file__)),\'gtInstances.json\')\nargs.distanceAvailable  = False\nargs.JSONOutput         = True\nargs.quiet              = False\nargs.csv                = False\nargs.colorized          = True\nargs.instLabels         = []\n\n# store some parameters for finding predictions in the args variable\n# the values are filled when the method getPrediction is first called\nargs.predictionPath = None\nargs.predictionWalk = None\n\n\n# Determine the labels that have instances\ndef setInstanceLabels(args):\n    args.instLabels = []\n    for label in labels:\n        if label.hasInstances and not label.ignoreInEval:\n            args.instLabels.append(label.name)\n\n# Read prediction info\n# imgFile, predId, confidence\ndef readPredInfo(predInfoFileName,args):\n    predInfo = {}\n    if (not os.path.isfile(predInfoFileName)):\n        printError(""Infofile \'{}\' for the predictions not found."".format(predInfoFileName))\n    with open(predInfoFileName, \'r\') as f:\n        for line in f:\n            splittedLine         = line.split("" "")\n            if len(splittedLine) != 3:\n                printError( ""Invalid prediction file. Expected content: relPathPrediction1 labelIDPrediction1 confidencePrediction1"" )\n            if os.path.isabs(splittedLine[0]):\n                printError( ""Invalid prediction file. First entry in each line must be a relative path."" )\n\n            filename             = os.path.join( os.path.dirname(predInfoFileName),splittedLine[0] )\n            filename             = os.path.abspath( filename )\n\n            # check if that file is actually somewhere within the prediction root\n            if os.path.commonprefix( [filename,args.predictionPath] ) != args.predictionPath:\n                printError( ""Predicted mask {} in prediction text file {} points outside of prediction path."".format(filename,predInfoFileName) )\n\n            imageInfo            = {}\n            imageInfo[""labelID""] = int(float(splittedLine[1]))\n            imageInfo[""conf""]    = float(splittedLine[2])\n            predInfo[filename]   = imageInfo\n\n    return predInfo\n\n# Routine to read ground truth image\ndef readGTImage(gtImageFileName,args):\n    return Image.open(gtImageFileName)\n\n# either read or compute a dictionary of all ground truth instances\ndef getGtInstances(groundTruthList,args):\n    gtInstances = {}\n    # if there is a global statistics json, then load it\n    if (os.path.isfile(args.gtInstancesFile)):\n        if not args.quiet:\n            print(""Loading ground truth instances from JSON."")\n        with open(args.gtInstancesFile) as json_file:\n            gtInstances = json.load(json_file)\n    # otherwise create it\n    else:\n        if (not args.quiet):\n            print(""Creating ground truth instances from png files."")\n        gtInstances = instances2dict(groundTruthList,not args.quiet)\n        writeDict2JSON(gtInstances, args.gtInstancesFile)\n\n    return gtInstances\n\n# Filter instances, ignore labels without instances\ndef filterGtInstances(singleImageInstances,args):\n    instanceDict = {}\n    for labelName in singleImageInstances:\n        if not labelName in args.instLabels:\n            continue\n        instanceDict[labelName] = singleImageInstances[labelName]\n    return instanceDict\n\n# match ground truth instances with predicted instances\ndef matchGtWithPreds(predictionList,groundTruthList,gtInstances,args):\n    matches = {}\n    if not args.quiet:\n        print(""Matching {} pairs of images..."".format(len(predictionList)))\n\n    count = 0\n    for (pred,gt) in zip(predictionList,groundTruthList):\n        # key for dicts\n        dictKey = os.path.abspath(gt)\n\n        # Read input files\n        gtImage  = readGTImage(gt,args)\n        predInfo = readPredInfo(pred,args)\n\n        # Get and filter ground truth instances\n        unfilteredInstances = gtInstances[ dictKey ]\n        curGtInstancesOrig  = filterGtInstances(unfilteredInstances,args)\n\n        # Try to assign all predictions\n        (curGtInstances,curPredInstances) = assignGt2Preds(curGtInstancesOrig, gtImage, predInfo, args)\n\n        # append to global dict\n        matches[ dictKey ] = {}\n        matches[ dictKey ][""groundTruth""] = curGtInstances\n        matches[ dictKey ][""prediction""]  = curPredInstances\n\n        count += 1\n        if not args.quiet:\n            print(""\\rImages Processed: {}"".format(count), end=\' \')\n            sys.stdout.flush()\n\n    if not args.quiet:\n        print("""")\n\n    return matches\n\n# For a given frame, assign all predicted instances to ground truth instances\ndef assignGt2Preds(gtInstancesOrig, gtImage, predInfo, args):\n    # In this method, we create two lists\n    #  - predInstances: contains all predictions and their associated gt\n    #  - gtInstances:   contains all gt instances and their associated predictions\n    predInstances    = {}\n    predInstCount    = 0\n\n    # Create a prediction array for each class\n    for label in args.instLabels:\n        predInstances[label] = []\n\n    # We already know about the gt instances\n    # Add the matching information array\n    gtInstances = deepcopy(gtInstancesOrig)\n    for label in gtInstances:\n        for gt in gtInstances[label]:\n            gt[""matchedPred""] = []\n\n    # Make the gt a numpy array\n    gtNp = np.array(gtImage)\n\n    # Get a mask of void labels in the groundtruth\n    voidLabelIDList = []\n    for label in labels:\n        if label.ignoreInEval:\n            voidLabelIDList.append(label.id)\n    boolVoid = np.in1d(gtNp, voidLabelIDList).reshape(gtNp.shape)\n\n    # Loop through all prediction masks\n    for predImageFile in predInfo:\n        # Additional prediction info\n        labelID  = predInfo[predImageFile][""labelID""]\n        predConf = predInfo[predImageFile][""conf""]\n\n        # label name\n        labelName = id2label[int(labelID)].name\n\n        # maybe we are not interested in that label\n        if not labelName in args.instLabels:\n            continue\n\n        # Read the mask\n        predImage = Image.open(predImageFile)\n        predImage = predImage.convert(""L"")\n        predNp    = np.array(predImage)\n\n        # make the image really binary, i.e. everything non-zero is part of the prediction\n        boolPredInst   = predNp != 0\n        predPixelCount = np.count_nonzero( boolPredInst )\n\n        # skip if actually empty\n        if not predPixelCount:\n            continue\n\n        # The information we want to collect for this instance\n        predInstance = {}\n        predInstance[""imgName""]          = predImageFile\n        predInstance[""predID""]           = predInstCount\n        predInstance[""labelID""]          = int(labelID)\n        predInstance[""pixelCount""]       = predPixelCount\n        predInstance[""confidence""]       = predConf\n        # Determine the number of pixels overlapping void\n        predInstance[""voidIntersection""] = np.count_nonzero( np.logical_and(boolVoid, boolPredInst) )\n\n        # A list of all overlapping ground truth instances\n        matchedGt = []\n\n        # Loop through all ground truth instances with matching label\n        # This list contains all ground truth instances that distinguish groups\n        # We do not know, if a certain instance is actually a single object or a group\n        # e.g. car or cargroup\n        # However, for now we treat both the same and do the rest later\n        for (gtNum,gtInstance) in enumerate(gtInstancesOrig[labelName]):\n\n            intersection = np.count_nonzero( np.logical_and( gtNp == gtInstance[""instID""] , boolPredInst) )\n\n            # If they intersect add them as matches to both dicts\n            if (intersection > 0):\n                gtCopy   = gtInstance.copy()\n                predCopy = predInstance.copy()\n\n                # let the two know their intersection\n                gtCopy[""intersection""]   = intersection\n                predCopy[""intersection""] = intersection\n\n                # append ground truth to matches\n                matchedGt.append(gtCopy)\n                # append prediction to ground truth instance\n                gtInstances[labelName][gtNum][""matchedPred""].append(predCopy)\n\n        predInstance[""matchedGt""] = matchedGt\n        predInstCount += 1\n        predInstances[labelName].append(predInstance)\n\n    return (gtInstances,predInstances)\n\n\ndef evaluateMatches(matches, args):\n    # In the end, we need two vectors for each class and for each overlap\n    # The first vector (y_true) is binary and is 1, where the ground truth says true,\n    # and is 0 otherwise.\n    # The second vector (y_score) is float [0...1] and represents the confidence of\n    # the prediction.\n    #\n    # We represent the following cases as:\n    #                                       | y_true |   y_score\n    #   gt instance with matched prediction |    1   | confidence\n    #   gt instance w/o  matched prediction |    1   |     0.0\n    #          false positive prediction    |    0   | confidence\n    #\n    # The current implementation makes only sense for an overlap threshold >= 0.5,\n    # since only then, a single prediction can either be ignored or matched, but\n    # never both. Further, it can never match to two gt instances.\n    # For matching, we vary the overlap and do the following steps:\n    #   1.) remove all predictions that satisfy the overlap criterion with an ignore region (either void or *group)\n    #   2.) remove matches that do not satisfy the overlap\n    #   3.) mark non-matched predictions as false positive\n\n    # AP\n    overlaps  = args.overlaps\n    # region size\n    minRegionSizes = args.minRegionSizes\n    # distance thresholds\n    distThs   = args.distanceThs\n    # distance confidences\n    distConfs = args.distanceConfs\n    # only keep the first, if distances are not available\n    if not args.distanceAvailable:\n        minRegionSizes = [ minRegionSizes[0] ]\n        distThs        = [ distThs       [0] ]\n        distConfs      = [ distConfs     [0] ]\n\n    # last three must be of same size\n    if len(distThs) != len(minRegionSizes):\n        printError(""Number of distance thresholds and region sizes different"")\n    if len(distThs) != len(distConfs):\n        printError(""Number of distance thresholds and confidences different"")\n\n    # Here we hold the results\n    # First dimension is class, second overlap\n    ap = np.zeros( (len(distThs) , len(args.instLabels) , len(overlaps)) , np.float )\n\n    for dI,(minRegionSize,distanceTh,distanceConf) in enumerate(zip(minRegionSizes,distThs,distConfs)):\n        for (oI,overlapTh) in enumerate(overlaps):\n            for (lI,labelName) in enumerate(args.instLabels):\n                y_true   = np.empty( 0 )\n                y_score  = np.empty( 0 )\n                # count hard false negatives\n                hardFns  = 0\n                # found at least one gt and predicted instance?\n                haveGt   = False\n                havePred = False\n\n                for img in matches:\n                    predInstances = matches[img][""prediction"" ][labelName]\n                    gtInstances   = matches[img][""groundTruth""][labelName]\n                    # filter groups in ground truth\n                    gtInstances   = [ gt for gt in gtInstances if gt[""instID""]>=1000 and gt[""pixelCount""]>=minRegionSize and gt[""medDist""]<=distanceTh and gt[""distConf""]>=distanceConf ]\n\n                    if gtInstances:\n                        haveGt = True\n                    if predInstances:\n                        havePred = True\n\n                    curTrue  = np.ones ( len(gtInstances) )\n                    curScore = np.ones ( len(gtInstances) ) * (-float(""inf""))\n                    curMatch = np.zeros( len(gtInstances) , dtype=np.bool )\n\n                    # collect matches\n                    for (gtI,gt) in enumerate(gtInstances):\n                        foundMatch = False\n                        for pred in gt[""matchedPred""]:\n                            overlap = float(pred[""intersection""]) / (gt[""pixelCount""]+pred[""pixelCount""]-pred[""intersection""])\n                            if overlap > overlapTh:\n                                # the score\n                                confidence = pred[""confidence""]\n\n                                # if we already hat a prediction for this groundtruth\n                                # the prediction with the lower score is automatically a false positive\n                                if curMatch[gtI]:\n                                    maxScore = max( curScore[gtI] , confidence )\n                                    minScore = min( curScore[gtI] , confidence )\n                                    curScore[gtI] = maxScore\n                                    # append false positive\n                                    curTrue  = np.append(curTrue,0)\n                                    curScore = np.append(curScore,minScore)\n                                    curMatch = np.append(curMatch,True)\n                                # otherwise set score\n                                else:\n                                    foundMatch = True\n                                    curMatch[gtI] = True\n                                    curScore[gtI] = confidence\n\n                        if not foundMatch:\n                            hardFns += 1\n\n                    # remove non-matched ground truth instances\n                    curTrue  = curTrue [ curMatch==True ]\n                    curScore = curScore[ curMatch==True ]\n\n                    # collect non-matched predictions as false positive\n                    for pred in predInstances:\n                        foundGt = False\n                        for gt in pred[""matchedGt""]:\n                            overlap = float(gt[""intersection""]) / (gt[""pixelCount""]+pred[""pixelCount""]-gt[""intersection""])\n                            if overlap > overlapTh:\n                                foundGt = True\n                                break\n                        if not foundGt:\n                            # collect number of void and *group pixels\n                            nbIgnorePixels = pred[""voidIntersection""]\n                            for gt in pred[""matchedGt""]:\n                                # group?\n                                if gt[""instID""] < 1000:\n                                    nbIgnorePixels += gt[""intersection""]\n                                # small ground truth instances\n                                if gt[""pixelCount""] < minRegionSize or gt[""medDist""]>distanceTh or gt[""distConf""]<distanceConf:\n                                    nbIgnorePixels += gt[""intersection""]\n                            proportionIgnore = float(nbIgnorePixels)/pred[""pixelCount""]\n                            # if not ignored\n                            # append false positive\n                            if proportionIgnore <= overlapTh:\n                                curTrue = np.append(curTrue,0)\n                                confidence = pred[""confidence""]\n                                curScore = np.append(curScore,confidence)\n\n                    # append to overall results\n                    y_true  = np.append(y_true,curTrue)\n                    y_score = np.append(y_score,curScore)\n\n                # compute the average precision\n                if haveGt and havePred:\n                    # compute precision recall curve first\n\n                    # sorting and cumsum\n                    scoreArgSort      = np.argsort(y_score)\n                    yScoreSorted      = y_score[scoreArgSort]\n                    yTrueSorted       = y_true[scoreArgSort]\n                    yTrueSortedCumsum = np.cumsum(yTrueSorted)\n\n                    # unique thresholds\n                    (thresholds,uniqueIndices) = np.unique( yScoreSorted , return_index=True )\n\n                    # since we need to add an artificial point to the precision-recall curve\n                    # increase its length by 1\n                    nbPrecRecall = len(uniqueIndices) + 1\n\n                    # prepare precision recall\n                    nbExamples     = len(yScoreSorted)\n                    nbTrueExamples = yTrueSortedCumsum[-1]\n                    precision      = np.zeros(nbPrecRecall)\n                    recall         = np.zeros(nbPrecRecall)\n\n                    # deal with the first point\n                    # only thing we need to do, is to append a zero to the cumsum at the end.\n                    # an index of -1 uses that zero then\n                    yTrueSortedCumsum = np.append( yTrueSortedCumsum , 0 )\n\n                    # deal with remaining\n                    for idxRes,idxScores in enumerate(uniqueIndices):\n                        cumSum = yTrueSortedCumsum[idxScores-1]\n                        tp = nbTrueExamples - cumSum\n                        fp = nbExamples     - idxScores - tp\n                        fn = cumSum + hardFns\n                        p  = float(tp)/(tp+fp)\n                        r  = float(tp)/(tp+fn)\n                        precision[idxRes] = p\n                        recall   [idxRes] = r\n\n                    # first point in curve is artificial\n                    precision[-1] = 1.\n                    recall   [-1] = 0.\n\n                    # compute average of precision-recall curve\n                    # integration is performed via zero order, or equivalently step-wise integration\n                    # first compute the widths of each step:\n                    # use a convolution with appropriate kernel, manually deal with the boundaries first\n                    recallForConv = np.copy(recall)\n                    recallForConv = np.append( recallForConv[0] , recallForConv )\n                    recallForConv = np.append( recallForConv    , 0.            )\n\n                    stepWidths = np.convolve(recallForConv,[-0.5,0,0.5],\'valid\')\n\n                    # integrate is now simply a dot product\n                    apCurrent = np.dot( precision , stepWidths )\n\n                elif haveGt:\n                    apCurrent = 0.0\n                else:\n                    apCurrent = float(\'nan\')\n                ap[dI,lI,oI] = apCurrent\n\n    return ap\n\ndef computeAverages(aps,args):\n    # max distance index\n    dInf  = np.argmax( args.distanceThs )\n    d50m  = np.where( np.isclose( args.distanceThs ,  50. ) )\n    d100m = np.where( np.isclose( args.distanceThs , 100. ) )\n    o50   = np.where(np.isclose(args.overlaps,0.5  ))\n\n    avgDict = {}\n    avgDict[""allAp""]       = np.nanmean(aps[ dInf,:,:  ])\n    avgDict[""allAp50%""]    = np.nanmean(aps[ dInf,:,o50])\n\n    if args.distanceAvailable:\n        avgDict[""allAp50m""]    = np.nanmean(aps[ d50m,:,  :])\n        avgDict[""allAp100m""]   = np.nanmean(aps[d100m,:,  :])\n        avgDict[""allAp50%50m""] = np.nanmean(aps[ d50m,:,o50])\n\n    avgDict[""classes""]  = {}\n    for (lI,labelName) in enumerate(args.instLabels):\n        avgDict[""classes""][labelName]             = {}\n        avgDict[""classes""][labelName][""ap""]       = np.average(aps[ dInf,lI,  :])\n        avgDict[""classes""][labelName][""ap50%""]    = np.average(aps[ dInf,lI,o50])\n        if args.distanceAvailable:\n            avgDict[""classes""][labelName][""ap50m""]    = np.average(aps[ d50m,lI,  :])\n            avgDict[""classes""][labelName][""ap100m""]   = np.average(aps[d100m,lI,  :])\n            avgDict[""classes""][labelName][""ap50%50m""] = np.average(aps[ d50m,lI,o50])\n\n    return avgDict\n\ndef printResults(avgDict, args):\n    sep     = ("",""         if args.csv       else """")\n    col1    = ("":""         if not args.csv   else """")\n    noCol   = (colors.ENDC if args.colorized else """")\n    bold    = (colors.BOLD if args.colorized else """")\n    lineLen = 50\n    if args.distanceAvailable:\n        lineLen += 40\n\n    print("""")\n    if not args.csv:\n        print(""#""*lineLen)\n    line  = bold\n    line += ""{:<15}"".format(""what""      ) + sep + col1\n    line += ""{:>15}"".format(""AP""        ) + sep\n    line += ""{:>15}"".format(""AP_50%""    ) + sep\n    if args.distanceAvailable:\n        line += ""{:>15}"".format(""AP_50m""    ) + sep\n        line += ""{:>15}"".format(""AP_100m""   ) + sep\n        line += ""{:>15}"".format(""AP_50%50m"" ) + sep\n    line += noCol\n    print(line)\n    if not args.csv:\n        print(""#""*lineLen)\n\n    for (lI,labelName) in enumerate(args.instLabels):\n        apAvg  = avgDict[""classes""][labelName][""ap""]\n        ap50o  = avgDict[""classes""][labelName][""ap50%""]\n        if args.distanceAvailable:\n            ap50m  = avgDict[""classes""][labelName][""ap50m""]\n            ap100m = avgDict[""classes""][labelName][""ap100m""]\n            ap5050 = avgDict[""classes""][labelName][""ap50%50m""]\n\n        line  = ""{:<15}"".format(labelName) + sep + col1\n        line += getColorEntry(apAvg , args) + sep + ""{:>15.3f}"".format(apAvg ) + sep\n        line += getColorEntry(ap50o , args) + sep + ""{:>15.3f}"".format(ap50o ) + sep\n        if args.distanceAvailable:\n            line += getColorEntry(ap50m , args) + sep + ""{:>15.3f}"".format(ap50m ) + sep\n            line += getColorEntry(ap100m, args) + sep + ""{:>15.3f}"".format(ap100m) + sep\n            line += getColorEntry(ap5050, args) + sep + ""{:>15.3f}"".format(ap5050) + sep\n        line += noCol\n        print(line)\n\n    allApAvg  = avgDict[""allAp""]\n    allAp50o  = avgDict[""allAp50%""]\n    if args.distanceAvailable:\n        allAp50m  = avgDict[""allAp50m""]\n        allAp100m = avgDict[""allAp100m""]\n        allAp5050 = avgDict[""allAp50%50m""]\n\n    if not args.csv:\n            print(""-""*lineLen)\n    line  = ""{:<15}"".format(""average"") + sep + col1\n    line += getColorEntry(allApAvg , args) + sep + ""{:>15.3f}"".format(allApAvg)  + sep\n    line += getColorEntry(allAp50o , args) + sep + ""{:>15.3f}"".format(allAp50o)  + sep\n    if args.distanceAvailable:\n        line += getColorEntry(allAp50m , args) + sep + ""{:>15.3f}"".format(allAp50m)  + sep\n        line += getColorEntry(allAp100m, args) + sep + ""{:>15.3f}"".format(allAp100m) + sep\n        line += getColorEntry(allAp5050, args) + sep + ""{:>15.3f}"".format(allAp5050) + sep\n    line += noCol\n    print(line)\n    print("""")\n\ndef prepareJSONDataForResults(avgDict, aps, args):\n    JSONData = {}\n    JSONData[""averages""] = avgDict\n    JSONData[""overlaps""] = args.overlaps.tolist()\n    JSONData[""minRegionSizes""]      = args.minRegionSizes.tolist()\n    JSONData[""distanceThresholds""]  = args.distanceThs.tolist()\n    JSONData[""minStereoDensities""]  = args.distanceConfs.tolist()\n    JSONData[""instLabels""] = args.instLabels\n    JSONData[""resultApMatrix""] = aps.tolist()\n\n    return JSONData\n\n# Work through image list\ndef evaluateImgLists(predictionList, groundTruthList, args):\n    # determine labels of interest\n    setInstanceLabels(args)\n    # get dictionary of all ground truth instances\n    gtInstances = getGtInstances(groundTruthList,args)\n    # match predictions and ground truth\n    matches = matchGtWithPreds(predictionList,groundTruthList,gtInstances,args)\n    writeDict2JSON(matches,""matches.json"")\n    # evaluate matches\n    apScores = evaluateMatches(matches, args)\n    # averages\n    avgDict = computeAverages(apScores,args)\n    # result dict\n    resDict = prepareJSONDataForResults(avgDict, apScores, args)\n    if args.JSONOutput:\n        # create output folder if necessary\n        path = os.path.dirname(args.exportFile)\n        ensurePath(path)\n        # Write APs to JSON\n        writeDict2JSON(resDict, args.exportFile)\n\n    if not args.quiet:\n         # Print results\n        printResults(avgDict, args)\n\n    return resDict\n\n# The main method\ndef main():\n    global args\n    argv = sys.argv[1:]\n\n    predictionImgList = []\n    groundTruthImgList = []\n\n    # the image lists can either be provided as arguments\n    if (len(argv) > 3):\n        for arg in argv:\n            if (""gt"" in arg or ""groundtruth"" in arg):\n                groundTruthImgList.append(arg)\n            elif (""pred"" in arg):\n                predictionImgList.append(arg)\n    # however the no-argument way is prefered\n    elif len(argv) == 0:\n        # use the ground truth search string specified above\n        groundTruthImgList = glob.glob(args.groundTruthSearch)\n        if not groundTruthImgList:\n            printError(""Cannot find any ground truth images to use for evaluation. Searched for: {}"".format(args.groundTruthSearch))\n        # get the corresponding prediction for each ground truth imag\n        for gt in groundTruthImgList:\n            predictionImgList.append( getPrediction(gt,args) )\n\n    # print some info for user\n    print(""Note that this tool uses the file \'{}\' to cache the ground truth instances."".format(args.gtInstancesFile))\n    print(""If anything goes wrong, or if you change the ground truth, please delete the file."")\n\n    # evaluate\n    evaluateImgLists(predictionImgList, groundTruthImgList, args)\n\n    return\n\n# call the main method\nif __name__ == ""__main__"":\n    main()\n'"
datasets/cityscapesscripts/evaluation/evalPixelLevelSemanticLabeling.py,0,"b'#!/usr/bin/python\n#\n# The evaluation script for pixel-level semantic labeling.\n# We use this script to evaluate your approach on the test set.\n# You can use the script to evaluate on the validation set.\n#\n# Please check the description of the ""getPrediction"" method below\n# and set the required environment variables as needed, such that\n# this script can locate your results.\n# If the default implementation of the method works, then it\'s most likely\n# that our evaluation server will be able to process your results as well.\n#\n# Note that the script is a lot faster, if you enable cython support.\n# WARNING: Cython only tested for Ubuntu 64bit OS.\n# To enable cython, run\n# setup.py build_ext --inplace\n#\n# To run this script, make sure that your results are images,\n# where pixels encode the class IDs as defined in labels.py.\n# Note that the regular ID is used, not the train ID.\n# Further note that many classes are ignored from evaluation.\n# Thus, authors are not expected to predict these classes and all\n# pixels with a ground truth label that is ignored are ignored in\n# evaluation.\n\n# python imports\nfrom __future__ import print_function\nimport os, sys\nimport platform\nimport fnmatch\n\ntry:\n    from itertools import izip\nexcept ImportError:\n    izip = zip\n\n# Cityscapes imports\nsys.path.append( os.path.normpath( os.path.join( os.path.dirname( __file__ ) , \'..\' , \'helpers\' ) ) )\nfrom datasets.cityscapesscripts.helpers.csHelpers import *\n\n# C Support\n# Enable the cython support for faster evaluation\n# Only tested for Ubuntu 64bit OS\nCSUPPORT = True\n# Check if C-Support is available for better performance\nif CSUPPORT:\n    try:\n        import addToConfusionMatrix\n    except:\n        CSUPPORT = False\n\n\n###################################\n# PLEASE READ THESE INSTRUCTIONS!!!\n###################################\n# Provide the prediction file for the given ground truth file.\n#\n# The current implementation expects the results to be in a certain root folder.\n# This folder is one of the following with decreasing priority:\n#   - environment variable CITYSCAPES_RESULTS\n#   - environment variable CITYSCAPES_DATASET/results\n#   - ../../results/""\n#\n# Within the root folder, a matching prediction file is recursively searched.\n# A file matches, if the filename follows the pattern\n# <city>_123456_123456*.png\n# for a ground truth filename\n# <city>_123456_123456_gtFine_labelIds.png\ndef getPrediction( args, groundTruthFile ):\n    # determine the prediction path, if the method is first called\n    args.predictionPath = ""/zfs/zhang/Cityscapes/results/val""\n    if not args.predictionPath:\n        rootPath = None\n        if \'CITYSCAPES_RESULTS\' in os.environ:\n            rootPath = os.environ[\'CITYSCAPES_RESULTS\']\n        elif \'CITYSCAPES_DATASET\' in os.environ:\n            rootPath = os.path.join( os.environ[\'CITYSCAPES_DATASET\'] , ""results"" )\n        else:\n            rootPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),\'..\',\'..\',\'results\')\n\n        if not os.path.isdir(rootPath):\n            printError(""Could not find a result root folder. Please read the instructions of this method."")\n\n        args.predictionPath = ""/zfs/zhang/Cityscapes/results/val""\n\n    # walk the prediction path, if not happened yet\n    if not args.predictionWalk:\n        walk = []\n        for root, dirnames, filenames in os.walk(args.predictionPath):\n            walk.append( (root,filenames) )\n        args.predictionWalk = walk\n\n    csFile = getCsFileInfo(groundTruthFile)\n    filePattern = ""{}_{}_{}*.png"".format( csFile.city , csFile.sequenceNb , csFile.frameNb )\n\n    predictionFile = None\n    for root, filenames in args.predictionWalk:\n        for filename in fnmatch.filter(filenames, filePattern):\n            if not predictionFile:\n                predictionFile = os.path.join(root, filename)\n            else:\n                printError(""Found multiple predictions for ground truth {}"".format(groundTruthFile))\n\n    if not predictionFile:\n        printError(""Found no prediction for ground truth {}"".format(groundTruthFile))\n\n    return predictionFile\n\n\n######################\n# Parameters\n######################\n\n\n# A dummy class to collect all bunch of data\nclass CArgs(object):\n    pass\n# And a global object of that class\nargs = CArgs()\n\n# Where to look for Cityscapes\nif \'CITYSCAPES_DATASET\' in os.environ:\n    args.cityscapesPath = os.environ[\'CITYSCAPES_DATASET\']\nelse:\n    args.cityscapesPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),\'..\',\'..\')\n\nargs.cityscapesPath = ""/zfs/zhang/Cityscapes""\n\nif \'CITYSCAPES_EXPORT_DIR\' in os.environ:\n    export_dir = os.environ[\'CITYSCAPES_EXPORT_DIR\']\n    if not os.path.isdir(export_dir):\n        raise ValueError(""CITYSCAPES_EXPORT_DIR {} is not a directory"".format(export_dir))\n    args.exportFile = ""{}/resultPixelLevelSemanticLabeling.json"".format(export_dir)\nelse:\n    args.exportFile = os.path.join(args.cityscapesPath, ""evaluationResults"", ""resultPixelLevelSemanticLabeling.json"")\n# Parameters that should be modified by user\nargs.groundTruthSearch  = os.path.join( args.cityscapesPath , ""gtFine"" , ""val"" , ""*"", ""*_gtFine_labelIds.png"" )\n\n# Remaining params\nargs.evalInstLevelScore = True\nargs.evalPixelAccuracy  = False\nargs.evalLabels         = []\nargs.printRow           = 5\nargs.normalized         = True\nargs.colorized          = hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty() and platform.system()==\'Linux\'\nargs.bold               = colors.BOLD if args.colorized else """"\nargs.nocol              = colors.ENDC if args.colorized else """"\nargs.JSONOutput         = True\nargs.quiet              = False\n\nargs.avgClassSize       = {\n    ""bicycle""    :  4672.3249222261 ,\n    ""caravan""    : 36771.8241758242 ,\n    ""motorcycle"" :  6298.7200839748 ,\n    ""rider""      :  3930.4788056518 ,\n    ""bus""        : 35732.1511111111 ,\n    ""train""      : 67583.7075812274 ,\n    ""car""        : 12794.0202738185 ,\n    ""person""     :  3462.4756337644 ,\n    ""truck""      : 27855.1264367816 ,\n    ""trailer""    : 16926.9763313609 ,\n}\n\n# store some parameters for finding predictions in the args variable\n# the values are filled when the method getPrediction is first called\nargs.predictionPath = None\nargs.predictionWalk = None\n\n\n#########################\n# Methods\n#########################\n\n\n# Generate empty confusion matrix and create list of relevant labels\ndef generateMatrix(args):\n    args.evalLabels = []\n    for label in labels:\n        if (label.id < 0):\n            continue\n        # we append all found labels, regardless of being ignored\n        args.evalLabels.append(label.id)\n    maxId = max(args.evalLabels)\n    # We use longlong type to be sure that there are no overflows\n    return np.zeros(shape=(maxId+1, maxId+1),dtype=np.ulonglong)\n\ndef generateInstanceStats(args):\n    instanceStats = {}\n    instanceStats[""classes""   ] = {}\n    instanceStats[""categories""] = {}\n    for label in labels:\n        if label.hasInstances and not label.ignoreInEval:\n            instanceStats[""classes""][label.name] = {}\n            instanceStats[""classes""][label.name][""tp""] = 0.0\n            instanceStats[""classes""][label.name][""tpWeighted""] = 0.0\n            instanceStats[""classes""][label.name][""fn""] = 0.0\n            instanceStats[""classes""][label.name][""fnWeighted""] = 0.0\n    for category in category2labels:\n        labelIds = []\n        allInstances = True\n        for label in category2labels[category]:\n            if label.id < 0:\n                continue\n            if not label.hasInstances:\n                allInstances = False\n                break\n            labelIds.append(label.id)\n        if not allInstances:\n            continue\n\n        instanceStats[""categories""][category] = {}\n        instanceStats[""categories""][category][""tp""] = 0.0\n        instanceStats[""categories""][category][""tpWeighted""] = 0.0\n        instanceStats[""categories""][category][""fn""] = 0.0\n        instanceStats[""categories""][category][""fnWeighted""] = 0.0\n        instanceStats[""categories""][category][""labelIds""] = labelIds\n\n    return instanceStats\n\n\n# Get absolute or normalized value from field in confusion matrix.\ndef getMatrixFieldValue(confMatrix, i, j, args):\n    if args.normalized:\n        rowSum = confMatrix[i].sum()\n        if (rowSum == 0):\n            return float(\'nan\')\n        return float(confMatrix[i][j]) / rowSum\n    else:\n        return confMatrix[i][j]\n\n# Calculate and return IOU score for a particular label\ndef getIouScoreForLabel(label, confMatrix, args):\n    if id2label[label].ignoreInEval:\n        return float(\'nan\')\n\n    # the number of true positive pixels for this label\n    # the entry on the diagonal of the confusion matrix\n    tp = np.longlong(confMatrix[label,label])\n\n    # the number of false negative pixels for this label\n    # the row sum of the matching row in the confusion matrix\n    # minus the diagonal entry\n    fn = np.longlong(confMatrix[label,:].sum()) - tp\n\n    # the number of false positive pixels for this labels\n    # Only pixels that are not on a pixel with ground truth label that is ignored\n    # The column sum of the corresponding column in the confusion matrix\n    # without the ignored rows and without the actual label of interest\n    notIgnored = [l for l in args.evalLabels if not id2label[l].ignoreInEval and not l==label]\n    fp = np.longlong(confMatrix[notIgnored,label].sum())\n\n    # the denominator of the IOU score\n    denom = (tp + fp + fn)\n    if denom == 0:\n        return float(\'nan\')\n\n    # return IOU\n    return float(tp) / denom\n\n# Calculate and return IOU score for a particular label\ndef getInstanceIouScoreForLabel(label, confMatrix, instStats, args):\n    if id2label[label].ignoreInEval:\n        return float(\'nan\')\n\n    labelName = id2label[label].name\n    if not labelName in instStats[""classes""]:\n        return float(\'nan\')\n\n    tp = instStats[""classes""][labelName][""tpWeighted""]\n    fn = instStats[""classes""][labelName][""fnWeighted""]\n    # false postives computed as above\n    notIgnored = [l for l in args.evalLabels if not id2label[l].ignoreInEval and not l==label]\n    fp = np.longlong(confMatrix[notIgnored,label].sum())\n\n    # the denominator of the IOU score\n    denom = (tp + fp + fn)\n    if denom == 0:\n        return float(\'nan\')\n\n    # return IOU\n    return float(tp) / denom\n\n# Calculate prior for a particular class id.\ndef getPrior(label, confMatrix):\n    return float(confMatrix[label,:].sum()) / confMatrix.sum()\n\n# Get average of scores.\n# Only computes the average over valid entries.\ndef getScoreAverage(scoreList, args):\n    validScores = 0\n    scoreSum    = 0.0\n    for score in scoreList:\n        if not math.isnan(scoreList[score]):\n            validScores += 1\n            scoreSum += scoreList[score]\n    if validScores == 0:\n        return float(\'nan\')\n    return scoreSum / validScores\n\n# Calculate and return IOU score for a particular category\ndef getIouScoreForCategory(category, confMatrix, args):\n    # All labels in this category\n    labels = category2labels[category]\n    # The IDs of all valid labels in this category\n    labelIds = [label.id for label in labels if not label.ignoreInEval and label.id in args.evalLabels]\n    # If there are no valid labels, then return NaN\n    if not labelIds:\n        return float(\'nan\')\n\n    # the number of true positive pixels for this category\n    # this is the sum of all entries in the confusion matrix\n    # where row and column belong to a label ID of this category\n    tp = np.longlong(confMatrix[labelIds,:][:,labelIds].sum())\n\n    # the number of false negative pixels for this category\n    # that is the sum of all rows of labels within this category\n    # minus the number of true positive pixels\n    fn = np.longlong(confMatrix[labelIds,:].sum()) - tp\n\n    # the number of false positive pixels for this category\n    # we count the column sum of all labels within this category\n    # while skipping the rows of ignored labels and of labels within this category\n    notIgnoredAndNotInCategory = [l for l in args.evalLabels if not id2label[l].ignoreInEval and id2label[l].category != category]\n    fp = np.longlong(confMatrix[notIgnoredAndNotInCategory,:][:,labelIds].sum())\n\n    # the denominator of the IOU score\n    denom = (tp + fp + fn)\n    if denom == 0:\n        return float(\'nan\')\n\n    # return IOU\n    return float(tp) / denom\n\n# Calculate and return IOU score for a particular category\ndef getInstanceIouScoreForCategory(category, confMatrix, instStats, args):\n    if not category in instStats[""categories""]:\n        return float(\'nan\')\n    labelIds = instStats[""categories""][category][""labelIds""]\n\n    tp = instStats[""categories""][category][""tpWeighted""]\n    fn = instStats[""categories""][category][""fnWeighted""]\n\n    # the number of false positive pixels for this category\n    # same as above\n    notIgnoredAndNotInCategory = [l for l in args.evalLabels if not id2label[l].ignoreInEval and id2label[l].category != category]\n    fp = np.longlong(confMatrix[notIgnoredAndNotInCategory,:][:,labelIds].sum())\n\n    # the denominator of the IOU score\n    denom = (tp + fp + fn)\n    if denom == 0:\n        return float(\'nan\')\n\n    # return IOU\n    return float(tp) / denom\n\n\n# create a dictionary containing all relevant results\ndef createResultDict( confMatrix, classScores, classInstScores, categoryScores, categoryInstScores, perImageStats, args ):\n    # write JSON result file\n    wholeData = {}\n    wholeData[""confMatrix""] = confMatrix.tolist()\n    wholeData[""priors""] = {}\n    wholeData[""labels""] = {}\n    for label in args.evalLabels:\n        wholeData[""priors""][id2label[label].name] = getPrior(label, confMatrix)\n        wholeData[""labels""][id2label[label].name] = label\n    wholeData[""classScores""] = classScores\n    wholeData[""classInstScores""] = classInstScores\n    wholeData[""categoryScores""] = categoryScores\n    wholeData[""categoryInstScores""] = categoryInstScores\n    wholeData[""averageScoreClasses""] = getScoreAverage(classScores, args)\n    wholeData[""averageScoreInstClasses""] = getScoreAverage(classInstScores, args)\n    wholeData[""averageScoreCategories""] = getScoreAverage(categoryScores, args)\n    wholeData[""averageScoreInstCategories""] = getScoreAverage(categoryInstScores, args)\n\n    if perImageStats:\n        wholeData[""perImageScores""] = perImageStats\n\n    return wholeData\n\ndef writeJSONFile(wholeData, args):\n    path = os.path.dirname(args.exportFile)\n    ensurePath(path)\n    writeDict2JSON(wholeData, args.exportFile)\n\n# Print confusion matrix\ndef printConfMatrix(confMatrix, args):\n    # print line\n    print(""\\b{text:{fill}>{width}}"".format(width=15, fill=\'-\', text="" ""), end=\' \')\n    for label in args.evalLabels:\n        print(""\\b{text:{fill}>{width}}"".format(width=args.printRow + 2, fill=\'-\', text="" ""), end=\' \')\n    print(""\\b{text:{fill}>{width}}"".format(width=args.printRow + 3, fill=\'-\', text="" ""))\n\n    # print label names\n    print(""\\b{text:>{width}} |"".format(width=13, text=""""), end=\' \')\n    for label in args.evalLabels:\n        print(""\\b{text:^{width}} |"".format(width=args.printRow, text=id2label[label].name[0]), end=\' \')\n    print(""\\b{text:>{width}} |"".format(width=6, text=""Prior""))\n\n    # print line\n    print(""\\b{text:{fill}>{width}}"".format(width=15, fill=\'-\', text="" ""), end=\' \')\n    for label in args.evalLabels:\n        print(""\\b{text:{fill}>{width}}"".format(width=args.printRow + 2, fill=\'-\', text="" ""), end=\' \')\n    print(""\\b{text:{fill}>{width}}"".format(width=args.printRow + 3, fill=\'-\', text="" ""))\n\n    # print matrix\n    for x in range(0, confMatrix.shape[0]):\n        if (not x in args.evalLabels):\n            continue\n        # get prior of this label\n        prior = getPrior(x, confMatrix)\n        # skip if label does not exist in ground truth\n        if prior < 1e-9:\n            continue\n\n        # print name\n        name = id2label[x].name\n        if len(name) > 13:\n            name = name[:13]\n        print(""\\b{text:>{width}} |"".format(width=13,text=name), end=\' \')\n        # print matrix content\n        for y in range(0, len(confMatrix[x])):\n            if (not y in args.evalLabels):\n                continue\n            matrixFieldValue = getMatrixFieldValue(confMatrix, x, y, args)\n            print(getColorEntry(matrixFieldValue, args) + ""\\b{text:>{width}.2f}  "".format(width=args.printRow, text=matrixFieldValue) + args.nocol, end=\' \')\n        # print prior\n        print(getColorEntry(prior, args) + ""\\b{text:>{width}.4f} "".format(width=6, text=prior) + args.nocol)\n    # print line\n    print(""\\b{text:{fill}>{width}}"".format(width=15, fill=\'-\', text="" ""), end=\' \')\n    for label in args.evalLabels:\n        print(""\\b{text:{fill}>{width}}"".format(width=args.printRow + 2, fill=\'-\', text="" ""), end=\' \')\n    print(""\\b{text:{fill}>{width}}"".format(width=args.printRow + 3, fill=\'-\', text="" ""), end=\' \')\n\n# Print intersection-over-union scores for all classes.\ndef printClassScores(scoreList, instScoreList, args):\n    if (args.quiet):\n        return\n    print(args.bold + ""classes          IoU      nIoU"" + args.nocol)\n    print(""--------------------------------"")\n    for label in args.evalLabels:\n        if (id2label[label].ignoreInEval):\n            continue\n        labelName = str(id2label[label].name)\n        iouStr = getColorEntry(scoreList[labelName], args) + ""{val:>5.3f}"".format(val=scoreList[labelName]) + args.nocol\n        niouStr = getColorEntry(instScoreList[labelName], args) + ""{val:>5.3f}"".format(val=instScoreList[labelName]) + args.nocol\n        print(""{:<14}: "".format(labelName) + iouStr + ""    "" + niouStr)\n\n# Print intersection-over-union scores for all categorys.\ndef printCategoryScores(scoreDict, instScoreDict, args):\n    if (args.quiet):\n        return\n    print(args.bold + ""categories       IoU      nIoU"" + args.nocol)\n    print(""--------------------------------"")\n    for categoryName in scoreDict:\n        if all( label.ignoreInEval for label in category2labels[categoryName] ):\n            continue\n        iouStr  = getColorEntry(scoreDict[categoryName], args) + ""{val:>5.3f}"".format(val=scoreDict[categoryName]) + args.nocol\n        niouStr = getColorEntry(instScoreDict[categoryName], args) + ""{val:>5.3f}"".format(val=instScoreDict[categoryName]) + args.nocol\n        print(""{:<14}: "".format(categoryName) + iouStr + ""    "" + niouStr)\n\n# Evaluate image lists pairwise.\ndef evaluateImgLists(predictionImgList, groundTruthImgList, args):\n    if len(predictionImgList) != len(groundTruthImgList):\n        printError(""List of images for prediction and groundtruth are not of equal size."")\n    confMatrix    = generateMatrix(args)\n    instStats     = generateInstanceStats(args)\n    perImageStats = {}\n    nbPixels      = 0\n\n    if not args.quiet:\n        print(""Evaluating {} pairs of images..."".format(len(predictionImgList)))\n\n    # Evaluate all pairs of images and save them into a matrix\n    for i in range(len(predictionImgList)):\n        predictionImgFileName = predictionImgList[i]\n        groundTruthImgFileName = groundTruthImgList[i]\n        #print ""Evaluate "", predictionImgFileName, ""<>"", groundTruthImgFileName\n        nbPixels += evaluatePair(predictionImgFileName, groundTruthImgFileName, confMatrix, instStats, perImageStats, args)\n\n        # sanity check\n        if confMatrix.sum() != nbPixels:\n            printError(\'Number of analyzed pixels and entries in confusion matrix disagree: contMatrix {}, pixels {}\'.format(confMatrix.sum(),nbPixels))\n\n        if not args.quiet:\n            print(""\\rImages Processed: {}"".format(i+1), end=\' \')\n            sys.stdout.flush()\n    if not args.quiet:\n        print(""\\n"")\n\n    # sanity check\n    if confMatrix.sum() != nbPixels:\n        printError(\'Number of analyzed pixels and entries in confusion matrix disagree: contMatrix {}, pixels {}\'.format(confMatrix.sum(),nbPixels))\n\n    # print confusion matrix\n    if (not args.quiet):\n        printConfMatrix(confMatrix, args)\n\n    # Calculate IOU scores on class level from matrix\n    classScoreList = {}\n    for label in args.evalLabels:\n        labelName = id2label[label].name\n        classScoreList[labelName] = getIouScoreForLabel(label, confMatrix, args)\n\n    # Calculate instance IOU scores on class level from matrix\n    classInstScoreList = {}\n    for label in args.evalLabels:\n        labelName = id2label[label].name\n        classInstScoreList[labelName] = getInstanceIouScoreForLabel(label, confMatrix, instStats, args)\n\n    # Print IOU scores\n    if (not args.quiet):\n        print("""")\n        print("""")\n        printClassScores(classScoreList, classInstScoreList, args)\n        iouAvgStr  = getColorEntry(getScoreAverage(classScoreList, args), args) + ""{avg:5.3f}"".format(avg=getScoreAverage(classScoreList, args)) + args.nocol\n        niouAvgStr = getColorEntry(getScoreAverage(classInstScoreList , args), args) + ""{avg:5.3f}"".format(avg=getScoreAverage(classInstScoreList , args)) + args.nocol\n        print(""--------------------------------"")\n        print(""Score Average : "" + iouAvgStr + ""    "" + niouAvgStr)\n        print(""--------------------------------"")\n        print("""")\n\n    # Calculate IOU scores on category level from matrix\n    categoryScoreList = {}\n    for category in category2labels.keys():\n        categoryScoreList[category] = getIouScoreForCategory(category,confMatrix,args)\n\n    # Calculate instance IOU scores on category level from matrix\n    categoryInstScoreList = {}\n    for category in category2labels.keys():\n        categoryInstScoreList[category] = getInstanceIouScoreForCategory(category,confMatrix,instStats,args)\n\n    # Print IOU scores\n    if (not args.quiet):\n        print("""")\n        printCategoryScores(categoryScoreList, categoryInstScoreList, args)\n        iouAvgStr = getColorEntry(getScoreAverage(categoryScoreList, args), args) + ""{avg:5.3f}"".format(avg=getScoreAverage(categoryScoreList, args)) + args.nocol\n        niouAvgStr = getColorEntry(getScoreAverage(categoryInstScoreList, args), args) + ""{avg:5.3f}"".format(avg=getScoreAverage(categoryInstScoreList, args)) + args.nocol\n        print(""--------------------------------"")\n        print(""Score Average : "" + iouAvgStr + ""    "" + niouAvgStr)\n        print(""--------------------------------"")\n        print("""")\n\n    # write result file\n    allResultsDict = createResultDict( confMatrix, classScoreList, classInstScoreList, categoryScoreList, categoryInstScoreList, perImageStats, args )\n    writeJSONFile( allResultsDict, args)\n\n    # return confusion matrix\n    return allResultsDict\n\n# Main evaluation method. Evaluates pairs of prediction and ground truth\n# images which are passed as arguments.\ndef evaluatePair(predictionImgFileName, groundTruthImgFileName, confMatrix, instanceStats, perImageStats, args):\n    # Loading all resources for evaluation.\n    try:\n        predictionImg = Image.open(predictionImgFileName)\n        predictionNp  = np.array(predictionImg)\n    except:\n        printError(""Unable to load "" + predictionImgFileName)\n    try:\n        groundTruthImg = Image.open(groundTruthImgFileName)\n        groundTruthNp = np.array(groundTruthImg)\n    except:\n        printError(""Unable to load "" + groundTruthImgFileName)\n    # load ground truth instances, if needed\n    if args.evalInstLevelScore:\n        groundTruthInstanceImgFileName = groundTruthImgFileName.replace(""labelIds"",""instanceIds"")\n        try:\n            instanceImg = Image.open(groundTruthInstanceImgFileName)\n            instanceNp  = np.array(instanceImg)\n        except:\n            printError(""Unable to load "" + groundTruthInstanceImgFileName)\n\n    # Check for equal image sizes\n    if (predictionImg.size[0] != groundTruthImg.size[0]):\n        printError(""Image widths of "" + predictionImgFileName + "" and "" + groundTruthImgFileName + "" are not equal."")\n    if (predictionImg.size[1] != groundTruthImg.size[1]):\n        printError(""Image heights of "" + predictionImgFileName + "" and "" + groundTruthImgFileName + "" are not equal."")\n    if ( len(predictionNp.shape) != 2 ):\n        printError(""Predicted image has multiple channels."")\n\n    imgWidth  = predictionImg.size[0]\n    imgHeight = predictionImg.size[1]\n    nbPixels  = imgWidth*imgHeight\n\n    # Evaluate images\n    if (CSUPPORT):\n        # using cython\n        confMatrix = addToConfusionMatrix.cEvaluatePair(predictionNp, groundTruthNp, confMatrix, args.evalLabels)\n    else:\n        # the slower python way\n        for (groundTruthImgPixel,predictionImgPixel) in izip(groundTruthImg.getdata(),predictionImg.getdata()):\n            if (not groundTruthImgPixel in args.evalLabels):\n                printError(""Unknown label with id {:}"".format(groundTruthImgPixel))\n\n            confMatrix[groundTruthImgPixel][predictionImgPixel] += 1\n\n    if args.evalInstLevelScore:\n        # Generate category masks\n        categoryMasks = {}\n        for category in instanceStats[""categories""]:\n            categoryMasks[category] = np.in1d( predictionNp , instanceStats[""categories""][category][""labelIds""] ).reshape(predictionNp.shape)\n\n        instList = np.unique(instanceNp[instanceNp > 1000])\n        for instId in instList:\n            labelId = int(instId/1000)\n            label = id2label[ labelId ]\n            if label.ignoreInEval:\n                continue\n\n            mask = instanceNp==instId\n            instSize = np.count_nonzero( mask )\n\n            tp = np.count_nonzero( predictionNp[mask] == labelId )\n            fn = instSize - tp\n\n            weight = args.avgClassSize[label.name] / float(instSize)\n            tpWeighted = float(tp) * weight\n            fnWeighted = float(fn) * weight\n\n            instanceStats[""classes""][label.name][""tp""]         += tp\n            instanceStats[""classes""][label.name][""fn""]         += fn\n            instanceStats[""classes""][label.name][""tpWeighted""] += tpWeighted\n            instanceStats[""classes""][label.name][""fnWeighted""] += fnWeighted\n\n            category = label.category\n            if category in instanceStats[""categories""]:\n                catTp = 0\n                catTp = np.count_nonzero( np.logical_and( mask , categoryMasks[category] ) )\n                catFn = instSize - catTp\n\n                catTpWeighted = float(catTp) * weight\n                catFnWeighted = float(catFn) * weight\n\n                instanceStats[""categories""][category][""tp""]         += catTp\n                instanceStats[""categories""][category][""fn""]         += catFn\n                instanceStats[""categories""][category][""tpWeighted""] += catTpWeighted\n                instanceStats[""categories""][category][""fnWeighted""] += catFnWeighted\n\n    if args.evalPixelAccuracy:\n        notIgnoredLabels = [l for l in args.evalLabels if not id2label[l].ignoreInEval]\n        notIgnoredPixels = np.in1d( groundTruthNp , notIgnoredLabels , invert=True ).reshape(groundTruthNp.shape)\n        erroneousPixels = np.logical_and( notIgnoredPixels , ( predictionNp != groundTruthNp ) )\n        perImageStats[predictionImgFileName] = {}\n        perImageStats[predictionImgFileName][""nbNotIgnoredPixels""] = np.count_nonzero(notIgnoredPixels)\n        perImageStats[predictionImgFileName][""nbCorrectPixels""]    = np.count_nonzero(erroneousPixels)\n\n    return nbPixels\n\n# The main method\ndef main():\n    global args\n    argv = sys.argv[1:]\n\n    predictionImgList = []\n    groundTruthImgList = []\n\n    # the image lists can either be provided as arguments\n    if (len(argv) > 3):\n        for arg in argv:\n            if (""gt"" in arg or ""groundtruth"" in arg):\n                groundTruthImgList.append(arg)\n            elif (""pred"" in arg):\n                predictionImgList.append(arg)\n    # however the no-argument way is prefered\n    elif len(argv) == 0:\n        # use the ground truth search string specified above\n        groundTruthImgList = glob.glob(args.groundTruthSearch)\n        if not groundTruthImgList:\n            printError(""Cannot find any ground truth images to use for evaluation. Searched for: {}"".format(args.groundTruthSearch))\n        # get the corresponding prediction for each ground truth imag\n        for gt in groundTruthImgList:\n            predictionImgList.append( getPrediction(args,gt) )\n\n    # evaluate\n    evaluateImgLists(predictionImgList, groundTruthImgList, args)\n\n    return\n\n\n# call the main method\nif __name__ == ""__main__"":\n    os.environ[""CITYSCAPES_RESULTS""] = ""/zfs/zhang/Cityscapes/results/val""\n    os.environ[""CITYSCAPES_DATASET""] = ""/zfs/zhang/Cityscapes""\n    main()\n'"
datasets/cityscapesscripts/evaluation/instance.py,0,"b'#!/usr/bin/python\n#\n# Instance class\n#\n\nimport json\n\n\nclass Instance(object):\n    instID     = 0\n    labelID    = 0\n    pixelCount = 0\n    medDist    = -1\n    distConf   = 0.0\n\n    def __init__(self, imgNp, instID):\n        if (instID == -1):\n            return\n        self.instID     = int(instID)\n        self.labelID    = int(self.getLabelID(instID))\n        self.pixelCount = int(self.getInstancePixels(imgNp, instID))\n\n    def getLabelID(self, instID):\n        if (instID < 1000):\n            return instID\n        else:\n            return int(instID / 1000)\n\n    def getInstancePixels(self, imgNp, instLabel):\n        return (imgNp == instLabel).sum()\n\n    def toJSON(self):\n        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n\n    def toDict(self):\n        buildDict = {}\n        buildDict[""instID""]     = self.instID\n        buildDict[""labelID""]    = self.labelID\n        buildDict[""pixelCount""] = self.pixelCount\n        buildDict[""medDist""]    = self.medDist\n        buildDict[""distConf""]   = self.distConf\n        return buildDict\n\n    def fromJSON(self, data):\n        self.instID     = int(data[""instID""])\n        self.labelID    = int(data[""labelID""])\n        self.pixelCount = int(data[""pixelCount""])\n        if (""medDist"" in data):\n            self.medDist    = float(data[""medDist""])\n            self.distConf   = float(data[""distConf""])\n\n    def __str__(self):\n        return ""(""+str(self.instID)+"")""\n'"
datasets/cityscapesscripts/evaluation/instances2dict.py,0,"b'#!/usr/bin/python\n#\n# Convert instances from png files to a dictionary\n#\n\nfrom __future__ import print_function\nimport os, sys\n\n# Cityscapes imports\nfrom instance import *\nsys.path.append( os.path.normpath( os.path.join( os.path.dirname( __file__ ) , \'..\' , \'helpers\' ) ) )\nfrom csHelpers import *\n\ndef instances2dict(imageFileList, verbose=False):\n    imgCount     = 0\n    instanceDict = {}\n\n    if not isinstance(imageFileList, list):\n        imageFileList = [imageFileList]\n\n    if verbose:\n        print(""Processing {} images..."".format(len(imageFileList)))\n\n    for imageFileName in imageFileList:\n        # Load image\n        img = Image.open(imageFileName)\n\n        # Image as numpy array\n        imgNp = np.array(img)\n\n        # Initialize label categories\n        instances = {}\n        for label in labels:\n            instances[label.name] = []\n\n        # Loop through all instance ids in instance image\n        for instanceId in np.unique(imgNp):\n            instanceObj = Instance(imgNp, instanceId)\n\n            instances[id2label[instanceObj.labelID].name].append(instanceObj.toDict())\n\n        imgKey = os.path.abspath(imageFileName)\n        instanceDict[imgKey] = instances\n        imgCount += 1\n\n        if verbose:\n            print(""\\rImages Processed: {}"".format(imgCount), end=\' \')\n            sys.stdout.flush()\n\n    if verbose:\n        print("""")\n\n    return instanceDict\n\ndef main(argv):\n    fileList = []\n    if (len(argv) > 2):\n        for arg in argv:\n            if (""png"" in arg):\n                fileList.append(arg)\n    instances2dict(fileList, True)\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
datasets/cityscapesscripts/evaluation/setup.py,0,"b'#!/usr/bin/python\n#\n# Enable cython support for eval scripts\n# Run as\n# setup.py build_ext --inplace\n#\n# WARNING: Only tested for Ubuntu 64bit OS.\n\ntry:\n    from distutils.core import setup\n    from Cython.Build import cythonize\nexcept:\n    print(""Unable to setup. Please use pip to install: cython"")\n    print(""sudo pip install cython"")\nimport os\nimport numpy\n\nos.environ[""CC""]  = ""g++""\nos.environ[""CXX""] = ""g++""\n\nsetup(ext_modules = cythonize(""addToConfusionMatrix.pyx""),include_dirs=[numpy.get_include()])\n'"
datasets/cityscapesscripts/helpers/__init__.py,0,b''
datasets/cityscapesscripts/helpers/annotation.py,0,"b'#!/usr/bin/python\n#\n# Classes to store, read, and write annotations\n#\n\nimport os\nimport json\nfrom collections import namedtuple\n  \n# get current date and time\nimport datetime\nimport locale\n\n# A point in a polygon\nPoint = namedtuple(\'Point\', [\'x\', \'y\'])\n\n# Class that contains the information of a single annotated object\nclass CsObject:\n    # Constructor\n    def __init__(self):\n        # the label\n        self.label    = """"\n        # the polygon as list of points\n        self.polygon  = []\n\n        # the object ID\n        self.id       = -1\n        # If deleted or not\n        self.deleted  = 0\n        # If verified or not\n        self.verified = 0\n        # The date string\n        self.date     = """"\n        # The username\n        self.user     = """"\n        # Draw the object\n        # Not read from or written to JSON\n        # Set to False if deleted object\n        # Might be set to False by the application for other reasons\n        self.draw     = True\n\n    def __str__(self):\n        polyText = """"\n        if self.polygon:\n            if len(self.polygon) <= 4:\n                for p in self.polygon:\n                    polyText += \'({},{}) \'.format( p.x , p.y )\n            else:\n                polyText += \'({},{}) ({},{}) ... ({},{}) ({},{})\'.format(\n                    self.polygon[ 0].x , self.polygon[ 0].y ,\n                    self.polygon[ 1].x , self.polygon[ 1].y ,\n                    self.polygon[-2].x , self.polygon[-2].y ,\n                    self.polygon[-1].x , self.polygon[-1].y )\n        else:\n            polyText = ""none""\n        text = ""Object: {} - {}"".format( self.label , polyText )\n        return text\n\n    def fromJsonText(self, jsonText, objId):\n        self.id = objId\n        self.label = str(jsonText[\'label\'])\n        self.polygon = [ Point(p[0],p[1]) for p in jsonText[\'polygon\'] ]\n        if \'deleted\' in jsonText.keys():\n            self.deleted = jsonText[\'deleted\']\n        else:\n            self.deleted = 0\n        if \'verified\' in jsonText.keys():\n            self.verified = jsonText[\'verified\']\n        else:\n            self.verified = 1\n        if \'user\' in jsonText.keys():\n            self.user = jsonText[\'user\']\n        else:\n            self.user = \'\'\n        if \'date\' in jsonText.keys():\n            self.date = jsonText[\'date\']\n        else:\n            self.date = \'\'\n        if self.deleted == 1:\n            self.draw = False\n        else:\n            self.draw = True\n\n    def toJsonText(self):\n        objDict = {}\n        objDict[\'label\'] = self.label\n        objDict[\'id\'] = self.id\n        objDict[\'deleted\'] = self.deleted\n        objDict[\'verified\'] = self.verified\n        objDict[\'user\'] = self.user\n        objDict[\'date\'] = self.date\n        objDict[\'polygon\'] = []\n        for pt in self.polygon:\n            objDict[\'polygon\'].append([pt.x, pt.y])\n\n        return objDict\n\n    def updateDate( self ):\n        try:\n            locale.setlocale( locale.LC_ALL , \'en_US\' )\n        except locale.Error:\n            locale.setlocale( locale.LC_ALL , \'us_us\' )\n        except:\n            pass\n        self.date = datetime.datetime.now().strftime(""%d-%b-%Y %H:%M:%S"")\n\n    # Mark the object as deleted\n    def delete(self):\n        self.deleted = 1\n        self.draw    = False\n\n# The annotation of a whole image\nclass Annotation:\n    # Constructor\n    def __init__(self):\n        # the width of that image and thus of the label image\n        self.imgWidth  = 0\n        # the height of that image and thus of the label image\n        self.imgHeight = 0\n        # the list of objects\n        self.objects = []\n\n    def toJson(self):\n        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n\n    def fromJsonText(self, jsonText):\n        jsonDict = json.loads(jsonText)\n        self.imgWidth  = int(jsonDict[\'imgWidth\'])\n        self.imgHeight = int(jsonDict[\'imgHeight\'])\n        self.objects   = []\n        for objId, objIn in enumerate(jsonDict[ \'objects\' ]):\n            obj = CsObject()\n            obj.fromJsonText(objIn, objId)\n            self.objects.append(obj)\n\n    def toJsonText(self):\n        jsonDict = {}\n        jsonDict[\'imgWidth\'] = self.imgWidth\n        jsonDict[\'imgHeight\'] = self.imgHeight\n        jsonDict[\'objects\'] = []\n        for obj in self.objects:\n            objDict = obj.toJsonText()\n            jsonDict[\'objects\'].append(objDict)\n  \n        return jsonDict\n\n    # Read a json formatted polygon file and return the annotation\n    def fromJsonFile(self, jsonFile):\n        if not os.path.isfile(jsonFile):\n            print(\'Given json file not found: {}\'.format(jsonFile))\n            return\n        with open(jsonFile, \'r\') as f:\n            jsonText = f.read()\n            self.fromJsonText(jsonText)\n\n    def toJsonFile(self, jsonFile):\n        with open(jsonFile, \'w\') as f:\n            f.write(self.toJson())\n            \n\n# a dummy example\nif __name__ == ""__main__"":\n    obj = CsObject()\n    obj.label = \'car\'\n    obj.polygon.append( Point( 0 , 0 ) )\n    obj.polygon.append( Point( 1 , 0 ) )\n    obj.polygon.append( Point( 1 , 1 ) )\n    obj.polygon.append( Point( 0 , 1 ) )\n\n    print(obj)'"
datasets/cityscapesscripts/helpers/csHelpers.py,0,"b'#!/usr/bin/python\n#\n# Various helper methods and includes for Cityscapes\n#\n\n# Python imports\nimport os, sys, getopt\nimport glob\nimport math\nimport json\nfrom collections import namedtuple\n\n# Image processing\n# Check if PIL is actually Pillow as expected\ntry:\n    from PIL import PILLOW_VERSION\nexcept:\n    print(""Please install the module \'Pillow\' for image processing, e.g."")\n    print(""pip install pillow"")\n    sys.exit(-1)\n\ntry:\n    import PIL.Image     as Image\n    import PIL.ImageDraw as ImageDraw\nexcept:\n    print(""Failed to import the image processing packages."")\n    sys.exit(-1)\n\n# Numpy for datastructures\ntry:\n    import numpy as np\nexcept:\n    print(""Failed to import numpy package."")\n    sys.exit(-1)\n\n# Cityscapes modules\ntry:\n    from annotation   import Annotation\n    from labels       import labels, name2label, id2label, trainId2label, category2labels\nexcept:\n    print(""Failed to find all Cityscapes modules"")\n    sys.exit(-1)\n\n# Print an error message and quit\ndef printError(message):\n    print(\'ERROR: \' + str(message))\n    sys.exit(-1)\n\n# Class for colors\nclass colors:\n    RED       = \'\\033[31;1m\'\n    GREEN     = \'\\033[32;1m\'\n    YELLOW    = \'\\033[33;1m\'\n    BLUE      = \'\\033[34;1m\'\n    MAGENTA   = \'\\033[35;1m\'\n    CYAN      = \'\\033[36;1m\'\n    BOLD      = \'\\033[1m\'\n    UNDERLINE = \'\\033[4m\'\n    ENDC      = \'\\033[0m\'\n\n# Colored value output if colorized flag is activated.\ndef getColorEntry(val, args):\n    if not args.colorized:\n        return """"\n    if not isinstance(val, float) or math.isnan(val):\n        return colors.ENDC\n    if (val < .20):\n        return colors.RED\n    elif (val < .40):\n        return colors.YELLOW\n    elif (val < .60):\n        return colors.BLUE\n    elif (val < .80):\n        return colors.CYAN\n    else:\n        return colors.GREEN\n\n# Cityscapes files have a typical filename structure\n# <city>_<sequenceNb>_<frameNb>_<type>[_<type2>].<ext>\n# This class contains the individual elements as members\n# For the sequence and frame number, the strings are returned, including leading zeros\nCsFile = namedtuple( \'csFile\' , [ \'city\' , \'sequenceNb\' , \'frameNb\' , \'type\' , \'type2\' , \'ext\' ] )\n\n# Returns a CsFile object filled from the info in the given filename\ndef getCsFileInfo(fileName):\n    baseName = os.path.basename(fileName)\n    parts = baseName.split(\'_\')\n    parts = parts[:-1] + parts[-1].split(\'.\')\n    if not parts:\n        printError( \'Cannot parse given filename ({}). Does not seem to be a valid Cityscapes file.\'.format(fileName) )\n    if len(parts) == 5:\n        csFile = CsFile( *parts[:-1] , type2="""" , ext=parts[-1] )\n    elif len(parts) == 6:\n        csFile = CsFile( *parts )\n    else:\n        printError( \'Found {} part(s) in given filename ({}). Expected 5 or 6.\'.format(len(parts) , fileName) )\n\n    return csFile\n\n# Returns the part of Cityscapes filenames that is common to all data types\n# e.g. for city_123456_123456_gtFine_polygons.json returns city_123456_123456\ndef getCoreImageFileName(filename):\n    csFile = getCsFileInfo(filename)\n    return ""{}_{}_{}"".format( csFile.city , csFile.sequenceNb , csFile.frameNb )\n\n# Returns the directory name for the given filename, e.g.\n# fileName = ""/foo/bar/foobar.txt""\n# return value is ""bar""\n# Not much error checking though\ndef getDirectory(fileName):\n    dirName = os.path.dirname(fileName)\n    return os.path.basename(dirName)\n\n# Make sure that the given path exists\ndef ensurePath(path):\n    if not path:\n        return\n    if not os.path.isdir(path):\n        os.makedirs(path)\n\n# Write a dictionary as json file\ndef writeDict2JSON(dictName, fileName):\n    with open(fileName, \'w\') as f:\n        f.write(json.dumps(dictName, default=lambda o: o.__dict__, sort_keys=True, indent=4))\n\n# dummy main\nif __name__ == ""__main__"":\n    printError(""Only for include, not executable on its own."")\n'"
datasets/cityscapesscripts/helpers/labels.py,0,"b'#!/usr/bin/python\n#\n# Cityscapes labels\n#\n\nfrom collections import namedtuple\n\n\n#--------------------------------------------------------------------------------\n# Definitions\n#--------------------------------------------------------------------------------\n\n# a label and all meta information\nLabel = namedtuple( \'Label\' , [\n\n    \'name\'        , # The identifier of this label, e.g. \'car\', \'person\', ... .\n                    # We use them to uniquely name a class\n\n    \'id\'          , # An integer ID that is associated with this label.\n                    # The IDs are used to represent the label in ground truth images\n                    # An ID of -1 means that this label does not have an ID and thus\n                    # is ignored when creating ground truth images (e.g. license plate).\n                    # Do not modify these IDs, since exactly these IDs are expected by the\n                    # evaluation server.\n\n    \'trainId\'     , # Feel free to modify these IDs as suitable for your method. Then create\n                    # ground truth images with train IDs, using the tools provided in the\n                    # \'preparation\' folder. However, make sure to validate or submit results\n                    # to our evaluation server using the regular IDs above!\n                    # For trainIds, multiple labels might have the same ID. Then, these labels\n                    # are mapped to the same class in the ground truth images. For the inverse\n                    # mapping, we use the label that is defined first in the list below.\n                    # For example, mapping all void-type classes to the same ID in training,\n                    # might make sense for some approaches.\n                    # Max value is 255!\n\n    \'category\'    , # The name of the category that this label belongs to\n\n    \'categoryId\'  , # The ID of this category. Used to create ground truth images\n                    # on category level.\n\n    \'hasInstances\', # Whether this label distinguishes between single instances or not\n\n    \'ignoreInEval\', # Whether pixels having this class as ground truth label are ignored\n                    # during evaluations or not\n\n    \'color\'       , # The color of this label\n    ] )\n\n\n#--------------------------------------------------------------------------------\n# A list of all labels\n#--------------------------------------------------------------------------------\n\n# Please adapt the train IDs as appropriate for you approach.\n# Note that you might want to ignore labels with ID 255 during training.\n# Further note that the current train IDs are only a suggestion. You can use whatever you like.\n# Make sure to provide your results using the original IDs and not the training IDs.\n# Note that many IDs are ignored in evaluation and thus you never need to predict these!\n\nlabels = [\n    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n    Label(  \'unlabeled\'            ,  0 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'ego vehicle\'          ,  1 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'rectification border\' ,  2 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'out of roi\'           ,  3 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'static\'               ,  4 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'dynamic\'              ,  5 ,      255 , \'void\'            , 0       , False        , True         , (111, 74,  0) ),\n    Label(  \'ground\'               ,  6 ,      255 , \'void\'            , 0       , False        , True         , ( 81,  0, 81) ),\n    Label(  \'road\'                 ,  7 ,        0 , \'flat\'            , 1       , False        , False        , (128, 64,128) ),\n    Label(  \'sidewalk\'             ,  8 ,        1 , \'flat\'            , 1       , False        , False        , (244, 35,232) ),\n    Label(  \'parking\'              ,  9 ,      255 , \'flat\'            , 1       , False        , True         , (250,170,160) ),\n    Label(  \'rail track\'           , 10 ,      255 , \'flat\'            , 1       , False        , True         , (230,150,140) ),\n    Label(  \'building\'             , 11 ,        2 , \'construction\'    , 2       , False        , False        , ( 70, 70, 70) ),\n    Label(  \'wall\'                 , 12 ,        3 , \'construction\'    , 2       , False        , False        , (102,102,156) ),\n    Label(  \'fence\'                , 13 ,        4 , \'construction\'    , 2       , False        , False        , (190,153,153) ),\n    Label(  \'guard rail\'           , 14 ,      255 , \'construction\'    , 2       , False        , True         , (180,165,180) ),\n    Label(  \'bridge\'               , 15 ,      255 , \'construction\'    , 2       , False        , True         , (150,100,100) ),\n    Label(  \'tunnel\'               , 16 ,      255 , \'construction\'    , 2       , False        , True         , (150,120, 90) ),\n    Label(  \'pole\'                 , 17 ,        5 , \'object\'          , 3       , False        , False        , (153,153,153) ),\n    Label(  \'polegroup\'            , 18 ,      255 , \'object\'          , 3       , False        , True         , (153,153,153) ),\n    Label(  \'traffic light\'        , 19 ,        6 , \'object\'          , 3       , False        , False        , (250,170, 30) ),\n    Label(  \'traffic sign\'         , 20 ,        7 , \'object\'          , 3       , False        , False        , (220,220,  0) ),\n    Label(  \'vegetation\'           , 21 ,        8 , \'nature\'          , 4       , False        , False        , (107,142, 35) ),\n    Label(  \'terrain\'              , 22 ,        9 , \'nature\'          , 4       , False        , False        , (152,251,152) ),\n    Label(  \'sky\'                  , 23 ,       10 , \'sky\'             , 5       , False        , False        , ( 70,130,180) ),\n    Label(  \'person\'               , 24 ,       11 , \'human\'           , 6       , True         , False        , (220, 20, 60) ),\n    Label(  \'rider\'                , 25 ,       12 , \'human\'           , 6       , True         , False        , (255,  0,  0) ),\n    Label(  \'car\'                  , 26 ,       13 , \'vehicle\'         , 7       , True         , False        , (  0,  0,142) ),\n    Label(  \'truck\'                , 27 ,       14 , \'vehicle\'         , 7       , True         , False        , (  0,  0, 70) ),\n    Label(  \'bus\'                  , 28 ,       15 , \'vehicle\'         , 7       , True         , False        , (  0, 60,100) ),\n    Label(  \'caravan\'              , 29 ,      255 , \'vehicle\'         , 7       , True         , True         , (  0,  0, 90) ),\n    Label(  \'trailer\'              , 30 ,      255 , \'vehicle\'         , 7       , True         , True         , (  0,  0,110) ),\n    Label(  \'train\'                , 31 ,       16 , \'vehicle\'         , 7       , True         , False        , (  0, 80,100) ),\n    Label(  \'motorcycle\'           , 32 ,       17 , \'vehicle\'         , 7       , True         , False        , (  0,  0,230) ),\n    Label(  \'bicycle\'              , 33 ,       18 , \'vehicle\'         , 7       , True         , False        , (119, 11, 32) ),\n    Label(  \'license plate\'        , -1 ,       -1 , \'vehicle\'         , 7       , False        , True         , (  0,  0,142) ),\n]\n\n\n#--------------------------------------------------------------------------------\n# Create dictionaries for a fast lookup\n#--------------------------------------------------------------------------------\n\n# Please refer to the main method below for example usages!\n\n# name to label object\nname2label      = { label.name    : label for label in labels           }\n# id to label object\nid2label        = { label.id      : label for label in labels           }\n# trainId to label object\ntrainId2label   = { label.trainId : label for label in reversed(labels) }\n# category to list of label objects\ncategory2labels = {}\nfor label in labels:\n    category = label.category\n    if category in category2labels:\n        category2labels[category].append(label)\n    else:\n        category2labels[category] = [label]\n\n#--------------------------------------------------------------------------------\n# Assure single instance name\n#--------------------------------------------------------------------------------\n\n# returns the label name that describes a single instance (if possible)\n# e.g.     input     |   output\n#        ----------------------\n#          car       |   car\n#          cargroup  |   car\n#          foo       |   None\n#          foogroup  |   None\n#          skygroup  |   None\ndef assureSingleInstanceName( name ):\n    # if the name is known, it is not a group\n    if name in name2label:\n        return name\n    # test if the name actually denotes a group\n    if not name.endswith(""group""):\n        return None\n    # remove group\n    name = name[:-len(""group"")]\n    # test if the new name exists\n    if not name in name2label:\n        return None\n    # test if the new name denotes a label that actually has instances\n    if not name2label[name].hasInstances:\n        return None\n    # all good then\n    return name\n\n#--------------------------------------------------------------------------------\n# Main for testing\n#--------------------------------------------------------------------------------\n\n# just a dummy main\nif __name__ == ""__main__"":\n    # Print all the labels\n    print(""List of cityscapes labels:"")\n    print("""")\n    print(""    {:>21} | {:>3} | {:>7} | {:>14} | {:>10} | {:>12} | {:>12}"".format( \'name\', \'id\', \'trainId\', \'category\', \'categoryId\', \'hasInstances\', \'ignoreInEval\' ))\n    print(""    "" + (\'-\' * 98))\n    for label in labels:\n        print(""    {:>21} | {:>3} | {:>7} | {:>14} | {:>10} | {:>12} | {:>12}"".format( label.name, label.id, label.trainId, label.category, label.categoryId, label.hasInstances, label.ignoreInEval ))\n    print("""")\n\n    print(""Example usages:"")\n\n    # Map from name to label\n    name = \'car\'\n    id   = name2label[name].id\n    print(""ID of label \'{name}\': {id}"".format( name=name, id=id ))\n\n    # Map from ID to label\n    category = id2label[id].category\n    print(""Category of label with ID \'{id}\': {category}"".format( id=id, category=category ))\n\n    # Map from trainID to label\n    trainId = 0\n    name = trainId2label[trainId].name\n    print(""Name of label with trainID \'{id}\': {name}"".format( id=trainId, name=name ))\n'"
datasets/cityscapesscripts/preparation/__init__.py,0,b''
datasets/cityscapesscripts/preparation/createTrainIdInstanceImgs.py,0,"b'#!/usr/bin/python\n#\n# Converts the polygonal annotations of the Cityscapes dataset\n# to images, where pixel values encode the ground truth classes and the\n# individual instance of that classes.\n#\n# The Cityscapes downloads already include such images\n#   a) *color.png             : the class is encoded by its color\n#   b) *labelIds.png          : the class is encoded by its ID\n#   c) *instanceIds.png       : the class and the instance are encoded by an instance ID\n# \n# With this tool, you can generate option\n#   d) *instanceTrainIds.png  : the class and the instance are encoded by an instance training ID\n# This encoding might come handy for training purposes. You can use\n# the file labes.py to define the training IDs that suit your needs.\n# Note however, that once you submit or evaluate results, the regular\n# IDs are needed.\n#\n# Please refer to \'json2instanceImg.py\' for an explanation of instance IDs.\n#\n# Uses the converter tool in \'json2instanceImg.py\'\n# Uses the mapping defined in \'labels.py\'\n#\n\n# python imports\nfrom __future__ import print_function\nimport os, glob, sys\n\n# cityscapes imports\nsys.path.append( os.path.normpath( os.path.join( os.path.dirname( __file__ ) , \'..\' , \'helpers\' ) ) )\nfrom csHelpers        import printError\nfrom json2instanceImg import json2instanceImg\n\n\n# The main method\ndef main():\n    # Where to look for Cityscapes\n    if \'CITYSCAPES_DATASET\' in os.environ:\n        cityscapesPath = os.environ[\'CITYSCAPES_DATASET\']\n    else:\n        cityscapesPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),\'..\',\'..\')\n    # how to search for all ground truth\n    searchFine   = os.path.join( cityscapesPath , ""gtFine""   , ""*"" , ""*"" , ""*_gt*_polygons.json"" )\n    searchCoarse = os.path.join( cityscapesPath , ""gtCoarse"" , ""*"" , ""*"" , ""*_gt*_polygons.json"" )\n\n    # search files\n    filesFine = glob.glob( searchFine )\n    filesFine.sort()\n    filesCoarse = glob.glob( searchCoarse )\n    filesCoarse.sort()\n\n    # concatenate fine and coarse\n    files = filesFine + filesCoarse\n    # files = filesFine # use this line if fine is enough for now.\n\n    # quit if we did not find anything\n    if not files:\n        printError( ""Did not find any files. Please consult the README."" )\n\n    # a bit verbose\n    print(""Processing {} annotation files"".format(len(files)))\n\n    # iterate through files\n    progress = 0\n    print(""Progress: {:>3} %"".format( progress * 100 / len(files) ), end=\' \')\n    for f in files:\n        # create the output filename\n        dst = f.replace( ""_polygons.json"" , ""_instanceTrainIds.png"" )\n\n        # do the conversion\n        try:\n            json2instanceImg( f , dst , ""trainIds"" )\n        except:\n            print(""Failed to convert: {}"".format(f))\n            raise\n\n        # status\n        progress += 1\n        print(""\\rProgress: {:>3} %"".format( progress * 100 / len(files) ), end=\' \')\n        sys.stdout.flush()\n\n\n# call the main\nif __name__ == ""__main__"":\n    main()\n'"
datasets/cityscapesscripts/preparation/createTrainIdLabelImgs.py,0,"b'#!/usr/bin/python\n#\n# Converts the polygonal annotations of the Cityscapes dataset\n# to images, where pixel values encode ground truth classes.\n#\n# The Cityscapes downloads already include such images\n#   a) *color.png             : the class is encoded by its color\n#   b) *labelIds.png          : the class is encoded by its ID\n#   c) *instanceIds.png       : the class and the instance are encoded by an instance ID\n# \n# With this tool, you can generate option\n#   d) *labelTrainIds.png     : the class is encoded by its training ID\n# This encoding might come handy for training purposes. You can use\n# the file labes.py to define the training IDs that suit your needs.\n# Note however, that once you submit or evaluate results, the regular\n# IDs are needed.\n#\n# Uses the converter tool in \'json2labelImg.py\'\n# Uses the mapping defined in \'labels.py\'\n#\n\n# python imports\nfrom __future__ import print_function\nimport os, glob, sys\n\n# cityscapes imports\nsys.path.append( os.path.normpath( os.path.join( os.path.dirname( __file__ ) , \'..\' , \'helpers\' ) ) )\nfrom csHelpers     import printError\nfrom json2labelImg import json2labelImg\n\n# The main method\ndef main():\n    # Where to look for Cityscapes\n    if \'CITYSCAPES_DATASET\' in os.environ:\n        cityscapesPath = os.environ[\'CITYSCAPES_DATASET\']\n    else:\n        cityscapesPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),\'..\',\'..\')\n    # how to search for all ground truth\n    searchFine   = os.path.join( cityscapesPath , ""gtFine""   , ""*"" , ""*"" , ""*_gt*_polygons.json"" )\n    searchCoarse = os.path.join( cityscapesPath , ""gtCoarse"" , ""*"" , ""*"" , ""*_gt*_polygons.json"" )\n\n    # search files\n    filesFine = glob.glob( searchFine )\n    filesFine.sort()\n    filesCoarse = glob.glob( searchCoarse )\n    filesCoarse.sort()\n\n    # concatenate fine and coarse\n    files = filesFine + filesCoarse\n    # files = filesFine # use this line if fine is enough for now.\n\n    # quit if we did not find anything\n    if not files:\n        printError( ""Did not find any files. Please consult the README."" )\n\n    # a bit verbose\n    print(""Processing {} annotation files"".format(len(files)))\n\n    # iterate through files\n    progress = 0\n    print(""Progress: {:>3} %"".format( progress * 100 / len(files) ), end=\' \')\n    for f in files:\n        # create the output filename\n        dst = f.replace( ""_polygons.json"" , ""_labelTrainIds.png"" )\n\n        # do the conversion\n        try:\n            json2labelImg( f , dst , ""trainIds"" )\n        except:\n            print(""Failed to convert: {}"".format(f))\n            raise\n\n        # status\n        progress += 1\n        print(""\\rProgress: {:>3} %"".format( progress * 100 / len(files) ), end=\' \')\n        sys.stdout.flush()\n\n\n# call the main\nif __name__ == ""__main__"":\n    main()\n'"
datasets/cityscapesscripts/preparation/json2instanceImg.py,0,"b'#!/usr/bin/python\n#\n# Reads labels as polygons in JSON format and converts them to instance images,\n# where each pixel has an ID that represents the ground truth class and the\n# individual instance of that class.\n#\n# The pixel values encode both, class and the individual instance.\n# The integer part of a division by 1000 of each ID provides the class ID,\n# as described in labels.py. The remainder is the instance ID. If a certain\n# annotation describes multiple instances, then the pixels have the regular\n# ID of that class.\n#\n# Example:\n# Let\'s say your labels.py assigns the ID 26 to the class \'car\'.\n# Then, the individual cars in an image get the IDs 26000, 26001, 26002, ... .\n# A group of cars, where our annotators could not identify the individual\n# instances anymore, is assigned to the ID 26.\n#\n# Note that not all classes distinguish instances (see labels.py for a full list).\n# The classes without instance annotations are always directly encoded with\n# their regular ID, e.g. 11 for \'building\'.\n#\n# Usage: json2instanceImg.py [OPTIONS] <input json> <output image>\n# Options:\n#   -h   print a little help text\n#   -t   use train IDs\n#\n# Can also be used by including as a module.\n#\n# Uses the mapping defined in \'labels.py\'.\n#\n# See also createTrainIdInstanceImgs.py to apply the mapping to all annotations in Cityscapes.\n#\n\n# python imports\nimport os, sys, getopt\n\n# Image processing\n# Check if PIL is actually Pillow as expected\ntry:\n    from PIL import PILLOW_VERSION\nexcept:\n    print(""Please install the module \'Pillow\' for image processing, e.g."")\n    print(""pip install pillow"")\n    sys.exit(-1)\n\ntry:\n    import PIL.Image     as Image\n    import PIL.ImageDraw as ImageDraw\nexcept:\n    print(""Failed to import the image processing packages."")\n    sys.exit(-1)\n\n\n# cityscapes imports\nsys.path.append( os.path.normpath( os.path.join( os.path.dirname( __file__ ) , \'..\' , \'helpers\' ) ) )\nfrom annotation import Annotation\nfrom labels     import labels, name2label\n\n# Print the information\ndef printHelp():\n    print(\'{} [OPTIONS] inputJson outputImg\'.format(os.path.basename(sys.argv[0])))\n    print(\'\')\n    print(\' Reads labels as polygons in JSON format and converts them to instance images,\')\n    print(\' where each pixel has an ID that represents the ground truth class and the\')\n    print(\' individual instance of that class.\')\n    print(\'\')\n    print(\' The pixel values encode both, class and the individual instance.\')\n    print(\' The integer part of a division by 1000 of each ID provides the class ID,\')\n    print(\' as described in labels.py. The remainder is the instance ID. If a certain\')\n    print(\' annotation describes multiple instances, then the pixels have the regular\')\n    print(\' ID of that class.\')\n    print(\'\')\n    print(\' Example:\')\n    print(\' Let\\\'s say your labels.py assigns the ID 26 to the class ""car"".\')\n    print(\' Then, the individual cars in an image get the IDs 26000, 26001, 26002, ... .\')\n    print(\' A group of cars, where our annotators could not identify the individual\')\n    print(\' instances anymore, is assigned to the ID 26.\')\n    print(\'\')\n    print(\' Note that not all classes distinguish instances (see labels.py for a full list).\')\n    print(\' The classes without instance annotations are always directly encoded with\')\n    print(\' their regular ID, e.g. 11 for ""building"".\')\n    print(\'\')\n    print(\'Options:\')\n    print(\' -h                 Print this help\')\n    print(\' -t                 Use the ""trainIDs"" instead of the regular mapping. See ""labels.py"" for details.\')\n\n# Print an error message and quit\ndef printError(message):\n    print(\'ERROR: {}\'.format(message))\n    print(\'\')\n    print(\'USAGE:\')\n    printHelp()\n    sys.exit(-1)\n\n# Convert the given annotation to a label image\ndef createInstanceImage(annotation, encoding):\n    # the size of the image\n    size = ( annotation.imgWidth , annotation.imgHeight )\n\n    # the background\n    if encoding == ""ids"":\n        backgroundId = name2label[\'unlabeled\'].id\n    elif encoding == ""trainIds"":\n        backgroundId = name2label[\'unlabeled\'].trainId\n    else:\n        print(""Unknown encoding \'{}\'"".format(encoding))\n        return None\n\n    # this is the image that we want to create\n    instanceImg = Image.new(""I"", size, backgroundId)\n\n    # a drawer to draw into the image\n    drawer = ImageDraw.Draw( instanceImg )\n\n    # a dict where we keep track of the number of instances that\n    # we already saw of each class\n    nbInstances = {}\n    for labelTuple in labels:\n        if labelTuple.hasInstances:\n            nbInstances[labelTuple.name] = 0\n\n    # loop over all objects\n    for obj in annotation.objects:\n        label   = obj.label\n        polygon = obj.polygon\n\n        # If the object is deleted, skip it\n        if obj.deleted:\n            continue\n\n        # if the label is not known, but ends with a \'group\' (e.g. cargroup)\n        # try to remove the s and see if that works\n        # also we know that this polygon describes a group\n        isGroup = False\n        if ( not label in name2label ) and label.endswith(\'group\'):\n            label = label[:-len(\'group\')]\n            isGroup = True\n\n        if not label in name2label:\n            printError( ""Label \'{}\' not known."".format(label) )\n\n        # the label tuple\n        labelTuple = name2label[label]\n\n        # get the class ID\n        if encoding == ""ids"":\n            id = labelTuple.id\n        elif encoding == ""trainIds"":\n            id = labelTuple.trainId\n\n        # if this label distinguishs between individual instances,\n        # make the id a instance ID\n        if labelTuple.hasInstances and not isGroup and id != 255:\n            id = id * 1000 + nbInstances[label]\n            nbInstances[label] += 1\n\n        # If the ID is negative that polygon should not be drawn\n        if id < 0:\n            continue\n\n        try:\n            drawer.polygon( polygon, fill=id )\n        except:\n            print(""Failed to draw polygon with label {} and id {}: {}"".format(label,id,polygon))\n            raise\n\n    return instanceImg\n\n# A method that does all the work\n# inJson is the filename of the json file\n# outImg is the filename of the instance image that is generated\n# encoding can be set to\n#     - ""ids""      : classes are encoded using the regular label IDs\n#     - ""trainIds"" : classes are encoded using the training IDs\ndef json2instanceImg(inJson,outImg,encoding=""ids""):\n    annotation = Annotation()\n    annotation.fromJsonFile(inJson)\n    instanceImg = createInstanceImage( annotation , encoding )\n    instanceImg.save( outImg )\n\n# The main method, if you execute this script directly\n# Reads the command line arguments and calls the method \'json2instanceImg\'\ndef main(argv):\n    trainIds = False\n    try:\n        opts, args = getopt.getopt(argv,""ht"")\n    except getopt.GetoptError:\n        printError( \'Invalid arguments\' )\n    for opt, arg in opts:\n        if opt == \'-h\':\n            printHelp()\n            sys.exit(0)\n        elif opt == \'-t\':\n            trainIds = True\n        else:\n            printError( ""Handling of argument \'{}\' not implementend"".format(opt) )\n\n    if len(args) == 0:\n        printError( ""Missing input json file"" )\n    elif len(args) == 1:\n        printError( ""Missing output image filename"" )\n    elif len(args) > 2:\n        printError( ""Too many arguments"" )\n\n    inJson = args[0]\n    outImg = args[1]\n\n    if trainIds:\n        json2instanceImg( inJson , outImg , \'trainIds\' )\n    else:\n        json2instanceImg( inJson , outImg )\n\n# call the main method\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
datasets/cityscapesscripts/preparation/json2labelImg.py,0,"b'#!/usr/bin/python\n#\n# Reads labels as polygons in JSON format and converts them to label images,\n# where each pixel has an ID that represents the ground truth label.\n#\n# Usage: json2labelImg.py [OPTIONS] <input json> <output image>\n# Options:\n#   -h   print a little help text\n#   -t   use train IDs\n#\n# Can also be used by including as a module.\n#\n# Uses the mapping defined in \'labels.py\'.\n#\n# See also createTrainIdLabelImgs.py to apply the mapping to all annotations in Cityscapes.\n#\n\n# python imports\nimport os, sys, getopt\n\n# Image processing\n# Check if PIL is actually Pillow as expected\ntry:\n    from PIL import PILLOW_VERSION\nexcept:\n    print(""Please install the module \'Pillow\' for image processing, e.g."")\n    print(""pip install pillow"")\n    sys.exit(-1)\n\ntry:\n    import PIL.Image     as Image\n    import PIL.ImageDraw as ImageDraw\nexcept:\n    print(""Failed to import the image processing packages."")\n    sys.exit(-1)\n\n\n# cityscapes imports\nsys.path.append( os.path.normpath( os.path.join( os.path.dirname( __file__ ) , \'..\' , \'helpers\' ) ) )\nfrom annotation import Annotation\nfrom labels     import name2label\n\n# Print the information\ndef printHelp():\n    print(\'{} [OPTIONS] inputJson outputImg\'.format(os.path.basename(sys.argv[0])))\n    print(\'\')\n    print(\'Reads labels as polygons in JSON format and converts them to label images,\')\n    print(\'where each pixel has an ID that represents the ground truth label.\')\n    print(\'\')\n    print(\'Options:\')\n    print(\' -h                 Print this help\')\n    print(\' -t                 Use the ""trainIDs"" instead of the regular mapping. See ""labels.py"" for details.\')\n\n# Print an error message and quit\ndef printError(message):\n    print(\'ERROR: {}\'.format(message))\n    print(\'\')\n    print(\'USAGE:\')\n    printHelp()\n    sys.exit(-1)\n\n# Convert the given annotation to a label image\ndef createLabelImage(annotation, encoding, outline=None):\n    # the size of the image\n    size = ( annotation.imgWidth , annotation.imgHeight )\n\n    # the background\n    if encoding == ""ids"":\n        background = name2label[\'unlabeled\'].id\n    elif encoding == ""trainIds"":\n        background = name2label[\'unlabeled\'].trainId\n    elif encoding == ""color"":\n        background = name2label[\'unlabeled\'].color\n    else:\n        print(""Unknown encoding \'{}\'"".format(encoding))\n        return None\n\n    # this is the image that we want to create\n    if encoding == ""color"":\n        labelImg = Image.new(""RGBA"", size, background)\n    else:\n        labelImg = Image.new(""L"", size, background)\n\n    # a drawer to draw into the image\n    drawer = ImageDraw.Draw( labelImg )\n\n    # loop over all objects\n    for obj in annotation.objects:\n        label   = obj.label\n        polygon = obj.polygon\n\n        # If the object is deleted, skip it\n        if obj.deleted:\n            continue\n\n        # If the label is not known, but ends with a \'group\' (e.g. cargroup)\n        # try to remove the s and see if that works\n        if ( not label in name2label ) and label.endswith(\'group\'):\n            label = label[:-len(\'group\')]\n\n        if not label in name2label:\n            printError( ""Label \'{}\' not known."".format(label) )\n\n        # If the ID is negative that polygon should not be drawn\n        if name2label[label].id < 0:\n            continue\n\n        if encoding == ""ids"":\n            val = name2label[label].id\n        elif encoding == ""trainIds"":\n            val = name2label[label].trainId\n        elif encoding == ""color"":\n            val = name2label[label].color\n\n        try:\n            if outline:\n                drawer.polygon( polygon, fill=val, outline=outline )\n            else:\n                drawer.polygon( polygon, fill=val )\n        except:\n            print(""Failed to draw polygon with label {}"".format(label))\n            raise\n\n    return labelImg\n\n# A method that does all the work\n# inJson is the filename of the json file\n# outImg is the filename of the label image that is generated\n# encoding can be set to\n#     - ""ids""      : classes are encoded using the regular label IDs\n#     - ""trainIds"" : classes are encoded using the training IDs\n#     - ""color""    : classes are encoded using the corresponding colors\ndef json2labelImg(inJson,outImg,encoding=""ids""):\n    annotation = Annotation()\n    annotation.fromJsonFile(inJson)\n    labelImg   = createLabelImage( annotation , encoding )\n    labelImg.save( outImg )\n\n# The main method, if you execute this script directly\n# Reads the command line arguments and calls the method \'json2labelImg\'\ndef main(argv):\n    trainIds = False\n    try:\n        opts, args = getopt.getopt(argv,""ht"")\n    except getopt.GetoptError:\n        printError( \'Invalid arguments\' )\n    for opt, arg in opts:\n        if opt == \'-h\':\n            printHelp()\n            sys.exit(0)\n        elif opt == \'-t\':\n            trainIds = True\n        else:\n            printError( ""Handling of argument \'{}\' not implementend"".format(opt) )\n\n    if len(args) == 0:\n        printError( ""Missing input json file"" )\n    elif len(args) == 1:\n        printError( ""Missing output image filename"" )\n    elif len(args) > 2:\n        printError( ""Too many arguments"" )\n\n    inJson = args[0]\n    outImg = args[1]\n\n    if trainIds:\n        json2labelImg( inJson , outImg , ""trainIds"" )\n    else:\n        json2labelImg( inJson , outImg )\n\n# call the main method\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
datasets/cityscapesscripts/viewer/__init__.py,0,b''
datasets/cityscapesscripts/viewer/cityscapesViewer.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n\n#################\n## Import modules\n#################\n\n# pyqt for everything graphical\nfrom PyQt4 import QtGui, QtCore\n# get command line parameters\nimport sys\n# walk directories\nimport glob\n# access to OS functionality\nimport os\n# call processes\nimport subprocess\n# copy things\nimport copy\n# numpy\nimport numpy as np\n# matplotlib for colormaps\ntry:\n    import matplotlib.colors\n    import matplotlib.cm\n    from PIL import PILLOW_VERSION\n    from PIL import Image\nexcept:\n    pass\n\n#################\n## Helper classes\n#################\n\n# annotation helper\nsys.path.append( os.path.normpath( os.path.join( os.path.dirname( __file__ ) , \'..\' , \'helpers\' ) ) )\nfrom annotation import Annotation\nfrom labels     import name2label, assureSingleInstanceName\n\n\n#################\n## Main GUI class\n#################\n\n# The main class which is a QtGui -> Main Window\nclass CityscapesViewer(QtGui.QMainWindow):\n\n    #############################\n    ## Construction / Destruction\n    #############################\n\n    # Constructor\n    def __init__(self):\n        # Construct base class\n        super(CityscapesViewer, self).__init__()\n\n        # This is the configuration.\n\n        # The filename of the image we currently working on\n        self.currentFile       = """"\n        # The filename of the labels we currently working on\n        self.currentLabelFile  = """"\n        # The path of the images of the currently loaded city\n        self.city              = """"\n        # The name of the currently loaded city\n        self.cityName          = """"\n        # The path of the labels. In this folder we expect a folder for each city\n        # Within these city folders we expect the label with a filename matching\n        # the images, except for the extension\n        self.labelPath         = """"\n        # The transparency of the labels over the image\n        self.transp            = 0.5\n        # The zoom toggle\n        self.zoom              = False\n        # The zoom factor\n        self.zoomFactor        = 1.5\n        # The size of the zoom window. Currently there is no setter or getter for that\n        self.zoomSize          = 400 #px\n\n        # The width that we actually use to show the image\n        self.w                 = 0\n        # The height that we actually use to show the image\n        self.h                 = 0\n        # The horizontal offset where we start drawing within the widget\n        self.xoff              = 0\n        # The vertical offset where we start drawing withing the widget\n        self.yoff              = 0\n        # A gap that we  leave around the image as little border\n        self.bordergap         = 20\n        # The scale that was used, ie\n        # self.w = self.scale * self.image.width()\n        # self.h = self.scale * self.image.height()\n        self.scale             = 1.0\n        # Filenames of all images in current city\n        self.images            = []\n        # Image extension\n        self.imageExt          = ""_leftImg8bit.png""\n        # Ground truth extension\n        self.gtExt             = ""_gt*_polygons.json""\n        # Current image as QImage\n        self.image             = QtGui.QImage()\n        # Index of the current image within the city folder\n        self.idx               = 0\n        # All annotated objects in current image, i.e. list of labelObject\n        self.annotation        = []\n        # The current object the mouse points to. It\'s index in self.labels\n        self.mouseObj          = -1\n        # The object that is highlighted and its label. An object instance\n        self.highlightObj      = None\n        self.highlightObjLabel = None\n        # The position of the mouse\n        self.mousePosOrig      = None\n        # The position of the mouse scaled to label coordinates\n        self.mousePosScaled    = None\n        # If the mouse is outside of the image\n        self.mouseOutsideImage = True\n        # The position of the mouse upon enabling the zoom window\n        self.mousePosOnZoom    = None\n        # A list of toolbar actions that need an image\n        self.actImage          = []\n        # A list of toolbar actions that need an image that is not the first\n        self.actImageNotFirst  = []\n        # A list of toolbar actions that need an image that is not the last\n        self.actImageNotLast   = []\n        # Toggle status of the play icon\n        self.playState         = False\n        # Enable disparity visu in general\n        self.enableDisparity   = True\n        # Show disparities instead of labels\n        self.showDisparity     = False\n        # The filename of the disparity map we currently working on\n        self.currentDispFile   = """"\n        # The disparity image\n        self.dispImg           = None\n        # As overlay\n        self.dispOverlay       = None\n        # The disparity search path\n        self.dispPath          = None\n        # Disparity extension\n        self.dispExt           = ""_disparity.png""\n        # Generate colormap\n        try:\n            norm = matplotlib.colors.Normalize(vmin=3,vmax=100)\n            cmap = matplotlib.cm.plasma\n            self.colormap = matplotlib.cm.ScalarMappable( norm=norm , cmap=cmap )\n        except:\n            self.enableDisparity = False\n        # check if pillow was imported, otherwise no disparity visu possible\n        if not \'PILLOW_VERSION\' in globals():\n            self.enableDisparity = False\n\n        # Default label\n        self.defaultLabel = \'static\'\n        if self.defaultLabel not in name2label:\n            print(\'The {0} label is missing in the internal label definitions.\'.format(self.defaultLabel))\n            return\n        # Last selected label\n        self.lastLabel = self.defaultLabel\n\n        # Setup the GUI\n        self.initUI()\n\n        # If we already know a city from the saved config -> load it\n        self.loadCity()\n        self.imageChanged()\n\n    # Destructor\n    def __del__(self):\n        return\n\n    # Construct everything GUI related. Called by constructor\n    def initUI(self):\n        # Create a toolbar\n        self.toolbar = self.addToolBar(\'Tools\')\n\n        # Add the tool buttons\n        iconDir = os.path.join( os.path.dirname(sys.argv[0]) , \'icons\' )\n\n        # Loading a new city\n        loadAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'open.png\' )), \'&Tools\', self)\n        loadAction.setShortcuts([\'o\'])\n        self.setTip( loadAction, \'Open city\' )\n        loadAction.triggered.connect( self.getCityFromUser )\n        self.toolbar.addAction(loadAction)\n\n        # Open previous image\n        backAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'back.png\')), \'&Tools\', self)\n        backAction.setShortcut(\'left\')\n        backAction.setStatusTip(\'Previous image\')\n        backAction.triggered.connect( self.prevImage )\n        self.toolbar.addAction(backAction)\n        self.actImageNotFirst.append(backAction)\n\n        # Open next image\n        nextAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'next.png\')), \'&Tools\', self)\n        nextAction.setShortcut(\'right\')\n        self.setTip( nextAction, \'Next image\' )\n        nextAction.triggered.connect( self.nextImage )\n        self.toolbar.addAction(nextAction)\n        self.actImageNotLast.append(nextAction)\n\n        # Play\n        playAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'play.png\')), \'&Tools\', self)\n        playAction.setShortcut(\' \')\n        playAction.setCheckable(True)\n        playAction.setChecked(False)\n        self.setTip( playAction, \'Play all images\' )\n        playAction.triggered.connect( self.playImages )\n        self.toolbar.addAction(playAction)\n        self.actImageNotLast.append(playAction)\n        self.playAction = playAction\n\n        # Select image\n        selImageAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'shuffle.png\' )), \'&Tools\', self)\n        selImageAction.setShortcut(\'i\')\n        self.setTip( selImageAction, \'Select image\' )\n        selImageAction.triggered.connect( self.selectImage )\n        self.toolbar.addAction(selImageAction)\n        self.actImage.append(selImageAction)\n\n        # Enable/disable disparity visu. Toggle button\n        if self.enableDisparity:\n            dispAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'disp.png\' )), \'&Tools\', self)\n            dispAction.setShortcuts([\'d\'])\n            dispAction.setCheckable(True)\n            dispAction.setChecked(self.showDisparity)\n            self.setTip( dispAction, \'Enable/disable depth visualization\' )\n            dispAction.toggled.connect( self.dispToggle )\n            self.toolbar.addAction(dispAction)\n            self.actImage.append(dispAction)\n\n        # Enable/disable zoom. Toggle button\n        zoomAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'zoom.png\' )), \'&Tools\', self)\n        zoomAction.setShortcuts([\'z\'])\n        zoomAction.setCheckable(True)\n        zoomAction.setChecked(self.zoom)\n        self.setTip( zoomAction, \'Enable/disable permanent zoom\' )\n        zoomAction.toggled.connect( self.zoomToggle )\n        self.toolbar.addAction(zoomAction)\n        self.actImage.append(zoomAction)\n\n        # Decrease transparency\n        minusAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'minus.png\' )), \'&Tools\', self)\n        minusAction.setShortcut(\'-\')\n        self.setTip( minusAction, \'Decrease transparency\' )\n        minusAction.triggered.connect( self.minus )\n        self.toolbar.addAction(minusAction)\n\n        # Increase transparency\n        plusAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'plus.png\' )), \'&Tools\', self)\n        plusAction.setShortcut(\'+\')\n        self.setTip( plusAction, \'Increase transparency\' )\n        plusAction.triggered.connect( self.plus )\n        self.toolbar.addAction(plusAction)\n\n        # Display path to current image in message bar\n        displayFilepathAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'filepath.png\' )), \'&Tools\', self)\n        displayFilepathAction.setShortcut(\'f\')\n        self.setTip( displayFilepathAction, \'Show path to current image\' )\n        displayFilepathAction.triggered.connect( self.displayFilepath )\n        self.toolbar.addAction(displayFilepathAction)\n\n        # Display help message\n        helpAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'help19.png\' )), \'&Tools\', self)\n        helpAction.setShortcut(\'h\')\n        self.setTip( helpAction, \'Help\' )\n        helpAction.triggered.connect( self.displayHelpMessage )\n        self.toolbar.addAction(helpAction)\n\n        # Close the application\n        exitAction = QtGui.QAction(QtGui.QIcon( os.path.join( iconDir , \'exit.png\' )), \'&Tools\', self)\n        exitAction.setShortcuts([\'Esc\'])\n        self.setTip( exitAction, \'Exit\' )\n        exitAction.triggered.connect( self.close )\n        self.toolbar.addAction(exitAction)\n\n        # The default text for the status bar\n        self.defaultStatusbar = \'Ready\'\n        # Create a statusbar. Init with default\n        self.statusBar().showMessage( self.defaultStatusbar )\n\n        # Enable mouse move events\n        self.setMouseTracking(True)\n        self.toolbar.setMouseTracking(True)\n        # Open in full screen\n        self.showFullScreen( )\n        # Set a title\n        self.applicationTitle = \'Cityscapes Viewer v1.0\'\n        self.setWindowTitle(self.applicationTitle)\n        self.displayHelpMessage()\n        self.getCityFromUser()\n        # And show the application\n        self.show()\n\n    #############################\n    ## Toolbar call-backs\n    #############################\n\n    # Switch to previous image in file list\n    # Load the image\n    # Load its labels\n    # Update the mouse selection\n    # View\n    def prevImage(self):\n        if not self.images:\n            return\n        if self.idx > 0:\n            self.idx -= 1\n            self.imageChanged()\n        else:\n            message = ""Already at the first image""\n            self.statusBar().showMessage(message)\n        return\n\n    # Switch to next image in file list\n    # Load the image\n    # Load its labels\n    # Update the mouse selection\n    # View\n    def nextImage(self):\n        if not self.images:\n            return\n        if self.idx < len(self.images)-1:\n            self.idx += 1\n            self.imageChanged()\n        elif self.playState:\n            self.playState = False\n            self.playAction.setChecked(False)\n        else:\n            message = ""Already at the last image""\n            self.statusBar().showMessage(message)\n        if self.playState:\n            QtCore.QTimer.singleShot(0, self.nextImage)\n        return\n\n    # Play images, i.e. auto-switch to next image\n    def playImages(self, status):\n        self.playState = status\n        if self.playState:\n            QtCore.QTimer.singleShot(0, self.nextImage)\n\n\n    # Switch to a selected image of the file list\n    # Ask the user for an image\n    # Load the image\n    # Load its labels\n    # Update the mouse selection\n    # View\n    def selectImage(self):\n        if not self.images:\n            return\n\n        dlgTitle = ""Select image to load""\n        self.statusBar().showMessage(dlgTitle)\n        items = QtCore.QStringList( [ os.path.basename(i) for i in self.images ] )\n        (item, ok) = QtGui.QInputDialog.getItem(self, dlgTitle, ""Image"", items, self.idx, False)\n        if (ok and item):\n            idx = items.indexOf(item)\n            if idx != self.idx:\n                self.idx = idx\n                self.imageChanged()\n        else:\n            # Restore the message\n            self.statusBar().showMessage( self.defaultStatusbar )\n\n\n    # Toggle zoom\n    def zoomToggle(self, status):\n        self.zoom = status\n        if status :\n            self.mousePosOnZoom = self.mousePosOrig\n        self.update()\n\n    # Toggle disparity visu\n    def dispToggle(self, status):\n        self.showDisparity = status\n        self.imageChanged()\n\n\n    # Increase label transparency\n    def minus(self):\n        self.transp = max(self.transp-0.1,0.0)\n        self.update()\n\n\n    def displayFilepath(self):\n        self.statusBar().showMessage(""Current image: {0}"".format( self.currentFile ))\n        self.update()\n\n    def displayHelpMessage(self):\n\n        message = self.applicationTitle + ""\\n\\n""\n        message += ""INSTRUCTIONS\\n""\n        message += "" - select a city from drop-down menu\\n""\n        message += "" - browse images and labels using\\n""\n        message += ""   the toolbar buttons or the controls below\\n""\n        message += ""\\n""\n        message += ""CONTROLS\\n""\n        message += "" - select city [o]\\n""\n        message += "" - highlight objects [move mouse]\\n""\n        message += "" - next image [left arrow]\\n""\n        message += "" - previous image [right arrow]\\n""\n        message += "" - toggle autoplay [space]\\n""\n        message += "" - increase/decrease label transparency\\n""\n        message += ""   [ctrl+mousewheel] or [+ / -]\\n""\n        if self.enableDisparity:\n            message += "" - show disparity/depth overlay (if available) [d]\\n""\n\n        message += "" - open zoom window [z]\\n""\n        message += ""       zoom in/out [mousewheel]\\n""\n        message += ""       enlarge/shrink zoom window [shift+mousewheel]\\n""\n        message += "" - select a specific image [i]\\n""\n        message += "" - show path to image below [f]\\n""\n        message += "" - exit viewer [esc]\\n""\n\n        QtGui.QMessageBox.about(self, ""HELP!"", message)\n        self.update()\n\n\n    # Decrease label transparency\n    def plus(self):\n        self.transp = min(self.transp+0.1,1.0)\n        self.update()\n\n    # Close the application\n    def closeEvent(self,event):\n         event.accept()\n\n\n    #############################\n    ## Custom events\n    #############################\n\n    def imageChanged(self):\n        # Load the first image\n        self.loadImage()\n        # Load its labels if available\n        self.loadLabels()\n        # Load disparities if available\n        self.loadDisparities()\n        # Update the object the mouse points to\n        self.updateMouseObject()\n        # Update the GUI\n        self.update()\n\n    #############################\n    ## File I/O\n    #############################\n\n    # Load the currently selected city if possible\n    def loadCity(self):\n        # Search for all *.pngs to get the image list\n        self.images = []\n        if os.path.isdir(self.city):\n            self.images = glob.glob( os.path.join( self.city , \'*\' + self.imageExt ) )\n            self.images.sort()\n            if self.currentFile in self.images:\n                self.idx = self.images.index(self.currentFile)\n            else:\n                self.idx = 0\n\n    # Load the currently selected image\n    # Does only load if not previously loaded\n    # Does not refresh the GUI\n    def loadImage(self):\n        success = False\n        message = self.defaultStatusbar\n        if self.images:\n            filename = self.images[self.idx]\n            filename = os.path.normpath( filename )\n            if not self.image.isNull() and filename == self.currentFile:\n                success = True\n            else:\n                self.image = QtGui.QImage(filename)\n                if self.image.isNull():\n                    message = ""Failed to read image: {0}"".format( filename )\n                else:\n                    message = ""Read image: {0}"".format( filename )\n                    self.currentFile = filename\n                    success = True\n\n        # Update toolbar actions that need an image\n        for act in self.actImage:\n            act.setEnabled(success)\n        for act in self.actImageNotFirst:\n            act.setEnabled(success and self.idx > 0)\n        for act in self.actImageNotLast:\n            act.setEnabled(success and self.idx < len(self.images)-1)\n\n        self.statusBar().showMessage(message)\n\n    # Load the labels from file\n    # Only loads if they exist\n    # Otherwise the filename is stored and that\'s it\n    def loadLabels(self):\n        filename = self.getLabelFilename()\n        if not filename:\n            self.clearAnnotation()\n            return\n\n        # If we have everything and the filename did not change, then we are good\n        if self.annotation and filename == self.currentLabelFile:\n            return\n\n        # Clear the current labels first\n        self.clearAnnotation()\n\n        try:\n            self.annotation = Annotation()\n            self.annotation.fromJsonFile(filename)\n        except IOError as e:\n            # This is the error if the file does not exist\n            message = ""Error parsing labels in {0}. Message: {1}"".format( filename, e.strerror )\n            self.statusBar().showMessage(message)\n\n        # Remember the filename loaded\n        self.currentLabelFile = filename\n\n        # Remeber the status bar message to restore it later\n        restoreMessage = self.statusBar().currentMessage()\n\n        # Restore the message\n        self.statusBar().showMessage( restoreMessage )\n\n\n    # Load the disparity map from file\n    # Only loads if they exist\n    def loadDisparities(self):\n        if not self.enableDisparity:\n            return\n        if not self.showDisparity:\n            return\n\n        filename = self.getDisparityFilename()\n        if not filename:\n            self.dispImg = None\n            return\n\n        # If we have everything and the filename did not change, then we are good\n        if self.dispImg and filename == self.currentDispFile:\n            return\n\n        # Clear the current labels first\n        self.dispImg = None\n\n        try:\n            self.dispImg = Image.open(filename)\n        except IOError as e:\n            # This is the error if the file does not exist\n            message = ""Error parsing disparities in {0}. Message: {1}"".format( filename, e.strerror )\n            self.statusBar().showMessage(message)\n            self.dispImg = None\n\n        if self.dispImg:\n            dispNp = np.array( self.dispImg )\n            dispNp /= 128\n            dispNp.round()\n            dispNp = np.array( dispNp , dtype=np.uint8 )\n\n            dispQt = QtGui.QImage( dispNp.data , dispNp.shape[1] , dispNp.shape[0] , QtGui.QImage.Format_Indexed8 )\n\n            colortable = []\n            for i in range(256):\n                color = self.colormap.to_rgba(i)\n                colorRgb = ( int(color[0]*255) , int(color[1]*255) , int(color[2]*255) )\n                colortable.append( QtGui.qRgb( *colorRgb ) )\n\n            dispQt.setColorTable( colortable )\n            dispQt = dispQt.convertToFormat( QtGui.QImage.Format_ARGB32_Premultiplied )\n            self.dispOverlay = dispQt\n\n        # Remember the filename loaded\n        self.currentDispFile = filename\n\n        # Remember the status bar message to restore it later\n        restoreMessage = self.statusBar().currentMessage()\n\n        # Restore the message\n        self.statusBar().showMessage( restoreMessage )\n\n    #############################\n    ## Drawing\n    #############################\n\n    # This method is called when redrawing everything\n    # Can be manually triggered by self.update()\n    # Note that there must not be any other self.update within this method\n    # or any methods that are called within\n    def paintEvent(self, event):\n        # Create a QPainter that can perform draw actions within a widget or image\n        qp = QtGui.QPainter()\n        # Begin drawing in the application widget\n        qp.begin(self)\n        # Update scale\n        self.updateScale(qp)\n        # Determine the object ID to highlight\n        self.getHighlightedObject(qp)\n        # Draw the image first\n        self.drawImage(qp)\n\n        if self.enableDisparity and self.showDisparity:\n            # Draw the disparities on top\n            overlay = self.drawDisp(qp)\n        else:\n            # Draw the labels on top\n            overlay = self.drawLabels(qp)\n            # Draw the label name next to the mouse\n            self.drawLabelAtMouse(qp)\n\n        # Draw the zoom\n        self.drawZoom(qp, overlay)\n\n        # Thats all drawing\n        qp.end()\n\n        # Forward the paint event\n        QtGui.QMainWindow.paintEvent(self,event)\n\n    # Update the scaling\n    def updateScale(self, qp):\n        if not self.image.width() or not self.image.height():\n            return\n        # Horizontal offset\n        self.xoff  = self.bordergap\n        # Vertical offset\n        self.yoff  = self.toolbar.height()+self.bordergap\n        # We want to make sure to keep the image aspect ratio and to make it fit within the widget\n        # Without keeping the aspect ratio, each side of the image is scaled (multiplied) with\n        sx = float(qp.device().width()  - 2*self.xoff) / self.image.width()\n        sy = float(qp.device().height() - 2*self.yoff) / self.image.height()\n        # To keep the aspect ratio while making sure it fits, we use the minimum of both scales\n        # Remember the scale for later\n        self.scale = min( sx , sy )\n        # These are then the actual dimensions used\n        self.w     = self.scale * self.image.width()\n        self.h     = self.scale * self.image.height()\n\n    # Determine the highlighted object for drawing\n    def getHighlightedObject(self, qp):\n        # This variable we want to fill\n        self.highlightObj = None\n\n        # Without labels we cannot do so\n        if not self.annotation:\n            return\n\n        # If available its the selected object\n        highlightObjId = -1\n        # If not available but the polygon is empty or closed, its the mouse object\n        if highlightObjId < 0 and not self.mouseOutsideImage:\n            highlightObjId = self.mouseObj\n        # Get the actual object that is highlighted\n        if highlightObjId >= 0:\n            self.highlightObj = self.annotation.objects[highlightObjId]\n            self.highlightObjLabel = self.annotation.objects[highlightObjId].label\n\n    # Draw the image in the given QPainter qp\n    def drawImage(self, qp):\n        # Return if no image available\n        if self.image.isNull():\n            return\n\n        # Save the painters current setting to a stack\n        qp.save()\n        # Draw the image\n        qp.drawImage(QtCore.QRect( self.xoff, self.yoff, self.w, self.h ), self.image)\n        # Restore the saved setting from the stack\n        qp.restore()\n\n    def getPolygon(self, obj):\n        poly = QtGui.QPolygonF()\n        for pt in obj.polygon:\n            point = QtCore.QPointF(pt.x,pt.y)\n            poly.append( point )\n        return poly\n\n    # Draw the labels in the given QPainter qp\n    # optionally provide a list of labels to ignore\n    def drawLabels(self, qp, ignore = []):\n        if self.image.isNull() or self.w == 0 or self.h == 0:\n            return\n        if not self.annotation:\n            return\n\n        # The overlay is created in the viewing coordinates\n        # This way, the drawing is more dense and the polygon edges are nicer\n        # We create an image that is the overlay\n        # Within this image we draw using another QPainter\n        # Finally we use the real QPainter to overlay the overlay-image on what is drawn so far\n\n        # The image that is used to draw the overlays\n        overlay = QtGui.QImage( self.w, self.h, QtGui.QImage.Format_ARGB32_Premultiplied )\n        # Fill the image with the default color\n        defaultLabel = name2label[self.defaultLabel]\n        col = QtGui.QColor( *defaultLabel.color )\n        overlay.fill( col )\n        # Create a new QPainter that draws in the overlay image\n        qp2 = QtGui.QPainter()\n        qp2.begin(overlay)\n\n        # The color of the outlines\n        qp2.setPen(QtGui.QColor(\'white\'))\n        # Draw all objects\n        for obj in self.annotation.objects:\n\n            # The label of the object\n            name      = assureSingleInstanceName( obj.label )\n            # If we do not know a color for this label, warn the user\n            if name not in name2label:\n                print(""The annotations contain unkown labels. This should not happen. Please inform the datasets authors. Thank you!"")\n                print(""Details: label \'{}\', file \'{}\'"".format(name,self.currentLabelFile))\n                continue\n\n            poly = self.getPolygon(obj)\n\n            # Scale the polygon properly\n            polyToDraw = poly * QtGui.QTransform.fromScale(self.scale,self.scale)\n\n            # Default drawing\n            # Color from color table, solid brush\n            col   = QtGui.QColor( *name2label[name].color     )\n            brush = QtGui.QBrush( col, QtCore.Qt.SolidPattern )\n            qp2.setBrush(brush)\n            # Overwrite drawing if this is the highlighted object\n            if self.highlightObj and obj == self.highlightObj:\n                # First clear everything below of the polygon\n                qp2.setCompositionMode( QtGui.QPainter.CompositionMode_Clear )\n                qp2.drawPolygon( polyToDraw )\n                qp2.setCompositionMode( QtGui.QPainter.CompositionMode_SourceOver )\n                # Set the drawing to a special pattern\n                brush = QtGui.QBrush(col,QtCore.Qt.DiagCrossPattern)\n                qp2.setBrush(brush)\n\n            qp2.drawPolygon( polyToDraw )\n\n        # Draw outline of selected object dotted\n        if self.highlightObj:\n            brush = QtGui.QBrush(QtCore.Qt.NoBrush)\n            qp2.setBrush(brush)\n            qp2.setPen(QtCore.Qt.DashLine)\n            polyToDraw = self.getPolygon(self.highlightObj) * QtGui.QTransform.fromScale(self.scale,self.scale)\n            qp2.drawPolygon( polyToDraw )\n\n        # End the drawing of the overlay\n        qp2.end()\n        # Save QPainter settings to stack\n        qp.save()\n        # Define transparency\n        qp.setOpacity(self.transp)\n        # Draw the overlay image\n        qp.drawImage(self.xoff,self.yoff,overlay)\n        # Restore settings\n        qp.restore()\n\n        return overlay\n\n\n    # Draw the label name next to the mouse\n    def drawLabelAtMouse(self, qp):\n        # Nothing to do without a highlighted object\n        if not self.highlightObj:\n            return\n        # Nothing to without a mouse position\n        if not self.mousePosOrig:\n            return\n\n        # Save QPainter settings to stack\n        qp.save()\n\n        # That is the mouse positiong\n        mouse = self.mousePosOrig\n\n        # Will show zoom\n        showZoom = self.zoom and not self.image.isNull() and self.w and self.h\n\n        # The text that is written next to the mouse\n        mouseText = self.highlightObj.label\n\n        # Where to write the text\n        # Depends on the zoom (additional offset to mouse to make space for zoom?)\n        # The location in the image (if we are at the top we want to write below of the mouse)\n        off = 36\n        if showZoom:\n            off += self.zoomSize/2\n        if mouse.y()-off > self.toolbar.height():\n            top = mouse.y()-off\n            btm = mouse.y()\n            vAlign = QtCore.Qt.AlignTop\n        else:\n            # The height of the cursor\n            if not showZoom:\n                off += 20\n            top = mouse.y()\n            btm = mouse.y()+off\n            vAlign = QtCore.Qt.AlignBottom\n\n        # Here we can draw\n        rect = QtCore.QRect()\n        rect.setTopLeft(QtCore.QPoint(mouse.x()-200,top))\n        rect.setBottomRight(QtCore.QPoint(mouse.x()+200,btm))\n\n        # The color\n        qp.setPen(QtGui.QColor(\'white\'))\n        # The font to use\n        font = QtGui.QFont(""Helvetica"",20,QtGui.QFont.Bold)\n        qp.setFont(font)\n        # Non-transparent\n        qp.setOpacity(1)\n        # Draw the text, horizontally centered\n        qp.drawText(rect,QtCore.Qt.AlignHCenter|vAlign,mouseText)\n        # Restore settings\n        qp.restore()\n\n    # Draw the zoom\n    def drawZoom(self,qp,overlay):\n        # Zoom disabled?\n        if not self.zoom:\n            return\n        # No image\n        if self.image.isNull() or not self.w or not self.h:\n            return\n        # No mouse\n        if not self.mousePosOrig:\n            return\n\n        # Abbrevation for the zoom window size\n        zoomSize = self.zoomSize\n        # Abbrevation for the mouse position\n        mouse = self.mousePosOrig\n\n        # The pixel that is the zoom center\n        pix = self.mousePosScaled\n        # The size of the part of the image that is drawn in the zoom window\n        selSize = zoomSize / ( self.zoomFactor * self.zoomFactor )\n        # The selection window for the image\n        sel  = QtCore.QRectF(pix.x()  -selSize/2 ,pix.y()  -selSize/2 ,selSize,selSize  )\n        # The selection window for the widget\n        view = QtCore.QRectF(mouse.x()-zoomSize/2,mouse.y()-zoomSize/2,zoomSize,zoomSize)\n        if overlay :\n            overlay_scaled = overlay.scaled(self.image.width(), self.image.height())\n        else :\n            overlay_scaled = QtGui.QImage( self.image.width(), self.image.height(), QtGui.QImage.Format_ARGB32_Premultiplied )\n\n        # Show the zoom image\n        qp.save()\n        qp.drawImage(view,self.image,sel)\n        qp.setOpacity(self.transp)\n        qp.drawImage(view,overlay_scaled,sel)\n        qp.restore()\n\n    # Draw disparities\n    def drawDisp( self , qp ):\n        if not self.dispOverlay:\n            return\n\n        # Save QPainter settings to stack\n        qp.save()\n        # Define transparency\n        qp.setOpacity(self.transp)\n        # Draw the overlay image\n        qp.drawImage(QtCore.QRect( self.xoff, self.yoff, self.w, self.h ),self.dispOverlay)\n        # Restore settings\n        qp.restore()\n\n        return self.dispOverlay\n\n\n\n    #############################\n    ## Mouse/keyboard events\n    #############################\n\n    # Mouse moved\n    # Need to save the mouse position\n    # Need to drag a polygon point\n    # Need to update the mouse selected object\n    def mouseMoveEvent(self,event):\n        if self.image.isNull() or self.w == 0 or self.h == 0:\n            return\n\n        mousePosOrig = QtCore.QPointF( event.x() , event.y() )\n        mousePosScaled = QtCore.QPointF( float(mousePosOrig.x() - self.xoff) / self.scale , float(mousePosOrig.y() - self.yoff) / self.scale )\n        mouseOutsideImage = not self.image.rect().contains( mousePosScaled.toPoint() )\n\n        mousePosScaled.setX( max( mousePosScaled.x() , 0. ) )\n        mousePosScaled.setY( max( mousePosScaled.y() , 0. ) )\n        mousePosScaled.setX( min( mousePosScaled.x() , self.image.rect().right() ) )\n        mousePosScaled.setY( min( mousePosScaled.y() , self.image.rect().bottom() ) )\n\n        if not self.image.rect().contains( mousePosScaled.toPoint() ):\n            print(self.image.rect())\n            print(mousePosScaled.toPoint())\n            self.mousePosScaled = None\n            self.mousePosOrig = None\n            self.updateMouseObject()\n            self.update()\n            return\n\n        self.mousePosScaled    = mousePosScaled\n        self.mousePosOrig      = mousePosOrig\n        self.mouseOutsideImage = mouseOutsideImage\n\n        # Redraw\n        self.updateMouseObject()\n        self.update()\n\n    # Mouse left the widget\n    def leaveEvent(self, event):\n        self.mousePosOrig = None\n        self.mousePosScaled = None\n        self.mouseOutsideImage = True\n\n\n    # Mouse wheel scrolled\n    def wheelEvent(self, event):\n        ctrlPressed = event.modifiers() & QtCore.Qt.ControlModifier\n\n        deltaDegree = event.delta() / 8 # Rotation in degree\n        deltaSteps  = deltaDegree / 15 # Usually one step on the mouse is 15 degrees\n\n        if ctrlPressed:\n            self.transp = max(min(self.transp+(deltaSteps*0.1),1.0),0.0)\n            self.update()\n        else:\n            if self.zoom:\n                # If shift is pressed, change zoom window size\n                if event.modifiers() and QtCore.Qt.Key_Shift:\n                    self.zoomSize += deltaSteps * 10\n                    self.zoomSize = max( self.zoomSize, 10   )\n                    self.zoomSize = min( self.zoomSize, 1000 )\n                # Change zoom factor\n                else:\n                    self.zoomFactor += deltaSteps * 0.05\n                    self.zoomFactor = max( self.zoomFactor, 0.1 )\n                    self.zoomFactor = min( self.zoomFactor, 10 )\n                self.update()\n\n\n    #############################\n    ## Little helper methods\n    #############################\n\n    # Helper method that sets tooltip and statustip\n    # Provide an QAction and the tip text\n    # This text is appended with a hotkeys and then assigned\n    def setTip( self, action, tip ):\n        tip += "" (Hotkeys: \'"" + ""\', \'"".join([str(s.toString()) for s in action.shortcuts()]) + ""\')""\n        action.setStatusTip(tip)\n        action.setToolTip(tip)\n\n    # Update the object that is selected by the current mouse curser\n    def updateMouseObject(self):\n        self.mouseObj   = -1\n        if self.mousePosScaled is None:\n            return\n        for idx in reversed(range(len(self.annotation.objects))):\n            obj = self.annotation.objects[idx]\n            if self.getPolygon(obj).containsPoint(self.mousePosScaled, QtCore.Qt.OddEvenFill):\n                self.mouseObj = idx\n                break\n\n    # Clear the current labels\n    def clearAnnotation(self):\n        self.annotation = None\n        self.currentLabelFile = """"\n\n    def getCityFromUser(self):\n        # Reset the status bar to this message when leaving\n        restoreMessage = self.statusBar().currentMessage()\n\n        if \'CITYSCAPES_DATASET\' in os.environ:\n            csPath = os.environ[\'CITYSCAPES_DATASET\']\n        else:\n            csPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),\'..\',\'..\')\n\n        availableCities = []\n        annotations = [ ""gtFine"" , ""gtCoarse"" ]\n        splits      = [ ""train_extra"" , ""train""  , ""val"" , ""test"" ]\n        for gt in annotations:\n            for split in splits:\n                cities = glob.glob(os.path.join(csPath, gt, split, \'*\'))\n                cities.sort()\n                availableCities.extend( [ (split,gt,os.path.basename(c)) for c in cities if os.listdir(c) ] )\n\n        # List of possible labels\n        items = [split + "", "" + gt + "", "" + city for (split,gt,city) in availableCities]\n\n        # Specify title\n        dlgTitle = ""Select new city""\n        message = dlgTitle\n        question = dlgTitle\n        message = ""Select city for viewing""\n        question = ""Which city would you like to view?""\n        self.statusBar().showMessage(message)\n\n        if items:\n            # Create and wait for dialog\n            (item, ok) = QtGui.QInputDialog.getItem(self, dlgTitle, question,\n                                                    items, 0, False)\n\n            # Restore message\n            self.statusBar().showMessage(restoreMessage)\n\n            if ok and item:\n                (split, gt, city) = [str(i) for i in item.split(\', \')]\n                if split == \'test\' and not self.showDisparity:\n                    self.transp = 0.1\n                else:\n                    self.transp = 0.5\n                self.city      = os.path.normpath(os.path.join(csPath, ""leftImg8bit"", split, city))\n                self.labelPath = os.path.normpath(os.path.join(csPath, gt           , split, city))\n                self.dispPath  = os.path.normpath(os.path.join(csPath, ""disparity""  , split, city))\n                self.loadCity()\n                self.imageChanged()\n\n        else:\n\n            warning = """"\n            warning += ""The data was not found. Please:\\n\\n""\n            warning += "" - make sure the scripts folder is in the Cityscapes root folder\\n""\n            warning += ""or\\n""\n            warning += "" - set CITYSCAPES_DATASET to the Cityscapes root folder\\n""\n            warning += ""       e.g. \'export CITYSCAPES_DATASET=<root_path>\'\\n""\n\n            reply = QtGui.QMessageBox.information(self, ""ERROR!"", warning,\n                                                  QtGui.QMessageBox.Ok)\n            if reply == QtGui.QMessageBox.Ok:\n                sys.exit()\n\n        return\n\n    # Determine if the given candidate for a label path makes sense\n    def isLabelPathValid(self, labelPath):\n        return os.path.isdir(labelPath)\n\n    # Get the filename where to load labels\n    # Returns empty string if not possible\n    def getLabelFilename(self):\n        # And we need to have a directory where labels should be searched\n        if not self.labelPath:\n            return """"\n        # Without the name of the current images, there is also nothing we can do\n        if not self.currentFile:\n            return """"\n        # Check if the label directory is valid.\n        if not self.isLabelPathValid(self.labelPath):\n            return """"\n\n        # Generate the filename of the label file\n        filename = os.path.basename(self.currentFile)\n        filename = filename.replace(self.imageExt, self.gtExt)\n        filename = os.path.join(self.labelPath, filename)\n        search   = glob.glob(filename)\n        if not search:\n            return """"\n        filename = os.path.normpath(search[0])\n        return filename\n\n    # Get the filename where to load disparities\n    # Returns empty string if not possible\n    def getDisparityFilename( self ):\n        # And we need to have a directory where disparities should be searched\n        if not self.dispPath:\n            return """"\n        # Without the name of the current images, there is also nothing we can do\n        if not self.currentFile:\n            return """"\n        # Check if the label directory is valid.\n        if not os.path.isdir(self.dispPath):\n            return """"\n\n        # Generate the filename of the label file\n        filename = os.path.basename( self.currentFile )\n        filename = filename.replace( self.imageExt , self.dispExt )\n        filename = os.path.join( self.dispPath , filename )\n        filename = os.path.normpath(filename)\n        return filename\n\n    # Disable the popup menu on right click\n    def createPopupMenu(self):\n        pass\n\n\ndef main():\n\n    app = QtGui.QApplication(sys.argv)\n    tool = CityscapesViewer()\n    sys.exit(app.exec_())\n\n\nif __name__ == \'__main__\':\n    main()\n'"
