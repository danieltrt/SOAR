file_path,api_count,code
checkpoint/__init__.py,0,b''
datasets/__init__.py,0,b''
datasets/augmentations.py,0,"b'# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# LightNet++: Boosted Light-weighted Networks for Real-time Semantic Segmentation\n# ---------------------------------------------------------------------------------------------------------------- #\n# Data Augmentations for Semantic Segmentation\n# Adapted from https://github.com/ZijunDeng/pytorch-semantic-segmentation/blob/master/utils/joint_transforms.py\n# ---------------------------------------------------------------------------------------------------------------- #\n# Author: Huijun Liu M.Sc.\n# Date:   10.10.2018\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nimport math\nimport random\nimport numbers\n\nfrom PIL import Image, ImageOps\n\n\nclass Compose(object):\n    def __init__(self, augmentations):\n        self.augmentations = augmentations\n\n    def __call__(self, image, mask):\n        """"""\n\n        :param image: <PIL.Image> Image to be augmented, mode=\'RGB\'\n        :param mask: <PIL.Image> Mask to be augmented, mode=\'L\'\n        :return: image, mask\n        """"""\n        assert image.size == mask.size, ""> The size of Image and Mask mismatch!!!""\n\n        for aug in self.augmentations:\n            image, mask = aug(image, mask)\n        return image, mask\n\n\nclass RandomCrop(object):\n    def __init__(self, size, padding=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, image, mask):\n        if self.padding > 0:\n            image = ImageOps.expand(image, border=self.padding, fill=0)\n            mask = ImageOps.expand(mask, border=self.padding, fill=0)\n\n        assert image.size == mask.size, ""> The size of Image and Mask mismatch!!!""\n\n        w, h = image.size\n        th, tw = self.size\n        if w == tw and h == th:\n            return image, mask\n        if w < tw or h < th:\n            return image.resize((tw, th), Image.BILINEAR), mask.resize((tw, th), Image.NEAREST)\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n        return image.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass CenterCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, image, mask):\n        assert image.size == mask.size, ""> The size of Image and Mask mismatch!!!""\n\n        w, h = image.size\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        return image.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass RandomHorizontallyFlip(object):\n    def __call__(self, image, mask):\n        if random.random() < 0.5:\n            return image.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT)\n        return image, mask\n\n\nclass FreeScale(object):\n    def __init__(self, size):\n        self.size = tuple(reversed(size))  # size: (h, w)\n\n    def __call__(self, image, mask):\n        assert image.size == mask.size, ""> The size of Image and Mask mismatch!!!""\n\n        return image.resize(self.size, Image.BILINEAR), mask.resize(self.size, Image.NEAREST)\n\n\nclass Scale(object):\n    def __init__(self, size):  # size: (h, w)\n        self.size = size\n\n    def __call__(self, image, mask):\n        assert image.size == mask.size, ""> The size of Image and Mask mismatch!!!""\n\n        w, h = image.size\n        if (w >= h and w == self.size[1]) or (h >= w and h == self.size[0]):\n            return image, mask\n\n        oh, ow = self.size\n        return image.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)\n\n\nclass RandomScale(object):\n    def __init__(self, limit):\n        """"""\n\n        :param limit: <tuple of float> for example: (0.725, 1.25)\n        """"""\n        assert isinstance(limit, tuple), ""> limit must be a tuple, for example: (0.725, 1.25)""\n        self.limit = limit\n\n    def __call__(self, image, mask):\n        scale = random.uniform(self.limit[0], self.limit[1])\n        w = int(scale * image.size[0])\n        h = int(scale * image.size[1])\n\n        return image.resize((w, h), Image.BILINEAR), mask.resize((w, h), Image.NEAREST)\n\n\nclass RandomSizedCrop(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image, mask):\n        assert image.size == mask.size, ""> The size of Image and Mask mismatch!!!""\n\n        for attempt in range(12):\n            area = image.size[0] * image.size[1]\n            target_area = random.uniform(0.45, 1.0) * area\n            aspect_ratio = random.uniform(0.5, 2)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= image.size[0] and h <= image.size[1]:\n                x1 = random.randint(0, image.size[0] - w)\n                y1 = random.randint(0, image.size[1] - h)\n\n                image = image.crop((x1, y1, x1 + w, y1 + h))\n                mask = mask.crop((x1, y1, x1 + w, y1 + h))\n                assert (image.size == (w, h))\n\n                return image.resize((self.size, self.size), Image.BILINEAR), \\\n                       mask.resize((self.size, self.size), Image.NEAREST)\n\n        # Fallback\n        scale = Scale(self.size)\n        crop = CenterCrop(self.size)\n        return crop(*scale(image, mask))\n\n\nclass RandomRotate(object):\n    def __init__(self, degree):\n        self.degree = degree\n\n    def __call__(self, image, mask):\n        rotate_degree = random.random() * 2 * self.degree - self.degree\n        return image.rotate(rotate_degree, Image.BILINEAR), mask.rotate(rotate_degree, Image.NEAREST)\n\n'"
deploy/__init__.py,0,b''
deploy/weight_release.py,4,"b'import torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport numpy as np\nimport torch\nimport time\nimport os\n\ntry:\n    from apex.fp16_utils import *\n    from apex import amp\n    import apex\nexcept ImportError:\n    raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to run this example."")\n\n\nif __name__ == ""__main__"":\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""3""\n    log_path = ""/home/huijun/TrainLog""\n\n    dataset = ""cityscapes""\n    method = ""shufflenetv2plus_x1.0""  # shufflenetv2plus_x1.0 shufflenetv2plus_x0.5  mobilenetv2plus\n    checkpoint_path = ""{}/weights/{}_{}_best_model.pkl"".format(log_path, dataset, method)\n\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Loading Original Checkpoint..."")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    checkpoint = torch.load(checkpoint_path)\n    beat_iou = checkpoint[\'best_iou\']\n    checkpoint = checkpoint[\'model_state\']\n\n    mean = np.array([0.2997, 0.3402, 0.3072])\n    std = np.array([0.1380, 0.1579, 0.1803])\n\n    state = {""model_state"": checkpoint,\n             ""mean"": mean,\n             ""std"": std}\n\n    save_path = ""{}/release/{}_{}_{:.3f}.pkl"".format(log_path, dataset, method, beat_iou)\n    torch.save(state, save_path)\n    print(""> 2. Checkpoint released!!!"")\n    print(""> # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n'"
models/__init__.py,0,b''
models/mixnetseg.py,20,"b'# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# LightNet++: Boosted Light-weighted Networks for Real-time Semantic Segmentation\n# ---------------------------------------------------------------------------------------------------------------- #\n# PyTorch implementation for MixNetSeg\n# class:\n#       > Swish\n#       > SEBlock\n#       > GPConv\n#       > MDConv\n#       > MixDepthBlock\n#       > MixNetSeg(S, M, L)\n# ---------------------------------------------------------------------------------------------------------------- #\n# Author: Huijun Liu M.Sc.\n# Date:   15.02.2020\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nfrom torch.nn import functional as F\nfrom collections import OrderedDict\nfrom kornia import gaussian_blur2d\nfrom torch import nn\nimport torch\nimport math\n\n\ndef channel_shuffle(x, groups):\n    batch_size, num_channels, height, width = x.size()\n\n    channels_per_group = num_channels // groups\n\n    # 1. Reshape\n    x = x.view(batch_size, groups, channels_per_group, height, width)\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # 2. Flatten\n    x = x.view(batch_size, -1, height, width)\n\n    return x\n\n\ndef usm(x, kernel_size=(7, 7), amount=1.0, threshold=0):\n    res = x.clone()\n\n    blurred = gaussian_blur2d(x, kernel_size=kernel_size, sigma=(1.0, 1.0))\n    sharpened = res * (amount + 1.0) - amount * blurred\n\n    if threshold > 0:\n        sharpened = torch.where(torch.abs(res - blurred) < threshold, sharpened, res)\n\n    return sharpened\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Swish: Swish Activation Function\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass Swish(nn.Module):\n    def __init__(self, inplace=True):\n        super(Swish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return x.mul_(x.sigmoid()) if self.inplace else x.mul(x.sigmoid())\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# SEBlock: Squeeze & Excitation (SCSE)\n#          namely, Channel-wise Attention\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass SEBlock(nn.Module):\n    def __init__(self, in_planes, reduced_dim, act_type=""relu""):\n        super(SEBlock, self).__init__()\n        self.channel_se = nn.Sequential(OrderedDict([\n            (""linear1"", nn.Conv2d(in_planes, reduced_dim, kernel_size=1, stride=1, padding=0, bias=True)),\n            (""act"", Swish(inplace=True) if act_type == ""swish"" else nn.LeakyReLU(inplace=True, negative_slope=0.01)),\n            (""linear2"", nn.Conv2d(reduced_dim, in_planes, kernel_size=1, stride=1, padding=0, bias=True))\n        ]))\n\n    def forward(self, x):\n        x_se = torch.sigmoid(self.channel_se(F.adaptive_avg_pool2d(x, output_size=(1, 1))))\n        return torch.mul(x, x_se)\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1,\n                 groups=1, dilate=1, act_type=""relu""):\n        super(ConvBlock, self).__init__()\n        assert stride in [1, 2]\n        dilate = 1 if stride > 1 else dilate\n        padding = ((kernel_size - 1) // 2) * dilate\n\n        self.conv_block = nn.Sequential(OrderedDict([\n            (""conv"", nn.Conv2d(in_channels=in_planes, out_channels=out_planes,\n                               kernel_size=kernel_size, stride=stride, padding=padding,\n                               dilation=dilate, groups=groups, bias=False)),\n            (""norm"", nn.BatchNorm2d(num_features=out_planes,\n                                    eps=1e-3, momentum=0.01)),\n            (""act"", Swish(inplace=True) if act_type == ""swish"" else nn.LeakyReLU(inplace=True, negative_slope=0.01))\n        ]))\n\n    def forward(self, x):\n        return self.conv_block(x)\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# GPConv: Grouped Point-wise Convolution for MixDepthBlock\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass GPConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_sizes):\n        super(GPConv, self).__init__()\n        self.num_groups = len(kernel_sizes)\n        assert in_planes % self.num_groups == 0\n        sub_in_dim = in_planes // self.num_groups\n        sub_out_dim = out_planes // self.num_groups\n\n        self.group_point_wise = nn.ModuleList()\n        for _ in kernel_sizes:\n            self.group_point_wise.append(nn.Conv2d(sub_in_dim, sub_out_dim,\n                                                   kernel_size=1, stride=1, padding=0,\n                                                   groups=1, dilation=1, bias=False))\n\n    def forward(self, x):\n        if self.num_groups == 1:\n            return self.group_point_wise[0](x)\n\n        chunks = torch.chunk(x, chunks=self.num_groups, dim=1)\n        mix = [self.group_point_wise[stream](chunks[stream]) for stream in range(self.num_groups)]\n        return torch.cat(mix, dim=1)\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# MDConv: Mixed Depth-wise Convolution for MixDepthBlock\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass MDConv(nn.Module):\n    def __init__(self, in_planes, kernel_sizes, stride=1, dilate=1):\n        super(MDConv, self).__init__()\n        self.num_groups = len(kernel_sizes)\n        assert in_planes % self.num_groups == 0\n        sub_hidden_dim = in_planes // self.num_groups\n\n        assert stride in [1, 2]\n        dilate = 1 if stride > 1 else dilate\n\n        self.mixed_depth_wise = nn.ModuleList()\n        for kernel_size in kernel_sizes:\n            padding = ((kernel_size - 1) // 2) * dilate\n            self.mixed_depth_wise.append(nn.Conv2d(sub_hidden_dim, sub_hidden_dim,\n                                                   kernel_size=kernel_size, stride=stride, padding=padding,\n                                                   groups=sub_hidden_dim, dilation=dilate, bias=False))\n\n    def forward(self, x):\n        if self.num_groups == 1:\n            return self.mixed_depth_wise[0](x)\n\n        chunks = torch.chunk(x, chunks=self.num_groups, dim=1)\n        mix = [self.mixed_depth_wise[stream](chunks[stream]) for stream in range(self.num_groups)]\n        return torch.cat(mix, dim=1)\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# MixDepthBlock: MixDepthBlock for MixNetSeg\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass MixDepthBlock(nn.Module):\n    def __init__(self, in_planes, out_planes,\n                 expand_ratio, exp_kernel_sizes, kernel_sizes, poi_kernel_sizes, stride, dilate,\n                 reduction_ratio=4, dropout_rate=0.2, act_type=""swish""):\n        super(MixDepthBlock, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.expand_ratio = expand_ratio\n\n        self.groups = len(kernel_sizes)\n        self.use_se = (reduction_ratio is not None) and (reduction_ratio > 1)\n        self.use_residual = in_planes == out_planes and stride == 1\n\n        assert stride in [1, 2]\n        dilate = 1 if stride > 1 else dilate\n        hidden_dim = in_planes * expand_ratio\n\n        # step 1. Expansion phase/Point-wise convolution\n        if expand_ratio != 1:\n            self.expansion = nn.Sequential(OrderedDict([\n                (""conv"", GPConv(in_planes, hidden_dim, kernel_sizes=exp_kernel_sizes)),\n                (""norm"", nn.BatchNorm2d(hidden_dim, eps=1e-3, momentum=0.01)),\n                (""act"", Swish(inplace=True) if act_type == ""swish"" else nn.LeakyReLU(inplace=True, negative_slope=0.01))\n            ]))\n\n        # step 2. Depth-wise convolution phase\n        self.depth_wise = nn.Sequential(OrderedDict([\n            (""conv"", MDConv(hidden_dim, kernel_sizes=kernel_sizes, stride=stride, dilate=dilate)),\n            (""norm"", nn.BatchNorm2d(hidden_dim, eps=1e-3, momentum=0.01)),\n            (""act"", Swish(inplace=True) if act_type == ""swish"" else nn.LeakyReLU(inplace=True, negative_slope=0.01))\n        ]))\n\n        # step 3. Squeeze and Excitation\n        if self.use_se:\n            reduced_dim = max(1, int(in_planes / reduction_ratio))\n            self.se_block = SEBlock(hidden_dim, reduced_dim, act_type=act_type)\n\n        # step 4. Point-wise convolution phase\n        self.point_wise = nn.Sequential(OrderedDict([\n            (""conv"", GPConv(hidden_dim, out_planes, kernel_sizes=poi_kernel_sizes)),\n            (""norm"", nn.BatchNorm2d(out_planes, eps=1e-3, momentum=0.01))\n        ]))\n\n    def forward(self, x):\n        res = x\n\n        # step 1. Expansion phase/Point-wise convolution\n        if self.expand_ratio != 1:\n            x = self.expansion(x)\n\n        # step 2. Depth-wise convolution phase\n        x = self.depth_wise(x)\n\n        # step 3. Squeeze and Excitation\n        if self.use_se:\n            x = self.se_block(x)\n\n        # step 4. Point-wise convolution phase\n        x = self.point_wise(x)\n\n        # step 5. Skip connection and drop connect\n        if self.use_residual:\n            if self.training and (self.dropout_rate is not None):\n                x = F.dropout2d(input=x, p=self.dropout_rate,\n                                training=self.training, inplace=True)\n            x = x + res\n\n        return x\n\n\nclass DSASPPBlock(nn.Module):\n    def __init__(self, in_chs, out_chs, up_ratio=2, aspp_dilate=(6, 12, 18)):\n        super(DSASPPBlock, self).__init__()\n        self.up_ratio = up_ratio\n\n        self.gave_pool = nn.Sequential(OrderedDict([(""gavg"", nn.AdaptiveAvgPool2d((3, 3))),\n                                                    (""conv1_0"",\n                                                     ConvBlock(in_chs, out_chs, kernel_size=1, stride=1, dilate=1,\n                                                               act_type=""relu""))]))\n\n        self.conv1x1 = ConvBlock(in_chs, out_chs, kernel_size=1, stride=1,\n                                 dilate=1, act_type=""relu"")\n        self.aspp_bra1 = nn.Sequential(\n            OrderedDict([(""conv"", MDConv(in_planes=in_chs, kernel_sizes=[3, 5, 7, 9], stride=1, dilate=aspp_dilate[0])),\n                         (""norm"", nn.BatchNorm2d(in_chs, eps=1e-3, momentum=0.01)),\n                         (""act"", nn.LeakyReLU(inplace=True, negative_slope=0.01))]))\n        self.aspp_bra2 = nn.Sequential(\n            OrderedDict([(""conv"", MDConv(in_planes=in_chs, kernel_sizes=[3, 5, 7, 9], stride=1, dilate=aspp_dilate[1])),\n                         (""norm"", nn.BatchNorm2d(in_chs, eps=1e-3, momentum=0.01)),\n                         (""act"", nn.LeakyReLU(inplace=True, negative_slope=0.01))]))\n        self.aspp_bra3 = nn.Sequential(\n            OrderedDict([(""conv"", MDConv(in_planes=in_chs, kernel_sizes=[3, 5, 7, 9], stride=1, dilate=aspp_dilate[2])),\n                         (""norm"", nn.BatchNorm2d(in_chs, eps=1e-3, momentum=0.01)),\n                         (""act"", nn.LeakyReLU(inplace=True, negative_slope=0.01))]))\n\n        self.aspp_catdown = ConvBlock((3 * in_chs + 2 * out_chs), out_chs,\n                                      kernel_size=1, stride=1, dilate=1, act_type=""relu"")\n\n    def forward(self, x):\n        _, _, feat_h, feat_w = x.size()\n        # ------------------------------------------------- #\n        # 1. Atrous Spacial Pyramid Pooling\n        # ------------------------------------------------- #\n        x = self.aspp_catdown(torch.cat((self.aspp_bra1(x),\n                                         F.interpolate(input=self.gave_pool(x),\n                                                       size=(feat_h, feat_w),\n                                                       mode=""bilinear"",\n                                                       align_corners=True),\n                                         self.aspp_bra2(x),\n                                         self.conv1x1(x),\n                                         self.aspp_bra3(x)), dim=1))\n        # ------------------------------------------------- #\n        # 2. up-sampling the feature-map\n        # ------------------------------------------------- #\n        return F.interpolate(input=x,\n                             size=(int(feat_h * self.up_ratio),\n                                   int(feat_w * self.up_ratio)),\n                             mode=""bilinear"", align_corners=True)\n\n\nclass BiFPNBlock(nn.Module):\n    """"""\n    Bi-directional Feature Pyramid Network\n    """"""\n\n    def __init__(self, feature_size=64, expand_ratio=1, epsilon=0.0001):\n        super(BiFPNBlock, self).__init__()\n        self.epsilon = epsilon\n\n        self.p1_td = MixDepthBlock(feature_size, feature_size, expand_ratio=expand_ratio,\n                                   exp_kernel_sizes=[1], kernel_sizes=[3, 5, 7, 9], poi_kernel_sizes=[1],\n                                   stride=1, dilate=1, reduction_ratio=2, dropout_rate=0.0, act_type=""relu"")\n        self.p2_td = MixDepthBlock(feature_size, feature_size, expand_ratio=expand_ratio,\n                                   exp_kernel_sizes=[1], kernel_sizes=[3, 5, 7, 9], poi_kernel_sizes=[1],\n                                   stride=1, dilate=1, reduction_ratio=2, dropout_rate=0.0, act_type=""relu"")\n        self.p3_td = MixDepthBlock(feature_size, feature_size, expand_ratio=expand_ratio,\n                                   exp_kernel_sizes=[1], kernel_sizes=[3, 5, 7, 9], poi_kernel_sizes=[1],\n                                   stride=1, dilate=1, reduction_ratio=2, dropout_rate=0.0, act_type=""relu"")\n        self.p4_td = MixDepthBlock(feature_size, feature_size, expand_ratio=expand_ratio,\n                                   exp_kernel_sizes=[1], kernel_sizes=[3, 5, 7, 9], poi_kernel_sizes=[1],\n                                   stride=1, dilate=1, reduction_ratio=2, dropout_rate=0.0, act_type=""relu"")\n\n        self.p2_bu = MixDepthBlock(feature_size, feature_size, expand_ratio=expand_ratio,\n                                   exp_kernel_sizes=[1], kernel_sizes=[3, 5, 7, 9], poi_kernel_sizes=[1],\n                                   stride=1, dilate=1, reduction_ratio=2, dropout_rate=0.0, act_type=""relu"")\n        self.p3_bu = MixDepthBlock(feature_size, feature_size, expand_ratio=expand_ratio,\n                                   exp_kernel_sizes=[1], kernel_sizes=[3, 5, 7, 9], poi_kernel_sizes=[1],\n                                   stride=1, dilate=1, reduction_ratio=2, dropout_rate=0.0, act_type=""relu"")\n        self.p4_bu = MixDepthBlock(feature_size, feature_size, expand_ratio=expand_ratio,\n                                   exp_kernel_sizes=[1], kernel_sizes=[3, 5, 7, 9], poi_kernel_sizes=[1],\n                                   stride=1, dilate=1, reduction_ratio=2, dropout_rate=0.0, act_type=""relu"")\n        self.p5_bu = MixDepthBlock(feature_size, feature_size, expand_ratio=expand_ratio,\n                                   exp_kernel_sizes=[1], kernel_sizes=[3, 5, 7, 9], poi_kernel_sizes=[1],\n                                   stride=1, dilate=1, reduction_ratio=2, dropout_rate=0.0, act_type=""relu"")\n\n        self.w1 = nn.Parameter(torch.Tensor(2, 4).fill_(0.5))\n        self.w2 = nn.Parameter(torch.Tensor(3, 4).fill_(0.5))\n\n    def forward(self, inputs):\n        p1_x, p2_x, p3_x, p4_x, p5_x = inputs\n\n        w1 = F.relu(self.w1)\n        w1 /= torch.sum(w1, dim=0) + self.epsilon\n        w2 = F.relu(self.w2)\n        w2 /= torch.sum(w2, dim=0) + self.epsilon\n\n        p5_td = p5_x\n        p4_td = self.p4_td(w1[0, 0] * p4_x + w1[1, 0] * p5_td)\n        p3_td = self.p3_td(w1[0, 1] * p3_x + w1[1, 1] * p4_td)\n        p2_td = self.p2_td(w1[0, 2] * p2_x + w1[1, 2] * p3_td)\n        p1_td = self.p1_td(w1[0, 3] * p1_x + w1[1, 3] * F.interpolate(p2_td, scale_factor=2, mode=""bilinear"", align_corners=True))\n\n        # Calculate Bottom-Up Pathway\n        p1_bu = p1_td\n        p2_bu = self.p2_bu(\n            w2[0, 0] * p2_x + w2[1, 0] * p2_td + w2[2, 0] * F.interpolate(p1_bu, scale_factor=0.5, mode=""bilinear"", align_corners=True))\n        p3_bu = self.p3_bu(w2[0, 1] * p3_x + w2[1, 1] * p3_td + w2[2, 1] * p2_bu)\n        p4_bu = self.p4_bu(w2[0, 2] * p4_x + w2[1, 2] * p4_td + w2[2, 2] * p3_bu)\n        p5_bu = self.p5_bu(w2[0, 3] * p5_x + w2[1, 3] * p5_td + w2[2, 3] * p4_bu)\n\n        return p1_bu, p2_bu, p3_bu, p4_bu, p5_bu\n\n\nclass BiFPNDecoder(nn.Module):\n    def __init__(self, bone_feat_sizes, feature_size=64, expand_ratio=1, fpn_repeats=3):\n        super(BiFPNDecoder, self).__init__()\n        self.p1 = ConvBlock(bone_feat_sizes[0], feature_size, kernel_size=1, stride=1, act_type=""relu"")\n        self.p2 = ConvBlock(bone_feat_sizes[1], feature_size, kernel_size=1, stride=1, act_type=""relu"")\n        self.p3 = ConvBlock(bone_feat_sizes[2], feature_size, kernel_size=1, stride=1, act_type=""relu"")\n        self.p4 = ConvBlock(bone_feat_sizes[3], feature_size, kernel_size=1, stride=1, act_type=""relu"")\n        self.p5 = ConvBlock(bone_feat_sizes[4], feature_size, kernel_size=1, stride=1, act_type=""relu"")\n\n        bifpns_seq = []\n        for bifpn_id in range(fpn_repeats):\n            bifpns_seq.append((""bi_fpn%d"" % (bifpn_id + 1), BiFPNBlock(feature_size=feature_size,\n                                                                       expand_ratio=expand_ratio)))\n        self.bifpns = nn.Sequential(OrderedDict(bifpns_seq))\n\n    def forward(self, feat1, feat2, feat3, feat4, feat5):\n        # Calculate the input column of BiFPNDecoder\n        return self.bifpns([self.p1(feat1), self.p2(feat2), self.p3(feat3), self.p4(feat4), self.p5(feat5)])\n\n\nclass MixNetSeg(nn.Module):\n    def __init__(self, arch=""s"", decoder_feat=64, fpn_repeats=3, num_classes=19):\n        super(MixNetSeg, self).__init__()\n        self.num_classes = num_classes\n        params = {\n            \'s\': (16, [\n                # t, c, n, k, ek, pk, s, d, a, se\n                [1, 16, 1, [3], [1], [1], 1, 1, ""relu"", None],\n\n                [6, 24, 1, [3], [1, 1], [1, 1], 2, 1, ""relu"", None],\n                [3, 24, 1, [3], [1, 1], [1, 1], 1, 1, ""relu"", None],\n\n                [6, 40, 1, [3, 5, 7], [1], [1], 2, 1, ""relu"", 2],\n                [6, 40, 3, [3, 5], [1, 1], [1, 1], 1, 1, ""relu"", 2],\n\n                [6, 80, 1, [3, 5, 7], [1], [1, 1], 1, 2, ""relu"", 4],\n                [6, 80, 2, [3, 5], [1], [1, 1], 1, 2, ""relu"", 4],\n                [6, 120, 1, [3, 5, 7], [1, 1], [1, 1], 1, 3, ""relu"", 2],\n                [3, 120, 2, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, ""relu"", 2],\n\n                [6, 200, 1, [3, 5, 7, 9, 11], [1], [1], 1, 4, ""relu"", 2],\n                [6, 200, 2, [3, 5, 7, 9], [1], [1, 1], 1, 4, ""relu"", 2]\n            ], 1.0, 1.0, 0.2),\n            \'m\': (24, [\n                # t, c, n, k, ek, pk, s, d, a, se\n                [1, 24, 1, [3], [1], [1], 1, 1, ""relu"", None],\n\n                [6, 32, 1, [3, 5, 7], [1, 1], [1, 1], 2, 1, ""relu"", None],\n                [3, 32, 1, [3], [1, 1], [1, 1], 1, 1, ""relu"", None],\n\n                [6, 40, 1, [3, 5, 7, 9], [1], [1], 2, 1, ""relu"", 2],\n                [6, 40, 3, [3, 5], [1, 1], [1, 1], 1, 1, ""relu"", 2],\n\n                [6, 80, 1, [3, 5, 7], [1], [1], 1, 2, ""relu"", 4],\n                [6, 80, 3, [3, 5, 7, 9], [1, 1], [1, 1], 1, 2, ""relu"", 4],\n                [6, 120, 1, [3], [1], [1], 1, 3, ""relu"", 2],\n                [3, 120, 3, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, ""relu"", 2],\n\n                [6, 200, 1, [3, 5, 7, 9], [1], [1], 1, 4, ""relu"", 2],\n                [6, 200, 3, [3, 5, 7, 9], [1], [1, 1], 1, 4, ""relu"", 2]\n            ], 1.0, 1.0, 0.25),\n            \'l\': (24, [\n                # t, c, n, k, ek, pk, s, d, a, se\n                [1, 24, 1, [3], [1], [1], 1, 1, ""relu"", None],\n\n                [6, 32, 1, [3, 5, 7], [1, 1], [1, 1], 2, 1, ""relu"", None],\n                [3, 32, 1, [3], [1, 1], [1, 1], 1, 1, ""relu"", None],\n\n                [6, 40, 1, [3, 5, 7, 9], [1], [1], 2, 1, ""relu"", 2],\n                [6, 40, 3, [3, 5], [1, 1], [1, 1], 1, 1, ""relu"", 2],\n\n                [6, 80, 1, [3, 5, 7], [1], [1], 1, 2, ""relu"", 4],\n                [6, 80, 3, [3, 5, 7, 9], [1, 1], [1, 1], 1, 2, ""relu"", 4],\n                [6, 120, 1, [3], [1], [1], 1, 3, ""relu"", 2],\n                [3, 120, 3, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, ""relu"", 2],\n\n                [6, 200, 1, [3, 5, 7, 9], [1], [1], 1, 4, ""relu"", 2],\n                [6, 200, 3, [3, 5, 7, 9], [1], [1, 1], 1, 4, ""relu"", 2]\n            ], 1.3, 1.0, 0.25),\n        }\n\n        stem_planes, settings, width_multi, depth_multi, self.dropout_rate = params[arch]\n        out_channels = self._round_filters(stem_planes, width_multi)\n        self.mod1 = ConvBlock(3, out_channels, kernel_size=3, stride=2,\n                              groups=1, dilate=1, act_type=""relu"")\n\n        in_channels = out_channels\n        mod_id = 0\n        for t, c, n, k, ek, pk, s, d, a, se in settings:\n            out_channels = self._round_filters(c, width_multi)\n            repeats = self._round_repeats(n, depth_multi)\n\n            # Create blocks for module\n            blocks = []\n            for block_id in range(repeats):\n                stride = s if block_id == 0 else 1\n                dilate = d if stride == 1 else 1\n\n                blocks.append((""block%d"" % (block_id + 1), MixDepthBlock(in_channels, out_channels,\n                                                                         expand_ratio=t, exp_kernel_sizes=ek,\n                                                                         kernel_sizes=k, poi_kernel_sizes=pk,\n                                                                         stride=stride, dilate=dilate,\n                                                                         reduction_ratio=se,\n                                                                         dropout_rate=0.0,\n                                                                         act_type=a)))\n\n                in_channels = out_channels\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n            mod_id += 1\n\n        org_last_planes = (\n                settings[0][1] + settings[2][1] + settings[4][1] + settings[6][1] + settings[8][1] + settings[10][\n            1])\n        last_feat = 256\n        self.feat_fuse = MixDepthBlock(org_last_planes, last_feat,\n                                       expand_ratio=3, exp_kernel_sizes=[1],\n                                       kernel_sizes=[3, 5, 7, 9], poi_kernel_sizes=[1],\n                                       stride=1, dilate=1, reduction_ratio=1, dropout_rate=0.0, act_type=""relu"")\n\n        self.bifpn_decoder = BiFPNDecoder(bone_feat_sizes=[settings[2][1], settings[4][1],\n                                                           settings[6][1], settings[8][1], last_feat],\n                                          feature_size=decoder_feat, expand_ratio=2, fpn_repeats=fpn_repeats)\n\n        self.aux_head = nn.Conv2d(last_feat, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\n        self.cls_head = nn.Conv2d(decoder_feat, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, a=0.10, mode=\'fan_in\', nonlinearity=\'leaky_relu\')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                fan_out = m.weight.size(0)\n                init_range = 1.0 / math.sqrt(fan_out)\n                nn.init.uniform_(m.weight, -init_range, init_range)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    @staticmethod\n    def _make_divisible(value, divisor=8):\n        new_value = max(divisor, int(value + divisor / 2) // divisor * divisor)\n        if new_value < 0.9 * value:\n            new_value += divisor\n        return new_value\n\n    def _round_filters(self, filters, width_multi):\n        if width_multi == 1.0:\n            return filters\n        return int(self._make_divisible(filters * width_multi))\n\n    @staticmethod\n    def _round_repeats(repeats, depth_multi):\n        if depth_multi == 1.0:\n            return repeats\n        return int(math.ceil(depth_multi * repeats))\n\n    @staticmethod\n    def usm(x, kernel_size=(7, 7), amount=1.0, threshold=0):\n        res = x.clone()\n\n        blurred = gaussian_blur2d(x, kernel_size=kernel_size, sigma=(1.0, 1.0))\n        sharpened = res * (amount + 1.0) - amount * blurred\n\n        if threshold > 0:\n            sharpened = torch.where(torch.abs(res - blurred) < threshold, sharpened, res)\n\n        return F.relu(sharpened, inplace=True)\n\n    def forward(self, x):\n        _, _, in_h, in_w = x.size()\n        assert (in_h % 32 == 0 and in_w % 32 == 0), ""> in_size must product of 32!!!""\n\n        feat1 = self.mod2(self.mod1(x))  # (N, C,   H/2, W/2)\n        feat1_1 = F.max_pool2d(input=feat1, kernel_size=3, stride=2, padding=1)\n\n        feat2 = self.mod4(self.mod3(feat1))  # (N, C,   H/4, W/4)\n        feat3 = self.mod6(self.mod5(feat2))  # (N, C,   H/8, W/8) 1\n        feat4 = self.mod8(self.mod7(feat3))  # (N, C,   H/8, W/8) 2\n        feat5 = self.mod10(self.mod9(feat4))  # (N, C,   H/8, W/8) 3\n        feat6 = self.mod12(self.mod11(feat5))  # (N, C,   H/8, W/8) 4\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        feat = self.feat_fuse(torch.cat([feat4, F.max_pool2d(input=feat1_1, kernel_size=3, stride=2, padding=1),\n                                         feat3, feat6, F.max_pool2d(input=feat2, kernel_size=3, stride=2, padding=1),\n                                         feat5], dim=1))\n        feat = feat + F.interpolate(F.adaptive_avg_pool2d(feat, output_size=(3, 3)),\n                                    size=(feat.size(2), feat.size(3)), mode=""bilinear"", align_corners=True)\n        aux_score = self.aux_head(feat)\n\n        # compute contrast feature\n        feat_de2, feat_de3, feat_de4, feat_de5, feat_de = self.bifpn_decoder(feat2, feat3, feat4, feat5, feat)\n        feat_final = feat_de2 + F.interpolate((feat_de3 + feat_de4 + feat_de5 + feat_de),\n                                              scale_factor=2, mode=""bilinear"", align_corners=True)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 3. Classifier: pixel-wise classification-segmentation\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        main_score = self.cls_head(feat_final)\n        main_score = F.interpolate(input=main_score, size=(in_h, in_w), mode=""bilinear"", align_corners=True)\n        aux_score = F.interpolate(input=aux_score, size=(in_h, in_w), mode=""bilinear"", align_corners=True)\n        return aux_score, main_score\n\n\nif __name__ == \'__main__\':\n    import os\n    from torchstat import stat\n    net_h, net_w = 512, 1024\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n    model = MixNetSeg(arch=""s"", decoder_feat=64, fpn_repeats=3, num_classes=19)\n    stat(model, (3, net_h, net_w))\n    # model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n    #\n    # model.eval()\n    # with torch.no_grad():\n    #     while True:\n    #         dummy_in = torch.randn(2, 3, net_h, net_w).cuda()\n    #         start_time = time.time()\n    #         dummy_out = model(dummy_in)\n    #         torch.cuda.synchronize()\n    #         del dummy_out\n    #\n    #         print(""> Inference Time: {}"".format(time.time() - start_time))\n'"
models/mobilenetv2plus.py,8,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn import init\nfrom modules.attentions import PBCSABlock\nfrom modules.aspp import DSASPPInPlaceABNBlock\nfrom modules.inplace_abn.iabn import InPlaceABN, InPlaceABNSync, ABN\nfrom modules.mobile import InvertedResidual, InvertedResidualIABN\nfrom collections import OrderedDict\n\n\nclass MobileNetV2Plus(nn.Module):\n    def __init__(self, num_classes=19,\n                 width_multi=1.0,\n                 fuse_chns=512,\n                 aspp_chns=256,\n                 aspp_dilate=(12, 24, 36),\n                 norm_act=InPlaceABN):\n        """"""\n        MobileNetV2Plus: MobileNetV2 based Semantic Segmentation\n        :param num_classes:    (int)  Number of classes\n        :param width_multi: (float) Network width multiplier\n        :param aspp_chns:    (tuple) Number of the output channels of the ASPP Block\n        :param aspp_dilate:   (tuple) Dilation rates used in ASPP\n        """"""\n        super(MobileNetV2Plus, self).__init__()\n        self.num_classes = num_classes\n\n        # setting of inverted residual blocks\n        self.inverted_residual_setting = [\n            # t, c, n, s, d\n            [1, 16, 1, 1, 1],    # 1/2\n            [6, 24, 2, 2, 1],    # 1/4\n            [6, 32, 3, 2, 1],    # 1/8\n            [6, 64, 4, 1, 2],    # 1/8\n            [6, 96, 3, 1, 4],    # 1/8\n            [6, 160, 3, 1, 8],   # 1/8\n            [6, 320, 1, 1, 16],  # 1/8\n        ]\n\n        # building first layer\n        input_channel = int(32 * width_multi)\n        self.mod1 = nn.Sequential(OrderedDict([(""conv"", nn.Conv2d(3, input_channel,\n                                                                  kernel_size=3, stride=2, padding=1,\n                                                                  bias=False)),\n                                               (""norm"", nn.BatchNorm2d(input_channel)),\n                                               (""act"", nn.LeakyReLU(inplace=True, negative_slope=0.01))]))\n\n        # building inverted residual blocks\n        mod_id = 0\n        for t, c, n, s, d in self.inverted_residual_setting:\n            output_channel = int(c * width_multi)\n\n            # Create blocks for module\n            blocks = []\n            for block_id in range(n):\n                if block_id == 0 and s == 2:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=s,\n                                                                                dilate=1,\n                                                                                expand_ratio=t)))\n                else:\n                    blocks.append((""block%d"" % (block_id + 1), InvertedResidual(inp=input_channel,\n                                                                                oup=output_channel,\n                                                                                stride=1,\n                                                                                dilate=d,\n                                                                                expand_ratio=t)))\n\n                input_channel = output_channel\n\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n            mod_id += 1\n\n        # building last several layers\n        org_last_chns = (self.inverted_residual_setting[0][1] +\n                         self.inverted_residual_setting[1][1] +\n                         self.inverted_residual_setting[2][1] +\n                         self.inverted_residual_setting[3][1] +\n                         self.inverted_residual_setting[4][1] +\n                         self.inverted_residual_setting[5][1] +\n                         self.inverted_residual_setting[6][1])\n\n        feat_chns = int(org_last_chns * width_multi) if width_multi > 1.0 else org_last_chns\n        self.feat_fuse = InvertedResidualIABN(inp=feat_chns, oup=fuse_chns, stride=1, dilate=1,\n                                              expand_ratio=1, norm_act=norm_act)\n\n        self.pyramid_pool = DSASPPInPlaceABNBlock(fuse_chns, aspp_chns, aspp_dilate=aspp_dilate, norm_act=norm_act)\n\n        feat_chns = aspp_chns + self.inverted_residual_setting[1][1] + self.inverted_residual_setting[0][1]\n        self.final_fuse = InvertedResidualIABN(inp=feat_chns, oup=aspp_chns, stride=1, dilate=1,\n                                               expand_ratio=1, norm_act=norm_act)\n\n        self.aspp_scse = PBCSABlock(in_chns=aspp_chns, reduct_ratio=16, dilation=16, use_res=True)\n        self.score = nn.Sequential(OrderedDict([(""dropout"", nn.Dropout2d(0.175)),\n                                                (""norm"", norm_act(aspp_chns)),\n                                                (""conv"", nn.Conv2d(aspp_chns, self.num_classes,\n                                                                   kernel_size=1, stride=1,\n                                                                   padding=0, bias=True))]))\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, a=0.10, mode=\'fan_in\', nonlinearity=\'leaky_relu\')\n                if hasattr(m, ""bias"") and m.bias is not None:\n                    init.constant_(m.bias, 0.0)\n\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, ABN) or \\\n                    isinstance(m, InPlaceABN) or isinstance(m, InPlaceABNSync):\n                init.normal_(m.weight, 1.0, 0.0256)\n                init.constant_(m.bias, 0.)\n            elif isinstance(m, nn.Linear):\n                init.xavier_uniform_(m.weight, .1)\n                init.constant_(m.bias, 0.)\n\n    def train(self, mode=True, freeze_bn=False, freeze_bn_affine=False):\n        super(MobileNetV2Plus, self).train()\n        # if freeze_bn:\n        #     print(""> Freezing Mean/Var of BatchNorm2D."")\n        #     if freeze_bn_affine:\n        #         print(""> Freezing Weight/Bias of BatchNorm2D."")\n        if freeze_bn:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n                    if freeze_bn_affine:\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n\n    def forward(self, x):\n        _, _, in_h, in_w = x.size()\n        assert (in_h % 8 == 0 and in_w % 8 == 0), ""> in_size must product of 8!!!""\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        stg1 = self.mod1(x)     # (N, 32,   H/2, W/2)  1/2\n        stg1 = self.mod2(stg1)  # (N, 16,   H/2, W/2)  1/2 -> 1/4 -> 1/8\n        stg1_1 = F.max_pool2d(input=stg1, kernel_size=3, stride=2, padding=1)  # 1/4\n        stg1_2 = F.max_pool2d(input=stg1_1, kernel_size=3, stride=2, padding=1)  # 1/8\n\n        stg2 = self.mod3(stg1)  # (N, 24,   H/4, W/4)  1/4 -> 1/8\n        stg2_1 = F.max_pool2d(input=stg2, kernel_size=3, stride=2, padding=1)  # 1/8\n\n        stg3 = self.mod4(stg2)  # (N, 32,   H/8, W/8)  1/8\n        stg4 = self.mod5(stg3)  # (N, 64,   H/8, W/8)  1/8 dilation=2\n        stg5 = self.mod6(stg4)  # (N, 96,   H/8, W/8)  1/8 dilation=4\n        stg6 = self.mod7(stg5)  # (N, 160,  H/8, W/8)  1/8 dilation=8\n        stg7 = self.mod8(stg6)  # (N, 320,  H/8, W/8)  1/8 dilation=16\n\n        # (N, 712, 56,  112)  1/8  (16+24+32+64+96+160+320)\n        feat = self.feat_fuse(torch.cat((stg3, stg4, stg5, stg6, stg7, stg1_2, stg2_1), dim=1))\n\n        # dsn = None\n        # if self.training:\n        #     dsn = self.dsn(feat)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # (N, 256+24+16=296, H/4, W/4)\n        feat = self.aspp_scse(self.final_fuse(torch.cat((stg2, self.pyramid_pool(feat), stg1_1), dim=1)))\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 3. Classifier: pixel-wise classification-segmentation\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        feat = F.interpolate(input=self.score(feat), size=(in_h, in_w), mode=""bilinear"", align_corners=True)\n        return feat\n\n\nif __name__ == \'__main__\':\n    import time\n    import torch\n    # from utils.utils import freeze_bn\n\n    model = MobileNetV2Plus(num_classes=19,\n                            width_multi=1.0,\n                            fuse_chns=512,\n                            aspp_chns=256,\n                            aspp_dilate=(12, 24, 36),\n                            norm_act=InPlaceABNSync)\n    model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n\n    model.eval()\n    with torch.no_grad():\n        while True:\n            dummy_in = torch.randn(2, 3, 768, 768).cuda()\n            start_time = time.time()\n            dummy_out = model(dummy_in)\n            del dummy_out\n            print(""> Inference Time: {}"".format(time.time() - start_time))\n'"
models/shufflenetv2plus.py,7,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn import init\nfrom collections import OrderedDict\n\nfrom modules.attentions import PBCSABlock\nfrom modules.aspp import DSASPPInPlaceABNBlock\nfrom modules.usm import UnsharpMask, UnsharpMaskV2\nfrom modules.shuffle import ShuffleRes, ShuffleResIABN\nfrom modules.inplace_abn.iabn import InPlaceABN, InPlaceABNSync, ABN\n\n\nclass ShuffleNetV2Plus(nn.Module):\n    def __init__(self, num_classes=19,\n                 fuse_chns=512,\n                 aspp_chns=256,\n                 aspp_dilate=(12, 24, 36),\n                 width_multi=1.0,\n                 norm_act=InPlaceABN):\n        super(ShuffleNetV2Plus, self).__init__()\n        self.stg_repeats = [4, 8, 4]\n        self.stride = [2, 1, 1]\n        self.dilate = [1, 2, 4]\n\n        if width_multi == 0.5:\n            self.stg_chns = [-1, 24, 48, 96, 192]\n        elif width_multi == 1.0:\n            self.stg_chns = [-1, 24, 116, 232, 464]\n        elif width_multi == 1.5:\n            self.stg_chns = [-1, 24, 176, 352, 704]\n        elif width_multi == 2.0:\n            self.stg_chns = [-1, 24, 224, 488, 976]\n        else:\n            raise ValueError(\n                """"""{} width_multi is not supported"""""".format(width_multi))\n\n        # building first layer\n        input_channel = self.stg_chns[1]\n        self.mod1 = nn.Sequential(OrderedDict([(""conv"", nn.Conv2d(3, input_channel,\n                                                                  kernel_size=3, stride=2, padding=1,\n                                                                  bias=False)),\n                                               (""norm"", nn.BatchNorm2d(input_channel)),\n                                               (""act"", nn.LeakyReLU(inplace=True, negative_slope=0.01)),\n                                               (""pool"", nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))\n\n        self.features = []\n        mod_id = 0\n        # building inverted residual blocks\n        for stg_idx in range(len(self.stg_repeats)):\n            num_repeat = self.stg_repeats[stg_idx]\n            stride = self.stride[stg_idx]\n            dilate = self.dilate[stg_idx]\n\n            output_channel = self.stg_chns[stg_idx + 2]\n\n            # Create blocks for module\n            blocks = []\n            for block_id in range(num_repeat):\n                if block_id == 0:\n                    blocks.append((""block%d"" % (block_id + 1), ShuffleRes(input_channel, output_channel,\n                                                                          stride=stride, dilate=1, branch_model=2)))\n\n                else:\n                    blocks.append((""block%d"" % (block_id + 1), ShuffleRes(input_channel, output_channel,\n                                                                          stride=1, dilate=dilate, branch_model=1)))\n                input_channel = output_channel\n\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n            mod_id += 1\n\n        feat_chns = self.stg_chns[1] + self.stg_chns[2] + self.stg_chns[3] + self.stg_chns[4]\n        self.feat_fusion = ShuffleResIABN(feat_chns, fuse_chns, stride=1,\n                                          dilate=2, branch_model=2, norm_act=norm_act)\n\n        # self.dsn = nn.Sequential(OrderedDict([(""norm"", norm_act(fuse_chns)),\n        #                                       (""conv"", nn.Conv2d(fuse_chns, num_classes,\n        #                                                          kernel_size=1, stride=1,\n        #                                                          padding=0, bias=True))]))\n\n        self.pyramid_pool = DSASPPInPlaceABNBlock(fuse_chns, aspp_chns, aspp_dilate=aspp_dilate, norm_act=norm_act)\n\n        feat_chns = aspp_chns + self.stg_chns[1]\n        self.final_fusion = ShuffleResIABN(feat_chns, aspp_chns, stride=1,\n                                           dilate=2, branch_model=2, norm_act=norm_act)\n        # self.usm = UnsharpMask(aspp_chns, kernel_size=9, padding=4,\n        #                        sigma=1.0, amount=1.0, threshold=0, norm_act=norm_act)\n        self.usm = UnsharpMaskV2(aspp_chns, kernel_size=9, padding=4, amount=1.0, threshold=0, norm_act=norm_act)\n\n        self.aspp_scse = PBCSABlock(in_chns=aspp_chns, reduct_ratio=16, dilation=4, use_res=True)\n\n        self.score = nn.Sequential(OrderedDict([(""dropout"", nn.Dropout2d(0.175)),\n                                                (""norm"", norm_act(aspp_chns)),\n                                                (""conv"", nn.Conv2d(aspp_chns, num_classes,\n                                                                   kernel_size=1, stride=1,\n                                                                   padding=0, bias=True))]))\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, a=0.01, mode=\'fan_in\', nonlinearity=\'leaky_relu\')\n                if hasattr(m, ""bias"") and m.bias is not None:\n                    init.constant_(m.bias, 0.0)\n\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, ABN) or \\\n                    isinstance(m, InPlaceABN) or isinstance(m, InPlaceABNSync):\n                init.normal_(m.weight, 1.0, 0.0256)\n                init.constant_(m.bias, 0.)\n            elif isinstance(m, nn.Linear):\n                init.xavier_uniform_(m.weight, .1)\n                init.constant_(m.bias, 0.)\n\n    def train(self, mode=True, freeze_bn=False, freeze_bn_affine=False):\n        super(ShuffleNetV2Plus, self).train()\n        # if freeze_bn:\n        #     print(""> Freezing Mean/Var of BatchNorm2D."")\n        #     if freeze_bn_affine:\n        #         print(""> Freezing Weight/Bias of BatchNorm2D."")\n        if freeze_bn:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n                    if freeze_bn_affine:\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n\n    def forward(self, x):\n        _, _, in_h, in_w = x.size()\n        assert (in_h % 8 == 0 and in_w % 8 == 0), ""> Error, in_size must be product of 8!!!""\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Encoder: feature extraction\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        stg1 = self.mod1(x)\n        stg1_1 = F.max_pool2d(input=stg1, kernel_size=3, stride=2, padding=1)  # 1/8\n        stg2 = self.mod2(stg1)\n        stg3 = self.mod3(stg2)\n        stg4 = self.mod4(stg3)\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Decoder: multi-scale feature fusion\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        feat = self.feat_fusion(torch.cat((stg2, stg4, stg3, stg1_1), dim=1))\n        feat = self.aspp_scse(self.usm(self.final_fusion(torch.cat((stg1, self.pyramid_pool(feat)), dim=1))))\n\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 3. Classifier: pixel-wise classification-segmentation\n        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        feat = F.interpolate(input=self.score(feat), size=(in_h, in_w), mode=""bilinear"", align_corners=True)\n        return feat\n\n\nif __name__ == ""__main__"":\n    import os\n    import time\n\n    model = ShuffleNetV2Plus(num_classes=19, fuse_chns=512, aspp_chns=256, width_multi=1.0, norm_act=InPlaceABN).cuda()\n    model.eval()\n\n    with torch.no_grad():\n        while True:\n            dummy_in = torch.randn(1, 3, 768, 768).cuda()\n            start_time = time.time()\n            dummy_out = model(dummy_in)\n            dummy_out = F.softmax(dummy_out, dim=1).argmax(dim=1)\n            print(""> Inference Time: {}"".format(time.time() - start_time))\n            del dummy_out\n\n'"
modules/__init__.py,0,b''
modules/aspp.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom collections import OrderedDict\nfrom modules.inplace_abn.iabn import InPlaceABN\n\n\nclass ASPPBlock(nn.Module):\n    def __init__(self, in_chs, out_chs, up_ratio=2, aspp_dilate=(4, 8, 12)):\n        super(ASPPBlock, self).__init__()\n        self.up_ratio = up_ratio\n\n        # --------------------------------------- #\n        # 1. For image-level feature\n        # --------------------------------------- #\n        self.gave_pool = nn.Sequential(OrderedDict([(""gavg"", nn.AdaptiveAvgPool2d((3, 3))),\n                                                    (""conv1_0"", nn.Conv2d(in_chs, out_chs,\n                                                                          kernel_size=1, stride=1, padding=0,\n                                                                          groups=1, bias=False, dilation=1)),\n                                                    (""bn1_1"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        # --------------------------------------- #\n        # 2. Convolution: 1x1\n        # --------------------------------------- #\n        self.conv1x1 = nn.Sequential(OrderedDict([(""conv1_1"", nn.Conv2d(in_chs, out_chs, kernel_size=1,\n                                                                        stride=1, padding=0, bias=False,\n                                                                        groups=1, dilation=1)),\n                                                  (""bn1_1"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        # ------------------------------------------------- #\n        # 3. Convolution: 3x3, dilation: aspp_dilate[0]\n        # ------------------------------------------------- #\n        self.aspp_bra1 = nn.Sequential(OrderedDict([(""conv2_1"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                          stride=1, padding=aspp_dilate[0], bias=False,\n                                                                          groups=1, dilation=aspp_dilate[0])),\n                                                    (""bn2_1"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        # ------------------------------------------------- #\n        # 4. Convolution: 3x3, dilation: aspp_dilate[1]\n        # ------------------------------------------------- #\n        self.aspp_bra2 = nn.Sequential(OrderedDict([(""conv2_2"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                          stride=1, padding=aspp_dilate[1], bias=False,\n                                                                          groups=1, dilation=aspp_dilate[1])),\n                                                    (""bn2_2"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        # ------------------------------------------------- #\n        # 5. Convolution: 3x3, dilation: aspp_dilate[2]\n        # ------------------------------------------------- #\n        self.aspp_bra3 = nn.Sequential(OrderedDict([(""conv2_3"", nn.Conv2d(in_chs, out_chs, kernel_size=3,\n                                                                          stride=1, padding=aspp_dilate[2], bias=False,\n                                                                          groups=1, dilation=aspp_dilate[2])),\n                                                    (""bn2_3"", nn.BatchNorm2d(num_features=out_chs))]))\n\n        # ------------------------------------------------- #\n        # 6. down channel after concatenate\n        # ------------------------------------------------- #\n        self.aspp_catdown = nn.Sequential(OrderedDict([(""conv_down"", nn.Conv2d(5*out_chs, out_chs, kernel_size=1,\n                                                                               stride=1, padding=1, bias=False,\n                                                                               groups=1, dilation=1)),\n                                                       (""bn_down"", nn.BatchNorm2d(num_features=out_chs)),\n                                                       (""act"", nn.LeakyReLU(inplace=True, negative_slope=0.1)),\n                                                       (""dropout"", nn.Dropout2d(p=0.25, inplace=True))]))\n\n    def forward(self, x):\n        _, _, feat_h, feat_w = x.size()\n\n        # ------------------------------------------------- #\n        # 1. Atrous Spacial Pyramid Pooling\n        # ------------------------------------------------- #\n        x = torch.cat((self.aspp_bra1(x),\n                       F.interpolate(input=self.gave_pool(x), size=(feat_h, feat_w),\n                                     mode=""bilinear"", align_corners=True),\n                       self.aspp_bra2(x),\n                       self.conv1x1(x),\n                       self.aspp_bra3(x)), dim=1)\n\n        # ------------------------------------------------- #\n        # 2. up-sampling the feature-map\n        # ------------------------------------------------- #\n        return F.interpolate(input=self.aspp_catdown(x),\n                             size=(int(feat_h * self.up_ratio),\n                                   int(feat_w * self.up_ratio)),\n                             mode=""bilinear"", align_corners=True)\n\n\nclass ASPPInPlaceABNBlock(nn.Module):\n    def __init__(self, in_chs, out_chs, up_ratio=2, aspp_dilate=(12, 24, 36), norm_act=InPlaceABN):\n        super(ASPPInPlaceABNBlock, self).__init__()\n        self.up_ratio = up_ratio\n\n        self.in_norm = norm_act(in_chs)\n        self.gave_pool = nn.Sequential(OrderedDict([(""gavg"", nn.AdaptiveAvgPool2d((3, 3))),\n                                                    (""conv1_0"", nn.Conv2d(in_chs, out_chs,\n                                                                          kernel_size=1, stride=1, padding=0,\n                                                                          groups=1, bias=False, dilation=1))]))\n\n        self.conv1x1 = nn.Conv2d(in_chs, out_chs,\n                                 kernel_size=1, stride=1, padding=0,\n                                 groups=1, dilation=1, bias=False)\n\n        self.aspp_bra1 = nn.Conv2d(in_chs, out_chs,\n                                   kernel_size=3, stride=1, padding=aspp_dilate[0],\n                                   groups=1, dilation=aspp_dilate[0], bias=False)\n\n        self.aspp_bra2 = nn.Conv2d(in_chs, out_chs,\n                                   kernel_size=3, stride=1, padding=aspp_dilate[1],\n                                   groups=1, dilation=aspp_dilate[1], bias=False)\n\n        self.aspp_bra3 = nn.Conv2d(in_chs, out_chs,\n                                   kernel_size=3, stride=1, padding=aspp_dilate[2],\n                                   groups=1, dilation=aspp_dilate[2], bias=False)\n\n        self.aspp_catdown = nn.Sequential(OrderedDict([(""norm_act"", norm_act(5*out_chs)),\n                                                       (""conv_down"", nn.Conv2d(5*out_chs, out_chs, kernel_size=1,\n                                                                               stride=1, padding=1, bias=False,\n                                                                               groups=1, dilation=1)),\n                                                       (""dropout"", nn.Dropout2d(p=0.25, inplace=True))]))\n\n    def forward(self, x):\n        _, _, feat_h, feat_w = x.size()\n\n        # ------------------------------------------------- #\n        # 1. Atrous Spacial Pyramid Pooling\n        # ------------------------------------------------- #\n        x = self.in_norm(x)\n        x = torch.cat((self.aspp_bra1(x),\n                       F.interpolate(input=self.gave_pool(x), size=(feat_h, feat_w),\n                                     mode=""bilinear"", align_corners=True),\n                       self.aspp_bra2(x),\n                       self.conv1x1(x),\n                       self.aspp_bra3(x)), dim=1)\n        # ------------------------------------------------- #\n        # 2. up-sampling the feature-map\n        # ------------------------------------------------- #\n        return F.interpolate(input=self.aspp_catdown(x),\n                             size=(int(feat_h * self.up_ratio),\n                                   int(feat_w * self.up_ratio)),\n                             mode=""bilinear"", align_corners=True)\n\n\nclass DSASPPInPlaceABNBlock(nn.Module):\n    def __init__(self, in_chs, out_chs, up_ratio=2, aspp_dilate=(12, 24, 36), norm_act=InPlaceABN):\n        super(DSASPPInPlaceABNBlock, self).__init__()\n        self.up_ratio = up_ratio\n\n        self.in_norm = norm_act(in_chs)\n        self.gave_pool = nn.Sequential(OrderedDict([(""gavg"", nn.AdaptiveAvgPool2d((3, 3))),\n                                                    (""conv1_0"", nn.Conv2d(in_chs, out_chs,\n                                                                          kernel_size=1, stride=1, padding=0,\n                                                                          groups=1, dilation=1,\n                                                                          bias=False))]))\n\n        self.conv1x1 = nn.Conv2d(in_chs, out_chs,\n                                 kernel_size=1, stride=1, padding=0,\n                                 groups=1, dilation=1, bias=False)\n\n        self.aspp_bra1 = nn.Conv2d(in_chs, in_chs, kernel_size=3, stride=1, padding=aspp_dilate[0],\n                                   groups=in_chs, dilation=aspp_dilate[0], bias=False)\n\n        self.aspp_bra2 = nn.Conv2d(in_chs, in_chs, kernel_size=3, stride=1, padding=aspp_dilate[1],\n                                   groups=in_chs, dilation=aspp_dilate[1], bias=False)\n\n        self.aspp_bra3 = nn.Conv2d(in_chs, in_chs, kernel_size=3, stride=1, padding=aspp_dilate[2],\n                                   groups=in_chs, dilation=aspp_dilate[2], bias=False)\n\n        self.aspp_catdown = nn.Sequential(OrderedDict([(""norm_act"", norm_act(3 * in_chs + 2 * out_chs)),\n                                                       (""conv_down"", nn.Conv2d(3 * in_chs + 2 * out_chs, out_chs,\n                                                                               kernel_size=1, stride=1, padding=1,\n                                                                               groups=1, dilation=1, bias=False))]))\n\n    def forward(self, x):\n        _, _, feat_h, feat_w = x.size()\n\n        # ------------------------------------------------- #\n        # 1. Atrous Spacial Pyramid Pooling\n        # ------------------------------------------------- #\n        x = self.in_norm(x)\n        x = self.aspp_catdown(torch.cat((self.aspp_bra1(x),\n                                         F.interpolate(input=self.gave_pool(x),\n                                                       size=(feat_h, feat_w),\n                                                       mode=""bilinear"",\n                                                       align_corners=True),\n                                         self.aspp_bra2(x),\n                                         self.conv1x1(x),\n                                         self.aspp_bra3(x)), dim=1))\n        # ------------------------------------------------- #\n        # 2. up-sampling the feature-map\n        # ------------------------------------------------- #\n        return F.interpolate(input=x,\n                             size=(int(feat_h * self.up_ratio),\n                                   int(feat_w * self.up_ratio)),\n                             mode=""bilinear"", align_corners=True)\n\n\nclass DenseAsppBlock(nn.Sequential):\n    """""" ConvNet block for building DenseASPP. """"""\n\n    def __init__(self, input_num, num1, num2, dilation_rate, drop_out, norm_act=InPlaceABN):\n        super(DenseAsppBlock, self).__init__()\n\n        self.add_module(\'norm_1\', norm_act(input_num))\n        self.add_module(\'conv_1\', nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1))\n        self.add_module(\'norm_2\', norm_act(num1))\n        self.add_module(\'conv_2\', nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n                                            dilation=dilation_rate, padding=dilation_rate))\n\n        self.drop_rate = drop_out\n\n    def forward(self, _input):\n        feature = super(DenseAsppBlock, self).forward(_input)\n\n        if self.drop_rate > 0:\n            feature = F.dropout2d(feature, p=self.drop_rate, training=self.training)\n\n        return feature\n\n\n# class RFBlock(nn.Module):\n#     def __init__(self, in_chs, out_chs, scale=0.1, feat_res=(56, 112), aspp_dilate=(12, 24, 36),\n#                  up_ratio=2, norm_act=InPlaceABN):\n#         super(RFBlock, self).__init__()\n#         self.feat_res = feat_res\n#         self.up_ratio = up_ratio\n#\n#         self.scale = scale\n#\n#         self.down_chs = nn.Sequential(OrderedDict([(""norm_act"", norm_act(in_chs)),\n#                                                    (""down_conv1x1"", nn.Conv2d(in_chs, out_chs,\n#                                                                               kernel_size=1, stride=1,\n#                                                                               padding=0, bias=False))]))\n#\n#         self.gave_pool = nn.Sequential(OrderedDict([(""norm_act"", norm_act(out_chs)),\n#                                                     (""gavg"", nn.AdaptiveAvgPool2d((1, 1))),\n#                                                     (""conv1_0"", nn.Conv2d(out_chs, out_chs,\n#                                                                           kernel_size=1, stride=1, padding=0,\n#                                                                           groups=1, bias=False, dilation=1)),\n#                                                     (""up0"", nn.Upsample(size=feat_res, mode=\'bilinear\'))]))\n#\n#         self.branch0 = nn.Sequential(OrderedDict([(""norm_act"", norm_act(out_chs)),\n#                                                   (""conv1x1"", nn.Conv2d(out_chs, out_chs,\n#                                                                         kernel_size=1, stride=1,\n#                                                                         padding=0, bias=False)),\n#                                                   (""norm_act"", norm_act(out_chs)),\n#                                                   (""aconv1"", nn.Conv2d(out_chs, out_chs,\n#                                                                        kernel_size=3, stride=1,\n#                                                                        padding=1, dilation=1,\n#                                                                        bias=False))]))\n#\n#         self.branch1 = nn.Sequential(OrderedDict([(""norm_act"", norm_act(out_chs)),\n#                                                   (""conv1x3"", nn.Conv2d(out_chs, (out_chs // 2) * 3,\n#                                                                         kernel_size=(1, 3), stride=1,\n#                                                                         padding=(0, 1), bias=False)),\n#                                                   (""norm_act"", norm_act((out_chs // 2) * 3)),\n#                                                   (""conv3x1"", nn.Conv2d((out_chs // 2) * 3, out_chs,\n#                                                                         kernel_size=(3, 1), stride=1,\n#                                                                         padding=(1, 0), bias=False)),\n#                                                   (""norm_act"", norm_act(out_chs)),\n#                                                   (""aconv3"", nn.Conv2d(out_chs, out_chs,\n#                                                                        kernel_size=3, stride=1,\n#                                                                        padding=aspp_dilate[0],\n#                                                                        dilation=aspp_dilate[0],\n#                                                                        bias=False))]))\n#\n#         self.branch2 = nn.Sequential(OrderedDict([(""norm_act"", norm_act(out_chs)),\n#                                                   (""conv1x5"", nn.Conv2d(out_chs, (out_chs // 2) * 3,\n#                                                                         kernel_size=(1, 5), stride=1,\n#                                                                         padding=(0, 2), bias=False)),\n#                                                   (""norm_act"", norm_act((out_chs // 2) * 3)),\n#                                                   (""conv5x1"", nn.Conv2d((out_chs // 2) * 3, out_chs,\n#                                                                         kernel_size=(5, 1), stride=1,\n#                                                                         padding=(2, 0), bias=False)),\n#                                                   (""norm_act"", norm_act(out_chs)),\n#                                                   (""aconv5"", nn.Conv2d(out_chs, out_chs,\n#                                                                        kernel_size=3, stride=1,\n#                                                                        padding=aspp_dilate[1],\n#                                                                        dilation=aspp_dilate[1],\n#                                                                        bias=False))]))\n#\n#         self.branch3 = nn.Sequential(OrderedDict([(""norm_act"", norm_act(out_chs)),\n#                                                   (""conv1x7"", nn.Conv2d(out_chs, (out_chs // 2) * 3,\n#                                                                         kernel_size=(1, 7), stride=1,\n#                                                                         padding=(0, 3), bias=False)),\n#                                                   (""norm_act"", norm_act((out_chs // 2) * 3)),\n#                                                   (""conv7x1"", nn.Conv2d((out_chs // 2) * 3, out_chs,\n#                                                                         kernel_size=(7, 1), stride=1,\n#                                                                         padding=(3, 0), bias=False)),\n#                                                   (""norm_act"", norm_act(out_chs)),\n#                                                   (""aconv7"", nn.Conv2d(out_chs, out_chs,\n#                                                                        kernel_size=3, stride=1,\n#                                                                        padding=aspp_dilate[2],\n#                                                                        dilation=aspp_dilate[2],\n#                                                                        bias=False))]))\n#\n#         self.conv_linear = nn.Sequential(OrderedDict([(""conv1x1_linear"", nn.Conv2d(out_chs * 5, out_chs,\n#                                                                                    kernel_size=1, stride=1,\n#                                                                                    padding=0, bias=False))]))\n#\n#     def forward(self, x):\n#         down = self.down_chs(x)\n#         out = torch.cat([self.gave_pool(down.clone()),\n#                          self.branch0(down.clone()),\n#                          self.branch1(down.clone()),\n#                          self.branch2(down.clone()),\n#                          self.branch3(down.clone())], dim=1)\n#\n#         return F.interpolate(input=self.conv_linear(x),\n#                              size=(int(self.feat_res[0]*self.up_ratio),\n#                                    int(self.feat_res[1]*self.up_ratio)),\n#                              mode=""bilinear"", align_corners=True)\n'"
modules/attentions.py,23,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .inplace_abn.iabn import InPlaceABN\nfrom collections import OrderedDict\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Large Separable Convolution Block\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass LightHeadBlock(nn.Module):\n    def __init__(self, in_chs, mid_chs=64, out_chs=256, kernel_size=15, norm_act=InPlaceABN):\n        super(LightHeadBlock, self).__init__()\n        pad = int((kernel_size - 1) / 2)\n\n        # kernel size had better be odd number so as to avoid alignment error\n        self.abn = norm_act(in_chs)\n        self.conv_l = nn.Sequential(OrderedDict([(""conv_lu"", nn.Conv2d(in_chs, mid_chs,\n                                                                       kernel_size=(kernel_size, 1),\n                                                                       stride=(1, 1),\n                                                                       padding=(pad, 0),\n                                                                       bias=False)),\n                                                 (""norm"", norm_act(mid_chs)),\n                                                 (""conv_ld"", nn.Conv2d(mid_chs, out_chs,\n                                                                       kernel_size=(1, kernel_size),\n                                                                       stride=(1, 1),\n                                                                       padding=(0, pad),\n                                                                       bias=False))]))\n\n        self.conv_r = nn.Sequential(OrderedDict([(""conv_ru"", nn.Conv2d(in_chs, mid_chs,\n                                                                       kernel_size=(1, kernel_size),\n                                                                       stride=(1, 1),\n                                                                       padding=(0, pad),\n                                                                       bias=False)),\n                                                 (""norm"", norm_act(mid_chs)),\n                                                 (""conv_rd"", nn.Conv2d(mid_chs, out_chs,\n                                                                       kernel_size=(kernel_size, 1),\n                                                                       padding=(pad, 0),\n                                                                       bias=False))]))\n\n    def forward(self, x):\n        x = self.abn(x)\n        x_l = self.conv_l(x)\n        x_r = self.conv_r(x)\n        return torch.add(x_l, 1, x_r)\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# SEBlock: Squeeze & Excitation (SCSE)\n#          namely, Channel-wise Attention\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduct_ratio=16):\n        super(SEBlock, self).__init__()\n        self.channel_se = nn.Sequential(OrderedDict([(""avgpool"", nn.AdaptiveAvgPool2d(1)),\n                                                     (""linear1"", nn.Conv2d(channel, channel // reduct_ratio,\n                                                                           kernel_size=1, stride=1, padding=0)),\n                                                     (""relu"", nn.ReLU(inplace=True)),\n                                                     (""linear2"", nn.Conv2d(channel // reduct_ratio, channel,\n                                                                           kernel_size=1, stride=1, padding=0))]))\n\n    def forward(self, x):\n        inputs = x\n        chn_se = self.channel_se(x).sigmoid().exp()\n        return torch.mul(inputs, chn_se)\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# SCSEBlock: Spatial-Channel Squeeze & Excitation (SCSE)\n#            namely, Spatial-wise and Channel-wise Attention\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nclass SCSEBlock(nn.Module):\n    def __init__(self, channel, reduct_ratio=16, is_res=True):\n        super(SCSEBlock, self).__init__()\n        self.is_res = is_res\n\n        self.channel_se = nn.Sequential(OrderedDict([(""avgpool"", nn.AdaptiveAvgPool2d(1)),\n                                                     (""linear1"", nn.Conv2d(channel, channel // reduct_ratio,\n                                                                           kernel_size=1, stride=1, padding=0)),\n                                                     (""relu"", nn.ReLU(inplace=True)),\n                                                     (""linear2"", nn.Conv2d(channel // reduct_ratio, channel,\n                                                                           kernel_size=1, stride=1, padding=0))]))\n\n        self.spatial_se = nn.Sequential(OrderedDict([(""conv"", nn.Conv2d(channel, 1, kernel_size=1, stride=1,\n                                                                        padding=0, bias=False))]))\n\n    def forward(self, x):\n        inputs = x\n\n        chn_se = self.channel_se(x).sigmoid().exp()\n        spa_se = self.spatial_se(x).sigmoid().exp()\n\n        if self.is_res:\n            torch.mul(torch.mul(inputs, chn_se), spa_se) + inputs\n\n        return torch.mul(torch.mul(inputs, chn_se), spa_se)\n\n\nclass ModifiedSCSEBlock(nn.Module):\n    def __init__(self, in_chns, reduct_ratio=16, is_res=True):\n        super(ModifiedSCSEBlock, self).__init__()\n        self.is_res = is_res\n\n        # ------------------------------------------ #\n        # Channel-wise Attention Model\n        # ------------------------------------------ #\n        self.ch_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.ch_max_pool = nn.AdaptiveMaxPool2d(1)\n        self.channel_se = nn.Sequential(nn.Conv2d(in_chns, in_chns // reduct_ratio,\n                                                  kernel_size=1, stride=1, padding=0),\n                                        nn.ReLU(inplace=True),\n                                        nn.Conv2d(in_chns // reduct_ratio, in_chns,\n                                                  kernel_size=1, stride=1, padding=0),\n                                        nn.BatchNorm2d(in_chns))\n\n        self.spatial_se = nn.Sequential(nn.Conv2d(in_chns, 1, kernel_size=1, stride=1,\n                                                  padding=0, bias=False),\n                                        nn.BatchNorm2d(1))\n\n    def forward(self, x):\n        res = x\n\n        ch_att = self.channel_se((self.ch_avg_pool(x) + self.ch_max_pool(x)))\n        ch_att = torch.mul(x, ch_att.sigmoid().exp())\n\n        # ------------------------------------------ #\n        # 2. Spatial-wise Attention Model\n        # ------------------------------------------ #\n        sp_att = torch.mul(x, self.spatial_se(x).sigmoid().exp())\n\n        if self.is_res:\n            ch_att + res + sp_att\n\n        return ch_att + sp_att\n\n\nclass SCSABlock(nn.Module):\n    def __init__(self, in_chns, reduct_ratio=16, is_res=True):\n        super(SCSABlock, self).__init__()\n        self.is_res = is_res\n\n        if is_res:\n            self.gamma = nn.Parameter(torch.ones(1))\n        # ------------------------------------------ #\n        # Channel-wise Attention Model\n        # ------------------------------------------ #\n        self.ch_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.ch_max_pool = nn.AdaptiveMaxPool2d(1)\n        self.se_block = nn.Sequential(nn.Conv2d(in_chns, in_chns // reduct_ratio,\n                                                kernel_size=1, stride=1, padding=0),\n                                      nn.ReLU(inplace=True),\n                                      nn.Conv2d(in_chns // reduct_ratio, in_chns,\n                                                kernel_size=1, stride=1, padding=0),\n                                      nn.BatchNorm2d(in_chns))\n\n        self.sp_conv = nn.Sequential(nn.Conv2d(2, 1, kernel_size=7, stride=1, padding=3, bias=False),\n                                     nn.BatchNorm2d(1))\n\n    def forward(self, x):\n        # ------------------------------------------ #\n        # 1. Channel-wise Attention Model\n        # ------------------------------------------ #\n        res = x\n        avg_p = self.se_block(self.ch_avg_pool(x))\n        max_p = self.se_block(self.ch_max_pool(x))\n\n        ch_att = torch.mul(x, (avg_p + max_p).sigmoid().exp())\n\n        # ------------------------------------------ #\n        # 2. Spatial-wise Attention Model\n        # ------------------------------------------ #\n        ch_avg = torch.mean(ch_att, dim=1, keepdim=True)\n        ch_max = torch.max(ch_att, dim=1, keepdim=True)[0]\n\n        sp_att = torch.mul(ch_att, self.sp_conv(torch.cat([ch_avg, ch_max], dim=1)).sigmoid().exp())\n\n        if self.is_res:\n            return sp_att + self.gamma * res\n        return sp_att\n\n\nclass PBCSABlock(nn.Module):\n    """"""\n    Parallel Bottleneck Channel-Spatial Attention Block\n    """"""\n    def __init__(self, in_chns, reduct_ratio=16, dilation=4, use_res=True):\n        super(PBCSABlock, self).__init__()\n        self.use_res = use_res\n        #\n        # if is_res:\n        #     self.gamma = nn.Parameter(torch.ones(1))\n        # ------------------------------------------ #\n        # Channel-wise Attention Model\n        # ------------------------------------------ #\n        self.ch_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.ch_max_pool = nn.AdaptiveMaxPool2d((1, 1))\n\n        self.se_block = nn.Sequential(nn.Conv2d(in_chns, in_chns // reduct_ratio,\n                                                kernel_size=1, stride=1, padding=0),\n                                      nn.ReLU(inplace=True),\n                                      nn.Conv2d(in_chns // reduct_ratio, in_chns,\n                                                kernel_size=1, stride=1, padding=0))\n\n        self.sp_conv = nn.Sequential(nn.Conv2d(in_chns, in_chns // reduct_ratio,\n                                               kernel_size=1, stride=1, padding=0, bias=False),\n                                     nn.Conv2d(in_chns // reduct_ratio, in_chns // reduct_ratio,\n                                               kernel_size=3, stride=1, padding=dilation,\n                                               dilation=dilation, bias=False),\n\n                                     nn.Conv2d(in_chns // reduct_ratio, in_chns // reduct_ratio,\n                                               kernel_size=3, stride=1, padding=dilation,\n                                               dilation=dilation, bias=False),\n                                     nn.Conv2d(in_chns // reduct_ratio, 1, kernel_size=1,\n                                               stride=1, padding=0, bias=False),\n                                     nn.BatchNorm2d(1))\n\n    def forward(self, x):\n        # ------------------------------------------ #\n        # 1. Channel-wise Attention Model\n        # ------------------------------------------ #\n        # res = x\n        ch_att = self.se_block(self.ch_avg_pool(x) + self.ch_max_pool(x))\n        ch_att = torch.mul(x, ch_att.sigmoid().exp())\n\n        # ------------------------------------------ #\n        # 2. Spatial-wise Attention Model\n        # ------------------------------------------ #\n        sp_att = torch.mul(x, self.sp_conv(x).sigmoid().exp())\n\n        if self.use_res:\n            return sp_att + x + ch_att\n\n        return sp_att + ch_att\n    \n\nclass PABlock(nn.Module):\n    """"""Position Attention Block""""""\n    def __init__(self, in_chns, reduct_ratio=8):\n        super(PABlock, self).__init__()\n        self.in_chns = in_chns\n\n        self.query = nn.Conv2d(in_channels=in_chns, out_channels=in_chns//reduct_ratio,\n                               kernel_size=1, stride=1, padding=0)\n\n        self.key = nn.Conv2d(in_channels=in_chns, out_channels=in_chns//reduct_ratio,\n                             kernel_size=1, stride=1, padding=0)\n\n        self.value = nn.Conv2d(in_channels=in_chns, out_channels=in_chns,\n                               kernel_size=1, stride=1, padding=0)\n\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        batch_size, channels, feat_h, feat_w = x.size()\n\n        proj_query = self.query(x).view(batch_size, -1, feat_h * feat_w).permute(0, 2, 1)\n        proj_key = self.key(x).view(batch_size, -1, feat_h * feat_w)\n        proj_value = self.value(x).view(batch_size, -1, feat_h * feat_w)\n\n        energy = torch.bmm(proj_query, proj_key)\n        attention = self.softmax(energy).permute(0, 2, 1)\n\n        out = torch.bmm(proj_value, attention).view(batch_size, channels, feat_h, feat_w)\n\n        return self.gamma * out + x\n\n\nclass CABlock(nn.Module):\n    """"""Channel Attention Block""""""\n    def __init__(self):\n        super(CABlock, self).__init__()\n\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        batch_size, channels, feat_h, feat_w = x.size()\n\n        proj_query = x.view(batch_size, channels, -1)\n        proj_key = x.view(batch_size, channels, -1).permute(0, 2, 1)\n        proj_value = x.view(batch_size, channels, -1)\n\n        energy = torch.bmm(proj_query, proj_key)\n        energy_new = torch.max(energy, dim=-1, keepdim=True)[0].expand_as(energy) - energy\n\n        attention = self.softmax(energy_new)\n\n        out = torch.bmm(attention, proj_value).view(batch_size, channels, feat_h, feat_w)\n\n        return self.gamma * out + x\n'"
modules/dense.py,3,"b'from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\nfrom modules.inplace_abn.iabn import ABN\n\n\nclass DenseModule(nn.Module):\n    def __init__(self, in_channels, growth, layers, bottleneck_factor=4, norm_act=ABN, dilation=1):\n        super(DenseModule, self).__init__()\n        self.in_channels = in_channels\n        self.growth = growth\n        self.layers = layers\n\n        self.convs1 = nn.ModuleList()\n        self.convs3 = nn.ModuleList()\n        for i in range(self.layers):\n            self.convs1.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(in_channels)),\n                (""conv"", nn.Conv2d(in_channels, self.growth * bottleneck_factor, 1, bias=False))\n            ])))\n            self.convs3.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(self.growth * bottleneck_factor)),\n                (""conv"", nn.Conv2d(self.growth * bottleneck_factor, self.growth, 3, padding=dilation, bias=False,\n                                   dilation=dilation))\n            ])))\n            in_channels += self.growth\n\n    @property\n    def out_channels(self):\n        return self.in_channels + self.growth * self.layers\n\n    def forward(self, x):\n        inputs = [x]\n        for i in range(self.layers):\n            x = torch.cat(inputs, dim=1)\n            x = self.convs1[i](x)\n            x = self.convs3[i](x)\n            inputs += [x]\n\n        return torch.cat(inputs, dim=1)\n'"
modules/dropout.py,4,"b'import torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass DropBlock2D(nn.Module):\n    r""""""Randomly zeroes spatial blocks of the input tensor.\n    As described in the paper\n    `DropBlock: A regularization method for convolutional networks`_ ,\n    dropping whole blocks of feature map allows to remove semantic\n    information as compared to regular dropout.\n    Args:\n        keep_prob (float, optional): probability of an element to be kept.\n        Authors recommend to linearly decrease this value from 1 to desired\n        value.\n        block_size (int, optional): size of the block. Block size in paper\n        usually equals last feature map dimensions.\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n    .. _DropBlock: A regularization method for convolutional networks:\n       https://arxiv.org/abs/1810.12890\n    """"""\n\n    def __init__(self, keep_prob=0.9, block_size=7):\n        super(DropBlock2D, self).__init__()\n        self.keep_prob = keep_prob\n        self.block_size = block_size\n\n    def forward(self, input):\n        if not self.training or self.keep_prob == 1:\n            return input\n        gamma = (1. - self.keep_prob) / self.block_size ** 2\n        for sh in input.shape[2:]:\n            gamma *= sh / (sh - self.block_size + 1)\n        M = torch.bernoulli(torch.ones_like(input) * gamma)\n        Msum = F.conv2d(M,\n                        torch.ones((input.shape[1], 1, self.block_size, self.block_size)).to(device=input.device,\n                                                                                             dtype=input.dtype),\n                        padding=self.block_size // 2,\n                        groups=input.shape[1])\n        torch.set_printoptions(threshold=5000)\n        mask = (Msum < 1).to(device=input.device, dtype=input.dtype)\n        return input * mask * mask.numel() / mask.sum()  # TODO input * mask * self.keep_prob ?\n'"
modules/efficient.py,8,"b'# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# LightNet++: Boosted Light-weighted Networks for Real-time Semantic Segmentation\n# ---------------------------------------------------------------------------------------------------------------- #\n# PyTorch implementation for EfficientNet\n# class:\n#       > Swish\n#       > SEBlock\n#       > MBConvBlock\n# ---------------------------------------------------------------------------------------------------------------- #\n# Author: Huijun Liu M.Sc.\n# Date:   08.02.2020\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nfrom collections import OrderedDict\n\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\n\nclass DSConvBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, dilate=1):\n        super(DSConvBlock, self).__init__()\n        dilate = 1 if stride > 1 else dilate\n        padding = ((kernel_size - 1) // 2) * dilate\n        self.depth_wise = nn.Sequential(OrderedDict([(""conv"", nn.Conv2d(in_planes, in_planes,\n                                                                        kernel_size, stride, padding, dilate,\n                                                                        groups=in_planes, bias=False)),\n                                                     (""norm"", nn.BatchNorm2d(num_features=out_planes, eps=1e-3, momentum=0.01))\n                                                     ]))\n        self.point_wise = nn.Sequential(OrderedDict([(""conv"", nn.Conv2d(in_planes, out_planes,\n                                                                        kernel_size=1, stride=1, padding=0,\n                                                                        dilation=1, groups=1, bias=False)),\n                                                     (""norm"", nn.BatchNorm2d(num_features=out_planes, eps=1e-3, momentum=0.01)),\n                                                     (""act"", nn.LeakyReLU(negative_slope=0.01, inplace=True))\n                                                     ]))\n\n    def forward(self, x):\n        return self.point_wise(self.depth_wise(x))\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Swish: Swish Activation Function\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass Swish(nn.Module):\n    def __init__(self, inplace=True):\n        super(Swish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return x.mul_(x.sigmoid()) if self.inplace else x.mul(x.sigmoid())\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, groups=1, dilate=1):\n\n        super(ConvBlock, self).__init__()\n        dilate = 1 if stride > 1 else dilate\n        padding = ((kernel_size - 1) // 2) * dilate\n\n        self.conv_block = nn.Sequential(OrderedDict([\n           (""conv"", nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride, padding=padding,\n                              dilation=dilate, groups=groups, bias=False)),\n            (""norm"", nn.BatchNorm2d(num_features=out_planes,\n                                    eps=1e-3, momentum=0.01)),\n            (""act"", nn.LeakyReLU(negative_slope=0.01, inplace=True))\n        ]))\n\n    def forward(self, x):\n        return self.conv_block(x)\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# SEBlock: Squeeze & Excitation (SCSE)\n#          namely, Channel-wise Attention\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass SEBlock(nn.Module):\n    def __init__(self, in_planes, reduced_dim):\n        super(SEBlock, self).__init__()\n        self.channel_se = nn.Sequential(OrderedDict([\n            (""linear1"", nn.Conv2d(in_planes, reduced_dim, kernel_size=1, stride=1, padding=0, bias=True)),\n            (""act"", nn.LeakyReLU(negative_slope=0.01, inplace=True)),\n            (""linear2"", nn.Conv2d(reduced_dim, in_planes, kernel_size=1, stride=1, padding=0, bias=True))\n        ]))\n\n    def forward(self, x):\n        x_se = torch.sigmoid(self.channel_se(F.adaptive_avg_pool2d(x, output_size=(1, 1))))\n        return torch.mul(x, x_se)\n\n\nclass BiFPNBlock(nn.Module):\n    """"""\n    Bi-directional Feature Pyramid Network\n    """"""\n\n    def __init__(self, feature_size=64, epsilon=0.0001):\n        super(BiFPNBlock, self).__init__()\n        self.epsilon = epsilon\n\n        self.p1_td = DSConvBlock(feature_size, feature_size)\n        self.p2_td = DSConvBlock(feature_size, feature_size)\n        self.p3_td = DSConvBlock(feature_size, feature_size)\n        self.p4_td = DSConvBlock(feature_size, feature_size)\n\n        self.p2_out = DSConvBlock(feature_size, feature_size)\n        self.p3_out = DSConvBlock(feature_size, feature_size)\n        self.p4_out = DSConvBlock(feature_size, feature_size)\n        self.p5_out = DSConvBlock(feature_size, feature_size)\n\n        self.w1 = nn.Parameter(torch.Tensor(2, 4).fill_(0.5))\n        self.w2 = nn.Parameter(torch.Tensor(3, 4).fill_(0.5))\n\n    def forward(self, inputs):\n        p1_x, p2_x, p3_x, p4_x, p5_x = inputs\n\n        w1 = F.relu(self.w1)\n        w1 /= torch.sum(w1, dim=0) + self.epsilon\n        w2 = F.relu(self.w2)\n        w2 /= torch.sum(w2, dim=0) + self.epsilon\n\n        p5_td = p5_x\n        p4_td = self.p4_td(w1[0, 0] * p4_x + w1[1, 0] * F.interpolate(p5_td, scale_factor=2, mode=""bilinear"", align_corners=True))\n        p3_td = self.p3_td(w1[0, 1] * p3_x + w1[1, 1] * F.interpolate(p4_td, scale_factor=2, mode=""bilinear"", align_corners=True))\n        p2_td = self.p2_td(w1[0, 2] * p2_x + w1[1, 2] * F.interpolate(p3_td, scale_factor=2, mode=""bilinear"", align_corners=True))\n        p1_td = self.p1_td(w1[0, 3] * p1_x + w1[1, 3] * F.interpolate(p2_td, scale_factor=2, mode=""bilinear"", align_corners=True))\n\n        # Calculate Bottom-Up Pathway\n        p1_out = p1_td\n        p2_out = self.p2_out(w2[0, 0] * p2_x + w2[1, 0] * p2_td + w2[2, 0] * F.interpolate(p1_out, scale_factor=0.5, mode=""bilinear"", align_corners=True))\n        p3_out = self.p3_out(w2[0, 1] * p3_x + w2[1, 1] * p3_td + w2[2, 1] * F.interpolate(p2_out, scale_factor=0.5, mode=""bilinear"", align_corners=True))\n        p4_out = self.p4_out(w2[0, 2] * p4_x + w2[1, 2] * p4_td + w2[2, 2] * F.interpolate(p3_out, scale_factor=0.5, mode=""bilinear"", align_corners=True))\n        p5_out = self.p5_out(w2[0, 3] * p5_x + w2[1, 3] * p5_td + w2[2, 3] * F.interpolate(p4_out, scale_factor=0.5, mode=""bilinear"", align_corners=True))\n\n        return p1_out, p2_out, p3_out, p4_out, p5_out\n\n\nclass BiFPNDecoder(nn.Module):\n    def __init__(self, bone_feat_sizes, feature_size=64, fpn_repeats=2):\n        super(BiFPNDecoder, self).__init__()\n        self.p1 = ConvBlock(bone_feat_sizes[0], feature_size, kernel_size=1, stride=1)\n        self.p2 = ConvBlock(bone_feat_sizes[1], feature_size, kernel_size=1, stride=1)\n        self.p3 = ConvBlock(bone_feat_sizes[2], feature_size, kernel_size=1, stride=1)\n        self.p4 = ConvBlock(bone_feat_sizes[3], feature_size, kernel_size=1, stride=1)\n        self.p5 = ConvBlock(bone_feat_sizes[4], feature_size, kernel_size=1, stride=1)\n\n        bifpns_seq = []\n        for bifpn_id in range(fpn_repeats):\n            bifpns_seq.append((""bi_fpn%d"" % (bifpn_id + 1), BiFPNBlock(feature_size)))\n        self.bifpns = nn.Sequential(OrderedDict(bifpns_seq))\n\n    def forward(self, feat1, feat2, feat3, feat4, feat5):\n\n        # Calculate the input column of BiFPNDecoder\n        p1 = self.p1(feat1)\n        p2 = self.p2(feat2)\n        p3 = self.p3(feat3)\n        p4 = self.p4(feat4)\n        p5 = self.p5(feat5)\n\n        return self.bifpns([p1, p2, p3, p4, p5])\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# MBConvBlock: MBConvBlock for EfficientSeg\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass MBConvBlock(nn.Module):\n    def __init__(self, in_planes, out_planes,\n                 expand_ratio,  kernel_size, stride, dilate,\n                 reduction_ratio=4, dropout_rate=0.2):\n        super(MBConvBlock, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.expand_ratio = expand_ratio\n        self.use_se = (reduction_ratio is not None) and (reduction_ratio > 1)\n        self.use_residual = in_planes == out_planes and stride == 1\n\n        assert stride in [1, 2]\n        assert kernel_size in [3, 5]\n        dilate = 1 if stride > 1 else dilate\n        hidden_dim = in_planes * expand_ratio\n        reduced_dim = max(1, int(in_planes / reduction_ratio))\n\n        # step 1. Expansion phase/Point-wise convolution\n        if expand_ratio != 1:\n            self.expansion = ConvBlock(in_planes, hidden_dim, 1)\n\n        # step 2. Depth-wise convolution phase\n        self.depth_wise = ConvBlock(hidden_dim, hidden_dim, kernel_size,\n                                    stride=stride, groups=hidden_dim, dilate=dilate)\n        # step 3. Squeeze and Excitation\n        if self.use_se:\n            self.se_block = SEBlock(hidden_dim, reduced_dim)\n\n        # step 4. Point-wise convolution phase\n        self.point_wise = nn.Sequential(OrderedDict([\n            (""conv"", nn.Conv2d(hidden_dim, out_planes, kernel_size=1,\n                               stride=1, padding=0, dilation=1, groups=1, bias=False)),\n            (""norm"", nn.BatchNorm2d(out_planes, eps=1e-3, momentum=0.01))\n        ]))\n\n    def forward(self, x):\n        res = x\n\n        # step 1. Expansion phase/Point-wise convolution\n        if self.expand_ratio != 1:\n            x = self.expansion(x)\n\n        # step 2. Depth-wise convolution phase\n        x = self.depth_wise(x)\n\n        # step 3. Squeeze and Excitation\n        if self.use_se:\n            x = self.se_block(x)\n\n        # step 4. Point-wise convolution phase\n        x = self.point_wise(x)\n\n        # step 5. Skip connection and drop connect\n        if self.use_residual:\n            if self.training and (self.dropout_rate is not None):\n                x = F.dropout2d(input=x, p=self.dropout_rate,\n                                training=self.training, inplace=True)\n            x = x + res\n\n        return x\n'"
modules/misc.py,8,"b'import torch\nimport torch.nn as nn\n\nfrom collections import OrderedDict\n\n\nclass CoordInfo(nn.Module):\n    def __init__(self, with_r=True):\n        super(CoordInfo, self).__init__()\n        self.with_r = with_r\n\n    def forward(self, x):\n        """"""\n        Add Cartesian Coordination Info to Current Tensor\n        :param x: shape(N, C, H, W)\n        :return:  shape(N, C+2 or C+3, H, W)\n        """"""\n        batch_size, _, height, width = x.size()\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 1. Meshgrid using Pytorch\n        # i_coords([[[0., 0., 0., 0., 0., 0.],\n        #            [1., 1., 1., 1., 1., 1.],\n        #            [2., 2., 2., 2., 2., 2.],\n        #            [3., 3., 3., 3., 3., 3.]]])\n        #\n        # j_coords([[[0., 1., 2., 3., 4., 5.],\n        #            [0., 1., 2., 3., 4., 5.],\n        #            [0., 1., 2., 3., 4., 5.],\n        #            [0., 1., 2., 3., 4., 5.]]])\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        i_coords = torch.arange(height).repeat(1, width, 1).transpose(1, 2)  # [1, H, W]\n        j_coords = torch.arange(width).repeat(1, height, 1)                  # [1, H, W]\n\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        # 2. Normalization (-1, 1)\n        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n        i_coords = i_coords.float() / (height - 1)\n        j_coords = j_coords.float() / (width - 1)\n\n        i_coords = i_coords * 2 - 1\n        j_coords = j_coords * 2 - 1\n\n        i_coords = i_coords.repeat(batch_size, 1, 1, 1)  # [N, 1, H, W]\n        j_coords = j_coords.repeat(batch_size, 1, 1, 1)  # [N, 1, H, W]\n\n        ret = torch.cat([x, i_coords.type_as(x), j_coords.type_as(x)], dim=1)\n\n        if self.with_r:\n            rr = torch.sqrt(torch.pow(i_coords.type_as(x) - 0.5, 2) + torch.pow(j_coords.type_as(x) - 0.5, 2))\n            ret = torch.cat([ret, rr], dim=1)\n\n        return ret\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Large Separable Convolution Block\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass LightHeadBlock(nn.Module):\n    def __init__(self, in_chs, mid_chs=64, out_chs=256, kernel_size=15):\n        super(LightHeadBlock, self).__init__()\n        pad = int((kernel_size - 1) / 2)\n\n        # kernel size had better be odd number so as to avoid alignment error\n        self.conv_l = nn.Sequential(OrderedDict([(""conv_lu"", nn.Conv2d(in_chs, mid_chs,\n                                                                       kernel_size=(kernel_size, 1),\n                                                                       padding=(pad, 0))),\n                                                 (""conv_ld"", nn.Conv2d(mid_chs, out_chs,\n                                                                       kernel_size=(1, kernel_size),\n                                                                       padding=(0, pad)))]))\n\n        self.conv_r = nn.Sequential(OrderedDict([(""conv_ru"", nn.Conv2d(in_chs, mid_chs,\n                                                                       kernel_size=(1, kernel_size),\n                                                                       padding=(0, pad))),\n                                                 (""conv_rd"", nn.Conv2d(mid_chs, out_chs,\n                                                                       kernel_size=(kernel_size, 1),\n                                                                       padding=(pad, 0)))]))\n\n    def forward(self, x):\n        x_l = self.conv_l(x)\n        x_r = self.conv_r(x)\n        return torch.add(x_l, 1, x_r)\n\n\nif __name__ == ""__main__"":\n    import os\n    import time\n\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\n    net_h, net_w = 4, 6\n    dummy_in = torch.randn(12, 3, net_h, net_w).requires_grad_()\n\n    co_info = CoordInfo()\n\n    while True:\n        start_time = time.time()\n        dummy_out = co_info(dummy_in)\n        end_time = time.time()\n        print(""Inference time: {}s"".format(end_time - start_time))\n'"
modules/mobile.py,1,"b'import torch\nimport torch.nn as nn\nfrom modules.inplace_abn.iabn import InPlaceABN\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, dilate, expand_ratio):\n        """"""\n        InvertedResidual: Core block of the MobileNetV2\n        :param inp:    (int) Number of the input channels\n        :param oup:    (int) Number of the output channels\n        :param stride: (int) Stride used in the Conv3x3\n        :param dilate: (int) Dilation used in the Conv3x3\n        :param expand_ratio: (int) Expand ratio of the Channel Width of the Block\n        """"""\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        self.conv = nn.Sequential(\n            # step 1. point-wise convolution\n            nn.Conv2d(in_channels=inp, out_channels=inp * expand_ratio,\n                      kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False),\n            nn.BatchNorm2d(num_features=inp * expand_ratio),\n            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n\n            # step 2. depth-wise convolution\n            nn.Conv2d(in_channels=inp * expand_ratio, out_channels=inp * expand_ratio,\n                      kernel_size=3, stride=stride, padding=dilate, dilation=dilate,\n                      groups=inp * expand_ratio, bias=False),\n            nn.BatchNorm2d(num_features=inp * expand_ratio),\n            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n\n            # step 3. point-wise convolution\n            nn.Conv2d(in_channels=inp * expand_ratio, out_channels=oup,\n                      kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False),\n            nn.BatchNorm2d(num_features=oup),\n        )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass InvertedResidualIABN(nn.Module):\n    def __init__(self, inp, oup, stride, dilate, expand_ratio, norm_act=InPlaceABN):\n        """"""\n        InvertedResidual: Core block of the MobileNetV2\n        :param inp:    (int) Number of the input channels\n        :param oup:    (int) Number of the output channels\n        :param stride: (int) Stride used in the Conv3x3\n        :param dilate: (int) Dilation used in the Conv3x3\n        :param expand_ratio: (int) Expand ratio of the Channel Width of the Block\n        """"""\n        super(InvertedResidualIABN, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        self.conv = nn.Sequential(\n            # step 1. point-wise convolution\n            norm_act(inp),\n            nn.Conv2d(in_channels=inp, out_channels=inp * expand_ratio,\n                      kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False),\n\n            # step 2. depth-wise convolution\n            norm_act(inp * expand_ratio),\n            nn.Conv2d(in_channels=inp * expand_ratio, out_channels=inp * expand_ratio,\n                      kernel_size=3, stride=stride, padding=dilate, dilation=dilate,\n                      groups=inp * expand_ratio, bias=False),\n\n            # step 3. point-wise convolution\n            norm_act(inp * expand_ratio),\n            nn.Conv2d(in_channels=inp * expand_ratio, out_channels=oup,\n                      kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False)\n        )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n'"
modules/residual.py,1,"b'import torch.nn as nn\nfrom .attentions import SEBlock\nfrom collections import OrderedDict\nfrom modules.inplace_abn.iabn import ABN\n\n\nclass IdentityResidualBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 channels,\n                 stride=1,\n                 dilation=1,\n                 groups=1,\n                 norm_act=ABN,\n                 use_se=False,\n                 dropout=None):\n        """"""Configurable identity-mapping residual block\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels.\n        channels : list of int\n            Number of channels in the internal feature maps. Can either have two or three elements: if three construct\n            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then\n            `3 x 3` then `1 x 1` convolutions.\n        stride : int\n            Stride of the first `3 x 3` convolution\n        dilation : int\n            Dilation to apply to the `3 x 3` convolutions.\n        groups : int\n            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with\n            bottleneck blocks.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        dropout: callable\n            Function to create Dropout Module.\n        """"""\n        super(IdentityResidualBlock, self).__init__()\n\n        # Check parameters for inconsistencies\n        if len(channels) != 2 and len(channels) != 3:\n            raise ValueError(""channels must contain either two or three values"")\n        if len(channels) == 2 and groups != 1:\n            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")\n\n        is_bottleneck = len(channels) == 3\n        need_proj_conv = stride != 1 or in_channels != channels[-1]\n\n        self.bn1 = norm_act(in_channels)\n        if not is_bottleneck:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,\n                                    dilation=dilation)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    dilation=dilation))\n            ]\n            if dropout is not None:\n                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]\n        else:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    groups=groups, dilation=dilation)),\n                (""bn3"", norm_act(channels[1])),\n                (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))\n            ]\n            if use_se:\n                layers.append((""se_block"", SEBlock(channels[2], reduct_ratio=16)))\n\n            if dropout is not None:\n                layers = layers[0:5] + [(""dropout"", dropout())] + layers[5:]\n        self.convs = nn.Sequential(OrderedDict(layers))\n\n        if need_proj_conv:\n            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n\n    def forward(self, x):\n        if hasattr(self, ""proj_conv""):\n            bn1 = self.bn1(x)\n            shortcut = self.proj_conv(bn1)\n        else:\n            shortcut = x.clone()\n            bn1 = self.bn1(x)\n\n        out = self.convs(bn1)\n        out.add_(shortcut)\n\n        return out\n'"
modules/shuffle.py,6,"b'import torch\nimport torch.nn as nn\nfrom modules.inplace_abn.iabn import InPlaceABN\n\n\ndef channel_shuffle(x, groups):\n    batch_size, num_channels, height, width = x.size()\n\n    channels_per_group = num_channels // groups\n\n    # 1. Reshape\n    x = x.view(batch_size, groups, channels_per_group, height, width)\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # 2. Flatten\n    x = x.view(batch_size, -1, height, width)\n\n    return x\n\n\nclass ShuffleRes(nn.Module):\n    def __init__(self, in_chns, out_chns, stride, dilate, branch_model):\n        super(ShuffleRes, self).__init__()\n        self.branch_model = branch_model\n\n        assert stride in [1, 2]\n        self.stride = stride\n\n        mid_chns = out_chns // 2\n\n        if self.branch_model == 1:\n            self.branch2 = nn.Sequential(\n                # step 1. point-wise convolution\n                nn.Conv2d(mid_chns, mid_chns, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(mid_chns),\n                nn.LeakyReLU(inplace=True, negative_slope=0.01),\n\n                # step 2. depth-wise convolution\n                nn.Conv2d(mid_chns, mid_chns, kernel_size=3, stride=stride, padding=dilate,\n                          dilation=dilate, groups=mid_chns, bias=False),\n                nn.BatchNorm2d(mid_chns),\n\n                # step 3. point-wise convolution\n                nn.Conv2d(mid_chns, mid_chns, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(mid_chns),\n                nn.LeakyReLU(inplace=True, negative_slope=0.01),\n            )\n        else:\n            self.branch1 = nn.Sequential(\n                # step 1. depth-wise convolution\n                nn.Conv2d(in_chns, in_chns, kernel_size=3, stride=stride, padding=dilate,\n                          dilation=dilate,  groups=in_chns, bias=False),\n                nn.BatchNorm2d(in_chns),\n\n                # step 2. point-wise convolution\n                nn.Conv2d(in_chns, mid_chns, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(mid_chns),\n                nn.LeakyReLU(inplace=True, negative_slope=0.01),\n            )\n\n            self.branch2 = nn.Sequential(\n                # step 1. point-wise convolution\n                nn.Conv2d(in_chns, mid_chns, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(mid_chns),\n                nn.LeakyReLU(inplace=True, negative_slope=0.01),\n\n                # step 2. depth-wise convolution\n                nn.Conv2d(mid_chns, mid_chns, kernel_size=3, stride=stride, padding=dilate,\n                          dilation=dilate, groups=mid_chns, bias=False),\n                nn.BatchNorm2d(mid_chns),\n\n                # step 3. point-wise convolution\n                nn.Conv2d(mid_chns, mid_chns, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(mid_chns),\n                nn.LeakyReLU(inplace=True, negative_slope=0.01),\n            )\n\n    def forward(self, x):\n        if 1 == self.branch_model:\n            x1 = x[:, :(x.shape[1] // 2), :, :]\n            x2 = x[:, (x.shape[1] // 2):, :, :]\n            out = torch.cat((x1, self.branch2(x2)), dim=1)\n        elif 2 == self.branch_model:\n            out = torch.cat((self.branch1(x), self.branch2(x)), dim=1)\n\n        return channel_shuffle(out, 2)\n\n\nclass ShuffleResIABN(nn.Module):\n    def __init__(self, in_chns, out_chns, stride, dilate, branch_model, norm_act=InPlaceABN):\n        super(ShuffleResIABN, self).__init__()\n        self.branch_model = branch_model\n        self.stride = stride\n        assert stride in [1, 2]\n\n        mid_chns = out_chns // 2\n\n        if self.branch_model == 1:\n            self.in_norm = norm_act(mid_chns)\n            self.branch2 = nn.Sequential(\n                # step 1. point-wise convolution\n                nn.Conv2d(mid_chns, mid_chns, 1, 1, 0, bias=False),\n\n                # step 2. depth-wise convolution\n                norm_act(mid_chns),\n                nn.Conv2d(mid_chns, mid_chns, kernel_size=3, stride=stride, padding=dilate,\n                          dilation=dilate, groups=mid_chns, bias=False),\n\n                # step 3. point-wise convolution\n                norm_act(mid_chns),\n                nn.Conv2d(mid_chns, mid_chns, 1, 1, 0, bias=False),\n            )\n        else:\n            self.in_norm = norm_act(in_chns)\n            self.branch1 = nn.Sequential(\n                # step 1. depth-wise convolution\n                nn.Conv2d(in_chns, in_chns, kernel_size=3, stride=stride, padding=dilate,\n                          dilation=dilate,  groups=in_chns, bias=False),\n\n                # step 2. point-wise convolution\n                norm_act(in_chns),\n                nn.Conv2d(in_chns, mid_chns, 1, 1, 0, bias=False),\n            )\n\n            self.branch2 = nn.Sequential(\n                # step 1. point-wise convolution\n                nn.Conv2d(in_chns, mid_chns, 1, 1, 0, bias=False),\n\n                # step 2. depth-wise convolution\n                norm_act(mid_chns),\n                nn.Conv2d(mid_chns, mid_chns, kernel_size=3, stride=stride, padding=dilate,\n                          dilation=dilate, groups=mid_chns, bias=False),\n\n                # step 3. point-wise convolution\n                norm_act(mid_chns),\n                nn.Conv2d(mid_chns, mid_chns, 1, 1, 0, bias=False),\n            )\n\n    def forward(self, x):\n        if 1 == self.branch_model:\n            x = self.in_norm(x)\n            x1 = x[:, :(x.shape[1] // 2), :, :]\n            x2 = x[:, (x.shape[1] // 2):, :, :]\n            out = torch.cat((x1, self.branch2(x2)), dim=1)\n        elif 2 == self.branch_model:\n            x = self.in_norm(x)\n            out = torch.cat((self.branch1(x), self.branch2(x)), dim=1)\n\n        return channel_shuffle(out, groups=2)\n'"
modules/usm.py,16,"b'import math\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom modules.inplace_abn.iabn import ABN\n\n\ndef gauss_kernel(kernel_size, sigma):\n    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n    x_cord = torch.arange(kernel_size)\n    x_grid = x_cord.repeat(kernel_size).view(kernel_size, kernel_size)\n    y_grid = x_grid.t()\n    xy_grid = torch.stack([x_grid, y_grid], dim=-1)\n\n    mean = (kernel_size - 1) / 2.\n    variance = sigma ** 2\n\n    # Calculate the 2-dimensional gaussian kernel which is\n    # the product of two gaussian distributions for two different\n    # variables (in this case called x and y)\n    gaussian_kernel = (1. / (2. * math.pi * variance)) * \\\n                      torch.exp(-torch.sum((xy_grid.float() - mean) ** 2, dim=-1) / (2. * variance))\n\n    # Make sure sum of values in gaussian kernel equals 1.\n    gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n\n    # Reshape to 2d depth-wise convolution weight\n    gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)  # [1, 1 , H, W]\n    return gaussian_kernel\n\n\n# class UnsharpMask(nn.Module):\n#     def __init__(self, channels, padding=3, amount=1.0, threshold=0, norm_act=ABN):\n#         super(UnsharpMask, self).__init__()\n#         self.channels = channels\n#         self.padding = padding\n#\n#         self.amount = amount\n#         self.threshold = threshold\n#\n#         self.norm_act = norm_act(channels)\n#\n#     def forward(self, x, gauss_filter):\n#         x = self.norm_act(x)\n#         res = x.clone()\n#\n#         gauss_filter = gauss_filter.repeat(self.channels, 1, 1, 1)\n#         blurred = F.conv2d(input=x, weight=gauss_filter, stride=1, padding=self.padding, groups=x.size(1), bias=None)\n#\n#         sharpened = res * (self.amount + 1.0) - blurred * self.amount\n#\n#         if self.threshold > 0:\n#             sharpened = torch.where(torch.abs(res - blurred) < self.threshold, sharpened, res)\n#\n#         return sharpened  # , res - blurred\n\n\nclass UnsharpMaskV2(nn.Module):\n    def __init__(self, channel, kernel_size=7, padding=3, amount=1.0, threshold=0, norm_act=ABN):\n        super(UnsharpMaskV2, self).__init__()\n        self.kernel_size = kernel_size\n        self.padding = padding\n\n        self.amount = amount\n        self.threshold = threshold\n        self.norm_act = norm_act(channel)\n\n    def forward(self, x):\n        x = self.norm_act(x)\n        res = x.clone()\n\n        blurred = F.avg_pool2d(input=x, kernel_size=self.kernel_size, stride=1, padding=self.padding,\n                               ceil_mode=False, count_include_pad=True)\n\n        sharpened = res * (self.amount + 1.0) - blurred * self.amount\n\n        if self.threshold > 0:\n            sharpened = torch.where(torch.abs(res - blurred) < self.threshold, sharpened, res)\n\n        return sharpened  # , res - blurred\n\n\nclass GaussianBlur(nn.Module):\n    def __init__(self, channels, kernel_size=11, padding=5, sigma=1.6):\n        super(GaussianBlur, self).__init__()\n        self.kernel_size = kernel_size\n        self.channels = channels\n        self.padding = padding\n        self.sigma = sigma\n\n        weights = self.calculate_weights()\n        self.register_buffer(\'gaussian_filter\', weights)\n\n    def calculate_weights(self):\n        # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n        x_cord = torch.arange(self.kernel_size)\n        x_grid = x_cord.repeat(self.kernel_size).view(self.kernel_size, self.kernel_size)\n        y_grid = x_grid.t()\n        xy_grid = torch.stack([x_grid, y_grid], dim=-1)\n\n        mean = (self.kernel_size - 1) / 2.\n        variance = self.sigma ** 2\n\n        # Calculate the 2-dimensional gaussian kernel which is\n        # the product of two gaussian distributions for two different\n        # variables (in this case called x and y)\n        gaussian_kernel = (1. / (2. * math.pi * variance)) * \\\n                          torch.exp(-torch.sum((xy_grid.float() - mean) ** 2, dim=-1) / (2. * variance))\n\n        # Make sure sum of values in gaussian kernel equals 1.\n        gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n\n        # Reshape to 2d depthwise convolutional weight\n        gaussian_kernel = gaussian_kernel.view(1, 1, self.kernel_size, self.kernel_size)\n        gaussian_kernel = gaussian_kernel.repeat(self.channels, 1, 1, 1)\n        return gaussian_kernel\n\n    def forward(self, x):\n        return F.conv2d(input=x, weight=self.gaussian_filter,\n                        stride=1, padding=self.padding, groups=x.size(1), bias=None)\n\n\nclass UnsharpMask(nn.Module):\n    def __init__(self, channels, kernel_size=11, padding=5, sigma=1.0, amount=1.0, threshold=0, norm_act=ABN):\n        super(UnsharpMask, self).__init__()\n        self.amount = amount\n        self.threshold = threshold\n        self.norm_act = norm_act(channels)\n\n        self.gauss_blur = GaussianBlur(channels=channels, kernel_size=kernel_size, padding=padding, sigma=sigma)\n\n    def forward(self, x):\n        x = self.norm_act(x)\n\n        res = x.clone()\n        blurred = self.gauss_blur(x)\n\n        sharpened = res * (self.amount + 1.0) - blurred * self.amount\n\n        if self.threshold > 0:\n            sharpened = torch.where(torch.abs(res - blurred) < self.threshold, sharpened, res)\n\n        return sharpened\n\n\nif __name__ == ""__main__"":\n    import imageio\n    import matplotlib\n    import matplotlib.pyplot as plt\n\n    image = imageio.imread(""/home/liuhuijun/PycharmProjects/LightNet++/deploy/cityscapes/examples/images/munster_000168_000019_leftImg8bit.png"")\n    # image = np.array(image[:, :, ::-1], dtype=np.uint8)\n    img_copy = image.copy()\n\n    image = image.transpose(2, 0, 1)  # From HWC to CHW (For PyTorch we use N*C*H*W tensor)\n    image = torch.from_numpy(image).float()\n    image = torch.unsqueeze(image, dim=0).cuda()  # [N, C, H, W]\n\n    usm = UnsharpMask(channels=3, kernel_size=11, padding=5, sigma=1.6).cuda()\n    usm.eval()\n\n    with torch.no_grad():\n        dummy_out = usm(image)\n        dummy_out = np.squeeze(dummy_out.cpu().numpy()).transpose(1, 2, 0).astype(np.uint8)\n        # mask = np.squeeze(mask.cpu().numpy()).transpose(1, 2, 0).astype(np.uint8)\n        # blur = np.squeeze(blur.cpu().numpy()).transpose(1, 2, 0).astype(np.uint8)\n\n        fig, axs = plt.subplots(ncols=3, figsize=(13.5, 6))\n        axs[0].imshow(img_copy)\n        axs[0].get_xaxis().set_visible(False)\n        axs[0].get_yaxis().set_visible(False)\n        axs[0].set_title(""Org Image"")\n\n        axs[1].imshow(dummy_out)\n        axs[1].get_xaxis().set_visible(False)\n        axs[1].get_yaxis().set_visible(False)\n        axs[1].set_title(""Sharpened Image"")\n\n        # axs[2].imshow(mask, cmap=""gray"")\n        # axs[2].get_xaxis().set_visible(False)\n        # axs[2].get_yaxis().set_visible(False)\n        # axs[2].set_title(""Mask Image"")\n\n        plt.tight_layout()\n        plt.show()\n'"
netviz/__init__.py,0,b''
netviz/feat_viz.py,5,"b'import torch\nimport torch.nn as nn\n\n\nclass HookBasedFeatureExtractor(nn.Module):\n    def __init__(self, model, layer_name, upscale=False):\n        super(HookBasedFeatureExtractor, self).__init__()\n\n        self.model = model\n        self.model.eval()\n        self.layer_name = layer_name\n        self.outputs_size = None\n        self.outputs = None\n        self.inputs = None\n        self.inputs_size = None\n        self.upscale = upscale\n\n    def get_input_array(self, m, i, o):\n        if isinstance(i, tuple):\n            self.inputs = [i[index].data.clone() for index in range(len(i))]\n            self.inputs_size = [input.size() for input in self.inputs]\n        else:\n            self.inputs = i.data.clone()\n            self.inputs_size = self.input.size()\n        print(\'Input Array Size: \', self.inputs_size)\n\n    def get_output_array(self, m, i, o):\n        if isinstance(o, tuple):\n            self.outputs = [o[index].data.clone() for index in range(len(o))]\n            self.outputs_size = [output.size() for output in self.outputs]\n        else:\n            self.outputs = o.data.clone()\n            self.outputs_size = self.outputs.size()\n        print(\'Output Array Size: \', self.outputs_size)\n\n    def rescale_output_array(self, newsize):\n        us = nn.Upsample(size=newsize[2:], mode=\'bilinear\')\n        if isinstance(self.outputs, list):\n            for index in range(len(self.outputs)): self.outputs[index] = us(self.outputs[index]).data()\n        else:\n            self.outputs = us(self.outputs).data()\n\n    def forward(self, x):\n        layers = self.model._modules[\'module\']._modules\n        target_layer = layers[self.layer_name]\n\n        # Collect the output tensor\n        h_inp = target_layer.register_forward_hook(self.get_input_array)\n        h_out = target_layer.register_forward_hook(self.get_output_array)\n\n        self.model(x)\n        h_inp.remove()\n        h_out.remove()\n\n        # Rescale the feature-map if it\'s required\n        if self.upscale:\n            self.rescale_output_array(x.size())\n\n        return self.inputs, self.outputs\n\n\ndef data_preprocess(check, mean_check, std_check):\n    check = check.astype(np.float32) / 255.0\n    check -= mean_check\n    check /= std_check\n\n    # HWC -> CHW\n    check = check.transpose(2, 0, 1)\n    check = np.expand_dims(check, 0)\n    check = torch.from_numpy(check).float()\n\n    return check\n\n\nif __name__ == ""__main__"":\n    import os\n    import numpy as np\n    from PIL import Image\n    import scipy.misc as misc\n\n    from modules.inplace_abn.iabn import InPlaceABNSync\n    from models.mobilenetv2plus import MobileNetV2Plus\n    from models.shufflenetv2plus import ShuffleNetV2Plus\n    import torch.backends.cudnn as cudnn\n    from functools import partial\n    import matplotlib\n    import matplotlib.pyplot as plt\n\n    try:\n        from apex.fp16_utils import *\n        from apex import amp\n        import apex\n    except ImportError:\n        raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to run this example."")\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""3""\n    cudnn.benchmark = True\n    assert torch.backends.cudnn.enabled, ""fp16 mode requires cudnn backend to be enabled.""\n    # Initialize Amp\n    amp_handle = amp.init(enabled=True)\n\n    root = ""/home/huijun/TrainLog/weights""\n    dataset = ""deepdrive""  # cityscapes  deepdrive\n    method = ""shufflenetv2plus_x1.0""  # mobilenetv2plus  shufflenetv2plus_x1.0 shufflenetv2plus_x0.5\n    # model = MobileNetV2Plus(num_classes=19, width_multi=1.0, fuse_chns=512,\n    #                         aspp_chns=256, aspp_dilate=(12, 24, 36),\n    #                         norm_act=partial(InPlaceABNSync, activation=""leaky_relu"", slope=0.01))\n\n    model = ShuffleNetV2Plus(num_classes=19, fuse_chns=512,\n                             aspp_chns=256, aspp_dilate=(12, 24, 36), width_multi=1.0,\n                             norm_act=partial(InPlaceABNSync, activation=""leaky_relu"", slope=0.01))\n\n    # model = ShuffleNetV2Plus(num_classes=19, fuse_chns=256, aspp_chns=128, aspp_dilate=(12, 24, 36), width_multi=0.5,\n    #                          norm_act=partial(InPlaceABNSync, activation=""leaky_relu"", slope=0.01))\n    model = apex.parallel.convert_syncbn_model(model)\n    model = nn.DataParallel(model, device_ids=[0]).cuda()\n\n    pre_weight = torch.load(os.path.join(root, ""{}_{}_best_model.pkl"").format(dataset, method))\n\n    # mean = pre_weight[\'mean\']\n    # std = pre_weight[\'std\']\n    mean = [0.2997, 0.3402, 0.3072]\n    std = [0.1549, 0.1579, 0.1552]\n    pre_weight = pre_weight[""model_state""]\n    model_dict = model.state_dict()\n    pretrained_dict = {k: v for k, v in pre_weight.items() if k in model_dict}\n    model_dict.update(pretrained_dict)\n    model.load_state_dict(model_dict)\n\n    layer_name = ""usm""\n    extractor = HookBasedFeatureExtractor(model=model, layer_name=layer_name).cuda()\n\n    image_root = ""/home/huijun/PycharmProjects/LightNet++/netviz/images""\n    image_name = ""munster_000168_000019_leftImg8bit.png""\n    image_path = os.path.join(image_root, image_name)\n\n    net_h, net_w = 1024, 2048\n    img_org = Image.open(image_path).convert(\'RGB\')\n    img_org = np.array(img_org, dtype=np.uint8).copy()\n    img_org_copy = img_org.copy()\n    img_org = np.array(img_org[:, :, ::-1], dtype=np.uint8)\n\n    image = data_preprocess(img_org, mean, std).cuda()\n    _, out_feat = extractor.forward(image)\n\n    out_feat = out_feat.view(-1, out_feat.size(2), out_feat.size(3)).cpu().numpy()\n    out_feat = np.squeeze(out_feat)\n\n    # save_root = ""/home/huijun/PycharmProjects/LightNet++/netviz/feat_vizs""\n    # save_root = os.path.join(save_root, method, layer_name, image_name)\n    # if not os.path.exists(save_root):\n    #     os.makedirs(save_root)\n\n    for chn, feat in enumerate(out_feat):\n        print(""> +++++++++++++++++++++++++++++++++++++++++++++ <"")\n        print(""> Processing Channel: {}..."".format(chn))\n        feat = 255.0 * ((feat - feat.min().min()) / (feat.max().max() - feat.min().min() + 1e-6))\n        feat = feat.astype(np.uint8)\n        feat = misc.imresize(feat, (net_h, net_w), interp=""bilinear"")\n\n        # cv2.namedWindow(""ImageOut"", cv2.WINDOW_NORMAL)\n        # cv2.imshow(""ImageOut"", feat)\n        # cv2.waitKey()\n\n        fig, axs = plt.subplots(ncols=2)\n        fig.suptitle(layer_name, fontsize=16)\n\n        axs[0].imshow(img_org_copy)\n        axs[0].get_xaxis().set_visible(False)\n        axs[0].get_yaxis().set_visible(False)\n        axs[0].set_title(""Base Image"")\n\n        axs[1].imshow(feat)\n        # pos = axs[1].imshow(img_org_copy, alpha=0.025)\n        axs[1].get_xaxis().set_visible(False)\n        axs[1].get_yaxis().set_visible(False)\n        axs[1].set_title(""HeatMap on Image"")\n        # fig.colorbar(pos, ax=axs[1])\n        # plt.savefig(os.path.join(save_root, str(chn) + ""_heatmap_2d.png""))\n\n        plt.show()\n'"
utils/__init__.py,0,b''
utils/adabound.py,6,"b'import math\nimport torch\nfrom torch.optim import Optimizer\n\n\nclass AdaBound(Optimizer):\n    """"""Implements AdaBound algorithm.\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): Adam learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n        https://openreview.net/forum?id=Bkg3g2R9FX\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\n                 eps=1e-8, weight_decay=0, amsbound=False):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        if not 0.0 <= final_lr:\n            raise ValueError(""Invalid final learning rate: {}"".format(final_lr))\n        if not 0.0 <= gamma < 1.0:\n            raise ValueError(""Invalid gamma parameter: {}"".format(gamma))\n        defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\n                        weight_decay=weight_decay, amsbound=amsbound)\n        super(AdaBound, self).__init__(params, defaults)\n\n        self.base_lrs = list(map(lambda group: group[\'lr\'], self.param_groups))\n\n    def __setstate__(self, state):\n        super(AdaBound, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsbound\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'Adam does not support sparse gradients, please consider SparseAdam instead\')\n                amsbound = group[\'amsbound\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n                    if amsbound:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsbound:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsbound:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                # Applies bounds on actual learning rate\n                # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay\n                final_lr = group[\'final_lr\'] * group[\'lr\'] / base_lr\n                lower_bound = final_lr * (1 - 1 / (group[\'gamma\'] * state[\'step\'] + 1))\n                upper_bound = final_lr * (1 + 1 / (group[\'gamma\'] * state[\'step\']))\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom).clamp_(lower_bound, upper_bound).mul_(exp_avg)\n\n                p.data.add_(-step_size)\n\n        return loss\n'"
utils/losses.py,49,"b'# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# LightNet++: Boosted Light-weighted Networks for Real-time Semantic Segmentation\n# ---------------------------------------------------------------------------------------------------------------- #\n# Compute Metrics for Semantic Segmentation\n# class:\n#       > BootstrappedCrossEntropy2D\n#       > DiceLoss2D\n# ---------------------------------------------------------------------------------------------------------------- #\n# Author: Huijun Liu M.Sc.\n# Date:   10.10.2018\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nimport torch.nn.functional as F\nimport scipy.ndimage as nd\nimport torch.nn as nn\nimport numpy as np\nimport torch\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Bootstrapped CrossEntropy2D\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass BootstrappedCrossEntropy2D(nn.Module):\n    def __init__(self, top_k=128, ignore_index=-100):\n        """"""\n        Bootstrapped CrossEntropy2D: The pixel-bootstrapped cross entropy loss\n\n        :param weight: <torch.Tensor, optional> A manual rescaling weight given to each class.\n                                                If given, has to be a Tensor of size C, where C = number of classes.\n        :param ignore_index: <int, optional> Specifies a target value that is ignored and does not\n                                             contribute to the input gradient.\n        """"""\n        super(BootstrappedCrossEntropy2D, self).__init__()\n        self.weight = torch.FloatTensor([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n                                         5.58540548, 3.56563995, 0.12704978, 1., 0.46783719, 1.34551528,\n                                         5.29974114, 0.28342531, 0.9396095, 0.81551811, 0.42679146, 3.6399074,\n                                         2.78376194]).cuda()\n        self.top_k = top_k\n        self.ignore_index = ignore_index\n\n    def _update_topk(self, top_k):\n        self.top_k = top_k\n\n    def forward(self, predictions, targets):\n        """"""\n\n        :param predictions: <torch.FloatTensor> Network Predictions of size [N, C, H, W], where C = number of classes\n        :param targets: <torch.LongTensor> Ground Truth label of size [N, H, W]\n        :param top_k: <int> Top-K worst predictions\n        :return: <torch.Tensor> loss\n        """"""\n        loss_fuse = 0.0\n        if isinstance(predictions, tuple):\n            for predict in predictions:\n                batch_size, channels, feat_h, feat_w = predict.size()\n\n                # ------------------------------------------------------------------------ #\n                # 1. Compute CrossEntropy Loss without Reduction\n                # ------------------------------------------------------------------------ #\n                # target_mask = (targets >= 0) * (targets != self.ignore_index)\n                # targets = targets[target_mask]\n\n                batch_loss = F.cross_entropy(input=predict, target=targets,\n                                             weight=None,\n                                             ignore_index=self.ignore_index,  reduction=\'none\')\n\n                # ------------------------------------------------------------------------ #\n                # 2. Bootstrap from each image not entire batch\n                #    For each element in the batch, collect the top K worst predictions\n                # ------------------------------------------------------------------------ #\n                loss = 0.0\n\n                for idx in range(batch_size):\n                    single_loss = batch_loss[idx].view(feat_h*feat_w)\n\n                    topk_loss, _ = single_loss.topk(self.top_k)\n                    loss += topk_loss.sum() / self.top_k\n\n                loss_fuse += loss / float(batch_size)\n        else:\n            batch_size, channels, feat_h, feat_w = predictions.size()\n\n            # ------------------------------------------------------------------------ #\n            # 1. Compute CrossEntropy Loss without Reduction\n            # ------------------------------------------------------------------------ #\n            # target_mask = (targets >= 0) * (targets != self.ignore_index)\n            # targets = targets[target_mask]\n\n            batch_loss = F.cross_entropy(input=predictions, target=targets,\n                                         weight=None,\n                                         ignore_index=self.ignore_index, reduction=\'none\')\n\n            # ------------------------------------------------------------------------ #\n            # 2. Bootstrap from each image not entire batch\n            #    For each element in the batch, collect the top K worst predictions\n            # ------------------------------------------------------------------------ #\n            loss = 0.0\n            for idx in range(batch_size):\n                single_loss = batch_loss[idx].view(feat_h * feat_w)\n\n                topk_loss, _ = single_loss.topk(self.top_k)\n                loss += topk_loss.sum() / self.top_k\n\n            loss_fuse += loss / float(batch_size)\n\n        return loss_fuse\n\n\nclass CriterionDSN(nn.Module):\n    \'\'\'\n    DSN : We need to consider two supervision for the model.\n    \'\'\'\n    def __init__(self, top_k=512*512, ignore_index=255):\n        super(CriterionDSN, self).__init__()\n        self.ignore_index = ignore_index\n        self.criterion = BootstrappedCrossEntropy2D(top_k=top_k, ignore_index=ignore_index)\n\n    def _update_topk(self, top_k):\n        self.criterion._update_topk(top_k)\n\n    def forward(self, predictions, targets):\n        loss1 = self.criterion(predictions[0], targets)\n        loss2 = self.criterion(predictions[1], targets)\n\n        return loss1 + loss2 * 0.4\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Bootstrapped CrossEntropy2D\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass OHEMBootstrappedCrossEntropy2D(nn.Module):\n    def __init__(self, factor=8.0, thresh=0.7, min_kept=100000, top_k=128, ignore_index=-100):\n        """"""\n        Bootstrapped CrossEntropy2D: The pixel-bootstrapped cross entropy loss\n\n        :param weight: <torch.Tensor, optional> A manual rescaling weight given to each class.\n                                                If given, has to be a Tensor of size C, where C = number of classes.\n        :param ignore_index: <int, optional> Specifies a target value that is ignored and does not\n                                             contribute to the input gradient.\n        """"""\n        super(OHEMBootstrappedCrossEntropy2D, self).__init__()\n        self.weight = torch.FloatTensor([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n                                         5.58540548, 3.56563995, 0.12704978, 1., 0.46783719, 1.34551528,\n                                         5.29974114, 0.28342531, 0.9396095, 0.81551811, 0.42679146, 3.6399074,\n                                         2.78376194])\n        self.top_k = top_k\n        self.ignore_index = ignore_index\n\n        self.factor = factor\n        self.thresh = thresh\n        self.min_kept = int(min_kept)\n\n    def find_threshold(self, np_predict, np_target):\n        # downsample 1/8\n        factor = self.factor\n        predict = nd.zoom(np_predict, (1.0, 1.0, 1.0 / factor, 1.0 / factor), order=1)\n        target = nd.zoom(np_target, (1.0, 1.0 / factor, 1.0 / factor), order=0)\n\n        n, c, h, w = predict.shape\n        min_kept = self.min_kept // (factor * factor)  # int(self.min_kept_ratio * n * h * w)\n\n        input_label = target.ravel().astype(np.int32)\n        input_prob = np.rollaxis(predict, 1).reshape((c, -1))\n\n        valid_flag = input_label != self.ignore_index\n        valid_inds = np.where(valid_flag)[0]\n        label = input_label[valid_flag]\n        num_valid = valid_flag.sum()\n\n        threshold = 1.0\n        if min_kept >= num_valid:\n            threshold = 1.0\n        elif num_valid > 0:\n            prob = input_prob[:, valid_flag]\n            pred = prob[label, np.arange(len(label), dtype=np.int32)]\n            threshold = self.thresh\n\n            if min_kept > 0:\n                k_th = min(len(pred), int(min_kept)) - 1\n                new_array = np.partition(pred, int(k_th))\n                new_threshold = new_array[k_th]\n\n                if new_threshold > self.thresh:\n                    threshold = new_threshold\n        return threshold\n\n    def generate_new_target(self, predict, target):\n        np_predict = predict.data.cpu().numpy()\n        np_target = target.data.cpu().numpy()\n        n, c, h, w = np_predict.shape\n\n        threshold = self.find_threshold(np_predict, np_target)\n\n        input_label = np_target.ravel().astype(np.int32)\n        input_prob = np.rollaxis(np_predict, 1).reshape((c, -1))\n\n        valid_flag = input_label != self.ignore_index\n        valid_inds = np.where(valid_flag)[0]\n        label = input_label[valid_flag]\n        num_valid = valid_flag.sum()\n\n        if num_valid > 0:\n            prob = input_prob[:, valid_flag]\n            pred = prob[label, np.arange(len(label), dtype=np.int32)]\n            kept_flag = pred <= threshold\n            valid_inds = valid_inds[kept_flag]\n            # print(\'Labels: {} {}\'.format(len(valid_inds), threshold))\n\n        label = input_label[valid_inds].copy()\n        input_label.fill(self.ignore_index)\n        input_label[valid_inds] = label\n        new_target = torch.from_numpy(input_label.reshape(target.size())).long().cuda(target.get_device())\n\n        return new_target\n\n    def _update_topk(self, top_k):\n        self.top_k = top_k\n\n    def forward(self, predictions, targets):\n        """"""\n\n        :param predictions: <torch.FloatTensor> Network Predictions of size [N, C, H, W], where C = number of classes\n        :param targets: <torch.LongTensor> Ground Truth label of size [N, H, W]\n        :param top_k: <int> Top-K worst predictions\n        :return: <torch.Tensor> loss\n        """"""\n        loss_fuse = 0.0\n        if isinstance(predictions, tuple):\n            for predict in predictions:\n                batch_size, channels, feat_h, feat_w = predict.size()\n\n                # ------------------------------------------------------------------------ #\n                # 1. Compute CrossEntropy Loss without Reduction\n                # ------------------------------------------------------------------------ #\n                # target_mask = (targets >= 0) * (targets != self.ignore_index)\n                # targets = targets[target_mask]\n\n                input_prob = F.softmax(predict, dim=1)\n                targets = self.generate_new_target(input_prob, targets)\n                batch_loss = F.cross_entropy(input=predict, target=targets,\n                                             weight=self.weight.cuda(predictions.get_device()),\n                                             ignore_index=self.ignore_index,  reduction=\'none\')\n\n                # ------------------------------------------------------------------------ #\n                # 2. Bootstrap from each image not entire batch\n                #    For each element in the batch, collect the top K worst predictions\n                # ------------------------------------------------------------------------ #\n                loss = 0.0\n\n                for idx in range(batch_size):\n                    single_loss = batch_loss[idx].view(feat_h*feat_w)\n\n                    topk_loss, _ = single_loss.topk(self.top_k)\n                    loss += topk_loss.sum() / self.top_k\n\n                loss_fuse += loss / float(batch_size)\n        else:\n            batch_size, channels, feat_h, feat_w = predictions.size()\n\n            # ------------------------------------------------------------------------ #\n            # 1. Compute CrossEntropy Loss without Reduction\n            # ------------------------------------------------------------------------ #\n            # target_mask = (targets >= 0) * (targets != self.ignore_index)\n            # targets = targets[target_mask]\n            input_prob = F.softmax(predictions, dim=1)\n            targets = self.generate_new_target(input_prob, targets)\n\n            batch_loss = F.cross_entropy(input=predictions, target=targets,\n                                         weight=self.weight.cuda(predictions.get_device()),\n                                         ignore_index=self.ignore_index, reduction=\'none\')\n\n            # ------------------------------------------------------------------------ #\n            # 2. Bootstrap from each image not entire batch\n            #    For each element in the batch, collect the top K worst predictions\n            # ------------------------------------------------------------------------ #\n            loss = 0.0\n\n            for idx in range(batch_size):\n                single_loss = batch_loss[idx].view(feat_h * feat_w)\n\n                topk_loss, _ = single_loss.topk(self.top_k)\n                loss += topk_loss.sum() / self.top_k\n\n            loss_fuse += loss / float(batch_size)\n\n        return loss_fuse\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Focal Loss\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass FocalLoss2D(nn.Module):\n    """"""\n    Focal Loss, which is proposed in:\n        ""Focal Loss for Dense Object Detection (https://arxiv.org/abs/1708.02002v2)""\n    """"""\n    def __init__(self, top_k=128, ignore_label=250, alpha=0.25, gamma=2):\n        """"""\n        Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n\n        :param ignore_label:  <int> ignore label\n        :param alpha:         <torch.Tensor> the scalar factor\n        :param gamma:         <float> gamma > 0;\n                                      reduces the relative loss for well-classified examples (probabilities > .5),\n                                      putting more focus on hard, mis-classified examples\n        """"""\n        super(FocalLoss2D, self).__init__()\n        self.weight = torch.FloatTensor([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n                                         5.58540548, 3.56563995, 0.12704978, 1., 0.46783719, 1.34551528,\n                                         5.29974114, 0.28342531, 0.9396095, 0.81551811, 0.42679146, 3.6399074,\n                                         2.78376194])\n\n        self.alpha = alpha\n        self.gamma = gamma\n        self.top_k = top_k\n\n        self.ignore_label = ignore_label\n        self.one_hot = torch.eye(self.num_classes)\n\n    def _update_topk(self, top_k):\n        self.top_k = top_k\n\n    def forward(self, predictions, targets):\n        """"""\n\n        :param predictions: <torch.FloatTensor> Network Predictions of size [N, C, H, W], where C = number of classes\n        :param targets: <torch.LongTensor> Ground Truth label of size [N, H, W]\n        :return: <torch.Tensor> loss\n        """"""\n        assert not targets.requires_grad\n\n        loss_fuse = 0.0\n        if isinstance(predictions, tuple):\n            for predict in predictions:\n                batch_size, channels, feat_h, feat_w = predict.size()\n\n                # ------------------------------------------------------------------------ #\n                # 1. Compute CrossEntropy Loss without Reduction\n                # ------------------------------------------------------------------------ #\n                batch_loss = F.cross_entropy(input=predict, target=targets,\n                                             weight=self.weight.cuda(predictions.get_device()),\n                                             ignore_index=self.ignore_index, reduction=\'none\')\n\n                # ------------------------------------------------------------------------ #\n                # 2. Bootstrap from each image not entire batch\n                #    For each element in the batch, collect the top K worst predictions\n                # ------------------------------------------------------------------------ #\n                loss = 0.0\n\n                for idx in range(batch_size):\n                    single_loss = batch_loss[idx].view(feat_h * feat_w)\n\n                    topk_loss, _ = single_loss.topk(self.top_k)\n                    loss += topk_loss.sum() / self.top_k\n\n                loss_fuse += loss / float(batch_size)\n\n        else:\n            batch_size, channels, feat_h, feat_w = predictions.size()\n\n            # ------------------------------------------------------------------------ #\n            # 1. Compute CrossEntropy Loss without Reduction\n            # ------------------------------------------------------------------------ #\n\n            batch_loss = F.cross_entropy(input=predictions, target=targets,\n                                         weight=self.weight.cuda(predictions.get_device()),\n                                         ignore_index=self.ignore_index, reduction=\'none\')\n\n            # ------------------------------------------------------------------------ #\n            # 2. Bootstrap from each image not entire batch\n            #    For each element in the batch, collect the top K worst predictions\n            # ------------------------------------------------------------------------ #\n            loss = 0.0\n\n            for idx in range(batch_size):\n                single_loss = batch_loss[idx].view(feat_h * feat_w)\n\n                topk_loss, _ = single_loss.topk(self.top_k)\n                loss += topk_loss.sum() / self.top_k\n\n            loss_fuse += loss / float(batch_size)\n\n        log_pt = -loss_fuse\n        return -((1.0 - torch.exp(log_pt)) ** self.gamma) * self.alpha * log_pt\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Semantic Encoding Loss\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass SemanticEncodingLoss(nn.Module):\n    def __init__(self, num_classes=19, ignore_label=250, weight=None, alpha=0.25):\n        """"""\n        Semantic Encoding Loss\n        :param num_classes: <int> Number of classes\n        :param ignore_label: <int, optional> Specifies a target value that is ignored and does not\n                                             contribute to the input gradient.\n        :param weight: <torch.Tensor, optional> A manual rescaling weight given to each class.\n                                                If given, has to be a Tensor of size C, where C = number of classes.\n        :param alpha: <float> A manual rescaling weight given to Semantic Encoding Loss\n        """"""\n        super(SemanticEncodingLoss, self).__init__()\n        self.alpha = alpha\n\n        self.num_classes = num_classes\n        self.weight = weight\n        self.ignore_label = ignore_label\n\n    def __unique_encode(self, msk_targets):\n        """"""\n\n        :param cls_targets: <torch.FloatTensor> Network Predictions of size [N, C, H, W], where C = number of classes\n        :return:\n        """"""\n        batch_size, _, _ = msk_targets.size()\n        target_mask = (msk_targets >= 0) * (msk_targets != self.ignore_label)\n        cls_targets = [msk_targets[idx].masked_select(target_mask[idx]) for idx in np.arange(batch_size)]\n\n        # unique_cls = [np.unique(label.numpy(), return_counts=True) for label in cls_targets]\n        unique_cls = [torch.unique(label) for label in cls_targets]\n\n        encode = torch.zeros(batch_size, self.num_classes, dtype=torch.float32, requires_grad=False)\n\n        for idx in np.arange(batch_size):\n            index = unique_cls[idx].long()\n            encode[idx].index_fill_(dim=0, index=index, value=1.0)\n\n        return encode\n\n    def forward(self, predictions, targets):\n        """"""\n        \n        :param predictions: <torch.FloatTensor> Network Predictions of size [N, C, H, W], where C = number of classes\n        :param targets: <torch.LongTensor> Ground Truth label of size [N, H, W]\n        :return:\n        """"""\n        enc_targets = self.__unique_encode(targets)\n        se_loss = F.binary_cross_entropy_with_logits(predictions, enc_targets, weight=self.weight,\n                                                     reduction=""elementwise_mean"")\n\n        return self.alpha * se_loss\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Dice Loss\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass DiceLoss2D(nn.Module):\n    def __init__(self, weight=None, ignore_index=-100):\n        """"""\n        Dice Loss for Semantic Segmentation\n\n        :param weight: <torch.Tensor, optional> A manual rescaling weight given to each class.\n                                                If given, has to be a Tensor of size C, where C = number of classes.\n        :param ignore_index: <int, optional> Specifies a target value that is ignored and does not\n                                             contribute to the input gradient.\n        """"""\n        super(DiceLoss2D, self).__init__()\n        self.weight = weight\n        self.ignore_index = ignore_index\n\n    def forward(self, predictions, targets):\n        """"""\n\n        :param predictions: <torch.FloatTensor> Network Predictions of size [N, C, H, W], where C = number of classes\n        :param targets: <torch.LongTensor> Ground Truth label of size [N, H, W]\n        :return: <torch.Tensor> loss\n        """"""\n        smooth = 1.0e-6\n\n        loss_fuse = 0.0\n        if isinstance(predictions, tuple):\n            for predict in predictions:\n\n                predict = F.softmax(predict, dim=1)\n                encoded_target = predict.detach() * 0  # The result will never require gradient.\n\n                # ----------------------------------------------------- #\n                # 1. Targets Pre-processing & Encoding\n                # ----------------------------------------------------- #\n                mask = None\n                if self.ignore_index is not None:\n                    mask = targets == self.ignore_index\n                    targets = targets.clone()\n                    targets[mask] = 0\n\n                    encoded_target.scatter_(dim=1, index=targets.unsqueeze(dim=1), value=1.0)\n                    mask = mask.unsqueeze(dim=1).expand_as(encoded_target)\n                    encoded_target[mask] = 0\n                else:\n                    encoded_target.scatter_(dim=1, index=targets.unsqueeze(dim=1), value=1.0)\n\n                if self.weight is None:\n                    self.weight = 1.0\n\n                # ----------------------------------------------------- #\n                # 2. Compute Dice Coefficient\n                # ----------------------------------------------------- #\n                intersection = predictions * encoded_target\n                denominator = predictions + encoded_target\n\n                if self.ignore_index is not None:\n                    denominator[mask] = 0\n\n                numerator = 2.0 * intersection.sum(dim=0).sum(dim=1).sum(dim=1) + smooth\n                denominator = denominator.sum(dim=0).sum(dim=1).sum(dim=1) + smooth\n\n                # ----------------------------------------------------- #\n                # 3. Compute Weighted Dice Loss\n                # ----------------------------------------------------- #\n                loss_per_channel = self.weight * (1.0 - (numerator / denominator))\n\n                loss_fuse = loss_per_channel.sum() / predictions.size(1)\n        else:\n            predict = F.softmax(predictions, dim=1)\n            encoded_target = predict.detach() * 0  # The result will never require gradient.\n\n            # ----------------------------------------------------- #\n            # 1. Targets Pre-processing & Encoding\n            # ----------------------------------------------------- #\n            mask = None\n            if self.ignore_index is not None:\n                mask = targets == self.ignore_index\n                targets = targets.clone()\n                targets[mask] = 0\n\n                encoded_target.scatter_(dim=1, index=targets.unsqueeze(dim=1), value=1.0)\n                mask = mask.unsqueeze(dim=1).expand_as(encoded_target)\n                encoded_target[mask] = 0\n            else:\n                encoded_target.scatter_(dim=1, index=targets.unsqueeze(dim=1), value=1.0)\n\n            if self.weight is None:\n                self.weight = 1.0\n\n            # ----------------------------------------------------- #\n            # 2. Compute Dice Coefficient\n            # ----------------------------------------------------- #\n            intersection = predictions * encoded_target\n            denominator = predictions + encoded_target\n\n            if self.ignore_index is not None:\n                denominator[mask] = 0\n\n            numerator = 2.0 * intersection.sum(dim=0).sum(dim=1).sum(dim=1) + smooth\n            denominator = denominator.sum(dim=0).sum(dim=1).sum(dim=1) + smooth\n\n            # ----------------------------------------------------- #\n            # 3. Compute Weighted Dice Loss\n            # ----------------------------------------------------- #\n            loss_per_channel = self.weight * (1.0 - (numerator / denominator))\n\n            loss_fuse = loss_per_channel.sum() / predictions.size(1)\n\n        return loss_fuse\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Soft-Jaccard Loss\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass SoftJaccardLoss2D(nn.Module):\n    def __init__(self, weight=None, ignore_index=-100):\n        """"""\n        Soft-Jaccard Loss for Semantic Segmentation\n\n        :param weight: <torch.Tensor, optional> A manual rescaling weight given to each class.\n                                                If given, has to be a Tensor of size C, where C = number of classes.\n        :param ignore_index: <int, optional> Specifies a target value that is ignored and does not\n                                             contribute to the input gradient.\n        """"""\n        super(SoftJaccardLoss2D, self).__init__()\n        self.weight = weight\n        self.ignore_index = ignore_index\n\n    def forward(self, predictions, targets):\n        """"""\n\n        :param predictions: <torch.FloatTensor> Network Predictions of size [N, C, H, W], where C = number of classes\n        :param targets: <torch.LongTensor> Ground Truth label of size [N, H, W]\n        :return: <torch.Tensor> loss\n        """"""\n        smooth = 1.0\n\n        predictions = F.softmax(predictions, dim=1)\n        encoded_target = predictions.detach() * 0  # The result will never require gradient.\n\n        # ----------------------------------------------------- #\n        # 1. Targets Pre-processing & Encoding\n        # ----------------------------------------------------- #\n        mask = None\n        if self.ignore_index is not None:\n            mask = targets == self.ignore_index\n            targets = targets.clone()\n            targets[mask] = 0\n\n            encoded_target.scatter_(dim=1, index=targets.unsqueeze(dim=1), value=1.0)\n            mask = mask.unsqueeze(dim=1).expand_as(encoded_target)\n            encoded_target[mask] = 0\n        else:\n            encoded_target.scatter_(dim=1, index=targets.unsqueeze(dim=1), value=1.0)\n\n        if self.weight is None:\n            self.weight = 1.0\n\n        # ----------------------------------------------------- #\n        # 2. Compute Jaccard Coefficient\n        # ----------------------------------------------------- #\n        intersection = predictions * encoded_target\n        denominator = predictions + encoded_target\n\n        if self.ignore_index is not None:\n            denominator[mask] = 0\n\n        numerator = intersection.sum(dim=0).sum(dim=1).sum(dim=1)\n        denominator = denominator.sum(dim=0).sum(dim=1).sum(dim=1)\n\n        # ----------------------------------------------------- #\n        # 3. Compute Weighted Soft-Jaccard Loss\n        # ----------------------------------------------------- #\n        # loss_per_channel = self.weight * (1.0 - torch.log(((numerator + smooth) / (denominator - numerator + smooth))))\n        loss_per_channel = self.weight * (1.0 - ((numerator + smooth) / (denominator - numerator + smooth)))\n\n        return loss_per_channel.sum() / predictions.size(1)\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Tversky Loss\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass TverskyLoss2D(nn.Module):\n    def __init__(self, alpha=0.4, beta=0.6, weight=None, ignore_index=-100):\n        """"""\n        Tversky Loss for Semantic Segmentation\n\n        :param alpha: <int> Parameter to control precision and recall\n        :param beta: <int> Parameter to control precision and recall\n        :param weight: <torch.Tensor, optional> A manual rescaling weight given to each class.\n                                                If given, has to be a Tensor of size C, where C = number of classes.\n        :param ignore_index: <int, optional> Specifies a target value that is ignored and does not\n                                             contribute to the input gradient.\n        """"""\n        super(TverskyLoss2D, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.weight = weight\n        self.ignore_index = ignore_index\n\n    def forward(self, predictions, targets):\n        """"""\n\n        :param predictions: <torch.FloatTensor> Network Predictions of size [N, C, H, W], where C = number of classes\n        :param targets: <torch.LongTensor> Ground Truth label of size [N, H, W]\n        :return: <torch.Tensor> loss\n        """"""\n        smooth = 1.0\n\n        predictions = F.softmax(predictions, dim=1)\n        encoded_target = predictions.detach() * 0  # The result will never require gradient.\n\n        # ----------------------------------------------------- #\n        # 1. Targets Pre-processing & Encoding\n        # ----------------------------------------------------- #\n        mask = None\n        if self.ignore_index is not None:\n            mask = targets == self.ignore_index\n            targets = targets.clone()\n            targets[mask] = 0\n\n            encoded_target.scatter_(dim=1, index=targets.unsqueeze(dim=1), value=1.0)\n            mask = mask.unsqueeze(dim=1).expand_as(encoded_target)\n            encoded_target[mask] = 0\n        else:\n            encoded_target.scatter_(dim=1, index=targets.unsqueeze(dim=1), value=1.0)\n\n        if self.weight is None:\n            self.weight = 1.0\n\n        # ----------------------------------------------------- #\n        # 2. Compute Tversky Index\n        # ----------------------------------------------------- #\n        intersection = predictions * encoded_target\n        numerator = intersection.sum(dim=0).sum(dim=1).sum(dim=1) + smooth\n\n        ones = torch.ones_like(predictions)\n        item1 = predictions * (ones - encoded_target)\n        item2 = (ones - predictions) * encoded_target\n        denominator = numerator + self.alpha * item1.sum(dim=0).sum(dim=1).sum(dim=1) + \\\n                      self.beta * item2.sum(dim=0).sum(dim=1).sum(dim=1)\n\n        if self.ignore_index is not None:\n            denominator[mask] = 0\n\n        # ----------------------------------------------------- #\n        # 3. Compute Weighted Tversky Loss\n        # ----------------------------------------------------- #\n        # loss_per_channel = self.weight * (1.0 - torch.log(((numerator) / (denominator - numerator))))\n        loss_per_channel = self.weight * (1.0 - (numerator / (denominator - numerator)))\n\n        return loss_per_channel.sum() / predictions.size(1)\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Asymmetric Similarity Loss Function to Balance Precision and Recall in\n# Highly Unbalanced Deep Medical Image Segmentation\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass AsymmetricSimilarityLoss2D(nn.Module):\n    def __init__(self, beta=0.6, weight=None, ignore_index=-100):\n        """"""\n        Tversky Loss for Semantic Segmentation\n\n        :param beta: <int> Parameter to control precision and recall\n        :param weight: <torch.Tensor, optional> A manual rescaling weight given to each class.\n                                                If given, has to be a Tensor of size C, where C = number of classes.\n        :param ignore_index: <int, optional> Specifies a target value that is ignored and does not\n                                             contribute to the input gradient.\n        """"""\n        super(AsymmetricSimilarityLoss2D, self).__init__()\n        self.beta = beta\n        self.weight = weight\n        self.ignore_index = ignore_index\n\n    def forward(self, predictions, targets):\n        """"""\n\n        :param predictions: <torch.FloatTensor> Network Predictions of size [N, C, H, W], where C = number of classes\n        :param targets: <torch.LongTensor> Ground Truth label of size [N, H, W]\n        :return: <torch.Tensor> loss\n        """"""\n        eps = 1e-8\n        beta = self.beta ** 2\n\n        predictions = F.softmax(predictions, dim=1)\n        encoded_target = predictions.detach() * 0  # The result will never require gradient.\n\n        # ----------------------------------------------------- #\n        # 1. Targets Pre-processing & Encoding\n        # ----------------------------------------------------- #\n        mask = None\n        if self.ignore_index is not None:\n            mask = targets == self.ignore_index\n            targets = targets.clone()\n            targets[mask] = 0\n\n            encoded_target.scatter_(dim=1, index=targets.unsqueeze(dim=1), value=1.0)\n            mask = mask.unsqueeze(dim=1).expand_as(encoded_target)\n            encoded_target[mask] = 0\n        else:\n            encoded_target.scatter_(dim=1, index=targets.unsqueeze(dim=1), value=1.0)\n\n        if self.weight is None:\n            self.weight = 1.0\n\n        # ----------------------------------------------------- #\n        # 2. Compute F-beta score\n        # ----------------------------------------------------- #\n        intersection = predictions * encoded_target\n        numerator = (1.0 + beta) * intersection.sum(dim=0).sum(dim=1).sum(dim=1)\n\n        ones = torch.ones_like(predictions)\n        item1 = predictions * (ones - encoded_target)\n        item2 = (ones - predictions) * encoded_target\n        denominator = numerator + beta * item1.sum(dim=0).sum(dim=1).sum(dim=1) + \\\n                      item2.sum(dim=0).sum(dim=1).sum(dim=1) + eps\n\n        if self.ignore_index is not None:\n            denominator[mask] = 0\n\n        # ----------------------------------------------------- #\n        # 3. Compute Weighted Tversky Loss\n        # ----------------------------------------------------- #\n        # loss_per_channel = self.weight * (1.0 - torch.log(((numerator) / (denominator - numerator))))\n        loss_per_channel = self.weight * (1.0 - (numerator / (denominator - numerator)))\n\n        return loss_per_channel.sum() / predictions.size(1)\n\n\n# ---------------------------------------------- #\n# Test code\n# ---------------------------------------------- #\nif __name__ == ""__main__"":\n    dummy_in = torch.LongTensor(32, 19).random_(0, 19).requires_grad_()\n    dummy_gt = torch.LongTensor(32, 32, 32).random_(0, 19)\n\n    se_loss = SemanticEncodingLoss(num_classes=19, ignore_label=250, weight=None, alpha=0.25)\n\n    while True:\n        top_k = 256\n        loss = se_loss(dummy_in, dummy_gt)\n\n        print(""Loss: {}"".format(loss.item()))\n'"
utils/lr_scheduler.py,0,"b'import math\n\n\nclass LRScheduler(object):\n    """"""\n    Learning Rate Scheduler\n        Step mode: ``lr = baselr * 0.1 ^ {floor(epoch-1 / lr_step)}``\n        Cosine mode: ``lr = baselr * 0.5 * (1 + cos(iter/maxiter))``\n        Poly mode: ``lr = baselr * (1 - iter/maxiter) ^ 0.9``\n\n    Args:\n        args:  :attr:`args.lr_scheduler` lr scheduler mode (`cos`, `poly`),\n          :attr:`args.lr` base learning rate, :attr:`args.epochs` number of epochs,\n          :attr:`args.lr_step`\n\n        iters_per_epoch: number of iterations per epoch\n    """"""\n    def __init__(self, mode, base_lr, num_epochs, iters_per_epoch=0, lr_step=0, restart_step=20, warmup_epochs=0):\n        self.mode = mode\n        print(\'> Using {} LR Scheduler!\'.format(self.mode))\n\n        self.lr = base_lr\n\n        if mode == \'step\':\n            assert lr_step\n\n        self.lr_step = lr_step\n        self.restart_step = restart_step\n        self.iters_per_epoch = iters_per_epoch\n\n        self.N = num_epochs * iters_per_epoch\n        self.restart_period = restart_step * iters_per_epoch\n\n        self.epoch = -1\n        self.warmup_iters = warmup_epochs * iters_per_epoch\n\n    def __call__(self, optimizer, i, epoch):\n        T = epoch * self.iters_per_epoch + i\n\n        if self.mode == \'cos\':\n            batch_idx = float(T)\n            while batch_idx / self.restart_period > 1.0:\n                batch_idx = batch_idx - self.restart_period\n\n            lr = self.lr * 0.5 * (1.0 + math.cos(1.0 * (batch_idx / self.restart_period) * math.pi))\n        elif self.mode == \'poly\':\n            lr = self.lr * pow((1 - 1.0 * T / self.N), 0.9)\n        elif self.mode == \'step\':\n            lr = self.lr * (0.1 ** (epoch // self.lr_step))\n        else:\n            raise NotImplemented\n\n        # warm up lr schedule\n        if self.warmup_iters > 0 and T < self.warmup_iters:\n            lr = lr * 1.0 * T / self.warmup_iters\n\n        if epoch > self.epoch:\n            # print(\'\\n=>Epoches %i, learning rate = %.4f, \\\n            #     previous best = %.4f\' % (epoch, lr, best_pred))\n            self.epoch = epoch\n\n        assert lr >= 0\n        self._adjust_learning_rate(optimizer, lr)\n\n        return lr\n\n    def _adjust_learning_rate(self, optimizer, lr):\n        if len(optimizer.param_groups) == 1:\n            optimizer.param_groups[0][\'lr\'] = lr\n        else:\n            # enlarge the lr at the head\n            optimizer.param_groups[0][\'lr\'] = lr\n            for i in range(1, len(optimizer.param_groups)):\n                optimizer.param_groups[i][\'lr\'] = lr * 10\n\n\ndef cosine_annealing_lr(optimizer, init_lr, period, batch_idx):\n    # returns normalised anytime sgdr schedule given period and batch_idx\n    # best performing settings reported in paper are T_0 = 10, T_mult=2\n    # so always use T_mult=2\n    # \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n    # \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n\n    batch_idx = float(batch_idx)\n    restart_period = period\n    while batch_idx / restart_period > 1.:\n        batch_idx = batch_idx - restart_period\n        # restart_period = restart_period * 2.\n\n    radians = math.pi * (batch_idx / restart_period)\n    lr = init_lr * 0.5 * (1.0 + math.cos(radians))\n\n    # callback to set the learning rate in an optimizer, without rebuilding the whole optimizer\n    for param_group in optimizer.param_groups:\n        if param_group[\'lr\'] > 0.0:\n            param_group[\'lr\'] = lr\n\n    return optimizer, lr\n'"
utils/metrics.py,0,"b'# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# LightNet++: Boosted Light-weighted Networks for Real-time Semantic Segmentation\n# ---------------------------------------------------------------------------------------------------------------- #\n# Compute Metrics for Semantic Segmentation\n# class:\n#       > AverageMeter\n#       > RunningMetrics\n# ---------------------------------------------------------------------------------------------------------------- #\n# Author: Huijun Liu M.Sc.\n# Date:   10.10.2018\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nimport numpy as np\n\n\nclass AverageMeter(object):\n    """"""\n    Computes and stores the average and current value\n    """"""\n\n    def __init__(self):\n        self.val = 0.0\n        self.avg = 0.0\n        self.sum = 0.0\n        self.count = 0.0\n\n    def reset(self):\n        self.val = 0.0\n        self.avg = 0.0\n        self.sum = 0.0\n        self.count = 0.0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass RunningMetrics(object):\n    def __init__(self, num_classes):\n        """"""\n        Computes and stores the Metric values from Confusion Matrix\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n\n        For reference, please see: https://en.wikipedia.org/wiki/Confusion_matrix\n\n        :param num_classes: <int> number of classes\n        """"""\n        self.num_classes = num_classes\n        self.confusion_matrix = np.zeros((num_classes, num_classes))\n\n    def __fast_hist(self, label_gt, label_pred):\n        """"""\n        Collect values for Confusion Matrix\n        For reference, please see: https://en.wikipedia.org/wiki/Confusion_matrix\n\n        :param label_gt: <np.array> ground-truth\n        :param label_pred: <np.array> prediction\n        :return: <np.ndarray> values for confusion matrix\n        """"""\n        mask = (label_gt >= 0) & (label_gt < self.num_classes)\n        hist = np.bincount(self.num_classes * label_gt[mask].astype(int) + label_pred[mask],\n                           minlength=self.num_classes**2).reshape(self.num_classes, self.num_classes)\n        return hist\n\n    def update(self, label_gts, label_preds):\n        """"""\n        Compute Confusion Matrix\n        For reference, please see: https://en.wikipedia.org/wiki/Confusion_matrix\n\n        :param label_gts: <np.ndarray> ground-truths\n        :param label_preds: <np.ndarray> predictions\n        :return:\n        """"""\n        for lt, lp in zip(label_gts, label_preds):\n            self.confusion_matrix += self.__fast_hist(lt.flatten(), lp.flatten())\n\n    def reset(self):\n        """"""\n        Reset Confusion Matrix\n        :return:\n        """"""\n        self.confusion_matrix = np.zeros((self.num_classes, self.num_classes))\n\n    def get_scores(self):\n        """"""\n        Returns score about:\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n\n        For reference, please see: https://en.wikipedia.org/wiki/Confusion_matrix\n\n        :return:\n        """"""\n        hist = self.confusion_matrix\n        tp = np.diag(hist)\n        sum_a1 = hist.sum(axis=1)\n\n        # ---------------------------------------------------------------------- #\n        # 1. Accuracy & Class Accuracy\n        # ---------------------------------------------------------------------- #\n        acc = tp.sum() / (hist.sum() + np.finfo(np.float32).eps)\n\n        acc_cls = tp / (sum_a1 + np.finfo(np.float32).eps)\n        acc_cls = np.nanmean(acc_cls)\n\n        # ---------------------------------------------------------------------- #\n        # 2. Frequency weighted Accuracy & Mean IoU\n        # ---------------------------------------------------------------------- #\n        iu = tp / (sum_a1 + hist.sum(axis=0) - tp + np.finfo(np.float32).eps)\n        mean_iu = np.nanmean(iu)\n\n        freq = sum_a1 / (hist.sum() + np.finfo(np.float32).eps)\n        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n\n        cls_iu = dict(zip(range(self.num_classes), iu))\n\n        return {\'Overall_Acc\': acc,\n                \'Mean_Acc\': acc_cls,\n                \'FreqW_Acc\': fwavacc,\n                \'Mean_IoU\': mean_iu}, cls_iu\n\n\n# ---------------------------------------------- #\n# Test code\n# ---------------------------------------------- #\nif __name__ == ""__main__"":\n    score = RunningMetrics(2)\n\n    gt = np.array([1, 0, 0, 1, 1, 0, 1, 0, 1, 0])\n    pred = np.array([1, 1, 0, 1, 0, 0, 1, 1, 0, 0])\n\n    score.update(gt, pred)\n    print(score.confusion_matrix)\n'"
utils/parallel.py,9,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## ECE Department, Rutgers University\n## Email: zhang.hang@rutgers.edu\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n""""""Encoding Data Parallel""""""\nimport threading\nimport functools\nimport torch\nfrom torch.autograd import Function\nimport torch.cuda.comm as comm\nfrom torch.nn.parallel.data_parallel import DataParallel\nfrom torch.nn.parallel.parallel_apply import get_a_var\nfrom torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n\ntorch_ver = torch.__version__[:3]\n\n__all__ = [\'allreduce\', \'DataParallelModel\', \'DataParallelCriterion\',\n           \'patch_replication_callback\']\n\n\ndef allreduce(*inputs):\n    """"""Cross GPU all reduce autograd operation for calculate mean and\n    variance in SyncBN.\n    """"""\n    return AllReduce.apply(*inputs)\n\n\nclass AllReduce(Function):\n    @staticmethod\n    def forward(ctx, num_inputs, *inputs):\n        ctx.num_inputs = num_inputs\n        ctx.target_gpus = [inputs[i].get_device() for i in range(0, len(inputs), num_inputs)]\n        inputs = [inputs[i:i + num_inputs]\n                 for i in range(0, len(inputs), num_inputs)]\n        # sort before reduce sum\n        inputs = sorted(inputs, key=lambda i: i[0].get_device())\n        results = comm.reduce_add_coalesced(inputs, ctx.target_gpus[0])\n        outputs = comm.broadcast_coalesced(results, ctx.target_gpus)\n        return tuple([t for tensors in outputs for t in tensors])\n\n    @staticmethod\n    def backward(ctx, *inputs):\n        inputs = [i.data for i in inputs]\n        inputs = [inputs[i:i + ctx.num_inputs]\n                 for i in range(0, len(inputs), ctx.num_inputs)]\n        results = comm.reduce_add_coalesced(inputs, ctx.target_gpus[0])\n        outputs = comm.broadcast_coalesced(results, ctx.target_gpus)\n        return (None,) + tuple([t for tensors in outputs for t in tensors])\n\n\nclass Reduce(Function):\n    @staticmethod\n    def forward(ctx, *inputs):\n        ctx.target_gpus = [inputs[i].get_device() for i in range(len(inputs))]\n        inputs = sorted(inputs, key=lambda i: i.get_device())\n        return comm.reduce_add(inputs)\n\n    @staticmethod\n    def backward(ctx, gradOutput):\n        return Broadcast.apply(ctx.target_gpus, gradOutput)\n\n\nclass DataParallelModel(DataParallel):\n    """"""Implements data parallelism at the module level.\n\n    This container parallelizes the application of the given module by\n    splitting the input across the specified devices by chunking in the\n    batch dimension.\n    In the forward pass, the module is replicated on each device,\n    and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.\n    Note that the outputs are not gathered, please use compatible\n    :class:`encoding.parallel.DataParallelCriterion`.\n\n    The batch size should be larger than the number of GPUs used. It should\n    also be an integer multiple of the number of GPUs so that each chunk is\n    the same size (so that each GPU processes the same number of samples).\n\n    Args:\n        module: module to be parallelized\n        device_ids: CUDA devices (default: all devices)\n\n    Reference:\n        Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi,\n        Amit Agrawal. \xe2\x80\x9cContext Encoding for Semantic Segmentation.\n        *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018*\n\n    Example::\n\n        >>> net = encoding.nn.DataParallelModel(model, device_ids=[0, 1, 2])\n        >>> y = net(x)\n    """"""\n    def gather(self, outputs, output_device):\n        return outputs\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelModel, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n\nclass DataParallelCriterion(DataParallel):\n    """"""\n    Calculate loss in multiple-GPUs, which balance the memory usage for\n    Semantic Segmentation.\n\n    The targets are splitted across the specified devices by chunking in\n    the batch dimension. Please use together with :class:`encoding.parallel.DataParallelModel`.\n\n    Reference:\n        Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi,\n        Amit Agrawal. \xe2\x80\x9cContext Encoding for Semantic Segmentation.\n        *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018*\n\n    Example::\n\n        >>> net = encoding.nn.DataParallelModel(model, device_ids=[0, 1, 2])\n        >>> criterion = encoding.nn.DataParallelCriterion(criterion, device_ids=[0, 1, 2])\n        >>> y = net(x)\n        >>> loss = criterion(y, target)\n    """"""\n    def forward(self, inputs, *targets, **kwargs):\n        # input should be already scatterd\n        # scattering the targets instead\n        if not self.device_ids:\n            return self.module(inputs, *targets, **kwargs)\n\n        targets, kwargs = self.scatter(targets, kwargs, self.device_ids)\n\n        # targets = list(targets)\n        # for idx in range(len(targets)):\n        #     targets[idx] = targets[idx][0]\n\n        if len(self.device_ids) == 1:\n            return self.module(inputs, *targets[0], **kwargs[0])\n\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n        targets = tuple(targets_per_gpu[0] for targets_per_gpu in targets)\n        outputs = _criterion_parallel_apply(replicas, inputs, targets, kwargs)\n\n        return Reduce.apply(*outputs) / len(outputs)\n        #return self.gather(outputs, self.output_device).mean()\n\n\ndef _criterion_parallel_apply(modules, inputs, targets, kwargs_tup=None, devices=None):\n    assert len(modules) == len(inputs)\n    assert len(targets) == len(inputs)\n\n    if kwargs_tup:\n        assert len(modules) == len(kwargs_tup)\n    else:\n        kwargs_tup = ({},) * len(modules)\n    if devices is not None:\n        assert len(modules) == len(devices)\n    else:\n        devices = [None] * len(modules)\n\n    lock = threading.Lock()\n    results = {}\n    if torch_ver != ""0.3"":\n        grad_enabled = torch.is_grad_enabled()\n\n    def _worker(i, module, input, target, kwargs, device=None):\n        if torch_ver != ""0.3"":\n            torch.set_grad_enabled(grad_enabled)\n\n        if device is None:\n            device = get_a_var(input).get_device()\n        try:\n            with torch.cuda.device(device):\n                output = module(input, target)\n            with lock:\n                results[i] = output\n        except Exception as e:\n            with lock:\n                results[i] = e\n\n    if len(modules) > 1:\n        threads = [threading.Thread(target=_worker,\n                                    args=(i, module, input, target,\n                                          kwargs, device),)\n                   for i, (module, input, target, kwargs, device) in\n                   enumerate(zip(modules, inputs, targets, kwargs_tup, devices))]\n\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n    else:\n        _worker(0, modules[0], inputs[0], kwargs_tup[0], devices[0])\n\n    outputs = []\n    for i in range(len(inputs)):\n        output = results[i]\n        if isinstance(output, Exception):\n            raise output\n        outputs.append(output)\n    return outputs\n\n\n###########################################################################\n# Adapted from Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n#\nclass CallbackContext(object):\n    pass\n\n\ndef execute_replication_callbacks(modules):\n    """"""\n    Execute an replication callback `__data_parallel_replicate__` on each module created\n    by original replication.\n\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\n    (shared among multiple copies of this module on different devices).\n    Through this context, different copies can share some information.\n\n    We guarantee that the callback on the master copy (the first copy) will be called ahead\n    of calling the callback of any slave copies.\n    """"""\n    master_copy = modules[0]\n    nr_modules = len(list(master_copy.modules()))\n    ctxs = [CallbackContext() for _ in range(nr_modules)]\n\n    for i, module in enumerate(modules):\n        for j, m in enumerate(module.modules()):\n            if hasattr(m, \'__data_parallel_replicate__\'):\n                m.__data_parallel_replicate__(ctxs[j], i)\n\n\ndef patch_replication_callback(data_parallel):\n    """"""\n    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n    Useful when you have customized `DataParallel` implementation.\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n        > patch_replication_callback(sync_bn)\n        # this is equivalent to\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n    """"""\n\n    assert isinstance(data_parallel, DataParallel)\n\n    old_replicate = data_parallel.replicate\n\n    @functools.wraps(old_replicate)\n    def new_replicate(module, device_ids):\n        modules = old_replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n    data_parallel.replicate = new_replicate\n'"
utils/utils.py,2,"b'import torch.nn.init as init\nimport torch.nn as nn\nimport math\nimport os\n\n\ndef recursive_glob(rootdir=\'.\', suffix=\'\'):\n    """"""Performs recursive glob with given suffix and rootdir\n        :param rootdir is the root directory\n        :param suffix is the suffix to be searched\n    """"""\n    return [os.path.join(looproot, filename)\n            for looproot, _, filenames in os.walk(rootdir)\n            for filename in filenames if filename.endswith(suffix)]\n\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find(\'Conv\') != -1:\n        init.normal(m.weight.data, 0.0, 0.02)\n    elif classname.find(\'Linear\') != -1:\n        init.normal(m.weight.data, 0.0, 0.02)\n    elif classname.find(\'BatchNorm\') != -1:\n        init.normal(m.weight.data, 1.0, 0.02)\n        init.constant(m.bias.data, 0.0)\n\n\ndef weights_init_xavier(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find(\'Conv\') != -1:\n        init.xavier_normal(m.weight.data, gain=1)\n    elif classname.find(\'Linear\') != -1:\n        init.xavier_normal(m.weight.data, gain=1)\n    elif classname.find(\'BatchNorm\') != -1:\n        init.normal(m.weight.data, 1.0, 0.02)\n        init.constant(m.bias.data, 0.0)\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find(\'Conv\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n    elif classname.find(\'Linear\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n    elif classname.find(\'BatchNorm\') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_orthogonal(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find(\'Conv\') != -1:\n        init.orthogonal(m.weight.data, gain=1)\n    elif classname.find(\'Linear\') != -1:\n        init.orthogonal(m.weight.data, gain=1)\n    elif classname.find(\'BatchNorm\') != -1:\n        init.normal(m.weight.data, 1.0, 0.02)\n        init.constant(m.bias.data, 0.0)\n\n\ndef init_weights(net, init_type=\'normal\'):\n    # print(\'initialization method [%s]\' % init_type)\n    if init_type == \'normal\':\n        net.apply(weights_init_normal)\n    elif init_type == \'xavier\':\n        net.apply(weights_init_xavier)\n    elif init_type == \'kaiming\':\n        net.apply(weights_init_kaiming)\n    elif init_type == \'orthogonal\':\n        net.apply(weights_init_orthogonal)\n    else:\n        raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n\n\ndef step_topk_scheduler(init_topk1, init_topk2, epoch, step_size, limit=(384, 384), ratio=0.875):\n    multiplier = ratio ** (epoch // step_size)\n    topk1 = int(init_topk1 * multiplier)\n    topk2 = int(init_topk2 * multiplier)\n\n    if topk1 <= limit[0] and topk2 <= limit[1]:\n        topk1, topk2 = limit[0], limit[1]\n\n    return topk1, topk2\n\n\ndef freeze_bn(m, freeze_bn_affine=False):\n    if isinstance(m, nn.BatchNorm2d):\n        m.eval()\n        if freeze_bn_affine:\n            m.weight.requires_grad = False\n            m.bias.requires_grad = False\n'"
datasets/cityscapes/__init__.py,0,b''
datasets/cityscapes/cityscapes.py,5,"b'# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# LightNet++: Boosted Light-weighted Networks for Real-time Semantic Segmentation\n# ---------------------------------------------------------------------------------------------------------------- #\n# DataReader for Cityscapes Dataset\n#\n# ---------------------------------------------------------------------------------------------------------------- #\n# Author: Huijun Liu M.Sc.\n# Date:   10.10.2018\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nimport torchvision.transforms as transforms\nfrom random import shuffle\nimport numpy as np\nimport torch\nimport os\n\nfrom PIL import Image, ImageEnhance, ImageFilter\nfrom datasets.augmentations import *\nfrom torch.utils import data\n\n\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n# Lighting data augmentation take from:\n# https://github.com/eladhoffer/convNet.pytorch/blob/master/preprocess.py\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\nclass Lighting(object):\n    """"""Lighting noise(AlexNet - style PCA - based noise)""""""\n\n    def __init__(self, alphastd, eigval, eigvec):\n        self.alphastd = alphastd\n        self.eigval = eigval\n        self.eigvec = eigvec\n\n    def __call__(self, img):\n        if self.alphastd == 0:\n            return img\n\n        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n        rgb = self.eigvec.type_as(img).clone()\\\n            .mul(alpha.view(1, 3).expand(3, 3))\\\n            .mul(self.eigval.view(1, 3).expand(3, 3))\\\n            .sum(1).squeeze()\n        return img.add(rgb.view(3, 1, 1).expand_as(img))\n\n\nclass Cityscapes(data.Dataset):\n    """"""\n        Data Reader for Cityscapes Dataset\n\n        https://www.cityscapes-dataset.com\n\n        Data is derived from CityScapes, and can be downloaded from here:\n        https://www.cityscapes-dataset.com/downloads/\n\n        Many Thanks to @fvisin for the loader repo:\n        https://github.com/fvisin/dataset_loaders/blob/master/dataset_loaders/images/cityscapes.py\n        """"""\n    colors = [  # [  0,   0,   0],\n        [128, 64, 128],\n        [244, 35, 232],\n        [70, 70, 70],\n        [102, 102, 156],\n        [190, 153, 153],\n        [153, 153, 153],\n        [250, 170, 30],\n        [220, 220, 0],\n        [107, 142, 35],\n        [152, 251, 152],\n        [0, 130, 180],\n        [220, 20, 60],\n        [255, 0, 0],\n        [0, 0, 142],\n        [0, 0, 70],\n        [0, 60, 100],\n        [0, 80, 100],\n        [0, 0, 230],\n        [119, 11, 32]]\n\n    label_colours = dict(zip(range(19), colors))\n\n    def __init__(self, data_root, list_path, num_classes=19, phase=""train"",\n                 augmentations=None, use_transform=True, use_lighting=True,\n                 mean=[0.2997, 0.3402, 0.3072], std=[0.1549, 0.1579, 0.1552]):\n        super(Cityscapes, self).__init__()\n        self.data_root = data_root\n        self.num_classes = num_classes\n        self.phase = phase\n\n        self.augmentations = augmentations\n        self.use_transform = use_transform\n        self.use_lighting = use_lighting\n\n        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n        self.class_names = [\'unlabelled\', \'road\', \'sidewalk\', \'building\', \'wall\', \'fence\',\n                            \'pole\', \'traffic_light\', \'traffic_sign\', \'vegetation\', \'terrain\',\n                            \'sky\', \'person\', \'rider\', \'car\', \'truck\', \'bus\', \'train\',\n                            \'motorcycle\', \'bicycle\']\n\n        self.ignore_index = 255\n        self.class_map = dict(zip(self.valid_classes, range(19)))\n\n        self.aug_color = transforms.ColorJitter(brightness=0.125, contrast=0.375, saturation=0.275, hue=0.125)\n\n        imagenet_pca = {\n            \'eigval\': torch.Tensor([0.2175, 0.0188, 0.0045]),\n            \'eigvec\': torch.Tensor([\n                [-0.5675, 0.7192, 0.4009],\n                [-0.5808, -0.0045, -0.8140],\n                [-0.5836, -0.6948, 0.4203],\n            ])\n        }\n        self.trans_lighting = transforms.Compose([transforms.ToTensor(),\n                                                  Lighting(0.1, imagenet_pca[\'eigval\'],\n                                                           imagenet_pca[\'eigvec\']),\n                                                  transforms.Normalize(mean=mean, std=std)])\n\n        self.trans_norm = transforms.Compose([transforms.ToTensor(),\n                                              transforms.Normalize(mean=mean, std=std)])\n\n        self.image_paths = [i_id.strip() for i_id in open(""{}"".format(list_path))]\n        shuffle(self.image_paths)\n\n        if not self.image_paths:\n            raise Exception(""> No files found in %s"" % os.path.basename(list_path))\n\n        print(""> Found %d %s images..."" % (len(self.image_paths), os.path.basename(list_path)))\n\n    def encode_segmap(self, mask):\n        # Put all void classes to zero\n        for _voidc in self.void_classes:\n            mask[mask == _voidc] = self.ignore_index\n        for _validc in self.valid_classes:\n            mask[mask == _validc] = self.class_map[_validc]\n        return mask\n\n    def __len__(self):\n        """"""__len__""""""\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        """"""__getitem__\n\n        :param index:\n        """"""\n        # line = self.files[self.split][index]\n        # splits = line.split("" "")\n        # image_path, mask_path = os.path.join(self.data_root, splits[0]), os.path.join(self.data_root, splits[1])\n        image_path = os.path.join(self.data_root, self.image_paths[index])\n        mask_path = image_path.replace(""leftImg8bit"", ""gtFine"")\n        mask_path = mask_path.replace("".png"", ""_labelIds.png"")\n\n        if not os.path.isfile(image_path) or not os.path.exists(image_path):\n            raise Exception(""> Image: {} is not a file or not exist, can not be opened."".format(image_path))\n\n        if not os.path.isfile(mask_path) or not os.path.exists(mask_path):\n            raise Exception(""> Mask: {} is not a file or not exist, can not be opened."".format(mask_path))\n\n        # --------------------------------------------------------- #\n        # 1. Read Image and Mask\n        # --------------------------------------------------------- #\n        image = Image.open(image_path).convert(\'RGB\')\n        mask = Image.open(mask_path).convert(\'L\')\n\n        if not (""deepdrive"" in os.path.basename(mask_path)):\n            mask = Image.fromarray(self.encode_segmap(np.array(mask, dtype=np.uint8)), mode=\'L\')\n\n        # --------------------------------------------------------- #\n        # 2. Data Augmentation used in training phase\n        # --------------------------------------------------------- #\n        if self.augmentations is not None:\n            image, mask = self.augmentations(image, mask)\n\n        # --------------------------------------------------------- #\n        # 3. Image Transformation\n        # --------------------------------------------------------- #\n        # 3.1 Image Color Jitter\n        if (self.aug_color is not None) and random.random() < 0.5 and self.phase == ""train"":\n            b_scale = random.uniform(0.125, 0.45)\n            c_scale = random.uniform(0.125, 0.45)\n            s_scale = random.uniform(0.125, 0.45)\n            h_scale = random.uniform(-0.325, 0.325)\n            self.aug_color = transforms.ColorJitter(brightness=b_scale,\n                                                    contrast=c_scale,\n                                                    saturation=s_scale,\n                                                    hue=h_scale)\n\n            # 3.2 Sharpen/GaussianBlur\n        if random.random() < 0.5 and self.phase == ""train"":\n            scale = random.uniform(1.125, 2.0)\n            enhancer = ImageEnhance.Sharpness(image)\n            image = enhancer.enhance(scale)\n\n        if random.random() < 0.5 and self.phase == ""train"":\n            scale = random.uniform(1.25, 2.0)\n            image = image.filter(ImageFilter.GaussianBlur(radius=scale))\n\n        image = np.array(image, dtype=np.uint8).copy()\n        mask = np.array(mask, dtype=np.uint8).copy()\n\n        # 3.2 Image transformation\n        image = np.array(image[:, :, ::-1], dtype=np.uint8)  # From RGB to BGR\n        if self.use_transform:\n            if self.use_lighting and random.random() < 0.5 and self.phase == ""train"":\n                image = self.trans_lighting(image)\n            else:\n                image = self.trans_norm(image)\n\n            return image, torch.from_numpy(mask).long()\n        else:\n            # --------------------------------- #\n            # Only for DataLoader test\n            # --------------------------------- #\n            image = image.transpose(2, 0, 1)  # From HWC to CHW (For PyTorch we use N*C*H*W tensor)\n            image = image.astype(np.uint8)\n            return torch.from_numpy(image).float(), torch.from_numpy(mask).long()\n\n\n# +++++++++++++++++++++++++++++++++++++++++++++ #\n# Test the code of \'CityscapesReader\'\n# +++++++++++++++++++++++++++++++++++++++++++++ #\nif __name__ == \'__main__\':\n    import matplotlib\n    from matplotlib import pyplot as plt\n\n    net_h, net_w = 768, 768\n    augment = Compose([RandomHorizontallyFlip(), RandomScale((0.75, 1.25)),\n                       RandomRotate(5), RandomCrop((net_h, net_w))])\n\n    local_path = ""/home/huijun/Datasets/Cityscapes""\n    list_path = ""/home/huijun/PycharmProjects/LightNet++/datasets/cityscapes/list/deepdrive.lst""\n    reader = Cityscapes(data_root=local_path, list_path=list_path, num_classes=19, phase=""train"",\n                        augmentations=augment, use_transform=False, use_lighting=False,\n                        mean=[0.2997, 0.3402, 0.3072],\n                        std=[0.1549, 0.1579, 0.1552])\n\n    train_loader = data.DataLoader(dataset=reader, batch_size=1, num_workers=1, shuffle=True)\n    for idx, data in enumerate(train_loader):\n        print(""batch :"", idx)\n        imgs, msks = data\n\n        imgs = imgs.numpy()  # From PyTorch Tensor to Numpy NArray, From BGR to RGB\n        imgs = np.squeeze(imgs.astype(np.uint8))\n        imgs = imgs.transpose(1, 2, 0)\n        imgs = imgs[:, :, ::-1]\n\n        msks = msks.numpy()\n        msks = np.squeeze(msks.astype(np.uint8))\n\n        fig, axs = plt.subplots(ncols=2, sharey=True, figsize=(12, 4))\n\n        axs[0].imshow(imgs)\n        # axs[0].imshow(mask, origin=\'left\', alpha=0.225)\n        axs[0].get_xaxis().set_visible(False)\n        axs[0].get_yaxis().set_visible(False)\n        axs[0].set_title(""Image"")\n\n        axs[1].imshow(msks)\n        # axs[1].imshow(msks, alpha=0.225)\n        axs[1].get_xaxis().set_visible(False)\n        axs[1].get_yaxis().set_visible(False)\n        axs[1].set_title(""Mask Image"")\n        plt.tight_layout()\n\n        # plt.savefig(os.path.join(save_root, str(chn) + ""_heatmap_2d.png""))\n        plt.show()\n'"
datasets/utils/__init__.py,0,b''
datasets/utils/bdd2cityscapes.py,0,"b'import os\nimport shutil\nimport imageio\nimport numpy as np\n\nfrom PIL import Image\nfrom utils.utils import recursive_glob\n\n\nif __name__ == ""__main__"":\n    img_h, img_w = 1024, 2048\n    deepdrive_root = ""/home/huijun/Datasets/DeepDrive""\n    cityscapes_root = ""/home/huijun/Datasets/Cityscapes""\n\n    cvt_img_root = os.path.join(cityscapes_root, ""leftImg8bit"", ""deepdrive"")\n    cvt_msk_root = os.path.join(cityscapes_root, ""gtFine"", ""deepdrive"")\n\n    print(""> # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    if os.path.exists(cvt_img_root):\n        print(""> Path {} is exist, delete it..."".format(cvt_img_root))\n        shutil.rmtree(cvt_img_root)\n    if os.path.exists(cvt_msk_root):\n        print(""> Path {} is exist, delete it..."".format(cvt_msk_root))\n        shutil.rmtree(cvt_msk_root)\n\n    if not os.path.exists(cvt_img_root):\n        print(""> Path {} is not exist, create it..."".format(cvt_img_root))\n        os.mkdir(cvt_img_root)\n    if not os.path.exists(cvt_msk_root):\n        print(""> Path {} is not exist, create it..."".format(cvt_msk_root))\n        os.mkdir(cvt_msk_root)\n\n    img_list = recursive_glob(rootdir=os.path.join(deepdrive_root, ""images""), suffix="".jpg"")\n\n    for idx, img_path in enumerate(img_list):\n        img_name = os.path.basename(img_path)\n        msk_name = img_name.replace("".jpg"", ""_train_id.png"")\n        msk_path = os.path.join(deepdrive_root, ""labels"", msk_name)\n\n        print(""> # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n        print(""> Processing {}..."".format(img_name))\n        image = Image.open(img_path).convert(\'RGB\')\n        mask = Image.open(msk_path).convert(\'L\')\n\n        image = image.resize((img_w, img_h), Image.BILINEAR)\n        mask = mask.resize((img_w, img_h), Image.NEAREST)\n\n        image = np.array(image, dtype=np.uint8).copy()\n        mask = np.array(mask, dtype=np.uint8).copy()\n\n        save_img_name = ""deepdrive_000000_{}_leftImg8bit.png"".format(str(idx).zfill(6))\n        save_img_path = os.path.join(cvt_img_root, save_img_name)\n        save_msk_name = ""deepdrive_000000_{}_gtFine_labelIds.png"".format(str(idx).zfill(6))\n        save_msk_path = os.path.join(cvt_msk_root, save_msk_name)\n        imageio.imsave(save_img_path, image)\n        imageio.imsave(save_msk_path, mask)\n'"
datasets/utils/get_mean_std.py,5,"b'import torch\nfrom tqdm import tqdm\n\n\ndef get_mean_and_std(dataloader):\n    \'\'\'\n    Compute the mean and std value of dataset.\n    \'\'\'\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n\n    print(\'> Computing mean and std of images in the dataset..\')\n    pbar = tqdm(np.arange(len(dataloader)))\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:, i, :, :].mean()  # [N, C, H, W]  BGR\n            std[i] += inputs[:, i, :, :].std()    # [N, C, H, W]  BGR\n        pbar.update(1)\n\n    pbar.close()\n    mean.div_(len(dataloader))\n    std.div_(len(dataloader))\n    return mean, std\n\n\nif __name__ == ""__main__"":\n    import time\n    import numpy as np\n    from datasets.cityscapes.cityscapes import Cityscapes\n    from datasets.augmentations import *\n\n    net_h, net_w = 768, 768\n    augment = Compose([RandomHorizontallyFlip(), RandomScale((0.75, 1.25)),\n                       RandomRotate(5), RandomCrop((net_h, net_w))])\n\n    local_path = ""/home/huijun/Datasets/Cityscapes""\n    list_path = ""/home/huijun/PycharmProjects/LightNet++/datasets/cityscapes/list/train+.lst""\n    reader = Cityscapes(data_root=local_path, list_path=list_path, num_classes=19, phase=""train"",\n                        augmentations=augment, use_transform=False, use_lighting=False,\n                        mean=[0.41738699, 0.45732192, 0.46886091],\n                        std=[0.25685097, 0.26509955, 0.29067996])\n\n    dataloader = torch.utils.data.DataLoader(dataset=reader, batch_size=1, num_workers=20, shuffle=False)\n\n    count = 3\n    mmean = torch.zeros(3)\n    mstd = torch.zeros(3)\n    time_cost = 0.0\n    for idx in np.arange(count):\n        print(""> +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ <"")\n        print(""> Epoch: {}..."".format(idx + 1))\n        start_time = time.time()\n        mean, std = get_mean_and_std(dataloader)\n        mmean = mmean + mean\n        mstd = mstd + std\n\n        end_time = time.time() - start_time\n        time_cost += end_time\n        print(""> Time: {}..., "".format(end_time))\n        print(""> Mean (BGR): {}"".format(mean / 255.0))\n        print(""> STD  (BGR): {}"".format(std / 255.0))\n\n    print(""> +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ <"")\n    print(""> Done, Time: {}s"".format(time_cost))\n    print(""> Mean (BGR): {}"".format((mmean / count) / 255.0))\n    print(""> STD  (BGR): {}"".format((mstd / count) / 255.0))\n    print(""> +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ <"")\n'"
deploy/cityscapes/__init__.py,0,b''
modules/deformable/__init__.py,0,"b""from .functions.deform_conv import deform_conv, modulated_deform_conv\nfrom .functions.deform_pool import deform_roi_pooling\nfrom .modules.deform_conv import (DeformConv, ModulatedDeformConv,\n                                  ModulatedDeformConvPack)\nfrom .modules.deform_pool import (DeformRoIPooling, DeformRoIPoolingPack,\n                                  ModulatedDeformRoIPoolingPack)\n\n__all__ = [\n    'DeformConv', 'DeformRoIPooling', 'DeformRoIPoolingPack',\n    'ModulatedDeformRoIPoolingPack', 'ModulatedDeformConv',\n    'ModulatedDeformConvPack', 'deform_conv',\n    'modulated_deform_conv', 'deform_roi_pooling'\n]\n"""
modules/deformable/setup.py,1,"b""from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='deform_conv',\n    ext_modules=[\n        CUDAExtension('deform_conv_cuda', [\n            'src/deform_conv_cuda.cpp',\n            'src/deform_conv_cuda_kernel.cu',\n        ]),\n        CUDAExtension('deform_pool_cuda', [\n            'src/deform_pool_cuda.cpp', 'src/deform_pool_cuda_kernel.cu'\n        ]),\n    ],\n    cmdclass={'build_ext': BuildExtension})\n"""
modules/inplace_abn/__init__.py,0,b''
modules/inplace_abn/functions.py,10,"b'from os import path\nimport torch\nimport torch.distributed as dist\nimport torch.autograd as autograd\nimport torch.cuda.comm as comm\nfrom torch.autograd.function import once_differentiable\nfrom torch.utils.cpp_extension import load\n\n_src_path = path.join(path.dirname(path.abspath(__file__)), ""src"")\n_backend = load(name=""inplace_abn"",\n                extra_cflags=[""-O3""],\n                sources=[path.join(_src_path, f) for f in [\n                    ""inplace_abn.cpp"",\n                    ""inplace_abn_cpu.cpp"",\n                    ""inplace_abn_cuda.cu"",\n                    ""inplace_abn_cuda_half.cu""\n                ]],\n                extra_cuda_cflags=[""--expt-extended-lambda""])\n\n# Activation names\nACT_RELU = ""relu""\nACT_LEAKY_RELU = ""leaky_relu""\nACT_ELU = ""elu""\nACT_NONE = ""none""\n\n\ndef _check(fn, *args, **kwargs):\n    success = fn(*args, **kwargs)\n    if not success:\n        raise RuntimeError(""CUDA Error encountered in {}"".format(fn))\n\n\ndef _broadcast_shape(x):\n    out_size = []\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            out_size.append(1)\n        else:\n            out_size.append(s)\n    return out_size\n\n\ndef _reduce(x):\n    if len(x.size()) == 2:\n        return x.sum(dim=0)\n    else:\n        n, c = x.size()[0:2]\n        return x.contiguous().view((n, c, -1)).sum(2).sum(0)\n\n\ndef _count_samples(x):\n    count = 1\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            count *= s\n    return count\n\n\ndef _act_forward(ctx, x):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_forward(x, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_forward(x)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\ndef _act_backward(ctx, x, dx):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_backward(x, dx, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_backward(x, dx)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\nclass InPlaceABN(autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        count = _count_samples(x)\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0, dtype=torch.float32)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0, dtype=torch.float32)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * count / (count - 1))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n        else:\n            # TODO: implement simplified CUDA backward for inference mode\n            edz = dz.new_zeros(dz.size(1))\n            eydz = dz.new_zeros(dz.size(1))\n\n        dx = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        # dweight = eydz * weight.sign() if ctx.affine else None\n        dweight = eydz if ctx.affine else None\n        if dweight is not None:\n            dweight[weight < 0] *= -1\n        dbias = edz if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\n\nclass InPlaceABNSync(autograd.Function):\n    @classmethod\n    def forward(cls, ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01, equal_batches=True):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        ctx.world_size = dist.get_world_size() if dist.is_initialized() else 1\n\n        # count = _count_samples(x)\n        batch_size = x.new_tensor([x.shape[0]], dtype=torch.long)\n\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0, dtype=torch.float32)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0, dtype=torch.float32)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n            if ctx.world_size > 1:\n                # get global batch size\n                if equal_batches:\n                    batch_size *= ctx.world_size\n                else:\n                    dist.all_reduce(batch_size, dist.ReduceOp.SUM)\n\n                ctx.factor = x.shape[0] / float(batch_size.item())\n\n                mean_all = mean.clone() * ctx.factor\n                dist.all_reduce(mean_all, dist.ReduceOp.SUM)\n\n                var_all = (var + (mean - mean_all) ** 2) * ctx.factor\n                dist.all_reduce(var_all, dist.ReduceOp.SUM)\n\n                mean = mean_all\n                var = var_all\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            count = batch_size.item() * x.view(x.shape[0], x.shape[1], -1).shape[-1]\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * (float(count) / (count - 1)))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n            edz_local = edz.clone()\n            eydz_local = eydz.clone()\n\n            if ctx.world_size > 1:\n                edz *= ctx.factor\n                dist.all_reduce(edz, dist.ReduceOp.SUM)\n\n                eydz *= ctx.factor\n                dist.all_reduce(eydz, dist.ReduceOp.SUM)\n        else:\n            edz_local = edz = dz.new_zeros(dz.size(1))\n            eydz_local = eydz = dz.new_zeros(dz.size(1))\n\n        dx = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        # dweight = eydz_local * weight.sign() if ctx.affine else None\n        dweight = eydz_local if ctx.affine else None\n        if dweight is not None:\n            dweight[weight < 0] *= -1\n        dbias = edz_local if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\n\ninplace_abn = InPlaceABN.apply\ninplace_abn_sync = InPlaceABNSync.apply\n\n__all__ = [""inplace_abn"", ""inplace_abn_sync"", ""ACT_RELU"", ""ACT_LEAKY_RELU"", ""ACT_ELU"", ""ACT_NONE""]\n'"
modules/inplace_abn/iabn.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\n\ntry:\n    from queue import Queue\nexcept ImportError:\n    from Queue import Queue\n\nfrom .functions import *\n\n\nclass ABN(nn.Module):\n    """"""Activated Batch Normalization\n\n    This gathers a `BatchNorm2d` and an activation function in a single module\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(ABN, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        self.activation = activation\n        self.slope = slope\n        if self.affine:\n            self.weight = nn.Parameter(torch.ones(num_features))\n            self.bias = nn.Parameter(torch.zeros(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.constant_(self.running_mean, 0)\n        nn.init.constant_(self.running_var, 1)\n        if self.affine:\n            nn.init.constant_(self.weight, 1)\n            nn.init.constant_(self.bias, 0)\n\n    def forward(self, x):\n        x = functional.batch_norm(x, self.running_mean, self.running_var, self.weight, self.bias,\n                                  self.training, self.momentum, self.eps)\n\n        if self.activation == ACT_RELU:\n            return functional.relu(x, inplace=True)\n        elif self.activation == ACT_LEAKY_RELU:\n            return functional.leaky_relu(x, negative_slope=self.slope, inplace=True)\n        elif self.activation == ACT_ELU:\n            return functional.elu(x, inplace=True)\n        else:\n            return x\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \', slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass InPlaceABN(ABN):\n    """"""InPlace Activated Batch Normalization""""""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an InPlace Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(InPlaceABN, self).__init__(num_features, eps, momentum, affine, activation, slope)\n\n    def forward(self, x):\n        return inplace_abn(x, self.weight, self.bias, self.running_mean, self.running_var,\n                           self.training, self.momentum, self.eps, self.activation, self.slope)\n\n\nclass InPlaceABNSync(ABN):\n    """"""InPlace Activated Batch Normalization with cross-GPU synchronization\n    This assumes that it will be replicated across GPUs using the same mechanism as in `nn.DistributedDataParallel`.\n    """"""\n\n    def forward(self, x):\n        return inplace_abn_sync(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                self.training, self.momentum, self.eps, self.activation, self.slope)\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \', slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n'"
modules/inplace_abn/setup.py,4,"b'import torch\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\n\nprint(""torch.__version__  = "", torch.__version__)\nTORCH_MAJOR = int(torch.__version__.split(\'.\')[0])\n\nif TORCH_MAJOR < 1:\n    raise RuntimeError(""Inplace ABN requires Pytorch 1.0 or newer.\\n"" +\n                       ""The latest stable release can be obtained from https://pytorch.org/"")\nversion = \'0.1.2\'\nsetup(\n    name=\'inplace_abn\',\n    version=version,\n    ext_modules=[\n        CUDAExtension(\'inplace_abn\', [\n            \'src/inplace_abn.cpp\',\n            \'src/inplace_abn_cpu.cpp\',\n            \'src/inplace_abn_cuda.cu\',\n            \'src/inplace_abn_cuda_half.cu\',\n        ], extra_compile_args={\'cxx\': [\'-O3\', ],\n                               \'nvcc\': [\'-O3\',\n                                        ""-DCUDA_HAS_FP16=1"",\n                                        \'--expt-extended-lambda\',\n                                        \'--use_fast_math\']})\n    ],\n    description=\'PyTorch Extensions: Inplace ABN\',\n    cmdclass={\n        \'build_ext\': BuildExtension\n    })\n'"
datasets/cityscapes/list/__init__.py,0,b''
datasets/cityscapes/list/make_list.py,0,"b'import os\n\n\ndef recursive_glob(rootdir=\'.\', suffix=\'\'):\n    """"""Performs recursive glob with given suffix and rootdir\n        :param rootdir is the root directory\n        :param suffix is the suffix to be searched\n    """"""\n    return [os.path.join(looproot, filename)\n            for looproot, _, filenames in os.walk(rootdir)\n            for filename in filenames if filename.endswith(suffix)]\n\n\nfile_list = recursive_glob(rootdir=""/home/huijun/Datasets/Cityscapes/leftImg8bit/deepdrive"", suffix=\'.png\')\nfile_list.sort()\nprint(""> ok !!!"")\nlist_save = os.path.join(""deepdrive.lst"")\n\nwith open(list_save, \'w\') as f:\n    for idx, file_path in enumerate(file_list):\n        print(""> Processing {}"".format(str(idx)))\n        save_path = file_path.replace(""/home/huijun/Datasets/Cityscapes/"", """")\n\n        f.write(save_path + os.linesep)\n\n'"
datasets/cityscapes/list/select_hard.py,0,"b'import os\nimport numpy as np\nfrom PIL import Image\n\n\nif __name__ == ""__main__"":\n    data_root = ""/home/huijun/Datasets/Cityscapes""\n    list_path = ""/home/huijun/PycharmProjects/LightNet++/datasets/cityscapes/list/train+.lst""\n    image_paths = [i_id.strip() for i_id in open(""{}"".format(list_path))]\n\n    list_save = ""hard.lst""\n    # hard_id = {""wall"": 12, ""fence"": 13, ""truck"": 27, ""train"": 31}\n    hard_id = {""wall"": 12, ""truck"": 27, ""train"": 31}\n\n    with open(list_save, \'w\') as f:\n        for idx, image_path_sub in enumerate(image_paths):\n            image_path = os.path.join(data_root, image_path_sub)\n            mask_path = image_path.replace(""leftImg8bit"", ""gtFine"")\n            mask_path = mask_path.replace("".png"", ""_labelIds.png"")\n\n            if not os.path.isfile(image_path) or not os.path.exists(image_path):\n                raise Exception(""> Image: {} is not a file or not exist, can not be opened."".format(image_path))\n\n            if not os.path.isfile(mask_path) or not os.path.exists(mask_path):\n                raise Exception(""> Mask: {} is not a file or not exist, can not be opened."".format(mask_path))\n\n            # --------------------------------------------------------- #\n            # 1. Read Image and Mask\n            # --------------------------------------------------------- #\n            # image = Image.open(image_path).convert(\'RGB\')\n            mask = Image.open(mask_path).convert(\'L\')\n            mask = np.array(mask, dtype=np.uint8).copy()\n\n            fun_classes = np.unique(mask)\n            print(\'> {} Classes found: {}\'.format(len(fun_classes), fun_classes))\n\n            has_hard = False\n            for key, hid in hard_id.items():\n                if hid in fun_classes:\n                    has_hard = True\n                    print(""> {} is hard id, put this image into hard list!!!"".format(hid))\n\n            if has_hard:\n                f.write(image_path_sub + os.linesep)\n\n\n\n'"
deploy/cityscapes/evaluation/__init__.py,0,b''
deploy/cityscapes/evaluation/eval.py,7,"b'import torchvision.transforms as transforms\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nimport imageio\nimport torch\nimport time\nimport cv2\nimport os\n\nfrom PIL import Image\n\ntry:\n    from apex.fp16_utils import *\n    from apex import amp\n    import apex\nexcept ImportError:\n    raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to run this example."")\n\n\ndef evaluate(data_root, model, result_path, split):\n    train_id2label_id = {0: 7,\n                         1: 8,\n                         2: 11,\n                         3: 12,\n                         4: 13,\n                         5: 17,\n                         6: 19,\n                         7: 20,\n                         8: 21,\n                         9: 22,\n                         10: 23,\n                         11: 24,\n                         12: 25,\n                         13: 26,\n                         14: 27,\n                         15: 28,\n                         16: 31,\n                         17: 32,\n                         18: 33}\n    mean = [0.2997, 0.3402, 0.3072]\n    std = [0.1549, 0.1579, 0.1552]\n\n    trans_norm = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Normalize(mean=mean, std=std)])\n\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Inference Model\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    data_root = os.path.join(data_root, split)\n    org_data_sub = os.listdir(data_root)\n    org_data_sub.sort()\n\n    tt_time = time.time()\n    for idx in np.arange(len(org_data_sub)):\n        city_name = org_data_sub[idx]\n        print(""> # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n        print(""> 2. Processing City # {}..."".format(city_name))\n        curr_city_path = os.path.join(data_root, city_name)\n        images_name = os.listdir(curr_city_path)\n        images_name.sort()\n\n        for img_id in np.arange(len(images_name)):\n            curr_image = images_name[img_id]\n            # print(""> # ------------------------------------------------------------------------- #"")\n            print(""> Processing City # {}, Image: {}..."".format(city_name, curr_image))\n\n            with torch.no_grad():\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                # 2.1 Pre-processing Image\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                curr_img_path = os.path.join(curr_city_path, curr_image)\n                image = Image.open(curr_img_path).convert(\'RGB\')\n                image = np.array(image, dtype=np.uint8)\n                image = np.array(image[:, :, ::-1], dtype=np.uint8)  # From RGB to BGR\n                image = trans_norm(image)\n                image = torch.unsqueeze(image, dim=0).cuda()  # [N, C, H, W]\n\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                # 2.2 Prediction/Inference\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                start_time = time.time()\n                prediction = F.softmax(model(image), dim=1).argmax(dim=1)\n                print(""> Inference Time: {}s"".format(time.time() - start_time))\n                prediction = np.squeeze(prediction.cpu().numpy())\n\n                mapper = lambda t: train_id2label_id[t]\n                vfunc = np.vectorize(mapper)\n                prediction = vfunc(prediction)\n\n                # fun_classes = np.unique(prediction)\n                # print(\'> {} Classes found: {}\'.format(len(fun_classes), fun_classes))\n                print(""> Processed City #{}, Image: {}, Time: {}s"".format(city_name, curr_image,\n                                                                         (time.time() - start_time)))\n\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                # 2.3 Saving prediction result\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                save_path = os.path.join(result_path, city_name)\n                if not os.path.exists(save_path):\n                    os.makedirs(save_path, exist_ok=True)\n\n                # cv2.namedWindow(""Prediction"", cv2.WINDOW_NORMAL)\n                # cv2.imshow(""Prediction"", prediction)\n                # cv2.waitKey(0)\n\n                prediction = prediction.astype(np.uint8)\n                save_name = os.path.basename(curr_image)[:-15] + \'pred_labelIds.png\'\n                save_path = os.path.join(save_path, save_name)\n                imageio.imsave(save_path, prediction)\n\n    print(""> # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Total Time Cost: {}"".format(time.time() - tt_time))\n    print(""> Done!!!"")\n    print(""> # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == ""__main__"":\n    from models.shufflenetv2plus import ShuffleNetV2Plus\n    from models.mobilenetv2plus import MobileNetV2Plus\n    from modules.inplace_abn.iabn import InPlaceABNSync\n    from functools import partial\n    import shutil\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\n    cudnn.benchmark = True\n    assert torch.backends.cudnn.enabled, ""fp16 mode requires cudnn backend to be enabled.""\n    # Initialize Amp\n    amp_handle = amp.init(enabled=True)\n\n    split = ""test""\n    method = ""shufflenetv2plus_x1.0""  # shufflenetv2plus_x1.0  shufflenetv2plus_x0.5  mobilenetv2plus\n    data_path = ""/home/huijun/Datasets/Cityscapes/leftImg8bit""\n    result_path = ""/home/huijun/Datasets/Cityscapes/results/{}"".format(split)\n    weight_path = ""/home/huijun/TrainLog/weights/cityscapes_{}_best_model.pkl"".format(method)\n\n    if os.path.exists(result_path):\n        print(""> Path {} is exist, delete it..."".format(result_path))\n        shutil.rmtree(result_path)\n\n    if not os.path.exists(result_path):\n        print(""> Path {} is not exist, create it..."".format(result_path))\n        os.mkdir(result_path)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n    # model = MobileNetV2Plus(num_classes=19, width_multi=1.0, fuse_chns=512,\n    #                         aspp_chns=256, aspp_dilate=(12, 24, 36),\n    #                         norm_act=partial(InPlaceABNSync, activation=""leaky_relu"", slope=0.01))\n    model = ShuffleNetV2Plus(num_classes=19, fuse_chns=512, aspp_chns=256,\n                             aspp_dilate=(12, 24, 36), width_multi=1.0,\n                             norm_act=partial(InPlaceABNSync, activation=""leaky_relu"", slope=0.01))\n    # model = ShuffleNetV2Plus(num_classes=19, fuse_chns=256,\n    #                          aspp_chns=128, aspp_dilate=(12, 24, 36), width_multi=0.5,\n    #                          norm_act=partial(InPlaceABNSync, activation=""leaky_relu"", slope=0.01))\n    model = apex.parallel.convert_syncbn_model(model)\n    model = nn.DataParallel(model, device_ids=[0]).cuda()\n\n    pre_weight = torch.load(weight_path)\n    pre_weight = pre_weight[\'model_state\']\n    model.load_state_dict(pre_weight)\n    del pre_weight\n\n    model.eval()\n    evaluate(data_path, model, result_path, split)\n\n'"
deploy/cityscapes/examples/__init__.py,0,b''
deploy/cityscapes/video_demo/__init__.py,0,b''
deploy/cityscapes/video_demo/video_demo.py,7,"b'import torchvision.transforms as transforms\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nimport imageio\nimport torch\nimport time\nimport cv2\nimport os\n\nfrom modules.inplace_abn.iabn import InPlaceABNSync\nfrom functools import partial\nfrom PIL import Image\n\ntry:\n    from apex.fp16_utils import *\n    from apex import amp\n    import apex\nexcept ImportError:\n    raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to run this example."")\n\n\ndef decode_segmap(pred, label_colours, num_classes):\n    r = pred.copy()\n    g = pred.copy()\n    b = pred.copy()\n    for l in range(0, num_classes):\n        r[pred == l] = label_colours[l][0]\n        g[pred == l] = label_colours[l][1]\n        b[pred == l] = label_colours[l][2]\n\n    rgb = np.zeros((pred.shape[0], pred.shape[1], 3))\n    rgb[:, :, 0] = r\n    rgb[:, :, 1] = g\n    rgb[:, :, 2] = b\n    return rgb\n\n\ndef video_demo(data_root, model, method, result_path, split):\n    colors = [  # [  0,   0,   0],\n              [128, 64, 128],\n              [244, 35, 232],\n              [70, 70, 70],\n              [102, 102, 156],\n              [190, 153, 153],\n              [153, 153, 153],\n              [250, 170, 30],\n              [220, 220, 0],\n              [107, 142, 35],\n              [152, 251, 152],\n              [0, 130, 180],\n              [220, 20, 60],\n              [255, 0, 0],\n              [0, 0, 142],\n              [0, 0, 70],\n              [0, 60, 100],\n              [0, 80, 100],\n              [0, 0, 230],\n              [119, 11, 32]]\n\n    label_colours = dict(zip(range(19), colors))\n    class_names = [\'unlabelled\', \'road\', \'sidewalk\', \'building\', \'wall\', \'fence\',\n                   \'pole\', \'traffic_light\', \'traffic_sign\', \'vegetation\', \'terrain\',\n                   \'sky\', \'person\', \'rider\', \'car\', \'truck\', \'bus\', \'train\',\n                   \'motorcycle\', \'bicycle\']\n\n    net_h, net_w, color_bar_w = 1024, 2048, 120\n    frame_size = (net_w + color_bar_w, net_h)\n    codec = cv2.VideoWriter_fourcc(*\'MJPG\')\n\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 0. Setup Color Bar\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    color_map = label_colours\n    num_classes = 19\n\n    grid_height = int(net_h // num_classes)\n    start_pixel = int((net_h % num_classes) / 2)\n\n    color_bar = np.ones((net_h, color_bar_w, 3), dtype=np.uint8) * 128\n    for train_id in np.arange(num_classes):\n        end_pixel = start_pixel + grid_height\n        color_bar[start_pixel:end_pixel, :, :] = color_map[train_id]\n\n        font = cv2.FONT_HERSHEY_TRIPLEX\n        cv2.putText(color_bar, class_names[train_id + 1],\n                    (2, start_pixel + 5 + int(grid_height // 2)),\n                    font, 0.55, (255, 255, 255), 1, cv2.LINE_AA)\n\n        start_pixel = end_pixel\n    color_bar = color_bar[:, :, ::-1]\n    my_writer = cv2.VideoWriter(""{}_video_demo.avi"".format(method), codec, 24.0, frame_size)\n\n    mean = [0.2997, 0.3402, 0.3072]\n    std = [0.1549, 0.1579, 0.1552]\n    trans_norm = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Normalize(mean=mean, std=std)])\n\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Inference Model\n    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    data_root = os.path.join(data_root, split)\n    org_data_sub = os.listdir(data_root)\n    org_data_sub.sort()\n\n    for idx in np.arange(len(org_data_sub)):\n        city_name = org_data_sub[idx]\n        print(""> # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n        print(""> 2. Processing City # {}..."".format(city_name))\n        curr_city_path = os.path.join(data_root, city_name)\n        images_name = os.listdir(curr_city_path)\n        images_name.sort()\n\n        for img_id in np.arange(len(images_name)):\n            curr_image = images_name[img_id]\n            print(""> Processing City #{} Image: {}..."".format(city_name, curr_image))\n\n            with torch.no_grad():\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                # 2.1 Pre-processing Image\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                curr_img_path = os.path.join(curr_city_path, curr_image)\n                image = Image.open(curr_img_path).convert(\'RGB\')\n                image = np.array(image, dtype=np.uint8)\n                image_org = image.copy()\n                image = np.array(image[:, :, ::-1], dtype=np.uint8)  # From RGB to BGR\n                image = trans_norm(image)\n                image = torch.unsqueeze(image, dim=0).cuda()  # [N, C, H, W]\n\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                # 2.2 Prediction/Inference\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                start_time = time.time()\n                prediction = F.softmax(model(image), dim=1).argmax(dim=1)\n                print(""> Inference Time: {}s"".format(time.time() - start_time))\n                prediction = np.squeeze(prediction.cpu().numpy())\n                prediction = decode_segmap(prediction, label_colours, num_classes)\n                prediction = prediction.astype(np.uint8)\n\n                print(""> Processed City #{} Image: {}, Time: {}s"".format(city_name, curr_image,\n                                                                         (time.time() - start_time)))\n\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                # 2.3 Saving prediction result\n                # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n                save_path = os.path.join(result_path, city_name)\n                if not os.path.exists(save_path):\n                    os.makedirs(save_path, exist_ok=True)\n\n                # cv2.namedWindow(""Prediction"", cv2.WINDOW_NORMAL)\n                # cv2.imshow(""Prediction"", prediction)\n                # cv2.waitKey(0)\n\n                prediction = prediction.astype(np.uint8)\n                save_name = os.path.basename(curr_image)[:-15] + \'pred_labelIds.png\'\n                save_path = os.path.join(save_path, save_name)\n                imageio.imsave(save_path, prediction)\n\n                img_msk = cv2.addWeighted(image_org, 0.55, prediction, 0.45, 0)\n                img_msk = img_msk[:, :, ::-1]  # RGB\n                img_msk_color = np.concatenate((img_msk, color_bar), axis=1)\n\n                cv2.imshow(""show"", img_msk_color)\n                cv2.waitKey(0)\n                my_writer.write(img_msk_color)\n\n    my_writer.release()\n    print(""> # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> Done!!!"")\n    print(""> # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n\n\nif __name__ == ""__main__"":\n    from models.mobilenetv2plus import MobileNetV2Plus\n    from models.shufflenetv2plus import ShuffleNetV2Plus\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\n    cudnn.benchmark = True\n    assert torch.backends.cudnn.enabled, ""fp16 mode requires cudnn backend to be enabled.""\n    # Initialize Amp\n    amp_handle = amp.init(enabled=True)\n\n    split = ""demoVideo""\n    method = ""shufflenetv2plus_x1.0""\n    data_path = ""/home/huijun/Datasets/Cityscapes/leftImg8bit""\n    result_path = ""/home/huijun/Datasets/Cityscapes/results/{}"".format(split)\n    weight_path = ""/home/huijun/TrainLog/weights/cityscapes_{}_best_model.pkl"".format(method)\n\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    # 1. Setup Model\n    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n    print(""> # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #"")\n    print(""> 1. Setting up Model..."")\n    # model = MobileNetV2Plus(num_classes=19, width_multi=1.0,\n    #                         aspp_chns=256, aspp_dilate=(12, 24, 36),\n    #                         norm_act=partial(InPlaceABNSync, eps=1e-05, momentum=0.1,\n    #                                          activation=""leaky_relu"", slope=0.01))\n\n    model = ShuffleNetV2Plus(num_classes=19, fuse_chns=512, aspp_chns=256, aspp_dilate=(12, 24, 36), width_multi=1.0,\n                             norm_act=partial(InPlaceABNSync, activation=""leaky_relu"", slope=0.01))\n\n    model = apex.parallel.convert_syncbn_model(model)\n    model = nn.DataParallel(model, device_ids=[0]).cuda()\n\n    pre_weight = torch.load(weight_path)\n    pre_weight = pre_weight[\'model_state\']\n    model.load_state_dict(pre_weight)\n    del pre_weight\n\n    model.eval()\n    video_demo(data_path, model, method, result_path, split)\n\n'"
modules/deformable/functions/__init__.py,0,b''
modules/deformable/functions/deform_conv.py,10,"b'import torch\nfrom torch.autograd import Function\nfrom torch.nn.modules.utils import _pair\n\nfrom .. import deform_conv_cuda\n\n\nclass DeformConvFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                weight,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,\n                deformable_groups=1,\n                im2col_step=64):\n        if input is not None and input.dim() != 4:\n            raise ValueError(\n                ""Expected 4D tensor as input, got {}D tensor instead."".format(\n                    input.dim()))\n        ctx.stride = _pair(stride)\n        ctx.padding = _pair(padding)\n        ctx.dilation = _pair(dilation)\n        ctx.groups = groups\n        ctx.deformable_groups = deformable_groups\n        ctx.im2col_step = im2col_step\n\n        ctx.save_for_backward(input, offset, weight)\n\n        output = input.new_empty(\n            DeformConvFunction._output_size(input, weight, ctx.padding,\n                                            ctx.dilation, ctx.stride))\n\n        ctx.bufs_ = [input.new_empty(0), input.new_empty(0)]  # columns, ones\n\n        if not input.is_cuda:\n            raise NotImplementedError\n        else:\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n            deform_conv_cuda.deform_conv_forward_cuda(\n                input, weight, offset, output, ctx.bufs_[0], ctx.bufs_[1],\n                weight.size(3), weight.size(2), ctx.stride[1], ctx.stride[0],\n                ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                ctx.dilation[0], ctx.groups, ctx.deformable_groups,\n                cur_im2col_step)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, offset, weight = ctx.saved_tensors\n\n        grad_input = grad_offset = grad_weight = None\n\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        else:\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n\n            if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:\n                grad_input = torch.zeros_like(input)\n                grad_offset = torch.zeros_like(offset)\n                deform_conv_cuda.deform_conv_backward_input_cuda(\n                    input, offset, grad_output, grad_input,\n                    grad_offset, weight, ctx.bufs_[0], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.groups, ctx.deformable_groups,\n                    cur_im2col_step)\n\n            if ctx.needs_input_grad[2]:\n                grad_weight = torch.zeros_like(weight)\n                deform_conv_cuda.deform_conv_backward_parameters_cuda(\n                    input, offset, grad_output,\n                    grad_weight, ctx.bufs_[0], ctx.bufs_[1], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.groups, ctx.deformable_groups, 1,\n                    cur_im2col_step)\n\n        return (grad_input, grad_offset, grad_weight, None, None, None, None,\n                None)\n\n    @staticmethod\n    def _output_size(input, weight, padding, dilation, stride):\n        channels = weight.size(0)\n        output_size = (input.size(0), channels)\n        for d in range(input.dim() - 2):\n            in_size = input.size(d + 2)\n            pad = padding[d]\n            kernel = dilation[d] * (weight.size(d + 2) - 1) + 1\n            stride_ = stride[d]\n            output_size += ((in_size + (2 * pad) - kernel) // stride_ + 1, )\n        if not all(map(lambda s: s > 0, output_size)):\n            raise ValueError(\n                ""convolution input is too small (output would be {})"".format(\n                    \'x\'.join(map(str, output_size))))\n        return output_size\n\n\nclass ModulatedDeformConvFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                mask,\n                weight,\n                bias=None,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,\n                deformable_groups=1):\n        ctx.stride = stride\n        ctx.padding = padding\n        ctx.dilation = dilation\n        ctx.groups = groups\n        ctx.deformable_groups = deformable_groups\n        ctx.with_bias = bias is not None\n        if not ctx.with_bias:\n            bias = input.new_empty(1)  # fake tensor\n        if not input.is_cuda:\n            raise NotImplementedError\n        if weight.requires_grad or mask.requires_grad or offset.requires_grad \\\n                or input.requires_grad:\n            ctx.save_for_backward(input, offset, mask, weight, bias)\n        output = input.new_empty(\n            ModulatedDeformConvFunction._infer_shape(ctx, input, weight))\n        ctx._bufs = [input.new_empty(0), input.new_empty(0)]\n        deform_conv_cuda.modulated_deform_conv_cuda_forward(\n            input, weight, bias, ctx._bufs[0], offset, mask, output,\n            ctx._bufs[1], weight.shape[2], weight.shape[3], ctx.stride,\n            ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation,\n            ctx.groups, ctx.deformable_groups, ctx.with_bias)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        input, offset, mask, weight, bias = ctx.saved_tensors\n        grad_input = torch.zeros_like(input)\n        grad_offset = torch.zeros_like(offset)\n        grad_mask = torch.zeros_like(mask)\n        grad_weight = torch.zeros_like(weight)\n        grad_bias = torch.zeros_like(bias)\n        deform_conv_cuda.modulated_deform_conv_cuda_backward(\n            input, weight, bias, ctx._bufs[0], offset, mask, ctx._bufs[1],\n            grad_input, grad_weight, grad_bias, grad_offset, grad_mask,\n            grad_output, weight.shape[2], weight.shape[3], ctx.stride,\n            ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation,\n            ctx.groups, ctx.deformable_groups, ctx.with_bias)\n        if not ctx.with_bias:\n            grad_bias = None\n\n        return (grad_input, grad_offset, grad_mask, grad_weight, grad_bias,\n                None, None, None, None, None)\n\n    @staticmethod\n    def _infer_shape(ctx, input, weight):\n        n = input.size(0)\n        channels_out = weight.size(0)\n        height, width = input.shape[2:4]\n        kernel_h, kernel_w = weight.shape[2:4]\n        height_out = (height + 2 * ctx.padding -\n                      (ctx.dilation * (kernel_h - 1) + 1)) // ctx.stride + 1\n        width_out = (width + 2 * ctx.padding -\n                     (ctx.dilation * (kernel_w - 1) + 1)) // ctx.stride + 1\n        return n, channels_out, height_out, width_out\n\n\ndeform_conv = DeformConvFunction.apply\nmodulated_deform_conv = ModulatedDeformConvFunction.apply\n'"
modules/deformable/functions/deform_pool.py,3,"b'import torch\nfrom torch.autograd import Function\n\nfrom .. import deform_pool_cuda\n\n\nclass DeformRoIPoolingFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                data,\n                rois,\n                offset,\n                spatial_scale,\n                out_size,\n                out_channels,\n                no_trans,\n                group_size=1,\n                part_size=None,\n                sample_per_part=4,\n                trans_std=.0):\n        ctx.spatial_scale = spatial_scale\n        ctx.out_size = out_size\n        ctx.out_channels = out_channels\n        ctx.no_trans = no_trans\n        ctx.group_size = group_size\n        ctx.part_size = out_size if part_size is None else part_size\n        ctx.sample_per_part = sample_per_part\n        ctx.trans_std = trans_std\n\n        assert 0.0 <= ctx.trans_std <= 1.0\n        if not data.is_cuda:\n            raise NotImplementedError\n\n        n = rois.shape[0]\n        output = data.new_empty(n, out_channels, out_size, out_size)\n        output_count = data.new_empty(n, out_channels, out_size, out_size)\n        deform_pool_cuda.deform_psroi_pooling_cuda_forward(\n            data, rois, offset, output, output_count, ctx.no_trans,\n            ctx.spatial_scale, ctx.out_channels, ctx.group_size, ctx.out_size,\n            ctx.part_size, ctx.sample_per_part, ctx.trans_std)\n\n        if data.requires_grad or rois.requires_grad or offset.requires_grad:\n            ctx.save_for_backward(data, rois, offset)\n        ctx.output_count = output_count\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n\n        data, rois, offset = ctx.saved_tensors\n        output_count = ctx.output_count\n        grad_input = torch.zeros_like(data)\n        grad_rois = None\n        grad_offset = torch.zeros_like(offset)\n\n        deform_pool_cuda.deform_psroi_pooling_cuda_backward(\n            grad_output, data, rois, offset, output_count, grad_input,\n            grad_offset, ctx.no_trans, ctx.spatial_scale, ctx.out_channels,\n            ctx.group_size, ctx.out_size, ctx.part_size, ctx.sample_per_part,\n            ctx.trans_std)\n        return (grad_input, grad_rois, grad_offset, None, None, None, None,\n                None, None, None, None)\n\n\ndeform_roi_pooling = DeformRoIPoolingFunction.apply\n'"
modules/deformable/modules/__init__.py,0,b''
modules/deformable/modules/deform_conv.py,8,"b""import math\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.modules.utils import _pair\n\nfrom ..functions.deform_conv import deform_conv, modulated_deform_conv\n\n\nclass DeformConv(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=False):\n        assert not bias\n        super(DeformConv, self).__init__()\n\n        assert in_channels % groups == 0, \\\n            'in_channels {} cannot be divisible by groups {}'.format(\n                in_channels, groups)\n        assert out_channels % groups == 0, \\\n            'out_channels {} cannot be divisible by groups {}'.format(\n                out_channels, groups)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n        self.groups = groups\n        self.deformable_groups = deformable_groups\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels // self.groups,\n                         *self.kernel_size))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, offset):\n        return deform_conv(input, offset, self.weight, self.stride,\n                           self.padding, self.dilation, self.groups,\n                           self.deformable_groups)\n\n\nclass ModulatedDeformConv(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=True):\n        super(ModulatedDeformConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.deformable_groups = deformable_groups\n        self.with_bias = bias\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels // groups,\n                         *self.kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.zero_()\n\n    def forward(self, input, offset, mask):\n        return modulated_deform_conv(\n            input, offset, mask, self.weight, self.bias, self.stride,\n            self.padding, self.dilation, self.groups, self.deformable_groups)\n\n\nclass ModulatedDeformConvPack(ModulatedDeformConv):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=True):\n        super(ModulatedDeformConvPack, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            groups, deformable_groups, bias)\n\n        self.conv_offset_mask = nn.Conv2d(\n            self.in_channels // self.groups,\n            self.deformable_groups * 3 * self.kernel_size[0] *\n            self.kernel_size[1],\n            kernel_size=self.kernel_size,\n            stride=_pair(self.stride),\n            padding=_pair(self.padding),\n            bias=True)\n        self.init_offset()\n\n    def init_offset(self):\n        self.conv_offset_mask.weight.data.zero_()\n        self.conv_offset_mask.bias.data.zero_()\n\n    def forward(self, input):\n        out = self.conv_offset_mask(input)\n        o1, o2, mask = torch.chunk(out, 3, dim=1)\n        offset = torch.cat((o1, o2), dim=1)\n        mask = torch.sigmoid(mask)\n        return modulated_deform_conv(\n            input, offset, mask, self.weight, self.bias, self.stride,\n            self.padding, self.dilation, self.groups, self.deformable_groups)\n"""
modules/deformable/modules/deform_pool.py,0,"b'from torch import nn\n\nfrom ..functions.deform_pool import deform_roi_pooling\n\n\nclass DeformRoIPooling(nn.Module):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0):\n        super(DeformRoIPooling, self).__init__()\n        self.spatial_scale = spatial_scale\n        self.out_size = out_size\n        self.out_channels = out_channels\n        self.no_trans = no_trans\n        self.group_size = group_size\n        self.part_size = out_size if part_size is None else part_size\n        self.sample_per_part = sample_per_part\n        self.trans_std = trans_std\n\n    def forward(self, data, rois, offset):\n        if self.no_trans:\n            offset = data.new_empty(0)\n        return deform_roi_pooling(\n            data, rois, offset, self.spatial_scale, self.out_size,\n            self.out_channels, self.no_trans, self.group_size, self.part_size,\n            self.sample_per_part, self.trans_std)\n\n\nclass DeformRoIPoolingPack(DeformRoIPooling):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0,\n                 deform_fc_channels=1024):\n        super(DeformRoIPoolingPack,\n              self).__init__(spatial_scale, out_size, out_channels, no_trans,\n                             group_size, part_size, sample_per_part, trans_std)\n\n        self.deform_fc_channels = deform_fc_channels\n\n        if not no_trans:\n            self.offset_fc = nn.Sequential(\n                nn.Linear(self.out_size * self.out_size * self.out_channels,\n                          self.deform_fc_channels),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_channels, self.deform_fc_channels),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_channels,\n                          self.out_size * self.out_size * 2))\n            self.offset_fc[-1].weight.data.zero_()\n            self.offset_fc[-1].bias.data.zero_()\n\n    def forward(self, data, rois):\n        assert data.size(1) == self.out_channels\n        if self.no_trans:\n            offset = data.new_empty(0)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std)\n        else:\n            n = rois.shape[0]\n            offset = data.new_empty(0)\n            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                   self.out_size, self.out_channels, True,\n                                   self.group_size, self.part_size,\n                                   self.sample_per_part, self.trans_std)\n            offset = self.offset_fc(x.view(n, -1))\n            offset = offset.view(n, 2, self.out_size, self.out_size)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std)\n\n\nclass ModulatedDeformRoIPoolingPack(DeformRoIPooling):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0,\n                 deform_fc_channels=1024):\n        super(ModulatedDeformRoIPoolingPack, self).__init__(\n            spatial_scale, out_size, out_channels, no_trans, group_size,\n            part_size, sample_per_part, trans_std)\n\n        self.deform_fc_channels = deform_fc_channels\n\n        if not no_trans:\n            self.offset_fc = nn.Sequential(\n                nn.Linear(self.out_size * self.out_size * self.out_channels,\n                          self.deform_fc_channels),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_channels, self.deform_fc_channels),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_channels,\n                          self.out_size * self.out_size * 2))\n            self.offset_fc[-1].weight.data.zero_()\n            self.offset_fc[-1].bias.data.zero_()\n            self.mask_fc = nn.Sequential(\n                nn.Linear(self.out_size * self.out_size * self.out_channels,\n                          self.deform_fc_channels),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_channels,\n                          self.out_size * self.out_size * 1),\n                nn.Sigmoid())\n            self.mask_fc[2].weight.data.zero_()\n            self.mask_fc[2].bias.data.zero_()\n\n    def forward(self, data, rois):\n        assert data.size(1) == self.out_channels\n        if self.no_trans:\n            offset = data.new_empty(0)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std)\n        else:\n            n = rois.shape[0]\n            offset = data.new_empty(0)\n            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                   self.out_size, self.out_channels, True,\n                                   self.group_size, self.part_size,\n                                   self.sample_per_part, self.trans_std)\n            offset = self.offset_fc(x.view(n, -1))\n            offset = offset.view(n, 2, self.out_size, self.out_size)\n            mask = self.mask_fc(x.view(n, -1))\n            mask = mask.view(n, 1, self.out_size, self.out_size)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std) * mask\n'"
