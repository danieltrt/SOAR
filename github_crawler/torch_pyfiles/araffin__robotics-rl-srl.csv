file_path,api_count,code
conftest.py,0,"b'import pytest\n\n\ndef pytest_addoption(parser):\n    parser.addoption(""--fast"", action=""store_true"", default=False,\n                     help=""only run RL with ground truth, and quickly test all the envs"")\n    parser.addoption(""--all"", action=""store_true"", default=False,\n                     help=""run all the tests"")\n\n\ndef pytest_collection_modifyitems(config, items):\n    if config.getoption(""--fast"") and config.getoption(""--all""):\n        raise AssertionError(""Error: incompatible test flags requested"")\n    elif config.getoption(""--fast""):\n        skip_not_fast = pytest.mark.skip(reason=""need to remove --fast option to run"")\n        for item in items:\n            if ""fast"" not in item.keywords:\n                item.add_marker(skip_not_fast)\n    elif not config.getoption(""--all""):\n        skip_slow = pytest.mark.skip(reason=""need to add --all option to run"")\n        for item in items:\n            if ""slow"" in item.keywords:\n                item.add_marker(skip_slow)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'S-RL Toolbox\'\ncopyright = \'2018, Antonin Raffin\'\nauthor = \'Antonin Raffin\'\n\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\nrelease = \'\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# Fix for read the docs\non_rtd = os.environ.get(\'READTHEDOCS\') == \'True\'\nif on_rtd:\n    html_theme = \'default\'\nelse:\n    html_theme = \'sphinx_rtd_theme\'\n\ndef setup(app):\n    app.add_stylesheet(""css/custom_theme.css"")\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'S-RLToolboxdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'S-RLToolbox.tex\', \'S-RL Toolbox Documentation\',\n     \'Antonin Raffin\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'s-rltoolbox\', \'S-RL Toolbox Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'S-RLToolbox\', \'S-RL Toolbox Documentation\',\n     author, \'S-RLToolbox\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n'"
environments/__init__.py,0,b'from enum import Enum\n\n\nclass PlottingType(Enum):\n    PLOT_2D = 1\n    PLOT_3D = 2\n\n\nclass ThreadingType(Enum):\n    PROCESS = 1\n    THREADING = 2\n    NONE = 3\n'
environments/change_to_relative_pos.py,0,"b""import argparse\nfrom os.path import join\nimport shutil\n\nimport numpy as np\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Change existed dataset whose ground_truth is global position to relative position')\n    parser.add_argument('--data-src', type=str, default=None, help='source data folder (global position)')\n    parser.add_argument('--data-dst', type=str, default=None, help='destination data folder, (relative position)')\n\n    args = parser.parse_args()\n    assert args.data_src is not None\n    assert args.data_dst is not None\n    ground_truth = np.load(join(args.data_src, 'ground_truth.npz'))\n    preprocessed_data = np.load(join(args.data_src, 'preprocessed_data.npz'))\n\n    shutil.copytree(args.data_src, args.data_dst)\n    episode_starts = preprocessed_data['episode_starts']\n    print(ground_truth.keys())\n    ground_truth_states = ground_truth['ground_truth_states']\n    target_position = ground_truth['target_positions']\n\n    episode_num = -1\n    \n    print(ground_truth_states.shape)\n    for i in range(ground_truth_states.shape[0]):\n        if episode_starts[i] is True:\n            episode_num += 1\n        ground_truth_states[i, :] = ground_truth_states[i, :] - target_position[episode_num]\n    new_ground_truth = {}\n    for key in ground_truth.keys():\n        if key != 'ground_truth_states':\n            new_ground_truth[key] = ground_truth[key]\n    new_ground_truth['ground_truth_states'] = ground_truth_states\n    np.savez(join(args.data_dst, 'ground_truth.npz'), **new_ground_truth)\n\n\nif __name__ == '__main__':\n    main()\n"""
environments/dataset_fusioner.py,0,"b'from __future__ import division, absolute_import, print_function\n\nimport glob\nimport argparse\nimport os\nimport shutil\n\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Dataset Manipulator: useful to merge two datasets by concatenating \'\n                                                 + \'episodes. PS: Deleting sources after merging into the destination \'\n                                                 + \'folder.\')\n    group = parser.add_mutually_exclusive_group()\n    group.add_argument(\'--merge\', type=str, nargs=3, metavar=(\'source_1\', \'source_2\', \'destination\'),\n                       default=argparse.SUPPRESS,\n                       help=\'Merge two datasets by appending the episodes, deleting sources right after.\')\n\n    args = parser.parse_args()\n\n    if \'merge\' in args:\n        # let make sure everything is in order\n        assert os.path.exists(args.merge[0]), ""Error: dataset \'{}\' could not be found"".format(args.merge[0])\n        assert (not os.path.exists(args.merge[2])), \\\n            ""Error: dataset \'{}\' already exists, cannot rename \'{}\' to \'{}\'"".format(args.merge[2], args.merge[0],\n                                                                                    args.merge[2])\n        # create the output\n        os.mkdir(args.merge[2])\n\n        # copy files from first source\n        os.rename(args.merge[0] + ""/dataset_config.json"", args.merge[2] + ""/dataset_config.json"")\n        os.rename(args.merge[0] + ""/env_globals.json"", args.merge[2] + ""/env_globals.json"")\n\n        for record in sorted(glob.glob(args.merge[0] + ""/record_[0-9]*/*"")):\n            s = args.merge[2] + ""/"" + record.split(""/"")[-2] + \'/\' + record.split(""/"")[-1]\n            os.renames(record, s)\n\n        num_episode_dataset_1 = int(record.split(""/"")[-2][7:]) + 1\n\n        # copy files from second source\n        for record in sorted(glob.glob(args.merge[1] + ""/record_[0-9]*/*"")):\n            episode = str(num_episode_dataset_1 + int(record.split(""/"")[-2][7:]))\n            new_episode = record.split(""/"")[-2][:-len(episode)] + episode\n            s = args.merge[2] + ""/"" + new_episode + \'/\' + record.split(""/"")[-1]\n            os.renames(record, s)\n        num_episode_dataset_2 = int(record.split(""/"")[-2][7:]) + 1\n\n        # load and correct ground_truth\n        ground_truth = {}\n        ground_truth_load = np.load(args.merge[0] + ""/ground_truth.npz"")\n        ground_truth_load_2 = np.load(args.merge[1] + ""/ground_truth.npz"")\n        ground_truth[""images_path""] = []\n        num_episode_dataset = num_episode_dataset_1\n\n        index_slash = args.merge[2].find(""/"")\n        index_margin_str = len(""/record_"")\n        directory_str = args.merge[2][index_slash+1:]\n\n        for idx_, gt_load in enumerate([ground_truth_load, ground_truth_load_2], 1):\n            for arr in gt_load.files:\n                if arr == ""images_path"":\n                    # here, we want to rename just the folder containing the records, hence the black magic\n\n                    for i in tqdm(range(len(gt_load[""images_path""])),\n                                  desc=""Update of paths (Folder "" + str(1+idx_) + "")""):\n                        # find the ""record_"" position\n                        path = gt_load[""images_path""][i]\n                        end_pos = path.find(""/record_"")\n                        inter_pos = path.find(""/frame"")  # pos in the complete path.\n\n                        if idx_ > 1:\n                            episode = str(num_episode_dataset_1 + int(path[end_pos + index_margin_str: inter_pos]))\n                            episode = episode.zfill(3)\n                            new_record_path = ""/record_"" + episode + path[inter_pos:]\n                        else:\n                            new_record_path = path[end_pos:]\n                        ground_truth[""images_path""].append(directory_str + new_record_path)\n                else:\n                    # anything that isnt image_path, we dont need to change\n                    gt_arr = gt_load[arr]\n\n                    if idx_ > 1:\n                        num_episode_dataset = num_episode_dataset_2\n\n                    # HERE check before overwritting that the target is random !+\n                    if gt_load[arr].shape[0] < num_episode_dataset:\n                        gt_arr = np.repeat(gt_load[arr], num_episode_dataset, axis=0)\n\n                    if idx_ > 1:\n                        ground_truth[arr] = np.concatenate((ground_truth[arr], gt_arr), axis=0)\n                    else:\n                        ground_truth[arr] = gt_arr\n\n        # save the corrected ground_truth\n        np.savez(args.merge[2] + ""/ground_truth.npz"", **ground_truth)\n\n        # load and correct the preprocessed data (actions, rewards etc)\n        preprocessed = {}\n        preprocessed_load = np.load(args.merge[0] + ""/preprocessed_data.npz"")\n        preprocessed_load_2 = np.load(args.merge[1] + ""/preprocessed_data.npz"")\n\n        for prepro_load in [preprocessed_load, preprocessed_load_2]:\n            for arr in prepro_load.files:\n                pr_arr = prepro_load[arr]\n                preprocessed[arr] = np.concatenate((preprocessed.get(arr, []), pr_arr), axis=0)\n                if arr == ""episode_starts"":\n                    preprocessed[arr] = preprocessed[arr].astype(bool)\n                else:\n                    preprocessed[arr] = preprocessed[arr].astype(int)\n\n        np.savez(args.merge[2] + ""/preprocessed_data.npz"", ** preprocessed)\n\n        # remove the old folders\n        shutil.rmtree(args.merge[0])\n        shutil.rmtree(args.merge[1])\n\n\nif __name__ == \'__main__\':\n    main()\n'"
environments/dataset_generator.py,0,"b'from __future__ import division, absolute_import, print_function\n\nimport argparse\nimport glob\nimport multiprocessing\nimport os\nimport shutil\nimport time\n\nimport numpy as np\nfrom stable_baselines import PPO2\nfrom stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\nfrom stable_baselines.common.policies import CnnPolicy\n\nfrom environments import ThreadingType\nfrom environments.registry import registered_env\nfrom real_robots.constants import USING_OMNIROBOT\nfrom srl_zoo.utils import printRed, printYellow\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # used to remove debug info of tensorflow\n\n\ndef convertImagePath(args, path, record_id_start):\n    """"""\n    Used to convert an image path, from one location, to another\n    :param args: (ArgumentParser object)\n    :param path: (str)\n    :param record_id_start: (int) where does the current part start counting its records\n    :return:\n    """"""\n    image_name = path.split(""/"")[-1]\n    # get record id for output, by adding the current offset with the record_id\n    # of the folder\n    new_record_id = record_id_start + int(path.split(""/"")[-2].split(""_"")[-1])\n    return args.name + ""/record_{:03d}"".format(new_record_id) + ""/"" + image_name\n\n\ndef env_thread(args, thread_num, partition=True, use_ppo2=False):\n    """"""\n    Run a session of an environment\n    :param args: (ArgumentParser object)\n    :param thread_num: (int) The thread ID of the environment session\n    :param partition: (bool) If the output should be in multiple parts (default=True)\n    :param use_ppo2: (bool) Use ppo2 to generate the dataset\n    """"""\n    env_kwargs = {\n        ""max_distance"": args.max_distance,\n        ""random_target"": args.random_target,\n        ""force_down"": True,\n        ""is_discrete"": not args.continuous_actions,\n        ""renders"": thread_num == 0 and args.display,\n        ""record_data"": not args.no_record_data,\n        ""multi_view"": args.multi_view,\n        ""save_path"": args.save_path,\n        ""shape_reward"": args.shape_reward\n    }\n\n    if partition:\n        env_kwargs[""name""] = args.name + ""_part-"" + str(thread_num)\n    else:\n        env_kwargs[""name""] = args.name\n\n    env_class = registered_env[args.env][0]\n    env = env_class(**env_kwargs)\n    using_real_omnibot = args.env == ""OmnirobotEnv-v0"" and USING_OMNIROBOT\n\n    model = None\n    if use_ppo2:\n        # Additional env when using a trained ppo agent to generate data\n        # instead of a random agent\n        train_env = env_class(**{**env_kwargs, ""record_data"": False, ""renders"": False})\n        train_env = DummyVecEnv([lambda: train_env])\n        train_env = VecNormalize(train_env, norm_obs=True, norm_reward=False)\n\n        model = PPO2(CnnPolicy, train_env).learn(args.ppo2_timesteps)\n\n    frames = 0\n    start_time = time.time()\n    # divide evenly, then do an extra one for only some of them in order to get the right count\n    for i_episode in range(args.num_episode // args.num_cpu + 1 * (args.num_episode % args.num_cpu > thread_num)):\n        # seed + position in this slice + size of slice (with reminder if uneven partitions)\n        seed = args.seed + i_episode + args.num_episode // args.num_cpu * thread_num + \\\n               (thread_num if thread_num <= args.num_episode % args.num_cpu else args.num_episode % args.num_cpu)\n\n        env.seed(seed)\n        env.action_space.seed(seed)  # this is for the sample() function from gym.space\n        obs = env.reset()\n        done = False\n        t = 0\n        episode_toward_target_on = False\n        while not done:\n            env.render()\n\n            if use_ppo2:\n                action, _ = model.predict([obs])\n            else:\n                # Using a target reaching policy (untrained, from camera) when collecting data from real OmniRobot\n                if episode_toward_target_on and np.random.rand() < args.toward_target_timesteps_proportion and \\\n                        using_real_omnibot:\n                    action = [env.actionPolicyTowardTarget()]\n                else:\n                    action = [env.action_space.sample()]\n\n            action_to_step = action[0]\n            _, _, done, _ = env.step(action_to_step)\n\n            frames += 1\n            t += 1\n            if done:\n                if np.random.rand() < args.toward_target_timesteps_proportion and using_real_omnibot:\n                    episode_toward_target_on = True\n                else:\n                    episode_toward_target_on = False\n                print(""Episode finished after {} timesteps"".format(t + 1))\n\n        if thread_num == 0:\n            print(""{:.2f} FPS"".format(frames * args.num_cpu / (time.time() - start_time)))\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Deteministic dataset generator for SRL training \' +\n                                                 \'(can be used for environment testing)\')\n    parser.add_argument(\'--num-cpu\', type=int, default=1, help=\'number of cpu to run on\')\n    parser.add_argument(\'--num-episode\', type=int, default=50, help=\'number of episode to run\')\n    parser.add_argument(\'--save-path\', type=str, default=\'srl_zoo/data/\',\n                        help=\'Folder where the environments will save the output\')\n    parser.add_argument(\'--name\', type=str, default=\'kuka_button\', help=\'Folder name for the output\')\n    parser.add_argument(\'--env\', type=str, default=\'KukaButtonGymEnv-v0\', help=\'The environment wanted\',\n                        choices=list(registered_env.keys()))\n    parser.add_argument(\'--display\', action=\'store_true\', default=False)\n    parser.add_argument(\'--no-record-data\', action=\'store_true\', default=False)\n    parser.add_argument(\'--max-distance\', type=float, default=0.28,\n                        help=\'Beyond this distance from the goal, the agent gets a negative reward\')\n    parser.add_argument(\'-c\', \'--continuous-actions\', action=\'store_true\', default=False)\n    parser.add_argument(\'--seed\', type=int, default=0, help=\'the seed\')\n    parser.add_argument(\'-f\', \'--force\', action=\'store_true\', default=False,\n                        help=\'Force the save, even if it overrides something else,\' +\n                             \' including partial parts if they exist\')\n    parser.add_argument(\'-r\', \'--random-target\', action=\'store_true\', default=False,\n                        help=\'Set the button to a random position\')\n    parser.add_argument(\'--multi-view\', action=\'store_true\', default=False, help=\'Set a second camera to the scene\')\n    parser.add_argument(\'--shape-reward\', action=\'store_true\', default=False,\n                        help=\'Shape the reward (reward = - distance) instead of a sparse reward\')\n    parser.add_argument(\'--reward-dist\', action=\'store_true\', default=False,\n                        help=\'Prints out the reward distribution when the dataset generation is finished\')\n    parser.add_argument(\'--run-ppo2\', action=\'store_true\', default=False,\n                        help=\'runs a ppo2 agent instead of a random agent\')\n    parser.add_argument(\'--ppo2-timesteps\', type=int, default=1000,\n                        help=\'number of timesteps to run PPO2 on before generating the dataset\')\n    parser.add_argument(\'--toward-target-timesteps-proportion\', type=float, default=0.0,\n                        help=""propotion of timesteps that use simply towards target policy, should be 0.0 to 1.0"")\n    args = parser.parse_args()\n\n    assert (args.num_cpu > 0), ""Error: number of cpu must be positive and non zero""\n    assert (args.max_distance > 0), ""Error: max distance must be positive and non zero""\n    assert (args.num_episode > 0), ""Error: number of episodes must be positive and non zero""\n    assert not args.reward_dist or not args.shape_reward, \\\n        ""Error: cannot display the reward distribution for continuous reward""\n    assert not(registered_env[args.env][3] is ThreadingType.NONE and args.num_cpu != 1), \\\n        ""Error: cannot have more than 1 CPU for the environment {}"".format(args.env)\n    if args.num_cpu > args.num_episode:\n        args.num_cpu = args.num_episode\n        printYellow(""num_cpu cannot be greater than num_episode, defaulting to {} cpus."".format(args.num_cpu))\n\n    # this is done so seed 0 and 1 are different and not simply offset of the same datasets.\n    args.seed = np.random.RandomState(args.seed).randint(int(1e10))\n\n    # File exists, need to deal with it\n    if not args.no_record_data and os.path.exists(args.save_path + args.name):\n        assert args.force, ""Error: save directory \'{}\' already exists"".format(args.save_path + args.name)\n\n        shutil.rmtree(args.save_path + args.name)\n        for part in glob.glob(args.save_path + args.name + ""_part-[0-9]*""):\n            shutil.rmtree(part)\n    if not args.no_record_data:\n        # create the output\n        os.mkdir(args.save_path + args.name)\n\n    if args.num_cpu == 1:\n        env_thread(args, 0, partition=False, use_ppo2=args.run_ppo2)\n    else:\n        # try and divide into multiple processes, with an environment each\n        try:\n            jobs = []\n            for i in range(args.num_cpu):\n                process = multiprocessing.Process(target=env_thread, args=(args, i, True, args.run_ppo2))\n                jobs.append(process)\n\n            for j in jobs:\n                j.start()\n\n            try:\n                for j in jobs:\n                    j.join()\n            except Exception as e:\n                printRed(""Error: unable to join thread"")\n                raise e\n\n        except Exception as e:\n            printRed(""Error: unable to start thread"")\n            raise e\n\n    if not args.no_record_data and args.num_cpu > 1:\n        # sleep 1 second, to avoid congruency issues from multiprocess (eg., files still writing)\n        time.sleep(1)\n        # get all the parts\n        file_parts = sorted(glob.glob(args.save_path + args.name + ""_part-[0-9]*""), key=lambda a: int(a.split(""-"")[-1]))\n\n        # move the config files from any as they are identical\n        os.rename(file_parts[0] + ""/dataset_config.json"", args.save_path + args.name + ""/dataset_config.json"")\n        os.rename(file_parts[0] + ""/env_globals.json"", args.save_path + args.name + ""/env_globals.json"")\n\n        ground_truth = None\n        preprocessed_data = None\n\n        # used to convert the part record_id to the fused record_id\n        record_id = 0\n        for part in file_parts:\n            # sort the record names alphabetically, then numerically\n            records = sorted(glob.glob(part + ""/record_[0-9]*""), key=lambda a: int(a.split(""_"")[-1]))\n\n            record_id_start = record_id\n            for record in records:\n                os.renames(record, args.save_path + args.name + ""/record_{:03d}"".format(record_id))\n                record_id += 1\n\n            # fuse the npz files together, in the right order\n            if ground_truth is None:\n                # init\n                ground_truth = {}\n                preprocessed_data = {}\n                ground_truth_load = np.load(part + ""/ground_truth.npz"")\n                preprocessed_data_load = np.load(part + ""/preprocessed_data.npz"")\n\n                for arr in ground_truth_load.files:\n                    if arr == ""images_path"":\n                        ground_truth[arr] = np.array(\n                            [convertImagePath(args, path, record_id_start) for path in ground_truth_load[arr]])\n                    else:\n                        ground_truth[arr] = ground_truth_load[arr]\n                for arr in preprocessed_data_load.files:\n                    preprocessed_data[arr] = preprocessed_data_load[arr]\n\n            else:\n                ground_truth_load = np.load(part + ""/ground_truth.npz"")\n                preprocessed_data_load = np.load(part + ""/preprocessed_data.npz"")\n\n                for arr in ground_truth_load.files:\n                    if arr == ""images_path"":\n                        sanitised_paths = np.array(\n                            [convertImagePath(args, path, record_id_start) for path in ground_truth_load[arr]])\n                        ground_truth[arr] = np.concatenate((ground_truth[arr], sanitised_paths))\n                    else:\n                        ground_truth[arr] = np.concatenate((ground_truth[arr], ground_truth_load[arr]))\n                for arr in preprocessed_data_load.files:\n                    preprocessed_data[arr] = np.concatenate((preprocessed_data[arr], preprocessed_data_load[arr]))\n\n            # remove the current part folder\n            shutil.rmtree(part)\n\n        # save the fused outputs\n        np.savez(args.save_path + args.name + ""/ground_truth.npz"", **ground_truth)\n        np.savez(args.save_path + args.name + ""/preprocessed_data.npz"", **preprocessed_data)\n\n    if args.reward_dist:\n        rewards, counts = np.unique(np.load(args.save_path + args.name + ""/preprocessed_data.npz"")[\'rewards\'],\n                                    return_counts=True)\n        counts = [""{:.2f}%"".format(val * 100) for val in counts / np.sum(counts)]\n        print(""reward distribution:"")\n        [print("" "", reward, count) for reward, count in list(zip(rewards, counts))]\n\n\nif __name__ == \'__main__\':\n    main()\n'"
environments/registry.py,0,"b'import subprocess\n\nimport gym\nfrom gym.envs import registry\n\nfrom environments import PlottingType, ThreadingType\nfrom environments.srl_env import SRLGymEnv\nfrom environments.kuka_gym.kuka_button_gym_env import KukaButtonGymEnv\nfrom environments.kuka_gym.kuka_rand_button_gym_env import KukaRandButtonGymEnv\nfrom environments.kuka_gym.kuka_2button_gym_env import Kuka2ButtonGymEnv\nfrom environments.kuka_gym.kuka_moving_button_gym_env import KukaMovingButtonGymEnv\nfrom environments.mobile_robot.mobile_robot_env import MobileRobotGymEnv\nfrom environments.mobile_robot.mobile_robot_2target_env import MobileRobot2TargetGymEnv\nfrom environments.mobile_robot.mobile_robot_1D_env import MobileRobot1DGymEnv\nfrom environments.mobile_robot.mobile_robot_line_target_env import MobileRobotLineTargetGymEnv\nfrom environments.gym_baxter.baxter_env import BaxterEnv\nfrom environments.robobo_gym.robobo_env import RoboboEnv\nfrom environments.omnirobot_gym.omnirobot_env import OmniRobotEnv\n\ndef register(_id, **kvargs):\n    if _id in registry.env_specs:\n        return\n    else:\n        return gym.envs.registration.register(_id, **kvargs)\n\n\ndef isXAvailable():\n    """"""\n    check to see if running in terminal with X or not\n    :return: (bool)\n    """"""\n    try:\n        p = subprocess.Popen([""xset"", ""-q""], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        p.communicate()\n        return p.returncode == 0\n    except FileNotFoundError:\n        # Return False if xset is not present on the machine\n        return False\n\n\nregistered_env = {\n    ""KukaButtonGymEnv-v0"":            (KukaButtonGymEnv, SRLGymEnv, PlottingType.PLOT_3D, ThreadingType.PROCESS),\n    ""KukaRandButtonGymEnv-v0"":        (KukaRandButtonGymEnv, KukaButtonGymEnv, PlottingType.PLOT_3D, ThreadingType.PROCESS),\n    ""Kuka2ButtonGymEnv-v0"":           (Kuka2ButtonGymEnv, KukaButtonGymEnv, PlottingType.PLOT_3D, ThreadingType.PROCESS),\n    ""KukaMovingButtonGymEnv-v0"":      (KukaMovingButtonGymEnv, KukaButtonGymEnv, PlottingType.PLOT_3D, ThreadingType.PROCESS),\n    ""MobileRobotGymEnv-v0"":           (MobileRobotGymEnv, SRLGymEnv, PlottingType.PLOT_2D, ThreadingType.PROCESS),\n    ""MobileRobot2TargetGymEnv-v0"":    (MobileRobot2TargetGymEnv, MobileRobotGymEnv, PlottingType.PLOT_2D, ThreadingType.PROCESS),\n    ""MobileRobot1DGymEnv-v0"":         (MobileRobot1DGymEnv, MobileRobotGymEnv, PlottingType.PLOT_2D, ThreadingType.PROCESS),\n    ""MobileRobotLineTargetGymEnv-v0"": (MobileRobotLineTargetGymEnv, MobileRobotGymEnv, PlottingType.PLOT_2D, ThreadingType.PROCESS),\n    ""Baxter-v0"":                      (BaxterEnv, SRLGymEnv, PlottingType.PLOT_3D, ThreadingType.NONE),\n    ""RoboboGymEnv-v0"":                (RoboboEnv, SRLGymEnv, PlottingType.PLOT_2D, ThreadingType.NONE),\n    ""OmnirobotEnv-v0"":                (OmniRobotEnv, SRLGymEnv, PlottingType.PLOT_2D, ThreadingType.PROCESS),\n}\n\n# Environments only available when running in a terminal with X (hence only imported when available):\nif isXAvailable():\n    # Catch if X available, but GL context unavailable. \n    # This prevents SSH crashing when X is passed without GL context.\n    try:\n        from environments.car_racing.car_env import CarRacingEnv\n        registered_env[""CarRacingGymEnv-v0""] = (CarRacingEnv, SRLGymEnv, PlottingType.PLOT_2D, ThreadingType.NONE)\n    except:\n        pass\n\n\nfor name, (env_class, _, _, _) in registered_env.items():\n    register(\n        _id=name,\n        entry_point=env_class.__module__ + "":"" + env_class.__name__,\n        timestep_limit=None,  # This limit is changed in the file\n        reward_threshold=None  # Threshold at which the environment is considered as solved\n    )\n'"
environments/srl_env.py,0,"b'import gym\nfrom gym.utils import seeding\n\n\nclass SRLGymEnv(gym.Env):\n    metadata = {\n        \'render.modes\': [\'human\', \'rgb_array\'],\n        \'video.frames_per_second\': 50\n    }\n\n    def __init__(self, *, srl_model, relative_pos, env_rank, srl_pipe):\n        """"""\n        Gym wrapper for SRL environments\n\n        :param srl_model: (str) The SRL_model used\n        :param relative_pos: (bool) position for ground truth\n        :param env_rank: (int) the number ID of the environment\n        :param srl_pipe: (Queue, [Queue]) contains the input and output of the SRL model\n        """"""\n        # the * here, means that the rest of the args need to be called as kwargs.\n        # This is done to avoid unwanted situations where we might add a parameter\n        #  later and not realise that srl_pipe was not set by an unchanged subclass.\n        self.env_rank = env_rank\n        self.srl_pipe = srl_pipe\n        self.srl_model = srl_model\n        self.relative_pos = relative_pos\n        self.np_random = None\n\n        # Create numpy random generator\n        # This seed can be changed later\n        self.seed(0)\n\n    def getSRLState(self, observation):\n        """"""\n        get the SRL state for this environement with a given observation\n        :param observation: (numpy float) image\n        :return: (numpy float)\n        """"""\n        if self.srl_model == ""ground_truth"":\n            if self.relative_pos:\n                return self.getGroundTruth() - self.getTargetPos()\n            return self.getGroundTruth()\n        else:\n            # srl_pipe is a tuple that contains:\n            #  Queue: input to the SRL model, sends origin (where does the message comes from, here the rank of the environment)\n            #  and observation that needs to be transformed into a state\n            #  [Queue]: input for all the envs, sends state associated to the observation\n            self.srl_pipe[0].put((self.env_rank, observation))\n            return self.srl_pipe[1][self.env_rank].get()\n\n    def getTargetPos(self):\n        """"""\n        :return (numpy array): Position of the target (button)\n        """"""\n        raise NotImplementedError()\n\n    @staticmethod\n    def getGroundTruthDim():\n        """"""\n        :return: (int)\n        """"""\n        raise NotImplementedError()\n\n    def getGroundTruth(self):\n        """"""\n        Alias for getArmPos for compatibility between envs\n        :return: (numpy array)\n        """"""\n        raise NotImplementedError()\n\n    def seed(self, seed=None):\n        """"""\n        Seed random generator\n        :param seed: (int)\n        :return: ([int])\n        """"""\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def close(self):\n        # TODO: implement close function to close GUI\n        pass\n\n    def step(self, action):\n        """"""\n        :param action: (int or [float])\n        """"""\n        raise NotImplementedError()\n\n    def reset(self):\n        """"""\n        Reset the environment\n        :return: (numpy tensor) first observation of the env\n        """"""\n        raise NotImplementedError()\n\n    def render(self, mode=\'human\'):\n        """"""\n        :param mode: (str)\n        :return: (numpy array)\n        """"""\n        raise NotImplementedError()\n'"
environments/utils.py,0,"b'# Modified version of https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/envs.py\n\nimport importlib\nimport os\n\nfrom gym.envs.registration import registry, patch_deprecated_methods, load\nfrom stable_baselines import bench\n\n\ndef dynamicEnvLoad(env_id):\n    """"""\n    Get from Gym, the module where the environment is stored\n    :param env_id: (str)\n    :return: (module, str, str) module_env, class_name, env_module_path\n    """"""\n    # Get from the env_id, the entry_point, and distinguish if it is a callable, or a string\n    entry_point = registry.spec(env_id)._entry_point\n    if callable(entry_point):\n        class_name = entry_point.__name__\n        env_module_path = entry_point.__module__\n    else:\n        class_name = entry_point.split(\':\')[1]\n        env_module_path = entry_point.split(\':\')[0]\n    # Lets try and dynamically load the module_env, in order to fetch the globals.\n    # If it fails, it means that it was unable to load the path from the entry_point\n    # should this occure, it will mean that some parameters will not be correctly saved.\n    try:\n        module_env = importlib.import_module(env_module_path)\n    except ImportError:\n        raise AssertionError(""Error: could not import module {}, "".format(env_module_path) +\n                             ""Halting execution. Are you sure this is a valid environement?"")\n\n    return module_env, class_name, env_module_path\n\n\ndef makeEnv(env_id, seed, rank, log_dir, allow_early_resets=False, env_kwargs=None):\n    """"""\n    Instantiate gym env\n    :param env_id: (str)\n    :param seed: (int)\n    :param rank: (int)\n    :param log_dir: (str)\n    :param allow_early_resets: (bool) Allow reset before the enviroment is done\n    :param env_kwargs: (dict) The extra arguments for the environment\n    """"""\n\n    # define a place holder function to be returned to the caller.\n    def _thunk():\n        local_env_kwargs = dict(env_kwargs)  # copy this to avoid altering the others\n        local_env_kwargs[""env_rank""] = rank\n        env = _make(env_id, env_kwargs=local_env_kwargs)\n        env.seed(seed + rank)\n        if log_dir is not None:\n            env = bench.Monitor(env, os.path.join(log_dir, str(rank)), allow_early_resets=allow_early_resets)\n        return env\n\n    return _thunk\n\n\ndef _make(id_, env_kwargs=None):\n    """"""\n    Recreating the gym make function from gym/envs/registration.py\n    as such as it can support extra arguments for the environment\n    :param id_: (str) The environment ID\n    :param env_kwargs: (dict) The extra arguments for the environment\n    """"""\n    if env_kwargs is None:\n        env_kwargs = {}\n\n    # getting the spec from the ID we want\n    spec = registry.spec(id_)\n\n    # Keeping the checks and safe guards of the old code\n    assert spec._entry_point is not None, \'Attempting to make deprecated env {}. \' \\\n                                          \'(HINT: is there a newer registered version of this env?)\'.format(spec.id_)\n\n    if callable(spec._entry_point):\n        env = spec._entry_point(**env_kwargs)\n    else:\n        cls = load(spec._entry_point)\n        # create the env, with the original kwargs, and the new ones overriding them if needed\n        env = cls(**{**spec._kwargs, **env_kwargs})\n\n    # Make the enviroment aware of which spec it came from.\n    env.unwrapped.spec = spec\n\n    # Keeping the old patching system for _reset, _step and timestep limit\n    if hasattr(env, ""_reset"") and hasattr(env, ""_step"") and not getattr(env, ""_gym_disable_underscore_compat"", False):\n        patch_deprecated_methods(env)\n    if (env.spec.timestep_limit is not None) and not spec.tags.get(\'vnc\'):\n        from gym.wrappers.time_limit import TimeLimit\n        env = TimeLimit(env,\n                        max_episode_steps=env.spec.max_episode_steps,\n                        max_episode_seconds=env.spec.max_episode_seconds)\n    return env\n'"
real_robots/__init__.py,0,b''
real_robots/constants.py,0,"b'# coding=utf-8\nfrom __future__ import print_function, absolute_import, division\n\nimport numpy as np\nfrom enum import Enum\n\n# ==== CONSTANTS FOR BAXTER ROBOT ====\n# Socket port\nSERVER_PORT = 7777\nHOSTNAME = \'localhost\'\nUSING_REAL_BAXTER = False\nUSING_ROBOBO = False\nUSING_OMNIROBOT = False\nUSING_OMNIROBOT_SIMULATOR = True\nassert sum([USING_REAL_BAXTER, USING_ROBOBO, USING_OMNIROBOT, USING_OMNIROBOT_SIMULATOR]) <= 1, \\\n    ""You can only use one real robot at a time""\n# For compatibility with teleop_client\nMove = None\nDELTA_POS = 0\n\nZ_TABLE, MAX_DISTANCE = 0, 0\n\n# Calibrated values for Real Baxter\nif USING_REAL_BAXTER:\n    # Initial position of the arm\n    LEFT_ARM_INIT_POS = [0.69850099, 0.14505832, 0.08032852]\n    # Initial orientation\n    LEFT_ARM_ORIENTATION = [0.99893116, -0.04207143, -0.00574656, -0.01826233]\n    # Button position (target)\n    BUTTON_POS = [0.7090276, 0.13833109, -0.11170768]\n    # Distance below which the target is considered to be reached\n    DIST_TO_TARGET_THRESHOLD = 0.035\n    # Max distance between end effector and the button (for negative reward)\n    MAX_DISTANCE = 0.18\n    # Used by the inverse kinematics\n    IK_SEED_POSITIONS = None\n    # Constant distance delta for actions\n    DELTA_POS = 0.02\n    Z_TABLE = - 0.10\n    # Max number of steps per episode\n    MAX_STEPS = 100\n    # ROS Topics\n    IMAGE_TOPIC = ""/kinect2/qhd/image_color""\n    # Set the second cam topic to None if there is only one camera\n    SECOND_CAM_TOPIC = ""/camera/rgb/image_raw""\n    DATA_FOLDER_SECOND_CAM = ""real_baxter_second_cam""\n\nelif USING_ROBOBO:\n\n    # ROS Topics\n    IMAGE_TOPIC = ""/camera/rgb/image_raw""\n    # SECOND_CAM_TOPIC = ""/camera/image_repub""\n    SECOND_CAM_TOPIC = None\n    DATA_FOLDER_SECOND_CAM = ""real_robobo_second_cam""\n    # Max number of steps per episode\n    MAX_STEPS = 20\n    # Initial area in the image of the target\n    # It must be calibrated after changing the target position\n    TARGET_INITIAL_AREA = 3700\n    # HSV thresholds, MUST be calibrated before starting the experiment\n    # using for instance https://github.com/sergionr2/RacingRobot/blob/v0.3/opencv/dev/threshold.py\n    LOWER_RED = np.array([120, 130, 0])\n    UPPER_RED = np.array([135, 255, 255])\n    # Change in percent of the target area to consider\n    # that the target was reached\n    MIN_DELTA_AREA = 0.2  # 20% covered to considered it reached\n    # Boundaries\n    MIN_X, MAX_X = -3, 3\n    MIN_Y, MAX_Y = -4, 3\n\n    # Define the possible Moves\n    class Move(Enum):\n        FORWARD = 0\n        BACKWARD = 1\n        LEFT = 2\n        RIGHT = 3\n        STOP = 4\nelif USING_OMNIROBOT or USING_OMNIROBOT_SIMULATOR:\n\n    # Reward definition\n    REWARD_TARGET_REACH = 1\n    REWARD_NOTHING = 0\n    REWARD_BUMP_WALL = -1\n    # ROS Topics\n    IMAGE_TOPIC = ""/camera/image_raw""\n   \n    SECOND_CAM_TOPIC = None  # not support currently\n   \n    # Max number of steps per episode\n    MAX_STEPS = 250\n    # Boundaries\n    MIN_X, MAX_X = -0.85, 0.85  # center of robot should be in this interval\n    MIN_Y, MAX_Y = -0.85, 0.85\n\n    # inital position\'s boudnaries\n    INIT_MIN_X, INIT_MAX_X = -0.7, 0.7\n    INIT_MIN_Y, INIT_MAX_Y = -0.7, 0.7\n\n    # Target Boundaries\n    TARGET_MIN_X, TARGET_MAX_X = -0.7, 0.7\n    TARGET_MIN_Y, TARGET_MAX_Y = -0.7, 0.7\n\n    # Control frequence when RL is used for controlling velocity or wheelspeeds directly\n    RL_CONTROL_FREQ = 20.0\n\n    # Geometry data of omnirobot\n    OMNIROBOT_L = 0.120  # m\n\n    # error threshold\n    DIST_TO_TARGET_THRESHOLD = 0.2\n\n    # For discrete action, \n    # Define the possible Moves\n    class Move(Enum):\n        FORWARD = 0\n        BACKWARD = 1\n        LEFT = 2\n        RIGHT = 3\n        STOP = 4\n\n    STEP_DISTANCE = 0.1  # meter, distance for each step\n    \n    # For continuous action,\n    # Define the action_bounds\n    ACTION_POSITIVE_LOW = 0.0\n    ACTION_POSITIVE_HIGH = 0.1\n    ACTION_NEGATIVE_LOW = -0.1\n    ACTION_NEGATIVE_HIGH = 0.0\n\n    # camera info files (generated by ROS node calibrate_camera)\n    CAMERA_INFO_PATH = ""real_robots/omnirobot_utils/cam_calib_info.yaml""\n\n    # camera installation info\n    CAMERA_POS_COORD_GROUND = [0, 0, 2.9]  # camera position in ground coordinate [x, y, z]\n    # camera rotation in ground coordinate, euler angle presented in xyz, in degree\n    CAMERA_ROT_EULER_COORD_GROUND = [0, 180, 0]\n    ORIGIN_SIZE = [640, 480]  # camera\'s original resolution\n    CROPPED_SIZE = [480, 480]  # cropped to a square, attention, this is not the output image size (RENDER_SIZE)\n# Gazebo\nelse:\n    LEFT_ARM_INIT_POS = [0.6, 0.30, 0.20]\n    # [\'left_e0\', \'left_e1\', \'left_s0\', \'left_s1\', \'left_w0\', \'left_w1\', \'left_w2\']\n    IK_SEED_POSITIONS = [-1.535, 1.491, -0.038, 0.194, 1.546, 1.497, -0.520]\n    DELTA_POS = 0.05\n    Z_TABLE = -0.14\n    IMAGE_TOPIC = ""/cameras/head_camera_2/image""\n    MAX_STEPS = 100\n    MAX_DISTANCE = 0.35\n\n# Arrow keys for teleoperation\nUP_KEY = 82  # the arrow key ""up""\nDOWN_KEY = 84\nRIGHT_KEY = 83\nLEFT_KEY = 81\nENTER_KEY = 10\nSPACE_KEY = 32\nEXIT_KEYS = [113, 27]  # Escape and q\nD_KEY = 100  # the letter ""d""\nU_KEY = 117  # The letter ""u""\nR_KEY = 114  # the letter ""r""\n'"
real_robots/gazebo_server.py,0,"b'#!/usr/bin/env python2\nfrom __future__ import division, print_function, absolute_import\n\nimport subprocess\n\nimport arm_scenario_simulator as arm_sim\nimport baxter_interface\nimport numpy as np\nimport rospy\nimport zmq\nfrom arm_scenario_experiments import baxter_utils\nfrom arm_scenario_experiments import utils as arm_utils\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom geometry_msgs.msg import Point, Vector3, Vector3Stamped\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import Header\n\nfrom .constants import *\nfrom .utils import sendMatrix, getActions\n\nassert not USING_REAL_BAXTER, ""Please set USING_REAL_BAXTER to False in real_robots/constants.py""\n\nbridge = CvBridge()\n\n\nclass ImageCallback(object):\n    def __init__(self):\n        super(ImageCallback, self).__init__()\n        self.valid_img = None\n\n    def imageCallback(self, msg):\n        try:\n            # Convert your ROS Image message to OpenCV\n            cv2_img = bridge.imgmsg_to_cv2(msg, ""rgb8"")\n            self.valid_img = cv2_img\n        except CvBridgeError as e:\n            print(""CvBridgeError:"", e)\n\n\ndef move_left_arm_to_init():\n    """"""\n    Initialize robot left arm to starting position (hardcoded)\n    :return: ([float])\n    """"""\n    joints = None\n    position = LEFT_ARM_INIT_POS\n    while not joints:\n        try:\n            joints = baxter_utils.IK(left_arm, position, ee_orientation, IK_SEED_POSITIONS)\n        except Exception:\n            try:\n                joints = baxter_utils.IK(left_arm, position, ee_orientation, IK_SEED_POSITIONS)\n            except Exception:\n                raise\n    left_arm.move_to_joint_positions(joints)\n    return position\n\n\nrospy.init_node(\'gym_gazebo_server\', anonymous=True)\n\n# Connect to ROS Topics\nimage_cb_wrapper = ImageCallback()\nimg_sub = rospy.Subscriber(IMAGE_TOPIC, Image, image_cb_wrapper.imageCallback)\n\n# Retrieve the different gazebo objects\nleft_arm = baxter_interface.Limb(\'left\')\nright_arm = baxter_interface.Limb(\'right\')\nee_orientation = baxter_utils.get_ee_orientation(left_arm)\nlever = arm_sim.Lever(\'lever1\')\nbutton = arm_sim.Button(\'button1\')\n\nbutton_pos = button.get_state().pose.position\n\nbaxter = arm_sim.Button(\'baxter\')\nbaxter_pose = baxter.get_state().pose\nbaxter_position = arm_utils.point2array(baxter_pose.position)\nbaxter_orientation = arm_utils.quat2array(baxter_pose.orientation)\n\n# ===== Get list of allowed actions ====\npossible_actions = getActions(DELTA_POS, n_actions=6)\nrospy.sleep(1)\n\nprint(""Initializing robot..."")\n# Init robot pose\nsubprocess.call([""rosrun"", ""arm_scenario_experiments"", ""button_init_pose""])\nprint(""Init Robot pose over"")\nend_point_position = baxter_utils.get_ee_position(left_arm)\n# end_point_position = move_left_arm_to_init()\n\nprint(\'Starting up on port number {}\'.format(SERVER_PORT))\ncontext = zmq.Context()\nsocket = context.socket(zmq.PAIR)\n\nsocket.bind(""tcp://*:{}"".format(SERVER_PORT))\n\n\nprint(""Waiting for client..."")\nsocket.send_json({\'msg\': \'hello\'})\nprint(""Connected to client"")\n\naction = [0, 0, 0]\njoints = None\n\ntry:\n    while True:\n        msg = socket.recv_json()\n        command = msg.get(\'command\', \'\')\n        if command == \'reset\':\n            subprocess.call([""rosrun"", ""arm_scenario_experiments"", ""button_init_pose""])\n            end_point_position = baxter_utils.get_ee_position(left_arm)\n            print(\'Environment reset\')\n            action = [0, 0, 0]\n\n        elif command == \'action\':\n            action = np.array(msg[\'action\'])\n            print(""action:"", action)\n\n        elif command == ""exit"":\n            break\n        else:\n            raise ValueError(""Unknown command: {}"".format(msg))\n\n        # action = randomAction(possible_actions)\n        end_point_position_candidate = end_point_position + action\n\n        print(""End-effector Position:"", end_point_position_candidate)\n        joints = None\n        try:\n            joints = baxter_utils.IK(left_arm, end_point_position_candidate, ee_orientation)\n        except Exception as e:\n            print(""[ERROR] no joints position returned by the Inverse Kinematic fn"")\n            print(""end_point_position_candidate:{}"".format(end_point_position_candidate))\n            print(e)\n\n        if joints:\n            end_point_position = end_point_position_candidate\n            left_arm.move_to_joint_positions(joints, timeout=3)\n        else:\n            print(""No joints position, returning previous one"")\n\n        # Get current position of the button\n        button_pos = button.get_state().pose.position\n        button_pos_absolute = arm_utils.point2array(button_pos)\n        # Button position relative to baxter\n        button_pos_relative = arm_utils.change_CS(button_pos_absolute, baxter_position, baxter_orientation)\n\n        # Send arm position, button position, ...\n        socket.send_json(\n            {\n                # XYZ position\n                ""position"": list(end_point_position),\n                ""reward"": int(button.is_pressed()),\n                ""button_pos"": list(button_pos_relative)\n            },\n            flags=zmq.SNDMORE\n        )\n\n        img = image_cb_wrapper.valid_img\n        # to contiguous, otherwise ZMQ will complain\n        img = np.ascontiguousarray(img, dtype=np.uint8)\n        sendMatrix(socket, img)\nexcept KeyboardInterrupt:\n    pass\n\n\n# TODO:  avoid socket pid running and \'Address already in use\' error relaunching, this is not enough\nprint("" Exiting server - closing socket..."")\nsocket.close()\n'"
real_robots/omnirobot_server.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import division, print_function, absolute_import\nimport rospy\n\nimport os\nimport time\n\n\nimport numpy as np\nimport zmq\nimport argparse\nimport yaml\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Vector3, PoseStamped, Twist\nfrom std_msgs.msg import Bool\nfrom omnirobot_msgs.msg import WheelSpeeds\n\n\nfrom tf.transformations import euler_from_quaternion, quaternion_from_euler\n\nfrom real_robots.constants import *\nfrom real_robots.utils import sendMatrix\nfrom real_robots.omnirobot_utils.omnirobot_manager_base import OmnirobotManagerBase\nassert USING_OMNIROBOT, ""Please set USING_OMNIROBOT to True in real_robots/constants.py""\n\nNO_TARGET_MODE = False\n\nbridge = CvBridge()\nshould_exit = [False]\n\n\nclass OmniRobot(object):\n    def __init__(self, init_x, init_y, init_yaw):\n        """"""\n        Class for controlling omnirobot based on topic mechanism of ros\n        """"""\n\n        super(OmniRobot, self).__init__()\n\n        # Initialize the direction\n        self.init_pos = [init_x, init_y]\n        self.init_yaw = init_yaw\n\n        # OmniRobot\'s real position on the grid\n        self.robot_pos = [0, 0]\n        self.robot_yaw = 0  # in rad\n\n        # OmniRobot\'s position command on the grid\n        self.robot_pos_cmd = self.init_pos[:]\n        self.robot_yaw_cmd = self.init_yaw\n\n        # Target\'s real position on the grid\n        self.target_pos = [0, 0]\n        self.target_yaw = 0\n\n        # status of moving\n        self.move_finished = False\n        self.target_pos_changed = False\n\n        # Distance for each step\n        self.step_distance = STEP_DISTANCE\n\n        self.visual_robot_sub = rospy.Subscriber(\n            ""/visual_robot_pose"", PoseStamped, self.visualRobotCallback, queue_size=10)\n        self.visual_target_sub = rospy.Subscriber(\n            ""/visual_target_pose"", PoseStamped, self.visualTargetCallback, queue_size=10)\n\n        self.pos_cmd_pub = rospy.Publisher(\n            ""/position_commands"", Vector3, queue_size=10)\n        self.move_finished_sub = rospy.Subscriber(\n            ""/finished"", Bool, self.moveFinishedCallback, queue_size=10)\n        self.stop_signal_pub = rospy.Publisher(""/stop"", Bool, queue_size=10)\n        self.reset_odom_pub = rospy.Publisher(\n            ""/reset_odom"", Vector3, queue_size=10)\n        self.reset_signal_pub = rospy.Publisher(""/reset"", Bool, queue_size=10)\n        self.switch_velocity_controller_pub = rospy.Publisher(""/switch/velocity_controller"", Bool, queue_size=10)\n        self.switch_pos_controller_pub = rospy.Publisher(""/switch/pos_controller"", Bool, queue_size=10)\n\n        self.wheel_speeds_command_pub = rospy.Publisher(""/wheel_speeds_commands"", WheelSpeeds, queue_size=10)\n        self.velocity_command_pub = rospy.Publisher(""/velocity_commands"", Twist,  queue_size=10)\n        # known issues, without sleep 1 second, publishers could not been setup\n        rospy.sleep(1)\n        # https://answers.ros.org/question/9665/test-for-when-a-rospy-publisher-become-available/?answer=14125#post-id-14125\n\n        # enable pos_controller, velocity_controller by default\n        self.enabled_pos_controller = True\n        self.enabled_velocity_controller = True\n        self.switch_pos_controller_pub.publish(Bool(True))\n        self.switch_velocity_controller_pub.publish(Bool(True))\n\n    def setRobotCmdConstrained(self, x, y, yaw):\n        """"""\n        set the position command for the robot, the command will be automatically constrained\n        x, y, yaw are in the global frame\n        Note: the command will be not published until pubPosCmd() is called\n        """"""\n        self.robot_pos_cmd[0] = max(x, MIN_X)\n        self.robot_pos_cmd[0] = min(x, MAX_X)\n\n        self.robot_pos_cmd[1] = max(y, MIN_Y)\n        self.robot_pos_cmd[1] = min(y, MAX_Y)\n        self.robot_yaw_cmd = self.normalizeAngle(yaw)\n\n    def setRobotCmd(self, x, y, yaw):\n        """"""\n        set the position command for the robot\n        x, y, yaw are in the global frame\n        Note: the command will be not published until pubPosCmd() is called\n        """"""\n        self.robot_pos_cmd[0] = x\n        self.robot_pos_cmd[1] = y\n        self.robot_yaw_cmd = self.normalizeAngle(yaw)\n\n    def pubPosCmd(self):\n        """"""\n        Publish the position command for the robot\n        x, y, yaw are in the global frame\n        """"""\n        assert self.enabled_pos_controller and self.enabled_velocity_controller, \\\n            ""pos_controller and velocity_controller should be both enabled to execute positional command""\n        msg = Vector3(\n            self.robot_pos_cmd[0], self.robot_pos_cmd[1], self.robot_yaw_cmd)\n        self.pos_cmd_pub.publish(msg)\n        self.move_finished = False\n        print(""move to x: {:.4f} y:{:.4f} yaw: {:.4f}"".format(\n            msg.x, msg.y, msg.z))\n\n    def resetOdom(self, x, y, yaw):\n        """"""\n        The odometry of robot will be reset to x, y, yaw (in global frame)\n        """"""\n        msg = Vector3()\n        msg.x = x\n        msg.y = y\n        msg.z = yaw\n        self.reset_odom_pub.publish(msg)\n\n    def reset(self):\n        """"""\n        Publish the reset signal to robot (quit the stop state)\n        The odometry will not be reset automatically\n        """"""\n        msg = Bool()\n        self.reset_signal_pub.publish(msg)\n\n    def stop(self):\n        """"""\n        Publish the stop signal to robot\n        """"""\n        msg = Bool()\n        msg.data = True\n        self.stop_signal_pub.publish(msg)\n        self.move_finished = True\n\n    def visualRobotCallback(self, pose_stamped_msg):\n        """"""\n        Callback for ROS topic\n        Get the new updated position of robot from camera\n        :param pose_stamped_msg: (PoseStamped ROS message)\n        """"""\n        self.robot_pos[0] = pose_stamped_msg.pose.position.x\n        self.robot_pos[1] = pose_stamped_msg.pose.position.y\n        self.robot_yaw = euler_from_quaternion([pose_stamped_msg.pose.orientation.x, pose_stamped_msg.pose.orientation.y,\n                                                pose_stamped_msg.pose.orientation.z, pose_stamped_msg.pose.orientation.w])[2]\n\n        if NO_TARGET_MODE and self.target_pos_changed:\n            # simulate the target\'s position update\n            self.target_pos[0] = 0.0\n            self.target_pos[1] = 0.0\n            self.target_yaw = 0.0\n            self.target_pos_changed = False\n\n    def visualTargetCallback(self, pose_stamped_msg):\n        """"""\n        Callback for ROS topic\n        Get the new updated position of robot from camera\n        Only update when target position should have been moved (eg. reset env)\n        :param pose_stamped_msg: (PoseStamped ROS message)\n        """"""\n\n        if self.target_pos_changed:\n            if pose_stamped_msg.pose.position.x < TARGET_MAX_X and pose_stamped_msg.pose.position.x > TARGET_MIN_X  \\\n                    and pose_stamped_msg.pose.position.y > TARGET_MIN_Y and pose_stamped_msg.pose.position.y < TARGET_MAX_Y:\n                self.target_pos[0] = pose_stamped_msg.pose.position.x\n                self.target_pos[1] = pose_stamped_msg.pose.position.y\n                self.target_yaw = euler_from_quaternion([pose_stamped_msg.pose.orientation.x, pose_stamped_msg.pose.orientation.y,\n                                                         pose_stamped_msg.pose.orientation.z, pose_stamped_msg.pose.orientation.w])[2]\n                self.target_pos_changed = False\n\n    def moveFinishedCallback(self, move_finished_msg):\n        """"""\n        Callback for ROS topic\n        receive \'finished\' signal when robot moves to the target\n        """"""\n        self.move_finished = move_finished_msg.data\n\n    def forward(self):\n        """"""\n        Move one step forward (Translation)\n        """"""\n        self.setRobotCmd(\n            self.robot_pos_cmd[0] + self.step_distance, self.robot_pos_cmd[1], self.robot_yaw_cmd)\n        self.pubPosCmd()\n\n    def backward(self):\n        """"""\n        Move one step backward\n        """"""\n        self.setRobotCmd(\n            self.robot_pos_cmd[0] - self.step_distance, self.robot_pos_cmd[1], self.robot_yaw_cmd)\n        self.pubPosCmd()\n\n    def left(self):\n        """"""\n        Translate to the left\n        """"""\n        self.setRobotCmd(\n            self.robot_pos_cmd[0], self.robot_pos_cmd[1] + self.step_distance, self.robot_yaw_cmd)\n        self.pubPosCmd()\n\n    def right(self):\n        """"""\n        Translate to the right\n        """"""\n        self.setRobotCmd(\n            self.robot_pos_cmd[0], self.robot_pos_cmd[1] - self.step_distance, self.robot_yaw_cmd)\n        self.pubPosCmd()\n\n    def moveContinous(self, action):\n        """"""\n        Perform a continuous displacement of dx, dy\n        """"""\n        self.setRobotCmd(\n            self.robot_pos_cmd[0] + action[0], self.robot_pos_cmd[1] + action[1], self.robot_yaw_cmd)\n        self.pubPosCmd()\n\n    def disableVelocityController(self):\n        """"""\n        Disable the velocity controller, this server send the wheel_speed_command instead\n        """"""\n        msg = Bool(False)\n        self.switch_velocity_controller_pub.publish(msg)\n        self.enabled_velocity_controller = False\n\n    def enableVelocityController(self):\n        """"""\n        enable the velocity controller\n        """"""\n        msg = Bool(True)\n        self.switch_velocity_controller_pub.publish(msg)\n        self.enabled_velocity_controller = True\n    \n    def disablePosController(self):\n        """"""\n        Disable the pos controller\n        """"""\n        msg = Bool(False)\n        self.switch_pos_controller_pub.publish(msg)\n        self.enabled_pos_controller = False\n\n    def enablePosController(self):\n        """"""\n        enable the pos controller\n        """"""\n        msg = Bool(True)\n        self.switch_pos_controller_pub.publish(msg)\n        self.enabled_pos_controller = True\n\n    def moveByWheelsCmd(self, left_speed, front_speed, right_speed):\n        """"""\n        Send wheel speeds command to robot directly\n        Attention: to use this function, you should firstly \n                   make sure pos_controller and velocity_controller are disabled. \n        A single wheel speed commands will only be executed within 1 second maximum,  \n        after 1 second, if no new command come in, the robot will brake, thus make \n        sure this function called by a frequency more than 1Hz.\n        In contrary, if new wheel speed commands arrives within 1 second, the robot\n        will execute new command immediately. \n        \n        :param left_speed: (float) linear speed of left wheel (meter/s)\n        :param front_speed: (float) linear speed of front wheel (meter/s)\n        :param right_speed: (float) linear speed of right wheel (meter/s)\n        """"""\n        assert self.enabled_pos_controller == False and self.enabled_velocity_controller == False,\\\n            ""you should disable pos_controller and velocity controller before controlling wheel speed directly""\n        wheel_speed_msg = WheelSpeeds()\n        wheel_speed_msg.stamp = rospy.now()\n        wheel_speed_msg.left = left_speed\n        wheel_speed_msg.front = front_speed\n        wheel_speed_msg.right = right_speed\n        self.wheel_speeds_command_pub.publish(wheel_speed_msg)\n\n    def moveByVelocityCmd(self, speed_x, speed_y, speed_yaw):\n        """"""\n        Send velocity command to robot directly, the velocity should be presented in robot local frame.\n        positive x: front, positive y: left\n        Attention: to use this function, you should firstly \n                   make sure pos_controller is disabled. \n        A single velocity commands will only be executed within 1 second maximum, \n        after 1 second, if no new command come in, the robot will brake, thus make \n        sure this function called by a frequency more than 1Hz.\n        In contrary, if new velocity commands arrives within 1 second, the robot \n        will execute new command immediately. \n        \n        :param speed_x: (float) linear speed along x-axis (m/s) (forward-backward), in robot local coordinate\n        :param speed_y: (float) linear speed along y-axis (m/s) (left-right), in robot local coordinate\n        :param speed_yaw: (float) rotation speed of robot around z-axis (rad/s), in robot local coordinate\n        """"""\n        assert self.enabled_pos_controller == False and self.enabled_velocity_controller == True, \\\n            ""you should disable pos_controller but enable velocity controller before controlling robot velocity directly""\n        velocity_command_msg = Twist()\n        velocity_command_msg.linear.x = speed_x\n        velocity_command_msg.linear.y = speed_y\n        velocity_command_msg.linear.z = 0\n        velocity_command_msg.angular.x = 0\n        velocity_command_msg.angular.y = 0\n        velocity_command_msg.angular.z = speed_yaw\n        self.velocity_command_pub.publish(velocity_command_msg)\n        \n\n    @staticmethod\n    def normalizeAngle(angle):\n        """"""\n        :param angle: (float) (in rad)\n        :return: (float) the angle in [-pi, pi] (in rad)\n        """"""\n        while angle > np.pi:\n            angle -= 2 * np.pi\n        while angle < -np.pi:\n            angle += 2 * np.pi\n        return angle\n\n\nclass ImageCallback(object):\n    """"""\n    Image callback for ROS\n    """"""\n\n    def __init__(self, camera_matrix, distortion_coefficients):\n        super(ImageCallback, self).__init__()\n        self.valid_img = None\n        self.valid_box = None\n        self.first_msg = True\n        self.camera_matrix = camera_matrix\n        self.distortion_coefficients = distortion_coefficients\n\n    def imageCallback(self, msg):\n        try:    \n            # Convert your ROS Image message to OpenCV\n            cv2_img = bridge.imgmsg_to_cv2(msg, ""rgb8"")\n            \n            if self.first_msg:\n                shape = cv2_img.shape\n                min_length = min(shape[0], shape[1])\n                up_margin = int((shape[0] - min_length) / 2)  # row\n                left_margin = int((shape[1] - min_length) / 2)  # col\n                self.valid_box = [up_margin, up_margin + min_length, left_margin, left_margin + min_length]\n                print(""origin size: {}x{}"".format(shape[0],shape[1]))\n                print(""crop each image to a square image, cropped size: {}x{}"".format(min_length, min_length))\n                self.first_msg = False\n            \n            undistort_image = cv2.undistort(cv2_img, self.camera_matrix, self.distortion_coefficients)\n            self.valid_img = undistort_image[self.valid_box[0]:self.valid_box[1], self.valid_box[2]:self.valid_box[3]]\n\n        except CvBridgeError as e:\n            print(""CvBridgeError:"", e)\n\ndef saveSecondCamImage(im, episode_folder, episode_step, path=""omnirobot_2nd_cam""):\n    """"""\n    Write an image to disk\n    :param im: (numpy matrix) BGR image\n    :param episode_folder: (str)\n    :param episode_step: (int)\n    :param path: (str)\n    """"""\n    image_path = ""{}/{}/frame{:06d}.jpg"".format(\n        path, episode_folder, episode_step)\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    cv2.imwrite(""srl_zoo/data/{}"".format(image_path), im)\n\n\ndef waitTargetUpdate(omni_robot, timeout):\n    """"""\n    Wait for target updating\n    :param omni_robot: (OmniRobot)\n    :param timeout: (time to wait for updating)\n    """"""\n    omni_robot.target_pos_changed = True\n    time = 0.0  # second\n    while time < timeout and not rospy.is_shutdown():\n        if not omni_robot.target_pos_changed:  # updated\n            return True\n        else:\n            rospy.sleep(0.1)\n            time += 0.1\n    return False\n\nclass OmnirobotManager(OmnirobotManagerBase):\n    """"""\n    Omnirobot magager for real robot\n    """"""\n    def __init__(self):\n        super(OmnirobotManager, self).__init__(second_cam_topic=SECOND_CAM_TOPIC)\n        self.robot = OmniRobot(0,0,0) # assign the real robot object to manager\n        self.episode_idx = 0\n        self.episode_step = 0\n\n    def resetEpisode(self):\n        """"""\n        override orignal method\n        Give the correct sequance of commands to the robot \n        to rest environment between the different episodes\n        """"""\n        if self.second_cam_topic is not None:\n            assert NotImplementedError\n            episode_folder = ""record_{:03d}"".format(episode_idx)\n            try:\n                os.makedirs(\n                    ""srl_zoo/data/{}/{}"".format(DATA_FOLDER_SECOND_CAM, episode_folder))\n            except OSError:\n                pass\n\n        print(\'Environment reset, choose random position\')\n        self.episode_idx += 1\n        self.episode_step = 0\n        self.robot.reset()\n\n        # Env reset\n        random_init_position = self.sampleRobotInitalPosition()\n        self.robot.setRobotCmd(random_init_position[0], random_init_position[1], 0)\n        self.robot.pubPosCmd()\n\n        while True:  # check the new target can be seen\n            if not NO_TARGET_MODE:\n                raw_input(\n                    ""please set the target position, then press \'enter\' !"")\n\n            if waitTargetUpdate(self.robot, timeout=0.5):\n                break\n            else:\n                print(""Can\'t see the target, please move it into the boundary!"")\n\n\nif __name__ == \'__main__\':\n    with open(CAMERA_INFO_PATH, \'r\') as stream:\n        try:\n            contents = yaml.load(stream)\n            camera_matrix = np.array(contents[\'camera_matrix\'][\'data\']).reshape((3,3))\n            distortion_coefficients = np.array(\n                contents[\'distortion_coefficients\'][\'data\']).reshape((1, 5))\n        except yaml.YAMLError as exc:\n            print(exc)\n    rospy.init_node(\'omni_robot_server\', anonymous=True)\n    # warning for no target mode\n    if NO_TARGET_MODE:\n        rospy.logwarn(\n            ""ATTENTION: This script is running under NO TARGET mode!!!"")\n    # Connect to ROS Topics\n    if IMAGE_TOPIC is not None:\n        image_cb_wrapper = ImageCallback(camera_matrix, distortion_coefficients)\n        img_sub = rospy.Subscriber(\n            IMAGE_TOPIC, Image, image_cb_wrapper.imageCallback, queue_size=1)\n\n    if SECOND_CAM_TOPIC is not None:\n        assert NotImplementedError\n        image_cb_wrapper_2 = ImageCallback(camera_matrix, distortion_coefficients)\n        img_2_sub = rospy.Subscriber(\n            SECOND_CAM_TOPIC, Image, image_cb_wrapper_2.imageCallback, queue_size=1)\n\n    print(\'Starting up on port number {}\'.format(SERVER_PORT))\n    context = zmq.Context()\n    socket = context.socket(zmq.PAIR)\n\n    socket.bind(""tcp://*:{}"".format(SERVER_PORT))\n\n    print(""Waiting for client..."")\n    socket.send_json({\'msg\': \'hello\'})\n    print(""Connected to client"")\n\n    action = 0\n    episode_step = 0\n    episode_idx = -1\n    episode_folder = None\n\n    omnirobot_manager = OmnirobotManager()\n    omnirobot_manager.robot = OmniRobot(0, 0, 0)  # yaw is in rad\n    omnirobot_manager.robot.stop()  # after stop, the robot need to be reset\n    omnirobot_manager.robot.resetOdom(0, 0, 0)\n    omnirobot_manager.robot.reset()\n\n    omnirobot_manager.robot.pubPosCmd()\n    r = rospy.Rate(RL_CONTROL_FREQ) \n    while not rospy.is_shutdown():\n        print(""wait for new command"")\n        msg = socket.recv_json()\n\n        print(""msg: {}"".format(msg))\n        omnirobot_manager.processMsg(msg)\n\n        # wait for robot to finish the action, timeout 30s\n        timeout = 30  # second\n        for i in range(timeout):\n            readable_list, _, _ = zmq.select([socket], [], [], 0)\n\n            if len(readable_list) > 0:\n                print(""New command incomes, ignore the current command"")\n                continue\n            if omnirobot_manager.robot.move_finished:\n                print(""action done"")\n                break\n\n            elif i == timeout - 1:\n                print(""Error: timeout for action finished signal"")\n                exit()\n            time.sleep(1)\n\n        if IMAGE_TOPIC is not None:\n            # Retrieve last image from image topic\n            original_image = np.copy(image_cb_wrapper.valid_img)\n\n        print(""reward: {}"".format(omnirobot_manager.reward))\n        print(""omni_robot position"", omnirobot_manager.robot.robot_pos)\n        print(""target position"", omnirobot_manager.robot.target_pos)\n        socket.send_json(\n            {\n                # XYZ position\n                ""position"": omnirobot_manager.robot.robot_pos,\n                ""reward"": omnirobot_manager.reward,\n                ""target_pos"": omnirobot_manager.robot.target_pos\n            },\n            flags=zmq.SNDMORE if IMAGE_TOPIC is not None else 0\n        )\n\n        if SECOND_CAM_TOPIC is not None:\n            saveSecondCamImage(image_cb_wrapper_2.valid_img,\n                               episode_folder, episode_step, DATA_FOLDER_SECOND_CAM)\n            episode_step += 1\n\n        if IMAGE_TOPIC is not None:\n            # to contiguous, otherwise ZMQ will complain\n            img = np.ascontiguousarray(original_image, dtype=np.uint8)\n            sendMatrix(socket, img)\n        r.sleep()\n\n    print(""Exiting server - closing socket..."")\n    socket.close()\n'"
real_robots/omnirobot_simulator_server.py,0,"b'from __future__ import division, print_function, absolute_import\n\nfrom multiprocessing import Process, Pipe\nimport yaml\nimport cv2\n# Konwn issue: - No module named \'scipy.spatial.transform\', To resolve, try pip3 install scipy==1.2\nfrom scipy.spatial.transform import Rotation as R\n\nfrom real_robots.constants import *\nfrom real_robots.omnirobot_utils.marker_finder import MakerFinder\nfrom real_robots.omnirobot_utils.marker_render import MarkerRender\nfrom real_robots.omnirobot_utils.omnirobot_manager_base import OmnirobotManagerBase\nfrom real_robots.omnirobot_utils.utils import PosTransformer\n\nassert USING_OMNIROBOT_SIMULATOR, ""Please set USING_OMNIROBOT_SIMULATOR to True in real_robots/constants.py""\nNOISE_VAR_ROBOT_POS = 0.01  # meter\nNOISE_VAR_ROBOT_YAW = np.pi/180 * 2.5  # 5 Deg\nNOISE_VAR_TARGET_PIXEL = 2  # pixel noise on target marker\nNOISE_VAR_ROBOT_PIXEL = 2\nNOISE_VAR_ENVIRONMENT = 0.03  # pixel noise of final image on LAB space\nNOISE_VAR_ROBOT_SIZE_PROPOTION = 0.05  # noise of robot size propotion\nNOISE_VAR_TARGET_SIZE_PROPOTION = 0.05\n\n\nclass OmniRobotEnvRender():\n    def __init__(self, init_x, init_y, init_yaw, origin_size, cropped_size,\n                 back_ground_path, camera_info_path,\n                 robot_marker_path, robot_marker_margin, target_marker_path, target_marker_margin,\n                 robot_marker_code, target_marker_code,\n                 robot_marker_length, target_marker_length, output_size, **_):\n        """"""\n        Class for rendering Omnirobot environment\n        :param init_x: (float) initial x position of robot\n        :param init_y: (float) initial y position of robot\n        :param init_yaw: (float) initial yaw position of robot\n        :param origin_size: (list of int) original camera\'s size (eg. [640,480]), the camera matrix should be corresponding to this size\n        :param cropped_size: (list of int) cropped image\'s size (eg. [480,480])\n        :param back_ground_path: (str) back ground image\'s path, the image should be undistorted.\n        :param camera_info_path: (str) camera info file\'s path (containing camera matrix)\n        :param robot_marker_path: (str) robot maker\'s path, the marker should have a margin with several pixels \n        :param robot_marker_margin: (list of int) marker\'s margin (eg. [3,3,3,3])\n        :param target_marker_path: (str) target maker\'s path, the marker should have a margin with several pixels \n        :param target_marker_margin: (list of int) marker\'s margin (eg. [3,3,3,3])\n        :param robot_marker_code: (currently not supported, should be ""None"" by default) (numpy ndarray) optional, the code of robot marker, only used for detecting position directly from the image.\n        :param target_marker_code: (currently not supported, should be ""None"" by default) (numpy ndarray) optional, the code of target marker, only used for detecting position directly from the image.\n        :param robot_marker_length: (float) the physical length of the marker (in meter)\n        :param target_marker_length: (float) the physical length of the marker (in meter)\n        :param output_size: (list of int) the output image\'s size (eg. [224,224])\n        :param **_: other input params not used, so they are dropped\n        """"""\n        super(OmniRobotEnvRender, self).__init__()\n\n        self.output_size = output_size\n\n        # store the size of robot marker\n        self.robot_marker_size_proprotion = 1.0\n\n        # Initialize the direction\n        self.init_pos = [init_x, init_y]\n        self.init_yaw = init_yaw\n\n        # OmniRobot\'s real position on the grid\n        self.robot_pos = np.float32([0, 0])\n        self.robot_yaw = 0  # in rad\n\n        # Last velocity command, used for simulating the controlling of velocity directly\n        self.last_linear_velocity_cmd = np.float32(\n            [0, 0])  # in m/s, in robot local frame\n        self.last_rot_velocity_cmd = 0  # in rad/s\n\n        # last wheel speeds command, used for simulating the controlling of wheel speed directly\n        # [left_speed, front_speed, right_speed]\n        self.last_wheel_speeds_cmd = np.float32([0, 0, 0])\n\n        # OmniRobot\'s position command on the grid\n        self.robot_pos_cmd = np.float32(self.init_pos[:])\n        self.robot_yaw_cmd = self.init_yaw\n\n        # Target\'s set position on the grid\n        self.target_pos_cmd = np.float32([0, 0])\n        self.target_yaw_cmd = 0.0\n\n        # Target\'s real position on the grid\n        self.target_pos = np.float32([0, 0])\n        self.target_yaw = 0\n\n        # status of moving\n        self.move_finished = False\n        self.target_pos_changed = False\n\n        # Distance for each step\n        self.step_distance = STEP_DISTANCE\n\n        with open(camera_info_path, \'r\') as stream:\n            try:\n                contents = yaml.load(stream)\n                camera_matrix = np.array(contents[\'camera_matrix\'][\'data\'])\n                self.origin_size = np.array(\n                    [contents[\'image_height\'], contents[\'image_width\']])\n                self.camera_matrix = np.reshape(camera_matrix, (3, 3))\n                self.dist_coeffs = np.array(\n                    contents[""distortion_coefficients""][""data""]).reshape((1, 5))\n            except yaml.YAMLError as exc:\n                print(exc)\n        self.cropped_size = [np.min(self.origin_size), np.min(\n            self.origin_size)]  # size after being cropped\n\n        # restore the image before being cropped\n        self.bg_img = np.zeros([*self.origin_size, 3], np.uint8)\n\n        self.cropped_margin = (self.origin_size - self.cropped_size)/2.0\n        self.cropped_range = np.array([self.cropped_margin[0], self.cropped_margin[0]+self.cropped_size[0],\n                                       self.cropped_margin[1],\n                                       self.cropped_margin[1]+self.cropped_size[1]]).astype(np.int)\n\n        back_ground_img = cv2.imread(back_ground_path)\n        if(back_ground_img.shape[0:2] != self.cropped_size):\n            print(""input back ground image\'s size: "", back_ground_img.shape)\n            print(""resize to "", self.cropped_size)\n            self.bg_img[self.cropped_range[0]:self.cropped_range[1], self.cropped_range[2]:self.cropped_range[3], :] \\\n                = cv2.resize(back_ground_img, tuple(self.cropped_size))  # background image\n        else:\n            self.bg_img[self.cropped_range[0]:self.cropped_range[1], self.cropped_range[2]:self.cropped_range[3], :] \\\n                = back_ground_img  # background image\n\n        self.bg_img = cv2.undistort(\n            self.bg_img, self.camera_matrix, self.dist_coeffs)\n        # Currently cannot find a solution to re-distort a image...\n\n        self.target_bg_img = self.bg_img  # background image with target.\n        self.image = self.bg_img  # image with robot and target\n\n        # camera installation info\n        r = R.from_euler(\'xyz\', CAMERA_ROT_EULER_COORD_GROUND, degrees=True)\n        camera_rot_mat_coord_ground = r.as_dcm()\n\n        self.pos_transformer = PosTransformer(self.camera_matrix, self.dist_coeffs,\n                                              CAMERA_POS_COORD_GROUND, camera_rot_mat_coord_ground)\n\n        self.target_render = MarkerRender(noise_var=NOISE_VAR_TARGET_PIXEL)\n        self.robot_render = MarkerRender(noise_var=NOISE_VAR_ROBOT_PIXEL)\n        self.robot_render.setMarkerImage(cv2.imread(\n            robot_marker_path, cv2.IMREAD_COLOR), robot_marker_margin)\n        self.target_render.setMarkerImage(cv2.imread(\n            target_marker_path, cv2.IMREAD_COLOR), target_marker_margin)\n\n        if robot_marker_code is not None and target_marker_code is not None:\n            self.marker_finder = MakerFinder(camera_info_path)\n            self.marker_finder.setMarkerCode(\n                \'robot\', robot_marker_code, robot_marker_length)\n            self.marker_finder.setMarkerCode(\n                \'target\', target_marker_code, target_marker_length)\n\n    def renderEnvLuminosityNoise(self, origin_image, noise_var=0.1, in_RGB=False, out_RGB=False):\n        """"""\n        render the different environment luminosity\n        """"""\n        # variate luminosity and color\n        origin_image_LAB = cv2.cvtColor(\n            origin_image, cv2.COLOR_RGB2LAB if in_RGB else cv2.COLOR_BGR2LAB, cv2.CV_32F)\n        origin_image_LAB[:, :, 0] = origin_image_LAB[:,\n                                                     :, 0] * (np.random.randn() * noise_var + 1.0)\n        origin_image_LAB[:, :, 1] = origin_image_LAB[:,\n                                                     :, 1] * (np.random.randn() * noise_var + 1.0)\n        origin_image_LAB[:, :, 2] = origin_image_LAB[:,\n                                                     :, 2] * (np.random.randn() * noise_var + 1.0)\n        out_image = cv2.cvtColor(\n            origin_image_LAB, cv2.COLOR_LAB2RGB if out_RGB else cv2.COLOR_LAB2BGR, cv2.CV_8UC3)\n        return out_image\n\n    def renderTarget(self):\n        """"""\n        render the target\n        """"""\n        self.target_bg_img = self.target_render.addMarker(self.bg_img,\n                                                          self.pos_transformer.phyPosGround2PixelPos(\n                                                              self.target_pos.reshape(2, 1)),\n                                                          self.target_yaw, np.random.randn() * NOISE_VAR_TARGET_SIZE_PROPOTION + 1.0)\n\n    def renderRobot(self):\n        """"""\n        render the image.\n        """"""\n        self.image = self.robot_render.addMarker(self.target_bg_img,\n                                                 self.pos_transformer.phyPosGround2PixelPos(\n                                                     self.robot_pos.reshape(2, 1)),\n                                                 self.robot_yaw, self.robot_marker_size_proprotion)\n\n    def getCroppedImage(self):\n        return self.image[self.cropped_range[0]:self.cropped_range[1], self.cropped_range[2]:self.cropped_range[3], :]\n\n    def findMarkers(self):\n        assert NotImplementedError\n        # this is not tested\n        tags_trans_coord_cam, tags_rot_coord_cam = self.marker_finder.getMarkerPose(\n            self.image, [\'robot\', \'target\'], True)\n        if \'robot\' in tags_trans_coord_cam.keys():\n            self.robot_pos = self.pos_transformer.phyPosCam2PhyPosGround(\n                tags_trans_coord_cam[\'robot\'])\n            tag_rot_coord_ground = np.matmul(\n                self.pos_transformer.camera_2_ground_trans[0:3, 0:3], tags_rot_coord_cam[\'robot\'])[0:3, 0:3]\n            self.robot_yaw = R.from_dcm(\n                tag_rot_coord_ground).as_euler(\'zyx\', degree=False)\n            print(""robot_error: "". self.robot_pos - self.robot_pos_cmd)\n            print(""robot_yaw_error: "". self.robot_yaw - self.robot_yaw_cmd)\n\n        if \'target\' in tags_trans_coord_cam.keys():\n            self.target_pos = self.pos_transformer.phyPosCam2PhyPosGround(\n                tags_trans_coord_cam[\'target\'])\n            tag_rot_coord_ground = np.matmul(self.pos_transformer.camera_2_ground_trans[0:3, 0:3],\n                                             tags_rot_coord_cam[\'target\'])[0:3, 0:3]\n            self.target_yaw = R.from_dcm(\n                tag_rot_coord_ground).as_euler(\'zyx\', degree=False)\n            print(""target_error: "", self.target_pos - self.target_pos_cmd)\n            print(""target_yaw_error: "", self.target_yaw - self.target_yaw_cmd)\n\n    def setRobotCmdConstrained(self, x, y, yaw):\n        self.robot_pos_cmd[0] = max(x, MIN_X)\n        self.robot_pos_cmd[0] = min(x, MAX_X)\n\n        self.robot_pos_cmd[1] = max(y, MIN_Y)\n        self.robot_pos_cmd[1] = min(y, MAX_Y)\n        self.robot_yaw_cmd = self.normalizeAngle(yaw)\n\n    def setRobotCmd(self, x, y, yaw):\n        self.robot_pos_cmd[0] = x\n        self.robot_pos_cmd[1] = y\n        self.robot_yaw_cmd = self.normalizeAngle(yaw)\n\n        self.robot_pos = self.robot_pos_cmd + \\\n            np.random.randn(2) * NOISE_VAR_ROBOT_POS  # add noise\n        self.robot_yaw = self.normalizeAngle(\n            self.robot_yaw_cmd + np.random.randn() * NOISE_VAR_ROBOT_YAW)  # add noise\n\n    def setTargetCmd(self, x, y, yaw):\n        self.target_pos_cmd[0] = x\n        self.target_pos_cmd[1] = y\n        self.target_yaw_cmd = self.normalizeAngle(yaw)\n\n        self.target_pos = self.target_pos_cmd\n        self.target_yaw = self.normalizeAngle(self.target_yaw_cmd)\n\n    def forward(self, action=None):\n        """"""\n        Move one step forward (Translation)\n        """"""\n        self.setRobotCmd(\n            self.robot_pos_cmd[0] + self.step_distance, self.robot_pos_cmd[1], self.robot_yaw_cmd)\n\n    def backward(self, action=None):\n        """"""\n        Move one step backward\n        """"""\n        self.setRobotCmd(\n            self.robot_pos_cmd[0] - self.step_distance, self.robot_pos_cmd[1], self.robot_yaw_cmd)\n\n    def left(self, action=None):\n        """"""\n        Translate to the left\n        """"""\n        self.setRobotCmd(\n            self.robot_pos_cmd[0], self.robot_pos_cmd[1] + self.step_distance, self.robot_yaw_cmd)\n\n    def right(self, action=None):\n        """"""\n        Translate to the right\n        """"""\n        self.setRobotCmd(\n            self.robot_pos_cmd[0], self.robot_pos_cmd[1] - self.step_distance, self.robot_yaw_cmd)\n\n    def moveContinous(self, action):\n        """"""\n        Perform a continuous displacement of dx, dy\n        """"""\n        self.setRobotCmd(\n            self.robot_pos_cmd[0] + action[0], self.robot_pos_cmd[1] + action[1], self.robot_yaw_cmd)\n\n    def moveByVelocityCmd(self, speed_x, speed_y, speed_yaw):\n        """"""\n        simuate the robot moved by velocity command\n        This function is assumed to be called at a frequency RL_CONTROL_FREQ in the simulation world\n\n        :param speed_x: (float) linear speed along x-axis (m/s) (forward-backward), in robot local coordinate\n        :param speed_y: (float) linear speed along y-axis (m/s) (left-right), in robot local coordinate\n        :param speed_yaw: (float) rotation speed of robot around z-axis (rad/s), in robot local coordinate\n        """"""\n        # calculate the robot position that it should be at this moment, so it should be driven by last command\n        # Assume in 1/RL_CONTROL_FREQ, the heading remains the same (not true,\n        #   but should be approximately work if RL_CONTROL_FREQ is high enough)\n        # translate the last velocity cmd in robot local coordiante to position cmd in gound coordiante\n        cos_direction = np.cos(self.robot_yaw)\n        sin_direction = np.sin(self.robot_yaw)\n\n        ground_pos_cmd_x = self.robot_pos[0] + (self.last_linear_velocity_cmd[0] *\n                                                cos_direction - self.last_linear_velocity_cmd[1] * sin_direction)/RL_CONTROL_FREQ\n        ground_pos_cmd_y = self.robot_pos[1] + (self.last_linear_velocity_cmd[1] *\n                                                cos_direction + self.last_linear_velocity_cmd[0] * sin_direction)/RL_CONTROL_FREQ\n        ground_yaw_cmd = self.robot_yaw + self.last_rot_velocity_cmd/RL_CONTROL_FREQ\n        self.setRobotCmd(ground_pos_cmd_x, ground_pos_cmd_y, ground_yaw_cmd)\n\n        #\xc2\xa0save the command of this moment\n        self.last_linear_velocity_cmd[0] = speed_x\n        self.last_linear_velocity_cmd[1] = speed_y\n        self.last_rot_velocity_cmd = speed_yaw\n\n    def moveByWheelsCmd(self, left_speed, front_speed, right_speed):\n        """"""\n        simuate the robot moved by wheel speed command\n        This function is assumed to be called at a frequency RL_CONTROL_FREQ in the simulation world\n\n        :param left_speed: (float) linear speed of left wheel (meter/s)\n        :param front_speed: (float) linear speed of front wheel (meter/s)\n        :param right_speed: (float) linear speed of right wheel (meter/s)\n        """"""\n\n        # calculate the robot position by omnirobot\'s kinematic equations\n        # Assume in 1/RL_CONTROL_FREQ, the heading remains the same (not true,\n        # but should be approximately work if RL_CONTROL_FREQ is high enough)\n\n        # translate the last wheel speeds cmd in last velocity cmd\n        local_speed_x = self.last_wheel_speeds_cmd[0] / np.sqrt(3.0) \\\n            - self.last_wheel_speeds_cmd[2] / np.sqrt(3.0)\n        local_speed_y = - self.last_wheel_speeds_cmd[1] / 1.5 + \\\n            self.last_wheel_speeds_cmd[0] / 3.0 + \\\n            self.last_wheel_speeds_cmd[2] / 3.0\n        local_rot_speed = - self.last_wheel_speeds_cmd[1] / (3.0 * OMNIROBOT_L) \\\n            - self.last_wheel_speeds_cmd[0] / (3.0 * OMNIROBOT_L) \\\n            - self.last_wheel_speeds_cmd[2] / (3.0 * OMNIROBOT_L)\n            \n        # translate the last velocity cmd in robot local coordiante to position cmd in gound coordiante\n        cos_direction = np.cos(self.robot_yaw)\n        sin_direction = np.sin(self.robot_yaw)\n\n        ground_pos_cmd_x = self.robot_pos[0] + (local_speed_x *\n                                                cos_direction - local_speed_y * sin_direction)/RL_CONTROL_FREQ\n        ground_pos_cmd_y = self.robot_pos[1] + (local_speed_y *\n                                                cos_direction + local_speed_x * sin_direction)/RL_CONTROL_FREQ\n        ground_yaw_cmd = self.robot_yaw + local_rot_speed/RL_CONTROL_FREQ\n        self.setRobotCmd(ground_pos_cmd_x, ground_pos_cmd_y, ground_yaw_cmd)\n\n        self.last_wheel_speeds_cmd = np.float32(\n            [left_speed, front_speed, right_speed])\n\n    @staticmethod\n    def normalizeAngle(angle):\n        """"""\n        :param angle: (float) (in rad)\n        :return: (float) the angle in [-pi, pi] (in rad)\n        """"""\n        while angle > np.pi:\n            angle -= 2 * np.pi\n        while angle < -np.pi:\n            angle += 2 * np.pi\n        return angle\n\n\nclass OmniRobotSimulatorSocket(OmnirobotManagerBase):\n    def __init__(self, **args):\n        \'\'\'\n        Simulate the zmq socket like real omnirobot server \n        :param **args  arguments \n\n        \'\'\'\n        super(OmniRobotSimulatorSocket, self).__init__()\n        defalt_args = {\n            ""back_ground_path"": ""real_robots/omnirobot_utils/back_ground.jpg"",\n            ""camera_info_path"": CAMERA_INFO_PATH,\n            ""robot_marker_path"": ""real_robots/omnirobot_utils/robot_margin3_pixel_only_tag.png"",\n            ""robot_marker_margin"": [3, 3, 3, 3],\n            # for black target, use target_margin4_pixel.png"",\n            ""target_marker_path"": ""real_robots/omnirobot_utils/red_target_margin4_pixel_480x480.png"",\n            ""target_marker_margin"": [4, 4, 4, 4],\n            ""robot_marker_code"": None,\n            ""target_marker_code"": None,\n            ""robot_marker_length"": 0.18,\n            ""target_marker_length"": 0.18,\n            ""output_size"": [224, 224],\n            ""init_x"": 0,\n            ""init_y"": 0,\n            ""init_yaw"": 0,\n            ""origin_size"": ORIGIN_SIZE,\n            ""cropped_size"": CROPPED_SIZE\n        }\n        # overwrite the args if it exists\n        self.new_args = {**defalt_args, **args}\n\n        assert len(self.new_args[\'robot_marker_margin\']) == 4\n        assert len(self.new_args[\'target_marker_margin\']) == 4\n        assert len(self.new_args[\'output_size\']) == 2\n\n        self.robot = OmniRobotEnvRender(**self.new_args)\n        self.episode_idx = 0\n        self._random_target = self.new_args[""random_target""]\n        self.resetEpisode()  # for a random target initial position\n\n    def resetEpisode(self):\n        """"""\n        override the original method\n        Give the correct sequance of commands to the robot \n        to rest environment between the different episodes\n        """"""\n        if self.second_cam_topic is not None:\n            assert NotImplementedError\n        # Env reset\n        random_init_position = self.sampleRobotInitalPosition()\n        self.robot.setRobotCmd(\n            random_init_position[0], random_init_position[1], 0)\n\n        self.robot_marker_size_proprotion = np.random.randn(\n        ) * NOISE_VAR_ROBOT_SIZE_PROPOTION + 1.0\n\n        # target reset\n        if self._random_target or self.episode_idx == 0:\n            random_init_x = np.random.random_sample() * (TARGET_MAX_X - TARGET_MIN_X) + \\\n                TARGET_MIN_X\n            random_init_y = np.random.random_sample() * (TARGET_MAX_Y - TARGET_MIN_Y) + \\\n                TARGET_MIN_Y\n            self.robot.setTargetCmd(\n                random_init_x, random_init_y, 2 * np.pi * np.random.rand() - np.pi)\n\n        # render the target and robot\n        self.robot.renderTarget()\n        self.robot.renderRobot()\n\n    def send_json(self, msg):\n        # env send msg to render\n        self.processMsg(msg)\n\n        self.robot.renderRobot()\n\n        self.img = self.robot.getCroppedImage()\n        self.img = self.robot.renderEnvLuminosityNoise(self.img, noise_var=NOISE_VAR_ENVIRONMENT, in_RGB=False,\n                                                       out_RGB=True)\n        self.img = cv2.resize(self.img, tuple(self.robot.output_size))\n\n    def recv_json(self):\n        msg = {\n            # XYZ position\n            ""position"": self.robot.robot_pos.tolist(),\n            ""reward"": self.reward,\n            ""target_pos"": self.robot.target_pos.tolist()\n        }\n        return msg\n\n    def recv_image(self):\n        return self.img\n'"
real_robots/real_baxter_debug.py,0,"b'#!/usr/bin/env python2\nfrom __future__ import division, print_function, absolute_import\n\nimport subprocess\nimport signal\n\nimport arm_scenario_simulator as arm_sim\nimport baxter_interface\nimport numpy as np\nimport rospy\nimport zmq\nfrom baxter_interface import Limb, Head, Gripper, CHECK_VERSION\nfrom arm_scenario_experiments import baxter_utils\nfrom arm_scenario_experiments import utils as arm_utils\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom geometry_msgs.msg import Point, Vector3, Vector3Stamped\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import Header\n\nfrom .constants import *\n\nassert USING_REAL_BAXTER, ""Please set USING_REAL_BAXTER to True in real_robots/constants.py""\n\nshould_exit = [False]\n\n\n# exit the script on ctrl+c\ndef ctrl_c(signum, frame):\n    should_exit[0] = True\n\n\nsignal.signal(signal.SIGINT, ctrl_c)\n\n\ndef resetPose():\n    rs = baxter_interface.RobotEnable(baxter_interface.CHECK_VERSION)\n    if rs.state().enabled:\n        print(""Robot already enabled"")\n    else:\n        print(""Enabling robot... "")\n        rs.enable()\n        # Untuck arms\n        subprocess.call([\'rosrun\', \'baxter_tools\', \'tuck_arms.py\', \'-u\'])\n\n\nrospy.init_node(\'real_baxter_server\', anonymous=True)\n\n# Retrieve the different gazebo objects\nleft_arm = baxter_interface.Limb(\'left\')\nright_arm = baxter_interface.Limb(\'right\')\nee_orientation = baxter_utils.get_ee_orientation(left_arm)\n\nrospy.sleep(1)\n\nprint(""Initializing robot..."")\n# Init robot pose\nresetPose()\nprint(""Init Robot pose over"")\n\ntry:\n    while not should_exit[0]:\n        for name, arm in zip([""left_arm"", ""right_arm""], [left_arm, right_arm]):\n            arm_pos = baxter_utils.get_ee_position(arm)\n            if name == ""left_arm"":\n                ee_orientation = baxter_utils.get_ee_orientation(left_arm)\n                print(np.linalg.norm(BUTTON_POS - arm_pos, 2))\n        rospy.sleep(0.5)\nexcept KeyboardInterrupt:\n    print(""Exiting...."")\n'"
real_robots/real_baxter_server.py,0,"b'#!/usr/bin/env python2\nfrom __future__ import division, print_function, absolute_import\n\nimport os\nimport subprocess\nimport signal\n\nimport baxter_interface\nimport numpy as np\nimport rospy\nimport zmq\nimport cv2\nfrom arm_scenario_experiments import baxter_utils\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom sensor_msgs.msg import Image\n\nfrom .constants import *\nfrom .utils import sendMatrix\n\nassert USING_REAL_BAXTER, ""Please set USING_REAL_BAXTER to True in real_robots/constants.py""\n\nbridge = CvBridge()\nshould_exit = [False]\n\n\n# exit the script on ctrl+c\ndef ctrl_c(signum, frame):\n    should_exit[0] = True\n\n\nsignal.signal(signal.SIGINT, ctrl_c)\n\n\ndef resetPose():\n    """"""\n    Enable Baxter robot (if necessary) and reset the lefy arm position\n    """"""\n    rs = baxter_interface.RobotEnable(baxter_interface.CHECK_VERSION)\n    if rs.state().enabled:\n        print(""Robot already enabled"")\n    else:\n        print(""Enabling robot... "")\n        rs.enable()\n        # Untuck arms\n        subprocess.call([\'rosrun\', \'baxter_tools\', \'tuck_arms.py\', \'-u\'])\n    print(""Moving left arm to init"")\n    moveLeftArmToInit()\n\n\nclass ImageCallback(object):\n    """"""\n    Image callback for ROS\n    """"""\n    def __init__(self):\n        super(ImageCallback, self).__init__()\n        self.valid_img = None\n\n    def imageCallback(self, msg):\n        try:\n            # Convert your ROS Image message to OpenCV\n            cv2_img = bridge.imgmsg_to_cv2(msg, ""rgb8"")\n            self.valid_img = cv2_img\n        except CvBridgeError as e:\n            print(""CvBridgeError:"", e)\n\n\ndef moveLeftArmToInit():\n    """"""\n    Initialize robot left arm to starting position (hardcoded)\n    :return: ([float])\n    """"""\n    joints = None\n    position = LEFT_ARM_INIT_POS\n    while not joints:\n        try:\n            joints = baxter_utils.IK(left_arm, position, LEFT_ARM_ORIENTATION, IK_SEED_POSITIONS)\n        except Exception:\n            try:\n                joints = baxter_utils.IK(left_arm, position, LEFT_ARM_ORIENTATION, IK_SEED_POSITIONS)\n            except Exception:\n                raise\n    left_arm.move_to_joint_positions(joints)\n    return position\n\n\ndef saveSecondCamImage(im, episode_folder, episode_step, path=""real_baxter_2nd_cam""):\n    """"""\n    Write an image to disk\n    :param im: (numpy matrix) BGR image\n    :param episode_folder: (str)\n    :param episode_step: (int)\n    :param path: (str)\n    """"""\n    image_path = ""{}/{}/frame{:06d}.jpg"".format(path, episode_folder, episode_step)\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    cv2.imwrite(""srl_zoo/data/{}"".format(image_path), im)\n\n\nrospy.init_node(\'real_baxter_server\', anonymous=True)\n\n# Connect to ROS Topics\nimage_cb_wrapper = ImageCallback()\nimg_sub = rospy.Subscriber(IMAGE_TOPIC, Image, image_cb_wrapper.imageCallback)\n\nif SECOND_CAM_TOPIC is not None:\n    image_cb_wrapper_2 = ImageCallback()\n    img_2_sub = rospy.Subscriber(SECOND_CAM_TOPIC, Image, image_cb_wrapper_2.imageCallback)\n\n# Retrieve the different gazebo objects\nleft_arm = baxter_interface.Limb(\'left\')\nright_arm = baxter_interface.Limb(\'right\')\n\nprint(""Initializing robot..."")\n# Init robot pose\nresetPose()\nprint(""Init Robot pose over"")\nend_point_position = baxter_utils.get_ee_position(left_arm)\nee_orientation = baxter_utils.get_ee_orientation(left_arm)\n\nprint(\'Starting up on port number {}\'.format(SERVER_PORT))\ncontext = zmq.Context()\nsocket = context.socket(zmq.PAIR)\n\nsocket.bind(""tcp://*:{}"".format(SERVER_PORT))\n\nprint(""Waiting for client..."")\nsocket.send_json({\'msg\': \'hello\'})\nprint(""Connected to client"")\n\naction = [0, 0, 0]\njoints = None\nepisode_step = 0\nepisode_idx = -1\nepisode_folder = None\n\nwhile not should_exit[0]:\n    msg = socket.recv_json()\n    command = msg.get(\'command\', \'\')\n\n    if command == \'reset\':\n        resetPose()\n        end_point_position = baxter_utils.get_ee_position(left_arm)\n        print(\'Environment reset\')\n        action = [0, 0, 0]\n        episode_idx += 1\n        episode_step = 0\n\n        if SECOND_CAM_TOPIC is not None:\n            episode_folder = ""record_{:03d}"".format(episode_idx)\n            try:\n                os.makedirs(""srl_zoo/data/{}/{}"".format(DATA_FOLDER_SECOND_CAM, episode_folder))\n            except OSError:\n                pass\n\n    elif command == \'action\':\n        action = np.array(msg[\'action\'])\n        print(""action:"", action)\n\n    elif command == ""exit"":\n        break\n    else:\n        raise ValueError(""Unknown command: {}"".format(msg))\n\n    end_point_position_candidate = end_point_position + action\n\n    print(""End-effector Position:"", end_point_position_candidate)\n    joints = None\n    try:\n        joints = baxter_utils.IK(left_arm, end_point_position_candidate, ee_orientation)\n    except Exception as e:\n        print(""[ERROR] no joints position returned by the Inverse Kinematic fn"")\n        print(""end_point_position_candidate:{}"".format(end_point_position_candidate))\n        print(e)\n\n    if joints:\n        end_point_position = end_point_position_candidate\n        left_arm.move_to_joint_positions(joints, timeout=3)\n    else:\n        print(""No joints position, returning previous one"")\n\n    reward = 0\n    # Consider that we touched the button if we are close enough\n    if np.linalg.norm(BUTTON_POS - end_point_position, 2) < DIST_TO_TARGET_THRESHOLD:\n        reward = 1\n        print(""Button touched!"")\n\n    # Send arm position, button position, ...\n    socket.send_json(\n        {\n            # XYZ position\n            ""position"": list(end_point_position),\n            ""reward"": reward,\n            ""button_pos"": list(BUTTON_POS)\n        },\n        flags=zmq.SNDMORE\n    )\n    # Retrieve last image from image topic\n    img = image_cb_wrapper.valid_img\n\n    if SECOND_CAM_TOPIC is not None:\n        saveSecondCamImage(image_cb_wrapper_2.valid_img, episode_folder, episode_step, DATA_FOLDER_SECOND_CAM)\n        episode_step += 1\n    # to contiguous, otherwise ZMQ will complain\n    img = np.ascontiguousarray(img, dtype=np.uint8)\n    sendMatrix(socket, img)\n\nprint("" Exiting server - closing socket..."")\nsocket.close()\n'"
real_robots/real_robobo_server.py,0,"b'#!/usr/bin/env python2\nfrom __future__ import division, print_function, absolute_import\n\nimport os\nimport signal\nimport time\n\nimport numpy as np\nimport rospy\nimport zmq\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom sensor_msgs.msg import Image\n\nfrom com_mytechia_robobo_ros_msgs.srv import Command\nfrom com_mytechia_robobo_ros_msgs.msg import KeyValue, Status\n\nfrom .constants import *\nfrom .utils import sendMatrix\n\nassert USING_ROBOBO, ""Please set USING_ROBOBO to True in real_robots/constants.py""\n\nbridge = CvBridge()\nshould_exit = [False]\n\n\n# exit the script on ctrl+c\ndef ctrl_c(signum, frame):\n    should_exit[0] = True\n\n\nsignal.signal(signal.SIGINT, ctrl_c)\n\n\nclass Robobo(object):\n    """"""\n    Class for controlling Robobo\n    """"""\n\n    def __init__(self):\n        super(Robobo, self).__init__()\n        # Duration of the ""FORWARD"" action\n        self.time_forward = 1.7\n\n        self.speed = 10\n        # Angle that robobo achieve in one second\n        # at a given speed\n        self.angle_offset = 38\n        # Degree per s when turning after the 1st second\n        # at a given speed\n        # From calibration\n        self.angle_coeff = 50\n        # Robobo\'s position on the grid\n        self.position = [0, 0]\n\n        # Initialize the different for moving the robot\n        self.directions = {\'left\': 90, \'right\': -90}\n        self.faces = [\'west\', \'north\', \'east\']\n        self.current_face_idx = 1\n        self.yaw_error = 0\n        self.yaw_target = 0\n        self.yaw_north = 0\n        self.yaw = 0\n        self.left_encoder_pos = 0\n        self.right_encoder_pos = 0\n        self.angles = {}\n\n        # Attempt connection to Robobo\'s service\n        try:\n            self.robobo_command = rospy.ServiceProxy(\'/command\', Command)\n        except rospy.ServiceException as e:\n            print(""Service exception"", str(e))\n            exit(1)\n\n        self.status_sub = rospy.Subscriber(""/status"", Status, self.statusCallback)\n\n    def moveForever(self, lspeed, rspeed, speed):\n        """"""\n        :param lspeed: (str) ""forward"" or ""backward""\n        :param rspeed: (str)\n        :param speed: (int)\n        """"""\n        command_name = \'MOVE-FOREVER\'\n        command_parameters = [KeyValue(\'lspeed\', lspeed), KeyValue(\'rspeed\', rspeed), KeyValue(\'speed\', str(speed))]\n        self.robobo_command(command_name, 0, command_parameters)\n\n    def statusCallback(self, status):\n        """"""\n        Callback for ROS topic\n        :param status: (Status ROS message)\n        """"""\n        # Update the current yaw using phone gyroscope\n        # NOTE: this may not work depending on the phone\n        if status.name == \'ORIENTATION\':\n            for KeyV in status.value:\n                if KeyV.key == \'yaw\':\n                    self.yaw = float(KeyV.value)\n        # Update position of the two encoders\n        if status.name == \'WHEELS\':\n            for KeyV in status.value:\n                if KeyV.key == \'wheelPosL\':\n                    self.left_encoder_pos = float(KeyV.value)\n                elif KeyV.key == \'wheelPosR\':\n                    self.right_encoder_pos = float(KeyV.value)\n\n    def stop(self):\n        """"""\n        Stop robobo\n        """"""\n        self.moveForever(\'forward\', \'forward\', 0)\n\n    def initYawNorth(self):\n        """"""\n        Initialize the reference yaw that represents the North\n        """"""\n        self.yaw_north = self.yaw\n        self.angles = {\n            \'north\': self.yaw_north,\n            \'east\': self.normalizeAngle(self.yaw_north - 90),\n            \'west\': self.normalizeAngle(self.yaw_north + 90)\n        }\n        self.current_face_idx = 1\n        self.yaw_target = self.yaw_north\n        self.yaw_error = 0\n\n    def forward(self):\n        """"""\n        Move one step forward (Translation)\n        """"""\n        self.move(self.time_forward, self.speed)\n        time.sleep(1.1 * self.time_forward)\n\n    def backward(self):\n        """"""\n        Move one step backward\n        """"""\n        self.move(self.time_forward, -self.speed)\n        time.sleep(1.1 * self.time_forward)\n\n    def turnLeft(self):\n        turn_time = self.computeTime(\'left\')\n        assert self.current_face_idx > 0\n        self.current_face_idx -= 1\n        self.updateTarget()\n        self.turn(turn_time, -self.speed)\n        self.updateError()\n\n    def turnRight(self):\n        turn_time = self.computeTime(\'right\')\n        assert self.current_face_idx < len(self.faces)\n        self.current_face_idx += 1\n        self.updateTarget()\n        self.turn(turn_time, self.speed)\n        self.updateError()\n\n    def updateError(self):\n        """"""\n        Update the error between desired yaw and current one\n        """"""\n        self.yaw_error = self.normalizeAngle(self.yaw_target - self.yaw)\n        print(""yaw_error"", self.yaw_error)\n\n    def updateTarget(self):\n        """"""\n        Update the target angle\n        """"""\n        self.yaw_target = self.angles[self.faces[self.current_face_idx]]\n        # print(""face_idx"", self.current_face_idx, self.faces[self.current_face_idx])\n        # print(""yaw_target"", self.yaw_target)\n\n    def computeTime(self, direction):\n        """"""\n        Compute the time needed for a rotation to face a given direction\n        It is meant to correct the previous error\n        however this does not seems to work for now\n        :param direction: (str) ""left"" or ""right""\n        :return: (float)\n        """"""\n        # Cancel the error, gives better performance (less drift)\n        self.yaw_error = 0\n        t = (abs(self.directions[direction] + self.yaw_error) - self.angle_offset) / self.angle_coeff + 1\n        # print(""yaw"", self.yaw, ""current_face"", self.faces[self.current_face_idx])\n        # print(""yaw_north"", self.yaw_north, \'direction\', direction)\n        # print(""time:"", t)\n        print(""yaw"", self.yaw, ""yaw_north"", self.yaw_north)\n        return t\n\n    def move(self, t, speed):\n        """"""\n        Translation move\n        :param t: (float) duration of Translation\n        :param speed: (int)\n        """"""\n        command_parameters = [KeyValue(\'lspeed\', str(speed)), KeyValue(\'rspeed\', str(speed)), KeyValue(\'time\', str(t))]\n        self.robobo_command(""MOVE"", 0, command_parameters)\n\n    def turn(self, t, speed):\n        """"""\n        Rotation move\n        :param t: (float) duration of Rotation\n        :param speed: (int)\n        """"""\n        command_parameters = [KeyValue(\'lspeed\', str(speed)), KeyValue(\'rspeed\', str(-speed)), KeyValue(\'time\', str(t))]\n        self.robobo_command(""MOVE"", 0, command_parameters)\n        time.sleep(1.1 * t + 2)\n        print(""MOVED"")\n\n    @staticmethod\n    def normalizeAngle(angle):\n        """"""\n        :param angle: (float)\n        :return: (float) the angle in [-pi, pi]\n        """"""\n        while angle > 180:\n            angle -= 2 * 180\n        while angle < -180:\n            angle += 2 * 180\n        return angle\n\n\ndef findTarget(image, debug=False):\n    """"""\n    Find the target in the image using color thresholds\n    :param image: (bgr image)\n    :param debug: (bool) Whether to display the image or not\n    :return: (int, int, float, bool)\n    """"""\n    error = False\n\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    # Threshold the HSV image\n    mask = cv2.inRange(hsv, LOWER_RED, UPPER_RED)\n\n    # Remove noise\n    kernel_erode = np.ones((4, 4), np.uint8)\n    eroded_mask = cv2.erode(mask, kernel_erode, iterations=2)\n\n    kernel_dilate = np.ones((6, 6), np.uint8)\n    dilated_mask = cv2.dilate(eroded_mask, kernel_dilate, iterations=2)\n\n    if debug:\n        cv2.imshow(\'mask\', mask)\n        cv2.imshow(\'eroded\', eroded_mask)\n        cv2.imshow(\'dilated\', dilated_mask)\n\n    # Retrieve contours\n    _, contours, _ = cv2.findContours(dilated_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Sort by area\n    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]\n    if debug:\n        cv2.drawContours(image, contours, 0, (0, 255, 0), 3)\n\n    if len(contours) > 0:\n        M = cv2.moments(contours[0])\n        # Centroid\n        cx = int(M[\'m10\'] / M[\'m00\'])\n        cy = int(M[\'m01\'] / M[\'m00\'])\n        area = cv2.contourArea(contours[0])\n    else:\n        cx, cy = 0, 0\n        area = 0\n        error = True\n\n    if debug:\n        if error:\n            print(""No centroid found"")\n        else:\n            print(""Found centroid at ({}, {})"".format(cx, cy))\n        cv2.circle(image, (cx, cy), radius=10, color=(0, 0, 255),\n                   thickness=1, lineType=8, shift=0)\n        cv2.imshow(\'result\', image)\n    return cx, cy, area, error\n\n\nclass ImageCallback(object):\n    """"""\n    Image callback for ROS\n    """"""\n\n    def __init__(self):\n        super(ImageCallback, self).__init__()\n        self.valid_img = None\n\n    def imageCallback(self, msg):\n        try:\n            # Convert your ROS Image message to OpenCV\n            cv2_img = bridge.imgmsg_to_cv2(msg, ""rgb8"")\n            self.valid_img = cv2_img\n        except CvBridgeError as e:\n            print(""CvBridgeError:"", e)\n\n\ndef saveSecondCamImage(im, episode_folder, episode_step, path=""robobo_2nd_cam""):\n    """"""\n    Write an image to disk\n    :param im: (numpy matrix) BGR image\n    :param episode_folder: (str)\n    :param episode_step: (int)\n    :param path: (str)\n    """"""\n    image_path = ""{}/{}/frame{:06d}.jpg"".format(path, episode_folder, episode_step)\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    cv2.imwrite(""srl_zoo/data/{}"".format(image_path), im)\n\n\nrospy.init_node(\'robobo_server\', anonymous=True)\n\n# Connect to ROS Topics\nif IMAGE_TOPIC is not None:\n    image_cb_wrapper = ImageCallback()\n    img_sub = rospy.Subscriber(IMAGE_TOPIC, Image, image_cb_wrapper.imageCallback, queue_size=1)\n\nif SECOND_CAM_TOPIC is not None:\n    image_cb_wrapper_2 = ImageCallback()\n    img_2_sub = rospy.Subscriber(SECOND_CAM_TOPIC, Image, image_cb_wrapper_2.imageCallback, queue_size=1)\n\nprint(\'Starting up on port number {}\'.format(SERVER_PORT))\ncontext = zmq.Context()\nsocket = context.socket(zmq.PAIR)\n\nsocket.bind(""tcp://*:{}"".format(SERVER_PORT))\n\nprint(""Waiting for client..."")\nsocket.send_json({\'msg\': \'hello\'})\nprint(""Connected to client"")\n\naction = 0\nepisode_step = 0\nepisode_idx = -1\nepisode_folder = None\n\nrobobo = Robobo()\nrobobo.stop()\n\n# Init robot yaw angle\nrobobo.turn(robobo.computeTime(\'left\'), -robobo.speed)\nrobobo.turn(robobo.computeTime(\'right\'), robobo.speed)\nrobobo.initYawNorth()\n\nwhile not should_exit[0]:\n    msg = socket.recv_json()\n    command = msg.get(\'command\', \'\')\n\n    if command == \'reset\':\n        print(\'Environment reset\')\n        action = None\n        episode_idx += 1\n        episode_step = 0\n\n        if SECOND_CAM_TOPIC is not None:\n            episode_folder = ""record_{:03d}"".format(episode_idx)\n            try:\n                os.makedirs(""srl_zoo/data/{}/{}"".format(DATA_FOLDER_SECOND_CAM, episode_folder))\n            except OSError:\n                pass\n\n    elif command == \'action\':\n        print(""action (int)"", msg[\'action\'])\n        action = Move(msg[\'action\'])\n        print(""action (move):"", action)\n\n    elif command == ""exit"":\n        break\n    else:\n        raise ValueError(""Unknown command: {}"".format(msg))\n\n    has_bumped = False\n    # We are always facing North\n    if action == Move.FORWARD:\n        if robobo.position[1] < MAX_Y:\n            robobo.forward()\n            robobo.position[1] += 1\n        else:\n            has_bumped = True\n    elif action == Move.STOP:\n        robobo.stop()\n    elif action == Move.RIGHT:\n        if robobo.position[0] < MAX_X:\n            robobo.turnRight()\n            robobo.forward()\n            robobo.turnLeft()\n            robobo.position[0] += 1\n        else:\n            has_bumped = True\n    elif action == Move.LEFT:\n        if robobo.position[0] > MIN_X:\n            robobo.turnLeft()\n            robobo.forward()\n            robobo.turnRight()\n            robobo.position[0] -= 1\n        else:\n            has_bumped = True\n    elif action == Move.BACKWARD:\n        if robobo.position[1] > MIN_Y:\n            robobo.backward()\n            robobo.position[1] -= 1\n        else:\n            has_bumped = True\n    elif action is None:\n        # Env reset\n        pass\n    else:\n        print(""Unsupported action"")\n\n    if IMAGE_TOPIC is not None:\n        # Retrieve last image from image topic\n        original_image = np.copy(image_cb_wrapper.valid_img)\n        # Find the target in the image using color thresholds\n        cx, cy, area, error = findTarget(original_image.copy(), debug=False)\n        # Compute the change in the target area compared to initial detected area\n        delta_area_rate = (TARGET_INITIAL_AREA - area) / TARGET_INITIAL_AREA\n\n        print(""Image processing:"", cx, cy, area, error, delta_area_rate)\n    else:\n        delta_area_rate = 0\n\n    reward = 0\n    # Consider that we reached the target if we are close enough\n    # we detect that computing the difference in area between TARGET_INITIAL_AREA\n    # current detected area of the target\n    if delta_area_rate > MIN_DELTA_AREA:\n        reward = 1\n        print(""Target reached!"")\n\n    if has_bumped:\n        reward = -1\n        print(""Bumped into wall"")\n        print()\n\n    print(""Robobo position"", robobo.position)\n    socket.send_json(\n        {\n            # XYZ position\n            ""position"": list(robobo.position),\n            ""reward"": reward,\n            ""target_pos"": list([cx, cy])\n        },\n        flags=zmq.SNDMORE if IMAGE_TOPIC is not None else 0\n    )\n\n    if SECOND_CAM_TOPIC is not None:\n        saveSecondCamImage(image_cb_wrapper_2.valid_img, episode_folder, episode_step, DATA_FOLDER_SECOND_CAM)\n        episode_step += 1\n\n    if IMAGE_TOPIC is not None:\n        # to contiguous, otherwise ZMQ will complain\n        img = np.ascontiguousarray(original_image, dtype=np.uint8)\n        sendMatrix(socket, img)\n\nprint(""Exiting server - closing socket..."")\nsocket.close()\n'"
real_robots/teleop_client.py,0,"b'#!/usr/bin/env python\n""""""\nTeleoperation client:\n- control the robobo with arrows keys + U for stopping the robot\n- control the baxter robot with arrows keys + D and U keys (for moving along the z-axis)\nPress esc or q to exit the client\n""""""\nfrom __future__ import division, print_function, absolute_import\n\nimport time\nimport signal\n\nimport cv2\nimport numpy as np\nimport zmq\n\nfrom .constants import SERVER_PORT, HOSTNAME, UP_KEY, DOWN_KEY, LEFT_KEY, \\\n    RIGHT_KEY, D_KEY, U_KEY, EXIT_KEYS, R_KEY, Move, IMAGE_TOPIC, DELTA_POS, USING_ROBOBO\nfrom .utils import recvMatrix\n\nnp.set_printoptions(precision=4)\n\n# Connect to the Gym bridge ROS node\ncontext = zmq.Context()\nsocket = context.socket(zmq.PAIR)\nsocket.connect(""tcp://{}:{}"".format(HOSTNAME, SERVER_PORT))\n\nprint(""Waiting for server..."")\nmsg = socket.recv_json()\nprint(""Connected to server"")\n\ntimes = []\naction = [0, 0, 0]\nif USING_ROBOBO:\n    action_dict = {\n        UP_KEY: Move.FORWARD.value,\n        DOWN_KEY: Move.BACKWARD.value,\n        LEFT_KEY: Move.LEFT.value,\n        RIGHT_KEY: Move.RIGHT.value,\n        D_KEY: Move.STOP.value,\n        U_KEY: Move.STOP.value\n    }\nelse:\n    action_dict = {\n        UP_KEY: [- DELTA_POS, 0, 0],\n        DOWN_KEY: [DELTA_POS, 0, 0],\n        LEFT_KEY: [0, - DELTA_POS, 0],\n        RIGHT_KEY: [0, DELTA_POS, 0],\n        D_KEY: [0, 0, - DELTA_POS],\n        U_KEY: [0, 0, DELTA_POS]\n\n    }\n\n# Create dark image to listen to keyboard events\ncv2.imshow(""Image"", np.zeros((100, 100, 3), dtype=np.uint8))\n\nshould_exit = [False]\n\n\n# exit the script on ctrl+c\ndef ctrl_c(signum, frame):\n    should_exit[0] = True\n\n\nsignal.signal(signal.SIGINT, ctrl_c)\n\nwhile not should_exit[0]:\n    # Retrieve pressed key\n    key = cv2.waitKey(0) & 0xff\n\n    if key in EXIT_KEYS:\n        break\n    elif key in action_dict.keys():\n        action = action_dict[key]\n        socket.send_json({""command"": ""action"", ""action"": action})\n    elif key == R_KEY:\n        socket.send_json({""command"": ""reset""})\n    else:\n        print(""Unknown key: {}"".format(key))\n        continue\n\n    start_time = time.time()\n    # Receive state data (position, etc)\n    state_data = socket.recv_json()\n    print(\'state data: {}\'.format(state_data))\n\n    if IMAGE_TOPIC is not None:\n        # Receive a camera image from the server\n        img = recvMatrix(socket)\n        cv2.imshow(""Image"", img)\n\n    times.append(time.time() - start_time)\n\nsocket.send_json({""command"": ""exit""})\ncv2.destroyAllWindows()\n\nprint(""Client exiting..."")\nprint(""{:.2f} FPS"".format(len(times) / np.sum(times)))\n\nsocket.close()\n'"
real_robots/utils.py,0,"b'from __future__ import print_function, absolute_import, division\n\nimport sys\n\nimport zmq\nimport numpy as np\n\nif sys.version_info > (3,):\n    buffer = memoryview\n\n\ndef recvMatrix(socket):\n    """"""\n    Receive a numpy array over zmq\n    :param socket: (zmq socket)\n    :return: (Numpy matrix)\n    """"""\n    metadata = socket.recv_json()\n    msg = socket.recv(copy=True, track=False)\n    A = np.frombuffer(buffer(msg), dtype=metadata[\'dtype\'])\n    return A.reshape(metadata[\'shape\'])\n\n\ndef sendMatrix(socket, mat):\n    """"""\n    Send a numpy mat with metadata over zmq\n    :param socket:\n    :param mat: (numpy matrix)\n    """"""\n    metadata = dict(\n        dtype=str(mat.dtype),\n        shape=mat.shape,\n    )\n    # SNDMORE flag specifies this is a multi-part message\n    socket.send_json(metadata, flags=zmq.SNDMORE)\n    return socket.send(mat, flags=0, copy=True, track=False)\n\n\ndef getActions(delta_pos, n_actions):\n    """"""\n    Get list of possible actions\n    :param delta_pos: (float)\n    :param n_actions: (int)\n    :return: (numpy matrix)\n    """"""\n    possible_deltas = [i * delta_pos for i in range(-1, 2)]\n    actions = []\n    for dx in possible_deltas:\n        for dy in possible_deltas:\n            for dz in possible_deltas:\n                if dx == 0 and dy == 0 and dz == 0:\n                    continue\n                # Allow only move in one direction\n                if abs(dx) + abs(dy) + abs(dz) > delta_pos:\n                    continue\n                actions.append([dx, dy, dz])\n\n    assert len(actions) == n_actions, ""Wrong number of actions: {}"".format(len(actions))\n\n    return np.array(actions)\n\n\ndef randomAction(possible_actions):\n    """"""\n    Take a random action for a list of possible actions\n    :param possible_actions: [[float]\n    :return: [float]\n    """"""\n    action_idx = np.random.randint(len(possible_actions))\n    return possible_actions[action_idx]\n'"
replay/__init__.py,0,b''
replay/aggregate_plots.py,0,"b'import argparse\nimport json\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib.ticker import FuncFormatter\n\nfrom rl_baselines.visualize import loadCsv, movingAverage, loadData\nfrom srl_zoo.utils import printGreen, printYellow, printRed\n\n# Init seaborn\nsns.set()\n# Style for the title\nfontstyle = {\'fontname\': \'DejaVu Sans\', \'fontsize\': 16}\n\n# Modified Colorbrewer Paired_12, you can use palettable to retrieve it\ncolors = [[166, 206, 227], [31, 120, 180], [178, 223, 138], [51, 160, 44], [251, 154, 153], [227, 26, 28],\n          [253, 191, 111], [255, 127, 0], [202, 178, 214], [106, 61, 154], [143, 156, 212], [64, 57, 178], [255, 255, 153], [177, 89, 40],\n          [10, 10, 10], [0, 0, 0]]\ncolors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\nlightcolors = colors[0::2]\ndarkcolors = colors[1::2]\n\n# Default y-limits for the plot\n# Kuka Arm\nY_LIM_SPARSE_REWARD = [0, 5]\n# Mobile robot\nY_LIM_SPARSE_REWARD = [-3, 250]\n# Relative: [-150, -50]\n# Normal: [-70, -35]\nY_LIM_SHAPED_REWARD = [-150, -50]\n\n\ndef loadEpisodesData(folder):\n    """"""\n    :param folder: (str)\n    :return: (numpy array, numpy array) or (None, None)\n    """"""\n    result, _ = loadCsv(folder)\n\n    if len(result) == 0:\n        return None, None\n\n    y = np.array(result)[:, 1]\n    x = np.arange(len(y))\n    return x, y\n\n\ndef millions(x, pos):\n    """"""\n    formatter for matplotlib\n    The two args are the value and tick position\n    :param x: (float)\n    :param pos: (int) tick position (not used here\n    :return: (str)\n    """"""\n    return \'{:.1f}M\'.format(x * 1e-6)\n\n\ndef plotGatheredExperiments(folders, algo, y_limits, window=40, title="""", min_num_x=-1,\n                            timesteps=False, output_file="""", no_display=False):\n    """"""\n    Compute mean and standard error for several experiments and plot the learning curve\n    :param folders: ([str]) Log folders, where the monitor.csv are stored\n    :param window: (int) Smoothing window\n    :param algo: (str) name of the RL algo\n    :param title: (str) plot title\n    :param min_num_x: (int) Minimum number of episode/timesteps to keep an experiment (default: -1, no minimum)\n    :param timesteps: (bool) Plot timesteps instead of episodes\n    :param y_limits: ([float]) y-limits for the plot\n    :param output_file: (str) Path to a file where the plot data will be saved\n    :param no_display: (bool) Set to true, the plot won\'t be displayed (useful when only saving plot)\n    """"""\n    y_list = []\n    x_list = []\n    ok = False\n    for folder in folders:\n        if timesteps:\n            x, y = loadData(folder, smooth=1, bin_size=100)\n            if x is not None:\n                x, y = np.array(x), np.array(y)\n        else:\n            x, y = loadEpisodesData(folder)\n\n        if x is None or (min_num_x > 0 and y.shape[0] < min_num_x):\n            printYellow(""Skipping {}"".format(folder))\n            continue\n\n        if y.shape[0] <= window:\n            printYellow(""Folder {}"".format(folder))\n            printYellow(""Not enough episodes for current window size = {}"".format(window))\n            continue\n        ok = True\n        y = movingAverage(y, window)\n        y_list.append(y)\n\n        # Truncate x\n        x = x[len(x) - len(y):]\n        x_list.append(x)\n\n    if not ok:\n        printRed(""Not enough data to plot anything with current config."" +\n                 "" Consider decreasing --min-x"")\n        return\n\n    lengths = list(map(len, x_list))\n    min_x, max_x = np.min(lengths), np.max(lengths)\n\n    print(""Min x: {}"".format(min_x))\n    print(""Max x: {}"".format(max_x))\n\n    for i in range(len(x_list)):\n        x_list[i] = x_list[i][:min_x]\n        y_list[i] = y_list[i][:min_x]\n\n    x = np.array(x_list)[0]\n    y = np.array(y_list)\n\n    printGreen(""{} Experiments"".format(y.shape[0]))\n    print(""Min, Max rewards:"", np.min(y), np.max(y))\n\n    fig = plt.figure(title)\n    # Compute mean for different seeds\n    m = np.mean(y, axis=0)\n    # Compute standard error\n    s = np.squeeze(np.asarray(np.std(y, axis=0)))\n    n = y.shape[0]\n    plt.fill_between(x, m - s / np.sqrt(n), m + s / np.sqrt(n), color=lightcolors[0])\n    plt.plot(x, m, color=darkcolors[0], label=algo, linewidth=1)\n\n    if timesteps:\n        formatter = FuncFormatter(millions)\n        plt.xlabel(\'Number of Timesteps\')\n        fig.axes[0].xaxis.set_major_formatter(formatter)\n    else:\n        plt.xlabel(\'Number of Episodes\')\n    plt.ylabel(\'Rewards\')\n\n    plt.title(title, **fontstyle)\n    plt.ylim(y_limits)\n\n    plt.legend(framealpha=0.5, labelspacing=0.01, loc=\'lower right\', fontsize=16)\n\n    if output_file != """":\n        printGreen(""Saving aggregated data to {}.npz"".format(output_file))\n        np.savez(output_file, x=x, y=y)\n\n    if not no_display:\n        plt.show()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""Plot trained agent"")\n    parser.add_argument(\'-i\', \'--log-dir\', help=\'folder with the saved agent model\', type=str, required=True)\n    parser.add_argument(\'-o\', \'--output-file\', help=\'Where to save the aggregated data\', type=str, default="""")\n    parser.add_argument(\'--episode_window\', type=int, default=40,\n                        help=\'Episode window for moving average plot (default: 40)\')\n    parser.add_argument(\'--min-x\', type=int, default=-1,\n                        help=\'Minimum number of x-ticks to keep an experiment (default: -1, no minimum)\')\n    parser.add_argument(\'--y-lim\', nargs=2, type=float, default=[-1, -1], help=""limits for the y axis"")\n    parser.add_argument(\'--shape-reward\', action=\'store_true\', default=False,\n                        help=\'Shape the reward (reward = - distance) instead of a sparse reward\')\n    parser.add_argument(\'--timesteps\', action=\'store_true\', default=False,\n                        help=\'Plot timesteps instead of episodes\')\n    parser.add_argument(\'--no-display\', action=\'store_true\', default=False, help=\'Do not display plot\')\n    args = parser.parse_args()\n\n    y_limits = args.y_lim\n    if y_limits[0] == y_limits[1]:\n        if args.shape_reward:\n            y_limits = Y_LIM_SHAPED_REWARD\n        else:\n            y_limits = Y_LIM_SPARSE_REWARD\n        print(""Using default limits:"", y_limits)\n\n    # TODO: check that the parameters are the same between Experiments\n    folders = []\n    other = []\n    train_args = {}\n    for folder in os.listdir(args.log_dir):\n        path = ""{}/{}/"".format(args.log_dir, folder)\n        env_globals = json.load(open(path + ""env_globals.json"", \'r\'))\n        train_args = json.load(open(path + ""args.json"", \'r\'))\n        if train_args[""shape_reward""] == args.shape_reward:\n            folders.append(path)\n        else:\n            other.append(path)\n\n    if len(folders) == 0 and len(other) == 0:\n        printYellow(""No experiment found. Is the folder path {} correct?"".format(args.log_dir))\n        exit()\n    elif len(folders) == 0:\n        printYellow(""No experiments found with the given criterion. However {} experiments"".format(len(other)) +\n                    "" where found {} reward shaping. "".format(""without"" if args.shape_reward else ""with"") +\n                    ""Did you mean {} the flag \'--shape-reward\'?"".format(""without"" if args.shape_reward else ""with""))\n        exit()\n\n    srl_model = train_args[\'srl_model\'] if train_args[\'srl_model\'] != """" else ""raw pixels""\n    if args.timesteps:\n        title = srl_model + "" [Timesteps]""\n    else:\n        title = srl_model + "" [Episodes]""\n\n    plotGatheredExperiments(folders, train_args[\'algo\'], y_limits=y_limits, window=args.episode_window,\n                            title=title, min_num_x=args.min_x, no_display=args.no_display,\n                            timesteps=args.timesteps, output_file=args.output_file)\n'"
replay/compare_plots.py,0,"b'import argparse\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib.ticker import FuncFormatter\nfrom matplotlib import rc\n\nfrom replay.aggregate_plots import lightcolors, darkcolors, Y_LIM_SHAPED_REWARD, Y_LIM_SPARSE_REWARD, millions\nfrom srl_zoo.utils import printGreen, printRed\n\n# Init seaborn\nsns.set()\n# Style for the title\nfontstyle = {\'fontname\': \'DejaVu Sans\', \'fontsize\': 22, \'fontweight\': \'bold\'}\nrc(\'font\', weight=\'bold\')\n\ndef comparePlots(path, plots, y_limits, title=""Learning Curve"",\n                 timesteps=False, truncate_x=-1, no_display=False):\n    """"""\n    :param path: (str) path to the folder where the plots are stored\n    :param plots: ([str]) List of saved plots as npz file\n    :param y_limits: ([float]) y-limits for the plot\n    :param title: (str) plot title\n    :param timesteps: (bool) Plot timesteps instead of episodes\n    :param truncate_x: (int) Truncate the experiments after n ticks on the x-axis\n    :param no_display: (bool) Set to true, the plot won\'t be displayed (useful when only saving plot)\n    """"""\n    y_list = []\n    x_list = []\n    for plot in plots:\n        saved_plot = np.load(\'{}/{}\'.format(path, plot))\n        x_list.append(saved_plot[\'x\'])\n        y_list.append(saved_plot[\'y\'])\n\n    lengths = list(map(len, x_list))\n    min_x, max_x = np.min(lengths), np.max(lengths)\n\n    print(""Min x: {}"".format(min_x))\n    print(""Max x: {}"".format(max_x))\n\n    if truncate_x > 0:\n        min_x = min(truncate_x, min_x)\n    print(""Truncating the x-axis at {}"".format(min_x))\n\n    x = np.array(x_list[0][:min_x])\n\n    printGreen(""{} Experiments"".format(len(y_list)))\n    # print(""Min, Max rewards:"", np.min(y), np.max(y))\n\n    fig = plt.figure(title)\n    for i in range(len(y_list)):\n        label = plots[i].split(\'.npz\')[0]\n        y = y_list[i][:, :min_x]\n        print(\'{}: {} experiments\'.format(label, len(y)))\n        # Compute mean for different seeds\n        m = np.mean(y, axis=0)\n        # Compute standard error\n        s = np.squeeze(np.asarray(np.std(y, axis=0)))\n        n = y.shape[0]\n        plt.fill_between(x, m - s / np.sqrt(n), m + s / np.sqrt(n), color=lightcolors[i % len(lightcolors)], alpha=0.5)\n        plt.plot(x, m, color=darkcolors[i % len(darkcolors)], label=label, linewidth=2)\n\n    if timesteps:\n        formatter = FuncFormatter(millions)\n        plt.xlabel(\'Number of Timesteps\', fontsize=20, fontweight=\'bold\')\n        fig.axes[0].xaxis.set_major_formatter(formatter)\n    else:\n        plt.xlabel(\'Number of Episodes\')\n    plt.ylabel(\'Rewards\', fontsize=20, fontweight=\'bold\')\n\n    plt.title(title, **fontstyle)\n    plt.ylim(y_limits)\n\n    plt.legend(framealpha=0.8, frameon=True, labelspacing=0.01, loc=\'lower right\', fontsize=18)\n\n    if not no_display:\n        plt.show()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""Plot trained agent"")\n    parser.add_argument(\'-i\', \'--input-dir\', help=\'folder with the plots as npz files\', type=str, required=True)\n    parser.add_argument(\'-t\', \'--title\', help=\'Plot title\', type=str, default=\'Learning Curve\')\n    parser.add_argument(\'--episode_window\', type=int, default=40,\n                        help=\'Episode window for moving average plot (default: 40)\')\n    parser.add_argument(\'--shape-reward\', action=\'store_true\', default=False,\n                        help=\'Change the y_limit to correspond shaped reward bounds\')\n    parser.add_argument(\'--y-lim\', nargs=2, type=float, default=[-1, -1], help=""limits for the y axis"")\n    parser.add_argument(\'--truncate-x\', type=int, default=-1,\n                        help=""Truncate the experiments after n ticks on the x-axis (default: -1, no truncation)"")\n    parser.add_argument(\'--timesteps\', action=\'store_true\', default=False,\n                        help=\'Plot timesteps instead of episodes\')\n    parser.add_argument(\'--no-display\', action=\'store_true\', default=False, help=\'Do not display plot\')\n    args = parser.parse_args()\n\n    y_limits = args.y_lim\n    if y_limits[0] == y_limits[1]:\n        if args.shape_reward:\n            y_limits = Y_LIM_SHAPED_REWARD\n        else:\n            y_limits = Y_LIM_SPARSE_REWARD\n        print(""Using default limits:"", y_limits)\n\n    plots = [f for f in os.listdir(args.input_dir) if f.endswith(\'.npz\')]\n    plots.sort()\n\n    if len(plots) == 0:\n        printRed(""No npz files found in {}"".format(args.input_dir))\n        exit(-1)\n\n    comparePlots(args.input_dir, plots, title=args.title, y_limits=y_limits, no_display=args.no_display,\n                timesteps=args.timesteps, truncate_x=args.truncate_x)\n'"
replay/enjoy_baselines.py,0,"b'""""""\nEnjoy script for OpenAI Baselines\n""""""\nimport argparse\nimport json\nimport os\nfrom datetime import datetime\n\nimport yaml\nimport numpy as np\nimport tensorflow as tf\nfrom stable_baselines.common import set_global_seeds\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\nfrom rl_baselines import AlgoType\nfrom rl_baselines.registry import registered_rl\nfrom rl_baselines.utils import createTensorflowSession, computeMeanReward, WrapFrameStack, softmax\nfrom srl_zoo.utils import printYellow, printGreen\n# has to be imported here, as otherwise it will cause loading of undefined functions\nfrom environments import PlottingType\nfrom environments.registry import registered_env\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # used to remove debug info of tensorflow\n\n\ndef fixStateDim(states, min_state_dim=3):\n    """"""\n    Fix for plotting when state_dim < min_state_dim\n    :param states: (numpy array or [float])\n    :param min_state_dim: (int) the minimal dim needed for the plotting of the states\n    :return: (numpy array)\n    """"""\n    states = np.array(states)\n    state_dim = states.shape[1]\n    if state_dim < min_state_dim:\n        tmp = np.zeros((states.shape[0], min_state_dim))\n        tmp[:, :state_dim] = states\n        return tmp\n    return states\n\n\ndef parseArguments():\n    """"""\n\n    :return: (Arguments)\n    """"""\n    parser = argparse.ArgumentParser(description=""Load trained RL model"")\n    parser.add_argument(\'--seed\', type=int, default=0, help=\'random seed (default: 0)\')\n    parser.add_argument(\'--num-cpu\', help=\'Number of processes\', type=int, default=1)\n    parser.add_argument(\'--log-dir\', help=\'folder with the saved agent model\', type=str, required=True)\n    parser.add_argument(\'--num-timesteps\', type=int, default=int(1e4))\n    parser.add_argument(\'--render\', action=\'store_true\', default=False,\n                        help=\'Render the environment (show the GUI)\')\n    parser.add_argument(\'--shape-reward\', action=\'store_true\', default=False,\n                        help=\'Shape the reward (reward = - distance) instead of a sparse reward\')\n    parser.add_argument(\'--plotting\', action=\'store_true\', default=False,\n                        help=\'display in the latent space the current observation.\')\n    parser.add_argument(\'--action-proba\', action=\'store_true\', default=False,\n                        help=\'display the probability of actions\')\n    return parser.parse_args()\n\n\ndef loadConfigAndSetup(load_args):\n    """"""\n    Get the training config and setup the parameters\n    :param load_args: (Arguments)\n    :return: (dict, str, str, str, dict)\n    """"""\n    algo_name = """"\n    for algo in list(registered_rl.keys()):\n        if algo in load_args.log_dir:\n            algo_name = algo\n            break\n    algo_class, algo_type, _ = registered_rl[algo_name]\n    if algo_type == AlgoType.OTHER:\n        raise ValueError(algo_name + "" is not supported for replay"")\n    printGreen(""\\n"" + algo_name + ""\\n"")\n\n    load_path = ""{}/{}_model.pkl"".format(load_args.log_dir, algo_name)\n\n    env_globals = json.load(open(load_args.log_dir + ""env_globals.json"", \'r\'))\n    train_args = json.load(open(load_args.log_dir + ""args.json"", \'r\'))\n\n    env_kwargs = {\n        ""renders"": load_args.render,\n        ""shape_reward"": load_args.shape_reward,  # Reward sparse or shaped\n        ""action_joints"": train_args[""action_joints""],\n        ""is_discrete"": not train_args[""continuous_actions""],\n        ""random_target"": train_args.get(\'random_target\', False),\n        ""srl_model"": train_args[""srl_model""]\n    }\n\n    # load it, if it was defined\n    if ""action_repeat"" in env_globals:\n        env_kwargs[""action_repeat""] = env_globals[\'action_repeat\']\n\n    # Remove up action\n    if train_args[""env""] == ""Kuka2ButtonGymEnv-v0"":\n        env_kwargs[""force_down""] = env_globals.get(\'force_down\', True)\n    else:\n        env_kwargs[""force_down""] = env_globals.get(\'force_down\', False)\n\n    srl_model_path = None\n    if train_args[""srl_model""] != ""raw_pixels"":\n        train_args[""policy""] = ""mlp""\n        path = env_globals.get(\'srl_model_path\')\n\n        if path is not None:\n            env_kwargs[""use_srl""] = True\n            # Check that the srl saved model exists on the disk\n            assert os.path.isfile(env_globals[\'srl_model_path\']), ""{} does not exist"".format(env_globals[\'srl_model_path\'])\n            srl_model_path = env_globals[\'srl_model_path\']\n            env_kwargs[""srl_model_path""] = srl_model_path\n\n    return train_args, load_path, algo_name, algo_class, srl_model_path, env_kwargs\n\n\ndef createEnv(load_args, train_args, algo_name, algo_class, env_kwargs, log_dir=""/tmp/gym/test/""):\n    """"""\n    Create the Gym environment\n    :param load_args: (Arguments)\n    :param train_args: (dict)\n    :param algo_name: (str)\n    :param algo_class: (Class) a BaseRLObject subclass\n    :param env_kwargs: (dict) The extra arguments for the environment\n    :param log_dir: (str) Log dir for testing the agent\n    :return: (str, SubprocVecEnv)\n    """"""\n    # Log dir for testing the agent\n    log_dir += ""{}/{}/"".format(algo_name, datetime.now().strftime(""%y-%m-%d_%Hh%M_%S""))\n    os.makedirs(log_dir, exist_ok=True)\n\n    args = {\n        ""env"": train_args[\'env\'],\n        ""seed"": load_args.seed,\n        ""num_cpu"": load_args.num_cpu,\n        ""num_stack"": train_args[""num_stack""],\n        ""srl_model"": train_args[""srl_model""],\n        ""algo_type"": train_args.get(\'algo_type\', None),\n        ""log_dir"": log_dir\n    }\n    algo_args = type(\'attrib_dict\', (), args)()  # anonymous class so the dict looks like Arguments object\n    envs = algo_class.makeEnv(algo_args, env_kwargs=env_kwargs, load_path_normalise=load_args.log_dir)\n\n    return log_dir, envs, algo_args\n\n\ndef main():\n    load_args = parseArguments()\n    train_args, load_path, algo_name, algo_class, srl_model_path, env_kwargs = loadConfigAndSetup(load_args)\n    log_dir, envs, algo_args = createEnv(load_args, train_args, algo_name, algo_class, env_kwargs)\n\n    assert (not load_args.plotting and not load_args.action_proba)\\\n        or load_args.num_cpu == 1, ""Error: cannot run plotting with more than 1 CPU""\n\n    tf.reset_default_graph()\n    set_global_seeds(load_args.seed)\n    # createTensorflowSession()\n\n    printYellow(""Compiling Policy function...."")\n    method = algo_class.load(load_path, args=algo_args)\n\n    dones = [False for _ in range(load_args.num_cpu)]\n    # HACK: check for custom vec env by checking if the last wrapper is WrapFrameStack\n    # this is used for detecting algorithms that have a similar wrapping to deepq\n    # is considered a hack because we are unable to detect if this wrapper was added earlier to the environment object\n    using_custom_vec_env = isinstance(envs, WrapFrameStack)\n\n    obs = envs.reset()\n    if using_custom_vec_env:\n        obs = obs.reshape((1,) + obs.shape)\n\n    # plotting init\n    if load_args.plotting:\n        plt.pause(0.1)\n        fig = plt.figure()\n        old_obs = []\n        if registered_env[train_args[""env""]][2] == PlottingType.PLOT_3D:\n            ax = fig.add_subplot(111, projection=\'3d\')\n            line, = ax.plot([], [], [], c=[1, 0, 0, 1], label=""episode 0"")\n            point = ax.scatter([0], [0], [0], c=[1, 0, 0, 1])\n            min_zone = [+np.inf, +np.inf, +np.inf]\n            max_zone = [-np.inf, -np.inf, -np.inf]\n            amplitude = [0, 0, 0]\n            min_state_dim = 3\n        else:\n            ax = fig.add_subplot(111)\n            line, = ax.plot([], [], c=[1, 0, 0, 1], label=""episode 0"")\n            point = ax.scatter([0], [0], c=[1, 0, 0, 1])\n            min_zone = [+np.inf, +np.inf]\n            max_zone = [-np.inf, -np.inf]\n            amplitude = [0, 0]\n            min_state_dim = 2\n        fig.legend()\n\n        if train_args[""srl_model""] in [""ground_truth"", ""supervised""]:\n            delta_obs = [envs.get_original_obs()[0]]\n        else:\n            # we need to rebuild the PCA representation, in order to visualize correctly in 3D\n            # load the saved representations\n            path = srl_model_path.split(""/"")[:-1] + ""/image_to_state.json""\n            X = np.array(list(json.load(open(path, \'r\')).values()))\n\n            X = fixStateDim(X, min_state_dim=min_state_dim)\n\n            # estimate the PCA\n            if registered_env[train_args[""env""]][2] == PlottingType.PLOT_3D:\n                pca = PCA(n_components=3)\n            else:\n                pca = PCA(n_components=2)\n            pca.fit(X)\n            delta_obs = [pca.transform(fixStateDim([obs[0]], min_state_dim=min_state_dim))[0]]\n        plt.pause(0.00001)\n\n    # check if the algorithm has a defined getActionProba function before allowing action_proba plotting\n    if load_args.action_proba:\n        if not hasattr(method, ""getActionProba""):\n            printYellow(""Warning: requested flag --action-proba, ""\n                        ""but the algorihtm {} does not implement \'getActionProba\'"".format(algo_name))\n        else:\n            fig_prob = plt.figure()\n            ax_prob = fig_prob.add_subplot(111)\n            old_obs = []\n            if train_args[""continuous_actions""]:\n                ax_prob.set_ylim(np.min(envs.action_space.low), np.max(envs.action_space.high))\n                bar = ax_prob.bar(np.arange(np.prod(envs.action_space.shape)),\n                                  np.array([0] * np.prod(envs.action_space.shape)),\n                                  color=plt.get_cmap(\'viridis\')(int(1 / np.prod(envs.action_space.shape) * 255)))\n            else:\n                ax_prob.set_ylim(0, 1)\n                bar = ax_prob.bar(np.arange(envs.action_space.n), np.array([0] * envs.action_space.n),\n                                  color=plt.get_cmap(\'viridis\')(int(1 / envs.action_space.n * 255)))\n            plt.pause(1)\n            background_prob = fig_prob.canvas.copy_from_bbox(ax_prob.bbox)\n\n    n_done = 0\n    last_n_done = 0\n    episode = 0\n    for i in range(load_args.num_timesteps):\n        actions = method.getAction(obs, dones)\n        obs, rewards, dones, _ = envs.step(actions)\n        if using_custom_vec_env:\n            obs = obs.reshape((1,) + obs.shape)\n\n        # plotting\n        if load_args.plotting:\n            if train_args[""srl_model""] in [""ground_truth"", ""supervised""]:\n                ajusted_obs = envs.get_original_obs()[0]\n            else:\n                ajusted_obs = pca.transform(fixStateDim([obs[0]], min_state_dim=min_state_dim))[0]\n\n            # create a new line, if the episode is finished\n            if np.sum(dones) > 0:\n                old_obs.append(np.array(delta_obs))\n                line.set_c(sns.color_palette()[episode % len(sns.color_palette())])\n                episode += 1\n                if registered_env[train_args[""env""]][2] == PlottingType.PLOT_3D:\n                    line, = ax.plot([], [], [], c=[1, 0, 0, 1], label=""episode "" + str(episode))\n                else:\n                    line, = ax.plot([], [], c=[1, 0, 0, 1], label=""episode "" + str(episode))\n                fig.legend()\n                delta_obs = [ajusted_obs]\n            else:\n                delta_obs.append(ajusted_obs)\n\n            coor_plt = fixStateDim(np.array(delta_obs), min_state_dim=min_state_dim)[1:]\n            unstack_val = coor_plt.shape[1] // train_args.get(""num_stack"", 1)\n            coor_plt = coor_plt[:, -unstack_val:]\n\n            # updating the 3d vertices for the line and the dot drawing, to avoid redrawing the entire image\n            if registered_env[train_args[""env""]][2] == PlottingType.PLOT_3D:\n                line._verts3d = (coor_plt[:, 0], coor_plt[:, 1], coor_plt[:, 2])\n                point._offsets3d = (coor_plt[-1:, 0], coor_plt[-1:, 1], coor_plt[-1:, 2])\n                if coor_plt.shape[0] > 0:\n                    min_zone = np.minimum(np.amin(coor_plt, axis=0), min_zone)\n                    max_zone = np.maximum(np.amax(coor_plt, axis=0), max_zone)\n                    amplitude = max_zone - min_zone + 1e-10\n                ax.set_xlim(min_zone[0] - abs(amplitude[0] * 0.2), max_zone[0] + abs(amplitude[0] * 0.2))\n                ax.set_ylim(min_zone[1] - abs(amplitude[1] * 0.2), max_zone[1] + abs(amplitude[1] * 0.2))\n                ax.set_zlim(min_zone[2] - abs(amplitude[2] * 0.2), max_zone[2] + abs(amplitude[2] * 0.2))\n            else:\n                line.set_xdata(coor_plt[:, 0])\n                line.set_ydata(coor_plt[:, 1])\n                point._offsets = coor_plt[-1:, :]\n                if coor_plt.shape[0] > 0:\n                    min_zone = np.minimum(np.amin(coor_plt, axis=0), min_zone)\n                    max_zone = np.maximum(np.amax(coor_plt, axis=0), max_zone)\n                    amplitude = max_zone - min_zone + 1e-10\n                ax.set_xlim(min_zone[0] - abs(amplitude[0] * 0.2), max_zone[0] + abs(amplitude[0] * 0.2))\n                ax.set_ylim(min_zone[1] - abs(amplitude[1] * 0.2), max_zone[1] + abs(amplitude[1] * 0.2))\n\n            # Draw every 5 frames to avoid UI freezing\n            if i % 5 == 0:\n                fig.canvas.draw()\n                plt.pause(0.000001)\n\n        if load_args.action_proba and hasattr(method, ""getActionProba""):\n            # When continuous actions are needed, we cannot plot the action probability of every action\n            # in the action space, so we show the action directly instead\n            if train_args[""continuous_actions""]:\n                pi = method.getAction(obs, dones)\n            else:\n                pi = method.getActionProba(obs, dones)\n\n            fig_prob.canvas.restore_region(background_prob)\n            for act, rect in enumerate(bar):\n                if train_args[""continuous_actions""]:\n                    rect.set_height(pi[0][act])\n                    color_val = np.abs(pi[0][act]) / max(np.max(envs.action_space.high),\n                                                         np.max(np.abs(envs.action_space.low)))\n                else:\n                    rect.set_height(softmax(pi[0])[act])\n                    color_val = softmax(pi[0])[act]\n                rect.set_color(plt.get_cmap(\'viridis\')(int(color_val * 255)))\n                ax_prob.draw_artist(rect)\n            fig_prob.canvas.blit(ax_prob.bbox)\n\n        if using_custom_vec_env:\n            if dones:\n                obs = envs.reset()\n                obs = obs.reshape((1,) + obs.shape)\n\n        n_done += np.sum(dones)\n        if (n_done - last_n_done) > 1:\n            last_n_done = n_done\n            _, mean_reward = computeMeanReward(log_dir, n_done)\n            print(""{} episodes - Mean reward: {:.2f}"".format(n_done, mean_reward))\n\n    _, mean_reward = computeMeanReward(log_dir, n_done)\n    print(""{} episodes - Mean reward: {:.2f}"".format(n_done, mean_reward))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
replay/gather_results.py,0,"b'""""""\nCreate csv result from a folder of methods\n""""""\nfrom __future__ import print_function, division, absolute_import\n\nimport argparse\nimport json\nimport os\nimport glob\nfrom collections import OrderedDict\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind as welch_test\n\nfrom srl_zoo.utils import printYellow, printGreen\n\n\ndef runWelchTest(args, exp_results, methods, log_dir, ts_budget=None):\n    """"""\n    get the welch t test results\n\n    :param args: (parseArgument) the call arguments\n    :param exp_results: ({str, [object]} the experiment results\n    :param methods: ([str]) the methods found\n    :param log_dir: (str) the logging directory\n    :param ts_budget: (int) optional: define the timestep  budget\n    """"""\n    for welch_baseline in args.welch_test:\n        method_idx = None\n        for idx, name in enumerate(methods):\n            if name == welch_baseline:\n                method_idx = idx\n                break\n\n        if method_idx is None:\n            print(""the method {} was not found in the directory {}, will not performe welch test.""\n                  .format(welch_baseline, log_dir))\n        else:\n            welch_perf = []\n\n            if ts_budget is not None:\n                for rewards in exp_results[\'rewards_{}\'.format(ts_budget)]:\n                    welch_perf.append(\n                        welch_test(exp_results[\'rewards_{}\'.format(ts_budget)][method_idx], rewards)[1])\n                exp_results[""welch_{}_{}"".format(welch_baseline, ts_budget)] = welch_perf\n            else:\n                for rewards in exp_results[\'rewards\']:\n                    welch_perf.append(\n                        welch_test(exp_results[\'rewards\'][method_idx], rewards)[1])\n                exp_results[""welch_{}"".format(welch_baseline)] = welch_perf\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Create a report file for a given algo\')\n    parser.add_argument(\'-i\', \'--log-dir\', type=str, default="""", required=True,\n                        help=\'Path to a base log folder (environment level)\')\n    parser.add_argument(\'--timestep-budget\', type=int, nargs=\'+\',  default=[], help=\'the timesteps budget\')\n    parser.add_argument(\'--min-timestep\', type=int, default=None,\n                        help=\'the minimum timesteps for a monitoring to count\')\n    parser.add_argument(\'--episode-window\', type=int, default=100,\n                        help=\'The expected reward over the number of episodes\')\n    parser.add_argument(\'--welch-test\', type=str, nargs=\'+\', default=[],\n                        help=\'The name of the baseline you wish to compare with the welch test\')\n    args = parser.parse_args()\n\n    assert os.path.isdir(args.log_dir), ""--log-dir must be a path to a valid folder""\n\n    log_dir = args.log_dir\n\n    args.timestep_budget = sorted(list(set(args.timestep_budget)))\n\n    # Add here keys from exp_config.json that should be saved in the csv report file\n    exp_configs = [(\'srl_model_path\', [])]\n    if len(args.timestep_budget) > 0:\n        exp_results = []\n        for ts_budget in args.timestep_budget:\n            exp_results.append((\'mean_reward_{}\'.format(ts_budget), []))\n            exp_results.append((\'rewards_{}\'.format(ts_budget), []))\n            exp_results.append((\'stderr_reward_{}\'.format(ts_budget), []))\n    else:\n        exp_results = [(\'mean_reward\', []),\n                       (\'rewards\', []),\n                       (\'stderr_reward\', [])]\n\n    exp_configs = OrderedDict(exp_configs)\n    exp_results = OrderedDict(exp_results)\n\n    methods = []\n    algos = []\n\n    for method in os.listdir(log_dir):\n        algos_dir = ""{}/{}/"".format(log_dir, method)\n        if not os.path.isdir(algos_dir):\n            continue\n        for algo in os.listdir(algos_dir):\n            if not os.path.isdir(algos_dir + algo):\n                continue\n            algos.append(algo)\n            methods.append(method)\n            exp_dir = ""{}/{}/"".format(algos_dir, algo)\n\n            print(""Experiment dir:"", exp_dir)\n\n            data = [[] for _ in args.timestep_budget]  # the RL data for all the requested budgets\n            env_globals = None\n            valid_exp_folder = False\n            for exp_id, exp in enumerate(os.listdir(exp_dir)):\n                path = ""{}/{}/"".format(exp_dir, exp)\n\n                try:\n                    env_globals = json.load(open(path + ""/env_globals.json"", \'r\'))\n                    train_args = json.load(open(path + ""/args.json"", \'r\'))\n                    pass\n                except FileNotFoundError:\n                    printYellow(""config files not found for {}"".format(exp))\n                    continue\n\n                valid_exp_folder = True\n                run_acc = None  # the accumulated RL data for the run\n                monitor_files = sorted(glob.glob(path + ""/*.monitor.csv""))\n                for monitor_file in monitor_files:\n                    run = np.array(pd.read_csv(monitor_file, skiprows=1)[[""l"", ""r""]])\n\n                    if len(run) == 0:\n                        continue\n\n                    if run_acc is None:\n                        run_acc = run\n                    else:\n                        # helps with uneven runs\n                        if run.shape[0] < run_acc.shape[0]:\n                            run_acc = run_acc[:run.shape[0], :]\n                        run_acc += run[:run_acc.shape[0], :]\n\n                # make sure there is data here, and that there it is above the minimum timestep threashold\n                if run_acc is not None and (args.min_timestep is None or np.sum(run_acc[:, 0]) > args.min_timestep):\n                    run_acc[:, 1] = run_acc[:, 1] / len(monitor_files)\n                    run_acc[:, 0] = np.cumsum(run_acc[:, 0])\n\n                    if len(args.timestep_budget) > 0:  # extract the episodes for the requested budget\n                        for i, ts_budget in enumerate(args.timestep_budget):\n                            if np.all(run_acc[:, 0] < ts_budget):\n                                printYellow(""warning, budget too high for {} using {}, ""\n                                      ""the highest logged will be for {} timesteps.""\n                                      .format(algo, method, np.max(run_acc[:, 0])))\n                            budget_acc = run_acc[run_acc[:, 0] < ts_budget]\n\n                            if budget_acc.shape[0] == 0:\n                                printYellow(""budget too low for {} using {}"".format(algo, method))\n                                continue\n\n                            data[i].append(budget_acc[-args.episode_window:, 1])\n                    else:  # otherwise do for the highest value possible\n                        data.append(run_acc[-args.episode_window:, 1])\n\n\n            # Suppress them from results\n            if not valid_exp_folder:\n                algos.pop(-1)\n                methods.pop(-1)\n\n            if len(data) > 0 and env_globals is not None:\n                if len(args.timestep_budget) > 0:  # mean and std for every budget requested\n                    for i, ts_budget in enumerate(args.timestep_budget):\n                        mean_rew = np.mean(data[i])\n                        stderr_rew = np.std(data[i]) / np.sqrt(len(data[i]))\n                        exp_results[\'mean_reward_{}\'.format(ts_budget)].append(mean_rew)\n                        exp_results[\'rewards_{}\'.format(ts_budget)].append(data[i])\n                        exp_results[\'stderr_reward_{}\'.format(ts_budget)].append(stderr_rew)\n                else:\n                    mean_rew = np.mean(data)\n                    stderr_rew = np.std(data) / np.sqrt(len(data))\n                    exp_results[\'mean_reward\'].append(mean_rew)\n                    exp_results[\'rewards\'].append(data)\n                    exp_results[\'stderr_reward\'].append(stderr_rew)\n\n                for key in exp_configs.keys():\n                    exp_configs[key].append(env_globals.get(key, None))\n\n    if len(args.timestep_budget) > 0:\n        for ts_budget in args.timestep_budget:\n            runWelchTest(args, exp_results, methods, log_dir, ts_budget=ts_budget)\n    else:\n        runWelchTest(args, exp_results, methods, log_dir)\n\n    filtered_exp_results = [(k, v) for k, v in exp_results.items() if not k.startswith(""rewards"")]\n\n    if len(args.timestep_budget) > 0:\n        exp_results = OrderedDict(sorted(filtered_exp_results, key=lambda a: a[0].split(\'_\')[-1]))\n    else:\n        exp_results = OrderedDict(filtered_exp_results)\n\n    exp_configs.update({\'methods\': methods, \'rl_algo\': algos})\n    exp_configs.update(exp_results)\n    # Export to csv\n    result_df = pd.DataFrame(exp_configs)\n    result_df.to_csv(\'{}/results.csv\'.format(log_dir), sep="","", index=False)\n    printGreen(""Saved results to {}/results.csv"".format(log_dir))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
replay/plots.py,0,"b'""""""\nPlot past experiment in visdom\n""""""\nimport argparse\nimport json\n\nfrom visdom import Visdom\n\nfrom rl_baselines.visualize import timestepsPlot, episodePlot\n\nparser = argparse.ArgumentParser(description=""Plot trained agent using Visdom"")\nparser.add_argument(\'--log-dir\', help=\'folder with the saved agent model\', required=True)\nparser.add_argument(\'--episode_window\', type=int, default=40,\n                    help=\'Episode window for moving average plot (default: 40)\')\nargs = parser.parse_args()\n\nviz = Visdom()\n\nenv_globals = json.load(open(args.log_dir + ""env_globals.json"", \'r\'))\ntrain_args = json.load(open(args.log_dir + ""args.json"", \'r\'))\n\nsrl_model = train_args[\'srl_model\'] if train_args[\'srl_model\'] != """" else ""raw pixels""\nepisodePlot(viz, None, args.log_dir, train_args[\'env\'], train_args[\'algo\'],\n            title=srl_model + "" [Episodes]"", window=args.episode_window)\ntimestepsPlot(viz, None, args.log_dir, train_args[\'env\'], train_args[\'algo\'], title=srl_model)\n'"
rl_baselines/__init__.py,0,"b""from enum import Enum\n\n\nclass AlgoType(Enum):\n    REINFORCEMENT_LEARNING = 1\n    EVOLUTION_STRATEGIES = 2\n    OTHER = 3  # used to define other algorithms that can't be run in enjoy_baselines.py (ex: Random_agent)\n\n\nclass ActionType(Enum):\n    DISCRETE = 1\n    CONTINUOUS = 2\n\n"""
rl_baselines/base_classes.py,0,"b'import os\nimport pickle\n\nfrom stable_baselines.common.policies import CnnPolicy, CnnLstmPolicy, CnnLnLstmPolicy, MlpPolicy, MlpLstmPolicy, \\\n    MlpLnLstmPolicy\n\nfrom rl_baselines.utils import createEnvs\n\n\nclass BaseRLObject:\n    """"""\n    Base object for RL algorithms\n    """"""\n\n    # if callback frequency needs to be changed, overwrite this.\n    LOG_INTERVAL = 100  # log RL model performance every 100 steps\n    SAVE_INTERVAL = 200  # Save RL model every 200 steps\n\n    def __init__(self):\n        pass\n\n    def save(self, save_path, _locals=None):\n        """"""\n        Save the model to a path\n        :param save_path: (str)\n        :param _locals: (dict) local variable from callback, if present\n        """"""\n        raise NotImplementedError()\n\n    @classmethod\n    def load(cls, load_path, args=None):\n        """"""\n        Load the model from a path\n        :param load_path: (str)\n        :param args: (dict) the arguments used\n        :return: (BaseRLObject)\n        """"""\n        raise NotImplementedError()\n\n    def customArguments(self, parser):\n        """"""\n        Added arguments for training\n        :param parser: (ArgumentParser Object)\n        :return: (ArgumentParser Object)\n        """"""\n        raise NotImplementedError()\n\n    def getAction(self, observation, dones=None):\n        """"""\n        From an observation returns the associated action\n        :param observation: (numpy float)\n        :param dones: ([bool])\n        :return: (numpy float)\n        """"""\n        raise NotImplementedError()\n\n    @classmethod\n    def getOptParam(cls):\n        return None\n\n    @classmethod\n    def parserHyperParam(cls, hyperparam):\n        """"""\n        parses the hyperparameters into the expected type\n\n        :param hyperparam: (dict) the input hyperparameters (can be None)\n        :return: (dict) the parsed hyperparam dict (returns at least an empty dict)\n        """"""\n        opt_param = cls.getOptParam()\n        parsed_hyperparam = {}\n\n        if opt_param is not None and hyperparam is not None:\n            for name, val in hyperparam.items():\n                if name not in opt_param:\n                    raise AssertionError(""Error: hyperparameter {} not in list of valid hyperparameters"".format(name))\n                if isinstance(opt_param[name][0], tuple):\n                    parsed_hyperparam[name] = opt_param[name][0][1](val)\n                else:\n                    parsed_hyperparam[name] = opt_param[name][0](val)\n\n        return parsed_hyperparam\n\n    @classmethod\n    def makeEnv(cls, args, env_kwargs=None, load_path_normalise=None):\n        """"""\n        Makes an environment and returns it\n        :param args: (argparse.Namespace Object)\n        :param env_kwargs: (dict) The extra arguments for the environment\n        :param load_path_normalise: (str) the path to loading the rolling average, None if not available or wanted.\n        :return: (Gym env)\n        """"""\n        return createEnvs(args, env_kwargs=env_kwargs, load_path_normalise=load_path_normalise)\n\n    def train(self, args, callback, env_kwargs=None, train_kwargs=None):\n        """"""\n        Makes an environment and trains the model on it\n        :param args: (argparse.Namespace Object)\n        :param callback: (function)\n        :param env_kwargs: (dict) The extra arguments for the environment\n        :param train_kwargs: (dict) The list of all training agruments (used in hyperparameter search)\n        """"""\n        raise NotImplementedError()\n\n\nclass StableBaselinesRLObject(BaseRLObject):\n    """"""\n    Base object for the Stable Baselines RL algorithms\n    """"""\n\n    def __init__(self, name, model_class):\n        super(StableBaselinesRLObject, self).__init__()\n        self.name = name\n        self.model_class = model_class\n        self.model = None\n        self.states = None\n        self.ob_space = None\n        self.ac_space = None\n        self.policy = None\n        self.load_rl_model_path = None\n\n    def save(self, save_path, _locals=None):\n        """"""\n        Save the model to a path\n        :param save_path: (str)\n        :param _locals: (dict) local variable from callback, if present\n        """"""\n        assert self.model is not None, ""Error: must train or load model before use""\n        model_save_name = self.name + "".pkl""\n        if os.path.basename(save_path) == model_save_name:\n            model_save_name = self.name + ""_model.pkl""\n\n        self.model.save(os.path.dirname(save_path) + ""/"" + model_save_name)\n        save_param = {\n            ""ob_space"": self.ob_space,\n            ""ac_space"": self.ac_space,\n            ""policy"": self.policy\n        }\n        with open(save_path, ""wb"") as f:\n            pickle.dump(save_param, f)\n\n    def setLoadPath(self, load_path):\n        """"""\n        Set the path to later load the parameters of a trained rl model\n        :param load_path: (str)\n        :return: None\n        """"""\n        self.load_rl_model_path = load_path\n       \n    @classmethod\n    def load(cls, load_path, args=None):\n        """"""\n        Load the model from a path\n        :param load_path: (str)\n        :param args: (dict) the arguments used\n        :return: (BaseRLObject)\n        """"""\n        with open(load_path, ""rb"") as f:\n            save_param = pickle.load(f)\n\n        loaded_model = cls()\n        loaded_model.__dict__ = {**loaded_model.__dict__, **save_param}\n\n        model_save_name = loaded_model.name + "".pkl""\n        if os.path.basename(load_path) == model_save_name:\n            model_save_name = loaded_model.name + ""_model.pkl""\n\n        loaded_model.model = loaded_model.model_class.load(os.path.dirname(load_path) + ""/"" + model_save_name)\n        loaded_model.states = loaded_model.model.initial_state\n\n        return loaded_model\n\n    def customArguments(self, parser):\n        """"""\n        Added arguments for training\n        :param parser: (ArgumentParser Object)\n        :return: (ArgumentParser Object)\n        """"""\n        parser.add_argument(\'--policy\', help=\'Policy architecture\', choices=[\'feedforward\', \'lstm\', \'lnlstm\'],\n                            default=\'feedforward\')\n        return parser\n\n    def getActionProba(self, observation, dones=None):\n        """"""\n        From an observation returns the associated action probability\n        :param observation: (numpy float)\n        :param dones: ([bool])\n        :return: (numpy float)\n        """"""\n        assert self.model is not None, ""Error: must train or load model before use""\n        return self.model.action_probability(observation, self.states, dones)\n\n    def getAction(self, observation, dones=None):\n        """"""\n        From an observation returns the associated action\n        :param observation: (numpy float)\n        :param dones: ([bool])\n        :return: (numpy float)\n        """"""\n        assert self.model is not None, ""Error: must train or load model before use""\n        actions, self.states = self.model.predict(observation, self.states, dones)\n        return actions\n\n    @classmethod\n    def makeEnv(cls, args, env_kwargs=None, load_path_normalise=None):\n        """"""\n        Makes an environment and returns it\n        :param args: (argparse.Namespace Object)\n        :param env_kwargs: (dict) The extra arguments for the environment\n        :param load_path_normalise: (str) the path to loading the rolling average, None if not available or wanted.\n        :return: (Gym env)\n        """"""\n        return createEnvs(args, env_kwargs=env_kwargs, load_path_normalise=load_path_normalise)\n\n    def train(self, args, callback, env_kwargs=None, train_kwargs=None):\n        """"""\n        Makes an environment and trains the model on it\n        :param args: (argparse.Namespace Object)\n        :param callback: (function)\n        :param env_kwargs: (dict) The extra arguments for the environment\n        :param train_kwargs: (dict) The list of all training agruments (used in hyperparameter search)\n        """"""\n        envs = self.makeEnv(args, env_kwargs=env_kwargs)\n\n        if train_kwargs is None:\n            train_kwargs = {}\n\n        # get the associated policy for the architecture requested\n        if args.srl_model == ""raw_pixels"":\n            if args.policy == ""feedforward"":\n                args.policy = ""cnn""\n            else:\n                args.policy = ""cnn-"" + args.policy\n        else:\n            if args.policy == ""feedforward"":\n                args.policy = ""mlp""\n\n        self.policy = args.policy\n        self.ob_space = envs.observation_space\n        self.ac_space = envs.action_space\n\n        policy_fn = {\'cnn\': ""CnnPolicy"",\n                     \'cnn-lstm\': ""CnnLstmPolicy"",\n                     \'cnn-lnlstm\': ""CnnLnLstmPolicy"",\n                     \'mlp\': ""MlpPolicy"",\n                     \'lstm\': ""MlpLstmPolicy"",\n                     \'lnlstm\': ""MlpLnLstmPolicy""}[args.policy]\n        if self.load_rl_model_path is not None:\n            print(""Load trained model from the path: "", self.load_rl_model_path)\n            self.model = self.model_class.load(self.load_rl_model_path, envs, **train_kwargs)\n        else:\n            self.model = self.model_class(policy_fn, envs, **train_kwargs)\n        self.model.learn(total_timesteps=args.num_timesteps, seed=args.seed, callback=callback)\n        envs.close()\n'"
rl_baselines/hyperparam_search.py,0,"b'import argparse\nimport subprocess\nimport os\nimport shutil\nimport glob\nimport pprint\nimport math\nimport time\n\nimport pandas as pd\nimport numpy as np\nimport hyperopt\n\nfrom rl_baselines.registry import registered_rl\nfrom environments.registry import registered_env\nfrom state_representation.registry import registered_srl\nfrom srl_zoo.utils import printGreen\n\nITERATION_SCALE = 10000\nMIN_ITERATION = 30000\n\n\nclass HyperParameterOptimizer(object):\n    def __init__(self, opt_param, train, seed=0):\n        """"""\n        the base class for hyper parameter optimizer\n\n        :param opt_param: (dict) the parameters to optimize\n        :param train: (function (dict, int, int): float) the function that take:\n\n            - params: (dict) the hyper parameters to train with\n            - num_iters (int) the number of iterations to train (can be None)\n            - train_id: (int) the current iteration number in the hyperparameter search (can be None)\n            - returns: (float) the score of the training to minimize\n\n        :param seed: (int) the initial seed for the random number generator\n        """"""\n        self.opt_param = opt_param\n        self.train = train\n        self.seed = seed\n\n        self.history = []\n\n    def run(self):\n        """"""\n        run the hyper parameter search\n        """"""\n        raise NotImplementedError\n\n\nclass Hyperband(HyperParameterOptimizer):\n    def __init__(self, opt_param, train, seed=0, max_iter=100, eta=3.0):\n        """"""\n        A Hyperband implementation, it is similar to a targeted random search\n\n        Hyperband: https://arxiv.org/abs/1603.06560\n\n        :param opt_param: (dict) the parameters to optimize\n        :param train: (function (dict, int, int): float) the function that take:\n\n            - params: (dict) the hyper parameters to train with\n            - num_iters (int) the number of iterations to train (can be None)\n            - train_id: (int) the current iteration number in the hyperparameter search (can be None)\n            - returns: (float) the score of the training to minimize\n\n        :param seed: (int) the initial seed for the random number generator\n        :param max_iter: (int) the maximum budget for hyperband\'s search\n        :param eta: (float) the reduction factor of the search\n        """"""\n        super(Hyperband, self).__init__(opt_param, train, seed=seed)\n        self.max_iter = max_iter\n        self.eta = eta\n        self.max_steps = int(math.floor(math.log(self.max_iter) / math.log(self.eta)))\n        self.budget = (self.max_steps + 1) * self.max_iter\n\n        self.rng = np.random.RandomState(seed)\n        self.param_sampler = self._generate_sampler()\n\n    def _generate_sampler(self):\n        # will generate a hyperparameter sampler for Hyperband\n        def _sample():\n            params = {}\n            for name, (param_type, val) in self.opt_param.items():\n                if param_type == int:\n                    params[name] = self.rng.randint(val[0], val[1])\n                elif param_type == float:\n                    params[name] = self.rng.uniform(val[0], val[1])\n                elif isinstance(param_type, tuple) and param_type[0] == list:\n                    params[name] = val[self.rng.randint(len(val))]\n                else:\n                    raise AssertionError(""Error: unknown type {}"".format(param_type))\n\n            return params\n        return _sample\n\n    def run(self):\n        for step in reversed(range(self.max_steps + 1)):\n            max_n_param_sampled = int(math.ceil(self.budget / self.max_iter * self.eta**step / (step + 1)))\n            max_iters = self.max_iter * self.eta**(-step)\n\n            all_parameters = np.array([self.param_sampler() for _ in range(max_n_param_sampled)])\n            for i in range(step + 1):\n                printGreen(""\\npop_itt:{}/{}, itt:{}/{}, pop_size:{}"".format(self.max_steps - step, self.max_steps + 1,\n                                                                            i, step+1, len(all_parameters)))\n                n_param_sampled = int(math.floor(max_n_param_sampled * self.eta**(-i)))\n                num_iters = max_iters * self.eta**i\n                losses = [self.train(params, num_iters, train_id) for train_id, params in enumerate(all_parameters)]\n\n                self.history.extend(zip([(params, num_iters) for params in all_parameters], losses))\n                all_parameters = all_parameters[np.argsort(losses)[:int(math.floor(n_param_sampled / self.eta))]]\n\n        return self.history[int(np.argmin([val[1] for val in self.history]))]\n\n\nclass Hyperopt(HyperParameterOptimizer):\n    def __init__(self, opt_param, train, seed=0, num_eval=100):\n        """"""\n        A Hyperopt implementation, it is similar to a bayesian search\n\n        Hyperopt: https://www.lri.fr/~kegl/research/PDFs/BeBaBeKe11.pdf\n\n        :param opt_param: (dict) the parameters to optimize\n        :param train: (function (dict, int, int): float) the function that take:\n\n            - params: (dict) the hyper parameters to train with\n            - num_iters (int) the number of iterations to train (can be None)\n            - train_id: (int) the current iteration number in the hyperparameter search (can be None)\n            - returns: (float) the score of the training to minimize\n\n        :param seed: (int) the initial seed for the random number generator\n        :param num_eval: (int) the number of evaluation to do\n        """"""\n        super(Hyperopt, self).__init__(opt_param, train, seed=seed)\n        self.num_eval = num_eval\n        self.search_space = {}\n        for name, (param_type, val) in self.opt_param.items():\n            if param_type == int:\n                self.search_space[name] = hyperopt.hp.choice(name, np.arange(int(val[0]), int(val[1]), dtype=int))\n            elif param_type == float:\n                self.search_space[name] = hyperopt.hp.uniform(name, val[0], val[1])\n            elif isinstance(param_type, tuple) and param_type[0] == list:\n                self.search_space[name] = hyperopt.hp.choice(name, val)\n            else:\n                raise AssertionError(""Error: unknown type {}"".format(param_type))\n\n    def run(self):\n        trials = hyperopt.Trials()\n        hyperopt.fmin(fn=lambda kwargs: {\'loss\': self.train(kwargs), \'status\': hyperopt.STATUS_OK},\n                      space=self.search_space,\n                      algo=hyperopt.tpe.suggest,\n                      max_evals=self.num_eval,\n                      trials=trials,\n                      verbose=10)\n\n        # from the trials, get the values for every parameter\n        # set the number of iter to None as they are not changed in Hyperopt\n        # and zip the loss\n        self.history.extend(zip([(\n            {name: val[0] for name, val in params[""misc""][""vals""].items()}, None)\n            for params in trials.trials], trials.losses()))\n        return self.history[int(np.argmin([val[1] for val in self.history]))]\n\n\ndef makeRlTrainingFunction(args, train_args):\n    """"""\n    makes a training function for the hyperparam optimizers\n\n    :param args: (ArgumentParser) the optimizer arguments\n    :param train_args: (ArgumentParser) the remaining arguments\n    :return: (function (dict, int, int): float) the function that take:\n\n        - params: (dict) the hyper parameters to train with\n        - num_iters (int) the number of iterations to train (can be None)\n        - train_id: (int) the current iteration number in the hyperparameter search (can be None)\n        - returns: (float) the score of the training to minimize\n    """"""\n    if args.verbose:\n        # None here means stdout of terminal for subprocess.call\n        stdout = None\n    else:\n        stdout = open(os.devnull, \'w\')\n\n    def _train(params, num_iters=None, train_id=None):\n        # generate a print string\n        print_str = ""\\nID_num={}, ""\n        format_args = []\n        if train_id is None:\n            if not hasattr(_train, ""current_id""):\n                _train.current_id = 0\n            train_id = _train.current_id\n            _train.current_id += 1\n        format_args.append(train_id)\n        if num_iters is not None:\n            print_str += ""Num-timesteps={}, ""\n            format_args.append(int(max(MIN_ITERATION, num_iters * ITERATION_SCALE)))\n\n        print_str += ""Param:""\n        printGreen(print_str.format(*format_args))\n        pprint.pprint(params)\n\n        # cleanup old files\n        if os.path.exists(args.log_dir):\n            shutil.rmtree(args.log_dir)\n\n        # add the training args that where parsed for the hyperparam optimizers\n        if num_iters is not None:\n            loop_args = [\'--num-timesteps\', str(int(max(MIN_ITERATION, num_iters * ITERATION_SCALE)))]\n        else:\n            loop_args = [\'--num-timesteps\', str(int(args.num_timesteps))]\n\n        # redefine the hyperparam args for rl_baselines.train\n        if len(params) > 0:\n            loop_args.append(""--hyperparam"")\n            for param_name, param_val in params.items():\n                loop_args.append(""{}:{}"".format(param_name, param_val))\n\n        # call the training\n        ok = subprocess.call([\'python\', \'-m\', \'rl_baselines.train\'] + train_args + loop_args, stdout=stdout)\n        if ok != 0:\n            # throw the error down to the terminal\n            raise ChildProcessError(""An error occured, error code: {}"".format(ok))\n\n        # load the logging of the training, and extract the reward\n        folders = glob.glob(""{}/{}/{}/{}/*"".format(args.log_dir, args.env, args.srl_model, args.algo))\n        assert len(folders) != 0, ""Error: Could not find generated directory, halting {} search."".format(args.optimizer)\n        rewards = []\n        for monitor_path in glob.glob(folders[0] + ""/*.monitor.csv""):\n            rewards.append(np.mean(pd.read_csv(monitor_path, skiprows=1)[""r""][-10:]))\n        if np.isnan(rewards).any():\n            rewards = -np.inf\n        print(""reward: "", np.mean(rewards))\n\n        # negative reward, as we are minimizing with hyperparameter search\n        return -np.mean(rewards)\n    return _train\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""Hyperparameter search for implemented RL models"")\n    parser.add_argument(\'--optimizer\', default=\'hyperband\', choices=[\'hyperband\', \'hyperopt\'], type=str,\n                        help=\'The hyperparameter optimizer to choose from\')\n    parser.add_argument(\'--algo\', default=\'ppo2\', choices=list(registered_rl.keys()), help=\'OpenAI baseline to use\',\n                        type=str)\n    parser.add_argument(\'--env\', type=str, help=\'environment ID\', default=\'KukaButtonGymEnv-v0\',\n                        choices=list(registered_env.keys()))\n    parser.add_argument(\'--seed\', type=int, default=0, help=\'random seed (default: 0)\')\n    parser.add_argument(\'--srl-model\', type=str, default=\'raw_pixels\', choices=list(registered_srl.keys()),\n                        help=\'SRL model to use\')\n    parser.add_argument(\'--num-timesteps\', type=int, default=1e6, help=\'number of timesteps the baseline should run\')\n    parser.add_argument(\'-v\', \'--verbose\', action=\'store_true\', default=False, help=\'Display baseline STDOUT\')\n    parser.add_argument(\'--max-eval\', type=int, default=100, help=\'Number of evalutation to try for hyperopt\')\n\n    args, train_args = parser.parse_known_args()\n    args.log_dir = ""logs/_{}_search/"".format(args.optimizer)\n\n    train_args.extend([\'--srl-model\', args.srl_model, \'--seed\', str(args.seed), \'--algo\', args.algo, \'--env\', args.env,\n                       \'--log-dir\', args.log_dir, \'--no-vis\'])\n\n    # verify the algorithm has defined it, and that it returnes an expected value\n    try:\n        opt_param = registered_rl[args.algo][0].getOptParam()\n        assert opt_param is not None\n    except AttributeError or AssertionError:\n        raise AssertionError(""Error: {} algo does not support hyperparameter search."".format(args.algo))\n\n    if args.optimizer == ""hyperband"":\n        opt = Hyperband(opt_param, makeRlTrainingFunction(args, train_args), seed=args.seed,\n                        max_iter=args.num_timesteps // ITERATION_SCALE)\n    elif args.optimizer == ""hyperopt"":\n        opt = Hyperopt(opt_param, makeRlTrainingFunction(args, train_args), seed=args.seed, num_eval=args.max_eval)\n    else:\n        raise ValueError(""Error: optimizer {} was defined but not implemented, Halting."".format(args.optimizer))\n\n    t_start = time.time()\n    opt.run()\n    all_params, loss = zip(*opt.history)\n    idx = np.argmin(loss)\n    opt_params, nb_iter = all_params[idx]\n    reward = loss[idx]\n    print(\'\\ntime to run : {}s\'.format(int(time.time() - t_start)))\n    print(\'Total nb. evaluations : {}\'.format(len(all_params)))\n    if nb_iter is not None:\n        print(\'Best nb. of iterations : {}\'.format(int(nb_iter)))\n    print(\'Best params : \')\n    pprint.pprint(opt_params)\n    print(\'Best reward : {:.3f}\'.format(-reward))\n\n    param_dict, timesteps = zip(*all_params)\n    output = pd.DataFrame(list(param_dict))\n    # make sure we returned a timestep value to log, otherwise ignore\n    if not any([el is None for el in timesteps]):\n        output[""timesteps""] = np.array(np.maximum(MIN_ITERATION, np.array(timesteps) * ITERATION_SCALE).astype(int))\n    output[""reward""] = -np.array(loss)\n    output.to_csv(""logs/{}_{}_{}_{}_seed{}_numtimestep{}.csv""\n                  .format(args.optimizer, args.algo, args.env, args.srl_model, args.seed, args.num_timesteps))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
rl_baselines/pipeline.py,0,"b'""""""\nbaseline benchmark script for openAI RL Baselines\n""""""\nimport os\nimport argparse\nimport subprocess\n\nimport yaml\nimport numpy as np\n\nfrom rl_baselines.registry import registered_rl\nfrom environments.registry import registered_env\nfrom state_representation.registry import registered_srl\nfrom state_representation import SRLType\nfrom srl_zoo.utils import printGreen, printRed\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # used to remove debug info of tensorflow\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""OpenAI RL Baselines Benchmark"",\n                                     epilog=\'After the arguments are parsed, the rest are assumed to be arguments for\' +\n                                            \' rl_baselines.train\')\n    parser.add_argument(\'--algo\', type=str, default=\'ppo2\', help=\'OpenAI baseline to use\',\n                        choices=list(registered_rl.keys()))\n    parser.add_argument(\'--env\', type=str, nargs=\'+\', default=[""KukaButtonGymEnv-v0""], help=\'environment ID(s)\',\n                        choices=list(registered_env.keys()))\n    parser.add_argument(\'--srl-model\', type=str, nargs=\'+\', default=[""raw_pixels""], help=\'SRL model(s) to use\',\n                        choices=list(registered_srl.keys()))\n    parser.add_argument(\'--num-timesteps\', type=int, default=1e6, help=\'number of timesteps the baseline should run\')\n    parser.add_argument(\'-v\', \'--verbose\', action=\'store_true\', default=False, help=\'Display baseline STDOUT\')\n    parser.add_argument(\'--num-iteration\', type=int, default=15,\n                        help=\'number of time each algorithm should be run for each unique combination of environment \' +\n                             \' and srl-model.\')\n    parser.add_argument(\'--seed\', type=int, default=0,\n                        help=\'initial seed for each unique combination of environment and srl-model.\')\n    parser.add_argument(\'--srl-config-file\', type=str, default=""config/srl_models.yaml"",\n                        help=\'Set the location of the SRL model path configuration.\')\n\n    # returns the parsed arguments, and the rest are assumed to be arguments for rl_baselines.train\n    args, train_args = parser.parse_known_args()\n\n    # Sanity check\n    assert args.num_timesteps >= 1, ""Error: --num-timesteps cannot be less than 1""\n    assert args.num_iteration >= 1, ""Error: --num-iteration cannot be less than 1""\n\n    # Removing duplicates and sort\n    srl_models = list(set(args.srl_model))\n    envs = list(set(args.env))\n    srl_models.sort()\n    envs.sort()\n\n    # LOAD SRL models list\n    assert os.path.exists(args.srl_config_file), \\\n        ""Error: cannot load \\""--srl-config-file {}\\"", file not found!"".format(args.srl_config_file)\n    with open(args.srl_config_file, \'rb\') as f:\n        all_models = yaml.load(f)\n\n    # Checking definition and presence of all requested srl_models\n    valid = True\n    for env in envs:\n        # validated the env definition\n        if env not in all_models:\n            printRed(""Error: \'srl_models.yaml\' missing definition for environment {}"".format(env))\n            valid = False\n            continue  # skip to the next env, this one is not valid\n\n        # checking log_folder for current env\n        missing_log = ""log_folder"" not in all_models[env]\n        if missing_log:\n            printRed(""Error: \'srl_models.yaml\' missing definition for log_folder in environment {}"".format(env))\n            valid = False\n\n        # validate each model for the current env definition\n        for model in srl_models:\n            if registered_srl[model][0] == SRLType.ENVIRONMENT:\n                continue  # not an srl model, skip to the next model\n            elif model not in all_models[env]:\n                printRed(""Error: \'srl_models.yaml\' missing srl_model {} for environment {}"".format(model, env))\n                valid = False\n            elif (not missing_log) and (not os.path.exists(all_models[env][""log_folder""] + all_models[env][model])):\n                # checking presence of srl_model path, if and only if log_folder exists\n                printRed(""Error: srl_model {} for environment {} was defined in "".format(model, env) +\n                         ""\'srl_models.yaml\', however the file {} it was tagetting does not exist."".format(\n                             all_models[env][""log_folder""] + all_models[env][model]))\n                valid = False\n\n    assert valid, ""Errors occured due to malformed \'srl_models.yaml\', cannot continue.""\n\n    # check that all the SRL_models can be run on all the environments\n    valid = True\n    for env in envs:\n        for model in srl_models:\n            if registered_srl[model][1] is not None:\n                found = False\n                for compatible_class in registered_srl[model][1]:\n                    if issubclass(compatible_class, registered_env[env][0]):\n                        found = True\n                        break\n                if not found:\n                    valid = False\n                    printRed(""Error: srl_model {}, is not compatible with the {} environment."".format(model, env))\n    assert valid, ""Errors occured due to an incompatible combination of srl_model and environment, cannot continue.""\n\n    # the seeds used in training the baseline.\n    seeds = list(np.arange(args.num_iteration) + args.seed)\n\n    if args.verbose:\n        # None here means stdout of terminal for subprocess.call\n        stdout = None\n    else:\n        stdout = open(os.devnull, \'w\')\n\n    printGreen(""\\nRunning {} benchmarks {} times..."".format(args.algo, args.num_iteration))\n    print(""\\nSRL-Models:\\t{}"".format(srl_models))\n    print(""environments:\\t{}"".format(envs))\n    print(""verbose:\\t{}"".format(args.verbose))\n    print(""timesteps:\\t{}"".format(args.num_timesteps))\n    for model in srl_models:\n        for env in envs:\n            for i in range(args.num_iteration):\n\n                printGreen(\n                    ""\\nIteration_num={} (seed: {}), Environment=\'{}\', SRL-Model=\'{}\'"".format(i, seeds[i], env, model))\n\n                # redefine the parsed args for rl_baselines.train\n                loop_args = [\'--srl-model\', model, \'--seed\', str(seeds[i]), \'--algo\', args.algo, \'--env\', env,\n                             \'--num-timesteps\', str(int(args.num_timesteps)), \'--srl-config-file\', args.srl_config_file]\n\n                ok = subprocess.call([\'python\', \'-m\', \'rl_baselines.train\'] + train_args + loop_args, stdout=stdout)\n\n                if ok != 0:\n                    # throw the error down to the terminal\n                    raise ChildProcessError(""An error occured, error code: {}"".format(ok))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
rl_baselines/random_agent.py,0,"b'""""""\nRandom agent: randomly sample actions from the action space\n""""""\nimport time\n\nfrom rl_baselines.base_classes import BaseRLObject\n\n\nclass RandomAgentModel(BaseRLObject):\n    def __init__(self):\n        super(RandomAgentModel, self).__init__()\n\n    def save(self, save_path, _locals=None):\n        pass\n\n    @classmethod\n    def load(cls, load_path, args=None):\n        raise ValueError(""Error: loading a saved random agent is not useful"")\n\n    def customArguments(self, parser):\n        parser.add_argument(\'--num-cpu\', help=\'Number of processes\', type=int, default=1)\n        return parser\n\n    def getAction(self, observation, dones=None):\n        # Action space is not available here, so we do not support it.\n        raise ValueError(""Error: getAction is not supported for random agent."")\n\n    def train(self, args, callback, env_kwargs=None, train_kwargs=None):\n        env = self.makeEnv(args, env_kwargs=env_kwargs)\n\n        obs = env.reset()\n        num_updates = int(args.num_timesteps) // args.num_cpu\n        start_time = time.time()\n\n        for step in range(num_updates):\n            actions = [env.action_space.sample() for _ in range(args.num_cpu)]\n            obs, reward, done, info = env.step(actions)\n            if callback is not None:\n                callback(locals(), globals())\n            if (step + 1) % 500 == 0:\n                total_steps = step * args.num_cpu\n                print(""{} steps - {:.2f} FPS"".format(total_steps, total_steps / (time.time() - start_time)))\n'"
rl_baselines/registry.py,0,"b'from rl_baselines import AlgoType, ActionType\nfrom rl_baselines.base_classes import BaseRLObject\nfrom rl_baselines.rl_algorithm.a2c import A2CModel\nfrom rl_baselines.rl_algorithm.acer import ACERModel\nfrom rl_baselines.rl_algorithm.acktr import ACKTRModel\nfrom rl_baselines.evolution_strategies.ars import ARSModel\nfrom rl_baselines.evolution_strategies.cma_es import CMAESModel\nfrom rl_baselines.rl_algorithm.ddpg import DDPGModel\nfrom rl_baselines.rl_algorithm.deepq import DQNModel\nfrom rl_baselines.rl_algorithm.ppo1 import PPO1Model\nfrom rl_baselines.rl_algorithm.ppo2 import PPO2Model\nfrom rl_baselines.random_agent import RandomAgentModel\nfrom rl_baselines.rl_algorithm.sac import SACModel\nfrom rl_baselines.rl_algorithm.trpo import TRPOModel\n\n# Register, name: (algo class, algo type, list of action types)\nregistered_rl = {\n    ""a2c"":          (A2CModel, AlgoType.REINFORCEMENT_LEARNING, [ActionType.DISCRETE, ActionType.CONTINUOUS]),\n    ""acer"":         (ACERModel, AlgoType.REINFORCEMENT_LEARNING, [ActionType.DISCRETE]),\n    ""acktr"":        (ACKTRModel, AlgoType.REINFORCEMENT_LEARNING, [ActionType.DISCRETE]),\n    ""ars"":          (ARSModel, AlgoType.EVOLUTION_STRATEGIES, [ActionType.DISCRETE, ActionType.CONTINUOUS]),\n    ""cma-es"":       (CMAESModel, AlgoType.EVOLUTION_STRATEGIES, [ActionType.DISCRETE, ActionType.CONTINUOUS]),\n    ""ddpg"":         (DDPGModel, AlgoType.REINFORCEMENT_LEARNING, [ActionType.CONTINUOUS]),\n    ""deepq"":        (DQNModel, AlgoType.REINFORCEMENT_LEARNING, [ActionType.DISCRETE]),\n    ""ppo1"":         (PPO1Model, AlgoType.REINFORCEMENT_LEARNING, [ActionType.DISCRETE, ActionType.CONTINUOUS]),\n    ""ppo2"":         (PPO2Model, AlgoType.REINFORCEMENT_LEARNING, [ActionType.DISCRETE, ActionType.CONTINUOUS]),\n    ""random_agent"": (RandomAgentModel, AlgoType.OTHER, [ActionType.DISCRETE, ActionType.CONTINUOUS]),\n    ""sac"":          (SACModel, AlgoType.REINFORCEMENT_LEARNING, [ActionType.CONTINUOUS]),\n    ""trpo"":         (TRPOModel, AlgoType.REINFORCEMENT_LEARNING, [ActionType.DISCRETE, ActionType.CONTINUOUS])\n}\n\n# Checking validity of the registered RL algorithms\nfor _, val in registered_rl.items():\n    assert issubclass(val[0], BaseRLObject), ""Error: tried to load {} as a BaseRLObject"".format(val[0])\n'"
rl_baselines/train.py,0,"b'""""""\nTrain script for RL algorithms\n""""""\nimport argparse\nimport inspect\nimport json\nimport os\nimport sys\nimport time\nfrom datetime import datetime\nfrom pprint import pprint\n\nimport yaml\nfrom stable_baselines.common import set_global_seeds\nfrom visdom import Visdom\n\nfrom environments.registry import registered_env\nfrom environments.srl_env import SRLGymEnv\nfrom rl_baselines import AlgoType, ActionType\nfrom rl_baselines.registry import registered_rl\nfrom rl_baselines.utils import computeMeanReward\nfrom rl_baselines.utils import filterJSONSerializableObjects\nfrom rl_baselines.visualize import timestepsPlot, episodePlot\nfrom srl_zoo.utils import printGreen, printYellow\nfrom state_representation import SRLType\nfrom state_representation.registry import registered_srl\n\nVISDOM_PORT = 8097\nLOG_INTERVAL = 0  # initialised during loading of the algorithm\nLOG_DIR = """"\nALGO = None\nALGO_NAME = """"\nENV_NAME = """"\nPLOT_TITLE = """"\nEPISODE_WINDOW = 40  # For plotting moving average\nviz = None\nn_steps = 0\nSAVE_INTERVAL = 0  # initialised during loading of the algorithm\nN_EPISODES_EVAL = 100  # Evaluate the performance on the last 100 episodes\nMIN_EPISODES_BEFORE_SAVE = 100  # Number of episodes to train on before saving best model\nparams_saved = False\nbest_mean_reward = -10000\n\nwin, win_smooth, win_episodes = None, None, None\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # used to remove debug info of tensorflow\n\n\ndef saveEnvParams(kuka_env_globals, env_kwargs):\n    """"""\n    :param kuka_env_globals: (dict)\n    :param env_kwargs: (dict) The extra arguments for the environment\n    """"""\n    params = filterJSONSerializableObjects({**kuka_env_globals, **env_kwargs})\n    with open(LOG_DIR + ""env_globals.json"", ""w"") as f:\n        json.dump(params, f)\n\n\ndef latestPath(path):\n    """"""\n    :param path: path to the log folder (defined in srl_model.yaml) (str)\n    :return: path to latest learned model in the same dataset folder (str)\n    """"""\n    return max(\n        [path + ""/"" + d for d in os.listdir(path) if not d.startswith(\'baselines\') and os.path.isdir(path + ""/"" + d)],\n        key=os.path.getmtime) + \'/srl_model.pth\'\n\n\ndef configureEnvAndLogFolder(args, env_kwargs, all_models):\n    """"""\n    :param args: (ArgumentParser object)\n    :param env_kwargs: (dict) The extra arguments for the environment\n    :param all_models: (dict) The location of all the trained SRL models\n    :return: (ArgumentParser object, dict)\n    """"""\n    global PLOT_TITLE, LOG_DIR\n    # Reward sparse or shaped\n    env_kwargs[""shape_reward""] = args.shape_reward\n    # Actions in joint space or relative position space\n    env_kwargs[""action_joints""] = args.action_joints\n    args.log_dir += args.env + ""/""\n\n    models = all_models[args.env]\n    PLOT_TITLE = args.srl_model\n    path = models.get(args.srl_model)\n    args.log_dir += args.srl_model + ""/""\n\n    env_kwargs[""srl_model""] = args.srl_model\n    if registered_srl[args.srl_model][0] == SRLType.SRL:\n        env_kwargs[""use_srl""] = True\n        if args.latest:\n            printYellow(""Using latest srl model in {}"".format(models[\'log_folder\']))\n            env_kwargs[""srl_model_path""] = latestPath(models[\'log_folder\'])\n        else:\n            assert path is not None, ""Error: SRL path not defined for {} in {}"".format(args.srl_model,\n                                                                                       args.srl_config_file)\n            # Path depending on whether to load the latest model or not\n            srl_model_path = models[\'log_folder\'] + path\n            env_kwargs[""srl_model_path""] = srl_model_path\n\n    # Add date + current time\n    args.log_dir += ""{}/{}/"".format(ALGO_NAME, datetime.now().strftime(""%y-%m-%d_%Hh%M_%S""))\n    LOG_DIR = args.log_dir\n    # wait one second if the folder exist to avoid overwritting logs\n    time.sleep(1)\n    os.makedirs(args.log_dir, exist_ok=True)\n\n    return args, env_kwargs\n\n\ndef callback(_locals, _globals):\n    """"""\n    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n    :param _locals: (dict)\n    :param _globals: (dict)\n    """"""\n    global win, win_smooth, win_episodes, n_steps, viz, params_saved, best_mean_reward\n    # Create vizdom object only if needed\n    if viz is None:\n        viz = Visdom(port=VISDOM_PORT)\n\n    is_es = registered_rl[ALGO_NAME][1] == AlgoType.EVOLUTION_STRATEGIES\n\n    # Save RL agent parameters\n    if not params_saved:\n        # Filter locals\n        params = filterJSONSerializableObjects(_locals)\n        with open(LOG_DIR + ""rl_locals.json"", ""w"") as f:\n            json.dump(params, f)\n        params_saved = True\n\n    # Save the RL model if it has improved\n    if (n_steps + 1) % SAVE_INTERVAL == 0:\n        # Evaluate network performance\n        ok, mean_reward = computeMeanReward(LOG_DIR, N_EPISODES_EVAL, is_es=is_es, return_n_episodes=True)\n        if ok:\n            # Unpack mean reward and number of episodes\n            mean_reward, n_episodes = mean_reward\n            print(\n                ""Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}"".format(best_mean_reward, mean_reward))\n        else:\n            # Not enough episode\n            mean_reward = -10000\n            n_episodes = 0\n\n        # Save Best model\n        if mean_reward > best_mean_reward and n_episodes >= MIN_EPISODES_BEFORE_SAVE:\n            # Try saving the running average (only valid for mlp policy)\n            try:\n                if \'env\' in _locals:\n                    _locals[\'env\'].save_running_average(LOG_DIR)\n                else:\n                    _locals[\'self\'].env.save_running_average(LOG_DIR)\n            except AttributeError:\n                pass\n\n            best_mean_reward = mean_reward\n            printGreen(""Saving new best model"")\n            ALGO.save(LOG_DIR + ALGO_NAME + ""_model.pkl"", _locals)\n\n    # Plots in visdom\n    if viz and (n_steps + 1) % LOG_INTERVAL == 0:\n        win = timestepsPlot(viz, win, LOG_DIR, ENV_NAME, ALGO_NAME, bin_size=1, smooth=0, title=PLOT_TITLE, is_es=is_es)\n        win_smooth = timestepsPlot(viz, win_smooth, LOG_DIR, ENV_NAME, ALGO_NAME, title=PLOT_TITLE + "" smoothed"",\n                                   is_es=is_es)\n        win_episodes = episodePlot(viz, win_episodes, LOG_DIR, ENV_NAME, ALGO_NAME, window=EPISODE_WINDOW,\n                                   title=PLOT_TITLE + "" [Episodes]"", is_es=is_es)\n    n_steps += 1\n    return True\n\n\ndef main():\n    # Global variables for callback\n    global ENV_NAME, ALGO, ALGO_NAME, LOG_INTERVAL, VISDOM_PORT, viz\n    global SAVE_INTERVAL, EPISODE_WINDOW, MIN_EPISODES_BEFORE_SAVE\n    parser = argparse.ArgumentParser(description=""Train script for RL algorithms"")\n    parser.add_argument(\'--algo\', default=\'ppo2\', choices=list(registered_rl.keys()), help=\'RL algo to use\',\n                        type=str)\n    parser.add_argument(\'--env\', type=str, help=\'environment ID\', default=\'KukaButtonGymEnv-v0\',\n                        choices=list(registered_env.keys()))\n    parser.add_argument(\'--seed\', type=int, default=0, help=\'random seed (default: 0)\')\n    parser.add_argument(\'--episode_window\', type=int, default=40,\n                        help=\'Episode window for moving average plot (default: 40)\')\n    parser.add_argument(\'--log-dir\', default=\'/tmp/gym/\', type=str,\n                        help=\'directory to save agent logs and model (default: /tmp/gym)\')\n    parser.add_argument(\'--num-timesteps\', type=int, default=int(1e6))\n    parser.add_argument(\'--srl-model\', type=str, default=\'raw_pixels\', choices=list(registered_srl.keys()),\n                        help=\'SRL model to use\')\n    parser.add_argument(\'--num-stack\', type=int, default=1, help=\'number of frames to stack (default: 1)\')\n    parser.add_argument(\'--action-repeat\', type=int, default=1,\n                        help=\'number of times an action will be repeated (default: 1)\')\n    parser.add_argument(\'--port\', type=int, default=8097, help=\'visdom server port (default: 8097)\')\n    parser.add_argument(\'--no-vis\', action=\'store_true\', default=False, help=\'disables visdom visualization\')\n    parser.add_argument(\'--shape-reward\', action=\'store_true\', default=False,\n                        help=\'Shape the reward (reward = - distance) instead of a sparse reward\')\n    parser.add_argument(\'-c\', \'--continuous-actions\', action=\'store_true\', default=False)\n    parser.add_argument(\'-joints\', \'--action-joints\', action=\'store_true\', default=False,\n                        help=\'set actions to the joints of the arm directly, instead of inverse kinematics\')\n    parser.add_argument(\'-r\', \'--random-target\', action=\'store_true\', default=False,\n                        help=\'Set the button to a random position\')\n    parser.add_argument(\'--srl-config-file\', type=str, default=""config/srl_models.yaml"",\n                        help=\'Set the location of the SRL model path configuration.\')\n    parser.add_argument(\'--hyperparam\', type=str, nargs=\'+\', default=[])\n    parser.add_argument(\'--min-episodes-save\', type=int, default=100,\n                        help=""Min number of episodes before saving best model"")\n    parser.add_argument(\'--latest\', action=\'store_true\', default=False,\n                        help=\'load the latest learned model (location:srl_zoo/logs/DatasetName/)\')\n    parser.add_argument(\'--load-rl-model-path\', type=str, default=None,\n                        help=""load the trained RL model, should be with the same algorithm type"")\n    \n    # Ignore unknown args for now\n    args, unknown = parser.parse_known_args()\n    env_kwargs = {}\n\n    # LOAD SRL models list\n    assert os.path.exists(args.srl_config_file), \\\n        ""Error: cannot load \\""--srl-config-file {}\\"", file not found!"".format(args.srl_config_file)\n    with open(args.srl_config_file, \'rb\') as f:\n        all_models = yaml.load(f)\n\n    # Sanity check\n    assert args.episode_window >= 1, ""Error: --episode_window cannot be less than 1""\n    assert args.num_timesteps >= 1, ""Error: --num-timesteps cannot be less than 1""\n    assert args.num_stack >= 1, ""Error: --num-stack cannot be less than 1""\n    assert args.action_repeat >= 1, ""Error: --action-repeat cannot be less than 1""\n    assert 0 <= args.port < 65535, ""Error: invalid visdom port number {}, "".format(args.port) + \\\n                                   ""port number must be an unsigned 16bit number [0,65535].""\n    assert registered_srl[args.srl_model][0] == SRLType.ENVIRONMENT or args.env in all_models, \\\n        ""Error: the environment {} has no srl_model defined in \'srl_models.yaml\'. Cannot continue."".format(args.env)\n    # check that all the SRL_model can be run on the environment\n    if registered_srl[args.srl_model][1] is not None:\n        found = False\n        for compatible_class in registered_srl[args.srl_model][1]:\n            if issubclass(compatible_class, registered_env[args.env][0]):\n                found = True\n                break\n        assert found, ""Error: srl_model {}, is not compatible with the {} environment."".format(args.srl_model, args.env)\n\n    ENV_NAME = args.env\n    ALGO_NAME = args.algo\n    VISDOM_PORT = args.port\n    EPISODE_WINDOW = args.episode_window\n    MIN_EPISODES_BEFORE_SAVE = args.min_episodes_save\n\n    if args.no_vis:\n        viz = False\n\n    algo_class, algo_type, action_type = registered_rl[args.algo]\n    algo = algo_class()\n    ALGO = algo\n    \n\n    # if callback frequency needs to be changed\n    LOG_INTERVAL = algo.LOG_INTERVAL\n    SAVE_INTERVAL = algo.SAVE_INTERVAL\n\n    if not args.continuous_actions and ActionType.DISCRETE not in action_type:\n        raise ValueError(args.algo + "" does not support discrete actions, please use the \'--continuous-actions\' "" +\n                         ""(or \'-c\') flag."")\n    if args.continuous_actions and ActionType.CONTINUOUS not in action_type:\n        raise ValueError(args.algo + "" does not support continuous actions, please remove the \'--continuous-actions\' "" +\n                         ""(or \'-c\') flag."")\n\n    env_kwargs[""is_discrete""] = not args.continuous_actions\n\n    printGreen(""\\nAgent = {} \\n"".format(args.algo))\n\n    env_kwargs[""action_repeat""] = args.action_repeat\n    # Random init position for button\n    env_kwargs[""random_target""] = args.random_target\n    # Allow up action\n    # env_kwargs[""force_down""] = False\n\n    # allow multi-view\n    env_kwargs[\'multi_view\'] = args.srl_model == ""multi_view_srl""\n    parser = algo.customArguments(parser)\n    args = parser.parse_args()\n\n    args, env_kwargs = configureEnvAndLogFolder(args, env_kwargs, all_models)\n    args_dict = filterJSONSerializableObjects(vars(args))\n    # Save args\n    with open(LOG_DIR + ""args.json"", ""w"") as f:\n        json.dump(args_dict, f)\n\n    env_class = registered_env[args.env][0]\n    # env default kwargs\n    default_env_kwargs = {k: v.default\n                          for k, v in inspect.signature(env_class.__init__).parameters.items()\n                          if v is not None}\n\n    globals_env_param = sys.modules[env_class.__module__].getGlobals()\n\n    super_class = registered_env[args.env][1]\n    # reccursive search through all the super classes of the asked environment, in order to get all the arguments.\n    rec_super_class_lookup = {dict_class: dict_super_class for _, (dict_class, dict_super_class, _, _) in\n                              registered_env.items()}\n    while super_class != SRLGymEnv:\n        assert super_class in rec_super_class_lookup, ""Error: could not find super class of {}"".format(super_class) + \\\n                                                      "", are you sure \\""registered_env\\"" is correctly defined?""\n        super_env_kwargs = {k: v.default\n                            for k, v in inspect.signature(super_class.__init__).parameters.items()\n                            if v is not None}\n        default_env_kwargs = {**super_env_kwargs, **default_env_kwargs}\n\n        globals_env_param = {**sys.modules[super_class.__module__].getGlobals(), **globals_env_param}\n\n        super_class = rec_super_class_lookup[super_class]\n\n    # Print Variables\n    printYellow(""Arguments:"")\n    pprint(args_dict)\n    printYellow(""Env Globals:"")\n    pprint(filterJSONSerializableObjects({**globals_env_param, **default_env_kwargs, **env_kwargs}))\n    # Save env params\n    saveEnvParams(globals_env_param, {**default_env_kwargs, **env_kwargs})\n    # Seed tensorflow, python and numpy random generator\n    set_global_seeds(args.seed)\n    # Augment the number of timesteps (when using mutliprocessing this number is not reached)\n    args.num_timesteps = int(1.1 * args.num_timesteps)\n    # Get the hyperparameter, if given (Hyperband)\n    hyperparams = {param.split("":"")[0]: param.split("":"")[1] for param in args.hyperparam}\n    hyperparams = algo.parserHyperParam(hyperparams)\n    \n    if args.load_rl_model_path is not None:\n        #use a small learning rate\n        print(""use a small learning rate: {:f}"".format(1.0e-4))\n        hyperparams[""learning_rate""] = lambda f: f * 1.0e-4\n        \n    # Train the agent\n\n    if args.load_rl_model_path is not None:\n        algo.setLoadPath(args.load_rl_model_path)\n    algo.train(args, callback, env_kwargs=env_kwargs, train_kwargs=hyperparams)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
rl_baselines/utils.py,0,"b'from collections import OrderedDict\nfrom multiprocessing import Queue, Process\n\nimport numpy as np\nimport tensorflow as tf\nimport torch as th\nfrom stable_baselines.common.vec_env import VecEnv, VecNormalize, DummyVecEnv, SubprocVecEnv, VecFrameStack\n\nfrom environments import ThreadingType\nfrom environments.utils import makeEnv, dynamicEnvLoad\nfrom rl_baselines.visualize import loadCsv\nfrom srl_zoo.utils import printYellow, printGreen\nfrom state_representation.models import loadSRLModel, getSRLDim\n\n\ndef createTensorflowSession():\n    """"""\n    Create tensorflow session with specific argument\n    to prevent it from taking all gpu memory\n    """"""\n    # Let Tensorflow choose the device\n    config = tf.ConfigProto(allow_soft_placement=True)\n    # Prevent tensorflow from taking all the gpu memory\n    config.gpu_options.allow_growth = True\n    tf.Session(config=config).__enter__()\n\n\ndef computeMeanReward(log_dir, last_n_episodes, is_es=False, return_n_episodes=False):\n    """"""\n    Compute the mean reward for the last n episodes\n    :param log_dir: (str)\n    :param last_n_episodes: (int)\n    :param is_es: (bool)\n    :param return_n_episodes: (bool)\n    :return: (bool, numpy array or tuple when return_n_episodes is True)\n    """"""\n    result, _ = loadCsv(log_dir, is_es=is_es)\n    if len(result) == 0:\n        return False, 0\n    y = np.array(result)[:, 1]\n\n    if return_n_episodes:\n        return True, (y[-last_n_episodes:].mean(), len(y))\n    return True, y[-last_n_episodes:].mean()\n\n\ndef isJsonSafe(data):\n    """"""\n    Check if an object is json serializable\n    :param data: (python object)\n    :return: (bool)\n    """"""\n    if data is None:\n        return True\n    elif isinstance(data, (bool, int, float, str)):\n        return True\n    elif isinstance(data, (tuple, list)):\n        return all(isJsonSafe(x) for x in data)\n    elif isinstance(data, dict):\n        return all(isinstance(k, str) and isJsonSafe(v) for k, v in data.items())\n    return False\n\n\ndef filterJSONSerializableObjects(input_dict):\n    """"""\n    Filter and sort entries of a dictionnary\n    to save it as a json\n    :param input_dict: (dict)\n    :return: (OrderedDict)\n    """"""\n    output_dict = OrderedDict()\n    for key in sorted(input_dict.keys()):\n        if isJsonSafe(input_dict[key]):\n            output_dict[key] = input_dict[key]\n    return output_dict\n\n\nclass CustomDummyVecEnv(VecEnv):\n    """"""Dummy class in order to use FrameStack with SAC""""""\n\n    def __init__(self, env_fns):\n        """"""\n        :param env_fns: ([function])\n        """"""\n        assert len(env_fns) == 1, ""This dummy class does not support multiprocessing""\n        self.envs = [fn() for fn in env_fns]\n        env = self.envs[0]\n        VecEnv.__init__(self, len(env_fns), env.observation_space, env.action_space)\n        self.env = self.envs[0]\n        self.actions = None\n        self.obs = None\n        self.reward, self.done, self.infos = None, None, None\n\n    def step_wait(self):\n        self.obs, self.reward, self.done, self.infos = self.env.step(self.actions[0])\n        return np.copy(self.obs[None]), self.reward, [self.done], [self.infos]\n\n    def step_async(self, actions):\n        """"""\n        :param actions: ([int])\n        """"""\n        self.actions = actions\n\n    def reset(self):\n        return self.env.reset()\n\n    def close(self):\n        return\n\n    def get_images(self):\n        return [env.render(mode=\'rgb_array\') for env in self.envs]\n\n\nclass WrapFrameStack(VecFrameStack):\n    """"""\n    Wrap VecFrameStack in order to be usable with SAC\n    and scale output if necessary\n    """"""\n\n    def __init__(self, venv, n_stack, normalize=True):\n        super(WrapFrameStack, self).__init__(venv, n_stack)\n        self.factor = 255.0 if normalize else 1\n\n    def step(self, action):\n        self.step_async([action])\n        stackedobs, rewards, dones, infos = self.step_wait()\n        return stackedobs[0] / self.factor, rewards, dones[0], infos[0]\n\n    def reset(self):\n        """"""\n        Reset all environments\n        """"""\n        stackedobs = super(WrapFrameStack, self).reset()\n        return stackedobs[0] / self.factor\n\n    def get_original_obs(self):\n        """"""\n        Hack to use VecNormalize\n        :return: (numpy float)\n        """"""\n        return self.venv.get_original_obs()\n\n    def saveRunningAverage(self, path):\n        """"""\n        Hack to use VecNormalize\n        :param path: (str) path to log dir\n        """"""\n        self.venv.save_running_average(path)\n\n    def loadRunningAverage(self, path):\n        """"""\n        Hack to use VecNormalize\n        :param path: (str) path to log dir\n        """"""\n        self.venv.load_running_average(path)\n\n    # Compatibility with stable-baselines\n    save_running_average = saveRunningAverage\n    load_running_average = loadRunningAverage\n\n\nclass MultiprocessSRLModel:\n    """"""\n    Allows multiple environments to use a single SRL model\n    :param num_cpu: (int) the number of environments that will spawn\n    :param env_id: (str) the environment id string\n    :param env_kwargs: (dict)\n    """"""\n\n    def __init__(self, num_cpu, env_id, env_kwargs):\n        # Create a duplex pipe between env and srl model, where all the inputs are unified and the origin\n        # marked with a index number\n        self.pipe = (Queue(), [Queue() for _ in range(num_cpu)])\n        module_env, class_name, _ = dynamicEnvLoad(env_id)\n        # we need to know the expected dim output of the SRL model, before it is created\n        self.state_dim = getSRLDim(env_kwargs.get(""srl_model_path"", None), module_env.__dict__[class_name])\n        self.p = Process(target=self._run, args=(env_kwargs,))\n        self.p.daemon = True\n        self.p.start()\n\n    def _run(self, env_kwargs):\n        # this is to control the number of CPUs that torch is allowed to use.\n        # By default it will use all CPUs, even with GPU acceleration\n        th.set_num_threads(1)\n        self.model = loadSRLModel(env_kwargs.get(""srl_model_path"", None), th.cuda.is_available(), self.state_dim,\n                                  env_object=None)\n        # run until the end of the caller thread\n        while True:\n            # pop an item, get state, and return to sender.\n            env_id, var = self.pipe[0].get()\n            self.pipe[1][env_id].put(self.model.getState(var, env_id=env_id))\n\n\ndef createEnvs(args, allow_early_resets=False, env_kwargs=None, load_path_normalise=None):\n    """"""\n    :param args: (argparse.Namespace Object)\n    :param allow_early_resets: (bool) Allow reset before the enviroment is done, usually used in ES to halt the envs\n    :param env_kwargs: (dict) The extra arguments for the environment\n    :param load_path_normalise: (str) the path to loading the rolling average, None if not available or wanted.\n    :return: (Gym VecEnv)\n    """"""\n    # imported here to prevent cyclic imports\n    from environments.registry import registered_env\n    from state_representation.registry import registered_srl, SRLType\n\n    assert not (registered_env[args.env][3] is ThreadingType.NONE and args.num_cpu != 1), \\\n        ""Error: cannot have more than 1 CPU for the environment {}"".format(args.env)\n\n    if env_kwargs is not None and registered_srl[args.srl_model][0] == SRLType.SRL:\n        srl_model = MultiprocessSRLModel(args.num_cpu, args.env, env_kwargs)\n        env_kwargs[""state_dim""] = srl_model.state_dim\n        env_kwargs[""srl_pipe""] = srl_model.pipe\n    envs = [makeEnv(args.env, args.seed, i, args.log_dir, allow_early_resets=allow_early_resets, env_kwargs=env_kwargs)\n            for i in range(args.num_cpu)]\n\n    if len(envs) == 1:\n        # No need for subprocesses when having only one env\n        envs = DummyVecEnv(envs)\n    else:\n        envs = SubprocVecEnv(envs)\n\n    envs = VecFrameStack(envs, args.num_stack)\n\n    if args.srl_model != ""raw_pixels"":\n        printYellow(""Using MLP policy because working on state representation"")\n        envs = VecNormalize(envs, norm_obs=True, norm_reward=False)\n        envs = loadRunningAverage(envs, load_path_normalise=load_path_normalise)\n\n    return envs\n\n\ndef loadRunningAverage(envs, load_path_normalise=None):\n    if load_path_normalise is not None:\n        try:\n            printGreen(""Loading saved running average"")\n            envs.load_running_average(load_path_normalise)\n            envs.training = False\n        except FileNotFoundError:\n            envs.training = True\n            printYellow(""Running Average files not found for VecNormalize, switching to training mode"")\n    return envs\n\n\ndef softmax(x):\n    """"""\n    Numerically stable implementation of softmax.\n    :param x: (numpy float)\n    :return: (numpy float)\n    """"""\n    e_x = np.exp(x.T - np.max(x.T, axis=0))\n    return (e_x / e_x.sum(axis=0)).T\n'"
rl_baselines/visualize.py,0,"b'""""""\nModified version of https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/visualize.py\nScript used to send plot data to visdom\n""""""\nimport glob\nimport os\n\nimport numpy as np\nfrom scipy.signal import medfilt\n\n\ndef smoothRewardCurve(x, y):\n    """"""\n    :param x: (numpy array)\n    :param y: (numpy array)\n    :return: (numpy array, numpy array)\n    """"""\n    # Halfwidth of our smoothing convolution\n    halfwidth = min(31, int(np.ceil(len(x) / 30)))\n    k = halfwidth\n    xsmoo = x[k:-k]\n    ysmoo = np.convolve(y, np.ones(2 * k + 1), mode=\'valid\') / \\\n            np.convolve(np.ones_like(y), np.ones(2 * k + 1), mode=\'valid\')\n    downsample = max(int(np.floor(len(xsmoo) / 1e3)), 1)\n    return xsmoo[::downsample], ysmoo[::downsample]\n\n\ndef fixPoint(x, y, interval):\n    """"""\n    :param x: (numpy array)\n    :param y: (numpy array)\n    :param interval: (int)\n    :return: (numpy array, numpy array)\n    """"""\n    np.insert(x, 0, 0)\n    np.insert(y, 0, 0)\n\n    fx, fy = [], []\n    pointer = 0\n\n    ninterval = int(max(x) / interval + 1)\n\n    for i in range(ninterval):\n        tmpx = interval * i\n\n        while pointer + 1 < len(x) and tmpx > x[pointer + 1]:\n            pointer += 1\n\n        if pointer + 1 < len(x):\n            alpha = (y[pointer + 1] - y[pointer]) / \\\n                    (x[pointer + 1] - x[pointer])\n            tmpy = y[pointer] + alpha * (tmpx - x[pointer])\n            fx.append(tmpx)\n            fy.append(tmpy)\n\n    return fx, fy\n\n\ndef loadCsv(log_folder, is_es=False):\n    """"""\n    Load data (rewards for each episode) from csv file (generated by gym monitor)\n    :param log_folder: (str)\n    :param is_es: (bool) Set the loading to get the best agent from the envs\n    :return: (numpy array, numpy array)\n    """"""\n    datas = []\n    monitor_files = glob.glob(os.path.join(log_folder, \'*.monitor.csv\'))\n\n    for input_file in monitor_files:\n        data = []\n        with open(input_file, \'r\') as f:\n            f.readline()\n            f.readline()\n            for line in f:\n                tmp = line.split(\',\')\n                t_time = float(tmp[2])\n                tmp = [t_time, int(tmp[1]), float(tmp[0])]\n                data.append(tmp)\n        datas.append(data)\n\n    if is_es:\n        max_data_len = np.max([len(data) for data in datas])\n\n        # get the reward for each file, and make sure they are all the same length\n        r_list = []\n        for data in datas:\n            if len(data) == max_data_len:\n                r_list.append([x[2] for x in data])\n            else:\n                r_list.append([x[2] for x in data] + ((max_data_len - len(data)) * [-np.inf]))\n        max_sess = np.argmax(r_list, axis=0)\n\n        result = []\n        timesteps = 0\n        for i in range(max_data_len):\n            result.append([timesteps, datas[max_sess[i]][i][-1]])\n            timesteps += datas[max_sess[i]][i][1]\n    else:\n        datas = [x for data in datas for x in data]\n        datas = sorted(datas, key=lambda d_entry: d_entry[0])\n        result = []\n        timesteps = 0\n        for i in range(len(datas)):\n            result.append([timesteps, datas[i][-1]])\n            timesteps += datas[i][1]\n\n    return result, timesteps\n\n\ndef loadData(log_folder, smooth, bin_size, is_es=False):\n    """"""\n    :param log_folder: (str)\n    :param smooth: (int) Smoothing method\n    :param bin_size: (int)\n    :param is_es: (bool)\n    :return:\n    """"""\n    result, timesteps = loadCsv(log_folder, is_es=is_es)\n\n    if len(result) < bin_size:\n        return [None, None]\n\n    x, y = np.array(result)[:, 0], np.array(result)[:, 1]\n\n    if smooth == 1:\n        x, y = smoothRewardCurve(x, y)\n\n    if smooth == 2:\n        y = medfilt(y, kernel_size=9)\n\n    x, y = fixPoint(x, y, bin_size)\n    return [x, y]\n\n\ndef movingAverage(values, window):\n    """"""\n    Smooth values by doing a moving average\n    :param values: (numpy array)\n    :param window: (int)\n    :return: (numpy array)\n    """"""\n    weights = np.repeat(1.0, window) / window\n    return np.convolve(values, weights, \'valid\')\n\n\ndef episodePlot(viz, win, folder, game, name, window=5, title="""", is_es=False):\n    """"""\n    Create/Update a vizdom plot of reward per episode\n    :param viz: (visdom object)\n    :param win: (str) Window name, it is the unique id of each plot\n    :param folder: (str) Log folder, where the monitor.csv is stored\n    :param game: (str) Name of the environment\n    :param name: (str) Algo name\n    :param window: (int) Smoothing window\n    :param title: (str) additional info to display in the plot title\n    :param is_es: (bool)\n    :return: (str)\n    """"""\n    result, _ = loadCsv(folder, is_es=is_es)\n\n    if len(result) == 0:\n        return win\n\n    y = np.array(result)[:, 1]\n    x = np.arange(len(y))\n\n    if y.shape[0] < window:\n        return win\n\n    y = movingAverage(y, window)\n\n    if len(y) == 0:\n        return win\n\n    # Truncate x\n    x = x[len(x) - len(y):]\n    opts = {\n        ""title"": ""{}\\n{}"".format(game, title),\n        ""xlabel"": ""Number of Episodes"",\n        ""ylabel"": ""Rewards"",\n        ""legend"": [name]\n    }\n    return viz.line(y, x, win=win, opts=opts)\n\n\ndef timestepsPlot(viz, win, folder, game, name, bin_size=100, smooth=1, title="""", is_es=False):\n    """"""\n    Create/Update a vizdom plot of reward per timesteps\n    :param viz: (visdom object)\n    :param win: (str) Window name, it is the unique id of each plot\n    :param folder: (str) Log folder, where the monitor.csv is stored\n    :param game: (str) Name of the environment\n    :param name: (str) Algo name\n    :param bin_size: (int)\n    :param smooth: (int) Smoothing method (0 for no smoothing)\n    :param title: (str) additional info to display in the plot title\n    :param is_es: (bool)\n    :return: (str)\n    """"""\n    tx, ty = loadData(folder, smooth, bin_size, is_es=is_es)\n    if tx is None or ty is None:\n        return win\n\n    if len(tx) * len(ty) == 0:\n        return win\n\n    tx, ty = np.array(tx), np.array(ty)\n\n    opts = {\n        ""title"": ""{}\\n{}"".format(game, title),\n        ""xlabel"": ""Number of Timesteps"",\n        ""ylabel"": ""Rewards"",\n        ""legend"": [name]\n    }\n    return viz.line(ty, tx, win=win, opts=opts)\n'"
state_representation/__init__.py,0,"b'from enum import Enum\n\n\nclass SRLType(Enum):\n    SRL = 1\n    ENVIRONMENT = 2  # defined as anything from the environment (joints, ground_truth, ...)\n'"
state_representation/client.py,0,"b'""""""\nClient to communicate with SRL server\n""""""\nimport os\nimport json\nfrom enum import Enum\n\nimport zmq\n\nHOSTNAME = \'localhost\'\nSERVER_PORT = 7777\n\n\nclass Command(Enum):\n    HELLO = 0\n    LEARN = 1\n    READY = 2\n    ERROR = 3\n    EXIT = 4\n\n\nclass SRLClient(object):\n    def __init__(self, data_folder, hostname=\'localhost\', server_port=7777):\n        super(SRLClient, self).__init__()\n        self.hostname = hostname\n        self.server_port = server_port\n        context = zmq.Context()\n        self.socket = context.socket(zmq.PAIR)\n        self.socket.connect(""tcp://{}:{}"".format(hostname, server_port))\n        self.path_to_srl_server = None\n        self.data_folder = data_folder\n\n    def __del__(self):\n        self.socket.close()\n\n    def waitForServer(self):\n        print(""Waiting for server..."")\n        msg = self.socket.recv_json()\n        assert Command(msg[\'command\']) == Command.HELLO\n        self.path_to_srl_server = msg.get(\'path\')\n        self.socket.send_json({""command"": Command.HELLO.value, \'data_folder\': self.data_folder})\n        print(""Connected to server"")\n\n    def sendLearnCommand(self, state_dim, seed=1):\n        """"""\n        :param state_dim: (int)\n        :param seed: (int)\n        """"""\n        self.socket.send_json({""command"": Command.LEARN.value, \'state_dim\': state_dim, \'seed\': seed})\n\n    def sendExitCommand(self):\n        self.socket.send_json({""command"": Command.EXIT.value})\n\n    def receiveMessage(self):\n        """"""\n        :return: (Command, dict)\n        """"""\n        msg = self.socket.recv_json()\n        try:\n            # Convert to a command object\n            command = Command(msg.get(\'command\'))\n        except ValueError:\n            raise ValueError(""Unknown command: {}"".format(msg))\n        return command, msg\n\n    def waitForSRLModel(self, state_dim):\n        """"""\n        Wait until SRL is trained\n        :param state_dim: (int)\n        :return: (bool, str) (True if no error, path to learned model)\n        """"""\n        self.sendLearnCommand(state_dim)\n        command, msg = self.receiveMessage()\n        if command == Command.ERROR:\n            print(""An error occured during SRL"")\n            return False, """"\n        elif command != Command.READY:\n            print(""Unsupported command:{}"".format(command))\n            return False, """"\n        else:\n            path_to_model = msg.get(\'path\') + \'/srl_model.pth\'\n        return True, path_to_model\n\n\nif __name__ == \'__main__\':\n    data_folder = \'test_server\'\n    os.makedirs(\'srl_zoo/data/\' + data_folder, exist_ok=True)\n\n    dataset_config = {\'relative_pos\': False}\n    with open(""srl_zoo/data/{}/dataset_config.json"".format(data_folder), ""w"") as f:\n        json.dump(dataset_config, f)\n\n    socket_client = SRLClient(data_folder)\n    socket_client.waitForServer()\n    try:\n        while True:\n            ok, path_to_model = socket_client.waitForSRLModel(state_dim=3)\n            print(path_to_model)\n            break\n    except KeyboardInterrupt:\n        pass\n\n    socket_client.sendExitCommand()\n    print(""Client exiting..."")\n'"
state_representation/episode_saver.py,0,"b'import os\nimport json\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom srl_zoo.utils import printYellow\nfrom rl_baselines.utils import filterJSONSerializableObjects\nfrom state_representation.client import SRLClient\n\n\nclass EpisodeSaver(object):\n    """"""\n    Save the experience data from a gym env to a file\n    and notify the srl server so it learns from the gathered data\n    :param name: (str)\n    :param max_dist: (float)\n    :param state_dim: (int)\n    :param globals_: (dict) Environments globals\n    :param learn_every: (int)\n    :param learn_states: (bool)\n    :param path: (str)\n    :param relative_pos: (bool)\n    """"""\n\n    def __init__(self, name, max_dist, state_dim=-1, globals_=None, learn_every=3, learn_states=False,\n                 path=\'data/\', relative_pos=False):\n        super(EpisodeSaver, self).__init__()\n        self.name = name\n        self.data_folder = path + name\n        self.path = path\n        try:\n            os.makedirs(self.data_folder)\n        except OSError:\n            printYellow(""Folder already exist"")\n\n        self.actions = []\n        self.rewards = []\n        self.images = []\n        self.target_positions = []\n        self.episode_starts = []\n        self.ground_truth_states = []\n        self.images_path = []\n        self.episode_step = 0\n        self.episode_idx = -1\n        self.episode_folder = None\n        self.episode_success = False\n        self.state_dim = state_dim\n        self.learn_states = learn_states\n        self.learn_every = learn_every  # Every n episodes, learn a state representation\n        self.srl_model_path = """"\n        self.n_steps = 0\n        self.max_steps = 10000\n\n        self.dataset_config = {\'relative_pos\': relative_pos, \'max_dist\': str(max_dist)}\n        with open(""{}/dataset_config.json"".format(self.data_folder), ""w"") as f:\n            json.dump(self.dataset_config, f)\n\n        if globals_ is not None:\n            # Save environments parameters\n            with open(""{}/env_globals.json"".format(self.data_folder), ""w"") as f:\n                json.dump(filterJSONSerializableObjects(globals_), f)\n\n        if self.learn_states:\n            self.socket_client = SRLClient(self.name)\n            self.socket_client.waitForServer()\n\n    def saveImage(self, observation):\n        """"""\n        Write an image to disk\n        :param observation: (numpy matrix) BGR image\n        """"""\n        image_path = ""{}/{}/frame{:06d}"".format(self.data_folder, self.episode_folder, self.episode_step)\n        relative_path = ""{}/{}/frame{:06d}"".format(self.name, self.episode_folder, self.episode_step)\n        self.images_path.append(relative_path)\n        # in the case of dual/multi-camera\n        if observation.shape[2] > 3:\n            observation1 = cv2.cvtColor(observation[:, :, :3], cv2.COLOR_BGR2RGB)\n            observation2 = cv2.cvtColor(observation[:, :, 3:], cv2.COLOR_BGR2RGB)\n\n            cv2.imwrite(""{}_1.jpg"".format(image_path), observation1)\n            cv2.imwrite(""{}_2.jpg"".format(image_path), observation2)\n        else:\n            observation = cv2.cvtColor(observation, cv2.COLOR_BGR2RGB)\n            cv2.imwrite(""{}.jpg"".format(image_path), observation)\n\n    def reset(self, observation, target_pos, ground_truth):\n        """"""\n        Called when starting a new episode\n        :param observation: (numpy matrix) BGR Image\n        :param target_pos: (numpy array)\n        :param ground_truth: (numpy array)\n        """"""\n        if len(self.episode_starts) == 0 or self.episode_starts[-1] is False:\n            self.episode_idx += 1\n\n            if self.learn_states and (self.episode_idx + 1) % self.learn_every == 0 and self.n_steps <= self.max_steps:\n                print(""Learning a state representation ..."")\n                start_time = time.time()\n                ok, self.srl_model_path = self.socket_client.waitForSRLModel(self.state_dim)\n                print(""Took {:.2f}s"".format(time.time() - start_time))\n\n            self.episode_step = 0\n            self.episode_success = False\n            self.episode_folder = ""record_{:03d}"".format(self.episode_idx)\n            os.makedirs(""{}/{}"".format(self.data_folder, self.episode_folder), exist_ok=True)\n\n            self.episode_starts.append(True)\n            self.target_positions.append(target_pos)\n            self.ground_truth_states.append(ground_truth)\n            self.saveImage(observation)\n\n    def step(self, observation, action, reward, done, ground_truth_state):\n        """"""\n        :param observation: (numpy matrix) BGR Image\n        :param action: (int)\n        :param reward: (float)\n        :param done: (bool) whether the episode is done or not\n        :param ground_truth_state: (numpy array)\n        """"""\n        \n        self.episode_step += 1\n        self.n_steps += 1\n        self.rewards.append(reward)\n        self.actions.append(action)\n        if reward > 0:\n            self.episode_success = True\n\n        if not done:\n            self.episode_starts.append(False)\n            self.ground_truth_states.append(ground_truth_state)\n            self.saveImage(observation)\n        else:   \n            # Save the gathered data at the end of each episode\n            self.save()\n    \n    def save(self):\n        """"""\n        Write data and ground truth to disk\n        """"""\n        # Sanity checks\n        assert len(self.actions) == len(self.rewards)\n        assert len(self.actions) == len(self.episode_starts)\n        assert len(self.actions) == len(self.images_path)\n        assert len(self.actions) == len(self.ground_truth_states)\n        assert len(self.target_positions) == self.episode_idx + 1\n\n        data = {\n            \'rewards\': np.array(self.rewards),\n            \'actions\': np.array(self.actions),\n            \'episode_starts\': np.array(self.episode_starts)\n        }\n\n        ground_truth = {\n            \'target_positions\': np.array(self.target_positions),\n            \'ground_truth_states\': np.array(self.ground_truth_states),\n            \'images_path\': np.array(self.images_path)\n        }\n        print(""Saving preprocessed data..."")\n        np.savez(\'{}/preprocessed_data.npz\'.format(self.data_folder), **data)\n        np.savez(\'{}/ground_truth.npz\'.format(self.data_folder), **ground_truth)\n\n\nclass LogRLStates(object):\n    """"""\n    Save the experience data (states, normalized states, actions, rewards) from a gym env to a file\n    during RL training. It is useful to debug SRL models.\n    :param log_folder: (str)\n    """"""\n\n    def __init__(self, log_folder):\n        super(LogRLStates, self).__init__()\n\n        self.log_folder = log_folder + \'log_srl/\'\n        try:\n            os.makedirs(self.log_folder)\n        except OSError:\n            printYellow(""Folder already exist"")\n\n        self.actions = []\n        self.rewards = []\n        self.states = []\n        self.normalized_states = []\n\n    def reset(self, normalized_state, state):\n        """"""\n        Called when starting a new episode\n        :param normalized_state: (numpy array)\n        :param state: (numpy array)\n        """"""\n        # self.episode_starts.append(True)\n        self.normalized_states.append(normalized_state)\n        self.states.append(np.squeeze(state))\n\n    def step(self, normalized_state, state, action, reward, done):\n        """"""\n        :param normalized_state: (numpy array)\n        :param state: (numpy array)\n        :param action: (int)\n        :param reward: (float)\n        :param done: (bool) whether the episode is done or not\n        """"""\n        self.rewards.append(reward)\n        self.actions.append(action)\n\n        if not done:\n            self.normalized_states.append(normalized_state)\n            self.states.append(np.squeeze(state))\n        else:\n            # Save the gathered data at the end of each episode\n            self.save()\n\n    def save(self):\n        """"""\n        Write data to disk\n        """"""\n        # Sanity checks\n        assert len(self.actions) == len(self.rewards)\n        assert len(self.actions) == len(self.normalized_states)\n        assert len(self.actions) == len(self.states)\n\n        data = {\n            \'rewards\': np.array(self.rewards),\n            \'actions\': np.array(self.actions),\n            \'states\': np.array(self.states),\n            \'normalized_states\': np.array(self.normalized_states),\n        }\n\n        np.savez(\'{}/full_log.npz\'.format(self.log_folder), **data)\n        np.savez(\'{}/states_rewards.npz\'.format(self.log_folder),\n                 **{\'states\': data[\'states\'], \'rewards\': data[\'rewards\']})\n        np.savez(\'{}/normalized_states_rewards.npz\'.format(self.log_folder),\n                 **{\'states\': data[\'normalized_states\'], \'rewards\': data[\'rewards\']})\n'"
state_representation/models.py,0,"b'import json\nimport pickle as pkl\nfrom collections import OrderedDict\n\nimport numpy as np\nimport torch as th\n\nimport srl_zoo.preprocessing as preprocessing\nfrom srl_zoo.models import CustomCNN, ConvolutionalNetwork, SRLModules, SRLModulesSplit\nfrom srl_zoo.preprocessing import preprocessImage, getNChannels\nfrom srl_zoo.utils import printGreen, printYellow\n\nNOISE_STD = 1e-6  # To avoid NaN for SRL\n\n\ndef getSRLDim(path=None, env_object=None):\n    """"""\n    Get the dim of SRL model\n    :param path: (str) Path to a srl model\n    :param env_object: (gym env object)\n    :return: (int)\n    """"""\n    if path is not None:\n        # Get path to the log folder\n        log_folder = \'/\'.join(path.split(\'/\')[:-1]) + \'/\'\n\n        with open(log_folder + \'exp_config.json\', \'r\') as f:\n            exp_config = json.load(f, object_pairs_hook=OrderedDict)\n        try:\n            return exp_config[\'state-dim\']\n        except KeyError:\n            # Old format\n            return exp_config[\'state_dim\']\n    else:\n        return env_object.getGroundTruthDim()\n\n\ndef loadSRLModel(path=None, cuda=False, state_dim=None, env_object=None):\n    """"""\n    Load a trained SRL model, it will try to guess the model type from the path\n    :param path: (str) Path to a srl model\n    :param cuda: (bool)\n    :param state_dim: (int)\n    :param env_object: (gym env object)\n    :return: (srl model)\n    """"""\n\n    model_type, losses, n_actions, model = None, None, None, None\n\n    if path is not None:\n        # Get path to the log folder\n        log_folder = \'/\'.join(path.split(\'/\')[:-1]) + \'/\'\n        with open(log_folder + \'exp_config.json\', \'r\') as f:\n            # IMPORTANT: keep the order for the losses\n            # so the json is loaded as an OrderedDict\n            exp_config = json.load(f, object_pairs_hook=OrderedDict)\n\n        state_dim = exp_config.get(\'state-dim\', None)\n        losses = exp_config.get(\'losses\', None)  # None in the case of baseline models (pca, supervised)\n        n_actions = exp_config.get(\'n_actions\', None)  # None in the case of baseline models (pca, supervised)\n        model_type = exp_config.get(\'model-type\', None)\n        use_multi_view = exp_config.get(\'multi-view\', False)\n        inverse_model_type = exp_config.get(\'inverse-model-type\', \'linear\')\n\n        assert state_dim is not None, \\\n            ""Please make sure you are loading an up to date model with a conform exp_config file.""\n\n        split_dimensions = exp_config.get(\'split-dimensions\')\n        if isinstance(split_dimensions, OrderedDict):\n            n_dims = sum(split_dimensions.values())\n            # Combine losses instead of splitting\n            if n_dims == 0:\n                split_dimensions = None\n    else:\n        assert env_object is not None or state_dim > 0, \\\n            ""When learning states, state_dim must be > 0. Otherwise, set SRL_MODEL_PATH \\\n            to a srl_model.pth file with learned states.""\n\n    if path is not None:\n        if \'baselines\' in path:\n            if \'pca\' in path:\n                model_type = \'pca\'\n                model = SRLPCA(state_dim)\n\n    assert model_type is not None or model is not None, \\\n        ""Model type not supported. In order to use loadSRLModel, a path to an SRL model must be given.""\n    assert not (losses is None and not model_type == \'pca\'), \\\n        ""Please make sure you are loading an up to date model with a conform exp_config file.""\n    assert not (n_actions is None and not (model_type == \'pca\' or \'supervised\' in losses)), \\\n        ""Please make sure you are loading an up to date model with a conform exp_config file.""\n\n    if model is None:\n        if use_multi_view:\n            preprocessing.preprocess.N_CHANNELS = 6\n\n        model = SRLNeuralNetwork(state_dim, cuda, model_type, n_actions=n_actions, losses=losses,\n                                 split_dimensions=split_dimensions, inverse_model_type=inverse_model_type)\n\n    model_name = model_type\n    if \'baselines\' not in path:\n        model_name += "" with "" + "", "".join(losses)\n    printGreen(""\\nSRL: Using {} \\n"".format(model_name))\n\n    if path is not None:\n        printYellow(""Loading trained model...{}"".format(path))\n        model.load(path)\n    return model\n\n\nclass SRLBaseClass(object):\n    """"""Base class for state representation learning models""""""\n\n    def __init__(self, state_dim, cuda=False):\n        """"""\n        :param state_dim: (int)\n        :param cuda: (bool)\n        """"""\n        super(SRLBaseClass, self).__init__()\n        self.state_dim = state_dim\n        self.cuda = cuda\n        self.model = None\n\n    def load(self, path):\n        """"""\n        Load a trained SRL model\n        :param path: (str)\n        """"""\n        raise NotImplementedError(""load() not implemented"")\n\n    def getState(self, observation, env_id=0):\n        """"""\n        Predict the state for a given observation\n\n        :param observation: (numpy Number) the input observation\n        :param env_id: (int) the environment ID for multi env systems (default=0)\n        :return: (numpy Number)\n        """"""\n        raise NotImplementedError(""getState() not implemented"")\n\n\nclass SRLNeuralNetwork(SRLBaseClass):\n    """"""SRL using a neural network as a state representation model""""""\n\n    def __init__(self, state_dim, cuda, model_type=""custom_cnn"", n_actions=None, losses=None, split_dimensions=None,\n                 inverse_model_type=""linear""):\n        """"""\n        :param state_dim: (int)\n        :param cuda: (bool)\n        :param model_type: (string)\n        :param n_actions: action space dimensions (int)\n        :param losses: list of optimized losses defining the model (list of string)\n        :param split_dimensions: (OrderedDict) Number of dimensions for the different losses\n        :param inverse_model_type: (string)\n        """"""\n        super(SRLNeuralNetwork, self).__init__(state_dim, cuda)\n\n        self.model_type = model_type\n        if ""supervised"" in losses:\n            if ""cnn"" in model_type:\n                self.model = CustomCNN(state_dim)\n            elif model_type == ""resnet"":\n                self.model = ConvolutionalNetwork(state_dim)\n        elif isinstance(split_dimensions, OrderedDict):\n            self.model = SRLModulesSplit(state_dim=state_dim, action_dim=n_actions, model_type=model_type,\n                                         cuda=self.cuda, losses=losses, split_dimensions=split_dimensions,\n                                         inverse_model_type=inverse_model_type)\n        else:\n            self.model = SRLModules(state_dim=state_dim, action_dim=n_actions, model_type=model_type,\n                                    cuda=self.cuda, losses=losses, inverse_model_type=inverse_model_type)\n        self.model.eval()\n\n        self.device = th.device(""cuda"" if th.cuda.is_available() and cuda else ""cpu"")\n        self.model = self.model.to(self.device)\n\n    def load(self, path):\n        self.model.load_state_dict(th.load(path))\n\n    def getState(self, observation, env_id=0):\n        if getNChannels() > 3:\n            observation = np.dstack((preprocessImage(observation[:, :, :3], convert_to_rgb=False),\n                                     preprocessImage(observation[:, :, 3:], convert_to_rgb=False)))\n        else:\n            observation = preprocessImage(observation, convert_to_rgb=False)\n\n        # Create 4D Tensor\n        observation = observation.reshape(1, *observation.shape)\n        # Channel first\n        observation = np.transpose(observation, (0, 3, 2, 1))\n        observation = th.from_numpy(observation).float().to(self.device)\n\n        with th.no_grad():\n            state = self.model.getStates(observation)[0]\n        return state.to(th.device(""cpu"")).detach().numpy()\n\n\nclass SRLPCA(SRLBaseClass):\n    """"""PCA as a state representation""""""\n\n    def __init__(self, state_dim):\n        super(SRLPCA, self).__init__(state_dim)\n\n    def load(self, path):\n        try:\n            with open(path, ""rb"") as f:\n                self.model = pkl.load(f)\n        except UnicodeDecodeError:\n            # Load pickle files saved with python 2\n            with open(path, ""rb"") as f:\n                self.model = pkl.load(f, encoding=\'latin1\')\n\n    def getState(self, observation, env_id=0):\n        observation = observation[None]  # Add a dimension\n        # n_features = width * height * n_channels\n        n_features = np.prod(observation.shape[1:])\n        # Convert to a 1D array\n        observation = observation.reshape(-1, n_features)\n        return self.model.transform(observation)[0]\n'"
state_representation/registry.py,0,"b'from state_representation import SRLType\nfrom environments.kuka_gym.kuka_button_gym_env import KukaButtonGymEnv\n\n# format NAME: (SRLType, LIMITED_TO_ENV)\nregistered_srl = {\n    ""raw_pixels"": (SRLType.ENVIRONMENT, None),\n    ""ground_truth"": (SRLType.ENVIRONMENT, None),\n    ""joints"": (SRLType.ENVIRONMENT, [KukaButtonGymEnv]),\n    ""joints_position"": (SRLType.ENVIRONMENT, [KukaButtonGymEnv]),\n    ""robotic_priors"": (SRLType.SRL, None),\n    ""inverse"": (SRLType.SRL, None),\n    ""forward"": (SRLType.SRL, None),\n    ""multi_view_srl"": (SRLType.SRL, None),\n    ""srl_combination"": (SRLType.SRL, None),\n    ""supervised"": (SRLType.SRL, None),\n    ""autoencoder"": (SRLType.SRL, None),\n    ""autoencoder_inverse"": (SRLType.SRL, None),\n    ""autoencoder_reward"": (SRLType.SRL, None),\n    ""autoencoder_forward"": (SRLType.SRL, None),\n    ""random"": (SRLType.SRL, None),\n    ""random_inverse"": (SRLType.SRL, None),\n    ""reward_inverse"": (SRLType.SRL, None),\n    ""srl_splits"": (SRLType.SRL, None),\n    ""srl_split_forward"": (SRLType.SRL, None),\n    ""srl_3_splits"": (SRLType.SRL, None),\n    ""reward"": (SRLType.SRL, None),\n    ""vae"": (SRLType.SRL, None),\n    ""dae"": (SRLType.SRL, None),\n    ""pca"": (SRLType.SRL, None)\n}\n'"
tests/__init__.py,0,b''
tests/test_dataset_manipulation.py,0,"b'import subprocess\nimport pytest\nimport os\nimport shutil\n\nDATA_FOLDER_NAME_1 = ""kuka_test_f1""\nDATA_FOLDER_NAME_2 = ""kuka_test_f2""\nDATA_FOLDER_NAME_3 = ""kuka_test_f3""\nDEFAULT_ENV = ""KukaButtonGymEnv-v0""\nPATH_SRL = ""srl_zoo/data/""\n\n\ndef assertEq(left, right):\n    assert left == right, ""{} != {}"".format(left, right)\n\n\n@pytest.mark.fast\ndef testDataGenForFusion():\n    args_1 = [\'--num-cpu\', 4, \'--num-episode\', 8, \'--name\', DATA_FOLDER_NAME_1, \'--force\', \'--env\', DEFAULT_ENV]\n    args_1 = list(map(str, args_1))\n\n    ok = subprocess.call([\'python\', \'-m\', \'environments.dataset_generator\'] + args_1)\n    assertEq(ok, 0)\n\n    args_2 = [\'--num-cpu\', 4, \'--num-episode\', 8, \'--name\', DATA_FOLDER_NAME_2, \'--force\', \'--env\', DEFAULT_ENV]\n    args_2 = list(map(str, args_2))\n\n    ok = subprocess.call([\'python\', \'-m\', \'environments.dataset_generator\'] + args_2)\n    assertEq(ok, 0)\n\n    args_3 = [\'--merge\', PATH_SRL + DATA_FOLDER_NAME_1, PATH_SRL + DATA_FOLDER_NAME_2, PATH_SRL + DATA_FOLDER_NAME_3]\n    args_3 = list(map(str, args_3))\n\n    ok = subprocess.call([\'python\', \'-m\', \'environments.dataset_fusioner\'] + args_3)\n    assertEq(ok, 0)\n\n    # Checking inexistance of original datasets to be merged\n    assert not os.path.isdir(PATH_SRL + DATA_FOLDER_NAME_1)\n    assert not os.path.isdir(PATH_SRL + DATA_FOLDER_NAME_2)\n    assert os.path.isdir(PATH_SRL + DATA_FOLDER_NAME_3)\n\n    # Removing fusionned test dataset\n    shutil.rmtree(PATH_SRL + DATA_FOLDER_NAME_3)\n'"
tests/test_end_to_end.py,0,"b'from __future__ import print_function, division, absolute_import\n\nimport subprocess\nimport os\nimport json\nfrom collections import OrderedDict\n\nimport pytest\n\nfrom srl_zoo.utils import createFolder\n\nDEFAULT_ALGO = ""ppo2""\nDEFAULT_ENV = ""KukaButtonGymEnv-v0""\nDEFAULT_SRL = ""supervised""\nNUM_ITERATION = 1\nNUM_TIMESTEP = 1600\nDEFAULT_SRL_CONFIG_YAML = ""config/srl_models_test.yaml""\n\nDATA_FOLDER_NAME = ""RL_test""\nTEST_DATA_FOLDER = ""data/"" + DATA_FOLDER_NAME\nTEST_DATA_FOLDER_DUAL_CAMERA = ""data/kuka_gym_dual_test""\n\nNUM_EPOCHS = 1\nSTATE_DIM = 3\nTRAINING_SET_SIZE = 2000\nKNN_SAMPLES = 1000\n\nSEED = 0\n\n\ndef buildTestConfig():\n    cfg = {\n        ""batch-size"": 32,\n        ""model-type"": ""custom_cnn"",\n        ""epochs"": NUM_EPOCHS,\n        ""knn-samples"": KNN_SAMPLES,\n        ""knn-seed"": 1,\n        ""l1-reg"": 0,\n        ""training-set-size"": TRAINING_SET_SIZE,\n        ""learning-rate"": 0.001,\n        ""data-folder"": TEST_DATA_FOLDER,\n        ""relative-pos"": False,\n        ""seed"": SEED,\n        ""state-dim"": STATE_DIM,\n        ""use-continuous"": False\n    }\n    return cfg\n\n\ndef assertEq(left, right):\n    assert left == right, ""{} != {}"".format(left, right)\n\n\ndef assertNeq(left, right):\n    assert left != right, ""{} == {}"".format(left, right)\n\n\ndef createFolders(log_folder_name):\n    createFolder(""srl_zoo/"" + log_folder_name, ""Test log folder already exist"")\n    folder_path = \'srl_zoo/{}/NearestNeighbors/\'.format(log_folder_name)\n    createFolder(folder_path, ""NearestNeighbors folder already exist"")\n\n\ndef testDataGen():\n    args = [\'--num-cpu\', 4, \'--num-episode\', 8, \'--name\', DATA_FOLDER_NAME, \'--force\', \'--env\', DEFAULT_ENV,\n            \'--reward-dist\']\n    args = list(map(str, args))\n\n    ok = subprocess.call([\'python\', \'-m\', \'environments.dataset_generator\'] + args)\n    assertEq(ok, 0)\n\n\n@pytest.mark.parametrize(""baseline"", [""supervised"", ""vae"", ""autoencoder""])\ndef testBaselineTrain(baseline):\n    """"""\n    Testing baseline models\n    :param baseline: (str) the baseline name to test\n    """"""\n    if baseline == \'supervised\':\n        args = [\'--no-display-plots\', \'--data-folder\', TEST_DATA_FOLDER,\n                \'--epochs\', NUM_EPOCHS, \'--training-set-size\', TRAINING_SET_SIZE,\n                \'--seed\', SEED, \'--model-type\', \'cnn\']\n        args = list(map(str, args))\n\n        ok = subprocess.call([\'python\', \'-m\', \'srl_baselines.supervised\'] + args, cwd=os.getcwd() + ""/srl_zoo"")\n        assertEq(ok, 0)\n    else:\n        exp_name = baseline + \'_cnn_ST_DIM3_SEED0_NOISE0_EPOCHS1_BS32\'\n        LOG_BASELINE = \'logs/\' + DATA_FOLDER_NAME + \'/\' + exp_name\n        createFolders(LOG_BASELINE)\n        exp_config = buildTestConfig()\n        exp_config[""log-folder""] = LOG_BASELINE\n        exp_config[""experiment-name""] = exp_name\n        exp_config[""losses""] = baseline\n        exp_config[""batch-size""] = 32\n        exp_config[""n_actions""] = 6\n        print(""log baseline: "", LOG_BASELINE)\n        args = [\'--no-display-plots\', \'--data-folder\', TEST_DATA_FOLDER,\n                \'--epochs\', NUM_EPOCHS, \'--training-set-size\', TRAINING_SET_SIZE,\n                \'--seed\', SEED, \'--model-type\', \'custom_cnn\',\n                \'--state-dim\', STATE_DIM, \'-bs\', 32,\n                \'--losses\', baseline,\n                \'--log-folder\', LOG_BASELINE]\n        args = list(map(str, args))\n\n        with open(""{}/exp_config.json"".format(""srl_zoo/"" + exp_config[\'log-folder\']), ""w"") as f:\n            json.dump(exp_config, f)\n        ok = subprocess.call([\'python\', \'train.py\'] + args, cwd=os.getcwd() + ""/srl_zoo"")\n        assertEq(ok, 0)\n\n\n@pytest.mark.parametrize(""loss_type"", [""priors"", ""inverse"", ""forward"", ""triplet""])\ndef testSrlTrain(loss_type):\n    """"""\n    Testing the training of srl models to be later used for RL\n    :param loss_type: (str) the model loss to test\n    """"""\n    exp_name = loss_type + \'_cnn_ST_DIM3_SEED0_NOISE0_EPOCHS1_BS32\'\n    log_name = \'logs/\' + DATA_FOLDER_NAME + \'/\' + exp_name\n    createFolders(log_name)\n    exp_config = buildTestConfig()\n\n    args = [\'--no-display-plots\', \'--epochs\', NUM_EPOCHS, \'--training-set-size\', TRAINING_SET_SIZE,\n            \'--seed\', SEED, \'--val-size\', 0.1, \'--state-dim\', STATE_DIM, \'--model-type\', \'custom_cnn\', \'-bs\', 32,\n            \'--log-folder\', log_name,\'--losses\', loss_type]\n\n    # Testing multi-view\n    if loss_type == ""triplet"":\n        exp_config[""multi-view""] = True\n        args.extend([\'--multi-view\', \'--data-folder\', TEST_DATA_FOLDER_DUAL_CAMERA])\n    else:\n        args.extend([\'--data-folder\', TEST_DATA_FOLDER])\n\n    args = list(map(str, args))\n\n    exp_config[""log-folder""] = log_name\n    exp_config[""experiment-name""] = exp_name\n    exp_config[""losses""] = loss_type\n    exp_config[""n_actions""] = 6\n    exp_config = OrderedDict(sorted(exp_config.items()))\n    with open(""{}/exp_config.json"".format(""srl_zoo/"" + exp_config[\'log-folder\']), ""w"") as f:\n        json.dump(exp_config, f)\n    ok = subprocess.call([\'python\', \'train.py\'] + args, cwd=os.getcwd() + ""/srl_zoo"")\n    assertEq(ok, 0)\n\n\ndef testSrlCombiningTrain():\n    # Combining models\n    exp_name = \'vae_inverse_forward_cnn_ST_DIM3_SEED0_NOISE0_EPOCHS1_BS32\'\n    log_name = \'logs/\' + DATA_FOLDER_NAME + \'/\' + exp_name\n    createFolders(log_name)\n    args = [\'--no-display-plots\', \'--data-folder\', TEST_DATA_FOLDER,\n            \'--epochs\', NUM_EPOCHS, \'--training-set-size\', TRAINING_SET_SIZE,\n            \'--seed\', SEED, \'--val-size\', 0.1, \'--state-dim\', STATE_DIM, \'--model-type\', \'custom_cnn\', \'-bs\', 32,\n            \'--log-folder\', log_name, \'--losses\', ""forward"", ""inverse"", ""vae""]\n    args = list(map(str, args))\n\n    exp_config = buildTestConfig()\n    exp_config[""log-folder""] = log_name\n    exp_config[""experiment-name""] = exp_name\n    exp_config[""losses""] = [""forward"", ""inverse"", ""vae""]\n    exp_config[""n_actions""] = 6\n    exp_config = OrderedDict(sorted(exp_config.items()))\n    with open(""{}/exp_config.json"".format(""srl_zoo/"" + exp_config[\'log-folder\']), ""w"") as f:\n        json.dump(exp_config, f)\n    ok = subprocess.call([\'python\', \'train.py\'] + args, cwd=os.getcwd() + ""/srl_zoo"")\n    assertEq(ok, 0)\n\n\n@pytest.mark.parametrize(""model_type"", [\'vae\', \'autoencoder\', ""robotic_priors"", ""inverse"", ""forward"", ""srl_combination"", ""multi_view_srl""])\ndef testAllRLOnSrlTrain(model_type):\n    """"""\n    Testing all the previously learned srl models on the RL pipeline\n    :param model_type: (str) the srl model to run\n    """"""\n    args = [\'--algo\', DEFAULT_ALGO, \'--env\', DEFAULT_ENV, \'--srl-model\', model_type,\n            \'--num-timesteps\', NUM_TIMESTEP, \'--seed\', SEED, \'--num-iteration\', NUM_ITERATION,\n            \'--no-vis\', \'--srl-config-file\', DEFAULT_SRL_CONFIG_YAML]\n    args = list(map(str, args))\n\n    ok = subprocess.call([\'python\', \'-m\', \'rl_baselines.pipeline\'] + args)\n    assertEq(ok, 0)\n\n\n@pytest.mark.parametrize(""algo"", [\'a2c\', \'acer\', \'acktr\', \'ars\', \'cma-es\', \'ddpg\', \'deepq\', \'ppo1\', \'ppo2\', \'random_agent\', \'sac\', \'trpo\'])\ndef testAllSrlonRLTrain(algo):\n    """"""\n    Testing RL pipeline on previously learned models\n    :param algo: (str) RL algorithm name\n    """"""\n    args = [\'--algo\', algo, \'--env\', DEFAULT_ENV, \'--srl-model\', DEFAULT_SRL,\n            \'--num-timesteps\', NUM_TIMESTEP, \'--seed\', SEED, \'--num-iteration\', NUM_ITERATION,\n            \'--no-vis\', \'--srl-config-file\', DEFAULT_SRL_CONFIG_YAML]\n    if algo == ""ddpg"" or algo == ""sac"":\n        mem_limit = 100 if DEFAULT_SRL == \'raw_pixels\' else 100000\n        args.extend([\'-c\'])\n        if algo == ""ddpg"":\n            args.extend([\'--memory-limit\', mem_limit])\n    elif algo == ""acer"":\n        args.extend([\'--num-stack\', 4])\n\n    args = list(map(str, args))\n\n    ok = subprocess.call([\'python\', \'-m\', \'rl_baselines.pipeline\'] + args)\n    assertEq(ok, 0)\n'"
tests/test_enjoy.py,0,"b'from __future__ import print_function, division, absolute_import\n\nimport subprocess\nimport glob\nimport shutil\nimport os\n\nimport pytest\n\nDEFAULT_ALGO = ""ppo2""\nDEFAULT_ENV = ""KukaButtonGymEnv-v0""\nDEFAULT_SRL = ""ground_truth""\nDEFAULT_LOG_DIR = ""logs/test_enjoy/""\nNUM_ITERATION = 1\nNUM_TRAIN_TIMESTEP = 3000\nNUM_ENJOY_TIMESTEP = 700\nSEED = 0\n\n# cleanup to remove the cluter\nif os.path.exists(DEFAULT_LOG_DIR):\n    shutil.rmtree(DEFAULT_LOG_DIR)\n\n\ndef isXAvailable():\n    """"""\n    check to see if running in terminal with X or not\n    :return: (bool)\n    """"""\n    try:\n        p = subprocess.Popen([""xset"", ""-q""], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        p.communicate()\n        return p.returncode == 0\n    except FileNotFoundError:\n        # Return False if xset is not present on the machine\n        return False\n\n\ndef assertEq(left, right):\n    assert left == right, ""{} != {}"".format(left, right)\n\n\ndef assertNeq(left, right):\n    assert left != right, ""{} == {}"".format(left, right)\n\n\n@pytest.mark.fast\n@pytest.mark.parametrize(""algo"", [\'a2c\', \'acer\', \'acktr\', \'ars\', \'cma-es\', \'ddpg\', \'deepq\', \'ppo1\', \'ppo2\', \'sac\',\n                                  \'trpo\'])\ndef testBaselineTrain(algo):\n    """"""\n    test for the given RL algorithm\n    :param algo: (str) RL algorithm name\n    """"""\n    args = [\'--algo\', algo, \'--srl-model\', DEFAULT_SRL, \'--num-timesteps\', NUM_TRAIN_TIMESTEP, \'--seed\', SEED,\n            \'--num-iteration\', NUM_ITERATION, \'--no-vis\', \'--log-dir\', DEFAULT_LOG_DIR, \'--env\', DEFAULT_ENV,\n            \'--min-episodes-save\', 1]\n    if algo == ""ddpg"" or algo == ""sac"":\n        args.extend([\'-c\'])\n        if algo == ""ddpg"":\n            args.extend([\'--memory-limit\', 100])\n    elif algo == ""acer"":\n        args.extend([\'--num-stack\', 4])\n\n    if algo in [""acer"", ""a2c"", ""ppo2""]:\n        args.extend([\'--num-cpu\', 4])\n\n    args = list(map(str, args))\n\n    ok = subprocess.call([\'python\', \'-m\', \'rl_baselines.pipeline\'] + args)\n    assertEq(ok, 0)\n\n\n@pytest.mark.slow\n@pytest.mark.parametrize(""algo"", [\'a2c\', \'acer\', \'acktr\', \'ars\', \'cma-es\', \'ddpg\', \'deepq\', \'ppo1\', \'ppo2\', \'sac\',\n                                  \'trpo\'])\ndef testEnjoyBaselines(algo):\n    """"""\n    test the enjoy script for the given RL algorithm\n    :param algo: (str) RL algorithm name\n    """"""\n    if isXAvailable():\n        directory = sorted(glob.glob(""logs/test_enjoy/{}/{}/{}/*"".format(DEFAULT_ENV, DEFAULT_SRL, algo)))[-1] + ""/""\n\n        args = [\'--log-dir\', directory, \'--num-timesteps\', NUM_ENJOY_TIMESTEP, \'--plotting\', \'--action-proba\']\n        args = list(map(str, args))\n\n        ok = subprocess.call([\'python\', \'-m\', \'replay.enjoy_baselines\'] + args)\n        assertEq(ok, 0)\n    else:\n        print(""X not available, ignoring test"")\n'"
tests/test_hyperparam_search.py,0,"b'from __future__ import print_function, division, absolute_import\n\nimport subprocess\n\nimport pytest\n\nDEFAULT_OPTIMIZER = \'hyperband\'\nDEFAULT_ALGO = ""ppo2""\nDEFAULT_ENV = ""MobileRobotGymEnv-v0""\nDEFAULT_SRL = ""ground_truth""\nNUM_ITERATION = 1\nNUM_TIMESTEP = 10000  # this should be long enough to call a reset of the environment\nMAX_EVAL = 2  # hyperopt evals\nSEED = 0\n\n\ndef assertEq(left, right):\n    assert left == right, ""{} != {}"".format(left, right)\n\n\ndef assertNeq(left, right):\n    assert left != right, ""{} == {}"".format(left, right)\n\n\n@pytest.mark.slow\n@pytest.mark.parametrize(""optimizer"", [\'hyperband\', \'hyperopt\'])\ndef testHyperparamOptimizer(optimizer):\n    """"""\n    test for the given hyperparam optimizer\n    :param optimizer: (str) RL algorithm name\n    """"""\n    args = [\'--optimizer\', optimizer, \'--algo\', DEFAULT_ALGO, \'--srl-model\', DEFAULT_SRL, \'--max-eval\', MAX_EVAL,\n            \'--num-timesteps\', NUM_TIMESTEP, \'--seed\', SEED, \'--env\', DEFAULT_ENV, ""--num-cpu"", 4]\n\n    args = list(map(str, args))\n\n    ok = subprocess.call([\'python\', \'-m\', \'rl_baselines.hyperparam_search\'] + args)\n    assertEq(ok, 0)\n\n\n@pytest.mark.slow\n@pytest.mark.parametrize(""algo"", [\'a2c\', \'acer\', \'acktr\', \'ars\', \'cma-es\', \'ddpg\', \'deepq\', \'ppo1\', \'sac\', \'trpo\'])\ndef testRLHyperparamSearch(algo):\n    """"""\n    test for the given RL algorithm\n    :param algo: (str) RL algorithm name\n    """"""\n    args = [\'--optimizer\', DEFAULT_OPTIMIZER, \'--algo\', algo, \'--srl-model\', DEFAULT_SRL, \'--max-eval\', MAX_EVAL,\n            \'--num-timesteps\', NUM_TIMESTEP, \'--seed\', SEED, \'--env\', DEFAULT_ENV]\n    if algo == ""ddpg"" or algo == ""sac"":\n        args.extend([\'-c\'])\n        if algo == ""ddpg"":\n            args.extend([\'--memory-limit\', 100000])\n    elif algo == ""acer"":\n        args.extend([\'--num-stack\', 4])\n\n    if algo in [""acer"", ""a2c"", ""ppo2""]:\n        args.extend([""--num-cpu"", 4])\n\n    args = list(map(str, args))\n\n    ok = subprocess.call([\'python\', \'-m\', \'rl_baselines.hyperparam_search\'] + args)\n    assertEq(ok, 0)\n'"
tests/test_pipeline.py,0,"b'from __future__ import print_function, division, absolute_import\n\nimport subprocess\n\nimport pytest\n\nfrom environments import ThreadingType\nfrom environments.registry import registered_env\n\nDEFAULT_ALGO = ""ppo2""\nDEFAULT_ENV = ""MobileRobotGymEnv-v0""\nDEFAULT_SRL = ""ground_truth""\nNUM_ITERATION = 1\nNUM_TIMESTEP = 1600  # this should be long enough to call a reset of the environment\nSEED = 0\n\n\ndef isXAvailable():\n    """"""\n    check to see if running in terminal with X or not\n    :return: (bool)\n    """"""\n    p = subprocess.Popen([""xset"", ""-q""], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    p.communicate()\n    return p.returncode == 0\n\n\ndef assertEq(left, right):\n    assert left == right, ""{} != {}"".format(left, right)\n\n\ndef assertNeq(left, right):\n    assert left != right, ""{} == {}"".format(left, right)\n\n\n# ignoring \'acktr\', as it will run out of memory and crash tensorflow\'s allocation\n@pytest.mark.parametrize(""algo"", [\'a2c\', \'acer\', \'ars\', \'cma-es\', \'ddpg\', \'deepq\', \'ppo1\', \'ppo2\', \'random_agent\',\n                                  \'sac\', \'trpo\'])\n@pytest.mark.parametrize(""model_type"", [\'raw_pixels\'])\ndef testBaselineTrain(algo, model_type):\n    """"""\n    test for the given RL algorithm\n    :param algo: (str) RL algorithm name\n    :param model_type: (str) the model type to test\n    """"""\n    args = [\'--algo\', algo, \'--srl-model\', model_type, \'--num-timesteps\', NUM_TIMESTEP, \'--seed\', SEED,\n            \'--num-iteration\', NUM_ITERATION, \'--no-vis\', \'--env\', DEFAULT_ENV]\n    if algo == ""ddpg"" or algo == ""sac"":\n        # Prevent RAM issue because of the replay buffer\n        mem_limit = 100 if model_type == \'raw_pixels\' else 100000\n        args.extend([\'-c\'])\n        if algo == ""ddpg"":\n            args.extend([\'--memory-limit\', mem_limit])\n    elif algo == ""acer"":\n        args.extend([\'--num-stack\', 4])\n\n    if algo in [""acer"", ""a2c"", ""ppo2""]:\n        args.extend([""--num-cpu"", 4])\n\n    args = list(map(str, args))\n\n    ok = subprocess.call([\'python\', \'-m\', \'rl_baselines.pipeline\'] + args)\n    assertEq(ok, 0)\n\n\n@pytest.mark.parametrize(""model_type"", [\'ground_truth\', \'raw_pixels\', \'joints\', \'joints_position\'])\n@pytest.mark.parametrize(""env"", [""KukaButtonGymEnv-v0"", ""MobileRobotGymEnv-v0"", ""CarRacingGymEnv-v0""])\ndef testEnvSRLTrain(model_type, env):\n    """"""\n    test the environment states model on RL algorithms\n    :param model_type: (str) the model type to test\n    :param env: (str) the environment type to test\n    """"""\n    if env in [""CarRacingGymEnv-v0""] and isXAvailable():\n        # Catch if X available, but GL context unavailable.\n        # This prevents SSH crashing when X is passed without GL context.\n        try:\n            from environments.car_racing.car_env import CarRacingEnv\n        except:\n            return\n\n    if model_type in [\'joints\', \'joints_position\'] and env != ""KukaButtonGymEnv-v0"":\n        return\n\n    args = [\'--algo\', DEFAULT_ALGO, \'--env\', env, \'--srl-model\', model_type, \'--num-timesteps\', NUM_TIMESTEP,\n            \'--seed\', SEED, \'--num-iteration\', NUM_ITERATION, \'--no-vis\']\n    if registered_env[env][3] != ThreadingType.NONE:\n        args.extend([\'--num-cpu\', 4])\n    args = list(map(str, args))\n\n    ok = subprocess.call([\'python\', \'-m\', \'rl_baselines.pipeline\'] + args)\n    assertEq(ok, 0)\n\n\n@pytest.mark.fast\n@pytest.mark.parametrize(""env"", [""KukaRandButtonGymEnv-v0"", ""Kuka2ButtonGymEnv-v0"", ""KukaMovingButtonGymEnv-v0"",\n                                 ""MobileRobot2TargetGymEnv-v0"", ""MobileRobot1DGymEnv-v0"",\n                                 ""MobileRobotLineTargetGymEnv-v0"", ""OmnirobotEnv-v0""])\ndef testEnvTrain(env):\n    """"""\n    test the environment on the RL pipeline\n    :param env: (str) the environment type to test\n    """"""\n    args = [\'--algo\', DEFAULT_ALGO, \'--env\', env, \'--srl-model\', DEFAULT_SRL, \'--num-timesteps\', NUM_TIMESTEP,\n            \'--seed\', SEED, \'--num-iteration\', NUM_ITERATION, \'--no-vis\']\n    if registered_env[env][3] != ThreadingType.NONE:\n        args.extend([\'--num-cpu\', 4])\n    args = list(map(str, args))\n\n    ok = subprocess.call([\'python\', \'-m\', \'rl_baselines.pipeline\'] + args)\n    assertEq(ok, 0)\n\n\n@pytest.mark.fast\n@pytest.mark.parametrize(""env"", [""KukaButtonGymEnv-v0"", ""MobileRobotGymEnv-v0"", ""CarRacingGymEnv-v0"",\n                                 ""OmnirobotEnv-v0""])\n@pytest.mark.parametrize(""algo"", [\'a2c\', \'ppo1\', \'ppo2\', \'sac\', \'trpo\'])\ndef testContinousEnvTrain(env, algo):\n    """"""\n    test the environment on the RL pipeline with continuous actions\n    :param env: (str) the environment type to test\n    :param algo: (str) RL algorithm name\n    """"""\n    if env in [""CarRacingGymEnv-v0""] and isXAvailable():\n        # Catch if X available, but GL context unavailable.\n        # This prevents SSH crashing when X is passed without GL context.\n        try:\n            from environments.car_racing.car_env import CarRacingEnv\n        except:\n            return\n\n    args = [\'--algo\', algo, \'--env\', env, \'--srl-model\', DEFAULT_SRL, \'--num-timesteps\', NUM_TIMESTEP,\n            \'--seed\', SEED, \'--num-iteration\', NUM_ITERATION, \'--no-vis\', \'-c\']\n    if algo in [\'ppo2\'] and registered_env[env][3] != ThreadingType.NONE:\n        args.extend([\'--num-cpu\', 4])\n    args = list(map(str, args))\n\n    ok = subprocess.call([\'python\', \'-m\', \'rl_baselines.pipeline\'] + args)\n    assertEq(ok, 0)\n'"
environments/car_racing/__init__.py,0,b''
