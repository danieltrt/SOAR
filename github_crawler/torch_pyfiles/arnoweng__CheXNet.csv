file_path,api_count,code
model.py,12,"b'# encoding: utf-8\n\n""""""\nThe main CheXNet model implementation.\n""""""\n\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom read_data import ChestXrayDataSet\nfrom sklearn.metrics import roc_auc_score\n\n\nCKPT_PATH = \'model.pth.tar\'\nN_CLASSES = 14\nCLASS_NAMES = [ \'Atelectasis\', \'Cardiomegaly\', \'Effusion\', \'Infiltration\', \'Mass\', \'Nodule\', \'Pneumonia\',\n                \'Pneumothorax\', \'Consolidation\', \'Edema\', \'Emphysema\', \'Fibrosis\', \'Pleural_Thickening\', \'Hernia\']\nDATA_DIR = \'./ChestX-ray14/images\'\nTEST_IMAGE_LIST = \'./ChestX-ray14/labels/test_list.txt\'\nBATCH_SIZE = 64\n\n\ndef main():\n\n    cudnn.benchmark = True\n\n    # initialize and load the model\n    model = DenseNet121(N_CLASSES).cuda()\n    model = torch.nn.DataParallel(model).cuda()\n\n    if os.path.isfile(CKPT_PATH):\n        print(""=> loading checkpoint"")\n        checkpoint = torch.load(CKPT_PATH)\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        print(""=> loaded checkpoint"")\n    else:\n        print(""=> no checkpoint found"")\n\n    normalize = transforms.Normalize([0.485, 0.456, 0.406],\n                                     [0.229, 0.224, 0.225])\n\n    test_dataset = ChestXrayDataSet(data_dir=DATA_DIR,\n                                    image_list_file=TEST_IMAGE_LIST,\n                                    transform=transforms.Compose([\n                                        transforms.Resize(256),\n                                        transforms.TenCrop(224),\n                                        transforms.Lambda\n                                        (lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n                                        transforms.Lambda\n                                        (lambda crops: torch.stack([normalize(crop) for crop in crops]))\n                                    ]))\n    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE,\n                             shuffle=False, num_workers=8, pin_memory=True)\n\n    # initialize the ground truth and output tensor\n    gt = torch.FloatTensor()\n    gt = gt.cuda()\n    pred = torch.FloatTensor()\n    pred = pred.cuda()\n\n    # switch to evaluate mode\n    model.eval()\n\n    for i, (inp, target) in enumerate(test_loader):\n        target = target.cuda()\n        gt = torch.cat((gt, target), 0)\n        bs, n_crops, c, h, w = inp.size()\n        input_var = torch.autograd.Variable(inp.view(-1, c, h, w).cuda(), volatile=True)\n        output = model(input_var)\n        output_mean = output.view(bs, n_crops, -1).mean(1)\n        pred = torch.cat((pred, output_mean.data), 0)\n\n    AUROCs = compute_AUCs(gt, pred)\n    AUROC_avg = np.array(AUROCs).mean()\n    print(\'The average AUROC is {AUROC_avg:.3f}\'.format(AUROC_avg=AUROC_avg))\n    for i in range(N_CLASSES):\n        print(\'The AUROC of {} is {}\'.format(CLASS_NAMES[i], AUROCs[i]))\n\n\ndef compute_AUCs(gt, pred):\n    """"""Computes Area Under the Curve (AUC) from prediction scores.\n\n    Args:\n        gt: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n          true binary labels.\n        pred: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n          can either be probability estimates of the positive class,\n          confidence values, or binary decisions.\n\n    Returns:\n        List of AUROCs of all classes.\n    """"""\n    AUROCs = []\n    gt_np = gt.cpu().numpy()\n    pred_np = pred.cpu().numpy()\n    for i in range(N_CLASSES):\n        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n    return AUROCs\n\n\nclass DenseNet121(nn.Module):\n    """"""Model modified.\n\n    The architecture of our model is the same as standard DenseNet121\n    except the classifier layer which has an additional sigmoid function.\n\n    """"""\n    def __init__(self, out_size):\n        super(DenseNet121, self).__init__()\n        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n        num_ftrs = self.densenet121.classifier.in_features\n        self.densenet121.classifier = nn.Sequential(\n            nn.Linear(num_ftrs, out_size),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.densenet121(x)\n        return x\n\n\nif __name__ == \'__main__\':\n    main()'"
read_data.py,2,"b'# encoding: utf-8\n\n""""""\nRead images and corresponding labels.\n""""""\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport os\n\n\nclass ChestXrayDataSet(Dataset):\n    def __init__(self, data_dir, image_list_file, transform=None):\n        """"""\n        Args:\n            data_dir: path to image directory.\n            image_list_file: path to the file containing images\n                with corresponding labels.\n            transform: optional transform to be applied on a sample.\n        """"""\n        image_names = []\n        labels = []\n        with open(image_list_file, ""r"") as f:\n            for line in f:\n                items = line.split()\n                image_name= items[0]\n                label = items[1:]\n                label = [int(i) for i in label]\n                image_name = os.path.join(data_dir, image_name)\n                image_names.append(image_name)\n                labels.append(label)\n\n        self.image_names = image_names\n        self.labels = labels\n        self.transform = transform\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index: the index of item\n\n        Returns:\n            image and its labels\n        """"""\n        image_name = self.image_names[index]\n        image = Image.open(image_name).convert(\'RGB\')\n        label = self.labels[index]\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, torch.FloatTensor(label)\n\n    def __len__(self):\n        return len(self.image_names)\n\n'"
