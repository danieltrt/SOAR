file_path,api_count,code
setup.py,0,"b'\n\n""""""\nsetup.py\n""""""\n\nfrom setuptools import setup, find_packages\nfrom typing import Dict\nimport os\n\n\nNAME = ""flambe""\nAUTHOR = ""ASAPP Inc.""\nEMAIL = ""flambe@asapp.com""\nDESCRIPTION = ""Pytorch based library for robust prototyping, standardized  \\\n               benchmarking, and effortless experiment management""\n\n\ndef readme():\n    with open(\'README-pypi.rst\', encoding=\'utf-8\') as f:\n        return f.read()\n\n\ndef required():\n    with open(\'requirements.txt\') as f:\n        return f.read().splitlines()\n\n\n# So that we don\'t import flambe.\nVERSION: Dict[str, str] = {}\nwith open(""flambe/version.py"", ""r"") as version_file:\n    exec(version_file.read(), VERSION)\n\nsetup(\n\n    name=NAME,\n    version=os.environ.get(""TAG_VERSION"", VERSION[\'VERSION\']),\n\n    description=DESCRIPTION,\n    long_description=readme(),\n    long_description_content_type=""text/x-rst; charset=UTF-8"",\n\n    # Author information\n    author=AUTHOR,\n    author_email=EMAIL,\n\n    # What is packaged here.\n    packages=find_packages(exclude=(""tests"", ""tests.*"", ""extensions"")),\n    scripts=[\n        \'bin/flambe\',\n        \'bin/flambe-site\'\n    ],\n\n    install_requires=required(),\n    include_package_data=True,\n\n    python_requires=\'>=3.6.1\',\n    zip_safe=True\n\n)\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Flamb\xc3\xa9 documentation build configuration file, created by\n# sphinx-quickstart on Fri Oct 12 02:40:58 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another\n# directory, add these directories to sys.path here. If the directory\n# is relative to the documentation root, use os.path.abspath to make it\n# absolute, like shown here.\n#\n\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\'../\'))\n\nversion_dict = dict()  # type: ignore\nwith open(""../flambe/version.py"", ""r"") as version_file:\n    exec(version_file.read(), version_dict)\n\nVERSION = version_dict[\'VERSION\']\n\n# -- General configuration -------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\n\nextensions = [\'sphinx.ext.autodoc\',\n              \'sphinx.ext.autosummary\',\n              \'sphinx.ext.napoleon\',\n              \'sphinx_autodoc_typehints\',\n              \'sphinx.ext.coverage\',\n              \'sphinx.ext.viewcode\',\n              \'sphinx.ext.githubpages\',\n              \'autoapi.extension\']\n\n\nautoapi_dirs = [\'../flambe\']\nautoapi_add_toctree_entry = False\n# autoapi_generate_api_docs = False\n\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'Flamb\xc3\xa9\'\ncopyright = \'2019, ASAPP Inc.\'\nauthor = \'ASAPP Inc.\'\n\n# The version info for the project you\'re documenting, acts as\n# replacement for |version| and |release|, also used in various other\n# places throughout the built documents.\n#\n# The short X.Y version.\nversion = VERSION  # ""."".join(VERSION.split(\'.\')[:2])\n# The full version, including alpha/beta/rc tags.\nrelease = VERSION\n\n# The language for content autogenerated by Sphinx. Refer to\n# documentation for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n# Options: [\'default\', \'emacs\', \'friendly\', \'colorful\', \'autumn\', \'murphy\', \'manni\', \'monokai\', \'perldoc\', \'pastie\', \'borland\', \'trac\', \'native\', \'fruity\', \'bw\', \'vim\', \'vs\', \'tango\', \'rrt\', \'xcode\', \'igor\', \'paraiso-light\', \'paraiso-dark\', \'lovelace\', \'algol\', \'algol_nu\', \'arduino\', \'rainbow_dash\', \'abap\', \'solarized-dark\', \'solarized-light\', \'sas\', \'stata\', \'stata-light\', \'stata-dark\']\n\n# If true, `todo` and `todoList` produce output,\n# else they produce nothing.\ntodo_include_todos = False\nautodoc_inherit_docstrings = False\nautoclass_content = \'both\'\nnumpydoc_show_class_members = False\n\n\n# -- Options for HTML output -----------------------------------------\n\n# The theme to use for HTML and HTML Help pages.\n# See the documentation for a list of builtin themes.\n#\n# html_theme = \'pytorch_sphinx_theme\'\nhtml_theme = ""sphinx_rtd_theme""\n# html_theme = ""press""\n\n\n# html_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a\n# theme further.  For a list of options available for each theme,\n# see the documentation.\n#\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': True,\n}\n\n# Add any paths that contain custom static files (such as style sheets)\n# here, relative to this directory. They are copied after the builtin\n# static files, so a file named ""default.css"" will overwrite the builtin\n# ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps\n# document names to template names.\n#\n# This is required for the alabaster theme. refs:\n# http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    \'**\': [\n        \'about.html\',\n        \'navigation.html\',\n        \'relations.html\',  # needs \'show_related\': True theme option to display\n        \'searchbox.html\',\n        \'donate.html\',\n    ]\n}\n\nhtml_css_files = [\n    \'css/custom.css\',\n]\n\nhtml_logo = ""image/Flambe_Logo_RGB_FullColor.png""\n\nhtml_favicon = \'image/ASAPP_Flambe_Symbol_RGB_Pink.png\'\n\n# -- Options for HTMLHelp output --------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'Flambdoc\'\n\n\n# -- Options for LaTeX output -----------------------------------------\n\nlatex_elements = {  # type: ignore\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'Flambe.tex\', \'Flamb\xc3\xa9 Documentation\',\n     \'ASAPP Inc.\', \'manual\'),\n]\n\n\n# -- Options for manual page output -----------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'flambe\', \'Flamb\xc3\xa9 Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ---------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'Flamb\xc3\xa9\', \'Flamb\xc3\xa9 Documentation\',\n     author, \'Flamb\xc3\xa9\', \'Everyday Flamb\xc3\xa9, Flamb\xc3\xa9 everyday.\',\n     \'Miscellaneous\'),\n]\n\n\n# def setup(app):\n#     app.add_css_file(\'custom.css\')\n'"
flambe/__init__.py,0,"b""# flake8: noqa: E402\n\nimport logging as main_logging\n\n# Work based on https://github.com/tensorflow/tensorflow/issues/26691\n# This check is done to avoid tensorflow import (done by pytorch 1.1)\n# break logging.\ntry:\n    # Tensorflow uses Google's abseil-py library, which uses a Google-specific\n    # wrapper for logging. That wrapper will write a warning to sys.stderr if\n    # the Google command-line flags library has not been initialized.\n    #\n    # https://github.com/abseil/abseil-py/blob/pypi-v0.7.1/absl/logging/__init__.py#L819-L825\n    #\n    # This is not right behavior for Python code that is invoked outside of a\n    # Google-authored main program. Use knowledge of abseil-py to disable that\n    # warning; ignore and continue if something goes wrong.\n    import absl.logging\n\n    # https://github.com/abseil/abseil-py/issues/99\n    main_logging.root.removeHandler(absl.logging._absl_handler)\n    # https://github.com/abseil/abseil-py/issues/102\n    absl.logging._warn_preinit_stderr = False\nexcept Exception:\n    pass\n\nmain_logging.disable(main_logging.WARNING)\n\nfrom flambe.compile import Component, Schema, save, load\nfrom flambe.compile import save_state_to_file, load_state_from_file\nfrom flambe.logging import log\nfrom flambe import compile, dataset, experiment, field, learn, nlp, vision, export, model\nfrom flambe import cluster, metric, nn, runner, sampler, runnable, tokenizer, optim\nfrom flambe.version import VERSION as __version__\nfrom flambe.logo import ASCII_LOGO\n\n\n__all__ = ['Component', 'Schema', 'log', 'tokenizer',\n           'compile', 'dataset', 'experiment', 'field', 'learn', 'export',\n           'cluster', 'metric', 'nn', 'model', 'optim', 'runner', 'runnable', 'sampler',\n            'nlp', 'vision', '__version__', 'ASCII_LOGO',  'save', 'load',\n           'load_state_from_file', 'save_state_to_file']\n\n\nmain_logging.disable(main_logging.NOTSET)\n"""
flambe/const.py,0,"b'from pathlib import Path\nimport os\n\nFLAMBE_GLOBAL_FOLDER = os.path.join(str(Path.home()), "".flambe"")\n'"
flambe/logo.py,0,"b'ASCII_LOGO = """"""\n\n/$$$$$$$$ /$$                         /$$             /$\n| $$_____/| $$                        | $$           /$\n| $$      | $$  /$$$$$$  /$$$$$$/$$$$ | $$$$$$$   /$$$$$$\n| $$$$$   | $$ |____  $$| $$_  $$_  $$| $$__  $$ /$$__  $$\n| $$__/   | $$  /$$$$$$$| $$ \\ $$ \\ $$| $$  \\ $$| $$$$$$$$\n| $$      | $$ /$$__  $$| $$ | $$ | $$| $$  | $$| $$_____/\n| $$      | $$|  $$$$$$$| $$ | $$ | $$| $$$$$$$/|  $$$$$$$\n|__/      |__/ \\_______/|__/ |__/ |__/|_______/  \\_______/\n\n\n""""""  # noqa: W605""""""\n\nASCII_LOGO_DEV = """"""\n\n/$$$$$$$$ /$$                         /$$             /$\n| $$_____/| $$                        | $$           /$\n| $$      | $$  /$$$$$$  /$$$$$$/$$$$ | $$$$$$$   /$$$$$$\n| $$$$$   | $$ |____  $$| $$_  $$_  $$| $$__  $$ /$$__  $$     _\n| $$__/   | $$  /$$$$$$$| $$ \\ $$ \\ $$| $$  \\ $$| $$$$$$$$    | |\n| $$      | $$ /$$__  $$| $$ | $$ | $$| $$  | $$| $$_____/  __| | _____   __\n| $$      | $$|  $$$$$$$| $$ | $$ | $$| $$$$$$$/|  $$$$$$$ / _` |/ _ \\ \\ / /\n|__/      |__/ \\_______/|__/ |__/ |__/|_______/  \\_______/| (_| |  __/\\ V /\n                                                           \\__,_|\\___| \\_/\n\n""""""  # noqa: W605""""""\n'"
flambe/version.py,0,"b'MAJOR = ""0""\nMINOR = ""4""\nPATCH = ""18""\n\nVERSION = f\'{MAJOR}.{MINOR}.{PATCH}\'\n'"
scripts/deploy_documentation.py,0,"b'import mimetypes\nimport boto3\nimport os\nfrom tqdm import tqdm\nimport sys\n\n\nif len(sys.argv) < 2:\n    raise ValueError(""Need to provide the Bucket Name"")\n\n\nFLAMBE_BUCKET_NAME = sys.argv[1]\n\n\ndef get_flambe_bucket(s3):\n    for b in s3.buckets.all():\n        if b.name == FLAMBE_BUCKET_NAME:\n            return b\n\n    raise Exception(""Flambe bucket not found"")\n\n\ndef get_mime_type(path):\n    mimetype, _ = mimetypes.guess_type(path)\n    if mimetype is None:\n        raise Exception(""Failed to guess mimetype"")\n    return mimetype\n\n\ndef upload_documentation(bucket, doc_html_dir):\n    # enumerate local files recursively\n    for root, dirs, files in tqdm(os.walk(doc_html_dir)):\n        for filename in files:\n            # construct the full local path\n            local_path = os.path.join(root, filename)\n\n            # construct the full path\n            s3_path = os.path.relpath(local_path, doc_html_dir)\n            try:\n                bucket.upload_file(local_path, s3_path, ExtraArgs={\n                    ""ContentType"": get_mime_type(local_path)\n                })\n            except Exception:\n                print(f""Could not upload {s3_path}."")\n\n\nif __name__ == ""__main__"":\n    _dir = os.path.dirname(os.path.abspath(__file__))\n    html_dir = os.path.join(_dir, "".."", ""docs"", ""_build"", ""html"")\n\n    if not os.path.exists(html_dir):\n        print(""Docuementation HTML not found. Were the docs built?"")\n        sys.exit(1)\n\n    s3 = boto3.resource(\'s3\')\n    b = get_flambe_bucket(s3)\n    upload_documentation(b, html_dir)\n'"
tests/__init__.py,0,b''
tests/conftest.py,0,b'import os\nimport pytest\n\n\n@pytest.fixture\ndef top_level():\n    parent = os.path.dirname\n    return parent(parent(os.path.abspath(__file__)))\n'
flambe/cluster/__init__.py,0,"b""from flambe.cluster.cluster import Cluster\nfrom flambe.cluster.aws import AWSCluster\nfrom flambe.cluster.ssh import SSHCluster\n\n\n__all__ = ['Cluster', 'AWSCluster', 'SSHCluster']\n"""
flambe/cluster/aws.py,0,"b'""""""Implementation of a Cluster with AWS EC2 as the cloud provider\n\n""""""\n\nimport boto3\nimport botocore\nimport logging\nfrom datetime import datetime, timedelta\n\nfrom typing import Generator, Dict, Tuple, List, TypeVar, Type, Any, Optional\n\nfrom flambe.cluster import instance, const\nfrom flambe.cluster.cluster import Cluster, FactoryInsT\nfrom flambe.cluster import errors\nfrom flambe.logging import coloredlogs as cl\n\nfrom concurrent.futures import ThreadPoolExecutor\n\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(""T"", instance.OrchestratorInstance, instance.GPUFactoryInstance,\n            instance.CPUFactoryInstance)\n\n\nclass AWSCluster(Cluster):\n    """"""This Cluster implementation uses AWS EC2 as the cloud provider.\n\n    This cluster works with AWS Instances that are defined in:\n    `flambe.remote.instance.aws`\n\n    Parameters\n    ----------\n    name: str\n        The unique name for the cluster\n    factories_num : int\n        The amount of factories to use. This is not the amount of\n        workers, as each factories can contain multiple GPUs and\n        therefore, multiple workers.\n    factories_type : str\n        The type of instance to use for the Factory Instances.\n        GPU instances are required for AWS the AWSCluster.\n        ""p2"" and ""p3"" instances are recommended.\n    factory_ami : str\n        The AMI to be used for the Factory instances. Custom Flambe AMI\n        are provided based on Ubuntu 18.04 distribution.\n    orchestrator_type : str\n        The type of instance to use for the Orchestrator Instances.\n        This may not be a GPU instances. At least a ""t2.small"" instance\n        is recommended.\n    key_name: str\n        The key name that will be used to connect into the instance.\n    creator: str\n        The creator should be a user identifier for the instances.\n        This information will create a tag called \'creator\' and it will\n        also be used to retrieve existing hosts owned by the user.\n    key: str\n        The path to the ssh key used to communicate to all instances.\n        IMPORTANT: all instances must be accessible with the same key.\n    volume_type: str\n        The type of volume in AWS to use. Only \'gp2\' and \'io1\' are\n        currently available. If \'io1\' is used, then IOPS will be\n        fixed to 5000. IMPORTANT: \'io1\' volumes are significantly\n        more expensive than \'gp2\' volumes.\n        Defaults to \'gp2\'.\n    region_name: Optional[str]\n        The region name to use. If not specified, it uses the locally\n        configured region name or \'us-east-1\' in case it\'s not\n        configured.\n    username: str\n        The username of the instances the cluster will handle. Defaults\n        to \'ubuntu\'.\n        IMPORTANT: for now all instances need to have the same username.\n    tags: Dict[str, str]\n        A dictionary with tags that will be added to all created hosts.\n    security_group: str\n        The security group to use to create the instances.\n    subnet_id: str\n        The subnet ID to use.\n    orchestrator_ami : str\n        The AMI to be used for the Factory instances. Custom Flambe\n        AMI are provided based on Ubuntu 18.04 distribution.\n    dedicated: bool\n        Wether all created instances are dedicated instances or shared.\n    orchestrator_timeout: int\n        Number of consecutive hours before terminating the orchestrator\n        once the experiment is over (either success of failure).\n        Specify -1 to disable automatic shutdown (the orchestrator\n        will stay on until manually terminated) and 0 to shutdown when\n        the experiment is over. For example, if specifying 24, then the\n        orchestrator will be shut down one day after the experiment is\n        over. ATTENTION: This also applies when the experiment ends\n        with an error. Default is -1.\n    factories_timeout: int\n        Number of consecutive hours to automatically terminate factories\n        once the experiment is over (either success or failure).\n        Specify -1 to disable automatic shutdown (the factories will\n        stay on until manually terminated) and 0 to shutdown when the\n        experiment is over. For example, if specifying 10, then the\n        factories will be shut down 10 hours after the experiment is\n        over. ATTENTION: This also applies when the experiment ends\n        with an error. Default is 1.\n    volume_size: int\n        The disk size in GB that all hosts will contain. Defaults to\n        100 GB.\n    setup_cmds: Optional[List[str]]\n        A list of commands to be run on all hosts for setup purposes.\n        These commands can be used to mount volumes, install software,\n        etc. Defaults to None.\n        IMPORTANT: the commands need to be idempotent and they shouldn\'t\n        expect user input.\n\n\n    """"""\n    def __init__(self,\n                 name: str,\n                 factories_num: int,\n                 factories_type: str,\n                 orchestrator_type: str,\n                 key_name: str,\n                 security_group: str,\n                 subnet_id: str,\n                 creator: str,\n                 key: str,\n                 volume_type: str = \'gp2\',\n                 region_name: Optional[str] = None,\n                 username: str = ""ubuntu"",\n                 tags: Dict[str, str] = None,\n                 orchestrator_ami: str = None,\n                 factory_ami: str = None,\n                 dedicated: bool = False,\n                 orchestrator_timeout: int = -1,\n                 factories_timeout: int = 1,\n                 volume_size: int = 100,\n                 setup_cmds: Optional[List[str]] = None) -> None:\n        super().__init__(name, factories_num, key, username, setup_cmds)\n\n        self.factories_type = factories_type\n        self.orchestrator_type = orchestrator_type\n\n        self.region_name = region_name\n        self.sess = self._get_boto_session(region_name)\n\n        self.ec2_resource = self.sess.resource(\'ec2\')\n        self.ec2_cli = self.sess.client(\'ec2\')\n        self.cloudwatch = self.sess.client(\'cloudwatch\')\n\n        self.factory_ami = factory_ami\n        self.orchestrator_ami = orchestrator_ami\n\n        self.key_name = key_name\n\n        self.creator = creator\n        self.tags = tags\n\n        self.security_group = security_group\n        self.subnet_id = subnet_id\n        self.volume_size = volume_size\n\n        self.dedicated = dedicated\n\n        self.orchestrator_timeout = orchestrator_timeout\n        self.factories_timeout = factories_timeout\n\n        self.created_instances_ids: List[str] = []\n\n        if volume_type not in [\'gp2\', \'io1\']:\n            raise ValueError(""Only gp2 and io1 drives available."")\n\n        self.volume_type = volume_type\n\n    def _get_boto_session(self, region_name: Optional[str]) -> boto3.Session:\n        """"""Get the boto3 Session from which the resources\n        and clients will be created.\n\n        This method is called by the contructor.\n\n        Parameters\n        ----------\n        region_name: Optional[str]\n            The region to use. If None, boto3 will resolve to the\n            locally configured region_name or \'us-east-1\' if not\n            configured.\n\n        Returns\n        -------\n        boto3.Session\n            The boto3 Session to use\n\n        """"""\n        return boto3.Session(region_name=region_name)\n\n    def load_all_instances(self) -> None:\n        """"""Launch all instances for the experiment.\n\n        This method launches both  the orchestrator and the factories.\n\n        """"""\n        boto_orchestrator, boto_factories = self._existing_cluster()\n\n        with ThreadPoolExecutor() as executor:\n            future_orch, future_factories = None, None\n\n            if boto_orchestrator:\n                self.orchestrator = self.get_orchestrator(\n                    self._get_boto_public_host(boto_orchestrator),\n                    self._get_boto_private_host(boto_orchestrator)\n                )\n                logger.info(cl.BL(\n                    f""Found existing orchestrator ({boto_orchestrator.instance_type}) "" +\n                    f""{self.orchestrator.host}""\n                ))\n\n            else:\n                future_orch = executor.submit(self._create_orchestrator)\n\n            for f in boto_factories:\n                factory = self.get_factory(\n                    self._get_boto_public_host(f),\n                    self._get_boto_private_host(f)\n                )\n                if factory.contains_gpu():\n                    factory = self.get_gpu_factory(\n                        self._get_boto_public_host(f),\n                        self._get_boto_private_host(f)\n                    )\n                self.factories.append(factory)\n\n            if len(self.factories) > 0:\n                logger.info(cl.BL(f""Found {len(self.factories)} existing factories "" +\n                                  f""({str([f.host for f in self.factories])}).""))\n\n            pending_new_factories = self.factories_num - len(self.factories)\n\n            logger.debug(f""Creating {pending_new_factories} factories"")\n            if pending_new_factories > 0:\n                future_factories = executor.submit(\n                    self._create_factories,\n                    number=pending_new_factories\n                )\n            elif pending_new_factories < 0:\n                logger.info(cl.BL(f""Reusing existing {len(boto_factories)} factories.""))\n\n            try:\n                if future_orch:\n                    self.orchestrator = future_orch.result()\n                    logger.info(cl.BL(f""New orchestrator created {self.orchestrator.host}""))\n\n                if future_factories:\n                    new_factories = future_factories.result()\n                    self.factories.extend(new_factories)\n                    logger.info(cl.BL(\n                        f""{pending_new_factories} factories {self.factories_type} created "" +\n                        f""({str([f.host for f in new_factories])}).""))\n            except botocore.exceptions.ClientError as e:\n                raise errors.ClusterError(\n                    ""Error creating the instances. Check that the provided configuration "" +\n                    f"" is correct. Original error: {e}""\n                )\n\n        self.name_hosts()\n        self.update_tags()\n        self.remove_existing_events()\n        self.create_cloudwatch_events()\n\n    def _existing_cluster(self) -> Tuple[Any, List[Any]]:\n        """"""Whether there is an existing cluster that matches name.\n\n        The cluster should also match all other tags, including Creator)\n\n        Returns\n        -------\n        Tuple[Any, List[Any]]\n            Returns the (boto_orchestrator, [boto_factories])\n            that match the experiment\'s name.\n\n        """"""\n        candidates: List[Tuple[Any, str]] = []\n        for ins, role, cluster_name in self.flambe_own_running_instances():\n            if role and cluster_name:\n                if cluster_name == self.name:\n                    candidates.append((ins, role))\n                    logger.debug(f""Found existing {role} host {ins.public_ip_address}"")\n\n        orchestrator = None\n        factories = []\n\n        for ins, role in candidates:\n            if role == \'Orchestrator\':\n                if orchestrator:\n                    raise errors.ClusterError(\n                        ""Found 2 Orchestrator instances with same experiment name. "" +\n                        ""This should never happen. "" +\n                        ""Please remove manually all instances with tag "" +\n                        f""\'Cluster-Name\': \'{self.name}\' and retry.""\n                    )\n\n                orchestrator = ins\n            elif role == \'Factory\':\n                factories.append(ins)\n\n        return orchestrator, factories\n\n    def _get_existing_tags(self,\n                           boto_instance: ""boto3.resources.factory.ec2.Instance"") -> Dict[str, str]:\n        """"""Gets the tags of a EC2 instances\n\n        Parameters\n        ----------\n        boto_instance : BotoIns\n            The EC2 instance to access the tags.\n\n        Returns\n        -------\n        Dict[str, str]\n            Key, Value for the specified tags.\n\n        """"""\n        ret = {}\n        if boto_instance.tags:\n            for t in boto_instance.tags:\n                ret[t[\'Key\']] = t[\'Value\']\n\n        return ret\n\n    def flambe_own_running_instances(\n            self\n    ) -> Generator[Tuple[\'boto3.resources.factory.ec2.Instance\',\n                         Optional[str], Optional[str]], None, None]:\n        """"""Get running instances with matching tags.\n\n        Yields\n        -------\n        Tuple[\'boto3.resources.factory.ec2.Instance\', str]\n            A tuple with the instance and the name of the EC2 instance.\n\n        """"""\n        boto_instances = self.ec2_resource.instances.filter(\n            Filters=[{\'Name\': \'instance-state-name\', \'Values\': [\'running\']}])\n\n        for ins in boto_instances:\n            tags = self._get_existing_tags(ins)\n            if all((\n                    ""creator"" in tags and tags[""creator""] == self.creator,\n                    ""Purpose"" in tags and tags[""Purpose""] == ""flambe"",\n            )):\n                yield ins, tags.get(""Role""), tags.get(""Cluster-Name"")\n\n    def name_hosts(self) -> None:\n        """"""Name the orchestrator and factories.\n\n        """"""\n        if not self.orchestrator:\n            raise errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        self.name_instance(self._get_boto_instance_by_host(self.orchestrator.host),\n                           self.get_orchestrator_name())\n\n        for i, f in enumerate(self.factories):\n            self.name_instance(self._get_boto_instance_by_host(f.host),\n                               f""{self.get_factory_basename()}_{i+1}"")\n\n    def _get_all_tags(self) -> Dict[str, str]:\n        """"""Get user tags + default tags to add to the instances and\n        volumes.\n\n        """"""\n        ret = {\n            ""creator"": self.creator,\n            ""Purpose"": ""flambe"",\n            ""Cluster-Name"": self.name,\n        }\n\n        if self.tags:\n            for k, v in self.tags.items():\n                ret[k] = v\n\n        return ret\n\n    def update_tags(self) -> None:\n        """"""Update user provided tags to all hosts.\n\n        In case there is an existing cluster that do not contain all the\n        tags, by executing this all hosts will have the user specified\n        tags.\n\n        This won\'t remove existing tags in the hosts.\n\n        """"""\n        if not self.orchestrator:\n            raise errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        tags = self._get_all_tags()\n\n        orch_tags = tags.copy()\n        orch_tags[\'Role\'] = \'Orchestrator\'\n        self._update_tags(self._get_boto_instance_by_host(self.orchestrator.host), orch_tags)\n\n        factory_tags = tags.copy()\n        factory_tags[\'Role\'] = \'Factory\'\n        for i, f in enumerate(self.factories):\n            self._update_tags(self._get_boto_instance_by_host(f.host), factory_tags)\n\n    def _update_tags(\n            self,\n            boto_instance: \'boto3.resources.factory.ec2.Instance\',\n            tags: Dict[str, str]) -> None:\n        """"""Create/Overwrite tags on an EC2 instance and its volumes.\n\n        Parameters\n        ----------\n        boto_instance : \'boto3.resources.factory.ec2.Instance\'\n            The EC2 instance\n        tags : Dict[str, str]\n            The tags to create/overwrite\n\n        """"""\n        boto_tags = [{""Key"": k, ""Value"": v} for k, v in tags.items()]\n\n        boto_instance.create_tags(Tags=boto_tags)\n        for v in boto_instance.volumes.all():\n            v.create_tags(Tags=boto_tags)\n\n    def name_instance(\n            self,\n            boto_instance: \'boto3.resources.factory.ec2.Instance\',\n            name: str) -> None:\n        """"""Renames a EC2 instance\n\n        Parameters\n        ----------\n        boto_instance : \'boto3.resources.factory.ec2.Instance\'\n            The EC2 instance\n        name : str\n            The new name\n\n        """"""\n        name_tag = [{""Key"": ""Name"", ""Value"": name}]\n        boto_instance.create_tags(Tags=name_tag)\n\n        for v in boto_instance.volumes.all():\n            v.create_tags(Tags=name_tag)\n\n    def _create_orchestrator(self) -> instance.OrchestratorInstance:\n        """"""Create a new EC2 instance to be the Orchestrator instance.\n\n        This new machine receives all tags defined in the *.ini file.\n\n        Returns\n        -------\n        instance.AWSOrchestratorInstance\n            The new orchestrator instance.\n\n        """"""\n        if not self.orchestrator_ami:\n            ami = self._find_default_ami(_type=""orchestrator"")\n            if ami is None:\n                raise errors.ClusterError(""Could not find matching AMI for the orchestrator."")\n        else:\n            ami = self.orchestrator_ami\n\n        return self._generic_launch_instances(instance.OrchestratorInstance,\n                                              1, self.orchestrator_type,\n                                              ami, role=""Orchestrator"")[0]\n\n    def _create_factories(self,\n                          number: int = 1) -> List[FactoryInsT]:\n        """"""Creates new AWS EC2 instances to be the Factory instances.\n\n        These new machines receive all tags defined in the *.ini file.\n        Factory instances will be named using the factory basename plus\n        an index. For example, ""seq2seq_factory_0"", ""seq2seq_factory_1"".\n\n        Parameters\n        ----------\n        number : int\n            The number of factories to be created.\n\n        Returns\n        -------\n        List[instance.AWSGPUFactoryInstance]\n            The new factory instances.\n\n        """"""\n        if not self.factory_ami:\n            ami = self._find_default_ami(_type=""factory"")\n            if ami is None:\n                raise errors.ClusterError(""Could not find matching AMI for the factory."")\n        else:\n            ami = self.factory_ami\n\n        factories = self._generic_launch_instances(instance.CPUFactoryInstance,\n                                                   number, self.factories_type,\n                                                   ami, role=""Factory"")\n\n        for i, f in enumerate(factories):\n            f.wait_until_accessible()\n            if f.contains_gpu():\n                factories[i] = instance.GPUFactoryInstance(f.host, f.private_host, f.username,\n                                                           self.key, self.config, self.debug)\n\n        return factories\n\n    def _generic_launch_instances(\n            self, instance_class: Type[T], number: int,\n            instance_type: str, instance_ami: str, role: str\n    ) -> List[T]:\n        """"""Generic method to launch instances in AWS EC2 using boto3.\n\n        This method should not be used outside this module.\n\n        Parameters\n        ----------\n        instance_class: Type[T]\n            The instance class.\n            It can be AWSOrchestratorInstance or AWSGPUFactoryInstance.\n        number : int\n            The amount of instances to create\n        instance_type : str\n            The instance type\n        instance_ami : str\n            The AMI to be used. Should be an Ubuntu 18.04 based AMI.\n        role: str\n            Wether is \'Orchestrator\' or \'Factory\'\n\n        Returns\n        -------\n        List[Union[AWSOrchestratorInstance, AWSGPUFactoryInstance]]\n            The new Instances.\n\n        """"""\n        # Set the tags based on the users + custom flambe tags.\n        tags = self._get_all_tags()\n        tags[\'Role\'] = role\n\n        # Assign a temporary name for the machine at creation time\n        tags[\'Name\'] = self._get_creation_name(role)\n\n        boto_tags: List[Dict[str, str]] = [{\'Key\': k, \'Value\': v} for k, v in tags.items()]\n\n        ebs = {\n            \'VolumeSize\': self.volume_size,\n            \'DeleteOnTermination\': True,\n            \'VolumeType\': self.volume_type,\n        }\n        if self.volume_type == \'io1\':\n            ebs[\'Iops\'] = 5000\n\n        bdm = [\n            {\n                \'DeviceName\': \'/dev/sda1\',\n                \'Ebs\': ebs,\n            }\n        ]\n        tags_param = [\n            {\n                \'ResourceType\': \'instance\',\n                \'Tags\': boto_tags\n            },\n            {\n                \'ResourceType\': \'volume\',\n                \'Tags\': boto_tags\n            },\n        ]\n        placement = {\n            \'Tenancy\': \'dedicated\' if self.dedicated else \'default\',\n        }\n        # IMPORTANT: when using dedicated Instances\n        # it should be a supported instance type (for example,\n        # p2 series are NOT supported).\n        # For a list of supported instance types go to:\n        # https://aws.amazon.com/ec2/purchasing-options/dedicated-instances/\n\n        boto_instances = self.ec2_resource.create_instances(\n            ImageId=instance_ami,\n            InstanceType=instance_type,\n            KeyName=self.key_name,\n            SecurityGroupIds=[self.security_group],\n            MaxCount=number, MinCount=1, SubnetId=self.subnet_id,\n            BlockDeviceMappings=bdm, TagSpecifications=tags_param, Placement=placement,\n            EbsOptimized=True)\n\n        self.created_instances_ids.extend(ins.id for ins in boto_instances)\n        logger.debug(f""Created {len(boto_instances)} {instance_type}"")\n\n        # Blocks until all instances are running.\n        for idx, ins in enumerate(boto_instances):\n            ins.wait_until_running()\n\n        logger.debug(f""Created instances running"")\n\n        ret = []\n        for idx, ins in enumerate(boto_instances):\n            ins.reload()  # Update instance information now that is running\n\n            ret.append(instance_class(\n                self._get_boto_public_host(ins),\n                self._get_boto_private_host(ins),\n                self.username, self.key, self.config, debug=self.debug))\n\n        if len(boto_instances) < number:\n            logger.debug(f""Less {instance_type} instances were created. ""\n                         f""{len(boto_instances)} out of {number}"")\n\n        return ret\n\n    def _get_boto_public_host(self, boto_ins: \'boto3.resource.factory.ec2.Instance\') -> str:\n        """"""Return the boto instance IP or DNS that will be used by\n        the local process to reach the current instance.\n\n        This method abstracts the way the local process will access\n        the instances in the case it\'s not the public IP.\n\n        Parameters\n        ----------\n        boto_ins: \'boto3.resources.factory.ec2.Instance\'\n            The boto instance\n\n        Returns\n        -------\n        str\n            The host information.\n\n        """"""\n        return boto_ins.public_ip_address\n\n    def _get_boto_private_host(self, boto_ins: \'boto3.resource.factory.ec2.Instance\') -> str:\n        """"""Return the boto instance IP or DNS that will be used by\n        the other instances to reach the current instance.\n\n        This method abstracts the way the other instances will access\n        the instance in the case it\'s not the private IP.\n\n        Parameters\n        ----------\n        boto_ins: \'boto3.resources.factory.ec2.Instance\'\n            The boto instance\n\n        Returns\n        -------\n        str\n            The host information.\n\n        """"""\n        return boto_ins.private_ip_address\n\n    def terminate_instances(self) -> None:\n        """"""Terminates all instances.\n\n        """"""\n        boto_instances = self.ec2_resource.instances.filter(\n            Filters=[{\n                \'Name\': \'instance-id\',\n                \'Values\': self.created_instances_ids\n            }]\n        )\n\n        for boto_ins in boto_instances:\n            boto_ins.terminate()\n            logger.info(cl.RE(f""Terminating {boto_ins.id}""))\n\n    def rollback_env(self) -> None:\n        """"""Rollback the environment.\n\n        This occurs when an error is caucht during the local stage of\n        the remote experiment (i.e. creating the cluster, sending the\n        data and submitting jobs), this method handles cleanup stages.\n\n        """"""\n        # If no factories are created (because of quota exceeded)\n        # but orchestrator was created, terminate it.\n        if self.orchestrator is not None and len(self.factories) == 0:\n            self.terminate_instances()\n\n        # If factories are created but no orchestrator was.\n        if len(self.factories) > 0 and self.orchestrator is None:\n            self.terminate_instances()\n\n    def parse(self) -> None:\n        """"""Checks if the AWSCluster configuration is valid.\n\n        This checks that the factories are never terminated after\n        the orchestrator is. Avoids the scenario where the cluster has\n        only factories and no orchestrator, which is useless.\n\n        Raises\n        ------\n        errors.ClusterConfigurationError\n            If configuration is not valid.\n\n        """"""\n        if self.orchestrator_timeout > -1:\n            if (\n                self.factories_timeout == -1 or\n                self.factories_timeout > self.orchestrator_timeout\n            ):\n                raise errors.ClusterConfigurationError(\n                    ""Factories can\'t be terminated after the orchestrator is terminated""\n                )\n\n        if self.tags and ""creator"" in [x.lower() for x in self.tags.keys()]:\n            raise errors.ClusterConfigurationError(\n                ""AWS Cluster tags can\'t include a \'creator\' tag. "" +\n                ""The \'creator\' attribute declared in the object will be used.""\n            )\n\n        if self.tags and ""name"" in [x.lower() for x in self.tags.keys()]:\n            raise errors.ClusterConfigurationError(\n                ""AWS Cluster tags can\'t include a \'name\' tag. "" +\n                ""The \'name\' attribute declared in the object will be used.""\n            )\n\n    def _get_boto_instance_by_host(\n            self,\n            public_host: str) -> Optional[""boto3.resources.factory.ec2.Instance""]:\n        """"""Returns the instance id given the public host\n\n        This method will use `_get_boto_public_host` to search\n        for the given host.\n\n        Parameters\n        ----------\n        public_host: str\n            The host. Depending on how the host was set,\n            it can be an IP or DNS.\n\n        Returns\n        -------\n        Optional[boto3.resources.factory.ec2.Instance]\n            The id if found else None\n\n        """"""\n        boto_instances = self.ec2_resource.instances.all()\n\n        for ins in boto_instances:\n            if self._get_boto_public_host(ins) == public_host:\n                return ins\n\n        return None\n\n    def _get_instance_id_by_host(self, public_host: str) -> Optional[str]:\n        """"""Returns the instance id given the public host\n\n        Parameters\n        ----------\n        public_host: str\n            The host. Depending on how the host was set,\n            it can be an IP or DNS.\n\n        Returns\n        -------\n        Optional[str]\n            The id if found else None\n\n        """"""\n        ins = self._get_boto_instance_by_host(public_host)\n        return ins.id if ins else None\n\n    def _get_alarm_name(self, instance_id: str) -> str:\n        """"""Get the alarm name to be used for the given instance.\n\n        Parameters\n        ----------\n        instance_id: str\n            The id of the instance\n\n        Returns\n        -------\n        str\n            The name of the corresponding alarm\n\n        """"""\n        return f""Flambe_Instance_Terminate_CPU_Utilization_{instance_id}""\n\n    def has_alarm(self, instance_id: str) -> bool:\n        """"""Whether the instance has an alarm set.\n\n        Parameters\n        ----------\n        instance_id: str\n            The id of the instance\n\n        Returns\n        -------\n        bool\n            True if an alarm is set. False otherwise.\n\n        """"""\n        try:\n            ret = self.cloudwatch.describe_alarms(AlarmNames=[self._get_alarm_name(instance_id)])\n            return len(ret[\'MetricAlarms\']) > 0\n        except botocore.exceptions.ParamValidationError:\n            raise errors.ClusterError(f""Could not retrieve alarm for {instance_id}"")\n\n    def remove_existing_events(self) -> None:\n        """"""Remove the current alarm.\n\n        In case the orchestrator or factories had an alarm,\n        we remove it to reset the new policies.\n\n        """"""\n        if not self.orchestrator:\n            raise errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        orch_host = self.orchestrator.host\n        orch_id = self._get_instance_id_by_host(orch_host)\n        if orch_id:\n            self._delete_cloudwatch_event(orch_id)\n\n        for f in self.factories:\n            f_id = self._get_instance_id_by_host(f.host)\n            if f_id:\n                self._delete_cloudwatch_event(f_id)\n\n    def create_cloudwatch_events(self) -> None:\n        """"""Creates cloudwatch events for orchestrator and factories.\n\n        """"""\n        if not self.orchestrator:\n            raise errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        fact_t = self.factories_timeout\n        # Create events for factories to shut down\n        if fact_t >= 0:\n            for f in self.factories:\n                f_id = self._get_instance_id_by_host(f.host)\n                if f_id:\n                    mins = fact_t * 60 if fact_t > 0 else 5\n                    # Adding fake data for CPU usage to avoid the alarm\n                    # triggering immediately in case the machine was\n                    # already idle\n                    self._put_fake_cloudwatch_data(f_id, value=100, points=10)\n                    self._create_cloudwatch_event(f_id, mins=mins, cpu_thresh=0.5)\n                    logger.info(cl.YE(f""{f.host} timeout of {mins} mins set""))\n        else:\n            logger.info(cl.YE(f""Factories have no timeout""))\n\n        orch_host = self.orchestrator.host\n        orch_id = self._get_instance_id_by_host(orch_host)\n        if orch_id:\n            orch_t = self.orchestrator_timeout\n            if orch_t >= 0:\n                mins = orch_t * 60 if orch_t > 0 else 5\n                # Adding fake data for CPU usage to avoid the alarm\n                # triggering immediately in case the machine was already\n                # idle\n                self._put_fake_cloudwatch_data(orch_id, value=100, points=10)\n                self._create_cloudwatch_event(orch_id, mins=mins, cpu_thresh=4)\n                logger.info(cl.YE(f""{self.orchestrator.host} timeout of {mins} set""))\n            else:\n                logger.info(cl.YE(f""Orchestrator {self.orchestrator.host} has no timeout""))\n\n    def _delete_cloudwatch_event(self, instance_id: str) -> None:\n        """"""Deletes the alarm related to the instance.\n\n        """"""\n        try:\n            self.cloudwatch.delete_alarms(\n                AlarmNames=[self._get_alarm_name(instance_id)]\n            )\n            logger.debug(f""Removed existing alarm for id {instance_id}"")\n\n        except botocore.exceptions.ParamValidationError:\n            raise errors.ClusterError(f""Could not delete alarm for {instance_id}"")\n\n    def _put_fake_cloudwatch_data(self,\n                                  instance_id: str,\n                                  value: int = 100,\n                                  points: int = 10) -> None:\n        """"""Put fake CPU Usage metric in an instance.\n\n        This method is useful to avoid triggering alarms when they are\n        created. For example, is an instance was idle for 10 hours and\n        an termination alarm is set for 5 hours, it will be triggered\n        immediately. Adding a fake point will allow the alarms to start\n        the timer from the current moment.\n\n        Parameters\n        ----------\n        instance_id: str\n            The ID of the EC2 instance\n        value: int\n            The CPU percent value to use. Defaults to 100\n        points: int\n            The amount of past minutes from the current time\n            to generate metric points. For example, if points is 10,\n            then 10 data metrics will be generated for the past 10\n            minutes, one per minute.\n\n        """"""\n        try:\n            for i in range(10):\n                self.cloudwatch.put_metric_data(\n                    Namespace=\'AWS/EC2\',\n                    MetricData=[\n                        {\n                            \'MetricName\': \'CPUUtilization\',\n                            \'Dimensions\': [\n                                {\n                                    \'Name\': \'InstanceId\',\n                                    \'Value\': instance_id\n                                },\n                            ],\n                            \'Timestamp\': datetime.utcnow() - timedelta(minutes=i),\n                            \'Value\': 100,\n                            \'Unit\': \'Percent\',\n                        },\n                    ]\n                )\n            logger.debug(f""Added fake CPU usage for {instance_id}"")\n        except botocore.exceptions.ParamValidationError:\n            raise errors.ClusterError(f""Could not put metric data for {instance_id}"")\n\n    def _create_cloudwatch_event(self, instance_id: str, mins: int = 60,\n                                 cpu_thresh: float = 0.1) -> None:\n        """"""Create CloudWatch alarm.\n\n        The alarm is used to terminate an instance based on CPU usage.\n\n        Parameters\n        ----------\n        instance_id: str\n            The ID of the EC2 instance\n        mins: int\n            Number of minutes to trigger the termination event.\n            The evaluation preriod will be always one minute.\n        cpu_thresh: float\n            Percentage specifying upper bound for triggering event.\n            If mins is 60 and cpu_thresh is 0.1, then this instance\n            will be deleted after 1 hour of average CPU below 0.1.\n\n        """"""\n\n        # Create alarm with actions enabled\n        try:\n            self.cloudwatch.put_metric_alarm(\n                AlarmName=self._get_alarm_name(instance_id),\n                ComparisonOperator=\'LessThanThreshold\',\n                EvaluationPeriods=mins,\n                MetricName=\'CPUUtilization\',\n                Namespace=\'AWS/EC2\',\n                Period=60,\n                Statistic=\'Average\',\n                Threshold=cpu_thresh,\n                ActionsEnabled=True,\n                AlarmActions=[\n                    f\'arn:aws:automate:{self.region_name}:ec2:terminate\'\n                ],\n                AlarmDescription=f\'Terminate when CPU < {cpu_thresh}%\',\n                Dimensions=[\n                    {\n                        \'Name\': \'InstanceId\',\n                        \'Value\': instance_id\n                    },\n                ],\n                Unit=\'Percent\'\n            )\n            logger.debug(f""Created alarm for id {instance_id}"")\n        except botocore.exceptions.ParamValidationError:\n            raise errors.ClusterError(f""Could not setup cloudwatch for {instance_id}"")\n\n    def _get_images(self) -> Dict:\n        """"""Get the official AWS public AMIs created by Flambe.\n\n        ATTENTION: why not just search the tags? We need to make sure\n        the AMIs we pick were created by the Flambe team. Because of\n        tags values not being unique, anyone can create a public AMI\n        with \'Creator: flambe@asapp.com\' as a tag.\n        If we pick that AMI, then we could potentially be Creating\n        instances with unknown AMIs, causing potential security issues.\n        By filtering by our acount id (which can be public), then we can\n        make sure that all AMIs that are being scanned were created\n        by Flambe team.\n\n        Returns\n        -------\n        Dict:\n            The boto3 API response\n\n        """"""\n        return self.ec2_cli.describe_images(Owners=[const.AWS_FLAMBE_ACCOUNT])\n\n    def _get_ami(self, _type: str, version: str) -> Optional[str]:\n        """"""Given a type and a version, get the correct Flambe AMI.\n\n\n        IMPORTANT: we keep the version logic in case we add versioned\n        AMIs in the future.\n\n        Parameters\n        ----------\n        _type: str\n            It can be either \'factory\' or \'orchestrator\'.\n            Note that the type is lowercase in the AMI tag.\n        version: str\n            For example, ""0.2.1"" or ""2.0"".\n\n        Returns\n        -------\n        The ImageId if it\'s found. None if not.\n\n        """"""\n        images = self._get_images()\n        for i in images[\'Images\']:\n            # Name is for example \'flambe-orchestrator 0.0.0\'\n            if i[\'Name\'] == f""flambe-{_type.lower()}-ami {version}"":\n                return i[\'ImageId\']\n\n        return None\n\n    def _find_default_ami(self, _type: str) -> Optional[str]:\n        """"""Returns an AMI with version 0.0.0, which is the default.\n        This means that doesn\'t contain flambe itself but it has\n        some heavy dependencies already installed (like pytorch).\n\n        Parameters\n        ----------\n        _type: str\n            Wether is ""orchestrator"" or ""factory""\n\n        Returns\n        -------\n        Optional[str]\n            The ImageId or None if not found.\n\n        """"""\n        return self._get_ami(_type, \'0.0.0\')\n\n    def _get_creation_name(self, role: str) -> str:\n        """"""Get an initial name the instance will receive at\n        creation time.\n\n        This name can be updated later using the \'name_hosts\' method or\n        the \'name_instance\' method.\n\n        Parameters\n        ----------\n        role: str\n            \'Orchestrator\' or \'Factory\'\n\n        Returns\n        -------\n        str\n            The initial instance name.\n\n        """"""\n        if role not in [\'Orchestrator\', \'Factory\']:\n            raise ValueError(f""Incorrect instance role {role}"")\n\n        return self.get_orchestrator_name() if role == \'Orchestrator\' \\\n            else self.get_factory_basename()\n'"
flambe/cluster/cluster.py,0,"b'""""""This module contains the base implementation of a Cluster.\n\nA Cluster is in charge of dealing with the different Instance\nobjects that will be part of the remote runnable.\n\n""""""\nfrom flambe.compile import yaml\nfrom flambe.runnable import Runnable\nfrom flambe.runnable.error import MissingSecretsError\nfrom flambe.runnable.utils import is_dev_mode\nfrom flambe.runner.utils import get_size_MB\nfrom flambe.cluster import const\nfrom flambe.cluster.instance import errors\nfrom flambe.cluster import errors as man_errors\nfrom flambe.cluster.instance.instance import OrchestratorInstance, GPUFactoryInstance, \\\n    CPUFactoryInstance, Instance\nfrom flambe.logging import coloredlogs as cl\n\nfrom flambe.runnable.environment import RemoteEnvironment\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom typing import Optional, Type, List, TypeVar, Union, Dict, Callable\nfrom types import TracebackType\n\nimport logging\nimport os\nimport traceback\nimport sys\nimport configparser\n\nimport errno\n\nfrom ruamel.yaml.compat import StringIO\n\nimport tempfile\n\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.backends import default_backend\n\nlogger = logging.getLogger(__name__)\n\nGPUFactoryInsT = TypeVar(""GPUFactoryInsT"", bound=GPUFactoryInstance)\nCPUFactoryInsT = TypeVar(""CPUFactoryInsT"", bound=CPUFactoryInstance)\nFactoryInsT = Union[GPUFactoryInsT, CPUFactoryInsT]\n\n\nUPLOAD_WARN_LIMIT_MB = 10\n\n\nclass Cluster(Runnable):\n    """"""Basic implementation of a Cluster.\n\n    The cluster is in charge of creating the cluster of instances where\n    one host is the Orchestrator while the other ones are Factories.\n\n    This implementation should not be used by an end user.\n    In order to give support to a cloud service provider (ex: AWS),\n    a child class must be implemented inheriting from the Cluster class.\n\n    *Important: when possible, Clusters should context managers*\n\n    Parameters\n    ----------\n    name: str\n        The name of the cluster, used to name the remote instances.\n    factories_num : int\n        The amount of factories to use. Note that this differs from\n        the number of workers,  as each factories can contain multiple\n        GPUs and therefore, multiple workers.\n    key: str\n        The path to the ssh key used to communicate to all instances.\n        IMPORTANT: all instances must be accessible with the same key.\n    username: str\n        The username of the instances the cluster will handle.\n        IMPORTANT: for now all instances need to have the same username.\n    setup_cmds: Optional[List[str]]\n        A list of commands to be run on all hosts for setup purposes.\n        These commands can be used to mount volumes, install software,\n        etc. Defaults to None.\n        IMPORTANT: the commands need to be idempotent and they shouldn\'t\n        expect user input.\n\n    """"""\n    def __init__(self,\n                 name: str,\n                 factories_num: int,\n                 key: str,\n                 username: str,\n                 setup_cmds: Optional[List[str]] = None) -> None:\n        super().__init__()\n        self.name = name\n        self.factories_num = factories_num\n        self.setup_cmds = setup_cmds\n\n        self.key = os.path.abspath(os.path.expanduser(key))\n        self.username = username\n\n        self.debug = is_dev_mode()\n\n        self.orchestrator: Optional[OrchestratorInstance] = None\n        self.factories: List[FactoryInsT] = []\n\n    def __enter__(self) -> ""Cluster"":\n        """"""A Cluster should be used with a context cluster\n        to handle all possible errors in a clear way.\n\n        Examples\n        --------\n\n        >>> with cluster as cl:\n        >>>     cl.launch_orchestrator()\n        >>>     cl.build_cluster()\n        >>>     ...\n\n        """"""\n        return self\n\n    def __exit__(self, exc_type: Optional[Type[BaseException]], exc_value: Optional[BaseException],\n                 tb: Optional[TracebackType]):\n        """"""Exit method for the context cluster.\n\n        This method will catch any exception, log it and return True.\n        This means that all exceptions produced in a Cluster\n        (used with the context cluster) will not continue to raise.\n\n        """"""\n        if exc_type is not None:\n            self.rollback_env()\n            traceback.print_exception(exc_type, exc_value, tb)\n            if exc_type == man_errors.ClusterError:\n                sys.exit(errno.EREMOTE)\n            if exc_type == MissingSecretsError:\n                sys.exit(""Double check the information provided in the secrets file. "" +\n                         ""Consider that it can be provided in the \'secrets\' parameter or if "" +\n                         ""not provided it looks in ~/.flambe.ini"")\n            else:\n                sys.exit(-1)\n\n        return False\n\n    def get_orchestrator_name(self) -> str:\n        """"""Get the orchestrator name.\n\n        The name is given by `name` with the \'_orchestrator\' suffix.\n        For example, if name is \'seq2seq-en-fr\', then the orchestrator\n        name will be \'seq2seq-en-fr_orchestrator\'.\n\n        This is an auxiliary method that can be used in child classes.\n\n        Returns\n        -------\n        str\n            The orcehstrator name\n\n        """"""\n        return f""{self.name}_orchestrator""\n\n    def get_factory_basename(self) -> str:\n        """"""Get the factory base name.\n\n        The name is `name` with the \'_factory\' suffix.\n        For example, if name is \'seq2seq-en-fr\', then the factory\n        basename will be \'seq2seq-en-fr_factory\'.\n\n        The base name can be used to generate all the factories\' names\n        (for example, by also appending an index to the basename).\n\n        This is an auxiliary method that can be used in child classes.\n\n        Returns\n        -------\n        str\n            The factory basename\n\n        """"""\n        return f""{self.name}_factory""\n\n    def load_all_instances(self) -> None:\n        """"""Method to make all hosts accessible.\n\n        Depending on the Cluster type, it behaves differently.\n        For example, AWSCluster or GCPCluster can create the instances\n        in this step. The SSHCluster does nothing (the machines are\n        already created).\n\n        """"""\n        raise NotImplementedError\n\n    def _get_all_hosts(self):\n        """"""Auxiliary method to get all the hosts in a list.append(\n\n        """"""\n        instances = self.factories[:]\n        instances.append(self.orchestrator)\n        return instances\n\n    def create_dirs(self, relative_dirs: List[str]) -> None:\n        """"""Create folders in all hostss.\n\n        If some of the already exist, it will do nothing.\n\n        Parameters\n        ----------\n        relative_dirs: List[str]\n            The directories to create. They should be relative paths\n            and $HOME of each host will be used to add the prefix.\n\n        """"""\n        with ThreadPoolExecutor() as executor:\n            futures = {}\n\n            for ins in self._get_all_hosts():\n                futures[executor.submit(ins.create_dirs, relative_dirs)] = ins\n\n            for f in futures.keys():\n                try:\n                    f.result()\n                except errors.RemoteCommandError:\n                    raise\n                except Exception as exc:\n                    logger.error(f\'Generated an exception: {exc}\')\n                    raise\n                else:\n                    logger.debug(f\'{futures[f].host} ready\')\n\n        logger.info(cl.GR(""All instances prepared""))\n\n    def prepare_all_instances(self) -> None:\n        """"""Prepare all the instances (both orchestrator and factories).\n\n        This method assumes that the hosts are running and accesible.\n        It will call the \'prepare\' method from all hosts.\n\n        """"""\n        with ThreadPoolExecutor() as executor:\n            futures = {}\n\n            for ins in self._get_all_hosts():\n                futures[executor.submit(ins.prepare)] = ins\n\n            for f in futures.keys():\n                try:\n                    f.result()\n                except errors.RemoteCommandError:\n                    raise\n                except Exception as exc:\n                    logger.error(f\'Generated an exception: {exc}\')\n                    raise\n                else:\n                    logger.debug(f\'{futures[f].host} ready\')\n\n        logger.info(cl.GR(""All instances prepared""))\n\n    def run(self, force: bool = False, **kwargs) -> None:\n        """"""Run a cluster and load all the instances.\n\n        After this metho runs, the orchestrator and factories\n        objects will be populated.\n\n        If a runnable is provided, then the cluster will execute\n        the runnable remotely in the cluster. Currently, only\n        ClusterRunnable is supported.\n\n        This method should be idempotent (ie if called N times with\n        the same configuration, only one cluster will be created.)\n\n        Parameters\n        ----------\n        force: bool, defaults to False\n            If true, current executions of the same runnable in the\n            cluster will be overriden by a new execution.\n\n        """"""\n        self.load_all_instances()\n        logger.info(cl.GR(""Cluster loaded""))\n\n        for ins in self._get_all_hosts():\n            ins.wait_until_accessible()\n\n        logger.debug(""All instances accessible."")\n        self.distribute_keys()\n\n        self.create_dirs([""extensions""])\n        logger.debug(""Created flambe folder to store content"")\n\n        if self.setup_cmds is not None:\n            self.run_cmds(self.setup_cmds)\n\n        self.prepare_all_instances()\n        logger.info(cl.GR(""Flambe installed in all hosts""))\n\n    def run_cmds(self, setup_cmds: List[str]) -> None:\n        """"""Run setup commands in all hosts\n\n        Parameters\n        ----------\n        setup_cmds: List[str]\n            The list of commands\n\n        Raises\n        ------\n        errors.RemoteCommandError\n            If at least one commands is not successful in at\n            least one host.\n\n        """"""\n        with ThreadPoolExecutor() as executor:\n            futures = []\n\n            for ins in self._get_all_hosts():\n                futures.append(executor.submit(ins.run_cmds, setup_cmds))\n\n            for f in futures:\n                try:\n                    f.result()\n                except errors.RemoteCommandError:\n                    raise\n                except Exception as exc:\n                    logger.error(\'Generated an unknown exception: {}\'.format(exc))\n                    raise\n\n        logger.info(cl.GR(""Custom commands ran successfully in all hosts""))\n\n    def get_orchestrator(self, ip: str, private_ip: str = None,\n                         use_public: bool = True) -> OrchestratorInstance:\n        """"""Get an orchestrator instance""""""\n        return OrchestratorInstance(ip, private_ip if private_ip else ip,\n                                    self.username, self.key, self.config, self.debug, use_public)\n\n    def get_orch_home_path(self) -> str:\n        """"""Return the orchestrator home path\n\n        Returns\n        -------\n        str\n\n        """"""\n        if not self.orchestrator:\n            raise man_errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        return self.orchestrator.get_home_path()\n\n    def get_factory(self, ip: str, private_ip: str = None,\n                    use_public: bool = True) -> CPUFactoryInstance:\n        """"""Get an CPU factory instance""""""\n        return CPUFactoryInstance(ip, private_ip if private_ip else ip,\n                                  self.username, self.key, self.config,\n                                  self.debug, use_public)\n\n    def get_gpu_factory(self, ip: str, private_ip: str = None,\n                        use_public: bool = True) -> GPUFactoryInstance:\n        """"""Get an GPU factory instance""""""\n        return GPUFactoryInstance(ip, private_ip if private_ip else ip,\n                                  self.username, self.key, self.config,\n                                  self.debug, use_public)\n\n    def launch_ray_cluster(self) -> None:\n        """"""Create a ray cluster.\n\n        The main node is going to be located in the orchestrator machine\n        and all other nodes in the factories.\n\n        The main node is executed with --num-cpus=0 flag so that\n        it doesn\'t do any work and all work is done by the factories.\n\n        """"""\n        for ins in self._get_all_hosts():\n            if ins.is_node_running():\n                raise man_errors.ClusterError(\n                    f""Node {ins.host} is running in an existing cluster. Aborting."")\n\n        port = const.RAY_REDIS_PORT\n\n        # The orchestator needs to exist at this point\n        if not self.orchestrator:\n            raise man_errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        self.orchestrator.launch_node(port)\n\n        redis_address = f""{self.orchestrator.private_host}:{port}""\n\n        with ThreadPoolExecutor(max_workers=self.factories_num) as executor:\n            futures = {}\n\n            for ins in self.factories:\n                futures[executor.submit(ins.launch_node, redis_address)] = ins\n\n            for f in futures.keys():\n                try:\n                    f.result()\n                except errors.RemoteCommandError:\n                    raise\n                except Exception as exc:\n                    logger.error(\'Generated an exception: {}\'.format(exc))\n                    raise\n                else:\n                    logger.debug(\'{} Ray worker ready\'.format(futures[f].host))\n\n        logger.info(cl.GR(""Ray cluster launched""))\n\n    def check_ray_cluster(self) -> bool:\n        """"""Check if ray cluster was build successfully.\n\n        Compares the name of workers available with the requested ones.\n\n        Returns\n        -------\n        bool\n            Whether the number of workers in the node\n            matches the number of factories\n\n        """"""\n        # The orchestator needs to exist at this point\n        if not self.orchestrator:\n            raise man_errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        workers = self.orchestrator.worker_nodes()\n        return len(workers) == self.factories_num\n\n    def shutdown_ray_cluster(self) -> None:\n        """"""Shut down the ray cluster.\n\n        Shut down the main node running in the orchestrator.\n\n        """"""\n        for f in self.factories:\n            f.shutdown_node()\n\n        if self.orchestrator:\n            self.orchestrator.shutdown_node()\n\n        logger.debug(""Ray cluster shutdown"")\n\n    def existing_ray_cluster(self) -> List[Instance]:\n        """"""Return a list of the nodes in the Ray cluster.\n\n        Returns\n        -------\n        List[Instance]\n            The list of nodes\n\n        """"""\n        ret = []\n        for h in self._get_all_hosts():\n            if h.is_node_running():\n                ret.append(h)\n\n        return ret\n\n    def existing_flambe_execution(self) -> List[Instance]:\n        """"""Return a list of the hosts that are running flambe.\n\n        Returns\n        -------\n        List[Instance]\n            The list of nodes\n\n        """"""\n        ret = []\n        for h in self._get_all_hosts():\n            if h.is_flambe_running():\n                ret.append(h)\n\n        return ret\n\n    def shutdown_flambe_execution(self) -> None:\n        """"""Shut down any flambe execution in the hosts.\n\n        """"""\n        # The orchestator needs to exist at this point\n        if not self.orchestrator:\n            raise man_errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        # The orchestator needs to exist at this point\n        if not self.factories:\n            raise man_errors.ClusterError(""Factories instances were not loaded."")\n\n        for f in self.factories:\n            f.shutdown_flambe()\n\n        self.orchestrator.shutdown_flambe()\n\n        # Flambe runs in tmux, so killing the session may be also\n        # needed\n        if self.orchestrator.existing_tmux_session(""flambe""):\n            self.orchestrator.kill_tmux_session(""flambe"")\n\n        logger.debug(""Flambe execution shutdown"")\n\n    def existing_dir(self, _dir: str) -> bool:\n        """"""Determine if _dir exists in at least one host\n\n        """"""\n        for h in self._get_all_hosts():\n            if h.existing_dir(_dir):\n                return True\n\n        return False\n\n    def is_ray_cluster_up(self) -> bool:\n        """"""Return if the ray cluster is running.\n\n        Returns\n        -------\n        bool\n\n        """"""\n        # The orchestator needs to exist at this point\n        if not self.orchestrator:\n            raise man_errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        return self.orchestrator.is_node_running()\n\n    def rollback_env(self) -> None:\n        """"""Rollback the enviornment.\n\n        When an error occures during the local stage of the remote\n        runnable (i.e. creating the cluster, sending the data and\n        submitting jobs), this method may be used to destroy the\n        cluster that has been built.\n\n        """"""\n        raise NotImplementedError()\n\n    def parse(self) -> None:\n        """"""Parse the cluster object.\n\n        Look for configurations mistakes that don\'t allow the remote\n        runnable to run. Each different cluster will have it\'s own\n        policies. For example, AWSCluster could check the instance\n        types that are allowed. By default, checks nothing.\n\n        Raises\n        ------\n        man_errors.ClusterConfigurationError\n            In case the Runnable is not able to run.\n\n        """"""\n        pass\n\n    def send_local_content(self,\n                           content: Dict[str, str],\n                           dest: str,\n                           all_hosts: bool = False) -> Dict[str, str]:\n        """"""Send local content to the cluster\n\n        Parameters\n        ----------\n        content: Dict[str, str]\n            The dict of resources key -> local path\n        dest: str\n            The orchestator\'s destination folder\n        all_hosts: bool\n            If False, only send the content to the orchestrator.\n            If True, send to all factories.\n\n        Returns\n        -------\n        Dict[str, str]\n            The new dict of content with orchestrator\'s paths.\n\n        """"""\n        ret = {}\n\n        # The orchestator needs to exist at this point\n        if not self.orchestrator:\n            raise man_errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        for k, c in content.items():\n            c = os.path.expanduser(c)\n            base: str = """"\n            if os.path.exists(c):\n\n                size = get_size_MB(c)\n                if size > UPLOAD_WARN_LIMIT_MB:\n                    logger.info(cl.YE(\n                        f""Uploading \'{c}\' ({int(size)} MB) which may take a while. "" +\n                        ""Double check you want to be transferring this file "" +\n                        ""(note we automatically sync extensions, experiment resources "" +\n                        ""and potentially the flambe repo if installed in dev mode)""))\n\n                if os.path.isdir(c):\n                    if not c.endswith(os.sep):\n                        c = f""{c}{os.sep}""\n                    base = os.path.basename(os.path.dirname(c))\n                elif os.path.isfile(c):\n                    base = os.path.basename(c)\n\n                new_c = os.path.join(dest, f""{k}__{base}"")\n                self.orchestrator.send_rsync(c, new_c)\n                logger.debug(f""Content {k}: {c} sent to cluster"")\n\n                ret[k] = new_c\n            else:\n                ret[k] = c\n\n        if all_hosts:\n            self.rsync_orch(dest)\n\n        return ret\n\n    def rsync_orch(self, folder):\n        """"""Rsync the orchestrator\'s folder with all factories\n\n        Parameters\n        ----------\n        folder: str\n            The folder to rsync. It should be a relative path.\n            $HOME value will be automatically added.\n\n        """"""\n        orch = self.orchestrator\n        content = os.path.join(orch.get_home_path(), folder)\n\n        for f in self.factories:\n            f_path = os.path.join(f.get_home_path(), folder)\n            f_loc = f""{f.username}@{f.private_host}:{f_path}""\n            orch.rsync_folder(content, f_loc)\n\n    def send_secrets(self, whitelist: List[str] = None) -> str:\n        """"""Send the secrets file to the orchestrator.\n\n        This file will be located in $HOME/secrets.ini\n        The injected secrets file will be used.\n\n        Parameters\n        ----------\n        whitelist: List[str]\n            A list of sections to filter. For example: [""AWS"", ""GITHUB""]\n\n        """"""\n        if not self.orchestrator:\n            raise man_errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        config = configparser.ConfigParser()\n        for section, section_dict in self.config.items():\n            if not whitelist or section in whitelist:\n                config[section] = {k: v for k, v in section_dict.items()}\n\n        secrets_path = (\n            f""{self.orchestrator.get_home_path()}/secret.ini""\n        )\n\n        with tempfile.NamedTemporaryFile(""w"") as t:\n            config.write(t)\n            t.flush()\n            self.orchestrator.send_rsync(t.name, secrets_path)\n            logger.debug(""New secrets file sent to cluster"")\n\n        return secrets_path\n\n    def execute(self,\n                cluster_runnable,\n                extensions: Dict[str, str],\n                new_secrets: str,\n                force: bool) -> None:\n        """"""Execute a ClusterRunnable in the cluster.\n\n        It will first upload the runnable file + extensions to the\n        orchestrator (under $HOME/flambe.yaml) and then it will\n        execute it based on the provided secrets\n\n        Parameters\n        ----------\n        cluster_runnable: ClusterRunnable\n            The ClusterRunnable to run in the cluster\n        extensions: Dict[str, str]\n            The extensions for the ClusterRunnable\n        new_secrets: str\n            The path (relative to the orchestrator) where\n            the secrets are located.\n            IMPORTANT: previous to calling this method, the secrets\n            should have been uploaded to the orchestrator\n        force: bool\n            The force parameter provided when running flambe locally\n\n        """"""\n        if not self.orchestrator:\n            raise man_errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        orch_exp = (\n            f""{self.orchestrator.get_home_path()}/flambe.yaml""\n        )\n\n        with tempfile.NamedTemporaryFile(""w"") as t:\n            with StringIO() as s:\n                yaml.dump_all([extensions, cluster_runnable], s)\n                t.write(s.getvalue())\n            t.flush()\n            self.orchestrator.send_rsync(t.name, orch_exp)\n            logger.info(cl.BL(""Remote runnable file sent to orchestrator""))\n\n        self.orchestrator.launch_flambe(orch_exp, new_secrets, force)\n\n    def remove_dir(self, _dir: str, content_only: bool = True, all_hosts: bool = True) -> None:\n        """""" Remove a directory in the ClusterError\n\n        Parameters\n        ----------\n        _dir: str\n            The directory to remove\n        content_only: bool\n            To remove the content only or the folder also.\n            Defaults to True.\n        all_hosts: bool\n            To remove it in all hosts or only in the Orchestrator.\n            Defaults to True (in all hosts).\n\n        """"""\n        if not self.orchestrator:\n            raise man_errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        if all_hosts:\n            for ins in self._get_all_hosts():\n                ins.remove_dir(_dir, content_only)\n\n        else:\n            self.orchestrator.remove_dir(_dir, content_only)\n\n    def cluster_has_key(self) -> bool:\n        """"""Whether the cluster already contains a valid common key.\n\n        The key must be in all hosts.\n\n        Returns\n        -------\n        bool\n            If the cluster has a key in all hosts.\n\n        """"""\n        pub_key_content = None\n        for ins in self._get_all_hosts():\n            private_key = f""{ins.get_home_path()}/{const.PRIVATE_KEY}""\n            public_key = f""{ins.get_home_path()}/{const.PUBLIC_KEY}""\n            pub_ret = ins._run_cmd(f""ls {public_key}"")\n            priv_ret = ins._run_cmd(f""ls {private_key}"")\n\n            if not (pub_ret.success and priv_ret.success):\n                return False\n\n            pub_key_ret = ins._run_cmd(f""cat {public_key}"")\n            if not pub_key_ret.success:\n                logger.debug(f""Not able to read file from {ins.host}"")\n                return False  # Not able to read key\n\n            if pub_key_ret.success:\n                curr_key_content = pub_key_ret.msg\n                if pub_key_content is None or pub_key_content == curr_key_content:\n                    pub_key_content = curr_key_content\n                else:\n                    logger.debug(f""Key in {ins.host} differs from others"")\n                    return False  # Keys mismatch\n\n        logger.debug(f""All hosts contain same key pair"")\n        return True\n\n    def distribute_keys(self) -> None:\n        """"""Create a new key pair and distributes it to all hosts.\n\n        Ensure that the hosts have a safe communication.\n        The name of the key is the cluster\'s name\n\n        """"""\n        if self.cluster_has_key():\n            logger.info(cl.GR(""Cluster has already configured key pair""))\n            return\n\n        # generate private/public key pair\n        key = rsa.generate_private_key(backend=default_backend(), public_exponent=65537,\n                                       key_size=2048)\n\n        # get public key in OpenSSH format\n        public_key = key.public_key().public_bytes(serialization.Encoding.OpenSSH,\n                                                   serialization.PublicFormat.OpenSSH)\n\n        # get private key in PEM container format\n        pem = key.private_bytes(encoding=serialization.Encoding.PEM,\n                                format=serialization.PrivateFormat.TraditionalOpenSSL,\n                                encryption_algorithm=serialization.NoEncryption())\n\n        # decode to printable strings\n        private_key_str = pem.decode(\'utf-8\')\n        public_key_str = public_key.decode(\'utf-8\')\n        logger.debug(""New key pair generated"")\n\n        def m(ins):\n            ins._run_cmd(f""rm -rf {ins.get_home_path()}/{const.PUBLIC_KEY}"")\n            ins._run_cmd(f""rm -rf {ins.get_home_path()}/{const.PRIVATE_KEY}"")\n\n            ret = ins._run_cmd(\n                f""echo \'{public_key_str}\' >> {ins.get_home_path()}/.ssh/authorized_keys"",\n                retries=3\n            )\n            if not ret.success:\n                raise man_errors.ClusterError(""Could not send key to authorized_keys"")\n\n            with tempfile.NamedTemporaryFile(""w"") as t:\n                t.write(private_key_str)\n                t.flush()\n                ins.send_rsync(t.name, f""{ins.get_home_path()}/{const.PRIVATE_KEY}"")\n                ins._run_cmd(f""chmod 600 {ins.get_home_path()}/{const.PRIVATE_KEY}"")\n\n            with tempfile.NamedTemporaryFile(""w"") as t:\n                t.write(public_key_str)\n                t.flush()\n                ins.send_rsync(t.name, f""{ins.get_home_path()}/{const.PUBLIC_KEY}"")\n                logger.debug(f""New key pair sent to {ins.host}"")\n\n        with ThreadPoolExecutor() as executor:\n            futures = {}\n\n            for ins in self._get_all_hosts():\n                futures[executor.submit(m, ins)] = ins\n\n            for f in futures.keys():\n                try:\n                    f.result()\n                except errors.RemoteCommandError:\n                    raise\n                except Exception as exc:\n                    logger.error(\'Generated an exception: {}\'.format(exc))\n                    raise\n\n        logger.info(cl.GR(""Distributed keys""))\n\n    def contains_gpu_factories(self) -> bool:\n        """"""Return if the factories contain GPU.\n\n        For now, all factories are same machine type,\n        so as soon as a GPU is found, then this method returns.\n\n        """"""\n        for f in self.factories:\n            if f.contains_gpu():\n                return True\n\n        return False\n\n    def get_max_resources(self) -> Dict[str, int]:\n        """"""Return the max common CPU/GPU devices in the factories\n\n        For example, if one factory contains 32 CPU + 1 GPU\n        and the other factory contains 16 CPU + 2 GPU, this\n        method will return {""cpu"": 16, ""gpu"": 1} available\n\n        Returns\n        -------\n        Dict[str, int]\n            The devices, in {""cpu"": N, ""gpu"": M} format\n\n        """"""\n        ret: Dict[str, int] = {}\n        for f in self.factories:\n            cpus = f.num_cpus()\n\n            if \'cpu\' not in ret:\n                ret[\'cpu\'] = cpus\n            else:\n                ret[\'cpu\'] = min(ret[\'cpu\'], cpus)\n\n            gpus = f.num_gpus()\n\n            if \'gpu\' not in ret:\n                ret[\'gpu\'] = gpus\n            else:\n                ret[\'gpu\'] = min(ret[\'gpu\'], gpus)\n\n        return ret\n\n    def install_extensions_in_orchestrator(self, extensions: Dict[str, str]) -> None:\n        """"""Install local + pypi extensions in the orchestrator\n\n        Parameters\n        ----------\n        extension: Dict[str, str]\n            The extensions, as a dict from module_name to location\n\n        Raises\n        ------\n        errors.RemoteCommandError\n            If could not install an extension.\n        man_errors.ClusterError\n            If the orchestrator was not loaded.\n\n        """"""\n        if not self.orchestrator:\n            raise man_errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        self.orchestrator.install_extensions(extensions)\n\n    def install_extensions_in_factories(self, extensions: Dict[str, str]) -> None:\n        """"""Install local + pypi extensions in all the factories.\n\n        Parameters\n        ----------\n        extension: Dict[str, str]\n            The extensions, as a dict from module_name to location\n\n        Raises\n        ------\n        errors.RemoteCommandError\n            If could not install an extension\n\n        """"""\n        for f in self.factories:\n            f.install_extensions(extensions)\n\n    def get_remote_env(self, user_provider: Callable[[], str]) -> RemoteEnvironment:\n        """"""Get the RemoteEnvironment for this cluster.\n\n        The IPs stored will be the private IPs\n\n        Returns\n        -------\n        RemoteEnvironment\n            The RemoteEnvironment with information about this cluster.\n\n        """"""\n        if not self.orchestrator:\n            raise man_errors.ClusterError(""Orchestrator instance was not loaded."")\n\n        # Use compile method so that is serializable\n        return RemoteEnvironment(\n            key=f""{self.orchestrator.get_home_path()}/{const.PRIVATE_KEY}"",\n            orchestrator_ip=self.orchestrator.private_host,\n            factories_ips=[f.private_host for f in self.factories],\n            user=self.orchestrator.username,\n            local_user=user_provider(),\n            public_orchestrator_ip=self.orchestrator.host,\n            public_factories_ips=[f.host for f in self.factories]\n        )\n'"
flambe/cluster/const.py,0,"b'# Logging socket config\nSOCKET_TIMEOUT = 50\n\n# SSH connection settings\nRETRY_DELAY = 1\nRETRIES = 60\n\nTENSORBOARD_IMAGE = ""tensorflow/tensorflow:1.15.0""\n\nRAY_REDIS_PORT = 12345\n\n# This names are specific because of tune stuff!\n# NOTE: DO NOT CHANGE THIS FILE NAMES\n# TODO: do this in a cleaner way\n\nPRIVATE_KEY = ""ray_bootstrap_key.pem""\nPUBLIC_KEY = ""ray_bootstrap_key.pub""\n\nREPORT_SITE_PORT = 49558\nTENSORBOARD_PORT = 49556\n\n# This is the account number for Flambe AWS account.\n# There is not risk of it being public.\nAWS_FLAMBE_ACCOUNT = ""808129580301""\n'"
flambe/cluster/errors.py,0,"b'\nclass ClusterError(Exception):\n    """"""Error raised in case of any unexpected error in the Ray cluster.\n\n    """"""\n    pass\n\n\nclass ClusterConfigurationError(Exception):\n    """"""Error raised when the configuration of the Cluster is not valid.\n\n    """"""\n    pass\n'"
flambe/cluster/ssh.py,0,"b'""""""Implementation of the Manager for SSH hosts""""""\n\nimport logging\n\nfrom typing import List, TypeVar, Union, Optional\n\nfrom flambe.cluster import instance\nfrom flambe.cluster.cluster import Cluster, FactoryInsT\n\nimport os\n\n\nlogger = logging.getLogger(__name__)\n\nFactoryT = TypeVar(""FactoryT"", instance.CPUFactoryInstance, instance.GPUFactoryInstance)\n\n\nclass SSHCluster(Cluster):\n    """"""The SSH Manager needs to be used when having running instances.\n\n    For example when having on-prem hardware or just a couple\n    of AWS EC2 instances running.\n\n    When using this cluster, the user needs to specify the IPs of\n    the machines to use, both the public one and private one.\n\n    """"""\n\n    def __init__(self,\n                 name: str,\n                 orchestrator_ip: Union[str, List[str]],\n                 factories_ips: Union[List[str], List[List[str]]],\n                 key: str,\n                 username: str,\n                 remote_context=None,\n                 use_public: bool = True,\n                 setup_cmds: Optional[List[str]] = None) -> None:\n        """"""Initialize the SSHCluster.""""""\n        super().__init__(name, len(factories_ips), key, username, setup_cmds)\n        self.orchestrator_ip = orchestrator_ip\n        self.factories_ips = factories_ips\n        self.remote_context = remote_context\n\n        self.use_public = use_public\n\n        if remote_context:\n            self.cluster_id = self.remote_context.cluster_id\n\n    def load_all_instances(self, exp_name: str = None, force: bool = False) -> None:\n        """"""This manager assumed that instances are running.\n\n        This method loads the Python objects to the manager\'s variables.\n\n        Parameters\n        ----------\n        exp_name: str\n            The name of the experiment\n        force: bool\n            Whether to override the current experiment of the same name\n\n        """"""\n        if isinstance(self.orchestrator_ip, list):\n            self.orchestrator = self.get_orchestrator(self.orchestrator_ip[0],\n                                                      self.orchestrator_ip[1],\n                                                      use_public=self.use_public)\n        else:\n            self.orchestrator = self.get_orchestrator(self.orchestrator_ip,\n                                                      use_public=self.use_public)\n\n        aux: FactoryInsT\n        for each in self.factories_ips:\n            if isinstance(each, list):\n                factory = self.get_factory(each[0], each[1], use_public=self.use_public)\n                if factory.contains_gpu():\n                    factory = self.get_gpu_factory(each[0], each[1], use_public=self.use_public)\n            else:\n                factory = self.get_factory(each, use_public=self.use_public)\n                if factory.contains_gpu():\n                    factory = self.get_gpu_factory(each, use_public=self.use_public)\n\n            self.factories.append(factory)\n\n    def rollback_env(self) -> None:\n        pass\n\n    def rsync_hosts(self):\n        """"""Rsyncs the host\'s result folders.\n\n        First, it rsyncs all worker folders to the orchestrator main\n        folder. After that, so that every worker gets the last changes,\n        the orchestrator rsync with all of them.\n\n        """"""\n        if not self.remote_context:\n            logger.error(""Can\'t rsyn without a remote context"")\n            return\n\n        exclude = [""state.pkl""]\n\n        orch = self.orchestrator\n        orch_save_path = os.path.join(f""{orch.get_home_path()}"", self.remote_context.save_folder)\n        orch_loc = f""{orch_save_path}""\n\n        for f in self.factories:\n            f_save_path = os.path.join(f""{orch.get_home_path()}"", self.remote_context.save_folder)\n            f_loc = f""{f.username}@{f.private_host}:{f_save_path}""\n            orch.rsync_folder(f_loc, orch_loc, exclude)\n\n        for f in self.factories:\n            f_save_path = os.path.join(f""{f.get_home_path()}"", self.remote_context.save_folder)\n            f_loc = f""{f.username}@{f.private_host}:{f_save_path}""\n            orch.rsync_folder(orch_loc, f_loc, exclude)\n'"
flambe/cluster/utils.py,0,"b""from collections import namedtuple\n\n\nRemoteCommand = namedtuple('RemoteCommand', ['success', 'msg'])\n"""
flambe/compile/__init__.py,0,"b""from flambe.compile.registrable import RegistrationError, Registrable, alias, yaml, \\\n    register, registrable_factory, registration_context, MappedRegistrable\nfrom flambe.compile.component import Schema, Component, Link, dynamic_component\nfrom flambe.compile.utils import make_component, all_subclasses\nfrom flambe.compile.serialization import save, load, save_state_to_file, load_state_from_file, \\\n    State\n\n\n__all__ = ['RegistrationError', 'Registrable', 'alias', 'Schema',\n           'Link', 'Component', 'yaml', 'register', 'dynamic_component',\n           'make_component', 'all_subclasses', 'registrable_factory',\n           'registration_context', 'save', 'load', 'State',\n           'save_state_to_file', 'load_state_from_file', 'MappedRegistrable']\n"""
flambe/compile/component.py,20,"b'# from __future__ import annotations\nimport inspect\nimport dill\nimport logging\nfrom reprlib import recursive_repr\nfrom warnings import warn\nfrom typing import Type, TypeVar, Any, Mapping, Dict, Optional, List, Union\nfrom typing import Generator, MutableMapping, Callable, Set, Tuple, Sequence\nfrom functools import WRAPPER_ASSIGNMENTS\nfrom collections import OrderedDict\nimport copy\n\nimport ray\nimport torch\nfrom io import StringIO\nfrom ruamel.yaml.representer import RepresenterError\nfrom ruamel.yaml import ScalarNode\nfrom ruamel.yaml.comments import (CommentedMap, CommentedOrderedMap, CommentedSet,\n                                  CommentedKeySeq, CommentedSeq, TaggedScalar,\n                                  CommentedKeyMap)\n\nfrom flambe.compile.serialization import load_state_from_file, State, load as flambe_load, \\\n    save as flambe_save\nfrom flambe.compile.registrable import Registrable, alias, yaml, registrable_factory\nfrom flambe.compile.const import STATE_DICT_DELIMETER, FLAMBE_SOURCE_KEY, FLAMBE_CLASS_KEY, \\\n    FLAMBE_CONFIG_KEY, FLAMBE_DIRECTORIES_KEY, KEEP_VARS_KEY, VERSION_KEY, FLAMBE_STASH_KEY\n\n\n_EMPTY = inspect.Parameter.empty\nA = TypeVar(\'A\')\nC = TypeVar(\'C\', bound=""Component"")\n\nYAML_TYPES = (CommentedMap, CommentedOrderedMap, CommentedSet, CommentedKeySeq, CommentedSeq,\n              TaggedScalar, CommentedKeyMap)\n\nlogger = logging.getLogger(__name__)\n\n\nclass CompilationError(Exception):\n    pass\n\n\nclass LoadError(Exception):\n    pass\n\n\nclass Schema(MutableMapping[str, Any]):\n    """"""Holds and recursively initializes Component\'s with kwargs\n\n    Holds a Component subclass and keyword arguments to that class\'s\n    compile method. When an instance is called it will perform the\n    recursive compilation process, turning the nested structure of\n    Schema\'s into initialized Component objects\n\n    Implements MutableMapping methods to facilitate inspection and\n    updates to the keyword args. Implements dot-notation access to\n    the keyword args as well.\n\n    Parameters\n    ----------\n    component_subclass : Type[Component]\n        Subclass of Component that will be compiled\n    **keywords : Any\n        kwargs passed into the Schema\'s `compile` method\n\n    Examples\n    -------\n    Create a Schema from a Component subclass\n\n    >>> class Test(Component):\n    ...     def __init__(self, x=2):\n    ...         self.x = x\n    ...\n    >>> tp = Schema(Test)\n    >>> t1 = tp()\n    >>> t2 = tp()\n    >>> assert t1 is t2  # the same Schema always gives you same obj\n    >>> tp = Schema(Test) # create a new Schema\n    >>> tp[\'x\'] = 3\n    >>> t3 = tp()\n    >>> assert t1.x == 3  # dot notation works as well\n\n    Attributes\n    ----------\n    component_subclass : Type[Component]\n        Subclass of Schema that will be compiled\n    keywords : Dict[str, Any]\n        kwargs passed into the Schema\'s `compile` method\n\n    """"""\n\n    def __init__(self,\n                 component_subclass: Type[C],\n                 _flambe_custom_factory_name: Optional[str] = None,\n                 **keywords: Any) -> None:\n        # Check if `Component` instead of just checking if callable\n        if not issubclass(component_subclass, Component):\n            raise TypeError(""The first argument must be Component"")\n\n        self.component_subclass: Type[C] = component_subclass\n        self.factory_method: Optional[str] = _flambe_custom_factory_name\n        self.keywords: Dict[str, Any] = keywords\n        self.post_init_hooks: Sequence[Callable] = []\n        self._compiled: Optional[C] = None\n        self._extensions: Dict[str, str] = {}\n        # Flag changes getattr functionality (for dot notation access)\n        self._created: bool = True\n\n    def __call__(self, stash: Optional[Dict[str, Any]] = None, **keywords: Any) -> C:\n        # The same yaml node => the same Schema => the same object\n        # So cache the compiled object\n        if self._compiled is not None:\n            return self._compiled\n        newkeywords = self.keywords.copy()\n        newkeywords.update(keywords)\n        compiled = self.component_subclass.compile(\n            _flambe_custom_factory_name=self.factory_method,\n            _flambe_extensions=self.aggregate_extensions_metadata(),\n            _flambe_stash=stash,\n            **newkeywords)\n        self._compiled = compiled\n        compiled._schema = self  # type: ignore\n        compiled._created_with_tag = self._created_with_tag  # type: ignore\n        for hook in self.post_init_hooks:\n            hook(compiled)\n        return compiled\n\n    def add_extensions_metadata(self, extensions: Dict[str, str]) -> None:\n        """"""Add extensions used when loading this schema and children\n\n        Uses ``component_subclass.__module__`` to filter for only the\n        single relevant extension for this object; extensions relevant\n        for children are saved only on those children schemas directly.\n        Use ``aggregate_extensions_metadata`` to generate a dictionary\n        of all extensions used in the object hierarchy.\n\n        """"""\n        # Get top level module\n        modules = self.component_subclass.__module__.split(\'.\')\n        # None sentinel won\'t be in extensions\n        top_level_module = modules[0] if len(modules) > 0 else None\n        if top_level_module is not None and top_level_module in extensions:\n            self._extensions = {top_level_module: extensions[top_level_module]}\n        else:\n            self._extensions = {}\n\n        def helper(data):\n            if isinstance(data, Schema):\n                data.add_extensions_metadata(extensions)\n            elif isinstance(data, list) or isinstance(data, tuple):\n                for val in data:\n                    helper(val)\n            elif isinstance(data, Mapping):\n                for val in data.values():\n                    helper(val)\n\n        for child in self.keywords.values():\n            helper(child)\n\n    def aggregate_extensions_metadata(self) -> Dict[str, str]:\n        """"""Aggregate extensions used in object hierarchy""""""\n        exts = dict(self._extensions or {})  # non-nested so shallow copy ok\n\n        def helper(data):\n            if isinstance(data, Schema):\n                exts.update(data.aggregate_extensions_metadata())\n            elif isinstance(data, list) or isinstance(data, tuple):\n                for val in data:\n                    helper(val)\n            elif isinstance(data, Mapping):\n                for val in data.values():\n                    helper(val)\n\n        for child in self.keywords.values():\n            helper(child)\n\n        return exts\n\n    def contains(self, schema: \'Schema\', original_link: \'Link\') -> Tuple[bool, List[str]]:\n        if self is schema:\n            return True, []\n\n        def helper(current, schematic_path):\n            if current is schema:\n                schematic_path = [\'\'] if len(schematic_path) == 0 else schematic_path\n                return True, schematic_path\n            for key, child in current.keywords.items():\n                new_path = schematic_path[:] + [key]\n                if isinstance(child, Schema):\n                    present, temp = helper(current=child, schematic_path=new_path)\n                    if present:\n                        return present, temp\n                if isinstance(child, Link) and child is not original_link:\n                    resolved = child.resolved\n                    if hasattr(resolved, \'_schema\'):\n                        # Pass through the link to the original object\n                        # TODO enforce links to earlier objects\n                        present, temp = helper(current=child.resolved._schema,\n                                               schematic_path=new_path)\n                        if present:\n                            return present, temp\n                elif child is original_link:\n                    return False, []\n            return False, []\n\n        return helper(self, [])\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        self.keywords[key] = value\n\n    def __getitem__(self, key: str) -> Any:\n        return self.keywords[key]\n\n    def __delitem__(self, key: str) -> None:\n        del self.keywords[key]\n\n    def __iter__(self) -> Generator[str, None, None]:\n        yield from self.keywords\n\n    def __len__(self) -> int:\n        return len(self.keywords)\n\n    def __getattr__(self, item: str) -> Any:\n        if \'_created\' not in self.__dict__ or item in self.__dict__:\n            # TODO typing - not sure why __getattr__ isn\'t defined for\n            # super, it works fine\n            return super().__getattr__(self, item)  # type: ignore\n        elif item in self.keywords:\n            return self.__getitem__(item)\n        else:\n            raise AttributeError(f""Object {type(self).__name__}""\n                                 f""[{self.component_subclass.__name__}] has no attribute {item}."")\n\n    def __setattr__(self, key: str, value: Any) -> None:\n        if \'_created\' not in self.__dict__ or key in self.__dict__:\n            super().__setattr__(key, value)\n        else:\n            self.__setitem__(key, value)\n\n    @recursive_repr()\n    def __repr__(self) -> str:\n        """"""Identical to super (schema), but sorts keywords""""""\n        keywords = "", "".join(""{}={!r}"".format(k, v) for k, v in sorted(self.keywords.items()))\n        format_string = ""{module}.{cls}({component_subclass}, {keywords})""\n        return format_string.format(module=self.__class__.__module__,\n                                    cls=self.__class__.__qualname__,\n                                    component_subclass=self.component_subclass,\n                                    keywords=keywords)\n\n    @classmethod\n    def to_yaml(cls, representer: Any, node: Any, tag: str = \'\') -> Any:\n        if tag == \'\':\n            tag = Registrable.get_default_tag(node.component_subclass, node.factory_method)\n        return representer.represent_mapping(tag, node.keywords)\n\n    @staticmethod\n    def serialize(obj: Any) -> Dict[str, Any]:\n        """"""Return dictionary representation of schema\n\n        Includes yaml as a string, and extensions\n\n        Parameters\n        ----------\n        obj: Any\n            Should be schema or dict of schemas\n\n        Returns\n        -------\n        Dict[str, Any]\n            dictionary containing yaml and extensions dictionary\n\n        """"""\n        with StringIO() as stream:\n            yaml.dump(obj, stream)\n            serialized = stream.getvalue()\n        exts: Dict[str, str] = {}\n        # TODO: temporary until Pipeline object exists\n        if isinstance(obj, dict):\n            for value in obj.values():\n                exts.update(value.aggregate_extensions_metadata())\n        else:\n            exts.update(obj.aggregate_extensions_metadata())\n        rep = {\'yaml\': serialized, \'extensions\': exts}\n        return rep\n\n    @staticmethod\n    def deserialize(data: Dict[str, Any]) -> Any:\n        """"""Construct Schema from dict returned by Schema.serialize\n\n        Parameters\n        ----------\n        data: Dict[str, Any]\n            dictionary returned by ``Schema.serialize``\n\n        Returns\n        -------\n        Any\n            Schema or dict of schemas (depending on yaml in ``data``)\n\n        """"""\n        yaml_str = data[\'yaml\']\n        extensions = data[\'extensions\']\n        obj = yaml.load(yaml_str)\n        # TODO: temporary until Pipeline object exists\n        if isinstance(obj, dict):\n            for value in obj.values():\n                value.add_extensions_metadata(extensions)\n        else:\n            obj.add_extensions_metadata(extensions)\n        return obj\n\n\n# Add representer for dumping Schema back to original yaml\n# Behaves just like Component `to_yaml` but compilation not needed\nyaml.representer.add_representer(Schema, Schema.to_yaml)\n\n\n# Used to contextualize the representation of links during YAML\n# representation\n_link_root_obj: Optional[\'Component\'] = None\n_link_context_active = False\n_link_obj_stash: Dict[str, Any] = {}\n\n\nclass contextualized_linking:\n    """"""Context manager used to change the representation of links\n\n    Links are always defined in relation to some root object and an\n    attribute path, so when representing some piece of a larger object\n    all the links need to be redefined in relation to the target object\n\n    """"""\n\n    def __init__(self, root_obj: Any, prefix: str) -> None:\n        self.root_obj = root_obj\n        self.prefix = prefix\n        self.old_root: Optional[\'Component\'] = None\n        self.old_active = False\n        self.old_stash: Dict[str, Any] = {}\n\n    def __enter__(self) -> \'contextualized_linking\':\n        global _link_root_obj\n        global _link_context_active\n        global _link_obj_stash\n        self.old_root = _link_root_obj\n        self.old_active = _link_context_active\n        self.old_stash = _link_obj_stash\n        _link_root_obj = self.root_obj\n        _link_context_active = True\n        _link_obj_stash = {}\n        return self\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        global _link_root_obj\n        global _link_context_active\n        global _link_obj_stash\n        _link_root_obj = self.old_root\n        _link_context_active = self.old_active\n        _link_obj_stash = self.old_stash\n\n\n@alias(\'$\')\nclass PickledDataLink(Registrable):\n\n    def __init__(self, obj_id: str, value: Any = None):\n        self.obj_id = obj_id\n        self.obj_value = value\n\n    def __call__(self, stash: Dict[str, Any]) -> Any:\n        if self.obj_value is not None:\n            diff_obj_stash = self.obj_value != stash[self.obj_id]\n            is_tensor = isinstance(diff_obj_stash, torch.Tensor)\n\n            # == comparison between tensors returns a tensor, not bool\n            if (is_tensor and torch.any(diff_obj_stash)) or (not is_tensor and diff_obj_stash):\n                warn(""PickledDataLink called second time with different stash"")\n            return self.obj_value\n        self.obj_value = stash[self.obj_id]\n        return self.obj_value\n\n    @classmethod\n    def to_yaml(cls, representer: Any, node: Any, tag: str) -> Any:\n        global _link_obj_stash\n        obj_id = str(len(_link_obj_stash.keys()))\n        _link_obj_stash[obj_id] = node.obj_value\n        return representer.represent_scalar(tag, obj_id)\n\n    @classmethod\n    def from_yaml(cls, constructor: Any, node: Any, factory_name: str) -> \'PickledDataLink\':\n        obj_id = constructor.construct_scalar(node)\n        return cls(obj_id=obj_id)\n\n\nclass LinkError(Exception):\n    pass\n\n\nclass MalformedLinkError(LinkError):\n    pass\n\n\nclass UnpreparedLinkError(LinkError):\n    pass\n\n\ndef parse_link_str(link_str: str) -> Tuple[Sequence[str], Sequence[str]]:\n    """"""Parse link to extract schematic and attribute paths\n\n    Links should be of the format ``obj[key1][key2].attr1.attr2`` where\n    obj is the entry point; in a pipeline, obj would be the stage name,\n    in a single-object config obj would be the target keyword at the\n    top level. The following keys surrounded in brackets traverse\n    the nested dictionary structure that appears in the config; this\n    is intentonally analagous to how you would access properties in the\n    dictionary when loaded into python. Then, you can use the dot\n    notation to access the runtime instance attributes of the object\n    at that location.\n\n    Parameters\n    ----------\n    link_str : str\n        Link to earlier object in the config of the format\n        ``obj[key1][key2].attr1.attr2``\n\n    Returns\n    -------\n    Tuple[Sequence[str], Sequence[str]]\n        Tuple of the schematic and attribute paths respectively\n\n    Raises\n    -------\n    MalformedLinkError\n        If the link is written incorrectly\n\n    Examples\n    -------\n    Examples should be written in doctest format, and\n    should illustrate how to use the function/class.\n    >>> parse_link_str(\'obj[key1][key2].attr1.attr2\')\n    ([\'obj\', \'key1\', \'key2\'], [\'attr1\', \'attr2\'])\n\n    """"""\n    schematic_path: List[str] = []\n    attr_path: List[str] = []\n    temp: List[str] = []\n    x = link_str\n    # Parse schematic path\n    bracket_open = False\n    root_extracted = False\n    while \'[\' in x or \']\' in x:\n        if bracket_open:\n            temp = x.split(\']\', 1)\n            if \'[\' in temp[0]:\n                raise MalformedLinkError(f""Previous bracket unclosed in {link_str}"")\n            if len(temp) != 2:\n                # Error case: [ not closed\n                raise MalformedLinkError(f""Open bracket \'[\' not closed in {link_str}"")\n            schematic_path.append(temp[0])\n            bracket_open = False\n        else:\n            # No bracket open yet\n            temp = x.split(\'[\', 1)\n            if \']\' in temp[0]:\n                raise MalformedLinkError(f""Close \']\' before open in {link_str}"")\n            if len(temp) != 2:\n                # Error case: ] encountered without [\n                raise MalformedLinkError(f""\']\' encountered before \'[\' in {link_str}"")\n            if len(temp[0]) != 0:\n                if len(schematic_path) != 0:\n                    # Error case: ]text[\n                    raise MalformedLinkError(f""Text between brackets in {link_str}"")\n                # Beginning object name\n                schematic_path.append(temp[0])\n                root_extracted = True\n            else:\n                if len(schematic_path) == 0:\n                    raise MalformedLinkError(f""No top level object in {link_str}"")\n            bracket_open = True\n        # First part already added to schematic path, keep remainder\n        x = temp[1]\n    # Parse attribute path\n    attr_path = x.split(\'.\')\n    if not root_extracted:\n        if len(attr_path[0]) == 0:\n            raise MalformedLinkError(f""No top level object in {link_str}"")\n        schematic_path.append(attr_path[0])\n    elif len(attr_path) > 1:\n        # Schematic processing did happen, so leading dot\n        if attr_path[0] != \'\':\n            # Error case: attr without dot beforehand\n            raise MalformedLinkError(f""Attribute without preceeding dot notation in {link_str}"")\n        if attr_path[-1] == \'\':\n            # Error case: trailing dot\n            raise MalformedLinkError(f""Trailing dot in {link_str}"")\n    attr_path = attr_path[1:]\n    return schematic_path, attr_path\n\n\ndef create_link_str(schematic_path: Sequence[str],\n                    attr_path: Optional[Sequence[str]] = None) -> str:\n    """"""Create a string representation of the specified link\n\n    Performs the reverse operation of\n    :func:`~flambe.compile.component.parse_link_str`\n\n    Parameters\n    ----------\n    schematic_path : Sequence[str]\n        List of entries corresponding to dictionary keys in a nested\n        :class:`~flambe.compile.Schema`\n    attr_path : Optional[Sequence[str]]\n        List of attributes to access on the target object\n        (the default is None).\n\n    Returns\n    -------\n    str\n        The string representation of the schematic + attribute paths\n\n    Raises\n    -------\n    MalformedLinkError\n        If the schematic_path is empty\n\n    Examples\n    -------\n    Examples should be written in doctest format, and\n    should illustrate how to use the function/class.\n    >>> create_link_str([\'obj\', \'key1\', \'key2\'], [\'attr1\', \'attr2\'])\n    \'obj[key1][key2].attr1.attr2\'\n\n    """"""\n    if len(schematic_path) == 0:\n        raise MalformedLinkError(""Can\'t create link without schematic path"")\n    root, schematic_path = schematic_path[0], schematic_path[1:]\n    schematic_str = \'\'\n    attr_str = \'\'\n    if len(schematic_path) > 0:\n        schematic_str = \'[\' + ""]["".join(schematic_path) + \']\'\n    if attr_path is not None and len(attr_path) > 0:\n        attr_str = \'.\' + \'.\'.join(attr_path)\n    return root + schematic_str + attr_str\n\n\n@alias(\'@\')\n@alias(\'link\')\nclass Link(Registrable):\n    """"""Delayed access to another object in an object hiearchy\n\n    Currently only supported in the context of Experiment but this\n    may be updated in a future release\n\n    A Link delays the access of some property, or the calling of some\n    method, until the Link is called. Links can be passed directly\n    into a Component subclass `compile`, Component\'s method called\n    compile will automatically record the links and call them to\n    access their values before running `__new__` and `__init__`. The\n    recorded links will show up in the config if `yaml.dump()` is\n    called on your object hierarchy. This typically happens when\n    logging individual configs during a grid search, and when\n    serializing between multiple processes.\n\n    For example, if the schematic path is [\'model\', \'encoder\'] and the\n    attribute path is [\'rnn\', \'hidden_size\'] then before the link can\n    be compiled, the target attribute should be set to point to the\n    model schema (this is handled automatically by Experiment) then,\n    during compilation the child schema \'encoder\' will be accessed,\n    and finally the attribute encoder.rnn.hidden_size will be returned\n\n    Parameters\n    ----------\n    schematic_path : Sequence[str]\n        Path to the relevant schema denoted by dictionary-like bracket\n        access e.g. [\'model\', \'encoder\']\n    attr_path : Sequence[str]\n        Path to the relevant attribute on the given schema (after it\'s\n        been compiled) using standard attribute dot notation e.g.\n        [\'rnn\', \'hidden_size\']\n    target : Optional[Schema]\n        The root object corresponding to the first element in the\n        schematic path; needs to be passed in here or set later before\n        link can be resolved\n    local : bool\n        if true, changes tune convert behavior to insert a dummy link;\n        used for links to global variables (""resources"" in config)\n        (defaults to True)\n\n    """"""\n\n    def __init__(self,\n                 schematic_path: Sequence[str],\n                 attr_path: Optional[Sequence[str]] = None,\n                 target: Optional[Schema] = None,\n                 local: bool = True) -> None:\n        self.schematic_path = schematic_path\n        self.attr_path = attr_path\n        self.target = target\n        self.target_leaf: Optional[Schema] = None\n        self.local = local\n        self.resolved: Optional[Any] = None\n        self.post_init_hooks: Sequence[Callable] = []\n\n    @property\n    def root_schema(self) -> str:\n        return self.schematic_path[0]\n\n    def __repr__(self) -> str:\n        return f\'link({create_link_str(self.schematic_path, self.attr_path)})\'\n\n    def __call__(self) -> Any:\n        # Link already resolved once, so return cached result\n        if self.resolved is not None:\n            return self.resolved  # type: ignore\n        # The relevant root object must be set to resolve the link\n        if self.target is None:\n            raise UnpreparedLinkError(\'Link object was not properly updated\')\n        current_obj: Any = self.target\n\n        def auto_resolve_link_and_move_to_schema(obj):\n            if isinstance(obj, Link):\n                obj()\n                obj = obj.resolved\n            if isinstance(obj, Component):\n                obj = obj._schema\n            return obj\n\n        # Traverse the schematic path, automatically resolving\n        # any chained links; only traverse the schema structure\n        current_obj = auto_resolve_link_and_move_to_schema(current_obj)\n        for schema in self.schematic_path[1:]:\n            try:\n                current_obj = current_obj.keywords[schema]\n            except KeyError:\n                raise KeyError(f\'Could not resolve link {schema}. (Check all !@ entries.)\')\n            current_obj = auto_resolve_link_and_move_to_schema(current_obj)\n        self.target_leaf = current_obj\n        # At the end of the schematic path, access the compiled object\n        # If it\'s not compiled, it\'s either later in the config\n        # or is a parent of the link, both of which are invalid links\n        if isinstance(current_obj, Schema):\n            if current_obj._compiled is not None:\n                current_obj = current_obj._compiled\n            else:\n                raise UnpreparedLinkError(""Cannot resolve link to non-compiled object ""\n                                          f""{current_obj}. Remember only non-parent objects above ""\n                                          ""the link in the config  will be compiled when the link ""\n                                          ""is resolved"")\n        # Then access the attributes of the compiled object\n        if self.attr_path is not None:\n            for attr in self.attr_path:\n                current_obj = getattr(current_obj, attr)\n        self.resolved = current_obj\n        for hook in self.post_init_hooks:\n            hook(current_obj)\n        return current_obj\n\n    @classmethod\n    def to_yaml(cls, representer: Any, node: Any, tag: str) -> Any:\n        """"""Build contextualized link based on the root node\n\n        If the link refers to something inside of the current object\n        hierarchy (as determined by the schema of ``_link_root_obj``)\n        then it will be represented as a link; if the link refers to\n        something out-of-scope, i.e. not inside the current object\n        hiearchy, then replace the link with the resolved value. If\n        the value cannot be represented, pickle it and include a\n        reference to its id in the object stash that will be saved\n        alongside the config\n\n        """"""\n        global _link_root_obj\n        global _link_context_active\n        global _link_obj_stash\n        if _link_context_active:\n            if _link_root_obj is None:\n                raise Exception(""Contextual linking requires root object to be set"")\n            present = False\n            if _link_root_obj._schema is not None:\n                present, schematic_path = _link_root_obj._schema.contains(node.target_leaf, node)\n            if present:\n                link_str = create_link_str(schematic_path, node.attr_path)\n                return representer.represent_scalar(tag, link_str)\n            else:\n                if isinstance(node.resolved, Registrable):\n                    return node.resolved.to_yaml(representer, node.resolved,\n                                                 node.resolved._created_with_tag)  # type: ignore  # noqa: E501\n                else:\n                    try:\n                        return representer.represent_data(node.resolved)\n                    except RepresenterError:\n                        data_link = PickledDataLink(obj_id=\'0\', value=node.resolved)\n                        return PickledDataLink.to_yaml(representer, data_link, \'!$\')\n        # No contextualization necessary\n        link_str = create_link_str(node.schematic_path, node.attr_path)\n        return representer.represent_scalar(tag, link_str)\n\n    @classmethod\n    def from_yaml(cls, constructor: Any, node: Any, factory_name: str) -> \'Link\':\n        link_str = constructor.construct_scalar(node)\n        schematic_path, attr_path = parse_link_str(link_str)\n        kwargs = {\'schematic_path\': schematic_path, \'attr_path\': attr_path}\n        return cls(**kwargs)  # type: ignore\n\n    def convert(self) -> Callable[..., Any]:\n        if self.local:\n            return ray.tune.function(lambda spec: eval(f\'spec\'))  # TODO what do here\n        return ray.tune.function(lambda spec: eval(f\'spec.config.params.{self.root_schema}\'))\n\n\n@alias(\'call\')\nclass FunctionCallLink(Link):\n    """"""Calls the link attribute instead of just accessing it""""""\n\n    def __call__(self) -> Any:\n        if self.resolved is not None:\n            return self.resolved\n        fn = super().__call__()\n        self.resolved = fn()\n        return self.resolved\n\n\nK = TypeVar(\'K\')\n\n\ndef fill_defaults(kwargs: Dict[str, Any], function: Callable[..., Any]) -> Dict[str, Any]:\n    """"""Use function signature to add missing kwargs to a dictionary""""""\n    signature = inspect.signature(function)\n    kwargs_with_defaults = kwargs.copy()\n    for name, param in signature.parameters.items():\n        if name == ""self"":\n            continue\n        default = param.default\n        if name not in kwargs and default != _EMPTY:\n            kwargs_with_defaults[name] = default\n    return kwargs_with_defaults\n\n\ndef merge_kwargs(kwargs: Dict[str, Any], compiled_kwargs: Dict[str, Any]) -> Dict[str, Any]:\n    """"""Replace non links in kwargs with corresponding compiled values\n\n    For every key in `kwargs` if the value is NOT a link and IS a\n    Schema, replace with the corresponding value in `compiled_kwargs`\n\n    Parameters\n    ----------\n    kwargs : Dict[str, Any]\n        Original kwargs containing Links and Schemas\n    compiled_kwargs : Dict[str, Any]\n        Processes kwargs containing no links and no Schemas\n\n    Returns\n    -------\n    Dict[str, Any]\n        kwargs with links, but with Schemas replaced by compiled\n        objects\n\n    """"""\n    merged_kwargs = {}\n    for kw in kwargs:\n        if not isinstance(kwargs[kw], Link) and isinstance(kwargs[kw], Schema):\n            if kw not in compiled_kwargs:\n                raise CompilationError(\'Non matching kwargs and compiled_kwargs\')\n            merged_kwargs[kw] = compiled_kwargs[kw]\n        else:\n            merged_kwargs[kw] = kwargs[kw]\n    return merged_kwargs\n\n\nclass Component(Registrable):\n    """"""Class which can be serialized to yaml and implements `compile`\n\n    IMPORTANT: ALWAYS inherit from Component BEFORE `torch.nn.Module`\n\n    Automatically registers subclasses via Registrable and\n    facilitates immediate usage in YAML with tags. When loaded,\n    subclasses\' initialization is delayed; kwargs are wrapped in a\n    custom schema called Schema that can be easily initialized\n    later.\n\n    """"""\n\n    _flambe_version = \'0.0.0\'  # >0.0.0 opts into semantic versioning\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._schema: Optional[Schema] = None\n        if isinstance(self, torch.nn.Module):\n            self._register_state_dict_hook(self._state_dict_hook)\n            self._register_load_state_dict_pre_hook(self._load_state_dict_hook)\n\n    def run(self) -> bool:\n        """"""Run a single computational step.\n\n        When used in an experiment, this computational step should\n        be on the order of tens of seconds to about 10 minutes of work\n        on your intended hardware; checkpoints will be performed in\n        between calls to run, and resources or search algorithms will\n        be updated. If you want to run everything all at once, make\n        sure a single call to run does all the work and return False.\n\n        Returns\n        -------\n        bool\n            True if should continue running later i.e. more work to do\n\n        """"""\n        # By default it doesn\'t do anything and doesn\'t continue\n        continue_ = False\n        return continue_\n\n    def metric(self) -> Optional[float]:\n        """"""Override this method to enable scheduling and searching.\n\n        Returns\n        -------\n        float\n            The metric to compare different variants of your Component\n\n        """"""\n        return None\n\n    @property\n    def _config_str(self):\n        """"""Represent object\'s architecture as a YAML string\n\n        Includes the extensions relevant to the object as well; NOTE:\n        currently this section may include a superset of the extensions\n        actually needed, but this will be changed in a future release.\n\n        """"""\n        stream = None\n        if not hasattr(self, \'_saved_kwargs\'):\n            raise AttributeError(f""{type(self).__name__} object was not compiled from YAML (or ""\n                                 ""created via the factory method \'compile\') and does not have an""\n                                 "" associated config"")\n        try:\n            config = """"\n            stream = StringIO()\n            try:\n                exts = self.aggregate_extensions_metadata()\n                if exts is not None and len(exts) > 0:\n                    yaml.dump_all([exts, self], stream)\n                else:\n                    yaml.dump(self, stream)\n                config = stream.getvalue()\n            except RepresenterError as re:\n                print(re)\n                logger.warn(""Exception representing attribute in yaml... "", re)\n            finally:\n                if not stream.closed:\n                    stream.close()\n                return config\n        except AttributeError as a:\n            if stream is not None and not stream.closed:\n                stream.close()\n            print(a)\n            raise AttributeError(f""{type(self).__name__} object was not compiled from YAML (or ""\n                                 ""created via the factory method \'compile\') and does not have an""\n                                 ""associated config"")\n        except Exception as e:\n            if stream is not None and not stream.closed:\n                stream.close()\n            raise e\n\n    def register_attrs(self, *names: str) -> None:\n        """"""Set attributes that should be included in state_dict\n\n        Equivalent to overriding `obj._state` and `obj._load_state` to\n        save and load these attributes. Recommended usage: call inside\n        `__init__` at the end: `self.register_attrs(attr1, attr2, ...)`\n        Should ONLY be called on existing attributes.\n\n        Parameters\n        ----------\n        *names : str\n            The names of the attributes to register\n\n        Raises\n        -------\n        AttributeError\n            If `self` does not have existing attribute with that name\n\n        """"""\n        if not hasattr(self, \'_registered_attributes\'):\n            self._registered_attributes: Set[str] = set()\n        for name in names:\n            if not hasattr(self, name):\n                raise AttributeError(f""{type(self).__name__} object has no attribute {name}, so ""\n                                     ""it cannot be registered"")\n        self._registered_attributes.update(names)\n\n    @staticmethod\n    def _state_dict_hook(self,\n                         state_dict: State,\n                         prefix: str,\n                         local_metadata: Dict[str, Any]) -> State:\n        """"""Add metadata and recurse on Component children\n\n        This hook is used to integrate with the PyTorch `state_dict`\n        mechanism; as either `nn.Module.state_dict` or\n        `Component.get_state` recurse, this hook is responsible for\n        adding Flambe specific metadata and recursing further on any\n        Component children of `self` that are not also nn.Modules,\n        as PyTorch will handle recursing to the latter.\n\n        Flambe specific metadata includes the class version specified\n        in the `Component._flambe_version` class property, the name\n        of the class, the source code, and the fact that this class is\n        a `Component` and should correspond to a directory in our\n        hiearchical save format\n\n        Finally, this hook calls a helper `_state` that users can\n        implement to add custom state to a given class\n\n        Parameters\n        ----------\n        state_dict : State\n            The state_dict as defined by PyTorch; a flat dictionary\n            with compound keys separated by \'.\'\n        prefix : str\n            The current prefix for new compound keys that reflects the\n            location of this instance in the object hierarchy being\n            represented\n        local_metadata : Dict[str, Any]\n            A subset of the metadata relevant just to this object and\n            its children\n\n        Returns\n        -------\n        type\n            The modified state_dict\n\n        Raises\n        -------\n        ExceptionName\n            Why the exception is raised.\n\n        """"""\n        warn_use_state = False\n        if FLAMBE_DIRECTORIES_KEY not in state_dict._metadata:\n            state_dict._metadata[FLAMBE_DIRECTORIES_KEY] = set()\n            warn_use_state = True\n        if KEEP_VARS_KEY not in state_dict._metadata:\n            state_dict._metadata[KEEP_VARS_KEY] = False\n            warn_use_state = True\n        if warn_use_state:\n            warn(""Use \'.get_state()\' on flambe objects, not state_dict ""\n                 f""(from {type(self).__name__})"")\n        # 1 need to add in any extras like config\n        local_metadata[VERSION_KEY] = self._flambe_version\n        local_metadata[FLAMBE_CLASS_KEY] = type(self).__name__\n        local_metadata[FLAMBE_SOURCE_KEY] = dill.source.getsource(type(self))\n        # All links should be relative to the current object `self`\n        with contextualized_linking(root_obj=self, prefix=prefix[:-1]):\n            try:\n                local_metadata[FLAMBE_CONFIG_KEY] = self._config_str\n                global _link_obj_stash\n                if len(_link_obj_stash) > 0:\n                    local_metadata[FLAMBE_STASH_KEY] = copy.deepcopy(_link_obj_stash)\n            except AttributeError:\n                pass\n        # 2 need to recurse on Components\n        # Iterating over __dict__ does NOT include pytorch children\n        # modules, parameters or buffers\n        # torch.optim.Optimizer does exist so ignore mypy\n        for name, attr in self.__dict__.items():\n            current_path = prefix + name\n            if isinstance(attr, Component) and not isinstance(attr, (\n                    torch.optim.Optimizer, torch.optim.lr_scheduler._LRScheduler)):  # type: ignore\n                # If self is not nn.Module, need to recurse because\n                # that will not happen elsewhere\n                # If self *is* an nn.Module, don\'t need to recurse on\n                # child nn.Module\'s because pytorch will already do\n                # that; just recurse on non-nn.Module\'s\n                # The latter case shouldn\'t happen, this is just an\n                # extra check for safety;\n                # child modules are not stored in __dict__\n                if not isinstance(self, torch.nn.Module) or not isinstance(attr, torch.nn.Module):\n                    state_dict = attr.get_state(destination=state_dict,\n                                                prefix=current_path + STATE_DICT_DELIMETER,\n                                                keep_vars=state_dict._metadata[KEEP_VARS_KEY])\n                state_dict._metadata[FLAMBE_DIRECTORIES_KEY].add(current_path)\n            # Iterate over modules to make sure NON-Component\n            # nn.Modules\' state is added. Only needed if self is not\n            # nn.Module, because otherwise this hook is being called\n            # via nn.Module.state_dict, and will already recurse to\n            # all children modules\n            if not isinstance(self, torch.nn.Module) and isinstance(attr, torch.nn.Module) \\\n                    and not isinstance(attr, Component):\n                state_dict = attr.state_dict(destination=state_dict,\n                                             prefix=current_path + STATE_DICT_DELIMETER,\n                                             keep_vars=state_dict._metadata[KEEP_VARS_KEY])\n        state_dict._metadata[FLAMBE_DIRECTORIES_KEY].add(prefix[:-1])\n        state_dict = self._add_registered_attrs(state_dict, prefix)\n        state_dict = self._state(state_dict, prefix, local_metadata)\n        return state_dict\n\n    def _add_registered_attrs(self, state_dict: State, prefix: str) -> State:\n        if hasattr(self, \'_registered_attributes\'):\n            for attr_name in self._registered_attributes:\n                state_dict[prefix + attr_name] = getattr(self, attr_name)\n        return state_dict\n\n    def _state(self, state_dict: State, prefix: str, local_metadata: Dict[str, Any]) -> State:\n        """"""Add custom state to state_dict\n\n        Parameters\n        ----------\n        state_dict : State\n            The state_dict as defined by PyTorch; a flat dictionary\n            with compound keys separated by \'.\'\n        prefix : str\n            The current prefix for new compound keys that reflects the\n            location of this instance in the object hierarchy being\n            represented\n        local_metadata : Dict[str, Any]\n            A subset of the metadata relevant just to this object and\n            its children\n\n        Returns\n        -------\n        State\n            The modified state_dict\n\n        """"""\n        return state_dict\n\n    def get_state(self,\n                  destination: Optional[State] = None,\n                  prefix: str = \'\',\n                  keep_vars: bool = False) -> State:\n        """"""Extract PyTorch compatible state_dict\n\n        Adds Flambe specific properties to the state_dict, including\n        special metadata (the class version, source code, and class\n        name). By default, only includes state that PyTorch `nn.Module`\n        includes (Parameters, Buffers, child Modules). Custom state can\n        be added via the `_state` helper method which subclasses should\n        override.\n\n        The metadata `_flambe_directories` indicates which objects are\n        Components and should be a subdirectory in our hierarchical\n        save format. This object will recurse on `Component` and\n        `nn.Module` children, but NOT `torch.optim.Optimizer`\n        subclasses, `torch.optim.lr_scheduler._LRScheduler` subclasses,\n        or any other arbitrary python objects.\n\n        Parameters\n        ----------\n        destination : Optional[State]\n            The state_dict as defined by PyTorch; a flat dictionary\n            with compound keys separated by \'.\'\n        prefix : str\n            The current prefix for new compound keys that reflects the\n            location of this instance in the object hierarchy being\n            represented\n        keep_vars : bool\n            Whether or not to keep Variables (only used by PyTorch)\n            (the default is False).\n\n        Returns\n        -------\n        State\n            The state_dict object\n\n        Raises\n        -------\n        ExceptionName\n            Why the exception is raised.\n\n        """"""\n        if destination is None:\n            destination = State()\n            destination._metadata = OrderedDict({FLAMBE_DIRECTORIES_KEY: set(),\n                                                 KEEP_VARS_KEY: keep_vars})\n            destination._metadata[FLAMBE_DIRECTORIES_KEY].add(prefix)\n        if isinstance(self, torch.nn.Module):\n            destination = self.state_dict(destination, prefix, keep_vars)\n        # torch.optim.Optimizer does exist so ignore mypy\n        elif isinstance(self, (torch.optim.Optimizer,  # type: ignore\n                               torch.optim.lr_scheduler._LRScheduler)):\n            pass\n        else:\n            local_metadata: Dict[str, Any] = {}\n            destination._metadata[prefix[:-1]] = local_metadata\n            destination = self._state_dict_hook(self, destination, prefix, local_metadata)\n\n        return destination  # type: ignore\n\n    def _load_state_dict_hook(self,\n                              state_dict: State,\n                              prefix: str,\n                              local_metadata: Dict[str, Any],\n                              strict: bool,\n                              missing_keys: List[Any],\n                              unexpected_keys: List[Any],\n                              error_msgs: List[Any]) -> None:\n        """"""Load flambe-specific state\n\n        Parameters\n        ----------\n        state_dict : State\n            The state_dict as defined by PyTorch; a flat dictionary\n            with compound keys separated by \'.\'\n        prefix : str\n            The current prefix for new compound keys that reflects the\n            location of this instance in the object hierarchy being\n            represented\n        local_metadata : Dict[str, Any]\n            A subset of the metadata relevant just to this object and\n            its children\n        strict : bool\n            Whether missing or unexpected keys should be allowed;\n            should always be False in Flambe\n        missing_keys : List[Any]\n            Missing keys so far\n        unexpected_keys : List[Any]\n            Unexpected keys so far\n        error_msgs : List[Any]\n            Any error messages so far\n\n        Raises\n        -------\n        LoadError\n            If the state for some object does not have a matching major\n            version number\n\n        """"""\n        # Custom subclass behavior\n        self._load_state(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,\n                         error_msgs)\n        self._load_registered_attrs(state_dict, prefix)\n        # Check state compatibility\n        version = local_metadata[VERSION_KEY].split(\'.\')\n        if min(map(int, version)) > 0:\n            # Opt-in to semantic versioning\n            versions = local_metadata[VERSION_KEY], type(self)._flambe_version\n            load_version, current_version = map(lambda x: x.split(\'.\'), versions)\n            if load_version[0] != current_version[0]:\n                raise LoadError(f\'Incompatible Versions: {load_version} and {current_version}\')\n            if load_version[1] != current_version[1]:\n                logger.warn(f\'Differing Versions (Minor): {load_version} and {current_version}\')\n            if load_version[2] != current_version[2]:\n                logger.debug(f\'Differing Versions (Patch): {load_version} and {current_version}\')\n        else:\n            original_source = local_metadata[FLAMBE_SOURCE_KEY]\n            current_source = dill.source.getsource(type(self))\n            if original_source != current_source:\n                # Warn / Error\n                logger.warn(f""Source code for object {self} does not match the source code saved ""\n                            f""with the state dict\\nSource code: {current_source}\\n""\n                            f""Original source code:{original_source}\\n"")\n\n    def _load_registered_attrs(self, state_dict: State, prefix: str):\n        if hasattr(self, \'_registered_attributes\'):\n            for attr_name in self._registered_attributes:\n                setattr(self, attr_name, state_dict[prefix + attr_name])\n\n    def _load_state(self,\n                    state_dict: State,\n                    prefix: str,\n                    local_metadata: Dict[str, Any],\n                    strict: bool,\n                    missing_keys: List[Any],\n                    unexpected_keys: List[Any],\n                    error_msgs: List[Any]) -> None:\n        """"""Load custom state (that was included via `_state`)\n\n        Subclasses should override this function to add custom state\n        that isn\'t normally included by PyTorch nn.Module\n\n        Parameters\n        ----------\n        state_dict : State\n            The state_dict as defined by PyTorch; a flat dictionary\n            with compound keys separated by \'.\'\n        prefix : str\n            The current prefix for new compound keys that reflects the\n            location of this instance in the object hierarchy being\n            represented\n        local_metadata : Dict[str, Any]\n            A subset of the metadata relevant just to this object and\n            its children\n        strict : bool\n            Whether missing or unexpected keys should be allowed;\n            should always be False in Flambe\n        missing_keys : List[Any]\n            Missing keys so far\n        unexpected_keys : List[Any]\n            Unexpected keys so far\n        error_msgs : List[Any]\n            Any error messages so far\n\n        """"""\n        pass\n\n    def load_state(self, state_dict: State, strict: bool = False) -> None:\n        """"""Load `state_dict` into `self`\n\n        Loads state produced by `get_state` into the current object,\n        recursing on child `Component` and `nn.Module` objects\n\n        Parameters\n        ----------\n        state_dict : State\n            The state_dict as defined by PyTorch; a flat dictionary\n            with compound keys separated by \'.\'\n        strict : bool\n            Whether missing or unexpected keys should be allowed;\n            should ALWAYS be False in Flambe (the default is False).\n\n        Raises\n        -------\n        LoadError\n            If the state for some object does not have a matching major\n            version number\n\n        """"""\n        missing_keys: List[str] = []\n        unexpected_keys: List[str] = []\n        error_msgs: List[str] = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \'_metadata\', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        # For loading, the _load_from_state_dict and\n        # _load_state_dict_hook are NOT recursive.\n        # We emulate PyTorch\'s structure by having a recursive\n        # helper here, for compatibility reasons.\n\n        def load(module, prefix=\'\'):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            if isinstance(module, torch.nn.Module):\n                module_load_fn = module._load_from_state_dict\n            else:\n                module_load_fn = module._load_state_dict_hook\n            module_load_fn(state_dict, prefix, local_metadata, True, missing_keys,\n                           unexpected_keys, error_msgs)\n            for name, child in module.__dict__.items():\n                if child is not None and isinstance(child, Component):\n                    if not isinstance(child, (torch.optim.Optimizer,\n                                              torch.optim.lr_scheduler._LRScheduler)):\n                        load(child, prefix + name + STATE_DICT_DELIMETER)\n            if isinstance(module, torch.nn.Module):\n                for name, child in module._modules.items():\n                    if child is not None:\n                        load(child, prefix + name + STATE_DICT_DELIMETER)\n        load(self)\n        # PyTorch 1.1 error handling\n        if strict:\n            if len(unexpected_keys) > 0:\n                error_msgs.insert(0, \'Unexpected key(s) in state_dict: \'\n                                     f\'{"", "".join(f""{k}"" for k in unexpected_keys)}. \')\n            if len(missing_keys) > 0:\n                error_msgs.insert(0, \'Missing key(s) in state_dict: \'\n                                     f\'{"", "".join(f""{k}"" for k in missing_keys)}. \')\n        if len(error_msgs) > 0:\n            newline_tab = \'\\n\\t\'\n            raise RuntimeError(\'Error(s) in loading state_dict for \'\n                               f\'{self.__class__.__name__}:{newline_tab}\'\n                               f\'{newline_tab.join(error_msgs)}\')\n\n    @registrable_factory\n    @classmethod\n    def load_from_path(cls,\n                       path: str,\n                       map_location: Union[torch.device, str] = None,\n                       use_saved_config_defaults: bool = True,\n                       **kwargs: Any):\n        if use_saved_config_defaults:\n            instance = flambe_load(path, map_location=map_location)\n        else:\n            loaded_state = load_state_from_file(path, map_location=map_location)\n            instance = cls(**kwargs)\n            instance.load_state(loaded_state)\n        return instance\n\n    def save(self, path: str, **kwargs: Any):\n        flambe_save(self, path, **kwargs)\n\n    @classmethod\n    def to_yaml(cls, representer: Any, node: Any, tag: str) -> Any:\n        return representer.represent_mapping(tag, node._saved_kwargs)\n\n    @classmethod\n    def from_yaml(cls: Type[C], constructor: Any, node: Any, factory_name: str) -> Schema:\n        # Normally you would create an instance of this class with\n        # cls(...) but in this case we don\'t want to init the object\n        # yet so we create a modified schema that will recursively\n        # initialize kwargs via compile when the top level compilation\n        # begins\n        if inspect.isabstract(cls):\n            msg = f""You\'re trying to initialize an abstract class {cls}. "" \\\n                  + ""If you think it\'s concrete, double check you\'ve spelled "" \\\n                  + ""all the method names correctly.""\n            raise Exception(msg)\n        if isinstance(node, ScalarNode):\n            nothing = constructor.construct_yaml_null(node)\n            if nothing is not None:\n                warn(f""Non-null scalar argument to {cls.__name__} will be ignored"")\n            return Schema(cls, _flambe_custom_factory_name=factory_name)\n        # NOTE: construct_yaml_map is a generator that yields the\n        # constructed data and then updates it\n        kwargs, = list(constructor.construct_yaml_map(node))\n        return Schema(cls, _flambe_custom_factory_name=factory_name, **kwargs)\n\n    @classmethod\n    def precompile(cls: Type[C], **kwargs: Any) -> None:\n        """"""Change kwargs before compilation occurs.\n\n        This hook is called after links have been activated, but before\n        calling the recursive initialization process on all other\n        objects in kwargs. This is useful in a number of cases, for\n        example, in Trainer, we compile several objects ahead of time\n        and move them to the GPU before compiling the optimizer,\n        because it needs to be initialized with the model parameters\n        *after* they have been moved to GPU.\n\n        Parameters\n        ----------\n        cls : Type[C]\n            Class on which method is called\n        **kwargs : Any\n            Current kwargs that will be compiled and used to initialize\n            an instance of cls after this hook is called\n\n        """"""\n        return\n\n    def aggregate_extensions_metadata(self) -> Dict[str, str]:\n        """"""Aggregate extensions used in object hierarchy\n\n        TODO: remove or combine with schema implementation in refactor\n\n        """"""\n        # non-nested so shallow copy ok\n        exts = dict(self._extensions or {})  # type: ignore\n\n        def helper(data):\n            if isinstance(data, Component):\n                exts.update(data.aggregate_extensions_metadata())\n            elif isinstance(data, list) or isinstance(data, tuple):\n                for val in data:\n                    helper(val)\n            elif isinstance(data, Mapping):\n                for val in data.values():\n                    helper(val)\n\n        for child in self._saved_kwargs.values():  # type: ignore\n            helper(child)\n\n        return exts\n\n    @classmethod\n    def compile(cls: Type[C],\n                _flambe_custom_factory_name: Optional[str] = None,\n                _flambe_extensions: Optional[Dict[str, str]] = None,\n                _flambe_stash: Optional[Dict[str, Any]] = None,\n                **kwargs: Any) -> C:\n        """"""Create instance of cls after recursively compiling kwargs\n\n        Similar to normal initialization, but recursively initializes\n        any arguments that should be compiled and allows overriding\n        arbitrarily deep kwargs before initializing if needed. Also\n        activates any Link instances passed in as kwargs, and saves\n        the original kwargs for dumping to yaml later.\n\n        Parameters\n        ----------\n        **kwargs : Any\n            Keyword args that should be forwarded to the initialization\n            function (a specified factory, or the normal `__new__`\n            and `__init__` methods)\n\n        Returns\n        -------\n        C\n            An instance of the class `cls`\n\n        """"""\n        extensions: Dict[str, str] = _flambe_extensions or {}\n        stash: Dict[str, Any] = _flambe_stash or {}\n        # Allow objects to do custom operations such as adding hooks\n        cls.precompile(**kwargs)\n        # Recursively compile any remaining un-compiled kwargs\n\n        def helper(obj: Any) -> Any:\n            if isinstance(obj, Schema):\n                obj.add_extensions_metadata(extensions)\n                out = obj(stash)  # type: ignore\n            elif isinstance(obj, Link):\n                out = obj()\n            elif isinstance(obj, PickledDataLink):\n                out = obj(stash)\n            # string passes as sequence\n            elif isinstance(obj, list) or isinstance(obj, tuple):\n                out = []\n                for value in obj:\n                    out.append(helper(value))\n            elif isinstance(obj, Mapping):\n                out = {}\n                for key, value in obj.items():\n                    out[key] = helper(value)\n            else:\n                out = obj\n            return out\n\n        newkeywords = helper(kwargs)\n        # Check for remaining yaml types\n        for kw in newkeywords:\n            if isinstance(newkeywords[kw], YAML_TYPES):\n                msg = f""\'{cls}\' property \'{kw}\' is still yaml type {type(newkeywords[kw])}\\n""\n                msg += f""This could be because of a typo or the class is not registered properly""\n                warn(msg)\n        # Find intended constructor in case using some factory\n        factory_method: Callable[..., Any] = cls\n        if _flambe_custom_factory_name is not None:\n            factory_method = getattr(cls, _flambe_custom_factory_name)\n        # Replace non link Schemas with compiled objects in kwargs\n        # for dumping\n        kwargs_non_links_compiled = merge_kwargs(kwargs, newkeywords)\n        # Fill the *original* kwargs with defaults specified by factory\n        kwargs_with_defaults = fill_defaults(kwargs_non_links_compiled, factory_method)\n        # Creat the compiled instance of `cls`\n        try:\n            instance = factory_method(**newkeywords)\n        except TypeError as te:\n            print(f""class {cls} method {_flambe_custom_factory_name} failed with ""\n                  f""keyword args:\\n{newkeywords}"")\n            raise te\n        # Record kwargs used for compilation for YAML dumping later\n        # Includes defaults for better safety / reproducibility\n        instance._saved_kwargs = kwargs_with_defaults\n        instance._extensions = extensions\n        return instance\n\n\ndef dynamic_component(class_: Type[A],\n                      tag: str,\n                      tag_namespace: Optional[str] = None,\n                      parent_component_class: Type[Component] = Component) -> Type[Component]:\n    """"""Decorate given class, creating a dynamic `Component`\n\n    Creates a dynamic subclass of `class_` that inherits from\n    `Component` so it will be registered with the yaml loader and\n    receive the appropriate functionality (`from_yaml`, `to_yaml` and\n    `compile`). `class_` should not implement any of the aforementioned\n    functions.\n\n    Parameters\n    ----------\n    class_ : Type[A]\n        Class to register with yaml and the compilation system\n    tag : str\n        Tag that will be used with yaml\n    tag_namespace : str\n        Namespace aka the prefix, used. e.g. for `!torch.Adam` torch is\n        the namespace\n\n    Returns\n    -------\n    Type[Component]\n        New subclass of `_class` and `Component`\n\n    """"""\n    if not issubclass(parent_component_class, Component):\n        raise Exception(""Only a subclass of Component should be used for \'parent_component_class\'"")\n    if issubclass(class_, parent_component_class):\n        return class_\n\n    # Copy over class attributes so it still looks like the original\n    # Useful for inspection and debugging purposes\n    _MISSING = object()\n    copied_attrs = {}\n    for k in WRAPPER_ASSIGNMENTS:\n        v = getattr(class_, k, _MISSING)\n        if v is not _MISSING:\n            copied_attrs[k] = v\n\n    # Create new subclass of `class_` and `Component`\n    # Ignore mypy, extra kwargs are okay in python 3.6+ usage of type\n    # and Registrable uses them\n    new_component = type(class_.__name__,  # type: ignore\n                         (parent_component_class, class_),\n                         copied_attrs,\n                         tag_override=tag,\n                         tag_namespace=tag_namespace)  # type: ignore\n\n    return new_component\n'"
flambe/compile/const.py,0,"b""STATE_DICT_DELIMETER = '.'\nFLAMBE_SOURCE_KEY = '_flambe_source'\nFLAMBE_CLASS_KEY = '_flambe_class'\nFLAMBE_CONFIG_KEY = '_flambe_config'\nFLAMBE_DIRECTORIES_KEY = '_flambe_directories'\nFLAMBE_STASH_KEY = '_flambe_stash'\nKEEP_VARS_KEY = 'keep_vars'\nVERSION_KEY = '_flambe_version'\nHIGHEST_SERIALIZATION_PROTOCOL_VERSION = 1\nDEFAULT_SERIALIZATION_PROTOCOL_VERSION = 1\n\nDEFAULT_PROTOCOL = 2  # For pickling\nSTATE_FILE_NAME = 'state.pt'\nVERSION_FILE_NAME = 'version.txt'\nSOURCE_FILE_NAME = 'source.py'\nCONFIG_FILE_NAME = 'config.yaml'\nSTASH_FILE_NAME = 'stash.pkl'\nPROTOCOL_VERSION_FILE_NAME = 'protocol_version.txt'\nREQUIREMENTS_FILE_NAME = 'requirements.txt'\n"""
flambe/compile/downloader.py,0,"b'from contextlib import contextmanager\nfrom urllib.parse import urlparse, ParseResult\nimport boto3\nimport botocore\nimport os\nimport subprocess\nimport tempfile\nimport requests\nfrom typing import Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef s3_exists(url: ParseResult) -> bool:\n    """"""Return is an S3 resource exists.\n\n    Parameters\n    ----------\n    url: ParseResult\n        The parsed URL.\n\n    Returns\n    -------\n    bool\n        True if it exists. False otherwise.\n\n    """"""\n    s3 = boto3.resource(\'s3\')\n    try:\n        bucket = s3.Bucket(url.netloc)\n        path = url.path[1:]  # Not consider starting \'/\'\n        objs = list(bucket.objects.filter(Prefix=path))\n        return len(objs) > 0\n    except s3.meta.client.exceptions.NoSuchBucket:\n        return False\n\n\ndef s3_remote_file(url: ParseResult) -> bool:\n    """"""Check if an existing S3 hosted artifact is a file or a folder.\n\n    Parameters\n    ----------\n    url: ParseResult\n        The parsed URL.\n\n    Returns\n    -------\n    bool\n        True if it\'s a file, False if it\'s a folder.\n\n    """"""\n    s3 = boto3.resource(\'s3\')\n    bucket = s3.Bucket(url.netloc)\n    path = url.path[1:]  # Not consider starting \'/\'\n    objs = list(bucket.objects.filter(Prefix=path))\n    if len(objs) == 1 and objs[0].key == path:\n        return True\n\n    return False\n\n\ndef download_s3_file(url: str, destination: str) -> None:\n    """"""Download an S3 file.\n\n    Parameters\n    ----------\n    url: str\n        The S3 URL. Should follow the format:\n        \'s3://<bucket-name>[/path/to/file]\'\n    destination: str\n        The output file where to copy the content\n\n    """"""\n    try:\n        parsed_url = urlparse(url)\n        s3 = boto3.client(\'s3\')\n        s3.download_file(parsed_url.netloc, parsed_url.path[1:], destination)\n    except botocore.client.ClientError:\n        raise ValueError(f""Error downlaoding artifact from s3."")\n\n\ndef http_exists(url: str) -> bool:\n    """"""Check if an HTTP/HTTPS file exists.\n\n    Parameters\n    ----------\n    url: str\n        The HTTP/HTTPS URL.\n\n    Returns\n    -------\n    bool\n        True if the HTTP file exists\n\n    """"""\n    try:\n        r = requests.head(url, allow_redirects=True)\n        return r.status_code != 404\n    except requests.ConnectionError:\n        return False\n\n\ndef download_http_file(url: str, destination: str) -> None:\n    """"""Download an HTTP/HTTPS file.\n\n    Parameters\n    ----------\n    url: str\n        The HTTP/HTTPS URL.\n    destination: str\n        The output file where to copy the content. Needs to support\n        binary writing.\n\n    """"""\n    r = requests.get(url, allow_redirects=True)\n    with open(destination, \'wb\') as f:\n        f.write(r.content)\n\n\ndef download_s3_folder(url: str, destination: str) -> None:\n    """"""Download an S3 folder.\n\n    Parameters\n    ----------\n    url: str\n        The S3 URL. Should follow the format:\n        \'s3://<bucket-name>[/path/to/folder]\'\n    destination: str\n        The output folder where to copy the content\n\n    """"""\n    try:\n        subprocess.check_output(\n            f""aws s3 cp --recursive {url} {destination}"".split(),\n            stderr=subprocess.STDOUT,\n            universal_newlines=True\n        )\n    except subprocess.CalledProcessError as exc:\n        logger.debug(exc.output)\n        raise ValueError(f""Error downlaoding artifacts from s3. "" +\n                         ""Check logs for more information"")\n\n\n@contextmanager\ndef download_manager(path: str, destination: Optional[str] = None):\n    """"""Manager for downloading remote URLs\n\n    Parameters\n    ----------\n    path: str\n        The remote URL to download. Currently, only S3 and http/https\n        URLs are supported.\n        In case it\'s already a local path, it yields the same path.\n    destination: Optional[str]\n        The path where the artifact will be downloaded (this includes\n        the file/folder name also).\n        In case of not given, a temporary directory will be used and the\n        name of the artifact will be inferred from the path.\n\n    Examples\n    --------\n\n    >>> with download_manager(""https://host.com/my/file.zip"") as path:\n    >>>     os.path.exists(path)\n    >>> True\n\n    Yields\n    ------\n    str\n        The new local path\n\n    """"""\n    url = urlparse(path)\n\n    if not url.scheme:\n        # \'path\' is a local path\n        if os.path.exists(os.path.expanduser(path)):\n            yield path\n        else:\n            raise ValueError(f""Path: \'{path}\' does not exist locally."")\n\n    else:\n        tmp_dir = None\n\n        if not destination:\n            tmp_dir = tempfile.TemporaryDirectory()\n            trailing_url = url.path[:-1] if url.path.endswith(\'/\') else url.path\n            fname = trailing_url[trailing_url.rfind(\'/\') + 1:]\n            destination = os.path.join(tmp_dir.name, fname)\n\n        # \'path\' is a remote URL\n        if url.scheme == \'s3\':\n            if not s3_exists(url):\n                raise ValueError(f""S3 url: \'{path}\' is not available"")\n\n            if s3_remote_file(url):\n                download_s3_file(path, destination)\n            else:\n                download_s3_folder(path, destination)\n\n            yield destination\n\n        elif url.scheme == \'http\' or url.scheme == \'https\':\n            if not http_exists(path):\n                raise ValueError(f""HTTP url: \'{path}\' is not available"")\n\n            download_http_file(path, destination)\n            yield destination\n\n        else:\n            raise ValueError(\n                f""\'{path}\' is not a valid remote URL. Only S3 and http/https URLs are supported.""\n            )\n\n        if tmp_dir:\n            tmp_dir.cleanup()\n'"
flambe/compile/extensions.py,5,"b'""""""\nThis module provides methods to orchestrate all extensions\n""""""\n\nimport os\nfrom shutil import which\nimport re\n\nfrom urllib.parse import urlparse\nfrom git import Repo, NoSuchPathError\nimport subprocess\n\nimport importlib\nimport importlib.util\nfrom typing import Dict, Optional, Iterable, Union\nfrom flambe.logging import coloredlogs as cl\nfrom flambe.compile.utils import _is_url\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef download_extensions(extensions: Dict[str, str],\n                        container_folder: str) -> Dict[str, str]:\n    """"""Iterate through the extensions and download the remote urls.\n\n    Parameters\n    ----------\n    extensions: Dict[str, str]\n        The extensions that may contain both local or remote locations.\n    container_folder: str\n        The auxiliary folder where to download the remote repo\n\n    Returns\n    -------\n    Dict[str, str]\n        A new extensions dict with the local paths instead of remote\n        urls. The local paths contain the downloaded remote resources.\n\n    """"""\n    ret = {}\n    for key, inc in extensions.items():\n        if _is_url(inc):\n            loc = os.path.join(container_folder, key)\n            new_inc = _download_remote_extension(inc, loc)\n            ret[key] = new_inc\n        else:\n            expanded_inc = os.path.abspath(os.path.expanduser(inc))  # Could be path with ~, or rel\n            if os.path.exists(expanded_inc):\n                ret[key] = expanded_inc\n            else:\n                ret[key] = inc\n\n    return ret\n\n\ndef _download_remote_extension(extension_url: str,\n                               location: str) -> str:\n    """"""Download a remote hosted extension.\n\n    It fully supports github urls only (for now).\n\n    Parameters\n    ----------\n    extension_url: str\n        The github url pointing to an extension. For example:\n        https://github.com/user/folder/tree/branch/path/to/ext\n    location: str\n        The location to download the repo\n\n    Returns\n    -------\n    str\n        The location of the installed package (which it could not\n        match the location passed as parameter)\n\n    """"""\n    url = urlparse(extension_url)\n    https_ext = url.scheme == \'https\'\n    desc = list(filter(lambda x: len(x) > 0, url.path.split(\'/\')))\n\n    if https_ext and \'github\' not in url.netloc:\n        raise ImportError(""We only support Github hosted extensions for now through https."")\n\n    if https_ext and len(desc) > 4 and _has_svn():\n        # Special case: folder inside github repo\n        # In this case we download with SVN (if available) as it\n        # downloads only the folder instead of full repo\n        # Ex: https://github.com/user/some_repo/tree/branch/path/to/ext\n        user, repository, branch = desc[0], desc[1], desc[3]\n        content = desc[4:]\n        svn_url = (\n            f""{url.scheme}://{url.hostname}/{user}/{repository}/""\n            f""branches/{branch}/{\'/\'.join(content)}""\n        )\n        _download_svn(svn_url, location)\n        logger.debug(f""Downloaded {extension_url} using svn"")\n    else:\n        # Entire git repo (could be github or other)\n        original_location = location\n\n        # Add support for branch URLs in github.\n        # github URL\'s path follow this structure:\n        # {username}/{repo}/tree/{branch}\n        if https_ext and len(desc) >= 4:\n            user, repository, branch = desc[0], desc[1], desc[3]\n            new_url = f""{url.scheme}://{url.hostname}/{user}/{repository}""\n            location = f""{location}/{\'/\'.join(desc[4:])}""\n            url_path = f""{user}/{repository}""\n        else:\n            # In case of ssh url, then remove the \'ssh://\',\n            # if not GitPython fails.\n            new_url = extension_url if https_ext else extension_url[6:]\n            url_path = url.path\n            branch = ""master""\n\n        try:\n            repo = Repo(original_location)\n            logger.debug(f""{extension_url} already exists in {original_location}"")\n\n            remote_url = list(repo.remotes[0].urls)[0]  # Pick origin url\n\n            # Previous extensions does not match this one\n            if not remote_url.endswith(url_path):\n                subprocess.check_call(f""rm -rf {original_location}"".split(),\n                                      stdout=subprocess.DEVNULL,\n                                      stderr=subprocess.DEVNULL)\n                repo = Repo.clone_from(new_url, original_location)\n                logger.debug(f""{extension_url} git cloned as it had a different origin"")\n\n        except NoSuchPathError:\n            # Repo was not downloaded before\n            repo = Repo.clone_from(new_url, original_location)\n            logger.debug(f""Downloaded {extension_url} using git clone"")\n\n        repo.remotes.origin.fetch()\n        repo.git.checkout(branch)\n        repo.remotes.origin.pull()\n        logger.debug(f""Pulled latest changes from {extension_url}"")\n\n    logger.info(cl.YE(f""Downloaded extension {extension_url}""))\n    return location\n\n\ndef _has_svn() -> bool:\n    """"""Return if the host has svn installed""""""\n    return which(\'svn\') is not None\n\n\ndef _download_svn(svn_url: str, location: str,\n                  username: Optional[str] = None, password: Optional[str] = None) -> None:\n    """"""Use svn to download a specific folder inside a git repo.\n\n    This works only with remote Github repositories.\n\n    Parameters\n    ----------\n    svn_url: str\n        The github URL adapted to use the SVN protocol\n    location: str\n        The location to download the folder\n    username: str\n        The username\n    password: str\n        The password\n\n    """"""\n    cmd = [\'svn\', \'export\']\n    if username:\n        cmd.extend([\'--username\', username])\n    if password:\n        cmd.extend([\'--password\', password])\n\n    cmd.extend([\'--force\', svn_url, location])\n\n    ret = subprocess.check_call(cmd, stdout=subprocess.DEVNULL,\n                                stderr=subprocess.DEVNULL)\n    if ret != 0:\n        raise ImportError(f""Could not download folder through svn {svn_url}"")\n\n\ndef install_extensions(extensions: Dict[str, str],\n                       user_flag: bool = False) -> None:\n    """"""Install extensions.\n\n    At this point, all extensions must be either local paths or\n    valid pypi packages.\n\n    Remote extensions hosted in Github must have been download first.\n\n    Parameters\n    ----------\n    extensions: Dict[str, str]\n        Dictionary of extensions\n    user_flag: bool\n        Use --user flag when running pip install\n\n    """"""\n    cmd = [\'python3\', \'-m\', \'pip\', \'install\', \'-U\']\n    if user_flag:\n        cmd.append(\'--user\')\n    for ext, resource in extensions.items():\n        curr_cmd = cmd[:]\n\n        try:\n            if os.path.exists(resource):\n                # Package is local\n                if os.sep not in resource:\n                    resource = f""./{resource}""\n            else:\n                # Package follows pypi notation: ""torch>=0.4.1,<1.1""\n                resource = f""{resource}""\n\n            curr_cmd.append(resource)\n\n            output: Union[bytes, str]\n            output = subprocess.check_output(\n                curr_cmd,\n                stderr=subprocess.DEVNULL\n            )\n\n            output = output.decode(""utf-8"")\n\n            for l in output.splitlines():\n                logger.debug(l)\n                r = re.search(r\'Successfully uninstalled (?P<pkg_name>\\D*)-(?P<version>.*)\', l)\n                if r and \'pkg_name\' in r.groupdict():\n                    logger.info(cl.RE(f""WARNING: While installing {ext}, "" +\n                                      f""existing {r.groupdict()[\'pkg_name\']}-"" +\n                                      f""{r.groupdict()[\'version\']} was uninstalled.""))\n        except subprocess.CalledProcessError:\n            raise ImportError(f""Could not install package in {resource}"")\n\n        logger.info(cl.GR(f""Successfully installed {ext}""))\n\n\ndef is_installed_module(module_name: str) -> bool:\n    """"""Whether the module is installed.\n\n    Parameters\n    ----------\n    module_name: str\n        The name of the module to check for\n\n    Returns\n    -------\n    bool\n        True if the module is installed locally, False otherwise.\n\n    """"""\n    return importlib.util.find_spec(module_name) is not None\n\n\ndef import_modules(modules: Iterable[str]) -> None:\n    """"""Dinamically import modules\n\n    Parameters\n    ----------\n    modules: Iterable[str]\n        An iterable of strings containing the modules\n        to import\n\n    """"""\n    for mod_name in modules:\n        try:\n            # Importing modules adds undesired handlers to\n            # the root logger.\n            # We will backup the handlers and updates them\n            # after importing\n            backup_handlers = logging.root.handlers[:]\n\n            importlib.import_module(mod_name)\n\n            # Remove all existing root handlers and\n            # re-apply the backed up root handlers\n            for x in logging.root.handlers[:]:\n                logging.root.removeHandler(x)\n            for x in backup_handlers:\n                logging.root.addHandler(x)\n\n            logger.info(cl.YE(f""Imported extensions {mod_name}""))\n        except ModuleNotFoundError as e:\n            raise ImportError(\n                f""Error importing {mod_name}: {e}. Please \'pip install\' "" +\n                ""the package manually or use \'-i\' flag (only applies when running "" +\n                ""flambe as cmd line program)""\n            )\n\n\ndef setup_default_modules():\n    from flambe.compile.utils import make_component\n    from flambe.optim import LRScheduler\n    import torch\n    import ray\n    exclude = [\'torch.nn.quantized\', \'torch.nn.qat\']\n    make_component(torch.nn.Module, only_module=\'torch.nn\', exclude=exclude)\n    make_component(torch.optim.Optimizer, only_module=\'torch.optim\')\n    make_component(torch.optim.lr_scheduler._LRScheduler,\n                   only_module=\'torch.optim.lr_scheduler\', parent_component_class=LRScheduler)\n    make_component(ray.tune.schedulers.TrialScheduler)\n    make_component(ray.tune.suggest.SearchAlgorithm)\n'"
flambe/compile/registrable.py,1,"b'from typing import Type, TypeVar, Callable, Mapping, Dict, List, Any, Optional, Set\nfrom abc import abstractmethod, ABC\nfrom collections import defaultdict\nfrom warnings import warn\nimport functools\nimport logging\nimport inspect\n\nfrom ruamel.yaml import YAML, ScalarNode\n\n\nlogger = logging.getLogger(__name__)\n\nyaml = YAML()\n\n_reg_prefix: Optional[str] = None\n\nR = TypeVar(\'R\', bound=\'Registrable\')\nA = TypeVar(\'A\')\nRT = TypeVar(\'RT\', bound=Type[\'Registrable\'])\n\n\nclass RegistrationError(Exception):\n    """"""Error thrown when acessing yaml tag on a non-registered class\n\n    Thrown when trying to access the default yaml tag for a class\n    typically occurs when called on an abstract class\n    """"""\n\n    pass\n\n\ndef make_from_yaml_with_metadata(from_yaml_fn: Callable[..., Any],\n                                 tag: str,\n                                 factory_name: Optional[str] = None) -> Callable[..., Any]:\n    @functools.wraps(from_yaml_fn)\n    def wrapped(constructor: Any, node: Any) -> Any:\n        obj = from_yaml_fn(constructor, node, factory_name=factory_name)\n        # Access dict directly because obj may be a Schema, and have\n        # special dot notation access behavior\n        obj.__dict__[\'_created_with_tag\'] = tag\n        return obj\n    return wrapped\n\n\ndef make_to_yaml_with_metadata(to_yaml_fn: Callable[..., Any]) -> Callable[..., Any]:\n    @functools.wraps(to_yaml_fn)\n    def wrapped(representer: Any, node: Any) -> Any:\n        if hasattr(node, \'_created_with_tag\'):\n            tag = node._created_with_tag\n        else:\n            tag = Registrable.get_default_tag(type(node))\n        return to_yaml_fn(representer, node, tag=tag)\n    return wrapped\n\n\nclass registration_context:\n\n    def __init__(self, namespace: str) -> None:\n        self._namespace = namespace\n\n    def __enter__(self) -> None:\n        global _reg_prefix\n        self._prev_reg_prefix = _reg_prefix\n        _reg_prefix = self._namespace\n\n    def __exit__(self, *args: Any) -> int:\n        global _reg_prefix\n        _reg_prefix = self._prev_reg_prefix\n        return False\n\n    def __call__(self, func: Callable[..., Any]) -> Callable[..., Any]:\n        @functools.wraps(func)\n        def decorate_reg_context(*args: Any, **kwargs: Any) -> Any:\n            with self:\n                return func(*args, **kwargs)\n        return decorate_reg_context\n\n\nclass Registrable(ABC):\n    """"""Subclasses automatically registered as yaml tags\n\n    Automatically registers subclasses with the yaml loader by\n    adding a constructor and representer which can be overridden\n    """"""\n\n    _yaml_tags: Dict[Any, List[str]] = defaultdict(list)\n    _yaml_tag_namespace: Dict[Type, str] = defaultdict(str)\n    _yaml_registered_factories: Set[str] = set()\n\n    def __init_subclass__(cls: Type[R],\n                          should_register: Optional[bool] = True,\n                          tag_override: Optional[str] = None,\n                          tag_namespace: Optional[str] = None,\n                          **kwargs: Mapping[str, Any]) -> None:\n        super().__init_subclass__(**kwargs)  # type: ignore\n        # Copy parent set so that factories are inherited\n        # But not shared across cousin classes\n        cls._yaml_registered_factories = set(cls._yaml_registered_factories)\n        if should_register:\n            default_tag = cls.__name__ if tag_override is None else tag_override\n            # NOTE: abstract classes are registered too. This allows us\n            # to raise an exception if you actually try to use one,\n            # in case you think a class should be concrete but is\n            # actually still abstract\n            Registrable.register_tag(cls, default_tag, tag_namespace)\n\n    @staticmethod\n    def register_tag(class_: RT, tag: str, tag_namespace: Optional[str] = None) -> None:\n        modules = class_.__module__.split(\'.\')\n        top_level_module_name = modules[0] if len(modules) > 0 else None\n        global _reg_prefix\n        if _reg_prefix is not None:\n            tag_namespace = _reg_prefix\n        elif tag_namespace is not None:\n            tag_namespace = tag_namespace\n        elif (tag_namespace is None and top_level_module_name is not None) and \\\n                (top_level_module_name != \'flambe\' and top_level_module_name != \'tests\'):\n            tag_namespace = top_level_module_name\n        else:\n            tag_namespace = None\n        # Create a tag that includes namespace e.g. `!torch.Adam`\n        if tag_namespace is not None:\n            full_tag = f""!{tag_namespace}.{tag}""\n        else:\n            full_tag = f""!{tag}""\n        # full_tag = f""!{tag_namespace}.{tag}"" if tag_namespace is\n        # not None else f""!{tag}""\n        if class_ in class_._yaml_tag_namespace:\n            if tag_namespace != class_._yaml_tag_namespace[class_]:\n                # Don\'t register anything not matching the already set\n                # namespace\n                # Helps limit chance of tag collisions\n                msg = (f""You are trying to register class {class_} with namespace ""\n                       f""{tag_namespace} != {class_._yaml_tag_namespace[class_]} ""\n                       ""so ignoring"")\n                warn(msg)\n                return\n        elif tag_namespace is not None:\n            # Set namespace so that the above branch can catch\n            # accidentally forgetting namespace\n            class_._yaml_tag_namespace[class_] = tag_namespace\n        # Ensure all tags are only associated with that specific class,\n        # NOT any subclasses\n        class_._yaml_tags[class_].append(full_tag)\n        # Code based on the ruamel.yaml yaml_object decorator\n        # Look for to_yaml and from_yaml methods -- if not present\n        # default to built in default flow style\n\n        def registration_helper(factory_name: Optional[str] = None) -> None:\n            from_yaml_tag = full_tag if factory_name is None else full_tag + ""."" + factory_name\n            logger.debug(f""Registering tag: {from_yaml_tag}"")\n            try:\n                to_yaml = class_.to_yaml\n            except AttributeError:\n                def t_y(representer: Any, node: Any, tag: str) -> Any:\n                    return representer.represent_yaml_object(\n                        tag, node, class_, flow_style=representer.default_flow_style\n                    )\n                to_yaml = t_y\n            finally:\n                yaml.representer.add_representer(class_, make_to_yaml_with_metadata(to_yaml))\n            try:\n                from_yaml = class_.from_yaml\n            except AttributeError:\n                def f_y(constructor: Any, node: Any, factory_name: str) -> Any:\n                    return constructor.construct_yaml_object(node, class_)\n                from_yaml = f_y\n            finally:\n                yaml.constructor.add_constructor(\n                    from_yaml_tag,\n                    make_from_yaml_with_metadata(from_yaml, from_yaml_tag, factory_name)\n                )\n\n        registration_helper()\n        for factory_name in class_._yaml_registered_factories:\n            # Add factory tag to registry\n            factory_full_tag = f\'{full_tag}.{factory_name}\'\n            class_._yaml_tags[(class_, factory_name)] = [factory_full_tag]\n            # Every time we register a new tag, make sure that you can\n            # use each factory with that new tag\n            registration_helper(factory_name)\n\n    @staticmethod\n    def get_default_tag(class_: RT, factory_name: Optional[str] = None) -> str:\n        """"""Retrieve default yaml tag for class `cls`\n\n        Retrieve the default tag (aka the last one, which will\n        be the only one, or the alias if it exists) for use in\n        yaml representation\n        """"""\n        if class_ in class_._yaml_tags:\n            tag = class_._yaml_tags[class_][-1]\n            if (factory_name is not None) and \\\n                    (factory_name not in class_._yaml_registered_factories):\n                raise RegistrationError(f""This class has no factory {factory_name}"")\n            elif factory_name is not None:\n                tag = tag + \'.\' + factory_name\n            return tag\n        raise RegistrationError(""This class has no registered tags"")\n\n    @classmethod\n    @abstractmethod\n    def to_yaml(cls, representer: Any, node: Any, tag: str) -> Any:\n        """"""Use representer to create yaml representation of node\n\n        See Component class, and experiment/options for examples\n\n        """"""\n        pass\n\n    @classmethod\n    @abstractmethod\n    def from_yaml(cls, constructor: Any, node: Any, factory_name: str) -> Any:\n        """"""Use constructor to create an instance of cls\n\n        See Component class, and experiment/options for examples\n\n        """"""\n        pass\n\n\ndef alias(tag: str,\n          tag_namespace: Optional[str] = None) -> Callable[[RT], RT]:\n    """"""Decorate a Registrable subclass with a new tag\n\n    Can be added multiple times to give a class multiple aliases,\n    however the top most alias tag will be the default tag which means\n    it will be used when representing the class in YAML\n\n    """"""\n\n    def decorator(cls: RT) -> RT:\n        Registrable.register_tag(cls, tag, tag_namespace)\n        return cls\n\n    return decorator\n\n\ndef register(cls: Type[A], tag: str) -> Type[A]:\n    """"""Safely register a new tag for a class\n\n    Similar to alias, but it\'s intended to be used on classes that are\n    not already subclasses of Registrable, and it is NOT a decorator\n\n    """"""\n    if not hasattr(cls, \'_yaml_tags\'):\n        cls._yaml_tags = defaultdict(list)  # type: ignore\n    if not hasattr(cls, \'_yaml_tag_namespace\'):\n        cls._yaml_tag_namespace = defaultdict(str)  # type: ignore\n    if not hasattr(cls, \'_yaml_registered_factories\'):\n        cls._yaml_registered_factories = set()  # type: ignore\n    return alias(tag)(cls)  # type: ignore\n\n\nclass registrable_factory:\n    """"""Decorate Registrable factory method for use in the config\n\n    This Descriptor class will set properties that allow the factory\n    method to be specified directly in the config as a suffix to the\n    tag; for example:\n\n    .. code-block:: python\n\n        class MyModel(Component):\n\n            @registrable_factory\n            def from_file(cls, path):\n                # load instance from path\n                ...\n                return instance\n\n    defines the factory, which can then be used in yaml:\n\n    .. code-block:: yaml\n\n        model: !MyModel.from_file\n            path: some/path/to/file.pt\n\n    """"""\n\n    def __init__(self, fn: Any) -> None:\n        self.fn = fn\n\n    def __set_name__(self, owner: type, name: str) -> None:\n        if not hasattr(owner, \'_yaml_registered_factories\'):\n            raise RegistrationError(f""class {owner} doesn\'t have property ""\n                                    f""_yaml_registered_factories; {owner} should subclass ""\n                                    ""Registrable or Component"")\n        owner._yaml_registered_factories.add(name)  # type: ignore\n        setattr(owner, name, self.fn)\n\n\nclass MappedRegistrable(Registrable):\n\n    @classmethod\n    def to_yaml(cls, representer: Any, node: Any, tag: str) -> Any:\n        """"""Use representer to create yaml representation of node""""""\n        return representer.represent_mapping(tag, node._saved_kwargs)\n\n    @classmethod\n    def from_yaml(cls, constructor: Any, node: Any, factory_name: str) -> Any:\n        """"""Use constructor to create an instance of cls""""""\n        if inspect.isabstract(cls):\n            msg = f""You\'re trying to initialize an abstract class {cls.__name__}. "" \\\n                  + ""If you think it\'s concrete, double check you\'ve spelled "" \\\n                  + ""all the originally abstract method names correctly.""\n            raise Exception(msg)\n        if isinstance(node, ScalarNode):\n            nothing = constructor.construct_yaml_null(node)\n            if nothing is not None:\n                warn(f""Non-null scalar argument to {cls.__name__} will be ignored. A map of kwargs""\n                     "" should be used instead."")\n            return cls()\n        # NOTE: construct_yaml_map is a generator that yields the\n        # constructed data and then updates it\n        kwargs, = list(constructor.construct_yaml_map(node))\n        if factory_name is not None:\n            factory_method = getattr(cls, factory_name)\n        else:\n            factory_method = cls\n        instance = factory_method(**kwargs)\n        instance._saved_kwargs = kwargs\n        return instance\n'"
flambe/compile/serialization.py,9,"b'import logging\nimport os\nfrom collections import OrderedDict\nfrom typing import Dict, Any, Iterable, Tuple, Optional, Sequence, NamedTuple, List, Mapping\nimport tarfile\nimport tempfile\n\nimport dill\nimport torch\n\nfrom flambe.compile.registrable import yaml\nfrom flambe.compile.utils import write_deps\nfrom flambe.compile.downloader import download_manager\nfrom flambe.compile.extensions import import_modules, is_installed_module, install_extensions, \\\n    setup_default_modules\n\n# Constants used for state representation & serialization\nfrom flambe.compile.const import STATE_DICT_DELIMETER, FLAMBE_SOURCE_KEY, FLAMBE_CLASS_KEY, \\\n    FLAMBE_CONFIG_KEY, FLAMBE_DIRECTORIES_KEY, VERSION_KEY, \\\n    HIGHEST_SERIALIZATION_PROTOCOL_VERSION, DEFAULT_SERIALIZATION_PROTOCOL_VERSION, \\\n    DEFAULT_PROTOCOL, STATE_FILE_NAME, VERSION_FILE_NAME, SOURCE_FILE_NAME, CONFIG_FILE_NAME, \\\n    PROTOCOL_VERSION_FILE_NAME, FLAMBE_STASH_KEY, STASH_FILE_NAME, REQUIREMENTS_FILE_NAME\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass LoadError(Exception):\n    """"""Error thrown because of fatal error when loading""""""\n\n\nclass SaveTreeNode(NamedTuple):\n    """"""Tree representation corresponding to the directory save format""""""\n\n    state: Dict[str, Any]\n    version: str\n    class_name: str\n    source_code: str\n    config: str\n    object_stash: Dict[str, Any]\n    children: Dict[str, Any]  # Nested typing not supported yet\n\n\nclass State(OrderedDict):\n    """"""A state object for Flambe.""""""\n\n    _metadata: Dict[str, Any]  # TODO should be instance property\n\n\n# Private Helpers\n\ndef _convert_to_tree(metadata: Dict[str, Any]) -> SaveTreeNode:\n    root_state: Dict[str, Any] = OrderedDict()\n    # PyTorch / flambe states need this property\n    root_state._metadata = {}  # type: ignore\n    tree = SaveTreeNode(state=root_state, version=metadata[\'\'][VERSION_KEY],\n                        class_name=metadata[\'\'][FLAMBE_CLASS_KEY],\n                        source_code=metadata[\'\'][FLAMBE_SOURCE_KEY],\n                        config=metadata[\'\'].get(FLAMBE_CONFIG_KEY, \'\'),\n                        object_stash=metadata[\'\'].get(FLAMBE_STASH_KEY, {}),\n                        children={})\n    for compound_key in metadata[FLAMBE_DIRECTORIES_KEY]:\n        if compound_key == \'\':\n            continue\n        current_dict = tree\n        component_keys = compound_key.split(STATE_DICT_DELIMETER)\n        last_node_i = 0\n        for i in range(len(component_keys)):\n            key = STATE_DICT_DELIMETER.join(component_keys[last_node_i:i + 1])\n            if STATE_DICT_DELIMETER.join(component_keys[:i + 1]) \\\n                    not in metadata[FLAMBE_DIRECTORIES_KEY]:\n                continue\n            last_node_i += 1\n            prefix = STATE_DICT_DELIMETER.join(component_keys[:i + 1])\n            current_value = current_dict.children.get(key)\n            if key not in current_dict.children:\n                # nested key not yet created\n                m = metadata[prefix]\n                new_state: Dict[str, Any] = OrderedDict()\n                new_state._metadata = {}  # type: ignore\n                current_dict.children[key] = SaveTreeNode(state=new_state,\n                                                          version=m[VERSION_KEY],\n                                                          class_name=m[FLAMBE_CLASS_KEY],\n                                                          source_code=m[FLAMBE_SOURCE_KEY],\n                                                          config=m.get(FLAMBE_CONFIG_KEY, \'\'),\n                                                          object_stash=m.get(FLAMBE_STASH_KEY, {}),\n                                                          children={})\n                current_dict = current_dict.children[key]\n            elif isinstance(current_value, SaveTreeNode):\n                # key was already created, descend a layer further\n                current_dict = current_value\n            else:\n                # key was already created but shouldn\'t have been\n                raise Exception()\n        if not isinstance(current_dict, SaveTreeNode):\n            raise Exception(\'current dict not save tree node\')\n\n    return tree\n\n\ndef _fetch_tree_item(save_tree: SaveTreeNode, key: Sequence[str]) -> Tuple[str, SaveTreeNode]:\n    current = save_tree\n    last_i = 0\n    for i, _ in enumerate(key):\n        current_key = STATE_DICT_DELIMETER.join(key[last_i:i + 1])\n        if current_key in current.children:\n            current = current.children[current_key]  # type: ignore\n            last_i = i + 1\n            current_key = STATE_DICT_DELIMETER.join(key[last_i:i + 1])\n    return current_key, current\n\n\ndef _update_save_tree(save_tree: SaveTreeNode, key: Sequence[str], value: Any) -> None:\n    current_key, current = _fetch_tree_item(save_tree, key)\n    current.state[current_key] = value\n\n\ndef _update_save_tree_metadata(save_tree: SaveTreeNode, key: Sequence[str], value: Any) -> None:\n    current_key, current = _fetch_tree_item(save_tree, key)\n    current.state._metadata[current_key] = value  # type: ignore\n\n\ndef _traverse_all_nodes(save_tree: SaveTreeNode,\n                        path: Optional[List[str]] = None\n                        ) -> Iterable[Tuple[List[str], SaveTreeNode]]:\n    if path is None:\n        path = []\n    yield path, save_tree\n    for key, value in save_tree.children.items():\n        yield from _traverse_all_nodes(value, path + [key])\n\n\ndef _extract_prefix(root, directory):\n    if directory.startswith(root):\n        return directory[len(root):].lstrip(os.sep).replace(os.sep, STATE_DICT_DELIMETER)\n    else:\n        raise Exception()  # TODO\n\n\ndef _prefix_keys(state, prefix):\n    for key in set(state.keys()):\n        val = state[key]\n        del state[key]\n        state[prefix + key] = val\n    return state\n\n\ndef traverse(nested: Mapping[str, Any], path: Optional[List[str]] = None) -> Iterable[Any]:\n    """"""Iterate over a nested mapping returning the path and key, value.\n\n    Parameters\n    ----------\n    nested : Mapping[str, Any]\n        Mapping where some values are also mappings that should be\n        traversed\n    path : List[str]\n        List of keys that were used to reach the current mapping\n\n    Returns\n    -------\n    Iterable[Any]\n        Iterable of path, key, value triples\n\n    """"""\n    if path is None:\n        path = []\n    for key, value in nested.items():\n        if isinstance(value, Mapping):\n            yield from traverse(value, path + [key])\n        else:\n            yield path, key, value\n\n\ndef _update_link_refs(schema: Mapping) -> None:\n    """"""Resolve links in schemas at `block_id`.\n\n    Parameters\n    ----------\n    schema : Dict[str, Schema[Any]]\n        Map from `block_id` to `Schema` object\n\n    """"""\n    from flambe.compile.component import Link\n    for _, _, value in traverse(schema):\n        if isinstance(value, Link):\n            value.target = schema[value.root_schema]\n\n\n# Public Serialization Functions\n\n\ndef save_state_to_file(state: State,\n                       path: str,\n                       compress: bool = False,\n                       pickle_only: bool = False,\n                       overwrite: bool = False,\n                       pickle_module=dill,\n                       pickle_protocol=DEFAULT_PROTOCOL) -> None:\n    """"""Save state to given path\n\n    By default the state will be saved in directory structure that\n    mirrors the object hierarchy, so that you can later inspect the\n    save file and load individual components more easily. If you would\n    like to compress this directory structure using tar + gz, set\n    `compress` to True. You can also use pickle to write\n    a single output file, more similar to how PyTorch\'s save function\n    operates.\n\n    Parameters\n    ----------\n    state : State\n        The state_dict as defined by PyTorch; a flat dictionary\n        with compound keys separated by \'.\'\n    path : str\n        Location to save the file / save directory to; This should be a\n        new non-existent path; if the path is an existing directory\n        and it contains files an exception will be raised. This is\n        because the path includes the final name of the save file (if\n        using pickle or compress) or the final name of the save\n        directory.\n    compress : bool\n        Whether to compress the save file / directory via tar + gz\n    pickle_only : bool\n        Use given pickle_module instead of the hiearchical save format\n        (the default is False).\n    overwrite : bool\n        If true, overwrites the contents of the given path to create a\n        directory at that location\n    pickle_module : type\n        Pickle module that has load and dump methods; dump should\n        accpet a pickle_protocol parameter (the default is dill).\n    pickle_protocol : type\n        Pickle protocol to use; see pickle for more details (the\n        default is 2).\n\n    Raises\n    ------\n    ValueError\n        If the given path exists, is a directory, and already contains\n        some files.\n\n    """"""\n    if os.path.exists(path):\n        if os.path.isdir(path):\n            dir_contents = os.listdir(path)\n            if len(dir_contents) != 0 and not overwrite:\n                raise ValueError(f\'The given path ({path}) points to an existing directory \'\n                                 f\'containing files:\\n{dir_contents}\\n\'\n                                 \'Please use a new path, or an existing directory without files, \'\n                                 \'or specify overwrite=True.\')\n        else:\n            if not overwrite:\n                raise ValueError(f\'The given path ({path}) points to an existing file. Specify \'\n                                 \'overwrite=True to overwrite the file with a save file / dir.\')\n    if compress:\n        original_path = path\n        temp = tempfile.TemporaryDirectory()\n        path = os.path.join(temp.name, os.path.basename(original_path))\n    if pickle_only:\n        head, tail = os.path.split(path)\n        if tail == \'\':\n            path = head + \'.pkl\'\n        else:\n            path = path + \'.pkl\'\n        if compress:\n            orig_head, orig_tail = os.path.split(original_path)\n            if orig_tail == \'\':\n                original_path = orig_head + \'.pkl\'\n            else:\n                original_path = original_path + \'.pkl\'\n        with open(path, \'wb\') as f_pkl:\n            pickle_module.dump(state, f_pkl, protocol=pickle_protocol)\n    else:\n        save_tree = _convert_to_tree(state._metadata)\n        for key in state.keys():\n            _update_save_tree(save_tree, key.split(STATE_DICT_DELIMETER), state[key])\n        for key in state._metadata.keys():\n            _update_save_tree_metadata(save_tree, key.split(STATE_DICT_DELIMETER),\n                                       state._metadata[key])\n        for node_path, node in _traverse_all_nodes(save_tree):\n            current_path = os.path.join(path, *node_path)\n            if not os.path.isdir(current_path):\n                os.makedirs(current_path, exist_ok=True)\n            torch.save(node.state, os.path.join(current_path, STATE_FILE_NAME), pickle_module,\n                       pickle_protocol)\n            with open(os.path.join(current_path, VERSION_FILE_NAME), \'w\') as f_version:\n                version_info = f""{node.class_name}:{node.version}""\n                f_version.write(version_info)\n            with open(os.path.join(current_path, SOURCE_FILE_NAME), \'w\') as f_source:\n                f_source.write(node.source_code)\n            with open(os.path.join(current_path, CONFIG_FILE_NAME), \'w\') as f_config:\n                f_config.write(node.config)\n            with open(os.path.join(current_path, PROTOCOL_VERSION_FILE_NAME), \'w\') as f_proto:\n                f_proto.write(str(DEFAULT_SERIALIZATION_PROTOCOL_VERSION))\n            with open(os.path.join(current_path, STASH_FILE_NAME), \'wb\') as f_stash:\n                torch.save(node.object_stash, f_stash, pickle_module, pickle_protocol)\n\n        write_deps(os.path.join(path, REQUIREMENTS_FILE_NAME))\n\n    if compress:\n        compressed_file_name = original_path + \'.tar.gz\'\n        with tarfile.open(name=compressed_file_name, mode=\'w:gz\') as tar_gz:\n            tar_gz.add(path, arcname=os.path.basename(path))\n        temp.cleanup()\n\n\n# TODO fix type of object to be Component without circular dependency\n\ndef save(obj: Any,\n         path: str,\n         compress: bool = False,\n         pickle_only: bool = False,\n         overwrite: bool = False,\n         pickle_module=dill,\n         pickle_protocol=DEFAULT_PROTOCOL) -> None:\n    """"""Save `Component` object to given path\n\n    See `save_state_to_file` for a more detailed explanation\n\n    Parameters\n    ----------\n    obj : Component\n        The component to save.\n    path : str\n        Location to save the file / save directory to\n    compress : bool\n        Whether to compress the save file / directory via tar + gz\n    pickle_only : bool\n        Use given pickle_module instead of the hiearchical save format\n        (the default is False).\n    overwrite : bool\n        If true, overwrites the contents of the given path to create a\n        directory at that location\n    pickle_module : type\n        Pickle module that has load and dump methods; dump should\n        accept a pickle_protocol parameter (the default is dill).\n    pickle_protocol : type\n        Pickle protocol to use; see pickle for more details (the\n        default is 2).\n\n    """"""\n    state = obj.get_state()\n    save_state_to_file(state, path, compress, pickle_only, overwrite,\n                       pickle_module, pickle_protocol)\n\n\ndef load_state_from_file(path: str,\n                         map_location=None,\n                         pickle_module=dill,\n                         **pickle_load_args) -> State:\n    """"""Load state from the given path\n\n    Loads a flambe save directory, pickled save object, or a compressed\n    version of one of these two formats (using tar + gz). Will\n    automatically infer the type of save format and if the directory\n    structure is used, the serialization protocol version as well.\n\n    Parameters\n    ----------\n    path : str\n        Path to the save file or directory\n    map_location : type\n        Location (device) where items will be moved. ONLY used when the\n        directory save format is used. See torch.load documentation for\n        more details (the default is None).\n    pickle_module : type\n        Pickle module that has load and dump methods; dump should\n        accept a pickle_protocol parameter (the default is dill).\n    **pickle_load_args : type\n        Additional args that `pickle_module` should use to load; see\n        torch.load documentation for more details\n\n    Returns\n    -------\n    State\n        state_dict that can be loaded into a compatible Component\n\n    """"""\n    with download_manager(path) as path:\n        state = State()\n        state._metadata = OrderedDict({FLAMBE_DIRECTORIES_KEY: set()})\n        temp = None\n        try:\n            if not os.path.isdir(path) and tarfile.is_tarfile(path):\n                temp = tempfile.TemporaryDirectory()\n                with tarfile.open(path, \'r:gz\') as tar_gz:\n                    tar_gz.extractall(path=temp.name)\n                    expected_name = tar_gz.getnames()[0]\n                path = os.path.join(temp.name, expected_name)\n            if os.path.isdir(path):\n                for current_dir, subdirs, files in os.walk(path):\n                    prefix = _extract_prefix(path, current_dir)\n                    protocol_version_file = os.path.join(current_dir, PROTOCOL_VERSION_FILE_NAME)\n                    with open(protocol_version_file) as f_proto:\n                        saved_protocol_version = int(f_proto.read())\n                        if saved_protocol_version > HIGHEST_SERIALIZATION_PROTOCOL_VERSION:\n                            raise Exception(\'This version of Flambe only supports serialization\'\n                                            f\'protocol versions <= \'\n                                            f\'{HIGHEST_SERIALIZATION_PROTOCOL_VERSION}. \'\n                                            \'Found version \'\n                                            f\'{saved_protocol_version} at {protocol_version_file}\')\n                    component_state = torch.load(os.path.join(current_dir, STATE_FILE_NAME),\n                                                 map_location, pickle_module, **pickle_load_args)\n                    with open(os.path.join(current_dir, VERSION_FILE_NAME)) as f_version:\n                        version_info = f_version.read()\n                        class_name, version = version_info.split(\':\')\n                    with open(os.path.join(current_dir, SOURCE_FILE_NAME)) as f_source:\n                        source = f_source.read()\n                    with open(os.path.join(current_dir, CONFIG_FILE_NAME)) as f_config:\n                        config = f_config.read()\n                    with open(os.path.join(current_dir, STASH_FILE_NAME), \'rb\') as f_stash:\n                        stash = torch.load(f_stash, map_location, pickle_module, **pickle_load_args)\n                    local_metadata = {VERSION_KEY: version, FLAMBE_CLASS_KEY: class_name,\n                                      FLAMBE_SOURCE_KEY: source, FLAMBE_CONFIG_KEY: config}\n                    if len(stash) > 0:\n                        local_metadata[FLAMBE_STASH_KEY] = stash\n                    full_prefix = prefix + STATE_DICT_DELIMETER if prefix != \'\' else prefix\n                    _prefix_keys(component_state, full_prefix)\n                    state.update(component_state)\n                    if hasattr(component_state, \'_metadata\'):\n                        _prefix_keys(component_state._metadata, full_prefix)\n                        # Load torch.nn.Module metadata\n                        state._metadata.update(component_state._metadata)\n                    # Load flambe.nn.Module metadata\n                    state._metadata[prefix] = local_metadata\n                    state._metadata[FLAMBE_DIRECTORIES_KEY].add(prefix)\n            else:\n                with open(path, \'rb\') as f_pkl:\n                    state = pickle_module.load(f_pkl)\n        except Exception as e:\n            raise e\n        finally:\n            if temp is not None:\n                temp.cleanup()\n        return state\n\n\ndef load(path: str,\n         map_location=None,\n         auto_install=False,\n         pickle_module=dill,\n         **pickle_load_args):\n    """"""Load object with state from the given path\n\n    Loads a flambe object by using the saved config files, and then\n    loads the saved state into said object. See `load_state_from_file`\n    for details regarding how the state is loaded from the save file or\n    directory.\n\n    Parameters\n    ----------\n    path : str\n        Path to the save file or directory\n    map_location : type\n        Location (device) where items will be moved. ONLY used when the\n        directory save format is used. See torch.load documentation for\n        more details (the default is None).\n    auto_install : bool\n        If True, automatically installs extensions as needed.\n    pickle_module : type\n        Pickle module that has load and dump methods; dump should\n        accept a pickle_protocol parameter (the default is dill).\n    **pickle_load_args : type\n        Additional args that `pickle_module` should use to load; see\n        torch.load documentation for more details\n\n    Returns\n    -------\n    Component\n        object with both the architecture (config) and state that was\n        saved to path\n\n    Raises\n    ------\n    LoadError\n        If a Component object is not loadable from the given path\n        because extensions are not installed, or the config is empty,\n        nonexistent, or otherwise invalid.\n\n    """"""\n    state = load_state_from_file(path, map_location, pickle_module, **pickle_load_args)\n    yaml_config = state._metadata[\'\'][FLAMBE_CONFIG_KEY]\n    stash = state._metadata[\'\'][FLAMBE_STASH_KEY] \\\n        if FLAMBE_STASH_KEY in state._metadata[\'\'] else None\n    setup_default_modules()\n    yamls = list(yaml.load_all(yaml_config))\n\n    if yamls is None:\n        raise LoadError(""Cannot load schema from empty config. This object may not have been saved""\n                        "" for any of the following reasons:\\n - The object was not created from a""\n                        ""config or with compile method\\n - The object originally linked to other""\n                        ""objects that cannot be represented in YAML"")\n    if len(yamls) > 2:\n        raise LoadError(f""{os.path.join(path, CONFIG_FILE_NAME)} should contain an (optional) ""\n                        ""extensions section and the main object."")\n    if len(yamls) == 2:\n        if yamls[0] is not None:\n            extensions = dict(yamls[0])\n            custom_modules = extensions.keys()\n            for x in custom_modules:\n                if not is_installed_module(x):\n                    if auto_install:\n                        logger.warn(f""auto_install==True, installing missing Module ""\n                                    f""{x}: {extensions[x]}"")\n                        install_extensions({x: extensions[x]})\n                        logger.debug(f""Installed module {x} from {extensions[x]}"")\n                    else:\n                        raise ImportError(\n                            f""Module {x} is required and not installed. Please \'pip install\'""\n                            ""the package containing the module or set auto_install flag""\n                            "" to True.""\n                        )\n                import_modules([x])\n                logger.debug(f""Automatically imported {x}"")\n\n            # Reload with extensions\' module imported (and registered)\n            schema = list(yaml.load_all(yaml_config))[1]\n\n            # Set the extensions to the schema so that they are\n            # passed when compiling the component.\n            schema.add_extensions_metadata(extensions)\n        else:\n            schema = yamls[1]\n    elif len(yamls) == 1:\n        schema = yamls[0]\n    else:\n        raise LoadError(""No config found at location; cannot load. Try just loading state with ""\n                        ""the function \'load_state_from_file\'"")\n\n    if schema is None:\n        raise LoadError(""Cannot load schema from empty config. This object may not have been saved""\n                        "" for any of the following reasons:\\n - The object was not created from a""\n                        ""config or with compile method\\n - The object originally linked to other""\n                        ""objects that cannot be represented in YAML"")\n\n    _update_link_refs(schema)\n    # TODO: maybe replace with instance check if solution to circular\n    # dependency with component is found\n\n    try:\n        instance = schema(stash)\n    except TypeError:\n        raise LoadError(f""Loaded object is not callable - likely because an extension is not ""\n                        f""installed. Check if {os.path.join(path, CONFIG_FILE_NAME)} has an ""\n                        f""extensions section at the top and install as necessary. Alternatively ""\n                        f""set auto_install=True"")\n    instance.load_state(state)\n    return instance\n'"
flambe/compile/utils.py,2,"b'from typing import Type, Set, Any, Optional, List, Iterable\n\nfrom urllib.parse import urlparse\n\ntry:\n    from pip._internal.operations import freeze\nexcept ImportError:  # pip < 10.0\n    from pip.operations import freeze\n\n\ndef all_subclasses(class_: Type[Any]) -> Set[Type[Any]]:\n    """"""Return a set of all subclasses for a given class object\n\n    Recursively collects all subclasses of `class_` down the object\n    hierarchy into one set.\n\n    Parameters\n    ----------\n    class_ : Type[Any]\n        Class to retrieve all subclasses for\n\n    Returns\n    -------\n    Set[Type[Any]]\n        All subclasses of class_\n\n    """"""\n    subsubclasses = set([s for c in class_.__subclasses__() for s in all_subclasses(c)])\n    return set(class_.__subclasses__()).union(subsubclasses)\n\n\ndef make_component(class_: type,\n                   tag_namespace: Optional[str] = None,\n                   only_module: Optional[str] = None,\n                   parent_component_class: Optional[Type] = None,\n                   exclude: Optional[List[str]] = None) -> None:\n    """"""Make class and all its children a `Component`\n\n    For example a call to `make_component(torch.optim.Adam, ""torch"")`\n    will make the tag `!torch.Adam` accessible in any yaml configs.\n    This does *NOT* monkey patch (aka swizzle) torch, but instead\n    creates a dynamic subclass which will be used by the yaml\n    constructor i.e. only classes loaded from the config will be\n    affected, anything imported and used in code.\n\n    Parameters\n    ----------\n    class_ : type\n        To be registered with yaml as a component, along with all its\n        children\n    tag_prefix : str\n        Added to beginning of all the tags\n    only_module : str\n        Module prefix used to limit the scope of what gets registered\n    parent_component_class : Type\n        Parent class to use for creating a new component class; should\n        be a subclass of :class:``~flambe.compile.Component`` (defaults\n        to ``Component``)\n    exclude: List[str], optional\n        A list of modules to ignore\n\n    Returns\n    -------\n    None\n\n    """"""\n    from flambe.compile import dynamic_component, Component\n    if parent_component_class is None:\n        parent_component_class = Component\n    elif not issubclass(parent_component_class, Component):\n        raise Exception(""Only a subclass of Component should be used for \'parent_component_class\'"")\n    for subclass in all_subclasses(class_):\n        if exclude and any(ex in subclass.__module__ for ex in exclude):\n            continue\n        if only_module is None or subclass.__module__.startswith(only_module):\n            dynamic_component(subclass, tag=subclass.__name__, tag_namespace=tag_namespace,\n                              parent_component_class=parent_component_class)\n\n\ndef _is_url(resource: str) -> bool:\n    """"""Whether a given resource is a remote URL.\n\n    Resolve by searching for a scheme.\n\n    Parameters\n    ----------\n    resource: str\n        The given resource\n\n    Returns\n    -------\n    bool\n        If the resource is a remote URL.\n\n    """"""\n    scheme = urlparse(resource).scheme\n    return scheme != \'\'\n\n\ndef get_frozen_deps() -> Iterable[str]:\n    """"""Get the frozen dependencies that are locally installed.\n\n    This should yield the same results as runnning \'pip freeze\'.\n\n    Returns\n    -------\n    Iterable[str]\n        The frozen dependencies as strings.\n\n    """"""\n    return freeze.freeze()\n\n\ndef write_deps(filename: str, deps: Optional[Iterable[str]] = None) -> None:\n    """"""Write dependencies on a filename, following Python\'s convention\n    for requiremnets.\n\n    Parameters\n    ----------\n    filename: str\n        The filename where the dependencies will be written.\n    deps: Optional[Iterable[str]]\n        Optional dependencies to write. If not provided,\n        this method will get the dependencies automatically.\n        This parameter should be used for testing purposes only.\n\n    """"""\n    deps = deps or get_frozen_deps()\n    with open(filename, ""w+"") as f:\n        f.writelines(\'\\n\'.join(deps))\n        f.flush()\n'"
flambe/dataset/__init__.py,0,"b""from flambe.dataset.dataset import Dataset\nfrom flambe.dataset.tabular import TabularDataset\n\n\n__all__ = ['Dataset', 'TabularDataset']\n"""
flambe/dataset/dataset.py,0,"b'from abc import abstractmethod\nfrom typing import Sequence\n\nfrom flambe import Component\n\n\nclass Dataset(Component):\n    """"""Base Dataset interface.\n\n    Dataset objects offer the main interface to loading data into the\n    experiment pipepine. Dataset objects have three attributes:\n    `train`, `dev`, and `test`, each pointing to a list of examples.\n\n    Note that Datasets should also be ""immutable"", and as such,\n    `__setitem__` and `__delitem__` will raise an error. Although this\n    does not mean that the object will not be mutated in other ways,\n    it should help avoid issues now and then.\n\n    """"""\n\n    @property\n    @abstractmethod\n    def train(self) -> Sequence[Sequence]:\n        """"""Returns the training data as a sequence of examples.""""""\n        pass\n\n    @property\n    @abstractmethod\n    def val(self) -> Sequence[Sequence]:\n        """"""Returns the validation data as a sequence of examples.""""""\n        pass\n\n    @property\n    @abstractmethod\n    def test(self) -> Sequence[Sequence]:\n        """"""Returns the test data as a sequence of examples.""""""\n        pass\n\n    def __setitem__(self):\n        """"""Raise an error.""""""\n        raise ValueError(""Dataset objects are immutable"")\n\n    def __delitem__(self):\n        """"""Raise an error.""""""\n        raise ValueError(""Dataset objects are immutable"")\n'"
flambe/dataset/tabular.py,0,"b'import os\nfrom typing import Optional, List, Tuple, Iterable, Dict, Union, Any\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nfrom flambe.dataset import Dataset\nfrom flambe.compile import registrable_factory\nfrom flambe.field import Field\n\n\nclass DataView:\n    """"""TabularDataset view for the train, val or test split. This class\n    must be used only internally in the TabularDataset class.\n\n    A DataView is a lazy Iterable that receives the operations\n    from the TabularDataset object. When __getitem__ is called, then\n    all the fields defined in the transform are applied.\n\n    This object can cache examples already transformed.\n    To enable this, make sure to use this view under a Singleton pattern\n    (there must only be one DataView per split in the TabularDataset).\n\n    """"""\n    def __init__(self,\n                 data: np.ndarray,\n                 transform_hooks: List[Tuple[Field, Union[int, List[int]]]],\n                 cache: bool) -> None:\n        """"""\n        Parameters\n        ----------\n        data: np.ndarray\n            A 2d numpy array holding the data\n        transform_hooks: List[Tuple[Field, Union[int, List[int]]]]\n            The transformations that will be applied to each example.\n        cache: bool\n            To apply cache or not.\n\n        """"""\n        self.data = data  # Stores the raw data\n        self.transform_hooks = transform_hooks\n        self.cache = cache\n\n        # Caches the transformed data\n        self.cached_data: Dict[int, Any] = {}\n\n    @property\n    def raw(self):\n        """"""Returns an subscriptable version of the data""""""\n        return self.data\n\n    def __getitem__(self, index):\n        """"""\n        Get an item from an index and apply the transformations\n        dinamically.\n\n        """"""\n        if self.data is None:\n            raise IndexError()\n\n        if self.cache and index in self.cached_data:\n            return self.cached_data[index]\n\n        ex = self.data[index]\n        if len(self.transform_hooks) > 0:\n            ret = []\n            for field, cols in self.transform_hooks:\n                _ex = ex[cols]\n                if isinstance(cols, List):\n                    processed_ex = field.process(*_ex)\n                else:\n                    processed_ex = field.process(_ex)\n\n                if isinstance(processed_ex, tuple):\n                    ret.extend(processed_ex)\n                else:\n                    ret.append(processed_ex)\n            ret = tuple(ret)\n        else:\n            ret = tuple(ex)\n\n        if self.cache:\n            self.cached_data[index] = ret\n\n        return ret\n\n    def is_empty(self) -> bool:\n        """"""\n        Return if the DataView has data\n\n        """"""\n        return len(self) == 0\n\n    def cols(self) -> int:\n        """""" Return the amount of columns the DataView has.""""""\n        if self.is_empty():\n            raise ValueError(""Empty DataView contains no columns"")\n\n        return len(self[0])\n\n    def __len__(self) -> int:\n        """"""\n        Return the length of the dataview, ie the amount\n        of examples it contains.\n\n        """"""\n        if self.data is None:\n            return 0\n        return len(self.data)\n\n    def __setitem__(self):\n        """"""Raise an error as DataViews are immutable.""""""\n        raise ValueError(""Dataset objects are immutable"")\n\n    def __delitem__(self):\n        """"""Raise an error as DataViews are immutable.""""""\n        raise ValueError(""Dataset objects are immutable"")\n\n\nclass TabularDataset(Dataset):\n    """"""Loader for tabular data, usually in `csv` or `tsv` format.\n\n    A TabularDataset can represent any data that can be organized\n    in a table. Internally, we store all information in a 2D numpy\n    generic array. This object also behaves\n    as a sequence over the whole dataset, chaining the training,\n    validation and test data, in that order. This is useful in creating\n    vocabularies or loading embeddings over the full datasets.\n\n    Attributes\n    ----------\n    train: np.ndarray\n        The list of training examples\n    val: np.ndarray\n        The list of validation examples\n    test: np.ndarray\n        The list of text examples\n\n    """"""\n\n    def __init__(self,\n                 train: Iterable[Iterable],\n                 val: Optional[Iterable[Iterable]] = None,\n                 test: Optional[Iterable[Iterable]] = None,\n                 cache: bool = True,\n                 named_columns: Optional[List[str]] = None,\n                 transform: Dict[str, Union[Field, Dict]] = None) -> None:\n        """"""Initialize the TabularDataset.\n\n        Parameters\n        ----------\n        train: Iterable[Iterable]\n            The train data\n        val: Iterable[Iterable], optional\n            The val data, optional\n        test: Iterable[Iterable], optional\n            The test data, optional\n        cache: bool\n            Whether to cache fetched examples. Only use True if the\n            dataset fits in memory. Defaults to False.\n        named_columns: Optional[List[Union[str, int]]]\n            The columns\' names of the dataset, in order.\n        transform: Dict[str, Dict[str, Any]]\n            The fields to be applied to the columns. Each field is\n            identified with a name for easy linking.\n            For example:\n            {\n               \'text\': {\'field\': SomeField(), \'columns\': [0, 1]},\n               \'label\': {\'field\': SomeOtherField(), \'columns\': 2}\n            }\n\n        """"""\n        self._train = np.array(train, dtype=np.object)\n        self._val = None\n        self._test = None\n\n        if val is not None:\n            self._val = np.array(val, dtype=np.object)\n        if test is not None:\n            self._test = np.array(test, dtype=np.object)\n\n        self.cache = cache\n\n        self.named_columns = named_columns\n\n        cols = []\n        # All datasets should be 2-dimensional\n        for k, d in {""val"": self._val, ""test"": self._test, ""train"": self._train}.items():\n            if d is not None:\n                cols.append(d.shape[-1])\n                if len(d.shape) != 2:\n                    # This happens when examples differ in the amount of\n                    # columns and numpy stores them in a 1-D tensor\n                    # (with tuples as values)\n                    raise ValueError(\n                        f""{k} dataset contains examples with different amount of columns""\n                    )\n\n        # Check that all splits contain same columns\n        if np.unique(cols).shape != (1,):\n            raise ValueError(""All splits containing data should have same amount of columns"")\n\n        if named_columns and len(named_columns) != cols[0]:\n            raise ValueError(""Columns parameter should have same size as the dataset\'s amount "" +\n                             "" of columns"")\n\n        # Store the hooks for lazy loading\n        self.transform_hooks: List[Tuple[Field, Union[int, List[int]]]] = []\n        self.transform = transform\n\n        if transform:\n            self._set_transforms(transform)\n\n        self.train_view: Optional[DataView] = None\n        self.val_view: Optional[DataView] = None\n        self.test_view: Optional[DataView] = None\n\n    def _set_transforms(self, transform: Dict[str, Union[Field, Dict]]) -> None:\n        """"""Set transformations attributes and hooks to the data splits.\n\n        This method adds attributes for each field in the transform\n        dict. It also adds hooks for the \'process\' call in each field.\n\n        ATTENTION: This method works with the _train, _val and _test\n        hidden attributes as this runs in the constructor and creates\n        the hooks to be used in creating the properties.\n\n        """"""\n        columns: Union[int, List[int]]\n\n        for k, t in enumerate(transform.items()):\n            name, value = t\n            if isinstance(value, Field):\n                field = value\n                columns = k\n            else:\n                try:\n                    field, tmp_cols = value[\'field\'], value.get(\'columns\', k)\n\n                    # Process as list to avoid repeating code\n                    if not isinstance(tmp_cols, List):\n                        tmp_cols = [tmp_cols]\n\n                    for i, c in enumerate(tmp_cols[:]):\n                        if isinstance(c, str):\n                            if not self.named_columns:\n                                raise ValueError(\n                                    ""Columns parameter is required for str-based indexing""\n                                )\n                            try:\n                                tmp_cols[i] = self.named_columns.index(c)\n                            except ValueError:\n                                raise ValueError(\n                                    f""Dataset has no column name {c}. "" +\n                                    f""Available columns: {self.named_columns}""\n                                )\n\n                    columns = tmp_cols\n\n                    # If it was a value originally then process\n                    # it as a single value\n                    if len(tmp_cols) == 1:\n                        columns = tmp_cols[0]\n\n                except KeyError:\n                    raise ValueError(\n                        f""If a dict is provided in \'transform\', then it must have the \'field\' key.""\n                        f"" transform item = {k, t}""\n                    )\n\n            setattr(self, name, field)\n            args = [self._train[:, columns]]\n            if self._val is not None:\n                args.append(self._val[:, columns])\n            if self._test is not None:\n                args.append(self._test[:, columns])\n            field.setup(*args)\n            self.transform_hooks.append((field, columns))\n\n    @registrable_factory\n    @classmethod\n    def from_path(cls,\n                  train_path: str,\n                  val_path: Optional[str] = None,\n                  test_path: Optional[str] = None,\n                  sep: Optional[str] = \'\\t\',\n                  header: Optional[str] = \'infer\',\n                  columns: Optional[Union[List[str], List[int]]] = None,\n                  encoding: Optional[str] = \'utf-8\',\n                  transform: Dict[str, Union[Field, Dict]] = None) -> \'TabularDataset\':\n        """"""Load a TabularDataset from the given file paths.\n\n        Parameters\n        ----------\n        train_path : str\n            The path to the train data\n        val_path : str, optional\n            The path to the optional validation data\n        test_path : str, optional\n            The path to the optional test data\n        sep: str\n            Separator to pass to the `read_csv` method\n        header: Optional[Union[str, int]]\n            Use 0 for first line, None for no headers, and \'infer\' to\n            detect it automatically, defaults to \'infer\'\n        columns: List[str]\n            List of columns to load, can be used to select a subset\n            of columns, or change their order at loading time\n        encoding: str\n            The encoding format passed to the pandas reader\n        transform: Dict[str, Union[Field, Dict]]\n            The fields to be applied to the columns. Each field is\n            identified with a name for easy linking.\n\n        """"""\n\n        if (\n            columns and\n            any(isinstance(c, int) for c in columns) and\n            any(isinstance(c, str) for c in columns)\n        ):\n            raise ValueError(""Columns parameters need to be all string or all integers."")\n\n        train, cols = cls._load_file(train_path, sep, header, columns, encoding)\n\n        val, test = None, None\n        if val_path is not None:\n            val, _ = cls._load_file(val_path, sep, header, columns, encoding)\n        if test_path is not None:\n            test, _ = cls._load_file(test_path, sep, header, columns, encoding)\n\n        return cls(train=train, val=val, test=test, transform=transform, named_columns=cols)\n\n    @registrable_factory\n    @classmethod\n    def autogen(cls,\n                data_path: str,\n                test_path: Optional[str] = None,\n                seed: Optional[int] = None,\n                test_ratio: Optional[float] = 0.2,\n                val_ratio: Optional[float] = 0.2,\n                sep: Optional[str] = \'\\t\',\n                header: Optional[str] = \'infer\',\n                columns: Optional[Union[List[str], List[int]]] = None,\n                encoding: Optional[str] = \'utf-8\',\n                transform: Dict[str, Union[Field, Dict]] = None) -> \'TabularDataset\':\n        """"""Generate a test and validation set from the given file\n        paths, then load a TabularDataset.\n\n        Parameters\n        ----------\n        data_path: str\n            The path to the data\n        test_path: Optional[str]\n            The path to the test data\n        seed: Optional[int]\n            Random seed to be used in test/val generation\n        test_ratio: Optional[float]\n            The ratio of the test dataset in relation to\n            the whole dataset. If `test_path` is specified, this field\n            has no effect.\n        val_ratio: Optional[float]\n            The ratio of the validation dataset in relation to\n            the training dataset (whole - test)\n        sep: str\n            Separator to pass to the `read_csv` method\n        header: Optional[Union[str, int]]\n            Use 0 for first line, None for no headers, and \'infer\' to\n            detect it automatically, defaults to \'infer\'\n        columns: List[str]\n            List of columns to load, can be used to select a subset\n            of columns, or change their order at loading time\n        encoding: str\n            The encoding format passed to the pandas reader\n        transform: Dict[str, Union[Field, Dict]]\n            The fields to be applied to the columns. Each field is\n            identified with a name for easy linking.\n\n        """"""\n        if (\n            columns and\n            any(isinstance(c, int) for c in columns) and\n            any(isinstance(c, str) for c in columns)\n        ):\n            raise ValueError(""Columns parameters need to be all string or all integers."")\n\n        data, cols = cls._load_file(data_path,\n                                    sep=sep,\n                                    header=header,\n                                    columns=columns,\n                                    encoding=encoding)\n\n        train, val, test = None, None, None\n        if test_path is not None:\n            train, val = train_test_split(data, test_size=val_ratio, random_state=seed)\n            test, _ = cls._load_file(test_path,\n                                     sep=sep,\n                                     header=header,\n                                     columns=columns,\n                                     encoding=encoding)\n        else:\n            train_val, test = train_test_split(data, test_size=test_ratio, random_state=seed)\n            train, val = train_test_split(train_val, test_size=val_ratio, random_state=seed)\n\n        return cls(train=train, val=val, test=test, transform=transform, named_columns=cols)\n\n    @classmethod\n    def _load_file(cls,\n                   path: str,\n                   sep: Optional[str] = \'\\t\',\n                   header: Optional[str] = \'infer\',\n                   columns: Optional[Union[List[str], List[int]]] = None,\n                   encoding: Optional[str] = \'utf-8\') -> Tuple[List[Tuple], Optional[List[str]]]:\n        """"""Load data from the given path.\n\n        The path may be either a single file or a directory. If it is\n        a directory, each file is loaded according to the specified\n        options and all the data is concatenated into a single list.\n        The files will be processed in order based on file name.\n\n        Parameters\n        ----------\n        path : str\n            Path to data, could be a directory, a file, or a\n            smart_open link\n        sep: str\n            Separator to pass to the `read_csv` method\n        header: Optional[Union[str, int]]\n            Use 0 for first line, None for no headers, and \'infer\' to\n            detect it automatically, defaults to \'infer\'\n        columns: Optional[Union[List[str], List[int]]]\n            List of columns to load, can be used to select a subset\n            of columns, or change their order at loading time\n        encoding: str\n            The encoding format passed to the pandas reader\n\n        Returns\n        -------\n        Tuple[List[Tuple], Optional[List[str]]]\n            A tuple containing the list of examples (where each example\n            is itself also a list or tuple of entries in the dataset)\n            and an optional list of named columns (one string for each\n            column in the dataset)\n\n        """"""\n        # Get all paths\n        if isinstance(path, str) and os.path.isdir(path):\n            file_paths = [os.path.join(path, name) for name in os.listdir(path)]\n            file_paths = sorted(file_paths)\n        else:\n            file_paths = [path]\n\n        data: List = []\n        for file_path in file_paths:\n            # Don\'t fail on buggy files\n            try:\n                examples = pd.read_csv(file_path,\n                                       sep=sep,\n                                       header=header,\n                                       index_col=False,\n                                       dtype=str,\n                                       encoding=encoding,\n                                       keep_default_na=False)\n                # Select columns\n                if columns is not None:\n                    examples = examples[columns]\n                data.extend(examples.values.tolist())\n            except Exception as e:\n                print(""Warning: failed to load file {file_path}"")\n                print(e)\n\n        if len(data) == 0:\n            raise ValueError(f""No data found at {path}"")\n\n        # Take the named columns from the columns parameter\n        # if they are strings or try to use the pd.DataFrame\n        # column names if they are strings.\n        named_cols: List[str] = []\n        if columns:\n            for i, c in enumerate(columns):  # type: ignore\n                if isinstance(c, str):\n                    named_cols.append(c)\n        elif all(isinstance(c, str) for c in examples.columns):\n            named_cols = examples.columns.tolist()\n\n        return data, named_cols if len(named_cols) > 0 else None\n\n    @property\n    def train(self) -> np.ndarray:\n        """"""Returns the training data as a numpy nd array""""""\n        if self.train_view is None:\n            self.train_view = DataView(self._train, self.transform_hooks, self.cache)\n\n        return self.train_view\n\n    @property\n    def val(self) -> np.ndarray:\n        """"""Returns the validation data as a numpy nd array""""""\n        if self.val_view is None:\n            self.val_view = DataView(self._val, self.transform_hooks, self.cache)\n\n        return self.val_view\n\n    @property\n    def test(self) -> np.ndarray:\n        """"""Returns the test data as a numpy nd array""""""\n        if self.test_view is None:\n            self.test_view = DataView(self._test, self.transform_hooks, self.cache)\n\n        return self.test_view\n\n    @property\n    def raw(self) -> np.ndarray:\n        """"""Returns all partitions of the data as a numpy nd array""""""\n        args = [self._train]\n        if not self.val.is_empty():\n            args.append(self.val.raw)\n        if not self.test.is_empty():\n            args.append(self.test.raw)\n        return np.concatenate(args, axis=0)\n\n    @property\n    def cols(self) -> int:\n        """"""Returns the amount of columns in the tabular dataset""""""\n        return self.train.cols()\n\n    def __len__(self):\n        """"""Get the length of the dataset.""""""\n        return len(self.train) + len(self.val) + len(self.test)\n\n    def __iter__(self):\n        """"""Iterate through the dataset.""""""\n        for i in range(len(self)):\n            yield self[i]\n\n    def __getitem__(self, index):\n        """"""Get the item at the given index.""""""\n        ceiling = len(self.train)\n        if index < ceiling:\n            return self.train[index]\n\n        offset = ceiling\n        ceiling += len(self.val)\n        if index < ceiling:\n            return self.val[index - offset]\n\n        offset = ceiling\n        ceiling += len(self.test)\n        if index < ceiling:\n            return self.test[index - offset]\n'"
flambe/experiment/__init__.py,0,"b""from flambe.experiment.experiment import Experiment\nfrom flambe.experiment.progress import ProgressState\nfrom flambe.experiment.tune_adapter import TuneAdapter\nfrom flambe.experiment.options import GridSearchOptions, SampledUniformSearchOptions\n\n\n__all__ = ['Experiment', 'TuneAdapter', 'GridSearchOptions',\n           'SampledUniformSearchOptions', 'ProgressState']\n"""
flambe/experiment/experiment.py,0,"b'# from __future__ import annotations\nimport os\nimport re\nimport logging\nfrom copy import deepcopy\nfrom typing import Dict, Optional, Union, Sequence, cast, Callable\nfrom collections import OrderedDict\nimport shutil\nimport tempfile\n\nfrom tqdm import tqdm\nfrom io import StringIO\nimport ray\nfrom ray.tune.suggest import SearchAlgorithm\nfrom ray.tune.schedulers import TrialScheduler\nfrom ray.tune.logger import DEFAULT_LOGGERS, TFLogger, tf2_compat_logger\n\nfrom flambe.compile import Schema, Component, yaml\nfrom flambe.runnable import ClusterRunnable\nfrom flambe.compile.downloader import download_manager\nfrom flambe.cluster import errors as man_errors\nfrom flambe.cluster import const\nfrom flambe.cluster import Cluster\nfrom flambe.experiment import utils, wording\nfrom flambe.experiment.options import ClusterResource\nfrom flambe.runnable import RemoteEnvironment\nfrom flambe.runnable import error\nfrom flambe.runnable import utils as run_utils\nfrom flambe.runner.utils import get_files\nfrom flambe.experiment.progress import ProgressState\nfrom flambe.experiment.tune_adapter import TuneAdapter\nfrom flambe.logging import coloredlogs as cl\nfrom flambe.experiment.utils import get_default_devices\n\nlogger = logging.getLogger(__name__)\n\nOptionalSearchAlgorithms = Optional[Dict[str, Union[SearchAlgorithm, Schema]]]\nOptionalTrialSchedulers = Optional[Dict[str, Union[TrialScheduler, Schema]]]\n\n\nclass Experiment(ClusterRunnable):\n    """"""A Experiment object.\n\n    The Experiment object is the top level module in the Flamb\xc3\xa9\n    workflow. The object is responsible for starting workers,\n    assiging the orchestrator machine, as well as converting the\n    input blocks into Ray Tune Experiment objects.\n\n    Parameters\n    ----------\n    name: str\n        A name for the experiment\n    pipeline: OrderedDict[str, Schema[Component]]\n        Ordered mapping from block id to a schema of the block\n    force: bool\n        When running a local experiment this flag will make flambe\n        override existing results from previous experiments. When\n        running remote experiments this flag will reuse an existing\n        cluster (in case of any) that is running an experiment\n        with the same name in the same cloud service.\n        The use of this flag is discouraged as you may lose useful data.\n    resume: Union[str, List[str]]\n        If a string is given, resume all blocks up until the given\n        block_id. If a list is given, resume all blocks in that list.\n    save_path: Optional[str]\n        A directory where to save the experiment.\n    devices: Dict[str, int]\n        Tune\'s resources per trial. For example: {""cpu"": 12, ""gpu"": 2}.\n    resources: Optional[Dict[str, Dict[str, Any]]]\n        Variables to use in the pipeline section with !@ notation.\n        This section is splitted into 2 sections: local and remote.\n    search : Mapping[str, SearchAlgorithm], optional\n        Map from block id to hyperparameter search space generator. May\n        have Schemas of SearchAlgorithm as well.\n    schedulers : Mapping[str, TrialScheduler], optional\n        Map from block id to search scheduler. May have Schemas of\n        TrialScheduler as well.\n    reduce: Mapping[str, int], optional\n        Map from block to number of trials to reduce to.\n    env: RemoteEnvironment\n        Contains remote information about the cluster. This object will\n        be received in case this Experiment is running remotely.\n    max_failures: int\n        Number of times to retry running the pipeline if it hits some\n        type of failure, defaults to one.\n    merge_plot: bool\n        Display all tensorboard logs in the same plot (per block type).\n        Defaults to True.\n    user_provider: Callable[[], str]\n        The logic for specifying the user triggering this\n        Runnable. If not passed, by default it will pick the computer\'s\n        user.\n\n    """"""\n\n    def __init__(self,\n                 name: str,\n                 pipeline: Dict[str, Schema],\n                 resume: Optional[Union[str, Sequence[str]]] = None,\n                 devices: Dict[str, int] = None,\n                 save_path: Optional[str] = None,\n                 resources: Optional[Dict[str, Union[str, ClusterResource]]] = None,\n                 search: OptionalSearchAlgorithms = None,\n                 schedulers: OptionalTrialSchedulers = None,\n                 reduce: Optional[Dict[str, int]] = None,\n                 env: RemoteEnvironment = None,\n                 max_failures: int = 1,\n                 stop_on_failure: bool = True,\n                 merge_plot: bool = True,\n                 user_provider: Callable[[], str] = None) -> None:\n        super().__init__(env=env, user_provider=user_provider)\n        self.name = name\n\n        self.original_save_path = save_path\n\n        if save_path is None or len(save_path) == 0:\n            save_path = os.path.join(os.getcwd(), ""flambe-output"")\n        else:\n            save_path = os.path.abspath(os.path.expanduser(save_path))\n\n        # Prepending \'output\' to the name in the output folder\n        # is a basic security mechanism to avoid removing user\n        # folders when using --force (if for example, save_path\n        # is \'/$HOME/Desktop\' and name is ""nlp"", and user has folder\n        # \'$HOME/Desktop/nlp\', then there is a risk of accidentally\n        # removing it when using --force)\n        self.output_folder_name = f""output__{name}""\n\n        self.full_save_path = os.path.join(\n            save_path,\n            self.output_folder_name\n        )\n\n        self.resume = resume\n        self.devices = devices\n        self.resources = resources or dict()\n        self.pipeline = pipeline\n        # Compile search algorithms if needed\n        self.search = search or dict()\n        for stage_name, search_alg in self.search.items():\n            if isinstance(search_alg, Schema):\n                self.search[stage_name] = search_alg()\n        # Compile schedulers if needed\n        self.schedulers = schedulers or dict()\n        for stage_name, scheduler in self.schedulers.items():\n            if isinstance(scheduler, Schema):\n                self.schedulers[stage_name] = scheduler()\n        self.reduce = reduce or dict()\n        self.max_failures = max_failures\n        self.stop_on_failure = stop_on_failure\n        self.merge_plot = merge_plot\n        if pipeline is None or not isinstance(pipeline, (Dict, OrderedDict)):\n            raise TypeError(""Pipeline argument is not of type Dict[str, Schema]. ""\n                            f""Got {type(pipeline).__name__} instead"")\n        self.pipeline = pipeline\n\n    def process_resources(\n        self,\n        resources: Dict[str, Union[str, ClusterResource]],\n        folder: str\n    ) -> Dict[str, Union[str, ClusterResource]]:\n        """"""Download resources that are not tagged with \'!cluster\'\n        into a given directory.\n\n        Parameters\n        ----------\n        resources: Dict[str, Union[str, ClusterResource]]\n            The resources dict\n        folder: str\n            The directory where the remote resources\n            will be downloaded.\n\n        Returns\n        -------\n        Dict[str, Union[str, ClusterResource]]\n            The resources dict where the remote urls that\n            don\'t contain \'!cluster\' point now to the local\n            path where the resource was downloaded.\n\n        """"""\n        # Keep the resources temporary dict for later cleanup\n        ret = {}\n        for k, v in resources.items():\n            if not isinstance(v, ClusterResource):\n                with download_manager(v, os.path.join(folder, k)) as path:\n                    ret[k] = path\n            else:\n                ret[k] = v\n\n        return ret\n\n    def run(self, force: bool = False, verbose: bool = False, debug: bool = False, **kwargs):\n        """"""Run an Experiment""""""\n\n        logger.info(cl.BL(""Launching local experiment""))\n\n        # Check if save_path/name already exists + is not empty\n        # + force and resume are False\n        if not self.resume and not force and os.path.exists(self.full_save_path) \\\n                and list(get_files(self.full_save_path)):\n            raise error.ParsingRunnableError(\n                f""Results from an experiment with the same name were located in the save path "" +\n                f""{self.full_save_path}. To overide this results, please use \'--force\' "" +\n                ""To use these results and resume the experiment, pick \'resume: True\' "" +\n                ""If not, just pick another save_path/name.""\n            )\n\n        full_save_path = self.full_save_path\n\n        if not self.env:\n            wording.print_useful_local_info(full_save_path)\n\n        # If running remotely then all folders were already created.\n        # in the \'setup\' method.\n        if not self.env:\n            if os.path.exists(full_save_path) and force:\n                shutil.rmtree(full_save_path)  # This deleted the folder also\n                logger.info(\n                    cl.RE(f""Removed previous existing from {full_save_path} "" +\n                          ""results as --force was specified""))\n\n            if not os.path.exists(full_save_path):\n                os.makedirs(full_save_path)\n                logger.debug(f""{full_save_path} created to store output"")\n\n        self._dump_experiment_file()\n\n        if any(map(lambda x: isinstance(x, ClusterResource), self.resources.values())):\n            raise ValueError(\n                f""Local experiments doesn\'t support resources with \'!cluster\' tags. "" +\n                ""The \'!cluster\' tag is used for those resources that need to be handled "" +\n                ""in the cluster when running remote experiments."")\n\n        if not self.env:\n            self.tmp_resources_dir = tempfile.TemporaryDirectory()\n            resources_folder = self.tmp_resources_dir.name\n        else:\n            resources_folder = f""{self.full_save_path}/_resources""\n\n        resources = self.process_resources(self.resources, resources_folder)\n\n        # rsync downloaded resources\n        if self.env:\n            run_utils.rsync_hosts(self.env.orchestrator_ip,\n                                  self.env.factories_ips,\n                                  self.env.user,\n                                  self.full_save_path,\n                                  self.env.key,\n                                  exclude=[""state.pkl""])\n\n        # Check that links are in order (i.e topologically in pipeline)\n        utils.check_links(self.pipeline, resources)\n\n        # Check that only computable blocks are given\n        # search algorithms and schedulers\n        utils.check_search(self.pipeline, self.search, self.schedulers)\n\n        # Initialize ray cluster\n        kwargs = {""logging_level"": logging.ERROR, ""include_webui"": False}\n        if debug:\n            kwargs[\'local_mode\'] = True\n\n        if self.env:\n            ray.init(redis_address=f""{self.env.orchestrator_ip}:{const.RAY_REDIS_PORT}"", **kwargs)\n        else:\n            ray.init(**kwargs)\n            logger.debug(f""Ray cluster up"")\n\n        # Initialize map from block to list of checkpoints\n        # This is used whe resolving links over other computable blocks\n        # TODO: in python 3.7 we can replace these with dict() or {}\n        checkpoints: OrderedDict = OrderedDict()\n        schemas: OrderedDict = OrderedDict()\n        success: OrderedDict = OrderedDict()\n\n        # By default use all CPUs if no GPU is present\n        devices = self.devices if self.devices else None\n        if devices is None:\n            devices = get_default_devices(debug=debug)\n\n        to_resume = None\n        if isinstance(self.resume, str):\n            index = list(self.pipeline.keys()).index(self.resume)\n            to_resume = list(self.pipeline.keys())[:index + 1]\n        elif isinstance(self.resume, Sequence):\n            to_resume = list(self.resume)\n\n        # Make experiment_tag easier to extract\n        def trial_name_creator(trial):\n            identifier = """"\n            if ""env"" in trial.config:\n                env = trial.config[""env""]\n                if isinstance(env, type):\n                    env = env.__name__\n                identifier += f""{env}""\n            if trial.experiment_tag:\n                hyper_params = {}\n                if ""_"" in trial.experiment_tag:\n                    num, tunable_params = trial.experiment_tag.split(""_"", 1)\n                    identifier += tunable_params\n                    param_list = [p.split(""="") for p in tunable_params.split("","")]\n                    hyper_params = {p[0]: p[1] for p in param_list}\n                else:\n                    identifier += trial.experiment_tag\n                trial.config[\'hyper_params\'] = hyper_params\n            return identifier.replace(""/"", ""_"")\n\n        trial_name_creator = ray.tune.function(trial_name_creator)\n\n        # Compute depedencies DAG\n        dependency_dag = {}\n        schemas_dag: OrderedDict = OrderedDict()\n        for block_id, schema_block in self.pipeline.items():\n            schemas_dag[block_id] = schema_block\n            relevant_ids = utils.extract_needed_blocks(schemas_dag, block_id, resources)\n            dependencies = deepcopy(relevant_ids)\n            dependencies.discard(block_id)\n\n            dependency_dag[block_id] = list(dependencies)\n\n        if self.env:\n            self.progress_state = ProgressState(\n                self.name, full_save_path, dependency_dag,\n                self.content, len(self.env.factories_ips))\n        else:\n            self.progress_state = ProgressState(self.name, full_save_path,\n                                                dependency_dag, self.content)\n\n        for block_id, schema_block in tqdm(self.pipeline.items()):\n            schema_block.add_extensions_metadata(self.extensions)\n            logger.debug(f""Starting {block_id}"")\n\n            # Add the block to the configuration so far\n            schemas[block_id] = schema_block\n            success[block_id] = True\n\n            self.progress_state.checkpoint_start(block_id)\n            relevant_ids = utils.extract_needed_blocks(schemas, block_id, resources)\n            relevant_schemas = {k: v for k, v in deepcopy(schemas).items() if k in relevant_ids}\n\n            # Set resume\n            resume = False if to_resume is None else (block_id in to_resume)\n\n            # If computable, convert to tune.Trainable\n            # Each Component block is an Experiment in ray.tune\n            if not isinstance(schema_block, Schema):\n                raise ValueError(\'schema block not of correct type Schema\')\n            if issubclass(schema_block.component_subclass, Component):\n\n                # Returns is a list non-nested configuration\n                divided_schemas = list(utils.divide_nested_grid_search_options(relevant_schemas))\n                divided_dict = [utils.extract_dict(x) for x in divided_schemas]\n                # Convert options and links\n                divided_dict_tune = [utils.convert_tune(x) for x in divided_dict]\n                # Execute block\n                tune_experiments = []\n                for param_dict, schemas_dict in zip(divided_dict_tune, divided_schemas):\n                    config = {\'name\': block_id,\n                              \'merge_plot\': self.merge_plot,\n                              \'params\': param_dict,\n                              \'schemas\': Schema.serialize(schemas_dict),\n                              \'checkpoints\': checkpoints,\n                              \'to_run\': block_id,\n                              \'global_vars\': resources,\n                              \'verbose\': verbose,\n                              \'custom_modules\': list(self.extensions.keys()),\n                              \'debug\': debug}\n                    # Filter out the tensorboard logger as we handle\n                    # general and tensorboard-specific logging ourselves\n                    tune_loggers = list(filter(lambda l: l != tf2_compat_logger and  # noqa: E741\n                                               not issubclass(l, TFLogger), DEFAULT_LOGGERS))\n                    tune_experiment = ray.tune.Experiment(name=block_id,\n                                                          run=TuneAdapter,\n                                                          trial_name_creator=trial_name_creator,\n                                                          config=deepcopy(config),\n                                                          local_dir=full_save_path,\n                                                          checkpoint_freq=1,\n                                                          checkpoint_at_end=True,\n                                                          max_failures=self.max_failures,\n                                                          resources_per_trial=devices,\n                                                          loggers=tune_loggers)\n                    logger.debug(f""Created tune.Experiment for {param_dict}"")\n                    tune_experiments.append(tune_experiment)\n\n                trials = ray.tune.run_experiments(tune_experiments,\n                                                  search_alg=self.search.get(block_id, None),\n                                                  scheduler=self.schedulers.get(block_id, None),\n                                                  queue_trials=True,\n                                                  verbose=False,\n                                                  resume=resume,\n                                                  raise_on_failed_trial=False)\n                logger.debug(f""Finish running all tune.Experiments for {block_id}"")\n\n                any_error = False\n                for t in trials:\n                    if t.status == t.ERROR:\n                        logger.error(cl.RE(f""Variant {t} of \'{block_id}\' ended with ERROR status.""))\n                        success[block_id] = False\n                        any_error = True\n                if any_error and self.stop_on_failure:\n                    self.teardown()\n                    self.progress_state.checkpoint_end(block_id, success[block_id])\n                    raise error.UnsuccessfulRunnableError(\n                        f""Stopping experiment at block \'{block_id}\' ""\n                        ""because there was an error and stop_on_failure == True.""\n                    )\n\n                # Save checkpoint location\n                # It should point from:\n                # block_id -> hash(variant) -> checkpoint\n                hashes = []\n                for t in trials:\n                    schema_with_params: Dict = OrderedDict()\n                    for b in schemas_dict:\n                        schema_copy = deepcopy(schemas_dict[b])\n                        utils.update_schema_with_params(schema_copy, t.config[\'params\'][b])\n                        schema_with_params[b] = schema_copy\n                    hashes.append(repr(schema_with_params))\n\n                paths = [t._checkpoint.value for t in trials]\n\n                # Mask out error trials\n                mask = [True] * len(trials)\n                for i, trial in enumerate(trials):\n                    if trial.status == ray.tune.trial.Trial.ERROR:\n                        mask[i] = False\n\n                # Mask out on reduce\n                reduce_k = self.reduce.get(block_id, None)\n                if reduce_k is not None and int(reduce_k) > 0:\n                    # Get best\n                    best_trials = utils.get_best_trials(trials, topk=int(reduce_k))\n                    best_trial_ids = set([t.trial_id for t in best_trials])\n                    # Mask out\n                    for i, trial in enumerate(trials):\n                        if trial.trial_id not in best_trial_ids:\n                            mask[i] = False\n\n                trial_checkpoints = {t_hash: path for t_hash, path in zip(hashes, paths)}\n                trial_mask = {t_hash: mask_value for t_hash, mask_value in zip(hashes, mask)}\n                checkpoints[block_id] = {\'paths\': trial_checkpoints, \'mask\': trial_mask}\n\n                # Rsync workers to main machine and back to all workers\n                # TODO specify callbacks. If not remote will not work\n                if self.env:\n                    run_utils.rsync_hosts(self.env.orchestrator_ip,\n                                          self.env.factories_ips,\n                                          self.env.user,\n                                          self.full_save_path,\n                                          self.env.key,\n                                          exclude=[""state.pkl""])\n\n            self.progress_state.checkpoint_end(block_id, success[block_id])\n            logger.debug(f""Done running {block_id}"")\n\n        self.teardown()\n\n        if all(success.values()):\n            logger.info(cl.GR(""Experiment ended successfully""))\n        else:\n            raise error.UnsuccessfulRunnableError(\n                ""Not all trials were successful. Check the logs for more information""\n            )\n\n    def teardown(self):\n        # Disconnect process from ray cluster\n        ray.shutdown()\n\n        # Shutdown ray cluster.\n        if self.env:\n            ret = utils.shutdown_ray_node()\n            logger.debug(f""Node shutdown {\'successful\' if ret == 0 else \'failed\'} in Orchestrator"")\n            for f in self.env.factories_ips:\n                ret = utils.shutdown_remote_ray_node(f, ""ubuntu"", self.env.key)\n                logger.debug(f""Node shutdown {\'successful\' if ret == 0 else \'failed\'} in {f}"")\n\n        self.progress_state.finish()\n\n        if hasattr(self, \'tmp_resources_dir\'):\n            self.tmp_resources_dir.cleanup()\n\n    def setup(self, cluster: Cluster, extensions: Dict[str, str], force: bool, **kwargs) -> None:\n        """"""Prepare the cluster for the Experiment remote execution.\n\n        This involves:\n\n        1) [Optional] Kill previous flambe execution\n        2) [Optional] Remove existing results\n        3) Create supporting dirs (exp/synced_results, exp/resources)\n        4) Install extensions in all factories\n        5) Launch ray cluster\n        6) Send resources\n        7) Launch Tensorboard + Report site\n\n        Parameters\n        ----------\n        cluster: Cluster\n            The cluster where this Runnable will be running\n        extensions: Dict[str, str]\n            The ClusterRunnable extensions\n        force: bool\n            The force value provided to Flambe\n\n        """"""\n\n        if cluster.existing_flambe_execution() or cluster.existing_ray_cluster():\n            if not force:\n                raise man_errors.ClusterError(""This cluster is currently used by other "" +\n                                              ""experiment. Use --force flag to reuse it. Aborting."")\n            else:\n                cluster.shutdown_flambe_execution()\n                cluster.shutdown_ray_cluster()\n                logger.info(cl.YE(""Forced resource to become available...""))\n\n        output_dir_remote = f""{self.name}/{self.output_folder_name}""\n        if cluster.existing_dir(output_dir_remote):\n            logger.debug(""This cluster already ran an experiment "" +\n                         ""with the same name."")\n\n            if self.resume:\n                logger.info(cl.YE(""Resuming previous experiment...""))\n            elif force:\n                cluster.remove_dir(output_dir_remote, content_only=True, all_hosts=True)\n            else:\n                raise man_errors.ClusterError(\n                    ""This cluster already has results for the same experiment name. "" +\n                    ""If you wish to reuse them, use resume: True or if you want to override them "" +\n                    ""use --force. Aborting.""\n                )\n\n        cluster.install_extensions_in_factories(extensions)\n        logger.info(cl.YE(""Extensions installed in all factories""))\n\n        # Add redundant check for typing\n        if not cluster.orchestrator:\n            raise man_errors.ClusterError(""The orchestrator needs to exist at this point"")\n\n        cluster.create_dirs([self.name,\n                             f""{self.name}/{self.output_folder_name}"",\n                             f""{self.name}/{self.output_folder_name}/_resources""])\n        logger.info(cl.YE(""Created supporting directories""))\n\n        cluster.launch_ray_cluster()\n\n        if not cluster.check_ray_cluster():\n            raise man_errors.ClusterError(""Ray cluster not launched correctly."")\n\n        local_resources = {k: v for k, v in self.resources.items()\n                           if not isinstance(v, ClusterResource)}\n\n        tmp_resources_dir = tempfile.TemporaryDirectory()\n\n        # This will download remote resources.\n        local_resources = self.process_resources(\n            local_resources, tmp_resources_dir.name)  # type: ignore\n\n        local_resources = cast(Dict[str, str], local_resources)\n\n        if local_resources:\n            new_resources = cluster.send_local_content(\n                local_resources,\n                os.path.join(cluster.orchestrator.get_home_path(), self.name,\n                             self.output_folder_name, ""_resources""),\n                all_hosts=True\n            )\n        else:\n            new_resources = dict()\n\n        tmp_resources_dir.cleanup()\n\n        # Add the cluster resources without the tag\n        new_resources.update({k: v.location for k, v in self.resources.items()\n                              if isinstance(v, ClusterResource)})\n\n        if cluster.orchestrator.is_tensorboard_running():\n            if force:\n                cluster.orchestrator.remove_tensorboard()\n            else:\n                raise man_errors.ClusterError(""Tensorboard was running on the orchestrator."")\n\n        cluster.orchestrator.launch_tensorboard(output_dir_remote, const.TENSORBOARD_PORT)\n\n        if cluster.orchestrator.is_report_site_running():\n            if force:\n                cluster.orchestrator.remove_report_site()\n            else:\n                raise man_errors.ClusterError(""Report site was running on the orchestrator"")\n\n        cluster.orchestrator.launch_report_site(\n            f""{output_dir_remote}/state.pkl"",\n            port=const.REPORT_SITE_PORT,\n            output_log=f""output.log"",\n            output_dir=output_dir_remote,\n            tensorboard_port=const.TENSORBOARD_PORT\n        )\n\n        self.set_serializable_attr(""resources"", new_resources)\n        self.set_serializable_attr(\n            ""save_path"", f""{cluster.orchestrator.get_home_path()}/{self.name}"")\n\n    def parse(self) -> None:\n        """"""Parse the experiment.\n\n        Parse the Experiment in search of errors that won\'t allow the\n        experiment to run successfully. If it finds any error, then it\n        raises an ParsingExperimentError.\n\n        Raises\n        ------\n        ParsingExperimentError\n            In case a parsing error is found.\n\n        """"""\n        # Check if name is None:\n        if self.name is None or len(self.name) == 0:\n            raise error.ParsingRunnableError(\n                ""Experiment should declare a name and it must not be empty""\n            )\n\n        # Check if name is valid\n        else:\n            if re.match(\'^([a-zA-Z0-9]+[_-]*)+$\', self.name) is None:\n                raise error.ParsingRunnableError(\n                    ""Experiment name should contain only alphanumeric characters "" +\n                    ""(with optional - or _ in between)""\n                )\n\n    def get_user(self) -> str:\n        """"""Get the user that triggered this experiment.\n\n        Returns\n        -------\n        str:\n            The user as a string.\n\n        """"""\n        return self.env.local_user if self.env else self.user_provider()\n\n    def _dump_experiment_file(self) -> None:\n        """"""Dump the experiment YAML representation\n        to the output folder.\n\n        """"""\n        destination = os.path.join(self.full_save_path, ""experiment.yaml"")\n        with open(destination, \'w\') as f:\n            with StringIO() as s:\n                yaml.dump_all([self.extensions, self], s)\n                f.write(s.getvalue())\n            f.flush()\n        logger.debug(f""Saved experiment file in {destination}"")\n'"
flambe/experiment/options.py,0,"b'# from __future__ import annotations\nfrom abc import ABC, abstractmethod\nfrom typing import Sequence, Any, Union, Dict\n\nimport numpy as np\nfrom ray.tune import grid_search\n\nfrom flambe.compile import Registrable, alias\n\n\nNumber = Union[float, int]\n\n\nclass Options(Registrable, ABC):\n\n    @classmethod\n    @abstractmethod\n    def from_sequence(cls, options: Sequence[Any]) -> \'Options\':\n        """"""Construct an options class from a sequence of values\n\n        Parameters\n        ----------\n        options : Sequence[Any]\n            Discrete sequence that defines what values to search over\n\n        Returns\n        -------\n        T\n            Returns a subclass of DiscreteOptions\n\n        """"""\n        pass\n\n    @abstractmethod\n    def convert(self) -> Dict:\n        """"""Convert the options to Ray Tune representation.\n\n        Returns\n        -------\n        Dict\n            The Ray Tune conversion\n\n        """"""\n        pass\n\n    @classmethod\n    def to_yaml(cls, representer: Any, node: Any, tag: str) -> Any:\n        return representer.represent_sequence(tag, node.elements)\n\n    @classmethod\n    def from_yaml(cls, constructor: Any, node: Any, factory_name: str) -> \'Options\':\n        args, = list(constructor.construct_yaml_seq(node))\n        if factory_name is None or factory_name == \'from_sequence\':\n            return cls.from_sequence(args)  # type: ignore\n        else:\n            factory = getattr(cls, factory_name)\n            return factory(args)\n\n\n@alias(\'g\')\nclass GridSearchOptions(Sequence[Any], Options):\n    """"""Discrete set of values used for grid search\n\n    Defines a finite, discrete set of values to be substituted\n    at the location where the set currently resides in the config\n\n    """"""\n\n    def __init__(self, elements: Sequence[Any]) -> None:\n        self.elements = elements\n\n    @classmethod\n    def from_sequence(cls, options: Sequence[Any]) -> \'GridSearchOptions\':\n        return cls(options)\n\n    def convert(self) -> Dict:\n        return grid_search(list(self.elements))\n\n    def __getitem__(self, key: Any) -> Any:\n        return self.elements[key]\n\n    def __len__(self) -> int:\n        return len(self.elements)\n\n    def __repr__(self) -> str:\n        return \'gridoptions(\' + repr(self.elements) + \')\'\n\n\n@alias(\'s\')\nclass SampledUniformSearchOptions(Sequence[Number], Options):\n    """"""Yields k values from the range (low, high)\n\n    Randomly yields k values from the range (low, high) to be\n    substituted at the location where the class currently resides in\n    the config\n\n    """"""\n\n    def __init__(self, low: Number, high: Number, k: int, decimals: int = 10) -> None:\n        self.elements: Sequence[Number]\n        k = int(k)\n        if k < 1:\n            raise ValueError(\'k (number of samples) must be >= 1\')\n        if isinstance(low, int) and isinstance(high, int):\n            self.elements = list(map(int, np.random.randint(low, high, k)))\n        else:\n            self.elements = list(map(float, np.round(np.random.uniform(low, high, k), decimals)))\n        self._low = low\n        self._high = high\n        self._k = k\n        self._decimals = decimals\n\n    @classmethod\n    def from_sequence(cls, options: Sequence[Any]) -> \'SampledUniformSearchOptions\':\n        if len(options) < 2 or len(options) > 4:\n            raise ValueError(f\'Incorrect number of args for {cls.__name__} - received: {options}\')\n        return cls(*options)\n\n    def convert(self) -> Dict:\n        return grid_search(list(self.elements))\n\n    def __getitem__(self, key: Any) -> Any:\n        return self.elements[key]\n\n    def __len__(self) -> int:\n        return len(self.elements)\n\n    def __repr__(self) -> str:\n        return \'randomoptions(\' + repr(self.elements) + \')\'\n\n    @classmethod\n    def to_yaml(cls, representer: Any, node: Any, tag: str) -> Any:\n        return representer.represent_sequence(\'!g\', node.elements)\n\n\n@alias(\'cluster\')\nclass ClusterResource(Registrable):\n\n    def __init__(self, location: str) -> None:\n        self.location = location\n\n    @classmethod\n    def to_yaml(cls, representer: Any, node: Any, tag: str) -> Any:\n        # NOTE: here we are using scalar even considering this\n        # is a string. Other representers are not able to representer\n        # correctly this information\n        return representer.represent_scalar(tag, node.location)\n\n    @classmethod\n    def from_yaml(cls, constructor: Any, node: Any, factory_name: str) -> \'ClusterResource\':\n        value = constructor.construct_yaml_str(node)\n        return cls(location=value)\n'"
flambe/experiment/progress.py,0,"b'import pickle\nimport os\nimport time\nimport json\n\n\nclass ProgressState:\n\n    def __init__(self, name, save_path, dependency_dag, config, factories_num=0):\n        self.fname = ""state.pkl""\n\n        self.config = config\n\n        self.name = name\n        self.save_path = save_path\n        self.total = len(dependency_dag)\n\n        self.done = 0\n\n        # 0 if running local,\n        # Number of factories in case running remotely\n        self.factories_num = factories_num\n\n        # 0 -> PENDING\n        # 1 -> RUNNING\n        # 2 -> SUCCESS\n        # 3 -> FAILURE\n        # 4 -> MASKED\n        self.block_state = {}\n        for b in dependency_dag.keys():\n            self.block_state[b] = 0\n\n        self.time_lapses = {}\n\n        self.variants = {}\n\n        self.finished = False\n\n        self.prev_time = None\n\n        self.dependency_dag = dependency_dag\n\n        self._save()\n\n    def checkpoint_start(self, block_id):\n        self.prev_time = time.time()\n        self.block_state[block_id] = 1\n\n        self._save()\n\n    def refresh(self):\n        self.variants = {}\n        for b in self.dependency_dag.keys():\n            self.variants[b] = []\n            block_path = os.path.join(self.save_path, b)\n            if os.path.exists(block_path):\n                variants = filter(\n                    lambda x: os.path.isdir(os.path.join(block_path, x)), os.listdir(block_path)\n                )\n                for v in variants:\n                    # Check if hparams were specified.\n                    # hparams follows this syntax:\n                    # ""hparam1=value1,hparam2=value2""\n                    if \'=\' in v:\n                        v = v[:v.find(""-"") - 5]  # Substract year of date. TODO replace for regex\n                        hparams = {}\n                        for h in v.split(\',\'):\n                            k, v = h.split(\'=\')\n                            hparams[k] = v\n                        self.variants[b].append(hparams)\n\n    def checkpoint_end(self, block_id, block_success):\n        curr_time = time.time()\n\n        if self.prev_time:\n            self.time_lapses[block_id] = curr_time - self.prev_time\n            self.prev_time = None\n\n        self.done += 1\n        self.block_state[block_id] = 2 if block_success else 3\n\n        self._save()\n\n    def finish(self):\n        self.finished = True\n        self._save()\n\n    def _save(self):\n        with open(os.path.join(self.save_path, self.fname), \'wb\') as f:\n            pickle.dump(self, f)\n\n    def toJSON(self):\n        self.refresh()\n        return json.dumps(self, default=lambda o: o.__dict__,\n                          sort_keys=True, indent=4)\n'"
flambe/experiment/tune_adapter.py,0,"b'# from __future__ import annotations\nimport os\nimport pickle\nfrom six import string_types\nfrom typing import Dict, Optional\nfrom copy import deepcopy\nfrom collections import OrderedDict\nimport shutil\n\nimport ray\n\nfrom flambe.compile import load_state_from_file, Schema, Component\nfrom flambe.compile.extensions import setup_default_modules, import_modules\nfrom flambe.experiment import utils\nfrom flambe.logging import TrialLogging\n\n\nclass TuneAdapter(ray.tune.Trainable):\n    """"""Adapter to the tune.Trainable inteface.""""""\n\n    def _setup(self, config: Dict):\n        """"""Subclasses should override this for custom initialization.""""""\n        # Set this flag to False, if we find an error, or reduce\n        self.name = config[\'name\']\n        self.run_flag = True\n        custom_modules = config[\'custom_modules\']\n        setup_default_modules()\n        import_modules(custom_modules)\n        # Get the current computation block\n        target_block_id = config[\'to_run\']\n        self.block_id = target_block_id\n        # Update the schemas with the configuration\n        schemas: Dict[str, Schema] = Schema.deserialize(config[\'schemas\'])\n        schemas_copy = deepcopy(schemas)\n        global_vars = config[\'global_vars\']\n        self.verbose = config[\'verbose\']\n        self.hyper_params = config[\'hyper_params\']\n        self.debug = config[\'debug\']\n\n        with TrialLogging(log_dir=self.logdir,\n                          verbose=self.verbose,\n                          console_prefix=self.block_id,\n                          hyper_params=self.hyper_params,\n                          capture_warnings=True):\n\n            # Compile, activate links, and load checkpoints\n            filled_schemas: Dict = OrderedDict()\n            for block_id, schema_block in schemas.items():\n\n                block_params = config[\'params\'][block_id]\n\n                utils.update_schema_with_params(schemas_copy[block_id], block_params)\n\n                # First activate links from previous blocks in the\n                # pipeline\n                utils.update_link_refs(schemas_copy, block_id, global_vars)\n                block: Component = schemas_copy[block_id]()\n                filled_schemas[block_id] = schemas_copy[block_id]\n\n                if block_id in config[\'checkpoints\']:\n                    # Get the block hash\n                    needed_set = utils.extract_needed_blocks(schemas, block_id, global_vars)\n                    needed_blocks = ((k, v) for k, v in filled_schemas.items() if k in needed_set)\n                    block_hash = repr(OrderedDict(needed_blocks))\n\n                    # Check the mask, if it\'s False then we end\n                    # immediately\n                    mask_value = config[\'checkpoints\'][block_id][\'mask\'][block_hash]\n                    if mask_value is False:\n                        self.run_flag = False\n                        return\n\n                    # There should be a checkpoint\n                    checkpoint = config[\'checkpoints\'][block_id][\'paths\'][block_hash]\n                    state = load_state_from_file(checkpoint)\n                    block.load_state(state)\n\n                # Holding compiled objects alongside schemas is okay\n                # but not fully expressed in our type annotations.\n                # TODO: fix this in our utils type annotations\n                schemas_copy[block_id] = block  # type: ignore\n\n        # If everything went well, just compile\n        self.block = schemas_copy[target_block_id]\n\n        # Add tb prefix to computables in case multiple plots are\n        # requested\n        if not config[\'merge_plot\']:\n            self.block.tb_log_prefix = self.name\n\n    def save(self, checkpoint_dir: Optional[str] = None) -> str:\n        """"""Override to replace checkpoint.""""""\n        checkpoint_dir = os.path.join(checkpoint_dir or self.logdir, ""checkpoint"")\n        if not self.run_flag:\n            return checkpoint_dir\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        checkpoint = self._save(checkpoint_dir)\n        saved_as_dict = False\n        if isinstance(checkpoint, string_types):\n            if (not checkpoint.startswith(checkpoint_dir) or checkpoint == checkpoint_dir):\n                raise ValueError(\n                    ""The returned checkpoint path must be within the ""\n                    ""given checkpoint dir {}: {}"".format(\n                        checkpoint_dir, checkpoint))\n            if not os.path.exists(checkpoint):\n                raise ValueError(\n                    ""The returned checkpoint path does not exist: {}"".format(\n                        checkpoint))\n            checkpoint_path = checkpoint\n        elif isinstance(checkpoint, dict):\n            saved_as_dict = True\n            checkpoint_path = os.path.join(checkpoint_dir, ""checkpoint"")\n            with open(checkpoint_path, ""wb"") as f:\n                pickle.dump(checkpoint, f)\n        else:\n            raise ValueError(\n                ""`_save` must return a dict or string type: {}"".format(\n                    str(type(checkpoint))))\n        with open(checkpoint_path + "".tune_metadata"", ""wb"") as f:\n            pickle.dump({\n                ""experiment_id"": self._experiment_id,\n                ""iteration"": self._iteration,\n                ""timesteps_total"": self._timesteps_total,\n                ""time_total"": self._time_total,\n                ""episodes_total"": self._episodes_total,\n                ""saved_as_dict"": saved_as_dict\n            }, f)\n        return checkpoint_path\n\n    def _train(self) -> Dict:\n        """"""Subclasses should override this to implement train().""""""\n        if self.run_flag is False:\n            return {\'done\': True}\n\n        with TrialLogging(log_dir=self.logdir,\n                          verbose=self.verbose,\n                          console_prefix=self.block_id,\n                          hyper_params=self.hyper_params,\n                          capture_warnings=True):\n            # Tune uses ""done"" instead of ""continue"" flag, so reverse\n            # the boolean\n            done = not self.block.run()\n            metric = self.block.metric()\n\n        report = {\'done\': done}\n        if metric is not None:\n            report[\'episode_reward_mean\'] = metric\n\n        return report\n\n    def _save(self, checkpoint_dir: str) -> str:\n        """"""Subclasses should override this to implement save().""""""\n        path = os.path.join(checkpoint_dir, ""checkpoint.flambe"")\n        self.block.save(path, overwrite=True)\n        return path\n\n    def _restore(self, checkpoint: str) -> None:\n        """"""Subclasses should override this to implement restore().""""""\n        state = load_state_from_file(checkpoint)\n        self.block.load_state(state)\n\n    def _stop(self):\n        """"""Subclasses should override this for any cleanup on stop.""""""\n        if not self.run_flag:\n            shutil.rmtree(self.logdir)\n\n        if hasattr(self, \'block\') and self.block is not None:\n            del self.block\n'"
flambe/experiment/utils.py,3,"b'import copy\nimport os\nfrom collections import abc\nfrom typing import Dict, List, Mapping, Any, Optional, Iterable, Set, Sequence, MutableMapping\n\nimport ray\nimport torch\nfrom ruamel.yaml.compat import StringIO\nfrom ruamel import yaml as original_yaml\nfrom ray.tune.trial import Trial\nfrom ray.tune.suggest import SearchAlgorithm\nfrom ray.tune.schedulers import TrialScheduler\n\nfrom flambe.compile import Link, Component\nfrom flambe.compile import Schema as Schema\nfrom flambe.runnable.error import LinkError, SearchComponentError\nfrom flambe.experiment.options import Options, GridSearchOptions, SampledUniformSearchOptions\n\n\ndef check_links(blocks: Dict[str, Schema],\n                global_vars: Optional[Dict[str, Any]] = None) -> None:\n    """"""Check validity of links between blocks.\n\n    Ensures dependency order, and that only Comparable\n    blocks are being reduced through a LinkBest object.\n\n    Parameters\n    ----------\n    blocks : OrderedDict[str, Schema[Component]]\n        The blocks to check, in order\n\n    Raises\n    ------\n    LinkError\n        On undeclared blocks (i.e not the right config order)\n    ProtocolError\n        Attempt to reduce a non-comparable block\n\n    """"""\n    visited: Set[str] = set()\n    if global_vars is not None:\n        visited |= global_vars.keys()\n\n    def helper(block):\n        """"""Explore block""""""\n        for _, value in block.items():\n            # Check link order\n            if isinstance(value, Link):\n                target_block_id = value.root_schema\n                if target_block_id not in visited:\n                    raise LinkError(block_id, target_block_id)\n\n            # Check recurse\n            if isinstance(value, Mapping):\n                helper(value)\n\n    for block_id, block in blocks.items():\n        visited.add(block_id)\n        helper(block)\n\n\ndef check_search(blocks: Dict[str, Schema],\n                 search: Mapping[str, SearchAlgorithm],\n                 schedulers: Mapping[str, TrialScheduler]):\n    """"""Check validity of links between blocks.\n\n    Ensures dependency order, and that only Comparable\n    blocks are being reduced through a LinkBest object.\n\n    Parameters\n    ----------\n    blocks : OrderedDict[str, Schema[Component]]\n        Ordered mapping from block id to a schema of the block\n    search : Mapping[str, SearchAlgorithm], optional\n        Map from block id to hyperparameter search space generator\n    schedulers : Mapping[str, TrialScheduler], optional\n        Map from block id to search scheduler\n\n    Raises\n    ------\n    ProtocolError\n        Non computable block assigned a search or scheduler.\n    ProtocolError\n        Non comparable block assigned a non default search or scheduler\n\n    """"""\n    hyper_blocks = list(search.keys()) + list(schedulers.keys())\n\n    for block_id in hyper_blocks:\n        # Check all hyper blocks are computable\n        block_type = blocks[block_id].component_subclass\n        if not issubclass(block_type, Component):  # type: ignore\n            raise SearchComponentError(block_id)\n\n\ndef convert_tune(data: Any):\n    """"""Convert the options and links in the block.\n\n    Convert Option objects to tune.grid_search or\n    tune.sample_from functions, depending on the type.\n\n    Parameters\n    ----------\n    data : Any\n        Input object that may contain Options objects that should be\n        converted to a Tune-compatible representation\n\n    """"""\n    if isinstance(data, Options) or isinstance(data, Link):\n        return data.convert()\n    elif isinstance(data, dict):\n        return {k: convert_tune(v) for k, v in data.items()}\n    elif isinstance(data, (tuple, list)):\n        return [convert_tune(el) for el in data]\n    elif isinstance(data, Options):\n        if hasattr(data, \'elements\'):  # TODO: Bit hacky, make this better\n            out = copy.deepcopy(data)\n            out.elements = [convert_tune(elm) for elm in data.elements]  # type: ignore\n            return out\n    return data\n\n\ndef traverse(nested: Mapping[str, Any], path: Optional[List[str]] = None) -> Iterable[Any]:\n    """"""Iterate over a nested mapping returning the path and key, value.\n\n    Parameters\n    ----------\n    nested : Mapping[str, Any]\n        Mapping where some values are also mappings that should be\n        traversed\n    path : List[str]\n        List of keys that were used to reach the current mapping\n\n    Returns\n    -------\n    Iterable[Any]\n        Iterable of path, key, value triples\n\n    """"""\n    if path is None:\n        path = []\n    for key, value in nested.items():\n        if isinstance(value, abc.Mapping):\n            yield from traverse(value, path + [key])\n        else:\n            yield path, key, value\n\n\ndef traverse_all(obj: Any) -> Iterable[Any]:\n    """"""Iterate over all nested mappings and sequences\n\n    Parameters\n    ----------\n    obj: Any\n\n    Returns\n    -------\n    Iterable[Any]\n        Iterable of child values to obj\n\n    """"""\n    if isinstance(obj, Mapping):\n        for key, value in obj.items():\n            yield from traverse_all(value)\n    elif isinstance(obj, Iterable) and not isinstance(obj, str):\n        for value in obj:\n            yield from traverse_all(value)\n    else:\n        yield obj\n\n\ndef traverse_spec(nested: Mapping[str, Any], path: Optional[List[str]] = None) -> Iterable[Any]:\n    """"""Iterate over a nested mapping returning the path and key, value.\n\n    Parameters\n    ----------\n    nested : Mapping[str, Any]\n        Mapping where some values are also mappings that should be\n        traversed\n    path : List[str]\n        List of keys that were used to reach the current mapping\n\n    Returns\n    -------\n    Iterable[Any]\n        Iterable of path, key, value triples\n\n    """"""\n    if path is None:\n        path = []\n    for key, value in nested.items():\n        if isinstance(value, Mapping):\n            yield from traverse_spec(value, path + [key])\n        else:\n            yield ""spec.config."" + ""."".join(path), key, value\n\n\ndef update_nested(nested: MutableMapping[str, Any],\n                  prefix: Iterable[str],\n                  key: str,\n                  new_value: Any) -> None:\n    """"""Multi-level set operation for nested mapping.\n\n    Parameters\n    ----------\n    nested : Mapping[str, Any]\n        Nested dictionary where keys are all strings\n    prefix : Iterable[str]\n        List of keys specifying path to value to be updated\n    key : str\n        Final key corresponding to value to be updated\n    new_value : Any\n        New value to set for `[p1]...[key]` in `nested`\n\n    """"""\n    current = nested\n    for step in prefix:\n        current = current[step]  # type: ignore\n    current[key] = new_value  # type: ignore\n\n\ndef get_nested(nested: Mapping[str, Any], prefix: Iterable[str], key: str) -> Any:\n    """"""Get nested value in standard Mapping.\n\n    Parameters\n    ----------\n    nested : Mapping[str, Any]\n        The mapping to index in\n    prefix : Iterable[str]\n        The path to the final key in the nested input\n    key : str\n        The key to query\n\n    Returns\n    -------\n    Any\n        The value at the given path and key\n\n    """"""\n    current = nested\n    for step in prefix:\n        current = current[step]\n    return current[key]\n\n\ndef update_schema_with_params(schema: Schema, params: Dict[str, Any]) -> Schema:\n    """"""Replace options in the schema recursivly.\n\n    Parameters\n    ----------\n    schema : Schema[Any]\n        The schema object to update\n    params : Dict[str, Any]\n        The corresponding nested diciontary with values\n\n    Returns\n    -------\n    Schema[Any]\n        The update schema (same object as the input, not a copy)\n\n    """"""\n    for path, key, value in traverse(schema):\n        if isinstance(value, Options):\n            selected_value = get_nested(params, path, key)\n            update_nested(schema, path, key, selected_value)\n    # Return schema for chaining purposes\n    return schema\n\n\ndef has_schemas_or_options(x: Any) -> bool:\n    """"""Check if object contains Schemas or Options.\n\n    Recurses for Mappings and Sequences\n\n    Parameters\n    ----------\n    x : Any\n        Input object to check for Schemas and Options\n\n    Returns\n    -------\n    bool\n        True iff contains any Options or Schemas.\n\n    """"""\n    if isinstance(x, Schema):\n        return True\n    elif (isinstance(x, GridSearchOptions) or isinstance(x, SampledUniformSearchOptions)):\n        return True\n    elif isinstance(x, Mapping):\n        return any(has_schemas_or_options(v) for k, v in x.items())\n    elif isinstance(x, Sequence) and not isinstance(x, str):\n        return any(has_schemas_or_options(e) for e in x)\n    else:\n        return False\n\n\ndef divide_nested_grid_search_options(\n        config: MutableMapping[str, Any]) -> Iterable[Mapping[str, Any]]:\n    """"""Divide config into a config Iterable to remove nested Options.\n\n    For every GridSearchOptions or SampledUniformSearchOptions, if any\n    values contain more Options or Schemas, create copies with a\n    single value selected in place of the option. Resulting configs\n    will have no nested options.\n\n    Parameters\n    ----------\n    config : MutableMapping[str, Any]\n        MutableMapping (or Schema) containing Options and Schemas\n\n    Returns\n    -------\n    Iterable[Mapping[str, Any]]\n        Each Mapping contains variants from original config without\n        nested options\n\n    """"""\n    no_options_yielded = True\n    for prefix, key, value in traverse(config):\n        if (isinstance(value, GridSearchOptions) or\n                isinstance(value, SampledUniformSearchOptions)) and \\\n                any(has_schemas_or_options(x) for x in value):\n            no_options_yielded = False\n            for option in value:\n                config_variant = copy.deepcopy(config)\n                # Create copy that has one selected value\n                update_nested(config_variant, prefix, key, option)\n                # Continue yielding to select other values\n                yield from divide_nested_grid_search_options(config_variant)\n            return\n    if no_options_yielded:\n        yield config\n\n\ndef extract_dict(config: Mapping[str, Any]) -> Dict[str, Any]:\n    """"""Turn the schema into a dictionary, ignoring types.\n\n    NOTE: We recurse if any value is itself a `Schema`, a `Sequence`\n    of `Schema`s, or a `Mapping` of `Schema`s. Other unconvential\n    collections will not be inspected.\n\n    Parameters\n    ----------\n    schema: Schema\n        The object to be converted into a dictionary\n\n    Returns\n    -------\n    Dict\n        The output dictionary representation.\n\n    """"""\n    def helper(obj):\n        if isinstance(obj, Schema):\n            out = helper(obj.keywords)\n        elif isinstance(obj, Link):\n            out = obj\n        elif isinstance(obj, Options):\n            if hasattr(obj, \'elements\'):  # TODO: Bit hacky, make this better\n                out = copy.deepcopy(obj)\n                out.elements = [helper(elm) for elm in obj]\n            else:\n                out = obj\n        elif isinstance(obj, list) or isinstance(obj, tuple):\n            out = [helper(elm) for elm in obj]\n        elif isinstance(obj, abc.Mapping):\n            out = {k: helper(v) for k, v in obj.items()}\n        else:\n            out = obj\n\n        return out\n\n    return helper(config)\n\n\ndef extract_needed_blocks(schemas: Dict[str, Schema],\n                          block_id: str,\n                          global_vars: Optional[Dict[str, Any]] = None) -> Set[str]:\n    """"""Returns the set of all blocks that the input block links to.\n\n    Parameters\n    ----------\n    schemas : Dict[str, Schema[Any]]\n        Map from `block_id` to `Schema` object\n    block_id : str\n        The block containing links\n\n    Returns\n    -------\n    List[str]\n        The list of ancestor block ids\n\n    """"""\n    needed = set()\n    this_block = schemas[block_id]\n\n    # Get this block\'s links\n    for _, _, value in traverse(this_block):\n        if isinstance(value, Link) and value.root_schema != block_id:\n            # Ensure intra-block links are not added to prevent inf loop\n            needed.add(value.root_schema)\n        elif isinstance(value, Iterable):\n            for element in value:\n                if isinstance(element, Link) and element.root_schema != block_id:\n                    needed.add(element.root_schema)\n\n    # Reccurse through the new needed blocks\n    for linked_block_id in needed.copy():\n        if linked_block_id not in schemas.keys():\n            if global_vars is None or linked_block_id not in global_vars.keys():\n                raise LinkError(block_id, linked_block_id)\n        else:\n            needed |= extract_needed_blocks(schemas, linked_block_id, global_vars)\n    needed |= {block_id}\n    return needed\n\n\ndef update_link_refs(schemas: Dict[str, Schema],\n                     block_id: str,\n                     global_vars: Dict[str, Any]) -> None:\n    """"""Resolve links in schemas at `block_id`.\n\n    Parameters\n    ----------\n    schemas : Dict[str, Schema[Any]]\n        Map from `block_id` to `Schema` object\n    block_id : str\n        The block where links should be activated\n    global_vars: Dict[str, Any]\n        The environment links (ex: resources)\n\n    """"""\n    this_block = schemas[block_id]\n    for value in traverse_all(this_block):\n        if isinstance(value, Link):\n            if value.root_schema in schemas:\n                value.target = schemas[value.root_schema]\n                if isinstance(value.target, Component):\n                    value.target = value.target._schema\n            elif value.root_schema in global_vars:\n                value.target = global_vars[value.root_schema]\n                value.local = False\n\n\ndef get_best_trials(trials: List[Trial], topk: int, metric=\'episode_reward_mean\') -> List[Trial]:\n    """"""Get the trials with the best result.\n\n    Parameters\n    ----------\n    trials : List[ray.tune.Trial]\n        The list of trials to examine\n    topk : int\n        The number of trials to reduce to\n    metric : str, optional\n        The metric used in comparaison (higher is better)\n\n    Returns\n    -------\n    List[ray.tune.Trial]\n        The list of best trials\n\n    """"""\n    if topk <= 0:\n        return []\n\n    sorted_trials = sorted(trials, key=lambda t: t.last_result.get(metric, 0), reverse=True)\n    return sorted_trials[:topk]\n\n\ndef get_non_remote_config(experiment):\n    """"""Returns a copy of the original config file without\n    the remote configuration\n\n    Parameters\n    ----------\n    experiment : Experiment\n        The experiment object\n\n    """"""\n    new_experiment = copy.deepcopy(experiment)\n    # Remove manager\n    experiment.manager = None\n\n    with StringIO() as s:\n        native_yaml = original_yaml.YAML()\n        native_yaml.dump(new_experiment, s)\n        return s.getvalue()\n\n\ndef local_has_gpu() -> bool:\n    """"""Returns is local process has GPU\n\n    Returns\n    -------\n    bool\n\n    """"""\n    return torch.cuda.is_available()\n\n\ndef rel_to_abs_paths(d: Dict[str, str]) -> Dict[str, str]:\n    """"""Convert relative paths to absolute paths.\n\n    Parameters\n    ----------\n    d: Dict[str, str]\n        A dict from name -> path.\n\n    Returns\n    -------\n    Dict[str, str]\n        The same dict received as parameter with relative paths\n        replaced with absolute.\n\n    """"""\n    ret = d.copy()\n    for k, v in ret.items():\n        if os.path.exists(v) and not os.path.isabs(v):\n            ret[k] = os.path.abspath(v)\n    return ret\n\n\ndef shutdown_ray_node() -> int:\n    """"""Call \'ray stop\' locally to terminate\n    the ray node.\n\n    """"""\n    return os.system(""bash -lc \'ray stop\'"")\n\n\ndef shutdown_remote_ray_node(host: str,\n                             user: str,\n                             key: str) -> int:\n    """"""Execute \'ray stop\' on a remote machine through ssh to\n    terminate the ray node.\n\n    IMPORTANT: this method is intended to be run in the cluster.\n\n    Parameters\n    ----------\n    host: str\n        The Orchestrator\'s IP that is visible by the factories\n        (usually the private IP)\n    user: str\n        The username for that machine.\n    key: str\n        The key that communicate with the machine.\n\n    """"""\n    cmd = f""ssh -i {key} -o StrictHostKeyChecking=no {user}@{host} \\""bash -lc \'ray stop\'\\""""\n    return os.system(cmd)\n\n\ndef get_default_devices(debug: bool = False,\n                        default_cpus: int = 1,\n                        default_gpus: int = 1) -> Dict[str, int]:\n    """"""Get the default devices to use if not provided.\n\n    Parameters\n    ----------\n    debug: bool, optional\n        If we are running in debug mode (where Ray is not available).\n        If debug is False, this method should be called after running\n        ``ray.init()``.\n    default_cpus: int, optional\n        The default number of CPU\'s to use. Default ``1``.\n    default_gpus: int, optional\n        The default number of GPU\'s to use. Default ``1``.\n\n    Returns\n    -------\n    devices: Dict[str, int]\n        Default set of devices to use. Should have at most two keys:\n        \'cpu\', and \'gpu\' if cuda is available.\n\n    Raises\n    ------\n    ValueError\n        If the total number of GPUs is larger than the number available.\n\n    """"""\n    # Get available resources\n    if not debug and ray.is_initialized():\n        cluster_devices = ray.cluster_resources()\n        num_cpus = max(cluster_devices.get(\'CPU\', 0), cluster_devices.get(\'cpu\', 0))\n        num_gpus = max(cluster_devices.get(\'GPU\', 0), cluster_devices.get(\'gpu\', 0))\n    elif torch.cuda.is_available():\n        num_cpus = os.cpu_count()\n        num_gpus = torch.cuda.device_count()\n    else:\n        num_cpus = os.cpu_count()\n        num_gpus = 0\n\n    # Check that requested is not larger than available\n    if default_cpus > num_cpus:\n        raise ValueError(f""Number of CPUs requested ({default_cpus}) is larger \\\n            than the total number available ({num_cpus})."")\n    if num_gpus > 0 and default_gpus > num_gpus:\n        raise ValueError(f""Number of GPUs requested ({default_gpus}) is larger \\\n            than the total number available ({num_gpus})."")\n\n    # Set provided defaults\n    if num_gpus > 0:\n        devices = {""cpu"": default_cpus, ""gpu"": default_gpus}\n    else:\n        devices = {""cpu"": default_cpus}\n\n    return devices\n'"
flambe/experiment/wording.py,0,"b'import colorama\nfrom colorama import Fore, Style\nimport logging\ncolorama.init()\n\nlogger = logging.getLogger(__name__)\n\n\ndef print_useful_local_info(full_save_path) -> None:\n    """"""Information to display before experiment is running.\n\n    """"""\n    logger.info(""###########################################################"")\n    logger.info(""                     IMPORTANT"")\n    logger.info(""###########################################################"")\n    logger.info("""")\n    logger.info(""Experiment will start running shortly. Results will start to become available"")\n    logger.info(f""at {full_save_path}."")\n    logger.info("""")\n    logger.info(""REPORT SITE"")\n    logger.info(""----------"")\n    logger.info(""To view the experiment\'s progress, run:"")\n    logger.info("""")\n    logger.info(f""$ flambe-site {full_save_path}/state.pkl [--port 12345]"")\n    logger.info("""")\n\n\ndef print_useful_remote_info(manager, experiment_name) -> None:\n    """""" Once the local process of the remote run is over,\n    this information is shown to the user.\n\n    """"""\n    logger.info(""###########################################################"")\n    logger.info(""                     IMPORTANT"")\n    logger.info(""###########################################################"")\n    logger.info("""")\n    logger.info(""MODEL DOWNLOADING"")\n    logger.info(""-----------------"")\n    logger.info(""Once experiment is over, you can download all results by executing:"")\n    logger.info(\n        f""scp -r -i <key> {manager.orchestrator.username}@{manager.orchestrator.host}:""\n        f""{manager.orchestrator.get_home_path()}/{experiment_name}/synced_results <local_dst>"")\n    logger.info("""")\n    logger.info(""Where <key> is the private key used to create the cluster and <local_dst>"")\n    logger.info(""is a local directory to store the results."")\n\n    logger.info("""")\n    logger.info(""INSTANCES"")\n    logger.info(""---------"")\n\n    if manager.factories_timeout > 0:\n        logger.info(\n            f""The factories instances  will be terminated after {manager.factories_timeout} "" +\n            ""hour of unusage."")\n    elif manager.factories_timeout == -1:\n        logger.info(""The factories instances  will remain running "" +\n                    ""(you are in charge of terminating them)"")\n    elif manager.factories_timeout == 0:\n        logger.info(""The factories instance  will be terminated once the experiment is over"")\n\n    if manager.orchestrator_timeout > 0:\n        logger.info(f""The orchestrator will be terminated after {manager.orchestrator_timeout} "" +\n                    ""hour of unusage."")\n    elif manager.orchestrator_timeout == -1:\n        logger.info(""The orchestrator instance  will remain running "" +\n                    ""(you are in charge of terminating it)"")\n    elif manager.orchestrator_timeout == 0:\n        logger.info(""The orchestrator instance  will be terminated once the experiment is over"")\n\n\ndef print_useful_metrics_only_info() -> None:\n    """"""Printable warning when debug flag is active\n\n    """"""\n    logger.info(Fore.YELLOW)\n    logger.info(""You specified \'debug\' to True and therefore, after experiment is done, "" +\n                ""all data will be lost!"")\n    logger.info(Style.RESET_ALL)\n\n\ndef print_extensions_cache_size_warning(location, limit) -> None:\n    """"""Print message when the extensions cache folder is getting big.\n\n    """"""\n    logger.info(Fore.YELLOW)\n    logger.info(\n        f""Be aware that your extensions cache for github extensions located in {location}) "" +\n        f""is increasing its size (it\'s currently bigger than {limit} MB)."")\n    logger.info(""Please remove unused extensions from that location."")\n    logger.info(Style.RESET_ALL)\n'"
flambe/export/__init__.py,0,"b""from flambe.export.builder import Builder\nfrom flambe.export.exporter import Exporter\n\n\n__all__ = ['Builder', 'Exporter']\n"""
flambe/export/builder.py,0,"b'import tempfile\nimport os\nimport boto3\nimport dill\n\nimport subprocess\nfrom urllib.parse import urlparse\n\nimport flambe\nfrom flambe.runnable import Runnable, error\nfrom flambe.compile import Component, Schema\nfrom flambe.compile.const import DEFAULT_PROTOCOL\nfrom flambe.logging import coloredlogs as cl\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass Builder(Runnable):\n    """"""Implement a Builder.\n\n    A builder is a simple object that can be used to create\n    any Component post-experiment, and export it to a local\n    or remote location.\n\n    Currently supports local, and S3 locations.\n\n    Attributes\n    ----------\n    config: configparser.ConfigParser\n        The secrets that the user provides. For example,\n        \'config[""AWS""][""ACCESS_KEY""]\'\n\n    """"""\n    def __init__(self,\n                 component: Schema,\n                 destination: str,\n                 storage: str = \'local\',\n                 compress: bool = False,\n                 pickle_only: bool = False,\n                 pickle_module=dill,\n                 pickle_protocol=DEFAULT_PROTOCOL) -> None:\n        """"""Initialize the Builder.\n\n        Parameters\n        ----------\n        component : Schema\n            The object to build, and export\n        destination : str\n            The destination where the object should be saved.\n            If an s3 bucket is specified, \'s3\' should also be\n            specified as the storage argument. s3 destinations\n            should have the following syntax:\n            \'s3://<bucket-name>[/path/to/folder]\'\n        storage: str\n            The storage location. One of: [local | s3]\n        compress : bool\n            Whether to compress the save file / directory via tar + gz\n        pickle_only : bool\n            Use given pickle_module instead of the hiearchical save\n            format (the default is False).\n        pickle_module : type\n            Pickle module that has load and dump methods; dump should\n            accept a pickle_protocol parameter (the default is dill).\n        pickle_protocol : type\n            Pickle protocol to use; see pickle for more details (the\n            default is 2).\n\n        """"""\n        super().__init__()\n\n        self.destination = destination\n        self.component = component\n\n        self.compiled_component: Component\n\n        self.storage = storage\n        self.serialization_args = {\n            \'compress\': compress,\n            \'pickle_only\': pickle_only,\n            \'pickle_module\': pickle_module,\n            \'pickle_protocol\': pickle_protocol\n        }\n\n    def run(self, force: bool = False, **kwargs) -> None:\n        """"""Run the Builder.""""""\n\n        # Add information about the extensions. This ensures\n        # the compiled component has the extensions information\n        self.component.add_extensions_metadata(self.extensions)\n\n        self.compiled_component = self.component()  # Compile Schema\n\n        if self.storage == \'local\':\n            self.save_local(force)\n        elif self.storage == \'s3\':\n            self.save_s3(force)\n        else:\n            msg = f""Unknown storage {self.storage}, should be one of: [local, s3]""\n            raise ValueError(msg)\n\n    def save_local(self, force) -> None:\n        """"""Save an object locally.\n\n        Parameters\n        ----------\n        force: bool\n            Wheter to use a non-empty folder or not\n\n        """"""\n        if (\n            os.path.exists(self.destination) and\n            os.listdir(self.destination) and\n            not force\n        ):\n            raise error.ParsingRunnableError(\n                f""Destination {self.destination} folder is not empty. "" +\n                ""Use --force to force the usage of this folder or "" +\n                ""pick another destination.""\n            )\n\n        flambe.save(self.compiled_component, self.destination, **self.serialization_args)\n\n    def get_boto_session(self):\n        """"""Get a boto Session\n\n        """"""\n        return boto3.Session()\n\n    def save_s3(self, force) -> None:\n        """"""Save an object to s3 using awscli\n\n        Parameters\n        ----------\n        force: bool\n            Wheter to use a non-empty bucket folder or not\n\n        """"""\n        url = urlparse(self.destination)\n\n        if url.scheme != \'s3\' or url.netloc == \'\':\n            raise error.ParsingRunnableError(\n                ""When uploading to s3, destination should be: "" +\n                ""s3://<bucket-name>[/path/to/dir]""\n            )\n\n        bucket_name = url.netloc\n        s3 = self.get_boto_session().resource(\'s3\')\n        bucket = s3.Bucket(bucket_name)\n\n        for content in bucket.objects.all():\n            path = url.path[1:]  # Remove first \'/\'\n            if content.key.startswith(path) and not force:\n                raise error.ParsingRunnableError(\n                    f""Destination {self.destination} is not empty. "" +\n                    ""Use --force to force the usage of this bucket folder or "" +\n                    ""pick another destination.""\n                )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            flambe.save(self.compiled_component, tmpdirname, **self.serialization_args)\n            try:\n                subprocess.check_output(\n                    f""aws s3 cp --recursive {tmpdirname} {self.destination}"".split(),\n                    stderr=subprocess.STDOUT,\n                    universal_newlines=True\n                )\n            except subprocess.CalledProcessError as exc:\n                logger.debug(exc.output)\n                raise ValueError(f""Error uploading artifacts to s3. "" +\n                                 ""Check logs for more information"")\n            else:\n                logger.info(cl.BL(f""Done uploading to {self.destination}""))\n'"
flambe/export/exporter.py,0,"b'from typing import Dict, Any\n\nfrom flambe import Component\n\n\nclass Exporter(Component):\n    """"""Implement an Exporter computable.\n\n    This object can be viewed as a dummy computable. It is useful\n    to group objects into a block when those get save, to more\n    easily refer to them later on, for instance in an object builder.\n\n    """"""\n\n    def __init__(self, **kwargs: Dict[str, Any]) -> None:\n        """"""Initialize the Exporter.\n\n        Parameters\n        ----------\n        kwargs: Dict[str, Any]\n            Mapping from name to any object to export\n\n        """"""\n        self.objects = kwargs\n\n        for name, obj in kwargs.items():\n            setattr(self, name, obj)\n            if not isinstance(obj, Component):\n                self.register_attrs(name)\n\n    def run(self) -> bool:\n        """"""Run the exporter.\n\n        Returns\n        -------\n        bool\n            False, as this is a single step Component.\n\n        """"""\n        _continue = False\n        return _continue\n'"
flambe/field/__init__.py,0,"b""from flambe.field.field import Field\nfrom flambe.field.text import TextField\nfrom flambe.field.bow import BoWField\nfrom flambe.field.label import LabelField\n\n\n__all__ = ['Field', 'TextField', 'LabelField', 'BoWField']\n"""
flambe/field/bow.py,2,"b'from typing import Dict, Optional\nfrom collections import OrderedDict as odict\n\nimport torch\n\nfrom flambe.field import Field\nfrom flambe.tokenizer import Tokenizer, NGramsTokenizer\n\n\nclass BoWField(Field):\n    """"""Featurize raw text inputs using bag of words (BoW)\n\n    This class performs tokenization and numericalization.\n\n    The pad, unk, when given, are assigned the first indices in the\n    vocabulary, in that order. This means, that whenever a pad token\n    is specified, it will always use the 0 index.\n\n    Examples\n    --------\n\n    >>> f = BoWField(min_freq=2, normalize=True)\n    >>> f.setup([\'thank you\', \'thank you very much\', \'thanks a lot\'])\n    >>> f._vocab.keys()\n    [\'thank\', you\']\n\n    Note that \'thank\' and \'you\' are the only ones that appear twice.\n\n    >>> f.process(""thank you really. You help was awesome"")\n    tensor([1, 2])\n\n    """"""\n\n    def __init__(self,  # nosec\n                 tokenizer: Optional[Tokenizer] = None,\n                 lower: bool = False,\n                 unk_token: str = \'<unk>\',\n                 min_freq: int = 5,\n                 normalize: bool = False,\n                 scale_factor: float = None) -> None:\n        """"""Initialize the BoW object.\n\n        Parameters\n        ----------\n        tokenizer : Tokenizer, optional\n            Tokenizer to use, by default NGramsTokenizer()\n        lower : bool, optional\n            If given, lowercase the input, by default False\n        unk_token : str, optional\n            The token to use for out of vocabulary tokens\n            (defaults to \'<unk>\')\n        min_freq : int, optional\n            Minimum frequency to include token in the vocabulary\n            (defaults to 5)\n        normalize : bool, optional\n            Normalize or not the bag of words using L1 norm\n            (defaults to False)\n        scale_factor : float, optional\n            Factor to scale the resulting normalized feature value.\n            Only available when normalize is True (defaults to 1.0)\n\n        """"""\n        self.tokenizer = tokenizer or NGramsTokenizer()\n        self.lower = lower\n        self.unk = unk_token\n\n        self.min_freq = min_freq\n        self.normalize = normalize\n        self.scale_factor = scale_factor\n\n        self.vocab: Dict[str, int] = odict()\n        self.vocab[unk_token] = 0\n        self.full_vocab: Dict[str, int] = {}\n\n        if scale_factor and not normalize:\n            raise ValueError(f""Cannot specify scale_factor without normalizing"")\n\n        self.register_attrs(\'vocab\', \'full_vocab\')\n\n    @property\n    def vocab_size(self) -> int:\n        """"""Get the vocabulary length.\n\n        Returns\n        -------\n        int\n            The length of the vocabulary\n\n        """"""\n        return len(self.vocab)\n\n    def process(self, example):\n        # Lowercase and tokenize\n        example = example.lower() if self.lower else example\n        tokens = self.tokenizer(example)\n\n        # Numericalize\n        numericals = [0] * len(self.vocab)\n        for token in tokens:\n            if token in self.vocab:\n                numericals[self.vocab[token]] += 1\n            else:\n                if token not in self.full_vocab:\n                    if self.unk is None or self.unk not in self.vocab:\n                        raise ValueError(""Encounterd out-of-vocabulary token \\\n                                          but the unk_token is either missing \\\n                                          or not defined in the vocabulary."")\n                    else:\n                        # Accumulate in numericals\n                        numericals[self.vocab[self.unk]] += 1  # type: ignore\n\n        processed = torch.tensor(numericals).float()\n        if self.normalize:\n            processed = torch.nn.functional.normalize(processed, dim=0, p=1)\n        if self.scale_factor:\n            processed = self.scale_factor * processed\n\n        return processed\n\n    def setup(self, *data) -> None:\n        for dataset in data:\n            for example in dataset:\n                # Lowercase if requested\n                example = example.lower() if self.lower else example\n                # Tokenize and accumulate in vocabulary\n                for token in self.tokenizer(example):\n                    self.full_vocab[token] = self.full_vocab.get(token, 0) + 1\n\n        # Filter only the once that have high frequency\n        for k, v in self.full_vocab.items():\n            if v >= self.min_freq:\n                self.vocab.setdefault(k, len(self.vocab))\n'"
flambe/field/field.py,5,"b'from abc import abstractmethod\nfrom typing import Any, Union, Tuple, List, Dict\n\nimport torch\nimport numpy as np\n\nfrom flambe import Component\n\n\nclass Field(Component):\n    """"""Base Field interface.\n\n    A field processes raw examples and produces Tensors.\n\n    """"""\n    def setup(self, *data: np.ndarray) -> None:\n        """"""Setup the field.\n\n        This method will be called with all the data in the dataset and\n        it can be used to compute aggregated information (for example,\n        vocabulary in Fields that process text).\n\n        ATTENTION: this method could be called multiple times in case\n        the same field is used in different datasets. Take this into\n        account and build a stateful implementation.\n\n        Parameters\n        ----------\n        *data: np.ndarray\n            Multiple 2d arrays (ex: train_data, dev_data, test_data).\n            First dimension is for the examples, second dimension for\n            the columns specified for this specific field.\n\n        """"""\n        pass\n\n    @abstractmethod\n    def process(self, *example: Any) \\\n            -> Union[torch.Tensor,\n                     Tuple[torch.Tensor, ...],\n                     List[torch.Tensor],\n                     Dict[str, torch.Tensor]]:\n        """"""Process an example into a Tensor or tuple of Tensor.\n\n        This method allows N to M mappings from example columns (N)\n        to tensors (M).\n\n        Parameters\n        ----------\n        *example: Any\n            Column values of the example\n\n        Returns\n        -------\n        Union[torch.Tensor, Tuple[torch.Tensor, ...]]\n            The processed example, as a tensor or tuple of tensors\n        """"""\n        pass\n'"
flambe/field/label.py,11,"b'from typing import Optional, Union, List, Iterable\nfrom collections import OrderedDict as odict\n\nimport torch\nimport numpy as np\n\nfrom flambe.field.field import Field\nfrom flambe.tokenizer import LabelTokenizer\n\n\nclass LabelField(Field):\n    """"""Featurizes input labels.\n\n    The class also handles multilabel inputs and one hot encoding.\n\n    """"""\n    def __init__(self,\n                 one_hot: bool = False,\n                 multilabel_sep: Optional[str] = None,\n                 labels: Optional[Union[Iterable[str], str]] = None) -> None:\n        """"""Initializes the LabelFetaurizer.\n\n        Parameters\n        ----------\n        one_hot : bool, optional\n            Set for one-hot encoded outputs, defaults to False\n        multilabel_sep : str, optional\n            If given, splits the input label into multiple labels\n            using the given separator, defaults to None.\n        labels: Union[Iterable[str], str], optional\n            If given, sets the labels and the ordering is used to map\n            the labels to indices. That means the first item in this\n            list will have label id 0, the next one id 1, etc..\n            When not provided, indices are assigned as labels are\n            encountered during preprocessing. The list can also be\n            provided as a file with a label on each line.\n\n        """"""\n        self.one_hot = one_hot\n        self.multilabel_sep = multilabel_sep\n        self.tokenizer = LabelTokenizer(multilabel_sep=self.multilabel_sep)\n\n        if labels is not None:\n            if isinstance(labels, str):\n                # Labels if a file\n                with open(labels, \'r\') as f:\n                    label_list: List[str] = f.read().splitlines()\n            elif isinstance(labels, Iterable):\n                label_list = list(labels)\n\n            self.label_given = True\n            self.vocab = odict((label, i) for i, label in enumerate(label_list))\n            self.label_count_dict = {label: 0 for label in self.vocab}\n        else:\n            self.label_given = False\n            self.vocab = odict()\n            self.label_count_dict = dict()\n\n        self.register_attrs(\'vocab\')\n        self.register_attrs(\'label_count_dict\')\n\n    def setup(self, *data: np.ndarray) -> None:\n        """"""Build the vocabulary.\n\n        Parameters\n        ----------\n        data : Iterable[str]\n            List of input strings.\n\n        """"""\n        # Iterate over all examples\n        examples = (e for dataset in data for e in dataset if dataset is not None)\n\n        for example in examples:\n            # Tokenize and add to vocabulary\n            for token in self.tokenizer(example):\n                if self.label_given:\n                    if token not in self.vocab:\n                        raise ValueError(f""Found label {token} not provided in label list."")\n                    else:\n                        self.label_count_dict[token] += 1\n                else:\n                    if token not in self.vocab:\n                        self.vocab[token] = len(self.vocab)\n                        self.label_count_dict[token] = 1\n                    else:\n                        self.label_count_dict[token] += 1\n\n    def process(self, example):\n        """"""Featurize a single example.\n\n        Parameters\n        ----------\n        example: str\n            The input label\n\n        Returns\n        -------\n        torch.Tensor\n            A list of integer tokens\n\n        """"""\n        tokens = self.tokenizer(example)\n\n        # Numericalize\n        numericals = []\n        for token in tokens:\n            if token not in self.vocab:\n                raise ValueError(""Encounterd out-of-vocabulary label {token}"")\n\n            numerical = self.vocab[token]  # type: ignore\n            numericals.append(numerical)\n\n        out = torch.tensor(numericals).long()\n\n        if self.one_hot:\n            out = [int(i in out) for i in range(len(self.vocab))]\n            out = torch.tensor(out).long()  # Back to Tensor\n\n        return out.squeeze()\n\n    @property\n    def vocab_list(self) -> List[str]:\n        """"""Get the list of tokens in the vocabulary.\n\n        Returns\n        -------\n        List[str]\n            The list of tokens in the vocabulary, ordered.\n\n        """"""\n        return list(self.vocab.keys())\n\n    @property\n    def vocab_size(self) -> int:\n        """"""Get the vocabulary length.\n\n        Returns\n        -------\n        int\n            The length of the vocabulary\n\n        """"""\n        return len(self.vocab)\n\n    @property\n    def label_count(self) -> torch.Tensor:\n        """"""Get the label count.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor containing the count for each label, indexed\n            by the id of the label in the vocabulary.\n        """"""\n        counts = [self.label_count_dict[label] for label in self.vocab]\n        return torch.tensor(counts).float()\n\n    @property\n    def label_freq(self) -> torch.Tensor:\n        """"""Get the frequency of each label.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor containing the frequency of each label, indexed\n            by the id of the label in the vocabulary.\n\n        """"""\n        counts = [self.label_count_dict[label] for label in self.vocab]\n        return torch.tensor(counts).float() / sum(counts)\n\n    @property\n    def label_inv_freq(self) -> torch.Tensor:\n        """"""Get the inverse frequency for each label.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor containing the inverse frequency of each label,\n            indexed by the id of the label in the vocabulary.\n\n        """"""\n        return 1. / self.label_freq  # type: ignore\n'"
flambe/field/text.py,12,"b'from typing import Dict, Iterable, List, Optional, Set, Tuple, NamedTuple, Union, Any\nfrom collections import OrderedDict as odict\nfrom itertools import chain\n\nimport torch\nimport warnings\nimport numpy as np\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom gensim.models import fasttext\nfrom gensim.test.utils import temporary_file\n\nfrom flambe.compile.registrable import registrable_factory\nfrom flambe.field import Field\nfrom flambe.tokenizer import Tokenizer, WordTokenizer\n\n\ndef get_embeddings(\n    embeddings: str,\n    embeddings_format: str = \'glove\',\n    embeddings_binary: bool = False,\n) -> KeyedVectors:\n    """"""\n    Get the embeddings model and matrix used in the setup function\n\n    Parameters\n    ----------\n    embeddings : Optional[str], optional\n        Path to pretrained embeddings, by default None\n    embeddings_format : str, optional\n        The format of the input embeddings, should be one of:\n        \'glove\', \'word2vec\', \'fasttext\' or \'gensim\'. The latter can\n        be used to download embeddings hosted on gensim on the fly.\n        See https://github.com/RaRe-Technologies/gensim-data\n        for the list of available embedding aliases.\n    embeddings_binary : bool, optional\n        Whether the input embeddings are provided in binary format,\n        by default False\n\n    Returns\n    -------\n    KeyedVectors\n        The embeddings object specified by the parameters.\n    """"""\n    model = None\n\n    if embeddings_format == \'glove\':\n        with temporary_file(\'temp.txt\') as temp:\n            glove2word2vec(embeddings, temp)\n            model = KeyedVectors.load_word2vec_format(temp, binary=embeddings_binary)\n    elif embeddings_format == \'word2vec\':\n        model = KeyedVectors.load_word2vec_format(embeddings,\n                                                  binary=embeddings_binary)\n    elif embeddings_format == \'fasttext\':\n        model = fasttext.load_facebook_vectors(embeddings)\n    elif embeddings_format == \'gensim\':\n        try:\n            model = KeyedVectors.load(embeddings)\n        except FileNotFoundError:\n            model = api.load(embeddings)\n    else:\n        raise ValueError(""Only formats supported are word2vec, fasttext and gensim"")\n\n    return model\n\n\nclass EmbeddingsInformation(NamedTuple):\n    """"""\n    Information about an embedding model.\n\n    Parameters\n    ----------\n    embeddings : str\n        Path to pretrained embeddings or the embedding name\n        in case format is gensim.\n    embeddings_format : str, optional\n        The format of the input embeddings, should be one of:\n        \'glove\', \'word2vec\', \'fasttext\' or \'gensim\'. The latter can\n        be used to download embeddings hosted on gensim on the fly.\n        See https://github.com/RaRe-Technologies/gensim-data\n        for the list of available embedding aliases.\n    embeddings_binary : bool, optional\n        Whether the input embeddings are provided in binary format,\n        by default False.\n    build_vocab_from_embeddings: bool\n        Controls if all words from the optional provided\n        embeddings will be added to the vocabulary and to the\n        embedding matrix. Defaults to False.\n    unk_init_all : bool, optional\n        If True, every token not provided in the input embeddings is\n        given a random embedding from a normal distribution.\n        Otherwise, all of them map to the \'<unk>\' token.\n    drop_unknown: bool\n        Whether to drop tokens that don\'t have embeddings\n        associated. Defaults to False.\n        Important: this flag will only work when using embeddings.\n\n    """"""\n    embeddings: str\n    embeddings_format: str = \'gensim\'\n    embeddings_binary: bool = False\n    build_vocab_from_embeddings: bool = False\n    unk_init_all: bool = False\n    drop_unknown: bool = False\n\n\nclass TextField(Field):\n    """"""Featurize raw text inputs\n\n    This class performs tokenization and numericalization, as well as\n    decorating the input sequences with optional start and end tokens.\n\n    When a vocabulary is passed during initialiazation, it is used to\n    map the the words to indices. However, the vocabulary can also be\n    generated from input data, through the `setup` method. Once\n    a vocabulary has been built, this object can also be used to load\n    external pretrained embeddings.\n\n    The pad, unk, sos and eos tokens, when given, are assigned the\n    first indices in the vocabulary, in that order. This means, that\n    whenever a pad token is specified, it will always use the 0 index.\n\n    """"""\n\n    def __init__(self,  # nosec\n                 tokenizer: Optional[Tokenizer] = None,\n                 lower: bool = False,\n                 pad_token: Optional[str] = \'<pad>\',\n                 unk_token: str = \'<unk>\',\n                 sos_token: Optional[str] = None,\n                 eos_token: Optional[str] = None,\n                 embeddings_info: Optional[EmbeddingsInformation] = None,\n                 embeddings: Optional[str] = None,\n                 embeddings_format: str = \'glove\',\n                 embeddings_binary: bool = False,\n                 unk_init_all: bool = False,\n                 drop_unknown: bool = False,\n                 max_seq_len: Optional[int] = None,\n                 truncate_end: bool = False,\n                 setup_all_embeddings: bool = False,\n                 additional_special_tokens: Optional[List[str]] = None,\n                 vocabulary: Optional[Union[Iterable[str], str]] = None) -> None:\n        """"""Initialize the TextField.\n\n        Parameters\n        ----------\n        tokenizer : Tokenizer, optional\n            Tokenizer to use, by default WordTokenizer()\n        lower : bool, optional\n            If given, lowercase the input, by default False\n        pad_token : str, optional\n            Reserved padding token. Note that this object does not\n            perform padding. Padding is done on the fly, when sampling.\n            (defaults to \'<pad>\')\n        unk_token : str, optional\n            The token to use for out of vocabulary tokens\n            (defaults to \'<unk>\')\n        sos_token : str, optional\n            Start of sentence tokens to add to the start of\n            each sequence (defaults to \'<sos>\')\n        eos : Iterable[str], optional\n            List of end of sentence tokens to add to the end of each\n            sequence (defaults to an empty list)\n        embeddings_info : EmbeddingsInformation, optional\n            The embeddings information. By default None\n        embeddings : str\n            WIlL BE DEPRECATED SOON. USE \'from_embeddings\'\n            FACTORY INSTEAD.\n            Path to pretrained embeddings or the embedding name\n            in case format is gensim.\n        embeddings_format : str, optional\n            WIlL BE DEPRECATED SOON. USE \'from_embeddings\'\n            FACTORY INSTEAD.\n            The format of the input embeddings, should be one of:\n            \'glove\', \'word2vec\', \'fasttext\' or \'gensim\'. The latter can\n            be used to download embeddings hosted on gensim on the fly.\n            See https://github.com/RaRe-Technologies/gensim-data\n            for the list of available embedding aliases.\n        embeddings_binary : bool, optional\n            WIlL BE DEPRECATED SOON. USE \'from_embeddings\'\n            FACTORY INSTEAD.\n            Whether the input embeddings are provided in binary format,\n            by default False\n        unk_init_all : bool, optional\n            If True, every token not provided in the input embeddings is\n            given a random embedding from a normal distribution.\n            Otherwise, all of them map to the \'<unk>\' token.\n        drop_unknown: bool\n            WIlL BE DEPRECATED SOON. USE \'from_embeddings\'\n            FACTORY INSTEAD.\n            Whether to drop tokens that don\'t have embeddings\n            associated. Defaults to False.\n            Important: this flag will only work when using embeddings.\n        max_seq_len: int, optional\n            The maximum length possibly output by the process func.\n            If len of input tokens is larger than this number - then\n            the output will be truncated as a post processing step.\n        truncate_end: bool\n            Determines the window of observed text in process if the\n            input is larger than max_seq_len. If this value is True\n            the window starts from the end of the utterance.\n            Defaults to False.\n            example: max_seq_len=3, input_text=1 2 3 4 5\n            truncate_end=false: output=1 2 3\n            truncate_end=true: output=3 4 5\n        setup_all_embeddings: bool\n            WIlL BE DEPRECATED SOON. USE \'from_embeddings\'\n            FACTORY INSTEAD.\n            Controls if all words from the optional provided\n            embeddings will be added to the vocabulary and to the\n            embedding matrix. Defaults to False.\n        additional_special_tokens: Optional[List[str]]\n            Additional special tokens beyond the pad, unk, eos and sos\n            tokens.\n        vocabulary: Union[List[str], str], optional\n            Can be either a list of tokens or a file with a token on\n            each line. If given, one can choose to allow expandion of\n            the vocabulary or to freeze it.\n\n        """"""\n        if embeddings:\n            if embeddings_info:\n                raise ValueError(\n                    ""Cannot submit embeddings information and use the embeddings parameters"" +\n                    ""simultaneously. Use the \'from_embeddings\' factory instead."")\n\n            warnings.warn(""The embeddings-exclusive parameters "" +\n                          ""(\'embeddings\', \'embeddings_format\', \'embeddings_binary\', "" +\n                          ""\'setup_all_embeddings\', \'drop_unknown\', \'unk_init_all\') "" +\n                          ""will be deprecated in a future release. "" +\n                          ""Please migrate to use the \'from_embeddings\' factory."")\n\n            embeddings_info = EmbeddingsInformation(\n                embeddings=embeddings,\n                embeddings_format=embeddings_format,\n                embeddings_binary=embeddings_binary,\n                build_vocab_from_embeddings=setup_all_embeddings,\n                unk_init_all=unk_init_all,\n                drop_unknown=drop_unknown\n            )\n\n        self.tokenizer = tokenizer or WordTokenizer()\n        self.lower = lower\n\n        self.pad = pad_token\n        self.unk = unk_token\n        self.sos = sos_token\n        self.eos = eos_token\n\n        self.embeddings_info = embeddings_info\n\n        self.embedding_matrix: Optional[torch.Tensor] = None\n\n        self.max_seq_len = max_seq_len\n        self.truncate_end = truncate_end\n\n        self.unk_numericals: Set[int] = set()\n\n        # Load vocabulary if given\n        if vocabulary is None:\n            self.vocab: Dict = odict()\n        elif isinstance(vocabulary, str):\n            with open(vocabulary, \'r\') as f:\n                self.vocab = odict((tok, i) for i, tok in enumerate(f.read().splitlines()))\n        elif isinstance(vocabulary, Iterable):\n            self.vocab = odict((tok, i) for i, tok in enumerate(vocabulary))\n\n        additional_special_tokens = additional_special_tokens or []\n        specials = [pad_token, unk_token, sos_token, eos_token, *additional_special_tokens]\n        self.specials = [special for special in specials if special is not None]\n        self.register_attrs(\'vocab\')\n\n    @property\n    def vocab_list(self) -> List[str]:\n        """"""Get the list of tokens in the vocabulary.\n\n        Returns\n        -------\n        List[str]\n            The list of tokens in the vocabulary, ordered.\n\n        """"""\n        return list(self.vocab.keys())\n\n    @property\n    def vocab_size(self) -> int:\n        """"""Get the vocabulary length.\n\n        Returns\n        -------\n        int\n            The length of the vocabulary\n\n        """"""\n        unique_ids = set(v for k, v in self.vocab.items())\n        return len(unique_ids)\n\n    def _flatten_to_str(self, data_sample: Union[List, Tuple, Dict]) -> str:\n        """"""Converts any nested data sample to a str\n\n        Used to build vocabs from complex file structures\n\n        Parameters\n        ----------\n        data_sample: Union[List, Tuple, Dict]\n\n        Returns\n        -------\n        str\n            the flattened version, for vocab building\n\n        """"""\n        if isinstance(data_sample, list) or isinstance(data_sample, tuple):\n            return \' \'.join(self._flatten_to_str(s) for s in data_sample)\n        elif isinstance(data_sample, dict):\n            return \' \'.join(self._flatten_to_str(s) for s in data_sample.values())\n        elif isinstance(data_sample, str):\n            return data_sample\n        else:\n            raise ValueError(f\'Cannot process type {type(data_sample)} for vocab building.\')\n\n    def _build_vocab(self, *data: np.ndarray) -> None:\n        """"""\n        Build the vocabulary for this object based on the special\n        tokens and the data provided.\n\n        This method is safe to be called multiple times.\n\n        Parameters\n        ----------\n        *data: np.ndarray\n            The data\n\n        """"""\n        examples: Iterable = (e for dataset in data for e in dataset if dataset is not None)\n\n        index = len(self.vocab) - 1\n\n        # First load special tokens\n        for token in self.specials:\n            if token not in self.vocab:\n                self.vocab[token] = index = index + 1\n\n        for example in examples:\n            example = self._flatten_to_str(example)\n            # Lowercase if requested\n            example = example.lower() if self.lower else example\n            # Tokenize and add to vocabulary\n            for token in self.tokenizer(example):\n                if token not in self.vocab:\n                    self.vocab[token] = index = index + 1\n\n    def _build_embeddings(self, model: KeyedVectors,\n                          setup_vocab_from_embeddings: bool,\n                          initialize_unknowns: bool) -> Tuple[odict, torch.Tensor]:\n        """"""\n        Create the embeddings matrix and the new vocabulary in\n        case this objects needs to use an embedding model.\n\n        A new vocabulary needs to be built because of the parameters\n        that could allow, for example, collapsing OOVs.\n\n        Parameters\n        ----------\n        model: KeyedVectors\n            The embeddings\n        setup_vocab_from_embeddings: bool\n            Controls if all words from the optional provided\n            embeddings will be added to the vocabulary and to the\n            embedding matrix. Defaults to False.\n        initialize_unknowns\n            If True, every unknown token will be assigned\n            a random embedding from a normal distribution.\n            Otherwise, all of them map to the \'<unk>\' token.\n\n        Returns\n        -------\n        Tuple[OrderedDict, torch.Tensor]\n            A tuple with the new embeddings and the embedding matrix\n        """"""\n        embedding_matrix: List[torch.Tensor] = []\n        new_vocab: odict[str, int] = odict()\n\n        new_index = -1\n\n        tokens: Iterable[str] = self.vocab.keys()\n\n        if setup_vocab_from_embeddings:\n            tokens = chain(tokens, model.vocab.keys())\n\n        for token in tokens:\n            if token not in new_vocab:\n                if token in model:\n                    embedding_matrix.append(torch.tensor(model[token]))\n                    new_vocab[token] = new_index = new_index + 1\n                elif token in self.specials:\n                    embedding_matrix.append(torch.randn(model.vector_size))\n                    new_vocab[token] = new_index = new_index + 1\n                else:\n                    self.unk_numericals.add(self.vocab[token])\n\n                    if initialize_unknowns:\n                        embedding_matrix.append(torch.randn(model.vector_size))\n                        new_vocab[token] = new_index = new_index + 1\n                    else:\n                        # Collapse all OOV\'s to the same <unk> token id\n                        new_vocab[token] = new_vocab[self.unk]\n\n        return new_vocab, torch.stack(embedding_matrix)\n\n    def setup(self, *data: np.ndarray) -> None:\n        """"""Build the vocabulary and sets embeddings.\n\n        Parameters\n        ----------\n        data : Iterable[str]\n            List of input strings.\n\n        """"""\n        self._build_vocab(*data)\n\n        if self.embeddings_info:\n            model = get_embeddings(self.embeddings_info.embeddings,\n                                   self.embeddings_info.embeddings_format,\n                                   self.embeddings_info.embeddings_binary)\n            self.vocab, self.embedding_matrix = self._build_embeddings(\n                model,\n                self.embeddings_info.build_vocab_from_embeddings,\n                self.embeddings_info.unk_init_all)\n\n    # TODO update when we add generics\n    def process(self, example:  # type: ignore\n                Union[str, Tuple[Any], List[Any], Dict[Any, Any]]) \\\n            -> Union[torch.Tensor, Tuple[torch.Tensor, ...],\n                     List[torch.Tensor], Dict[str, torch.Tensor]]:\n        """"""Process an example, and create a Tensor.\n\n        Parameters\n        ----------\n        example: str\n            The example to process, as a single string\n\n        Returns\n        -------\n        torch.Tensor\n            The processed example, tokenized and numericalized\n\n        """"""\n        # special case of list of examples:\n        if isinstance(example, list) or isinstance(example, tuple):\n            return [self.process(e) for e in example]  # type: ignore\n        elif isinstance(example, dict):\n            return dict([(key, self.process(val)) for key, val in example.items()])  # type: ignore\n\n        # Lowercase and tokenize\n        example = example.lower() if self.lower else example\n        tokens = self.tokenizer(example)\n\n        # Add extra tokens\n        if self.sos is not None:\n            tokens = [self.sos] + list(tokens)\n        if self.eos is not None:\n            tokens = list(tokens) + [self.eos]\n\n        # Numericalize\n        numericals = []\n        for token in tokens:\n            if token not in self.vocab:\n                if self.unk is None or self.unk not in self.vocab:\n                    raise ValueError(""Encounterd out-of-vocabulary token \\\n                                      but the unk_token is either missing \\\n                                      or not defined in the vocabulary."")\n                else:\n                    token = self.unk\n\n            numerical = self.vocab[token]  # type: ignore\n\n            if self.embeddings_info is not None and self.embeddings_info.drop_unknown and \\\n                    numerical in self.unk_numericals:\n                # Don\'t add unknown tokens in case the flag is activated\n                continue\n\n            numericals.append(numerical)\n\n        ret = torch.tensor(numericals).long()\n\n        if self.max_seq_len is not None:\n            if self.truncate_end:\n                ret = ret[-self.max_seq_len:]\n            else:\n                ret = ret[:self.max_seq_len]\n        return ret\n\n    @registrable_factory\n    @classmethod\n    def from_embeddings(\n        cls,\n        embeddings: str,\n        embeddings_format: str = \'glove\',\n        embeddings_binary: bool = False,\n        build_vocab_from_embeddings: bool = False,\n        unk_init_all: bool = False,\n        drop_unknown: bool = False,\n        additional_special_tokens: Optional[List[str]] = None,\n        **kwargs,\n    ):\n        """"""\n        Optional constructor to create TextField from embeddings params.\n\n        Parameters\n        ----------\n        embeddings : Optional[str], optional\n            Path to pretrained embeddings, by default None\n        embeddings_format : str, optional\n            The format of the input embeddings, should be one of:\n            \'glove\', \'word2vec\', \'fasttext\' or \'gensim\'. The latter can\n            be used to download embeddings hosted on gensim on the fly.\n            See https://github.com/RaRe-Technologies/gensim-data\n            for the list of available embedding aliases.\n        embeddings_binary : bool, optional\n            Whether the input embeddings are provided in binary format,\n            by default False\n        build_vocab_from_embeddings: bool\n            Controls if all words from the optional provided\n            embeddings will be added to the vocabulary and to the\n            embedding matrix. Defaults to False.\n        unk_init_all : bool, optional\n            If True, every token not provided in the input embeddings is\n            given a random embedding from a normal distribution.\n            Otherwise, all of them map to the \'<unk>\' token.\n        drop_unknown: bool\n            Whether to drop tokens that don\'t have embeddings\n            associated. Defaults to False.\n            Important: this flag will only work when using embeddings.\n        additional_special_tokens: Optional[List[str]]\n            Additional tokens that have a reserved interpretation in\n            the context of the current experiment, and that should\n            therefore never be treated as ""unknown"".\n            Passing them in here will make sure that they will have\n            their own embedding that can be trained.\n\n        Returns\n        -------\n        TextField\n            The constructed text field with the requested model.\n        """"""\n        embeddings_info = EmbeddingsInformation(\n            embeddings=embeddings,\n            embeddings_format=embeddings_format,\n            embeddings_binary=embeddings_binary,\n            build_vocab_from_embeddings=build_vocab_from_embeddings,\n            unk_init_all=unk_init_all,\n            drop_unknown=drop_unknown\n        )\n\n        return cls(\n            embeddings_info=embeddings_info,\n            additional_special_tokens=additional_special_tokens,\n            **kwargs,\n        )\n'"
flambe/learn/__init__.py,0,"b""from flambe.learn.train import Trainer\nfrom flambe.learn.eval import Evaluator\nfrom flambe.learn.script import Script\nfrom flambe.learn.distillation import DistillationTrainer\n\n\n__all__ = ['Trainer', 'Evaluator', 'Script', 'DistillationTrainer']\n"""
flambe/learn/distillation.py,17,"b'from typing import Optional, Tuple, List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\n\nfrom flambe.dataset import Dataset\nfrom flambe.metric import Metric\nfrom flambe.sampler import Sampler\nfrom flambe.learn import Trainer\nfrom flambe.nn import Module  # type: ignore[attr-defined]\n\n\nclass DistillationTrainer(Trainer):\n    """"""Implement a Distillation Trainer.\n\n    Perform knowledge distillation between a teacher and a student\n    model. Note that the model outputs are expected to be raw logits.\n    Make sure that you are not applying a softmax after the decoder.\n    You can replace the traditional Decoder with a MLPEncoder.\n\n    """"""\n\n    def __init__(self,\n                 dataset: Dataset,\n                 train_sampler: Sampler,\n                 val_sampler: Sampler,\n                 teacher_model: Module,\n                 student_model: Module,\n                 loss_fn: Metric,\n                 metric_fn: Metric,\n                 optimizer: Optimizer,\n                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n                 max_steps: int = 10,\n                 epoch_per_step: float = 1.0,\n                 iter_per_step: Optional[int] = None,\n                 batches_per_iter: int = 1,\n                 lower_is_better: bool = False,\n                 max_grad_norm: Optional[float] = None,\n                 max_grad_abs_val: Optional[float] = None,\n                 extra_validation_metrics: Optional[List[Metric]] = None,\n                 teacher_columns: Optional[Tuple[int, ...]] = None,\n                 student_columns: Optional[Tuple[int, ...]] = None,\n                 alpha_kl: float = 0.5,\n                 temperature: int = 1,\n                 unlabel_dataset: Optional[Dataset] = None,\n                 unlabel_sampler: Optional[Sampler] = None) -> None:\n        """"""Initialize the Trainer.\n\n        Parameters\n        ----------\n        dataset: Dataset\n            The dataset containing the first N columns of data for the\n            student model, and the last N columns for the target.\n        train_sampler : Sampler\n            The sampler to use over training examples\n        val_sampler : Sampler\n            The sampler to use over validation examples\n        model : Module\n            The model to train\n        optimizer : torch.optim.Optimizer\n            The optimizer to use\n        scheduler : torch.optim.lr_scheduler._LRScheduler, optional\n            An optional learning rate scheduler\n        iter_scheduler : torch.optim.lr_scheduler._LRScheduler, optional\n            An optional learning rate scheduler to run after each batch\n            (i.e iteration)\n        device: str, optional\n            The device to use in the computation. Only used by compile.\n        max_steps : int, optional\n            The maximum number of training steps to run\n        epoch_per_step : float, optional\n            Fraction of an epoch to perform in a single training step\n            (i.e before a checkpoint.) Defaults to 1.\n            Overriden by `iter_per_step`, if given.\n        iter_per_step : int, optional\n            Number of iterations to perform in a single training step.\n            Overrides `epoch_per_step` if given.\n        batches_per_iter : int, optional\n            Number of batches to pass through the model before\n            calling optimizer.step. Requires the sampler to have\n            drop_last set to True. (default set to 1 so optimizer.step\n            is called after every batch)\n        lower_is_better : bool, optional\n            If true, the lowest dev metric is considered best,\n            otherwise the highest. Defaults to False.\n        max_grad_norm : float, optional\n            Maximum Euclidean norm of gradient after clipping.\n        max_grad_abs_val: float, optional\n            Maximum absolute value of all gradient vector components\n            after clipping.\n        extra_validation_metrics: Optional[List[Metric]]\n            A list with extra metrics to show in each step\n            but which don\'t guide the training procedures\n            (i.e model selection through early stopping)\n        alpha_kl: float, optional\n            Weight applied to the distillation loss.\n        temperature: int, optional\n            The temperature applied to the logits\n        unlabel_dataset: Dataset, optional\n            Optional dataset of unlabel data\n        unlabel_sampler: Sampler, optional\n            Optional sampler over unlabel examples\n\n        """"""\n        super().__init__(dataset,\n                         train_sampler,  # type: ignore\n                         val_sampler,\n                         student_model,\n                         loss_fn,\n                         metric_fn,\n                         optimizer,\n                         scheduler,\n                         iter_scheduler,\n                         device,\n                         max_steps,\n                         epoch_per_step,\n                         iter_per_step,\n                         batches_per_iter,\n                         lower_is_better,\n                         max_grad_norm,\n                         max_grad_abs_val,\n                         extra_validation_metrics)\n\n        self.student_model = self.model\n        self.teacher_model = teacher_model\n\n        self.teacher_columns = teacher_columns\n        self.student_columns = student_columns\n\n        self.alpha_kl = alpha_kl\n        self.temp = temperature\n\n        self.unlabel_dataset = None\n        self.unlabel_sampler = None\n        if unlabel_sampler is not None and unlabel_dataset is not None:\n            self.unlabel_sampler = unlabel_sampler\n            self._unlabel_iterator = unlabel_sampler.sample(unlabel_dataset.train, -1)\n\n    def _compute_loss(self, batch: Tuple[torch.Tensor, ...]) -> torch.Tensor:\n        """"""Compute the loss for a single batch\n\n        Important: the student and teacher output predictions must\n        be the raw logits, so ensure that your decoder object is step\n        with `take_log=False`.\n\n        Parameters\n        ----------\n        batch: Tuple[torch.Tensor, ...]\n            The batch to train on\n\n        Returns\n        -------\n        torch.Tensor\n            The computed loss\n\n        """"""\n        student_columns = self.student_columns or range(len(batch))\n        teacher_columns = self.teacher_columns or range(len(batch))\n\n        student_batch = [batch[i] for i in student_columns]\n        teacher_batch = [batch[i].detach() for i in teacher_columns]\n\n        student_logits, student_target = self.student_model(*student_batch)\n\n        with torch.no_grad():\n            teacher_logits, _ = self.teacher_model(*teacher_batch)\n\n        loss = torch.tensor(0.).to(self.device)\n        student_pred = F.log_softmax(student_logits, dim=-1)\n\n        if self.alpha_kl < 1.0:\n            loss += (1 - self.alpha_kl) * self.loss_fn(student_pred, student_target)\n\n        # Add unlabelled batch\n        if self.unlabel_sampler is not None:\n            # Get next batch\n            unlabelled, = next(self._unlabel_iterator)\n\n            student_unlabel_logits = self.student_model(unlabelled)\n            teacher_unlabel_logits = self.teacher_model(unlabelled.detach())\n            student_logits = torch.cat((student_logits, student_unlabel_logits))\n            teacher_logits = torch.cat((teacher_logits, teacher_unlabel_logits))\n\n        student_pred = F.log_softmax(student_logits / self.temp, dim=1)\n        teacher_pred = F.softmax(teacher_logits / self.temp, dim=1)\n\n        kl_loss = F.kl_div(student_pred, teacher_pred, size_average=False) / teacher_pred.shape[0]\n        loss += (self.alpha_kl * self.temp**2) * kl_loss\n\n        return loss\n\n    def _aggregate_preds(self, data_iterator) -> Tuple[torch.Tensor, torch.Tensor, float]:\n        """"""Aggregate the predicitons and targets for the dataset.\n\n        Parameters\n        ----------\n        data_iterator: Iterator\n            Batches of data\n\n        Returns\n        -------\n        Tuple[torch.tensor, torch.tensor]\n            The predictions, and targets\n\n        """"""\n        preds, targets = [], []\n\n        for batch in data_iterator:\n            student_columns = self.student_columns or range(len(batch))\n            student_batch = [batch[i] for i in student_columns]\n\n            pred, target = self.model(*[t.to(self.device) for t in student_batch])\n            pred = F.log_softmax(pred, dim=-1)\n\n            preds.append(pred.cpu())\n            targets.append(target.cpu())\n\n        preds = torch.cat(preds, dim=0)\n        targets = torch.cat(targets, dim=0)\n        return preds, targets, 0.\n'"
flambe/learn/eval.py,1,"b'from typing import Optional, Dict, Union\n\nimport torch\n\nfrom flambe.compile import Component\nfrom flambe.dataset import Dataset\nfrom flambe.learn.utils import select_device\nfrom flambe.nn import Module  # type: ignore[attr-defined]\nfrom flambe.metric import Metric\nfrom flambe.sampler import Sampler, BaseSampler\nfrom flambe.logging import log\n\n\nclass Evaluator(Component):\n    """"""Implement an Evaluator block.\n\n    An `Evaluator` takes as input data, and a model and executes\n    the evaluation. This is a single step `Component` object.\n\n    """"""\n\n    def __init__(self,\n                 dataset: Dataset,\n                 model: Module,\n                 metric_fn: Metric,\n                 eval_sampler: Optional[Sampler] = None,\n                 eval_data: str = \'test\',\n                 device: Optional[str] = None) -> None:\n        """"""Initialize the evaluator.\n\n        Parameters\n        ----------\n        dataset : Dataset\n            The dataset to run evaluation on\n        model : Module\n            The model to train\n        metric_fn: Metric\n            The metric to use for evaluation\n        eval_sampler : Optional[Sampler]\n            The sampler to use over validation examples. By default\n            it will use `BaseSampler` with batch size 16 and without\n            shuffling.\n        eval_data: str\n            The data split to evaluate on: one of train, val or test\n        device: str, optional\n            The device to use in the computation.\n\n        """"""\n        self.eval_sampler = eval_sampler or BaseSampler(batch_size=16, shuffle=False)\n        self.model = model\n        self.metric_fn = metric_fn\n        self.dataset = dataset\n\n        self.device = select_device(device)\n\n        data = getattr(dataset, eval_data)\n        self._eval_iterator = self.eval_sampler.sample(data)\n\n        # By default, no prefix applied to tb logs\n        self.tb_log_prefix = None\n\n        self.eval_metric: Union[float, None] = None\n        self.register_attrs(\'eval_metric\')\n\n    def run(self, block_name: str = None) -> bool:\n        """"""Run the evaluation.\n\n        Returns\n        ------\n        bool\n            Whether the component should continue running.\n\n        """"""\n        self.model.to(self.device)\n        self.model.eval()\n        with torch.no_grad():\n            metric_state: Dict = {}\n\n            for batch in self._eval_iterator:\n                pred, target = self.model(*[t.to(self.device) for t in batch])\n                self.metric_fn.aggregate(metric_state, pred, target)\n\n            self.eval_metric = self.metric_fn.finalize(metric_state)\n\n            tb_prefix = f""{self.tb_log_prefix} "" if self.tb_log_prefix else """"\n\n            log(f\'{tb_prefix}Eval/{self.metric_fn}\',  # type: ignore\n                self.eval_metric, global_step=0)  # type: ignore\n\n        return False\n\n    def metric(self) -> Optional[float]:\n        """"""Override this method to enable scheduling.\n\n        Returns\n        -------\n        float\n            The metric to compare computable varients\n\n        """"""\n        return self.eval_metric\n'"
flambe/learn/script.py,0,"b'from typing import Any, Dict, Optional, List\nimport sys\nimport runpy\nfrom copy import deepcopy\n\nfrom flambe.logging import get_trial_dir\nfrom flambe.compile import Component\n\n\nclass Script(Component):\n    """"""Implement a Script computable.\n\n    The obejct can be used to turn any script into a Flamb\xc3\xa9 computable.\n    This is useful when you want to rapidly integrate code. Note\n    however that this computable does not enable checkpointing or\n    linking to internal components as it does not have any attributes.\n\n    To use this object, your script needs to be in a pip installable,\n    containing all dependencies. The script is run with the following\n    command:\n\n    .. code-block:: bash\n\n        python -m script.py --arg1 value1 --arg2 value2\n\n    """"""\n\n    def __init__(self,\n                 script: str,\n                 args: List[Any],\n                 kwargs: Optional[Dict[str, Any]] = None,\n                 output_dir_arg: Optional[str] = None) -> None:\n        """"""Initialize a Script.\n\n        Parameters\n        ----------\n        path: str\n            The script module\n        args: List[Any]\n            Argument List\n        kwargs: Optional[Dict[str, Any]]\n            Keyword argument dictionary\n        output_dir_arg: str, optional\n            The name of the argument corresponding to the output\n            directory, should there be one.\n\n        """"""\n        self.script = script\n        self.args = args\n        if kwargs is None:\n            self.kwargs: Dict[str, Any] = {}\n        else:\n            self.kwargs = kwargs\n\n        if output_dir_arg is not None:\n            self.kwargs[output_dir_arg] = get_trial_dir()\n\n    def run(self) -> bool:\n        """"""Run the evaluation.\n\n        Returns\n        -------\n        Dict[str, float]\n            Report dictionary to use for logging\n\n        """"""\n        parser_kwargs = {f\'--{k}\': v for k, v in self.kwargs.items()}\n        # Flatten the arguments into a single list to pass to sys.argv\n        parser_args_flat = [str(item) for item in self.args]\n        parser_args_flat += [str(item) for items in parser_kwargs.items() for item in items]\n\n        sys_save = deepcopy(sys.argv)\n        sys.argv = [\'\'] + parser_args_flat  # add dummy sys[0]\n        runpy.run_module(self.script, run_name=\'__main__\', alter_sys=True)\n        sys.argv = sys_save\n\n        continue_ = False  # Single step, so don\'t continue\n        return continue_\n'"
flambe/learn/train.py,22,"b'import math\nfrom typing import Dict, List, Optional, Any, Tuple, Iterator, Iterable, Union\n\nimport numpy as np\nimport torch\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler, ReduceLROnPlateau\nfrom torch.nn.utils.clip_grad import clip_grad_norm_, clip_grad_value_\n\nfrom flambe.dataset import Dataset\nfrom flambe.compile import Schema, State, Component, Link\nfrom flambe.learn.utils import select_device\nfrom flambe.nn import Module  # type: ignore[attr-defined]\nfrom flambe.sampler import Sampler\nfrom flambe.metric import Metric\nfrom flambe.logging import log\n\n\nclass Trainer(Component):\n    """"""Implement a Trainer block.\n\n    A `Trainer` takes as input data, model and optimizer,\n    and executes training incrementally in `run`.\n\n    Note that it is important that a trainer run be long enough\n    to not increase overhead, so at least a few seconds, and ideally\n    multiple minutes.\n\n    """"""\n\n    def __init__(self,\n                 dataset: Dataset,\n                 train_sampler: Sampler,\n                 val_sampler: Sampler,\n                 model: Module,\n                 loss_fn: Metric,\n                 metric_fn: Metric,\n                 optimizer: Optimizer,\n                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n                 max_steps: int = 10,\n                 epoch_per_step: float = 1.0,\n                 iter_per_step: Optional[int] = None,\n                 batches_per_iter: int = 1,\n                 lower_is_better: bool = False,\n                 max_grad_norm: Optional[float] = None,\n                 max_grad_abs_val: Optional[float] = None,\n                 extra_validation_metrics: Optional[Iterable[Metric]] = None,\n                 extra_training_metrics: Optional[Iterable[Metric]] = None,\n                 extra_training_metrics_log_interval: Optional[int] = None) \\\n            -> None:\n        """"""Initialize an instance of Trainer\n\n        Parameters\n        ----------\n        dataset : Dataset\n            The dataset to use in training the model\n        train_sampler : Sampler\n            The sampler to use over training examples during training\n        val_sampler : Sampler\n            The sampler to use over validation examples\n        model : Module\n            The model to train\n        loss_fn: Metric\n            The loss function to use in training the model\n        metric_fn: Metric\n            The metric function to use in evaluation\n        optimizer : torch.optim.Optimizer\n            The optimizer to use\n        scheduler : torch.optim.lr_scheduler._LRScheduler, optional\n            An optional learning rate scheduler to run after each step\n        iter_scheduler : torch.optim.lr_scheduler._LRScheduler, optional\n            An optional learning rate scheduler to run after each batch\n            (i.e iteration)\n        device: str, optional\n            The device to use in the computation.\n        max_steps : int, optional\n            The maximum number of training steps to run\n        epoch_per_step : float, optional\n            Fraction of an epoch to perform in a single training step\n            (i.e before a checkpoint.) Defaults to 1.\n            Overridden by `iter_per_step`, if given.\n        iter_per_step : int, optional\n            Number of iterations to perform in a single training step.\n            Overrides `epoch_per_step` if given.\n        batches_per_iter : int, optional\n            Number of batches to pass through the model before\n            calling optimizer.step. Requires the sampler to have\n            drop_last set to True. (default set to 1 so optimizer.step\n            is called after every batch)\n        lower_is_better : bool, optional\n            If true, the lowest val metric is considered best,\n            otherwise the highest. Defaults to False.\n        max_grad_norm : float, optional\n            Maximum Euclidean norm of gradient after clipping.\n        max_grad_abs_val: float, optional\n            Maximum absolute value of all gradient vector components\n            after clipping.\n        extra_validation_metrics: Optional[Iterable[Metric]]\n            A dict with extra metrics to show in each step\n            but which don\'t guide the training procedures\n            (i.e model selection through early stopping)\n            The key of the metric will be used for displaying\n            the values in tensorboard. Only logged during eval\n        extra_training_metrics: Optional[Iterable[Metric]]\n            A dict with extra metrics to show in each step\n            but which don\'t guide the training procedures\n            (i.e model selection through early stopping)\n            The key of the metric will be used for displaying\n            the values in tensorboard. Only logged during train\n        extra_training_metrics_log_interval: Optional[int]\n            The interval during training to log the\n            extra_training_metrics.\n            Set to None to disable and only log at the end of an epoch\n            For eval, the interval is _always_ the entire epoch.\n        """"""\n        self.dataset = dataset\n        self.train_sampler = train_sampler\n        self.val_sampler = val_sampler\n        self.model = model\n        self.loss_fn = loss_fn\n        self.metric_fn = metric_fn\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.iter_scheduler = iter_scheduler\n        self.lower_is_better = lower_is_better\n        self.max_grad_norm = max_grad_norm\n        self.max_grad_abs_val = max_grad_abs_val\n        self.extra_validation_metrics = extra_validation_metrics if \\\n            extra_validation_metrics is not None else []\n        self.training_metrics = extra_training_metrics if \\\n            extra_training_metrics is not None else []\n        self.extra_training_metrics_log_interval = -1 \\\n            if extra_training_metrics_log_interval is None \\\n            else extra_training_metrics_log_interval\n\n        # By default, no prefix applied to tb logs\n        self.tb_log_prefix = None\n\n        # Select right device\n        self.device = select_device(device)\n\n        if (not getattr(self.train_sampler, \'drop_last\', False) and batches_per_iter != 1):\n            raise ValueError(f\'batches_per_iter cannot be set to {batches_per_iter} \'\n                             \'if the sampler does not have `drop_last` set to True\')\n\n        self.batches_per_iter = batches_per_iter\n        n_batches = self.train_sampler.length(dataset.train)\n\n        if iter_per_step is None:\n            # Compute epoch per step\n            if self.batches_per_iter > n_batches:\n                raise Exception(f\'Please set batches_per_iter ({self.batches_per_iter}) \'\n                                f\'to be \xe2\x89\xa4 the length of your train_sampler \'\n                                f\'({n_batches})\')\n            iter_per_epoch = n_batches // self.batches_per_iter\n            iter_per_step = math.ceil(epoch_per_step * iter_per_epoch)\n        else:\n            # Iter per step takes precedent over epoch_per_step\n            epoch_per_step = iter_per_step / n_batches\n\n        self.iter_per_step = iter_per_step\n        self.max_steps = max_steps\n\n        self._step = 0\n        self._best_metric: Union[float, None] = None\n        self._last_train_log_step = 0\n        self._best_model: Dict[str, torch.Tensor] = dict()\n        self.register_attrs(\'_step\', \'_best_metric\', \'_best_model\')\n\n        self.n_epochs = math.ceil(epoch_per_step * max_steps)\n\n        self._create_train_iterator()\n\n    @property\n    def validation_metrics(self):\n        """"""Adding property for backwards compatibility""""""\n        return self.extra_validation_metrics\n\n    def _create_train_iterator(self):\n        self._train_iterator = self.train_sampler.sample(self.dataset.train, self.n_epochs)\n\n    def _batch_to_device(self, batch: Tuple[torch.Tensor, ...]) -> Tuple[torch.Tensor, ...]:\n        """"""Move the current batch on the correct device.\n\n        Can be overriden if a batch doesn\'t follow the expected\n        structure. For example if the batch is a dictionary.\n\n        Parameters\n        ----------\n        batch: Tuple[torch.Tensor, ...]\n            The batch to train on.\n\n        """"""\n        batch = tuple(t.to(self.device) for t in batch)\n        return batch\n\n    def _compute_loss(self, batch: Tuple[torch.Tensor, ...]) -> torch.Tensor:\n        """"""Compute the loss given a single batch\n        DEPRECATED, only exists for legacy compatibility with custom\n        trainers\n\n        Parameters\n        ----------\n        batch: Tuple[torch.Tensor, ...]\n            The batch to train on.\n\n        """"""\n        print(\'Warning: flambe.learn.train.Trainer._compute_loss is deprecated. \'\n              \'Please use flambe.learn.train.Trainer._compute_batch in the future.\')\n        batch = self._batch_to_device(batch)\n        pred, target = self.model(*batch)\n        loss = self.loss_fn(pred, target)\n        return loss\n\n    def _compute_batch(self, batch: Tuple[torch.Tensor, ...],\n                       metrics: List[Tuple] = []) \\\n            -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        """""" Computes a batch.\n\n        Does a model forward pass over a batch, and returns prediction,\n        target and loss.\n\n        Parameters\n        ----------\n        batch: Tuple[torch.Tensor, ...]\n            The batch to train on.\n\n        """"""\n        batch = self._batch_to_device(batch)\n        pred, target = self.model(*batch)\n        for metric, state in metrics:\n            metric.aggregate(state, pred, target)\n        loss = self.loss_fn(pred, target)\n        return pred, target, loss\n\n    @staticmethod\n    def _log_metrics(log_prefix: str,\n                     metrics_with_states: List[Tuple],\n                     global_step: int) -> None:\n        """"""Logs all provided metrics\n\n        Iterates through the provided list of metrics with states,\n        finalizes the metric, and logs it.\n\n        Parameters\n        ----------\n        log_prefix: str\n            A string, such as a tensorboard prefix\n        metrics_with_states: List[Tuple[Metric, Dict]]\n            a list of metric-state tuples\n        global_step: int\n            the global step for loggin\n        """"""\n        for metric, state in metrics_with_states:\n            log(f\'{log_prefix}/{metric}\', metric.finalize(state), global_step)\n\n    def _train_step(self) -> None:\n        """"""Run a training step over the training data.""""""\n        self.model.train()\n        metrics_with_states: List[Tuple] = [(metric, {}) for metric in self.training_metrics]\n        self._last_train_log_step = 0\n\n        log_prefix = f""{self.tb_log_prefix} "" if self.tb_log_prefix else """"\n        log_prefix += \'Training\'\n\n        with torch.enable_grad():\n            for i in range(self.iter_per_step):\n                # Zero the gradients and clear the accumulated loss\n                self.optimizer.zero_grad()\n                accumulated_loss = 0.0\n                for _ in range(self.batches_per_iter):\n                    # Get next batch\n                    try:\n                        batch = next(self._train_iterator)\n                    except StopIteration:\n                        self._create_train_iterator()\n                        batch = next(self._train_iterator)\n                    batch = self._batch_to_device(batch)\n\n                    # Compute loss\n                    _, _, loss = self._compute_batch(batch, metrics_with_states)\n                    accumulated_loss += loss.item() / self.batches_per_iter\n                    loss.backward()\n\n                # Log loss\n                global_step = (self.iter_per_step * self._step) + i\n\n                # Clip gradients if necessary\n                if self.max_grad_norm:\n                    clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n                if self.max_grad_abs_val:\n                    clip_grad_value_(self.model.parameters(), self.max_grad_abs_val)\n\n                log(f\'{log_prefix}/Loss\', accumulated_loss, global_step)\n                log(f\'{log_prefix}/Gradient_Norm\', self.model.gradient_norm,\n                    global_step)\n                log(f\'{log_prefix}/Parameter_Norm\', self.model.parameter_norm,\n                    global_step)\n\n                # Optimize\n                self.optimizer.step()\n\n                # Log the learning rate\n                if self.iter_scheduler is not None or self.scheduler is not None:\n                    lr = self.optimizer.param_groups[0][\'lr\']  # type: ignore\n                    log(f\'{log_prefix}/LR\', lr, global_step)\n\n                # Update iter scheduler\n                if self.iter_scheduler is not None:\n                    self.iter_scheduler.step()  # type: ignore\n\n                # Zero the gradients when exiting a train step\n                self.optimizer.zero_grad()\n                # logging train metrics\n                if self.extra_training_metrics_log_interval > self._last_train_log_step:\n                    self._log_metrics(log_prefix, metrics_with_states, global_step)\n                    self._last_train_log_step = i\n            if self._last_train_log_step != i:\n                # log again at end of step, if not logged at the end of\n                # step before\n                self._log_metrics(log_prefix, metrics_with_states, global_step)\n\n    def _aggregate_preds(self, data_iterator: Iterator) \\\n            -> Tuple[torch.Tensor, torch.Tensor, float]:\n        """""" DEPRECATED\n        Aggregate the predicitons,\n        targets and mean loss for the dataset.\n\n        Parameters\n        ----------\n        data_iterator: Iterator\n            Batches of data.\n\n        Returns\n        -------\n        Tuple[torch.tensor, torch.tensor, float]\n            The predictions, targets and mean loss.\n\n        DEPRECATED; only existed to aggregate for the metric functions.\n        The metric functions do this in-place now.\n\n        """"""\n        preds, targets, loss = [], [], []\n        for batch in data_iterator:\n            pred, target, batch_loss = self._compute_batch(batch)\n            loss.append(batch_loss.item())\n            preds.append(pred.cpu())\n            targets.append(target.cpu())\n        loss = sum(loss) / len(loss)\n        preds = torch.cat(preds, dim=0)\n        targets = torch.cat(targets, dim=0)\n        return preds, targets, loss\n\n    def _eval_step(self) -> None:\n        """"""Run an evaluation step over the validation data.""""""\n        self.model.eval()\n        metric_fn_state: Dict[Metric, Dict] = {}\n        metrics_with_states: List[Tuple] = \\\n            [(metric, {}) for metric in self.extra_validation_metrics]\n\n        # Initialize a 1-epoch iteration through the validation set\n        val_iterator = self.val_sampler.sample(self.dataset.val)\n\n        with torch.no_grad():\n            loss = []\n            for batch in val_iterator:\n                _, _, batch_loss = self._compute_batch(\n                    batch, [(self.metric_fn, metric_fn_state), *metrics_with_states])\n                loss.append(batch_loss.item())\n            val_loss = np.NaN if loss == [] else sum(loss) / len(loss)\n            val_metric = self.metric_fn.finalize(metric_fn_state)\n\n        # Update best model\n        sign = (-1)**(self.lower_is_better)\n        if self._best_metric is None or (sign * val_metric > sign * self._best_metric):\n            self._best_metric = val_metric\n            best_model_state = self.model.state_dict()\n            for k, t in best_model_state.items():\n                best_model_state[k] = t.cpu().detach()\n            self._best_model = best_model_state\n\n        # Update scheduler\n        if self.scheduler is not None:\n            if isinstance(self.scheduler, ReduceLROnPlateau):\n                self.scheduler.step(val_loss)\n            else:\n                # torch\'s _LRScheduler.step DOES have a default value\n                # so passing in no args is fine; it will automatically\n                # compute the current epoch\n                self.scheduler.step()  # type: ignore\n\n        tb_prefix = f""{self.tb_log_prefix} "" if self.tb_log_prefix else """"\n\n        # Log metrics\n        log(f\'{tb_prefix}Validation/Loss\', val_loss, self._step)\n        log(f\'{tb_prefix}Validation/{self.metric_fn}\', val_metric, self._step)\n        log(f\'{tb_prefix}Best/{self.metric_fn}\',\n            self._best_metric, self._step)  # type: ignore\n        for (metric, state) in metrics_with_states:\n            log(f\'{tb_prefix}Validation/{metric}\',\n                metric.finalize(state), self._step)  # type: ignore\n\n    def run(self) -> bool:\n        """"""Evaluate and then train until the next checkpoint\n\n        Returns\n        ------\n        bool\n            Whether the component should continue running.\n\n        """"""\n        self._eval_step()\n        if self._step < self.max_steps:\n            self._train_step()\n\n        # Simple stopping rule, if we exceed the max number of steps\n        self._step += 1\n        continue_ = self._step < self.max_steps\n        if not continue_:\n            self._eval_step()\n            self.model.cpu()\n            self.model.load_state_dict(self._best_model, strict=False)\n\n        return continue_\n\n    def metric(self) -> Optional[float]:\n        """"""Override this method to enable scheduling.\n\n        Returns\n        -------\n        float\n            The metric to compare computable variants.\n\n        """"""\n        return self._best_metric\n\n    def _state(self,\n               state_dict: State,\n               prefix: str,\n               local_metadata: Dict[str, Any]) -> State:\n        state_dict[prefix + \'optimizer\'] = self.optimizer.state_dict()\n        if self.scheduler is not None:\n            state_dict[prefix + \'scheduler\'] = self.scheduler.state_dict()\n        return state_dict\n\n    def _load_state(self,\n                    state_dict: State,\n                    prefix: str,\n                    local_metadata: Dict[str, Any],\n                    strict: bool,\n                    missing_keys: List[Any],\n                    unexpected_keys: List[Any],\n                    error_msgs: List[Any]) -> None:\n        self.optimizer.load_state_dict(state_dict[prefix + \'optimizer\'])\n        if self.scheduler is not None:\n            self.scheduler.load_state_dict(state_dict[prefix + \'scheduler\'])\n        # Useful when loading the model after training\n        done = self._step >= self.max_steps\n        if done:\n            self.model.load_state_dict(self._best_model, strict=False)\n\n    @classmethod\n    def precompile(cls, **kwargs):\n        """"""Override initialization.\n\n        Ensure that the model is compiled and pushed to the right\n        device before its parameters are passed to the optimizer.\n\n        """"""\n        # Select right device\n        device = kwargs.get(\'device\', None)\n        if device is None:\n            device = ""cuda"" if torch.cuda.is_available() else ""cpu""\n\n        def move_to_device(obj: Any):\n            if isinstance(obj, torch.nn.Module):\n                obj.to(device)\n\n        # Compile all objects and push Modules to the device\n        for k, obj in kwargs.items():\n            if isinstance(obj, (Schema, Link)):\n                obj.post_init_hooks.append(move_to_device)\n'"
flambe/learn/utils.py,2,"b'from typing import Optional\n\nimport torch\n\n\ndef select_device(device: Optional[str]) -> str:\n    """"""\n    Chooses the torch device to run in.\n\n     Parameters\n       ------------\n     device: Union[torch.device, str]\n         A device or a string representing a device, such as \'cpu\'\n\n     Returns\n     ---------\n     str\n         the passed-as-parameter device if any, otherwise\n         cuda if available. Last option is cpu.\n    """"""\n\n    if device is not None:\n        return device\n    else:\n        return ""cuda"" if torch.cuda.is_available() else ""cpu""\n'"
flambe/logging/__init__.py,0,"b""from flambe.logging.logging import TrialLogging, setup_global_logging\nfrom flambe.logging.datatypes import ScalarT, ScalarsT, HistogramT, TextT, ImageT, PRCurveT\nfrom flambe.logging.datatypes import EmbeddingT, GraphT\nfrom flambe.logging.utils import log, coloredlogs\nfrom flambe.logging.utils import log_scalar, log_scalars, log_text, log_image, log_histogram\nfrom flambe.logging.utils import log_pr_curve, get_trial_dir\n\n\n__all__ = ['ScalarT', 'ScalarsT', 'HistogramT', 'TextT', 'ImageT', 'EmbeddingT', 'GraphT',\n           'PRCurveT', 'TrialLogging', 'setup_global_logging', 'log', 'coloredlogs', 'log_scalar',\n           'log_scalars', 'log_text', 'log_image', 'log_histogram', 'log_pr_curve', 'get_trial_dir']\n"""
flambe/logging/datatypes.py,15,"b'import time\nimport logging\nfrom typing import Any, Sequence, NamedTuple, Union, Dict, Tuple, Optional\n\nimport torch\nimport numpy\n\n# Officially Supported Logging Data Types\n\n\nclass ScalarT(NamedTuple):\n    """"""A single scalar value\n\n    Supported by TensorboardX\n\n    Parameters\n    ----------\n    tag: str\n        Data identifier\n    scalar_value: float\n        The scalar value\n    global_step: int\n        Iteration associated with this value\n    walltime: float = time.time()\n        Wall clock time associated with this value\n\n    """"""\n\n    tag: str\n    scalar_value: float\n    global_step: int\n    walltime: float = time.time()\n\n    def __repr__(self) -> str:\n        return f\'{self.tag}#{self.global_step} = {self.scalar_value} \' \\\n               + f\'({time.strftime(""%H:%M:%S"", time.localtime(self.walltime))})\'\n\n\nclass ScalarsT(NamedTuple):\n    """"""A dictionary mapping tag keys to scalar values\n\n    Supported by TensorboardX\n\n    Parameters\n    ----------\n    main_tag: str\n        Parent name for all the children tags\n    tag_scalar_dict: Dict[str, float]\n        Mapping from scalar tags to their values\n    global_step: int\n        Iteration associated with this value\n    walltime: float = time.time()\n        Wall clock time associated with this value\n\n    """"""\n\n    main_tag: str\n    tag_scalar_dict: Dict[str, float]\n    global_step: int\n    walltime: float = time.time()\n\n    def __repr__(self) -> str:\n        return f\'{self.main_tag}#{self.global_step} = {self.tag_scalar_dict} \' \\\n               + f\'({time.strftime(""%H:%M:%S"", time.localtime(self.walltime))})\'\n\n\nclass HistogramT(NamedTuple):\n    """"""A histogram with an array of values\n\n    Supported by TensorboardX\n\n    Parameters\n    ----------\n    tag: str\n        Data identifier\n    values: Union[torch.Tensor, numpy.array]\n        Values to build histogram\n    global_step: int\n        Iteration associated with this value\n    bins: str\n        Determines how bins are made\n    walltime: float = time.time()\n        Wall clock time associated with this value\n\n    """"""\n\n    tag: str\n    values: Union[torch.Tensor, numpy.array]\n    global_step: int\n    bins: str  # https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\n    walltime: float = time.time()\n\n    def __repr__(self) -> str:\n        return f\'{self.tag}#{self.global_step} = {self.values} \' \\\n               + f\'({time.strftime(""%H:%M:%S"", time.localtime(self.walltime))})\'\n\n\nclass ImageT(NamedTuple):\n    """"""Image message\n\n    Supported by TensorboardX\n\n    Parameters\n    ----------\n    tag: str\n        Data identifier\n    img_tensor: Union\n        Image tensor to record\n    global_step: int\n        Iteration associated with this value\n    walltime: float\n        Wall clock time associated with this value\n\n    """"""\n\n    tag: str\n    img_tensor: Union[torch.Tensor, numpy.array]\n    global_step: int\n    walltime: float\n\n    def __repr__(self) -> str:\n        return f\'{self.tag}#{self.global_step} = {self.img_tensor} \' \\\n               + f\'({time.strftime(""%H:%M:%S"", time.localtime(self.walltime))})\'\n\n\nclass TextT(NamedTuple):\n    """"""Text message\n\n    Supported by TensorboardX\n\n    Parameters\n    ----------\n    tag: str\n        Data identifier\n    text_string: str\n        String to record\n    global_step: int\n        Iteration associated with this value\n    walltime: float\n        Wall clock time associated with this value\n\n    """"""\n\n    tag: str\n    text_string: str\n    global_step: int\n    walltime: float\n\n    def __repr__(self) -> str:\n        return f\'{self.tag}#{self.global_step} = {self.text_string} \' \\\n               + f\'({time.strftime(""%H:%M:%S"", time.localtime(self.walltime))})\'\n\n\nclass PRCurveT(NamedTuple):\n    """"""PRCurve message\n\n    Supported by TensorboardX\n\n    Parameters\n    ----------\n    tag: str\n        Data identifier\n    labels: Union[torch.Tensor, numpy.array]\n        Containing 0, 1 values\n    predictions: Union[torch.Tensor, numpy.array]\n        Containing 0<=x<=1 values. Needs to match labels size\n    num_thresholds: int = 127\n        The number of thresholds to evaluate. Max value allowed 127.\n    weights: Optional[float] = None\n        No description provided.\n    global_step: int\n        Iteration associated with this value\n    walltime: float\n        Wall clock time associated with this value\n\n    """"""\n    tag: str\n    labels: Union[torch.Tensor, numpy.array]\n    predictions: Union[torch.Tensor, numpy.array]\n    global_step: int\n    num_thresholds: int = 127\n    weights: Optional[float] = None\n    walltime: float = time.time()\n\n    def __repr__(self) -> str:\n        return f\'{self.tag}#{self.global_step} = ... \' \\\n               + f\'({time.strftime(""%H:%M:%S"", time.localtime(self.walltime))})\'\n\n\nclass EmbeddingT(NamedTuple):\n    """"""Embedding data, including array of vaues and metadata\n\n    Supported by TensorboardX\n\n    Parameters\n    ----------\n    mat: Union[torch.Tensor, numpy.array]\n        A matrix where each row is the feature vector of a data point\n    metadata: Sequence[str]\n        A list of labels; each element will be converted to string\n    label_img: torch.Tensor\n        Images corresponding to each data point\n    global_step: int\n        Iteration associated with this value\n    tag: str\n        Data identifier\n    metadata_header: Sequence[str]\n\n    Shape\n    -----\n    mat: :math:`(N, D)`\n        where N is number of data and D is feature dimension\n    label_img: :math:`(N, C, H, W)`\n\n    """"""\n\n    mat: Union[torch.Tensor, numpy.array]\n    metadata: Sequence[str]\n    label_img: torch.Tensor\n    global_step: int\n    tag: str\n    metadata_header: Sequence[str]\n\n    def __repr__(self) -> str:\n        return f\'{self.tag}#{self.global_step} ... ({self.metadata})\'\n\n\nclass GraphT(NamedTuple):\n    """"""PyTorch Model with input and other keyword args\n\n    Supported by ModelSave\n    NOT YET Supported by TensorboardX\n\n    Attributes\n    ----------\n    model: torch.nn.Module\n        PyTorch Model (should have `forward` and `state_dict` methods)\n    input_to_model: torch.autograd.Variable\n        Input to the model `forward` call\n    verbose: bool = False\n        Include extra detail\n    kwargs: Dict[str, Any] = {}\n        Other kwargs for model recording\n\n    """"""\n\n    model: torch.nn.Module\n    input_to_model: torch.autograd.Variable\n    verbose: bool = False\n    kwargs: Dict[str, Any] = {}\n\n\nDATA_TYPES = tuple([ScalarT, ScalarsT, HistogramT, TextT, ImageT, EmbeddingT, GraphT, PRCurveT])\n\n\nclass DataLoggingFilter(logging.Filter):\n    """"""Filters on `DATA_TYPES` otherwise returns `default`\n\n    `filter` returns `self.default` if record is not a `DATA_TYPES`\n    type; True if message is a `DATA_TYPES` type not in `dont_include`\n    and high enough level; otherwise False\n\n    Parameters\n    ----------\n    default : bool\n        Returned when record is not one `DATA_TYPES`\n    level : int\n        Minimum level of records that are `DATA_TYPES` to be accepted\n    dont_include : Sequence[Type[Any]]\n        Types from `DATA_TYPES` to be excluded\n    **kwargs : Any\n        Additional kwargs to pass to `logging.Filter`\n\n    Attributes\n    ----------\n    default : bool\n        Returned when record is not one `DATA_TYPES`\n    level : int\n        Minimum level of records that are `DATA_TYPES` to be accepted\n    dont_include : Tuple[Type[Any]]\n        Types from `DATA_TYPES` to be excluded\n\n    """"""\n    def __init__(self,\n                 default: bool = True,\n                 level: int = logging.NOTSET,\n                 dont_include: Optional[Tuple[type, ...]] = None,\n                 **kwargs: Any) -> None:\n        super().__init__(**kwargs)\n        self.default = default\n        self.level = level\n        self.dont_include = tuple(dont_include) if dont_include is not None else tuple()\n\n    def filter(self, record: logging.LogRecord) -> bool:\n        """"""Return True iff record should be accepted\n\n        Parameters\n        ----------\n        record : logging.LogRecord\n            logging record to be filtered\n\n        Returns\n        -------\n        bool\n            True iff record should be accepted. `self.default` if\n            record is not a `DATA_TYPES` type; True if message is a\n            `DATA_TYPES` type not in `dont_include` and high enough\n            level; otherwise False\n\n        """"""\n        if hasattr(record, \'raw_msg_obj\'):\n            message = record.raw_msg_obj  # type: ignore\n            if isinstance(message, DATA_TYPES):\n                if not isinstance(message, self.dont_include) \\\n                   and record.levelno >= self.level:\n                    return True\n                return False\n            return self.default\n        return self.default\n'"
flambe/logging/logging.py,0,"b'import sys\nimport os\nimport pathlib\nimport logging\nfrom logging import handlers\nfrom typing import Type, Any, List, AnyStr, Optional, Dict  # noqa: F401\nfrom types import TracebackType\n\nfrom tqdm import tqdm\n\nfrom flambe.logging.handler.tensorboard import TensorboardXHandler\nfrom flambe.logging.handler.contextual_file import ContextualFileHandler\nfrom flambe.logging.datatypes import DataLoggingFilter\nfrom flambe.const import FLAMBE_GLOBAL_FOLDER\n\nMB = 2**20\n\n\ndef setup_global_logging(console_log_level: int = logging.NOTSET) -> None:\n    """"""Set up flambe logging with a Stream handler and a\n    Rotating File handler.\n\n    This method should be set before consuming any logger as it\n    sets the basic configuration for all future logs.\n\n    After executing this method, all loggers will have the following\n    handlers:\n    * Stream handler: prints to std output all logs that above The\n    console_log_level\n    * Rotating File hanlder: 10MB log file located in Flambe global\n    folder. Configured to store all logs (min level DEBUG)\n\n    Parameters\n    ----------\n    console_log_level: int\n        The minimum log level for the Stream handler\n\n    """"""\n    colorize_exceptions()\n    logs_dir = os.path.join(FLAMBE_GLOBAL_FOLDER, \'logs\')\n    pathlib.Path(logs_dir).mkdir(parents=True, exist_ok=True)\n\n    fh = handlers.RotatingFileHandler(\n        os.path.join(FLAMBE_GLOBAL_FOLDER, \'logs\', \'log.log\'),\n        maxBytes=10 * MB, backupCount=5\n    )\n    formatter = logging.Formatter(\n        \'%(asctime)s | %(levelname)-8s | %(name)-8s | %(lineno)04d | %(message)s\'\n    )\n    fh.setFormatter(formatter)\n\n    tqdm_safe_out, tqdm_safe_err = map(TqdmFileWrapper, [sys.stdout, sys.stderr])\n    ch = logging.StreamHandler(stream=tqdm_safe_out)  # type: ignore\n    formatter = logging.Formatter(\'%(asctime)s | %(message)s\', ""%H:%M:%S"")\n    # Only flambe logs in stdout\n    ch.addFilter(FlambeFilter())\n    ch.setLevel(console_log_level)\n    ch.setFormatter(formatter)\n\n    logging.captureWarnings(True)\n    logging.basicConfig(level=logging.DEBUG, handlers=[fh, ch])\n\n\nclass FlambeFilter(logging.Filter):\n    """"""Filter all log records that don\'t come from flambe or main.\n\n    """"""\n    def filter(self, record: logging.LogRecord) -> bool:\n        n = record.name\n        return n.startswith(""flambe"") or n.startswith(""__main__"")\n\n\nclass TrialLogging:\n\n    def __init__(self,\n                 log_dir: str,\n                 verbose: bool = False,\n                 root_log_level: Optional[int] = None,\n                 capture_warnings: bool = True,\n                 console_prefix: Optional[str] = None,\n                 hyper_params: Optional[Dict] = None) -> None:\n        self.log_dir = log_dir\n        self.verbose = verbose\n        self.log_level = logging.NOTSET\n        self.capture_warnings = capture_warnings\n        self.listener: handlers.QueueListener\n        self.console_prefix = console_prefix\n        self.handlers: List[logging.Handler] = []\n        self.queue_handler: handlers.QueueHandler\n        self.old_root_log_level: int = logging.NOTSET\n        self.hyper_params: Dict = hyper_params or {}\n\n    def __enter__(self) -> logging.Logger:\n        colorize_exceptions()\n        logger = logging.root\n        self.old_root_log_level = logger.level\n        if self.log_level is not None:\n            logger.setLevel(self.log_level)\n        console_log_level = logging.NOTSET if self.verbose else logging.ERROR\n        console_data_log_level = logging.NOTSET if self.verbose else logging.ERROR\n        console_file_log_level = logging.NOTSET if self.verbose else logging.INFO\n        tensorboard_log_level = logging.NOTSET if self.verbose else logging.INFO\n\n        # CONSOLE LOGGING\n        tqdm_safe_out, tqdm_safe_err = map(TqdmFileWrapper, [sys.stdout, sys.stderr])\n        console = logging.StreamHandler(stream=tqdm_safe_out)  # type: ignore\n        console.setLevel(console_log_level)\n        console_formatter = logging.Formatter(\'%(name)s [block_%(_console_prefix)s] %(message)s\')\n        console.setFormatter(console_formatter)\n        console_data_filter = DataLoggingFilter(level=console_data_log_level)\n        console.addFilter(console_data_filter)\n        self.handlers.append(console)\n\n        # RECORD VERBOSE CONSOLE OUTPUT TO CONTEXT-SPECIFIC FILE\n        console_splitter = ContextualFileHandler(canonical_name=""console.out"", mode=\'a\')\n        console_splitter.setLevel(console_file_log_level)\n        console_splitter.setFormatter(console_formatter)\n        self.handlers.append(console_splitter)\n\n        # TENSORBOARDX LOGGING\n        try:\n            tbx = TensorboardXHandler()\n            tbx.setLevel(tensorboard_log_level)\n            self.handlers.append(tbx)\n        except ModuleNotFoundError:\n            print(""TensorboardX not found. Disabling logging handler."")\n\n        for handler in self.handlers:\n            logger.addHandler(handler)\n\n        # Route built-in Python warnings through our logger, defaulting\n        # to WARN severity This means warnings that come from 3rd party\n        # code e.g. Pandas, custom user code can be properly filtered\n        # and logged\n        logging.captureWarnings(self.capture_warnings)\n\n        self.old_factory = logging.getLogRecordFactory()\n\n        def record_factory(name, level, fn, lno, msg, args, exc_info,  # type: ignore\n                           func=None, sinfo=None, **kwargs):\n            record = self.old_factory(name, level, fn, lno, msg, args,\n                                      exc_info, func=None, sinfo=None, **kwargs)\n            # Always make raw message available by default so that\n            # handlers can manipulate native objects instead of\n            # string representations\n            record.raw_msg_obj = msg  # type: ignore\n            return record\n\n        logging.setLogRecordFactory(record_factory)\n        logging.root._log_dir = self.log_dir  # type: ignore\n\n        self.context_filter = ContextInjection(_console_prefix=self.console_prefix,\n                                               _tf_log_dir=self.log_dir,\n                                               _tf_hparams=self.hyper_params,\n                                               _console_log_dir=self.log_dir)\n\n        for handler in logger.handlers:\n            handler.addFilter(self.context_filter)\n        logger.addFilter(self.context_filter)\n\n        self.logger = logger\n        return logger\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        """"""Close the listener and restore original logging config""""""\n        for handler in self.logger.handlers:\n            handler.removeFilter(self.context_filter)\n        self.logger.removeFilter(self.context_filter)\n        for handler in self.handlers:\n            self.logger.removeHandler(handler)\n        self.logger.setLevel(self.old_root_log_level)\n        logging.setLogRecordFactory(self.old_factory)\n        delattr(logging.root, \'_log_dir\')\n\n\nclass ContextInjection:\n    """"""Add specified attributes to all log records\n\n    Parameters\n    ----------\n    **attrs : Any\n        Attributes that should be added to all log records, for use\n        in downstream handlers\n\n    """"""\n\n    def __init__(self, **attrs) -> None:\n        self.attrs = attrs\n\n    def filter(self, record: logging.LogRecord) -> int:\n        for k, v in self.attrs.items():\n            setattr(record, k, v)\n        return True\n\n    def __call__(self, record: logging.LogRecord) -> int:\n        return self.filter(record)\n\n\nclass TqdmFileWrapper:\n    """"""Dummy file-like that will write to tqdm\n\n    Based on canoncial tqdm example\n\n    """"""\n\n    def __init__(self, file: Any) -> None:\n        self.file = file\n\n    def write(self, x: AnyStr) -> int:\n        # Avoid print() second call (useless \\n)\n        if len(x.rstrip()) > 0:\n            return tqdm.write(x, file=self.file)\n        return 0\n\n    def flush(self) -> Any:\n        return getattr(self.file, ""flush"", lambda: None)()\n\n\ndef colorize_exceptions() -> None:\n    """"""Colorizes the system stderr ouput using pygments if installed""""""\n    try:\n        import traceback\n        from pygments import highlight\n        from pygments.lexers import get_lexer_by_name\n        from pygments.formatters import TerminalFormatter\n\n        def colorized_excepthook(type_: Type[BaseException],\n                                 value: BaseException,\n                                 tb: TracebackType) -> None:\n            tbtext = \'\'.join(traceback.format_exception(type_, value, tb))\n            lexer = get_lexer_by_name(""pytb"", stripall=True)\n            formatter = TerminalFormatter()\n            sys.stderr.write(highlight(tbtext, lexer, formatter))\n\n        sys.excepthook = colorized_excepthook  # type: ignore\n\n    except ModuleNotFoundError:\n        pass\n'"
flambe/logging/utils.py,6,"b'from typing import Dict, Union, Optional, Callable, Any\nimport time\nimport logging\nimport inspect\nfrom types import SimpleNamespace\n\nimport torch\nimport numpy\nimport random\n\nfrom flambe.logging import ScalarT, ScalarsT, TextT, ImageT, HistogramT, PRCurveT\nimport colorama\nfrom colorama import Fore, Style\ncolorama.init()\n\n\nValueT = Union[float, Dict[str, float], str]\n\n\nd: Dict[str, Callable] = {\n    ""GR"": lambda x: Fore.GREEN + x + Style.RESET_ALL,\n    ""RE"": lambda x: Fore.RED + x + Style.RESET_ALL,\n    ""YE"": lambda x: Fore.YELLOW + x + Style.RESET_ALL,\n    ""BL"": lambda x: Fore.CYAN + x + Style.RESET_ALL,\n    ""MA"": lambda x: Fore.MAGENTA + x + Style.RESET_ALL,\n    ""RA"": lambda x: random.choice([Fore.GREEN, Fore.RED, Fore.YELLOW,\n                                   Fore.CYAN, Fore.MAGENTA]) + x + Style.RESET_ALL\n}\ncoloredlogs = SimpleNamespace(**d)\n\n\ndef _get_context_logger() -> logging.Logger:\n    """"""Return the appropriate logger related to the module that\n    logs.\n\n    """"""\n    frame = inspect.stack()[1]\n    module = inspect.getmodule(frame[0])\n    logger = logging.getLogger(module.__name__)  # type: ignore\n    return logger\n\n\ndef get_trial_dir() -> str:\n    """"""Get the output path used by the currently active trial.\n\n    Returns\n    -------\n    str\n        The output path\n\n    """"""\n    return logging.root._log_dir  # type: ignore\n\n\ndef log(tag: str,\n        data: ValueT,\n        global_step: int,\n        walltime: Optional[float] = None) -> None:\n    """"""Log data to tensorboard and console (convenience function)\n\n    Inspects type of data and uses the appropriate wrapper for\n    tensorboard to consume the data. Supports floats (scalar),\n    dictionary mapping tags to gloats (scalars), and strings (text).\n\n    Parameters\n    ----------\n    tag : str\n        Name of data, used as the tensorboard tag\n    data : ValueT\n        The scalar or text to log\n    global_step : int\n        Iteration number associated with data\n    walltime : Optional[float]\n        Walltime for data (the default is None).\n\n    Examples\n    -------\n    Normally you would have to do the following to log a scalar\n    >>> import logging; from flambe.logging import ScalarT\n    >>> logger = logging.getLogger(__name__)\n    >>> logger.info(ScalarT(tag, data, step, walltime))\n    But this method allows you to write a more concise statement with\n    a common interface\n    >>> from flambe.logging import log\n    >>> log(tag, data, step)\n\n    """"""\n    fn: Callable[..., Any]\n    if isinstance(data, (float, int)) or isinstance(data, torch.Tensor):\n        if isinstance(data, torch.Tensor):\n            data = data.item()\n        fn = log_scalar\n    elif isinstance(data, dict):\n        fn = log_scalars\n    elif isinstance(data, str):\n        fn = log_text\n    else:\n        _get_context_logger().info(f""{tag} #{global_step}: {data} (not logged to tensorboard)"")\n        return\n    # Ignore type for complicated branching, fn could have a number of\n    # different signatures\n    fn(tag, data, global_step)  # type: ignore\n\n\ndef log_scalar(tag: str,\n               data: float,\n               global_step: int,\n               walltime: Optional[float] = None,\n               logger: Optional[logging.Logger] = None) -> None:\n    """"""Log tensorboard compatible scalar value with common interface\n\n    Parameters\n    ----------\n    tag : str\n        Tensorboard tag associated with scalar data\n    data : float\n        Scalar float value\n    global_step : int\n        The global step or iteration number\n    walltime : Optional[float]\n        Current walltime, for example from `time.time()`\n    logger: Optional[logging.Logger]\n        logger to use for logging the scalar\n\n    """"""\n    logger = logger or _get_context_logger()\n    logger.info(ScalarT(tag=tag, scalar_value=data, global_step=global_step,\n                        walltime=walltime or time.time()))\n\n\ndef log_scalars(tag: str,\n                data: Dict[str, float],\n                global_step: int,\n                walltime: Optional[float] = None,\n                logger: Optional[logging.Logger] = None) -> None:\n    """"""Log tensorboard compatible scalar values with common interface\n\n    Parameters\n    ----------\n    tag : str\n        Main tensorboard tag associated with all data\n    data : Dict[str, float]\n        Scalar float value\n    global_step : int\n        The global step or iteration number\n    walltime : Optional[float]\n        Current walltime, for example from `time.time()`\n    logger: Optional[logging.Logger]\n        logger to use for logging the scalar\n\n    """"""\n    logger = logger or _get_context_logger()\n    logger.info(ScalarsT(main_tag=tag,\n                         tag_scalar_dict=data,\n                         global_step=global_step,\n                         walltime=walltime or time.time()))\n\n\ndef log_text(tag: str,\n             data: str,\n             global_step: int,\n             walltime: Optional[float] = None,\n             logger: Optional[logging.Logger] = None) -> None:\n    """"""Log tensorboard compatible text value with common interface\n\n    Parameters\n    ----------\n    tag : str\n        Tensorboard tag associated with data\n    data : str\n        Scalar float value\n    global_step : int\n        The global step or iteration number\n    walltime : Optional[float]\n        Current walltime, for example from `time.time()`\n    logger: Optional[logging.Logger]\n        logger to use for logging the scalar\n\n    """"""\n    logger = logger or _get_context_logger()\n    logger.info(TextT(tag=tag, text_string=data, global_step=global_step,\n                      walltime=walltime or time.time()))\n\n\ndef log_image(tag: str,\n              data: str,\n              global_step: int,\n              walltime: Optional[float] = None,\n              logger: Optional[logging.Logger] = None) -> None:\n    """"""Log tensorboard compatible image value with common interface\n\n    Parameters\n    ----------\n    tag : str\n        Tensorboard tag associated with data\n    data : str\n        Scalar float value\n    global_step : int\n        The global step or iteration number\n    walltime : Optional[float]\n        Current walltime, for example from `time.time()`\n    logger: Optional[logging.Logger]\n        logger to use for logging the scalar\n\n    """"""\n    logger = logger or _get_context_logger()\n    logger.info(ImageT(tag=tag, img_tensor=data, global_step=global_step,\n                       walltime=walltime or time.time()))\n\n\ndef log_pr_curve(tag: str,\n                 labels: Union[torch.Tensor, numpy.array],\n                 predictions: Union[torch.Tensor, numpy.array],\n                 global_step: int,\n                 num_thresholds: int = 127,\n                 walltime: Optional[float] = None,\n                 logger: Optional[logging.Logger] = None) -> None:\n    """"""Log tensorboard compatible image value with common interface\n\n    Parameters\n    ----------\n    tag: str\n        Data identifier\n    labels: Union[torch.Tensor, numpy.array]\n        Containing 0, 1 values\n    predictions: Union[torch.Tensor, numpy.array]\n        Containing 0<=x<=1 values. Needs to match labels size\n    num_thresholds: int = 127\n        The number of thresholds to evaluate. Max value allowed 127.\n    weights: Optional[float] = None\n        No description provided.\n    global_step: int\n        Iteration associated with this value\n    walltime: float\n        Wall clock time associated with this value\n    logger: Optional[logging.Logger]\n        logger to use for logging the scalar\n\n    """"""\n    logger = logger or _get_context_logger()\n    logger.info(PRCurveT(tag=tag, labels=labels, predictions=predictions,\n                         num_thresholds=num_thresholds, global_step=global_step,\n                         walltime=walltime or time.time()))\n\n\ndef log_histogram(tag: str,\n                  data: str,\n                  global_step: int,\n                  bins: str = \'auto\',\n                  walltime: Optional[float] = None,\n                  logger: Optional[logging.Logger] = None) -> None:\n    """"""Log tensorboard compatible image value with common interface\n\n    Parameters\n    ----------\n    tag : str\n        Tensorboard tag associated with data\n    data : str\n        Scalar float value\n    global_step : int\n        The global step or iteration number\n    walltime : Optional[float]\n        Current walltime, for example from `time.time()`\n    logger: Optional[logging.Logger]\n        logger to use for logging the scalar\n\n    """"""\n    logger = logger or _get_context_logger()\n    logger.info(HistogramT(tag=tag, values=data, global_step=global_step, bins=bins,\n                           walltime=walltime or time.time()))\n'"
flambe/metric/__init__.py,0,"b""from flambe.metric.metric import Metric\nfrom flambe.metric.loss.cross_entropy import MultiLabelCrossEntropy\nfrom flambe.metric.loss.nll_loss import MultiLabelNLLLoss\nfrom flambe.metric.dev.accuracy import Accuracy\nfrom flambe.metric.dev.perplexity import Perplexity\nfrom flambe.metric.dev.bpc import BPC\nfrom flambe.metric.dev.auc import AUC, MultiClassAUC\nfrom flambe.metric.dev.binary import BinaryPrecision, BinaryRecall, BinaryAccuracy, F1\nfrom flambe.metric.dev.recall import Recall\n\n\n__all__ = ['Metric',\n           'Accuracy', 'AUC', 'Perplexity', 'BPC',\n           'MultiLabelCrossEntropy', 'MultiLabelNLLLoss',\n           'BinaryPrecision', 'BinaryRecall', 'BinaryAccuracy', 'F1',\n           'Recall', 'MultiClassAUC']\n"""
flambe/metric/metric.py,7,"b'from typing import Dict\nfrom abc import abstractmethod\n\nimport numpy as np\nimport torch\n\nfrom flambe.compile import Component\n\n\nclass Metric(Component):\n    """"""Base Metric interface.\n\n    Objects implementing this interface should take in a sequence of\n    examples and provide as output a processd list of the same size.\n\n    """"""\n\n    @abstractmethod\n    def compute(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        """"""Computes the metric over the given prediction and target.\n\n        Parameters\n        ----------\n        pred: torch.Tensor\n            The model predictions\n\n        target: torch.Tensor\n            The ground truth targets\n\n        Returns\n        -------\n        torch.Tensor\n            The computed metric\n\n        """"""\n        pass\n\n    def aggregate(self, state: dict, *args, **kwargs) -> Dict:\n        """"""Aggregates by simply storing preds and targets\n\n        Parameters\n        ----------\n        state: dict\n            the metric state\n        args: the pred, target tuple\n\n        Returns\n        -------\n        dict\n            the state dict\n        """"""\n        pred, target = args\n        if not state:\n            state[\'pred\'] = []\n            state[\'target\'] = []\n        state[\'pred\'].append(pred.cpu().detach())\n        state[\'target\'].append(target.cpu().detach())\n        return state\n\n    def finalize(self, state: Dict) -> float:\n        """"""Finalizes the metric computation\n\n        Parameters\n        ----------\n        state: dict\n            the metric state\n\n        Returns\n        -------\n        float\n            The final score.\n        """"""\n        if not state:\n            # call on empty state\n            return np.NaN\n        pred = torch.cat(state[\'pred\'], dim=0)\n        target = torch.cat(state[\'target\'], dim=0)\n        state[\'accumulated_score\'] = self.compute(pred, target).item()\n        return state[\'accumulated_score\']\n\n    def __call__(self, *args, **kwargs):\n        """"""Makes Featurizer a callable.""""""\n        return self.compute(*args, **kwargs)\n\n    def __str__(self) -> str:\n        """"""Return the name of the Metric (for use in logging).""""""\n        return self.__class__.__name__\n\n\nclass AverageableMetric(Metric):\n    """"""Metric interface for averageable metrics\n\n    Some metrics, such as accuracy, are averaged as a final step.\n    This allows for a more efficient metrics computation.\n    These metrics should inherit from this class.\n\n    """"""\n\n    def aggregate(self, state: dict, *args, **kwargs) -> Dict:\n        """"""\n\n        Parameters\n        ----------\n        state: dict\n            the state dictionary\n        args:\n            normally pred, target\n        kwargs\n\n        Returns\n        -------\n        dict\n            The updated state (even though the update happens in-place)\n        """"""\n        score = self.compute(*args, **kwargs)\n        score_np = score.cpu().detach().numpy() \\\n            if isinstance(score, torch.Tensor) \\\n            else score\n        try:\n            num_samples = args[0].size(0)\n        except (ValueError, AttributeError):\n            raise ValueError(f\'Cannot get size from {type(args[0])}\')\n        if not state:\n            state[\'accumulated_score\'] = 0.\n            state[\'sample_count\'] = 0\n        state[\'accumulated_score\'] = \\\n            (state[\'sample_count\'] * state[\'accumulated_score\'] +\n             num_samples * score_np.item()) / \\\n            (state[\'sample_count\'] + num_samples)\n        state[\'sample_count\'] = state[\'sample_count\'] + num_samples\n        return state\n\n    def finalize(self, state) -> float:\n        """"""\n        FInalizes the metric computation\n\n        Parameters\n        ----------\n        state: dict\n            the metric state\n\n        Returns\n        -------\n        Any\n            The final score. Can be anything, depending on metric.\n        """"""\n        return state.get(\'accumulated_score\')\n'"
flambe/model/__init__.py,0,"b""# type: ignore[attr-defined]\n\nfrom flambe.model.logistic_regression import LogisticRegression\n\n__all__ = ['LogisticRegression']\n"""
flambe/model/logistic_regression.py,1,"b'# type: ignore[override]\n\nfrom typing import Optional, Tuple, Union\n\nfrom torch import Tensor\nfrom torch.nn import Sigmoid\nfrom flambe.nn.module import Module  # type: ignore[attr-define]\nfrom flambe.nn import MLPEncoder\n\n\nclass LogisticRegression(Module):\n    """"""\n    Logistic regression model given an input vector v\n    the forward calculation is sigmoid(Wv+b), where\n    W is a weight vector and b a bias term. The result\n    is then passed to a sigmoid function, which maps it\n    as a real number in [0,1]. This is typically interpreted\n    in classification settings as the probability of belonging\n    to a given class.\n\n    Attributes\n    ----------\n    input_size : int\n        Dimension (number of features) of the input vector.\n    """"""\n\n    def __init__(self, input_size: int) -> None:\n        """"""\n        Initialize the Logistic Regression Model.\n        Parameters\n        ----------\n        input_size: int\n            The dimension of the input vector\n        """"""\n        super().__init__()\n        self.encoder = MLPEncoder(input_size, output_size=1,\n                                  n_layers=1, output_activation=Sigmoid())\n\n    def forward(self,\n                data: Tensor,\n                target: Optional[Tensor] = None) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n        """"""Forward pass that encodes data\n        Parameters\n        ----------\n        data : Tensor\n            input data to encode\n        target: Optional[Tensor]\n            target value, will be casted to a float tensor.\n        """"""\n        encoding = self.encoder(data)\n        return (encoding, target.float()) if target is not None else encoding\n'"
flambe/nlp/__init__.py,0,"b""from flambe.nlp import language_modeling\nfrom flambe.nlp import classification\nfrom flambe.nlp import fewshot\nfrom flambe.nlp import transformers\n\n__all__ = ['language_modeling', 'classification', 'fewshot', 'transformers']\n"""
flambe/nn/__init__.py,0,"b""# type: ignore[attr-defined]\n\nfrom flambe.nn.module import Module\nfrom flambe.nn.softmax import SoftmaxLayer\nfrom flambe.nn.mos import MixtureOfSoftmax\nfrom flambe.nn.embedding import Embeddings, Embedder\nfrom flambe.nn.mlp import MLPEncoder\nfrom flambe.nn.rnn import RNNEncoder, PooledRNNEncoder\nfrom flambe.nn.cnn import CNNEncoder\nfrom flambe.nn.sequential import Sequential\nfrom flambe.nn.pooling import FirstPooling, LastPooling, SumPooling, AvgPooling, \\\n    StructuredSelfAttentivePooling, GeneralizedPooling\nfrom flambe.nn.transformer import Transformer, TransformerEncoder, TransformerDecoder\nfrom flambe.nn.transformer_sru import TransformerSRU, TransformerSRUEncoder, TransformerSRUDecoder\n\n\n__all__ = ['Module', 'Embeddings', 'Embedder', 'RNNEncoder',\n           'PooledRNNEncoder', 'CNNEncoder', 'MLPEncoder',\n           'SoftmaxLayer', 'MixtureOfSoftmax', 'Sequential',\n           'Transformer', 'TransformerEncoder', 'TransformerDecoder',\n           'TransformerSRU', 'TransformerSRUEncoder', 'TransformerSRUDecoder',\n           'FirstPooling', 'LastPooling', 'SumPooling', 'AvgPooling',\n           'StructuredSelfAttentivePooling', 'GeneralizedPooling']\n"""
flambe/nn/cnn.py,1,"b'# type: ignore[override]\n\nfrom typing import Optional, Tuple, List, Union\n\nfrom torch import nn\nfrom torch import Tensor\n\nfrom flambe.nn.module import Module\n\n\ndef conv_block(conv_mod: nn.Module,\n               activation: nn.Module,\n               pooling: nn.Module,\n               dropout: float,\n               batch_norm: Optional[nn.Module] = None) -> nn.Module:\n    """"""Return a convolutional block.\n\n    """"""\n\n    mods = [conv_mod]\n\n    if pooling:\n        mods.append(pooling)\n\n    if batch_norm is None:\n        mods.append(batch_norm)\n\n    mods.append(activation)\n    mods.append(nn.Dropout(dropout))\n\n    return nn.Sequential(*mods)\n\n\nclass CNNEncoder(Module):\n    """"""Implements a multi-layer n-dimensional CNN.\n\n    This module can be used to create multi-layer CNN models.\n\n    Attributes\n    ----------\n    cnn: nn.Module\n        The cnn submodule\n\n    """"""\n    def __init__(self,\n                 input_channels: int,\n                 channels: List[int],\n                 conv_dim: int = 2,  # Support only for 1, 2 or 3\n                 kernel_size: Union[int, List[Union[Tuple[int, ...], int]]] = 3,\n                 activation: nn.Module = None,\n                 pooling: nn.Module = None,\n                 dropout: float = 0,\n                 batch_norm: bool = True,\n                 stride: int = 1,\n                 padding: int = 0) -> None:\n        """"""Initializes the CNNEncoder object.\n\n        Parameters\n        ----------\n        input_channels: int\n            The input\'s channels. For example, 3 for RGB images.\n        channels: List[int]\n            A list to specify the channels of the convolutional layers.\n            The length of this list will be the amount of convolutions\n            in the encoder.\n        conv_dim: int, optional\n            The dimension of the convolutions. Can be 1, 2 or 3.\n            Defaults to 2.\n        kernel_size: Union[int, List[Union[Tuple[int], int]]], optional\n            The kernel size for the convolutions. This could be an int\n            (the same kernel size for all convolutions and dimensions),\n            or a List where for each convolution you can specify an int\n            or a tuple (for different sizes per dimension, in which case\n            the length of the tuple must match the dimension of the\n            convolution). Defaults to 3.\n        activation: nn.Module, optional\n           The activation function to use in all layers.\n           Defaults to nn.ReLU\n        pooling: nn.Module, optional\n            The pooling function to use after all layers.\n            Defaults to None\n        dropout: float, optional\n            Amount of dropout to use between CNN layers, defaults to 0\n        batch_norm: bool, optional\n            Wether to user Batch Normalization or not. Defaults to True\n        stride: int, optional\n            The stride to use when doing convolutions. Defaults to 1\n        padding: int, optional\n            The padding to use when doing convolutions. Defaults to 0\n\n        Raises\n        ------\n        ValueError\n            The conv_dim should be 1, 2, 3.\n\n        """"""\n        super().__init__()\n\n        dim2mod = {\n            1: (nn.Conv1d, nn.BatchNorm1d, nn.MaxPool1d),\n            2: (nn.Conv2d, nn.BatchNorm2d, nn.MaxPool2d),\n            3: (nn.Conv3d, nn.BatchNorm3d, nn.MaxPool3d),\n        }\n\n        if conv_dim not in dim2mod:\n            raise ValueError(f""Invalid conv_dim value {conv_dim}. Values 1, 2, 3 supported"")\n\n        if isinstance(kernel_size, List) and len(kernel_size) != len(channels):\n            raise ValueError(""Kernel size list should have same length as channels list"")\n\n        conv, bn, pool = dim2mod[conv_dim]\n        activation = activation or nn.ReLU()\n\n        layers = []\n\n        prev_c = input_channels\n        for i, c in enumerate(channels):\n            k: Union[int, Tuple]\n            if isinstance(kernel_size, int):\n                k = kernel_size\n            else:\n                k = kernel_size[i]\n                if not isinstance(k, int) and len(k) != conv_dim:\n                    raise ValueError(""Kernel size tuple should have same length as conv_dim"")\n\n            layer = conv_block(\n                conv(prev_c, c, k, stride, padding),\n                activation,\n                pooling,\n                dropout,\n                bn(c)\n            )\n            layers.append(layer)\n            prev_c = c\n\n        self.cnn = nn.Sequential(*layers)\n\n    def forward(self, data: Tensor) -> Union[Tensor, Tuple[Tensor, ...]]:\n        """"""Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a float tensor\n\n        Returns\n        -------\n        Union[Tensor, Tuple[Tensor, ...]]\n            The encoded output, as a float tensor\n\n        """"""\n        return self.cnn(data)\n'"
flambe/nn/embedding.py,10,"b'# type: ignore[override]\n\nimport math\nfrom typing import Tuple, Union, Optional\n\nimport torch\nfrom torch import nn\nfrom torch import Tensor\n\nfrom flambe.compile import registrable_factory\nfrom flambe.nn.module import Module\n\n\nclass Embeddings(Module):\n    """"""Implement an Embeddings module.\n\n    This object replicates the usage of nn.Embedding but\n    registers the from_pretrained classmethod to be used inside\n    a Flamb\xc3\xa9 configuration, as this does not happen automatically\n    during the registration of PyTorch objects.\n\n    The module also adds optional positional encoding, which can\n    either be sinusoidal or learned during training. For the\n    non-learned positional embeddings, we use sine and cosine\n    functions of different frequencies.\n\n    .. math::\n        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n\n    """"""\n\n    def __init__(self,\n                 num_embeddings: int,\n                 embedding_dim: int,\n                 padding_idx: int = 0,\n                 max_norm: Optional[float] = None,\n                 norm_type: float = 2.,\n                 scale_grad_by_freq: bool = False,\n                 sparse: bool = False,\n                 positional_encoding: bool = False,\n                 positional_learned: bool = False,\n                 positonal_max_length: int = 5000) -> None:\n        """"""Initialize an Embeddings module.\n\n        Parameters\n        ----------\n        num_embeddings : int\n            Size of the dictionary of embeddings.\n        embedding_dim : int\n            The size of each embedding vector.\n        padding_idx : int, optional\n            Pads the output with the embedding vector at\n            :attr:`padding_idx` (initialized to zeros) whenever it\n            encounters the index, by default 0\n        max_norm : Optional[float], optional\n            If given, each embedding vector with norm larger than\n            :attr:`max_norm` is normalized to have norm :attr:`max_norm`\n        norm_type : float, optional\n            The p of the p-norm to compute for the :attr:`max_norm`\n            option. Default ``2``.\n        scale_grad_by_freq : bool, optional\n            If given, this will scale gradients by the inverse of\n            frequency of the words in the mini-batch. Default ``False``.\n        sparse : bool, optional\n            If ``True``, gradient w.r.t. :attr:`weight` matrix will\n            be a sparse tensor. See Notes for more details.\n        positional_encoding : bool, optional\n            If True, adds positonal encoding to the token embeddings.\n            By default, the embeddings are frozen sinusodial embeddings.\n            To learn these during training, set positional_learned.\n            Default ``False``.\n        positional_learned : bool, optional\n            Learns the positional embeddings during training instead\n            of using frozen sinusodial ones. Default ``False``.\n        positonal_max_length : int, optional\n            The maximum length of a sequence used for the positonal\n            embedding matrix. Default ``5000``.\n\n        """"""\n        super().__init__()\n\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.num_positions = positonal_max_length\n\n        self.token_embedding = nn.Embedding(num_embeddings,\n                                            embedding_dim,\n                                            padding_idx,\n                                            max_norm,\n                                            norm_type,\n                                            scale_grad_by_freq,\n                                            sparse)\n\n        self.pos_embedding = None\n        if positional_learned and not positional_encoding:\n            raise ValueError(""postional_encoding is False, but positonal_learned is True"")\n\n        elif positional_encoding and positional_learned:\n            self.pos_embedding = nn.Embedding(positonal_max_length, embedding_dim)\n\n        elif positional_encoding and not positional_learned:\n            # Use sinusodial encoding\n            position = torch.arange(0, positonal_max_length, dtype=torch.float).unsqueeze(1)\n            div_term = torch.arange(0, embedding_dim, 2).float()\n            div_term = torch.exp(div_term * (-math.log(10000.0) / embedding_dim))\n\n            pos_embedding = torch.zeros(positonal_max_length, embedding_dim)\n            pos_embedding[:, 0::2] = torch.sin(position * div_term)\n            pos_embedding[:, 1::2] = torch.cos(position * div_term)\n\n            self.pos_embedding = nn.Embedding.from_pretrained(pos_embedding, freeze=True)\n\n    @registrable_factory\n    @classmethod\n    def from_pretrained(cls,\n                        embeddings: Tensor,\n                        freeze: bool = True,\n                        padding_idx: int = 0,\n                        max_norm: Optional[float] = None,\n                        norm_type: float = 2.0,\n                        scale_grad_by_freq: bool = False,\n                        sparse: bool = False,\n                        positional_encoding: bool = False,\n                        positional_learned: bool = False,\n                        positonal_max_length: int = 5000,\n                        positonal_embeddings: Optional[Tensor] = None,\n                        positonal_freeze: bool = True):\n        """"""Create an Embeddings instance from pretrained embeddings.\n\n        Parameters\n        ----------\n        embeddings: torch.Tensor\n            FloatTensor containing weights for the Embedding.\n            First dimension is being passed to Embedding as\n            num_embeddings, second as embedding_dim.\n        freeze: bool\n            If True, the tensor does not get updated in the learning\n            process. Default: True\n        padding_idx : int, optional\n            Pads the output with the embedding vector at\n            :attr:`padding_idx` (initialized to zeros) whenever it\n            encounters the index, by default 0\n        max_norm : Optional[float], optional\n            If given, each embedding vector with norm larger than\n            :attr:`max_norm` is normalized to have norm :attr:`max_norm`\n        norm_type : float, optional\n            The p of the p-norm to compute for the :attr:`max_norm`\n            option. Default ``2``.\n        scale_grad_by_freq : bool, optional\n            If given, this will scale gradients by the inverse of\n            frequency of the words in the mini-batch. Default ``False``.\n        sparse : bool, optional\n            If ``True``, gradient w.r.t. :attr:`weight` matrix will\n            be a sparse tensor. See Notes for more details.\n        positional_encoding : bool, optional\n            If True, adds positonal encoding to the token embeddings.\n            By default, the embeddings are frozen sinusodial embeddings.\n            To learn these during training, set positional_learned.\n            Default ``False``.\n        positional_learned : bool, optional\n            Learns the positional embeddings during training instead\n            of using frozen sinusodial ones. Default ``False``.\n        positonal_embeddings: torch.Tensor, optional\n            If given, also replaces the positonal embeddings with\n            this matrix. The max length will be ignored and replaced\n            by the dimension of this matrix.\n        positonal_freeze: bool, optional\n            Whether the positonal embeddings should be frozen\n\n        """"""\n        if embeddings.dim() != 2:\n            raise ValueError(\'Embeddings parameter is expected to be 2-dimensional\')\n        if positonal_embeddings is not None:\n            if positonal_embeddings.dim() != 2:\n                raise ValueError(\'Positonal embeddings parameter is expected to be 2-dimensional\')\n            if positonal_embeddings.size() != embeddings.size():\n                raise ValueError(\'Both pretrained matrices must have the same dimensions\')\n\n        rows, cols = embeddings.shape\n        positional_encoding = positional_encoding or (positonal_embeddings is not None)\n\n        embedding = cls(num_embeddings=rows,\n                        embedding_dim=cols,\n                        padding_idx=padding_idx,\n                        max_norm=max_norm,\n                        norm_type=norm_type,\n                        scale_grad_by_freq=scale_grad_by_freq,\n                        sparse=sparse,\n                        positional_encoding=positional_encoding,\n                        positional_learned=positional_learned,\n                        positonal_max_length=positonal_max_length)\n\n        embedding.token_embedding.weight.data = embeddings\n        embedding.token_embedding.weight.requires_grad = not freeze\n\n        if positonal_embeddings is not None:\n            embedding.pos_embedding.weight.data = positonal_embeddings  # type: ignore\n            embedding.pos_embedding.weight.requires_grad = not positonal_freeze  # type: ignore\n\n        return embedding\n\n    def forward(self, data: Tensor) -> Tensor:\n        """"""Perform a forward pass.\n\n        Parameters\n        ----------\n        data : Tensor\n            The input tensor of shape [S x B]\n\n        Returns\n        -------\n        Tensor\n            The output tensor of shape [S x B x E]\n\n        """"""\n        out = self.token_embedding(data)\n\n        if self.pos_embedding is not None:\n            column = torch.arange(data.size(0)).unsqueeze(1)\n            positions = column.repeat(1, data.size(1)).to(data)\n            out = out + self.pos_embedding(positions)\n\n        return out\n\n\nclass Embedder(Module):\n    """"""Implements an Embedder module.\n\n    An Embedder takes as input a sequence of index tokens,\n    and computes the corresponding embedded representations, and\n    padding mask. The encoder may be initialized using a pretrained\n    embedding matrix.\n\n    Attributes\n    ----------\n    embeddings: Module\n        The embedding module\n    encoder: Module\n        The sub-encoder that this object is wrapping\n    pooling: Module\n        An optional pooling module\n    drop: nn.Dropout\n        The dropout layer\n\n    """"""\n\n    def __init__(self,\n                 embedding: Module,\n                 encoder: Module,\n                 pooling: Optional[Module] = None,\n                 embedding_dropout: float = 0,\n                 padding_idx: Optional[int] = 0,\n                 return_mask: bool = False) -> None:\n        """"""Initializes the TextEncoder module.\n\n        Extra arguments are passed to the nn.Embedding module.\n\n        Parameters\n        ----------\n        embedding: nn.Embedding\n            The embedding layer\n        encoder: Module\n            The encoder\n        pooling: Module, optional\n            An optioonal pooling module, takes a sequence of Tensor and\n            reduces them to a single Tensor.\n        embedding_dropout: float, optional\n            Amount of dropout between the embeddings and the encoder\n        padding_idx: int, optional\n            Passed the nn.Embedding object. See pytorch documentation.\n        return_mask: bool\n            If enabled, the forward call returns a tuple of\n            (encoding, mask)\n\n        """"""\n        super().__init__()\n\n        self.embedding = embedding\n        self.dropout = nn.Dropout(embedding_dropout)\n        self.encoder = encoder\n        self.pooling = pooling\n        self.padding_idx = padding_idx\n        self.return_mask = return_mask\n\n    def forward(self, data: Tensor) \\\n            -> Union[Tensor,\n                     Tuple[Tensor, Tensor],\n                     Tuple[Tuple[Tensor, Tensor], Tensor]]:\n        """"""Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a float tensor of shape [S x B]\n\n        Returns\n        -------\n        Union[Tensor, Tuple[Tensor, Tensor],\n                Tuple[Tuple[Tensor, Tensor], Tensor]\n            The encoded output, as a float tensor. May return a state\n            if the encoder is an RNN and no pooling is provided.\n            May also return a tuple if `return_mask` was passed in as\n            a constructor argument.\n\n        """"""\n        embedded = self.embedding(data)\n        embedded = self.dropout(embedded)\n\n        padding_mask: Optional[Tensor]\n        if self.padding_idx is not None:\n            padding_mask = (data != self.padding_idx)\n            encoding = self.encoder(embedded, padding_mask=padding_mask)\n        else:\n            padding_mask = None\n            encoding = self.encoder(embedded)\n\n        if self.pooling is not None:\n            # Ignore states from encoders such as RNN or TransformerSRU\n            encoding = encoding[0] if isinstance(encoding, tuple) else encoding\n            encoding = self.pooling(encoding, padding_mask)\n\n        if self.return_mask:\n            return encoding, padding_mask\n        else:\n            return encoding\n'"
flambe/nn/mlp.py,4,"b'# type: ignore[override]\n\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom flambe.nn.module import Module\n\n\nclass MLPEncoder(Module):\n    """"""Implements a multi layer feed forward network.\n\n    This module can be used to create output layers, or\n    more complex multi-layer feed forward networks.\n\n    Attributes\n    ----------\n    seq: nn.Sequential\n        the sequence of layers and activations\n\n    """"""\n    def __init__(self,\n                 input_size: int,\n                 output_size: int,\n                 n_layers: int = 1,\n                 dropout: float = 0.,\n                 output_activation: Optional[nn.Module] = None,\n                 hidden_size: Optional[int] = None,\n                 hidden_activation: Optional[nn.Module] = None) -> None:\n        """"""Initializes the FullyConnected object.\n\n        Parameters\n        ----------\n        input_size: int\n            Input_dimension\n        output_size: int\n            Output dimension\n        n_layers: int, optional\n            Number of layers in the network, defaults to 1\n        dropout: float, optional\n            Dropout to be used before each MLP layer.\n            Only used if n_layers > 1.\n        output_activation: nn.Module, optional\n            Any PyTorch activation layer, defaults to None\n        hidden_size: int, optional\n            Hidden dimension, used only if n_layers > 1.\n            If not given, defaults to the input_size\n        hidden_activation: nn.Module, optional\n            Any PyTorch activation layer, defaults to None\n\n        """"""\n        super().__init__()\n\n        # Gather the layers in a list to pass to Sequential\n        layers = []\n\n        # Add the hidden_layers\n        if n_layers == 1 or hidden_size is None:\n            hidden_size = input_size\n\n        if n_layers > 1:\n\n            # Add the first hidden layer\n            layers.append(nn.Linear(input_size, hidden_size))\n            if hidden_activation is not None:\n                layers.append(hidden_activation)\n            if dropout > 0:\n                layers.append(nn.Dropout(dropout))\n\n            for _ in range(1, n_layers - 1):\n                layers.append(nn.Linear(hidden_size, hidden_size))\n                if hidden_activation is not None:\n                    layers.append(hidden_activation)\n                if dropout > 0:\n                    layers.append(nn.Dropout(dropout))\n\n        layers.append(nn.Linear(hidden_size, output_size))\n        if output_activation is not None:\n            layers.append(output_activation)\n\n        self.seq = nn.Sequential(*layers)\n\n    def forward(self, data: torch.Tensor) -> torch.Tensor:\n        """"""Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data: torch.Tensor\n            input to the model of shape (batch_size, input_size)\n\n        Returns\n        -------\n        output: torch.Tensor\n            output of the model of shape (batch_size, output_size)\n\n        """"""\n        return self.seq(data)\n'"
flambe/nn/module.py,1,"b'import math\nfrom typing import Iterator, Tuple\n\nimport torch.nn as nn\n\nfrom flambe.compile import Component\n\n\nclass Module(Component, nn.Module):\n    """"""Base Flamb\xc3\xa9 Module inteface.\n\n    Provides the exact same interface as Pytorch\'s nn.Module, but\n    extends it with a useful set of methods to access and clip\n    parameters, as well as gradients.\n\n    This abstraction allows users to convert their modules with a\n    single line change, by importing from Flamb\xc3\xa9 instead. Just like\n    every Pytorch module, a forward method should be implemented.\n\n    """"""\n\n    @property\n    def named_trainable_params(self) -> Iterator[Tuple[str, nn.Parameter]]:\n        """"""Get all the named parameters with `requires_grad=True`.\n\n        Returns\n        -------\n        Iterator[Tuple[str, nn.Parameter]]\n            Iterator over the parameters and their name.\n\n        """"""\n        parameters = filter(lambda p: p[1].requires_grad, self.named_parameters())\n        return parameters\n\n    @property\n    def trainable_params(self) -> Iterator[nn.Parameter]:\n        """"""Get all the parameters with `requires_grad=True`.\n\n        Returns\n        -------\n        Iterator[nn.Parameter]\n            Iterator over the parameters\n\n        """"""\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        return parameters\n\n    @property\n    def gradient_norm(self) -> float:\n        """"""Compute the average gradient norm.\n\n        Returns\n        -------\n        float\n            The current average gradient norm\n\n        """"""\n        # Only compute over parameters that are being trained\n        parameters = filter(lambda p: p.requires_grad and p.grad is not None, self.parameters())\n        norm = math.sqrt(sum(\n            [param.grad.norm(p=2).item() ** 2 for param in parameters])  # type: ignore\n        )\n        return norm\n\n    @property\n    def parameter_norm(self) -> float:\n        """"""Compute the average parameter norm.\n\n        Returns\n        -------\n        float\n            The current average parameter norm\n\n        """"""\n        # Only compute over parameters that are being trained\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        norm = math.sqrt(\n            sum([param.norm(p=2).item() ** 2 for param in parameters])  # type: ignore\n        )\n        return norm\n\n    def num_parameters(self, trainable=False) -> int:\n        """"""Gets the number of parameters in the model.\n\n        Returns\n        ----------\n        int\n            number of model params\n\n        """"""\n        # filter by trainable parameters\n        if trainable:\n            model_params = list(filter(lambda p: p.requires_grad, self.parameters()))\n        else:\n            model_params = list(self.parameters())\n\n        return(sum([len(x.view(-1)) for x in model_params]))  # type: ignore\n\n    def clip_params(self, threshold: float):\n        """"""Clip the parameters to the given range.\n\n        Parameters\n        ----------\n        float\n            Values are clipped between -threshold, threshold\n\n        """"""\n        # Only compute over parameters that are being trained\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        for param in parameters:\n            param.data.clamp_(min=-threshold, max=threshold)\n\n    def clip_gradient_norm(self, threshold: float):\n        """"""Clip the norm of the gradient by the given value.\n\n        Parameters\n        ----------\n        float\n            Threshold to clip at\n\n        """"""\n        # Only compute over parameters that are being trained\n        parameters = filter(lambda p: p.requires_grad and p.grad is not None, self.parameters())\n        nn.utils.clip_grad_norm_(parameters, threshold)\n'"
flambe/nn/mos.py,3,"b'# type: ignore[override]\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom flambe.nn.mlp import MLPEncoder\nfrom flambe.nn.module import Module\n\n\nclass MixtureOfSoftmax(Module):\n    """"""Implement the MixtureOfSoftmax output layer.\n\n    Attributes\n    ----------\n    pi: FullyConnected\n        softmax layer over the different softmax\n    layers: [FullyConnected]\n        list of the k softmax layers\n\n    """"""\n    def __init__(self,\n                 input_size: int,\n                 output_size: int,\n                 k: int = 1,\n                 take_log: bool = True) -> None:\n        """"""Initialize the MOS layer.\n\n        Parameters\n        ----------\n        input_size: int\n            input dimension\n        output_size: int\n            output dimension\n        k: int (Default: 1)\n            number of softmax in the mixture\n\n        """"""\n        super().__init__()\n\n        self.pi_w = MLPEncoder(input_size, k)\n        self.softmax = nn.Softmax()\n\n        self.layers = [MLPEncoder(input_size, output_size) for _ in range(k)]\n        self.tanh = nn.Tanh()\n\n        self.activation = nn.LogSoftmax() if take_log else nn.Softmax()\n\n    def forward(self, data: Tensor) -> Tensor:\n        """"""Implement mixture of softmax for language modeling.\n\n        Parameters\n        ----------\n        data: torch.Tensor\n            seq_len x batch_size x hidden_size\n\n        Return\n        -------\n        out: Variable\n            output matrix of shape seq_len x batch_size x out_size\n\n        """"""\n        w = self.softmax(self.pi_w(data))\n        # Compute k softmax, and combine using above weights\n        out = [w[:, :, i] * self.tanh(W(data)) for i, W in enumerate(self.layers)]\n        out = torch.cat(out, dim=0).sum(dim=0)\n\n        return self.activation(out)\n'"
flambe/nn/pooling.py,54,"b'# type: ignore[override]\n\nfrom typing import Optional, Sequence\n\nimport torch\nfrom torch import nn\n\nfrom flambe.nn import Module\n\n\ndef _default_padding_mask(data: torch.Tensor) -> torch.Tensor:\n    """"""\n    Builds a 1s padding mask taking into account initial 2 dimensions\n    of input data.\n\n    Parameters\n    ----------\n    data : torch.Tensor\n        The input data, as a tensor of shape [B x S x H]\n\n    Returns\n    ----------\n    torch.Tensor\n        A padding mask , as a tensor of shape [B x S]\n    """"""\n    return torch.ones((data.size(0), data.size(1))).to(data)\n\n\ndef _sum_with_padding_mask(data: torch.Tensor,\n                           padding_mask: torch.Tensor) -> torch.Tensor:\n    """"""\n    Applies padding_mask and performs summation over the data\n\n    Parameters\n    ----------\n    data : torch.Tensor\n        The input data, as a tensor of shape [B x S x H]\n    padding_mask: torch.Tensor\n        The input mask, as a tensor of shape [B X S]\n    Returns\n    ----------\n    torch.Tensor\n        The result of the summation, as a tensor of shape [B x H]\n\n    """"""\n    return (data * padding_mask.unsqueeze(2)).sum(dim=1)\n\n\nclass FirstPooling(Module):\n    """"""Get the last hidden state of a sequence.""""""\n\n    def forward(self,\n                data: torch.Tensor,\n                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Performs a forward pass.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a tensor of shape [B x S x H]\n        padding_mask: torch.Tensor\n            The input mask, as a tensor of shape [B X S]\n\n        Returns\n        ----------\n        torch.Tensor\n            The output data, as a tensor of shape [B x H]\n\n        """"""\n        return data[:, 0, :]\n\n\nclass LastPooling(Module):\n    """"""Get the last hidden state of a sequence.""""""\n\n    def forward(self,\n                data: torch.Tensor,\n                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Performs a forward pass.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a tensor of shape [B x S x H]\n        padding_mask: torch.Tensor\n            The input mask, as a tensor of shape [B X S]\n\n        Returns\n        ----------\n        torch.Tensor\n            The output data, as a tensor of shape [B x H]\n\n        """"""\n        # Compute lengths\n        if padding_mask is None:\n            lengths = torch.tensor([data.size(1)] * data.size(0)).long()\n        else:\n            lengths = padding_mask.long().sum(dim=1)\n\n        return data[torch.arange(data.size(0)).long(), lengths - 1, :]\n\n\nclass SumPooling(Module):\n    """"""Get the sum of the hidden state of a sequence.""""""\n\n    def forward(self,\n                data: torch.Tensor,\n                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Performs a forward pass.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a tensor of shape [B x S x H]\n        padding_mask: torch.Tensor\n            The input mask, as a tensor of shape [B X S]\n\n        Returns\n        ----------\n        torch.Tensor\n            The output data, as a tensor of shape [B x H]\n\n        """"""\n        if padding_mask is None:\n            padding_mask = _default_padding_mask(data)\n\n        return _sum_with_padding_mask(data, padding_mask)\n\n\nclass AvgPooling(Module):\n    """"""Get the average of the hidden state of a sequence.""""""\n\n    def forward(self,\n                data: torch.Tensor,\n                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Performs a forward pass.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a tensor of shape [B x S x H]\n        padding_mask: torch.Tensor\n            The input mask, as a tensor of shape [B X S]\n\n        Returns\n        ----------\n        torch.Tensor\n            The output data, as a tensor of shape [B x H]\n\n        """"""\n        if padding_mask is None:\n            padding_mask = _default_padding_mask(data)\n\n        value_count = padding_mask.sum(dim=1).unsqueeze(1)\n        data = _sum_with_padding_mask(data, padding_mask)\n        return data / value_count\n\n\nclass StructuredSelfAttentivePooling(Module):\n    """"""Structured Self Attentive Pooling.""""""\n    def __init__(self,\n                 input_size: int,\n                 attention_heads: int = 16,\n                 attention_units: Sequence[int] = (300, ),\n                 output_activation: Optional[torch.nn.Module] = None,\n                 hidden_activation: Optional[torch.nn.Module] = None,\n                 is_biased: bool = False,\n                 input_dropout: float = 0.,\n                 attention_dropout: float = 0.,\n                 ):\n        """"""Initialize a self attention pooling layer\n\n        A generalized implementation of:\n        `A Structured Self-attentive Sentence Embedding`\n        https://arxiv.org/pdf/1703.03130.pdf\n\n        cite:\n        @article{lin2017structured,\n            title={A structured self-attentive sentence embedding},\n            author={Lin, Zhouhan and Feng, Minwei and\n            Santos, Cicero Nogueira dos and Yu, Mo and\n            Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},\n        journal={arXiv preprint arXiv:1703.03130},\n        year={2017}\n        }\n\n        Parameters\n        ----------\n        input_size : int\n            The input data dim\n        attention_heads: int\n            the number of attn heads\n        attention_units: Iterable[int]\n            the list of hidden dimensions of the MLP computing the attn\n        output_activation: Optional[torch.nn.Module]\n            The output activation to the attention weights.\n            Defaults to nn.Softmax, in accordance with the paper.\n        hidden_activation: Optional[torch.nn.Module]\n            The hidden activation to the attention weight computation.\n            Defaults to nn.Tanh, in accordance with the paper.\n        is_biased: bool\n            Whether the MLP should be biased. Defaults to false,\n            as in the paper.\n        input_dropout: float\n            dropout applied to the data argument of the forward method.\n        attention_dropout: float\n            dropout applied to the attention output before applying it\n            to the input for reduction. decouples the attn dropout\n            from the input dropout\n        """"""\n        super().__init__()\n        # creating dropout applied to input\n        self.in_drop = nn.Dropout(input_dropout) if input_dropout > 0. else nn.Identity()\n        # creating the MLP\n        # creating in, hidden and out dimensions\n        dimensions = [input_size, *attention_units, attention_heads]\n        layers = []\n        # iterating over hidden layers\n        for l in range(len(dimensions) - 2):\n            layers.append(nn.Linear(dimensions[l], dimensions[l + 1], bias=is_biased))\n            layers.append(nn.Tanh() if hidden_activation is None else hidden_activation)\n        # adding output layer\n        layers.append(nn.Linear(dimensions[-2], dimensions[-1], bias=False))\n        # adding attention output dropout\n        if attention_dropout > 0.:\n            layers.append(nn.Dropout(attention_dropout))\n        # instantiating the MLP\n        self.mlp = nn.Sequential(*layers)\n        # instantiating the ouput layer\n        self.output_activation = nn.Softmax(dim=1) \\\n            if output_activation is None else output_activation\n\n    def _compute_attention(self, data: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        """"""Computes the attention\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a tensor of shape [B x S x H]\n        mask: torch.Tensor\n            The input mask, as a tensor of shape [B X S]\n\n        Returns\n        ----------\n        torch.Tensor\n            The attention, as a tensor of shape [B x S x HEADS]\n\n        """"""\n        # input_tensor is 3D float tensor, batchsize x num_encs x dim\n        batch_size, num_encs, dim = data.shape\n        # apply input droput\n        data = self.in_drop(data)\n        # apply projection and reshape to\n        # batchsize x num_encs x num_heads\n        attention_logits = self.mlp(data.reshape(-1, dim)).reshape(batch_size, num_encs, -1)\n        # apply mask. dimension stays\n        # batchsize x num_encs x num_heads\n        if mask is not None:\n            mask = mask.unsqueeze(2).float()\n            attention_logits = attention_logits * mask + (1. - mask) * -1e20\n        # apply softmax. dimension stays\n        # batchsize x num_encs x num_heads\n        attention = self.output_activation(attention_logits)\n        return attention\n\n    def forward(self,\n                data: torch.Tensor,\n                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Performs a forward pass.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a tensor of shape [B x S x H]\n        mask: torch.Tensor\n            The input mask, as a tensor of shape [B X S]\n\n        Returns\n        ----------\n        torch.Tensor\n            The output data, as a tensor of shape [B x H]\n\n        """"""\n        # compute attention. attention is batchsize x num_encs x heads.\n        attention = self._compute_attention(data, mask)\n        # attend. attention is batchsize x num_encs x num_heads.\n        # data is batchsize x num_encs x dim\n        # resulting dim is batchsize x num_heads x dim\n        attended = torch.bmm(attention.transpose(1, 2), data)\n        # average over attention heads and return.\n        # dimension is batchsize x dim\n        return attended.mean(dim=1)\n\n\nclass GeneralizedPooling(StructuredSelfAttentivePooling):\n    """"""Self attention pooling.""""""\n\n    def __init__(self,\n                 input_size: int,\n                 attention_units: Sequence[int] = (300, ),\n                 output_activation: Optional[torch.nn.Module] = None,\n                 hidden_activation: Optional[torch.nn.Module] = None,\n                 is_biased: bool = True,\n                 input_dropout: float = 0.,\n                 attention_dropout: float = 0.,\n                 ):\n        """"""Initialize a self attention pooling layer\n\n        A generalized implementation of:\n        `Enhancing Sentence Embedding with Generalized Pooling`\n        https://arxiv.org/pdf/1806.09828.pdf\n\n        cite:\n        @article{chen2018enhancing,\n            title={Enhancing sentence embedding with\n                   generalized pooling},\n            author={Chen, Qian and Ling, Zhen-Hua and Zhu, Xiaodan},\n            journal={arXiv preprint arXiv:1806.09828},\n            year={2018}\n        }\n\n        Parameters\n        ----------\n        input_size : int\n            The input data dim\n        attention_units: Iterable[int]\n            the list of hidden dimensions of the MLP computing the attn\n        output_activation: Optional[torch.nn.Module]\n            The output activation to the attention weights.\n            Defaults to nn.Softmax, in accordance with the paper.\n        hidden_activation: Optional[torch.nn.Module]\n            The hidden activation to the attention weight computation.\n            Defaults to nn.Tanh, in accordance with the paper.\n        is_biased: bool\n            Whether the MLP should be biased. Defaults to true,\n            as in the paper.\n        input_dropout: float\n            dropout applied to the data argument of the forward method.\n        attention_dropout: float\n            dropout applied to the attention output before applying it\n            to the input for reduction. decouples the attn dropout\n            from the input dropout\n        """"""\n        super().__init__(\n            input_size=input_size,\n            attention_heads=input_size,\n            attention_units=attention_units,\n            output_activation=output_activation,\n            hidden_activation=nn.ReLU() if hidden_activation is None else hidden_activation,\n            is_biased=is_biased,\n            input_dropout=input_dropout,\n            attention_dropout=attention_dropout\n        )\n\n    def forward(self,\n                data: torch.Tensor,\n                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Performs a forward pass.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a tensor of shape [B x S x H]\n        mask: torch.Tensor\n            The input mask, as a tensor of shape [B X S]\n\n        Returns\n        ----------\n        torch.Tensor\n            The output data, as a tensor of shape [B x H]\n\n        """"""\n        # compute attention. attention is batchsize x num_encs x dim.\n        attention = self._compute_attention(data, mask)\n        # attend. attention is batchsize x num_encs x dim.\n        # data is batchsize x num_encs x dim\n        # resulting dim is batchsize x num_encs x dim\n        attended = attention * data\n        # average over attention heads and return.\n        # dimension is batchsize x dim\n        return attended.mean(dim=1)\n'"
flambe/nn/rnn.py,6,"b'# type: ignore[override]\n\nfrom typing import Optional, Tuple, cast\nimport warnings\nimport logging\n\nimport torch\nfrom torch import nn\nfrom torch import Tensor\n\nfrom flambe.nn.module import Module\n\nlogger = logging.getLogger(__name__)\n\n\nclass RNNEncoder(Module):\n    """"""Implements a multi-layer RNN.\n\n    This module can be used to create multi-layer RNN models, and\n    provides a way to reduce to output of the RNN to a single hidden\n    state by pooling the encoder states either by taking the maximum,\n    average, or by taking the last hidden state before padding.\n\n    Padding is dealt with by using torch\'s PackedSequence.\n\n    Attributes\n    ----------\n    rnn: nn.Module\n        The rnn submodule\n\n    """"""\n    def __init__(self,\n                 input_size: int,\n                 hidden_size: int,\n                 n_layers: int = 1,\n                 rnn_type: str = \'lstm\',\n                 dropout: float = 0,\n                 bidirectional: bool = False,\n                 layer_norm: bool = False,\n                 highway_bias: float = 0,\n                 rescale: bool = True,\n                 enforce_sorted: bool = False,\n                 **kwargs) -> None:\n        """"""Initializes the RNNEncoder object.\n\n        Parameters\n        ----------\n        input_size : int\n            The dimension the input data\n        hidden_size : int\n            The hidden dimension to encode the data in\n        n_layers : int, optional\n            The number of rnn layers, defaults to 1\n        rnn_type : str, optional\n           The type of rnn cell, one of: `lstm`, `gru`, `sru`\n           defaults to `lstm`\n        dropout : float, optional\n            Amount of dropout to use between RNN layers, defaults to 0\n        bidirectional : bool, optional\n            Set to use a bidrectional encoder, defaults to False\n        layer_norm : bool, optional\n            [SRU only] whether to use layer norm\n        highway_bias : float, optional\n            [SRU only] value to use for the highway bias\n        rescale : bool, optional\n            [SRU only] whether to use rescaling\n        enforce_sorted: bool\n            Whether rnn should enforce that sequences are ordered by\n            length. Requires True for ONNX support. Defaults to False.\n        kwargs\n            Additional parameters to be passed to SRU when building\n            the rnn.\n\n        Raises\n        ------\n        ValueError\n            The rnn type should be one of: `lstm`, `gru`, `sru`\n\n        """"""\n        super().__init__()\n\n        self.rnn_type = rnn_type\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.enforce_sorted = enforce_sorted\n        self.output_size = 2 * hidden_size if bidirectional else hidden_size\n\n        if rnn_type in [\'lstm\', \'gru\']:\n            if kwargs:\n                logger.warn(f""The following \'{kwargs}\' will be ignored "" +\n                            ""as they are only considered when using \'sru\' as "" +\n                            ""\'rnn_type\'"")\n\n            rnn_fn = nn.LSTM if rnn_type == \'lstm\' else nn.GRU\n            self.rnn = rnn_fn(input_size=input_size,\n                              hidden_size=hidden_size,\n                              num_layers=n_layers,\n                              dropout=dropout,\n                              bidirectional=bidirectional)\n        elif rnn_type == \'sru\':\n            from sru import SRU\n            try:\n                self.rnn = SRU(input_size,\n                               hidden_size,\n                               num_layers=n_layers,\n                               dropout=dropout,\n                               bidirectional=bidirectional,\n                               layer_norm=layer_norm,\n                               rescale=rescale,\n                               highway_bias=highway_bias,\n                               **kwargs)\n            except TypeError:\n                raise ValueError(f""Unkown kwargs passed to SRU: {kwargs}"")\n        else:\n            raise ValueError(f""Unkown rnn type: {rnn_type}, use of of: gru, sru, lstm"")\n\n    def forward(self,\n                data: Tensor,\n                state: Optional[Tensor] = None,\n                padding_mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n        """"""Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data : Tensor\n            The input data, as a float tensor of shape [B x S x E]\n        state: Tensor\n            An optional previous state of shape [L x B x H]\n        padding_mask: Tensor, optional\n            The padding mask of shape [B x S], dtype should be bool\n\n        Returns\n        -------\n        Tensor\n            The encoded output, as a float tensor of shape [B x S x H]\n        Tensor\n            The encoded state, as a float tensor of shape [L x B x H]\n\n        """"""\n        data = data.transpose(0, 1)\n        if padding_mask is not None:\n            padding_mask = padding_mask.transpose(0, 1)\n\n        if padding_mask is None:\n            # Default RNN behavior\n            output, state = self.rnn(data, state)\n        elif self.rnn_type == \'sru\':\n            # SRU takes a mask instead of PackedSequence objects\n            # ~ operator negates bool tensor in torch 1.3\n            output, state = self.rnn(data, state, mask_pad=(~padding_mask))\n        else:\n            # Deal with variable length sequences\n            lengths = padding_mask.long().sum(dim=0)\n            # Pass through the RNN\n            packed = nn.utils.rnn.pack_padded_sequence(data, lengths,\n                                                       enforce_sorted=self.enforce_sorted)\n            output, state = self.rnn(packed, state)\n            output, _ = nn.utils.rnn.pad_packed_sequence(output)\n\n        # TODO investigate why PyTorch returns type Any for output\n        return output.transpose(0, 1).contiguous(), state  # type: ignore\n\n\nclass PooledRNNEncoder(Module):\n    """"""Implement an RNNEncoder with additional pooling.\n\n    This class can be used to obtan a single encoded output for\n    an input sequence. It also ignores the state of the RNN.\n\n    """"""\n\n    def __init__(self,\n                 input_size: int,\n                 hidden_size: int,\n                 n_layers: int = 1,\n                 rnn_type: str = \'lstm\',\n                 dropout: float = 0,\n                 bidirectional: bool = False,\n                 layer_norm: bool = False,\n                 highway_bias: float = 0,\n                 rescale: bool = True,\n                 pooling: str = \'last\') -> None:\n        """"""Initializes the PooledRNNEncoder object.\n\n        Parameters\n        ----------\n        input_size : int\n            The dimension the input data\n        hidden_size : int\n            The hidden dimension to encode the data in\n        n_layers : int, optional\n            The number of rnn layers, defaults to 1\n        rnn_type : str, optional\n           The type of rnn cell, one of: `lstm`, `gru`, `sru`\n           defaults to `lstm`\n        dropout : float, optional\n            Amount of dropout to use between RNN layers, defaults to 0\n        bidirectional : bool, optional\n            Set to use a bidrectional encoder, defaults to False\n        layer_norm : bool, optional\n            [SRU only] whether to use layer norm\n        highway_bias : float, optional\n            [SRU only] value to use for the highway bias\n        rescale : bool, optional\n            [SRU only] whether to use rescaling\n        pooling : Optional[str], optional\n            If given, the output is pooled into a single hidden state,\n            through the given pooling routine. Should be one of:\n            ""first"", last"", ""average"", or ""sum"". Defaults to ""last""\n\n        Raises\n        ------\n        ValueError\n            The rnn type should be one of: `lstm`, `gru`, `sru`\n\n        """"""\n        super().__init__()\n\n        warnings.warn(""PooledRNNEncoder is deprecated, please use the Pooling \\\n                       module in the Embedder object"", DeprecationWarning)\n\n        self.pooling = pooling\n        self.rnn = RNNEncoder(input_size=input_size,\n                              hidden_size=hidden_size,\n                              n_layers=n_layers,\n                              rnn_type=rnn_type,\n                              dropout=dropout,\n                              bidirectional=bidirectional,\n                              layer_norm=layer_norm,\n                              highway_bias=highway_bias,\n                              rescale=rescale)\n        self.output_size = 2 * hidden_size if bidirectional else hidden_size\n\n    def forward(self,\n                data: Tensor,\n                state: Optional[Tensor] = None,\n                padding_mask: Optional[Tensor] = None) -> Tensor:\n        """"""Perform a forward pass through the network.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a float tensor of shape [B x S x E]\n        state: Tensor\n            An optional previous state of shape [L x B x H]\n        padding_mask: Tensor, optional\n            The padding mask of shape [B x S]\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded output, as a float tensor of shape [B x H]\n\n        """"""\n        output, _ = self.rnn(data, state=state, padding_mask=padding_mask)\n\n        # Apply pooling\n        if padding_mask is None:\n            padding_mask = torch.ones_like(output)\n\n        cast(torch.Tensor, padding_mask)\n        if self.pooling == \'average\':\n            output = (output * padding_mask.unsqueeze(2)).sum(dim=1)\n            output = output / padding_mask.sum(dim=1)\n        elif self.pooling == \'sum\':\n            output = (output * padding_mask.unsqueeze(2)).sum(dim=1)\n        elif self.pooling == \'last\':\n            lengths = padding_mask.long().sum(dim=1)\n            output = output[torch.arange(output.size(0)).long(), lengths - 1, :]\n        elif self.pooling == \'first\':\n            output = output[torch.arange(output.size(0)).long(), 0, :]\n        else:\n            raise ValueError(f""Invalid pooling type: {self.pooling}"")\n\n        return output\n'"
flambe/nn/sequential.py,6,"b'# type: ignore[override]\n\nfrom typing import Union, Dict\n\nimport torch\n\nfrom flambe.nn import Module\n\n\nclass Sequential(Module):\n    """"""Implement a Sequential module.\n\n    This class can be used in the same way as torch\'s nn.Sequential,\n    with the difference that it accepts kwargs arguments.\n\n    """"""\n    def __init__(self, **kwargs: Dict[str, Union[Module, torch.nn.Module]]) -> None:\n        """"""Initialize the Sequential module.\n\n        Parameters\n        ----------\n        kwargs: Dict[str, Union[Module, torch.nn.Module]]\n            The list of modules.\n\n        """"""\n        super().__init__()\n\n        modules = []\n        for name, module in kwargs.items():\n            setattr(self, name, module)\n            modules.append(module)\n\n        self.seq = torch.nn.Sequential(modules)\n\n    def forward(self, data: torch.Tensor) -> torch.Tensor:\n        """"""Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data: torch.Tensor\n            input to the model\n\n        Returns\n        -------\n        output: torch.Tensor\n            output of the model\n\n        """"""\n        return self.seq(data)\n'"
flambe/nn/softmax.py,3,"b'# type: ignore[override]\n\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\nfrom flambe.nn.mlp import MLPEncoder\nfrom flambe.nn.module import Module\n\n\nclass SoftmaxLayer(Module):\n    """"""Implement an SoftmaxLayer module.\n\n    Can be used to form a classifier out of any encoder.\n    Note: by default takes the log_softmax so that it can be fed to\n    the NLLLoss module. You can disable this behavior through the\n    `take_log` argument.\n\n    """"""\n    def __init__(self,\n                 input_size: int,\n                 output_size: int,\n                 mlp_layers: int = 1,\n                 mlp_dropout: float = 0.,\n                 mlp_hidden_activation: Optional[nn.Module] = None,\n                 take_log: bool = True) -> None:\n        """"""Initialize the SoftmaxLayer.\n\n        Parameters\n        ----------\n        input_size : int\n            Input size of the decoder, usually the hidden size of\n            some encoder.\n        output_size : int\n            The output dimension, usually the number of target labels\n        mlp_layers : int\n            The number of layers in the MLP\n        mlp_dropout: float, optional\n            Dropout to be used before each MLP layer\n        mlp_hidden_activation: nn.Module, optional\n            Any PyTorch activation layer, defaults to None\n        take_log: bool, optional\n            If ``True``, compute the LogSoftmax to be fed in NLLLoss.\n            Defaults to ``False``.\n\n        """"""\n        super().__init__()\n\n        softmax = nn.LogSoftmax(dim=-1) if take_log else nn.Softmax()\n        self.mlp = MLPEncoder(input_size=input_size, output_size=output_size,\n                              n_layers=mlp_layers, dropout=mlp_dropout,\n                              hidden_activation=mlp_hidden_activation,\n                              output_activation=softmax)\n\n    def forward(self, data: torch.Tensor) -> torch.Tensor:\n        """"""Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data: torch.Tensor\n            input to the model of shape (*, input_size)\n\n        Returns\n        -------\n        output: torch.Tensor\n            output of the model of shape (*, output_size)\n\n        """"""\n        return self.mlp(data)\n'"
flambe/nn/transformer.py,63,"b'# type: ignore[override]\n\n""""""\nCode taken from the PyTorch source code. Slightly modified to improve\nthe interface to the TransformerEncoder, and TransformerDecoder modules.\n\n""""""\nimport copy\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom flambe.nn import Module\n\n\nclass Transformer(Module):\n    """"""A Transformer model\n\n    User is able to modify the attributes as needed. The architechture\n    is based on the paper ""Attention Is All You Need"". Ashish Vaswani,\n    Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\n    Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\n    Attention is all you need. In Advances in Neural Information\n    Processing Systems, pages 6000-6010.\n\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 d_model: int = 512,\n                 nhead: int = 8,\n                 num_encoder_layers: int = 6,\n                 num_decoder_layers: int = 6,\n                 dim_feedforward: int = 2048,\n                 dropout: float = 0.1) -> None:\n        """"""Initialize the Transformer Model.\n\n        Parameters\n        ----------\n        input_size : int, optional\n            dimension of embeddings. If different from\n            d_model, then a linear layer is added to project from\n            input_size to d_model.\n        d_model : int, optional\n            the number of expected features in the\n            encoder/decoder inputs (default=512).\n        nhead : int, optional\n            the number of heads in the multiheadattention\n            models (default=8).\n        num_encoder_layers : int, optional\n            the number of sub-encoder-layers in the encoder\n            (default=6).\n        num_decoder_layers : int, optional\n            the number of sub-decoder-layers in the decoder\n            (default=6).\n        dim_feedforward : int, optional\n            the dimension of the feedforward network model\n            (default=2048).\n        dropout : float, optional\n            the dropout value (default=0.1).\n\n        """"""\n        super().__init__()\n\n        self.encoder = TransformerEncoder(input_size,\n                                          d_model,\n                                          nhead,\n                                          dim_feedforward,\n                                          num_encoder_layers,\n                                          dropout)\n\n        self.decoder = TransformerDecoder(input_size,\n                                          d_model,\n                                          nhead,\n                                          dim_feedforward,\n                                          num_encoder_layers,\n                                          dropout)\n\n    def forward(self,  # type: ignore\n                src: torch.Tensor,\n                tgt: torch.Tensor,\n                src_mask: Optional[torch.Tensor] = None,\n                tgt_mask: Optional[torch.Tensor] = None,\n                memory_mask: Optional[torch.Tensor] = None,\n                src_key_padding_mask: Optional[torch.Tensor] = None,\n                tgt_key_padding_mask: Optional[torch.Tensor] = None,\n                memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Take in and process masked source/target sequences.\n\n        Parameters\n        ----------\n        src: torch.Tensor\n            the sequence to the encoder (required).\n            shape: :math:`(N, S, E)`.\n        tgt: torch.Tensor\n            the sequence to the decoder (required).\n            shape: :math:`(N, T, E)`.\n        src_mask: torch.Tensor, optional\n            the additive mask for the src sequence (optional).\n            shape: :math:`(S, S)`.\n        tgt_mask: torch.Tensor, optional\n            the additive mask for the tgt sequence (optional).\n            shape: :math:`(T, T)`.\n        memory_mask: torch.Tensor, optional\n            the additive mask for the encoder output (optional).\n            shape: :math:`(T, S)`.\n        src_key_padding_mask: torch.Tensor, optional\n            the ByteTensor mask for src keys per batch (optional).\n            shape: :math:`(N, S)`\n        tgt_key_padding_mask: torch.Tensor, optional\n            the ByteTensor mask for tgt keys per batch (optional).\n            shape: :math:`(N, T)`.\n        memory_key_padding_mask: torch.Tensor, optional\n            the ByteTensor mask for memory keys per batch (optional).\n            shape"" :math:`(N, S)`.\n\n        Returns\n        -------\n        output: torch.Tensor\n            The output sequence, shape: :math:`(N, T, E)`.\n\n        Note: [src/tgt/memory]_mask should be filled with\n            float(\'-inf\') for the masked positions and float(0.0) else.\n            These masks ensure that predictions for position i depend\n            only on the unmasked positions j and are applied identically\n            for each sequence in a batch.\n            [src/tgt/memory]_key_padding_mask should be a ByteTensor\n            where False values are positions that should be masked with\n            float(\'-inf\') and True values will be unchanged.\n            This mask ensures that no information will be taken from\n            position i if it is masked, and has a separate mask for each\n            sequence in a batch.\n        Note: Due to the multi-head attention architecture in the\n            transformer model, the output sequence length of a\n            transformer is same as the input sequence\n            (i.e. target) length of the decode.\n\n            where S is the source sequence length, T is the target\n            sequence length, N is the batchsize, E is the feature number\n\n        """"""\n        if src.size(1) != tgt.size(1):\n            raise RuntimeError(""the batch number of src and tgt must be equal"")\n\n        if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n            raise RuntimeError(""the feature number of src and tgt must be equal to d_model"")\n\n        memory = self.encoder(src,\n                              mask=src_mask,\n                              padding_mask=src_key_padding_mask)\n        output = self.decoder(tgt,\n                              memory,\n                              tgt_mask=tgt_mask,\n                              memory_mask=memory_mask,\n                              padding_mask=tgt_key_padding_mask,\n                              memory_key_padding_mask=memory_key_padding_mask)\n        return output\n\n\nclass TransformerEncoder(Module):\n    """"""TransformerEncoder is a stack of N encoder layers.""""""\n\n    def __init__(self,\n                 input_size: int = 512,\n                 d_model: int = 512,\n                 nhead: int = 8,\n                 num_layers: int = 6,\n                 dim_feedforward: int = 2048,\n                 dropout: float = 0.1) -> None:\n        """"""Initialize the TransformerEncoder.\n\n        Parameters\n        ---------\n        input_size : int\n            The embedding dimension of the model.  If different from\n            d_model, a linear projection layer is added.\n        d_model : int\n            the number of expected features in encoder/decoder inputs.\n            Default ``512``.\n        nhead : int, optional\n            the number of heads in the multiheadattention\n            Default ``8``.\n        num_layers : int\n            the number of sub-encoder-layers in the encoder (required).\n            Default ``6``.\n        dim_feedforward : int, optional\n            the inner feedforard dimension. Default ``2048``.\n        dropout : float, optional\n            the dropout percentage. Default ``0.1``.\n\n        """"""\n        super().__init__()\n\n        self.input_size = input_size\n        self.d_model = d_model\n\n        if input_size != d_model:\n            self.proj = nn.Linear(input_size, d_model)\n\n        layer = TransformerEncoderLayer(d_model,\n                                        nhead,\n                                        dim_feedforward,\n                                        dropout)\n\n        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n        self.num_layers = num_layers\n\n        self._reset_parameters()\n\n    def forward(self,  # type: ignore\n                src: torch.Tensor,\n                memory: Optional[torch.Tensor] = None,\n                mask: Optional[torch.Tensor] = None,\n                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Pass the input through the endocder layers in turn.\n\n        Parameters\n        ----------\n        src: torch.Tensor\n            The sequence to the encoder (required).\n        memory: torch.Tensor, optional\n            Optional memory, unused by default.\n        mask: torch.Tensor, optional\n            The mask for the src sequence (optional).\n        padding_mask: torch.Tensor, optional\n            The mask for the src keys per batch (optional).\n            Should be True for tokens to leave untouched, and False\n            for padding tokens.\n\n        """"""\n        output = src.transpose(0, 1)\n\n        if self.input_size != self.d_model:\n            output = self.proj(output)\n\n        for i in range(self.num_layers):\n            output = self.layers[i](output,\n                                    memory=memory,\n                                    src_mask=mask,\n                                    padding_mask=padding_mask)\n\n        return output.transpose(0, 1)\n\n    def _reset_parameters(self):\n        """"""Initiate parameters in the transformer model.""""""\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n\nclass TransformerDecoder(Module):\n    """"""TransformerDecoder is a stack of N decoder layers""""""\n\n    def __init__(self,\n                 input_size: int,\n                 d_model: int,\n                 nhead: int,\n                 num_layers: int,\n                 dim_feedforward: int = 2048,\n                 dropout: float = 0.1) -> None:\n        """"""Initialize the TransformerDecoder.\n\n        Parameters\n        ---------\n        input_size : int\n            The embedding dimension of the model.  If different from\n            d_model, a linear projection layer is added.\n        d_model : int\n            The number of expected features in encoder/decoder inputs.\n        nhead : int, optional\n            The number of heads in the multiheadattention.\n        num_layers : int\n            The number of sub-encoder-layers in the encoder (required).\n        dim_feedforward : int, optional\n            The inner feedforard dimension, by default 2048.\n        dropout : float, optional\n            The dropout percentage, by default 0.1.\n\n        """"""\n        super().__init__()\n\n        self.input_size = input_size\n        self.d_model = d_model\n\n        if input_size != d_model:\n            self.proj = nn.Linear(input_size, d_model)\n\n        layer = TransformerDecoderLayer(d_model,\n                                        nhead,\n                                        dim_feedforward,\n                                        dropout)\n\n        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n        self.num_layers = num_layers\n\n        self._reset_parameters()\n\n    def forward(self,  # type: ignore\n                tgt: torch.Tensor,\n                memory: torch.Tensor,\n                tgt_mask: Optional[torch.Tensor] = None,\n                memory_mask: Optional[torch.Tensor] = None,\n                padding_mask: Optional[torch.Tensor] = None,\n                memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Pass the inputs (and mask) through the decoder layer in turn.\n\n        Parameters\n        ----------\n        tgt: torch.Tensor\n            The sequence to the decoder (required).\n        memory: torch.Tensor\n            The sequence from the last layer of the encoder (required).\n        tgt_mask: torch.Tensor, optional\n            The mask for the tgt sequence (optional).\n        memory_mask: torch.Tensor, optional\n            The mask for the memory sequence (optional).\n        padding_mask: torch.Tensor, optional\n            The mask for the tgt keys per batch (optional).\n            Should be True for tokens to leave untouched, and False\n            for padding tokens.\n        memory_key_padding_mask: torch.Tensor, optional\n            The mask for the memory keys per batch (optional).\n\n        Returns\n        -------\n        torch.Tensor\n\n        """"""\n        output = tgt.transpose(0, 1)\n\n        if self.input_size != self.d_model:\n            output = self.proj(output)\n\n        for i in range(self.num_layers):\n            output = self.layers[i](output,\n                                    memory,\n                                    tgt_mask=tgt_mask,\n                                    memory_mask=memory_mask,\n                                    padding_mask=padding_mask,\n                                    memory_key_padding_mask=memory_key_padding_mask)\n\n        return output.transpose(0, 1)\n\n    def _reset_parameters(self):\n        """"""Initiate parameters in the transformer model.""""""\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n\nclass TransformerEncoderLayer(Module):\n    """"""TransformerEncoderLayer is made up of self-attn and feedforward.\n\n    This standard encoder layer is based on the paper ""Attention Is\n    All You Need"". Ashish Vaswani, Noam Shazeer, Niki Parmar,\n    Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\n    Illia Polosukhin. 2017. Attention is all you need. In Advances in\n    Neural Information Processing Systems, pages 6000-6010. Users may\n    modify or implement in a different way during application.\n\n    """"""\n\n    def __init__(self,\n                 d_model: int,\n                 nhead: int,\n                 dim_feedforward: int = 2048,\n                 dropout: float = 0.1) -> None:\n        """"""Initialize a TransformerEncoderLayer.\n\n        Parameters\n        ----------\n        d_model : int\n            The number of expected features in the input.\n        n_head : int\n            The number of heads in the multiheadattention models.\n        dim_feedforward : int, optional\n            The dimension of the feedforward network (default=2048).\n        dropout : float, optional\n            The dropout value (default=0.1).\n\n        """"""\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n        self.dropout = nn.Dropout(dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self,  # type: ignore\n                src: torch.Tensor,\n                memory: Optional[torch.Tensor] = None,\n                src_mask: Optional[torch.Tensor] = None,\n                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Pass the input through the endocder layer.\n\n        Parameters\n        ----------\n        src: torch.Tensor\n            The seqeunce to the encoder layer (required).\n        memory: torch.Tensor, optional\n            Optional memory from previous sequence, unused by default.\n        src_mask: torch.Tensor, optional\n            The mask for the src sequence (optional).\n        padding_mask: torch.Tensor, optional\n            The mask for the src keys per batch (optional).\n            Should be True for tokens to leave untouched, and False\n            for padding tokens.\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor of shape [B x S x H]\n\n        """"""\n        # Transpose and reverse\n        if padding_mask is not None:\n            padding_mask = ~padding_mask.bool()\n\n        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n                              key_padding_mask=padding_mask)[0]\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src\n\n\nclass TransformerDecoderLayer(Module):\n    """"""A TransformerDecoderLayer.\n\n    A TransformerDecoderLayer is made up of self-attn, multi-head-attn\n    and feedforward network. This standard decoder layer is based on the\n    paper ""Attention Is All You Need"". Ashish Vaswani, Noam Shazeer,\n    Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\n    Kaiser, and Illia Polosukhin. 2017. Attention is all you need.\n    In Advances in Neural Information Processing Systems,\n    pages 6000-6010. Users may modify or implement in a different way\n    during application.\n\n    """"""\n\n    def __init__(self,\n                 d_model: int,\n                 nhead: int,\n                 dim_feedforward: int = 2048,\n                 dropout: float = 0.1) -> None:\n        """"""Initialize a TransformerDecoder.\n\n        Parameters\n        ----------\n        d_model : int\n            The number of expected features in the input.\n        n_head : int\n            The number of heads in the multiheadattention models.\n        dim_feedforward : int, optional\n            The dimension of the feedforward network (default=2048).\n        dropout : float, optional\n            The dropout value (default=0.1).\n\n        """"""\n        super().__init__()\n\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n        self.dropout = nn.Dropout(dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n    def forward(self,  # type: ignore\n                tgt: torch.Tensor,\n                memory: torch.Tensor,\n                tgt_mask: Optional[torch.Tensor] = None,\n                memory_mask: Optional[torch.Tensor] = None,\n                padding_mask: Optional[torch.Tensor] = None,\n                memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        r""""""Pass the inputs (and mask) through the decoder layer.\n\n        Parameters\n        ----------\n        tgt: torch.Tensor\n            The sequence to the decoder layer (required).\n        memory: torch.Tensor\n            The sequnce from the last layer of the encoder (required).\n        tgt_mask: torch.Tensor, optional\n            The mask for the tgt sequence (optional).\n        memory_mask: torch.Tensor, optional\n            the mask for the memory sequence (optional).\n        padding_mask: torch.Tensor, optional\n            the mask for the tgt keys per batch (optional).\n            Should be True for tokens to leave untouched, and False\n            for padding tokens.\n        memory_key_padding_mask: torch.Tensor, optional\n            the mask for the memory keys per batch (optional).\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor of shape [T x B x H]\n\n        """"""\n        # Transpose anr reverse\n        if padding_mask is not None:\n            padding_mask = ~padding_mask\n\n        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n                              key_padding_mask=padding_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n                                   key_padding_mask=memory_key_padding_mask)[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n\n        return tgt\n\n\ndef generate_square_subsequent_mask(self, sz):\n    r""""""Generate a square mask for the sequence.\n\n    The masked positions are filled with float(\'-inf\').\n    Unmasked positions are filled with float(0.0).\n    """"""\n    mask = (torch.triu(torch.ones(sz, sz)) == 1).t()\n    mask = mask.float().masked_fill(mask == 0, float(\'-inf\')).masked_fill(mask == 1, float(0.0))\n    return mask\n'"
flambe/nn/transformer_sru.py,68,"b'# type: ignore[override]\n\nimport copy\nfrom typing import Optional, Dict, Any, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom sru import SRUCell\n\nfrom flambe.nn import Module\n\n\nclass TransformerSRU(Module):\n    """"""A Transformer with an SRU replacing the FFN.""""""\n\n    def __init__(self,\n                 input_size: int = 512,\n                 d_model: int = 512,\n                 nhead: int = 8,\n                 num_encoder_layers: int = 6,\n                 num_decoder_layers: int = 6,\n                 dim_feedforward: int = 2048,\n                 dropout: float = 0.1,\n                 sru_dropout: Optional[float] = None,\n                 bidrectional: bool = False,\n                 **kwargs: Dict[str, Any]) -> None:\n        """"""Initialize the TransformerSRU Model.\n\n        Parameters\n        ----------\n        input_size : int, optional\n            dimension of embeddings (default=512). if different from\n            d_model, then a linear layer is added to project from\n            input_size to d_model.\n        d_model : int, optional\n            the number of expected features in the\n            encoder/decoder inputs (default=512).\n        nhead : int, optional\n            the number of heads in the multiheadattention\n            models (default=8).\n        num_encoder_layers : int, optional\n            the number of sub-encoder-layers in the encoder\n            (default=6).\n        num_decoder_layers : int, optional\n            the number of sub-decoder-layers in the decoder\n            (default=6).\n        dim_feedforward : int, optional\n            the dimension of the feedforward network model\n            (default=2048).\n        dropout : float, optional\n            the dropout value (default=0.1).\n        sru_dropout: float, optional\n            Dropout for the SRU cell. If not given, uses the same\n            dropout value as the rest of the transformer.\n        bidrectional: bool, optional\n            Whether the SRU Encoder module should be bidrectional.\n            Defaul ``False``.\n\n        Extra keyword arguments are passed to the SRUCell.\n\n        """"""\n        super().__init__()\n\n        self.encoder = TransformerSRUEncoder(input_size,\n                                             d_model,\n                                             nhead,\n                                             dim_feedforward,\n                                             num_encoder_layers,\n                                             dropout,\n                                             sru_dropout,\n                                             bidrectional,\n                                             **kwargs)\n\n        self.decoder = TransformerSRUDecoder(input_size,\n                                             d_model,\n                                             nhead,\n                                             dim_feedforward,\n                                             num_encoder_layers,\n                                             dropout,\n                                             sru_dropout,\n                                             **kwargs)\n\n    def forward(self,  # type: ignore\n                src: torch.Tensor,\n                tgt: torch.Tensor,\n                src_mask: Optional[torch.Tensor] = None,\n                tgt_mask: Optional[torch.Tensor] = None,\n                memory_mask: Optional[torch.Tensor] = None,\n                src_key_padding_mask: Optional[torch.Tensor] = None,\n                tgt_key_padding_mask: Optional[torch.Tensor] = None,\n                memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Take in and process masked source/target sequences.\n\n        Parameters\n        ----------\n        src: torch.Tensor\n            the sequence to the encoder (required).\n            shape: :math:`(N, S, E)`.\n        tgt: torch.Tensor\n            the sequence to the decoder (required).\n            shape: :math:`(N, T, E)`.\n        src_mask: torch.Tensor, optional\n            the additive mask for the src sequence (optional).\n            shape: :math:`(S, S)`.\n        tgt_mask: torch.Tensor, optional\n            the additive mask for the tgt sequence (optional).\n            shape: :math:`(T, T)`.\n        memory_mask: torch.Tensor, optional\n            the additive mask for the encoder output (optional).\n            shape: :math:`(T, S)`.\n        src_key_padding_mask: torch.Tensor, optional\n            the ByteTensor mask for src keys per batch (optional).\n            shape: :math:`(N, S)`.\n        tgt_key_padding_mask: torch.Tensor, optional\n            the ByteTensor mask for tgt keys per batch (optional).\n            shape: :math:`(N, T)`.\n        memory_key_padding_mask: torch.Tensor, optional\n            the ByteTensor mask for memory keys per batch (optional).\n            shape"" :math:`(N, S)`.\n\n        Returns\n        -------\n        output: torch.Tensor\n            The output sequence, shape: :math:`(T, N, E)`.\n\n        Note: [src/tgt/memory]_mask should be filled with\n            float(\'-inf\') for the masked positions and float(0.0) else.\n            These masks ensure that predictions for position i depend\n            only on the unmasked positions j and are applied identically\n            for each sequence in a batch.\n            [src/tgt/memory]_key_padding_mask should be a ByteTensor\n            where False values are positions that should be masked with\n            float(\'-inf\') and True values will be unchanged.\n            This mask ensures that no information will be taken from\n            position i if it is masked, and has a separate mask for each\n            sequence in a batch.\n        Note: Due to the multi-head attention architecture in the\n            transformer model, the output sequence length of a\n            transformer is same as the input sequence\n            (i.e. target) length of the decode.\n\n            where S is the source sequence length, T is the target\n            sequence length, N is the batchsize, E is the feature number\n\n        """"""\n        if src.size(1) != tgt.size(1):\n            raise RuntimeError(""the batch number of src and tgt must be equal"")\n\n        if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n            raise RuntimeError(""the feature number of src and tgt must be equal to d_model"")\n\n        memory, state = self.encoder(src,\n                                     mask=src_mask,\n                                     padding_mask=src_key_padding_mask)\n        output = self.decoder(tgt,\n                              memory,\n                              state=state,\n                              tgt_mask=tgt_mask,\n                              memory_mask=memory_mask,\n                              padding_mask=tgt_key_padding_mask,\n                              memory_key_padding_mask=memory_key_padding_mask)\n        return output\n\n\nclass TransformerSRUEncoder(Module):\n    """"""A TransformerSRUEncoder with an SRU replacing the FFN.""""""\n\n    def __init__(self,\n                 input_size: int = 512,\n                 d_model: int = 512,\n                 nhead: int = 8,\n                 num_layers: int = 6,\n                 dim_feedforward: int = 2048,\n                 dropout: float = 0.1,\n                 sru_dropout: Optional[float] = None,\n                 bidirectional: bool = False,\n                 **kwargs: Dict[str, Any]) -> None:\n        """"""Initialize the TransformerEncoder.\n\n        Parameters\n        ---------\n        input_size : int\n            The embedding dimension of the model.  If different from\n            d_model, a linear projection layer is added.\n        d_model : int\n            the number of expected features in encoder/decoder inputs.\n            Default ``512``.\n        nhead : int, optional\n            the number of heads in the multiheadattention\n            Default ``8``.\n        num_layers : int\n            the number of sub-encoder-layers in the encoder (required).\n            Default ``6``.\n        dim_feedforward : int, optional\n            the inner feedforard dimension. Default ``2048``.\n        dropout : float, optional\n            the dropout percentage. Default ``0.1``.\n        sru_dropout: float, optional\n            Dropout for the SRU cell. If not given, uses the same\n            dropout value as the rest of the transformer.\n        bidirectional: bool\n            Whether the SRU module should be bidrectional.\n            Defaul ``False``.\n\n        Extra keyword arguments are passed to the SRUCell.\n\n        """"""\n        super().__init__()\n\n        self.input_size = input_size\n        self.d_model = d_model\n\n        if input_size != d_model:\n            self.proj = nn.Linear(input_size, d_model)\n\n        layer = TransformerSRUEncoderLayer(d_model,\n                                           nhead,\n                                           dim_feedforward,\n                                           dropout,\n                                           sru_dropout,\n                                           bidirectional)\n\n        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n        self.num_layers = num_layers\n\n        self._reset_parameters()\n\n    def forward(self,  # type: ignore\n                src: torch.Tensor,\n                state: Optional[torch.Tensor] = None,\n                mask: Optional[torch.Tensor] = None,\n                padding_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Pass the input through the endocder layers in turn.\n\n        Parameters\n        ----------\n        src: torch.Tensor\n            The sequnce to the encoder (required).\n        state: Optional[torch.Tensor]\n            Optional state from previous sequence encoding.\n            Only passed to the SRU (not used to perform multihead\n            attention).\n        mask: torch.Tensor, optional\n            The mask for the src sequence (optional).\n        padding_mask: torch.Tensor, optional\n            The mask for the src keys per batch (optional).\n            Should be True for tokens to leave untouched, and False\n            for padding tokens.\n\n        """"""\n        output = src.transpose(0, 1)\n\n        if self.input_size != self.d_model:\n            output = self.proj(output)\n\n        new_states = []\n        for i in range(self.num_layers):\n            input_state = state[i] if state is not None else None\n            output, new_state = self.layers[i](output,\n                                               state=input_state,\n                                               src_mask=mask,\n                                               padding_mask=padding_mask)\n            new_states.append(new_state)\n\n        new_states = torch.stack(new_states, dim=0)\n        return output.transpose(0, 1), new_states\n\n    def _reset_parameters(self):\n        """"""Initiate parameters in the transformer model.""""""\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n\nclass TransformerSRUDecoder(Module):\n    """"""A TransformerSRUDecoderwith an SRU replacing the FFN.""""""\n\n    def __init__(self,\n                 input_size: int = 512,\n                 d_model: int = 512,\n                 nhead: int = 8,\n                 num_layers: int = 6,\n                 dim_feedforward: int = 2048,\n                 dropout: float = 0.1,\n                 sru_dropout: Optional[float] = None,\n                 **kwargs: Dict[str, Any]) -> None:\n        """"""Initialize the TransformerEncoder.\n\n        Parameters\n        ---------\n        input_size : int\n            The embedding dimension of the model.  If different from\n            d_model, a linear projection layer is added.\n        d_model : int\n            the number of expected features in encoder/decoder inputs.\n            Default ``512``.\n        nhead : int, optional\n            the number of heads in the multiheadattention\n            Default ``8``.\n        num_layers : int\n            the number of sub-encoder-layers in the encoder (required).\n            Default ``6``.\n        dim_feedforward : int, optional\n            the inner feedforard dimension. Default ``2048``.\n        dropout : float, optional\n            the dropout percentage. Default ``0.1``.\n        sru_dropout: float, optional\n            Dropout for the SRU cell. If not given, uses the same\n            dropout value as the rest of the transformer.\n\n        Extra keyword arguments are passed to the SRUCell.\n\n        """"""\n        super().__init__()\n\n        self.input_size = input_size\n        self.d_model = d_model\n\n        if input_size != d_model:\n            self.proj = nn.Linear(input_size, d_model)\n\n        layer = TransformerSRUDecoderLayer(d_model,\n                                           nhead,\n                                           dim_feedforward,\n                                           dropout,\n                                           sru_dropout)\n\n        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n        self.num_layers = num_layers\n\n        self._reset_parameters()\n\n    def forward(self,  # type: ignore\n                tgt: torch.Tensor,\n                memory: torch.Tensor,\n                state: Optional[torch.Tensor] = None,\n                tgt_mask: Optional[torch.Tensor] = None,\n                memory_mask: Optional[torch.Tensor] = None,\n                padding_mask: Optional[torch.Tensor] = None,\n                memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Pass the inputs (and mask) through the decoder layer in turn.\n\n        Parameters\n        ----------\n        tgt: torch.Tensor\n            The sequence to the decoder (required).\n        memory: torch.Tensor\n            The sequence from the last layer of the encoder (required).\n        state: Optional[torch.Tensor]\n            Optional state from previous sequence encoding.\n            Only passed to the SRU (not used to perform multihead\n            attention).\n        tgt_mask: torch.Tensor, optional\n            The mask for the tgt sequence (optional).\n        memory_mask: torch.Tensor, optional\n            The mask for the memory sequence (optional).\n        padding_mask: torch.Tensor, optional\n            The mask for the tgt keys per batch (optional).\n            Should be True for tokens to leave untouched, and False\n            for padding tokens.\n        memory_key_padding_mask: torch.Tensor, optional\n            The mask for the memory keys per batch (optional).\n\n        Returns\n        -------\n        torch.Tensor\n\n        """"""\n        output = tgt.transpose(0, 1)\n        state = state or [None] * self.num_layers\n\n        if self.input_size != self.d_model:\n            output = self.proj(output)\n\n        for i in range(self.num_layers):\n            output = self.layers[i](output,\n                                    memory,\n                                    state=state[i],\n                                    tgt_mask=tgt_mask,\n                                    memory_mask=memory_mask,\n                                    padding_mask=padding_mask,\n                                    memory_key_padding_mask=memory_key_padding_mask)\n\n        return output.transpose(0, 1)\n\n    def _reset_parameters(self):\n        """"""Initiate parameters in the transformer model.""""""\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n\nclass TransformerSRUEncoderLayer(Module):\n    """"""A TransformerSRUEncoderLayer with an SRU replacing the FFN.""""""\n\n    def __init__(self,\n                 d_model: int,\n                 nhead: int,\n                 dim_feedforward: int = 2048,\n                 dropout: float = 0.1,\n                 sru_dropout: Optional[float] = None,\n                 bidirectional: bool = False,\n                 **kwargs: Dict[str, Any]) -> None:\n        """"""Initialize a TransformerSRUEncoderLayer.\n\n        Parameters\n        ----------\n        d_model : int\n            The number of expected features in the input.\n        n_head : int\n            The number of heads in the multiheadattention models.\n        dim_feedforward : int, optional\n            The dimension of the feedforward network (default=2048).\n        dropout : float, optional\n            The dropout value (default=0.1).\n        sru_dropout: float, optional\n            Dropout for the SRU cell. If not given, uses the same\n            dropout value as the rest of the transformer.\n        bidirectional: bool\n            Whether the SRU module should be bidrectional.\n            Defaul ``False``.\n\n        Extra keyword arguments are passed to the SRUCell.\n\n        """"""\n        super().__init__()\n\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.sru = SRUCell(d_model,\n                           dim_feedforward,\n                           dropout,\n                           sru_dropout or dropout,\n                           bidirectional=bidirectional,\n                           has_skip_term=False, **kwargs)\n\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self,  # type: ignore\n                src: torch.Tensor,\n                state: Optional[torch.Tensor] = None,\n                src_mask: Optional[torch.Tensor] = None,\n                padding_mask: Optional[torch.Tensor] = None\n                ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Pass the input through the endocder layer.\n\n        Parameters\n        ----------\n        src: torch.Tensor\n            The sequence to the encoder layer (required).\n        state: Optional[torch.Tensor]\n            Optional state from previous sequence encoding.\n            Only passed to the SRU (not used to perform multihead\n            attention).\n        src_mask: torch.Tensor, optional\n            The mask for the src sequence (optional).\n        padding_mask: torch.Tensor, optional\n            The mask for the src keys per batch (optional).\n            Should be True for tokens to leave untouched, and False\n            for padding tokens.\n\n        Returns\n        -------\n        torch.Tensor\n            Output Tensor of shape [S x B x H]\n        torch.Tensor\n            Output state of the SRU of shape [N x B x H]\n\n        """"""\n        # Transpose and reverse\n        reversed_mask = None\n        if padding_mask is not None:\n            reversed_mask = ~padding_mask\n\n        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n                              key_padding_mask=reversed_mask)[0]\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src2, state = self.sru(src, state, mask_pad=padding_mask)\n        src2 = self.linear2(src2)\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n\n        return src, state\n\n\nclass TransformerSRUDecoderLayer(Module):\n    """"""A TransformerSRUDecoderLayer with an SRU replacing the FFN.""""""\n\n    def __init__(self,\n                 d_model: int,\n                 nhead: int,\n                 dim_feedforward: int = 2048,\n                 dropout: float = 0.1,\n                 sru_dropout: Optional[float] = None,\n                 **kwargs: Dict[str, Any]) -> None:\n        """"""Initialize a TransformerDecoder.\n\n        Parameters\n        ----------\n        d_model : int\n            The number of expected features in the input.\n        n_head : int\n            The number of heads in the multiheadattention models.\n        dim_feedforward : int, optional\n            The dimension of the feedforward network (default=2048).\n        dropout : float, optional\n            The dropout value (default=0.1).\n        sru_dropout: float, optional\n            Dropout for the SRU cell. If not given, uses the same\n            dropout value as the rest of the transformer.\n\n        Extra keyword arguments are passed to the SRUCell.\n\n        """"""\n        super().__init__()\n\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.sru = SRUCell(d_model,\n                           dim_feedforward,\n                           dropout,\n                           sru_dropout or dropout,\n                           bidirectional=False,\n                           has_skip_term=False, **kwargs)\n\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n    def forward(self,  # type: ignore\n                tgt: torch.Tensor,\n                memory: torch.Tensor,\n                state: Optional[torch.Tensor] = None,\n                tgt_mask: Optional[torch.Tensor] = None,\n                memory_mask: Optional[torch.Tensor] = None,\n                padding_mask: Optional[torch.Tensor] = None,\n                memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        r""""""Pass the inputs (and mask) through the decoder layer.\n\n        Parameters\n        ----------\n        tgt: torch.Tensor\n            The sequence to the decoder layer (required).\n        memory: torch.Tensor\n            The sequence from the last layer of the encoder (required).\n        state: Optional[torch.Tensor]\n            Optional state from previous sequence encoding.\n            Only passed to the SRU (not used to perform multihead\n            attention).\n        tgt_mask: torch.Tensor, optional\n            The mask for the tgt sequence (optional).\n        memory_mask: torch.Tensor, optional\n            the mask for the memory sequence (optional).\n        padding_mask: torch.Tensor, optional\n            the mask for the tgt keys per batch (optional).\n        memory_key_padding_mask: torch.Tensor, optional\n            the mask for the memory keys per batch (optional).\n\n        Returns\n        -------\n        torch.Tensor\n            Output Tensor of shape [S x B x H]\n\n        """"""\n        # Transpose and reverse\n        reversed_mask = None\n        if padding_mask is not None:\n            reversed_mask = ~padding_mask\n\n        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n                              key_padding_mask=reversed_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n                                   key_padding_mask=memory_key_padding_mask)[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        tgt2, _ = self.sru(tgt, state, mask_pad=padding_mask)\n        tgt2 = self.linear2(tgt2)\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n\n        return tgt\n'"
flambe/optim/__init__.py,0,"b""from flambe.optim.scheduler import LRScheduler, LambdaLR\nfrom flambe.optim.noam import NoamScheduler\nfrom flambe.optim.linear import WarmupLinearScheduler\nfrom flambe.optim.radam import RAdam\n\n\n__all__ = ['LRScheduler', 'LambdaLR',\n           'NoamScheduler', 'WarmupLinearScheduler', 'RAdam']\n"""
flambe/optim/linear.py,1,"b'from flambe.optim.scheduler import LambdaLR\n\n\nclass WarmupLinearScheduler(LambdaLR):\n    """"""Linear warmup and then linear decay.\n\n    Linearly increases learning rate from 0 to 1 over\n    `warmup` training steps.\n    Linearly decreases learning rate from 1. to 0. over\n    remaining `n_steps - warmup` steps.\n\n    This scheduler is generally used after every training batch.\n\n    """"""\n\n    def __init__(self,\n                 optimizer,\n                 warmup: int,\n                 n_steps: int):\n        """"""Initialize the WarmupLinearScheduler.\n\n        Parameters\n        ----------\n        optimizer : torch.optim.Optimizer\n            Wrapped optimizer.\n        warmup : int\n            The number of linear warmup phases\n        n_steps : int, optional\n            The index of last step. Default: -1\n\n        """"""\n        self.warmup = warmup\n        self.n_steps = n_steps\n        super().__init__(optimizer, lr_lambda=self.lr_lambda, last_epoch=-1)  # type: ignore\n\n    def lr_lambda(self, step: int) -> float:\n        """"""Compue the learning rate factor.\n\n        Parameters\n        ----------\n        step : int\n            The current step. Could be training over\n            validation steps.\n\n        Returns\n        -------\n        float\n            The output factor\n\n        """"""\n        if step < self.warmup:\n            return float(step) / float(max(1, self.warmup))\n        return max(0.0, float(self.n_steps - step) / float(max(1.0, self.n_steps - self.warmup)))\n'"
flambe/optim/noam.py,1,"b'from flambe.optim.scheduler import LambdaLR\n\n\nclass NoamScheduler(LambdaLR):\n    """"""Linear warmup and then quadratic decay.\n\n    Linearly increases the learning rate from 0 to 1 over\n    `warmup` steps.\n    Quadratically decreases the learning rate after.\n\n    This scheduler is generally used after every training batch.\n\n    """"""\n\n    def __init__(self,\n                 optimizer,\n                 warmup: int,\n                 d_model: int):\n        """"""Initialize the NoamScheduler.\n\n        Parameters\n        ----------\n        optimizer : torch.optim.Optimizer\n            Wrapped optimizer.\n        warmup : int\n            The number of linear warmup phases\n        d_model : int, optional\n            The index of last step. Default: -1\n\n        """"""\n        self.warmup = warmup\n        self.d_model = d_model\n        super().__init__(optimizer, lr_lambda=self.lr_lambda, last_epoch=-1)  # type: ignore\n\n    def lr_lambda(self, step: int) -> float:\n        """"""Compue the learning rate factor.\n\n        Parameters\n        ----------\n        step : int\n            The current step. Could be training over\n            validation steps.\n\n        Returns\n        -------\n        float\n            The output factor\n\n        """"""\n        if step == 0 and self.warmup == 0:\n            return 1. / (self.d_model ** 0.5)\n        else:\n            if step > self.warmup:\n                return 1. / (self.d_model ** 0.5) / (step ** 0.5)\n            else:\n                return step / (self.d_model ** 0.5) / (self.warmup ** 1.5)\n'"
flambe/optim/radam.py,3,"b'import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\nfrom flambe.compile import Component\n\n\nclass RAdam(Optimizer, Component):\n\n    \'\'\'\n    Rectified Adam optimizer.\n\n    Taken from https://github.com/LiyuanLucasLiu/RAdam.\n    \'\'\'\n\n    def __init__(self,\n                 params,\n                 lr=1e-3,\n                 betas=(0.9, 0.999),\n                 eps=1e-8,\n                 weight_decay=0,\n                 degenerated_to_sgd=True):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n\n        self.degenerated_to_sgd = degenerated_to_sgd\n        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n            for param in params:\n                betas_diff = (param[\'betas\'][0] != betas[0] or param[\'betas\'][1] != betas[1])\n                if \'betas\' in param and betas_diff:\n                    param[\'buffer\'] = [[None, None, None] for _ in range(10)]\n        defaults = dict(lr=lr,\n                        betas=betas,\n                        eps=eps,\n                        weight_decay=weight_decay,\n                        buffer=[[None, None, None] for _ in range(10)])\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def state_dict(self):\n        state_dict = super().state_dict()\n        return state_dict\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'RAdam does not support sparse gradients\')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[\'step\'] += 1\n                buffered = group[\'buffer\'][int(state[\'step\'] % 10)]\n                if state[\'step\'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\'step\']\n                    beta2_t = beta2 ** state[\'step\']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it\'s an approximated value\n                    if N_sma >= 5:\n                        term1 = (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4)\n                        term2 = (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)\n                        step_size = math.sqrt(term1 * term2) / (1 - beta1 ** state[\'step\'])\n                    elif self.degenerated_to_sgd:\n                        step_size = 1.0 / (1 - beta1 ** state[\'step\'])\n                    else:\n                        step_size = -1\n                    buffered[2] = step_size\n\n                # more conservative since it\'s an approximated value\n                if N_sma >= 5:\n                    if group[\'weight_decay\'] != 0:\n                        p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n                    p_data_fp32.addcdiv_(-step_size * group[\'lr\'], exp_avg, denom)\n                    p.data.copy_(p_data_fp32)\n                elif step_size > 0:\n                    if group[\'weight_decay\'] != 0:\n                        p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n                    p_data_fp32.add_(-step_size * group[\'lr\'], exp_avg)\n                    p.data.copy_(p_data_fp32)\n\n        return loss\n'"
flambe/optim/scheduler.py,2,"b""import torch\nfrom flambe.compile import Component\n\n\nclass LRScheduler(torch.optim.lr_scheduler._LRScheduler, Component):\n\n    def state_dict(self):\n        state_dict = super().state_dict()\n        del state_dict['_schema']\n        del state_dict['_saved_kwargs']\n        del state_dict['_extensions']\n        return state_dict\n\n\nclass LambdaLR(torch.optim.lr_scheduler.LambdaLR, Component):\n\n    def state_dict(self):\n        state_dict = super().state_dict()\n        del state_dict['_schema']\n        del state_dict['_saved_kwargs']\n        del state_dict['_extensions']\n        return state_dict\n"""
flambe/runnable/__init__.py,0,"b""from flambe.runnable.runnable import Runnable\nfrom flambe.runnable.cluster_runnable import ClusterRunnable\nfrom flambe.runnable.context import SafeExecutionContext\nfrom flambe.runnable.environment import RemoteEnvironment\n\n\n__all__ = ['Runnable', 'SafeExecutionContext', 'ClusterRunnable', 'RemoteEnvironment']\n"""
flambe/runnable/cluster_runnable.py,0,"b'from abc import abstractmethod\nfrom typing import Optional, Dict, Callable\n\nfrom flambe.runnable import Runnable\nfrom flambe.cluster import Cluster\nfrom flambe.runnable.environment import RemoteEnvironment\n\n\nclass ClusterRunnable(Runnable):\n    """"""Base class for all runnables that are able to run on cluster.\n\n    This type of Runnables must include logic in the \'run\' method to\n    deal with the fact that they could be running in a distributed\n    cluster of machines.\n\n    To provide useful information about the cluster, a RemoteEnvironment\n    object will be injected when running remotely.\n\n    Attributes\n    ----------\n    config: configparser.ConfigParser\n        The secrets that the user provides. For example,\n        \'config[""AWS""][""ACCESS_KEY""]\'\n    env: RemoteEnvironment\n        The remote environment has information about the cluster\n        where this ClusterRunnable will be running.\n        IMPORTANT: this object will be available only when\n        the ClusterRunnable is running remotely.\n    user_provider: Callable[[], str]\n        The logic for specifying the user triggering this\n        Runnable. If not passed, by default it will pick the computer\'s\n        user.\n\n    """"""\n    def __init__(self, user_provider: Callable[[], str] = None,\n                 env: Optional[RemoteEnvironment] = None, **kwargs) -> None:\n        super().__init__(user_provider=user_provider, **kwargs)\n        self.env = env\n\n    @abstractmethod\n    def setup(self, cluster: Cluster,\n              extensions: Dict[str, str],\n              force: bool, **kwargs) -> None:\n        """"""Setup the cluster.\n\n        Parameters\n        ----------\n        cluster: Cluster\n            The cluster where this Runnable will be running\n        extensions: Dict[str, str]\n            The ClusterRunnable extensions\n        force: bool\n            The force value provided to Flambe\n\n        """"""\n        raise NotImplementedError()\n\n    def setup_inject_env(self, cluster: Cluster,\n                         extensions: Dict[str, str],\n                         force: bool, **kwargs) -> None:\n        """"""Call setup and inject the RemoteEnvironment\n\n        Parameters\n        ----------\n        cluster: Cluster\n            The cluster where this Runnable will be running\n        extensions: Dict[str, str]\n            The ClusterRunnable extensions\n        force: bool\n            The force value provided to Flambe\n\n        """"""\n        self.setup(cluster=cluster, extensions=extensions, force=force, **kwargs)\n        self.set_serializable_attr(""env"", cluster.get_remote_env(self.user_provider))\n\n    def set_serializable_attr(self, attr, value):\n        """"""Set an attribute while keep supporting serializaton.\n        """"""\n        setattr(self, attr, value)\n        if hasattr(self, ""_saved_kwargs""):\n            self._saved_kwargs[attr] = value\n'"
flambe/runnable/context.py,0,"b'# from __future__ import annotations\nimport os\nfrom typing import Optional, List, Type, Any, Dict, cast, Tuple\nfrom types import TracebackType\n\nfrom io import StringIO\nimport sys\n\nimport errno\n\nimport logging\nimport configparser\n\nfrom flambe.compile import Registrable\nfrom flambe.compile import yaml\nfrom flambe.runnable import error\nfrom flambe.runnable.runnable import Runnable\nfrom flambe.compile.extensions import download_extensions\nfrom flambe.compile.extensions import install_extensions, import_modules\nfrom flambe.const import FLAMBE_GLOBAL_FOLDER\nfrom flambe.logging import coloredlogs as cl\n\nlogger = logging.getLogger(__name__)\n\n\nclass SafeExecutionContext:\n    """"""Context manager handling the experiment\'s creation and execution.\n\n    Parameters\n    ----------\n    yaml_file: str\n        The experiment filename\n\n    """"""\n    def __init__(self, yaml_file: str) -> None:\n        self.to_remove: List[str] = []\n        self.yaml_file = yaml_file\n        self.content: str = """"\n\n    def __enter__(self) -> ""SafeExecutionContext"":\n        """"""A SafeExecutionContext should be used as a context manager\n        to handle all possible errors in a clear way.\n\n        Examples\n        --------\n\n        >>> with SafeExecutionContext(...) as ex:\n        >>>     ...\n\n        """"""\n        return self\n\n    def __exit__(self,\n                 exc_type: Optional[Type[BaseException]],\n                 exc_value: Optional[BaseException],\n                 tb: Optional[TracebackType]):\n        """"""Exit method for the context manager.\n\n        This method will catch any exception, and return True. This\n        means that all exceptions produced in a SafeExecutionContext\n        (used with the context manager) will not continue to raise.\n\n        """"""\n        if exc_type is not None and exc_value is not None:\n            # Rollback and undo cluster in case or exception\n            logger.error(cl.RE(repr(exc_value)), exc_info=(exc_type, exc_value, tb))\n            if isinstance(exc_value, error.RunnableFileError):\n                sys.exit(errno.EINVAL)\n            else:\n                sys.exit(-1)\n\n        return False\n\n    def preprocess(self,\n                   secrets: Optional[str] = None,\n                   download_ext: bool = True,\n                   install_ext: bool = False,\n                   import_ext: bool = True,\n                   check_tags: bool = True,\n                   **kwargs) -> Tuple[Runnable, Dict[str, str]]:\n        """"""Preprocess the runnable file.\n\n        Looks for syntax errors, import errors, etc. Also injects\n        the secrets into the runnables.\n\n        If this method runs and ends without exceptions, then the\n        experiment is ok to be run. If this method raises an Error and\n        the SafeExecutionContext is used as context manager,\n        then the __exit__ method will be executed.\n\n        Parameters\n        ----------\n        secrets: Optional[str]\n            Optional path to the secrets file\n        install_ext: bool\n            Whether to install the extensions or not.\n            This process also downloads the remote extensions.\n            Defaults to False\n        install_ext: bool\n            Whether to import the extensions or not.\n            Defaults to True.\n        check_tags: bool\n            Whether to check that all tags are valid. Defaults to True.\n\n        Returns\n        -------\n        Tuple[Runnable, Dict[str, str]]\n            A tuple containing the compiled Runnable and a dict\n            containing the extensions the Runnable uses.\n\n        Raises\n        ------\n        Exception\n            Depending on the error.\n\n        """"""\n        content, extensions = self.first_parse()\n\n        config = configparser.ConfigParser()\n        if secrets:\n            config.read(secrets)\n\n        if install_ext:\n            t = os.path.join(FLAMBE_GLOBAL_FOLDER, ""extensions"")\n            extensions = download_extensions(extensions, t)\n            install_extensions(extensions, user_flag=False)\n\n        if import_ext:\n            import_modules(extensions.keys())\n\n        # Check that all tags are valid\n        if check_tags:\n            self.check_tags(content)\n\n        # Compile the runnable now that the extensions were imported.\n        runnable = self.compile_runnable(content)\n\n        if secrets:\n            runnable.inject_secrets(secrets)\n\n        if extensions:\n            runnable.inject_extensions(extensions)\n\n        runnable.parse()\n\n        return runnable, extensions\n\n    def first_parse(self) -> Tuple[str, Dict]:\n        """"""Check if valid YAML file and also load config\n\n        In this first parse the runnable does not get compiled because\n        it could be a custom Runnable, so it needs the extensions\n        to be imported first.\n\n        """"""\n        if not os.path.exists(self.yaml_file):\n            raise FileNotFoundError(\n                f""Configuration file \'{self.yaml_file}\' not found. Terminating.""\n            )\n\n        with open(self.yaml_file, \'r\') as f:\n            content = f.read()\n\n        try:\n            yamls = list(yaml.load_all(content))\n        except TypeError as e:\n            raise error.ParsingRunnableError(f""Syntax error compiling the runnable: {str(e)}"")\n\n        if len(yamls) > 2:\n            raise ValueError(f""{self.yaml_file} should contain an (optional) extensions sections"" +\n                             "" and the main runnable object."")\n\n        extensions: Dict[str, str] = {}\n        if len(yamls) == 2:\n            extensions = dict(yamls[0])\n\n        # We want self.content to be a string with the raw content\n        # We will precompile later once all extensions are registered.\n        with StringIO() as stream:\n            yaml.dump(yamls[-1], stream)\n            content = stream.getvalue()\n\n        return content, extensions\n\n    def check_tags(self, content: str):\n        """"""Check that all the tags are valid.\n\n        Parameters\n        ----------\n        content : str\n            The content of the YAML file\n\n        Raises\n        ------\n        TagError\n\n        """"""\n        # Get all the registered tags, and flatten\n        registered_tags = {t for _, tags in Registrable._yaml_tags.items() for t in tags}\n\n        # Check against tags in this config\n        parsing_events = yaml.parse(content)\n        for event in parsing_events:\n            if hasattr(event, \'tag\') and event.tag is not None:\n                if event.tag not in registered_tags:\n                    raise error.TagError(f""Unknown tag: {event.tag}. Make sure the class, \\\n                                            or factory was correctly registered."")\n\n    def compile_runnable(self, content: str) -> Runnable:\n        """"""Compiles and returns the Runnable.\n\n        IMPORTANT: This method should run after all\n        extensions were registered.\n\n        Parameters\n        ----------\n        content: str\n            The runnable, as a YAML string\n\n        Returns\n        -------\n        Runnable\n            The compiled experiment.\n\n        """"""\n        ret: Any = yaml.load(content)\n        if not isinstance(ret, Runnable):\n            raise ValueError(""Tried to run a non-Runnable"")\n        cast(Runnable, ret)\n        ret.inject_content(content)\n        return ret\n'"
flambe/runnable/environment.py,0,"b'from typing import List, Any, Optional\nfrom flambe.compile import Registrable\n\n\nclass RemoteEnvironment(Registrable):\n    """"""This objects contains information about the cluster\n\n    This object will be available on the remote execution of\n    the ClusterRunnable (as an attribute).\n\n    IMPORTANT: this object needs to be serializable, hence it Needs\n    to be created using \'compile\' method.\n\n    Attributes\n    ----------\n    key: str\n        The key that communicates the cluster\n    orchestrator_ip: str\n        The orchestrator visible IP for the factories (usually\n        the private IP)\n    factories_ips: List[str]\n        The list of factories IPs visible for other factories and\n        orchestrator (usually private IPs)\n    user: str\n        The username of all machines. This implementations assumes\n        same username for all machines\n    local_user: str\n        The username of the local process that launched the cluster\n    public_orchestrator_ip: Optional[str]\n        The public orchestrator IP, if available.\n    public_factories_ips: Optional[List[str]]\n        The public factories IPs, if available.\n\n    """"""\n\n    def __init__(self,\n                 key: str,\n                 orchestrator_ip: str,\n                 factories_ips: List[str],\n                 user: str,\n                 local_user: str,\n                 public_orchestrator_ip: Optional[str] = None,\n                 public_factories_ips: Optional[List[str]] = None,\n                 **kwargs) -> None:\n        self.key = key\n        self.orchestrator_ip = orchestrator_ip\n        self.factories_ips = factories_ips\n        self.user = user\n        self.local_user = local_user\n\n        self.public_orchestrator_ip = public_orchestrator_ip\n        self.public_factories_ips = public_factories_ips\n\n    @classmethod\n    def to_yaml(cls, representer: Any, node: Any, tag: str) -> Any:\n        """"""Use representer to create yaml representation of node""""""\n        kwargs = {\'key\': node.key,\n                  \'orchestrator_ip\': node.orchestrator_ip,\n                  \'factories_ips\': node.factories_ips,\n                  \'public_orchestrator_ip\': node.public_orchestrator_ip,\n                  \'public_factories_ips\': node.public_factories_ips,\n                  \'user\': node.user,\n                  \'local_user\': node.local_user}\n        return representer.represent_mapping(tag, kwargs)\n\n    @classmethod\n    def from_yaml(cls, constructor: Any, node: Any, factory_name: str) -> Any:\n        """"""Use constructor to create an instance of cls""""""\n        # NOTE: construct_yaml_map is a generator that yields the\n        # constructed data and then updates it\n        kwargs, = list(constructor.construct_yaml_map(node))\n        return cls(**kwargs)\n'"
flambe/runnable/error.py,0,"b'class ProtocolError(Exception):\n\n    def __init__(self, message: str) -> None:\n        """"""Base ProtocolError implementation.\n\n        Parameters\n        ----------\n        message : str\n            The message to display\n\n        """"""\n        self.message = message\n\n    def __repr__(self) -> str:\n        """"""Override output message to show ProtocolError\n\n        Returns\n        -------\n        str\n            The output message\n\n        """"""\n        return f""ProtocolError: {self.message}""\n\n\nclass LinkError(ProtocolError):\n\n    def __init__(self, block_id: str, target_block_id: str) -> None:\n        """"""Link error on undeclared block.\n\n        Parameters\n        ----------\n        block_id : str\n            The block including the link\n        target_block_id : str\n            The link\'s target block\n\n        """"""\n        default_message = (\n            f""Block \'{block_id}\' has a link to \'{target_block_id}\' ""\n            ""which has not yet been declared.""\n        )\n        super().__init__(default_message)\n\n\nclass SearchComponentError(ProtocolError):\n\n    def __init__(self, block_id: str) -> None:\n        """"""Search error on non-computable.\n\n        Parameters\n        ----------\n        block_id : str\n            The block with the wrong type\n\n        """"""\n        default_message = (\n            f""Block \'{block_id}\' is a non-component; ""\n            ""only Component objects can be in the pipeline""\n        )\n        super().__init__(default_message)\n\n\nclass UnsuccessfulRunnableError(RuntimeError):\n    pass\n\n\nclass RunnableFileError(Exception):\n    pass\n\n\nclass ResourceError(RunnableFileError):\n    pass\n\n\nclass NonExistentResourceError(RunnableFileError):\n    pass\n\n\nclass ExistentResourceError(RunnableFileError):\n    pass\n\n\nclass ParsingRunnableError(RunnableFileError):\n    pass\n\n\nclass TagError(RunnableFileError):\n    pass\n\n\nclass MissingSecretsError(Exception):\n    pass\n'"
flambe/runnable/runnable.py,0,"b'from abc import abstractmethod\nimport configparser\nfrom typing import Dict, Optional, Callable\n\nfrom flambe.compile import MappedRegistrable\nfrom flambe.runnable.utils import DEFAULT_USER_PROVIDER\n\n\nclass Runnable(MappedRegistrable):\n    """"""Base class for all runnables.\n\n    A runnable contains a single run method that needs to\n    be implemented. It must contain all the logic for\n    the runnable.\n\n    Each runnable has also access to the secrets the user\n    provides.\n\n    Examples of Runnables: Experiment, Cluster\n\n    Attributes\n    ----------\n    config: configparser.ConfigParser\n        The secrets that the user provides. For example,\n        \'config[""AWS""][""ACCESS_KEY""]\'\n    extensions: Dict[str, str]\n        The extensions used for this runnable.\n    content: Optional[str]\n        This attribute will hold the YAML representation\n        of the Runnable.\n    user_provider: Callable[[], str]\n        The logic for specifying the user triggering this\n        Runnable. If not passed, by default it will pick the computer\'s\n        user.\n\n    """"""\n    def __init__(self, user_provider: Callable[[], str] = None, **kwargs) -> None:\n        self.config = configparser.ConfigParser()\n        self.extensions: Dict[str, str] = {}\n        self.content: Optional[str] = None\n\n        self.user_provider = user_provider or DEFAULT_USER_PROVIDER\n\n    def inject_content(self, content: str) -> None:\n        """"""Inject the original YAML string that was used\n        to generate this Runnable instance.\n\n        Parameters\n        ----------\n        content: str\n            The YAML, as a string\n\n        """"""\n        self.content = content\n\n    def inject_secrets(self, secrets: str) -> None:\n        """"""Inject the secrets once the Runnable\n        was created.\n\n        Parameters\n        ----------\n        secrets: str\n            The filepath to the secrets\n\n        """"""\n        self.config.read(secrets)\n\n    def inject_extensions(self, extensions: Dict[str, str]) -> None:\n        """"""Inject extensions to the Runnable\n\n        Parameters\n        ----------\n        extensions: Dict[str, str]\n            The extensions\n\n        """"""\n        self.extensions = extensions\n\n    @abstractmethod\n    def run(self, **kwargs) -> None:\n        """"""Run the runnable.\n\n        Each implementation will implement its\n        own logic, with the parameters it needs.\n\n        """"""\n        raise NotImplementedError()\n\n    def parse(self) -> None:\n        """"""Parse the runnable to determine if it\'s able to run.\n        Raises\n        ------\n        ParsingExperimentError\n            In case a parsing error is found.\n        """"""\n        pass\n'"
flambe/runnable/utils.py,0,"b'import os\nimport getpass\nfrom flambe.compile.utils import _is_url\nfrom typing import List\n\nimport flambe\n\ntry:\n    from pip._internal.operations import freeze\nexcept ImportError:  # pip < 10.0\n    from pip.operations import freeze\n\n\nDEFAULT_USER_PROVIDER = getpass.getuser\n\n\ndef _contains_path(nested_dict) -> bool:\n    """"""Whether the nested dict contains any value that could be a path.\n\n    Parameters\n    ----------\n    nested_dict\n        The nested dict to evaluate\n\n    Returns\n    -------\n    bool\n\n    """"""\n    for v in nested_dict.values():\n        if hasattr(v, ""values""):\n            if _contains_path(v):\n                return True\n        if isinstance(v, str) and os.sep in v and not _is_url(v):\n            return True\n\n    return False\n\n\ndef is_dev_mode() -> bool:\n    """"""Detects if flambe was installed in editable mode.\n\n    For more information:\n    https://pip.pypa.io/en/latest/reference/pip_install/#editable-installs\n\n    Returns\n    -------\n    bool\n\n    """"""\n    x = freeze.freeze()\n    for pkg in x:\n        if pkg.startswith(""-e"") and pkg.endswith(""egg=flambe""):\n            return True\n\n    return False\n\n\ndef get_flambe_repo_location() -> str:\n    """"""Return where flambe repository is located\n\n    Returns\n    -------\n    str\n        The local path where flambe is located\n\n    Raises\n    ------\n    ValueError\n        If flambe was not installed in editable mode\n\n    """"""\n    if not is_dev_mode():\n        raise ValueError(""Flambe repo can\'t be located as it was not \\\n                          installed in editable mode"")\n\n    # Go form the top level __init__.py to the flambe repo\n    repo_location = os.path.join(flambe.__file__, os.pardir, os.pardir)\n    return os.path.abspath(repo_location)\n\n\ndef get_commit_hash() -> str:\n    """"""Get the commit hash of the current flambe development package.\n\n    This will only work if flambe was install from github in dev mode.\n\n    Returns\n    -------\n    str\n        The commit hash\n\n    Raises\n    ------\n    Exception\n        In case flambe was not installed in dev mode.\n\n    """"""\n    x = freeze.freeze()\n    for pkg in x:\n        if ""flambe"" in pkg:\n            if pkg.startswith(""-e""):\n                git_url = pkg.split("" "")[-1]\n                commit = git_url.split(""@"")[-1].split(""#"")[0]\n                return commit\n\n    raise Exception(""Tried to lookup commit hash in NOT development mode."")\n\n\ndef rsync_hosts(orch_ip: str,\n                factories_ips: List[str],\n                user: str,\n                folder: str,\n                key: str,\n                exclude: List[str]) -> None:\n    """"""Rsync the hosts in the cluster.\n\n    IMPORTANT: this method is intended to be run in the cluster.\n\n    Parameters\n    ----------\n    orch_ip: str\n        The Orchestrator\'s IP that is visible by the factories\n        (usually the private IP)\n    factories_ips: List[str]\n        The factories IPs that are visible by the Orchestrator\n        (usually the private IPs)\n    user: str\n        The username of all machines.\n        IMPORTANT: only machines with same username are supported\n    key: str\n        The key that communicate all machines\n    exclude: List[str]\n        A list of files to be excluded in the rsync\n\n    """"""\n    exc = """"\n    if exclude:\n        for x in exclude:\n            exc += f"" --exclude {x} ""\n\n    if not folder.endswith(os.sep):\n        folder = f""{folder}{os.sep}""\n\n    for f_ip in factories_ips:\n        f_loc = f""{user}@{f_ip}:{folder}""\n        cmd = (\n            f""rsync -ae \'ssh -i {key} ""\n            f""-o StrictHostKeyChecking=no\' {exc} ""\n            f""{f_loc} {folder}""\n        )\n        os.system(cmd)\n\n    for f_ip in factories_ips:\n        f_loc = f""{user}@{f_ip}:{folder}""\n        cmd = (\n            f""rsync -ae \'ssh -i {key} ""\n            f""-o StrictHostKeyChecking=no\' {exc} ""\n            f""{folder} {f_loc}""\n        )\n        os.system(cmd)\n'"
flambe/runner/__init__.py,0,b''
flambe/runner/report_site_run.py,0,"b'""""""Script to run the report web site\n\nIt takes the `app` defined in `flambe.remote.webapp.app` and runs it.\n\n""""""\nimport logging\nimport argparse\nfrom typing import Optional\nimport os\n\nfrom flambe.experiment.webapp.app import app\n\nlogger = logging.getLogger(__name__)\n\n\ndef launch_tensorboard(tracking_address) -> Optional[str]:\n    # https://stackoverflow.com/a/55708102\n    # tb will run in background but it will\n    # be stopped once the main process is stopped.\n    try:\n        from tensorboard import program\n        tb = program.TensorBoard()\n        tb.configure(argv=[None, \'--logdir\', tracking_address])\n        url = tb.launch()\n        if url.endswith(""/""):\n            url = url[:-1]\n\n        return url\n    except Exception:\n        return None\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Flambe webapp\')\n    parser.add_argument(\'progress_file\', type=str, default=\'localhost\',\n                        help=\'The location of the pickled progress file\')\n    parser.add_argument(\'--output-log\', type=str, default=None,\n                        help=\'The experiment log file\')\n    parser.add_argument(\'--output-dir\', type=str, default=None,\n                        help=\'The experiment output directory\')\n    parser.add_argument(\'--host\', type=str, default=""localhost"",\n                        help=\'Port in which the site will be running url\')\n    parser.add_argument(\'--port\', type=int, default=49558,\n                        help=\'Port in which the site will be running url\')\n    parser.add_argument(\'--tensorboard_url\', type=str, help=\'Tensorboard url\')\n    args = parser.parse_args()\n\n    app.config[\'progress_file\'] = args.progress_file\n    app.config[\'output_log\'] = args.output_log\n    app.config[\'output_dir\'] = args.output_dir\n    app.config[\'tensorboard_url\'] = args.tensorboard_url\n\n    if app.config[\'tensorboard_url\'] is None:\n        app.config[\'tensorboard_url\'] = launch_tensorboard(\n            os.path.dirname(app.config[\'progress_file\']))\n\n    # debug=False won\'t work remotely as Flask will not be able\n    # to bind 0.0.0.0. Make sure to use debug=True is the report site\n    # runs remotely.\n    # Only use False for debugging purposes.\n    app.run(host=args.host, port=args.port, debug=False)\n'"
flambe/runner/run.py,9,"b'""""""Local run script for flambe.\n""""""\n\nimport argparse\nimport logging\n\nimport torch\nimport ray\nimport os\n\nfrom flambe.const import FLAMBE_GLOBAL_FOLDER\nfrom flambe.compile import make_component\nfrom flambe.compile.extensions import download_extensions\nfrom flambe.optim import LRScheduler\nfrom flambe.runnable.utils import is_dev_mode, get_flambe_repo_location\nfrom flambe.logging import setup_global_logging\nfrom flambe.logging import coloredlogs as cl\nfrom flambe.runnable.context import SafeExecutionContext\nfrom flambe.runnable import ClusterRunnable\nfrom flambe.cluster import Cluster\nfrom flambe.runner.utils import check_system_reqs\nfrom flambe.logo import ASCII_LOGO, ASCII_LOGO_DEV\nimport flambe\n\nfrom typing import cast\n\n\nTORCH_TAG_PREFIX = ""torch""\nTUNE_TAG_PREFIX = ""tune""\n\n\ndef main(args: argparse.Namespace) -> None:\n    """"""Execute command based on given config""""""\n    if is_dev_mode():\n        print(cl.RA(ASCII_LOGO_DEV))\n        print(cl.BL(f""Location: {get_flambe_repo_location()}\\n""))\n    else:\n        print(cl.RA(ASCII_LOGO))\n        print(cl.BL(f""VERSION: {flambe.__version__}\\n""))\n\n    if args.debug:\n        print(cl.YE(f""Debug mode activated\\n""))\n        if args.cluster is not None:\n            raise ValueError(\'Will not run on cluster in debug mode. \' +\n                             \'Please disable debug mode or run locally.\')\n\n    # Pass original module for ray / pickle\n    exclude = [\'torch.nn.quantized\', \'torch.nn.qat\']\n    make_component(torch.nn.Module, only_module=\'torch.nn\', exclude=exclude)\n    # torch.optim.Optimizer exists, ignore mypy\n    make_component(torch.optim.Optimizer,  # type: ignore\n                   only_module=\'torch.optim\')\n    make_component(torch.optim.lr_scheduler._LRScheduler,\n                   only_module=\'torch.optim.lr_scheduler\', parent_component_class=LRScheduler)\n    make_component(ray.tune.schedulers.TrialScheduler)\n    make_component(ray.tune.suggest.SearchAlgorithm)\n\n    # TODO check first if there is cluster as if there is there\n    # is no need to install extensions\n    check_system_reqs()\n    with SafeExecutionContext(args.config) as ex:\n        if args.cluster is not None:\n            with SafeExecutionContext(args.cluster) as ex_cluster:\n                cluster, _ = ex_cluster.preprocess(secrets=args.secrets,\n                                                   install_ext=args.install_extensions)\n                runnable, extensions = ex.preprocess(import_ext=False,\n                                                     check_tags=False,\n                                                     secrets=args.secrets)\n                cluster.run(force=args.force, debug=args.debug)\n                if isinstance(runnable, ClusterRunnable):\n                    cluster = cast(Cluster, cluster)\n\n                    # This is independant to the type of ClusterRunnable\n                    destiny = os.path.join(cluster.get_orch_home_path(), ""extensions"")\n\n                    # Before sending the extensions, they need to be\n                    # downloaded (locally).\n                    t = os.path.join(FLAMBE_GLOBAL_FOLDER, ""extensions"")\n                    extensions = download_extensions(extensions, t)\n\n                    # At this point, all remote extensions\n                    # (except pypi extensions)\n                    # have local paths.\n                    new_extensions = cluster.send_local_content(extensions,\n                                                                destiny, all_hosts=True)\n\n                    new_secrets = cluster.send_secrets()\n\n                    # Installing the extensions is crutial as flambe\n                    # will execute without \'-i\' flag and therefore\n                    # will assume that the extensions are installed\n                    # in the orchestrator.\n                    cluster.install_extensions_in_orchestrator(new_extensions)\n                    logger.info(cl.GR(""Extensions installed in Orchestrator""))\n\n                    runnable.setup_inject_env(cluster=cluster,\n                                              extensions=new_extensions,\n                                              force=args.force)\n                    cluster.execute(runnable, new_extensions, new_secrets, args.force)\n                else:\n                    raise ValueError(""Only ClusterRunnables can be executed in a cluster."")\n        else:\n            runnable, _ = ex.preprocess(secrets=args.secrets,\n                                        install_ext=args.install_extensions)\n            runnable.run(force=args.force, verbose=args.verbose, debug=args.debug)\n\n\nif __name__ == \'__main__\':\n    # torch.multiprocessing exists, ignore mypy\n    torch.multiprocessing.set_start_method(\'fork\', force=True)  # type: ignore\n\n    parser = argparse.ArgumentParser(description=\'Run a flamb\xc3\xa9!\')\n    parser.add_argument(\'config\', type=str, help=\'Path to config file\')\n    parser.add_argument(\'-i\', \'--install-extensions\', action=\'store_true\', default=False,\n                        help=\'Install extensions automatically using pip. WARNING: \' +\n                             \'This could potentially override already installed packages.\')\n    parser.add_argument(\'-c\', \'--cluster\', type=str, default=None,\n                        help=\'Specify the cluster that will run the experiment. This option \' +\n                             \'works if the main config is an Experiment\')\n    parser.add_argument(\'-f\', \'--force\', action=\'store_true\', default=False,\n                        help=\'Override existing runnables. Be careful \' +\n                             \'when using this flag as it could have undesired effects.\')\n    parser.add_argument(\'-s\', \'--secrets\',\n                        type=str, default=os.path.join(FLAMBE_GLOBAL_FOLDER, ""secrets.ini""))\n    parser.add_argument(\'-d\', \'--debug\', action=\'store_true\',\n                        help=\'Enable debug mode. Each runnable specifies the debug behavior. \' +\n                             \'For example for an Experiment, Ray will run in a single thread \' +\n                             \'allowing user breakpoints\')\n    parser.add_argument(\'-v\', \'--verbose\', action=\'store_true\', help=\'Verbose console output\')\n    args = parser.parse_args()\n\n    setup_global_logging(logging.INFO if not args.verbose else logging.DEBUG)\n    logger = logging.getLogger(__name__)\n\n    try:\n        main(args)\n        logger.info(cl.GR(""------------------- Done -------------------""))\n    except KeyboardInterrupt:\n        logger.info(cl.RE(""---- Exiting early (Keyboard Interrupt) ----""))\n'"
flambe/runner/utils.py,0,"b'import os\nimport logging\nfrom typing import Iterable\n\nfrom flambe.const import FLAMBE_GLOBAL_FOLDER\nfrom flambe.experiment.wording import print_extensions_cache_size_warning\n\nlogger = logging.getLogger(__name__)\n\n\nMB = 2**20\nWARN_LIMIT_MB = 100\n\n\ndef get_files(path: str) -> Iterable[str]:\n    """"""Return the list of all files (recursively)\n    a directory has.\n\n    Parameters\n    ----------\n    path: str\n        The directory\'s path\n\n    Return\n    ------\n    List[str]\n        The list of files (each file with its path from\n        the given parameter)\n\n    Raise\n    -----\n    ValueError\n        In case the path does not exist\n\n    """"""\n    if not os.path.exists(path):\n        raise ValueError(f""{path} does not exist"")\n\n    def _wrapped():\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                yield fp\n\n    return _wrapped()\n\n\ndef get_size_MB(path: str) -> float:\n    """"""Return the size of a file/folder in MB.\n\n    Parameters\n    ----------\n    path: str\n        The path to the folder or file\n\n    Returns\n    -------\n    float\n        The size in MB\n\n    """"""\n    accum = 0\n    if os.path.isdir(path):\n        for fp in get_files(path):\n            if os.path.exists(fp) and not os.path.islink(fp):\n                accum += os.path.getsize(fp)\n    else:\n        accum = os.path.getsize(path)\n    return accum / MB\n\n\ndef check_system_reqs() -> None:\n    """"""Run system checks and prepare the system before a run.\n\n    This method should:\n        * Create folders, files that are needed for flambe\n        * Raise errors in case requirements are not met. This should\n        run under the SafeExecutionContext, so errors will be handled\n        * Warn the user in case something needs attention.\n\n    """"""\n    # Create the flambe folder if it does not exist\n    if not os.path.exists(FLAMBE_GLOBAL_FOLDER):\n        os.mkdir(FLAMBE_GLOBAL_FOLDER)\n\n    # Check if extensions folder is getting big\n    extensions_folder = os.path.join(FLAMBE_GLOBAL_FOLDER, ""extensions"")\n    if os.path.exists(extensions_folder) and get_size_MB(extensions_folder) > WARN_LIMIT_MB:\n        print_extensions_cache_size_warning(extensions_folder, WARN_LIMIT_MB)\n'"
flambe/sampler/__init__.py,0,"b""from flambe.sampler.sampler import Sampler\nfrom flambe.sampler.base import BaseSampler\nfrom flambe.sampler.episodic import EpisodicSampler\n\n\n__all__ = ['Sampler', 'BaseSampler', 'EpisodicSampler']\n"""
flambe/sampler/base.py,24,"b'import math\nimport warnings\nfrom collections import defaultdict, OrderedDict as odict\nfrom itertools import chain\nfrom functools import partial\nfrom typing import Iterator, Tuple, Union, Sequence, List, Dict, Set, Optional\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\nimport numpy as np\n\nfrom flambe.sampler.sampler import Sampler\n\n\ndef _bfs(obs: List, obs_idx: int) -> Tuple[Dict[int, List], Set[Tuple[int, ...]]]:\n    """"""\n    Given a single `obs`, itself a nested list, run BFS.\n\n    This function enumerates:\n\n        1. The lengths of each of the intermediary lists, by depth\n        2. All paths to the child nodes\n\n    Parameters\n    ----------\n    obs : List\n        A nested list of lists of arbitrary depth, with the child nodes,\n        i.e. deepest list elements, as `torch.Tensor`s\n\n    obs_idx : int\n        The index of `obs` in the batch.\n\n    Returns\n    -------\n    Set[Tuple[int]]\n        A set of all distinct paths to all children\n\n    Dict[int, List[int]]\n        A map containing the lengths of all intermediary lists, by depth\n\n    """"""\n    path, level, root = [obs_idx], 0, tuple(obs)\n    queue = [(path, level, root)]\n    paths = set()\n    lens: Dict[int, List] = defaultdict(list)\n    while queue:\n        path, level, item = queue.pop(0)\n        lens[level].append(len(item))\n        for i, c in enumerate(item):\n            if c.dim() == 0:  # We\'re iterating through child tensor itself\n                paths.add(tuple(path))\n            else:\n                queue.append((path + [i], level + 1, c))\n    return lens, paths\n\n\ndef _batch_from_nested_col(col: Tuple, pad: int) -> torch.Tensor:\n    """"""Compose a batch padded to the max-size along each dimension.\n\n    Parameters\n    ----------\n    col : List\n        A nested list of lists of arbitrary depth, with the child nodes,\n        i.e. deepest list elements, as `torch.Tensor`s\n\n        For example, a `col` might be:\n\n        [\n            [torch.Tensor([1, 2]), torch.Tensor([3, 4, 5])],\n            [torch.Tensor([5, 6, 7]), torch.Tensor([4, 5]),\n             torch.Tensor([5, 6, 7, 8])]\n        ]\n\n        Level 1 sizes: [2, 3]\n        Level 2 sizes: [2, 3]; [3, 2, 4]\n\n        The max-sizes along each dimension are:\n\n            * Dim 1: 3\n            * Dim 2: 4\n\n        As such, since this column contains 2 elements, with max-sizes\n        3 and 4 along the nested dimensions, our resulting batch would\n        have size (4, 3, 2), and the padded `Tensor`s would be inserted\n        at their respective locations.\n\n    Returns\n    -------\n    torch.Tensor\n        A (n+1)-dimensional torch.Tensor, where n is the nesting\n        depth, padded to the max-size along each dimension\n\n    """"""\n    bs = len(col)\n\n    # Compute lengths of child nodes, and the path to reach them\n    lens, paths = zip(*[_bfs(obs, obs_idx=i) for i, obs in enumerate(col)])\n\n    # Compute the max length for each level\n    lvl_to_lens: Dict[int, List] = defaultdict(list)\n    for l in lens:\n        for lvl, lns in l.items():\n            lvl_to_lens[lvl].extend(lns)\n    max_lens = odict([(lvl, max(lvl_to_lens[lvl])) for lvl in sorted(lvl_to_lens.keys())])\n\n    # Instantiate the empty batch\n    batch = torch.zeros(bs, *max_lens.values()).long() + pad\n\n    # Populate the batch with each child node\n    for p in chain.from_iterable(paths):\n        el = col\n        for i in p:\n            el = el[i]\n        diff = batch.size(-1) - len(el)\n        pad_tens = torch.zeros(diff).long() + pad\n        # TODO fix two typing errors below; likely because of typing\n        # on el which is reused multiple times\n        el = torch.cat((el, pad_tens))  # type: ignore\n        batch.index_put_(indices=[torch.tensor([i]) for i in p], values=el)  # type: ignore\n\n    return batch\n\n\ndef collate_fn(data: List[Tuple[torch.Tensor, ...]], pad: int) -> Tuple[torch.Tensor, ...]:\n    """"""Turn a list of examples into a mini-batch.\n\n    Handles padding on the fly on simple sequences, as well as\n    nested sequences.\n\n    Parameters\n    ----------\n    data : List[Tuple[torch.Tensor, ...]]\n        The list of sampled examples.\n        Each example is a tuple, each dimension representing a\n        column from the original dataset\n    pad: int\n        The padding index\n\n    Returns\n    -------\n    Tuple[torch.Tensor, ...]\n        The output batch of tensors\n\n    """"""\n    columns = list(zip(*data))\n\n    # Establish col-specific pad tokens\n    if isinstance(pad, (tuple, list)):\n        pad_tkns = pad\n        if len(pad_tkns) != len(columns):\n            raise Exception(f""The number of column-specific pad tokens \\\n                                ({len(pad_tkns)}) does not equal the number \\\n                                of columns in the batch ({len(columns)})"")\n    else:\n        pad_tkns = tuple([pad] * len(columns))\n\n    batch = []\n    for pad, column in zip(pad_tkns, columns):\n        # Prepare the tensors\n        is_nested = any([isinstance(example, (list, tuple)) for example in column])\n        if is_nested:\n            # Column contains nested observations\n            nested_tensors = _batch_from_nested_col(column, pad)\n            batch.append(nested_tensors)\n        else:\n            tensors = [torch.tensor(example) for example in column]\n            sizes = [tensor.size() for tensor in tensors]\n\n            if all(s == sizes[0] for s in sizes):\n                stacked_tensors = torch.stack(tensors)\n                batch.append(stacked_tensors)\n            else:\n                # Variable length sequences\n                padded_tensors = pad_sequence(tensors,\n                                              batch_first=True,\n                                              padding_value=pad)\n                batch.append(padded_tensors)\n\n    return tuple(batch)\n\n\nclass BaseSampler(Sampler):\n    """"""Implements a BaseSampler object.\n\n    This is the most basic implementation of a sampler.\n    It uses Pytorch\'s DataLoader object internally, and\n    offers the possiblity to override the sampling of the\n    examples and how to from a batch from them.\n\n    """"""\n\n    def __init__(self,\n                 batch_size: int = 64,\n                 shuffle: bool = True,\n                 pad_index: Union[int, Sequence[int]] = 0,\n                 n_workers: int = 0,\n                 pin_memory: bool = False,\n                 seed: Optional[int] = None,\n                 downsample: Optional[float] = None,\n                 downsample_max_samples: Optional[int] = None,\n                 downsample_seed: Optional[int] = None,\n                 drop_last: bool = False) -> None:\n        """"""Initialize the BaseSampler object.\n\n        Parameters\n        ----------\n        batch_size : int\n            The batch size to use\n        shuffle : bool, optional\n            Whether the data should be shuffled every epoch\n            (the default is True)\n        pad_index : int, optional\n            The index used for padding (the default is 0). Can be a\n            single pad_index applied to all columns, or a list or tuple\n            of pad_index\'s that apply to each column respectively.\n            (In this case, this list or tuple must have length equal\n            to the number of columns in the batch.)\n        n_workers : int, optional\n            Number of workers to pass to the DataLoader\n            (the default is 0, which means the main process)\n        pin_memory : bool, optional\n            Pin the memory when using cuda (the default is False)\n        seed: int, optional\n            Optional seed for the sampler\n        downsample: float, optional\n            Percentage of the data to downsample to. Seeded with\n            downsample_seed\n        downsample_max_samples: Optional[int]\n            Downsample to a specific number of samples. Uses\n            the same downsampling seed as downsample.\n        downsample_seed: int, optional\n            The seed to use in downsampling\n        drop_last: bool, optional\n            Set to True to drop the last incomplete batch if the dataset\n            size is not divisible by the batch size.\n            (the default is False)\n\n        """"""\n        self.pad = pad_index\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.drop_last = drop_last\n        self.n_workers = n_workers\n        self.pin_memory = pin_memory\n        if downsample is not None and not (0 < downsample <= 1):\n            raise ValueError(""Downsample value should be in the range (0, 1]"")\n        if downsample is not None and downsample_max_samples is not None:\n            raise ValueError(\'Please either specify `downsample` or `downsample_max_samples`, \'\n                             \'but not both.\')\n        self.downsample = downsample\n        self.downsample_max_samples = downsample_max_samples\n        self.downsample_seed = downsample_seed\n        self.random_generator = np.random if seed is None else np.random.RandomState(seed)\n\n    def sample(self,\n               data: Sequence[Sequence[torch.Tensor]],\n               n_epochs: int = 1) -> Iterator[Tuple[torch.Tensor, ...]]:\n        """"""Sample from the list of features and yields batches.\n\n        Parameters\n        ----------\n        data: Sequence[Sequence[torch.Tensor, ...]]\n            The input data to sample from\n        n_epochs: int, optional\n            The number of epochs to run in the output iterator.\n            Use -1 to run infinitely.\n\n        Yields\n        ------\n        Iterator[Tuple[Tensor]]\n            A batch of data, as a tuple of Tensors\n\n        """"""\n        if len(data) == 0:\n            raise ValueError(""No examples provided"")\n\n        if self.downsample is not None or self.downsample_max_samples is not None:\n            if self.downsample_max_samples is not None and self.downsample_max_samples >= len(data):\n                warnings.warn(\'`downsample_max_samples` was specified, but \'\n                              \'the number of specified samples exceeds the \'\n                              \'number available in the dataset.\')\n            if self.downsample_seed:\n                downsample_generator = np.random.RandomState(self.downsample_seed)\n            else:\n                downsample_generator = np.random\n            if self.downsample:\n                max_num_samples = int(self.downsample * len(data))\n            else:\n                max_num_samples = self.downsample_max_samples  # type: ignore\n            # creating random indices and downsampling\n            random_indices = downsample_generator.permutation(len(data))\n            data = [data[i] for i in random_indices[:max_num_samples]]\n\n        collate_fn_p = partial(collate_fn, pad=self.pad)\n        # TODO investigate dataset typing in PyTorch; sequence should\n        # be fine\n        loader = DataLoader(dataset=data,  # type: ignore\n                            shuffle=self.shuffle,\n                            batch_size=self.batch_size,\n                            collate_fn=collate_fn_p,\n                            num_workers=self.n_workers,\n                            pin_memory=self.pin_memory,\n                            drop_last=self.drop_last)\n\n        if n_epochs == -1:\n            while True:\n                yield from loader\n        else:\n            for _ in range(n_epochs):\n                yield from loader\n\n    def length(self, data: Sequence[Sequence[torch.Tensor]]) -> int:\n        """"""Return the number of batches in the sampler.\n\n        Parameters\n        ----------\n        data: Sequence[Sequence[torch.Tensor, ...]]\n            The input data to sample from\n\n        Returns\n        -------\n        int\n            The number of batches that would be created per epoch\n\n        """"""\n        downsample = self.downsample or 1\n        return math.ceil(downsample * len(data) / self.batch_size)\n'"
flambe/sampler/episodic.py,8,"b'from typing import Tuple, Iterator, Dict, Any, Sequence\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\n\nfrom flambe.sampler import Sampler\n\n\nclass EpisodicSampler(Sampler):\n    """"""Implement an EpisodicSample object.\n\n    Currently only supports sequence inputs.\n\n    """"""\n\n    def __init__(self,\n                 n_support: int,\n                 n_query: int,\n                 n_episodes: int,\n                 n_classes: int = None,\n                 pad_index: int = 0,\n                 balance_query: bool = False) -> None:\n        """"""Initialize the EpisodicSampler.\n\n        Parameters\n        ----------\n        n_support : int\n            The number of support points per class\n        n_query : int\n            If balance_query is True, this should be the number\n            of query points per class, otherwise, this is the total\n            number of query points for the episode\n        n_episodes : int\n            Number of episodes to run in one ""epoch""\n        n_classes : int, optional\n            The number of classes to sample per episode, defaults to all\n        pad_index : int, optional\n            The padding index used on sequences.\n        balance_query : bool, optional\n            If True, the same number of query points are sampled per\n            class, otherwise query points are sampled uniformly\n            from the input data.\n\n        """"""\n        self.pad = pad_index\n\n        self.n_support = n_support\n        self.n_query = n_query\n        self.n_classes = n_classes\n        self.n_episodes = n_episodes\n\n        self.balance_query = balance_query\n\n    def sample(self,\n               data: Sequence[Sequence[torch.Tensor]],\n               n_epochs: int = 1) -> Iterator[Tuple[torch.Tensor, ...]]:\n        """"""Sample from the list of features and yields batches.\n\n        Parameters\n        ----------\n        data: Sequence[Sequence[torch.Tensor, torch.Tensor]]\n            The input data as a list of (source, target) pairs\n        n_epochs: int, optional\n            The number of epochs to run in the output iterator.\n            For this object, the total number of batches will be\n            (n_episodes * n_epochs)\n\n        Yields\n        ------\n        Iterator[Tuple[Tensor, Tensor, Tensor, Tensor]]\n            In order: the query_source, the query_target\n            the support_source, and the support_target tensors.\n            For sequences, the batch is used as first dimension.\n\n        """"""\n        if len(data) == 0:\n            raise ValueError(""No examples provided"")\n\n        # Split dataset by target\n        target_to_examples: Dict[int, Any] = dict()\n        for source, target in data:\n            target_to_examples.setdefault(int(target), []).append((source, target))\n\n        all_classes = list(target_to_examples.keys())\n\n        for epoch in range(n_epochs):\n            for _ in range(self.n_episodes):\n                # Sample n_classes to run a training episode over\n                classes = all_classes\n                if self.n_classes is not None:\n                    classes = list(np.random.permutation(all_classes))[:self.n_classes]\n\n                # Sample n_support and n_query points per class\n                supports, queries = [], []\n                for i, target_class in enumerate(classes):\n                    examples = target_to_examples[target_class]\n                    indices = np.random.permutation(len(examples))\n                    supports.extend([(examples[j][0], i) for j in indices[:self.n_support]])\n\n                    if self.balance_query:\n                        query_indices = indices[self.n_support:self.n_support + self.n_query]\n                        queries.extend([(examples[j][0], i) for j in query_indices])\n                    else:\n                        queries.extend([(examples[j][0], i) for j in indices[self.n_support:]])\n\n                if not self.balance_query:\n                    indices = np.random.permutation(len(queries))\n                    queries = [queries[i] for i in indices[:self.n_query]]\n\n                query_source, query_target = list(zip(*queries))\n                support_source, support_target = list(zip(*supports))\n\n                query_source = pad_sequence(query_source,\n                                            batch_first=True,\n                                            padding_value=self.pad)\n                query_target = torch.tensor(query_target)\n\n                support_source = pad_sequence(support_source,\n                                              batch_first=True,\n                                              padding_value=self.pad)\n                support_target = torch.tensor(support_target)\n\n                if len(query_target.size()) == 2:\n                    query_target = query_target.squeeze()\n                if len(support_target.size()) == 2:\n                    support_target = support_target.squeeze()\n\n                yield (query_source.long(),\n                       query_target.long(),\n                       support_source.long(),\n                       support_target.long())\n\n    def length(self, data: Sequence[Sequence[torch.Tensor]]) -> int:\n        """"""Return the number of batches in the sampler.\n\n        Parameters\n        ----------\n        data: Sequence[Sequence[torch.Tensor, ...]]\n            The input data to sample from\n\n        Returns\n        -------\n        int\n            The number of batches that would be created per epoch\n\n        """"""\n        return self.n_episodes\n'"
flambe/sampler/sampler.py,5,"b'from abc import abstractmethod\nfrom typing import Iterator, Sequence, Tuple\n\nimport torch\n\nfrom flambe.compile import Component\n\n\nclass Sampler(Component):\n    """"""Base Sampler interface.\n\n    Objects implementing this interface should implement two methods:\n\n        - *sample*: takes a set of data and returns an iterator\n        - *lenght*: takes a set of data and return the length of the\n                    iterator that would be given by the sample method\n\n    Sampler objects are used inside the Trainer to provide the data to\n    the models. Note that pushing the data to the appropriate device\n    is usually done inside the Trainer.\n\n    """"""\n    @abstractmethod\n    def sample(self,\n               data: Sequence[Sequence[torch.Tensor]],\n               n_epochs: int = 1) -> Iterator[Tuple[torch.Tensor, ...]]:\n        """"""Sample from the list of features and yields batches.\n\n        Parameters\n        ----------\n        data: Sequence[Sequence[torch.Tensor, ...]]\n            The input data to sample from\n        n_epochs: int, optional\n            The number of epochs to run in the output iterator.\n\n        Yields\n        ------\n        Iterator[Tuple[Tensor]]\n            A batch of data, as a tuple of Tensors\n\n        """"""\n        pass\n\n    @abstractmethod\n    def length(self, data: Sequence[Sequence[torch.Tensor]]) -> int:\n        """"""Return the number of batches in the sampler.\n\n        Parameters\n        ----------\n        data: Sequence[Sequence[torch.Tensor, ...]]\n            The input data to sample from\n\n        Returns\n        -------\n        int\n            The number of batches that would be created per epoch\n\n        """"""\n        pass\n'"
flambe/tokenizer/__init__.py,0,"b""from flambe.tokenizer.tokenizer import Tokenizer\nfrom flambe.tokenizer.char import CharTokenizer\nfrom flambe.tokenizer.word import WordTokenizer, NLTKWordTokenizer, NGramsTokenizer\nfrom flambe.tokenizer.label import LabelTokenizer\n\n\n__all__ = ['Tokenizer', 'WordTokenizer', 'CharTokenizer',\n           'LabelTokenizer', 'NGramsTokenizer', 'NLTKWordTokenizer']\n"""
flambe/tokenizer/char.py,0,"b'from typing import List\n\nfrom flambe.tokenizer import Tokenizer\n\n\nclass CharTokenizer(Tokenizer):\n    """"""Implement a character level tokenizer.""""""\n\n    def tokenize(self, example: str) -> List[str]:\n        """"""Tokenize an input example.\n\n        Parameters\n        ----------\n        example : str\n            The input example, as a string\n\n        Returns\n        -------\n        List[str]\n            The output charachter tokens, as a list of strings\n\n        """"""\n        return list(example)\n'"
flambe/tokenizer/label.py,0,"b'\nfrom typing import Optional, List\n\nfrom flambe.tokenizer import Tokenizer\n\n\nclass LabelTokenizer(Tokenizer):\n    """"""Base label tokenizer.\n\n    This object tokenizes string labels into a list of a single or\n    multiple elements, depending on the provided separator.\n\n    """"""\n    def __init__(self, multilabel_sep: Optional[str] = None) -> None:\n        """"""Initialize the tokenizer.\n\n        Parameters\n        ----------\n        multilabel_sep : Optional[str], optional\n            Used to split multi label inputs, if given\n\n        """"""\n        self.multilabel_sep = multilabel_sep\n\n    def tokenize(self, example: str) -> List[str]:\n        """"""Tokenize an input example.\n\n        Parameters\n        ----------\n        example : str\n            The input example, as a string\n\n        Returns\n        -------\n        List[str]\n            The output tokens, as a list of strings\n\n        """"""\n        sep = self.multilabel_sep\n        return example.split(sep) if sep else [example]\n'"
flambe/tokenizer/tokenizer.py,0,"b'from abc import abstractmethod\nfrom typing import List\n\nfrom flambe import Component\n\n\nclass Tokenizer(Component):\n    """"""Base interface to a Tokenizer object.\n\n    Tokenizers implement the `tokenize` method, which takes a\n    string as input and produces a list of strings as output.\n\n    """"""\n\n    @abstractmethod\n    def tokenize(self, example: str) -> List[str]:\n        """"""Tokenize an input example.\n\n        Parameters\n        ----------\n        example : str\n            The input example, as a string\n\n        Returns\n        -------\n        List[str]\n            The output tokens, as a list of strings\n\n        """"""\n        pass\n\n    def __call__(self, example: str):\n        """"""Make a tokenizer callable.""""""\n        return self.tokenize(example)\n'"
flambe/tokenizer/word.py,0,"b'\nfrom typing import Union, List, Optional\n\nimport nltk\nfrom nltk import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom flambe.tokenizer import Tokenizer\n\n\nclass WordTokenizer(Tokenizer):\n    """"""Implement a word level tokenizer using\n       nltk.tokenize.word_tokenize """"""\n\n    def tokenize(self, example: str) -> List[str]:\n        """"""Tokenize an input example.\n\n        Parameters\n        ----------\n        example : str\n            The input example, as a string\n\n        Returns\n        -------\n        List[str]\n            The output word tokens, as a list of strings\n\n        """"""\n        return example.split()\n\n\nclass NLTKWordTokenizer(Tokenizer):\n    """"""Implement a word level tokenizer using\n       nltk.tokenize.word_tokenize """"""\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        nltk.download(\'punkt\', quiet=True)\n\n    def tokenize(self, example: str) -> List[str]:\n        """"""Tokenize an input example.\n\n        Parameters\n        ----------\n        example : str\n            The input example, as a string\n\n        Returns\n        -------\n        List[str]\n            The output word tokens, as a list of strings\n\n        """"""\n        return word_tokenize(example)\n\n\nclass NGramsTokenizer(Tokenizer):\n    """"""Implement a n-gram tokenizer\n\n    Examples\n    --------\n\n    >>> t = NGramsTokenizer(ngrams=2).tokenize(""hi how are you?"")\n    [\'hi, how\', \'how are\', \'are you?\']\n\n    >>> t = NGramsTokenizer(ngrams=[1,2]).tokenize(""hi how are you?"")\n    [\'hi,\', \'how\', \'are\', \'you?\', \'hi, how\', \'how are\', \'are you?\']\n\n    Parameters\n    ----------\n    ngrams: Union[int, List[int]]\n        An int or a list of ints. If it\'s a list of ints, all n-grams\n        (for each int) will be considered in the tokenizer.\n    exclude_stopwords: bool\n        Whether to exlude stopword or not.\n        See the related param stop_words\n    stop_words: Optional[List]\n        List of stop words to exclude when exclude_stopwords is True.\n        If None set to nltk.corpus.stopwords.\n\n    """"""\n    def __init__(self, ngrams: Union[int, List[int]] = 1,\n                 exclude_stopwords: bool = False,\n                 stop_words: Optional[List] = None) -> None:\n        """""" Initialize the NGramsTokenizer\n\n        Parameters\n        ----------\n        ngrams : Union[int, List[int]], optional\n            [description], by default 1\n        exclude_stopwords: bool\n            [description], by default False\n        stop_words: Optional[List]\n            [description], by default None\n\n        """"""\n        self.ngrams = ngrams\n        self.exclude_stopwords = exclude_stopwords\n\n        if self.exclude_stopwords:\n            self.stop_words = stop_words\n            if self.stop_words is None:\n                nltk.download(\'stopwords\', quiet=True)\n                self.stop_words = stopwords.words(\'english\')\n\n        nltk.download(\'punkt\', quiet=True)\n\n    @staticmethod\n    def _tokenize(example: str, n: int) -> List[str]:\n        """"""Tokenize an input example using ngrams.\n\n        """"""\n        return list("" "".join(x) if len(x) > 1 else x[0] for x in ngrams(word_tokenize(example), n))\n\n    def tokenize(self, example: str) -> List[str]:\n        """"""Tokenize an input example.\n\n        Parameters\n        ----------\n        example : str\n            The input example, as a string.\n\n        Returns\n        -------\n        List[str]\n            The output word tokens, as a list of strings\n\n        """"""\n        if self.exclude_stopwords and self.stop_words:\n            example = \' \'.join([word for word in word_tokenize(example)\n                                if word not in self.stop_words])\n\n        if isinstance(self.ngrams, List):\n            ret: List[str] = []\n            for i in self.ngrams:\n                ret.extend(self._tokenize(example, i))\n            return ret\n        else:\n            return NGramsTokenizer._tokenize(example, self.ngrams)\n'"
flambe/utils/__init__.py,0,"b""from flambe.utils.config import generate_config_from_template\n\n__all__ = ['generate_config_from_template']\n"""
flambe/utils/config.py,0,"b'import os\nimport re\nfrom typing import Dict\n\nimport jinja2\n\n\ndef generate_config_from_template(template_path: str,\n                                  config_path: str,\n                                  remove_comments: bool = False,\n                                  **template_kwargs: Dict[str, str]):\n    """"""\n    Parameters\n    ----------\n    template_path: str\n        The path to the config template\n    config_path: str\n        The path to which the rendered config should be written\n    remove_comments: bool\n        If `True`, removes comments from the rendered config before\n        writing it to disk\n    template_kwargs:\n        Keyword arguments to pass to your template, e.g.\n        `path=\'config.yaml\', foo=\'bar\'`\n\n    Example config:\n\n    ```yaml\n    !Experiment\n\n    foo: {{ bar }}\n        baz: {{ skittles }}\n    ```\n\n    If saved as config.yaml.template, then invoking:\n\n    ```python\n    generate_config_from_template(\'config.yaml.template\',\n        \'config.yaml\', bar=\'pickles\', skittles=\'yum\')\n    ```\n\n    the following config will be written to \'config.yaml\':\n\n    ```yaml\n    !Experiment\n\n    foo: pickles\n        baz: yum\n    ```\n    """"""\n    dirname = os.path.dirname(template_path)\n    basename = os.path.basename(template_path)\n    loader = jinja2.FileSystemLoader(searchpath=dirname)\n    env = jinja2.Environment(loader=loader, autoescape=True)\n    template = env.get_template(basename)\n    with open(config_path, \'w\') as f:\n        for line in template.render(**template_kwargs).split(\'\\n\'):\n            if remove_comments:\n                line = re.sub(\'# .*\', \'\', line).rstrip()\n            if line:\n                f.write(line + \'\\n\')\n'"
flambe/vision/__init__.py,0,"b""from flambe.vision import classification\n\n__all__ = ['classification']\n"""
tests/integration/test_builder.py,1,"b'import pytest\nimport flambe\nfrom flambe.nlp.classification import TextClassifier\nfrom flambe.compile import extensions\nfrom tempfile import TemporaryDirectory as tmpdir\nfrom tempfile import NamedTemporaryFile as tmpfile\nimport subprocess\nimport os\n\n\ndef module_equals(model1, model2):\n    """"""Check if 2 pytorch modules have the same weights\n\n    """"""\n    for p1, p2 in zip(model1.parameters(), model2.parameters()):\n        if p1.data.ne(p2.data).sum() > 0:\n            return False\n    return True\n\n\ndef test_exporter_builder(top_level):\n    with tmpdir() as d, tmpdir() as d2, tmpfile(mode=""w"", suffix="".yaml"") as f, tmpfile(mode=""w"", suffix="".yaml"") as f2:\n        # First run an experiment\n        exp = """"""\n!Experiment\n\nname: exporter\nsave_path: {save_path}\n\npipeline:\n  dataset: !TabularDataset.from_path\n    train_path: {top_level}/tests/data/dummy_tabular/train.csv\n    val_path: {top_level}/tests/data/dummy_tabular/val.csv\n    sep: \',\'\n    transform:\n      text: !TextField\n      label: !LabelField\n  model: !TextClassifier\n    embedder: !Embedder\n      embedding: !torch.Embedding\n        num_embeddings: !@ dataset.text.vocab_size\n        embedding_dim: 30\n      encoder: !PooledRNNEncoder\n        input_size: 30\n        rnn_type: lstm\n        n_layers: 1\n        hidden_size: 16\n    output_layer: !SoftmaxLayer\n      input_size: !@ model[embedder].encoder.rnn.hidden_size\n      output_size: !@ dataset.label.vocab_size\n\n  exporter: !Exporter\n    model: !@ model\n    text: !@ dataset.text\n""""""\n\n        exp = exp.format(save_path=d, top_level=top_level)\n        f.write(exp)\n        f.flush()\n        ret = subprocess.run([\'flambe\', f.name, \'-i\'])\n        assert ret.returncode == 0\n\n        # Then run a builder\n\n        builder = """"""\nflambe_inference: {top_level}/tests/data/dummy_extensions/inference/\n---\n\n!Builder\n\ndestination: {dest}\n\ncomponent: !flambe_inference.DummyInferenceEngine\n  model: !TextClassifier.load_from_path\n    path: {path}\n""""""\n        base = os.path.join(d, ""output__exporter"", ""exporter"")\n        path_aux = [x for x in os.listdir(base) if os.path.isdir(os.path.join(base, x))][0]  # Should be only 1 folder bc of no variants\n        model_path = os.path.join(base, path_aux, ""checkpoint"", ""checkpoint.flambe"", ""model"")\n\n        builder = builder.format(dest=d2, path=model_path, top_level=top_level)\n        f2.write(builder)\n        f2.flush()\n\n        ret = subprocess.run([\'flambe\', f2.name, \'-i\'])\n        assert ret.returncode == 0\n\n        # The extensions needs to be imported using extensions.py module\n        extensions.import_modules([""flambe_inference""])\n\n        # Import the module after import_modules (which registered tags already)\n        from flambe_inference import DummyInferenceEngine\n\n        eng1 = flambe.load(d2)\n\n        assert type(eng1) is DummyInferenceEngine\n        assert type(eng1.model) is TextClassifier\n\n        extension_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), ""tests/data/dummy_extensions/inference"")\n        assert eng1._extensions == {""flambe_inference"": extension_path}\n\n        eng2 = DummyInferenceEngine.load_from_path(d2)\n\n        assert type(eng2) is DummyInferenceEngine\n        assert type(eng2.model) is TextClassifier\n\n        assert eng2._extensions == {""flambe_inference"": extension_path}\n\n        assert module_equals(eng1.model, eng2.model)\n\n'"
tests/integration/test_examples.py,0,"b'import pytest\nfrom flambe.compile import Schema\nfrom flambe.experiment import Experiment\nfrom tempfile import TemporaryDirectory as tmpdir\nfrom tempfile import NamedTemporaryFile as tmpfile\nimport subprocess\nimport os\nfrom flambe.compile import yaml\n\n\ndef _reduce_iterations(d):\n    """"""Recursively update any iteration\'s config\n\n    """"""\n    for k, v in d.items():\n        if k == \'max_steps\' or k == \'iter_per_step\' or k == \'epoch_per_step\':\n            d[k] = 1\n\n        elif isinstance(v, Schema):\n            _reduce_iterations(v)\n\n\ndef _preprocess_experiment(fname, save_path):\n    content = list(yaml.load_all(open(fname)))\n    experiment = content[-1]\n    if isinstance(experiment, Experiment):\n        experiment.set_serializable_attr(""save_path"", save_path)\n        _reduce_iterations(experiment.pipeline)\n        return content\n\n    return None\n\n\ndef run_experiments(base, **kwargs):\n    """"""Run all experiments found in base param.\n    and check that flambe executes without errors.\n\n    Before running the configs, it updates the save_path to\n    be a tempdir and updates (potentially) the iteration\'s\n    params (if found) to be 1.\n\n    """"""\n    for fname in os.listdir(base):\n        full_f = os.path.join(base, fname)\n        if os.path.isfile(full_f) and fname.endswith(\'yaml\'):\n            with tmpdir() as d, tmpfile() as f, tmpfile(\'w\') as t:\n                content = open(full_f).read().format(**kwargs)\n                t.write(content)\n                t.flush()\n                new_exp = _preprocess_experiment(t.name, d)\n                if new_exp:\n                    yaml.dump_all(new_exp, f)\n                    ret = subprocess.run([\'flambe\', f.name, \'-i\'])\n                    assert ret.returncode == 0\n\n\n@pytest.mark.end2end\ndef test_end2end_experiments(top_level):\n    """"""Runs all experiments found in the integration\'s\n    folder\n\n    """"""\n    tests_base = os.path.dirname(os.path.dirname(__file__))\n    base = os.path.join(tests_base, ""integration"", ""end2end"")\n    run_experiments(base, top_level=top_level)\n\n\n@pytest.mark.examples\ndef test_examples_experiments():\n    """"""Runs all experiments found in top level examples\n    folder\n\n    """"""\n    tests_base = os.path.dirname(os.path.dirname(__file__))\n    base = os.path.join(os.path.dirname(tests_base), ""examples"")\n    run_experiments(base)\n'"
tests/integration/test_resources_experiment.py,0,"b'import tempfile\nimport subprocess\nimport os\n\n\ndef test_resources_config():\n    config = """"""\n!Experiment\n\nname: random\nsave_path: {}\n\nresources:\n  train: {}\n\npipeline:\n  dataset: !TabularDataset.from_path\n    train_path: !@ train\n\n""""""\n\n    with tempfile.TemporaryDirectory() as d, tempfile.NamedTemporaryFile(mode=""w"", suffix="".yaml"") as f:\n        test_data_folder = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), \'data\')\n        train_data = os.path.join(test_data_folder, \'dummy_tabular\', \'train.csv\')\n\n        exp = config.format(d, train_data)\n        f.write(exp)\n        f.flush()\n        ret = subprocess.run([\'flambe\', f.name])\n        assert ret.returncode == 0\n'"
tests/unit/__init__.py,0,b''
flambe/cluster/instance/__init__.py,0,"b""from flambe.cluster.instance.instance import Instance\nfrom flambe.cluster.instance.instance import CPUFactoryInstance, GPUFactoryInstance\nfrom flambe.cluster.instance.instance import OrchestratorInstance\n\n__all__ = ['CPUFactoryInstance', 'GPUFactoryInstance', 'OrchestratorInstance', 'Instance']\n"""
flambe/cluster/instance/errors.py,0,"b'\nclass RemoteCommandError(Exception):\n    """"""Error raised when any remote command/script fail in an Instance.\n\n    """"""\n    pass\n\n\nclass SSHConnectingError(Exception):\n    """"""Error raised when opening a SSH connection fails.\n\n    """"""\n    pass\n\n\nclass MissingAuthError(Exception):\n    """"""Error raised when there is missing authentication information.\n\n    """"""\n    pass\n\n\nclass RemoteFileTransferError(Exception):\n    """"""Error raised when sending a local file to an Instance fails.\n\n    """"""\n    pass\n'"
flambe/cluster/instance/instance.py,2,"b'""""""This modules includes base Instance classes to represent machines.\n\nAll Instance objects will be managed by Cluster objects\n(`flambe.cluster.cluster.Cluster`).\n\nThis base implementation is independant to the type of instance used.\n\nAny new instance that flambe should support should inherit from the\nclasses that are defined in this module.\n\n""""""\n# from __future__ import annotations\n\nimport time\nimport os\nimport paramiko\nfrom paramiko.client import SSHClient\nimport socket\nimport contextlib\nimport subprocess\n\nimport logging\nimport uuid\n\nfrom configparser import ConfigParser\nfrom typing import Optional, Type, Generator, TypeVar, List, Dict\nfrom types import TracebackType\n\nimport flambe\nfrom flambe.cluster import const\nfrom flambe.cluster.utils import RemoteCommand\nfrom flambe.cluster.instance import errors\n\nfrom flambe.runnable.utils import get_flambe_repo_location\nfrom flambe.logging import coloredlogs as cl\n\nlogger = logging.getLogger(__name__)\n\nInsT = TypeVar(""InsT"", bound=""Instance"")\n\n\nclass Instance(object):\n    """"""Encapsulates remote instances.\n\n    In this context, the instance is a running computer.\n\n    All instances used by flambe remote mode will inherit\n    `Intance`. This class provides high-level methods to deal with\n    remote instances (for example, sending a shell command over SSH).\n\n    *Important: Instance objects should be pickeable.* Make sure that\n    all child classes can be pickled.\n\n    The flambe local process will communicate with the remote instances\n    using SSH. The authentication mechanism will be using private keys.\n\n    Parameters\n    ----------\n    host : str\n        The public DNS host of the remote machine.\n    private_host : str\n        The private DNS host of the remote machine.\n    username : str\n        The machine\'s username.\n    key: str\n        The path to the ssh key used to communicate to the instance.\n    config : ConfigParser\n        The config object that contains useful information for the\n        instance. For example, `config[\'SSH\'][\'SSH_KEY\']` should\n        contain the path of the ssh key to login the remote instance.\n    debug : bool\n        True in case flambe was installed in dev mode, False otherwise.\n    use_public : bool\n        Wether this instance should use public or private IP. By\n        default, the public IP is used. Private host is used when\n        inside a private LAN.\n\n    """"""\n\n    def __init__(self,\n                 host: str,\n                 private_host: str,\n                 username: str,\n                 key: str,\n                 config: ConfigParser,\n                 debug: bool,\n                 use_public: bool = True) -> None:\n        self.host = host\n        self.private_host = private_host\n        self.username = username\n        self.key = key\n\n        self.config = config\n        self.fix_relpaths_in_config()\n\n        self.use_public = use_public\n        self.debug = debug\n\n        # Uses only one ssh client per instance object\n        self._cli: SSHClient = None\n\n    def fix_relpaths_in_config(self) -> None:\n        """"""Updates all paths to be absolute.\n        For example, if it contains ""~/a/b/c"" it will be change to\n        /home/user/a/b/c (the appropiate $HOME value)\n\n        """"""\n        for section in self.config:\n            for k, v in self.config[section].items():\n                if os.path.exists(v) and not os.path.isabs(v):\n                    self.config[section][k] = os.path.abspath(v)\n                if v.startswith(""~""):\n                    self.config[section][k] = os.path.expanduser(v)\n\n    def __enter__(self):\n        """"""Method to use `Instance` instances with context managers\n\n        Returns\n        -------\n        Instance\n            The current instance\n\n        """"""\n        return self\n\n    def __exit__(self,\n                 exc_type: Optional[Type[BaseException]],\n                 exc_value: Optional[BaseException],\n                 traceback: Optional[TracebackType]):\n        """"""Exit method for the context manager.\n\n        This method will catch any uprising exception and raise it.\n\n        """"""\n        if exc_value is not None:\n            print(f""{exc_value}"")\n\n        return False\n\n    def prepare(self) -> None:\n        """"""Runs all neccessary processes to prepare the instances.\n\n        The child classes should implement this method\n        according to the type of instance.\n\n        """"""\n        raise NotImplementedError()\n\n    def wait_until_accessible(self) -> None:\n        """"""Waits until the instance is accesible through SSHClient\n\n        It attempts `const.RETRIES` time to ping SSH port to See\n        if it\'s listening for incoming connections. In each attempt,\n        it waits `const.RETRY_DELAY`.\n\n        Raises\n        ------\n        ConnectionError\n            If the instance is unaccesible through SSH\n\n        """"""\n        retry_count = 0\n\n        while retry_count <= const.RETRIES:\n            if self.is_up():\n                logger.debug(f""Instance {self.host} is UP & accessible on port 22"")\n                return\n\n            time.sleep(const.RETRY_DELAY)\n            retry_count += 1\n            logger.debug(f""Instance {self.host} not accesible. Retrying"")\n\n        raise ConnectionError(f""{self.host} is unreachable through ssh."")\n\n    def is_up(self) -> bool:\n        """"""Tests wether port 22 is open to incoming SSH connections\n\n        Returns\n        -------\n        bool\n            True if instance is listening in port 22. False otherwise.\n\n        """"""\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(const.RETRY_DELAY)\n        result = sock.connect_ex((self.host if self.use_public else self.private_host, 22))\n        return result == 0\n\n    def _get_cli(self) -> paramiko.SSHClient:\n        """"""Get an `SSHClient` in order to execute commands.\n\n        This will cache an existing SSHClient to optimize resource.\n        This is a private method and should only be used in this module.\n\n        Returns\n        -------\n        paramiko.SSHClient\n            The client for latter use.\n\n        Raises\n        ------\n        SSHConnectingError\n            In case opening an SSH connection fails.\n\n        """"""\n        try:\n            if (self._cli is None or self._cli.get_transport() is None or\n                    not self._cli.get_transport().is_active()):\n                # Set cli in case it was not set or if it was closed\n                cli = paramiko.SSHClient()\n                cli.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n                hostname = self.host if self.use_public else self.private_host\n                cli.connect(hostname=hostname,\n                            username=self.username, key_filename=self.key,\n                            allow_agent=False, look_for_keys=False)\n                self._cli = cli\n            return self._cli\n        except paramiko.ssh_exception.SSHException:\n            raise errors.SSHConnectingError(f""Error opening SSH connection with {hostname}. ""\n                                            ""Double check information provided in the secrets file"")\n\n    def _run_cmd(self, cmd: str, retries: int = 1, wd: str = None) -> RemoteCommand:\n        """"""Runs a single shell command in the instance through SSH.\n\n        The command will be executed in one ssh connection.\n        Don\'t expect calling several time to `_run_cmd` expecting to\n        keep state between commands. To use mutliple commands, use:\n        `_run_script`\n\n        *Important: when running docker containers, don\'t use -it flag!*\n\n        This is a private method and should only be used in this module.\n\n        Parameters\n        ----------\n        cmd : str\n            The command to execute.\n        retries : int\n            The amount of attempts to run the command if it fails.\n            Default to 1.\n        wd : str\n            The working directory to \'cd\' before running the command\n\n        Returns\n        -------\n        RemoteCommand\n            A `RemoteCommand` instance with success boolean and message.\n\n        Examples\n        --------\n        To get $HOME env\n\n        >>> instance._run_cmd(""echo $HOME"")\n        RemoteCommand(True, ""/home/ubuntu"")\n\n        This will not work\n\n        >>> instance._run_cmd(""export var=10"")\n        >>> instance._run_cmd(""echo $var"")\n        RemoteCommand(False, """")\n\n        This will work\n\n        >>> instance._run_cmd(""export var=10; echo $var"")\n        RemoteCommand(True, ""10"")\n\n        Raises\n        ------\n        RemoteCommandError\n            In case the `cmd` failes after `retries` attempts.\n\n        """"""\n        if retries <= 0:\n            raise ValueError(""\'retries\' parameter should be > 0"")\n\n        for i in range(retries):\n            cli = self._get_cli()\n\n            try:\n                if wd:\n                    cmd = f""cd {wd}; {cmd}""\n\n                status, stdout, stderr = cli.exec_command(cmd)\n\n                # Blocks until done\n                while not stdout.channel.exit_status_ready():\n                    status = stdout.channel.recv_exit_status()\n\n                out, err = stdout.read(), stderr.read()\n\n                success = status == 0\n\n                if not success:\n                    logger.debug(f""Retry {i}. {cmd} failed with message: {err}"")\n                else:\n                    logger.debug(f""\'{cmd}\' ran successfully"")\n                    return RemoteCommand(success, out if success else err)\n\n            except errors.SSHConnectingError:\n                raise\n            except Exception as err:\n                raise errors.RemoteCommandError(err)\n\n        logger.debug(f""\'{cmd}\' returning after {retries} intents returning != 0"")\n        return RemoteCommand(success, out if success else err)\n\n    def _run_script(self, fname: str, desc: str) -> RemoteCommand:\n        """"""Runs a script by copyinh the script to the instance and\n        executing it.\n\n        This is a private method and should only be used in this module.\n\n        Parameters\n        ----------\n        fname : str\n            The script filename\n        desc : str\n            A description for the script purpose. This will be used\n            for the copied filename\n\n        Returns\n        -------\n        RemoteCommand\n            A `RemoteCommand` instance with success boolean and message.\n\n        Raises\n        ------\n        RemoteCommandError\n            In case the script fails.\n\n        """"""\n        # TODO it can exist\n        with self._remote_script(fname, desc) as rs:\n            return self._run_cmd(f""./{rs}"")\n\n    @contextlib.contextmanager\n    def _remote_script(self, host_fname: str, desc: str) -> Generator[str, None, None]:\n        """"""Sends a local file containing a script to the instance\n        using Paramiko SFTP.\n\n        It should be used as a context manager for latter execution of\n        the script. See `_run_script` on how to use it.\n\n        After the context manager exists, then the file is removed from\n        the instance.\n\n        This is a private method and should only be used in this module.\n\n        Parameters\n        ----------\n        host_fname : str\n            The local script filename\n        desc : str\n            A description for the script purpose. This will be used\n            for the copied filename\n\n        Yields\n        -------\n        str\n            The remote filename of the copied local file.\n\n        Raises\n        ------\n        RemoteCommandError\n            In case sending the script fails.\n\n        """"""\n        random_fname = f""{desc}_{uuid.uuid4().hex}.sh""\n\n        cli = paramiko.SSHClient()\n        cli.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        cli.connect(hostname=self.host, username=self.username,\n                    key_filename=self.key)\n        sftp = cli.open_sftp()\n\n        try:\n            random_fname = f""{desc}_{uuid.uuid4().hex}.sh""\n            sftp.put(host_fname, random_fname)\n            cmd = self._run_cmd(f""chmod +x {random_fname}"")\n\n            if cmd.success:\n                yield random_fname\n            else:\n                raise errors.RemoteCommandError(f""Error sending local script. {cmd.msg}"")\n\n        finally:\n            sftp.remove(random_fname)\n            sftp.close()\n            cli.close()\n\n    def run_cmds(self, setup_cmds: List[str]) -> None:\n        """"""Execute a list of sequential commands\n\n        Parameters\n        ----------\n        setup_cmds: List[str]\n            The list of commands\n\n        Returns\n        -------\n        RemoteCommandError\n            In case at least one command is not successful\n\n        """"""\n        for s in setup_cmds:\n            ret = self._run_cmd(s, retries=3)\n            if not ret.success:\n                raise errors.RemoteCommandError(f""Error executing {s} in {self.host}. "" +\n                                                f""{ret.msg}"")\n\n    def send_rsync(self, host_path: str, remote_path: str, params: List[str] = None) -> None:\n        """"""Send a local file or folder to a remote instance with rsync.\n\n        Parameters\n        ----------\n        host_path : str\n            The local filename or folder\n        remote_path : str\n            The remote filename or folder to use\n        params : List[str], optional\n            Extra parameters to be passed to rsync.\n            For example, [""--filter=\':- .gitignore\'""]\n\n        Raises\n        ------\n        RemoteFileTransferError\n            In case sending the file fails.\n\n        """"""\n        if not os.path.exists(host_path):\n            raise errors.RemoteFileTransferError(f""{host_path} does not exist."")\n\n        _from = host_path\n        if os.path.isdir(host_path) and not host_path.endswith(os.sep):\n            _from = f""{host_path}{os.sep}""\n\n        _to = f""{self.username}@{self.host if self.use_public else self.private_host}:{remote_path}""\n\n        rsync_params = """"\n        if params:\n            rsync_params = "" "".join(params)\n        cmd = (\n            f\'rsync {rsync_params} -ae ""ssh -i {self.key} -o StrictHostKeyChecking=no"" \'\n            f\'{_from} {_to}\'\n        )\n        try:\n            subprocess.check_call(cmd, stdout=subprocess.DEVNULL,\n                                  stderr=subprocess.DEVNULL,\n                                  shell=True)\n            logger.debug(f""rsync {host_path} -> {remote_path} successful"")\n        except subprocess.CalledProcessError as e:\n            raise errors.RemoteFileTransferError(e)\n\n    def get_home_path(self) -> str:\n        """"""Return the $HOME value of the instance.\n\n        Returns\n        -------\n        str\n            The $HOME env value.\n\n        Raises\n        ------\n        RemoteCommandError\n            If after 3 retries it is not able to get $HOME.\n\n        """"""\n        cmd = self._run_cmd(""echo $HOME"", retries=3)\n\n        if cmd.success:\n            return cmd.msg.decode(""utf-8"").strip()\n\n        raise errors.RemoteCommandError(f""Could not access $HOME env variable. {cmd.msg}"")\n\n    def clean_containers(self) -> None:\n        """"""Stop and remove all containers running\n\n        Raises\n        ------\n        RemoteCommandError\n            If command fails\n\n        """"""\n        cmd = f\'\'\'\n        docker stop $(docker ps -a -q);\n        docker rm $(docker ps -a -q);\n        \'\'\'\n\n        ret = self._run_cmd(cmd)\n\n        if not ret.success:\n            raise errors.RemoteCommandError(""Could not clean containers"")\n\n    def clean_container_by_image(self, image_name: str) -> None:\n        """"""Stop and remove all containers given an image name.\n\n        Parameters\n        ----------\n        image_name : str\n            The name of the image for which all containers\n            should be stopped and removed.\n\n        Raises\n        ------\n        RemoteCommandError\n            If command fails\n\n        """"""\n        cmd = f""docker rm $(docker stop ""\\\n              f""$(docker ps -a -q --filter ancestor={image_name} --format=\'{{{{.ID}}}}\'))""\n        res = self._run_cmd(cmd)\n\n        if not res.success:\n            raise errors.RemoteCommandError(f""Could not clean container {image_name}. {res.msg}"")\n\n    def clean_container_by_command(self, command: str) -> None:\n        """"""Stop and remove all containers with the given command.\n\n        Parameters\n        ----------\n        command : str\n            The command used to stop and remove the containers\n\n        Raises\n        ------\n        RemoteCommandError\n            If command fails\n\n        """"""\n        cmd = f""docker rm -f $(docker inspect -f \'{{{{.ID}}}} ""\\\n              f""{{{{.Config.Cmd}}}}\' $(docker ps -a -q) | grep {command} | awk \'{{print $1}}\')""\n        res = self._run_cmd(cmd)\n\n        if not res.success:\n            raise errors.RemoteCommandError(\n                f""Could not clean container with cmd {command}. {res.msg}"")\n\n    def install_docker(self) -> None:\n        """"""Install docker in a Ubuntu 18.04 distribution.\n\n        Raises\n        ------\n        RemoteCommandError\n            If it\'s not able to install docker.\n            ie. then the installation script fails\n\n        """"""\n        fname = os.path.join(os.path.dirname(__file__), ""scripts/install_docker.sh"")\n        cmd = self._run_script(fname, ""install_docker"")\n        if not cmd.success:\n            raise errors.RemoteCommandError(f""Could not install docker. {cmd.msg}"")\n\n    def install_extensions(self, extensions: Dict[str, str]) -> None:\n        """"""Install local + pypi extensions.\n\n        Parameters\n        ----------\n        extension: Dict[str, str]\n            The extensions, as a dict from module_name to location\n\n        Raises\n        ------\n        errors.RemoteCommandError\n            If could not install an extension\n\n        """"""\n        cmd = [\'python3\', \'-m\', \'pip\', \'install\', \'-U\', \'--user\']\n        for ext, resource in extensions.items():\n            curr_cmd = cmd[:]\n\n            if \'PIP\' in self.config:\n                host = self.config[\'PIP\'].get(\'HOST\', None)\n                if host:\n                    curr_cmd.extend([""--trusted-host"", host])\n\n                host_url = self.config[\'PIP\'].get(\'HOST_URL\', None)\n                if host_url:\n                    curr_cmd.extend([""--extra-index-url"", host_url])\n\n            if os.path.exists(resource):\n                # Package is local\n                if os.sep not in resource:\n                    resource = f""./{resource}""\n            else:\n                # Package follows pypi notation: ""torch>=0.4.1,<1.1""\n                resource = f""{resource}""\n\n            curr_cmd.append(resource)\n\n            ret = self._run_cmd("" "".join(curr_cmd))\n            if not ret.success:\n                raise errors.RemoteCommandError(\n                    f""Could not install package {resource} in {self.host}""\n                )\n\n    def install_flambe(self) -> None:\n        """"""Pip install Flambe.\n\n        If dev mode is activated, then it rsyncs the local flambe\n        folder and installs that version. If not, downloads from pypi.\n\n        Raises\n        ------\n        RemoteCommandError\n            If it\'s not able to install flambe.\n\n        """"""\n        flags = []\n        if \'PIP\' in self.config:\n            host = self.config[\'PIP\'].get(\'HOST\', None)\n            if host:\n                flags.append(f""--trusted-host {host}"")\n\n            host_url = self.config[\'PIP\'].get(\'HOST_URL\', None)\n            if host_url:\n                flags.append(f""--extra-index-url {host_url}"")\n\n        if not self.debug:\n            pip_flambe = ""flambe"" if not self.contains_gpu() else ""flambe[cuda]""\n            logger.debug(f""Installing flambe in {self.host} using pypi"")\n            ret = self._run_cmd(\n                f""python3 -m pip install --user --upgrade ""\n                f""{\' \'.join(flags)} {pip_flambe}=={flambe.__version__}"",\n                retries=3\n            )\n\n        else:\n            origin = get_flambe_repo_location()\n            destination = os.path.join(self.get_home_path(), ""extensions"", ""flambe"")\n\n            self.send_rsync(origin, destination, params=[""--exclude=\'.*\'"", ""--exclude=\'docs/*\'""])\n            logger.debug(f""Sent flambe {origin} -> {destination}"")\n            pip_destination = destination if not self.contains_gpu() else f""{destination}[cuda]""\n            ret = self._run_cmd(\n                f""python3 -m pip install --user --upgrade {\' \'.join(flags)} {pip_destination}"",\n                retries=3\n            )\n\n        if not ret.success:\n            raise errors.RemoteCommandError(f""Could not install flambe. {ret.msg}"")\n        else:\n            logger.debug(f""Installed flambe in {self.host} successfully"")\n\n    def is_docker_installed(self) -> bool:\n        """"""Check if docker is installed in the instance.\n\n        Executes command ""docker --version"" and expect it not to fail.\n\n        Returns\n        -------\n        bool\n            True if docker is installed. False otherwise.\n\n        """"""\n        cmd = self._run_cmd(""docker --version"")\n        return cmd.success\n\n    def is_flambe_installed(self, version: bool = True) -> bool:\n        """"""Check if flambe is installed and if it matches version.\n\n        Parameters\n        ----------\n        version: bool\n            If True, also the version will be used. That is, if flag\n            is True and the remote flambe version is different from the\n            local flambe version, then this method will return False.\n            If they match, then True. If version is False this method\n            will return if there is ANY flambe version in the host.\n\n        Returns\n        ------\n        bool\n\n        """"""\n        # First check if a version of flambe is installed\n        ret = self._run_cmd(""bash -lc \'flambe --help\'"")\n        if not ret.success:\n            return False\n\n        if version:\n            cmd = ""python3 -c \'import flambe; print(flambe.__version__)\'""\n            ret = self._run_cmd(cmd)\n\n            if not ret.success:\n                raise errors.RemoteCommandError(\n                    f""Could not run flambe in python at {self.host} even if binary was found.""\n                )\n\n            return ret.msg.strip() == bytes(flambe.__version__, \'utf-8\')\n\n        return True\n\n    def is_docker_running(self) -> bool:\n        """"""Check if docker is running in the instance.\n\n        Executes the command ""docker ps"" and expects it not to fail.\n\n        Returns\n        -------\n        bool\n            True if docker is running. False otherwise.\n\n        """"""\n        cmd = self._run_cmd(""docker ps"")\n        return cmd.success\n\n    def start_docker(self) -> None:\n        """"""Restart docker.\n\n        Raises\n        ------\n        RemoteCommandError\n            If it\'s not able to restart docker.\n\n        """"""\n        cmd = self._run_cmd(""sudo systemctl restart docker"")\n        if not cmd.success:\n            raise errors.RemoteCommandError(f""Could not start docker. {cmd.msg}"")\n\n    def is_node_running(self) -> bool:\n        """"""Return if the host is running a ray node\n\n        Returns\n        -------\n        bool\n\n        """"""\n        cmd = ""ps -e | grep ray""\n        ret = self._run_cmd(cmd)\n        return ret.success\n\n    def is_flambe_running(self) -> bool:\n        """"""Return if the host is running flambe\n\n        Returns\n        -------\n        bool\n\n        """"""\n        cmd = ""ps axco command | grep -P ^flambe$""\n        ret = self._run_cmd(cmd)\n        return ret.success\n\n    def existing_dir(self, _dir: str) -> bool:\n        """"""Return if a directory exists in the host\n\n        Parameters\n        ----------\n        _dir: str\n            The name of the directory. It needs to be relative to $HOME\n\n        Returns\n        -------\n        bool\n            True if exists. Otherwise, False.\n\n        """"""\n        cmd = f""[ -d {self.get_home_path()}/{_dir} ]""\n        ret = self._run_cmd(cmd)\n        return ret.success\n\n    def shutdown_node(self) -> None:\n        """"""Shut down the ray node in the host.\n\n        If the node is also the main node, then the entire\n        cluster will shut down\n\n        """"""\n        if not self.is_node_running():\n            logger.debug(""Tried to shutdown a non existing node"")\n            return\n\n        cmd = self._run_cmd(\n            ""bash -lc \'ray stop\'"")\n\n        if cmd.success:\n            logger.debug(f""Ray node stopped at {self.host}"")\n        else:\n            raise errors.RemoteCommandError(f""Ray node failed to stop. {cmd.msg}"")\n\n    def shutdown_flambe(self) -> None:\n        """"""Shut down flambe in the host\n\n        """"""\n        if not self.is_flambe_running():\n            logger.debug(""Tried to shutdown flambe in a host that it\'s not runing flambe"")\n            return\n\n        cmd = self._run_cmd(""killall -9 flambe"")\n\n        if cmd.success:\n            logger.debug(f""Flambe killed in {self.host}"")\n        else:\n            raise errors.RemoteCommandError(f""Flambe failed to be shutdown. {cmd.msg}"")\n\n    def create_dirs(self, relative_dirs: List[str]) -> None:\n        """"""Create the necessary folders in the host.\n\n        Parameters\n        ----------\n        relative_dirs: List[str]\n            The directories to create. They should be relative paths\n            and $HOME of each host will be used to add the prefix.\n\n        """"""\n        # Create user\'s folders to deal with the experiment\n        for d in relative_dirs:\n            ret = self._run_cmd(f""[ -d {d} ]"")\n            if not ret.success:\n                ret = self._run_cmd(f""mkdir -p {d}"")\n                if ret.success:\n                    logger.debug(f""Foldcreate_dirs {d} created in {self.host}"")\n            else:\n                logger.debug(f""Existing folder {d} in {self.host}"")\n\n    def remove_dir(self, _dir: str, content_only: bool = True) -> None:\n        """"""Delete the specified dir result folder.\n\n        Parameters\n        ----------\n        _dir: str\n            The directory. It needs to be relative to the $HOME path as\n            it will be prepended as a prefix.\n        content_only: bool\n            If True, the folder itseld will not be erased.\n\n        """"""\n        path = f""{self.get_home_path()}/{_dir}""\n        if content_only:\n            cmd = self._run_cmd(f""rm -rf {path}/*"")\n        else:\n            cmd = self._run_cmd(f""rm -rf {path}/"")\n        if cmd.success:\n            logger.debug(f""Removed {path} at {self.host}."")\n        else:\n            raise errors.RemoteCommandError(\n                f""Failed to remove {path} on {self.host}. {cmd.msg}""\n            )\n\n    def contains_gpu(self) -> bool:\n        """"""Return if this machine contains GPU.\n\n        This method will be used to possibly upgrade\n        this factory to a GPUFactoryInstance.\n\n        """"""\n        cmd = ""python3 -c \'import torch; print(torch.cuda.is_available())\'""\n        ret = self._run_cmd(cmd)\n\n        if not ret.success:\n            raise errors.RemoteCommandError(""Factory does not contain torch installed"")\n\n        return ret.msg.strip() == b""True""\n\n\nclass CPUFactoryInstance(Instance):\n    """"""This class represents a CPU Instance in the Ray cluster.\n\n    CPU Factories are instances that can run only one worker\n    (no GPUs available). This class is mostly useful debugging.\n\n    Factory instances will not keep any important information.\n    All information is going to be sent to an orchestrator machine.\n    """"""\n\n    def prepare(self) -> None:\n        """"""Prepare a CPU machine to be a worker node.\n\n        Checks if flambe is installed, and if not, installs it.\n\n        Raises\n        ------\n        RemoteCommandError\n            In case any step of the preparing process fails.\n\n        """"""\n        self.install_flambe()\n\n    def launch_node(self, redis_address: str) -> None:\n        """"""Launch the ray worker node.\n\n        Parameters\n        ----------\n        redis_address : str\n            The URL of the main node. Must be IP:port\n\n        Raises\n        ------\n        RemoteCommandError\n            If not able to run node.\n\n        """"""\n        # https://stackoverflow.com/a/18665363.\n        # `ray` is in ~/.local/bin that is not in $PATH in paramiko.\n        # For this, use bash and -lc flags\n        cmd = f""bash -lc \'ray start --redis-address {redis_address}\'""\n        ret = self._run_cmd(cmd)\n\n        if ret.success:\n            logger.debug(f""Ray worker node launched at {self.host}"")\n        else:\n            raise errors.RemoteCommandError(f""Could not launch worker node. {ret.msg}"")\n\n    def num_cpus(self) -> int:\n        """"""Return the number of CPUs this host contains.\n\n        """"""\n        cmd = self._run_cmd(f""python3 -c \'import multiprocessing; "" +\n                            ""print(multiprocessing.cpu_count())\'"")\n\n        if cmd.success:\n            return int(cmd.msg)\n\n        raise errors.RemoteCommandError(f""Could not find out the number of CPUs. {cmd.msg}"")\n\n    def num_gpus(self) -> int:\n        """"""Get the number of GPUs this host contains\n\n        Returns\n        -------\n        int\n            The number of GPUs\n\n        Raises\n        ------\n        RemoteCommandError\n            If command to get the number of GPUs fails.\n\n        """"""\n        cmd = self._run_cmd(f""python3 -c \'import torch; print(torch.cuda.device_count())\'"")\n\n        if cmd.success:\n            return int(cmd.msg)\n\n        raise errors.RemoteCommandError(f""Could not find out how many GPUs available. {cmd.msg}"")\n\n\nclass GPUFactoryInstance(CPUFactoryInstance):\n    """"""This class represents an Nvidia GPU Factory Instance.\n\n    Factory instances will not keep any important information.\n    All information is going to be sent to an Orchestrator machine.\n\n    """"""\n\n    def prepare(self) -> None:\n        """"""Prepare a GPU instance to run a ray worker node. For this, it\n        installs CUDA and flambe if not installed.\n\n        Raises\n        ------\n        RemoteCommandError\n            In case any step of the preparing process fails.\n\n        """"""\n        if not self.is_cuda_installed():\n            logger.debug(f""Installing CUDA at {self.host}"")\n            self.install_cuda()\n\n        super().prepare()\n\n    def install_cuda(self) -> None:\n        """"""Install CUDA 10.0 drivers in an Ubuntu 18.04 distribution.\n\n        Raises\n        ------\n        RemoteCommandError\n            If it\'s not able to install drivers. ie if script fails\n\n        """"""\n        fname = os.path.join(os.path.dirname(__file__), ""scripts/install_cuda_ubuntu1804.sh"")\n        cmd = self._run_script(fname, ""install_cuda"")\n\n        if not cmd.success:\n            raise errors.RemoteCommandError(f""Could not install CUDA. {cmd.msg}"")\n\n    def is_cuda_installed(self) -> bool:\n        """"""Check if CUDA is installed trying to execute `nvidia-smi`\n\n        Returns\n        -------\n        bool\n            True if CUDA is installed. False otherwise.\n\n        """"""\n        cmd = self._run_cmd(""nvidia-smi"")\n        return cmd.success\n\n\nclass OrchestratorInstance(Instance):\n    """"""The orchestrator instance will be the main machine in a cluster.\n\n    It is going to be the main node in the ray cluster and it will\n    also host other services. TODO: complete\n\n    All services besides ray will run in docker containers.\n\n    This instance does not needs to be a GPU machine.\n\n    """"""\n\n    def prepare(self) -> None:\n        """"""Install docker and flambe\n\n        Raises\n        ------\n        RemoteCommandError\n            In case any step of the preparing process fails.\n\n        """"""\n        if not self.is_docker_installed():\n            self.install_docker()\n\n        if not self.is_docker_running():\n            self.start_docker()\n\n        self.install_flambe()\n\n    def launch_report_site(self, progress_file: str,\n                           port: int,\n                           output_log: str,\n                           output_dir: str,\n                           tensorboard_port: int) -> None:\n        """"""Launch the report site.\n\n        The report site is a Flask web app.\n\n        Raises\n        ------\n        RemoteCommandError\n            In case the launch process fails\n\n        """"""\n        tensorboard_url = f""http://{self.host}:{tensorboard_port}""\n\n        cmd = (\n            f""tmux new-session -d -s \'flambe-site\' \'bash -lc \\""flambe-site {progress_file} ""\n            f""--tensorboard_url {tensorboard_url} ""\n            f""--host 0.0.0.0 --port {port} ""\n            f""--output-dir {output_dir} ""\n            f""--output-log {output_log} &>> outputsite.log\\""\'""\n        )\n\n        res = self._run_cmd(cmd)\n\n        # Sometimes tmux command returns failure (because of some\n        # timeout) but website is running.\n        # Adding this extra check in that case.\n        if res.success or self.is_report_site_running():\n            logger.info(cl.BL(f""Report site at http://{self.host}:{port}""))\n        else:\n            raise errors.RemoteCommandError(f""Report site failed to run. {res.msg}"")\n\n    def is_tensorboard_running(self) -> bool:\n        """"""Return wether tensorboard is running in the host as docker.\n\n        Returns\n        -------\n        bool\n            True if Tensorboard is running, False otherwise.\n\n        """"""\n        cmd = ""docker ps | grep tensorboard""\n        ret = self._run_cmd(cmd)\n        return ret.success\n\n    def is_report_site_running(self) -> bool:\n        """"""Return wether the report site is running in the host\n\n        Returns\n        -------\n        bool\n\n        """"""\n        cmd = ""ps axco command | grep -P ^flambe-site$""\n        ret = self._run_cmd(cmd)\n        return ret.success\n\n    def remove_tensorboard(self) -> None:\n        """"""Removes tensorboard from the orchestrator.\n\n        """"""\n        self.clean_container_by_command(""tensorboard"")\n\n    def remove_report_site(self) -> None:\n        """"""Remove report site from the orchestrator.\n\n        """"""\n        cmd = ""pkill flambe-site""\n        ret = self._run_cmd(cmd)\n\n        if self.existing_tmux_session(""flambe-site""):\n            self.kill_tmux_session(""flambe-site"")\n\n        return ret.success\n\n    def launch_tensorboard(self,\n                           logs_dir: str,\n                           tensorboard_port: int) -> None:\n        """"""Launch tensorboard.\n\n        Parameters\n        ----------\n        logs_dir : str\n            Tensorboard logs directory\n        tensorboard_port: int\n            The port where tensorboard will be available\n\n        Raises\n        ------\n        RemoteCommandError\n            In case the launch process fails\n\n        """"""\n        if not self.is_docker_installed():\n            logger.error(""Can\'t run tensorboard. Docker not installed."")\n            return\n\n        cmd = self._run_cmd(\n            f""docker run -d -p {tensorboard_port}:6006 -v "" +\n            f""{os.path.join(self.get_home_path(), logs_dir)}:"" +\n            f""/tensorboard_logs {const.TENSORBOARD_IMAGE} tensorboard --logdir /tensorboard_logs"")\n        if cmd.success:\n            logger.debug(f""Tensorboard running at http://{self.host}:{tensorboard_port} . "" +\n                         ""Be aware that it can take a while until it starts showing results."")\n        else:\n            raise errors.RemoteCommandError(f""Tensorboard stable failed to run. {cmd.msg}"")\n\n    def existing_tmux_session(self, session_name: str) -> bool:\n        """"""Return if there is an existing tmux session with the same\n        name\n\n        Parameters\n        ----------\n        session_name: str\n            The exact name of the searched tmux session\n\n        Returns\n        -------\n        bool\n\n        """"""\n        cmd = f\'tmux ls -F ""#{{session_name}}"" | grep -P ^{session_name}$\'\n        ret = self._run_cmd(cmd)\n        return ret.success\n\n    def kill_tmux_session(self, session_name: str) -> None:\n        """"""Kill an existing tmux session\n\n        Parameters\n        ----------\n        session_name: str\n            The exact name of the tmux session to be removed\n\n        """"""\n        cmd = f\'tmux kill-session -t {session_name}\'\n        ret = self._run_cmd(cmd)\n\n        if ret.success:\n            logger.debug(f""Remove existing tmux session {session_name}"")\n        else:\n            raise errors.RemoteCommandError(f""Tried to remove a session. {ret.msg}"")\n\n    def launch_flambe(self,\n                      config_file: str,\n                      secrets_file: str,\n                      force: bool) -> None:\n        """"""Launch flambe execution in the remote host\n\n        Parameters\n        ----------\n        config_file: str\n            The config filename relative to the orchestrator\n        secrets_file: str\n            The filepath containing the secrets for the orchestrator\n        force: bool\n            The force parameters that was originally passed to flambe\n\n        """"""\n        force_params = ""--force"" if force else """"\n        cmd = (\n            f""tmux new-session -d -s \'flambe\' "" +\n            f""\'bash -lc \\""flambe {config_file} --secrets {secrets_file} "" +\n            f""{force_params} &> output.log\\""\'""\n        )\n\n        ret = self._run_cmd(cmd)\n\n        # Sometimes tmux command returns failure (because of some\n        # timeout) but flambe is running.\n        # Adding this extra check in that case.\n        if ret.success or self.is_flambe_running():\n            logger.info(cl.GR(""Running flambe in Orchestrator""))\n        else:\n            raise errors.RemoteCommandError(f""Not able to run flambe. {ret.msg}"")\n\n    def launch_node(self, port: int) -> None:\n        """"""Launch the main ray node in given sftp server in port 49559.\n\n        Parameters\n        ----------\n        port: int\n            Available port to launch the redis DB of the main ray node\n\n        Raises\n        ------\n        RemoteCommandError\n            In case the launch process fails\n\n        """"""\n        # https://stackoverflow.com/a/18665363.\n        # `ray` is in ~/.local/bin that is not in $PATH in paramiko.\n        # For this, use bash and -lc flags\n        cmd = self._run_cmd(\n            f""bash -lc \'ray start --head --num-cpus=0 --redis-port={port}\'"")\n\n        if cmd.success:\n            logger.debug(f""Ray main node running in {self.host}"")\n        else:\n            raise errors.RemoteCommandError(f""Ray main node failed to run. {cmd.msg}"")\n\n    def worker_nodes(self) -> List[str]:\n        """"""Returns the list of worker nodes\n\n        Returns\n        -------\n        List[str]\n            The list of worker nodes identified by their hostname\n\n        """"""\n        redis_address = f""\\""{self.private_host}:{const.RAY_REDIS_PORT}\\""""\n        cmd = ""python3 -c \'\\n""\\\n              ""import time\\n""\\\n              ""import ray\\n""\\\n              f""ray.init(redis_address={redis_address})\\n""\\\n              ""@ray.remote\\n""\\\n              ""def f():\\n""\\\n              ""    time.sleep(0.01)\\n""\\\n              ""    return ray.services.get_node_ip_address()\\n""\\\n              ""print(set(ray.get([f.remote() for _ in range(1000)])))\\n\'""\n\n        ret = self._run_cmd(cmd)\n\n        if not ret.success:\n            raise errors.RemoteCommandError(f""Failed to run Python script. {ret.msg}"")\n\n        return [s[1:-1] for s in (ret.msg[1:-2]).decode(""utf-8"").split(\',\')]\n\n    def rsync_folder(self, _from, _to, exclude=None):\n        """"""Rsyncs folders or files.\n\n        One of the folders NEEDS to be local. The remaining one can\n        be remote if needed.\n\n        """"""\n        # -o StrictHostKeyChecking=no if not rsync asks for accepting\n        # key and it fails (the cmd is not providing the \'yes\' answer)\n        # with message Host keys verification failed. \\r\\nrsync:\n        # connection unexpectedly closed (0 bytes received so far)\n        exc = """"\n        if exclude:\n            for x in exclude:\n                exc += f"" --exclude {x} ""\n\n        if not _from.endswith(os.sep):\n            _from = f""{_from}{os.sep}""\n\n        cmd = (\n            f""rsync -ae \'ssh -i {self.get_home_path()}/{const.PRIVATE_KEY} ""\n            f""-o StrictHostKeyChecking=no\' {exc} ""\n            f""{_from} {_to}""\n        )\n\n        ret = self._run_cmd(cmd)\n\n        if not ret.success:\n            logger.debug(f""Could not rsync between {self.private_host} and {_to}"")\n        else:\n            logger.debug(f""Rsync successful between {_from} -> {_to}"")\n'"
flambe/experiment/webapp/__init__.py,0,b''
flambe/experiment/webapp/app.py,0,"b'""""""Report site using Flambe\n\nThe report site is the main control centre for the remote experiment.\nIt\'s responsible of showing the current status of the experiment and to\nprovide the resources that the experiment\nis producing (metrics, trained models, etc.)\n\nThe website consumes the files that flambe outputs, it doesn\'t require\nextra running resource (like for example, Redis DB).\n\nThe website is implemented using Flask.\n\n*Important: no typing is provided for now.*\n\n""""""\n\nimport os\nfrom flask import Flask, render_template, send_file, jsonify\n\nimport shutil\n\nimport pickle\nimport subprocess\nimport tempfile\nfrom time import sleep\n\napp = Flask(__name__)\n\n\ndef load_state():\n    """"""Get status about the blocks (runned, running, remaining)\n\n    """"""\n    progress_file = app.config.get(\'progress_file\')\n\n    if not os.path.exists(progress_file):\n        return None\n\n    with open(progress_file, ""rb"") as f:\n        state = pickle.load(f)\n\n    return state\n\n\ndef analyze_download_params(block, variant):\n    output_dir = app.config.get(\'output_dir\')\n    output_name = ""experiment""\n\n    if block is not None:\n        output_dir = os.path.join(output_dir, block)\n        output_name = f""{output_name}-{block}""\n\n        if variant is not None:\n            vs = [x for x in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, x))]\n            for v in vs:\n                if v.startswith(variant):\n                    output_dir = os.path.join(output_dir, v)\n                    output_name = f""{output_name}-{variant}""\n                    break\n\n    return output_dir, output_name\n\n\n@app.route(\'/download\', methods=[\'GET\'])\n@app.route(\'/download/<block>\', methods=[\'GET\'])\n@app.route(\'/download/<block>/<variant>\', methods=[\'GET\'])\ndef download(block=None, variant=None):\n    """"""\n    Downloads models + logs stored in the filesystem of the Orchestrator\n    """"""\n    output_dir, output_name = analyze_download_params(block, variant)\n\n    with tempfile.TemporaryDirectory() as d:\n        shutil.make_archive(os.path.join(d, ""output""), ""tar"", output_dir)\n        return send_file(os.path.join(d, ""output.tar""),\n                         as_attachment=True,\n                         attachment_filename=f""{output_name}.tar"")\n\n\n@app.route(\'/download_logs\', methods=[\'GET\'])\n@app.route(\'/download_logs/<block>\', methods=[\'GET\'])\n@app.route(\'/download_logs/<block>/<variant>\', methods=[\'GET\'])\ndef download_logs(block=None, variant=None):\n    """"""\n    Downloads all artifacts but the models stored in the filesystem\n    of the Orchestrator machine\n    """"""\n    output_dir, output_name = analyze_download_params(block, variant)\n\n    with tempfile.TemporaryDirectory() as d1, tempfile.TemporaryDirectory() as d2:\n        subprocess.check_call([\'rsync\', \'-av\', \'--exclude\', \'*.pt\', output_dir, d2])\n        shutil.make_archive(os.path.join(d1, ""output_no_models""), ""tar"", d2)\n        return send_file(os.path.join(d1, ""output_no_models.tar""),\n                         as_attachment=True,\n                         attachment_filename=f""{output_name}-logs.tar"")\n\n\n@app.route(\'/stream\')\ndef stream():\n    if app.config[\'output_log\'] is None:\n        return None\n\n    def generate():\n        p = 0\n        while True:\n            with open(app.config[\'output_log\'], \'r\') as f:\n                f.seek(p)\n                content = f.read()\n                p = f.tell()\n                # Attention: if this content is used as direct HTML\n                # it needs to be escaped. For example, using\n                # html.escape(content)\n                # Not escaping could have undesired effects, like\n                # commenting out the content (if case an \'<\' is present)\n                yield content\n                sleep(1)\n\n    return app.response_class(generate(), mimetype=\'text/event-stream\')\n\n\n@app.route(\'/state\')\ndef state():\n    state = load_state()\n    return jsonify(state.toJSON())\n\n\n@app.route(\'/console_log\')\ndef console_log():\n    """"""\n    View console with the logs\n    """"""\n    return render_template(\'console.html\')\n\n\n@app.route(\'/\')\ndef index():\n    """"""\n    Main endpoint. Works with the `index.html` template.\n    """"""\n    state = load_state()\n    tensorboard_url = app.config.get(\'tensorboard_url\')\n    output_log = app.config.get(\'output_log\')\n    output_dir = app.config.get(\'output_dir\')\n\n    return render_template(\'index.html\', state=state, tensorboard_url=tensorboard_url,\n                           output_log=output_log, output_dir=output_dir)\n'"
flambe/logging/handler/__init__.py,0,b''
flambe/logging/handler/contextual_file.py,0,"b'import os\nimport logging\n\n\nclass ContextualFileHandler(logging.FileHandler):\n    """"""Uses the record `current_log_dir` value to customize file path\n\n    Uses the LogRecord object\'s current_log_dir value to dynamically\n    determine a path for the output file name. Functions\n    the same as parent `logging.FileHandler` but always writes to\n    the file given by `current_log_dir + canonical_name`.\n\n    Parameters\n    ----------\n    canonical_name : str\n        Common name for each file\n    mode : str\n        See built-in `open` description of `mode`\n    encoding : type\n        See built-in `open` description of `encoding`\n\n    Attributes\n    ----------\n    current_log_dir : str\n        Most recently used prefix in an incoming `LogRecord`\n    canonical_name : str\n        Common name for each file\n    mode : str\n        See built-in `open` description of `mode`\n    delay : type\n        If true will delay opening of file until first use\n    stream : type\n        Currently open file stream for writing logs - should match\n        the file indicated by `base_path` + `current_prefix` +\n        `canonical_name`\n    encoding\n        See built-in `open` description of `encoding`\n\n    """"""\n\n    def __init__(self,  # type: ignore\n                 canonical_name: str,\n                 mode: str = \'a\',\n                 encoding=None) -> None:\n        self.current_log_dir = """"\n        self.canonical_name = canonical_name\n        self.mode = mode\n        self.encoding = encoding\n        self.delay = True\n        logging.Handler.__init__(self)\n        self.stream = None  # type: ignore\n\n    @property\n    def baseFilename(self) -> str:  # type: ignore\n        """"""Output filename; Override parent property to use prefix""""""\n        return os.path.join(self.current_log_dir, self.canonical_name)\n\n    def emit(self, record: logging.LogRecord) -> None:\n        """"""Emit a record\n\n        If the stream is invalidated by a new record `prefix` value\n        it will be closed and set to `None` before calling the super\n        `emit` which will handle opening a new stream to `baseFilename`\n\n        Parameters\n        ----------\n        record : logging.LogRecord\n            Record to be saved at `_console_log_dir`\n\n        Returns\n        -------\n        None\n\n        """"""\n        if self.current_log_dir != record._console_log_dir:  # type: ignore\n            self.current_log_dir = record._console_log_dir  # type: ignore\n            # Close current stream if it exists\n            self.close()\n            # Ensure stream is `None` in case close failed\n            self.stream = None  # type: ignore\n            new_path = self.current_log_dir\n            if not os.path.isdir(new_path):\n                os.makedirs(new_path)\n        super().emit(record)\n'"
flambe/logging/handler/tensorboard.py,0,"b'import logging\nfrom typing import Any, Dict\n\nfrom tensorboardX import SummaryWriter\n\nfrom flambe.logging.datatypes import ScalarT, ScalarsT, HistogramT, TextT, \\\n    ImageT, EmbeddingT, PRCurveT, GraphT, DataLoggingFilter\n\n\nclass TensorboardXHandler(logging.Handler):\n    """"""Implements Tensorboard message logging via TensorboardX\n\n    Parameters\n    ----------\n    writer : SummaryWriter\n        Initialized TensorboardX Writer\n    *args : Any\n        Other positional args for `logging.Handler`\n    **kwargs : Any\n        Other kwargs for `logging.Handler`\n\n    Attributes\n    ----------\n    writer : SummaryWriter\n        Initialized TensorboardX Writer\n\n    """"""\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n\n        # TODO remove dont_include when we add support for saving graph\n        self.addFilter(DataLoggingFilter(default=False, dont_include=(GraphT,)))\n\n        self.writers: Dict[str, SummaryWriter] = {}\n\n    def emit(self, record: logging.LogRecord) -> None:\n        """"""Save to tensorboard logging directory\n\n        Overrides `logging.Handler.emit`\n\n        Parameters\n        ----------\n        record : logging.LogRecord\n            LogRecord with data relevant to Tensorboard\n\n        Returns\n        -------\n        None\n\n        """"""\n        # Handler relies on access to raw objects which flambe logging\n        # provides\n        if not hasattr(record, ""raw_msg_obj""):\n            return\n        message = record.raw_msg_obj  # type: ignore\n        # Check for a log directory from the logging context\n        # This will be prepended to the final tag before saving to\n        # Tensorboard\n\n        if hasattr(record, ""_tf_log_dir""):\n            log_dir = record._tf_log_dir  # type: ignore\n            if log_dir in self.writers:\n                writer = self.writers[log_dir]\n            else:\n                writer = SummaryWriter(log_dir=log_dir)\n                hparams = getattr(record, ""_tf_hparams"", dict())\n                if len(hparams):\n                    writer.add_hparams_start(hparams=hparams)\n\n                self.writers[log_dir] = writer\n        else:\n            return\n        # Datatypes with a standard `tag` field\n        if isinstance(message, (ScalarT, HistogramT, TextT, EmbeddingT, ImageT, PRCurveT)):\n            kwargs = message._replace(tag=message.tag)._asdict()\n            fn = {\n                ScalarT: writer.add_scalar,\n                HistogramT: writer.add_histogram,\n                TextT: writer.add_text,\n                EmbeddingT: writer.add_embedding,\n                ImageT: writer.add_image,\n                PRCurveT: writer.add_pr_curve\n            }\n            fn[message.__class__](**kwargs)\n        # Datatypes with a special tag field\n        elif isinstance(message, ScalarsT):\n            kwargs = message._replace(main_tag=message.main_tag)._asdict()\n            writer.add_scalars(**kwargs)\n        # Datatypes without a tag field\n        elif isinstance(message, GraphT):\n            kwargs = message._asdict()\n            for k, v in kwargs[\'kwargs\']:\n                kwargs[k] = v\n            del kwargs[\'kwargs\']\n            writer.add_model(**kwargs)\n        writer.file_writer.flush()\n\n    def close(self) -> None:\n        """"""Teardown writers and teardown super\n\n        Returns\n        -------\n        None\n\n        """"""\n        # Use built-in writer `close` method to flush and close\n        for _, w in self.writers.items():\n            w.add_hparams_end()\n            w.close()\n\n        super().close()\n\n    def flush(self) -> None:\n        """"""Call flush on the Tensorboard writer\n\n        Returns\n        -------\n        None\n\n        """"""\n        # No public `flush` method is available on the writer, so\n        # copy the flushing logic from the TensorboardX `SummaryWriter`\n\n        for _, w in self.writers.items():\n            if w.file_writer:\n                w.file_writer.flush()\n\n        for _, w in self.writers.items():\n            for path, writer in w.all_writers.items():\n                writer.flush()\n\n        super().flush()\n'"
flambe/metric/dev/__init__.py,0,b''
flambe/metric/dev/accuracy.py,3,"b'import torch\n\nfrom flambe.metric.metric import AverageableMetric\n\n\nclass Accuracy(AverageableMetric):\n\n    def compute(self, pred: torch.Tensor, target: torch.Tensor) \\\n            -> torch.Tensor:\n        """"""Computes the loss.\n\n        Parameters\n        ----------\n        pred: Tensor\n            input logits of shape (B x N)\n        target: LontTensor\n            target tensor of shape (B) or (B x N)\n\n        Returns\n        -------\n        accuracy: torch.Tensor\n            single label accuracy, of shape (B)\n\n        """"""\n        # If 2-dimensional, select the highest score in each row\n        if len(target.size()) == 2:\n            target = target.argmax(dim=1)\n\n        acc = (pred.argmax(dim=1) == target)\n        return acc.float().mean()\n'"
flambe/metric/dev/auc.py,15,"b'import torch\nimport sklearn.metrics\nimport numpy as np\n\nfrom flambe.metric.metric import Metric\n\n\ndef one_hot(indices: torch.Tensor, width: int) -> torch.Tensor:\n    """"""Converts a list of ints into 1-hot format.\n\n    Parameters\n    ----------\n    indices: torch.Tensor\n        the indices to be converted\n    width: int\n        the width of the 1-hot encoding (= the maximal index value)\n\n    Returns\n    -------\n    torch.Tensor\n        A one-hot representation of the input indices.\n    """"""\n    indices = indices.squeeze()\n    return torch.zeros(indices.size(0), width).scatter_(1, indices.unsqueeze(1), 1.)\n\n\nclass AUC(Metric):\n\n    def __init__(self, max_fpr=1.0):\n        """"""Initialize the AUC metric.\n\n        Parameters\n        ----------\n        max_fpr : float, optional\n            Maximum false positive rate to compute the area under\n        """"""\n        self.max_fpr = max_fpr\n\n    def __str__(self) -> str:\n        """"""Return the name of the Metric (for use in logging).""""""\n        return f\'{self.__class__.__name__}@{self.max_fpr}\'\n\n    def compute(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        """"""Compute AUC at the given max false positive rate.\n\n        Parameters\n        ----------\n        pred : torch.Tensor\n            The model predictions of shape numsamples\n        target : torch.Tensor\n            The binary targets of shape numsamples\n\n        Returns\n        -------\n        torch.Tensor\n            The computed AUC\n\n        """"""\n        scores = np.array(pred)\n        targets = np.array(target)\n\n        # Case when number of elements added are 0\n        if not scores.size or not targets.size:\n            return torch.tensor(0.5)\n\n        fpr, tpr, _ = sklearn.metrics.roc_curve(targets, scores, sample_weight=None)\n\n        # Compute the area under the curve using trapezoidal rule\n        max_index = np.searchsorted(fpr, [self.max_fpr], side=\'right\').item()\n\n        # Ensure we integrate up to max_fpr\n        fpr, tpr = fpr.tolist(), tpr.tolist()\n        fpr, tpr = fpr[:max_index], tpr[:max_index]\n        fpr.append(self.max_fpr)\n        tpr.append(max(tpr))\n\n        area = np.trapz(tpr, fpr)\n\n        return torch.tensor(area / self.max_fpr).float()\n\n\nclass MultiClassAUC(AUC):\n    """"""N-Ary (Multiclass) AUC for k-way classification""""""\n\n    def compute(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        """"""Compute multiclass AUC at the given max false positive rate.\n\n        Parameters\n        ----------\n        pred : torch.Tensor\n            The model predictions of shape numsamples x numclasses\n        target : torch.Tensor\n            The binary targets of shape:\n             - numsamples. In this case the elements index into the\n               different classes\n             - numsamples x numclasses. This implementation only\n               considers the indices of the max values as positive\n               labels\n\n        Returns\n        -------\n        torch.Tensor\n            The computed AUC\n        """"""\n        if pred.numel() == target.numel() == 0:\n            return 0.5 * pred.new_ones(size=(1, 1)).squeeze()\n        num_samples, num_classes = pred.shape\n        pred_reshaped = pred.reshape(-1)\n        if target.numel() == num_samples:\n            # target consists of indices\n            target = one_hot(target, num_classes)\n        else:\n            # reconstructing targets to make sure that only\n            # one target is provided by taking the argmax along an axis\n            target = torch.argmax(target, dim=1)\n            target = one_hot(target, num_classes)\n        target_reshaped = target.reshape(-1)\n        if pred_reshaped.size() != target_reshaped.size():\n            raise RuntimeError(\n                \'Predictions could not be flattened for AUC computation. \'\n                \'Ensure all batches are the same size \'\n                \'(hint: try setting `drop_last = True` in Sampler).\')\n\n        return super().compute(pred_reshaped, target_reshaped)\n'"
flambe/metric/dev/binary.py,30,"b'from abc import abstractmethod\nimport torch\n\nfrom flambe.metric.metric import Metric\n\n\nclass BinaryMetric(Metric):\n\n    def __init__(self, threshold: float = 0.5) -> None:\n        """"""Initialize the Binary metric.\n\n        Parameters\n        ---------\n        threshold: float\n            Given a probability p of belonging to Positive class,\n            p < threshold will be considered tagged as Negative by\n            the classifier when computing the metric.\n            Defaults to 0.5\n        """"""\n        self.threshold = threshold\n\n    def __str__(self) -> str:\n        """"""Return the name of the Metric (for use in logging).""""""\n        return f\'{self.__class__.__name__}@{self.threshold}\'\n\n    def compute(self, pred: torch.Tensor, target: torch.Tensor) \\\n            -> torch.Tensor:\n        """"""Compute the metric given predictions and targets\n\n        Parameters\n        ----------\n        pred : Tensor\n            The model predictions\n        target : Tensor\n            The binary targets\n\n        Returns\n        -------\n        float\n            The computed binary metric\n\n        """"""\n        pred = pred.squeeze()\n        target = target.squeeze().bool()\n        pred = (pred > self.threshold)\n\n        return self.compute_binary(pred, target)\n\n    @abstractmethod\n    def compute_binary(self,\n                       pred: torch.Tensor,\n                       target: torch.Tensor) -> torch.Tensor:\n        """"""Compute a binary-input metric.\n\n        Parameters\n        ---------\n        pred: torch.Tensor\n            Predictions made by the model. It should be a probability\n            0 <= p <= 1 for each sample, 1 being the positive class.\n        target: torch.Tensor\n            Ground truth. Each label should be either 0 or 1.\n\n        Returns\n        ------\n        torch.float\n            The computed binary metric\n\n        """"""\n        pass\n\n\nclass BinaryAccuracy(BinaryMetric):\n    """"""Compute binary accuracy.\n\n    ```\n    |True positives + True negatives| / N\n    ```\n\n    """"""\n    def compute_binary(self,\n                       pred: torch.Tensor,\n                       target: torch.Tensor) -> torch.Tensor:\n        """"""Compute binary accuracy.\n\n        Parameters\n        ---------\n        pred: torch.Tensor\n            Predictions made by the model. It should be a probability\n            0 <= p <= 1 for each sample, 1 being the positive class.\n        target: torch.Tensor\n            Ground truth. Each label should be either 0 or 1.\n\n        Returns\n        ------\n        torch.float\n            The computed binary metric\n\n        """"""\n        acc = pred == target\n        N = target.size()[0] if target.dim() > 0 else 1\n\n        if N == 0:\n            return torch.tensor(0)\n\n        return acc.sum().float() / N\n\n\nclass BinaryPrecision(BinaryMetric):\n    """"""Compute Binary Precision.\n\n    An example is considered negative when its score is below the\n    specified threshold. Binary precition is computed as follows:\n\n    ```\n    |True positives| / |True Positives| + |False Positives|\n    ```\n\n    """"""\n\n    def __init__(self, threshold: float = 0.5, positive_label: int = 1) -> None:\n        """"""Initialize the Binary metric.\n\n        Parameters\n        ---------\n        threshold: float\n            Given a probability p of belonging to Positive class,\n            p < threshold will be considered tagged as Negative by\n            the classifier when computing the metric.\n            Defaults to 0.5\n        positive_label: int\n            Specify if the positive class should be 1 or 0.\n            Defaults to 1.\n\n        """"""\n        if positive_label not in [0, 1]:\n            raise ValueError(""positive_label should be either 0 or 1"")\n\n        super().__init__(threshold)\n        self.positive_label = positive_label\n\n    def compute_binary(self,\n                       pred: torch.Tensor,\n                       target: torch.Tensor) -> torch.Tensor:\n        """"""Compute binary precision.\n\n        Parameters\n        ---------\n        pred: torch.Tensor\n            Predictions made by the model. It should be a probability\n            0 <= p <= 1 for each sample, 1 being the positive class.\n        target: torch.Tensor\n            Ground truth. Each label should be either 0 or 1.\n\n        Returns\n        ------\n        torch.float\n            The computed binary metric\n\n        """"""\n        if self.positive_label == 0:\n            pred = ~pred\n            target = ~target\n\n        acc = pred == target\n        true_p = acc & target\n\n        if pred.sum() == 0:\n            metric = torch.tensor(0)\n        else:\n            # Again, weird typing from pytorch\n            # check periodically for a fix\n            metric = (true_p.sum().float() / pred.sum().float())\n\n        return metric\n\n    def __str__(self) -> str:\n        """"""Return the name of the Metric (for use in logging).""""""\n        invert_label = ""Negative"" if self.positive_label == 0 else ""Positive""\n        return f""{invert_label}{self.__class__.__name__}""\n\n\nclass BinaryRecall(BinaryMetric):\n    """"""Compute binary recall.\n\n    An example is considered negative when its score is below the\n    specified threshold. Binary precition is computed as follows:\n\n    ```\n    |True positives| / |True Positives| + |False Negatives|\n    ```\n\n    """"""\n    def __init__(self, threshold: float = 0.5, positive_label: int = 1) -> None:\n        """"""Initialize the Binary metric.\n\n        Parameters\n        ---------\n        threshold: float\n            Given a probability p of belonging to Positive class,\n            p < threshold will be considered tagged as Negative by\n            the classifier when computing the metric.\n            Defaults to 0.5\n        positive_label: int\n            Specify if the positive class should be 1 or 0.\n            Defaults to 1.\n\n        """"""\n        if positive_label not in [0, 1]:\n            raise ValueError(""positive_label should be either 0 or 1"")\n\n        super().__init__(threshold)\n        self.positive_label = positive_label\n\n    def compute_binary(self,\n                       pred: torch.Tensor,\n                       target: torch.Tensor) -> torch.Tensor:\n        """"""Compute binary recall.\n\n        Parameters\n        ---------\n        pred: torch.Tensor\n            Predictions made by the model. It should be a probability\n            0 <= p <= 1 for each sample, 1 being the positive class.\n        target: torch.Tensor\n            Ground truth. Each label should be either 0 or 1.\n\n        Returns\n        ------\n        torch.float\n            The computed binary metric\n\n        """"""\n        if self.positive_label == 0:\n            pred = ~pred\n            target = ~target\n\n        acc = pred == target\n        true_p = acc & target\n\n        if target.sum() == 0:\n            metric = torch.tensor(0)\n        else:\n            metric = true_p.sum().float() / target.sum().float()\n\n        return metric\n\n    def __str__(self) -> str:\n        """"""Return the name of the Metric (for use in logging).""""""\n        invert_label = ""Negative"" if self.positive_label == 0 else ""Positive""\n        return f""{invert_label}{self.__class__.__name__}""\n\n\nclass F1(BinaryMetric):\n\n    def __init__(self,\n                 threshold: float = 0.5,\n                 positive_label: int = 1,\n                 eps: float = 1e-8) -> None:\n        """"""\n        Parameters\n        ---------\n        threshold: float\n            Given a probability p of belonging to Positive class,\n            p < threshold will be considered tagged as Negative by\n            the classifier when computing the metric.\n            Defaults to 0.5\n        positive_label: int\n            Specify if the positive class should be 1 or 0.\n            Defaults to 1.\n        eps: float\n            Float to sum to the denominator, so that we avoid division\n            by zero. Defaults to 1e-8.\n\n        """"""\n\n        super().__init__(threshold)\n        self.recall = BinaryRecall(threshold, positive_label)\n        self.precision = BinaryPrecision(threshold, positive_label)\n        self.eps = eps\n\n    def compute_binary(self,\n                       pred: torch.Tensor,\n                       target: torch.Tensor) -> torch.Tensor:\n        """"""Compute F1. Score, the harmonic mean between precision and\n        recall.\n\n        Parameters\n        ---------\n        pred: torch.Tensor\n            Predictions made by the model. It should be a probability\n            0 <= p <= 1 for each sample, 1 being the positive class.\n        target: torch.Tensor\n            Ground truth. Each label should be either 0 or 1.\n\n        Returns\n        ------\n        torch.float\n            The computed binary metric\n\n        """"""\n        recall = self.recall.compute_binary(pred, target)\n        precision = self.precision.compute_binary(pred, target)\n\n        return 2 * precision * recall / (precision + recall + self.eps)\n'"
flambe/metric/dev/bpc.py,6,"b'from typing import Dict\n\nimport numpy as np\nimport torch\n\nfrom flambe.metric.dev.perplexity import Perplexity\n\n\nclass BPC(Perplexity):\n    """"""Bits per character. Computed as log_2(perplexity)\n\n    Inherits from Perplexity to share aggregate functionality.\n    """"""\n\n    def compute(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        """"""Compute the bits per character given the input and target.\n\n        Parameters\n        ----------\n        pred: torch.Tensor\n            input logits of shape (B x N)\n        target: torch.LontTensor\n            target tensor of shape (B)\n\n        Returns\n        -------\n        torch.float\n            Output perplexity\n\n        """"""\n        entropy = self.entropy(pred, target).mean()\n        return torch.log2(torch.exp(entropy))\n\n    def finalize(self, state: Dict) -> float:\n        """"""Finalizes the metric computation\n\n        Parameters\n        ----------\n        state: dict\n            the metric state\n\n        Returns\n        -------\n        float\n            The final score.\n\n        """"""\n        if not state or state[\'sample_count\'] == 0:\n            # call on empty state\n            return np.NaN\n        return torch.log2(torch.exp(state[\'accumulated_score\'] / state[\'sample_count\'])).item()\n'"
flambe/metric/dev/perplexity.py,7,"b'from typing import Dict\nimport numpy as np\nimport torch\n\nfrom flambe.metric import Metric\n\n\nclass Perplexity(Metric):\n    """"""Token level perplexity, computed a exp(cross_entropy).""""""\n\n    def __init__(self):\n        """"""Perplexity, computed as CrossEntropy""""""\n        self.entropy = torch.nn.CrossEntropyLoss(reduction=\'none\')\n\n    def compute(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        """"""Compute the preplexity given the input and target.\n\n        Parameters\n        ----------\n        pred: torch.Tensor\n            input logits of shape (B x N)\n        target: torch.LontTensor\n            target tensor of shape (B)\n\n        Returns\n        -------\n        torch.float\n            Output perplexity\n\n        """"""\n        entropy = self.entropy(pred, target).mean()\n        return torch.exp(entropy)\n\n    def aggregate(self, state: dict, *args, **kwargs) -> Dict:\n        """"""Aggregates by only storing entropy per sample\n\n        Parameters\n        ----------\n        state: dict\n            the metric state\n        args: the pred, target tuple\n\n        Returns\n        -------\n        dict\n            the state dict\n\n        """"""\n        pred, target = args\n        if not state:\n            state[\'accumulated_score\'] = 0.\n            state[\'sample_count\'] = 0\n        logits = self.entropy(pred, target).cpu().detach()\n        state[\'accumulated_score\'] += logits.sum()\n        state[\'sample_count\'] += logits.size(0)\n        return state\n\n    def finalize(self, state: Dict) -> float:\n        """"""Finalizes the metric computation\n\n        Parameters\n        ----------\n        state: dict\n            the metric state\n\n        Returns\n        -------\n        float\n            The final score.\n\n        """"""\n        if not state or state[\'sample_count\'] == 0:\n            # call on empty state\n            return np.NaN\n        return torch.exp(state[\'accumulated_score\'] / state[\'sample_count\']).item()\n'"
flambe/metric/dev/recall.py,5,"b'import torch\n\nfrom flambe.metric.metric import AverageableMetric\n\n\nclass Recall(AverageableMetric):\n\n    def __init__(self, top_k: int = 1) -> None:\n        """"""Initialize the Recall metric.\n\n        Parameters\n        ---------\n        top_k: int\n            used to compute recall@k. For k = 1, this becomes\n            accuracy\n        """"""\n        self.top_k = top_k\n\n    def __str__(self) -> str:\n        """"""Return the name of the Metric (for use in logging).""""""\n        return f\'{self.__class__.__name__}@{self.top_k}\'\n\n    def compute(self, pred: torch.Tensor, target: torch.Tensor) \\\n            -> torch.Tensor:\n        """"""Computes the recall @ k.\n\n        Parameters\n        ----------\n        pred: Tensor\n            input logits of shape (B x N)\n        target: LongTensor\n            target tensor of shape (B) or (B x N)\n\n        Returns\n        -------\n        recall: torch.Tensor\n            single label recall, of shape (B)\n\n        """"""\n        # If 2-dimensional, select the highest score in each row\n        if len(target.size()) == 2:\n            target = target.argmax(dim=1)\n\n        ranked_scores = torch.argsort(pred, dim=1)[:, -self.top_k:]\n        recalled = torch.sum((target.unsqueeze(1) == ranked_scores).float(), dim=1)\n        return recalled.mean()\n'"
flambe/metric/loss/__init__.py,0,b''
flambe/metric/loss/cross_entropy.py,9,"b'from typing import Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom flambe.metric.metric import Metric\n\n\nclass MultiLabelCrossEntropy(Metric):\n\n    def __init__(self,\n                 weight: Optional[torch.Tensor] = None,\n                 ignore_index: Optional[int] = None,\n                 reduction: str = \'mean\') -> None:\n        """"""Initialize the MultiLabelCrossEntropy.\n\n        Parameters\n        ----------\n        weight : Optional[torch.Tensor]\n            A manual rescaling weight given to each class.\n            If given, has to be a Tensor of size N, where N is the\n            number of classes.\n        ignore_index : Optional[int], optional\n            Specifies a target value that is ignored and does not\n            contribute to the input gradient. When size_average is\n            True, the loss is averaged over non-ignored targets.\n        reduction : str, optional\n            Specifies the reduction to apply to the output:\n            \'none\' | \'mean\' | \'sum\'.\n            \'none\': no reduction will be applied,\n            \'mean\': the output will be averaged\n            \'sum\': the output will be summed.\n        """"""\n        self.weight = weight\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n\n    def __str__(self) -> str:\n        """"""Return the name of the Metric (for use in logging).""""""\n        return \'MultiLabelCrossEntropy\' if self.weight is None \\\n            else \'WeightedMultiLabelCrossEntropy\'\n\n    def compute(self, pred: torch.Tensor, target: torch.Tensor) \\\n            -> torch.Tensor:\n        """"""Computes the multilabel cross entropy loss.\n\n        Parameters\n        ----------\n        pred: torch.Tensor\n            input logits of shape (B x N)\n        target: torch.LontTensor\n            target tensor of shape (B x N)\n\n        Returns\n        -------\n        loss: torch.Tensor\n            Multi label cross-entropy loss, of shape (B)\n\n        """"""\n        if self.ignore_index is not None:\n            target[:, self.ignore_index] = 0\n\n        if self.weight is None:\n            self.weight = torch.ones(pred.size(1)).to(pred)\n\n        norm_target = F.normalize(target.float(), p=1, dim=1)\n        loss = - (self.weight * norm_target *\n                  F.log_softmax(pred, dim=1)).sum(dim=1)\n\n        if self.reduction == \'mean\':\n            loss = loss.mean()\n        elif self.reduction == \'sum\':\n            loss = loss.sum()\n        elif self.reduction is not None:\n            raise ValueError(""Unknown reduction: {self.reduction}"")\n\n        return loss\n'"
flambe/metric/loss/nll_loss.py,9,"b'from typing import Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom flambe.metric.metric import Metric\n\n\nclass MultiLabelNLLLoss(Metric):\n\n    def __init__(self,\n                 weight: Optional[torch.Tensor] = None,\n                 ignore_index: Optional[int] = None,\n                 reduction: str = \'mean\') -> None:\n        """"""Initialize the MultiLabelNLLLoss.\n\n        Parameters\n        ----------\n        weight : Optional[torch.Tensor]\n            A manual rescaling weight given to each class.\n            If given, has to be a Tensor of size N, where N is the\n            number of classes.\n        ignore_index : Optional[int], optional\n            Specifies a target value that is ignored and does not\n            contribute to the input gradient. When size_average is\n            True, the loss is averaged over non-ignored targets.\n        reduction : str, optional\n            Specifies the reduction to apply to the output:\n            \'none\' | \'mean\' | \'sum\'.\n            \'none\': no reduction will be applied,\n            \'mean\': the output will be averaged\n            \'sum\': the output will be summed.\\\n        """"""\n        super().__init__()\n        self.weight = weight\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n\n    def __str__(self) -> str:\n        """"""Return the name of the Metric (for use in logging).""""""\n        return \'MultiLabelNLLLoss\' if self.weight is None \\\n            else \'WeightedMultiLabelNLLLoss\'\n\n    def compute(self, pred: torch.Tensor, target: torch.Tensor) \\\n            -> torch.Tensor:\n        """"""Computes the Negative log likelihood loss for multilabel.\n\n        Parameters\n        ----------\n        pred: torch.Tensor\n            input logits of shape (B x N)\n        target: torch.LontTensor\n            target tensor of shape (B x N)\n\n        Returns\n        -------\n        loss: torch.float\n            Multi label negative log likelihood loss, of shape (B)\n\n        """"""\n        if self.ignore_index is not None:\n            target[:, self.ignore_index] = 0\n\n        if self.weight is None:\n            self.weight = torch.ones(pred.size(1)).to(pred)\n\n        norm_target = F.normalize(target.float(), p=1, dim=1)\n        loss = - (self.weight * norm_target * pred).sum(dim=1)\n\n        if self.reduction == \'mean\':\n            loss = loss.mean()\n        elif self.reduction == \'sum\':\n            loss = loss.sum()\n        elif self.reduction is not None:\n            raise ValueError(""Unknown reduction: {self.reduction}"")\n\n        return loss\n'"
flambe/nlp/classification/__init__.py,0,"b""# type: ignore[attr-define]\n\nfrom flambe.nlp.classification.datasets import SSTDataset, TRECDataset, NewsGroupDataset\nfrom flambe.nlp.classification.model import TextClassifier\n\n\n__all__ = ['TextClassifier', 'SSTDataset', 'TRECDataset', 'NewsGroupDataset']\n"""
flambe/nlp/classification/datasets.py,0,"b'from typing import List, Tuple, Optional, Dict, Union\n\nfrom flambe.dataset import TabularDataset\nfrom flambe.field import Field\n\n\nclass SSTDataset(TabularDataset):\n    """"""The official SST-1 dataset.""""""\n\n    URL = ""https://raw.githubusercontent.com/harvardnlp/sent-conv-torch/master/data/""\n\n    def __init__(self,\n                 binary: bool = True,\n                 phrases: bool = False,\n                 cache: bool = True,\n                 transform: Dict[str, Union[Field, Dict]] = None) -> None:\n        """"""Initialize the SSTDataset builtin.\n\n        Parameters\n        ----------\n        binary: bool\n            Set to true to train and evaluate in binary mode.\n            Defaults to True.\n        phrases: bool\n            Set to true to train on phrases. Defaults to False.\n\n        """"""\n        binary_str = \'binary\' if binary else \'fine\'\n        phrases_str = \'.phrases\' if phrases else \'\'\n\n        train_path = self.URL + f""stsa.{binary_str}{phrases_str}.train""\n        dev_path = self.URL + f""stsa.{binary_str}.dev""\n        test_path = self.URL + f""stsa.{binary_str}.test""\n\n        train, _ = self._load_file(train_path, sep=\'\\t\', header=None)\n        val, _ = self._load_file(dev_path, sep=\'\\t\', header=None)\n        test, _ = self._load_file(test_path, sep=\'\\t\', header=None)\n\n        named_cols = [\'text\', \'label\']\n        super().__init__(train, val, test, cache, named_cols, transform)\n\n    @classmethod\n    def _load_file(cls,\n                   path: str,\n                   sep: Optional[str] = \'\\t\',\n                   header: Optional[str] = None,\n                   columns: Optional[Union[List[str], List[int]]] = None,\n                   encoding: Optional[str] = \'utf-8\') -> Tuple[List[Tuple], Optional[List[str]]]:\n        """"""Load data from the given path.""""""\n        data, named_cols = super()._load_file(path, sep, header, columns)\n        return [("""".join(d[0][2:]), d[0][0]) for d in data], named_cols\n\n\nclass TRECDataset(TabularDataset):\n    """"""The official TREC dataset.""""""\n\n    URL = ""https://raw.githubusercontent.com/harvardnlp/sent-conv-torch/master/data/""\n\n    def __init__(self, cache: bool = True,\n                 transform: Dict[str, Union[Field, Dict]] = None) -> None:\n        """"""Initialize the SSTDataset builtin.""""""\n        train_path = self.URL + ""TREC.train.all""\n        test_path = self.URL + ""TREC.test.all""\n\n        train, _ = self._load_file(train_path, sep=\'\\t\', header=None, encoding=\'latin-1\')\n        test, _ = self._load_file(test_path, sep=\'\\t\', header=None, encoding=\'latin-1\')\n\n        named_cols = [\'text\', \'label\']\n        super().__init__(\n            train=train,\n            val=None,\n            test=test,\n            cache=cache,\n            named_columns=named_cols,\n            transform=transform\n        )\n\n    @classmethod\n    def _load_file(cls,\n                   path: str,\n                   sep: Optional[str] = \'\\t\',\n                   header: Optional[str] = None,\n                   columns: Optional[Union[List[str], List[int]]] = None,\n                   encoding: Optional[str] = \'latin-1\') -> Tuple[List[Tuple], Optional[List[str]]]:\n        """"""Load data from the given path.""""""\n        data, named_cols = super()._load_file(path, sep, header, columns, encoding)\n        return [("""".join(d[0][2:]), d[0][0]) for d in data], named_cols\n\n\nclass NewsGroupDataset(TabularDataset):\n    """"""The official 20 news group dataset.""""""\n\n    def __init__(self,\n                 cache: bool = False,\n                 transform: Dict[str, Union[Field, Dict]] = None) -> None:\n        """"""Initialize the NewsGroupDataset builtin.""""""\n        try:\n            from sklearn.datasets import fetch_20newsgroups\n        except ImportError:\n            raise ImportError(""Install sklearn to use the NewsGroupDataset"")\n\n        train = fetch_20newsgroups(subset=\'train\')\n        test = fetch_20newsgroups(subset=\'test\')\n\n        train = [(\' \'.join(d.split()), str(t)) for d, t in zip(train[\'data\'], train[\'target\'])]\n        test = [(\' \'.join(d.split()), str(t)) for d, t in zip(test[\'data\'], test[\'target\'])]\n\n        named_cols = [\'text\', \'label\']\n        super().__init__(\n            train=train,\n            val=None,\n            test=test,\n            cache=cache,\n            named_columns=named_cols,\n            transform=transform\n        )\n'"
flambe/nlp/classification/model.py,1,"b'# type: ignore[override]\n\nfrom typing import Optional, Tuple, Union\n\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom flambe.nn import Embedder, Module\n\n\nclass TextClassifier(Module):\n    """"""Implements a standard classifier.\n\n    The classifier is composed of an encoder module, followed by\n    a fully connected output layer, with a dropout layer in between.\n\n    Attributes\n    ----------\n    embedder: Embedder\n        The embedder layer\n    output_layer : Module\n        The output layer, yields a probability distribution over targets\n    drop: nn.Dropout\n        the dropout layer\n    loss: Metric\n        the loss function to optimize the model with\n    metric: Metric\n        the dev metric to evaluate the model on\n\n    """"""\n\n    def __init__(self,\n                 embedder: Embedder,\n                 output_layer: Module,\n                 dropout: float = 0) -> None:\n        """"""Initialize the TextClassifier model.\n\n        Parameters\n        ----------\n        embedder: Embedder\n            The embedder layer\n        output_layer : Module\n            The output layer, yields a probability distribution\n        dropout : float, optional\n            Amount of dropout to include between layers (defaults to 0)\n\n        """"""\n        super().__init__()\n\n        self.embedder = embedder\n        self.output_layer = output_layer\n\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self,\n                data: Tensor,\n                target: Optional[Tensor] = None) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n        """"""Run a forward pass through the network.\n\n        Parameters\n        ----------\n        data: Tensor\n            The input data\n        target: Tensor, optional\n            The input targets, optional\n\n        Returns\n        -------\n        Union[Tensor, Tuple[Tensor, Tensor]\n            The output predictions, and optionally the targets\n\n        """"""\n        outputs = self.embedder(data)\n        if isinstance(outputs, tuple):\n            encoding = outputs[0]\n        else:\n            encoding = outputs\n\n        pred = self.output_layer(self.drop(encoding))\n        return (pred, target) if target is not None else pred\n'"
flambe/nlp/fewshot/__init__.py,0,"b""# type: ignore[attr-define]\n\nfrom flambe.nlp.fewshot.model import PrototypicalTextClassifier\n\n\n__all__ = ['PrototypicalTextClassifier']\n"""
flambe/nlp/fewshot/model.py,4,"b'# type: ignore[override]\n\nfrom typing import Tuple, Dict, Any, Union, Optional\n\nimport torch\nfrom torch import Tensor\n\nfrom flambe.nn import Embedder, Module\nfrom flambe.nn.distance import get_distance_module, get_mean_module\n\n\nclass PrototypicalTextClassifier(Module):\n    """"""Implements a standard classifier.\n\n    The classifier is composed of an encoder module, followed by\n    a fully connected output layer, with a dropout layer in between.\n\n    Attributes\n    ----------\n    encoder: Module\n        the encoder object\n    decoder: Decoder\n        the decoder layer\n    drop: nn.Dropout\n        the dropout layer\n    loss: Metric\n        the loss function to optimize the model with\n    metric: Metric\n        the dev metric to evaluate the model on\n\n    """"""\n\n    def __init__(self,\n                 embedder: Embedder,\n                 distance: str = \'euclidean\',\n                 detach_mean: bool = False) -> None:\n        """"""Initialize the TextClassifier model.\n\n        Parameters\n        ----------\n        embedder: Embedder\n            The embedder layer\n\n        """"""\n        super().__init__()\n\n        self.embedder = embedder\n\n        self.distance_module = get_distance_module(distance)\n        self.mean_module = get_mean_module(distance)\n        self.detach_mean = detach_mean\n\n    def compute_prototypes(self, support: Tensor, label: Tensor) -> Tensor:\n        """"""Set the current prototypes used for classification.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            Input encodings\n        label : torch.Tensor\n            Corresponding labels\n\n        """"""\n        means_dict: Dict[int, Any] = {}\n        for i in range(support.size(0)):\n            means_dict.setdefault(int(label[i]), []).append(support[i])\n\n        means = []\n        n_means = len(means_dict)\n\n        for i in range(n_means):\n            # Ensure that all contiguous indices are in the means dict\n            supports = torch.stack(means_dict[i], dim=0)\n            if supports.size(0) > 1:\n                mean = self.mean_module(supports).squeeze(0)\n            else:\n                mean = supports.squeeze(0)\n            means.append(mean)\n\n        prototypes = torch.stack(means, dim=0)\n        return prototypes\n\n    def forward(self,  # type: ignore\n                query: Tensor,\n                query_label: Optional[Tensor] = None,\n                support: Optional[Tensor] = None,\n                support_label: Optional[Tensor] = None,\n                prototypes: Optional[Tensor] = None) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n\n        """"""Run a forward pass through the network.\n\n        Parameters\n        ----------\n        data: Tensor\n            The input data\n\n        Returns\n        -------\n        Union[Tensor, Tuple[Tensor, Tensor]]\n            The output predictions\n\n        """"""\n        query_encoding = self.embedder(query)\n        if isinstance(query_encoding, tuple):  # RNN\n            query_encoding = query_encoding[0]\n\n        if prototypes is not None:\n            prototypes = prototypes\n        elif support is not None and support_label is not None:\n            if self.detach_mean:\n                support = support.detach()\n                support_label = support_label.detach()  # type: ignore\n\n            support_encoding = self.embedder(support)\n            if isinstance(support_encoding, tuple):  # RNN\n                support_encoding = support_encoding[0]\n\n            # Compute prototypes\n            prototypes = self.compute_prototypes(support_encoding, support_label)\n        else:\n            raise ValueError(""No prototypes set or provided"")\n\n        dist = self.distance_module(query_encoding, prototypes)\n        if query_label is not None:\n            return - dist, query_label\n        else:\n            return - dist\n'"
flambe/nlp/language_modeling/__init__.py,0,"b""# type: ignore[attr-define]\n\nfrom flambe.nlp.language_modeling.datasets import PTBDataset, Wiki103, Enwiki8\nfrom flambe.nlp.language_modeling.fields import LMField\nfrom flambe.nlp.language_modeling.model import LanguageModel\nfrom flambe.nlp.language_modeling.sampler import CorpusSampler\n\n\n__all__ = ['PTBDataset', 'Wiki103', 'Enwiki8', 'LanguageModel', 'LMField', 'CorpusSampler']\n"""
flambe/nlp/language_modeling/datasets.py,0,"b'from typing import List, Tuple, Optional, Union, Dict, Sequence\nfrom zipfile import ZipFile\nfrom io import BytesIO\nimport requests\n\nfrom flambe.dataset import TabularDataset\nfrom flambe.field import Field\n\n\nclass PTBDataset(TabularDataset):\n    """"""The official PTB dataset.""""""\n\n    PTB_URL = ""https://raw.githubusercontent.com/yoonkim/lstm-char-cnn/master/data/ptb/""\n\n    def __init__(self,  # nosec\n                 split_by_line: bool = False,\n                 end_of_line_token: Optional[str] = \'<eol>\',  # nosec\n                 cache: bool = False,\n                 transform: Dict[str, Union[Field, Dict]] = None) -> None:\n        """"""Initialize the PTBDataset builtin.\n\n        Parameters\n        ----------\n        split_by_line: bool, Optional\n            If true, tokenizes per line. Default ``False``.\n        end_of_line_token: str, Optional\n            Token added at the end of every line.\n\n        see TabularDataset for other arguments.\n\n        """"""\n        self.split_by_line = split_by_line\n        self.eol = end_of_line_token\n\n        train_path = self.PTB_URL + ""train.txt""\n        val_path = self.PTB_URL + ""valid.txt""\n        test_path = self.PTB_URL + ""test.txt""\n\n        train = self._process(requests.get(train_path).content)\n        val = self._process(requests.get(val_path).content)\n        test = self._process(requests.get(test_path).content)\n\n        super().__init__(train, val, test, cache=cache, transform=transform)\n\n    def _process(self, file: bytes) -> List[Tuple[str]]:\n        """"""Process the input file.\n\n        Parameters\n        ----------\n        field: str\n            The input file, as bytes\n\n        Returns\n        -------\n        List[Tuple[str]]\n            List of examples, where each example is a single\n            element tuple containing the text.\n\n        """"""\n        decoded_text = file.decode(\'utf-8\')\n        # Replace end of line tokens\n        if self.eol is not None and not self.split_by_line:\n            decoded_text = decoded_text.replace(\'\\n\', self.eol)\n\n        # Split by sentence or unroll\n        if self.split_by_line:\n            eol = self.eol or \'\'\n            text = [(sent.strip() + \' \' + eol,) for sent in decoded_text.split(\'\\n\')]\n        else:\n            text = [(decoded_text,)]\n\n        return text\n\n\nclass Wiki103(TabularDataset):\n    """"""The official WikiText103 dataset.""""""\n\n    WIKI_URL = ""https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip""\n\n    def __init__(self,  # nosec\n                 split_by_line: bool = False,\n                 end_of_line_token: Optional[str] = \'<eol>\',  # nosec\n                 remove_headers: bool = False,\n                 cache: bool = False,\n                 transform: Dict[str, Union[Field, Dict]] = None) -> None:\n        """"""Initialize the Wiki103 built-in.\n\n        Parameters\n        ----------\n        split_by_line: bool, Optional\n            If true, tokenizes per line. Default ``False``.\n        end_of_line_token: str, Optional\n            Token added at the end of every line.\n\n        see TabularDataset for other arguments.\n\n        """"""\n        self.split_by_line = split_by_line\n        self.eol = end_of_line_token\n        self.remove_headers = remove_headers\n        response = requests.get(self.WIKI_URL, stream=True)\n        with ZipFile(BytesIO(response.content), \'r\') as z:\n            train = self._process(z.read(\'wikitext-103/wiki.train.tokens\'))\n            val = self._process(z.read(\'wikitext-103/wiki.valid.tokens\'))\n            test = self._process(z.read(\'wikitext-103/wiki.test.tokens\'))\n\n        super().__init__(train, val, test, cache=cache, transform=transform)\n\n    def _process(self, file: bytes) -> List[Tuple[str]]:\n        """"""Process the input file.\n\n        Parameters\n        ----------\n        file: bytes\n            The input file, as a byte string\n\n        Returns\n        -------\n        List[Tuple[str]]\n            List of examples, where each example is a single\n            element tuple containing the text.\n\n        """"""\n        decoded_text = file.decode(\'utf-8\')\n        decoded_lines = decoded_text.split(\'\\n\')\n\n        # Remove titles of Wikipedia articles if desired\n        if self.remove_headers:\n            filtered_lines = []\n            for line in decoded_lines:\n                line_strip = line.strip()\n                if len(line_strip) > 0:\n                    if line_strip[0] != \'=\' and line_strip[-1] != \'=\':\n                        filtered_lines.append(line)\n            decoded_lines = filtered_lines\n\n        eol = self.eol or \'\'\n        if self.split_by_line:\n            text = [(line.lstrip() + eol,) for line in decoded_lines]\n        else:\n            text = [(eol.join(decoded_lines),)]\n\n        return text\n\n\nclass Enwiki8(TabularDataset):\n    """"""The official WikiText103 dataset.""""""\n\n    ENWIKI_URL = ""http://mattmahoney.net/dc/enwik8.zip""\n\n    def __init__(self,\n                 num_eval_symbols: int = 5000000,\n                 remove_end_of_line: bool = False,\n                 cache: bool = False,\n                 transform: Dict[str, Union[Field, Dict]] = None) -> None:\n        """"""Initialize the Wiki103 built-in.\n\n        Parameters\n        ----------.\n        num_eval_symbols: int, optional\n            The number of symbols to use for seach of validation,\n            and testing. Default ``5000000``.\n        remove_end_of_line: bool, optional\n            If True, remove end of line tokens. Default ``True``.\n\n        see TabularDataset for other arguments.\n\n        """"""\n        self.num_eval_symbols = num_eval_symbols\n        self.remove_end_of_line = remove_end_of_line\n        response = requests.get(self.ENWIKI_URL, stream=True)\n        with ZipFile(BytesIO(response.content), \'r\') as z:\n            train, val, test = self._process(z.read(\'enwik8\'))\n\n        super().__init__(train, val, test, cache=cache, transform=transform)\n\n    def _process(self, file: bytes) -> Sequence[List[Tuple[str]]]:\n        """"""Process the input file.\n\n        Parameters\n        ----------\n        file: bytes\n            The input file, as a byte string\n\n        Returns\n        -------\n        List[Tuple[str]]\n            List of examples, where each example is a single\n            element tuple containing the text.\n\n        """"""\n        train_data = file[: -2 * self.num_eval_symbols]\n        val_data = file[-2 * self.num_eval_symbols: -self.num_eval_symbols]\n        test_data = file[-self.num_eval_symbols:]\n\n        symbol = \'\' if self.remove_end_of_line else str(ord(\'\\n\'))\n        train = \' \'.join([str(c) if c != ord(\'\\n\') else symbol for c in train_data])\n        val = \' \'.join([str(c) if c != ord(\'\\n\') else symbol for c in val_data])\n        test = \' \'.join([str(c) if c != ord(\'\\n\') else symbol for c in test_data])\n\n        return [(train,)], [(val,)], [(test,)]\n'"
flambe/nlp/language_modeling/fields.py,2,"b'from typing import Tuple\nimport torch\n\nfrom flambe.field import TextField\n\n\nclass LMField(TextField):\n    """"""Language Model field.\n\n    Generates the original tensor alongside its shifted version.\n\n    """"""\n\n    def __init__(self,\n                 **kwargs) -> None:\n        super().__init__(**kwargs)\n\n    def process(self, example: str) -> Tuple[torch.Tensor, ...]:  # type: ignore\n        """"""Process an example and create 2 Tensors.\n\n        Parameters\n        ----------\n        example: str\n            The example to process, as a single string\n\n        Returns\n        -------\n        Tuple[torch.Tensor, ...]\n            The processed example, tokenized and numericalized\n\n        """"""\n        ret = super().process(example)\n        return ret[:-1], ret[1:]  # type: ignore\n'"
flambe/nlp/language_modeling/model.py,2,"b'# type: ignore[override]\n\nfrom typing import Tuple, Optional, Union\n\nimport torch.nn as nn\nfrom torch import Tensor\n\n\nfrom flambe.nn import Embedder, Module\n\n\nclass LanguageModel(Module):\n    """"""Implement an LanguageModel model for sequential classification.\n\n    This model can be used to language modeling, as well as other\n    sequential classification tasks. The full sequence predictions\n    are produced by the model, effectively making the number of\n    examples the batch size multiplied by the sequence length.\n\n    """"""\n\n    def __init__(self,\n                 embedder: Embedder,\n                 output_layer: Module,\n                 dropout: float = 0,\n                 pad_index: int = 0,\n                 tie_weights: bool = False,\n                 tie_weight_attr: str = \'embedding\') -> None:\n        """"""Initialize the LanguageModel model.\n\n        Parameters\n        ----------\n        embedder: Embedder\n            The embedder layer\n        output_layer : Decoder\n            Output layer to use\n        dropout : float, optional\n            Amount of droput between the encoder and decoder,\n            defaults to 0.\n        pad_index: int, optional\n            Index used for padding, defaults to 0\n        tie_weights : bool, optional\n            If true, the input and output layers share the same weights\n        tie_weight_attr: str, optional\n            The attribute to call on the embedder to get the weight\n            to tie. Only used if tie_weights is ``True``. Defaults\n            to ``embedding``. Multiple attributes can also be called\n            by adding another dot: ``embeddings.word_embedding``.\n\n        """"""\n        super().__init__()\n\n        self.embedder = embedder\n        self.output_layer = output_layer\n        self.drop = nn.Dropout(dropout)\n\n        self.pad_index = pad_index\n        self.tie_weights = tie_weights\n\n        if tie_weights:\n            module = self.embedder\n            for attr in tie_weight_attr.split(\'.\'):\n                module = getattr(module, attr)\n            self.output_layer.weight = module.weight\n\n    def forward(self,\n                data: Tensor,\n                target: Optional[Tensor] = None) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n        """"""Run a forward pass through the network.\n\n        Parameters\n        ----------\n        data: Tensor\n            The input data\n\n        Returns\n        -------\n        Union[Tensor, Tuple[Tensor, Tensor]]\n            The output predictions of shape seq_len x batch_size x n_out\n\n        """"""\n        outputs = self.embedder(data)\n        if isinstance(outputs, tuple):\n            encoding = outputs[0]\n        else:\n            encoding = outputs\n\n        if target is not None:\n            mask = (target != self.pad_index).float()\n            # Flatten to compute loss across batch and sequence\n            flat_mask = mask.view(-1).bool()\n            flat_encodings = encoding.view(-1, encoding.size(2))[flat_mask]\n            # Not sure why mypy won\'t detect contiguous, it is a\n            # method on torch.Tensor\n            flat_targets = target.contiguous().view(-1)[flat_mask]  # type: ignore\n            flat_pred = self.output_layer(self.drop(flat_encodings))\n            return flat_pred, flat_targets\n        else:\n            pred = self.output_layer(self.drop(encoding))\n            return pred\n'"
flambe/nlp/language_modeling/sampler.py,7,"b'from typing import Optional, Sequence, Tuple, Iterator\nimport math\n\nimport torch\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader\n\nfrom flambe.sampler.sampler import Sampler\n\n\nclass CorpusSampler(Sampler):\n    """"""Implement a CorpusSampler object.\n\n    This object is useful for iteration over a large corpus of\n    text in an ordered way. It takes as input a dataset with\n    a single example containing the sequence of tokens and will yield\n    batches that contain both source sequences of tensors corresponding\n    to the Corpus\'s text, and these same sequences shifted by one as\n    the target.\n\n    """"""\n\n    def __init__(self,\n                 batch_size: int = 128,\n                 unroll_size: int = 128,\n                 n_workers: int = 0,\n                 pin_memory: bool = False,\n                 downsample: Optional[float] = None,\n                 drop_last: bool = True) -> None:\n        """"""Initialize the CorpusSampler object.\n\n        Parameters\n        ----------\n        batch_size : int, optional\n            The batch size to use. Default ``128``.\n        unroll_size: int, optional\n            Make every sequence this length. Default ``128``.\n        n_workers : int, optional\n            Number of workers to pass to the DataLoader\n            (the default is 0, which means the main process)\n        pin_memory : bool, optional\n            Pin the memory when using cuda (the default is False)\n        downsample: float, optional\n            Percentage of the data to downsample to\n        drop_last: bool, optional\n            Set to True to drop the last incomplete batch if the dataset\n            size is not divisible by the batch size.\n            (the default is False)\n\n        """"""\n        self.unroll_size = unroll_size\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n        self.n_workers = n_workers\n        self.pin_memory = pin_memory\n        self.downsample = downsample\n\n    @staticmethod\n    def collate_fn(data: Sequence[Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tensor]:\n        """"""Create a batch from data.\n\n        Parameters\n        ----------\n        data : Sequence[Tuple[Tensor, Tensor]]\n            List of (source, target) tuples.\n\n        Returns\n        -------\n        Tuple[Tensor, Tensor]\n            Source and target Tensors.\n\n        """"""\n        x, y = zip(*data)\n        return torch.stack(x).t(), torch.stack(y).t()\n\n    def sample(self,\n               data: Sequence[Sequence[Tensor]],\n               n_epochs: int = 1) -> Iterator[Tuple[Tensor, ...]]:\n        """"""Sample from the list of features and yields batches.\n\n        Parameters\n        ----------\n        data: Sequence[Sequence[Tensor, ...]]\n            The input data to sample from\n        n_epochs: int, optional\n            The number of epochs to run in the output iterator.\n            Use -1 to run infinitely.\n\n        Yields\n        ------\n        Iterator[Tuple[Tensor]]\n            A batch of data, as a tuple of Tensors\n\n        """"""\n        if len(data) == 0:\n            raise ValueError(""No examples provided"")\n        elif len(data) > 1:\n            raise ValueError(""Expected a single input example"")\n\n        tensor = data[0][0]  # First example, first column\n\n        if self.downsample:\n            if not (0 < self.downsample <= 1):\n                raise ValueError(""Downsample value should be in the range (0, 1]"")\n            tensor = tensor[:int(self.downsample * tensor.size(0))]\n\n        # Organize batch-wise\n        final_length = (tensor.size(0) - 1) // self.batch_size * self.batch_size\n        x = torch.reshape(tensor[:final_length], (self.batch_size, -1)).t()\n        y = torch.reshape(tensor[1:final_length + 1], (self.batch_size, -1)).t()\n\n        loader = DataLoader(dataset=torch.utils.data.TensorDataset(x, y),\n                            collate_fn=self.collate_fn,\n                            shuffle=False,\n                            batch_size=self.unroll_size,\n                            num_workers=self.n_workers,\n                            pin_memory=self.pin_memory,\n                            drop_last=self.drop_last)\n\n        if n_epochs == -1:\n            while True:\n                yield from loader\n        else:\n            for _ in range(n_epochs):\n                yield from loader\n\n    def length(self, data: Sequence[Sequence[torch.Tensor]]) -> int:\n        """"""Return the number of batches in the sampler.\n\n        Parameters\n        ----------\n        data: Sequence[Sequence[torch.Tensor, ...]]\n            The input data to sample from\n\n        Returns\n        -------\n        int\n            The number of batches that would be created per epoch\n\n        """"""\n        tensor = data[0][0]\n        if self.drop_last:\n            return ((tensor.size(0) - 1) // self.batch_size) // self.unroll_size\n        else:\n            return math.ceil(((tensor.size(0) - 1) // self.batch_size) / self.unroll_size)\n'"
flambe/nlp/transformers/__init__.py,0,"b""# type: ignore[attr-define]\n\nfrom flambe.nlp.transformers.field import PretrainedTransformerField\nfrom flambe.nlp.transformers.model import PretrainedTransformerEmbedder\n\n\n__all__ = ['PretrainedTransformerField', 'PretrainedTransformerEmbedder']\n"""
flambe/nlp/transformers/field.py,4,"b'from typing import Optional, Union, List, Dict, Any, Tuple\n\nimport torch\nfrom transformers import AutoTokenizer\n\nfrom flambe.field import Field\n\n\nclass PretrainedTransformerField(Field):\n    """"""Field intergation of the transformers library.\n\n    Instantiate this object using any alias available in the\n    `transformers` library. More information can be found here:\n\n    https://huggingface.co/transformers/\n\n    """"""\n\n    def __init__(self,\n                 alias: str,\n                 cache_dir: Optional[str] = None,\n                 max_len_truncate: int = 500,\n                 add_special_tokens: bool = True, **kwargs) -> None:\n        """"""Initialize a pretrained tokenizer.\n\n        Parameters\n        ----------\n        alias: str\n            Alias of a pretrained tokenizer.\n        cache_dir: str, optional\n            A directory where to cache the downloaded vocabularies.\n        max_len_truncate: int, default = 500\n            Truncates the length of the tokenized sequence.\n            Because several pretrained models crash when this is\n            > 500, it defaults to 500\n        add_special_tokens: bool, optional\n            Add the special tokens to the inputs. Default ``True``.\n\n        """"""\n        self._tokenizer = AutoTokenizer.from_pretrained(alias, cache_dir=cache_dir, **kwargs)\n        self.max_len_truncate = max_len_truncate\n        self.add_special_tokens = add_special_tokens\n\n    @property\n    def padding_idx(self) -> int:\n        """"""Get the padding index.\n\n        Returns\n        -------\n        int\n            The padding index in the vocabulary\n\n        """"""\n        pad_token = self._tokenizer.pad_token\n        return self._tokenizer.convert_tokens_to_ids(pad_token)\n\n    @property\n    def vocab_size(self) -> int:\n        """"""Get the vocabulary length.\n\n        Returns\n        -------\n        int\n            The length of the vocabulary\n\n        """"""\n        return len(self._tokenizer)\n\n    def process(self, example:  # type: ignore\n                Union[str, Tuple[Any], List[Any], Dict[Any, Any]]) \\\n            -> Union[torch.Tensor, Tuple[torch.Tensor, ...],\n                     List[torch.Tensor], Dict[str, torch.Tensor]]:\n        """"""Process an example, and create a Tensor.\n\n        Parameters\n        ----------\n        example: str\n            The example to process, as a single string\n\n        Returns\n        -------\n        torch.Tensor\n            The processed example, tokenized and numericalized\n\n        """"""\n        # special case of list of examples:\n        if isinstance(example, list) or isinstance(example, tuple):\n            return [self.process(e) for e in example]  # type: ignore\n        elif isinstance(example, dict):\n            return dict([(key, self.process(val)) for key, val in example.items()])  # type: ignore\n\n        tokens = self._tokenizer.encode(example, add_special_tokens=self.add_special_tokens)\n\n        if self.max_len_truncate is not None:\n            tokens = tokens[:self.max_len_truncate]\n\n        return torch.tensor(tokens)\n'"
flambe/nlp/transformers/model.py,11,"b'# type: ignore[override]\n\nfrom typing import Optional, Any\n\nimport torch\nfrom transformers import AutoModel\n\nfrom flambe.nn import Module\n\n\nclass PretrainedTransformerEmbedder(Module):\n    """"""Embedder intergation of the transformers library.\n\n    Instantiate this object using any alias available in the\n    `transformers` library. More information can be found here:\n\n    https://huggingface.co/transformers/\n\n    """"""\n\n    def __init__(self,\n                 alias: str,\n                 cache_dir: Optional[str] = None,\n                 padding_idx: Optional[int] = None,\n                 pool: bool = False, **kwargs) -> None:\n        """"""Initialize from a pretrained model.\n\n        Parameters\n        ----------\n        alias: str\n            Alias of a pretrained model.\n        cache_dir: str, optional\n            A directory where to cache the downloaded vocabularies.\n        padding_idx: int, optional\n            The padding index used to compute the attention mask.\n        pool: optional, optional\n            Whether to return the pooled output or the full sequence\n            encoding. Default ``False``.\n\n        """"""\n        super().__init__()\n\n        if \'gpt2\' in alias and pool:\n            raise ValueError(\'GPT2 does not support pooling.\')\n\n        embedder = AutoModel.from_pretrained(alias, cache_dir=cache_dir, **kwargs)\n        self.config = embedder.config\n        self._embedder = embedder\n        self.padding_idx = padding_idx\n        self.pool = pool\n\n    def forward(self,\n                data: torch.Tensor,\n                token_type_ids: Optional[torch.Tensor] = None,\n                attention_mask: Optional[torch.Tensor] = None,\n                position_ids: Optional[torch.Tensor] = None,\n                head_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Perform a forward pass through the network.\n\n        If pool was provided, will only return the pooled output\n        of shape [B x H]. Otherwise, returns the full sequence encoding\n        of shape [S x B x H].\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data of shape [B x S]\n        token_type_ids : Optional[torch.Tensor], optional\n            Segment token indices to indicate first and second portions\n            of the inputs. Indices are selected in ``[0, 1]``: ``0``\n            corresponds to a `sentence A` token, ``1``\n            corresponds to a `sentence B` token. Has shape [B x S]\n        attention_mask : Optional[torch.Tensor], optional\n            FloatTensor of shape [B x S]. Masked values should\n            be 0 for padding tokens, 1 otherwise.\n        position_ids : Optional[torch.Tensor], optional\n            Indices of positions of each input sequence tokens\n            in the position embedding. Defaults to the order given\n            in the input. Has shape [B x S].\n        head_mask : Optional[torch.Tensor], optional\n            Mask to nullify selected heads of the self-attention\n            modules. Should be 0 for heads to mask, 1 otherwise.\n            Has shape [num_layers x num_heads]\n\n        Returns\n        -------\n        torch.Tensor\n            If pool is True, returns a tneosr of shape [B x H],\n            else returns an encoding for each token in the sequence\n            of shape [B x S x H].\n\n        """"""\n        if attention_mask is None and self.padding_idx is not None:\n            attention_mask = (data != self.padding_idx).float()\n\n        outputs = self._embedder(data,\n                                 token_type_ids=token_type_ids,\n                                 attention_mask=attention_mask,\n                                 position_ids=position_ids,\n                                 head_mask=head_mask)\n\n        output = outputs[0] if not self.pool else outputs[1]\n        return output\n\n    def __getattr__(self, name: str) -> Any:\n        """"""Override getattr to inspect config.\n\n        Parameters\n        ----------\n        name : str\n            The attribute to fetch\n\n        Returns\n        -------\n        Any\n            The attribute\n\n        """"""\n        try:\n            return super().__getattr__(name)\n        except AttributeError as e:\n            config = self.__dict__[\'config\']\n            if hasattr(config, name):\n                return getattr(config, name)\n            else:\n                raise e\n'"
flambe/nn/distance/__init__.py,0,"b'# type: ignore[attr-defined]\n\nfrom flambe.nn.distance.distance import DistanceModule, MeanModule\nfrom flambe.nn.distance.euclidean import EuclideanDistance, EuclideanMean\nfrom flambe.nn.distance.cosine import CosineDistance, CosineMean\nfrom flambe.nn.distance.hyperbolic import HyperbolicDistance, HyperbolicMean\n\n\ndef get_distance_module(metric: str) -> DistanceModule:\n    """"""Get the distance module from a string alias.\n\n    Currently available:\n    . `euclidean`\n    . `cosine`\n    . `hyperbolic`\n\n    Parameters\n    ----------\n    metric : str\n        The distance metric to use\n\n    Raises\n    ------\n    ValueError\n        Unvalid distance string alias provided\n\n    Returns\n    -------\n    DistanceModule\n        The instantiated distance module\n\n    """"""\n    if metric == \'euclidean\':\n        module = EuclideanDistance()\n    elif metric == \'cosine\':\n        module = CosineDistance()\n    elif metric == \'hyperbolic\':\n        module = HyperbolicDistance()\n    else:\n        raise ValueError(f""Unknown distance alias: {metric}"")\n\n    return module\n\n\ndef get_mean_module(metric: str) -> MeanModule:\n    """"""Get the mean module from a string alias.\n\n    Currently available:\n    . `euclidean`\n    . `cosine`\n    . `hyperbolic`\n\n    Parameters\n    ----------\n    metric : str\n        The distance metric to use\n\n    Raises\n    ------\n    ValueError\n        Unvalid distance string alias provided\n\n    Returns\n    -------\n    DistanceModule\n        The instantiated distance module\n\n    """"""\n    if metric == \'euclidean\':\n        module = EuclideanMean()\n    elif metric == \'cosine\':\n        module = CosineMean()\n    elif metric == \'hyperbolic\':\n        module = HyperbolicMean()\n    else:\n        raise ValueError(f""Unknown distance alias: {metric}"")\n\n    return module\n'"
flambe/nn/distance/cosine.py,6,"b'# type: ignore[override]\n\nimport torch\nfrom torch import Tensor\nfrom flambe.nn.distance import DistanceModule, MeanModule\n\n\nclass CosineDistance(DistanceModule):\n    """"""Implement a CosineDistance object.\n\n    """"""\n\n    def __init__(self, eps: float = 1e-8) -> None:\n        """"""Initialize the CosineDistance module.\n\n        Parameters\n        ----------\n        eps : float, optional\n            Used for numerical stability\n\n        """"""\n        super().__init__()\n        self.eps = eps\n\n    def forward(self, mat_1: Tensor, mat_2: Tensor) -> Tensor:\n        """"""Returns the cosine distance between each\n        element in mat_1 and each element in mat_2.\n\n        Parameters\n        ----------\n        mat_1: torch.Tensor\n            matrix of shape (n_1, n_features)\n        mat_2: torch.Tensor\n            matrix of shape (n_2, n_features)\n\n        Returns\n        -------\n        dist: torch.Tensor\n            distance matrix of shape (n_1, n_2)\n\n        """"""\n        w1 = mat_1.norm(p=2, dim=1, keepdim=True)\n        w2 = mat_2.norm(p=2, dim=1, keepdim=True)\n        return 1 - torch.mm(mat_1, mat_2.t()) / (w1 * w2.t()).clamp(min=self.eps)\n\n\nclass CosineMean(MeanModule):\n    """"""Implement a CosineMean object.\n\n    """"""\n    def forward(self, data: Tensor) -> Tensor:\n        """"""Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a float tensor\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded output, as a float tensor\n\n        """"""\n        data = data / (data.norm(dim=1, keepdim=True))\n        return data.mean(0)\n'"
flambe/nn/distance/distance.py,4,"b'# type: ignore[override]\n\nfrom torch import Tensor\n\nfrom flambe.nn.module import Module\n\n\nclass DistanceModule(Module):\n    """"""Implement a DistanceModule object.\n\n    """"""\n\n    def forward(self, mat_1: Tensor, mat_2: Tensor) -> Tensor:\n        """"""Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a float tensor\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded output, as a float tensor\n\n        """"""\n        raise NotImplementedError\n\n\nclass MeanModule(Module):\n    """"""Implement a MeanModule object.\n\n    """"""\n    def __init__(self, detach_mean: bool = False) -> None:\n        """"""Initilaize the MeanModule.\n\n        Parameters\n        ----------\n        detach_mean : bool, optional\n            Set to detach the mean computation, this is useful when the\n            mean computation does not admit a closed form.\n\n        """"""\n        super().__init__()\n        self.detach_mean = detach_mean\n\n    def forward(self, data: Tensor) -> Tensor:\n        """"""Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a float tensor\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded output, as a float tensor\n\n        """"""\n        raise NotImplementedError\n'"
flambe/nn/distance/euclidean.py,7,"b'# type: ignore[override]\n\nimport torch\nfrom torch import Tensor\nfrom flambe.nn.distance.distance import DistanceModule, MeanModule\n\n\nclass EuclideanDistance(DistanceModule):\n    """"""Implement a EuclideanDistance object.""""""\n\n    def forward(self, mat_1: Tensor, mat_2: Tensor) -> Tensor:\n        """"""Returns the squared euclidean distance between each\n        element in mat_1 and each element in mat_2.\n\n        Parameters\n        ----------\n        mat_1: torch.Tensor\n            matrix of shape (n_1, n_features)\n        mat_2: torch.Tensor\n            matrix of shape (n_2, n_features)\n\n        Returns\n        -------\n        dist: torch.Tensor\n            distance matrix of shape (n_1, n_2)\n\n        """"""\n        dist = [torch.sum((mat_1 - mat_2[i])**2, dim=1) for i in range(mat_2.size(0))]\n        dist = torch.stack(dist, dim=1)\n        return dist\n\n\nclass EuclideanMean(MeanModule):\n    """"""Implement a EuclideanMean object.""""""\n\n    def forward(self, data: Tensor) -> Tensor:\n        """"""Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a float tensor\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded output, as a float tensor\n\n        """"""\n        return data.mean(0)\n'"
flambe/nn/distance/hyperbolic.py,19,"b'# type: ignore[override]\n\nimport torch\nfrom torch import Tensor\nfrom flambe.nn.distance.distance import DistanceModule, MeanModule\n\n\nEPSILON = 1e-5\n\n\ndef arccosh(x):\n    """"""Compute the arcosh, numerically stable.""""""\n    x = torch.clamp(x, min=1 + EPSILON)\n    a = torch.log(x)\n    b = torch.log1p(torch.sqrt(x * x - 1) / x)\n    return a + b\n\n\ndef mdot(x, y):\n    """"""Compute the inner product.""""""\n    m = x.new_ones(1, x.size(1))\n    m[0, 0] = -1\n    return torch.sum(m * x * y, 1, keepdim=True)\n\n\ndef dist(x, y):\n    """"""Get the hyperbolic distance between x and y.""""""\n    return arccosh(-mdot(x, y))\n\n\ndef project(x):\n    """"""Project onto the hyeprboloid embedded in in n+1 dimensions.""""""\n    return torch.cat([torch.sqrt(1.0 + torch.sum(x * x, 1, keepdim=True)), x], 1)\n\n\ndef log_map(x, y):\n    """"""Perform the log step.""""""\n    d = dist(x, y)\n    return (d / torch.sinh(d)) * (y - torch.cosh(d) * x)\n\n\ndef norm(x):\n    """"""Compute the norm""""""\n    n = torch.sqrt(torch.abs(mdot(x, x)))\n    return n\n\n\ndef exp_map(x, y):\n    """"""Perform the exp step.""""""\n    n = torch.clamp(norm(y), min=EPSILON)\n    return torch.cosh(n) * x + (torch.sinh(n) / n) * y\n\n\ndef loss(x, y):\n    """"""Get the loss for the optimizer.""""""\n    return torch.sum(dist(x, y)**2)\n\n\nclass HyperbolicDistance(DistanceModule):\n    """"""Implement a HyperbolicDistance object.\n\n    """"""\n\n    def forward(self, mat_1: Tensor, mat_2: Tensor) -> Tensor:\n        """"""Returns the squared euclidean distance between each\n        element in mat_1 and each element in mat_2.\n\n        Parameters\n        ----------\n        mat_1: torch.Tensor\n            matrix of shape (n_1, n_features)\n        mat_2: torch.Tensor\n            matrix of shape (n_2, n_features)\n\n        Returns\n        -------\n        dist: torch.Tensor\n            distance matrix of shape (n_1, n_2)\n\n        """"""\n        # Get projected 1st dimension\n        mat_1_x_0 = torch.sqrt(1 + mat_1.pow(2).sum(dim=1, keepdim=True))\n        mat_2_x_0 = torch.sqrt(1 + mat_2.pow(2).sum(dim=1, keepdim=True))\n\n        # Compute bilinear form\n        left = mat_1_x_0.mm(mat_2_x_0.t())  # n_1 x n_2\n        right = mat_1[:, 1:].mm(mat_2[:, 1:].t())  # n_1 x n_2\n\n        # Arcosh\n        return arccosh(left - right).pow(2)\n\n\nclass HyperbolicMean(MeanModule):\n    """"""Compute the mean point in the hyperboloid model.""""""\n\n    def forward(self, data: Tensor) -> Tensor:\n        """"""Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a float tensor\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded output, as a float tensor\n\n        """"""\n        n_iter = 5 if self.training else 100\n\n        # Project the input data to n+1 dimensions\n        projected = project(data)\n\n        mean = torch.mean(projected, 0, keepdim=True)\n        mean = mean / norm(mean)\n\n        r = 1e-2\n        for i in range(n_iter):\n            g = -2 * torch.mean(log_map(mean, projected), 0, keepdim=True)\n            mean = exp_map(mean, -r * g)\n            mean = mean / norm(mean)\n\n        # The first dimension, is recomputed in the distance module\n        return mean.squeeze()[1:]\n'"
flambe/vision/classification/__init__.py,0,"b""from flambe.vision.classification.datasets import MNISTDataset\nfrom flambe.vision.classification.model import ImageClassifier\n\n__all__ = ['MNISTDataset', 'ImageClassifier']\n"""
flambe/vision/classification/datasets.py,6,"b'import gzip\nimport torch\nimport requests\nfrom typing import List, Tuple, Optional\n\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\n\nfrom flambe.dataset import Dataset\nfrom flambe.compile import registrable_factory\n\n\nclass MNISTDataset(Dataset):\n    """"""The official MNIST dataset.""""""\n\n    data_type = {\n        0x08: np.uint8,\n        0x09: np.int8,\n        0x0b: np.dtype(\'>i2\'),\n        0x0c: np.dtype(\'>i4\'),\n        0x0d: np.dtype(\'>f4\'),\n        0x0e: np.dtype(\'>f8\'),\n    }\n\n    URL = ""http://yann.lecun.com/exdb/mnist/""\n\n    def __init__(self,\n                 train_images: np.ndarray = None,\n                 train_labels: np.ndarray = None,\n                 test_images: np.ndarray = None,\n                 test_labels: np.ndarray = None,\n                 val_ratio: Optional[float] = 0.2,\n                 seed: Optional[int] = None) -> None:\n        """"""Initialize the MNISTDataset.\n\n        Parameters\n        ----------\n        train_images: np.ndarray\n            parsed train images as a numpy array\n        train_labels: np.ndarray\n            parsed train labels as a numpy array\n        test_images: np.ndarray\n            parsed test images as a numpy array\n        test_labels: np.ndarray\n            parsed test labels as a numpy array\n        val_ratio: Optional[float]\n            validation set ratio. Default 0.2\n        seed: Optional[int]\n            random seed for the validation set split\n        """"""\n        if train_images is None:\n            train_images = self._parse_downloaded_idx(self.URL + ""train-images-idx3-ubyte.gz"")\n        if train_labels is None:\n            train_labels = self._parse_downloaded_idx(self.URL + ""train-labels-idx1-ubyte.gz"")\n        if test_images is None:\n            test_images = self._parse_downloaded_idx(self.URL + ""t10k-images-idx3-ubyte.gz"")\n        if test_labels is None:\n            test_labels = self._parse_downloaded_idx(self.URL + ""t10k-labels-idx1-ubyte.gz"")\n\n        self.train_images, self.val_images, self.train_labels, self.val_labels = \\\n            train_test_split(train_images, train_labels, test_size=val_ratio, random_state=seed)\n        self.test_images = test_images\n        self.test_labels = test_labels\n\n        self._train = get_dataset(self.train_images, self.train_labels)\n        self._val = get_dataset(self.val_images, self.val_labels)\n        self._test = get_dataset(self.test_images, self.test_labels)\n\n    @registrable_factory\n    @classmethod\n    def from_path(cls,\n                  train_images_path: str,\n                  train_labels_path: str,\n                  test_images_path: str,\n                  test_labels_path: str,\n                  val_ratio: Optional[float] = 0.2,\n                  seed: Optional[int] = None) -> \'MNISTDataset\':\n        """"""Initialize the MNISTDataset from local files.\n\n        Parameters\n        ----------\n        train_images_path: str\n            path to the train images file in the idx format\n        train_labels_path: str\n            path to the train labels file in the idx format\n        test_images_path: str\n            path to the test images file in the idx format\n        test_labels_path: str\n            path to the test labels file in the idx format\n        val_ratio: Optional[float]\n            validation set ratio. Default 0.2\n        seed: Optional[int]\n            random seed for the validation set split\n        """"""\n        return cls(\n            cls._parse_local_gzipped_idx(train_images_path),\n            cls._parse_local_gzipped_idx(train_labels_path),\n            cls._parse_local_gzipped_idx(test_images_path),\n            cls._parse_local_gzipped_idx(test_labels_path),\n            val_ratio=val_ratio,\n            seed=seed,\n        )\n\n    @property\n    def train(self) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n        """"""Returns the training data""""""\n        return self._train\n\n    @property\n    def val(self) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n        """"""Returns the validation data""""""\n        return self._val\n\n    @property\n    def test(self) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n        """"""Returns the test data""""""\n        return self._test\n\n    @classmethod\n    def _parse_local_gzipped_idx(cls, path: str) -> np.ndarray:\n        """"""Parse a local gzipped idx file""""""\n        with gzip.open(path) as f:\n            return cls._parse_idx(f.read())\n\n    @classmethod\n    def _parse_downloaded_idx(cls, url: str) -> np.ndarray:\n        """"""Parse a downloaded idx file""""""\n        r = requests.get(url)\n        return cls._parse_idx(gzip.decompress(r.content))\n\n    @classmethod\n    def _parse_idx(cls, data: bytes) -> np.ndarray:\n        """"""Parse an idx filie""""""\n        # parse the magic number that contains the dimension\n        # and the data type\n        magic = int.from_bytes(data[0:4], \'big\')\n        dim = magic % 256\n        data_type = magic // 256\n\n        shape = [int.from_bytes(data[4 * (i + 1): 4 * (i + 2)], \'big\') for i in range(dim)]\n        return np.frombuffer(\n            data,\n            dtype=cls.data_type[data_type],\n            offset=4 * (dim + 1)\n        ).reshape(shape)\n\n\ndef get_dataset(images: np.ndarray, labels: np.ndarray) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n    return [(\n        torch.from_numpy(image).float().unsqueeze(0),\n        torch.tensor(label).long()\n    ) for image, label in zip(images, labels)]\n'"
flambe/vision/classification/model.py,1,"b'import torch\nfrom torch import Tensor\n\nfrom typing import Optional, Tuple, Union\nfrom flambe.nn import Module  # type: ignore[attr-defined]\n\n\nclass ImageClassifier(Module):\n    """"""Implements a simple image classifier.\n\n    This classifier consists of an encocder module, followed by\n    a fully connected output layer that outputs a probability\n    distribution.\n\n    Attributes\n    ----------\n    encoder: Moodule\n        The encoder layer\n    output_layer: Module\n        The output layer, yields a probability distribution over targets\n    """"""\n    def __init__(self,\n                 encoder: Module,\n                 output_layer: Module) -> None:\n        super().__init__()\n\n        self.encoder = encoder\n        self.output_layer = output_layer\n\n    def forward(self,\n                data: Tensor,\n                target: Optional[Tensor] = None) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n        """"""Run a forward pass through the network.\n\n        Parameters\n        ----------\n        data: Tensor\n            The input data\n        target: Tensor, optional\n            The input targets, optional\n\n        Returns\n        -------\n        Union[Tensor, Tuple[Tensor, Tensor]\n            The output predictions, and optionally the targets\n\n        """"""\n        encoded = self.encoder(data)\n        pred = self.output_layer(torch.flatten(encoded, 1))\n        return (pred, target) if target is not None else pred\n'"
tests/unit/cluster/test_aws.py,0,"b'from flambe.cluster import AWSCluster\n\nfrom moto import mock_ec2, mock_cloudwatch\n\nimport pytest\nimport boto3\nimport tempfile\nimport shutil\nimport sys\nfrom io import StringIO\nimport importlib\nimport copy\n\nimport mock\nfrom flambe.runnable import SafeExecutionContext, error\n\nfrom flambe.cluster import instance, errors\n\n# This fixture will run for each test method automatically\n@pytest.fixture(scope=\'function\', autouse=True)\ndef ec2_mock():\n    mock = mock_ec2()\n    mock_cw = mock_cloudwatch()\n\n    mock.start()\n    mock_cw.start()\n\n    yield\n\n    mock.stop()\n    mock_cw.stop()\n\n# Create a mock subnet_id\n@pytest.fixture\ndef subnet():\n    ec2 = boto3.resource(\'ec2\')\n    vpc = ec2.create_vpc(CidrBlock=\'192.168.0.0/16\')\n    yield ec2.create_subnet(CidrBlock=\'192.168.0.0/16\', VpcId=vpc.id)\n\n@pytest.fixture\ndef sec_group(subnet):\n    ec2 = boto3.resource(\'ec2\')\n    return ec2.create_security_group(GroupName=\'slice_0\', Description=\'slice_0 sec group\', VpcId=subnet.vpc.id)\n\n\n@pytest.fixture\ndef get_secrets():\n    t = tempfile.NamedTemporaryFile(mode=""w+"", delete=False)\n\n    def _get_secrets(secrets_content=None):\n        secrets_content = secrets_content or """"""\n            [SSH]\n            SSH_KEY = /path/to/key\n            USER = ubuntu\n        """"""\n        t.write(secrets_content)\n        t.flush()\n        return t.name\n\n    yield _get_secrets\n    t.close()\n\n\n@pytest.fixture\ndef get_cluster(subnet, sec_group):\n\n    def _get_cluster(**kwargs):\n        return AWSCluster(\n           name=kwargs.get(\'name\', \'some-name\'),\n           factories_num=kwargs.get(\'factories_num\', 2),\n           factories_type=kwargs.get(\'factories_type\', \'g3.4xlarge\'),\n           orchestrator_type=kwargs.get(\'orchestrator_type\', \'t3.large\'),\n           key_name=kwargs.get(\'key_name\', \'some-key\'),\n           security_group=sec_group.id,\n           tags=kwargs.get(\'tags\'),\n           orchestrator_ami=kwargs.get(\'orchestrator_ami\', \'ami-di32icco45kbis9bk\'),\n           factory_ami=kwargs.get(\'factory_ami\', \'ami-di32icco45kbis9bk\'),\n           username=kwargs.get(""username"", ""ubuntu""),\n           key=kwargs.get(""key"", ""/path/to/key""),\n           subnet_id=subnet.id,\n           creator=kwargs.get(\'creator\', \'leo.messi\')\n       )\n\n    return _get_cluster\n\n\n@pytest.fixture\ndef create_cluster(get_cluster):\n    def _create_cluster(**kwargs):\n        cluster = get_cluster(**kwargs)\n        cluster.load_all_instances()\n        return cluster\n\n    return _create_cluster\n\n\n@mock.patch(\'flambe.cluster.instance.instance.Instance.wait_until_accessible\')\ndef test_missing_secrets(get_cluster):\n    cluster = get_cluster()\n    cluster.load_all_instances()\n\n\ndef test_secrets(get_cluster, get_secrets):\n    cluster = get_cluster()\n\n    secrets = """"""\n        [SOME_SECTION]\n        RANDOM = random\n    """"""\n\n    cluster.inject_secrets(get_secrets(secrets))\n    assert cluster.config[\'SOME_SECTION\'][\'RANDOM\'] == \'random\'\n\n\ndef test_launch_orchestrator(subnet, sec_group, get_cluster):\n    ec2 = boto3.resource(\'ec2\')\n\n    cluster = get_cluster(\n        name=\'cluster_name\',\n        key_name=\'key_name\',\n        orchestrator_type=\'t3.xlarge\',\n        creator=\'leo.messi\',\n        volume_size=500\n    )\n\n    cluster._create_orchestrator()\n\n    instances = list(ec2.instances.all())\n\n    assert len(instances) == 1\n\n    orch = instances[0]\n\n    assert orch.instance_type == \'t3.xlarge\'\n    assert orch.subnet == subnet\n    assert orch.key_name == \'key_name\'\n    assert len(orch.security_groups) == 1\n    assert orch.security_groups[0][\'GroupId\'] == sec_group.id\n\n    assert len(orch.tags) == 5\n\n    keys = [\'creator\', \'Purpose\', \'Cluster-Name\', \'Role\', \'Name\']\n    for x in orch.tags:\n        k, v = x[\'Key\'], x[\'Value\']\n\n        assert k in keys\n\n        if k == \'creator\':\n            assert v == \'leo.messi\'\n        if k == \'Cluster-Name\':\n            assert v == \'cluster_name\'\n        if k == \'Role\':\n            assert v == \'Orchestrator\'\n        if k == \'Purpose\':\n            assert v == \'flambe\'\n        if k == \'Name\':\n            assert v == cluster._get_creation_name(\'Orchestrator\')\n\n    assert len(orch.block_device_mappings) == 1\n    assert orch.block_device_mappings[0][\'DeviceName\'] == \'/dev/sda1\'\n\n    # moto seems not able to mock volume size\n    # volume_id = orch.block_device_mappings[0][\'Ebs\'][\'VolumeId\']\n    # volume = list(orch.volumes.filter(VolumeIds=[volume_id]))[0]\n    # assert volume.size == 500\n\n\ndef test_existing_orch(get_cluster):\n    ec2 = boto3.resource(\'ec2\')\n\n    cluster = get_cluster()\n\n    cluster._create_orchestrator()\n\n    instances = list(ec2.instances.all())\n\n    assert len(instances) == 1\n\n    orch = instances[0]\n    instance_id = orch.id\n\n    cluster2 = get_cluster()\n\n    orch2, _ = cluster2._existing_cluster()\n\n    assert orch2.id == instance_id\n\n\ndef test_existing_orch2(get_cluster):\n    ec2 = boto3.resource(\'ec2\')\n\n    # Existing clusters are evaluated based on creator\n    # and name\n    cluster = get_cluster(\n        name=\'cluster_name\',\n        key_name=\'some_other_key\',\n        orchestrator_type=\'t3.medium\',\n        creator=\'leo.messi\',\n        volume_size=500\n    )\n\n    cluster._create_orchestrator()\n\n    instances = list(ec2.instances.all())\n\n    assert len(instances) == 1\n\n    orch = instances[0]\n    instance_id = orch.id\n\n\n    cluster2 = get_cluster(\n        name=\'cluster_name\',\n        key_name=\'some_key\', # Another key\n        orchestrator_type=\'t3.xlarge\', # Another type\n        creator=\'leo.messi\',\n    )\n\n    orch2, _ = cluster2._existing_cluster()\n\n    assert orch2.id == instance_id\n\n\ndef test_existing_multiple_orchs(get_cluster):\n    ec2 = boto3.resource(\'ec2\')\n\n    # Existing clusters are evaluated based on creator\n    # and name\n    for _ in range(3):\n        cluster = get_cluster(name=\'cluster_name\', creator=\'leo.messi\')\n\n        cluster._create_orchestrator()\n\n    instances = list(ec2.instances.all())\n    assert len(instances) == 3\n\n    with pytest.raises(errors.ClusterError):\n        orch2, _ = cluster._existing_cluster()\n\n\n@mock.patch(\'flambe.cluster.instance.instance.CPUFactoryInstance.contains_gpu\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.wait_until_accessible\')\ndef test_launch_factories(mock_wait, mock_contains_gpu, subnet, sec_group, get_cluster):\n    mock_contains_gpu.return_value = True\n\n    ec2 = boto3.resource(\'ec2\')\n\n    cluster = get_cluster(\n        name=\'cluster_name\',\n        factories_num=5,\n        factories_type=\'g3.4xlarge\',\n        creator=\'leo.messi\',\n        volume_size=500\n    )\n\n    cluster._create_factories(number=5)\n\n    instances = list(ec2.instances.all())\n\n    assert 0 < len(instances) <= 5\n    for f in instances:\n        assert f.instance_type == \'g3.4xlarge\'\n\n        assert f.subnet == subnet\n        assert f.key_name == \'some-key\'\n        assert len(f.security_groups) == 1\n        assert f.security_groups[0][\'GroupId\'] == sec_group.id\n\n        assert len(f.tags) == 5\n\n        keys = [\'creator\', \'Purpose\', \'Cluster-Name\', \'Role\', \'Name\']\n        for x in f.tags:\n            k, v = x[\'Key\'], x[\'Value\']\n\n            assert k in keys\n\n            if k == \'creator\':\n                assert v == \'leo.messi\'\n            if k == \'Cluster-Name\':\n                assert v == \'cluster_name\'\n            if k == \'Role\':\n                assert v == \'Factory\'\n            if k == \'Purpose\':\n                assert v == \'flambe\'\n            if k == \'Name\':\n                assert v == cluster._get_creation_name(\'Factory\')\n\n        assert len(f.block_device_mappings) == 1\n        assert f.block_device_mappings[0][\'DeviceName\'] == \'/dev/sda1\'\n\n        # moto seems not able to mock volume size\n        # volume_id = f.block_device_mappings[0][\'Ebs\'][\'VolumeId\']\n        # volume = list(f.volumes.filter(VolumeIds=[volume_id]))[0]\n        # assert volume.size == 500\n\n\n@mock.patch(\'flambe.cluster.instance.instance.CPUFactoryInstance.contains_gpu\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.wait_until_accessible\')\ndef test_launch_gpu_factories(mock_wait, mock_contains_gpu, create_cluster):\n    mock_contains_gpu.return_value = True\n\n    ec2 = boto3.resource(\'ec2\')\n    cluster = create_cluster()\n\n    instances = list(ec2.instances.all())\n    assert len(instances) == 2\n\n    assert cluster.orchestrator.__class__ == instance.OrchestratorInstance\n    assert len(cluster.factories) == 1\n    assert cluster.factories[0].__class__ == instance.GPUFactoryInstance\n\n\n@mock.patch(\'flambe.cluster.instance.instance.CPUFactoryInstance.contains_gpu\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.wait_until_accessible\')\ndef test_launch_cpu_factories(mock_wait, mock_contains_gpu, create_cluster):\n    mock_contains_gpu.return_value = False\n\n    ec2 = boto3.resource(\'ec2\')\n    cluster = create_cluster()\n\n    instances = list(ec2.instances.all())\n    assert len(instances) == 2\n\n    assert cluster.orchestrator.__class__ == instance.OrchestratorInstance\n    assert len(cluster.factories) == 1\n    assert cluster.factories[0].__class__ == instance.CPUFactoryInstance\n\n\n@mock.patch(\'flambe.cluster.instance.instance.CPUFactoryInstance.contains_gpu\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.wait_until_accessible\')\n@mock.patch(\'flambe.cluster.aws.AWSCluster._get_boto_private_host\')\n@mock.patch(\'flambe.cluster.aws.AWSCluster._get_boto_public_host\')\ndef test_get_host_abstractions(mock_public_host, mock_private_host,\n                               mock_wait, mock_contains_gpu, create_cluster):\n    """"""Test that _get_boto_[public|private]_host are being used instead\n    of the boto attributes.\n\n    """"""\n    mock_contains_gpu.return_value = False\n\n    # Check that the methods are called when creating an instance.\n    cluster = create_cluster()\n    mock_public_host.assert_called()\n    mock_private_host.assert_called()\n\n    mock_public_host.reset_mock()\n    mock_private_host.reset_mock()\n\n    # Check methods are called when it finds an existing cluster.\n    cluster2 = create_cluster()\n\n    boto_orchestrator, boto_factories = cluster2._existing_cluster()\n\n    assert boto_orchestrator is not None\n    assert len(boto_factories) == 2\n\n    mock_public_host.assert_called()\n    mock_private_host.assert_called()\n\n    mock_public_host.reset_mock()\n    mock_private_host.reset_mock()\n\n    # Check behavior when getting an instance by host\n    _ = cluster._get_boto_instance_by_host(cluster.orchestrator.host)\n    mock_public_host.assert_called()\n    mock_private_host.assert_not_called()  # The private host is not required in this case.\n\n\n@mock.patch(\'flambe.cluster.instance.instance.CPUFactoryInstance.contains_gpu\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.wait_until_accessible\')\ndef test_existing_factories(mock_wait, mock_contains_gpu, get_cluster):\n    mock_contains_gpu.return_value = False\n    ec2 = boto3.resource(\'ec2\')\n\n    cluster = get_cluster(factories_num=1)\n\n    cluster._create_factories()\n\n    instances = list(ec2.instances.all())\n    assert len(instances) == 1\n    instance_id = instances[0].id\n\n    cluster2 = get_cluster(factories_num=1)\n\n    _, factories = cluster2._existing_cluster()\n\n    assert len(factories) == 1\n\n    assert factories[0].id == instance_id\n\n\n@mock.patch(\'flambe.cluster.instance.instance.CPUFactoryInstance.contains_gpu\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.wait_until_accessible\')\ndef test_tags(mock_wait, mock_contains_gpu, create_cluster):\n    mock_contains_gpu.return_value = True\n    ec2 = boto3.resource(\'ec2\')\n\n    cluster = create_cluster(\n        tags={\n            \'random_tag\': \'random_value\',\n            \'another_random_tag\': \'another_random_value\'\n        }\n    )\n\n    instances = list(ec2.instances.all())\n\n    keys = [\'creator\', \'Purpose\', \'Cluster-Name\', \'Role\']\n    for i in instances:\n        tags = {x[\'Key\']: x[\'Value\'] for x in i.tags}\n\n        assert \'random_tag\' in tags.keys()\n        assert tags[\'random_tag\'] == \'random_value\'\n\n        assert \'another_random_tag\' in tags.keys()\n        assert tags[\'another_random_tag\'] == \'another_random_value\'\n\n\n@mock.patch(\'flambe.cluster.instance.instance.CPUFactoryInstance.contains_gpu\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.wait_until_accessible\')\ndef test_instances_lifecycle(mock_wait, mock_contains_gpu, create_cluster):\n    mock_contains_gpu.return_value = True\n    ec2 = boto3.resource(\'ec2\')\n\n    cluster = create_cluster()\n\n    instances = list(ec2.instances.all())\n\n    for i in instances:\n        assert i.state[\'Name\'] == \'running\'\n\n    cluster.terminate_instances()\n    instances = list(ec2.instances.all())  # Reload instances\n    for i in instances:\n        assert i.state[\'Name\'] == \'terminated\'\n\n\ndef test_get_creation_name(get_cluster):\n    cluster = get_cluster(name=\'my-cluster\')\n\n    assert cluster._get_creation_name(\'Orchestrator\') == \'my-cluster_orchestrator\'\n    assert cluster._get_creation_name(\'Factory\') == \'my-cluster_factory\'\n\n\n@pytest.mark.parametrize(\'invalid_role\', [\'\', \'orch\', \'orchestrator\', \'factory\', \'Factories\'])\ndef test_get_creation_name_invalid_role(invalid_role, get_cluster):\n    cluster = get_cluster(name=\'my-cluster\')\n\n    with pytest.raises(ValueError):\n        cluster._get_creation_name(invalid_role)\n'"
tests/unit/cluster/test_cluster.py,0,"b'import pytest\nimport configparser\n\nimport mock\nfrom flambe.runnable import ClusterRunnable\n\nfrom flambe.cluster import instance, Cluster\nfrom flambe.cluster import errors as man_errors\n\n\nclass DummyRunnable(ClusterRunnable):\n    def __init__(self, name=\'random\', env=None):\n        super().__init__(env)\n        self.name = name\n\n    def run(self):\n        pass\n\n    def setup(self):\n        pass\n\n    @classmethod\n    def to_yaml(cls, representer, node, tag):\n        kwargs = {}\n        return representer.represent_mapping(tag, kwargs)\n\n    @classmethod\n    def from_yaml(cls, constructor, node, factory_name):\n        kwargs, = list(constructor.construct_yaml_map(node))\n        return cls(**kwargs)\n\n\ndef get_instance(_class=None, **kwargs):\n    host = kwargs.get(\'host\', \'1.2.3.4\')\n    private_host = kwargs.get(\'private_host\', \'10.0.3.4\')\n    username = kwargs.get(\'username\', \'ubuntu\')\n    key = kwargs.get(\'key\', \'/path/to/ssh/key\')\n    config = kwargs.get(\'config\', configparser.ConfigParser())\n    debug = kwargs.get(\'debug\', False)\n    use_public = kwargs.get(\'use_public\', True)\n    if _class:\n        return _class(host=host, private_host=private_host, username=username,\n                      key=key, config=config, debug=debug, use_public=use_public)\n\n    return instance.Instance(host=host, private_host=private_host, username=username,\n                             key=key, config=config, debug=debug, use_public=use_public)\n\n\n@pytest.fixture\ndef get_cluster():\n\n    def _get_cluster(gpu_cluster: bool = True, fill: bool = True, **kwargs):\n        name = kwargs.get(\'name\', \'cluster\')\n        factories_num = kwargs.get(\'factories_num\', 2)\n        username = kwargs.get(""username"", ""ubuntu"")\n        key = kwargs.get(""key"", ""/path/to/key"")\n        setup_cmds = kwargs.get(""setup_cmds"", [])\n\n        cluster = Cluster(name=name, factories_num=factories_num, username=username,\n                          key=key, setup_cmds=setup_cmds)\n\n        if fill:\n            cluster.orchestrator = get_instance(instance.OrchestratorInstance)\n            if gpu_cluster:\n                cluster.factories = [get_instance(instance.GPUFactoryInstance) for _ in range(cluster.factories_num)]\n            else:\n                cluster.factories = [get_instance(instance.CPUFactoryInstance) for _ in range(cluster.factories_num)]\n\n        return cluster\n\n    return _get_cluster\n\n\ndef test_empty_cluster(get_cluster):\n    c = get_cluster(fill=False)\n    with pytest.raises(man_errors.ClusterError):\n        c.get_orch_home_path()\n\n\ndef test_empty_cluster_2(get_cluster):\n    c = get_cluster(fill=False)\n    with pytest.raises(man_errors.ClusterError):\n        c.execute(DummyRunnable(), {}, """", False)\n\n\n@mock.patch(\'flambe.cluster.instance.instance.Instance.get_home_path\')\ndef test_orch_home_path(mock_get_home_path, get_cluster):\n    mock_get_home_path.return_value = ""/path/to/home""\n    c = get_cluster()\n\n    assert c.get_orch_home_path() == ""/path/to/home""\n\n\n@mock.patch(\'flambe.cluster.instance.instance.Instance.send_rsync\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.get_home_path\')\ndef test_send_secrets(mock_get_home_path, mock_rsync, get_cluster):\n    mock_get_home_path.return_value = ""/path/to/home""\n\n    c = get_cluster()\n    c.send_secrets()\n    args, kwargs = mock_rsync.call_args\n\n    assert ""/path/to/home/secret.ini"" == args[-1]\n\n\n@mock.patch(\'flambe.cluster.instance.instance.OrchestratorInstance.rsync_folder\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.get_home_path\')\ndef test_rsync_orch(mock_get_home_path, mock_rsync, get_cluster):\n    mock_get_home_path.return_value = ""/path/to/home""\n\n    c = get_cluster()\n    c.rsync_orch(""/some/folder"")\n    assert mock_rsync.call_count == c.factories_num\n\n    mock_rsync.assert_called_with(""/some/folder"", f\'{c.username}@{c.factories[-1].private_host}:/some/folder\')\n\n\n@mock.patch(\'flambe.cluster.instance.instance.OrchestratorInstance.launch_flambe\')\n@mock.patch(\'flambe.cluster.instance.instance.OrchestratorInstance.send_rsync\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.get_home_path\')\ndef test_execute(mock_get_home_path, mock_rsync, mock_launch, get_cluster):\n    mock_get_home_path.return_value = ""/path/to/home""\n    c = get_cluster()\n\n    c.execute(DummyRunnable(), {}, """", False)\n\n    args, kwargs = mock_rsync.call_args\n    assert ""/path/to/home/flambe.yaml"" == args[-1]\n\n    mock_launch.assert_called_once()\n    args, kwargs = mock_launch.call_args\n\n    assert ""/path/to/home/flambe.yaml"" == args[0]\n\n\ndef test_get_max_resources(get_cluster):\n    c = get_cluster(factories_num=2)\n\n    c.factories[0].num_cpus = mock.Mock(return_value=100)\n    c.factories[0].num_gpus = mock.Mock(return_value=10)\n\n    c.factories[-1].num_cpus = mock.Mock(return_value=10)\n    c.factories[-1].num_gpus = mock.Mock(return_value=100)\n\n    resources = c.get_max_resources()\n\n    assert resources[\'cpu\'] == 10\n    assert resources[\'gpu\'] == 10\n\n\n@mock.patch(\'flambe.cluster.instance.instance.Instance.get_home_path\')\ndef test_get_remote_env(mock_get_home_path, get_cluster):\n    mock_get_home_path.return_value = ""/path/to/home""\n    c = get_cluster()\n\n    env = c.get_remote_env(user_provider=lambda: \'foobar\')\n\n    assert len(env.factories_ips) == c.factories_num\n    for f in c.factories:\n        assert f.private_host in env.factories_ips\n\n    assert env.orchestrator_ip == c.orchestrator.private_host\n    assert env.key.startswith(""/path/to/home"")\n'"
tests/unit/cluster/test_instance.py,0,"b'from flambe.cluster import AWSCluster\n\nfrom moto import mock_ec2, mock_cloudwatch\n\nimport pytest\nimport boto3\nimport tempfile\nimport shutil\nimport sys\nfrom io import StringIO\nimport importlib\nimport copy\nimport configparser\n\nimport flambe\n\nimport mock\nfrom flambe.runnable import SafeExecutionContext, error\n\nfrom flambe.cluster import instance\nfrom flambe.cluster.utils import RemoteCommand\nfrom flambe.cluster.instance import errors\n\n\nclass MockSSHClient():\n\n    def __init__(self, status, success_msg, error_msg):\n        self.status = status\n        self.success_msg = success_msg\n        self.error_msg = error_msg\n\n    def exec_command(self, cmd):\n        stdout = mock.MagicMock()\n        stderr = mock.MagicMock()\n        stdout.channel.exit_status_ready = mock.MagicMock(return_value=True)\n\n        stdout.read = mock.MagicMock(return_value=self.success_msg)\n        stderr.read = mock.MagicMock(return_value=self.error_msg)\n\n        return self.status, stdout, stderr\n\n\n@pytest.fixture\ndef get_instance():\n\n    def _get_instance(_class=None, **kwargs):\n        host=kwargs.get(\'host\', \'1.2.3.4\')\n        private_host=kwargs.get(\'private_host\', \'10.0.3.4\')\n        username=kwargs.get(\'username\', \'ubuntu\')\n        key=kwargs.get(\'key\', \'/path/to/ssh/key\')\n        config=kwargs.get(\'config\', configparser.ConfigParser())\n        debug=kwargs.get(\'debug\', False)\n        use_public=kwargs.get(\'use_public\', True)\n        if _class:\n            return _class(host=host, private_host=private_host, username=username,\n                          key=key, config=config, debug=debug, use_public=use_public)\n\n        return instance.Instance(host=host, private_host=private_host, username=username,\n                                 key=key, config=config, debug=debug, use_public=use_public)\n    return _get_instance\n\n\n@mock.patch(\'flambe.cluster.instance.instance.Instance.is_up\')\ndef test_reachable_instance(mock_is_up, get_instance):\n    mock_is_up.return_value = True\n    ins = get_instance()\n    ins.wait_until_accessible()\n\n\n@mock.patch(\'flambe.cluster.instance.instance.Instance.is_up\')\n@mock.patch(\'flambe.cluster.const.RETRIES\', 1)\n@mock.patch(\'flambe.cluster.const.RETRY_DELAY\', 1)\ndef test_unreachable_instance(mock_is_up, get_instance):\n    mock_is_up.return_value = False\n    ins = get_instance()\n\n    with pytest.raises(ConnectionError):\n        ins.wait_until_accessible()\n    \n\n@mock.patch(\'flambe.cluster.instance.instance.Instance._get_cli\')\ndef test_run_cmd(mock_ssh_cli, get_instance):\n    mock_ssh_cli.return_value = MockSSHClient(0, b""success"", b"""")\n    ins = get_instance()\n\n    ret = ins._run_cmd(""ls"")\n    assert ret.success is True\n    assert ret.msg == b""success""\n    \n\n    mock_ssh_cli.return_value = MockSSHClient(123, b"""", b""error"")\n    ins = get_instance()\n\n    ret = ins._run_cmd(""ls"")\n    assert ret.success is False\n    assert ret.msg == b""error""\n    \n\n@mock.patch(\'flambe.cluster.instance.instance.Instance._get_cli\')\ndef test_run_cmd2(mock_ssh_cli, get_instance):\n    mock_ssh_cli.return_value = MockSSHClient(0, b""/home/ubuntu"", b"""")\n    ins = get_instance()\n\n    assert ins.get_home_path() == ""/home/ubuntu""\n\n\n@mock.patch(\'flambe.cluster.instance.instance.Instance._get_cli\')\ndef test_run_cmds(mock_ssh_cli, get_instance):\n    mock_ssh_cli.return_value = MockSSHClient(0, b""/home/ubuntu"", b"""")\n    ins = get_instance()\n\n    ins.run_cmds([\'ls\', \'ls\'])\n    \n\n@mock.patch(\'flambe.cluster.instance.instance.Instance._get_cli\')\ndef test_failure_run_cmds(mock_ssh_cli, get_instance):\n    mock_ssh_cli.return_value = MockSSHClient(1, b"""", b""error"")\n    ins = get_instance()\n\n    with pytest.raises(errors.RemoteCommandError):\n        ins.run_cmds([\'ls\', \'ls\'])\n\n\n@mock.patch(\'flambe.cluster.instance.instance.Instance._run_cmd\')\ndef test_install_flambe(mock_run_cmd, get_instance):\n    mock_run_cmd.return_value = RemoteCommand(True, b"""")\n    ins = get_instance(debug=False)\n\n    ins.install_flambe()\n\n    cmd = f\'python3 -m pip install --user --upgrade  flambe=={flambe.__version__}\'\n    mock_run_cmd.assert_called_with(cmd, retries=3)\n\n\n@mock.patch(\'flambe.cluster.instance.instance.Instance._run_cmd\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.contains_gpu\')\ndef test_install_flambe_gpu(mock_contains_gpu, mock_run_cmd, get_instance):\n    mock_contains_gpu.return_value = True\n    mock_run_cmd.return_value = RemoteCommand(True, b"""")\n    ins = get_instance(_class=instance.GPUFactoryInstance, debug=False)\n\n    ins.install_flambe()\n\n    cmd = f\'python3 -m pip install --user --upgrade  flambe[cuda]=={flambe.__version__}\'\n    mock_run_cmd.assert_called_with(cmd, retries=3)\n\n\n@mock.patch(\'flambe.cluster.instance.instance.get_flambe_repo_location\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.get_home_path\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.send_rsync\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance._run_cmd\')\n@mock.patch(\'os.path.exists\')\ndef test_install_flambe_debug(mock_os_exists, mock_run_cmd, mock_rsync, mock_get_home_path, mock_flambe_loc, get_instance):\n    mock_os_exists.return_value = False\n    mock_flambe_loc.return_value = ""/home/user/flambe""\n\n    mock_get_home_path.return_value = ""/home/ubuntu""\n    mock_run_cmd.return_value = RemoteCommand(True, b"""")\n\n    ins = get_instance(debug=True)\n\n    ins.install_flambe()\n\n    cmd = f\'python3 -m pip install --user --upgrade  /home/ubuntu/extensions/flambe\'\n    mock_run_cmd.assert_called_with(cmd, retries=3)\n\n    mock_flambe_loc.assert_called_once()\n\n    mock_get_home_path.assert_called_once()\n    mock_rsync.assert_called_once_with(\'/home/user/flambe\', \'/home/ubuntu/extensions/flambe\', params=[""--exclude=\'.*\'"", ""--exclude=\'docs/*\'""])\n\n\n@mock.patch(\'flambe.cluster.instance.instance.get_flambe_repo_location\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.get_home_path\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance.send_rsync\')\n@mock.patch(\'flambe.cluster.instance.instance.Instance._run_cmd\')\n@mock.patch(\'os.path.exists\')\ndef test_install_flambe_debug_2(mock_os_exists, mock_run_cmd, mock_rsync, mock_get_home_path, mock_flambe_loc, get_instance):\n    mock_os_exists.return_value = True\n    mock_flambe_loc.return_value = ""/home/user/flambe""\n\n    mock_get_home_path.return_value = ""/home/ubuntu""\n    mock_run_cmd.return_value = RemoteCommand(True, b"""")\n\n    ins = get_instance(debug=True)\n\n    ins.install_flambe()\n\n    cmd = f\'python3 -m pip install --user --upgrade  /home/ubuntu/extensions/flambe\'\n    mock_run_cmd.assert_called_with(cmd, retries=3)\n\n    mock_flambe_loc.assert_called_once()\n\n    mock_get_home_path.assert_called_once()\n    mock_rsync.assert_called_once_with(\'/home/user/flambe\', \'/home/ubuntu/extensions/flambe\', params=[""--exclude=\'.*\'"", ""--exclude=\'docs/*\'""])\n\n\n@mock.patch(\'flambe.cluster.instance.instance.Instance._run_cmd\')\ndef test_install_flambe_custom_pypi(mock_run_cmd, get_instance):\n    mock_run_cmd.return_value = RemoteCommand(True, b"""")\n\n    config = configparser.ConfigParser()\n    config.add_section(""PIP"")\n    config[\'PIP\'][\'HOST\'] = \'some_host\'\n    config[\'PIP\'][\'HOST_URL\'] = \'https://some_url\'\n\n    ins = get_instance(debug=False, config=config)\n\n    ins.install_flambe()\n\n    cmd = (\n        \'python3 -m pip install --user --upgrade --trusted-host \' +\n        \'some_host --extra-index-url https://some_url \' +\n        f\'flambe=={flambe.__version__}\'\n    )\n    mock_run_cmd.assert_called_with(cmd, retries=3)\n'"
tests/unit/compile/__init__.py,0,b''
tests/unit/compile/test_compilable.py,0,"b'import pytest\n\nfrom flambe import Component\nfrom flambe.compile import yaml\nfrom flambe.compile.component import MalformedLinkError, parse_link_str, create_link_str\nfrom ruamel.yaml.compat import StringIO\n\n\n@pytest.fixture\ndef make_classes():\n\n    class A(Component):\n\n        def __init__(self, akw1=0, akw2=None):\n            self.akw1 = akw1\n            self.akw2 = akw2\n\n    class B(Component):\n\n        def __init__(self, bkw1=0, bkw2=\'\'):\n            self.bkw1 = bkw1\n            self.bkw2 = bkw2\n\n    return A, B\n\n\n@pytest.fixture\ndef make_classes_2():\n\n    class A(Component):\n\n        def __init__(self, akw1=0, akw2=None):\n            self.akw1 = akw1\n            self.akw2 = akw2\n\n    class B(Component):\n\n        def __init__(self, bkw1=0, bkw2=\'\', bkw3=99):\n            self.bkw1 = bkw1\n            self.bkw2 = bkw2\n            self.bkw3 = bkw3\n\n    return A, B\n\n\n@pytest.fixture\ndef make_instances():\n    class A(Component):\n        pass\n\n    def _factory():\n        config = ""!A {}\\n""\n        return yaml.load(config)()\n\n    return _factory\n\n\ndef test_component_basic(make_classes):\n    A, B = make_classes\n\n    txt = """"""\ntop: !A\n  akw1: 8\n  akw2: !B\n    bkw1: 1\n    bkw2: \'test\'\n""""""\n\n    config = yaml.load(txt)\n    a = config[\'top\']()\n    assert a.akw1 == 8\n    assert a.akw2 is not None\n    assert a.akw2.bkw1 == 1\n\n\ndef test_component_basic_top_level(make_classes):\n    A, B = make_classes\n\n    txt = """"""\n!A\nakw1: 8\nakw2: !B\n  bkw1: 1\n  bkw2: \'test\'\n""""""\n    a_schema = yaml.load(txt)\n    a = a_schema()\n    assert a.akw1 == 8\n    assert a.akw2 is not None\n    assert a.akw2.bkw1 == 1\n\n\ndef test_component_schema_dict_access(make_classes):\n    A, B = make_classes\n\n    txt = """"""\n!A\nakw1: 8\nakw2: !B\n  bkw1: 1\n  bkw2: \'test\'\n""""""\n    a_schema = yaml.load(txt)\n    assert a_schema[\'akw1\'] == 8\n    assert a_schema[\'akw2\'][\'bkw2\'] == \'test\'\n    a_schema[\'akw2\'][\'bkw1\'] = 13\n    assert a_schema[\'akw2\'][\'bkw1\'] == 13\n    a_schema.keywords[\'akw2\'].keywords[\'bkw1\'] = 14\n    a = a_schema()\n    assert a.akw2.bkw1 == 14\n\n\ndef test_component_override(make_classes):\n    A, B = make_classes\n\n    txt = """"""\n!A\nakw1: 8\nakw2: !B\n  bkw1: 1\n  bkw2: \'test\'\n""""""\n    a_schema = yaml.load(txt)\n    a = a_schema(akw1=9)\n    assert a.akw1 == 9\n\n\ndef test_component_dumping_with_defaults_and_comments(make_classes_2):\n    A, B = make_classes_2\n\n    txt = """"""!A\nakw1: 8\n# Comment Here\nakw2: !B\n  bkw1: 1\n  bkw2: !!str test\n""""""\n    txt_expected = """"""!A\nakw1: 8\nakw2: !B\n  bkw1: 1\n  bkw2: test\n  bkw3: 99\n""""""\n    a_schema = yaml.load(txt)\n    a = a_schema()\n    with StringIO() as stream:\n        yaml.dump(a, stream)\n        assert txt_expected == stream.getvalue()\n\n\ndef test_component_dumping_factory(make_instances):\n    a = make_instances()\n    config = ""!A {}\\n""\n    with StringIO() as stream:\n        yaml.dump(a, stream)\n        assert config == stream.getvalue()\n\n\ndef test_component_dumping_made_in_code(make_classes_2):\n    A, B = make_classes_2\n\n    txt_expected = """"""!A\nakw1: 8\nakw2: !B\n  bkw1: 1\n  bkw2: test\n  bkw3: 99\n""""""\n    b_custom = B.compile(bkw1=1, bkw2=\'test\')\n    a_custom = A.compile(akw1=8, akw2=b_custom)\n    with StringIO() as stream:\n        yaml.dump(a_custom, stream)\n        assert txt_expected == stream.getvalue()\n\n\ndef test_component_anchors_compile_to_same_instance(make_classes_2):\n\n    txt = """"""\none: !A\n  akw2: &theb !B\n    bkw2: test\n    bkw1: 1\n  akw1: 8\ntwo: !A\n  akw1: 8\n  # Comment Here\n  akw2: *theb\n""""""\n    config = yaml.load(txt)\n    a1 = config[""one""]()\n    a1.akw2.bkw1 = 6\n    a2 = config[""two""]()\n    assert a1.akw2 is a2.akw2\n    assert a1.akw2.bkw1 == a2.akw2.bkw1\n\n\ndef test_component_linking():\n    # TODO test links\n    pass\n\n\ndef test_component_dynamic():\n    # TODO test dynamically created Components\n    pass\n\n\ndef test_state_dict_basic(make_classes):\n#     A, B = make_classes\n#\n#     txt = """"""\n# top: !A\n#   akw1: 8\n#   akw2: !B\n#     bkw1: 1\n#     bkw2: \'test\'\n# """"""\n#     config = yaml.load(txt)\n#     a = config[\'top\']()\n#     s = a.state_dict()\n#     print(s)\n#     assert False\n    pass\n\n\ndef test_state_dict_roundtrip():\n    pass\n\n\ndef test_state_dict_basic_pytorch():\n    pass\n\n\ndef test_state_dict_roundtrip_original_source():\n    pass\n\n\ndef test_state_dict_roundtrip_new_source():\n    pass\n\n\ndef test_save_basic():\n    pass\n\n\ndef test_load_basic():\n    pass\n\n\ndef test_load_save_roundtrip():\n    pass\n\n\nclass TestLinkParser:\n\n    def test_only_obj(self):\n        link = \'model\'\n        assert parse_link_str(link) == ([\'model\'], [])\n\n    def test_only_attr(self):\n        link = \'model.emb\'\n        assert parse_link_str(link) == ([\'model\'], [\'emb\'])\n        link = \'model.emb.enc\'\n        assert parse_link_str(link) == ([\'model\'], [\'emb\', \'enc\'])\n\n    def test_only_schematic(self):\n        link = \'model[emb]\'\n        assert parse_link_str(link) == ([\'model\', \'emb\'], [])\n        link = \'model[emb][enc]\'\n        assert parse_link_str(link) == ([\'model\', \'emb\', \'enc\'], [])\n\n    def test_schematic_and_attr(self):\n        link = \'model[emb].attr1\'\n        assert parse_link_str(link) == ([\'model\', \'emb\'], [\'attr1\'])\n        link = \'model[emb][enc].attr1.attr2\'\n        assert parse_link_str(link) == ([\'model\', \'emb\', \'enc\'], [\'attr1\', \'attr2\'])\n\n    def test_close_unopen_schematic(self):\n        with pytest.raises(MalformedLinkError):\n            link = \'modelemb][enc].attr1.attr2\'\n            parse_link_str(link)\n\n    def test_close_unopen_schematic_2(self):\n        with pytest.raises(MalformedLinkError):\n            link = \'model[emb]enc].attr1.attr2\'\n            parse_link_str(link)\n\n    def test_reopen_schematic(self):\n        with pytest.raises(MalformedLinkError):\n            link = \'model[emb[enc].attr1.attr2\'\n            parse_link_str(link)\n\n    def test_attr_without_dot(self):\n        with pytest.raises(MalformedLinkError):\n            link = \'model[emb][enc]attr1.attr2\'\n            parse_link_str(link)\n\n    def test_no_root_obj(self):\n        with pytest.raises(MalformedLinkError):\n            link = \'[emb]\'\n            parse_link_str(link)\n        with pytest.raises(MalformedLinkError):\n            link = \'.attr2\'\n            parse_link_str(link)\n        with pytest.raises(MalformedLinkError):\n            link = \'[emb][enc].attr1.attr2\'\n            parse_link_str(link)\n\n\nclass TestLinkCreator:\n\n    def test_only_obj(self):\n        link = ([\'model\'], [])\n        assert create_link_str(*link) == \'model\'\n\n    def test_only_attr(self):\n        link = ([\'model\'], [\'emb\'])\n        assert create_link_str(*link) == \'model.emb\'\n        link = ([\'model\'], [\'emb\', \'enc\'])\n        assert create_link_str(*link) == \'model.emb.enc\'\n\n    def test_only_schematic(self):\n        link = ([\'model\', \'emb\'], [])\n        assert create_link_str(*link) == \'model[emb]\'\n        link = ([\'model\', \'emb\', \'enc\'], [])\n        assert create_link_str(*link) == \'model[emb][enc]\'\n\n    def test_schematic_and_attr(self):\n        link = ([\'model\', \'emb\'], [\'attr1\'])\n        assert create_link_str(*link) == \'model[emb].attr1\'\n        link = ([\'model\', \'emb\', \'enc\'], [\'attr1\', \'attr2\'])\n        assert create_link_str(*link) == \'model[emb][enc].attr1.attr2\'\n\n\nclass TestSchema:\n\n    def test_contains(self, make_classes):\n        A, B = make_classes\n\n        txt = """"""\ntop: !A\n  akw1: 8\n  akw2: !B\n    bkw1: 1\n    bkw2: \'test\'\n""""""\n        schema_a = yaml.load(txt)[\'top\']\n        assert schema_a.contains(schema_a, original_link=None)[0]\n        present, updated_path = schema_a.contains(schema_a.akw2, original_link=None)\n        print(f""present: {present}, updated_path: {updated_path}"")\n        assert present\n        assert updated_path == [\'akw2\']\n        assert not schema_a.akw2.contains(schema_a, original_link=None)[0]\n'"
tests/unit/compile/test_downloader.py,0,"b'from moto import mock_s3\n\nimport pytest\nimport mock\nimport os\nimport boto3\nimport responses\nimport tempfile\n\nfrom flambe.compile import downloader\nfrom urllib.parse import urlparse\n\n# This fixture will run for each test method automatically\n@pytest.fixture(scope=\'function\', autouse=True)\ndef s3_mock():\n    mock = mock_s3()\n    mock.start()\n\n    yield\n\n    mock.stop()\n\n\ndef test_s3_file():\n    s3 = boto3.client(\'s3\', region_name=\'us-east-1\')\n    s3.create_bucket(Bucket=\'mybucket\')\n\n    s3.put_object(Bucket=\'mybucket\', Key=""some_file.txt"", Body=""CONTENT"")\n    s3.put_object(Bucket=\'mybucket\', Key=""some_folder/some_file.txt"", Body=""CONTENT"")\n\n    assert downloader.s3_remote_file(urlparse(""s3://mybucket/some_file.txt"")) is True\n    assert downloader.s3_remote_file(urlparse(""s3://mybucket/some_folder"")) is False\n    assert downloader.s3_remote_file(urlparse(""s3://mybucket/some_folder/"")) is False\n\n\ndef test_s3_existing():\n    s3 = boto3.client(\'s3\', region_name=\'us-east-1\')\n    s3.create_bucket(Bucket=\'mybucket\')\n\n    s3.put_object(Bucket=\'mybucket\', Key=""some_file.txt"", Body=""CONTENT"")\n\n    assert downloader.s3_exists(urlparse(""s3://mybucket/some_file.txt"")) is True\n    assert downloader.s3_exists(urlparse(""s3://mybucket/some_other_file.txt"")) is False\n    assert downloader.s3_exists(urlparse(""s3://other/some_file.txt"")) is False\n    assert downloader.s3_exists(urlparse(""s3://some_file.txt"")) is False\n\n    s3.put_object(Bucket=\'mybucket\', Key=""some_folder/some_file.txt"", Body=""CONTENT"")\n    assert downloader.s3_exists(urlparse(""s3://mybucket/some_folder/some_file.txt"")) is True\n    assert downloader.s3_exists(urlparse(""s3://mybucket/some_folder/"")) is True\n\n\ndef test_edge_cases():\n    with pytest.raises(TypeError):\n        with downloader.download_manager(None) as _:\n            pass\n\n    with pytest.raises(ValueError):\n        with downloader.download_manager(\'\') as _:\n            pass\n\n\ndef test_local_file():\n    path = __file__\n    with downloader.download_manager(path) as p:\n        assert path == p\n\n\n@mock.patch(\'subprocess.check_output\')\ndef test_s3_file_given_folder(mock_check_output):\n    s3 = boto3.client(\'s3\', region_name=\'us-east-1\')\n    s3.create_bucket(Bucket=\'mybucket\')\n\n    s3.put_object(Bucket=\'mybucket\', Key=""some_file.txt"", Body=""CONTENT"")\n\n    with tempfile.TemporaryDirectory() as t:\n        destination = os.path.join(t, ""some_file.txt"")\n        with downloader.download_manager(""s3://mybucket/some_file.txt"", destination) as p:\n            assert p == destination\n\n    s3.put_object(Bucket=\'mybucket\', Key=""some_folder/some_other_file.txt"", Body=""CONTENT"")\n\n    with tempfile.TemporaryDirectory() as t:\n        destination = os.path.join(t, ""some_other_file.txt"")\n        with downloader.download_manager(""s3://mybucket/some_folder/some_other_file.txt"", destination) as p:\n            assert p == destination\n\n    with tempfile.TemporaryDirectory() as t:\n        destination = os.path.join(t, ""some_folder"")\n        with downloader.download_manager(""s3://mybucket/some_folder"", destination) as p:\n            assert p == destination\n\n    with tempfile.TemporaryDirectory() as t:\n        destination = os.path.join(t, ""some_folder"")\n        with downloader.download_manager(""s3://mybucket/some_folder/"", destination) as p:\n            assert p == destination\n\n\ndef test_invalid_local_file():\n    path = ""/some/unexistent/path/!@#$%RVMCDOCMSxxxxoemdow""\n\n    if not os.path.exists(path):\n        with pytest.raises(ValueError) as excinfo:\n            with downloader.download_manager(path) as p:\n                assert path == p\n\n        assert \'does not exist locally.\' in str(excinfo.value)\n\n\ndef test_invalid_protocol():\n    with pytest.raises(ValueError) as excinfo:\n        with downloader.download_manager(""sftp://something"") as p:\n            _ = p\n    assert \'Only S3 and http/https URLs are supported.\' in str(excinfo.value)\n\n\ndef test_s3_inexistent_path():\n    path = ""s3://inexistent_bucket/file.txt""\n    with pytest.raises(ValueError) as excinfo:\n        with downloader.download_manager(path) as p:\n            _ = p\n\n    assert f""S3 url: \'{path}\' is not available"" in str(excinfo.value)\n\n\n@responses.activate\ndef test_http_exists():\n    url = \'https://some_host.com/resource.zip\'\n    responses.add(responses.HEAD, url, status=200)\n\n    assert downloader.http_exists(url) is True\n    assert downloader.http_exists(""https://some_other_host.com/inexistent"") is False\n\n\n@responses.activate\ndef test_http_exists_2():\n    url = \'https://some_host.com/resource.zip\'\n    responses.add(responses.HEAD, url, status=404)\n\n    assert downloader.http_exists(url) is False\n\n\n@responses.activate\ndef test_http_download():\n    url = \'https://some_host.com/resource.txt\'\n    responses.add(responses.GET, url,\n                  body=\'CONTENT\')\n\n    with tempfile.NamedTemporaryFile(\'wb\') as f:\n        downloader.download_http_file(url, f.name)\n        assert open(f.name, \'rb\').read() == b\'CONTENT\'\n'"
tests/unit/compile/test_extensions.py,0,"b'import pytest\n\nfrom flambe.compile import extensions as exts\n\n\ndef test_download_extensions():\n    extensions = {\n            \'ext\': \'./local/file\',\n            \'ext1\': \'other/local/file\',\n            \'ext2\': \'other\',\n            \'ext2\': \'pypi==1.12.1\',\n    }\n\n    ret = exts.download_extensions(extensions, None)\n    for k, v in extensions.items():\n        assert k in ret\n        assert v == ret[k]\n\n\ndef test_is_installed_module():\n    assert exts.is_installed_module(""pytest"") is True\n    assert exts.is_installed_module(""some_inexistent_package_0987654321"") is False\n'"
tests/unit/compile/test_registrable.py,0,"b'import pytest\n\nfrom flambe.compile import yaml, Registrable, alias, register, registrable_factory, \\\n                                registration_context\nfrom ruamel.yaml.compat import StringIO\n\n\n@pytest.fixture\ndef make_classes():\n\n    class A(Registrable):\n\n        def __init__(self, akw1=0, akw2=None):\n            self.akw1 = akw1\n            self.akw2 = akw2\n\n        @registrable_factory\n        @classmethod\n        def some_factory(cls, akw1=0, akw2=None):\n            return cls(akw1, akw2)\n\n        @classmethod\n        def from_yaml(cls, constructor, node, factory_name):\n            kwargs, = list(constructor.construct_yaml_map(node))\n            if factory_name is not None:\n                return getattr(cls, factory_name)(**kwargs)\n            else:\n                return cls(**kwargs)\n\n        @classmethod\n        def to_yaml(cls, representer, node, tag):\n            return representer.represent_mapping(tag, {""akw1"": node.akw1, ""akw2"": node.akw2})\n\n    class B(Registrable):\n\n        def __init__(self, bkw1=0, bkw2=\'\'):\n            self.bkw1 = bkw1\n            self.bkw2 = bkw2\n\n        @classmethod\n        def from_yaml(cls, constructor, node, factory_name):\n            kwargs, = list(constructor.construct_yaml_map(node))\n            return cls(**kwargs)\n\n        @classmethod\n        def to_yaml(cls, representer, node, tag):\n            return representer.represent_mapping(tag, {""bkw1"": node.bkw1, ""bkw2"": node.bkw2})\n\n    return A, B\n\n\n@pytest.fixture\ndef make_namespace_classes():\n\n    with registration_context(""ns""):\n        class A(Registrable):\n\n            def __init__(self, akw1=0, akw2=None):\n                self.akw1 = akw1\n                self.akw2 = akw2\n\n            @registrable_factory\n            @classmethod\n            def some_factory(cls, akw1=0, akw2=None):\n                return cls(akw1, akw2)\n\n            @classmethod\n            def from_yaml(cls, constructor, node, factory_name):\n                kwargs, = list(constructor.construct_yaml_map(node))\n                if factory_name is not None:\n                    return getattr(cls, factory_name)(**kwargs)\n                else:\n                    return cls(**kwargs)\n\n            @classmethod\n            def to_yaml(cls, representer, node, tag):\n                return representer.represent_mapping(tag, {""akw1"": node.akw1, ""akw2"": node.akw2})\n\n        class B(Registrable):\n\n            def __init__(self, bkw1=0, bkw2=\'\'):\n                self.bkw1 = bkw1\n                self.bkw2 = bkw2\n\n            @classmethod\n            def from_yaml(cls, constructor, node, factory_name):\n                kwargs, = list(constructor.construct_yaml_map(node))\n                return cls(**kwargs)\n\n            @classmethod\n            def to_yaml(cls, representer, node, tag):\n                return representer.represent_mapping(tag, {""bkw1"": node.bkw1, ""bkw2"": node.bkw2})\n\n    return A, B\n\n\n@pytest.fixture\ndef make_aliased_classes():\n\n    @alias(\'a_class\')\n    class A(Registrable):\n\n        def __init__(self, akw1=0, akw2=None):\n            self.akw1 = akw1\n            self.akw2 = akw2\n\n        @registrable_factory\n        @classmethod\n        def some_factory(cls, akw1=0, akw2=None):\n            return cls(akw1, akw2)\n\n        @classmethod\n        def from_yaml(cls, constructor, node, factory_name):\n            kwargs, = list(constructor.construct_yaml_map(node))\n            if factory_name is not None:\n                return getattr(cls, factory_name)(**kwargs)\n            else:\n                return cls(**kwargs)\n\n        @classmethod\n        def to_yaml(cls, representer, node, tag):\n            return representer.represent_mapping(tag, {""akw1"": node.akw1, ""akw2"": node.akw2})\n\n    @alias(\'b_class\')\n    @alias(\'b_\')\n    class B(Registrable):\n\n        def __init__(self, bkw1=0, bkw2=\'\'):\n            self.bkw1 = bkw1\n            self.bkw2 = bkw2\n\n        @classmethod\n        def from_yaml(cls, constructor, node, factory_name):\n            kwargs, = list(constructor.construct_yaml_map(node))\n            return cls(**kwargs)\n\n        @classmethod\n        def to_yaml(cls, representer, node, tag):\n            return representer.represent_mapping(tag, {""bkw1"": node.bkw1, ""bkw2"": node.bkw2})\n\n    return A, B\n\n\n@pytest.fixture\ndef make_new_classes():\n\n    class A:\n\n        def __init__(self, akw1=0, akw2=None):\n            self.akw1 = akw1\n            self.akw2 = akw2\n\n        @classmethod\n        def from_yaml(cls, constructor, node, factory_name):\n            kwargs, = list(constructor.construct_yaml_map(node))\n            return cls(**kwargs)\n\n        @classmethod\n        def to_yaml(cls, representer, node, tag):\n            return representer.represent_mapping(tag, {""akw1"": node.akw1, ""akw2"": node.akw2})\n\n    class B:\n\n        def __init__(self, bkw1=0, bkw2=\'\'):\n            self.bkw1 = bkw1\n            self.bkw2 = bkw2\n\n        @classmethod\n        def from_yaml(cls, constructor, node, factory_name):\n            kwargs, = list(constructor.construct_yaml_map(node))\n            return cls(**kwargs)\n\n        @classmethod\n        def to_yaml(cls, representer, node, tag):\n            return representer.represent_mapping(tag, {""bkw1"": node.bkw1, ""bkw2"": node.bkw2})\n\n    register(A, \'a_class\')\n    register(B, \'b_class\')\n    return A, B\n\n\ndef test_registrable_load_basic(make_classes):\n    A, B = make_classes\n\n    txt = """"""a: !A\n  akw1: 8\n  akw2: !B\n    bkw1: 2\n    bkw2: hello world\n""""""\n    config = yaml.load(txt)\n    a = config[\'a\']\n    assert a.akw1 == 8\n    assert a.akw2 is not None\n    assert hasattr(a.akw2, ""bkw1"")\n    assert a.akw2.bkw1 == 2\n\n\ndef test_registrable_load_context(make_namespace_classes):\n    A, B = make_namespace_classes\n\n    txt = """"""a: !ns.A\n  akw1: 8\n  akw2: !ns.B\n    bkw1: 2\n    bkw2: hello world\n""""""\n    config = yaml.load(txt)\n    a = config[\'a\']\n    assert a.akw1 == 8\n    assert a.akw2 is not None\n    assert hasattr(a.akw2, ""bkw1"")\n    assert a.akw2.bkw1 == 2\n\n\ndef test_registrable_dump_basic(make_classes):\n    A, B = make_classes\n\n    txt = """"""!A\nakw1: 8\nakw2: !B\n  bkw1: 2\n  bkw2: hello world\n""""""\n\n    b = B(2, ""hello world"")\n    a = A(8, b)\n    with StringIO() as s:\n        yaml.dump(a, s)\n        assert s.getvalue() == txt\n\n\ndef test_registrable_roundtrip(make_classes):\n    A, B = make_classes\n\n    txt = """"""a: !A\n  akw1: 8\n  akw2: !B\n    bkw1: 2\n    bkw2: hello world\n""""""\n    config = yaml.load(txt)\n    with StringIO() as s:\n        yaml.dump(config, s)\n        assert s.getvalue() == txt\n\n\ndef test_registrable_load_alias(make_aliased_classes):\n    A, B = make_aliased_classes\n\n    txt = """"""a: !a_class\n  akw1: 8\n  akw2: !b_class\n    bkw1: 2\n    bkw2: hello world\n""""""\n    config = yaml.load(txt)\n    a = config[\'a\']\n    assert a.akw1 == 8\n    assert a.akw2 is not None\n    assert hasattr(a.akw2, ""bkw1"")\n    assert a.akw2.bkw1 == 2\n\n\ndef test_registrable_dump_alias(make_aliased_classes):\n    A, B = make_aliased_classes\n\n    txt = """"""!a_class\nakw1: 8\nakw2: !b_class\n  bkw1: 2\n  bkw2: hello world\n""""""\n\n    b = B(2, ""hello world"")\n    a = A(8, b)\n    with StringIO() as s:\n        yaml.dump(a, s)\n        assert s.getvalue() == txt\n\n\ndef test_registrable_roundtrip_alias_default(make_aliased_classes):\n    A, B = make_aliased_classes\n\n    txt = """"""a: !a_class\n  akw1: 8\n  akw2: !b_\n    bkw1: 2\n    bkw2: hello world\n""""""\n    txt_default_alias = """"""a: !a_class\n  akw1: 8\n  akw2: !b_\n    bkw1: 2\n    bkw2: hello world\n""""""\n    config = yaml.load(txt)\n    with StringIO() as s:\n        yaml.dump(config, s)\n        assert s.getvalue() == txt_default_alias\n\n\ndef test_registrable_load_new_class(make_new_classes):\n    A, B = make_new_classes\n\n    txt = """"""a: !a_class\n  akw1: 8\n  akw2: !b_class\n    bkw1: 2\n    bkw2: hello world\n""""""\n    config = yaml.load(txt)\n    a = config[\'a\']\n    assert a.akw1 == 8\n    assert a.akw2 is not None\n    assert hasattr(a.akw2, ""bkw1"")\n    assert a.akw2.bkw1 == 2\n\n\ndef test_registrable_dump_new_class(make_new_classes):\n    A, B = make_new_classes\n\n    txt = """"""!a_class\nakw1: 8\nakw2: !b_class\n  bkw1: 2\n  bkw2: hello world\n""""""\n\n    b = B(2, ""hello world"")\n    a = A(8, b)\n    with StringIO() as s:\n        yaml.dump(a, s)\n        assert s.getvalue() == txt\n\n\ndef test_registrable_roundtrip_new_default(make_new_classes):\n    A, B = make_new_classes\n\n    txt = """"""a: !a_class\n  akw1: 8\n  akw2: !b_\n    bkw1: 2\n    bkw2: hello world\n""""""\n    txt_default_alias = """"""a: !a_class\n  akw1: 8\n  akw2: !b_\n    bkw1: 2\n    bkw2: hello world\n""""""\n    config = yaml.load(txt)\n    with StringIO() as s:\n        yaml.dump(config, s)\n        assert s.getvalue() == txt_default_alias\n\n\ndef test_registrable_factory(make_classes):\n    A, B = make_classes\n\n    txt = """"""a: !A.some_factory\n  akw1: 8\n  akw2: !B\n    bkw1: 2\n    bkw2: hello world\n""""""\n    config = yaml.load(txt)\n    a = config[\'a\']\n    assert a.akw1 == 8\n    assert a.akw2 is not None\n    assert hasattr(a.akw2, ""bkw1"")\n    assert a.akw2.bkw1 == 2\n\n\ndef test_registrable_factory_roundtrip(make_classes):\n    A, B = make_classes\n\n    txt = """"""a: !A.some_factory\n  akw1: 8\n  akw2: !B\n    bkw1: 2\n    bkw2: hello world\n""""""\n    txt_default_alias = """"""a: !A.some_factory\n  akw1: 8\n  akw2: !B\n    bkw1: 2\n    bkw2: hello world\n""""""\n    config = yaml.load(txt)\n    with StringIO() as s:\n        yaml.dump(config, s)\n        assert s.getvalue() == txt_default_alias\n\n\ndef test_registrable_factory_roundtrip_alias(make_aliased_classes):\n    A, B = make_aliased_classes\n\n    txt = """"""a: !a_class.some_factory\n  akw1: 8\n  akw2: !b_\n    bkw1: 2\n    bkw2: hello world\n""""""\n    txt_default_alias = """"""a: !a_class.some_factory\n  akw1: 8\n  akw2: !b_\n    bkw1: 2\n    bkw2: hello world\n""""""\n    config = yaml.load(txt)\n    a = config[\'a\']\n    assert a.akw1 == 8\n    assert a.akw2 is not None\n    assert hasattr(a.akw2, ""bkw1"")\n    assert a.akw2.bkw1 == 2\n    assert isinstance(a, A)\n    with StringIO() as s:\n        yaml.dump(config, s)\n        assert s.getvalue() == txt_default_alias\n'"
tests/unit/compile/test_serialization.py,53,"b'import pytest\nimport tempfile\nfrom collections import abc, OrderedDict\nimport os\n\nimport torch\nimport tarfile\nimport dill\nimport mock\nfrom ruamel.yaml.compat import StringIO\nfrom ruamel.yaml import YAML\n\nfrom typing import Mapping, Any, Optional\n\n# from flambe.compile import yaml\nfrom flambe import Component, save_state_to_file, load_state_from_file, load, save\nfrom flambe.compile import Registrable, yaml, make_component, Schema\nfrom flambe.compile.serialization import _extract_prefix\n\n\nFLAMBE_SOURCE_KEY = \'_flambe_source\'\nFLAMBE_CLASS_KEY = \'_flambe_class\'\nFLAMBE_CONFIG_KEY = \'_flambe_config\'\nFLAMBE_DIRECTORIES_KEY = \'_flambe_directories\'\nKEEP_VARS_KEY = \'keep_vars\'\nVERSION_KEY = \'_flambe_version\'\n\n\ndef list_files(startpath):\n    for root, dirs, files in os.walk(startpath):\n        level = root.replace(startpath, \'\').count(os.sep)\n        indent = \' \' * 4 * (level)\n        print(\'{}{}/\'.format(indent, os.path.basename(root)))\n        subindent = \' \' * 4 * (level + 1)\n        for f in files:\n            print(\'{}{}\'.format(subindent, f))\n\n\ndef check_mapping_equivalence(x, y, exclude_config=False):\n    for key in x.keys():\n        if key == KEEP_VARS_KEY or key == \'version\':\n            continue\n        if key == FLAMBE_CONFIG_KEY and exclude_config:\n            continue\n        assert key in y\n        if isinstance(x[key], abc.Mapping):\n            check_mapping_equivalence(x[key], y[key], exclude_config=exclude_config)\n        elif isinstance(x[key], torch.Tensor):\n            assert isinstance(y[key], torch.Tensor)\n            torch.equal(x[key], y[key])\n        else:\n            assert x[key] == y[key]\n\n\nEXAMPLE_TRAINER_CONFIG = """"""\n!Trainer\ntrain_sampler: !BaseSampler\nval_sampler: !BaseSampler\ndataset: !TabularDataset\n  train: [[\'\']]\nmodel: !RNNEncoder\n  input_size: 300\n  rnn_type: lstm\n  n_layers: 2\n  hidden_size: 256\nloss_fn: !torch.NLLLoss\nmetric_fn: !Accuracy\noptimizer: !torch.Adam\n  params: []\nmax_steps: 2\niter_per_step: 2\n""""""\n\n\n@pytest.fixture\ndef make_classes_2():\n\n    class A(Component):\n\n        def __init__(self, akw1=0, akw2=None):\n            self.akw1 = akw1\n            self.akw2 = akw2\n\n    class B(Component):\n\n        def __init__(self, bkw1=0, bkw2=\'\', bkw3=99):\n            self.bkw1 = bkw1\n            self.bkw2 = bkw2\n            self.bkw3 = bkw3\n\n    class C(Component):\n\n        def __init__(self, one, two):\n            self.one = one\n            self.two = two\n\n    return A, B\n\n\nclass Basic(Component):\n    pass\n\n\nclass Composite(Component):\n    def __init__(self):\n        self.leaf = Basic()\n\n\nclass BasicStateful(Component):\n    def __init__(self, x):\n        self.x = x\n        self.register_attrs(\'x\')\n        # self.b = Basic()\n\n\nclass BasicStatefulTwo(Component, torch.nn.Module):\n    def __init__(self, y):\n        super().__init__()\n        self.y = y\n        self.register_attrs(\'y\')\n\n\nclass IntermediateTorch(Component, torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.leaf = Basic()\n\n# TODO fix usage for x\nclass IntermediateStatefulTorch(Component, torch.nn.Module):\n    def __init__(self, x):\n        super().__init__()\n        self.leaf = BasicStateful(x=x)\n        self.linear = torch.nn.Linear(2, 2)\n\n\nclass IntermediateTorchOnly(torch.nn.Module):\n    def __init__(self, component):\n        super().__init__()\n        self.child = component\n        self.linear = torch.nn.Linear(2, 2)\n\n\nclass RootTorch(Component):\n    def __init__(self, x):\n        super().__init__()\n        self.model = IntermediateStatefulTorch(x=x)\n        # self.linear = torch.nn.Linear(2, 2)\n        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()))\n        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 0.01)\n\n\nclass ComposableTorchStateful(Component, torch.nn.Module):\n    def __init__(self, a: Component, b: int, c: torch.nn.Module):\n        super().__init__()\n        self.child = a\n        self.other_data = b\n        self.linear = c\n        self.register_attrs(\'other_data\')\n\n\nclass ComposableTorchStatefulPrime(Component, torch.nn.Module):\n    def __init__(self, a: Component, b: int, c: torch.nn.Linear):\n        super().__init__()\n        self.child = a\n        self.other_data = b\n        self.linear = c\n\n    def _state(self, state_dict, prefix, local_metadata):\n        state_dict[prefix + \'other_data\'] = self.other_data\n        return state_dict\n\n    def _load_state(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n        assert prefix + \'other_data\' in state_dict\n        self.other_data = state_dict[prefix + \'other_data\']\n\n\nclass ComposableTorchStatefulTorchOnlyChild(Component, torch.nn.Module):\n    def __init__(self, a: Component, b: int, c: Component):\n        super().__init__()\n        self.child = a\n        self.other_data = b\n        self.torch_only = IntermediateTorchOnly(c)\n        self.register_attrs(\'other_data\')\n\n\nclass ComposableContainer(Component):\n    def __init__(self, item: Any):\n        self.item = item\n\n\nclass Org(Component):\n    def __init__(self, item: Any, extra):\n        self.item = item\n        self.torch_only_extra = extra\n\n\ndef create_factory(class_):\n    def _factory(from_config):\n        if from_config:\n            config = f""!{class_.__name__} {{}}\\n""\n            obj = yaml.load(config)()\n            return obj\n        else:\n            obj = class_()\n        return obj\n    return _factory\n\n\n@pytest.fixture\ndef basic_object():\n    return create_factory(Basic)\n\n\n@pytest.fixture\ndef nested_object():\n    return create_factory(Composite)\n\n\n@pytest.fixture\ndef basic_object_with_state():\n    def _factory(from_config, x=-1):\n        if from_config:\n            config = f""!BasicStateful\\nx: {x}\\n""\n            obj = yaml.load(config)()\n            return obj\n        else:\n            obj = BasicStateful(x=x)\n        return obj\n    return _factory\n\n\n@pytest.fixture\ndef alternating_nn_module_with_state():\n    def _factory(from_config, x=-1):\n        if from_config:\n            config = f""!RootTorch\\nx: {x}\\n""\n            obj = yaml.load(config)()\n            return obj\n        else:\n            obj = RootTorch(x=x)\n        return obj\n    return _factory\n\n\ndef schema_builder():\n    config = """"""\n!Basic\n""""""\n    obj = yaml.load(config)\n    return obj\n\n\ndef complex_builder(from_config, schema=False, x=-1):\n    if from_config:\n        config = """"""\n!ComposableTorchStateful\na: !ComposableTorchStateful\n  a: !ComposableTorchStateful\n    a: !BasicStateful\n      x: {}\n    b: 2021\n    c: !torch.Linear\n      in_features: 2\n      out_features: 2\n  b: 2022\n  c: !torch.Linear\n    in_features: 2\n    out_features: 2\nb: 2023\nc: !torch.Linear\n  in_features: 2\n  out_features: 2\n""""""\n        config.format(x)\n        obj = yaml.load(config)\n        if not schema:\n            obj = obj()\n        return obj\n    else:\n        a1 = BasicStateful(x=x)\n        b1 = 2021\n        c1 = torch.nn.Linear(2, 2)\n        a2 = ComposableTorchStateful(a1, b1, c1)\n        b2 = 2022\n        c2 = torch.nn.Linear(2, 2)\n        a3 = ComposableTorchStateful(a2, b2, c2)\n        b3 = 2023\n        c3 = torch.nn.Linear(2, 2)\n        obj = ComposableTorchStateful(a3, b3, c3)\n        return obj\n\n\ndef complex_builder_nontorch_root(from_config, schema=False, x=-1):\n    if from_config:\n        config = """"""\n!ComposableContainer\nitem:\n  !ComposableTorchStatefulPrime\n  a: !ComposableTorchStateful\n    a: !ComposableTorchStateful\n      a: !BasicStateful\n        x: {}\n      b: 2021\n      c: !torch.Linear\n        in_features: 2\n        out_features: 2\n    b: 2022\n    c: !torch.Linear\n      in_features: 2\n      out_features: 2\n  b: 2023\n  c: !torch.Linear\n    in_features: 2\n    out_features: 2\n""""""\n        config.format(x)\n        obj = yaml.load(config)\n        if not schema:\n            obj = obj()\n        return obj\n    else:\n        a1 = BasicStateful(x=x)\n        b1 = 2021\n        c1 = torch.nn.Linear(2, 2)\n        a2 = ComposableTorchStateful(a1, b1, c1)\n        b2 = 2022\n        c2 = torch.nn.Linear(2, 2)\n        a3 = ComposableTorchStateful(a2, b2, c2)\n        b3 = 2023\n        c3 = torch.nn.Linear(2, 2)\n        item = ComposableTorchStateful(a3, b3, c3)\n        obj = ComposableContainer(item)\n        return obj\n\n\n@pytest.fixture\ndef complex_multi_layered():\n    return complex_builder\n\n\n@pytest.fixture\ndef complex_multi_layered_nontorch_root():\n    return complex_builder_nontorch_root\n\n\n@pytest.fixture\ndef schema():\n    return schema_builder\n\nclass TestHelpers:\n\n    def test_extract_prefix(self):\n        _extract_prefix\n\n\nclass TestState:\n\n    def test_state_returns_not_None(self, basic_object):\n        obj = basic_object(from_config=True)\n        assert obj.get_state() is not None\n\n    def test_state_metadata(self, basic_object):\n        state = basic_object(from_config=True).get_state()\n        assert hasattr(state, \'_metadata\')\n        assert \'\' in state._metadata\n        assert FLAMBE_DIRECTORIES_KEY in state._metadata\n        assert FLAMBE_SOURCE_KEY in state._metadata[\'\']\n        assert VERSION_KEY in state._metadata[\'\']\n        assert state._metadata[\'\'][FLAMBE_SOURCE_KEY] == ""class Basic(Component):\\n    pass\\n""\n        # assert state[FLAMBE_CONFIG_KEY] == \'\'\n        assert \'\' in state._metadata[FLAMBE_DIRECTORIES_KEY] and len(state._metadata[FLAMBE_DIRECTORIES_KEY]) == 1\n        assert state._metadata[\'\'][VERSION_KEY] == \'0.0.0\'\n\n    def test_state_config(self, basic_object):\n        assert FLAMBE_CONFIG_KEY not in basic_object(from_config=False).get_state()._metadata[\'\']\n        obj = basic_object(from_config=True)\n        state = obj.get_state()\n        assert FLAMBE_CONFIG_KEY in state._metadata[\'\']\n        assert state._metadata[\'\'][FLAMBE_CONFIG_KEY] == ""!Basic {}\\n""\n\n    def test_state_nested_but_empty(self, nested_object):\n        expected_state = {}\n        expected_metadata = {\'\': {FLAMBE_SOURCE_KEY: ""class Composite(Component):\\n    def __init__(self):\\n        self.leaf = Basic()\\n"", VERSION_KEY: ""0.0.0"", FLAMBE_CLASS_KEY: \'Composite\'}, \'leaf\': {FLAMBE_SOURCE_KEY: \'class Basic(Component):\\n    pass\\n\', VERSION_KEY: \'0.0.0\', FLAMBE_CLASS_KEY: \'Basic\'}, FLAMBE_DIRECTORIES_KEY: {\'\', \'leaf\'}, KEEP_VARS_KEY: False}\n        obj = nested_object(from_config=False)\n        state = obj.get_state()\n        assert state == expected_state\n        check_mapping_equivalence(expected_metadata, state._metadata)\n        check_mapping_equivalence(state._metadata, expected_metadata)\n\n    def test_state_custom(self, basic_object_with_state):\n        obj = basic_object_with_state(from_config=True)\n        x = obj.x\n        expected_state = {\'x\': x}\n        assert obj.get_state() == expected_state\n\n    # def test_state_custom_nested(nested_object_with_state):\n    #     obj = nested_object_with_state()\n    #     expected_state = {}\n    #     assert obj.get_state() == expected_state\n    #\n    # def test_state_pytorch_empty(nn_modules):\n    #     cls, cls_torch_first = nn_modules\n    #     obj, obj_torch_first = cls(), cls_torch_first()\n    #     expected_state = {}\n    #     assert obj.get_state() == expected_state\n    #     assert obj_torch_first.get_state() == expected_state\n    #\n    # def test_state_pytorch_nested_no_modules_no_parameters(nested_nn_module):\n    #     obj = nested_nn_module()\n    #     expected_state = {}\n    #     assert obj.get_state() == expected_state\n    #\n    # def test_state_pytorch_alternating_nesting(alternating_nn_module):\n    #     obj = alternating_nn_module()\n    #     expected_state = {}\n    #     assert obj.get_state() == expected_state\n\n    def test_state_pytorch_alternating_nested_with_modules(self, alternating_nn_module_with_state):\n        obj = alternating_nn_module_with_state(from_config=True, x=1)\n        t1 = obj.model.linear.weight\n        t2 = obj.model.linear.bias\n        x = obj.model.leaf.x\n        expected_state = {\'model.leaf.x\': x, \'model.linear.weight\': t1, \'model.linear.bias\': t2}\n        root_source_code = dill.source.getsource(RootTorch)\n        intermediate_source_code = dill.source.getsource(IntermediateStatefulTorch)\n        leaf_source_code = dill.source.getsource(BasicStateful)\n        expected_metadata = OrderedDict({FLAMBE_DIRECTORIES_KEY: set([\'\', \'model\', \'model.leaf\']), \'keep_vars\': False, \'\': {VERSION_KEY: \'0.0.0\', FLAMBE_CLASS_KEY: \'RootTorch\', FLAMBE_SOURCE_KEY: root_source_code, FLAMBE_CONFIG_KEY: ""!RootTorch\\nx: 1\\n""},\n                                                                                             \'model\': {VERSION_KEY: \'0.0.0\', FLAMBE_CLASS_KEY: \'IntermediateStatefulTorch\', FLAMBE_SOURCE_KEY: intermediate_source_code, \'version\': 1},  # TODO add config back: FLAMBE_CONFIG_KEY: ""!IntermediateStatefulTorch {}\\n""\n                                                                                             \'model.leaf\': {VERSION_KEY: \'0.0.0\', FLAMBE_CLASS_KEY: \'BasicStateful\', FLAMBE_SOURCE_KEY: leaf_source_code},  # TODO add config back: FLAMBE_CONFIG_KEY: ""!BasicStateful {}\\n""\n                                                                                             \'model.linear\': {\'version\': 1}})\n        state = obj.get_state()\n        check_mapping_equivalence(state._metadata, expected_metadata)\n        check_mapping_equivalence(expected_metadata, state._metadata)\n        check_mapping_equivalence(state, expected_state)\n        check_mapping_equivalence(expected_state, state)\n\n\nclass TestLoadState:\n\n    def test_load_state_empty(self):\n        pass\n\n    def test_load_state_nested_empty(self):\n        pass\n\n    def test_load_state_custom_nested(self):\n        pass\n\n    def test_load_state_pytorch(self):\n        pass\n\n    def test_load_state_pytorch_alternating_nested(self):\n        pass\n\n    def test_state_complex_multilayered_nontorch_root(self, complex_multi_layered_nontorch_root):\n        TORCH_TAG_PREFIX = ""torch""\n        exclude = [\'torch.nn.quantized\', \'torch.nn.qat\']\n        make_component(\n            torch.nn.Module,\n            TORCH_TAG_PREFIX,\n            only_module=\'torch.nn\',\n            exclude=exclude\n        )\n\n        obj = complex_multi_layered_nontorch_root(from_config=True, x=1)\n        t1 = obj.item.child.linear.weight.data\n        state = obj.get_state()\n        new_obj = complex_multi_layered_nontorch_root(from_config=True, x=2)\n        new_obj.load_state(state)\n        t2 = new_obj.item.child.linear.weight.data\n        assert t1.equal(t2)\n        check_mapping_equivalence(new_obj.get_state(), obj.get_state())\n        check_mapping_equivalence(obj.get_state(), new_obj.get_state())\n\n    def test_custom_attrs_load(self, complex_multi_layered):\n        obj = complex_multi_layered(False)\n        state = obj.state_dict()\n        with pytest.raises(RuntimeError) as excinfo:\n            obj.load_state_dict(state, strict=True)\n        assert ""Unexpected key(s)"" in str(excinfo.value)\n        obj.load_state_dict(state, strict=False)\n\n\nclass TestClassSave:\n\n    def test_class_save(self):\n        pass\n\n\nclass TestClassLoad:\n\n    def test_class_load(self):\n        pass\n\n\nclass TestModuleSave:\n\n    def test_save_single_object(self, basic_object):\n        pass\n\n    def test_save_nested_object(self, nested_object):\n        pass\n\n    def test_save_pytorch_nested_alternating(self, alternating_nn_module_with_state):\n        pass\n\n\nclass TestModuleLoad:\n\n    def test_load_directory_single_file(self, basic_object):\n        pass\n\n    def test_load_nested_directory(self, nested_object):\n        pass\n\n    def test_load_pytorch_alternating(self, alternating_nn_module_with_state):\n        pass\n\n\nclass TestSerializationIntegration:\n\n    def test_state_and_load_roundtrip_single_object(self, basic_object):\n        old_obj = basic_object(from_config=True)\n        state = old_obj.get_state()\n        new_obj = basic_object(from_config=False)\n        new_obj.load_state(state, strict=False)\n        assert old_obj.get_state() == new_obj.get_state()\n\n    # def test_state_and_load_roundtrip_nested_object(self):\n    #     pass\n\n    def test_state_and_load_roundtrip_pytorch_alternating(self, alternating_nn_module_with_state):\n        old_obj = alternating_nn_module_with_state(from_config=True, x=1)\n        state = old_obj.get_state()\n        new_obj = alternating_nn_module_with_state(from_config=False, x=2)\n        new_obj.load_state(state, strict=False)\n        old_state = old_obj.get_state()\n        new_state = new_obj.get_state()\n        check_mapping_equivalence(new_state, old_state)\n        check_mapping_equivalence(old_state._metadata, new_state._metadata, exclude_config=True)\n\n    # def test_class_save_and_load_roundtrip():\n    #     pass\n    #\n    # def test_class_save_and_load_roundtrip_nested():\n    #     pass\n    #\n    # def test_class_save_and_load_roundtrip_pytorch():\n    #     pass\n\n    def test_save_to_file_and_load_from_file_roundtrip(self, basic_object):\n        old_obj = basic_object(from_config=True)\n        state = old_obj.get_state()\n        with tempfile.TemporaryDirectory() as path:\n            save_state_to_file(state, path)\n            state = load_state_from_file(path)\n        new_obj = basic_object(from_config=False)\n        new_obj.load_state(state, strict=False)\n        old_state = old_obj.get_state()\n        new_state = new_obj.get_state()\n        check_mapping_equivalence(new_state, old_state)\n        check_mapping_equivalence(old_state._metadata, new_state._metadata, exclude_config=True)\n\n    def test_save_to_file_and_load_from_file_roundtrip_pytorch(self, alternating_nn_module_with_state):\n        old_obj = alternating_nn_module_with_state(from_config=False, x=1)\n        state = old_obj.get_state()\n        with tempfile.TemporaryDirectory() as path:\n            save_state_to_file(state, path)\n            state = load_state_from_file(path)\n        new_obj = alternating_nn_module_with_state(from_config=False, x=2)\n        new_obj.load_state(state, strict=False)\n        old_state = old_obj.get_state()\n        new_state = new_obj.get_state()\n        check_mapping_equivalence(new_state, old_state)\n        check_mapping_equivalence(old_state._metadata, new_state._metadata, exclude_config=False)\n\n    def test_save_to_file_and_load_from_file_roundtrip_complex(self, complex_multi_layered):\n        TORCH_TAG_PREFIX = ""torch""\n        exclude = [\'torch.nn.quantized\', \'torch.nn.qat\']\n        make_component(\n            torch.nn.Module,\n            TORCH_TAG_PREFIX,\n            only_module=\'torch.nn\',\n            exclude=exclude\n        )\n        old_obj = complex_multi_layered(from_config=True, x=1)\n        # Test that the current state is actually saved, for a\n        # Component-only child of torch objects\n        old_obj.child.child.child.x = 24\n        state = old_obj.get_state()\n        with tempfile.TemporaryDirectory() as path:\n            save_state_to_file(state, path)\n            list_files(path)\n            state_loaded = load_state_from_file(path)\n            check_mapping_equivalence(state, state_loaded)\n            # assert False\n        new_obj = complex_multi_layered(from_config=True, x=2)\n        new_obj.load_state(state_loaded, strict=False)\n        old_state = old_obj.get_state()\n        new_state = new_obj.get_state()\n        check_mapping_equivalence(new_state, old_state)\n        check_mapping_equivalence(old_state._metadata, new_state._metadata, exclude_config=False)\n\n    @pytest.mark.parametrize(""pickle_only"", [True, False])\n    @pytest.mark.parametrize(""compress_save_file"", [True, False])\n    def test_save_to_file_and_load_from_file_roundtrip_complex_nontorch_root(self,\n            complex_multi_layered_nontorch_root, pickle_only, compress_save_file):\n        TORCH_TAG_PREFIX = ""torch""\n        exclude = [\'torch.nn.quantized\', \'torch.nn.qat\']\n        make_component(\n            torch.nn.Module,\n            TORCH_TAG_PREFIX,\n            only_module=\'torch.nn\',\n            exclude=exclude\n        )\n        old_obj = complex_multi_layered_nontorch_root(from_config=True, x=1)\n        state = old_obj.get_state()\n        with tempfile.TemporaryDirectory() as root_path:\n            path = os.path.join(root_path, \'savefile.flambe\')\n            save_state_to_file(state, path, compress_save_file, pickle_only)\n            list_files(path)\n            if pickle_only:\n                path += \'.pkl\'\n            if compress_save_file:\n                path += \'.tar.gz\'\n            state_loaded = load_state_from_file(path)\n            check_mapping_equivalence(state, state_loaded)\n            check_mapping_equivalence(state._metadata, state_loaded._metadata)\n        new_obj = complex_multi_layered_nontorch_root(from_config=True, x=2)\n        int_state = new_obj.get_state()\n        new_obj.load_state(state_loaded, strict=False)\n        old_state = old_obj.get_state()\n        new_state = new_obj.get_state()\n        check_mapping_equivalence(new_state, old_state)\n        check_mapping_equivalence(old_state._metadata, new_state._metadata)\n        check_mapping_equivalence(int_state._metadata, state_loaded._metadata)\n\n    @pytest.mark.parametrize(""pickle_only"", [True, False])\n    @pytest.mark.parametrize(""compress_save_file"", [True, False])\n    def test_module_save_and_load_roundtrip(self, basic_object, pickle_only, compress_save_file):\n        old_obj = basic_object(from_config=True)\n        with tempfile.TemporaryDirectory() as root_path:\n            path = os.path.join(root_path, \'savefile.flambe\')\n            save(old_obj, path, compress_save_file, pickle_only)\n            if pickle_only:\n                path += \'.pkl\'\n            if compress_save_file:\n                path += \'.tar.gz\'\n            new_obj = load(path)\n        old_state = old_obj.get_state()\n        new_state = new_obj.get_state()\n        check_mapping_equivalence(new_state, old_state)\n        check_mapping_equivalence(old_state._metadata, new_state._metadata, exclude_config=False)\n\n    @pytest.mark.parametrize(""compress_save_file"", [True, False])\n    @mock.patch(\'flambe.compile.utils.get_frozen_deps\')\n    def test_module_save_requirements_file(self, mock_freeze, compress_save_file, basic_object):\n        mock_freeze.return_value = [\'pkgA==1.2.3\', \'pkgB\']\n        old_obj = basic_object(from_config=True)\n        with tempfile.TemporaryDirectory() as root_path:\n            path = os.path.join(root_path, \'savefile.flambe\')\n            save(old_obj, path, compress=compress_save_file, pickle_only=False)\n            if compress_save_file:\n                with tarfile.open(f""{path}.tar.gz"", \'r:gz\') as tar_gz:\n                    tar_gz.extractall(path=root_path)\n\n            mock_freeze.assert_called_once()\n            assert os.path.exists(os.path.join(path, \'requirements.txt\'))\n\n            with open(os.path.join(path, \'requirements.txt\'), \'r\') as f:\n                assert f.read() == \'pkgA==1.2.3\\npkgB\'\n\n    @pytest.mark.parametrize(""pickle_only"", [True, False])\n    @pytest.mark.parametrize(""compress_save_file"", [True, False])\n    def test_module_save_and_load_roundtrip_pytorch(self,\n                                                    alternating_nn_module_with_state,\n                                                    pickle_only,\n                                                    compress_save_file):\n        old_obj = alternating_nn_module_with_state(from_config=True, x=1)\n        with tempfile.TemporaryDirectory() as root_path:\n            path = os.path.join(root_path, \'savefile.flambe\')\n            save(old_obj, path, compress_save_file, pickle_only)\n            if pickle_only:\n                path += \'.pkl\'\n            if compress_save_file:\n                path += \'.tar.gz\'\n            new_obj = load(path)\n        old_state = old_obj.get_state()\n        new_state = new_obj.get_state()\n        check_mapping_equivalence(new_state, old_state)\n        check_mapping_equivalence(old_state._metadata, new_state._metadata, exclude_config=False)\n\n    def test_module_save_and_load_roundtrip_pytorch_only_bridge(self):\n        a = BasicStateful.compile(x=3)\n        b = 100\n        c = BasicStatefulTwo.compile(y=0)\n        item = ComposableTorchStatefulTorchOnlyChild.compile(a=a, b=b, c=c)\n        extra = torch.nn.Linear(2, 2)\n        old_obj = Org.compile(item=item, extra=None)\n        # x for a2 should be different from instance a\n        a2 = BasicStateful.compile(x=4)\n        b2 = 101\n        # y for c2 should be different from instance c\n        c2 = BasicStatefulTwo.compile(y=1)\n        item2 = ComposableTorchStatefulTorchOnlyChild.compile(a=a2, b=b2, c=c2)\n        extra2 = torch.nn.Linear(2, 2)\n        new_obj = Org.compile(item=item2, extra=None)\n        with tempfile.TemporaryDirectory() as root_path:\n            path = os.path.join(root_path, \'asavefile2.flambe\')\n            old_state = old_obj.get_state()\n            save_state_to_file(old_state, path)\n            new_state = load_state_from_file(path)\n            new_obj.load_state(new_state)\n            # save(old_obj, path)\n            # new_obj = load(path)\n        old_state_get = old_obj.get_state()\n        new_state_get = new_obj.get_state()\n        check_mapping_equivalence(new_state, old_state)\n        check_mapping_equivalence(old_state._metadata, new_state._metadata, exclude_config=False)\n        check_mapping_equivalence(new_state_get, old_state_get)\n        check_mapping_equivalence(old_state_get._metadata, new_state_get._metadata, exclude_config=True)\n\n    # def test_module_save_and_load_example_encoder(self):\n    #     TORCH_TAG_PREFIX = ""torch""\n    #     make_component(torch.nn.Module, TORCH_TAG_PREFIX, only_module=\'torch.nn\')\n    #     make_component(torch.optim.Optimizer, TORCH_TAG_PREFIX, only_module=\'torch.optim\')\n    #     trainer = yaml.load(EXAMPLE_TRAINER_CONFIG)()\n    #     with tempfile.TemporaryDirectory() as path:\n    #         save(trainer, path)\n    #         loaded_trainer = load(path)\n    #     old_state = trainer.get_state()\n    #     new_state = loaded_trainer.get_state()\n    #     check_mapping_equivalence(new_state, old_state)\n    #     check_mapping_equivalence(old_state._metadata, new_state._metadata, exclude_config=False)\n\n    def test_module_save_and_load_single_instance_appears_twice(self, make_classes_2):\n        txt = """"""\n!C\none: !A\n  akw2: &theb !B\n    bkw2: test\n    bkw1: 1\n  akw1: 8\ntwo: !A\n  akw1: 8\n  # Comment Here\n  akw2: *theb\n""""""\n        c = yaml.load(txt)()\n        c.one.akw2.bkw1 = 6\n        assert c.one.akw2 is c.two.akw2\n        assert c.one.akw2.bkw1 == c.two.akw2.bkw1\n        with tempfile.TemporaryDirectory() as path:\n            save(c, path)\n            state = load_state_from_file(path)\n            loaded_c = load(path)\n        assert loaded_c.one.akw2 is loaded_c.two.akw2\n        assert loaded_c.one.akw2.bkw1 == loaded_c.two.akw2.bkw1\n\n\nclass TestSerializationExtensions:\n\n    EXTENSIONS = {\n        ""ext1"": ""my_extension_1"",\n        ""ext2"": ""my_extension_2"",\n        ""ext3"": ""my_extension_3"",\n    }\n\n    @pytest.mark.parametrize(""pickle_only"", [True, False])\n    @pytest.mark.parametrize(""compress_save_file"", [True, False])\n    @mock.patch(\'flambe.compile.serialization.is_installed_module\')\n    @mock.patch(\'flambe.compile.serialization.import_modules\')\n    @mock.patch(\'flambe.compile.component.Schema.add_extensions_metadata\')\n    def test_save_to_file_and_load_from_file_with_extensions(\n            self, mock_add_extensions,\n            mock_import_module, mock_installed_module,\n            compress_save_file, pickle_only, schema):\n        """"""Test that extensions are saved to the output config.yaml\n        and they are also added when loading back the object.""""""\n\n        mock_installed_module.return_value = True\n\n        schema_obj = schema()\n\n        # Add extensions manually because if we use add_extensions_metadata\n        # then no extensions will be added as the schema doesn\'t container_folder\n        # any prefix.\n        schema_obj._extensions = TestSerializationExtensions.EXTENSIONS\n\n        obj = schema_obj()\n        state = obj.get_state()\n\n        with tempfile.TemporaryDirectory() as root_path:\n            path = os.path.join(root_path, \'savefile.flambe\')\n            save_state_to_file(state, path, compress_save_file, pickle_only)\n\n            list_files(path)\n            if pickle_only:\n                path += \'.pkl\'\n            if compress_save_file:\n                path += \'.tar.gz\'\n            state_loaded = load_state_from_file(path)\n\n            check_mapping_equivalence(state, state_loaded)\n            check_mapping_equivalence(state._metadata, state_loaded._metadata)\n\n            _ = Basic.load_from_path(path)\n            mock_add_extensions.assert_called_once_with(TestSerializationExtensions.EXTENSIONS)\n\n\n    def test_add_extensions_metadata(self, schema):\n        """"""Test that add_extensions_metadata doesn\'t add extensions that are not used""""""\n\n        schema_obj = schema()\n        assert schema_obj._extensions == {}\n\n        schema_obj.add_extensions_metadata(TestSerializationExtensions.EXTENSIONS)\n        assert schema_obj._extensions == {}\n\n\n    def test_add_extensions_metadata_2(self):\n        """"""Test that add_extensions_metadata doesn\'t add extensions that are not used.\n\n        In this case we will use a config containing torch, but we will make_component\n        on torch so that it can be compiled. After that, we add_extensions_metadata with\n        torch, which is a valid extensions for the config (redundant, but valid).\n\n        """"""\n        TORCH_TAG_PREFIX = ""torch""\n        exclude = [\'torch.nn.quantized\', \'torch.nn.qat\']\n        make_component(\n            torch.nn.Module,\n            TORCH_TAG_PREFIX,\n            only_module=\'torch.nn\',\n            exclude=exclude\n        )\n\n        config = """"""\n        !torch.Linear\n          in_features: 2\n          out_features: 2\n        """"""\n\n        schema = yaml.load(config)\n        schema.add_extensions_metadata({""torch"": ""torch""})\n        assert schema._extensions == {""torch"": ""torch""}\n\n        mixed_ext = TestSerializationExtensions.EXTENSIONS.copy()\n        mixed_ext.update({""torch"": ""torch""})\n        schema.add_extensions_metadata(mixed_ext)\n        assert schema._extensions == {""torch"": ""torch""}\n\n\n    def test_add_extensions_metadata_3(self, complex_multi_layered_nontorch_root):\n        """"""Test that add_extensions_metadata doesn\'t add extensions that are not used\n\n        In this case we will use a config containing torch, but we will make_component\n        on torch so that it can be compiled. After that, we add_extensions_metadata with\n        torch, which is a valid extensions for the config (redundant, but valid).\n\n        """"""\n        TORCH_TAG_PREFIX = ""torch""\n        exclude = [\'torch.nn.quantized\', \'torch.nn.qat\']\n        make_component(\n            torch.nn.Module,\n            TORCH_TAG_PREFIX,\n            only_module=\'torch.nn\',\n            exclude=exclude\n        )\n\n        schema = complex_multi_layered_nontorch_root(from_config=True, schema=True)\n        schema.add_extensions_metadata({""torch"": ""torch""})\n\n        # This method asserts recursively that torch is added to extensions when\n        # there is a subcomponent that uses torch.\n        # It returns if at least one component with torch was found, that should\n        # always happen based on the complex_multi_layered_nontorch_root.\n        def helper(data):\n            found = False\n            if isinstance(data, Schema):\n                if data.component_subclass.__module__.startswith(""torch.""):\n                    found = True\n                    assert data._extensions == {""torch"": ""torch""}\n\n                for val in data.keywords.values():\n                    f = helper(val)\n                    if f:\n                        found = f\n\n            elif isinstance(data, Mapping):\n                for val in data.values():\n                    f = helper(val)\n                    if f:\n                        found = f\n            return found\n\n        assert helper(schema)\n'"
tests/unit/compile/test_utils.py,0,"b""import tempfile\nimport mock\n\n\nfrom flambe.compile.utils import write_deps\n\n\ndef test_write_deps():\n    dummy_dependencies = ['numpy==1.2.3', 'pip~=1.1.1', 'some_other-random dep']\n    with tempfile.NamedTemporaryFile() as tmpfile:\n        write_deps(tmpfile.name, dummy_dependencies)\n\n        assert tmpfile.read() == b'numpy==1.2.3\\npip~=1.1.1\\nsome_other-random dep'\n\n\n@mock.patch('flambe.compile.utils.get_frozen_deps')\ndef test_write_deps_default(mock_deps):\n    mock_deps.return_value = ['numpy==1.2.3', 'pip~=1.1.1', 'some_other-random dep']\n    with tempfile.NamedTemporaryFile() as tmpfile:\n        write_deps(tmpfile.name)\n        assert tmpfile.read() == b'numpy==1.2.3\\npip~=1.1.1\\nsome_other-random dep'\n        mock_deps.assert_called_once()\n"""
tests/unit/dataset/__init__.py,0,b''
tests/unit/dataset/test_tabular.py,3,"b'import pytest\nimport numpy as np\nimport torch\nfrom flambe.dataset import TabularDataset\nfrom flambe.field import Field, TextField, LabelField\n\n\n@pytest.fixture\ndef train_dataset():\n    """"""Dummy dataset from file""""""\n    return TabularDataset.from_path(\'tests/data/dummy_tabular/train.csv\', sep=\',\')\n\n\n@pytest.fixture\ndef train_dataset_no_header():\n    """"""Dummy dataset from file""""""\n    return TabularDataset.from_path(\'tests/data/no_header_dataset.csv\', sep=\',\',\n                                    header=None)\n\n\n@pytest.fixture\ndef train_dataset_reversed():\n    """"""Dummy dataset from file""""""\n    return TabularDataset.from_path(\'tests/data/dummy_tabular/train.csv\', sep=\',\',\n                                    columns=[\'label\', \'text\'])\n\n\n@pytest.fixture\ndef full_dataset():\n    """"""Dummy dataset from file""""""\n    return TabularDataset.from_path(train_path=\'tests/data/dummy_tabular/train.csv\',\n                                    val_path=\'tests/data/dummy_tabular/val.csv\', sep=\',\')\n\n\n@pytest.fixture\ndef dir_dataset():\n    """"""Dummy dataset from directory""""""\n    return TabularDataset.from_path(\'tests/data/dummy_tabular\', sep=\',\')\n\n\n@pytest.fixture\ndef autogen_dataset():\n    """"""Dummy dataset from file with auto-generated val and test""""""\n    return TabularDataset.autogen(\'tests/data/dummy_tabular/train.csv\',\n                                  seed=42,\n                                  sep=\',\')\n\n\n@pytest.fixture\ndef autogen_dataset_with_test():\n    """"""Dummy dataset from file with auto-generated val and given test""""""\n    return TabularDataset.autogen(\'tests/data/dummy_tabular/train.csv\',\n                                  test_path=\'tests/data/dummy_tabular_test/test.csv\',\n                                  seed=42,\n                                  sep=\',\')\n\n\n@pytest.fixture\ndef autogen_dataset_dir():\n    """"""Dummy dataset from directory with auto-generated val and test""""""\n    return TabularDataset.autogen(\'tests/data/dummy_tabular\',\n                                  seed=42,\n                                  sep=\',\')\n\n\n@pytest.fixture\ndef autogen_dataset_dir_with_test():\n    """"""Dummy dataset from dir with auto-generated val and given test""""""\n    return TabularDataset.autogen(\'tests/data/dummy_tabular\',\n                                  test_path=\'tests/data/dummy_tabular_test\',\n                                  seed=42,\n                                  sep=\',\')\n\n\n@pytest.fixture\ndef autogen_dataset_ratios():\n    """"""Dummy dataset from file with auto-generated val and test with\n    different ratios\n    """"""\n    return TabularDataset.autogen(\'tests/data/dummy_tabular/train.csv\',\n                                  seed=42,\n                                  sep=\',\',\n                                  test_ratio=0.5,\n                                  val_ratio=0.5)\n\n\n@pytest.fixture\ndef autogen_dataset_ratios_with_test():\n    """"""Dummy dataset from file with auto-generated val and given test\n    with different ratios\n    """"""\n    return TabularDataset.autogen(\'tests/data/dummy_tabular/train.csv\',\n                                  test_path=\'tests/data/dummy_tabular_test/test.csv\',\n                                  seed=42,\n                                  sep=\',\',\n                                  test_ratio=0.5,  # no effect\n                                  val_ratio=0.5)\n\n\n@pytest.fixture\ndef autogen_dataset_dir_ratios():\n    """"""Dummy dataset from directory with auto-generated val and test\n    with different ratios\n    """"""\n    return TabularDataset.autogen(\'tests/data/dummy_tabular\',\n                                  seed=42,\n                                  sep=\',\',\n                                  test_ratio=0.5,\n                                  val_ratio=0.5)\n\n\n@pytest.fixture\ndef autogen_dataset_dir_ratios_with_test():\n    """"""Dummy dataset from directory with auto-generated val\n    and given test with different ratios\n    """"""\n    return TabularDataset.autogen(\'tests/data/dummy_tabular\',\n                                  test_path=\'tests/data/dummy_tabular_test\',\n                                  seed=42,\n                                  sep=\',\',\n                                  test_ratio=0.5,  # no effect\n                                  val_ratio=0.5)\n\n\ndef test_valid_dataset():\n    """"""Test trivial dataset build process""""""\n    train = ((""Lorem ipsum dolor sit amet"", 3, 4.5),\n             (""Sed ut perspiciatis unde"", 5, 5.5))\n    val = ((""ipsum quia dolor sit"", 10, 3.5),)\n    test = ((""Ut enim ad minima veniam"", 100, 35),)\n\n    t = TabularDataset(train, val, test)\n\n    assert len(t) == 4\n    assert len(t.train) == 2\n    assert len(t.val) == 1\n    assert len(t.test) == 1\n\n    def check(d, t):\n        for i, tu in enumerate(d):\n            v0, v1, v2 = tu\n            assert t[i][0] == v0\n            assert t[i][1] == v1\n            assert t[i][2] == v2\n\n    check(train, t.train)\n    check(val, t.val)\n    check(test, t.test)\n\n\ndef test_invalid_dataset():\n    """"""Test dataset is invalid as it has different columns""""""\n    train = ((""Lorem ipsum dolor sit amet"", 3, 4.5),\n             (""Sed ut perspiciatis unde"", 5.5))\n    with pytest.raises(ValueError):\n        TabularDataset(train)\n\n\ndef test_invalid_dataset2():\n    """"""Test dataset is invalid as different splits contain different\n    columns\n    """"""\n    train = ((""Lorem ipsum dolor sit amet"", 3, 4.5),\n             (""Sed ut perspiciatis unde"", 4, 5.5))\n    val = ((""ipsum quia dolor sit"", 3.5),)\n    with pytest.raises(ValueError):\n        TabularDataset(train, val)\n\n\ndef test_incomplete_dataset():\n    """"""Test dataset missing either val or test""""""\n    train = ((""Lorem ipsum dolor sit amet"", 3, 4.5),\n             (""Sed ut perspiciatis unde"", 4, 5.5))\n    t = TabularDataset(train)\n\n    assert len(t.val) == 0\n    assert len(t.test) == 0\n\n\ndef test_cache_dataset():\n    """"""Test caching the dataset""""""\n    train = (\n            (""Lorem ipsum dolor sit amet"", 3, 4.5),\n            (""Sed ut perspiciatis unde"", 5, 5.5),\n            (""Lorem ipsum dolor sit amet"", 3, 4.5),\n            (""Sed ut perspiciatis unde"", 5, 5.5),\n            (""Lorem ipsum dolor sit amet"", 3, 4.5),\n            (""Sed ut perspiciatis unde"", 5, 5.5),\n            (""Lorem ipsum dolor sit amet"", 3, 4.5),\n            (""Sed ut perspiciatis unde"", 5, 5.5),\n            (""Lorem ipsum dolor sit amet"", 3, 4.5),\n            (""Sed ut perspiciatis unde"", 5, 5.5))\n\n    t = TabularDataset(train, cache=True)\n\n    assert len(t.train.cached_data) == 0\n    for i, _ in enumerate(t.train):\n        assert len(t.train.cached_data) == i + 1\n\n\ndef test_column_attr(train_dataset):\n    assert len(train_dataset.named_columns) == 2\n    assert train_dataset.named_columns == [\'text\', \'label\']\n\n\ndef test_column_attr2(train_dataset_no_header):\n    assert train_dataset_no_header.named_columns is None\n\n\ndef test_named_columns():\n    """"""Test dataset is invalid as it has different columns""""""\n    train = ((""Lorem ipsum dolor sit amet"", 3),\n             (""Sed ut perspiciatis unde"", 5.5))\n    TabularDataset(train, named_columns=[\'col1\', \'col2\'])\n\n\ndef test_invalid_columns():\n    """"""Test dataset is invalid as it has different columns""""""\n    train = ((""Lorem ipsum dolor sit amet"", 3),\n             (""Sed ut perspiciatis unde"", 5.5))\n    with pytest.raises(ValueError):\n        TabularDataset(train, named_columns=[\'some_random_col\'])\n\n\ndef test_dataset_from_file(train_dataset):\n    """"""Test loading a dataset from file""""""\n    dummy = ""justo. Praesent luctus. Curabitur egestas nunc sed libero. Proin sed""\n    assert train_dataset[0][0] == dummy\n    assert train_dataset[0][1] == \'6\'\n\n\ndef test_dataset_from_file_reversed(train_dataset_reversed):\n    """"""Test loading a dataset from file""""""\n    dummy = ""justo. Praesent luctus. Curabitur egestas nunc sed libero. Proin sed""\n    assert train_dataset_reversed[0][0] == \'6\'\n    assert train_dataset_reversed[0][1] == dummy\n\n\ndef test_full_dataset_from_file(full_dataset):\n    """"""Test loading a dataset from file""""""\n    train_dummy = ""justo. Praesent luctus. Curabitur egestas nunc sed libero. Proin sed""\n    val_dummy = ""malesuada. Integer id magna et ipsum cursus vestibulum. Mauris magna.""\n\n    assert full_dataset.train[0][0] == train_dummy\n    assert full_dataset.train[0][1] == \'6\'\n\n    assert full_dataset.val[0][0] == val_dummy\n    assert full_dataset.val[0][1] == \'8\'\n\n    assert full_dataset[0][0] == train_dummy\n    assert full_dataset[100][0] == val_dummy\n\n\ndef test_dataset_from_dir(dir_dataset):\n    """"""Test loading multiple datasets at once""""""\n    dummy = ""malesuada. Integer id magna et ipsum cursus vestibulum. Mauris magna.""\n    assert dir_dataset[0][0] == dummy\n    assert dir_dataset[0][1] == \'8\'\n\n    dummy = ""Sed molestie. Sed id risus quis diam luctus lobortis. Class""\n    assert dir_dataset[100][0] == dummy\n    assert dir_dataset[100][1] == \'6\'\n\n\ndef test_dataset_autogen(autogen_dataset):\n    """"""Test autogenerating val and test sets from a file""""""\n    train_dummy = ""eget, venenatis a, magna. Lorem ipsum dolor sit amet, consectetuer""\n    val_dummy = ""leo. Vivamus nibh dolor, nonummy ac, feugiat non, lobortis quis,""\n    test_dummy = ""turpis egestas. Aliquam fringilla cursus purus. Nullam scelerisque neque sed""\n\n    assert autogen_dataset.train[0][0] == train_dummy\n    assert autogen_dataset.train[0][1] == \'8\'\n    assert len(autogen_dataset.train) == 64\n\n    assert autogen_dataset.val[0][0] == val_dummy\n    assert autogen_dataset.val[0][1] == \'1\'\n    assert len(autogen_dataset.val) == 16\n\n    assert autogen_dataset.test[0][0] == test_dummy\n    assert autogen_dataset.test[0][1] == \'6\'\n    assert len(autogen_dataset.test) == 20\n\n\ndef test_dataset_autogen_with_test(autogen_dataset_with_test):\n    """"""Test autogenerating val and test sets from a file""""""\n    train_dummy = ""Etiam ligula tortor, dictum eu, placerat eget, venenatis a, magna.""\n    val_dummy = ""turpis egestas. Aliquam fringilla cursus purus. Nullam scelerisque neque sed""\n    test_dummy = ""a sollicitudin orci sem eget massa. Suspendisse eleifend. Cras sed""\n\n    assert autogen_dataset_with_test.train[0][0] == train_dummy\n    assert autogen_dataset_with_test.train[0][1] == \'6\'\n    assert len(autogen_dataset_with_test.train) == 80\n\n    assert autogen_dataset_with_test.val[0][0] == val_dummy\n    assert autogen_dataset_with_test.val[0][1] == \'6\'\n    assert len(autogen_dataset_with_test.val) == 20\n\n    assert autogen_dataset_with_test.test[0][0] == test_dummy\n    assert autogen_dataset_with_test.test[0][1] == \'3\'\n    assert len(autogen_dataset_with_test.test) == 50\n\n\ndef test_dataset_autogen_ratios(autogen_dataset_ratios):\n    """"""Test autogenerating val and test sets from a file with ratios""""""\n    train_dummy = ""leo. Vivamus nibh dolor, nonummy ac, feugiat non, lobortis quis,""\n    val_dummy = ""ac turpis egestas. Aliquam fringilla cursus purus. Nullam scelerisque neque""\n    test_dummy = ""turpis egestas. Aliquam fringilla cursus purus. Nullam scelerisque neque sed""\n\n    assert autogen_dataset_ratios.train[0][0] == train_dummy\n    assert autogen_dataset_ratios.train[0][1] == \'1\'\n    assert len(autogen_dataset_ratios.train) == 25\n\n    assert autogen_dataset_ratios.val[0][0] == val_dummy\n    assert autogen_dataset_ratios.val[0][1] == \'6\'\n    assert len(autogen_dataset_ratios.val) == 25\n\n    assert autogen_dataset_ratios.test[0][0] == test_dummy\n    assert autogen_dataset_ratios.test[0][1] == \'6\'\n    assert len(autogen_dataset_ratios.test) == 50\n\n\ndef test_dataset_autogen_ratios_with_test(autogen_dataset_ratios_with_test):\n    """"""Test autogenerating val set from a file with ratios with test""""""\n    train_dummy = ""leo. Vivamus nibh dolor, nonummy ac, feugiat non, lobortis quis,""\n    val_dummy = ""turpis egestas. Aliquam fringilla cursus purus. Nullam scelerisque neque sed""\n    test_dummy = ""a sollicitudin orci sem eget massa. Suspendisse eleifend. Cras sed""\n\n    assert autogen_dataset_ratios_with_test.train[0][0] == train_dummy\n    assert autogen_dataset_ratios_with_test.train[0][1] == \'1\'\n    assert len(autogen_dataset_ratios_with_test.train) == 50\n\n    assert autogen_dataset_ratios_with_test.val[0][0] == val_dummy\n    assert autogen_dataset_ratios_with_test.val[0][1] == \'6\'\n    assert len(autogen_dataset_ratios_with_test.val) == 50\n\n    assert autogen_dataset_ratios_with_test.test[0][0] == test_dummy\n    assert autogen_dataset_ratios_with_test.test[0][1] == \'3\'\n    assert len(autogen_dataset_ratios_with_test.test) == 50\n\n\ndef test_dataset_autogen_dir_val_test(autogen_dataset_dir):\n    """"""Test autogenerating val and test sets from a dir""""""\n    train_dummy = ""egestas blandit. Nam nulla magna, malesuada vel, convallis in, cursus""\n    val_dummy = ""turpis egestas. Aliquam fringilla cursus purus. Nullam scelerisque neque sed""\n    test_dummy = ""nibh. Aliquam ornare, libero at auctor ullamcorper, nisl arcu iaculis""\n\n    assert autogen_dataset_dir.train[0][0] == train_dummy\n    assert autogen_dataset_dir.train[0][1] == \'1\'\n    assert len(autogen_dataset_dir.train) == 160\n\n    assert autogen_dataset_dir.val[0][0] == val_dummy\n    assert autogen_dataset_dir.val[0][1] == \'6\'\n    assert len(autogen_dataset_dir.val) == 41\n\n    assert autogen_dataset_dir.test[0][0] == test_dummy\n    assert autogen_dataset_dir.test[0][1] == \'10\'\n    assert len(autogen_dataset_dir.test) == 51\n\n\ndef test_dataset_autogen_dir_with_test(autogen_dataset_dir_with_test):\n    """"""Test autogenerating val from a dir and given test""""""\n    train_dummy = ""Donec non justo. Proin non massa non ante bibendum ullamcorper.""\n    val_dummy = ""nibh. Aliquam ornare, libero at auctor ullamcorper, nisl arcu iaculis""\n    test_dummy = ""a sollicitudin orci sem eget massa. Suspendisse eleifend. Cras sed""\n\n    assert autogen_dataset_dir_with_test.train[0][0] == train_dummy\n    assert autogen_dataset_dir_with_test.train[0][1] == \'4\'\n    assert len(autogen_dataset_dir_with_test.train) == 201\n\n    assert autogen_dataset_dir_with_test.val[0][0] == val_dummy\n    assert autogen_dataset_dir_with_test.val[0][1] == \'10\'\n    assert len(autogen_dataset_dir_with_test.val) == 51\n\n    assert autogen_dataset_dir_with_test.test[0][0] == test_dummy\n    assert autogen_dataset_dir_with_test.test[0][1] == \'3\'\n    assert len(autogen_dataset_dir_with_test.test) == 50\n\n\ndef test_dataset_autogen_dir_val_test_ratios(autogen_dataset_dir_ratios):\n    """"""Test autogenerating val set from a dir with given test and\n    different ratios\n    """"""\n    train_dummy = ""Aenean euismod mauris eu elit. Nulla facilisi. Sed neque. Sed""\n    val_dummy = ""ut quam vel sapien imperdiet ornare. In faucibus. Morbi vehicula.""\n    test_dummy = ""nibh. Aliquam ornare, libero at auctor ullamcorper, nisl arcu iaculis""\n\n    assert autogen_dataset_dir_ratios.train[0][0] == train_dummy\n    assert autogen_dataset_dir_ratios.train[0][1] == \'9\'\n    assert len(autogen_dataset_dir_ratios.train) == 63\n\n    assert autogen_dataset_dir_ratios.val[0][0] == val_dummy\n    assert autogen_dataset_dir_ratios.val[0][1] == \'10\'\n    assert len(autogen_dataset_dir_ratios.val) == 63\n\n    assert autogen_dataset_dir_ratios.test[0][0] == test_dummy\n    assert autogen_dataset_dir_ratios.test[0][1] == \'10\'\n    assert len(autogen_dataset_dir_ratios.test) == 126\n\n\ndef test_dataset_autogen_dir_val_test_ratios_with_test(autogen_dataset_dir_ratios_with_test):\n    """"""Test autogenerating val and test sets from a file""""""\n    train_dummy = ""Donec egestas. Aliquam nec enim. Nunc ut erat. Sed nunc""\n    val_dummy = ""nibh. Aliquam ornare, libero at auctor ullamcorper, nisl arcu iaculis""\n    test_dummy = ""a sollicitudin orci sem eget massa. Suspendisse eleifend. Cras sed""\n\n    assert autogen_dataset_dir_ratios_with_test.train[0][0] == train_dummy\n    assert autogen_dataset_dir_ratios_with_test.train[0][1] == \'1\'\n    assert len(autogen_dataset_dir_ratios_with_test.train) == 126\n\n    assert autogen_dataset_dir_ratios_with_test.val[0][0] == val_dummy\n    assert autogen_dataset_dir_ratios_with_test.val[0][1] == \'10\'\n    assert len(autogen_dataset_dir_ratios_with_test.val) == 126\n\n    assert autogen_dataset_dir_ratios_with_test.test[0][0] == test_dummy\n    assert autogen_dataset_dir_ratios_with_test.test[0][1] == \'3\'\n    assert len(autogen_dataset_dir_ratios_with_test.test) == 50\n\n\ndef test_dataset_length(train_dataset, full_dataset):\n    """"""Test dataset length.""""""\n    assert len(train_dataset) == 100\n    assert len(full_dataset) == 200\n    assert len(full_dataset.train) == 100\n    assert len(full_dataset.val) == 100\n\n\ndef test_dataset_iter(train_dataset):\n    """"""Test that the dataset is iterable.""""""\n    for i, ex in enumerate(train_dataset):\n        assert np.array_equal(ex, train_dataset[i])\n\n\ndef test_dataset_setitem(train_dataset):\n    """"""Test dataset is immutable""""""\n    with pytest.raises(Exception):\n        train_dataset[0] = 0\n\n\ndef test_dataset_deltitem(train_dataset):\n    """"""Test dataset is immutable""""""\n    with pytest.raises(Exception):\n        del train_dataset[0]\n\n\ndef test_dataset_transform():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    transform = {\n        ""text"": TextField(),\n        ""label"": LabelField()\n    }\n\n    t = TabularDataset(train, transform=transform)\n\n    assert hasattr(t, ""text"")\n    assert hasattr(t, ""label"")\n\n    assert t.label.vocab_size == 2\n    assert t.text.vocab_size == 11\n\n\ndef test_dataset_transform_2():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    transform = {\n        ""text"": {\n            ""field"": TextField()\n        },\n        ""label"": {\n            ""field"": LabelField(),\n            ""columns"": 1\n        }\n    }\n\n    t = TabularDataset(train, transform=transform)\n\n    assert hasattr(t, ""text"")\n    assert hasattr(t, ""label"")\n\n    assert t.label.vocab_size == 2\n\n\ndef test_dataset_transform_3():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    transform = {\n        ""text"": {\n            ""columns"": 0\n        },\n        ""label"": {\n            ""field"": LabelField(),\n            ""columns"": 1\n        }\n    }\n\n    with pytest.raises(ValueError):\n        TabularDataset(train, transform=transform)\n\n\ndef test_dataset_transform_4():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    transform = {\n        ""t1"": {\n            ""field"": TextField(),\n            ""columns"": 1\n        },\n        ""t2"": {\n            ""field"": TextField(),\n            ""columns"": 1\n        }\n    }\n\n    t = TabularDataset(train, transform=transform)\n\n    assert t.train.cols() == 2\n\n\ndef test_dataset_transform_5():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    transform = {\n        ""t1"": {\n            ""field"": TextField(),\n            ""columns"": 0\n        },\n        ""t2"": {\n            ""field"": TextField(),\n            ""columns"": 0\n        }\n    }\n\n    t = TabularDataset(train, transform=transform)\n    assert t.train.cols() == 2\n\n\ndef test_dataset_transform_6():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    class DummyField(Field):\n        def setup(self, *data: np.ndarray) -> None:\n            pass\n\n        def process(self, ex1, ex2):\n            return torch.tensor(0)\n\n    transform = {\n        ""text"": {\n            ""field"": DummyField(),\n            ""columns"": [0, 1]\n        }\n    }\n\n    t = TabularDataset(train, transform=transform)\n    assert t.train.cols() == 1\n\n\ndef test_dataset_transform_7():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    class DummyField(Field):\n        def setup(self, *data: np.ndarray) -> None:\n            pass\n\n        def process(self, ex1, ex2):\n            return torch.tensor(0)\n\n    transform = {\n        ""text"": {\n            ""field"": DummyField(),\n            ""columns"": [0, 1]\n        },\n        ""other"": {\n            ""field"": DummyField(),\n            ""columns"": [0, 1]\n        },\n        ""other2"": {\n            ""field"": LabelField(),\n            ""columns"": 0\n        }\n    }\n\n    t = TabularDataset(train, transform=transform)\n    assert t.train.cols() == 3\n\n\ndef test_dataset_transform_8():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    transform = {\n        ""tx"": {\n            ""field"": LabelField(),\n            ""columns"": [0, 1]\n        }\n    }\n\n    with pytest.raises(TypeError):\n        t = TabularDataset(train, transform=transform)\n        t.train.cols()\n\n\ndef test_dataset_transform_with_named_cols():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    transform = {\n        ""tx"": {\n            ""field"": LabelField(),\n            ""columns"": \'label\'\n        }\n    }\n\n    t = TabularDataset(train, transform=transform, named_columns=[\'text\', \'label\'])\n    assert len(t.train[0]) == 1\n\n\ndef test_dataset_transform_with_invalid_named_cols():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    transform = {\n        ""tx"": {\n            ""field"": LabelField(),\n            ""columns"": \'none_existent\'\n        }\n    }\n\n    with pytest.raises(ValueError):\n        TabularDataset(train, transform=transform, named_columns=[\'text\', \'label\'])\n\n\ndef test_dataset_transform_with_mixed_cols():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    transform = {\n        ""label"": {\n            ""field"": LabelField(),\n            ""columns"": 1,\n        },\n        ""text"": {\n            ""field"": TextField(),\n            ""columns"": \'text\',\n        }\n    }\n\n    t = TabularDataset(train, transform=transform, named_columns=[\'text\', \'label\'])\n    assert len(t.train) == 2\n    assert len(t.train[0]) == 2\n\n\ndef test_dataset_transform_mixed_multiple_named_cols():\n    train = (\n            (""Lorem ipsum dolor sit amet"", ""POSITIVE""),\n            (""Sed ut perspiciatis unde"", ""NEGATIVE""))\n\n    class DummyField(Field):\n        def setup(self, *data: np.ndarray) -> None:\n            pass\n\n        def process(self, ex1, ex2):\n            return torch.tensor(0)\n\n    transform = {\n        ""text"": {\n            ""field"": DummyField(),\n            ""columns"": [\'text\', \'label\']\n        },\n        ""other"": {\n            ""field"": DummyField(),\n            ""columns"": [0, 1]\n        },\n        ""other2"": {\n            ""field"": DummyField(),\n            ""columns"": [0, \'label\']\n        }\n    }\n\n    t = TabularDataset(train, transform=transform, named_columns=[\'text\', \'label\'])\n    assert t.train.cols() == 3\n'"
tests/unit/experiment/__init__.py,0,b''
tests/unit/experiment/test_experiment.py,0,"b'from flambe.experiment import Experiment\nfrom flambe.runnable import RemoteEnvironment\n\nimport mock\nimport pytest\n\n\n@pytest.fixture\ndef get_experiment():\n    def wrapped(**kwargs):\n        return Experiment(name=kwargs.get(\'name\', \'test\'),\n                          pipeline=kwargs.get(\'pipeline\', {}),\n                          resume=kwargs.get(\'resume\', False),\n                          devices=kwargs.get(\'devices\', None),\n                          save_path=kwargs.get(\'save_path\', None),\n                          resources=kwargs.get(\'resources\', None),\n                          search=kwargs.get(\'search\', None),\n                          schedulers=kwargs.get(\'schedulers\', None),\n                          reduce=kwargs.get(\'reduce\', None),\n                          env=kwargs.get(\'env\', None),\n                          max_failures=kwargs.get(\'max_failures\', 1),\n                          stop_on_failure=kwargs.get(\'stop_on_failure\', True),\n                          merge_plot=kwargs.get(\'merge_plot\', True),\n                          user_provider=kwargs.get(\'user_provider\', None))\n    return wrapped\n\n\n@pytest.fixture\ndef get_env():\n    def wrapped(**kwargs):\n        env = RemoteEnvironment(\n            key=kwargs.get(\'key\', \'my-key\'),\n            orchestrator_ip=kwargs.get(\'orchestrator_ip\', \'1.1.1.1\'),\n            factories_ips=kwargs.get(\'factories_ips\', [\'1.1.1.1\']),\n            user=kwargs.get(\'user\', \'ubuntu\'),\n            local_user=kwargs.get(\'local_user\', \'some_user\'),\n        )\n        return env\n    return wrapped\n\n\ndef test_get_user(get_experiment, get_env):\n    exp = get_experiment(user_provider=lambda: ""foobar"")\n    assert exp.get_user() == \'foobar\'\n\n    exp = get_experiment(env=get_env(local_user=\'barfoo\'))\n    assert exp.get_user() == \'barfoo\'\n'"
tests/unit/experiment/test_experiment_preprocess.py,0,"b'import pytest\nimport tempfile\nimport sys\n\nfrom flambe.runnable import SafeExecutionContext, error\n\n\n@pytest.fixture\ndef context():\n    t = tempfile.NamedTemporaryFile(mode=""w+"")\n    def ex(config):\n        t.write(config)\n        t.flush()\n        return SafeExecutionContext(t.name)\n    yield ex\n    t.close()\n\n\ndef test_preprocessor_non_existing_file():\n    with pytest.raises(FileNotFoundError):\n        ex = SafeExecutionContext(""/non/existing/file"")\n        ex.preprocess()\n\n\ndef test_preprocessor_empty():\n    with pytest.raises(TypeError):\n        ex = SafeExecutionContext()\n        ex.preprocess()\n\n\ndef test_preprocessor_none_file():\n    with pytest.raises(TypeError):\n        ex = SafeExecutionContext(None)\n        ex.preprocess()\n\n\ndef clean_imported_pkg(name, added_paths):\n    """"""Run this everytime something gets imported.\n    It removes recursively all modules imported under name\n\n    """"""\n    for k in list(sys.modules.keys()):\n        if name == k or k.startswith(name + \'.\'):\n            del sys.modules[k]\n\n    sys.path = [x for x in sys.path if x not in added_paths]\n\n\ndef test_preprocessor_not_yaml(context):\n    config = """"""\nNot a YAML file content\n""""""\n    with pytest.raises(ValueError):\n        ex = context(config)\n        ex.preprocess()\n\n\ndef test_preprocessor_wrong_yaml(context):\n    with pytest.raises(error.ParsingRunnableError):\n        ex = SafeExecutionContext(""tests/data/dummy_configs/wrong_config.yaml"")\n        ex.first_parse()\n\n\ndef test_preprocessor_none_content(context):\n    config = """"""\n!Experiment\n\nname: some-name\n\nresources:\npipeline:\n\n""""""\n    with pytest.raises(error.ParsingRunnableError):\n        ex = context(config)\n        content, ext = ex.first_parse()\n        runnable = ex.compile_runnable(content)\n        runnable.parse()\n\n\ndef test_preprocessor_name_none(context):\n    config = """"""\n!Experiment\n\nname:\n\nresources:\n\npipeline:\n    mod: !Module\n\n""""""\n    with pytest.raises(error.ParsingRunnableError):\n        ex = context(config)\n        content, ext = ex.first_parse()\n        runnable = ex.compile_runnable(content)\n        runnable.parse()\n\n\ndef test_preprocessor_invalid_name(context):\n    config = """"""\n!Experiment\n\nname: some-invalid-name-!@#$%^&&&&&^%$#@\n\npipeline:\n    mod: !Module\n""""""\n    with pytest.raises(error.ParsingRunnableError):\n        ex = context(config)\n        content, ext = ex.first_parse()\n        runnable = ex.compile_runnable(content)\n        runnable.parse()\n\n\ndef test_preprocessor_invalid_name2(context):\n    config = """"""\n!Experiment\n\nname: /some/invalid/name\n\npipeline:\n    mod: !Module\n""""""\n    with pytest.raises(error.ParsingRunnableError):\n        ex = context(config)\n        content, ext = ex.first_parse()\n        runnable = ex.compile_runnable(content)\n        runnable.parse()\n\n\n\ndef test_preprocessor_valid_resources(context):\n    config = """"""\n!Experiment\n\nname: random-name\n\nresources:\n  r0: !cluster /remote/path\n\npipeline:\n    mod: !Module\n""""""\n    ex = context(config)\n    content, ext = ex.first_parse()\n    runnable = ex.compile_runnable(content)\n    runnable.parse()\n\n\ndef test_preprocessor_valid_paths(context):\n    config = """"""\n!Experiment\n\nname: random-name\n\npipeline:\n  b0: !Something\n    b1: !Other\n      b2: something\n""""""\n    ex = context(config)\n    content, ext = ex.first_parse()\n    runnable = ex.compile_runnable(content)\n    runnable.parse()\n\n\ndef test_preprocessor_extensions_non_package(context):\n    config = """"""\nflambe_script: tests/data/dummy_extensions/script/setup.py\n---\n!Experiment\n\nname: some-name\n\npipeline:\n    mod: !Module\n""""""\n    with pytest.raises(ImportError):\n        ex = context(config)\n        ex.preprocess(install_ext=True)\n\n\ndef test_preprocessor_extensions_invalid_mod_name(context):\n    config = """"""\nscript: tests/data/dummy_extensions/script\n---\n!Experiment\n\nname: some-name\n\npipeline:\n    mod: !Module\n""""""\n    with pytest.raises(ImportError):\n        ex = context(config)\n        ex.preprocess(install_ext=True)\n\n\ndef test_preprocessor_valid_extensions(context):\n    config = """"""\nflambe_script: tests/data/dummy_extensions/script\n---\n!Experiment\n\nname: some-name\n\npipeline:\n    mod: !Module\n""""""\n    ex = context(config)\n    ex.preprocess(install_ext=True)\n\n\ndef test_preprocessor_custom_experiment(context):\n    config = """"""\nflambe_runnable: tests/data/dummy_extensions/runnable\n---\n!flambe_runnable.DummyRunnable\n""""""\n    ex = context(config)\n    ex.preprocess(install_ext=True)\n\n\ndef test_preprocessor_custom_experiment_invalid(context):\n    # Importing the wrong package\n    config = """"""\nflambe_script: tests/data/dummy_extensions/runnable\n---\n!flambe_runnable.DummyRunnable\n""""""\n    with pytest.raises(AttributeError):\n        ex = context(config)\n        ex.preprocess(install_ext=True)\n\n\ndef test_preprocessor_unknown_tag(context):\n    config = """"""\n    !Experiment\n\n    name: random-name\n\n    pipeline:\n        b0: !Something\n    """"""\n    with pytest.raises(error.TagError):\n        context(\'\').check_tags(config)\n\n\ndef test_preprocessor_unknown_factory(context):\n    config = """"""\n    !Experiment\n\n    name: random-name\n\n    pipeline:\n        b0: !Trainer.something\n    """"""\n    with pytest.raises(error.TagError):\n        context(\'\').check_tags(config)\n'"
tests/unit/experiment/test_options.py,0,"b""import pytest\n\nfrom flambe.experiment import options\n\n\ndef test_g_tag():\n    l = list(range(5))\n    grid = options.GridSearchOptions.from_sequence(l)\n\n    for i, each in enumerate(grid):\n        assert each == l[i]\n\n\ndef test_s_tag():\n    # Test k\n\n    for k in range(5, 1):\n        l = [1, 10, k]\n        opts = options.SampledUniformSearchOptions.from_sequence(l)\n\n        assert len(opts) == k\n\n\ndef test_s_tag_2():\n    # Test int sampling\n    l = [1, 10, 1, 10]\n\n    opts = options.SampledUniformSearchOptions.from_sequence(l)\n\n    for each in opts:\n        assert isinstance(each, int)\n\n\n\ndef test_s_tag_3():\n    # Test float sampling\n\n    l = [1, 10.1, 1]\n\n    opts = options.SampledUniformSearchOptions.from_sequence(l)\n\n    for each in opts:\n        assert isinstance(each, float)\n\n    l = [1.1, 10.1, 1]\n\n    opts = options.SampledUniformSearchOptions.from_sequence(l)\n\n    for each in opts:\n        assert isinstance(each, float)\n\n\n    l = [1.1, 10, 1]\n\n    opts = options.SampledUniformSearchOptions.from_sequence(l)\n\n    for each in opts:\n        assert isinstance(each, float)\n\n\ndef test_s_tag_4():\n    # Test decimals\n    d = 5\n    l = [1, 10.1, 5, d]\n\n    opts = options.SampledUniformSearchOptions.from_sequence(l)\n\n    for each in opts:\n        decimals = len(str(each)[str(each).find('.') + 1:])\n        assert decimals <= d\n\n\ndef test_s_tag_incorrect_params():\n    # Test decimals\n    l = [1, 10.1, 5, 1, 1]\n\n    with pytest.raises(ValueError):\n        options.SampledUniformSearchOptions.from_sequence(l)\n\n\n    l = [1]\n\n    with pytest.raises(ValueError):\n        options.SampledUniformSearchOptions.from_sequence(l)\n"""
tests/unit/experiment/test_utils.py,4,"b'import pytest\nimport mock\n\nfrom flambe.compile import Component, yaml\nfrom flambe.experiment.utils import divide_nested_grid_search_options, get_default_devices\n\n\n@pytest.fixture\ndef make_classes():\n    class A(Component):\n        def __init__(self, akw1=0, akw2=None):\n            self.akw1 = akw1\n            self.akw2 = akw2\n\n    class B(Component):\n        def __init__(self, bkw1=0, bkw2=\'\'):\n            self.bkw1 = bkw1\n            self.bkw2 = bkw2\n    return A, B\n\n\ndef test_divide_nested_grid_search_options_no_options(make_classes):\n    A, B = make_classes\n    txt = """"""\n!A\nakw1: 8\nakw2: !B\n  bkw1: 1\n  bkw2: \'hello\'\n""""""\n    config = yaml.load(txt)\n    divided_configs = list(divide_nested_grid_search_options(config))\n    assert repr(divided_configs) == repr([config])\n\n\ndef test_divide_nested_grid_search_options_non_nested_options(make_classes):\n    A, B = make_classes\n    txt = """"""\n!A\nakw1: 8\nakw2: !B\n  bkw1: !g [1, 3, 5]\n  bkw2: \'hello\'\n""""""\n    config = yaml.load(txt)\n    divided_configs = list(divide_nested_grid_search_options(config))\n    assert repr(divided_configs) == repr([config])\n\n\ndef test_divide_nested_grid_search_options_nested_schemas(make_classes):\n    A, B = make_classes\n    txt = """"""\n!A\nakw1: 8\nakw2: !g\n  - !B\n    bkw1: 2\n    bkw2: \'first\'\n  - !B\n    bkw1: 3\n    bkw2: \'second\'\n""""""\n    txt_1 = """"""\n!A\nakw1: 8\nakw2: !B\n  bkw1: 2\n  bkw2: \'first\'\n""""""\n    txt_2 = """"""\n!A\nakw1: 8\nakw2: !B\n  bkw1: 3\n  bkw2: \'second\'\n""""""\n    config = yaml.load(txt)\n    config1 = yaml.load(txt_1)\n    config2 = yaml.load(txt_2)\n    divided_configs = list(divide_nested_grid_search_options(config))\n    assert repr(divided_configs) == repr([config1, config2])\n\n\ndef test_divide_nested_grid_search_options_nested_options(make_classes):\n    A, B = make_classes\n    txt = """"""\n!A\nakw1: 8\nakw2: !g\n  - !B\n    bkw1: !g [4, 5]\n    bkw2: \'first\'\n  - !B\n    bkw1: 3\n    bkw2: !g [\'second\', \'third\']\n""""""\n    txt_1 = """"""\n!A\nakw1: 8\nakw2: !B\n  bkw1: !g [4, 5]\n  bkw2: \'first\'\n""""""\n    txt_2 = """"""\n!A\nakw1: 8\nakw2: !B\n  bkw1: 3\n  bkw2: !g [\'second\', \'third\']\n""""""\n    config = yaml.load(txt)\n    config1 = yaml.load(txt_1)\n    config2 = yaml.load(txt_2)\n    divided_configs = list(divide_nested_grid_search_options(config))\n    assert repr(divided_configs) == repr([config1, config2])\n\n\n@pytest.mark.parametrize(""initialized"", [True, False])\n@pytest.mark.parametrize(""debug"", [True, False])\n@mock.patch(""flambe.experiment.utils.torch.cuda.device_count"")\n@mock.patch(""flambe.experiment.utils.os.cpu_count"")\n@mock.patch(""flambe.experiment.utils.ray.cluster_resources"")\n@mock.patch(""flambe.experiment.utils.ray.is_initialized"")\n@mock.patch(""flambe.experiment.utils.torch.cuda.is_available"")\ndef test_default_devices_cpu(cuda_available,\n                             ray_initialized,\n                             resources,\n                             cpu_count,\n                             gpu_count,\n                             debug,\n                             initialized):\n    cuda_available.return_value = False\n    ray_initialized.return_value = initialized\n    cpu_count.return_value = 2\n    gpu_count.return_value = 0\n    resources.return_value = {\'cpu\': 2}\n\n    devices = get_default_devices(debug=debug)\n    assert devices == {\'cpu\': 1}\n\n    devices = get_default_devices(debug=debug, default_cpus=2)\n    assert devices == {\'cpu\': 2}\n\n    devices = get_default_devices(debug=debug, default_gpus=1)\n    assert devices == {\'cpu\': 1}\n\n    with pytest.raises(ValueError):\n        get_default_devices(debug=debug, default_cpus=3)\n\n\n@pytest.mark.parametrize(""initialized"", [True, False])\n@pytest.mark.parametrize(""debug"", [True, False])\n@mock.patch(""flambe.experiment.utils.torch.cuda.device_count"")\n@mock.patch(""flambe.experiment.utils.os.cpu_count"")\n@mock.patch(""flambe.experiment.utils.ray.cluster_resources"")\n@mock.patch(""flambe.experiment.utils.ray.is_initialized"")\n@mock.patch(""flambe.experiment.utils.torch.cuda.is_available"")\ndef test_default_devices_gpu(cuda_available,\n                             ray_initialized,\n                             resources,\n                             cpu_count,\n                             gpu_count,\n                             debug,\n                             initialized):\n    cuda_available.return_value = True\n    ray_initialized.return_value = initialized\n    cpu_count.return_value = 2\n    gpu_count.return_value = 2\n    resources.return_value = {\'cpu\': 2, \'gpu\': 2}\n\n    devices = get_default_devices(debug=debug)\n    assert devices == {\'cpu\': 1, \'gpu\': 1}\n\n    devices = get_default_devices(debug=debug, default_cpus=2)\n    assert devices == {\'cpu\': 2, \'gpu\': 1}\n\n    devices = get_default_devices(debug=debug, default_gpus=2)\n    assert devices == {\'cpu\': 1, \'gpu\': 2}\n\n    devices = get_default_devices(debug=debug, default_cpus=2, default_gpus=2)\n    assert devices == {\'cpu\': 2, \'gpu\': 2}\n\n    with pytest.raises(ValueError):\n        get_default_devices(debug=debug, default_gpus=3)\n\n    with pytest.raises(ValueError):\n        get_default_devices(debug=debug, default_cpus=3)\n'"
tests/unit/field/__init__.py,0,b''
tests/unit/field/test_label_field.py,0,"b'import pytest\n\nfrom numpy import isclose\n\nfrom flambe.field import LabelField\nfrom flambe.tokenizer import LabelTokenizer\n\n\nNUMERIC_PRECISION = 1e-2\n\n\ndef test_tokenizer():\n    """"""Test one hot featurizer.""""""\n    tokenizer = LabelTokenizer()\n\n    dummy = \'LABEL1\'\n    assert tokenizer.tokenize(dummy) == [\'LABEL1\']\n\n    dummy = \'LABEL1,LABEL2\'\n    assert tokenizer.tokenize(dummy) == [\'LABEL1,LABEL2\']\n\n    dummy = \'LABEL1 LABEL2\'\n    assert tokenizer.tokenize(dummy) == [\'LABEL1 LABEL2\']\n\n\ndef test_tokenizer_multilabel():\n    """"""Test one hot featurizer.""""""\n    tokenizer = LabelTokenizer(multilabel_sep=\',\')\n\n    dummy = \'LABEL1\'\n    assert tokenizer.tokenize(dummy) == [\'LABEL1\']\n\n    dummy = \'LABEL1,LABEL2\'\n    assert tokenizer.tokenize(dummy) == [\'LABEL1\', \'LABEL2\']\n\n    dummy = \'LABEL1 LABEL2\'\n    assert tokenizer.tokenize(dummy) == [\'LABEL1 LABEL2\']\n\n\ndef test_label_process():\n    """"""Test label nuemricalization.""""""\n    dummy = [\'LABEL1\', \'LABEL3\', \'LABEL2\', \'LABEL2\']\n\n    field = LabelField()\n    field.setup(dummy)\n\n    assert len(field.vocab) == 3\n    assert int(field.process(\'LABEL1\')) == 0\n    assert int(field.process(\'LABEL2\')) == 2\n    assert int(field.process(\'LABEL3\')) == 1\n\n\ndef test_label_process_multilabel():\n    """"""Test label nuemricalization.""""""\n    dummy = [\'LABEL1,LABEL2\', \'LABEL3\', \'LABEL2,LABEL1\', \'LABEL2\']\n\n    field = LabelField()\n    field.setup(dummy)\n    assert len(field.vocab) == 4\n\n    field = LabelField(multilabel_sep=\',\')\n    field.setup(dummy)\n    assert len(field.vocab) == 3\n\n    assert list(field.process(\'LABEL1,LABEL2\')) == [0, 1]\n    assert list(field.process(\'LABEL2,LABEL1\')) == [1, 0]\n    assert int(field.process(\'LABEL2\')) == 1\n    assert int(field.process(\'LABEL3\')) == 2\n\n\ndef test_label_process_one_hot():\n    """"""Test label numericalization.""""""\n    dummy = [\'LABEL1\', \'LABEL3\', \'LABEL2\', \'LABEL2\']\n\n    field = LabelField(one_hot=True)\n    field.setup(dummy)\n\n    assert len(field.vocab) == 3\n    assert list(field.process(\'LABEL1\')) == [1, 0, 0]\n    assert list(field.process(\'LABEL2\')) == [0, 0, 1]\n    assert list(field.process(\'LABEL3\')) == [0, 1, 0]\n\n\ndef test_label_process_multilabel_one_hot():\n    """"""Test label numericalization.""""""\n    dummy = [\'LABEL1,LABEL2\', \'LABEL3\', \'LABEL2,LABEL1\', \'LABEL2\']\n\n    field = LabelField(multilabel_sep=\',\', one_hot=True)\n    field.setup(dummy)\n\n    assert len(field.vocab) == 3\n    assert list(field.process(\'LABEL1,LABEL2\')) == [1, 1, 0]\n    assert list(field.process(\'LABEL2,LABEL1\')) == [1, 1, 0]\n    assert list(field.process(\'LABEL2\')) == [0, 1, 0]\n    assert list(field.process(\'LABEL3\')) == [0, 0, 1]\n\n\ndef test_label_frequencies():\n    """"""Test label frequencies.""""""\n    dummy = [\'LABEL1\'] * 80\n    dummy.extend([\'LABEL2\'] * 20)\n\n    field = LabelField()\n    field.setup(dummy)\n\n    assert len(field.vocab) == 2\n\n    assert len(field.label_freq) == 2\n    assert isclose(field.label_freq[0].item(), 0.8, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_freq[1].item(), 0.2, rtol=NUMERIC_PRECISION)\n\n    assert len(field.label_count) == 2\n    assert isclose(field.label_count[0].item(), 80, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_count[1].item(), 20, rtol=NUMERIC_PRECISION)\n\n    assert len(field.label_inv_freq) == 2\n    assert isclose(field.label_inv_freq[0].item(), 1.25, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_inv_freq[1].item(), 5, rtol=NUMERIC_PRECISION)\n\n\ndef test_label_frequencies_2():\n    """"""Test label frequencies.""""""\n    dummy = [\'LABEL1\'] * 80\n\n    field = LabelField()\n    field.setup(dummy)\n\n    assert len(field.vocab) == 1\n\n    assert len(field.label_freq) == 1\n    assert isclose(field.label_freq[0].item(), 1, rtol=NUMERIC_PRECISION)\n\n    assert len(field.label_count) == 1\n    assert isclose(field.label_count[0].item(), 80, rtol=NUMERIC_PRECISION)\n\n    assert len(field.label_inv_freq) == 1\n    assert isclose(field.label_inv_freq[0].item(), 1, rtol=NUMERIC_PRECISION)\n\n\ndef test_label_frequencies_3():\n    """"""Test label frequencies.""""""\n    dummy = []\n\n    field = LabelField()\n    field.setup(dummy)\n\n    assert len(field.vocab) == 0\n\n    assert len(field.label_freq) == 0\n    assert len(field.label_count) == 0\n    assert len(field.label_inv_freq) == 0\n\n\ndef test_label_process_one_hot_frequencies():\n    """"""Test label numericalization.""""""\n    dummy = [\'LABEL1\', \'LABEL3\', \'LABEL2\', \'LABEL2\']\n\n    field = LabelField(one_hot=True)\n    field.setup(dummy)\n\n    assert len(field.label_freq) == 3\n    assert isclose(field.label_freq[0].item(), 0.25, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_freq[1].item(), 0.25, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_freq[2].item(), 0.5, rtol=NUMERIC_PRECISION)\n\n    assert len(field.label_count) == 3\n    assert isclose(field.label_count[0].item(), 1, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_count[1].item(), 1, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_count[2].item(), 2, rtol=NUMERIC_PRECISION)\n\n    assert len(field.label_inv_freq) == 3\n    assert isclose(field.label_inv_freq[0].item(), 4, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_inv_freq[1].item(), 4, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_inv_freq[2].item(), 2, rtol=NUMERIC_PRECISION)\n\n\ndef test_label_process_multilabel_one_hot_frequencies():\n    """"""Test label numericalization.""""""\n    dummy = [\'LABEL1,LABEL2\', \'LABEL3\', \'LABEL2,LABEL1\', \'LABEL2\']\n\n    field = LabelField(multilabel_sep=\',\', one_hot=True)\n    field.setup(dummy)\n\n    assert len(field.label_freq) == 3\n    assert isclose(field.label_freq[0].item(), 0.333, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_freq[1].item(), 0.5, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_freq[2].item(), 0.166, rtol=NUMERIC_PRECISION)\n\n    assert len(field.label_count) == 3\n    assert isclose(field.label_count[0].item(), 2, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_count[1].item(), 3, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_count[2].item(), 1, rtol=NUMERIC_PRECISION)\n\n    assert len(field.label_inv_freq) == 3\n    assert isclose(field.label_inv_freq[0].item(), 3, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_inv_freq[1].item(), 2, rtol=NUMERIC_PRECISION)\n    assert isclose(field.label_inv_freq[2].item(), 6, rtol=NUMERIC_PRECISION)\n\n\ndef test_pass_bool_labels():\n    """"""Test labels specified in the init""""""\n    dummy = [True, False, True, True]\n\n    field = LabelField(labels=[False, True])\n    field.setup(dummy)\n\n    assert len(field.vocab) == 2\n    assert int(field.process(False)) == 0\n    assert int(field.process(True)) == 1\n\n    field = LabelField(labels=[True, False])\n    field.setup(dummy)\n\n    assert len(field.vocab) == 2\n    assert int(field.process(False)) == 1\n    assert int(field.process(True)) == 0\n\n\ndef test_pass_labels():\n    """"""Test labels specified in the init""""""\n    dummy = [\'LABEL1\', \'LABEL3\', \'LABEL2\', \'LABEL2\']\n\n    field = LabelField(labels=[\'LABEL1\', \'LABEL2\', \'LABEL3\'])\n    field.setup(dummy)\n\n    assert len(field.vocab) == 3\n    assert int(field.process(\'LABEL1\')) == 0\n    assert int(field.process(\'LABEL2\')) == 1\n    assert int(field.process(\'LABEL3\')) == 2\n\n    field = LabelField(labels=[\'LABEL3\', \'LABEL1\', \'LABEL2\'])\n    field.setup(dummy)\n\n    assert len(field.vocab) == 3\n    assert int(field.process(\'LABEL1\')) == 1\n    assert int(field.process(\'LABEL2\')) == 2\n    assert int(field.process(\'LABEL3\')) == 0\n\n\ndef test_pass_labels_with_unkown_1():\n    """"""Test labels specified in the init""""""\n    dummy = [\'LABEL1\', \'LABEL3\', \'LABEL2\', \'LABEL2\']\n\n    field = LabelField(labels=[\'LABEL1\', \'LABEL2\'])\n    with pytest.raises(ValueError):\n        field.setup(dummy)\n\n\ndef test_pass_labels_with_unkown_2():\n    """"""Test labels specified in the init""""""\n    dummy = [\'LABEL1\', \'LABEL3\', \'LABEL2\', \'LABEL2\']\n\n    field = LabelField(labels=[\'LABEL1\', \'LABEL2\', \'LABEL3\'])\n    field.setup(dummy)\n    with pytest.raises(ValueError):\n        list(field.process(\'LABEL4\'))\n'"
tests/unit/field/test_text_field.py,11,"b'from collections import OrderedDict as odict\nfrom gensim.models import KeyedVectors\n\nimport numpy as np\n\nimport pytest\nimport tempfile\nimport torch\nfrom flambe.field import TextField, BoWField\nfrom flambe.tokenizer import WordTokenizer, CharTokenizer, NGramsTokenizer, NLTKWordTokenizer\n\n\nexample = (\n""Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ""\n""ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco ""\n""laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in ""\n""voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat ""\n""non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.""\n)\n\ndef test_word_tokenizer():\n    tokenizer = WordTokenizer()\n\n    dummy = ""justo. Praesent luctus.""\n    assert tokenizer(dummy) == [\'justo.\', \'Praesent\', \'luctus.\']\n    dummy = """"\n    assert tokenizer(dummy) == []\n\ndef test_nltk_word_tokenizer():\n    tokenizer = NLTKWordTokenizer()\n\n    dummy = ""justo. Praesent luctus.""\n    assert tokenizer(dummy) == [\'justo\', \'.\', \'Praesent\', \'luctus\', \'.\']\n    dummy = """"\n    assert tokenizer(dummy) == []\n\ndef test_ngram_tokenizer():\n    tokenizer = NGramsTokenizer(2)\n\n    dummy = ""justo. Praesent luctus.""\n    assert tokenizer(dummy) == [\'justo .\', \'. Praesent\', \'Praesent luctus\', \'luctus .\']\n    dummy = """"\n    assert tokenizer(dummy) == []\n\ndef test_ngram_tokenizer_equivalence():\n    t1 = NGramsTokenizer(1)\n    t2 = NLTKWordTokenizer()\n\n    assert t1(example) == t2(example)\n\ndef test_ngram_tokenizer_equivalence_2():\n    t = NGramsTokenizer([1,2,3])\n\n    ret = []\n    for i in [1, 2, 3]:\n        ret.extend(NGramsTokenizer(i)(example))\n\n    assert t(example) == ret\n\n\ndef test_ngram_tokenizer_stopwords():\n    tokenizer = NGramsTokenizer(2, exclude_stopwords=True)\n\n    dummy = ""justo. Praesent the luctus.""\n    assert tokenizer(dummy) == [\'justo .\', \'. Praesent\', \'Praesent luctus\', \'luctus .\']\n\n    tokenizer = NGramsTokenizer(1, exclude_stopwords=True)\n\n    dummy = ""justo. Praesent the luctus.""\n    assert tokenizer(dummy) == [\'justo\', \'.\', \'Praesent\', \'luctus\', \'.\']\n\n    tokenizer = NGramsTokenizer(2, exclude_stopwords=True, stop_words=[""Praesent"", ""the""])\n\n    dummy = ""justo. Praesent the luctus.""\n    assert tokenizer(dummy) == [\'justo .\', \'. luctus\', \'luctus .\']\n\n\ndef test_char_tokenizer():\n    tokenizer = CharTokenizer()\n\n    dummy = ""justo. Praesent""\n    assert tokenizer(dummy) == [""j"", ""u"", ""s"", ""t"", ""o"", ""."", "" "",\n                                ""P"", ""r"", ""a"", ""e"", ""s"", ""e"", ""n"", ""t""]\n    dummy = """"\n    assert tokenizer(dummy) == []\n\n\ndef test_build_vocab():\n    field = TextField(pad_token=\'<pad>\', unk_token=\'<unk>\')\n    field._build_vocab()\n    assert field.vocab == {\'<pad>\': 0, \'<unk>\': 1}\n\n    dummy = [""justo Praesent luctus"", ""luctus praesent""]\n    field._build_vocab(dummy)\n\n    vocab = {\'<pad>\': 0, \'<unk>\': 1, \'justo\': 2, \'Praesent\': 3,\n             \'luctus\': 4, \'praesent\': 5}\n    assert field.vocab == vocab\n\n\ndef test_build_vocab_list():\n    field = TextField()\n    dummy = [\n        [""justo Praesent luctus"", ""luctus praesent""],\n        [""justo Praesent luctus"", ""luctus praesent est""]]\n    field._build_vocab(dummy)\n\n    vocab = {\'<pad>\': 0, \'<unk>\': 1, \'justo\': 2, \'Praesent\': 3,\n             \'luctus\': 4, \'praesent\': 5, \'est\': 6}\n    assert field.vocab == vocab\n\n\ndef test_build_vocab_dict():\n    field = TextField()\n    dummy = [{\n        \'text1\': ""justo Praesent luctus luctus praesent"",\n        \'text2\': ""justo Praesent luctus luctus praesent est""}]\n    field._build_vocab(dummy)\n\n    vocab = {\'<pad>\': 0, \'<unk>\': 1, \'justo\': 2, \'Praesent\': 3,\n             \'luctus\': 4, \'praesent\': 5, \'est\': 6}\n    assert field.vocab == vocab\n\n\ndef test_build_vocab_nested_list_in_dict():\n    field = TextField()\n    dummy = [{\n        \'text1\': [""justo Praesent luctus"", ""luctus praesent""],\n        \'text2\': [""justo Praesent luctus"", ""luctus praesent est""]}]\n    field._build_vocab(dummy)\n\n    vocab = {\'<pad>\': 0, \'<unk>\': 1, \'justo\': 2, \'Praesent\': 3,\n             \'luctus\': 4, \'praesent\': 5, \'est\': 6}\n    assert field.vocab == vocab\n\n\ndef test_build_vocab_nested_dict_in_list_in_dict():\n    field = TextField()\n    dummy = [\n        {\'text1\': ""justo Praesent luctus"", \'text2\': ""luctus praesent""},\n        {\'text3\': [""justo Praesent luctus"", ""luctus praesent est""]}\n    ]\n    field._build_vocab(dummy)\n\n    vocab = {\'<pad>\': 0, \'<unk>\': 1, \'justo\': 2, \'Praesent\': 3,\n             \'luctus\': 4, \'praesent\': 5, \'est\': 6}\n    assert field.vocab == vocab\n\n\ndef test_bow_build_vocab():\n    field = BoWField(min_freq=2, unk_token=\'<unk>\')\n    assert field.vocab == {\'<unk>\': 0}\n\n    dummy = [""justo luctus Praesent luctus"", ""luctus praesent""]\n    field.setup(dummy)\n\n    vocab = {\'<unk>\': 0, \'luctus\': 1}\n    assert field.vocab == vocab\n\n\ndef test_build_vocab_lower():\n    field = TextField(lower=True, pad_token=None, unk_token=None)\n\n    dummy = [""justo Praesent luctus"", ""luctus praesent""]\n    field.setup(dummy)\n\n    vocab = {\'justo\': 0, \'praesent\': 1, \'luctus\': 2}\n    assert field.vocab == vocab\n\n\ndef test_bow_build_vocab_lower():\n    field = BoWField(min_freq=2)\n    assert field.vocab == {\'<unk>\': 0}\n\n    dummy = [""justo luctus Praesent Luctus"", ""luctus praesent""]\n    field.setup(dummy)\n\n    vocab = {\'<unk>\': 0, \'luctus\': 1}\n    assert field.vocab == vocab\n\n\ndef test_build_vocab_empty():\n    field = TextField(pad_token=None, unk_token=None)\n    assert field.vocab == dict()\n\n    dummy = [""justo Praesent luctus"", ""luctus praesent""]\n    field.setup(dummy)\n\n    vocab = {\'justo\': 0, \'Praesent\': 1, \'luctus\': 2, \'praesent\': 3}\n    assert field.vocab == vocab\n\n\ndef test_build_vocab_build_vocab_from_embeddings():\n    """"""\n    This test shows that all fields in the embeddings will be included.\n\n    In embeddings and data:\n        blue\n        green\n        yellow\n    In embeddings only:\n        purple\n        gold\n    In data only:\n        white\n\n    Expected vocab:\n        blue\n        green\n        yellow\n        purple\n        gold\n        white\n    """"""\n\n    model = KeyedVectors(10)\n    model.add(\'purple\', np.random.rand(10))\n    model.add(\'gold\', np.random.rand(10))\n    model.add(\'<unk>\', np.random.rand(10))\n    model.add(\'blue\', np.random.rand(10))\n    model.add(\'green\', np.random.rand(10))\n    model.add(\'<pad>\', np.random.rand(10))\n    model.add(\'yellow\', np.random.rand(10))\n\n    with tempfile.NamedTemporaryFile() as tmpfile:\n        model.save(tmpfile.name)\n\n        field = TextField.from_embeddings(\n            embeddings=tmpfile.name,\n            embeddings_format=\'gensim\',\n            build_vocab_from_embeddings=True,\n        )\n\n        dummy = [""blue green"", ""yellow"", \'white\']\n\n        field.setup(dummy)\n\n    # assert vocab setup in expected order\n    assert field.vocab == odict([\n        (\'<pad>\', 0), (\'<unk>\', 1), (\'blue\', 2), (\'green\', 3),\n        (\'yellow\', 4), (\'white\', 1), (\'purple\', 5), (\'gold\', 6),\n    ])\n\n    # assert embedding matrix organized in expected order\n    assert torch.equal(\n        field.embedding_matrix,\n        torch.stack([\n            torch.tensor(model[\'<pad>\']), torch.tensor(model[\'<unk>\']),\n            torch.tensor(model[\'blue\']), torch.tensor(model[\'green\']),\n            torch.tensor(model[\'yellow\']), torch.tensor(model[\'purple\']),\n            torch.tensor(model[\'gold\'])\n        ]),\n    )\n\n\ndef test_build_vocab_decorators_specials():\n    field = TextField(pad_token=\'<pad>\', unk_token=\'<unk>\',\n                      sos_token=\'<sos>\', eos_token=\'<eos>\')\n\n    field._build_vocab()\n\n    assert field.vocab == {\'<pad>\': 0, \'<unk>\': 1, \'<sos>\': 2, \'<eos>\': 3}\n    dummy = [""justo Praesent luctus"", ""luctus praesent""]\n    field._build_vocab(dummy)\n\n    vocab = {\'<pad>\': 0, \'<unk>\': 1, \'<sos>\': 2, \'<eos>\': 3,\n             \'justo\': 4, \'Praesent\': 5, \'luctus\': 6, \'praesent\': 7}\n    assert field.vocab == vocab\n\n\ndef test_build_vocab_decorators_missing_specials():\n    field = TextField(pad_token=None, unk_token=None,\n                      sos_token=\'<sos>\', eos_token=\'<eos>\')\n    field._build_vocab()\n\n    assert field.vocab == {\'<sos>\': 0, \'<eos>\': 1}\n    dummy = [""justo Praesent luctus"", ""luctus praesent""]\n    field._build_vocab(dummy)\n\n    vocab = {\'<sos>\': 0, \'<eos>\': 1, \'justo\': 2, \'Praesent\': 3, \'luctus\': 4, \'praesent\': 5}\n    assert field.vocab == vocab\n\n\ndef test_load_embeddings():\n    field = TextField.from_embeddings(\n        embeddings=""tests/data/dummy_embeddings/test.txt"",\n        pad_token=None,\n        unk_init_all=False,\n    )\n    dummy = ""a test !""\n    field.setup([dummy])\n\n    # Now we have embeddings to check against\n    true_embeddings = torch.tensor([[0.9, 0.1, 0.2, 0.3], [0.4, 0.5, 0.6, 0.7]])\n    assert len(field.embedding_matrix) == 3\n    assert torch.all(torch.eq(field.embedding_matrix[1:3], true_embeddings))\n\n\ndef test_load_embeddings_with_extra_tokens():\n    field = TextField.from_embeddings(\n        embeddings=""tests/data/dummy_embeddings/test.txt"",\n        pad_token=None,\n        unk_init_all=False,\n        additional_special_tokens=[\'<a>\', \'<b>\', \'<c>\']\n    )\n    dummy = ""a test ! <a> <b> ""\n    field.setup([dummy])\n    assert \'<a>\' in field.vocab and \'<b>\' in field.vocab and \'<c>\' in field.vocab\n    assert field.embedding_matrix[field.vocab[\'<a>\']].size(-1) == 4\n    assert field.embedding_matrix[field.vocab[\'<b>\']].size(-1) == 4\n    assert all(field.embedding_matrix[field.vocab[\'<b>\']] != field.embedding_matrix[field.vocab[\'<c>\']])\n\n\ndef test_load_embeddings_legacy():\n    field = TextField(\n        embeddings=""tests/data/dummy_embeddings/test.txt"",\n        pad_token=None,\n        unk_init_all=False,\n    )\n    dummy = ""a test !""\n    field.setup([dummy])\n\n    # Now we have embeddings to check against\n    true_embeddings = torch.tensor([[0.9, 0.1, 0.2, 0.3], [0.4, 0.5, 0.6, 0.7]])\n    assert len(field.embedding_matrix) == 3\n    assert torch.all(torch.eq(field.embedding_matrix[1:3], true_embeddings))\n\n\ndef test_load_embeddings_empty_voc():\n    field = TextField.from_embeddings(\n        embeddings=""tests/data/dummy_embeddings/test.txt"",\n        pad_token=None,\n        unk_init_all=True,\n    )\n\n    dummy = ""justo Praesent luctus justo praesent""\n    field.setup([dummy])\n\n    # No embeddings in the data, so get zeros\n    assert len(field.embedding_matrix) == 5\n\n    field = TextField.from_embeddings(\n        embeddings=""tests/data/dummy_embeddings/test.txt"",\n        pad_token=None,\n        unk_init_all=False,\n    )\n\n    dummy = ""justo Praesent luctus justo praesent""\n    field.setup([dummy])\n\n    # No embeddings in the data, so get zeros\n    assert len(field.embedding_matrix) == 1\n\n\ndef test_text_process():\n    field = TextField()\n    field.setup()\n\n    dummy = ""justo Praesent luctus justo praesent""\n    assert list(field.process(dummy)) == [1, 1, 1, 1, 1]\n\n    field.setup([dummy])\n    assert list(field.process(dummy)) == [2, 3, 4, 2, 5]\n\n\ndef test_bow_text_process():\n    field = BoWField(min_freq=2)\n\n    dummy = ""justo praesent luctus justo praesent""\n    field.setup([dummy])\n    assert list(field.process(dummy)) == [0, 2, 2]\n\n\ndef test_bow_text_process_normalize():\n    field = BoWField(min_freq=2, normalize=True)\n\n    dummy = ""justo praesent luctus justo praesent""\n    field.setup([dummy])\n    assert list(field.process(dummy)) == [0, 0.5, 0.5]\n\n\ndef test_bow_text_process_normalize_scale():\n    field = BoWField(min_freq=2, normalize=True, scale_factor=10)\n\n    dummy = ""justo praesent luctus justo praesent""\n    field.setup([dummy])\n    assert list(field.process(dummy)) == [0, 5, 5]\n\n\ndef test_bow_text_process_scale():\n    with pytest.raises(ValueError):\n        _ = BoWField(min_freq=2, scale_factor=10)\n\n\ndef test_text_process_lower():\n    field = TextField(lower=True)\n    field.setup()\n\n    dummy = ""justo Praesent luctus justo praesent""\n    assert list(field.process(dummy)) == [1, 1, 1, 1, 1]\n\n    field.setup([dummy])\n    assert list(field.process(dummy)) == [2, 3, 4, 2, 3]\n\n\ndef test_text_process_unk():\n    field = TextField(unk_token=None)\n\n    dummy = ""justo Praesent luctus justo praesent""\n    with pytest.raises(Exception):\n        field.process(dummy)\n\n\ndef recursive_tensor_to_list(data):\n    if isinstance(data, list):\n        return [recursive_tensor_to_list(d) for d in data]\n    elif isinstance(data, dict):\n        return dict((k, recursive_tensor_to_list(v)) for k, v in data.items())\n    elif isinstance(data, torch.Tensor):\n        return data.tolist()\n\n\ndef test_setup_with_extra_tokens():\n    field = TextField.from_embeddings(\n        embeddings=""tests/data/dummy_embeddings/test.txt"",\n        pad_token=None,\n        unk_init_all=False,\n        additional_special_tokens=[\'<a>\', \'<b>\', \'<c>\']\n    )\n\n    dummy = ""this is a test""\n    field.setup([dummy])\n    assert recursive_tensor_to_list(field.process(dummy)) == [4, 5, 6, 7]\n\n    dummy = ""this is a test <a> <c>""\n    assert recursive_tensor_to_list(field.process(dummy)) == [4, 5, 6, 7, 1, 3]\n\n\ndef test_text_process_list():\n    field = TextField(lower=True)\n    field.setup()\n    dummy = [\n        [""justo Praesent luctus"", ""luctus praesent""],\n        [""justo Praesent luctus"", ""luctus praesent est""]]\n    assert recursive_tensor_to_list(field.process(dummy)) == [\n        [[1, 1, 1], [1, 1]],\n        [[1, 1, 1], [1, 1, 1]]]\n\n    field.setup(dummy)\n    assert recursive_tensor_to_list(field.process(dummy)) == [\n        [[2, 3, 4], [4, 3]],\n        [[2, 3, 4], [4, 3, 5]]]\n\n\ndef test_text_process_dict():\n    field = TextField(lower=True)\n    field.setup()\n    dummy = {\n        \'text1\': ""justo Praesent luctus luctus praesent"",\n        \'text2\': ""justo Praesent luctus luctus praesent est""}\n    assert recursive_tensor_to_list(field.process(dummy)) == {\n        \'text1\': [1, 1, 1, 1, 1],\n        \'text2\': [1, 1, 1, 1, 1, 1]\n    }\n    field.setup([dummy])\n    assert recursive_tensor_to_list(field.process(dummy)) == {\n        \'text1\': [2, 3, 4, 4, 3],\n        \'text2\': [2, 3, 4, 4, 3, 5]\n    }\n\n\ndef test_text_process_nested_list_in_dict():\n    field = TextField(lower=True)\n    field.setup()\n    dummy = [{\n        \'text1\': [""justo Praesent luctus"", ""luctus praesent""],\n        \'text2\': [""justo Praesent luctus"", ""luctus praesent est""]}]\n    assert recursive_tensor_to_list(field.process(dummy)) == [{\n            \'text1\': [[1, 1, 1], [1, 1]],\n            \'text2\': [[1, 1, 1], [1, 1, 1]]\n        }]\n    field.setup(dummy)\n    assert recursive_tensor_to_list(field.process(dummy)) == [{\n            \'text1\': [[2, 3, 4], [4, 3]],\n            \'text2\': [[2, 3, 4], [4, 3, 5]]\n        }]\n\n\ndef test_text_process_nested_dict_in_list_in_dict():\n    field = TextField(lower=True)\n    field.setup()\n    dummy = [\n        {\'text1\': ""justo Praesent luctus"", \'text2\': ""luctus praesent""},\n        {\'text3\': [""justo Praesent luctus"", ""luctus praesent est""]}\n    ]\n    assert recursive_tensor_to_list(field.process(dummy)) == [{\n            \'text1\': [1, 1, 1],\n            \'text2\': [1, 1]}, {\n            \'text3\': [[1, 1, 1], [1, 1, 1]]\n        }]\n\n    field.setup(dummy)\n    assert recursive_tensor_to_list(field.process(dummy)) == [{\n            \'text1\': [2, 3, 4],\n            \'text2\': [4, 3]}, {\n            \'text3\': [[2, 3, 4], [4, 3, 5]]\n        }]\n'"
tests/unit/learn/test_trainer.py,2,"b""import pytest\n\nfrom torch.nn import NLLLoss\nfrom torch.optim import Adam\n\nfrom flambe.learn import train\n\nfrom flambe.dataset import Dataset\nfrom flambe.compile import Schema, State, Component, Link\nfrom flambe.learn.utils import select_device\nfrom flambe.nn import Module  # type: ignore[attr-defined]\nfrom flambe.sampler import BaseSampler\nfrom flambe.metric import Metric\nfrom flambe.logging import log\n\n\nclass DummyDataset(Dataset):\n    @property\n    def train(self):\n        return [['hello']]\n\n    @property\n    def val(self):\n        pass\n\n    @property\n    def test(self):\n        pass\n\n\nclass DummyModel(Module):\n    pass\n\n\n@pytest.fixture()\ndef trainer():\n    return train.Trainer(\n        dataset=DummyDataset(),\n        train_sampler=BaseSampler(),\n        val_sampler=BaseSampler(),\n        model=DummyModel(),\n        loss_fn=NLLLoss(),\n        metric_fn=NLLLoss(),\n        optimizer=Adam,\n        extra_validation_metrics=[NLLLoss()] * 3\n    )\n\n\ndef test_validation_metrics_property(trainer):\n    assert trainer.validation_metrics == trainer.extra_validation_metrics\n    assert len(trainer.validation_metrics) == 3\n"""
tests/unit/metrics/__init__.py,0,b''
tests/unit/metrics/test_dev.py,104,"b'import pytest\nimport torch\nimport sklearn.metrics\nimport numpy as np\n\nfrom flambe.metric import Accuracy, AUC, MultiClassAUC, Perplexity, BPC, F1\nfrom flambe.metric import BinaryRecall, BinaryPrecision, BinaryAccuracy\nfrom flambe.metric import Recall\nfrom pytest import approx\n\nNUMERIC_PRECISION = 1e-2\n\n\ndef metric_test_case(predicted, true, metric, expected):\n    assert metric(predicted, true).item() == approx(expected, NUMERIC_PRECISION)\n\n\ndef test_auc_full():\n    """"""Test random score list.""""""\n    y_pred = np.random.randint(0, 10000, 100)\n    y_true = np.random.randint(0, 2, 100)\n\n    metric_test_case(y_pred, y_true, AUC(), sklearn.metrics.roc_auc_score(y_true, y_pred))\n\n\ndef test_auc_threshold():\n    """"""To compute rates:\n         -> fpr: number of false positives / total number of negatives\n         -> tpr: number of true positives / total number of positives\n    In this example, consider the first threshold of 0.1, and consider everything\n    higher than 0.1 as positives, and everything lower as negatives:\n        fpr = 1 / 2 = 0.5\n        tpr = 1\n    Thus, if the max_fpr rate is 0.1, the tpr is 0, and if the max_fpr is 0.5,\n    then the tpr is 1. So we get an auc of 0 for the 0.1 threshold, and 0.5\n    for the 0.5 threshold.\n    For more details: https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n    """"""\n    y_pred = torch.tensor([0.1, 0.2, 0.3, 0.4])\n    y_true = torch.tensor([0, 1, 0, 1])\n\n    metric_test_case(y_pred, y_true, AUC(max_fpr=1.), sklearn.metrics.roc_auc_score(y_true, y_pred))\n    metric_test_case(y_pred, y_true, AUC(max_fpr=0.500001), 0.5)\n    metric_test_case(y_pred, y_true, AUC(max_fpr=0.1), 0.5)\n\n\ndef test_auc_empty():\n    """"""Should be completely random on an empty list""""""\n    y_pred = torch.tensor([])\n    y_true = torch.tensor([])\n\n    auc = AUC()\n    assert auc(y_pred, y_true).item() == approx(0.5, NUMERIC_PRECISION)\n\n\ndef test_multiclass_auc_full_one_hot():\n    """"""simple base case with one-hot labels""""""\n    n_classes = 100\n    y_pred = np.random.uniform(0, 1, (100, n_classes))\n    y_true = np.zeros((100, n_classes))\n    y_true[np.arange(100), np.random.randint(n_classes)] = 1\n    sklearn_score = sklearn.metrics.roc_auc_score(y_true.flatten(), y_pred.flatten())\n    metric_test_case(torch.tensor(y_pred), torch.tensor(y_true), MultiClassAUC(), sklearn_score)\n\n\ndef test_multiclass_auc_full_indexed():\n    """"""simple base case with index labels""""""\n    n_classes = 100\n    y_pred = np.random.uniform(0, 1, (100, n_classes))\n    y_true = np.random.randint(0, n_classes, 100)\n    # one hot conversion\n    y_onehot = np.zeros((100, n_classes))\n    y_onehot[np.arange(y_true.size), y_true] = 1\n    sklearn_score = sklearn.metrics.roc_auc_score(y_onehot.flatten(), y_pred.flatten())\n    metric_test_case(torch.tensor(y_pred), torch.tensor(y_true), MultiClassAUC(), sklearn_score)\n\n\ndef test_multiclass_auc_empty():\n    """"""Should be completely random on an empty list""""""\n    y_pred = torch.tensor([])\n    y_true = torch.tensor([])\n\n    auc = MultiClassAUC()\n    assert auc(y_pred, y_true).item() == approx(0.5, NUMERIC_PRECISION)\n\n\ndef test_accuracy():\n    """"""Test random score list.""""""\n    metric_test_case(torch.tensor([[0.1, 0.2], [0.9, 0.1]]), torch.tensor([1, 1]), Accuracy(), 0.5)\n    metric_test_case(torch.tensor([[1.0, 0.0], [0.6, 0.4]]), torch.tensor([1, 1]), Accuracy(), 0)\n\n\ndef test_recall_at_1_index_preds():\n    """"""Test random score list.""""""\n    # recall at 1 is just accuracy\n    # num_class sample dim\n    metric_test_case(torch.tensor([[0.1, 0.2], [0.9, 0.1]]), torch.tensor([1, 1]), Recall(), 0.5)\n    metric_test_case(torch.tensor([[1.0, 0.0], [0.6, 0.4]]), torch.tensor([1, 1]), Recall(), 0)\n\n\ndef test_recall_at_1_class_preds():\n    """"""Test random score list.""""""\n    # recall at 1 is just accuracy\n    # num_class sample dim\n    metric_test_case(torch.tensor([[0.1, 0.2], [0.9, 0.1]]), torch.tensor([[0.1, 0.5], [0.2, 0.7]]), Recall(), 0.5)\n    metric_test_case(torch.tensor([[1.0, 0.0], [0.6, 0.4]]), torch.tensor([[0.1, 0.5], [0.2, 0.7]]), Recall(), 0)\n\n\ndef test_recall_at_k_index_preds():\n    """"""Test random score list.""""""\n    # recall at 1 is just accuracy\n    # num_class sample dim\n    metric_test_case(\n        torch.tensor([\n            [0.1, 0.2, 0.3, 0.4],\n            [0.9, 0.8, 0.4, 0.1]]),\n        torch.tensor(\n            [3, 2]),\n        Recall(2), 0.5)\n    metric_test_case(\n        torch.tensor([\n            [0.1, 0.2, 0.3, 0.4],\n            [0.9, 0.8, 0.4, 0.1]]),\n        torch.tensor(\n            [3, 1]),\n        Recall(2), 1)\n    metric_test_case(\n        torch.tensor([\n            [0.1, 0.2, 0.3, 0.4],\n            [0.9, 0.8, 0.4, 0.1]]),\n        torch.tensor(\n            [1, 2]),\n        Recall(2), 0)\n\n\ndef test_recall_at_k_class_preds():\n    """"""Test random score list.""""""\n    # recall at 1 is just accuracy\n    # num_class sample dim\n    metric_test_case(\n        torch.tensor([\n            [0.1, 0.2, 0.3, 0.4],\n            [0.9, 0.8, 0.4, 0.1]]),\n        torch.tensor([\n            [0.1, 0.2, 0.4, 0.3],\n            [0.1, 0.2, 0.4, 0.5]]),\n        Recall(2), 0.5)\n    metric_test_case(\n        torch.tensor([\n            [0.1, 0.2, 0.3, 0.4],\n            [0.9, 0.8, 0.4, 0.1]]),\n        torch.tensor([\n            [0.01, 0.02, 0.2, 0.2],\n            [0., 0.8, 0., 0.]]),\n        Recall(2), 1)\n    metric_test_case(\n        torch.tensor([\n            [0.1, 0.2, 0.3, 0.4],\n            [0.9, 0.8, 0.4, 0.1]]),\n        torch.tensor([\n            [0.5, 0.2, 0.1, 0.],\n            [0., 0.1, 0.4, 0.3]]),\n        Recall(2), 0)\n\n\ndef test_aggregation_accuracy():\n    """""" Test the aggregation functionality """"""\n    metric_state = {}\n    metric = Accuracy()\n    metric.aggregate(metric_state, torch.tensor([[0.1, 0.2], [0.9, 0.1]]), torch.tensor([1, 1]))\n    assert metric_state[\'accumulated_score\'] == 0.5\n    metric.aggregate(metric_state, torch.tensor([[1.0, 0.0], [0.6, 0.4]]), torch.tensor([1, 1]))\n    assert metric_state[\'accumulated_score\'] == 0.25\n    assert metric_state[\'sample_count\'] == 4\n    assert metric.finalize(metric_state) == 0.25\n\n\ndef test_aggregation_auc():\n    # for AUC (different aggregation functionality)\n    metric_state = {}\n    metric = AUC()\n    y_pred = torch.randint(0, 10000, [100])\n    y_true = torch.randint(0, 2, [100])\n    for yp, yt in zip(torch.split(y_pred, 10), torch.split(y_true, 10)):\n        metric.aggregate(metric_state, yp, yt)\n    assert metric.finalize(metric_state) == approx(sklearn.metrics.roc_auc_score(y_true, y_pred), NUMERIC_PRECISION)\n\n\ndef test_aggregation_perplexity():\n    # for AUC (different aggregation functionality)\n    metric_state = {}\n    metric = Perplexity()\n    y_pred = torch.randn(size=(100, 50))\n    y_true = torch.randint(0, 50, (100, ))\n    for yp, yt in zip(torch.split(y_pred, 10), torch.split(y_true, 10)):\n        metric.aggregate(metric_state, yp, yt)\n    gt_value = torch.exp(torch.nn.CrossEntropyLoss()(y_pred, y_true).mean()).item()\n    assert metric.finalize(metric_state) == approx(gt_value, NUMERIC_PRECISION)\n\n\ndef test_perplexity():\n    """"""Test perplexity""""""\n    metric_test_case(torch.tensor([[0.2, 0.8], [0.9, 0.1]]), torch.tensor([0, 0]), Perplexity(), 2.022418975830078)\n\n\ndef test_bpc():\n    """"""Test BPC""""""\n    metric_test_case(torch.tensor([[0.2, 0.8], [0.9, 0.1]]), torch.tensor([0, 0]), BPC(), 1.0161)\n\n\ndef test_binary_precision():\n    """"""\n        precision = tp/tp+fp\n    """"""\n    y_pred = torch.tensor([0.8, 0.1, 0.7, 0.1])\n\n    metric_test_case(y_pred, torch.tensor([1, 1, 1, 1]), BinaryPrecision(), 1)\n    metric_test_case(y_pred, torch.tensor([1, 0, 1, 0]), BinaryPrecision(), 1)\n    metric_test_case(y_pred, torch.tensor([0, 0, 0, 0]), BinaryPrecision(), 0)\n    metric_test_case(y_pred, torch.tensor([0, 1, 0, 1]), BinaryPrecision(), 0)\n\n    # fp fn tp fn\n    metric_test_case(y_pred, torch.tensor([0, 1, 1, 1]), BinaryPrecision(), 0.5)\n\n    # tp tp\n    metric_test_case(y_pred, torch.tensor([0, 1, 1, 1]), BinaryPrecision(0.65), 0.5)\n\n    # tp tp\n    metric_test_case(y_pred, torch.tensor([0, 1, 1, 1]), BinaryPrecision(0.75), 0)\n\n    # tp fn tp tn\n    metric_test_case(y_pred, torch.tensor([1, 1, 1, 0]), BinaryPrecision(), 1)\n\n\ndef test_inverted_binary_precision():\n    """"""\n        precision = tp/tp+fp\n    """"""\n    y_pred = torch.tensor([0.8, 0.1, 0.7, 0.1])\n\n    metric_test_case(y_pred, torch.tensor([1, 1, 1, 1]), BinaryPrecision(positive_label=0), 0)\n    metric_test_case(y_pred, torch.tensor([1, 0, 1, 0]), BinaryPrecision(positive_label=0), 1)\n    metric_test_case(y_pred, torch.tensor([0, 0, 0, 0]), BinaryPrecision(positive_label=0), 1)\n    metric_test_case(y_pred, torch.tensor([0, 1, 0, 1]), BinaryPrecision(positive_label=0), 0)\n    metric_test_case(y_pred, torch.tensor([0, 0, 0, 1]), BinaryPrecision(positive_label=0), 0.5)\n    metric_test_case(y_pred, torch.tensor([0, 1, 1, 1]), BinaryPrecision(positive_label=0), 0)\n    metric_test_case(y_pred, torch.tensor([0, 0, 1, 1]), BinaryPrecision(0.65, positive_label=0), 0.5)\n    metric_test_case(y_pred, torch.tensor([0, 0, 0, 1]), BinaryPrecision(0.75, positive_label=0), 2 / 3)\n\n\ndef test_binary_recall():\n    """"""\n    recall = tp/tp+fn\n    """"""\n    y_pred = torch.tensor([0.75, 0.3, 0.6, 0.1])\n\n    # fp fn fp fn\n    metric_test_case(y_pred, torch.tensor([0, 1, 0, 1]), BinaryRecall(), 0)\n\n    # tp fn tp fn\n    metric_test_case(y_pred, torch.tensor([1, 1, 1, 1]), BinaryRecall(), 0.5)\n\n    # tp fn fn fn\n    metric_test_case(y_pred, torch.tensor([1, 1, 1, 1]), BinaryRecall(0.7), 0.25)\n\n    # tp tp tp fn\n    metric_test_case(y_pred, torch.tensor([1, 1, 1, 1]), BinaryRecall(0.25), 0.75)\n\n    # all tps\n    metric_test_case(y_pred, torch.tensor([1, 1, 1, 1]), BinaryRecall(0.05), 1)\n\n    # fp tn fp tn\n    metric_test_case(y_pred, torch.tensor([0, 0, 0, 0]), BinaryRecall(), 0)\n\n    # tp tn tp tn\n    metric_test_case(y_pred, torch.tensor([1, 0, 1, 0]), BinaryRecall(), 1)\n\n    # tp tn fn tn\n    metric_test_case(y_pred, torch.tensor([1, 0, 1, 0]), BinaryRecall(0.7), 0.5)\n\n    # fn tn fn tn\n    metric_test_case(y_pred, torch.tensor([1, 0, 1, 0]), BinaryRecall(0.8), 0)\n\n\n@pytest.mark.parametrize(""true_classes, metric, result"",\n                         [(torch.tensor([1, 1, 1, 1]), BinaryRecall(positive_label=0), 0),\n                          (torch.tensor([1, 0, 1, 0]), BinaryRecall(positive_label=0), 1),\n                          (torch.tensor([0, 0, 0, 0]), BinaryRecall(positive_label=0), 0.5),\n                          (torch.tensor([0, 1, 0, 1]), BinaryRecall(positive_label=0), 0),\n                          (torch.tensor([0, 0, 0, 1]), BinaryRecall(positive_label=0), 1 / 3),\n                          (torch.tensor([0, 1, 1, 1]), BinaryRecall(positive_label=0), 0),\n                          (torch.tensor([0, 0, 1, 1]), BinaryRecall(0.65, positive_label=0), 0.5),\n                          (torch.tensor([0, 0, 0, 1]), BinaryRecall(0.75, positive_label=0), 2 / 3)]\n                         )\ndef test_inverted_binary_recall(true_classes, metric, result):\n    """"""\n        precision = tp/tp+fn\n    """"""\n    y_pred = torch.tensor([0.8, 0.1, 0.7, 0.1])\n    metric_test_case(y_pred, true_classes, metric, result)\n\n\n@pytest.mark.parametrize(""true_classes, metric, result"",\n                         [(torch.tensor([1, 0, 1, 0]), BinaryAccuracy(), 1),\n                          (torch.tensor([1, 1, 1, 0]), BinaryAccuracy(), 3 / 4),\n                          (torch.tensor([1, 1, 1, 1]), BinaryAccuracy(), 0.5),\n                          (torch.tensor([0, 0, 0, 0]), BinaryAccuracy(), 0.5),\n                          (torch.tensor([0, 1, 0, 1]), BinaryAccuracy(), 0),\n                          (torch.tensor([1, 0, 0, 0]), BinaryAccuracy(0.75), 1)\n                          ])\ndef test_binary_accuracy(true_classes, metric, result):\n    """"""\n        precision = tp/tp+fn\n    """"""\n    y_pred = torch.tensor([0.8, 0.1, 0.7, 0.1])\n\n    metric_test_case(y_pred, true_classes, metric, result)\n\n\ndef test_multiple_dims():\n    """"""\n        precision = tp/tp+fn\n    """"""\n    y_pred = torch.tensor([[0.8], [0.1], [0.7], [0.1]])\n    metric_test_case(y_pred, torch.tensor([1, 0, 1, 0]), BinaryAccuracy(), 1)\n\n    y_pred = torch.tensor([[[0.8]], [[0.1]], [[0.7]], [[0.1]]])\n    metric_test_case(y_pred, torch.tensor([1, 0, 1, 0]), BinaryAccuracy(), 1)\n\n    y_pred = torch.tensor([[[0.8]], [[0.1]], [[0.7]], [[0.1]]])\n    metric_test_case(y_pred, torch.tensor([[1], [0], [1], [0]]), BinaryAccuracy(), 1)\n\n\n@pytest.mark.parametrize(""true_classes, metric, result"",\n                         [(torch.tensor([1, 0, 1, 0]), F1(), 1),\n                          (torch.tensor([1, 1, 1, 1]), F1(), 1 / 1.5),\n                          (torch.tensor([0, 0, 0, 0]), F1(), 0),\n                          (torch.tensor([0, 1, 0, 1]), F1(), 0),\n                          (torch.tensor([1, 0, 1, 1]), F1(), 0.80)\n                          ])\ndef test_f1(true_classes, metric, result):\n    y_pred = torch.tensor([0.8, 0.1, 0.7, 0.1])\n    metric_test_case(y_pred, true_classes, metric, result)\n\n\ndef test_global_aggregate():\n    metric_data = {\n        AUC: (torch.randint(0, 10000, (100, )), torch.randint(0, 2, (100, ))),\n        MultiClassAUC: (torch.rand(size=(100, 30)), torch.randint(0, 30, size=(100, ))),\n        Accuracy: (torch.randn((100, 30)), torch.randn((100, 30))),\n        BPC: (torch.randn((100, 30)), torch.randint(0, 30, (100, ))),\n        BinaryAccuracy: (torch.rand(size=(100, )), torch.randint(0, 2, size=(100, ))),\n        BinaryPrecision: (torch.rand(size=(100, )), torch.randint(0, 2, size=(100, ))),\n        BinaryRecall: (torch.rand(size=(100, )), torch.randint(0, 2, size=(100, ))),\n        F1: (torch.rand(size=(100, )), torch.randint(0, 2, size=(100, ))),\n        Recall: (torch.randn((100, 30)), torch.randint(0, 30, (100, ))),\n        Perplexity: (torch.randn((100, 30)), torch.randint(0, 30, (100, ))),\n    }\n\n    for metric_class, (y_pred, y_true) in metric_data.items():\n        metric = metric_class()\n        metric_state = {}\n        for yp, yt in zip(torch.split(y_pred, 10), torch.split(y_true, 10)):\n            metric.aggregate(metric_state, yp, yt)\n        assert metric.finalize(metric_state) == approx(metric(y_pred, y_true).item()), \\\n            f\'Metric {metric} failed aggregation.\'\n'"
tests/unit/metrics/test_loss.py,4,"b'import math\nimport torch\n\nfrom flambe.metric import MultiLabelCrossEntropy, MultiLabelNLLLoss\n\n\ndef test_cross_entropy_one_hot():\n    """"""Test cross entropy loss when one hot""""""\n    y_pred = torch.tensor([[0.2, 0.8], [0.9, 0.1]])\n    y_true = torch.tensor([[1, 0], [1, 0]])\n\n    loss = MultiLabelCrossEntropy()\n    assert abs(loss(y_pred, y_true).item() - 0.70429) < 1e-2\n\n\ndef test_nllloss_one_hot():\n    """"""Test negative log likelihood loss when one hot""""""\n    y_pred = torch.tensor([[0.2, 0.8], [0.9, 0.1]])\n    y_true = torch.tensor([[1, 0], [1, 0]])\n\n    loss = MultiLabelNLLLoss()\n    assert abs(loss(y_pred, y_true).item() + 0.55) < 1e-2\n'"
tests/unit/model/test_logistic_regression.py,1,"b'import torch\n\nfrom flambe.model import LogisticRegression\nfrom torch import Tensor\nfrom numpy import isclose\n\nNUMERIC_PRECISION = 1e-2\n\n\ndef test_number_of_parameters():\n    regression = LogisticRegression(2)\n    assert regression.num_parameters() == 3\n\n    regression = LogisticRegression(10)\n    assert regression.num_parameters() == 11\n\n\ndef test_forward_pass_with_target_param():\n    regression = LogisticRegression(2)\n    forward = regression(Tensor([[1, 4]]), target=Tensor([[5, 6]]))\n    transformed_target = forward[1]\n    assert transformed_target.dtype == torch.float32\n    assert isclose(transformed_target.numpy(), [[5., 6.]], rtol=NUMERIC_PRECISION).all()\n\n\ndef test_forward_pass_is_sigmoid():\n    regression = LogisticRegression(2)\n\n    set_parameters(regression, Tensor([[5, 6]]), Tensor([[3]]))\n\n    assert isclose(regression(Tensor([[1, 4]])).item(),   [[1.]], rtol=NUMERIC_PRECISION)\n    assert isclose(regression(Tensor([[0, 0]])).item(), [[0.95]], rtol=NUMERIC_PRECISION)\n    assert isclose(regression(Tensor([[1.5, 2.5]])).item(), [[1.]], rtol=NUMERIC_PRECISION)\n    assert isclose(regression(Tensor([[-1, 0]])).item(), [[0.12]], rtol=NUMERIC_PRECISION)\n    assert isclose(regression(Tensor([[-0.5, 0]])).item(), [[0.62]], rtol=NUMERIC_PRECISION)\n\n\ndef set_parameters(model, weight, bias):\n    """"""\n    This depends and has knowledge of the inner structure of objects. According\n    to https://github.com/pytorch/pytorch/issues/565 there\'s no plan of adding\n    a feature to inject weights/biases dependencies.\n    Probably we can iterate this idea to patch this.\n    """"""\n    for name, param in model.named_parameters():\n        if ""bias"" in name:\n            param.data = bias\n        if ""weight"" in name:\n            param.data = weight\n'"
tests/unit/nlp/__init__.py,0,b''
tests/unit/nn/test_cnn.py,5,"b'import pytest\nfrom flambe.nn import CNNEncoder\nimport torch\nimport random\n\n\ndef test_invalid_conv_dim():\n    with pytest.raises(ValueError):\n        cnn_enc = CNNEncoder(\n            input_channels=3,\n            channels=[8,8,8],\n            conv_dim=4\n        )\n\n\ndef test_invalid_conv_dim2():\n    with pytest.raises(ValueError):\n        cnn_enc = CNNEncoder(\n            input_channels=3,\n            channels=[8,8,8],\n            conv_dim=0\n        )\n\n\ndef test_invalid_channels_kernels():\n    with pytest.raises(ValueError):\n        cnn_enc = CNNEncoder(\n            input_channels=3,\n            channels=[8,8,8],\n            kernel_size=[3,3],\n            conv_dim=0\n        )\n\n\ndef test_cnn_blocks_size():\n    for i in range(5):\n        cnn_enc = CNNEncoder(\n            input_channels=3,\n            channels=list(range(8, 8+i))\n        )\n\n        assert len(cnn_enc.cnn) == i\n\n\ndef test_cnn_blocks_filters():\n    aux = [random.randint(5, 32) for _ in range(10)]\n\n    cnn_enc = CNNEncoder(\n        input_channels=aux[0],\n        channels=aux[1:]\n    )\n\n    for i, l in enumerate(cnn_enc.cnn):\n        conv = l[0]\n        assert conv.in_channels == aux[i]\n        assert conv.out_channels == aux[i+1]\n\n\ndef test_kernel_sizes():\n    channels = [random.randint(5, 32) for _ in range(10)]\n    kernels = [random.randint(5, 32) for _ in range(10)]\n\n    cnn_enc = CNNEncoder(\n        input_channels=3,\n        channels=channels,\n        kernel_size=5,\n        conv_dim=3\n    )\n    for i in cnn_enc.cnn:\n        conv = i[0]\n        assert conv.kernel_size == (5,5,5)\n\n\ndef test_kernel_sizes2():\n    channels = [random.randint(5, 32) for _ in range(10)]\n    kernels = [random.randint(5, 32) for _ in range(10)]\n\n    cnn_enc = CNNEncoder(\n        input_channels=3,\n        channels=channels,\n        kernel_size=kernels,\n        conv_dim=3\n    )\n    for i, b in enumerate(cnn_enc.cnn):\n        conv = b[0]\n        assert conv.kernel_size == (kernels[i], ) * 3\n\n\ndef test_kernel_sizes3():\n    channels = [random.randint(5, 32) for _ in range(10)]\n    kernels = [random.randint(5, 32) for _ in range(8)]\n\n    with pytest.raises(ValueError):\n        cnn_enc = CNNEncoder(\n            input_channels=3,\n            channels=channels,\n            kernel_size=kernels,\n            conv_dim=3\n        )\n\n\ndef test_kernel_sizes4():\n    channels = [10]\n    kernels = [(10, 10)]\n\n    with pytest.raises(ValueError):\n        cnn_enc = CNNEncoder(\n            input_channels=3,\n            channels=channels,\n            kernel_size=kernels,\n            conv_dim=3\n        )\n\n    kernels = [(10, 10, 10, 10)]\n\n    with pytest.raises(ValueError):\n        cnn_enc = CNNEncoder(\n            input_channels=3,\n            channels=channels,\n            kernel_size=kernels,\n            conv_dim=3\n        )\n\n\ndef test_kernel_sizes5():\n    channels = [random.randint(5, 32) for _ in range(10)]\n    kernels = [(random.randint(5, 32), random.randint(5, 32), random.randint(5, 32)) for _ in range(10)]\n\n    cnn_enc = CNNEncoder(\n        input_channels=3,\n        channels=channels,\n        kernel_size=kernels,\n        conv_dim=3\n    )\n\n    for i, b in enumerate(cnn_enc.cnn):\n        conv = b[0]\n        assert conv.kernel_size == kernels[i]\n\n\ndef test_forward_passes():\n    channels = [8, 16, 32]\n    batch_size = 32\n\n    cnn_enc = CNNEncoder(\n        input_channels=3,\n        channels=channels,\n        kernel_size=3,\n        conv_dim=2,\n        pooling=torch.nn.MaxPool2d(2),\n        stride=1,\n        padding=1,\n    )\n\n    _in = torch.rand((batch_size, 3, 32, 32))\n    out = cnn_enc(_in)\n\n    assert out.shape == torch.Size([batch_size, channels[-1], 4, 4])\n\n    cnn_enc = CNNEncoder(\n        input_channels=3,\n        channels=channels,\n        kernel_size=3,\n        conv_dim=2,\n        pooling=None,\n        stride=1,\n        padding=1,\n    )\n\n    _in = torch.rand((batch_size, 3, 32, 32))\n    out = cnn_enc(_in)\n\n    assert out.shape == torch.Size([batch_size, channels[-1], 32, 32])\n'"
tests/unit/nn/test_mlp.py,11,"b'from flambe.nn import MLPEncoder\nimport torch\n\n\ndef test_forward_pass_1_layer():\n    input_size = 128\n    output_size = 64\n    batch_size = 32\n\n    # no activation\n    mlp_enc = MLPEncoder(input_size, output_size)\n    _in = torch.rand((batch_size, input_size))\n    out = mlp_enc(_in)\n\n    assert out.shape == torch.Size((batch_size, output_size))\n\n    # with activation\n    mlp_enc = MLPEncoder(\n        input_size,\n        output_size,\n        output_activation=torch.nn.ReLU(),\n    )\n    _in = torch.rand((batch_size, input_size))\n    out = mlp_enc(_in)\n\n    assert out.shape == torch.Size((batch_size, output_size))\n\n\ndef test_forward_pass_multi_layers():\n    input_size = 256\n    hidden_size = 128\n    output_size = 64\n    batch_size = 32\n\n    # no activation\n    mlp_enc = MLPEncoder(\n        input_size,\n        output_size,\n        n_layers=3,\n        hidden_size=hidden_size,\n    )\n    _in = torch.rand((batch_size, input_size))\n    out = mlp_enc(_in)\n\n    assert out.shape == torch.Size((batch_size, output_size))\n\n    # with activation\n    mlp_enc = MLPEncoder(\n        input_size,\n        output_size,\n        n_layers=3,\n        output_activation=torch.nn.ReLU(),\n        hidden_size=hidden_size,\n        hidden_activation=torch.nn.ReLU(),\n    )\n    _in = torch.rand((batch_size, input_size))\n    out = mlp_enc(_in)\n\n    assert out.shape == torch.Size((batch_size, output_size))\n'"
tests/unit/nn/test_pooling.py,24,"b'import pytest\nimport torch\nimport torch.testing\n\n\nfrom torch import Tensor, allclose\nfrom flambe.nn import pooling\n\nfrom flambe.nn import AvgPooling, SumPooling, StructuredSelfAttentivePooling, \\\n    GeneralizedPooling\n\nfrom pytest import approx\n\n\nTENSOR = Tensor(\n    [\n        [\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16]\n        ],\n        [\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16]\n        ]\n    ]\n)\n\n\nMASK = Tensor(\n    [\n        [1, 1, 0, 0],\n        [1, 1, 1, 0],\n    ]\n)\n\n\n@pytest.mark.parametrize(""size"", [(10, 20, 30),\n                                  (10, 10, 10),\n                                  (30, 20, 10),\n                                  (1, 20, 30),\n                                  (20, 1, 30),\n                                  (1, 1, 1),\n                                  (0, 10, 1)])\n@pytest.mark.parametrize(""pooling_cls"", [pooling.LastPooling, pooling.FirstPooling,\n                                         pooling.AvgPooling, pooling.SumPooling])\ndef test_last_pooling(pooling_cls, size):\n    _in = torch.rand(*size)\n    p = pooling_cls()\n    out = p(_in)\n\n    assert len(out.size()) == len(_in.size()) - 1\n    assert out.size() == torch.Size([_in.size(0), _in.size(-1)])\n\n\ndef test_last_pooling_with_mask():\n    lp = pooling.LastPooling()\n    out = lp(TENSOR, padding_mask=MASK)\n\n    assert out.size() == torch.Size([2, 4])\n\n    expected = Tensor(\n        [\n            [5, 6, 7, 8],\n            [9, 10, 11, 12]\n        ]\n    )\n    torch.testing.assert_allclose(out, expected)\n\n\ndef test_first_pooling_with_mask():\n    lp = pooling.FirstPooling()\n    out = lp(TENSOR, padding_mask=MASK)\n\n    assert out.size() == torch.Size([2, 4])\n\n    expected = Tensor(\n        [\n            [1, 2, 3, 4],\n            [1, 2, 3, 4]\n        ]\n    )\n    torch.testing.assert_allclose(out, expected)\n\n\ndef test_sum_pooling_with_mask():\n    lp = pooling.SumPooling()\n    out = lp(TENSOR, padding_mask=MASK)\n\n    assert out.size() == torch.Size([2, 4])\n\n    expected = Tensor(\n        [\n            [6, 8, 10, 12],\n            [15, 18, 21, 24]\n        ]\n    )\n    torch.testing.assert_allclose(out, expected)\n\n\ndef test_avg_pooling_with_mask():\n    lp = pooling.AvgPooling()\n    out = lp(TENSOR, padding_mask=MASK)\n\n    assert out.size() == torch.Size([2, 4])\n\n    expected = Tensor(\n        [\n            [3, 4, 5, 6],\n            [5, 6, 7, 8]\n        ]\n    )\n    torch.testing.assert_allclose(out, expected)\n\n\ndef test_avg_pooling():\n    layer = AvgPooling()\n    average = layer.forward(build_tensor())\n    assert allclose(average[0], torch.tensor([10 / 4, 14 / 4, 18 / 4], dtype=torch.float32))\n    assert allclose(average[1], torch.tensor([0, 0, 0], dtype=torch.float32))\n\n\ndef test_sum_pooling():\n    layer = SumPooling()\n    summation = layer.forward(build_tensor())\n    assert allclose(summation[0], torch.tensor([10, 14, 18], dtype=torch.float32))\n    assert allclose(summation[1], torch.tensor([0, 0, 0], dtype=torch.float32))\n\n\ndef test_structured_self_attentive_pooling_shapes():\n    dim = 300\n    layer = StructuredSelfAttentivePooling(input_size=dim)\n    input = torch.randn(100, 50, dim)\n    output = layer(input)\n    assert list(output.shape) == [100, dim]\n\n\ndef test_structured_self_attentive_pooling_zeroes():\n    dim = 300\n    layer = StructuredSelfAttentivePooling(input_size=dim)\n    input = torch.zeros(100, 50, dim)\n    output = layer(input)\n    assert list(output.shape) == [100, dim]\n    assert output.min().item() == output.max().item() == approx(0.)\n\n\ndef test_structured_self_attentive_pooling_ones():\n    dim = 300\n    layer = StructuredSelfAttentivePooling(input_size=dim)\n    input = torch.ones(100, 50, dim)\n    output = layer(input)\n    assert list(output.shape) == [100, dim]\n    assert output.min().item() == output.max().item() == approx(1.)\n\n\ndef test_vector_based_generalized_pooling_shapes():\n    dim = 300\n    layer = GeneralizedPooling(input_size=dim)\n    input = torch.randn(100, 50, dim)\n    output = layer(input)\n    assert list(output.shape) == [100, dim]\n\n\ndef build_tensor():\n    batch_size = 2\n    items = 4\n    embedding_size = 3\n    data = torch.zeros([batch_size, items, embedding_size], dtype=torch.float32)\n    data[0][0] = torch.tensor([1, 2, 3])\n    data[0][1] = torch.tensor([4, 5, 6])\n    data[0][2] = torch.tensor([1, 2, 3])\n    data[0][3] = torch.tensor([4, 5, 6])\n    return data\n\n\nif __name__ == \'__main__\':\n    test_vector_based_generalized_pooling_shapes()\n'"
tests/unit/nn/test_rnn.py,3,"b'import pytest\nfrom flambe.nn import RNNEncoder\nimport torch\nimport mock\n\n\n@pytest.mark.parametrize(""rnn_type"", [\'LSTM\', \'random\', \'\'])\ndef test_invalid_type(rnn_type):\n    with pytest.raises(ValueError):\n        RNNEncoder(\n            input_size=10,\n            hidden_size=20,\n            rnn_type=rnn_type\n        )\n\n\ndef test_sru_kwargs():\n    rnn = RNNEncoder(\n        input_size=10,\n        hidden_size=20,\n        rnn_type=\'sru\',\n        use_tanh=True\n    )\n\n    for i in rnn.rnn.rnn_lst:\n        assert i.activation == \'tanh\'\n\n\ndef test_invalid_sru_kwargs():\n    with pytest.raises(ValueError):\n        _ = RNNEncoder(\n            input_size=10,\n            hidden_size=20,\n            rnn_type=\'sru\',\n            use_tanh=True,\n            some_invalid_param=123\n        )\n\n\n@pytest.mark.parametrize(""rnn_type"", [\'lstm\', \'gru\', \'sru\'])\ndef test_forward_pass(rnn_type):\n    input_size = 300\n    output_size = 10\n    seq_len = 20\n    batch_len = 32\n    rnn = RNNEncoder(\n        input_size=input_size,\n        hidden_size=output_size,\n        n_layers=4,\n        rnn_type=rnn_type)\n\n    input_t = torch.rand(batch_len, seq_len, input_size)\n\n    output, state = rnn(input_t)\n    assert output.shape == torch.Size((batch_len, seq_len, output_size))\n\n\n@pytest.mark.parametrize(""rnn_type"", [\'lstm\', \'gru\', \'sru\'])\ndef test_transpose_on_forward_pass(rnn_type):\n    input_size = 300\n    output_size = 10\n    rnn = RNNEncoder(\n        input_size=input_size,\n        hidden_size=output_size,\n        n_layers=4,\n        rnn_type=rnn_type)\n\n    input_t = torch.rand(10, 10, input_size)\n\n    input_t.transpose = mock.Mock(side_effect=input_t.transpose)\n    output, state = rnn(input_t)\n\n    input_t.transpose.assert_called()\n    input_t.transpose.assert_called_with(0, 1)\n'"
tests/unit/optim/test_scheduler.py,1,"b'import math\n\nfrom torch.optim import Adam\n\nfrom flambe.nn import MLPEncoder\nfrom flambe.optim import NoamScheduler, WarmupLinearScheduler\n\n\ndef test_warmup_linear():\n    model = MLPEncoder(10, 10)\n    optimizer = Adam(model.parameters(), lr=0.001)\n    scheduler = WarmupLinearScheduler(optimizer, warmup=100, n_steps=200)\n\n    assert scheduler.get_lr()[0] == 0\n    scheduler.step()\n    assert scheduler.get_lr()[0] == 1e-5\n\n    for _ in range(99):\n        scheduler.step()\n    assert scheduler.get_lr()[0] == 1e-3\n    for _ in range(100):\n        scheduler.step()\n    assert scheduler.get_lr()[0] == 0\n\n\ndef test_noam():\n    model = MLPEncoder(10, 10)\n    optimizer = Adam(model.parameters(), lr=0.001)\n    scheduler = NoamScheduler(optimizer, warmup=100, d_model=512)\n\n    assert scheduler.get_lr()[0] == 0\n    scheduler.step()\n    assert math.isclose(scheduler.get_lr()[0], 0.001 / (512 ** 0.5) / (100 ** 1.5))\n'"
tests/unit/remote/__init__.py,0,b''
tests/unit/runnable/test_runnable.py,0,"b'from flambe.cluster import AWSCluster\n\nimport pytest\nimport tempfile\nimport shutil\nimport sys\nfrom io import StringIO\nimport importlib\nimport copy\nimport configparser\n\nfrom flambe.runnable import Runnable\n\n@pytest.fixture\ndef runnable():\n    class DummyRunnable(Runnable):\n        def run(self, **kwargs) -> None:\n            pass\n\n    return DummyRunnable()\n\n\n@pytest.fixture\ndef get_secrets():\n    t = tempfile.NamedTemporaryFile(mode=""w+"")\n\n    def _get_secrets(secrets_content):\n        t.write(secrets_content)\n        t.flush()\n        return t.name\n\n    yield _get_secrets\n    t.close()\n\n\ndef test_valid_secrets(runnable, get_secrets):\n    secrets = """"""\n        [SOME_SECTION]\n        RANDOM = random\n    """"""\n\n    runnable.inject_secrets(get_secrets(secrets))\n\n\ndef test_invalid_secrets(runnable, get_secrets):\n    secrets = """"""\n        this is a text\n    """"""\n\n    with pytest.raises(configparser.MissingSectionHeaderError):\n        runnable.inject_secrets(get_secrets(secrets))\n\n\ndef test_invalid_secrets2(runnable, get_secrets):\n    secrets = """"""\n        {json: tru}\n    """"""\n\n    with pytest.raises(configparser.MissingSectionHeaderError):\n        runnable.inject_secrets(get_secrets(secrets))\n\n\ndef test_no_secrets(runnable):\n    assert len(runnable.config) == 1\n    assert list(runnable.config.keys())[0] == \'DEFAULT\'\n\n\ndef test_secrets(runnable, get_secrets):\n    secrets = """"""\n        [SOME_SECTION]\n        RANDOM = random\n    """"""\n\n    runnable.inject_secrets(get_secrets(secrets))\n\n    assert \'SOME_SECTION\' in runnable.config\n    assert \'RANDOM\' in runnable.config[\'SOME_SECTION\']\n    assert runnable.config[\'SOME_SECTION\'][\'RANDOM\'] == \'random\'\n'"
tests/unit/runner/test_args.py,0,"b'import pytest\nimport argparse\n\nfrom flambe.runner.run import main\nfrom flambe.runnable.runnable import Runnable\n\nimport mock\n\n\n@pytest.fixture(scope=\'function\')\ndef args():\n    args = argparse.Namespace()\n    args.config = \'config.yaml\'\n    args.debug = False\n    args.force = False\n    args.verbose = False\n    args.install_extensions = False\n    args.cluster = None\n    args.secrets = None\n    return args\n\n\nclass DummyRunnable(Runnable):\n\n    def run(self, **kwargs) -> None:\n        self.kwargs = kwargs\n\n\n@pytest.fixture(scope=\'function\')\ndef runnable():\n    return DummyRunnable()\n\n\ndef test_debug_remote(args):\n    args.debug = True\n    args.cluster = ""cluster.yaml""\n\n    with pytest.raises(ValueError):\n        main(args)\n\n\n@pytest.mark.parametrize(\'debug\', [True, False])\n@pytest.mark.parametrize(\'force\', [True, False])\n@mock.patch(\'flambe.runner.run.SafeExecutionContext.preprocess\')\ndef test_runnable_args(mock_preprocess, force, debug, runnable, args):\n    mock_preprocess.return_value = (runnable, None)\n    args.debug = debug\n    args.force = force\n\n    main(args)\n\n    assert runnable.kwargs[\'debug\'] == debug\n    assert runnable.kwargs[\'force\'] == force\n'"
tests/unit/runner/test_utils.py,0,"b'import pytest\nimport tempfile\nimport os\n\nfrom flambe.runner import utils\n\n\nMB = 2 ** 20\n\n\ndef create_file(filename, size_MB=1):\n    # From https://stackoverflow.com/a/8816154\n    with open(filename, ""wb"") as out:\n        out.truncate(size_MB * MB)\n\n\n@pytest.mark.parametrize(""mbs"", [1, 2, 3, 4])\ndef test_size_MB_file(mbs):\n    with tempfile.NamedTemporaryFile(""wb"") as t:\n        create_file(t.name, size_MB=mbs)\n        assert utils.get_size_MB(t.name) == mbs\n\n\n@pytest.mark.parametrize(""mbs"", [1, 2, 3, 4])\ndef test_size_MB_folder(mbs):\n    with tempfile.TemporaryDirectory() as t:\n        create_file(os.path.join(t, \'1.bin\'), size_MB=mbs)\n        create_file(os.path.join(t, \'2.bin\'), size_MB=mbs)\n        create_file(os.path.join(t, \'3.bin\'), size_MB=mbs)\n        create_file(os.path.join(t, \'4.bin\'), size_MB=mbs)\n        assert utils.get_size_MB(t) == 4 * mbs\n\n\ndef test_get_files():\n    with tempfile.TemporaryDirectory() as t:\n        f1 = os.path.join(t, \'some_file.txt\')\n        os.mkdir(os.path.join(t, \'folder\'))\n        f2 = os.path.join(t, \'folder\', \'some_file.txt\')\n        open(f1, \'w+\').close()\n        open(f2, \'w+\').close()\n\n        assert list(utils.get_files(t)) == [f1, f2]\n\n\ndef test_get_files_invalid():\n    with pytest.raises(ValueError):\n        utils.get_files(\'/some/non/existent/path/to/test\')\n'"
tests/unit/sampler/__init__.py,0,b''
tests/unit/sampler/test_base.py,30,"b'import pytest\nimport torch\n\nfrom flambe.sampler import BaseSampler\n\n\ndef test_compose_padded_batches_from_nested_seq():\n    """"""\n    In the first two observations:\n        * The max number of first-level elements is 4.\n        * The max size of a first-level element is 8.\n    In the second two observations:\n        * The max number of first-level elements is 2.\n        * The max size of a first-level element is 5.\n\n    > The first batch should be of size (bs, 4, 8)\n    > The second batch should be of size (bs, 2, 5)\n    """"""\n    bs = 2\n    data = (\n        (\n            torch.tensor([7]),  # Not nested\n            [torch.tensor([1, 2]), torch.tensor([3, 4, 5, 6, 7])],  # Nested\n            [torch.tensor([1, 2]), torch.tensor([1, 2]), torch.tensor([3, 4, 5])]  # Nested\n        ),\n        (\n            torch.tensor([7, 8]),  # Not nested\n            [torch.tensor([7, 8]), torch.tensor([7, 8]), torch.tensor([7, 8]), torch.tensor([7, 8])],  # Nested\n            [torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])]  # Nested\n        ),\n        (\n            torch.tensor([7, 8, 9]),  # Not nested\n            [torch.tensor([1, 2, 3]), torch.tensor([4, 5])],  # Nested\n            [torch.tensor([1, 2])]  # Nested\n        ),\n        (\n            torch.tensor([7, 8, 9, 10]),  # Not nested\n            [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6, 7, 8, 9])],  # Nested\n            [torch.tensor([1, 2, 4, 5, 6, 7, 8])]  # Nested\n        ),\n    )\n    sampler = BaseSampler(\n        batch_size=bs,\n        shuffle=False,\n        pad_index=0\n    )\n    sampler = sampler.sample(data)\n\n    batch = next(sampler)\n    a, b, c = batch\n    assert a.size() == (bs, 2)  # In first batch, first col: largest element has length 2\n    assert b.size() == (bs, 4, 5)  # In first batch, second col: largest child has length 5; largest seq. of children has length 4\n    assert c.size() == (bs, 3, 8)  # In first batch, third col: largest child has length 8; largest seq. of children has length 3\n\n    batch = next(sampler)\n    a, b, c = batch\n    assert a.size() == (bs, 4) # In second batch, first col: largest element has length 4\n    assert b.size() == (bs, 2, 6)  # In first batch, second col: largest child has length 6; largest seq. of children has length 2\n    assert c.size() == (bs, 1, 7)  # In first batch, third col: largest child has length 7; largest seq. of children has length 1\n\n\ndef test_column_specific_pad_indexes():\n    """"""\n    The first column should be padded to 10 elements; (10 - 1) + (10 - 4) + (10 - 10) of these should be -2.\n\n    The second column should be padded to 10 elements; (10 - 1) + (10 - 5) + (10 - 10) of these should be -1.\n\n    The third column should be padded to 10 elements; (10 - 1) + (10 - 6) + (10 - 10) of these should be 0.\n    """"""\n    bs = 3\n    data = (\n        (\n            torch.tensor(1 * [1]),\n            torch.tensor(1 * [2]),\n            torch.tensor(1 * [3])\n        ),\n        (\n            torch.tensor(4 * [1]),\n            torch.tensor(5 * [2]),\n            torch.tensor(6 * [3])\n        ),\n        (\n            torch.tensor(10 * [1]),\n            torch.tensor(10 * [2]),\n            torch.tensor(10 * [3])\n        )\n    )\n    sampler = BaseSampler(\n        batch_size=bs,\n        shuffle=False,\n        pad_index=(-2, -1, 0)\n    )\n    sampler = sampler.sample(data)\n    batch = next(sampler)\n    a, b, c = batch\n\n    assert (a == -2).sum() == (10 - 1) + (10 - 4) + (10 - 10)\n    assert (b == -1).sum() == (10 - 1) + (10 - 5) + (10 - 10)\n    assert (c == 0).sum() == (10 - 1) + (10 - 6) + (10 - 10)\n\n\ndef test_incorrect_num_column_specific_pad_indexes_raises_error():\n    """"""\n    The first column should be padded to 10 elements; (10 - 1) + (10 - 4) + (10 - 10) of these should be -2.\n\n    The second column should be padded to 10 elements; (10 - 1) + (10 - 5) + (10 - 10) of these should be -1.\n\n    The third column should be padded to 10 elements; (10 - 1) + (10 - 6) + (10 - 10) of these should be 0.\n    """"""\n    bs = 3\n    data = (\n        (\n            torch.tensor(1 * [1]),\n            torch.tensor(1 * [2]),\n            torch.tensor(1 * [3])\n        ),\n        (\n            torch.tensor(4 * [1]),\n            torch.tensor(5 * [2]),\n            torch.tensor(6 * [3])\n        ),\n        (\n            torch.tensor(10 * [1]),\n            torch.tensor(10 * [2]),\n            torch.tensor(10 * [3])\n        )\n    )\n    num_cols = len(data)\n\n    sampler = BaseSampler(\n        batch_size=bs,\n        shuffle=False,\n        pad_index=(num_cols + 99) * (0,)\n    )\n    sampler = sampler.sample(data)\n    with pytest.raises(Exception):\n        batch = next(sampler)\n'"
tests/data/dummy_extensions/inference/setup.py,0,"b""from setuptools import setup, find_packages\n\n\nsetup(\n    name='flambe-inference',\n    version=0.0,\n    description='Testing a script',\n    packages=find_packages(),\n    install_requires=['argparse']\n)\n"""
tests/data/dummy_extensions/runnable/setup.py,0,"b""from setuptools import setup, find_packages\n\n\nsetup(\n    name='flambe-runnable',\n    version=0.0,\n    description='Dummy Runnable',\n    packages=find_packages(),\n)\n"""
tests/data/dummy_extensions/script/setup.py,0,"b""from setuptools import setup, find_packages\n\n\nsetup(\n    name='flambe-script',\n    version=0.0,\n    description='Testing a script',\n    packages=find_packages(),\n    install_requires=['argparse']\n)\n"""
tests/unit/nlp/classification/__init__.py,0,b''
tests/unit/nlp/classification/test_tc_datasets.py,0,"b'from flambe.nlp.classification import SSTDataset, TRECDataset, NewsGroupDataset\n\n\ndef test_dataset_sst():\n    dataset = SSTDataset()\n    assert len(dataset.train) == 6920 \n    assert len(dataset.train[0]) == 2\n\n    assert len(dataset.val) == 872\n    assert len(dataset.val[0]) == 2\n\n    assert len(dataset.test) == 1821\n    assert len(dataset.test[0]) == 2\n\n\ndef test_dataset_trec():\n    dataset = TRECDataset()\n    assert len(dataset.train) == 5452\n    assert len(dataset.train[0]) == 2\n\n    assert len(dataset.val) == 0\n\n    assert len(dataset.test) == 500\n    assert len(dataset.test[0]) == 2\n\ndef test_dataset_news():\n    try:\n        from sklearn.datasets import fetch_20newsgroups\n\n        dataset = NewsGroupDataset()\n\n        assert len(dataset.train) == 11314\n        assert len(dataset.train[0]) == 2\n\n        assert len(dataset.val) == 0\n\n        assert len(dataset.test) == 7532\n        assert len(dataset.test[0]) == 2\n\n    except ImportError:\n        pass\n'"
tests/unit/nlp/language_modeling/__init__.py,0,b''
tests/unit/nlp/language_modeling/test_lm_datasets.py,0,"b'# TODO: uncomment after fixing circleCI\n\n# from flambe.nlp.language_modeling import PTBDataset, Wiki103, Enwiki8\n\n# def test_dataset_ptb():\n#     dataset = PTBDataset()\n#     assert len(dataset.train) == 1\n#     assert len(dataset.val) == 1\n#     assert len(dataset.test) == 1\n\n\n# def test_dataset_wikitext103():\n#     dataset = Wiki103()\n#     assert len(dataset.train) == 1\n#     assert len(dataset.val) == 1\n#     assert len(dataset.test) == 1\n\n\n# def test_dataset_enwiki8():\n#     dataset = Enwiki8()\n#     assert len(dataset.train) == 1\n#     assert len(dataset.val) == 1\n#     assert len(dataset.test) == 1\n'"
tests/data/dummy_extensions/inference/flambe_inference/__init__.py,0,"b'from flambe_inference.obj import DummyInferenceEngine\n\n\n__all__ = [""DummyInferenceEngine""]\n'"
tests/data/dummy_extensions/inference/flambe_inference/obj.py,0,"b'from flambe import Component\nfrom flambe.nlp.classification import TextClassifier\n\n\nclass DummyInferenceEngine(Component):\n\n    def __init__(self, model: TextClassifier) -> None:\n        self.model = model\n\n    def run(self):\n        print(self.model)\n'"
tests/data/dummy_extensions/runnable/flambe_runnable/__init__.py,0,"b'from flambe_runnable.runnable import DummyRunnable\n\n\n__all__ = [""DummyRunnable""]\n'"
tests/data/dummy_extensions/runnable/flambe_runnable/runnable.py,0,"b'from flambe.runnable import Runnable\n\n\nclass DummyRunnable(Runnable):\n\n    def run(self, **kwargs) -> None:\n        print(""Dummy Runnable"")\n'"
tests/data/dummy_extensions/script/flambe_script/__init__.py,0,b''
tests/data/dummy_extensions/script/flambe_script/train.py,0,"b'from flambe import log\nimport argparse\n\n\ndef my_script(arg1: str, arg2: str, kwarg1: int, kwarg2: str):\n    """"""Test script""""""\n    for i in range(10):\n        msg = f\'arg1: {arg1}, \'\n        msg += f\'arg2: {arg2}, \'\n        msg += f\'kwarg1: {kwarg1}, \'\n        msg += f\'kwarg2: {kwarg2}\'\n        log(msg, float(i), global_step=i)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'arg1\', type=str)\n    parser.add_argument(\'arg2\', type=str)\n    parser.add_argument(\'--kwarg1\', type=int)\n    parser.add_argument(\'--kwarg2\', type=str)\n    args = parser.parse_args()\n    my_script(args.arg1, args.arg2, args.kwarg1, args.kwarg2)\n'"
