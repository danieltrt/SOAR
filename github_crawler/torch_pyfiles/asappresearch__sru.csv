file_path,api_count,code
setup.py,0,"b'from setuptools import setup\nimport os\nimport codecs\n\nPACKAGE = \'sru\'\n\n\ndef readme():\n    """""" Return the README text.\n    """"""\n    with codecs.open(\'README.md\', encoding=\'utf-8\') as fh:\n        return fh.read()\n\n\ndef get_version():\n    """""" Gets the current version of the package.\n    """"""\n    version_py = os.path.join(os.path.dirname(__file__), \'sru/version.py\')\n    with open(version_py) as fh:\n        for line in fh:\n            if line.startswith(\'__version__\'):\n                return line.split(\'=\')[-1].strip() \\\n                    .replace(\'""\', \'\').replace(""\'"", \'\')\n    raise ValueError(\'Failed to parse version from: {}\'.format(version_py))\n\n\ndef get_requirements():\n    with open(\'requirements.txt\') as fh:\n        lines = fh.readlines()\n    lines = [line.strip() for line in lines]\n    return [line for line in lines if line]\n\n\nsetup(\n    # Package information\n    name=PACKAGE,\n    version=get_version(),\n    description=\'Simple Recurrent Units for Highly Parallelizable Recurrence\',\n    long_description=readme(),\n    long_description_content_type=""text/markdown"",  # make pypi render long description as markdown\n    keywords=\'deep learning rnn lstm cudnn sru fast pytorch torch\',\n    classifiers=[\n    ],\n\n    # Author information\n    url=\'https://github.com/taolei87/sru\',\n    author=\'Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai and Yoav Artzi\',\n    author_email=\'tao@asapp.com\',\n    license=\'MIT\',\n\n    # What is packaged here.\n    packages=[\'sru\'],\n\n    # What to include\n    package_data={\n        \'\': [\'*.txt\', \'*.rst\', \'*.md\', \'*.cpp\', \'*.cu\']\n    },\n\n    # Dependencies\n    install_requires=get_requirements(),\n    dependency_links=[\n    ],\n\n    zip_safe=False\n)\n'"
DrQA/prepro.py,0,"b'import re\nimport json\nimport spacy\nimport msgpack\nimport unicodedata\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport collections\nimport multiprocessing\nfrom concurrent.futures import ProcessPoolExecutor\nimport logging\n\nparser = argparse.ArgumentParser(\n    description=\'Preprocessing data files, about 10 minitues to run.\'\n)\nparser.add_argument(\'--wv_file\', default=\'glove/glove.840B.300d.txt\',\n                    help=\'path to word vector file.\')\nparser.add_argument(\'--wv_dim\', type=int, default=300,\n                    help=\'word vector dimension.\')\nparser.add_argument(\'--wv_cased\', type=bool, default=True,\n                    help=\'treat the words as cased or not.\')\nparser.add_argument(\'--sort_all\', action=\'store_true\',\n                    help=\'sort the vocabulary by frequencies of all words. \'\n                         \'Otherwise consider question words first.\')\nparser.add_argument(\'--sample_size\', type=int, default=0,\n                    help=\'size of sample data (for debugging).\')\nparser.add_argument(\'--threads\', type=int, default=multiprocessing.cpu_count(),\n                    help=\'number of threads for preprocessing.\')\nparser.add_argument(\'--batch_size\', type=int, default=64,\n                    help=\'batch size for multiprocess tokenizing and tagging.\')\n\nargs = parser.parse_args()\ntrn_file = \'SQuAD/train-v1.1.json\'\ndev_file = \'SQuAD/dev-v1.1.json\'\nwv_file = args.wv_file\nwv_dim = args.wv_dim\n\nlogging.basicConfig(format=\'%(asctime)s %(message)s\', level=logging.DEBUG,\n                    datefmt=\'%m/%d/%Y %I:%M:%S\')\nlog = logging.getLogger(__name__)\n\nlog.info(\'start data preparing...\')\n\n\ndef normalize_text(text):\n    return unicodedata.normalize(\'NFD\', text)\n\n\ndef load_wv_vocab(file):\n    \'\'\'Load tokens from word vector file.\n\n    Only tokens are loaded. Vectors are not loaded at this time for space efficiency.\n\n    Args:\n        file (str): path of pretrained word vector file.\n\n    Returns:\n        set: a set of tokens (str) contained in the word vector file.\n    \'\'\'\n    vocab = set()\n    with open(file) as f:\n        for line in f:\n            elems = line.split()\n            token = normalize_text(\'\'.join(elems[0:-wv_dim]))  # a token may contain space\n            vocab.add(token)\n    return vocab\nwv_vocab = load_wv_vocab(wv_file)\nlog.info(\'glove loaded.\')\n\n\ndef flatten_json(file, proc_func):\n    \'\'\'A multi-processing wrapper for loading SQuAD data file.\'\'\'\n    with open(file) as f:\n        data = json.load(f)[\'data\']\n    with ProcessPoolExecutor(max_workers=args.threads) as executor:\n        rows = executor.map(proc_func, data)\n    rows = sum(rows, [])\n    return rows\n\n\ndef proc_train(article):\n    \'\'\'Flatten each article in training data.\'\'\'\n    rows = []\n    for paragraph in article[\'paragraphs\']:\n        context = paragraph[\'context\']\n        for qa in paragraph[\'qas\']:\n            id_, question, answers = qa[\'id\'], qa[\'question\'], qa[\'answers\']\n            answer = answers[0][\'text\']  # in training data there\'s only one answer\n            answer_start = answers[0][\'answer_start\']\n            answer_end = answer_start + len(answer)\n            rows.append((id_, context, question, answer, answer_start, answer_end))\n    return rows\n\n\ndef proc_dev(article):\n    \'\'\'Flatten each article in dev data\'\'\'\n    rows = []\n    for paragraph in article[\'paragraphs\']:\n        context = paragraph[\'context\']\n        for qa in paragraph[\'qas\']:\n            id_, question, answers = qa[\'id\'], qa[\'question\'], qa[\'answers\']\n            answers = [a[\'text\'] for a in answers]\n            rows.append((id_, context, question, answers))\n    return rows\ntrain = flatten_json(trn_file, proc_train)\ntrain = pd.DataFrame(train,\n                     columns=[\'id\', \'context\', \'question\', \'answer\',\n                              \'answer_start\', \'answer_end\'])\ndev = flatten_json(dev_file, proc_dev)\ndev = pd.DataFrame(dev,\n                   columns=[\'id\', \'context\', \'question\', \'answers\'])\nlog.info(\'json data flattened.\')\n\nnlp = spacy.load(\'en\', parser=False, tagger=False, entity=False)\n\n\ndef pre_proc(text):\n    \'\'\'normalize spaces in a string.\'\'\'\n    text = re.sub(\'\\s+\', \' \', text)\n    return text\ncontext_iter = (pre_proc(c) for c in train.context)\ncontext_tokens = [[w.text for w in doc] for doc in nlp.pipe(\n    context_iter, batch_size=args.batch_size, n_threads=args.threads)]\nlog.info(\'got intial tokens.\')\n\n\ndef get_answer_index(context, context_token, answer_start, answer_end):\n    \'\'\'\n    Get exact indices of the answer in the tokens of the passage,\n    according to the start and end position of the answer.\n\n    Args:\n        context (str): the context passage\n        context_token (list): list of tokens (str) in the context passage\n        answer_start (int): the start position of the answer in the passage\n        answer_end (int): the end position of the answer in the passage\n\n    Returns:\n        (int, int): start index and end index of answer\n    \'\'\'\n    p_str = 0\n    p_token = 0\n    while p_str < len(context):\n        if re.match(\'\\s\', context[p_str]):\n            p_str += 1\n            continue\n        token = context_token[p_token]\n        token_len = len(token)\n        if context[p_str:p_str + token_len] != token:\n            return (None, None)\n        if p_str == answer_start:\n            t_start = p_token\n        p_str += token_len\n        if p_str == answer_end:\n            try:\n                return (t_start, p_token)\n            except UnboundLocalError as e:\n                return (None, None)\n        p_token += 1\n    return (None, None)\ntrain[\'answer_start_token\'], train[\'answer_end_token\'] = \\\n    zip(*[get_answer_index(a, b, c, d) for a, b, c, d in\n          zip(train.context, context_tokens,\n              train.answer_start, train.answer_end)])\ninitial_len = len(train)\ntrain.dropna(inplace=True)\nlog.info(\'drop {} inconsistent samples.\'.format(initial_len - len(train)))\nlog.info(\'answer pointer generated.\')\n\nquestions = list(train.question) + list(dev.question)\ncontexts = list(train.context) + list(dev.context)\n\nnlp = spacy.load(\'en\')\ncontext_text = [pre_proc(c) for c in contexts]\nquestion_text = [pre_proc(q) for q in questions]\nquestion_docs = [doc for doc in nlp.pipe(\n    iter(question_text), batch_size=args.batch_size, n_threads=args.threads)]\ncontext_docs = [doc for doc in nlp.pipe(\n    iter(context_text), batch_size=args.batch_size, n_threads=args.threads)]\nif args.wv_cased:\n    question_tokens = [[normalize_text(w.text) for w in doc] for doc in question_docs]\n    context_tokens = [[normalize_text(w.text) for w in doc] for doc in context_docs]\nelse:\n    question_tokens = [[normalize_text(w.text).lower() for w in doc] for doc in question_docs]\n    context_tokens = [[normalize_text(w.text).lower() for w in doc] for doc in context_docs]\ncontext_token_span = [[(w.idx, w.idx + len(w.text)) for w in doc] for doc in context_docs]\ncontext_tags = [[w.tag_ for w in doc] for doc in context_docs]\ncontext_ents = [[w.ent_type_ for w in doc] for doc in context_docs]\ncontext_features = []\nfor question, context in zip(question_docs, context_docs):\n    question_word = {w.text for w in question}\n    question_lower = {w.text.lower() for w in question}\n    question_lemma = {w.lemma_ if w.lemma_ != \'-PRON-\' else w.text.lower() for w in question}\n    match_origin = [w.text in question_word for w in context]\n    match_lower = [w.text.lower() in question_lower for w in context]\n    match_lemma = [(w.lemma_ if w.lemma_ != \'-PRON-\' else w.text.lower()) in question_lemma for w in context]\n    context_features.append(list(zip(match_origin, match_lower, match_lemma)))\nlog.info(\'tokens generated\')\n\n\ndef build_vocab(questions, contexts):\n    \'\'\'\n    Build vocabulary sorted by global word frequency, or consider frequencies in questions first,\n    which is controlled by `args.sort_all`.\n    \'\'\'\n    if args.sort_all:\n        counter = collections.Counter(w for doc in questions + contexts for w in doc)\n        vocab = sorted([t for t in counter if t in wv_vocab], key=counter.get, reverse=True)\n    else:\n        counter_q = collections.Counter(w for doc in questions for w in doc)\n        counter_c = collections.Counter(w for doc in contexts for w in doc)\n        counter = counter_c + counter_q\n        vocab = sorted([t for t in counter_q if t in wv_vocab], key=counter_q.get, reverse=True)\n        vocab += sorted([t for t in counter_c.keys() - counter_q.keys() if t in wv_vocab],\n                        key=counter.get, reverse=True)\n    total = sum(counter.values())\n    matched = sum(counter[t] for t in vocab)\n    log.info(\'vocab coverage {1}/{0} | OOV occurrence {2}/{3} ({4:.4f}%)\'.format(\n        len(counter), len(vocab), (total - matched), total, (total - matched) / total * 100))\n    vocab.insert(0, ""<PAD>"")\n    vocab.insert(1, ""<UNK>"")\n    return vocab, counter\n\n\ndef token2id(docs, vocab, unk_id=None):\n    w2id = {w: i for i, w in enumerate(vocab)}\n    ids = [[w2id[w] if w in w2id else unk_id for w in doc] for doc in docs]\n    return ids\nvocab, counter = build_vocab(question_tokens, context_tokens)\n# tokens\nquestion_ids = token2id(question_tokens, vocab, unk_id=1)\ncontext_ids = token2id(context_tokens, vocab, unk_id=1)\n# term frequency in document\ncontext_tf = []\nfor doc in context_tokens:\n    counter_ = collections.Counter(w.lower() for w in doc)\n    total = sum(counter_.values())\n    context_tf.append([counter_[w.lower()] / total for w in doc])\ncontext_features = [[list(w) + [tf] for w, tf in zip(doc, tfs)] for doc, tfs in\n                    zip(context_features, context_tf)]\n# tags\nvocab_tag = list(nlp.tagger.tag_names)\ncontext_tag_ids = token2id(context_tags, vocab_tag)\n# entities, build dict on the fly\ncounter_ent = collections.Counter(w for doc in context_ents for w in doc)\nvocab_ent = sorted(counter_ent, key=counter_ent.get, reverse=True)\nlog.info(\'Found {} POS tags.\'.format(len(vocab_tag)))\nlog.info(\'Found {} entity tags: {}\'.format(len(vocab_ent), vocab_ent))\ncontext_ent_ids = token2id(context_ents, vocab_ent)\nlog.info(\'vocab built.\')\n\n\ndef build_embedding(embed_file, targ_vocab, dim_vec):\n    vocab_size = len(targ_vocab)\n    emb = np.zeros((vocab_size, dim_vec))\n    w2id = {w: i for i, w in enumerate(targ_vocab)}\n    with open(embed_file) as f:\n        for line in f:\n            elems = line.split()\n            token = normalize_text(\'\'.join(elems[0:-wv_dim]))\n            if token in w2id:\n                emb[w2id[token]] = [float(v) for v in elems[-wv_dim:]]\n    return emb\nembedding = build_embedding(wv_file, vocab, wv_dim)\nlog.info(\'got embedding matrix.\')\n\ntrain.to_csv(\'SQuAD/train.csv\', index=False)\ndev.to_csv(\'SQuAD/dev.csv\', index=False)\nmeta = {\n    \'vocab\': vocab,\n    \'embedding\': embedding.tolist()\n}\nwith open(\'SQuAD/meta.msgpack\', \'wb\') as f:\n    msgpack.dump(meta, f)\nresult = {\n    \'trn_question_ids\': question_ids[:len(train)],\n    \'dev_question_ids\': question_ids[len(train):],\n    \'trn_context_ids\': context_ids[:len(train)],\n    \'dev_context_ids\': context_ids[len(train):],\n    \'trn_context_features\': context_features[:len(train)],\n    \'dev_context_features\': context_features[len(train):],\n    \'trn_context_tags\': context_tag_ids[:len(train)],\n    \'dev_context_tags\': context_tag_ids[len(train):],\n    \'trn_context_ents\': context_ent_ids[:len(train)],\n    \'dev_context_ents\': context_ent_ids[len(train):],\n    \'trn_context_text\': context_text[:len(train)],\n    \'dev_context_text\': context_text[len(train):],\n    \'trn_context_spans\': context_token_span[:len(train)],\n    \'dev_context_spans\': context_token_span[len(train):]\n}\nwith open(\'SQuAD/data.msgpack\', \'wb\') as f:\n    msgpack.dump(result, f)\nif args.sample_size:\n    sample_size = args.sample_size\n    sample = {\n        \'trn_question_ids\': result[\'trn_question_ids\'][:sample_size],\n        \'dev_question_ids\': result[\'dev_question_ids\'][:sample_size],\n        \'trn_context_ids\': result[\'trn_context_ids\'][:sample_size],\n        \'dev_context_ids\': result[\'dev_context_ids\'][:sample_size],\n        \'trn_context_features\': result[\'trn_context_features\'][:sample_size],\n        \'dev_context_features\': result[\'dev_context_features\'][:sample_size],\n        \'trn_context_tags\': result[\'trn_context_tags\'][:sample_size],\n        \'dev_context_tags\': result[\'dev_context_tags\'][:sample_size],\n        \'trn_context_ents\': result[\'trn_context_ents\'][:sample_size],\n        \'dev_context_ents\': result[\'dev_context_ents\'][:sample_size],\n        \'trn_context_text\': result[\'trn_context_text\'][:sample_size],\n        \'dev_context_text\': result[\'dev_context_text\'][:sample_size],\n        \'trn_context_spans\': result[\'trn_context_spans\'][:sample_size],\n        \'dev_context_spans\': result[\'dev_context_spans\'][:sample_size]\n    }\n    with open(\'SQuAD/sample.msgpack\', \'wb\') as f:\n        msgpack.dump(sample, f)\nlog.info(\'saved to disk.\')\n'"
DrQA/train.py,20,"b'import re\nimport os\nimport sys\nimport random\nimport string\nimport logging\nimport argparse\nfrom shutil import copyfile\nfrom datetime import datetime\nfrom collections import Counter\nimport torch\nimport msgpack\nimport pandas as pd\nfrom drqa.model import DocReaderModel\nfrom drqa.utils import str2bool\n\nparser = argparse.ArgumentParser(\n    description=\'Train a Document Reader model.\'\n)\n# system\nparser.add_argument(\'--log_file\', default=\'output.log\',\n                    help=\'path for log file.\')\nparser.add_argument(\'--log_per_updates\', type=int, default=3,\n                    help=\'log model loss per x updates (mini-batches).\')\nparser.add_argument(\'--data_file\', default=\'SQuAD/data.msgpack\',\n                    help=\'path to preprocessed data file.\')\nparser.add_argument(\'--model_dir\', default=\'models\',\n                    help=\'path to store saved models.\')\nparser.add_argument(\'--save_last_only\', action=\'store_true\',\n                    help=\'only save the final models.\')\nparser.add_argument(\'--eval_per_epoch\', type=int, default=1,\n                    help=\'perform evaluation per x epochs.\')\nparser.add_argument(\'--seed\', type=int, default=937,\n                    help=\'random seed for data shuffling, dropout, etc.\')\nparser.add_argument(""--cuda"", type=str2bool, nargs=\'?\',\n                    const=True, default=torch.cuda.is_available(),\n                    help=\'whether to use GPU acceleration.\')\n# training\nparser.add_argument(\'-e\', \'--epochs\', type=int, default=50)\nparser.add_argument(\'-bs\', \'--batch_size\', type=int, default=32)\nparser.add_argument(\'-rs\', \'--resume\', default=\'\',\n                    help=\'previous model file name (in `model_dir`). \'\n                         \'e.g. ""checkpoint_epoch_11.pt""\')\nparser.add_argument(\'-ro\', \'--resume_options\', action=\'store_true\',\n                    help=\'use previous model options, ignore the cli and defaults.\')\nparser.add_argument(\'-rlr\', \'--reduce_lr\', type=float, default=0.,\n                    help=\'reduce initial (resumed) learning rate by this factor.\')\nparser.add_argument(\'-op\', \'--optimizer\', default=\'adamax\',\n                    help=\'supported optimizer: adamax, sgd\')\nparser.add_argument(\'-gc\', \'--grad_clipping\', type=float, default=20)\nparser.add_argument(\'-wd\', \'--weight_decay\', type=float, default=0)\nparser.add_argument(\'-lr\', \'--learning_rate\', type=float, default=0.001,\n                    help=\'only applied to SGD.\')\nparser.add_argument(\'-mm\', \'--momentum\', type=float, default=0,\n                    help=\'only applied to SGD.\')\nparser.add_argument(\'-tp\', \'--tune_partial\', type=int, default=1000,\n                    help=\'finetune top-x embeddings.\')\nparser.add_argument(\'--fix_embeddings\', action=\'store_true\',\n                    help=\'if true, `tune_partial` will be ignored.\')\nparser.add_argument(\'--rnn_padding\', action=\'store_true\',\n                    help=\'perform rnn padding (much slower but more accurate).\')\n# model\nparser.add_argument(\'--question_merge\', default=\'self_attn\')\nparser.add_argument(\'--doc_layers\', type=int, default=5)\nparser.add_argument(\'--question_layers\', type=int, default=5)\nparser.add_argument(\'--hidden_size\', type=int, default=128)\nparser.add_argument(\'--num_features\', type=int, default=4)\nparser.add_argument(\'--pos\', type=str2bool, nargs=\'?\', const=True, default=True,\n                    help=\'use pos tags as a feature.\')\nparser.add_argument(\'--pos_size\', type=int, default=56,\n                    help=\'how many kinds of POS tags.\')\nparser.add_argument(\'--pos_dim\', type=int, default=56,\n                    help=\'the embedding dimension for POS tags.\')\nparser.add_argument(\'--ner\', type=str2bool, nargs=\'?\', const=True, default=True,\n                    help=\'use named entity tags as a feature.\')\nparser.add_argument(\'--ner_size\', type=int, default=19,\n                    help=\'how many kinds of named entity tags.\')\nparser.add_argument(\'--ner_dim\', type=int, default=19,\n                    help=\'the embedding dimension for named entity tags.\')\nparser.add_argument(\'--use_qemb\', type=str2bool, nargs=\'?\', const=True, default=True)\nparser.add_argument(\'--concat_rnn_layers\', type=str2bool, nargs=\'?\',\n                    const=True, default=False)\nparser.add_argument(\'--dropout_emb\', type=float, default=0.5)\nparser.add_argument(\'--dropout_rnn\', type=float, default=0.2)\nparser.add_argument(\'--dropout_rnn_output\', type=str2bool, nargs=\'?\',\n                    const=True, default=True)\nparser.add_argument(\'--max_len\', type=int, default=15)\nparser.add_argument(\'--rnn_type\', default=\'lstm\',\n                    help=\'supported types: rnn, gru, lstm\')\n\nargs = parser.parse_args()\n\n# set model dir\nmodel_dir = args.model_dir\nos.makedirs(model_dir, exist_ok=True)\nmodel_dir = os.path.abspath(model_dir)\n\n# set random seed\nseed = args.seed if args.seed >= 0 else int(random.random()*1000)\nprint (\'seed:\', seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\nif args.cuda:\n    torch.cuda.manual_seed(seed)\n\n# setup logger\nlog = logging.getLogger(__name__)\nlog.setLevel(logging.DEBUG)\nfh = logging.FileHandler(args.log_file)\nfh.setLevel(logging.DEBUG)\nch = logging.StreamHandler(sys.stdout)\nch.setLevel(logging.INFO)\nformatter = logging.Formatter(fmt=\'%(asctime)s %(message)s\', datefmt=\'%m/%d/%Y %I:%M:%S\')\nfh.setFormatter(formatter)\nch.setFormatter(formatter)\nlog.addHandler(fh)\nlog.addHandler(ch)\n\n\ndef main():\n    log.info(\'[program starts.]\')\n    train, dev, dev_y, embedding, opt = load_data(vars(args))\n    log.info(\'[Data loaded.]\')\n\n    if args.resume:\n        log.info(\'[loading previous model...]\')\n        checkpoint = torch.load(os.path.join(model_dir, args.resume))\n        if args.resume_options:\n            opt = checkpoint[\'config\']\n        state_dict = checkpoint[\'state_dict\']\n        model = DocReaderModel(opt, embedding, state_dict)\n        epoch_0 = checkpoint[\'epoch\'] + 1\n        for i in range(checkpoint[\'epoch\']):\n            random.shuffle(list(range(len(train))))  # synchronize random seed\n        if args.reduce_lr:\n            lr_decay(model.optimizer, lr_decay=args.reduce_lr)\n    else:\n        model = DocReaderModel(opt, embedding)\n        epoch_0 = 1\n\n    if args.cuda:\n        model.cuda()\n\n    if args.resume:\n        batches = BatchGen(dev, batch_size=1, evaluation=True, gpu=args.cuda)\n        predictions = []\n        for batch in batches:\n            predictions.extend(model.predict(batch))\n        em, f1 = score(predictions, dev_y)\n        log.info(""[dev EM: {} F1: {}]"".format(em, f1))\n        best_val_score = f1\n    else:\n        best_val_score = 0.0\n\n    for epoch in range(epoch_0, epoch_0 + args.epochs):\n        log.warn(\'Epoch {}\'.format(epoch))\n        # train\n        batches = BatchGen(train, batch_size=args.batch_size, gpu=args.cuda)\n        start = datetime.now()\n        for i, batch in enumerate(batches):\n            model.update(batch)\n            if i % args.log_per_updates == 0:\n                log.info(\'updates[{0:6}] train loss[{1:.5f}] remaining[{2}]\'.format(\n                    model.updates, model.train_loss.avg,\n                    str((datetime.now() - start) / (i + 1) * (len(batches) - i - 1)).split(\'.\')[0]))\n        # eval\n        if epoch % args.eval_per_epoch == 0:\n            batches = BatchGen(dev, batch_size=1, evaluation=True, gpu=args.cuda)\n            predictions = []\n            for batch in batches:\n                predictions.extend(model.predict(batch))\n            em, f1 = score(predictions, dev_y)\n            log.warn(""dev EM: {} F1: {}"".format(em, f1))\n        # save\n        if not args.save_last_only or epoch == epoch_0 + args.epochs - 1:\n            model_file = os.path.join(model_dir, \'checkpoint_epoch_{}.pt\'.format(epoch))\n            model.save(model_file, epoch)\n            if f1 > best_val_score:\n                best_val_score = f1\n                copyfile(\n                    model_file,\n                    os.path.join(model_dir, \'best_model.pt\'))\n                log.info(\'[new best model saved.]\')\n\n\ndef lr_decay(optimizer, lr_decay):\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] *= lr_decay\n    log.info(\'[learning rate reduced by {}]\'.format(lr_decay))\n    return optimizer\n\n\ndef load_data(opt):\n    with open(\'SQuAD/meta.msgpack\', \'rb\') as f:\n        meta = msgpack.load(f, encoding=\'utf8\')\n    embedding = torch.Tensor(meta[\'embedding\'])\n    opt[\'pretrained_words\'] = True\n    opt[\'vocab_size\'] = embedding.size(0)\n    opt[\'embedding_dim\'] = embedding.size(1)\n    if not opt[\'fix_embeddings\']:\n        embedding[1] = torch.normal(means=torch.zeros(opt[\'embedding_dim\']), std=1.)\n    with open(args.data_file, \'rb\') as f:\n        data = msgpack.load(f, encoding=\'utf8\')\n    train_orig = pd.read_csv(\'SQuAD/train.csv\')\n    dev_orig = pd.read_csv(\'SQuAD/dev.csv\')\n    train = list(zip(\n        data[\'trn_context_ids\'],\n        data[\'trn_context_features\'],\n        data[\'trn_context_tags\'],\n        data[\'trn_context_ents\'],\n        data[\'trn_question_ids\'],\n        train_orig[\'answer_start_token\'].tolist(),\n        train_orig[\'answer_end_token\'].tolist(),\n        data[\'trn_context_text\'],\n        data[\'trn_context_spans\']\n    ))\n    dev = list(zip(\n        data[\'dev_context_ids\'],\n        data[\'dev_context_features\'],\n        data[\'dev_context_tags\'],\n        data[\'dev_context_ents\'],\n        data[\'dev_question_ids\'],\n        data[\'dev_context_text\'],\n        data[\'dev_context_spans\']\n    ))\n    dev_y = dev_orig[\'answers\'].tolist()[:len(dev)]\n    dev_y = [eval(y) for y in dev_y]\n    return train, dev, dev_y, embedding, opt\n\n\nclass BatchGen:\n    def __init__(self, data, batch_size, gpu, evaluation=False):\n        \'\'\'\n        input:\n            data - list of lists\n            batch_size - int\n        \'\'\'\n        self.batch_size = batch_size\n        self.eval = evaluation\n        self.gpu = gpu\n\n        # shuffle\n        if not evaluation:\n            indices = list(range(len(data)))\n            random.shuffle(indices)\n            data = [data[i] for i in indices]\n        # chunk into batches\n        data = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __iter__(self):\n        for batch in self.data:\n            batch_size = len(batch)\n            batch = list(zip(*batch))\n            if self.eval:\n                assert len(batch) == 7\n            else:\n                assert len(batch) == 9\n\n            context_len = max(len(x) for x in batch[0])\n            context_id = torch.LongTensor(batch_size, context_len).fill_(0)\n            for i, doc in enumerate(batch[0]):\n                context_id[i, :len(doc)] = torch.LongTensor(doc)\n\n            feature_len = len(batch[1][0][0])\n            context_feature = torch.Tensor(batch_size, context_len, feature_len).fill_(0)\n            for i, doc in enumerate(batch[1]):\n                for j, feature in enumerate(doc):\n                    context_feature[i, j, :] = torch.Tensor(feature)\n\n            context_tag = torch.LongTensor(batch_size, context_len).fill_(0)\n            for i, doc in enumerate(batch[2]):\n                context_tag[i, :len(doc)] = torch.LongTensor(doc)\n\n            context_ent = torch.LongTensor(batch_size, context_len).fill_(0)\n            for i, doc in enumerate(batch[3]):\n                context_ent[i, :len(doc)] = torch.LongTensor(doc)\n            question_len = max(len(x) for x in batch[4])\n            question_id = torch.LongTensor(batch_size, question_len).fill_(0)\n            for i, doc in enumerate(batch[4]):\n                question_id[i, :len(doc)] = torch.LongTensor(doc)\n\n            context_mask = torch.eq(context_id, 0)\n            question_mask = torch.eq(question_id, 0)\n            if not self.eval:\n                y_s = torch.LongTensor(batch[5])\n                y_e = torch.LongTensor(batch[6])\n            text = list(batch[-2])\n            span = list(batch[-1])\n            if self.gpu:\n                context_id = context_id.pin_memory()\n                context_feature = context_feature.pin_memory()\n                context_tag = context_tag.pin_memory()\n                context_ent = context_ent.pin_memory()\n                context_mask = context_mask.pin_memory()\n                question_id = question_id.pin_memory()\n                question_mask = question_mask.pin_memory()\n            if self.eval:\n                yield (context_id, context_feature, context_tag, context_ent, context_mask,\n                       question_id, question_mask, text, span)\n            else:\n                yield (context_id, context_feature, context_tag, context_ent, context_mask,\n                       question_id, question_mask, y_s, y_e, text, span)\n\n\ndef _normalize_answer(s):\n    def remove_articles(text):\n        return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n    def white_space_fix(text):\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef _exact_match(pred, answers):\n    if pred is None or answers is None:\n        return False\n    pred = _normalize_answer(pred)\n    for a in answers:\n        if pred == _normalize_answer(a):\n            return True\n    return False\n\n\ndef _f1_score(pred, answers):\n    def _score(g_tokens, a_tokens):\n        common = Counter(g_tokens) & Counter(a_tokens)\n        num_same = sum(common.values())\n        if num_same == 0:\n            return 0\n        precision = 1. * num_same / len(g_tokens)\n        recall = 1. * num_same / len(a_tokens)\n        f1 = (2 * precision * recall) / (precision + recall)\n        return f1\n\n    if pred is None or answers is None:\n        return 0\n    g_tokens = _normalize_answer(pred).split()\n    scores = [_score(g_tokens, _normalize_answer(a).split()) for a in answers]\n    return max(scores)\n\n\ndef score(pred, truth):\n    assert len(pred) == len(truth)\n    f1 = em = total = 0\n    for p, t in zip(pred, truth):\n        total += 1\n        em += _exact_match(p, t)\n        f1 += _f1_score(p, t)\n    em = 100. * em / total\n    f1 = 100. * f1 / total\n    return em, f1\n\nif __name__ == \'__main__\':\n    main()\n'"
classification/dataloader.py,2,"b'import gzip\nimport os\nimport sys\nimport re\nimport random\n\nimport numpy as np\nimport torch\n\ndef clean_str(string, TREC=False):\n    """"""\n    Tokenization/string cleaning for all datasets except for SST.\n    Every dataset is lower cased except for TREC\n    """"""\n    string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n    string = re.sub(r"","", "" , "", string)\n    string = re.sub(r""!"", "" ! "", string)\n    string = re.sub(r""\\("", "" \\( "", string)\n    string = re.sub(r""\\)"", "" \\) "", string)\n    string = re.sub(r""\\?"", "" \\? "", string)\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    return string.strip() if TREC else string.strip().lower()\n\ndef read_corpus(path, clean=True, TREC=False, encoding=\'utf8\'):\n    data = []\n    labels = []\n    with open(path, encoding=encoding) as fin:\n        for line in fin:\n            label, sep, text = line.partition(\' \')\n            label = int(label)\n            text = clean_str(text.strip()) if clean else text.strip()\n            labels.append(label)\n            data.append(text.split())\n    return data, labels\n\ndef read_MR(path, seed=1234):\n    file_path = os.path.join(path, ""rt-polarity.all"")\n    data, labels = read_corpus(file_path, encoding=\'latin-1\')\n    random.seed(seed)\n    perm = list(range(len(data)))\n    random.shuffle(perm)\n    data = [ data[i] for i in perm ]\n    labels = [ labels[i] for i in perm ]\n    return data, labels\n\ndef read_SUBJ(path, seed=1234):\n    file_path = os.path.join(path, ""subj.all"")\n    data, labels = read_corpus(file_path, encoding=\'latin-1\')\n    random.seed(seed)\n    perm = list(range(len(data)))\n    random.shuffle(perm)\n    data = [ data[i] for i in perm ]\n    labels = [ labels[i] for i in perm ]\n    return data, labels\n\ndef read_CR(path, seed=1234):\n    file_path = os.path.join(path, ""custrev.all"")\n    data, labels = read_corpus(file_path)\n    random.seed(seed)\n    perm = list(range(len(data)))\n    random.shuffle(perm)\n    data = [ data[i] for i in perm ]\n    labels = [ labels[i] for i in perm ]\n    return data, labels\n\ndef read_MPQA(path, seed=1234):\n    file_path = os.path.join(path, ""mpqa.all"")\n    data, labels = read_corpus(file_path)\n    random.seed(seed)\n    perm = list(range(len(data)))\n    random.shuffle(perm)\n    data = [ data[i] for i in perm ]\n    labels = [ labels[i] for i in perm ]\n    return data, labels\n\ndef read_TREC(path, seed=1234):\n    train_path = os.path.join(path, ""TREC.train.all"")\n    test_path = os.path.join(path, ""TREC.test.all"")\n    train_x, train_y = read_corpus(train_path, TREC=True, encoding=\'latin-1\')\n    test_x, test_y = read_corpus(test_path, TREC=True, encoding=\'latin-1\')\n    random.seed(seed)\n    perm = list(range(len(train_x)))\n    random.shuffle(perm)\n    train_x = [ train_x[i] for i in perm ]\n    train_y = [ train_y[i] for i in perm ]\n    return train_x, train_y, test_x, test_y\n\ndef read_SST(path, seed=1234):\n    train_path = os.path.join(path, ""stsa.binary.phrases.train"")\n    valid_path = os.path.join(path, ""stsa.binary.dev"")\n    test_path = os.path.join(path, ""stsa.binary.test"")\n    train_x, train_y = read_corpus(train_path, False)\n    valid_x, valid_y = read_corpus(valid_path, False)\n    test_x, test_y = read_corpus(test_path, False)\n    random.seed(seed)\n    perm = list(range(len(train_x)))\n    random.shuffle(perm)\n    train_x = [ train_x[i] for i in perm ]\n    train_y = [ train_y[i] for i in perm ]\n    return train_x, train_y, valid_x, valid_y, test_x, test_y\n\ndef cv_split(data, labels, nfold, test_id):\n    assert (nfold > 1) and (test_id >= 0) and (test_id < nfold)\n    lst_x = [ x for i, x in enumerate(data) if i%nfold != test_id ]\n    lst_y = [ y for i, y in enumerate(labels) if i%nfold != test_id ]\n    test_x = [ x for i, x in enumerate(data) if i%nfold == test_id ]\n    test_y = [ y for i, y in enumerate(labels) if i%nfold == test_id ]\n    perm = list(range(len(lst_x)))\n    random.shuffle(perm)\n    M = int(len(lst_x)*0.9)\n    train_x = [ lst_x[i] for i in perm[:M] ]\n    train_y = [ lst_y[i] for i in perm[:M] ]\n    valid_x = [ lst_x[i] for i in perm[M:] ]\n    valid_y = [ lst_y[i] for i in perm[M:] ]\n    return train_x, train_y, valid_x, valid_y, test_x, test_y\n\ndef cv_split2(data, labels, nfold, valid_id):\n    assert (nfold > 1) and (valid_id >= 0) and (valid_id < nfold)\n    train_x = [ x for i, x in enumerate(data) if i%nfold != valid_id ]\n    train_y = [ y for i, y in enumerate(labels) if i%nfold != valid_id ]\n    valid_x = [ x for i, x in enumerate(data) if i%nfold == valid_id ]\n    valid_y = [ y for i, y in enumerate(labels) if i%nfold == valid_id ]\n    return train_x, train_y, valid_x, valid_y\n\ndef pad(sequences, pad_token=\'<pad>\', pad_left=True):\n    \'\'\' input sequences is a list of text sequence [[str]]\n        pad each text sequence to the length of the longest\n    \'\'\'\n    max_len = max(5,max(len(seq) for seq in sequences))\n    if pad_left:\n        return [ [pad_token]*(max_len-len(seq)) + seq for seq in sequences ]\n    return [ seq + [pad_token]*(max_len-len(seq)) for seq in sequences ]\n\n\ndef create_one_batch(x, y, map2id, oov=\'<oov>\'):\n    oov_id = map2id[oov]\n    x = pad(x)\n    length = len(x[0])\n    batch_size = len(x)\n    x = [ map2id.get(w, oov_id) for seq in x for w in seq ]\n    x = torch.LongTensor(x)\n    assert x.size(0) == length*batch_size\n    return x.view(batch_size, length).t().contiguous().cuda(), torch.LongTensor(y).cuda()\n\n\n# shuffle training examples and create mini-batches\ndef create_batches(x, y, batch_size, map2id, perm=None, sort=False):\n\n    lst = perm or range(len(x))\n\n    # sort sequences based on their length; necessary for SST\n    if sort:\n        lst = sorted(lst, key=lambda i: len(x[i]))\n\n    x = [ x[i] for i in lst ]\n    y = [ y[i] for i in lst ]\n\n    sum_len = 0.0\n    batches_x = [ ]\n    batches_y = [ ]\n    size = batch_size\n    nbatch = (len(x)-1) // size + 1\n    for i in range(nbatch):\n        bx, by = create_one_batch(x[i*size:(i+1)*size], y[i*size:(i+1)*size], map2id)\n        sum_len += len(bx)\n        batches_x.append(bx)\n        batches_y.append(by)\n\n    if sort:\n        perm = list(range(nbatch))\n        random.shuffle(perm)\n        batches_x = [ batches_x[i] for i in perm ]\n        batches_y = [ batches_y[i] for i in perm ]\n\n    sys.stdout.write(""{} batches, avg len: {:.1f}\\n"".format(\n        nbatch, sum_len/nbatch\n    ))\n\n    return batches_x, batches_y\n\n\ndef load_embedding_npz(path):\n    data = np.load(path)\n    return [ w.decode(\'utf8\') for w in data[\'words\'] ], data[\'vals\']\n\ndef load_embedding_txt(path):\n    file_open = gzip.open if path.endswith("".gz"") else open\n    words = [ ]\n    vals = [ ]\n    with file_open(path, encoding=\'utf-8\') as fin:\n        fin.readline()\n        for line in fin:\n            line = line.rstrip()\n            if line:\n                parts = line.split(\' \')\n                words.append(parts[0])\n                vals += [ float(x) for x in parts[1:] ]\n    return words, np.asarray(vals).reshape(len(words),-1)\n\ndef load_embedding(path):\n    if path.endswith("".npz""):\n        return load_embedding_npz(path)\n    else:\n        return load_embedding_txt(path)\n'"
classification/modules.py,4,"b'import sys\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef deep_iter(x):\n    if isinstance(x, list) or isinstance(x, tuple):\n        for u in x:\n            for v in deep_iter(u):\n                yield v\n    else:\n        yield x\n\nclass CNN_Text(nn.Module):\n\n    def __init__(self, n_in, widths=[3,4,5], filters=100):\n        super(CNN_Text,self).__init__()\n        Ci = 1\n        Co = filters\n        h = n_in\n        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (w, h)) for w in widths])\n\n    def forward(self, x):\n        # x is (batch, len, d)\n        x = x.unsqueeze(1) # (batch, Ci, len, d)\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(batch, Co, len), ...]\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]\n        x = torch.cat(x, 1)\n        return x\n\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, n_d, words, embs=None, fix_emb=True, oov=\'<oov>\', pad=\'<pad>\', normalize=True):\n        super(EmbeddingLayer, self).__init__()\n        word2id = {}\n        if embs is not None:\n            embwords, embvecs = embs\n            for word in embwords:\n                assert word not in word2id, ""Duplicate words in pre-trained embeddings""\n                word2id[word] = len(word2id)\n\n            sys.stdout.write(""{} pre-trained word embeddings loaded.\\n"".format(len(word2id)))\n            if n_d != len(embvecs[0]):\n                sys.stdout.write(""[WARNING] n_d ({}) != word vector size ({}). Use {} for embeddings.\\n"".format(\n                    n_d, len(embvecs[0]), len(embvecs[0])\n                ))\n                n_d = len(embvecs[0])\n\n        for w in deep_iter(words):\n            if w not in word2id:\n                word2id[w] = len(word2id)\n\n        if oov not in word2id:\n            word2id[oov] = len(word2id)\n\n        if pad not in word2id:\n            word2id[pad] = len(word2id)\n\n        self.word2id = word2id\n        self.n_V, self.n_d = len(word2id), n_d\n        self.oovid = word2id[oov]\n        self.padid = word2id[pad]\n        self.embedding = nn.Embedding(self.n_V, n_d)\n        self.embedding.weight.data.uniform_(-0.25, 0.25)\n\n        if embs is not None:\n            weight  = self.embedding.weight\n            weight.data[:len(embwords)].copy_(torch.from_numpy(embvecs))\n            sys.stdout.write(""embedding shape: {}\\n"".format(weight.size()))\n\n        if normalize:\n            weight = self.embedding.weight\n            norms = weight.data.norm(2,1)\n            if norms.dim() == 1:\n                norms = norms.unsqueeze(1)\n            weight.data.div_(norms.expand_as(weight.data))\n\n        if fix_emb:\n            self.embedding.weight.requires_grad = False\n\n    def forward(self, input):\n        return self.embedding(input)\n'"
classification/train_classifier.py,5,"b'\nimport os\nimport sys\nimport argparse\nimport time\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom sru import *\nimport dataloader\nimport modules\n\nclass Model(nn.Module):\n    def __init__(self, args, emb_layer, nclasses=2):\n        super(Model, self).__init__()\n        self.args = args\n        self.drop = nn.Dropout(args.dropout)\n        self.emb_layer = emb_layer\n        if args.cnn:\n            self.encoder = modules.CNN_Text(\n                emb_layer.n_d,\n                widths = [3,4,5]\n            )\n            d_out = 300\n        elif args.lstm:\n            self.encoder = nn.LSTM(\n                emb_layer.n_d,\n                args.d,\n                args.depth,\n                dropout = args.dropout,\n            )\n            d_out = args.d\n        else:\n            self.encoder = SRU(\n                emb_layer.n_d,\n                args.d,\n                args.depth,\n                dropout = args.dropout,\n            )\n            d_out = args.d\n        self.out = nn.Linear(d_out, nclasses)\n\n    def forward(self, input):\n        if self.args.cnn:\n            input = input.t()\n        emb = self.emb_layer(input)\n        emb = self.drop(emb)\n\n        if self.args.cnn:\n            output = self.encoder(emb)\n        else:\n            output, hidden = self.encoder(emb)\n            output = output[-1]\n\n        output = self.drop(output)\n        return self.out(output)\n\ndef eval_model(niter, model, valid_x, valid_y):\n    with torch.no_grad():\n        model.eval()\n        N = len(valid_x)\n        criterion = nn.CrossEntropyLoss()\n        correct = 0.0\n        cnt = 0\n        total_loss = 0.0\n        for x, y in zip(valid_x, valid_y):\n            output = model(x)\n            loss = criterion(output, y)\n            total_loss += loss.item()*x.size(1)\n            pred = output.data.max(1)[1]\n            correct += pred.eq(y).sum().item()\n            cnt += y.numel()\n        model.train()\n    return 1.0-correct/cnt\n\ndef train_model(epoch, model, optimizer,\n        train_x, train_y, valid_x, valid_y,\n        test_x, test_y,\n        best_valid, test_err):\n\n    model.train()\n    args = model.args\n    N = len(train_x)\n    niter = epoch*len(train_x)\n    criterion = nn.CrossEntropyLoss()\n\n    cnt = 0\n    for x, y in zip(train_x, train_y):\n        niter += 1\n        cnt += 1\n        model.zero_grad()\n        output = model(x)\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n\n    valid_err = eval_model(niter, model, valid_x, valid_y)\n\n    sys.stdout.write(""Epoch={} iter={} lr={:.6f} train_loss={:.6f} valid_err={:.6f}\\n"".format(\n        epoch, niter,\n        optimizer.param_groups[0][\'lr\'],\n        loss.item(),\n        valid_err\n    ))\n\n    if valid_err < best_valid:\n        best_valid = valid_err\n        test_err = eval_model(niter, model, test_x, test_y)\n    sys.stdout.write(""\\n"")\n    return best_valid, test_err\n\ndef main(args):\n    if args.dataset == \'mr\':\n        data, label = dataloader.read_MR(args.path)\n    elif args.dataset == \'subj\':\n        data, label = dataloader.read_SUBJ(args.path)\n    elif args.dataset == \'cr\':\n        data, label = dataloader.read_CR(args.path)\n    elif args.dataset == \'mpqa\':\n        data, label = dataloader.read_MPQA(args.path)\n    elif args.dataset == \'trec\':\n        train_x, train_y, test_x, test_y = dataloader.read_TREC(args.path)\n        data = train_x + test_x\n        label = None\n    elif args.dataset == \'sst\':\n        train_x, train_y, valid_x, valid_y, test_x, test_y = dataloader.read_SST(args.path)\n        data = train_x + valid_x + test_x\n        label = None\n    else:\n        raise Exception(""unknown dataset: {}"".format(args.dataset))\n\n    emb_layer = modules.EmbeddingLayer(\n        args.d, data,\n        embs = dataloader.load_embedding(args.embedding)\n    )\n\n    if args.dataset == \'trec\':\n        train_x, train_y, valid_x, valid_y = dataloader.cv_split2(\n            train_x, train_y,\n            nfold = 10,\n            valid_id = args.cv\n        )\n    elif args.dataset != \'sst\':\n        train_x, train_y, valid_x, valid_y, test_x, test_y = dataloader.cv_split(\n            data, label,\n            nfold = 10,\n            test_id = args.cv\n        )\n\n    nclasses = max(train_y)+1\n\n    train_x, train_y = dataloader.create_batches(\n        train_x, train_y,\n        args.batch_size,\n        emb_layer.word2id,\n    )\n    valid_x, valid_y = dataloader.create_batches(\n        valid_x, valid_y,\n        args.batch_size,\n        emb_layer.word2id,\n    )\n    test_x, test_y = dataloader.create_batches(\n        test_x, test_y,\n        args.batch_size,\n        emb_layer.word2id,\n    )\n\n    model = Model(args, emb_layer, nclasses).cuda()\n    need_grad = lambda x: x.requires_grad\n    optimizer = optim.Adam(\n        filter(need_grad, model.parameters()),\n        lr = args.lr\n    )\n    print (model)\n\n    best_valid = 1e+8\n    test_err = 1e+8\n    for epoch in range(args.max_epoch):\n        best_valid, test_err = train_model(epoch, model, optimizer,\n            train_x, train_y,\n            valid_x, valid_y,\n            test_x, test_y,\n            best_valid, test_err\n        )\n        if args.lr_decay>0:\n            optimizer.param_groups[0][\'lr\'] *= args.lr_decay\n\n    sys.stdout.write(""best_valid: {:.6f}\\n"".format(\n        best_valid\n    ))\n    sys.stdout.write(""test_err: {:.6f}\\n"".format(\n        test_err\n    ))\n\nif __name__ == ""__main__"":\n    argparser = argparse.ArgumentParser(sys.argv[0], conflict_handler=\'resolve\')\n    argparser.add_argument(""--cnn"", action=\'store_true\', help=""whether to use cnn"")\n    argparser.add_argument(""--lstm"", action=\'store_true\', help=""whether to use lstm"")\n    argparser.add_argument(""--dataset"", type=str, default=""mr"", help=""which dataset"")\n    argparser.add_argument(""--path"", type=str, required=True, help=""path to corpus directory"")\n    argparser.add_argument(""--embedding"", type=str, required=True, help=""word vectors"")\n    argparser.add_argument(""--batch_size"", ""--batch"", type=int, default=32)\n    argparser.add_argument(""--max_epoch"", type=int, default=100)\n    argparser.add_argument(""--d"", type=int, default=128)\n    argparser.add_argument(""--dropout"", type=float, default=0.5)\n    argparser.add_argument(""--depth"", type=int, default=2)\n    argparser.add_argument(""--lr"", type=float, default=0.001)\n    argparser.add_argument(""--lr_decay"", type=float, default=0)\n    argparser.add_argument(""--cv"", type=int, default=0)\n\n    args = argparser.parse_args()\n    print (args)\n    main(args)\n'"
language_model/train_enwik8.py,7,"b'import sys\nimport os\nimport argparse\nimport time\nimport random\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom tensorboardX import SummaryWriter\n\nimport sru\n\ndef read_corpus(path, num_test_symbols=5000000):\n    raw_data = open(path).read()\n    raw_data = np.fromstring(raw_data, dtype=np.uint8)\n    unique, data = np.unique(raw_data, return_inverse=True)\n    train_data = data[: -2 * num_test_symbols]\n    valid_data = data[-2 * num_test_symbols: -num_test_symbols]\n    test_data = data[-num_test_symbols:]\n    return train_data, valid_data, test_data, unique\n\ndef create_batches(data_ids, batch_size):\n    N = len(data_ids)\n    L = ((N-1) // batch_size) * batch_size\n    x = np.copy(data_ids[:L].reshape(batch_size,-1).T)\n    y = np.copy(data_ids[1:L+1].reshape(batch_size,-1).T)\n    x, y = torch.from_numpy(x), torch.from_numpy(y)\n    x, y = x.contiguous(), y.contiguous()\n    x, y = x.cuda(), y.cuda()\n    return x, y\n\nclass Model(nn.Module):\n    def __init__(self, words, args):\n        super(Model, self).__init__()\n        self.args = args\n        if args.n_e:\n            self.n_e = args.n_e\n        else:\n            self.n_e = len(words) if len(words) < args.n_d else args.n_d\n        self.n_d = args.n_d\n        self.depth = args.depth\n        self.drop = nn.Dropout(args.dropout)\n        self.embedding_layer = nn.Embedding(len(words), self.n_e)\n        self.n_V = len(words)\n        if args.lstm:\n            self.rnn = nn.LSTM(self.n_e, self.n_d,\n                self.depth,\n                dropout = args.dropout\n            )\n        else:\n            self.rnn = sru.SRU(self.n_e, self.n_d, self.depth,\n                dropout = args.dropout,\n                n_proj = args.n_proj,\n                #use_tanh = 0,\n                highway_bias = args.bias,\n                layer_norm = args.layer_norm\n            )\n        self.output_layer = nn.Linear(self.n_d, self.n_V)\n        self.init_weights()\n\n    def init_weights(self, val_range=None):\n        #val_range = val_range or (3.0/self.n_d)**0.5\n        params = list(self.embedding_layer.parameters()) + list(self.output_layer.parameters()) \\\n                + (list(self.rnn.parameters()) if self.args.lstm else [])\n        for p in params:\n            if p.dim() > 1:  # matrix\n                val = val_range or (3.0/p.size(0))**0.5\n                p.data.uniform_(-val, val)\n            else:\n                p.data.zero_()\n\n    def forward(self, x, hidden):\n        emb = self.drop(self.embedding_layer(x))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        output = output.view(-1, output.size(2))\n        output = self.output_layer(output)\n        return output, hidden\n\n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        zeros = Variable(weight.new(self.depth, batch_size, self.n_d).zero_())\n        if self.args.lstm:\n            return (zeros, zeros)\n        else:\n            return zeros\n\ndef calc_norm(lis):\n    l2_sum = sum(x.norm()**2 for x in lis)\n    return l2_sum**0.5\n\ndef reset_hidden(hidden, p=0.01, lstm=False):\n    w = hidden.data if not lstm else hidden[0].data  # (depth, batch_size, d)\n    bs = w.size(1)\n    mask = Variable(w.new(1, bs, 1).bernoulli_(1-p))\n    if not lstm:\n        return hidden*mask\n    else:\n        return (hidden[0]*mask, hidden[1]*mask)\n\ndef eval_model(model, valid):\n    with torch.no_grad():\n        model.eval()\n        args = model.args\n        batch_size = valid[0].size(1)\n        total_loss = 0.0\n        unroll_size = args.unroll_size\n        criterion = nn.CrossEntropyLoss(size_average=False)\n        hidden = model.init_hidden(batch_size)\n        N = (len(valid[0])-1)//unroll_size + 1\n        for i in range(N):\n            x = valid[0][i*unroll_size:(i+1)*unroll_size]\n            y = valid[1][i*unroll_size:(i+1)*unroll_size].view(-1)\n            x, y = Variable(x, volatile=True), Variable(y, volatile=True)\n            if args.lstm:\n                hidden[0].detach_()\n                hidden[1].detach_()\n            else:\n                hidden.detach_()\n            #hidden = (Variable(hidden[0].data), Variable(hidden[1].data)) if args.lstm \\\n            #    else Variable(hidden.data)\n            output, hidden = model(x, hidden)\n            loss = criterion(output, y)\n            total_loss += loss.item()  # loss.data[0]\n        avg_loss = total_loss / valid[1].numel()\n        ppl = np.exp(avg_loss)\n        model.train()\n    return ppl, avg_loss\n\ndef copy_model(model):\n    states = model.state_dict()\n    for k in states:\n        v = states[k]\n        states[k] = v.clone()\n    return states\n\ndef main(args):\n    train, dev, test, words  = read_corpus(args.data)\n    log_path = ""{}_{}"".format(args.log, random.randint(1,100))\n    train_writer = SummaryWriter(log_dir=log_path+""/train"")\n    dev_writer = SummaryWriter(log_dir=log_path+""/dev"")\n\n    model = Model(words, args)\n    model.cuda()\n    print (model)\n    sys.stdout.write(""vocab size: {}\\n"".format(\n        model.n_V\n    ))\n    sys.stdout.write(""num of parameters: {}\\n"".format(\n        sum(x.numel() for x in model.parameters() if x.requires_grad)\n    ))\n    sys.stdout.write(""\\n"")\n\n    dev_, test_ = dev, test\n    train = create_batches(train, args.batch_size)\n    dev = create_batches(dev, args.batch_size)\n    test = create_batches(test, args.batch_size)\n    lr = args.lr if not args.noam else args.lr/(args.n_d**0.5)/(args.warmup_steps**1.5)\n    optimizer = optim.Adam(\n        filter(lambda p: p.requires_grad, model.parameters()),\n        lr = lr,\n        weight_decay = args.weight_decay\n    )\n\n    plis = [ p for p in model.parameters() if p.requires_grad ]\n    niter = 1\n    unchanged = 0\n    best_dev = 1e+8\n    unroll_size = args.unroll_size\n    batch_size = args.batch_size\n    N = (len(train[0])-1)//unroll_size + 1\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(args.max_epoch):\n        start_time = time.time()\n        model.train()\n        total_loss = 0.0\n        hidden = model.init_hidden(batch_size)\n\n        for i in range(N):\n            x = train[0][i*unroll_size:(i+1)*unroll_size]\n            y = train[1][i*unroll_size:(i+1)*unroll_size].view(-1)\n            x, y =  Variable(x), Variable(y)\n            if args.lstm:\n                hidden[0].detach_()\n                hidden[1].detach_()\n            else:\n                hidden.detach_()\n            #hidden = reset_hidden(hidden, lstm=args.lstm)\n            #hidden = (Variable(hidden[0].data), Variable(hidden[1].data)) if args.lstm \\\n            #    else Variable(hidden.data)\n\n            model.zero_grad()\n            output, hidden = model(x, hidden)\n            loss = criterion(output, y)\n            loss.backward()\n\n            if args.clip_grad > 0:\n                torch.nn.utils.clip_grad_norm(model.parameters(), args.clip_grad)\n            optimizer.step()\n\n            if (niter - 1) % 100 == 0:\n                #sys.stdout.write(""\\r{}"".format(niter))\n                #sys.stdout.flush()\n                #train_writer.add_scalar(\'loss\', loss.data[0], niter)\n                train_writer.add_scalar(\'loss\', loss.item(), niter)\n                train_writer.add_scalar(\'pnorm\',\n                    calc_norm([ x.data for x in plis ]),\n                    niter\n                )\n                train_writer.add_scalar(\'gnorm\',\n                    calc_norm([ x.grad for x in plis ]),\n                    niter\n                )\n\n            if niter % args.log_period == 0 or i == N - 1:\n                elapsed_time = (time.time()-start_time)/60.0\n                dev_ppl, dev_loss = eval_model(model, dev)\n                sys.stdout.write(""\\rIter={}  lr={:.5f}  train_loss={:.4f}  dev_loss={:.4f}""\n                        ""  dev_bpc={:.2f}\\teta={:.1f}m\\t[{:.1f}m]\\n"".format(\n                    niter,\n                    optimizer.param_groups[0][\'lr\'],\n                    loss.item(),  # loss.data[0],\n                    dev_loss,\n                    np.log2(dev_ppl),\n                    elapsed_time*N/(i+1),\n                    elapsed_time\n                ))\n                if dev_ppl < best_dev:\n                    best_dev = dev_ppl\n                    checkpoint = copy_model(model)\n                sys.stdout.write(""\\n"")\n                sys.stdout.flush()\n                dev_writer.add_scalar(\'loss\', dev_loss, niter)\n                dev_writer.add_scalar(\'bpc\', np.log2(dev_ppl), niter)\n\n            niter += 1\n            if args.noam:\n                if niter >= args.warmup_steps:\n                    lr = args.lr/(args.n_d**0.5)/(niter**0.5)\n                else:\n                    lr = (args.lr/(args.n_d**0.5)/(args.warmup_steps**1.5))*niter\n                optimizer.param_groups[0][\'lr\'] = lr\n\n    train_writer.close()\n    dev_writer.close()\n\n    model.load_state_dict(checkpoint)\n    dev = create_batches(dev_, 1)\n    test = create_batches(test_, 1)\n    dev_ppl, dev_loss = eval_model(model, dev)\n    test_ppl, test_loss = eval_model(model, test)\n    sys.stdout.write(""dev_bpc={:.3f}  test_bpc={:.3f}\\n"".format(\n        np.log2(dev_ppl), np.log2(test_ppl)\n    ))\n\nif __name__ == ""__main__"":\n    argparser = argparse.ArgumentParser(sys.argv[0], conflict_handler=\'resolve\')\n    argparser.add_argument(""--log"", type=str, required=True)\n    argparser.add_argument(""--noam"", action=""store_true"")\n    argparser.add_argument(""--warmup_steps"", type=int, default=32000)\n    argparser.add_argument(""--layer_norm"", action=""store_true"")\n    argparser.add_argument(""--lstm"", action=""store_true"")\n    argparser.add_argument(""--data"", type=str, required=True, help=""training file"")\n    argparser.add_argument(""--batch_size"", ""--batch"", type=int, default=128)\n    argparser.add_argument(""--unroll_size"", type=int, default=100)\n    argparser.add_argument(""--max_epoch"", type=int, default=100)\n    argparser.add_argument(""--n_e"", type=int, default=0)\n    argparser.add_argument(""--n_d"", ""--d"", type=int, default=1024)\n    argparser.add_argument(""--n_proj"", type=int, default=0)\n    argparser.add_argument(""--dropout"", type=float, default=0.2,\n        help=""dropout probability""\n    )\n    argparser.add_argument(""--bias"", type=float, default=-3,\n        help=""intial bias of highway gates"",\n    )\n    argparser.add_argument(""--depth"", type=int, default=6)\n    argparser.add_argument(""--lr"", type=float, default=0.001)\n    argparser.add_argument(""--weight_decay"", type=float, default=1e-7)\n    argparser.add_argument(""--clip_grad"", type=float, default=0.3)\n    argparser.add_argument(""--log_period"", type=int, default=100000)\n\n    args = argparser.parse_args()\n    print (args)\n    main(args)\n'"
language_model/train_lm.py,5,"b'import sys\nimport os\nimport argparse\nimport time\nimport random\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport sru\n\n\ndef read_corpus(path, eos=""</s>""):\n    data = [ ]\n    with open(path) as fin:\n        for line in fin:\n            data += line.split() + [ eos ]\n    return data\n\ndef create_batches(data_text, map_to_ids, batch_size, cuda=True):\n    data_ids = map_to_ids(data_text)\n    N = len(data_ids)\n    L = ((N-1) // batch_size) * batch_size\n    x = np.copy(data_ids[:L].reshape(batch_size,-1).T)\n    y = np.copy(data_ids[1:L+1].reshape(batch_size,-1).T)\n    x, y = torch.from_numpy(x), torch.from_numpy(y)\n    x, y = x.contiguous(), y.contiguous()\n    if cuda:\n        x, y = x.cuda(), y.cuda()\n    return x, y\n\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, n_d, words, fix_emb=False):\n        super(EmbeddingLayer, self).__init__()\n        word2id = {}\n        for w in words:\n            if w not in word2id:\n                word2id[w] = len(word2id)\n\n        self.word2id = word2id\n        self.n_V, self.n_d = len(word2id), n_d\n        self.embedding = nn.Embedding(self.n_V, n_d)\n\n    def forward(self, x):\n        return self.embedding(x)\n\n    def map_to_ids(self, text):\n        return np.asarray([self.word2id[x] for x in text],\n                 dtype=\'int64\'\n        )\n\nclass Model(nn.Module):\n    def __init__(self, words, args):\n        super(Model, self).__init__()\n        self.args = args\n        self.n_d = args.d\n        self.depth = args.depth\n        self.drop = nn.Dropout(args.dropout)\n        self.embedding_layer = EmbeddingLayer(self.n_d, words)\n        self.n_V = self.embedding_layer.n_V\n        if args.lstm:\n            self.rnn = nn.LSTM(self.n_d, self.n_d,\n                self.depth,\n                dropout = args.rnn_dropout\n            )\n        else:\n            self.rnn = sru.SRU(self.n_d, self.n_d, self.depth,\n                dropout = args.rnn_dropout,\n                rnn_dropout = args.rnn_dropout,\n                use_tanh = 0,\n                rescale = False,  # make sure the behavior is the same as before\n                v1 = True,        #\n                highway_bias = args.bias\n            )\n        self.output_layer = nn.Linear(self.n_d, self.n_V)\n        # tie weights\n        self.output_layer.weight = self.embedding_layer.embedding.weight\n\n        self.init_weights()\n        #if not args.lstm:\n        #    self.rnn.set_bias(args.bias)\n\n    def init_weights(self):\n        val_range = (3.0/self.n_d)**0.5\n        params = list(self.embedding_layer.parameters()) + list(self.output_layer.parameters()) \\\n                + (list(self.rnn.parameters()) if self.args.lstm else [])\n        for p in params:\n            if p.dim() > 1:  # matrix\n                p.data.uniform_(-val_range, val_range)\n            else:\n                p.data.zero_()\n\n    def forward(self, x, hidden):\n        emb = self.drop(self.embedding_layer(x))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        output = output.view(-1, output.size(2))\n        output = self.output_layer(output)\n        return output, hidden\n\n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        zeros = Variable(weight.new(self.depth, batch_size, self.n_d).zero_())\n        if self.args.lstm:\n            return (zeros, zeros)\n        else:\n            return zeros\n\n    def print_pnorm(self):\n        norms = [ ""{:.0f}"".format(x.norm().item()) for x in self.parameters() ]\n        sys.stdout.write(""\\tp_norm: {}\\n"".format(\n            norms\n        ))\n\ndef train_model(epoch, model, train):\n    model.train()\n    args = model.args\n\n    unroll_size = args.unroll_size\n    batch_size = args.batch_size\n    N = (len(train[0])-1)//unroll_size + 1\n    lr = args.lr\n\n    total_loss = 0.0\n    criterion = nn.CrossEntropyLoss(size_average=False)\n    hidden = model.init_hidden(batch_size)\n    for i in range(N):\n        x = train[0][i*unroll_size:(i+1)*unroll_size]\n        y = train[1][i*unroll_size:(i+1)*unroll_size].view(-1)\n        x, y =  Variable(x), Variable(y)\n        hidden = (Variable(hidden[0].data), Variable(hidden[1].data)) if args.lstm \\\n            else Variable(hidden.data)\n\n        model.zero_grad()\n        output, hidden = model(x, hidden)\n        assert x.size(1) == batch_size\n        loss = criterion(output, y) / x.size(1)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip_grad)\n        for p in model.parameters():\n            if p.requires_grad:\n                if args.weight_decay > 0:\n                    p.data.mul_(1.0-args.weight_decay)\n                p.data.add_(-lr, p.grad.data)\n\n        if math.isnan(loss.item()) or math.isinf(loss.item()):\n            sys.exit(0)\n            return\n\n        total_loss += loss.item() / x.size(0)\n        if i%10 == 0:\n            sys.stdout.write(""\\r{}"".format(i))\n            sys.stdout.flush()\n\n    return np.exp(total_loss/N)\n\ndef eval_model(model, valid):\n    model.eval()\n    args = model.args\n    total_loss = 0.0\n    unroll_size = model.args.unroll_size\n    criterion = nn.CrossEntropyLoss(size_average=False)\n    hidden = model.init_hidden(1)\n    N = (len(valid[0])-1)//unroll_size + 1\n    for i in range(N):\n        x = valid[0][i*unroll_size:(i+1)*unroll_size]\n        y = valid[1][i*unroll_size:(i+1)*unroll_size].view(-1)\n        x, y = Variable(x, volatile=True), Variable(y)\n        hidden = (Variable(hidden[0].data), Variable(hidden[1].data)) if args.lstm \\\n            else Variable(hidden.data)\n        output, hidden = model(x, hidden)\n        loss = criterion(output, y)\n        total_loss += loss.item()  # loss.data[0]\n    avg_loss = total_loss / valid[1].numel()\n    ppl = np.exp(avg_loss)\n    return ppl\n\ndef main(args):\n    train = read_corpus(args.train)\n    dev = read_corpus(args.dev)\n    test = read_corpus(args.test)\n\n    model = Model(train, args)\n    model.cuda()\n    sys.stdout.write(""vocab size: {}\\n"".format(\n        model.embedding_layer.n_V\n    ))\n    sys.stdout.write(""num of parameters: {}\\n"".format(\n        sum(x.numel() for x in model.parameters() if x.requires_grad)\n    ))\n    model.print_pnorm()\n    sys.stdout.write(""\\n"")\n\n    map_to_ids = model.embedding_layer.map_to_ids\n    train = create_batches(train, map_to_ids, args.batch_size)\n    dev = create_batches(dev, map_to_ids, 1)\n    test = create_batches(test, map_to_ids, 1)\n\n    unchanged = 0\n    best_dev = 1e+8\n    for epoch in range(args.max_epoch):\n        start_time = time.time()\n        if args.lr_decay_epoch>0 and epoch>=args.lr_decay_epoch:\n            args.lr *= args.lr_decay\n        train_ppl = train_model(epoch, model, train)\n        dev_ppl = eval_model(model, dev)\n        sys.stdout.write(""\\rEpoch={}  lr={:.4f}  train_ppl={:.2f}  dev_ppl={:.2f}""\n                ""\\t[{:.2f}m]\\n"".format(\n            epoch,\n            args.lr,\n            train_ppl,\n            dev_ppl,\n            (time.time()-start_time)/60.0\n        ))\n        model.print_pnorm()\n        sys.stdout.flush()\n\n        if dev_ppl < best_dev:\n            unchanged = 0\n            best_dev = dev_ppl\n            start_time = time.time()\n            test_ppl = eval_model(model, test)\n            sys.stdout.write(""\\t[eval]  test_ppl={:.2f}\\t[{:.2f}m]\\n"".format(\n                test_ppl,\n                (time.time()-start_time)/60.0\n            ))\n            sys.stdout.flush()\n        else:\n            unchanged += 1\n        if unchanged >= 30: break\n        sys.stdout.write(""\\n"")\n\nif __name__ == ""__main__"":\n    argparser = argparse.ArgumentParser(sys.argv[0], conflict_handler=\'resolve\')\n    argparser.add_argument(""--lstm"", action=""store_true"")\n    argparser.add_argument(""--train"", type=str, required=True, help=""training file"")\n    argparser.add_argument(""--dev"", type=str, required=True, help=""dev file"")\n    argparser.add_argument(""--test"", type=str, required=True, help=""test file"")\n    argparser.add_argument(""--batch_size"", ""--batch"", type=int, default=32)\n    argparser.add_argument(""--unroll_size"", type=int, default=35)\n    argparser.add_argument(""--max_epoch"", type=int, default=300)\n    argparser.add_argument(""--d"", type=int, default=910)\n    argparser.add_argument(""--dropout"", type=float, default=0.7,\n        help=""dropout of word embeddings and softmax output""\n    )\n    argparser.add_argument(""--rnn_dropout"", type=float, default=0.2,\n        help=""dropout of RNN layers""\n    )\n    argparser.add_argument(""--bias"", type=float, default=-3,\n        help=""intial bias of highway gates"",\n    )\n    argparser.add_argument(""--depth"", type=int, default=6)\n    argparser.add_argument(""--lr"", type=float, default=1.0)\n    argparser.add_argument(""--lr_decay"", type=float, default=0.98)\n    argparser.add_argument(""--lr_decay_epoch"", type=int, default=175)\n    argparser.add_argument(""--weight_decay"", type=float, default=1e-5)\n    argparser.add_argument(""--clip_grad"", type=float, default=5)\n\n    args = argparser.parse_args()\n    print (args)\n    main(args)\n'"
misc/compare_cpu_speed_sru_gru.py,10,"b""import time\n\nimport ninja\nimport torch\nimport torch.nn as nn\nfrom sru import SRU, SRUCell\n\nbatch_size = 1\nseq_len = 32\ninput_size = 256\nhidden_size = 512\nn_directions = 1\nn_layers = 1\n\nimport multiprocessing\n#torch.set_num_threads(multiprocessing.cpu_count())\nprint('N_CPUS:', multiprocessing.cpu_count())\nprint('N_TORCH_THREADS', torch.get_num_threads())\n\ndef benchmark_gru_cpu():\n    print('-'*60)\n    print('GRU CPU benchmark:')\n\n    rnn = nn.GRU(input_size=input_size,\n                 hidden_size=hidden_size,\n                 batch_first=False,\n                 bidirectional=(n_directions == 2),\n                 num_layers=1)\n\n    input = torch.randn(seq_len, batch_size, input_size)\n    h0 = torch.randn(n_layers * n_directions, batch_size, hidden_size)\n    print('input.shape', input.shape)\n    print('h0.shape', h0.shape)\n    output, hn = rnn(input, h0)\n    print('output.shape', output.shape)\n    print('hn.shape', hn.shape)\n\n    n_iter = 1000\n    start = time.time()\n    with torch.no_grad():\n        rnn.eval()\n        for i in range(n_iter):\n            rnn.forward(input)\n    print('Time:', round((time.time() - start), 2), 'sec')\n\n\ndef benchmark_sru_cpu():\n    print('-' * 60)\n    print('SRU CPU benchmark:')\n\n    rnn = SRU(input_size=input_size,\n                 hidden_size=hidden_size,\n                 bidirectional=(n_directions == 2),\n                 num_layers=1)\n\n    input = torch.randn(seq_len, batch_size, input_size)\n    h0 = torch.randn(n_layers, batch_size, hidden_size * n_directions)\n    print('input.shape', input.shape)\n    print('h0.shape', h0.shape)\n    with torch.no_grad():\n        rnn.eval()\n        output, hn = rnn(input, h0)\n    print('output.shape', output.shape)\n    print('hn.shape', hn.shape)\n\n    n_iter = 1000\n    start = time.time()\n    with torch.no_grad():\n        rnn.eval()\n        for i in range(n_iter):\n            rnn.forward(input)\n    print('Time:', round((time.time() - start), 2), 'sec')\n\nif __name__ == '__main__':\n    benchmark_gru_cpu()\n    benchmark_sru_cpu()\n"""
misc/compare_gpu_speed_sru_gru.py,15,"b""import time\n\nimport torch\nimport torch.nn as nn\nfrom sru import SRU, SRUCell\n\nbatch_size = 32\nseq_len = 32\ninput_size = 128\nhidden_size = 128\nn_directions = 1\nn_layers = 1\n\nprint('CUDA:', torch.cuda.is_available())\nprint('Device:', torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))\n\nimport multiprocessing\n#torch.set_num_threads(multiprocessing.cpu_count())\nprint('N_CPUS:', multiprocessing.cpu_count())\nprint('N_TORCH_THREADS', torch.get_num_threads())\n\ndef benchmark_gru_gpu():\n    print('-'*60)\n    print('GRU GPU benchmark:')\n\n    rnn = nn.GRU(input_size=input_size,\n                 hidden_size=hidden_size,\n                 batch_first=False,\n                 bidirectional=(n_directions == 2),\n                 num_layers=1).cuda()\n\n    input = torch.randn(seq_len, batch_size, input_size).cuda()\n    h0 = torch.randn(n_layers * n_directions, batch_size, hidden_size).cuda()\n    print('input.shape', input.shape)\n    print('h0.shape', h0.shape)\n    output, hn = rnn(input, h0)\n    print('output.shape', output.shape)\n    print('hn.shape', hn.shape)\n    \n    torch.cuda.synchronize()\n    n_iter = 10000\n    start = time.time()\n    with torch.no_grad():\n        rnn.eval()\n        for i in range(n_iter):\n            rnn.forward(input)\n    torch.cuda.synchronize()\n    print('Time:', round((time.time() - start), 2), 'sec')\n\n\ndef benchmark_sru_gpu():\n    print('-' * 60)\n    print('SRU GPU benchmark:')\n\n    rnn = SRU(input_size=input_size,\n                 hidden_size=hidden_size,\n                 bidirectional=(n_directions == 2),\n                 num_layers=1).cuda()\n\n    input = torch.randn(seq_len, batch_size, input_size).cuda()\n    h0 = torch.randn(n_layers, batch_size, hidden_size * n_directions).cuda()\n    print('input.shape', input.shape)\n    print('h0.shape', h0.shape)\n    output, hn = rnn(input, h0)\n    print('output.shape', output.shape)\n    print('hn.shape', hn.shape)\n\n    torch.cuda.synchronize()\n    n_iter = 10000\n    start = time.time()\n    with torch.no_grad():\n        rnn.eval()\n        for i in range(n_iter):\n            rnn.forward(input)\n    torch.cuda.synchronize()\n    print('Time:', round((time.time()-start), 2), 'sec')\n\n\nif __name__ == '__main__':\n    benchmark_gru_gpu()\n    benchmark_sru_gpu()\n"""
misc/test_backward_with_transpose.py,6,"b'# Tested with:\n# torch==1.1.0\n# sru==1.6.2\n# cupy==6.4.0\n#\n# CUDA Version: 10.1\n# Author: Ivan Itzcovich (iitzcovich@asapp.com)\n\nfrom sru import SRU\nimport torch\n\ntorch.manual_seed(10)\n\ninput_t = torch.rand(50, 10, 200)\noutput_t = torch.empty(10, dtype=torch.long).random_(5)\nloss = torch.nn.CrossEntropyLoss()\n\nrnn = SRU(\n    input_size=200,\n    hidden_size=10,\n    num_layers=2,\n    dropout=0.0,\n    bidirectional=False,\n    layer_norm=False,\n    highway_bias=0,\n    rescale=True,\n)\n\n\ndef profile(rnn, input_t, output_t, device):\n    rnn.zero_grad()\n    input_t, output_t, rnn = input_t.to(device), output_t.to(device), rnn.to(device)\n    preds, state = rnn(input_t)\n    output = loss(preds[-1, :, :], output_t)\n    print(f""Loss: {output.item()}"")\n    output.backward()\n    grads = [p.grad.data.sum() for p in rnn.parameters() if p.requires_grad]\n    print(f""Sum of Gradients: {torch.stack(grads).sum()}"")\n\ndef profile_with_transpose(rnn, input_t, output_t, device):\n    rnn.zero_grad()\n    input_t, output_t, rnn = input_t.to(device), output_t.to(device), rnn.to(device)\n    preds, state = rnn(input_t)\n    preds = preds.transpose(0, 1)\n    output = loss(preds[:, -1, :], output_t)\n    print(f""Loss: {output.item()}"")\n    output.backward()\n    grads = [p.grad.data.sum() for p in rnn.parameters() if p.requires_grad]\n    print(f""sum of Gradients: {torch.stack(grads).sum()}"")\n\n\n# CPU\nprint(""CPU mode:"")\nprofile(rnn, input_t, output_t, \'cpu\')\nprint()\n\n# GPU\nprint(""GPU mode:"")\nprofile(rnn, input_t, output_t, \'cuda\')\nprint()\n\n# CPU\nprint(""CPU + Transposing mode:"")\nprofile_with_transpose(rnn, input_t, output_t, \'cpu\')\nprint()\n\n# GPU\nprint(""GPU + Transposing mode:"")\nprofile_with_transpose(rnn, input_t, output_t, \'cuda\')\n'"
misc/test_impl.py,11,"b'import cProfile, pstats, io\nimport torch\nfrom sru import SRUCell\n\ndef test_fwd_cpu():\n    cell = SRUCell(3, 5, use_tanh=True)\n    mask = torch.zeros(7, 1)\n    mask[0,0]=1\n    mask[6,0]=1\n    x = torch.randn(7, 1, 3)\n    with torch.no_grad():\n        out_1 = cell(x, mask_pad=mask)\n    out_2 = cell(x, mask_pad=mask)\n    print(""----------"")\n    print(""CPU fwd optimized: {} {}"".format(out_1[0].sum(), out_1[1].sum()))\n    print(""CPU fwd unoptimized: {} {}"".format(out_2[0].sum(), out_2[1].sum()))\n    print(""----------"")\n    print("""")\n\ndef test_bi_fwd_cpu():\n    cell = SRUCell(5, 5, bidirectional=True)\n    x = torch.randn(7, 1, 5)\n    mask = torch.zeros(7, 1)\n    mask[0,0]=1\n    mask[6,0]=1\n    with torch.no_grad():\n        out_1 = cell(x)\n    out_2 = cell(x)\n    print(""----------"")\n    print(""CPU bi-fwd optimized: {} {}"".format(out_1[0].sum(), out_1[1].sum()))\n    print(""CPU bi-fwd unoptimized: {} {}"".format(out_2[0].sum(), out_2[1].sum()))\n    print(""----------"")\n    print("""")\n\ndef profile_speed():\n    bcell = SRUCell(400, 200, bidirectional=True)\n    bcell.eval()\n    mask = torch.zeros(200, 1)\n    x = torch.randn(200, 1, 400)\n    pr = cProfile.Profile()\n    pr.enable()\n    with torch.no_grad():\n        for i in range(10):\n             r = bcell(x, mask_pad=mask)\n    pr.disable()\n    s = io.StringIO()\n    sortby = \'cumulative\'\n    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n    ps.print_stats()\n    print(s.getvalue())\n\n    pr = cProfile.Profile()\n    pr.enable()\n    #with torch.no_grad():\n    for i in range(10):\n        r = bcell(x, mask_pad=mask)\n    pr.disable()\n    s = io.StringIO()\n    sortby = \'cumulative\'\n    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n    ps.print_stats()\n    print(s.getvalue())\n\ndef test_custom_v(gpu=False):\n    x = torch.randn(10, 2, 5)\n    cell_1 = SRUCell(5, 5, bidirectional=True)\n    if gpu:\n        x = x.cuda()\n        cell_1 = cell_1.cuda()\n    weight = cell_1.weight\n    bias = cell_1.bias\n    weight_c = cell_1.weight_c\n    h_1, c_1 = cell_1(x)\n    loss_1 = h_1.sum()\n    loss_1.backward()\n    print(""----------"")\n    print(""SRU w/o custom_m:"")\n    print(""loss: {}"".format(loss_1))\n    print(""c: {}"".format(c_1.sum()))\n    print(""grad w: {}"".format(weight.grad.sum()))\n    print(""grad v: {}"".format(weight_c.grad.sum()))\n    print("""")\n\n    cell_1.zero_grad()\n    weight_c_custom = weight_c.view(2,-1).transpose(0, 1).contiguous().view(-1)\n    # weight_c is (2, bidir, d)\n    # but custom weight_c is providing (length, batch, bidir, d, 2)\n    def custom_m(input, **kwargs):\n        U = input.matmul(weight)\n        V = input.new_zeros(input.size(0), input.size(1), weight_c_custom.size(0))\n        return U, V\n\n    cell_2 = SRUCell(5, 5, bidirectional=True,\n            custom_m=custom_m\n    )\n    cell_2.weight_c.data.copy_(weight_c_custom)\n    cell_2.bias = bias\n    if gpu:\n        cell_2 = cell_2.cuda()\n    h_2, c_2 = cell_2(x)\n    loss_2 = h_2.sum()\n    loss_2.backward()\n    print(""SRU w/ custom_m:"")\n    print(""loss: {}"".format(loss_2))\n    print(""c: {}"".format(c_2.sum()))\n    print(""grad w: {}"".format(weight.grad.sum()))\n    print(""grad v: {}"".format(cell_2.weight_c.grad.sum()))\n    print(""----------"")\n    print("""")\n\ntest_fwd_cpu()\ntest_bi_fwd_cpu()\ntest_custom_v(gpu=False)\n#test_custom_v(gpu=True)\n#profile_speed()\n\n'"
misc/test_mm.py,12,"b'import time\nimport torch\nimport torch.nn as nn\n\nN = 200\n\ndef run_mm(X, weight):\n    torch.cuda.synchronize()\n    start = time.time()\n    for i in range(N):\n        x = X[i%10]\n        x_2d = x.contiguous().view(-1, x.size(-1))\n        U = x_2d.mm(weight)\n        loss = U.mean()\n        loss.backward()\n    torch.cuda.synchronize()\n    print (""mm() takes: {}"".format(time.time()-start))\n\ndef run_matmul(X, weight):\n    torch.cuda.synchronize()\n    start = time.time()\n    for i in range(N):\n        x = X[i%10]\n        U = x.matmul(weight)\n        loss = U.mean()\n        loss.backward()\n    torch.cuda.synchronize()\n    print (""matmul() takes: {}"".format(time.time()-start))\n\ndef run_addmm(X, weight, bias):\n    torch.cuda.synchronize()\n    start = time.time()\n    for i in range(N):\n        x = X[i%10]\n        x_2d = x.contiguous().view(-1, x.size(-1))\n        U = torch.addmm(bias, x_2d, weight)\n        loss = U.mean()\n        loss.backward()\n    torch.cuda.synchronize()\n    print (""addmm() takes: {}"".format(time.time()-start))\n\ndef run_test(L, B, D, D2):\n    print ("""")\n    print (""==========================="")\n    print (""L={} B={} D={} D2={}"".format(L, B, D, D2))\n    print (""---------------------------"")\n    weight = nn.Parameter(torch.Tensor(D, D2*3)).cuda()\n    weight.data.uniform_(-(3.0/D)**0.5, (3.0/D)**0.5)\n    weight_t = nn.Parameter(torch.Tensor(D2*3, D)).cuda()\n    weight_t.data.copy_(weight.data.t())\n    bias = nn.Parameter(torch.zeros(D2*3))\n    bias.requires_grad = False\n    bias = bias.cuda()\n    x = [ torch.randn(L, B, D, requires_grad=True).cuda() for i in range(10) ]\n    #print (type(weight), weight.type(), weight.requires_grad)\n    #print (type(bias), bias.type(), bias.requires_grad)\n    #print (type(x[0]), x[0].type(), x[0].requires_grad)\n    run_mm(x, weight)\n    run_matmul(x, weight)\n    run_addmm(x, weight, bias)\n    print(""Transposed:"")\n    run_mm(x, weight_t.t())\n    run_matmul(x, weight_t.t())\n    run_addmm(x, weight_t.t(), bias)\n    print (""==========================="")\n\ndef run_small():\n    L = 32\n    B = 32\n    D = D2 = 300\n    run_test(L, B, D, D2)\n\ndef run_large():\n    L = 64\n    B = 64\n    D = 512\n    D2 = 2048\n    run_test(L, B, D, D2)\n\nrun_small()\nrun_large()\n'"
misc/test_multigpu.py,4,"b'import time\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom cuda_functional import SRU\n\nclass Model(nn.Module):\n    def __init__(self, rnn):\n        super(Model, self).__init__()\n        self.rnn = rnn\n\n    def forward(self, x):\n        out, state = self.rnn(x)\n        #return out      # synchronize more data\n        return out[-1:]  # synchronize less data (only the last state)\n\ndef run(x, model, N):\n    start = time.time()\n    for _ in range(N):\n        h = model(x)\n    torch.cuda.synchronize()\n    print (""{:.4} sec\\n"".format(time.time()-start))\n\nN = 100\ninput_size, hidden_size = 512, 1024\nnum_layers = 2\nbatch_size = 128*4\nlength = 64\n\nx = Variable(torch.randn(length, batch_size, input_size).float(), volatile=True)\nx = x.cuda()\n\nprint ("""")\n\n# single gpu\nrnn = Model(SRU(input_size, hidden_size, num_layers))\nrnn.cuda()\nrnn(x)\nprint (""\\nSingle gpu:"")\nrun(x, rnn, N)\n\n# multiple gpu\nrnn_2 = Model(SRU(input_size, hidden_size, num_layers))\nrnn_2 = nn.DataParallel(rnn_2, dim=1)\nrnn_2.cuda()\nrnn_2(x)\nprint (""\\nMulti gpu:"")\nrun(x, rnn_2, N)\n'"
misc/test_sru.py,6,"b""from sru import SRU\nimport torch\nfrom torch import nn\nimport numpy as np\n\ndef test_packed():\n    N = 5\n    max_len = 7\n    V = 32\n    K = 8\n    K_out = 11\n\n    print('N', N, 'max_len', max_len, 'K', K, 'K_out', K_out)\n\n    torch.manual_seed(123)\n    np.random.seed(123)\n    lengths = torch.from_numpy(np.random.choice(max_len, N)) + 1\n    tensors = [torch.from_numpy(np.random.choice(V, l, replace=True)) for l in lengths.tolist()]\n    embedder = nn.Embedding(V, K)\n    tensors = nn.utils.rnn.pad_sequence(tensors)\n    print('tensors.size()', tensors.size())\n    embedded = embedder(tensors)\n    print('embedded.size()', embedded.size())\n    packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=False, enforce_sorted=False)\n    print(isinstance(packed, nn.utils.rnn.PackedSequence))\n\n    sru = SRU(K, K_out)\n    out1, state = sru(packed)\n    out1, lengths1 = nn.utils.rnn.pad_packed_sequence(out1)\n    print('out1.size()', out1.size())\n    assert (lengths != lengths1).sum().item() == 0\n    print('out1.sum()', out1.sum().item())\n\n    # change one of the indexes taht should not be masked out\n    tensors[6, 1] = 3\n    embedded = embedder(tensors)\n    packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=False, enforce_sorted=False)\n    out2, state = sru(packed)\n    out2, lengths2 = nn.utils.rnn.pad_packed_sequence(out2)\n    assert (lengths != lengths2).sum().item() == 0\n    print('out2.sum()', out2.sum().item())\n    assert out2.sum().item() == out1.sum().item()\n\n    # change one of the indexes taht should be masked out\n    tensors[1, 1] = 3\n    embedded = embedder(tensors)\n    packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=False, enforce_sorted=False)\n    out3, state = sru(packed)\n    out3, lengths3 = nn.utils.rnn.pad_packed_sequence(out3)\n    assert (lengths != lengths3).sum().item() == 0\n    print('out3.sum()', out3.sum().item())\n    assert out3.sum().item() != out1.sum().item()\n\ndef test_gru_compatible_state_return():\n    N = 5\n    max_len = 7\n    V = 32\n    K = 8\n    K_out = 11\n    num_layers = 3\n    bidirectional = True\n\n    print('N', N, 'max_len', max_len, 'num_layers', num_layers, 'bidirectional', bidirectional, 'K', K, 'K_out', K_out)\n\n    torch.manual_seed(123)\n    np.random.seed(123)\n    lengths = torch.from_numpy(np.random.choice(max_len, N)) + 1\n    tensors = [torch.from_numpy(np.random.choice(V, l, replace=True)) for l in lengths.tolist()]\n    embedder = nn.Embedding(V, K)\n    tensors = nn.utils.rnn.pad_sequence(tensors)\n    embedded = embedder(tensors)\n\n    sru = SRU(K, K_out, nn_rnn_compatible_return=True, bidirectional=bidirectional, num_layers=num_layers)\n    out, state = sru(embedded)\n    print('out.size()', out.size())\n    print('state.size()', state.size())\n\n    gru = nn.GRU(K, K_out, bidirectional=bidirectional, num_layers=num_layers)\n    gru_out, gru_state = gru(embedded)\n    print('gru_state.size()', gru_state.size())\n"""
sru/__init__.py,0,"b'""""""\nsru - Simple Recurrent Unit\n\nsru provides a PyTorch implementation of the simple recurrent neural network cell described\nin ""Simple Recurrent Units for Highly Parallelizable Recurrence.""\n""""""\nfrom .version import __version__\ntry:\n    from .sru_functional import *\nexcept:\n    from sru_functional import *\n'"
sru/cuda_functional.py,5,"b'import os\nimport torch\nfrom torch.autograd import Function\n\nfrom torch.utils.cpp_extension import load\nsources = [\n    os.path.join(os.path.dirname(__file__), ""sru_cuda_impl.cpp""),\n    os.path.join(os.path.dirname(__file__), ""sru_cuda_kernel.cu""),\n]\nsru_cuda_lib = load(\n    name=""sru_cuda_impl"",\n    sources=sources,\n    extra_cflags=[\'-O3\'],\n    verbose=False\n)\n\nempty_btensor = torch.ByteTensor()\nempty_ftensor = torch.FloatTensor()\n\nclass SRU_Compute_GPU(Function):\n\n    @staticmethod\n    def forward(ctx, u, x, weight_c, bias,\n                init,\n                activation_type,\n                d_out,\n                bidirectional,\n                has_skip_term,\n                scale_x,\n                mask_c=None,\n                mask_pad=None):\n\n        ctx.activation_type = activation_type\n        ctx.d_out = d_out\n        ctx.bidirectional = bidirectional\n        ctx.has_skip_term = has_skip_term\n        ctx.scale_x = scale_x\n         # ensure mask_pad is a byte tensor\n        mask_pad = mask_pad.byte().contiguous() if mask_pad is not None else None\n        ctx.mask_pad = mask_pad\n\n        bidir = 2 if bidirectional else 1\n        length = x.size(0) if x.dim() == 3 else 1\n        batch = x.size(-2)\n        d = d_out\n        if mask_pad is not None:\n            assert mask_pad.size(0) == length\n            assert mask_pad.size(1) == batch\n        k = u.size(-1) // d\n        k_ = k // 2 if bidirectional else k\n        skip_type = 0 if not has_skip_term else (1 if k_ == 3 else 2)\n        ncols = batch * d * bidir\n\n        is_custom = len(weight_c.size()) > 1\n\n        size = (length, batch, d * bidir) if x.dim() == 3 else (batch, d * bidir)\n        c = x.new_zeros(*size)\n        h = x.new_zeros(*size)\n\n        if skip_type > 0 and k_ == 3:\n            x_ = x.contiguous() * scale_x if scale_x is not None else x.contiguous()\n        else:\n            x_ = empty_ftensor\n\n        forward_func = sru_cuda_lib.sru_bi_forward if bidirectional else \\\n                sru_cuda_lib.sru_forward\n        forward_func(\n            h,\n            c,\n            u.contiguous(),\n            x_,\n            weight_c.contiguous(),\n            bias,\n            init.contiguous(),\n            mask_c if mask_c is not None else empty_ftensor,\n            mask_pad.contiguous() if mask_pad is not None else empty_btensor,\n            length,\n            batch,\n            d,\n            k_,\n            activation_type,\n            skip_type,\n            is_custom\n        )\n\n        ctx.save_for_backward(u, x, weight_c, bias, init, mask_c)\n        ctx.intermediate = c\n        if x.dim() == 2:\n            last_hidden = c\n        elif bidirectional:\n            last_hidden = torch.cat((c[-1, :, :d], c[0, :, d:]), dim=1)\n        else:\n            last_hidden = c[-1]\n        return h, last_hidden\n\n    @staticmethod\n    def backward(ctx, grad_h, grad_last):\n        bidir = 2 if ctx.bidirectional else 1\n        u, x, weight_c, bias, init, mask_c = ctx.saved_tensors\n        c = ctx.intermediate\n        scale_x = ctx.scale_x\n        mask_pad = ctx.mask_pad\n        length = x.size(0) if x.dim() == 3 else 1\n        batch = x.size(-2)\n        d = ctx.d_out\n        k = u.size(-1) // d\n        k_ = k // 2 if ctx.bidirectional else k\n        skip_type = 0 if not ctx.has_skip_term else (1 if k_ == 3 else 2)\n        ncols = batch * d * bidir\n\n        is_custom = len(weight_c.size()) > 1\n\n        grad_u = u.new_zeros(*u.size())\n        grad_init = x.new_zeros(batch, d * bidir)\n        grad_x = x.new_zeros(*x.size()) if skip_type > 0 and k_ == 3 else None\n        grad_bias = x.new_zeros(2, batch, bidir * d)\n        if not is_custom:\n            grad_wc = x.new_zeros(2, batch, bidir * d)\n        else:\n            grad_wc = weight_c.new_zeros(*weight_c.size())\n\n\n        if skip_type > 0 and k_ == 3:\n            x_ = x.contiguous() * scale_x if scale_x is not None else x.contiguous()\n        else:\n            x_ = empty_ftensor\n\n        backward_func = sru_cuda_lib.sru_bi_backward if ctx.bidirectional else \\\n                sru_cuda_lib.sru_backward\n        backward_func(\n            grad_u,\n            grad_x if skip_type > 0 and k_ == 3 else empty_ftensor,\n            grad_wc,\n            grad_bias,\n            grad_init,\n            u.contiguous(),\n            x_,\n            weight_c.contiguous(),\n            bias,\n            init.contiguous(),\n            mask_c if mask_c is not None else empty_ftensor,\n            mask_pad.contiguous() if mask_pad is not None else empty_btensor,\n            c,\n            grad_h.contiguous(),\n            grad_last.contiguous(),\n            length,\n            batch,\n            d,\n            k_,\n            ctx.activation_type,\n            skip_type,\n            is_custom\n        )\n\n        if skip_type > 0 and k_ == 3 and scale_x is not None:\n            grad_x.mul_(scale_x)\n        if not is_custom:\n            grad_wc = grad_wc.sum(1).view(-1)\n        return grad_u, grad_x, grad_wc, grad_bias.sum(1).view(-1), grad_init, \\\n               None, None, None, None, None, None, None\n\n'"
sru/sru_functional.py,15,"b'import os\nimport sys\nimport copy\nimport warnings\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nSRU_CPU_kernel = None\nSRU_GPU_kernel = None\n\n# load C++ implementation for CPU computation\ndef _lazy_load_cpu_kernel():\n    global SRU_CPU_kernel\n    if SRU_CPU_kernel is not None:\n        return SRU_CPU_kernel\n    try:\n        from torch.utils.cpp_extension import load\n        cpu_source = os.path.join(os.path.dirname(__file__), ""sru_cpu_impl.cpp"")\n        SRU_CPU_kernel = load(\n            name=""sru_cpu_impl"",\n            sources=[cpu_source],\n            extra_cflags=[\'-O3\'],\n            verbose=False\n        )\n    except:\n        # use Python version instead\n        SRU_CPU_kernel = False\n    return SRU_CPU_kernel\n\n# load C++ implementation for GPU computation\ndef _lazy_load_cuda_kernel():\n    try:\n        from .cuda_functional import SRU_Compute_GPU\n    except:\n        from cuda_functional import SRU_Compute_GPU\n    return SRU_Compute_GPU\n\n\nclass SRU_Compute_CPU():\n    """"""CPU version of the core SRU computation.\n\n    Has the same interface as SRU_Compute_GPU() but is a regular Python function\n    instead of a torch.autograd.Function because we don\'t implement backward()\n    explicitly.\n    """"""\n\n    @staticmethod\n    def apply(u, x, weight_c, bias,\n              init,\n              activation_type,\n              d,\n              bidirectional,\n              has_skip_term,\n              scale_x,\n              mask_c=None,\n              mask_pad=None):\n        """"""\n        An SRU is a recurrent neural network cell comprised of 5 equations, described\n        in ""Simple Recurrent Units for Highly Parallelizable Recurrence.""\n\n        The first 3 of these equations each require a matrix-multiply component,\n        i.e. the input vector x_t dotted with a weight matrix W_i, where i is in\n        {0, 1, 2}.\n\n        As each weight matrix W is dotted with the same input x_t, we can fuse these\n        computations into a single matrix-multiply, i.e. `x_t <dot> stack([W_0, W_1, W_2])`.\n        We call the result of this computation `U`.\n\n        sru_compute_cpu() accepts \'u\' and \'x\' (along with a tensor of biases,\n        an initial memory cell `c0`, and an optional dropout mask) and computes\n        equations (3) - (7). It returns a tensor containing all `t` hidden states\n        (where `t` is the number of elements in our input sequence) and the final\n        memory cell `c_T`.\n        """"""\n\n        bidir = 2 if bidirectional else 1\n        length = x.size(0) if x.dim() == 3 else 1\n        batch = x.size(-2)\n        k = u.size(-1) // d // bidir\n\n        is_custom = len(weight_c.size()) > 1\n\n        sru_cpu_impl = _lazy_load_cpu_kernel()\n        if (sru_cpu_impl is not None) and (sru_cpu_impl != False):\n            if not torch.is_grad_enabled():\n                assert mask_c is None\n                cpu_forward = sru_cpu_impl.cpu_bi_forward if bidirectional else \\\n                              sru_cpu_impl.cpu_forward\n                mask_pad_ = torch.FloatTensor() if mask_pad is None else mask_pad.float()\n                return cpu_forward(\n                    u.contiguous(),\n                    x.contiguous(),\n                    weight_c.contiguous(),\n                    bias,\n                    init,\n                    mask_pad_,\n                    length,\n                    batch,\n                    d,\n                    k,\n                    activation_type,\n                    has_skip_term,\n                    scale_x.item() if scale_x is not None else 1.0,\n                    is_custom\n                )\n            else:\n                warnings.warn(""Running SRU on CPU with grad_enabled=True. Are you sure?"")\n        else:\n            warnings.warn(""C++ kernel for SRU CPU inference was not loaded. ""\n                          ""Use Python version instead."")\n\n        mask_pad_ = mask_pad.view(length, batch, 1).float() if mask_pad is not None else mask_pad\n        u = u.contiguous().view(length, batch, bidir, d, k)\n\n        if is_custom:\n            weight_c = weight_c.view(length, batch, bidir, d, 2)\n            forget_wc = weight_c[..., 0]\n            reset_wc = weight_c[..., 1]\n        else:\n            forget_wc, reset_wc = weight_c.view(2, bidir, d)\n\n        forget_bias, reset_bias = bias.view(2, bidir, d)\n\n        if not has_skip_term:\n            x_prime = None\n        elif k == 3:\n            x_prime = x.view(length, batch, bidir, d)\n            x_prime = x_prime * scale_x if scale_x is not None else x_prime\n        else:\n            x_prime = u[..., 3]\n\n        h = x.new_zeros(length, batch, bidir, d)\n\n        if init is None:\n            c_init = x.new_zeros(size=(batch, bidir, d))\n        else:\n            c_init = init.view(batch, bidir, d)\n\n        c_final = []\n        for di in range(bidir):\n            if di == 0:\n                time_seq = range(length)\n            else:\n                time_seq = range(length - 1, -1, -1)\n\n            mask_c_ = 1 if mask_c is None else mask_c.view(batch, bidir, d)[:, di, :]\n            c_prev = c_init[:, di, :]\n            fb, rb = forget_bias[di], reset_bias[di]\n            if is_custom:\n                fw = forget_wc[:, :, di, :].chunk(length)\n                rw = reset_wc[:, :, di, :].chunk(length)\n            else:\n                fw = forget_wc[di].expand(batch, d)\n                rw = reset_wc[di].expand(batch, d)\n            u0 = u[:, :, di, :, 0].chunk(length)\n            u1 = (u[:, :, di, :, 1] + fb).chunk(length)\n            u2 = (u[:, :, di, :, 2] + rb).chunk(length)\n            if x_prime is not None:\n                xp = x_prime[:, :, di, :].chunk(length)\n\n            for t in time_seq:\n                if is_custom:\n                    forget_t = (u1[t] + c_prev*fw[t]).sigmoid()\n                    reset_t = (u2[t] + c_prev*rw[t]).sigmoid()\n                else:\n                    forget_t = (u1[t] + c_prev*fw).sigmoid()\n                    reset_t = (u2[t] + c_prev*rw).sigmoid()\n                c_t = u0[t] + (c_prev - u0[t]) * forget_t\n                if mask_pad_ is not None:\n                    c_t = c_t * (1-mask_pad_[t]) + c_prev * mask_pad_[t]\n                c_prev = c_t\n\n                if activation_type == 0:\n                    g_c_t = c_t\n                elif activation_type == 1:\n                    g_c_t = c_t.tanh()\n                else:\n                    raise ValueError(\'Activation type must be 0 or 1, not {}\'.format(activation_type))\n\n                if x_prime is not None:\n                    h_t = xp[t] + (g_c_t - xp[t]) * mask_c_ * reset_t\n                else:\n                    h_t = g_c_t * mask_c_ * reset_t\n                if mask_pad_ is not None:\n                    h_t = h_t * (1-mask_pad_[t])\n                h[t, :, di, :] = h_t\n\n            c_final.append(c_t.view(batch, d))\n        return h.view(length, batch, -1), torch.stack(c_final, dim=1).view(batch, -1)\n\n\nclass SRUCell(nn.Module):\n    """"""\n    An SRU cell, i.e. a single recurrent neural network cell,\n    as per `LSTMCell`, `GRUCell` and `RNNCell` in PyTorch.\n\n    Args:\n        input_size (int) : the number of dimensions in a single\n            input sequence element. For example, if the input sequence\n            is a sequence of word embeddings, `input_size` is the\n            dimensionality of a single word embedding, e.g. 300.\n        hidden_size (int) : the dimensionality of the hidden state\n            of this cell.\n        dropout (float) : a number between 0.0 and 1.0. The amount of dropout\n            applied to `g(c_t)` internally in this cell.\n        rnn_dropout (float) : the amount of dropout applied to the input of\n            this cell.\n        use_tanh (bool) : use tanh activation\n        is_input_normalized (bool) : whether the input is normalized (e.g. batch norm / layer norm)\n        bidirectional (bool) : whether or not to employ a bidirectional cell.\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 hidden_size,\n                 dropout=0,\n                 rnn_dropout=0,\n                 bidirectional=False,\n                 n_proj=0,\n                 use_tanh=0,\n                 highway_bias=0,\n                 has_skip_term=True,\n                 layer_norm=False,\n                 rescale=True,\n                 v1=False,\n                 custom_m=None):\n\n        super(SRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size  # hidden size per direction\n        self.output_size = hidden_size * 2 if bidirectional else hidden_size\n        self.rnn_dropout = rnn_dropout\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n        self.has_skip_term = has_skip_term\n        self.highway_bias = highway_bias\n        self.v1 = v1\n        self.rescale = rescale\n        self.activation_type = 0\n        self.activation = \'none\'\n        self.custom_m = custom_m\n        if use_tanh:\n            self.activation_type = 1\n            self.activation = \'tanh\'\n\n        # projection dimension\n        self.projection_size = 0\n        if n_proj > 0 and n_proj < self.input_size and n_proj < self.output_size:\n            self.projection_size = n_proj\n\n        # number of sub-matrices used in SRU\n        self.num_matrices = 3\n        if has_skip_term and self.input_size != self.output_size:\n            self.num_matrices = 4\n\n        # make parameters\n        if self.custom_m is None:\n            if self.projection_size == 0:\n                self.weight = nn.Parameter(torch.Tensor(\n                    input_size,\n                    self.output_size * self.num_matrices\n                ))\n            else:\n                self.weight_proj = nn.Parameter(torch.Tensor(input_size, self.projection_size))\n                self.weight = nn.Parameter(torch.Tensor(\n                    self.projection_size,\n                    self.output_size * self.num_matrices\n                ))\n        self.weight_c = nn.Parameter(torch.Tensor(2 * self.output_size))\n        self.bias = nn.Parameter(torch.Tensor(2 * self.output_size))\n\n        # scaling constant used in highway connections when rescale=True\n        self.register_buffer(\'scale_x\', torch.FloatTensor([0]))\n\n        if layer_norm:\n            self.layer_norm = nn.LayerNorm(self.input_size)\n        else:\n            self.layer_norm = None\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        """"""\n        Properly initialize the weights of SRU, following the same recipe as:\n            Xavier init:  http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n            Kaiming init: https://arxiv.org/abs/1502.01852\n\n        """"""\n        # initialize bias and scaling constant\n        self.bias.data.zero_()\n        bias_val, output_size = self.highway_bias, self.output_size\n        self.bias.data[output_size:].zero_().add_(bias_val)\n        self.scale_x.data[0] = 1\n        if self.rescale and self.has_skip_term:\n            # scalar used to properly scale the highway output\n            scale_val = (1 + math.exp(bias_val) * 2)**0.5\n            self.scale_x.data[0] = scale_val\n\n        if self.custom_m is None:\n            # initialize weights such that E[w_ij]=0 and Var[w_ij]=1/d\n            d = self.weight.size(0)\n            val_range = (3.0 / d)**0.5\n            self.weight.data.uniform_(-val_range, val_range)\n            if self.projection_size > 0:\n                val_range = (3.0 / self.weight_proj.size(0))**0.5\n                self.weight_proj.data.uniform_(-val_range, val_range)\n\n            # projection matrix as a tensor of size:\n            #    (input_size, bidirection, hidden_size, num_matrices)\n            w = self.weight.data.view(d, -1, self.hidden_size, self.num_matrices)\n\n            # re-scale weights for dropout and normalized input for better gradient flow\n            if self.dropout > 0:\n                w[:, :, :, 0].mul_((1 - self.dropout)**0.5)\n            if self.rnn_dropout > 0:\n                w.mul_((1 - self.rnn_dropout)**0.5)\n\n            # making weights smaller when layer norm is used. need more tests\n            if self.layer_norm:\n                w.mul_(0.1)\n                #self.weight_c.data.mul_(0.25)\n\n            # properly scale the highway output\n            if self.rescale and self.has_skip_term and self.num_matrices == 4:\n                scale_val = (1 + math.exp(bias_val) * 2)**0.5\n                w[:, :, :, 3].mul_(scale_val)\n        else:\n            if hasattr(self.custom_m, \'reset_parameters\'):\n                self.custom_m.reset_parameters()\n            else:\n                warnings.warn(""Unable to reset parameters for custom module. ""\n                              ""reset_parameters() method not found for custom module."")\n\n        if not self.v1:\n            # intialize weight_c such that E[w]=0 and Var[w]=1\n            self.weight_c.data.uniform_(-3.0**0.5, 3.0**0.5)\n\n            # rescale weight_c and the weight of sigmoid gates with a factor of sqrt(0.5)\n            if self.custom_m is None:\n                w[:, :, :, 1].mul_(0.5**0.5)\n                w[:, :, :, 2].mul_(0.5**0.5)\n            self.weight_c.data.mul_(0.5**0.5)\n        else:\n            self.weight_c.data.zero_()\n            self.weight_c.requires_grad = False\n\n    def forward(self, input, c0=None, mask_pad=None, **kwargs):\n        """"""\n        This method computes `U`. In addition, it computes the remaining components\n        in `SRU_Compute_GPU` or `SRU_Compute_CPU` and return the results.\n        """"""\n\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(""Input must be 2 or 3 dimensional"")\n\n        input_size, hidden_size = self.input_size, self.hidden_size\n        batch_size = input.size(-2)\n        if c0 is None:\n            c0 = input.new_zeros(batch_size, self.output_size)\n\n        # apply layer norm before activation (i.e. before SRU computation)\n        residual = input\n        if self.layer_norm:\n            input = self.layer_norm(input)\n\n        # apply dropout for multiplication\n        if self.training and (self.rnn_dropout > 0):\n            mask = self.get_dropout_mask_((batch_size, input.size(-1)), self.rnn_dropout)\n            input = input * mask.expand_as(input)\n\n        # compute U, V\n        #   U is (length, batch_size, output_size * num_matrices)\n        #   V is (output_size*2,) or (length, batch_size, output_size * 2) if provided\n        if self.custom_m is None:\n            U = self.compute_U(input)\n            V = self.weight_c\n        else:\n            ret = self.custom_m(input, c0=c0, mask_pad=mask_pad, **kwargs)\n            if isinstance(ret, tuple) or isinstance(ret, list):\n                if len(ret) > 2:\n                    raise Exception(""Custom module must return 1 or 2 tensors but got {}."".format(\n                        len(ret)\n                    ))\n                U, V = ret[0], ret[1] + self.weight_c\n            else:\n                U, V = ret, self.weight_c\n\n            if U.size(-1) != self.output_size * self.num_matrices:\n                raise ValueError(""U must have a last dimension of {} but got {}."".format(\n                    self.output_size * self.num_matrices,\n                    U.size(-1)\n                ))\n            if V.size(-1) != self.output_size * 2:\n                raise ValueError(""V must have a last dimension of {} but got {}."".format(\n                    self.output_size * 2,\n                    V.size(-1)\n                ))\n\n        # get the scaling constant; scale_x is a scalar\n        scale_val = self.scale_x if self.rescale else None\n\n        # get dropout mask\n        if self.training and (self.dropout > 0):\n            mask_c = self.get_dropout_mask_((batch_size, self.output_size), self.dropout)\n        else:\n            mask_c = None\n\n        SRU_Compute = _lazy_load_cuda_kernel() if input.is_cuda else SRU_Compute_CPU\n        h, c = SRU_Compute.apply(U, residual, V, self.bias, c0,\n                                 self.activation_type,\n                                 hidden_size,\n                                 self.bidirectional,\n                                 self.has_skip_term,\n                                 scale_val,\n                                 mask_c,\n                                 mask_pad)\n        return h, c\n\n    def compute_U(self, input):\n        """"""\n        SRU performs grouped matrix multiplication to transform\n        the input (length, batch_size, input_size) into a tensor\n        U of size (length * batch_size, output_size * num_matrices)\n        """"""\n        # collapse (length, batch_size) into one dimension if necessary\n        x = input if input.dim() == 2 else input.contiguous().view(-1, self.input_size)\n        if self.projection_size > 0:\n            x_projected = x.mm(self.weight_proj)\n            U = x_projected.mm(self.weight)\n        else:\n            U = x.mm(self.weight)\n        return U\n\n    def get_dropout_mask_(self, size, p):\n        """"""\n        Composes the dropout mask for the `SRUCell`.\n        """"""\n        b = self.bias.data\n        return b.new(*size).bernoulli_(1 - p).div_(1 - p)\n\n    def extra_repr(self):\n        s = ""{input_size}, {hidden_size}""\n        if self.projection_size > 0:\n            s += "", projection_size={projection_size}""\n        if self.dropout > 0:\n            s += "", dropout={dropout}""\n        if self.rnn_dropout > 0:\n            s += "", rnn_dropout={rnn_dropout}""\n        if self.bidirectional:\n            s += "", bidirectional={bidirectional}""\n        if self.highway_bias != 0:\n            s += "", highway_bias={highway_bias}""\n        if self.activation_type != 0:\n            s += "", activation={activation}""\n        if self.v1:\n            s += "", v1={v1}""\n        s += "", rescale={rescale}""\n        if not self.has_skip_term:\n            s += "", has_skip_term={has_skip_term}""\n        if self.layer_norm:\n            s += "", layer_norm=True""\n        if self.custom_m is not None:\n           s += "",\\n  custom_m="" + str(self.custom_m)\n        return s.format(**self.__dict__)\n\n    def __repr__(self):\n        s = self.extra_repr()\n        if len(s.split(\'\\n\')) == 1:\n            return ""{}({})"".format(self.__class__.__name__, s)\n        else:\n            return ""{}({}\\n)"".format(self.__class__.__name__, s)\n\n\nclass SRU(nn.Module):\n    """"""\n    PyTorch SRU model. In effect, simply wraps an arbitrary number of contiguous `SRUCell`s, and\n    returns the matrix and hidden states , as well as final memory cell (`c_t`), from the last of\n    these `SRUCell`s.\n\n    Args:\n        input_size (int) : the number of dimensions in a single input sequence element. For example,\n            if the input sequence is a sequence of word embeddings, `input_size` is the dimensionality\n            of a single word embedding, e.g. 300.\n        hidden_size (int) : the dimensionality of the hidden state of the SRU cell.\n        num_layers (int) : number of `SRUCell`s to use in the model.\n        dropout (float) : a number between 0.0 and 1.0. The amount of dropout applied to `g(c_t)`\n            internally in each `SRUCell`.\n        rnn_dropout (float) : the amount of dropout applied to the input of each `SRUCell`.\n        use_tanh (bool) : use tanh activation\n        layer_norm (bool) : whether or not to use layer normalization on the output of each layer\n        bidirectional (bool) : whether or not to use bidirectional `SRUCell`s.\n        highway_bias (float) : initial bias of the highway gate, typicially <= 0\n        nn_rnn_compatible_return (bool) : set to True to change the layout of returned state to\n            match that of pytorch nn.RNN, ie (num_layers * num_directions, batch, hidden_size)\n            (this will be slower, but can make SRU a drop-in replacement for nn.RNN and nn.GRU)\n        custom_m (nn.Module or List[nn.Module]) : use a custom module to compute the U matrix (and V\n            matrix) given the input. The module must take as input a tensor of shape (seq_len,\n            batch_size, hidden_size).\n            It returns a tensor U of shape (seq_len, batch_size, hidden_size * 3), or one optional\n            tensor V of shape (seq_len, batch_size, hidden_size * 2).\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 hidden_size,\n                 num_layers=2,\n                 dropout=0,\n                 rnn_dropout=0,\n                 bidirectional=False,\n                 projection_size=0,\n                 use_tanh=False,\n                 layer_norm=False,\n                 highway_bias=0,\n                 has_skip_term=True,\n                 rescale=False,\n                 v1=False,\n                 nn_rnn_compatible_return=False,\n                 custom_m=None,\n                 proj_input_to_hidden_first=False):\n\n        super(SRU, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = hidden_size * 2 if bidirectional else hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.rnn_dropout = rnn_dropout\n        self.projection_size = projection_size\n        self.rnn_lst = nn.ModuleList()\n        self.bidirectional = bidirectional\n        self.use_layer_norm = layer_norm\n        self.has_skip_term = has_skip_term\n        self.num_directions = 2 if bidirectional else 1\n        self.nn_rnn_compatible_return = nn_rnn_compatible_return\n        if proj_input_to_hidden_first and input_size != self.output_size:\n            first_layer_input_size = self.output_size\n            self.input_to_hidden = nn.Linear(input_size, self.output_size, bias=False)\n        else:\n            first_layer_input_size = input_size\n            self.input_to_hidden = None\n\n        for i in range(num_layers):\n            # get custom modules when provided\n            custom_m_i = None\n            if custom_m is not None:\n                custom_m_i = custom_m[i] if isinstance(custom_m, list) else copy.deepcopy(custom_m)\n            # create the i-th SRU layer\n            l = SRUCell(\n                first_layer_input_size if i == 0 else self.output_size,\n                self.hidden_size,\n                dropout=dropout if i + 1 != num_layers else 0,\n                rnn_dropout=rnn_dropout,\n                bidirectional=bidirectional,\n                n_proj=projection_size,\n                use_tanh=use_tanh,\n                layer_norm=layer_norm,\n                highway_bias=highway_bias,\n                has_skip_term=has_skip_term,\n                rescale=rescale,\n                v1=v1,\n                custom_m=custom_m_i\n            )\n            self.rnn_lst.append(l)\n\n    def forward(self, input, c0=None, mask_pad=None):\n        """"""\n        Feeds `input` forward through `num_layers` `SRUCell`s, where `num_layers`\n        is a parameter on the constructor of this class.\n\n        parameters:\n        - input (FloatTensor): (sequence_length, batch_size, input_size)\n        - c0 (FloatTensor): (num_layers, batch_size, hidden_size * num_directions)\n        - mask_pad (ByteTensor): (sequence_length, batch_size): set to 1 to ignore the value at that position\n\n        input can be packed, which will lead to worse execution speed, but is compatible with many usages\n        of nn.RNN.\n\n        Return:\n        - prevx: output: FloatTensor, (sequence_length, batch_size, num_directions * hidden_size)\n        - lstc_stack: state:\n            (FloatTensor): (num_layers, batch_size, num_directions * hidden_size) if not nn_rnn_compatible_return, else\n            (FloatTensor): (num_layers * num_directions, batch, hidden_size)\n        """"""\n\n        # unpack packed, if input is packed. packing and then unpacking will be slower than not packing\n        # at all, but makes SRU usage compatible with nn.RNN usage\n        input_packed = isinstance(input, nn.utils.rnn.PackedSequence)\n        if input_packed:\n            input, lengths = nn.utils.rnn.pad_packed_sequence(input)\n            max_length = lengths.max().item()\n            mask_pad = torch.ByteTensor([[0] * l + [1] * (max_length - l) for l in lengths.tolist()])\n            mask_pad = mask_pad.to(input.device).transpose(0, 1).contiguous()\n\n        # The dimensions of `input` should be: `(sequence_length, batch_size, input_size)`.\n        if input.dim() != 3:\n            raise ValueError(""There must be 3 dimensions for (length, batch_size, input_size)"")\n\n        if c0 is None:\n            zeros = input.data.new(\n                input.size(1), self.output_size\n            ).zero_()\n            c0 = [ zeros for i in range(self.num_layers) ]\n        else:\n            # The dimensions of `c0` should be: `(num_layers, batch_size, hidden_size * dir_)`.\n            if c0.dim() != 3:\n                raise ValueError(""There must be 3 dimensions for (num_layers, batch_size, output_size)"")\n            c0 = [ x.squeeze(0) for x in c0.chunk(self.num_layers, 0) ]\n\n        prevx = input if self.input_to_hidden is None else self.input_to_hidden(input)\n        lstc = []\n        for i, rnn in enumerate(self.rnn_lst):\n            h, c = rnn(prevx, c0[i], mask_pad=mask_pad)\n            prevx = h\n            lstc.append(c)\n\n        if input_packed:\n            prevx = nn.utils.rnn.pack_padded_sequence(prevx, lengths, enforce_sorted=False)\n\n        lstc_stack = torch.stack(lstc)\n        if self.nn_rnn_compatible_return:\n            batch_size = input.size(1)\n            lstc_stack = lstc_stack.view(self.num_layers, batch_size, self.num_directions, self.hidden_size)\n            lstc_stack = lstc_stack.transpose(1, 2).contiguous()\n            lstc_stack = lstc_stack.view(self.num_layers * self.num_directions, batch_size, self.hidden_size)\n        return prevx, lstc_stack\n\n    def reset_parameters(self):\n        for rnn in self.rnn_lst:\n            rnn.reset_parameters()\n\n    def make_backward_compatible(self):\n        self.nn_rnn_compatible_return = getattr(self, \'nn_rnn_compatible_return\', False)\n\n        # version <= 2.1.7\n        if hasattr(self, \'n_in\'):\n            if len(self.ln_lst):\n                raise Exception(""Layer norm is not backward compatible for sru<=2.1.7"")\n            if self.use_weight_norm:\n                raise Exception(""Weight norm removed in sru>=2.1.9"")\n            self.input_size = self.n_in\n            self.hidden_size = self.n_out\n            self.output_size = self.out_size\n            self.num_layers = self.depth\n            self.projection_size = self.n_proj\n            self.use_layer_norm = False\n            for cell in self.rnn_lst:\n                cell.input_size = cell.n_in\n                cell.hidden_size = cell.n_out\n                cell.output_size = cell.n_out * 2 if cell.bidirectional else cell.n_out\n                cell.num_matrices = cell.k\n                cell.projection_size = cell.n_proj\n                cell.layer_norm = None\n                if cell.activation_type > 1:\n                    raise Exception(""ReLU or SeLU activation removed in sru>=2.1.9"")\n\n        # version <= 2.1.9\n        if not hasattr(self, \'input_to_hidden\'):\n            self.input_to_hidden = None\n            for cell in self.rnn_lst:\n                cell.custom_m = None\n'"
sru/version.py,0,"b""__version__ = '2.3.5'\n"""
DrQA/drqa/layers.py,8,"b'# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree. An additional grant\n# of patent rights can be found in the PATENTS file in the same directory.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n# Origin: https://github.com/facebookresearch/ParlAI/tree/master/parlai/agents/drqa\n\n# ------------------------------------------------------------------------------\n# Modules\n# ------------------------------------------------------------------------------\n\nimport sru\n\nclass StackedBRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers,\n                 dropout_rate=0, dropout_output=False, rnn_type=nn.LSTM,\n                 concat_layers=False, padding=False):\n        super(StackedBRNN, self).__init__()\n        self.padding = padding\n        self.dropout_output = dropout_output\n        self.dropout_rate = dropout_rate\n        self.num_layers = num_layers\n        self.concat_layers = concat_layers\n        self.rnns = nn.ModuleList()\n        for i in range(num_layers):\n            input_size = input_size if i == 0 else 2 * hidden_size\n            #self.rnns.append(rnn_type(input_size, hidden_size,\n            #                          num_layers=1,\n            #                          bidirectional=True))\n            self.rnns.append(sru.SRUCell(input_size, hidden_size,\n                                      dropout=dropout_rate,\n                                      rnn_dropout=dropout_rate,\n                                      use_tanh=0,\n                                      bidirectional=True))\n\n    def forward(self, x, x_mask):\n        """"""Can choose to either handle or ignore variable length sequences.\n        Always handle padding in eval.\n        """"""\n        # No padding necessary.\n        if x_mask.data.sum() == 0:\n            return self._forward_unpadded(x, x_mask)\n        # Pad if we care or if its during eval.\n        #if self.padding or not self.training:\n        #    return self._forward_padded(x, x_mask)\n        # We don\'t care.\n        return self._forward_unpadded(x, x_mask)\n\n    def _forward_unpadded(self, x, x_mask):\n        """"""Faster encoding that ignores any padding.""""""\n        # Transpose batch and sequence dims\n        x = x.transpose(0, 1)\n\n        # Encode all layers\n        outputs = [x]\n        for i in range(self.num_layers):\n            rnn_input = outputs[-1]\n\n            # Apply dropout to hidden input\n#            if self.dropout_rate > 0:\n#                rnn_input = F.dropout(rnn_input,\n#                                      p=self.dropout_rate,\n#                                      training=self.training)\n            # Forward\n            rnn_output = self.rnns[i](rnn_input)[0]\n            outputs.append(rnn_output)\n\n        # Concat hidden layers\n        if self.concat_layers:\n            output = torch.cat(outputs[1:], 2)\n        else:\n            output = outputs[-1]\n\n        # Transpose back\n        output = output.transpose(0, 1)\n\n        # Dropout on output layer\n        if self.dropout_output and self.dropout_rate > 0:\n            output = F.dropout(output,\n                               p=self.dropout_rate,\n                               training=self.training)\n        return output.contiguous()\n\n    def _forward_padded(self, x, x_mask):\n        """"""Slower (significantly), but more precise,\n        encoding that handles padding.""""""\n        # Compute sorted sequence lengths\n        lengths = x_mask.data.eq(0).long().sum(1)\n        _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n        _, idx_unsort = torch.sort(idx_sort, dim=0)\n\n        lengths = list(lengths[idx_sort])\n        idx_sort = Variable(idx_sort)\n        idx_unsort = Variable(idx_unsort)\n\n        # Sort x\n        x = x.index_select(0, idx_sort)\n\n        # Transpose batch and sequence dims\n        x = x.transpose(0, 1)\n\n        # Pack it up\n        rnn_input = nn.utils.rnn.pack_padded_sequence(x, lengths)\n\n        # Encode all layers\n        outputs = [rnn_input]\n        for i in range(self.num_layers):\n            rnn_input = outputs[-1]\n\n            # Apply dropout to input\n            if self.dropout_rate > 0:\n                dropout_input = F.dropout(rnn_input.data,\n                                          p=self.dropout_rate,\n                                          training=self.training)\n                rnn_input = nn.utils.rnn.PackedSequence(dropout_input,\n                                                        rnn_input.batch_sizes)\n            outputs.append(self.rnns[i](rnn_input)[0])\n\n        # Unpack everything\n        for i, o in enumerate(outputs[1:], 1):\n            outputs[i] = nn.utils.rnn.pad_packed_sequence(o)[0]\n\n        # Concat hidden layers or take final\n        if self.concat_layers:\n            output = torch.cat(outputs[1:], 2)\n        else:\n            output = outputs[-1]\n\n        # Transpose and unsort\n        output = output.transpose(0, 1)\n        output = output.index_select(0, idx_unsort)\n\n        # Dropout on output layer\n        if self.dropout_output and self.dropout_rate > 0:\n            output = F.dropout(output,\n                               p=self.dropout_rate,\n                               training=self.training)\n        return output\n\n\nclass SeqAttnMatch(nn.Module):\n    """"""Given sequences X and Y, match sequence Y to each element in X.\n    * o_i = sum(alpha_j * y_j) for i in X\n    * alpha_j = softmax(y_j * x_i)\n    """"""\n    def __init__(self, input_size, identity=False):\n        super(SeqAttnMatch, self).__init__()\n        if not identity:\n            self.linear = nn.Linear(input_size, input_size)\n        else:\n            self.linear = None\n\n    def forward(self, x, y, y_mask):\n        """"""Input shapes:\n            x = batch * len1 * h\n            y = batch * len2 * h\n            y_mask = batch * len2\n        Output shapes:\n            matched_seq = batch * len1 * h\n        """"""\n        # Project vectors\n        if self.linear:\n            x_proj = self.linear(x.view(-1, x.size(2))).view(x.size())\n            x_proj = F.relu(x_proj)\n            y_proj = self.linear(y.view(-1, y.size(2))).view(y.size())\n            y_proj = F.relu(y_proj)\n        else:\n            x_proj = x\n            y_proj = y\n\n        # Compute scores\n        scores = x_proj.bmm(y_proj.transpose(2, 1))\n\n        # Mask padding\n        y_mask = y_mask.unsqueeze(1).expand(scores.size())\n        scores.data.masked_fill_(y_mask.data, -float(\'inf\'))\n\n        # Normalize with softmax\n        alpha_flat = F.softmax(scores.view(-1, y.size(1)))\n        alpha = alpha_flat.view(-1, x.size(1), y.size(1))\n\n        # Take weighted average\n        matched_seq = alpha.bmm(y)\n        return matched_seq\n\n\nclass BilinearSeqAttn(nn.Module):\n    """"""A bilinear attention layer over a sequence X w.r.t y:\n    * o_i = softmax(x_i\'Wy) for x_i in X.\n\n    Optionally don\'t normalize output weights.\n    """"""\n    def __init__(self, x_size, y_size, identity=False):\n        super(BilinearSeqAttn, self).__init__()\n        if not identity:\n            self.linear = nn.Linear(y_size, x_size)\n        else:\n            self.linear = None\n\n    def forward(self, x, y, x_mask):\n        """"""\n        x = batch * len * h1\n        y = batch * h2\n        x_mask = batch * len\n        """"""\n        Wy = self.linear(y) if self.linear is not None else y\n        xWy = x.bmm(Wy.unsqueeze(2)).squeeze(2)\n        xWy.data.masked_fill_(x_mask.data, -float(\'inf\'))\n        if self.training:\n            # In training we output log-softmax for NLL\n            alpha = F.log_softmax(xWy)\n        else:\n            # ...Otherwise 0-1 probabilities\n            alpha = F.softmax(xWy)\n        return alpha\n\n\nclass LinearSeqAttn(nn.Module):\n    """"""Self attention over a sequence:\n    * o_i = softmax(Wx_i) for x_i in X.\n    """"""\n    def __init__(self, input_size):\n        super(LinearSeqAttn, self).__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, x, x_mask):\n        """"""\n        x = batch * len * hdim\n        x_mask = batch * len\n        """"""\n        x_flat = x.view(-1, x.size(-1))\n        scores = self.linear(x_flat).view(x.size(0), x.size(1))\n        scores.data.masked_fill_(x_mask.data, -float(\'inf\'))\n        alpha = F.softmax(scores)\n        return alpha\n\n\n# ------------------------------------------------------------------------------\n# Functional\n# ------------------------------------------------------------------------------\n\n\ndef uniform_weights(x, x_mask):\n    """"""Return uniform weights over non-masked input.""""""\n    alpha = Variable(torch.ones(x.size(0), x.size(1)))\n    if x.data.is_cuda:\n        alpha = alpha.cuda()\n    alpha = alpha * x_mask.eq(0).float()\n    alpha = alpha / alpha.sum(1, keepdim=True).expand(alpha.size())\n    return alpha\n\n\ndef weighted_avg(x, weights):\n    """"""x = batch * len * d\n    weights = batch * len\n    """"""\n    return weights.unsqueeze(1).bmm(x).squeeze(1)\n'"
DrQA/drqa/model.py,6,"b'# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree. An additional grant\n# of patent rights can be found in the PATENTS file in the same directory.\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport logging\n\nfrom torch.autograd import Variable\nfrom .utils import AverageMeter\nfrom .rnn_reader import RnnDocReader\n\n# Modification:\n#   - change the logger name\n#   - save & load optimizer state dict\n#   - change the dimension of inputs (for POS and NER features)\n# Origin: https://github.com/facebookresearch/ParlAI/tree/master/parlai/agents/drqa\n\nlogger = logging.getLogger(__name__)\n\n\nclass DocReaderModel(object):\n    """"""High level model that handles intializing the underlying network\n    architecture, saving, updating examples, and predicting examples.\n    """"""\n\n    def __init__(self, opt, embedding=None, state_dict=None):\n        # Book-keeping.\n        self.opt = opt\n        self.updates = state_dict[\'updates\'] if state_dict else 0\n        self.train_loss = AverageMeter()\n\n        # Building network.\n        self.network = RnnDocReader(opt, embedding=embedding)\n        if state_dict:\n            new_state = set(self.network.state_dict().keys())\n            for k in list(state_dict[\'network\'].keys()):\n                if k not in new_state:\n                    del state_dict[\'network\'][k]\n            self.network.load_state_dict(state_dict[\'network\'])\n\n        # Building optimizer.\n        parameters = [p for p in self.network.parameters() if p.requires_grad]\n        if opt[\'optimizer\'] == \'sgd\':\n            self.optimizer = optim.SGD(parameters, opt[\'learning_rate\'],\n                                       momentum=opt[\'momentum\'],\n                                       weight_decay=opt[\'weight_decay\'])\n        elif opt[\'optimizer\'] == \'adamax\':\n            self.optimizer = optim.Adamax(parameters, opt[\'learning_rate\'],\n                                          weight_decay=opt[\'weight_decay\'])\n        else:\n            raise RuntimeError(\'Unsupported optimizer: %s\' % opt[\'optimizer\'])\n        if state_dict:\n            self.optimizer.load_state_dict(state_dict[\'optimizer\'])\n\t\n        num_params = sum(p.data.numel() for p in parameters\n            if p.data.data_ptr() != self.network.embedding.weight.data.data_ptr())\n        print (""{} parameters"".format(num_params))\n\n    def update(self, ex):\n        # Train mode\n        self.network.train()\n\n        # Transfer to GPU\n        if self.opt[\'cuda\']:\n            inputs = [Variable(e.cuda(async=True)) for e in ex[:7]]\n            target_s = Variable(ex[7].cuda(async=True))\n            target_e = Variable(ex[8].cuda(async=True))\n        else:\n            inputs = [Variable(e) for e in ex[:7]]\n            target_s = Variable(ex[7])\n            target_e = Variable(ex[8])\n\n        # Run forward\n        score_s, score_e = self.network(*inputs)\n\n        # Compute loss and accuracies\n        loss = F.nll_loss(score_s, target_s) + F.nll_loss(score_e, target_e)\n        self.train_loss.update(loss.data[0], ex[0].size(0))\n\n        # Clear gradients and run backward\n        self.optimizer.zero_grad()\n        loss.backward()\n\n        # Clip gradients\n        torch.nn.utils.clip_grad_norm(self.network.parameters(),\n                                      self.opt[\'grad_clipping\'])\n\n        # Update parameters\n        self.optimizer.step()\n        self.updates += 1\n\n        # Reset any partially fixed parameters (e.g. rare words)\n        self.reset_parameters()\n\n    def predict(self, ex):\n        # Eval mode\n        self.network.eval()\n\n        # Transfer to GPU\n        if self.opt[\'cuda\']:\n            inputs = [Variable(e.cuda(async=True), volatile=True)\n                      for e in ex[:7]]\n        else:\n            inputs = [Variable(e, volatile=True) for e in ex[:7]]\n\n        # Run forward\n        score_s, score_e = self.network(*inputs)\n\n        # Transfer to CPU/normal tensors for numpy ops\n        score_s = score_s.data.cpu()\n        score_e = score_e.data.cpu()\n\n        # Get argmax text spans\n        text = ex[-2]\n        spans = ex[-1]\n        predictions = []\n        max_len = self.opt[\'max_len\'] or score_s.size(1)\n        for i in range(score_s.size(0)):\n            scores = torch.ger(score_s[i], score_e[i])\n            scores.triu_().tril_(max_len - 1)\n            scores = scores.numpy()\n            s_idx, e_idx = np.unravel_index(np.argmax(scores), scores.shape)\n            s_offset, e_offset = spans[i][s_idx][0], spans[i][e_idx][1]\n            predictions.append(text[i][s_offset:e_offset])\n\n        return predictions\n\n    def reset_parameters(self):\n        # Reset fixed embeddings to original value\n        if self.opt[\'tune_partial\'] > 0:\n            offset = self.opt[\'tune_partial\'] + 2\n            if offset < self.network.embedding.weight.data.size(0):\n                self.network.embedding.weight.data[offset:] \\\n                    = self.network.fixed_embedding\n\n    def save(self, filename, epoch):\n        params = {\n            \'state_dict\': {\n                \'network\': self.network.state_dict(),\n                \'optimizer\': self.optimizer.state_dict(),\n                \'updates\': self.updates\n            },\n            \'config\': self.opt,\n            \'epoch\': epoch\n        }\n        try:\n            torch.save(params, filename)\n            logger.info(\'model saved to {}\'.format(filename))\n        except BaseException:\n            logger.warn(\'[ WARN: Saving failed... continuing anyway. ]\')\n\n    def cuda(self):\n        self.network.cuda()\n'"
DrQA/drqa/rnn_reader.py,2,"b'# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree. An additional grant\n# of patent rights can be found in the PATENTS file in the same directory.\nimport torch\nimport torch.nn as nn\nfrom . import layers\n\n# Modification: add \'pos\' and \'ner\' features.\n# Origin: https://github.com/facebookresearch/ParlAI/tree/master/parlai/agents/drqa\n\ndef normalize_emb_(data):\n    print (data.size(), data[:10].norm(2,1))\n    norms = data.norm(2,1) + 1e-8\n    if norms.dim() == 1:\n        norms = norms.unsqueeze(1)\n    data.div_(norms.expand_as(data))\n    print (data.size(), data[:10].norm(2,1))\n\nclass RnnDocReader(nn.Module):\n    """"""Network for the Document Reader module of DrQA.""""""\n    RNN_TYPES = {\'lstm\': nn.LSTM, \'gru\': nn.GRU, \'rnn\': nn.RNN}\n\n    def __init__(self, opt, padding_idx=0, embedding=None, normalize_emb=False):\n        super(RnnDocReader, self).__init__()\n        # Store config\n        self.opt = opt\n\n        # Word embeddings\n        if opt[\'pretrained_words\']:\n            assert embedding is not None\n            self.embedding = nn.Embedding(embedding.size(0),\n                                          embedding.size(1),\n                                          padding_idx=padding_idx)\n            if normalize_emb: normalize_emb_(embedding)\n            self.embedding.weight.data = embedding\n\n            if opt[\'fix_embeddings\']:\n                assert opt[\'tune_partial\'] == 0\n                for p in self.embedding.parameters():\n                    p.requires_grad = False\n            elif opt[\'tune_partial\'] > 0:\n                assert opt[\'tune_partial\'] + 2 < embedding.size(0)\n                fixed_embedding = embedding[opt[\'tune_partial\'] + 2:]\n                self.register_buffer(\'fixed_embedding\', fixed_embedding)\n                self.fixed_embedding = fixed_embedding\n        else:  # random initialized\n            self.embedding = nn.Embedding(opt[\'vocab_size\'],\n                                          opt[\'embedding_dim\'],\n                                          padding_idx=padding_idx)\n        if opt[\'pos\']:\n            self.pos_embedding = nn.Embedding(opt[\'pos_size\'], opt[\'pos_dim\'])\n            if normalize_emb: normalize_emb_(self.pos_embedding.weight.data)\n        if opt[\'ner\']:\n            self.ner_embedding = nn.Embedding(opt[\'ner_size\'], opt[\'ner_dim\'])\n            if normalize_emb: normalize_emb_(self.ner_embedding.weight.data)\n        # Projection for attention weighted question\n        if opt[\'use_qemb\']:\n            self.qemb_match = layers.SeqAttnMatch(opt[\'embedding_dim\'])\n\n        # Input size to RNN: word emb + question emb + manual features\n        doc_input_size = opt[\'embedding_dim\'] + opt[\'num_features\']\n        if opt[\'use_qemb\']:\n            doc_input_size += opt[\'embedding_dim\']\n        if opt[\'pos\']:\n            doc_input_size += opt[\'pos_dim\']\n        if opt[\'ner\']:\n            doc_input_size += opt[\'ner_dim\']\n\n        # RNN document encoder\n        self.doc_rnn = layers.StackedBRNN(\n            input_size=doc_input_size,\n            hidden_size=opt[\'hidden_size\'],\n            num_layers=opt[\'doc_layers\'],\n            dropout_rate=opt[\'dropout_rnn\'],\n            dropout_output=opt[\'dropout_rnn_output\'],\n            concat_layers=opt[\'concat_rnn_layers\'],\n            rnn_type=self.RNN_TYPES[opt[\'rnn_type\']],\n            padding=opt[\'rnn_padding\'],\n        )\n\n        # RNN question encoder\n        self.question_rnn = layers.StackedBRNN(\n            input_size=opt[\'embedding_dim\'],\n            hidden_size=opt[\'hidden_size\'],\n            num_layers=opt[\'question_layers\'],\n            dropout_rate=opt[\'dropout_rnn\'],\n            dropout_output=opt[\'dropout_rnn_output\'],\n            concat_layers=opt[\'concat_rnn_layers\'],\n            rnn_type=self.RNN_TYPES[opt[\'rnn_type\']],\n            padding=opt[\'rnn_padding\'],\n        )\n\n        # Output sizes of rnn encoders\n        doc_hidden_size = 2 * opt[\'hidden_size\']\n        question_hidden_size = 2 * opt[\'hidden_size\']\n        if opt[\'concat_rnn_layers\']:\n            doc_hidden_size *= opt[\'doc_layers\']\n            question_hidden_size *= opt[\'question_layers\']\n\n        # Question merging\n        if opt[\'question_merge\'] not in [\'avg\', \'self_attn\']:\n            raise NotImplementedError(\'question_merge = %s\' % opt[\'question_merge\'])\n        if opt[\'question_merge\'] == \'self_attn\':\n            self.self_attn = layers.LinearSeqAttn(question_hidden_size)\n\n        # Bilinear attention for span start/end\n        self.start_attn = layers.BilinearSeqAttn(\n            doc_hidden_size,\n            question_hidden_size,\n        )\n        self.end_attn = layers.BilinearSeqAttn(\n            doc_hidden_size,\n            question_hidden_size,\n        )\n\n    def forward(self, x1, x1_f, x1_pos, x1_ner, x1_mask, x2, x2_mask):\n        """"""Inputs:\n        x1 = document word indices             [batch * len_d]\n        x1_f = document word features indices  [batch * len_d * nfeat]\n        x1_pos = document POS tags             [batch * len_d]\n        x1_ner = document entity tags          [batch * len_d]\n        x1_mask = document padding mask        [batch * len_d]\n        x2 = question word indices             [batch * len_q]\n        x2_mask = question padding mask        [batch * len_q]\n        """"""\n        # Embed both document and question\n        x1_emb = self.embedding(x1)\n        x2_emb = self.embedding(x2)\n\n        if self.opt[\'dropout_emb\'] > 0:\n            x1_emb = nn.functional.dropout(x1_emb, p=self.opt[\'dropout_emb\'],\n                                               training=self.training)\n            x2_emb = nn.functional.dropout(x2_emb, p=self.opt[\'dropout_emb\'],\n                                           training=self.training)\n\n        drnn_input_list = [x1_emb, x1_f]\n        # Add attention-weighted question representation\n        if self.opt[\'use_qemb\']:\n            x2_weighted_emb = self.qemb_match(x1_emb, x2_emb, x2_mask)\n            drnn_input_list.append(x2_weighted_emb)\n        if self.opt[\'pos\']:\n            x1_pos_emb = self.pos_embedding(x1_pos)\n            if self.opt[\'dropout_emb\'] > 0:\n                x1_pos_emb = nn.functional.dropout(x1_pos_emb, p=self.opt[\'dropout_emb\'],\n                                               training=self.training)\n            drnn_input_list.append(x1_pos_emb)\n        if self.opt[\'ner\']:\n            x1_ner_emb = self.ner_embedding(x1_ner)\n            if self.opt[\'dropout_emb\'] > 0:\n                x1_ner_emb = nn.functional.dropout(x1_ner_emb, p=self.opt[\'dropout_emb\'],\n                                               training=self.training)\n            drnn_input_list.append(x1_ner_emb)\n        drnn_input = torch.cat(drnn_input_list, 2)\n\n        # Encode document with RNN\n        doc_hiddens = self.doc_rnn(drnn_input, x1_mask)\n\n        # Encode question with RNN + merge hiddens\n        question_hiddens = self.question_rnn(x2_emb, x2_mask)\n        if self.opt[\'question_merge\'] == \'avg\':\n            q_merge_weights = layers.uniform_weights(question_hiddens, x2_mask)\n        elif self.opt[\'question_merge\'] == \'self_attn\':\n            q_merge_weights = self.self_attn(question_hiddens, x2_mask)\n        question_hidden = layers.weighted_avg(question_hiddens, q_merge_weights)\n\n        # Predict start and end positions\n        start_scores = self.start_attn(doc_hiddens, question_hidden, x1_mask)\n        end_scores = self.end_attn(doc_hiddens, question_hidden, x1_mask)\n        return start_scores, end_scores\n'"
DrQA/drqa/utils.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree. An additional grant\n# of patent rights can be found in the PATENTS file in the same directory.\nimport argparse\n\n\n# Modification: remove unused functions and imports, add a boolean parser.\n# Origin: https://github.com/facebookresearch/ParlAI/tree/master/parlai/agents/drqa\n\n# ------------------------------------------------------------------------------\n# General logging utilities.\n# ------------------------------------------------------------------------------\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value.""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n'"
test/sru/test_sru.py,7,"b'import pytest\nimport sru\nimport torch\n\n\n@pytest.mark.parametrize(\n    ""cuda"",\n    [\n        False,\n        pytest.param(\n            True,\n            marks=pytest.mark.skipif(\n                not torch.cuda.is_available(), reason=""no cuda available""\n            ),\n        ),\n    ],\n)\n@pytest.mark.parametrize(""with_grad"", [False, True])\n@pytest.mark.parametrize(""compat"", [False, True])\ndef test_cell(cuda, with_grad, compat):\n    torch.manual_seed(123)\n    if cuda:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n    def run():\n        eps = 1e-4\n        num_sentences = 3\n        embedding_size = 7\n        rnn_hidden = 4\n        max_len = 4\n        layers = 5\n        bidirectional = True\n        encoder = sru.SRU(\n            embedding_size,\n            rnn_hidden,\n            layers,\n            bidirectional=bidirectional,\n            nn_rnn_compatible_return=compat,\n        )\n        words_embeddings = torch.rand(\n            (max_len, num_sentences, embedding_size), dtype=torch.float32\n        )\n        if cuda:\n            words_embeddings = words_embeddings.to(""cuda"")\n            encoder.cuda()\n        encoder.eval()\n        hidden, cell = encoder(words_embeddings)\n\n        def cell_to_emb(cell, batch_size):\n            if compat:\n                # should arrive as:\n                # (num_layers * num_directions, batch, hidden_size)\n                cell = cell.view(\n                    layers, 2 if bidirectional else 1, batch_size, rnn_hidden\n                )\n                cell = cell[-1].transpose(0, 1)\n                # (batch, num_directions, hidden_size)\n                cell = cell.contiguous().view(batch_size, -1)\n            else:\n                # should arrive as:\n                # (num_layers, batch_size, num_directions * hidden_size)\n                cell = cell[-1].view(batch_size, -1)\n            return cell\n\n        scores = cell_to_emb(cell, num_sentences)\n        for i in range(num_sentences):\n            hidden, cell = encoder(words_embeddings[:, i : i + 1])\n            score = cell_to_emb(cell, 1)\n            assert (score.detach() - scores[i].detach()).abs().max() <= eps\n\n    if with_grad:\n        run()\n    else:\n        with torch.no_grad():\n            run()\n'"
