file_path,api_count,code
setup.py,0,"b'import sys\nimport setuptools\n\nlong_description = """"""\nTexar-PyTorch is an open-source toolkit based on PyTorch,\naiming to support a broad set of machine learning especially text generation\ntasks, such as machine translation, dialog, summarization, content manipulation,\nlanguage modeling, and so on.\n\nTexar is designed for both researchers and practitioners for fast prototyping\nand experimentation. Checkout https://github.com/asyml/texar for the TensorFlow\nversion which has the same functionalities and (mostly) the same interfaces.\n""""""\n\nif sys.version_info < (3, 6):\n    sys.exit(\'Python>=3.6 is required by Texar-PyTorch.\')\n\nsetuptools.setup(\n    name=""texar-pytorch"",\n    version=""0.1.2-unreleased"",\n    url=""https://github.com/asyml/texar-pytorch"",\n\n    description=""Toolkit for Machine Learning and Text Generation"",\n    long_description=long_description,\n    license=\'Apache License Version 2.0\',\n\n    packages=[\n        f""texar.{name}""\n        for name in setuptools.find_packages(where=\'texar\')\n    ],\n    platforms=\'any\',\n\n    install_requires=[\n        \'regex>=2018.01.10\',\n        \'numpy\',\n        \'requests\',\n        \'funcsigs\',\n        \'sentencepiece>=0.1.8\',\n        \'mypy_extensions\',\n        \'packaging>=19.0\'\n    ],\n    extras_require={\n        \'torch\': [\'torch>=1.0.0\'],\n        \'examples\': [],\n        \'extras\': [\'Pillow>=3.0\', \'tensorboardX>=1.8\'],\n    },\n    package_data={\n        ""texar.torch"": [\n            ""../../bin/utils/multi-bleu.perl"",\n        ]\n    },\n    classifiers=[\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'Operating System :: OS Independent\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n    ],\n)\n'"
docs/conf.py,10,"b'# -*- coding: utf-8 -*-\n#\n# texar documentation build configuration file, created by\n# sphinx-quickstart on Mon Sep  4 21:15:05 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n#from unittest.mock import MagicMock\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'..\'))\nfrom texar.torch import __version__\nfrom texar.torch.version import VERSION_SHORT as __version_short__\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.extlinks\',\n    \'sphinx.ext.napoleon\',\n    \'recommonmark\',\n    \'sphinxcontrib.spelling\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\nsource_suffix = {\n    \'.rst\': \'restructuredtext\',\n    \'.md\': \'markdown\',\n}\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Texar\'\ncopyright = u\'2019, Texar\'\nauthor = u\'Texar\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\n#version = u\'{}\'.format(__version_short__)\nversion = u\'{}\'.format(__version__)\n# The full version, including alpha/beta/rc tags.\nrelease = u\'{}\'.format(__version__)\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = \'any\'\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n# html_theme = \'alabaster\'\n\nimport sphinx_rtd_theme\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': True,\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\nhtml_title = u\'Texar-PyTorch v0.1\'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\nhtml_logo = \'_static/img/logo_h.png\'\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_context = {\n    \'css_files\': [\n        \'https://fonts.googleapis.com/css?family=Lato\',\n        \'_static/css/custom_theme.css\'\n    ],\n}\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n#html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\', \'zh\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'texardoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'texar.tex\', u\'Texar Documentation\',\n     u\'Texar\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'texar\', u\'Texar Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'texar\', u\'Texar Documentation\',\n     author, \'Texar\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3/\', None),\n    # \'numpy\': (\'http://docs.scipy.org/docs/numpy/\', None),\n}\n\nextlinks = {\'torch_docs\': (\n                \'https://pytorch.org/docs/stable/%s\',\n                None),\n            \'torch\': (\n                \'https://pytorch.org/docs/stable/torch.html#torch.%s\',\n                \'torch.\'),\n            \'torch_nn\': (\n                \'https://pytorch.org/docs/stable/nn.html#torch.nn.%s\',\n                \'torch.nn.\'),\n            \'torch_tensor\': (\n                \'https://pytorch.org/docs/stable/tensors.html#torch.Tensor.%s\',\n                \'torch.Tensor.\'),\n            \'tensor\': (\n                \'https://pytorch.org/docs/stable/tensors.html#torch.%s\',\n                \'torch.\'),\n            \'gym\': (\n                \'https://gym.openai.com/docs/%s\',\n                None),\n            }\n\n##### Customize ######\n\n# Snippet to insert at beginning of each RST file.\nrst_prolog = r""""""\n.. role:: python(code)\n    :language: python\n""""""\n\nautodoc_member_order = \'bysource\'\nautodoc_typehints = \'none\'\n\nnapoleon_numpy_docstring = False\n\nspelling_lang = \'en_US\'\nspelling_word_list_filename = \'spelling_wordlist.txt\'\n\n## Exclude imports\n#autodoc_mock_imports = [\n#    ""torch""\n#]\n\n# Addresses import errors. Refer to:\n# https://docs.readthedocs.io/en/latest/faq.html#i-get-import-errors-on-libraries-that-depend-on-c-modules\n#class Mock(MagicMock):\n#    @classmethod\n#    def __getattr__(cls, name):\n#        return MagicMock()\n#MOCK_MODULES = [\'gym\']\n#sys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n'"
tests/hyperparams_test.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests of :class:`HParams`.\n""""""\n\nimport copy\nimport pickle\nimport tempfile\nimport unittest\n\nfrom texar.torch.hyperparams import HParams\n\n\nclass HParamsTest(unittest.TestCase):\n    r""""""Tests hyperparameter related operations.\n    """"""\n\n    def test_hparams(self):\n        r""""""Tests the HParams class.\n        """"""\n        default_hparams = {\n            ""str"": ""str"",\n            ""list"": [\'item1\', \'item2\'],\n            ""dict"": {\n                ""key1"": ""value1"",\n                ""key2"": ""value2""\n            },\n            ""nested_dict"": {\n                ""dict_l2"": {\n                    ""key1_l2"": ""value1_l2""\n                }\n            },\n            ""type"": ""type"",\n            ""kwargs"": {\n                ""arg1"": ""argv1""\n            },\n        }\n\n        # Test HParams.items() function\n        hparams_ = HParams(None, default_hparams)\n        names = []\n        for name, _ in hparams_.items():\n            names.append(name)\n        self.assertEqual(set(names), set(default_hparams.keys()))\n\n        hparams = {\n            ""dict"": {""key1"": ""new_value""},\n            ""kwargs"": {""arg2"": ""argv2""}\n        }\n\n        hparams_ = HParams(hparams, default_hparams)\n\n        # Test HParams construction\n        self.assertEqual(hparams_.str, default_hparams[""str""])\n        self.assertEqual(hparams_.list, default_hparams[""list""])\n        self.assertEqual(hparams_.dict.key1, hparams[""dict""][""key1""])\n        self.assertEqual(hparams_.kwargs.arg2, hparams[""kwargs""][""arg2""])\n        self.assertEqual(hparams_.nested_dict.dict_l2.key1_l2,\n                         default_hparams[""nested_dict""][""dict_l2""][""key1_l2""])\n\n        self.assertEqual(len(hparams_), len(default_hparams))\n\n        new_hparams = copy.deepcopy(default_hparams)\n        new_hparams[""dict""][""key1""] = hparams[""dict""][""key1""]\n        new_hparams[""kwargs""].update(hparams[""kwargs""])\n        self.assertEqual(hparams_.todict(), new_hparams)\n\n        self.assertTrue(""dict"" in hparams_)\n\n        self.assertIsNone(hparams_.get(\'not_existed_name\', None))\n        self.assertEqual(hparams_.get(\'str\'), default_hparams[\'str\'])\n\n        # Test HParams update related operations\n        hparams_.str = ""new_str""\n        hparams_.dict = {""key3"": ""value3""}\n        self.assertEqual(hparams_.str, ""new_str"")\n        self.assertEqual(hparams_.dict.key3, ""value3"")\n\n        hparams_.add_hparam(""added_str"", ""added_str"")\n        hparams_.add_hparam(""added_dict"", {""key4"": ""value4""})\n        hparams_.kwargs.add_hparam(""added_arg"", ""added_argv"")\n        self.assertEqual(hparams_.added_str, ""added_str"")\n        self.assertEqual(hparams_.added_dict.todict(), {""key4"": ""value4""})\n        self.assertEqual(hparams_.kwargs.added_arg, ""added_argv"")\n\n        # Test HParams I/O\n        hparams_file = tempfile.NamedTemporaryFile()\n        pickle.dump(hparams_, hparams_file)\n        with open(hparams_file.name, \'rb\') as hparams_file:\n            hparams_loaded = pickle.load(hparams_file)\n        self.assertEqual(hparams_loaded.todict(), hparams_.todict())\n\n    def test_typecheck(self):\n        r""""""Tests type-check functionality.\n        """"""\n\n        def _foo():\n            pass\n\n        def _bar():\n            pass\n\n        default_hparams = {\n            ""fn"": _foo,\n            ""fn_2"": _foo\n        }\n        hparams = {\n            ""fn"": _foo,\n            ""fn_2"": _bar\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        self.assertEqual(hparams_.fn, default_hparams[""fn""])\n\n    def test_type_kwargs(self):\n        r""""""The the special cases involving ""type"" and ""kwargs""\n        hyperparameters.\n        """"""\n        default_hparams = {\n            ""type"": ""type_name"",\n            ""kwargs"": {\n                ""arg1"": ""argv1""\n            }\n        }\n\n        hparams = {\n            ""type"": ""type_name""\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        self.assertEqual(hparams_.kwargs.todict(), default_hparams[""kwargs""])\n\n        hparams = {\n            ""type"": ""type_name"",\n            ""kwargs"": {\n                ""arg2"": ""argv2""\n            }\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        full_kwargs = {}\n        full_kwargs.update(default_hparams[""kwargs""])\n        full_kwargs.update(hparams[""kwargs""])\n        self.assertEqual(hparams_.kwargs.todict(), full_kwargs)\n\n        hparams = {\n            ""kwargs"": {\n                ""arg2"": ""argv2""\n            }\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        self.assertEqual(hparams_.kwargs.todict(), full_kwargs)\n\n        hparams = {\n            ""type"": ""type_name2""\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        self.assertEqual(hparams_.kwargs.todict(), {})\n\n        hparams = {\n            ""type"": ""type_name2"",\n            ""kwargs"": {\n                ""arg3"": ""argv3""\n            }\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        self.assertEqual(hparams_.kwargs.todict(), hparams[""kwargs""])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
texar/__init__.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
examples/bert/bert_classifier_main.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example of building a sentence classifier based on pre-trained BERT model.\n""""""\n\nimport argparse\nimport functools\nimport importlib\nimport logging\nimport os\nfrom typing import Any\n\nimport torch\nimport torch.nn.functional as F\nimport texar.torch as tx\n\nfrom utils import model_utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""--config-downstream"", default=""config_classifier"",\n    help=""Configuration of the downstream part of the model"")\nparser.add_argument(\n    \'--pretrained-model-name\', type=str, default=\'bert-base-uncased\',\n    choices=tx.modules.BERTEncoder.available_checkpoints(),\n    help=""Name of the pre-trained checkpoint to load."")\nparser.add_argument(\n    ""--config-data"", default=""config_data"", help=""The dataset config."")\nparser.add_argument(\n    ""--output-dir"", default=""output/"",\n    help=""The output directory where the model checkpoints will be written."")\nparser.add_argument(\n    ""--checkpoint"", type=str, default=None,\n    help=""Path to a model checkpoint (including bert modules) to restore from."")\nparser.add_argument(\n    ""--do-train"", action=""store_true"", help=""Whether to run training."")\nparser.add_argument(\n    ""--do-eval"", action=""store_true"",\n    help=""Whether to run eval on the dev set."")\nparser.add_argument(\n    ""--do-test"", action=""store_true"",\n    help=""Whether to run test on the test set."")\nargs = parser.parse_args()\n\nconfig_data: Any = importlib.import_module(args.config_data)\nconfig_downstream = importlib.import_module(args.config_downstream)\nconfig_downstream = {\n    k: v for k, v in config_downstream.__dict__.items()\n    if not k.startswith(\'__\') and k != ""hyperparams""}\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\nlogging.root.setLevel(logging.INFO)\n\n\ndef main() -> None:\n    """"""\n    Builds the model and runs.\n    """"""\n    tx.utils.maybe_create_dir(args.output_dir)\n\n    # Loads data\n    num_train_data = config_data.num_train_data\n\n    # Builds BERT\n    model = tx.modules.BERTClassifier(\n        pretrained_model_name=args.pretrained_model_name,\n        hparams=config_downstream)\n    model.to(device)\n\n    num_train_steps = int(num_train_data / config_data.train_batch_size *\n                          config_data.max_train_epoch)\n    num_warmup_steps = int(num_train_steps * config_data.warmup_proportion)\n\n    # Builds learning rate decay scheduler\n    static_lr = 2e-5\n\n    vars_with_decay = []\n    vars_without_decay = []\n    for name, param in model.named_parameters():\n        if \'layer_norm\' in name or name.endswith(\'bias\'):\n            vars_without_decay.append(param)\n        else:\n            vars_with_decay.append(param)\n\n    opt_params = [{\n        \'params\': vars_with_decay,\n        \'weight_decay\': 0.01,\n    }, {\n        \'params\': vars_without_decay,\n        \'weight_decay\': 0.0,\n    }]\n    optim = tx.core.BertAdam(\n        opt_params, betas=(0.9, 0.999), eps=1e-6, lr=static_lr)\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(\n        optim, functools.partial(model_utils.get_lr_multiplier,\n                                 total_steps=num_train_steps,\n                                 warmup_steps=num_warmup_steps))\n\n    train_dataset = tx.data.RecordData(hparams=config_data.train_hparam,\n                                       device=device)\n    eval_dataset = tx.data.RecordData(hparams=config_data.eval_hparam,\n                                      device=device)\n    test_dataset = tx.data.RecordData(hparams=config_data.test_hparam,\n                                      device=device)\n\n    iterator = tx.data.DataIterator(\n        {""train"": train_dataset, ""eval"": eval_dataset, ""test"": test_dataset}\n    )\n\n    def _compute_loss(logits, labels):\n        r""""""Compute loss.\n        """"""\n        if model.is_binary:\n            loss = F.binary_cross_entropy(\n                logits.view(-1), labels.view(-1), reduction=\'mean\')\n        else:\n            loss = F.cross_entropy(\n                logits.view(-1, model.num_classes),\n                labels.view(-1), reduction=\'mean\')\n        return loss\n\n    def _train_epoch():\n        r""""""Trains on the training set, and evaluates on the dev set\n        periodically.\n        """"""\n        iterator.switch_to_dataset(""train"")\n        model.train()\n\n        for batch in iterator:\n            optim.zero_grad()\n            input_ids = batch[""input_ids""]\n            segment_ids = batch[""segment_ids""]\n            labels = batch[""label_ids""]\n\n            input_length = (1 - (input_ids == 0).int()).sum(dim=1)\n\n            logits, _ = model(input_ids, input_length, segment_ids)\n\n            loss = _compute_loss(logits, labels)\n            loss.backward()\n            optim.step()\n            scheduler.step()\n            step = scheduler.last_epoch\n\n            dis_steps = config_data.display_steps\n            if dis_steps > 0 and step % dis_steps == 0:\n                logging.info(""step: %d; loss: %f"", step, loss)\n\n            eval_steps = config_data.eval_steps\n            if eval_steps > 0 and step % eval_steps == 0:\n                _eval_epoch()\n                model.train()\n\n    @torch.no_grad()\n    def _eval_epoch():\n        """"""Evaluates on the dev set.\n        """"""\n        iterator.switch_to_dataset(""eval"")\n        model.eval()\n\n        nsamples = 0\n        avg_rec = tx.utils.AverageRecorder()\n        for batch in iterator:\n            input_ids = batch[""input_ids""]\n            segment_ids = batch[""segment_ids""]\n            labels = batch[""label_ids""]\n\n            input_length = (1 - (input_ids == 0).int()).sum(dim=1)\n\n            logits, preds = model(input_ids, input_length, segment_ids)\n\n            loss = _compute_loss(logits, labels)\n            accu = tx.evals.accuracy(labels, preds)\n            batch_size = input_ids.size()[0]\n            avg_rec.add([accu, loss], batch_size)\n            nsamples += batch_size\n        logging.info(""eval accu: %.4f; loss: %.4f; nsamples: %d"",\n                     avg_rec.avg(0), avg_rec.avg(1), nsamples)\n\n    @torch.no_grad()\n    def _test_epoch():\n        """"""Does predictions on the test set.\n        """"""\n        iterator.switch_to_dataset(""test"")\n        model.eval()\n\n        _all_preds = []\n        for batch in iterator:\n            input_ids = batch[""input_ids""]\n            segment_ids = batch[""segment_ids""]\n\n            input_length = (1 - (input_ids == 0).int()).sum(dim=1)\n\n            _, preds = model(input_ids, input_length, segment_ids)\n\n            _all_preds.extend(preds.tolist())\n\n        output_file = os.path.join(args.output_dir, ""test_results.tsv"")\n        with open(output_file, ""w+"") as writer:\n            writer.write(""\\n"".join(str(p) for p in _all_preds))\n\n    if args.checkpoint:\n        ckpt = torch.load(args.checkpoint)\n        model.load_state_dict(ckpt[\'model\'])\n        optim.load_state_dict(ckpt[\'optimizer\'])\n        scheduler.load_state_dict(ckpt[\'scheduler\'])\n\n    if args.do_train:\n        for _ in range(config_data.max_train_epoch):\n            _train_epoch()\n        states = {\n            \'model\': model.state_dict(),\n            \'optimizer\': optim.state_dict(),\n            \'scheduler\': scheduler.state_dict(),\n        }\n        torch.save(states, os.path.join(args.output_dir, \'model.ckpt\'))\n\n    if args.do_eval:\n        _eval_epoch()\n\n    if args.do_test:\n        _test_epoch()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/bert/bert_classifier_using_executor_main.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example of building a sentence classifier on top of pre-trained BERT using\nTexar\'s Executor.\n""""""\n\nimport argparse\nimport functools\nimport importlib\nimport os\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport texar.torch as tx\nfrom texar.torch.run import *\n\nfrom utils import model_utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""--config-downstream"", default=""config_classifier"",\n    help=""Configuration of the downstream part of the model"")\nparser.add_argument(\n    \'--pretrained-model-name\', type=str, default=\'bert-base-uncased\',\n    choices=tx.modules.BERTEncoder.available_checkpoints(),\n    help=""Name of the pre-trained checkpoint to load."")\nparser.add_argument(\n    ""--config-data"", default=""config_data"", help=""The dataset config."")\nparser.add_argument(\n    ""--output-dir"", default=""output/"",\n    help=""The output directory where the model checkpoints will be written."")\nparser.add_argument(\n    ""--checkpoint"", type=str, default=None,\n    help=""Path to a model checkpoint (including bert modules) to restore from."")\nparser.add_argument(\n    ""--do-train"", action=""store_true"", help=""Whether to run training."")\nparser.add_argument(\n    ""--do-eval"", action=""store_true"",\n    help=""Whether to run eval on the dev set."")\nparser.add_argument(\n    ""--do-test"", action=""store_true"",\n    help=""Whether to run test on the test set."")\nargs = parser.parse_args()\n\nconfig_data: Any = importlib.import_module(args.config_data)\nconfig_downstream = importlib.import_module(args.config_downstream)\nconfig_downstream = {\n    k: v for k, v in config_downstream.__dict__.items()\n    if not k.startswith(\'__\') and k != ""hyperparams""}\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n\nclass ModelWrapper(nn.Module):\n    def __init__(self, model: tx.modules.BERTClassifier):\n        super().__init__()\n        self.model = model\n\n    def _get_outputs(self, batch: tx.data.Batch) \\\n            -> Tuple[torch.Tensor, torch.LongTensor]:\n        input_ids = batch[""input_ids""]\n        segment_ids = batch[""segment_ids""]\n        input_length = (1 - (input_ids == 0).int()).sum(dim=1)\n        logits, preds = self.model(input_ids, input_length, segment_ids)\n        return logits, preds\n\n    def forward(self,  # type: ignore\n                batch: tx.data.Batch) -> Dict[str, torch.Tensor]:\n        logits, preds = self._get_outputs(batch)\n        labels = batch[""label_ids""]\n        if self.model.is_binary:\n            loss = F.binary_cross_entropy(\n                logits.view(-1), labels.view(-1), reduction=\'mean\')\n        else:\n            loss = F.cross_entropy(\n                logits.view(-1, self.model.num_classes),\n                labels.view(-1), reduction=\'mean\')\n        return {""loss"": loss, ""preds"": preds}\n\n    def predict(self, batch: tx.data.Batch) -> Dict[str, torch.Tensor]:\n        _, preds = self._get_outputs(batch)\n        return {""preds"": preds}\n\n\nclass FileWriterMetric(metric.SimpleMetric[List[int], float]):\n    def __init__(self, file_path: Optional[Union[str, Path]] = None):\n        super().__init__(pred_name=""preds"", label_name=""input_ids"")\n        self.file_path = file_path\n\n    def _value(self) -> float:\n        path = self.file_path or tempfile.mktemp()\n        with open(path, ""w+"") as writer:\n            writer.write(""\\n"".join(str(p) for p in self.predicted))\n        return 1.0\n\n\ndef main() -> None:\n    """"""\n    Builds the model and runs.\n    """"""\n    tx.utils.maybe_create_dir(args.output_dir)\n\n    # Loads data\n    num_train_data = config_data.num_train_data\n\n    # Builds BERT\n    model = tx.modules.BERTClassifier(\n        pretrained_model_name=args.pretrained_model_name,\n        hparams=config_downstream)\n    model = ModelWrapper(model=model)\n\n    num_train_steps = int(num_train_data / config_data.train_batch_size *\n                          config_data.max_train_epoch)\n    num_warmup_steps = int(num_train_steps * config_data.warmup_proportion)\n\n    # Builds learning rate decay scheduler\n    static_lr = 2e-5\n\n    vars_with_decay = []\n    vars_without_decay = []\n    for name, param in model.named_parameters():\n        if \'layer_norm\' in name or name.endswith(\'bias\'):\n            vars_without_decay.append(param)\n        else:\n            vars_with_decay.append(param)\n\n    opt_params = [{\n        \'params\': vars_with_decay,\n        \'weight_decay\': 0.01,\n    }, {\n        \'params\': vars_without_decay,\n        \'weight_decay\': 0.0,\n    }]\n    optim = tx.core.BertAdam(\n        opt_params, betas=(0.9, 0.999), eps=1e-6, lr=static_lr)\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(\n        optim, functools.partial(model_utils.get_lr_multiplier,\n                                 total_steps=num_train_steps,\n                                 warmup_steps=num_warmup_steps))\n\n    train_dataset = tx.data.RecordData(hparams=config_data.train_hparam,\n                                       device=device)\n    eval_dataset = tx.data.RecordData(hparams=config_data.eval_hparam,\n                                      device=device)\n    test_dataset = tx.data.RecordData(hparams=config_data.test_hparam,\n                                      device=device)\n\n    batching_strategy = tx.data.TokenCountBatchingStrategy[Dict[str, Any]](\n        max_tokens=config_data.max_batch_tokens)\n\n    output_dir = Path(args.output_dir)\n    valid_metric = metric.Accuracy[float](\n        pred_name=""preds"", label_name=""label_ids"")\n\n    executor = Executor(\n        # supply executor with the model\n        model=model,\n        # define datasets\n        train_data=train_dataset,\n        valid_data=eval_dataset,\n        test_data=test_dataset,\n        batching_strategy=batching_strategy,\n        device=device,\n        # tbx logging\n        tbx_logging_dir=os.path.join(config_data.tbx_log_dir,\n                                     ""exp"" + str(config_data.exp_number)),\n        tbx_log_every=cond.iteration(config_data.tbx_logging_steps),\n        # training and stopping details\n        optimizer=optim,\n        lr_scheduler=scheduler,\n        stop_training_on=cond.epoch(config_data.max_train_epoch),\n        # logging details\n        log_destination=[sys.stdout, output_dir / ""log.txt""],\n        log_every=[cond.iteration(config_data.display_steps)],\n        # logging format\n        log_format=""{time} : Epoch {epoch:2d} @ {iteration:6d}it ""\n                   ""({progress}%, {speed}), lr = {lr:.3e}, loss = {loss:.3f}"",\n        valid_log_format=""{time} : Epoch {epoch}, ""\n                         ""{split} accuracy = {Accuracy:.3f}, loss = {loss:.3f}"",\n        valid_progress_log_format=""{time} : Evaluating on ""\n                                  ""{split} ({progress}%, {speed})"",\n        test_log_format=""{time} : Epoch {epoch}, ""\n                        ""{split} accuracy = {Accuracy:.3f}"",\n        # define metrics\n        train_metrics=[\n            (""loss"", metric.RunningAverage(1)),  # only show current loss\n            (""lr"", metric.LR(optim))],\n        valid_metrics=[valid_metric, (""loss"", metric.Average())],\n        test_metrics=[\n            valid_metric, FileWriterMetric(output_dir / ""test.output"")],\n        # freq of validation\n        validate_every=[cond.iteration(config_data.eval_steps)],\n        # checkpoint saving location\n        checkpoint_dir=args.output_dir,\n        save_every=cond.validation(better=True),\n        test_mode=\'predict\',\n        max_to_keep=1,\n        show_live_progress=True,\n    )\n\n    if args.checkpoint is not None:\n        executor.load(args.checkpoint)\n\n    if args.do_train:\n        executor.train()\n\n    if args.do_test:\n        executor.test()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/bert/bert_with_hypertuning_main.py,13,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport functools\nimport importlib\nimport sys\nimport logging\nimport shutil\nfrom typing import Dict, Any\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport hyperopt as hpo\n\nimport texar.torch as tx\nfrom texar.torch.run import *\nfrom texar.torch.modules import BERTClassifier\n\nfrom utils import model_utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""--config-downstream"", default=""config_classifier"",\n    help=""Configuration of the downstream part of the model"")\nparser.add_argument(\n    \'--pretrained-model-name\', type=str, default=\'bert-base-uncased\',\n    choices=tx.modules.BERTEncoder.available_checkpoints(),\n    help=""Name of the pre-trained checkpoint to load."")\nparser.add_argument(\n    ""--config-data"", default=""config_data"", help=""The dataset config."")\nparser.add_argument(\n    ""--output-dir"", default=""output/"",\n    help=""The output directory where the model checkpoints will be written."")\nparser.add_argument(\n    ""--checkpoint"", type=str, default=None,\n    help=""Path to a model checkpoint (including bert modules) to restore from."")\nargs = parser.parse_args()\n\nconfig_data: Any = importlib.import_module(args.config_data)\nconfig_downstream = importlib.import_module(args.config_downstream)\nconfig_downstream = {\n    k: v for k, v in config_downstream.__dict__.items()\n    if not k.startswith(\'__\')}\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\nlogging.root.setLevel(logging.INFO)\n\n\nclass ModelWrapper(nn.Module):\n    r""""""This class wraps a model (in this case a BERT classifier) and implements\n    :meth:`forward` and :meth:`predict` to conform to the requirements of\n    :class:`texar.torch.run.Executor` class. Particularly, :meth:`forward`\n    returns a dict with keys ""loss"" and ""preds"" and :meth:`predict` returns a\n    dict with key ""preds"".\n\n    Args:\n        `model`: BERTClassifier\n            A BERTClassifier model\n    """"""\n\n    def __init__(self, model: BERTClassifier):\n        super().__init__()\n        self.model = model\n\n    def _compute_loss(self, logits, labels):\n        if self.model.is_binary:\n            loss = F.binary_cross_entropy(\n                logits.view(-1), labels.view(-1), reduction=\'mean\')\n        else:\n            loss = F.cross_entropy(\n                logits.view(-1, self.model.num_classes),\n                labels.view(-1), reduction=\'mean\')\n        return loss\n\n    def forward(self,  # type: ignore\n                batch: tx.data.Batch) -> Dict[str, torch.Tensor]:\n        r""""""Run forward through the model and return a dict to be consumed\n        by the :class:`texar.torch.run.Executor`. This method will be called by\n        :class:`texar.torch.run.Executor` during training. See\n        https://texar-pytorch.readthedocs.io/en/latest/code/run.html#executor-general-args\n        for more details.\n\n        Args:\n            `batch`: :class:`texar.data.Batch`. (See\n                https://texar-pytorch.readthedocs.io/en/latest/code/data.html#texar.torch.data.Batch\n                for more details)\n                A batch of inputs to be passed through the model\n\n        Returns:\n            A dict with keys ""loss"" and ""preds"" containing the loss and\n            predictions on :attr:`batch` respectively.\n        """"""\n        input_ids = batch[""input_ids""]\n        segment_ids = batch[""segment_ids""]\n        labels = batch[""label_ids""]\n\n        input_length = (1 - (input_ids == 0).int()).sum(dim=1)\n\n        logits, preds = self.model(input_ids, input_length, segment_ids)\n\n        loss = self._compute_loss(logits, labels)\n\n        return {""loss"": loss, ""preds"": preds}\n\n    def predict(self, batch: tx.data.Batch) -> Dict[str, torch.Tensor]:\n        r""""""Predict the labels for the :attr:`batch` of examples. This method\n        will be called instead of :meth:`forward` during validation or testing,\n        if :class:`texar.torch.run.Executor`\'s :attr:`validate_mode` or\n        :attr:`test_mode` is set to ``""predict""`` instead of ``""eval""``.\n\n        Args:\n            `batch`: tx.data.Batch\n                A batch of inputs to run prediction on\n        """"""\n        input_ids = batch[""input_ids""]\n        segment_ids = batch[""segment_ids""]\n\n        input_length = (1 - (input_ids == 0).int()).sum(dim=1)\n\n        _, preds = self.model(input_ids, input_length, segment_ids)\n\n        return {""preds"": preds}\n\n\nclass TPE:\n    r"""""":class:`TPE` uses Tree-structured Parzen Estimator algorithm from\n    `hyperopt` for hyperparameter tuning.\n\n    Args:\n        model_config: Dict\n            A conf dict which is passed to BERT classifier\n        output_dir: str\n            A path to store the models\n    """"""\n\n    def __init__(self, model_config: Dict, output_dir: str = ""output/""):\n        tx.utils.maybe_create_dir(output_dir)\n\n        self.model_config = model_config\n\n        self.output_dir = output_dir\n\n        # create datasets\n        self.train_dataset = tx.data.RecordData(\n            hparams=config_data.train_hparam, device=device)\n        self.eval_dataset = tx.data.RecordData(\n            hparams=config_data.eval_hparam, device=device)\n\n        # Builds BERT\n        model = tx.modules.BERTClassifier(\n            pretrained_model_name=args.pretrained_model_name,\n            hparams=self.model_config)\n        self.model = ModelWrapper(model=model)\n        self.model.to(device)\n\n        # batching\n        self.batching_strategy = \\\n            tx.data.TokenCountBatchingStrategy[Dict[str, Any]](\n                max_tokens=config_data.max_batch_tokens)\n\n        # logging formats\n        self.log_format = ""{time} : Epoch {epoch:2d} @ {iteration:6d}it "" \\\n                          ""({progress}%, {speed}), "" \\\n                          ""lr = {lr:.9e}, loss = {loss:.3f}""\n        self.valid_log_format = ""{time} : Epoch {epoch}, "" \\\n                                ""{split} accuracy = {Accuracy:.3f}, "" \\\n                                ""loss = {loss:.3f}""\n        self.valid_progress_log_format = ""{time} : Evaluating on "" \\\n                                         ""{split} ({progress}%, {speed})""\n\n        # exp number\n        self.exp_number = 1\n\n    def objective_func(self, hyperparams: Dict):\n        r""""""Compute ""loss"" for a given hyperparameter values. This function is\n        passed to hyperopt\'s ``""fmin""`` (see the :meth:`run` method) function\n        and gets repeatedly called to find the best set of hyperparam values.\n        Below is an example of how to use this method\n\n        .. code-block:: python\n\n            import hyperopt as hpo\n\n            trials = hpo.Trials()\n            hpo.fmin(fn=self.objective_func,\n                     space=space,\n                     algo=hpo.tpe.suggest,\n                     max_evals=3,\n                     trials=trials)\n\n        Args:\n            hyperparams: Dict\n                A `(key, value)` dict representing the ``""value""`` to try for\n                the hyperparam ``""key""``\n\n        Returns:\n            A dict with keys ""loss"", ""status"" and ""model"" indicating the loss\n            for this trial, the status, and the path to the saved model.\n        """"""\n        print(f""Using {hyperparams} for trial {self.exp_number}"")\n\n        # Loads data\n        num_train_data = config_data.num_train_data\n        num_train_steps = int(num_train_data / config_data.train_batch_size *\n                              config_data.max_train_epoch)\n\n        # hyperparams\n        num_warmup_steps = hyperparams[""optimizer.warmup_steps""]\n        static_lr = hyperparams[""optimizer.static_lr""]\n\n        vars_with_decay = []\n        vars_without_decay = []\n        for name, param in self.model.named_parameters():\n            if \'layer_norm\' in name or name.endswith(\'bias\'):\n                vars_without_decay.append(param)\n            else:\n                vars_with_decay.append(param)\n\n        opt_params = [{\n            \'params\': vars_with_decay,\n            \'weight_decay\': 0.01,\n        }, {\n            \'params\': vars_without_decay,\n            \'weight_decay\': 0.0,\n        }]\n\n        optim = tx.core.BertAdam(\n            opt_params, betas=(0.9, 0.999), eps=1e-6, lr=static_lr)\n\n        scheduler = torch.optim.lr_scheduler.LambdaLR(\n            optim, functools.partial(model_utils.get_lr_multiplier,\n                                     total_steps=num_train_steps,\n                                     warmup_steps=num_warmup_steps))\n\n        valid_metric = metric.Accuracy[float](\n            pred_name=""preds"", label_name=""label_ids"")\n        checkpoint_dir = f""./{self.output_dir}/exp{self.exp_number}""\n        log_file = f""./{self.output_dir}/log.txt""\n\n        executor = Executor(\n            # supply executor with the model\n            model=self.model,\n            # define datasets\n            train_data=self.train_dataset,\n            valid_data=self.eval_dataset,\n            batching_strategy=self.batching_strategy,\n            device=device,\n            # training and stopping details\n            optimizer=optim,\n            lr_scheduler=scheduler,\n            stop_training_on=cond.epoch(config_data.max_train_epoch),\n            # logging details\n            log_every=[cond.epoch(1)],\n            log_destination=[sys.stdout, log_file],\n            # logging format\n            log_format=self.log_format,\n            # define metrics\n            train_metrics=[\n                (""loss"", metric.RunningAverage(1)),\n                (""lr"", metric.LR(optim))],\n            valid_metrics=[valid_metric, (""loss"", metric.Average())],\n            validate_every=cond.epoch(1),\n            save_every=cond.epoch(config_data.max_train_epoch),\n            checkpoint_dir=checkpoint_dir,\n            max_to_keep=1,\n            show_live_progress=True,\n            print_model_arch=False\n        )\n\n        if args.checkpoint is not None:\n            executor.load(args.checkpoint)\n\n        executor.train()\n\n        print(f""Loss on the valid dataset ""\n              f""{executor.valid_metrics[\'loss\'].value()}"")\n        self.exp_number += 1\n\n        return {\n            ""loss"": executor.valid_metrics[""loss""].value(),\n            ""status"": hpo.STATUS_OK,\n            ""model"": checkpoint_dir\n        }\n\n    def run(self, hyperparams: Dict):\n        r""""""Run the TPE algorithm with hyperparameters  :attr:`hyperparams`\n\n        Args:\n            hyperparams: Dict\n                The `(key, value)` pairs of hyperparameters along their range of\n                values.\n        """"""\n        space = {}\n        for k, v in hyperparams.items():\n            if isinstance(v, dict):\n                if v[""dtype""] == int:\n                    space[k] = hpo.hp.choice(\n                        k, range(v[""start""], v[""end""]))\n                else:\n                    space[k] = hpo.hp.uniform(k, v[""start""], v[""end""])\n        trials = hpo.Trials()\n        hpo.fmin(fn=self.objective_func,\n                 space=space,\n                 algo=hpo.tpe.suggest,\n                 max_evals=3,\n                 trials=trials)\n        _, best_trial = min((trial[""result""][""loss""], trial)\n                            for trial in trials.trials)\n\n        # delete all the other models\n        for trial in trials.trials:\n            if trial is not best_trial:\n                shutil.rmtree(trial[""result""][""model""])\n\n\ndef main():\n    model_config = {k: v for k, v in config_downstream.items() if\n                    k != ""hyperparams""}\n    tpe = TPE(model_config=model_config, output_dir=args.output_dir)\n    hyperparams = config_downstream[""hyperparams""]\n    tpe.run(hyperparams)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/bert/config_classifier.py,0,"b'name = ""bert_classifier""\nhidden_size = 768\nclas_strategy = ""cls_time""\ndropout = 0.1\nnum_classes = 2\n\n# This hyperparams is used in bert_with_hypertuning_main.py example\nhyperparams = {\n    ""optimizer.warmup_steps"": {""start"": 10000, ""end"": 20000, ""dtype"": int},\n    ""optimizer.static_lr"": {""start"": 1e-3, ""end"": 1e-2, ""dtype"": float}\n}\n'"
examples/bert/config_data.py,0,"b'pickle_data_dir = ""data/MRPC""\nmax_seq_length = 128\nnum_classes = 2\nnum_train_data = 3668\n\n# used for bert executor example\nmax_batch_tokens = 128\n\ntrain_batch_size = 32\nmax_train_epoch = 5\ndisplay_steps = 50  # Print training loss every display_steps; -1 to disable\n\n# tbx config\ntbx_logging_steps = 5  # log the metrics for tbX visualization\ntbx_log_dir = ""runs/""\nexp_number = 1  # experiment number\n\neval_steps = 100  # Eval on the dev set every eval_steps; -1 to disable\n# Proportion of training to perform linear learning rate warmup for.\n# E.g., 0.1 = 10% of training.\nwarmup_proportion = 0.1\neval_batch_size = 8\ntest_batch_size = 8\n\nfeature_types = {\n    # Reading features from pickled data file.\n    # E.g., Reading feature ""input_ids"" as dtype `int64`;\n    # ""FixedLenFeature"" indicates its length is fixed for all data instances;\n    # and the sequence length is limited by `max_seq_length`.\n    ""input_ids"": [""int64"", ""stacked_tensor"", max_seq_length],\n    ""input_mask"": [""int64"", ""stacked_tensor"", max_seq_length],\n    ""segment_ids"": [""int64"", ""stacked_tensor"", max_seq_length],\n    ""label_ids"": [""int64"", ""stacked_tensor""]\n}\n\ntrain_hparam = {\n    ""allow_smaller_final_batch"": False,\n    ""batch_size"": train_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_types"": feature_types,\n        ""files"": ""{}/train.pkl"".format(pickle_data_dir)\n    },\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 100\n}\n\neval_hparam = {\n    ""allow_smaller_final_batch"": True,\n    ""batch_size"": eval_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_types"": feature_types,\n        ""files"": ""{}/eval.pkl"".format(pickle_data_dir)\n    },\n    ""shuffle"": False\n}\n\ntest_hparam = {\n    ""allow_smaller_final_batch"": True,\n    ""batch_size"": test_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_types"": feature_types,\n        ""files"": ""{}/predict.pkl"".format(pickle_data_dir)\n    },\n    ""shuffle"": False\n}\n'"
examples/bert/prepare_data.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Produces pickle files and modifies data configuration file\n""""""\n\nimport argparse\nimport importlib\nimport logging\nimport os\nfrom typing import Any\n\nimport texar.torch as tx\n\nfrom utils import data_utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""--task"", type=str, default=""MRPC"",\n    choices=[\'COLA\', \'MNLI\', \'MRPC\', \'XNLI\', \'SST\'],\n    help=""The task to run experiment on."")\nparser.add_argument(\n    \'--pretrained-model-name\', type=str, default=\'bert-base-uncased\',\n    help=""The name of a pre-trained model to load selected in the ""\n         ""list of: `bert-base-uncased`, `bert-large-uncased`, ""\n         ""`bert-base-cased`, `bert-large-cased`, ""\n         ""`bert-base-multilingual-uncased`, `bert-base-multilingual-cased`, ""\n         ""and `bert-base-chinese`."")\nparser.add_argument(\n    ""--max-seq-length"", type=int, default=128,\n    help=""The maxium length of sequence, longer sequence will be trimmed."")\nparser.add_argument(\n    ""--output-dir"", type=str, default=None,\n    help=""The output directory where the pickled files will be generated. ""\n         ""By default it will be set to \'data/{task}\'. E.g.: if ""\n         ""task is \'MRPC\', it will be set as \'data/MRPC\'"")\nparser.add_argument(\n    ""--config-data"", default=""config_data"", help=""The dataset config."")\nargs = parser.parse_args()\n\nlogging.root.setLevel(logging.INFO)\n\n\ndef modify_config_data(max_seq_length, num_train_data, num_classes):\n    # Modify the data configuration file\n    config_data_exists = os.path.isfile(\'./config_data.py\')\n    if config_data_exists:\n        with open(""./config_data.py"", \'r\') as file:\n            filedata = file.read()\n            filedata_lines = filedata.split(\'\\n\')\n            idx = 0\n            while True:\n                if idx >= len(filedata_lines):\n                    break\n                line = filedata_lines[idx]\n                if (line.startswith(\'num_classes =\') or\n                        line.startswith(\'num_train_data =\') or\n                        line.startswith(\'max_seq_length =\')):\n                    filedata_lines.pop(idx)\n                    idx -= 1\n                idx += 1\n\n            if len(filedata_lines) > 0:\n                insert_idx = 1\n            else:\n                insert_idx = 0\n            filedata_lines.insert(\n                insert_idx, f\'{""num_train_data""} = {num_train_data}\')\n            filedata_lines.insert(\n                insert_idx, f\'{""num_classes""} = {num_classes}\')\n            filedata_lines.insert(\n                insert_idx, f\'{""max_seq_length""} = {max_seq_length}\')\n\n        with open(""./config_data.py"", \'w\') as file:\n            file.write(\'\\n\'.join(filedata_lines))\n        logging.info(""config_data.py has been updated"")\n    else:\n        logging.info(""config_data.py cannot be found"")\n\n    logging.info(""Data preparation finished"")\n\n\ndef main() -> None:\n    """""" Starts the data preparation\n    """"""\n    # Loads data\n    logging.info(""Loading data"")\n\n    task_datasets_rename = {\n        ""COLA"": ""CoLA"",\n        ""SST"": ""SST-2"",\n    }\n\n    data_dir = f\'data/{args.task}\'\n    if args.task.upper() in task_datasets_rename:\n        data_dir = f\'data/{task_datasets_rename[args.task]}\'\n\n    if args.output_dir is None:\n        output_dir = data_dir\n    else:\n        output_dir = args.output_dir\n    tx.utils.maybe_create_dir(output_dir)\n\n    processors = {\n        ""COLA"": data_utils.ColaProcessor,\n        ""MNLI"": data_utils.MnliProcessor,\n        ""MRPC"": data_utils.MrpcProcessor,\n        ""XNLI"": data_utils.XnliProcessor,\n        \'SST\': data_utils.SSTProcessor\n    }\n    processor = processors[args.task]()\n\n    num_classes = len(processor.get_labels())\n    num_train_data = len(processor.get_train_examples(data_dir))\n    logging.info(""num_classes: %d; num_train_data: %d"",\n                 num_classes, num_train_data)\n\n    config_data: Any = importlib.import_module(args.config_data)\n\n    tokenizer = tx.data.BERTTokenizer(\n        pretrained_model_name=args.pretrained_model_name)\n\n    # Produces pickled files\n    data_utils.prepare_record_data(\n        processor=processor,\n        tokenizer=tokenizer,\n        data_dir=data_dir,\n        max_seq_length=args.max_seq_length,\n        output_dir=output_dir,\n        feature_types=config_data.feature_types)\n    modify_config_data(args.max_seq_length, num_train_data, num_classes)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/gpt-2/config_train.py,0,"b'""""""Config file for GPT2 training.\n""""""\n\npickle_data_dir = ""data/toy""\nmax_seq_length = 128\nmax_decoding_length = max_seq_length\n\ntrain_batch_size = 32\nmax_train_epoch = 100\ndisplay_steps = 1  # Print training loss every display_steps; -1 to disable\neval_steps = 1  # Eval on the dev set every eval_steps; -1 to disable\n\neval_batch_size = 8\ntest_batch_size = 8\n\n# Optimization configs\n\nopt = {\n    \'optimizer\': {\n        \'type\': \'Adam\',\n        \'kwargs\': {\n            \'lr\': 0.001\n        }\n    }\n}\n\n# Data configs\n\nfeature_types = {\n    # Reading features from pickle data file.\n    # E.g., Reading feature ""text_ids"" as dtype `int64`;\n    # ""stacked_tensor"" indicates its length is fixed for all data instances;\n    # and the sequence length is limited by `max_seq_length`.\n    ""text_ids"": [""int64"", ""stacked_tensor"", max_seq_length],\n    ""length"": [""int64"", ""stacked_tensor""]\n}\n\ntrain_hparam = {\n    ""allow_smaller_final_batch"": False,\n    ""batch_size"": train_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_types"": feature_types,\n        ""files"": ""{}/train.pkl"".format(pickle_data_dir)\n    },\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 10000\n}\n\neval_hparam = {\n    ""allow_smaller_final_batch"": True,\n    ""batch_size"": eval_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_types"": feature_types,\n        ""files"": ""{}/dev.pkl"".format(pickle_data_dir)\n    },\n    ""shuffle"": False\n}\n\n# Set to `test_hparam` to `None` if generating from scratch\n# (instead of generating continuation) at test time\ntest_hparam = {\n    ""allow_smaller_final_batch"": True,\n    ""batch_size"": test_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_types"": feature_types,\n        ""files"": ""{}/test.pkl"".format(pickle_data_dir)\n    },\n    ""shuffle"": False\n}\n'"
examples/gpt-2/gpt2_generate_main.py,9,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example of building OpenAI GPT-2 language model for sample generation.\n""""""\nimport argparse\nimport random\nimport sys\n\nimport numpy as np\nimport torch\nimport texar.torch as tx\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \'--checkpoint\', type=str, default=None,\n    help=""Model checkpoint to load model weights from."")\nparser.add_argument(\n    ""--pretrained-model-name"", type=str, default=""gpt2-small"",\n    choices=tx.modules.GPT2Decoder.available_checkpoints(),\n    help=""Name of the pre-trained checkpoint to load."")\nparser.add_argument(\n    \'--seed\', type=int, default=None, help=""Random seed."")\nparser.add_argument(\n    \'--nsamples\', type=int, default=1, help=""The number of samples per input."")\nparser.add_argument(\n    \'--batch-size\', type=int, default=1, help=""The batch size of input."")\nparser.add_argument(\n    \'--max-decoding-length\', type=int, default=128,\n    help=""The maximun length of generated text."")\nparser.add_argument(\n    \'--temperature\', type=float, default=0.7,\n    help=""Softmax temperature for top-k sample decoding. Must be strictly ""\n         ""greater than 0. Defaults to 0.7."")\nparser.add_argument(\n    \'--top-k\', type=int, default=40,\n    help=""The number of top most likely candidates from a vocab distribution."")\nparser.add_argument(\n    \'--top-p\', type=float, default=None,\n    help=""Select tokens with cumulative probability of at most \'p\' when ""\n         ""arranged in decreasing order. This will use ""\n         ""TopPSampleEmbeddingHelper for decoding."")\nparser.add_argument(\n    \'--interactive\', action=\'store_true\', help=""Interactive mode or not."")\n\nargs = parser.parse_args()\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n\ndef main() -> None:\n    if args.seed:\n        random.seed(args.seed)\n        np.random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed(args.seed)\n\n    nsamples = args.nsamples\n    batch_size = args.batch_size\n    max_decoding_length = args.max_decoding_length\n\n    # Build the GPT-2 model\n    model = tx.modules.GPT2Decoder(args.pretrained_model_name)\n    if args.checkpoint:\n        ckpt = torch.load(args.checkpoint)\n        model.load_state_dict(ckpt[\'model\'])\n    model.to(device)\n\n    if max_decoding_length > model.hparams.position_size:\n        raise ValueError(\n            ""max_decoding_length should not be greater than position size"")\n\n    # Create a GPT-2 tokenizer (BPE encoding)\n    tokenizer = tx.data.GPT2Tokenizer(\n        pretrained_model_name=args.pretrained_model_name)\n    end_token = tokenizer.map_token_to_id(\'<|endoftext|>\')\n\n    print(""\\nFinished loading\\n"")\n\n    def _get_helper(start_tokens):\n        if args.top_p:\n            helper = tx.modules.TopPSampleEmbeddingHelper(\n                start_tokens=start_tokens,\n                end_token=end_token,\n                p=args.top_p,\n                softmax_temperature=args.temperature)\n        else:\n            helper = tx.modules.TopKSampleEmbeddingHelper(\n                start_tokens=start_tokens,\n                end_token=end_token,\n                top_k=args.top_k,\n                softmax_temperature=args.temperature)\n        return helper\n\n    if args.interactive:\n        # Generate continuations of context\n        while True:\n\n            try:\n                raw_text = input(""Model input >>> "")\n                while not raw_text:\n                    print(\'Input should not be empty!\')\n                    raw_text = input(""Model input >>> "")\n            except EOFError:\n                print(""EOF entered, quitting."")\n                sys.exit()\n\n            context_tokens = tokenizer.map_text_to_id(raw_text)\n            context = torch.tensor(\n                [context_tokens for _ in range(batch_size)],\n                device=device)\n            context_length = torch.tensor(\n                [len(context_tokens) for _ in range(batch_size)],\n                device=device)\n\n            start_tokens = context[:, 0]\n\n            helper = _get_helper(start_tokens)\n\n            generated = 0\n            for _ in range(nsamples // batch_size):\n                output, _ = model(\n                    context=context,\n                    context_sequence_length=context_length,\n                    max_decoding_length=max_decoding_length,\n                    helper=helper)\n\n                sample_id = output.sample_id\n                for i in range(batch_size):\n                    generated += 1\n                    print(""="" * 40 +\n                          "" SAMPLE "" + str(generated) + "" "" + ""="" * 40)\n                    si = sample_id[i][len(context_tokens):]\n                    print(tokenizer.map_id_to_text(si.tolist()))\n\n            print(""="" * 80)\n    else:\n        # Generate samples from scratch\n        start_tokens = torch.full(\n            (batch_size,), end_token, dtype=torch.int64, device=device)\n\n        generated = 0\n        while nsamples == 0 or generated < nsamples:\n\n            helper = _get_helper(start_tokens)\n\n            output, _ = model(\n                max_decoding_length=max_decoding_length,\n                helper=helper)\n            sample_id = output.sample_id\n            for i in range(batch_size):\n                generated += batch_size\n                text = tokenizer.map_id_to_text(sample_id[i].tolist())\n                print(""="" * 40 +\n                      "" SAMPLE "" + str(generated) + "" "" + ""="" * 40)\n                print(text)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/gpt-2/gpt2_train_main.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example of fine-tuning OpenAI GPT-2 language model.\n""""""\n\nimport argparse\nimport importlib\nimport os\nfrom typing import Any\n\nimport torch\nimport texar.torch as tx\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \'--checkpoint\', type=str, default=None,\n    help=""Model checkpoint to load model weights from."")\nparser.add_argument(\n    ""--pretrained-model-name"", type=str, default=""gpt2-small"",\n    choices=tx.modules.GPT2Decoder.available_checkpoints(),\n    help=""Name of the pre-trained checkpoint to load."")\nparser.add_argument(\n    \'--config-train\', type=str, default=""config_train"",\n    help=""Configurations of GPT-2 training, including data and ""\n         ""optimization hyperparameters."")\nparser.add_argument(\n    ""--output-dir"", default=""output/"",\n    help=""The output directory where the model checkpoints will be written."")\nparser.add_argument(\n    \'--temperature\', type=float, default=0.7,\n    help=""Softmax temperature for top-k sample decoding. Must be strictly ""\n         ""greater than 0. Defaults to 0.7."")\nparser.add_argument(\n    \'--top-k\', type=int, default=40,\n    help=""The number of top most likely candidates from a vocab distribution."")\nparser.add_argument(\n    \'--top-p\', type=float, default=None,\n    help=""Select tokens with cumulative probability of at most \'p\' when ""\n         ""arranged in decreasing order. This will use ""\n         ""TopPSampleEmbeddingHelper for decoding."")\nparser.add_argument(\n    ""--do-train"", action=""store_true"", help=""Whether to run training."")\nparser.add_argument(\n    ""--do-eval"", action=""store_true"",\n    help=""Whether to run eval on the dev set."")\nparser.add_argument(\n    ""--do-test"", action=""store_true"",\n    help=""Whether to run test on the test set."")\n\nargs = parser.parse_args()\n\nconfig_train: Any = importlib.import_module(args.config_train)\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n\ndef main() -> None:\n    """"""\n    Builds the model and runs.\n    """"""\n    tx.utils.maybe_create_dir(args.output_dir)\n\n    max_decoding_length = config_train.max_decoding_length\n\n    # Build the GPT-2 model\n    model = tx.modules.GPT2Decoder(args.pretrained_model_name)\n    if args.checkpoint:\n        ckpt = torch.load(args.checkpoint)\n        model.load_state_dict(ckpt[\'model\'])\n    model.to(device)\n\n    if max_decoding_length > model.hparams.position_size:\n        raise ValueError(\n            ""max_decoding_length should not be greater than position size"")\n\n    # Create a GPT-2 tokenizer (BPE encoding)\n    tokenizer = tx.data.GPT2Tokenizer(\n        pretrained_model_name=args.pretrained_model_name)\n\n    # Loads data\n    datasets = {}\n    if args.do_train:\n        train_dataset = tx.data.RecordData(\n            hparams=config_train.train_hparam, device=device)\n        datasets[\'train\'] = train_dataset\n    if args.do_eval:\n        eval_dataset = tx.data.RecordData(\n            hparams=config_train.eval_hparam, device=device)\n        datasets[\'eval\'] = eval_dataset\n    if args.do_test:\n        test_dataset = tx.data.RecordData(\n            hparams=config_train.test_hparam, device=device)\n        datasets[\'test\'] = test_dataset\n    iterator = tx.data.DataIterator(datasets)\n\n    # For training\n    train_op = tx.core.get_train_op(\n        params=model.parameters(), hparams=config_train.opt)\n\n    end_token = tokenizer.map_token_to_id(\'<|endoftext|>\')\n\n    def _get_helper(start_tokens):\n        if args.top_p:\n            helper = tx.modules.TopPSampleEmbeddingHelper(\n                start_tokens=start_tokens,\n                end_token=end_token,\n                p=args.top_p,\n                softmax_temperature=args.temperature)\n        else:\n            helper = tx.modules.TopKSampleEmbeddingHelper(\n                start_tokens=start_tokens,\n                end_token=end_token,\n                top_k=args.top_k,\n                softmax_temperature=args.temperature)\n        return helper\n\n    dis_steps = config_train.display_steps\n    eval_steps = config_train.eval_steps\n\n    eval_best = {""loss"": 1e8, ""ppl"": 1e8}\n\n    def _train_epoch():\n        r""""""Trains on the training set, and evaluates on the dev set\n        periodically.\n        """"""\n        iterator.switch_to_dataset(""train"")\n        model.train()\n\n        step = 0\n        for batch in iterator:\n            input_ids = batch[""text_ids""]\n\n            outputs = model(inputs=input_ids, decoding_strategy=\'train_greedy\')\n\n            loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n                labels=batch[\'text_ids\'][:, 1:],\n                logits=outputs.logits[:, :-1, :],\n                sequence_length=batch[\'length\'] - 1,\n                average_across_timesteps=True,\n                sum_over_timesteps=False)\n            loss.backward()\n            train_op()\n\n            if dis_steps > 0 and step % dis_steps == 0:\n                print(""step={}, loss={:.4f}"".format(step, loss))\n\n            if eval_steps > 0 and step % eval_steps == 0:\n                _eval_epoch()\n                model.train()\n\n            step += 1\n\n    @torch.no_grad()\n    def _eval_epoch():\n        r""""""Evaluates on the dev set.\n        """"""\n        iterator.switch_to_dataset(""eval"")\n        model.eval()\n\n        nsamples = 0\n        avg_rec = tx.utils.AverageRecorder()\n        for batch in iterator:\n            input_ids = batch[""text_ids""]\n\n            outputs = model(inputs=input_ids)\n\n            loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n                labels=batch[\'text_ids\'][:, 1:],\n                logits=outputs.logits[:, :-1, :],\n                sequence_length=batch[\'length\'] - 1,\n                average_across_timesteps=True,\n                sum_over_timesteps=False)\n            ppl = torch.exp(loss)\n            batch_size = input_ids.size()[0]\n            avg_rec.add([loss, ppl], batch_size)\n            nsamples += batch_size\n\n        print(""eval loss: {:.4f}; ppl: {:.4f}; ""\n              ""nsamples: {:d}"".format(avg_rec.avg(0), avg_rec.avg(1), nsamples))\n\n        if args.do_train and avg_rec.avg(0) < eval_best[""loss""]:\n            eval_best[""loss""] = avg_rec.avg(0)\n            eval_best[""ppl""] = avg_rec.avg(1)\n            ckpt_fn = os.path.join(args.output_dir, \'model_best.ckpt\')\n            torch.save(model.state_dict(), ckpt_fn)\n            print(""Checkpoint best to {}"".format(ckpt_fn))\n\n    @torch.no_grad()\n    def _test_epoch():\n        r""""""Generates samples on the test set.\n        """"""\n        iterator.switch_to_dataset(""test"")\n        model.eval()\n\n        _all_inputs = []\n        _all_samples = []\n\n        for batch in iterator:\n            input_ids = batch[""text_ids""]\n            length = batch[""length""]\n            start_tokens = input_ids[:, 0]\n            helper = _get_helper(start_tokens)\n\n            output, _ = model(\n                context=input_ids,\n                context_sequence_length=length,\n                max_decoding_length=max_decoding_length,\n                helper=helper)\n            sample_id = output.sample_id\n\n            _inputs = []\n            for i, l in zip(input_ids, length):\n                # Delete padding\n                _inputs.append(i[:l].tolist())\n            _all_inputs.extend(_inputs)\n\n            _samples = []\n            for s, l in zip(sample_id, length):\n                # Delte inputs from samples\n                _samples.append(s[l:].tolist())\n            _all_samples.extend(_samples)\n\n        # Parse samples and write to file\n\n        eos_token_id = tokenizer.map_token_to_id(\'<|endoftext|>\')\n\n        _all_input_text = []\n        for i in _all_inputs:\n            if i[0] == eos_token_id:\n                # \'<|endoftext|>\' is used as the BOS token. Delete it here\n                i = i[1:]\n            i_text = tokenizer.map_id_to_text(i)\n            _all_input_text.append(i_text)\n        # \'<|endoftext|>\' is used as the PAD token. Delete them here\n        _all_input_text = tx.utils.strip_eos(_all_input_text,\n                                             eos_token=\'<|endoftext|>\')\n\n        _all_samples_text = []\n        for i, s in zip(_all_inputs, _all_samples):\n            s_text = tokenizer.map_id_to_text(s)\n            s_text = s_text.replace(\'\\n\', \' \')\n            _all_samples_text.append(s_text)\n        _all_samples_text = tx.utils.strip_eos(_all_samples_text,\n                                               eos_token=\'<|endoftext|>\')\n\n        output_file = os.path.join(args.output_dir, ""test_samples.tsv"")\n        print(\'Write samples to {}\'.format(output_file))\n        tx.utils.write_paired_text(\n            _all_input_text, _all_samples_text, output_file)\n\n    if args.do_train:\n        for _ in range(config_train.max_train_epoch):\n            _train_epoch()\n        torch.save(model.state_dict(),\n                   os.path.join(args.output_dir, \'model.ckpt\'))\n\n    if args.do_eval:\n        _eval_epoch()\n\n    if args.do_test:\n        _test_epoch()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/gpt-2/prepare_data.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Preprocesses raw data and produces pickle files\n""""""\n\nimport argparse\nimport importlib\nfrom typing import Any\n\nimport texar.torch as tx\n\nfrom utils import data_utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \'--data-dir\', type=str, default=\'data/toy\',\n    help=""The directory of raw data, wherein data files must be named as ""\n         ""\'train.txt\', \'dev.txt\', or \'test.txt\'."")\nparser.add_argument(\n    \'--max-seq-length\', type=int, default=128,\n    help=""The maxium length of sequence, longer sequence will be trimmed."")\nparser.add_argument(\n    \'--output-dir\', type=str, default=None,\n    help=""The output directory where the pickle files will be generated. ""\n         ""By default it is set to be the same as `--data-dir`."")\nparser.add_argument(\n    \'--pretrained-model-name\', type=str, default=\'gpt2-small\',\n    choices=tx.modules.GPT2Decoder.available_checkpoints(),\n    help=""Name of the pre-trained checkpoint to load."")\nparser.add_argument(\n    \'--config-train\', type=str, default=""config_train"",\n    help=""Configurations of GPT-2 training, including data and ""\n         ""optimization hyperparameters."")\n\nargs = parser.parse_args()\n\n\ndef main() -> None:\n    """"""Preprocess raw data and produces pickled files.""""""\n    data_dir = args.data_dir\n    if args.output_dir is None:\n        pickle_output_dir = data_dir\n    else:\n        pickle_output_dir = args.output_dir\n\n    tx.utils.maybe_create_dir(pickle_output_dir)\n\n    # Create a GPT-2 tokenizer (BPE encoding)\n    tokenizer = tx.data.GPT2Tokenizer(\n        pretrained_model_name=args.pretrained_model_name)\n\n    config_train: Any = importlib.import_module(args.config_train)\n\n    # Produces pickle files\n    data_utils.prepare_pickle_data(\n        data_dir=data_dir,\n        max_seq_length=args.max_seq_length,\n        tokenizer=tokenizer,\n        output_dir=pickle_output_dir,\n        feature_types=config_train.feature_types)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/sentence_classifier/classifier_main.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example for building a sentence convolutional classifier.\n\nUse `./sst_data_preprocessor.py` to download and clean the SST binary data.\n\nTo run:\n\n$ python clas_main.py --config=config_kim\n""""""\n\nfrom typing import Any, Dict, Tuple\n\nimport argparse\nimport importlib\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport texar.torch as tx\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \'--config\', type=str, default=\'config_kim\',\n    help=\'The config to use.\')\nargs = parser.parse_args()\n\nconfig: Any = importlib.import_module(args.config)\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n\nclass SentenceClassifier(nn.Module):\n\n    def __init__(self, vocab_size: int, max_seq_length: int,\n                 emb_dim: int, hparams: Dict[str, Any]):\n        super().__init__()\n\n        self.embedder = tx.modules.WordEmbedder(\n            vocab_size=vocab_size, hparams=hparams[\'embedder\'])\n        self.classifier = tx.modules.Conv1DClassifier(\n            in_channels=max_seq_length,\n            in_features=emb_dim, hparams=hparams[\'classifier\'])\n\n    def forward(self,  # type: ignore\n                batch: tx.data.Batch) -> Tuple[torch.Tensor, torch.Tensor]:\n        logits, pred = self.classifier(\n            self.embedder(batch[\'sentence_text_ids\']))\n        loss = F.cross_entropy(logits, batch[\'label\'])\n        return pred, loss\n\n\ndef main() -> None:\n    # Data\n    train_data = tx.data.MultiAlignedData(config.train_data, device=device)\n    val_data = tx.data.MultiAlignedData(config.val_data, device=device)\n    test_data = tx.data.MultiAlignedData(config.test_data, device=device)\n    data_iterator = tx.data.TrainTestDataIterator(\n        train=train_data, val=val_data, test=test_data)\n\n    hparams = {\n        \'embedder\': config.emb,\n        \'classifier\': config.clas\n    }\n    model = SentenceClassifier(vocab_size=train_data.vocab(\'sentence\').size,\n                               max_seq_length=config.max_seq_length,\n                               emb_dim=config.emb_dim,\n                               hparams=hparams)\n    model.to(device)\n    train_op = tx.core.get_train_op(params=model.parameters(),\n                                    hparams=config.opt)\n\n    def _run_epoch(mode, epoch):\n\n        step = 0\n        avg_rec = tx.utils.AverageRecorder()\n        for batch in data_iterator:\n            pred, loss = model(batch)\n            if mode == ""train"":\n                loss.backward()\n                train_op()\n            accu = tx.evals.accuracy(batch[\'label\'], pred)\n            step += 1\n            if step == 1 or step % 100 == 0:\n                print(f""epoch: {epoch:2} step: {step:4} accu: {accu:.4f}"")\n\n            batch_size = batch[\'label\'].size(0)\n            avg_rec.add([accu], batch_size)\n\n        return avg_rec.avg(0)\n\n    best_val_accu = -1\n    for epoch in range(config.num_epochs):\n        # Train\n        data_iterator.switch_to_train_data()\n        model.train()\n        train_accu = _run_epoch(""train"", epoch)\n\n        # Val\n        data_iterator.switch_to_val_data()\n        model.eval()\n        val_accu = _run_epoch(""val"", epoch)\n        print(f\'epoch: {epoch:2} train accu: {train_accu:.4f} \'\n              f\'val accu: {val_accu:.4f}\')\n\n        # Test\n        if val_accu > best_val_accu:\n            best_val_accu = val_accu\n            data_iterator.switch_to_test_data()\n            model.eval()\n            test_accu = _run_epoch(""test"", epoch)\n            print(f\'test accu: {test_accu:.4f}\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/sentence_classifier/config_kim.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# type: ignore\n""""""Sentence convolutional classifier config.\n\nThis is (approximately) the config of the paper:\n(Kim) Convolutional Neural Networks for Sentence Classification\n  https://arxiv.org/pdf/1408.5882.pdf\n""""""\n\nimport copy\n\nnum_epochs = 15\nmax_seq_length = 56\nemb_dim = 300\n\ntrain_data = {\n    ""batch_size"": 50,\n    ""datasets"": [\n        {\n            ""files"": ""./data/sst2.train.sentences.txt"",\n            ""vocab_file"": ""./data/sst2.vocab"",\n            # Discards samples with length > 56\n            ""max_seq_length"": max_seq_length,\n            ""length_filter_mode"": ""discard"",\n            # Do not append BOS/EOS tokens to the sentences\n            ""bos_token"": """",\n            ""eos_token"": """",\n            ""data_name"": ""sentence""\n        },\n        {\n            ""files"": ""./data/sst2.train.labels.txt"",\n            ""data_type"": ""int64"",\n            ""data_name"": ""label""\n        }\n    ]\n}\n# The val and test data have the same config with the train data, except\n# for the file names\nval_data = copy.deepcopy(train_data)\nval_data[""datasets""][0][""files""] = ""./data/sst2.dev.sentences.txt""\nval_data[""datasets""][1][""files""] = ""./data/sst2.dev.labels.txt""\ntest_data = copy.deepcopy(train_data)\ntest_data[""datasets""][0][""files""] = ""./data/sst2.test.sentences.txt""\ntest_data[""datasets""][1][""files""] = ""./data/sst2.test.labels.txt""\n\n# Word embedding\nemb = {\n    ""dim"": emb_dim\n}\n\n# Classifier\nclas = {\n    ""num_conv_layers"": 1,\n    ""out_channels"": 100,\n    ""kernel_size"": [3, 4, 5],\n    ""conv_activation"": ""ReLU"",\n    ""pooling"": ""MaxPool1d"",\n    ""num_dense_layers"": 0,\n    ""dropout_conv"": [1],\n    ""dropout_rate"": 0.5,\n    ""num_classes"": 2\n}\n\n# Optimization\n# Just use the default config, e.g., Adam Optimizer\nopt = {}\n'"
examples/sentence_classifier/sst_data_preprocessor.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Preparing the SST2 dataset.\n""""""\n\nfrom typing import Tuple\n\nimport argparse\nimport os\nimport re\n\nimport texar.torch as tx\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \'--data-path\', type=str, default=\'./data\',\n    help=""E.g., ./data/sst2.train.sentences.txt. If not exists, the directory ""\n         ""will be created and SST raw data will be downloaded."")\nargs = parser.parse_args()\n\n\ndef clean_sst_text(text: str) -> str:\n    """"""Cleans tokens in the SST data, which has already been tokenized.\n    """"""\n    text = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", text)\n    text = re.sub(r""\\s{2,}"", "" "", text)\n    return text.strip().lower()\n\n\ndef transform_raw_sst(data_path: str, raw_filename: str, new_filename: str) -> \\\n        Tuple[str, str]:\n    """"""Transforms the raw data format to a new format.\n    """"""\n    fout_x_name = os.path.join(data_path, new_filename + \'.sentences.txt\')\n    fout_x = open(fout_x_name, \'w\', encoding=\'utf-8\')\n    fout_y_name = os.path.join(data_path, new_filename + \'.labels.txt\')\n    fout_y = open(fout_y_name, \'w\', encoding=\'utf-8\')\n\n    fin_name = os.path.join(data_path, raw_filename)\n    with open(fin_name, \'r\', encoding=\'utf-8\') as fin:\n        for line in fin:\n            parts = line.strip().split()\n            label = parts[0]\n            sent = \' \'.join(parts[1:])\n            sent = clean_sst_text(sent)\n            fout_x.write(sent + \'\\n\')\n            fout_y.write(label + \'\\n\')\n\n    return fout_x_name, fout_y_name\n\n\ndef main() -> None:\n    """"""Preprocesses SST2 data.\n    """"""\n    train_path = os.path.join(args.data_path, ""sst.train.sentences.txt"")\n    if not os.path.exists(train_path):\n        url = (\'https://raw.githubusercontent.com/ZhitingHu/\'\n               \'logicnn/master/data/raw/\')\n        files = [\'stsa.binary.phrases.train\', \'stsa.binary.dev\',\n                 \'stsa.binary.test\']\n        for fn in files:\n            tx.data.maybe_download(url + fn, args.data_path, extract=True)\n\n    fn_train, _ = transform_raw_sst(\n        args.data_path, \'stsa.binary.phrases.train\', \'sst2.train\')\n    transform_raw_sst(args.data_path, \'stsa.binary.dev\', \'sst2.dev\')\n    transform_raw_sst(args.data_path, \'stsa.binary.test\', \'sst2.test\')\n\n    vocab = tx.data.make_vocab(fn_train)\n    fn_vocab = os.path.join(args.data_path, \'sst2.vocab\')\n    with open(fn_vocab, \'w\', encoding=\'utf-8\') as f_vocab:\n        for v in vocab:\n            f_vocab.write(v + \'\\n\')\n\n    print(\'Preprocessing done: {}\'.format(args.data_path))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/seq2seq_attn/config_iwslt14.py,0,"b'num_epochs = 15\ndisplay = 50\n\nsource_vocab_file = \'./data/iwslt14/vocab.de\'\ntarget_vocab_file = \'./data/iwslt14/vocab.en\'\n\ntrain = {\n    \'batch_size\': 32,\n    \'allow_smaller_final_batch\': False,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/train.de\',\n        \'vocab_file\': source_vocab_file,\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/train.en\',\n        \'vocab_file\': target_vocab_file,\n    }\n}\n\nval = {\n    \'batch_size\': 32,\n    \'shuffle\': False,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/valid.de\',\n        \'vocab_file\': source_vocab_file,\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/valid.en\',\n        \'vocab_file\': target_vocab_file,\n    }\n}\n\ntest = {\n    \'batch_size\': 32,\n    \'shuffle\': False,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/test.de\',\n        \'vocab_file\': source_vocab_file,\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/test.en\',\n        \'vocab_file\': target_vocab_file,\n    }\n}\n'"
examples/seq2seq_attn/config_model.py,0,"b""# Attentional Seq2seq model.\n# Hyperparameters not specified here will take the default values.\n\nnum_units = 256\nbeam_width = 10\n\nembedder = {\n    'dim': num_units\n}\n\nencoder = {\n    'rnn_cell_fw': {\n        'kwargs': {\n            'num_units': num_units\n        }\n    }\n}\n\ndecoder = {\n    'rnn_cell': {\n        'kwargs': {\n            'num_units': num_units\n        },\n    },\n    'attention': {\n        'kwargs': {\n            'num_units': num_units,\n        },\n        'attention_layer_size': num_units\n    },\n    'max_decoding_length_infer': 60,\n}\n\nopt = {\n    'optimizer': {\n        'type':  'Adam',\n        'kwargs': {\n            'lr': 0.001,\n        },\n    },\n}\n"""
examples/seq2seq_attn/config_model_full.py,0,"b'# The full possible hyperparameters for the attentional seq2seq model.\n# Most of the hyperparameters take the default values and are not necessary to\n# specify explicitly. The config here results in the same model with the\n# `config_model.py`.\n\nnum_units = 256\nbeam_width = 10\n\n# --------------------- Embedder --------------------- #\nembedder = {\n    \'dim\': num_units,\n    ""initializer"": None,\n    ""dropout_rate"": 0.,\n    ""dropout_strategy"": \'element\',\n    ""name"": ""word_embedder"",\n}\n\n# --------------------- Encoder --------------------- #\nencoder = {\n    \'rnn_cell_fw\': {\n        \'type\': \'LSTMCell\',\n        \'kwargs\': {\n            \'num_units\': num_units,\n        },\n        \'num_layers\': 1,\n        \'dropout\': {\n            \'input_keep_prob\': 1.0,\n            \'output_keep_prob\': 1.0,\n            \'state_keep_prob\': 1.0,\n            \'variational_recurrent\': False,\n        },\n        \'residual\': False,\n        \'highway\': False,\n    },\n    \'rnn_cell_bw\': {\n        \'type\': \'LSTMCell\',\n        \'kwargs\': {\n            \'num_units\': num_units,\n        },\n        \'num_layers\': 1,\n        \'dropout\': {\n            \'input_keep_prob\': 1.0,\n            \'output_keep_prob\': 1.0,\n            \'state_keep_prob\': 1.0,\n            \'variational_recurrent\': False,\n        },\n        \'residual\': False,\n        \'highway\': False,\n    },\n    \'rnn_cell_share_config\': True,\n    \'output_layer_fw\': {\n        ""num_layers"": 0,\n        ""layer_size"": 128,\n        ""activation"": ""Identity"",\n        ""final_layer_activation"": None,\n        ""other_dense_kwargs"": None,\n        ""dropout_layer_ids"": [],\n        ""dropout_rate"": 0.5,\n        ""variational_dropout"": False,\n    },\n    \'output_layer_bw\': {\n        ""num_layers"": 0,\n        ""layer_size"": 128,\n        ""activation"": ""Identity"",\n        ""final_layer_activation"": None,\n        ""other_dense_kwargs"": None,\n        ""dropout_layer_ids"": [],\n        ""dropout_rate"": 0.5,\n        ""variational_dropout"": False,\n    },\n    \'output_layer_share_config\': True,\n    \'name\': \'bidirectional_rnn_encoder\'\n}\n\n# --------------------- Decoder --------------------- #\ndecoder = {\n    \'rnn_cell\': {\n        \'type\': \'LSTMCell\',\n        \'kwargs\': {\n            \'num_units\': 256,\n        },\n        \'num_layers\': 1,\n        \'dropout\': {\n            \'input_keep_prob\': 1.0,\n            \'output_keep_prob\': 1.0,\n            \'state_keep_prob\': 1.0,\n            \'variational_recurrent\': False,\n        },\n        \'residual\': False,\n        \'highway\': False,\n    },\n    \'attention\': {\n        ""type"": ""LuongAttention"",\n        ""kwargs"": {\n            ""num_units"": 256,\n        },\n        ""attention_layer_size"": 256,\n        ""alignment_history"": False,\n        ""output_attention"": True,\n    },\n    \'helper_train\': {\n        \'type\': \'TrainingHelper\',\n        \'kwargs\': {}\n    },\n    \'helper_infer\': {\n        \'type\': \'SampleEmbeddingHelper\',\n        \'kwargs\': {}\n    },\n    \'max_decoding_length_train\': None,\n    \'max_decoding_length_infer\': 60,\n    \'output_layer_bias\': True,\n    \'name\': \'attention_rnn_decoder\'\n}\n# --------------------- Optimization --------------------- #\nopt = {\n    \'optimizer\': {\n        \'type\':  \'Adam\',\n        \'kwargs\': {\n            \'lr\': 0.001,\n        },\n    },\n    \'learning_rate_decay\': {\n            ""type"": """",\n            ""kwargs"": {}\n        },\n    \'gradient_clip\': {\n            ""type"": """",\n            ""kwargs"": {}\n        },\n    \'gradient_noise_scale\': None,\n    \'name\': None\n}\n'"
examples/seq2seq_attn/config_toy_copy.py,0,"b'num_epochs = 4\ndisplay = 50\n\nsource_vocab_file = \'./data/toy_copy/train/vocab.sources.txt\'\ntarget_vocab_file = \'./data/toy_copy/train/vocab.targets.txt\'\n\ntrain = {\n    \'batch_size\': 32,\n    \'source_dataset\': {\n        ""files"": \'./data/toy_copy/train/sources.txt\',\n        \'vocab_file\': source_vocab_file\n    },\n    \'target_dataset\': {\n        \'files\': \'./data/toy_copy/train/targets.txt\',\n        \'vocab_file\': target_vocab_file\n    }\n}\n\nval = {\n    \'batch_size\': 32,\n    \'source_dataset\': {\n        ""files"": \'./data/toy_copy/dev/sources.txt\',\n        \'vocab_file\': source_vocab_file\n    },\n    \'target_dataset\': {\n        ""files"": \'./data/toy_copy/dev/targets.txt\',\n        \'vocab_file\': target_vocab_file\n    }\n}\n\ntest = {\n    \'batch_size\': 32,\n    \'source_dataset\': {\n        ""files"": \'./data/toy_copy/test/sources.txt\',\n        \'vocab_file\': source_vocab_file\n    },\n    \'target_dataset\': {\n        ""files"": \'./data/toy_copy/test/targets.txt\',\n        \'vocab_file\': target_vocab_file\n    }\n}\n'"
examples/seq2seq_attn/prepare_data.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Downloads data.\n""""""\n\nimport argparse\n\nimport texar.torch as tx\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \'--data\', type=str, default=""iwslt14"",\n    help=""Data to download [iwslt14|toy_copy]"")\nargs = parser.parse_args()\n\n\ndef main() -> None:\n    """"""Downloads data.\n    """"""\n    if args.data == \'iwslt14\':\n        tx.data.maybe_download(\n            urls=\'https://drive.google.com/file/d/\'\n                 \'1y4mUWXRS2KstgHopCS9koZ42ENOh6Yb9/view?usp=sharing\',\n            path=\'./\',\n            filenames=\'iwslt14.zip\',\n            extract=True)\n    elif args.data == \'toy_copy\':\n        tx.data.maybe_download(\n            urls=\'https://drive.google.com/file/d/\'\n                 \'1fENE2rakm8vJ8d3voWBgW4hGlS6-KORW/view?usp=sharing\',\n            path=\'./\',\n            filenames=\'toy_copy.zip\',\n            extract=True)\n    else:\n        raise ValueError(f\'Unknown dataset: {args.data}\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/seq2seq_attn/seq2seq_attn.py,5,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Attentional Seq2seq.\n""""""\n\nimport argparse\nimport importlib\nfrom typing import Any\n\nimport torch\nimport torch.nn as nn\n\nimport texar.torch as tx\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \'--config-model\', type=str, default=""config_model"",\n    help=""The model config."")\nparser.add_argument(\n    \'--config-data\', type=str, default=""config_iwslt14"",\n    help=""The dataset config."")\nargs = parser.parse_args()\n\nconfig_model: Any = importlib.import_module(args.config_model)\nconfig_data: Any = importlib.import_module(args.config_data)\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n\nclass Seq2SeqAttn(nn.Module):\n\n    def __init__(self, train_data):\n        super().__init__()\n\n        self.source_vocab_size = train_data.source_vocab.size\n        self.target_vocab_size = train_data.target_vocab.size\n\n        self.bos_token_id = train_data.target_vocab.bos_token_id\n        self.eos_token_id = train_data.target_vocab.eos_token_id\n\n        self.source_embedder = tx.modules.WordEmbedder(\n            vocab_size=self.source_vocab_size,\n            hparams=config_model.embedder)\n\n        self.target_embedder = tx.modules.WordEmbedder(\n            vocab_size=self.target_vocab_size,\n            hparams=config_model.embedder)\n\n        self.encoder = tx.modules.BidirectionalRNNEncoder(\n            input_size=self.source_embedder.dim,\n            hparams=config_model.encoder)\n\n        self.decoder = tx.modules.AttentionRNNDecoder(\n            token_embedder=self.target_embedder,\n            encoder_output_size=(self.encoder.cell_fw.hidden_size +\n                                 self.encoder.cell_bw.hidden_size),\n            input_size=self.target_embedder.dim,\n            vocab_size=self.target_vocab_size,\n            hparams=config_model.decoder)\n\n    def forward(self, batch, mode):\n        enc_outputs, _ = self.encoder(\n            inputs=self.source_embedder(batch[\'source_text_ids\']),\n            sequence_length=batch[\'source_length\'])\n\n        memory = torch.cat(enc_outputs, dim=2)\n\n        if mode == ""train"":\n            helper_train = self.decoder.create_helper(\n                decoding_strategy=""train_greedy"")\n\n            training_outputs, _, _ = self.decoder(\n                memory=memory,\n                memory_sequence_length=batch[\'source_length\'],\n                helper=helper_train,\n                inputs=batch[\'target_text_ids\'][:, :-1],\n                sequence_length=batch[\'target_length\'] - 1)\n\n            mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n                labels=batch[\'target_text_ids\'][:, 1:],\n                logits=training_outputs.logits,\n                sequence_length=batch[\'target_length\'] - 1)\n\n            return mle_loss\n        else:\n            start_tokens = memory.new_full(\n                batch[\'target_length\'].size(), self.bos_token_id,\n                dtype=torch.int64)\n\n            infer_outputs = self.decoder(\n                start_tokens=start_tokens,\n                end_token=self.eos_token_id,\n                memory=memory,\n                memory_sequence_length=batch[\'source_length\'],\n                beam_width=config_model.beam_width)\n\n            return infer_outputs\n\n\ndef main() -> None:\n    train_data = tx.data.PairedTextData(\n        hparams=config_data.train, device=device)\n    val_data = tx.data.PairedTextData(\n        hparams=config_data.val, device=device)\n    test_data = tx.data.PairedTextData(\n        hparams=config_data.test, device=device)\n    data_iterator = tx.data.TrainTestDataIterator(\n        train=train_data, val=val_data, test=test_data)\n\n    model = Seq2SeqAttn(train_data)\n    model.to(device)\n    train_op = tx.core.get_train_op(\n        params=model.parameters(), hparams=config_model.opt)\n\n    def _train_epoch():\n        data_iterator.switch_to_train_data()\n        model.train()\n\n        step = 0\n        for batch in data_iterator:\n            loss = model(batch, mode=""train"")\n            loss.backward()\n            train_op()\n            if step % config_data.display == 0:\n                print(""step={}, loss={:.4f}"".format(step, loss))\n            step += 1\n\n    @torch.no_grad()\n    def _eval_epoch(mode):\n        if mode == \'val\':\n            data_iterator.switch_to_val_data()\n        else:\n            data_iterator.switch_to_test_data()\n        model.eval()\n\n        refs, hypos = [], []\n        for batch in data_iterator:\n            infer_outputs = model(batch, mode=""val"")\n            output_ids = infer_outputs[""sample_id""][:, :, 0].cpu()\n            target_texts_ori = [text[1:] for text in batch[\'target_text\']]\n            target_texts = tx.utils.strip_special_tokens(\n                target_texts_ori, is_token_list=True)\n            output_texts = tx.data.vocabulary.map_ids_to_strs(\n                ids=output_ids, vocab=val_data.target_vocab)\n\n            for hypo, ref in zip(output_texts, target_texts):\n                hypos.append(hypo)\n                refs.append([ref])\n\n        return tx.evals.corpus_bleu_moses(\n            list_of_references=refs, hypotheses=hypos)\n\n    best_val_bleu = -1.\n    for i in range(config_data.num_epochs):\n        _train_epoch()\n\n        val_bleu = _eval_epoch(\'val\')\n        best_val_bleu = max(best_val_bleu, val_bleu)\n        print(\'val epoch={}, BLEU={:.4f}; best-ever={:.4f}\'.format(\n            i, val_bleu, best_val_bleu))\n\n        test_bleu = _eval_epoch(\'test\')\n        print(\'test epoch={}, BLEU={:.4f}\'.format(i, test_bleu))\n\n        print(\'=\' * 50)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/transformer/bleu_main.py,0,"b'# Copyright 2018 The Tensor2Tensor Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Modifications copyright (C) 2018 Texar\n# ==============================================================================\n""""""BLEU metric utililities used for MT eval.\n\nUsage: python bleu_main.py --translation=my-wmt13.de --reference=wmt13_deen.de\n""""""\n# This also:\n# Put compounds in ATAT format (comparable to papers like GNMT, ConvS2S).\n# See https://nlp.stanford.edu/projects/nmt/ :\n# \'Also, for historical reasons, we split compound words, e.g.,\n#    ""rich-text format"" --> rich ##AT##-##AT## text format.""\'\n# BLEU score will be similar to the one obtained using: mteval-v14.pl\n# Note:compound splitting is not implemented in this module\n\n\nfrom argparse import ArgumentParser\n\nimport texar.torch as tx\n\n\ndef main() -> None:\n    parser = ArgumentParser(\n        description=""Compute BLEU score. \\\n        Usage: t2t-bleu --translation=my-wmt13.de --reference=wmt13_deen.de""\n    )\n\n    parser.add_argument(""--translation"", type=str)\n    parser.add_argument(""--reference"", type=str)\n    args = parser.parse_args()\n\n    bleu = tx.evals.file_bleu(\n        args.reference, args.translation, case_sensitive=False)\n    print(f""BLEU_uncased = {bleu:6.2f}"")\n    bleu = tx.evals.file_bleu(\n        args.reference, args.translation, case_sensitive=True)\n    print(f""BLEU_cased = {bleu:6.2f}"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/transformer/config_iwslt15.py,0,"b'max_batch_tokens = 2048\ntest_batch_size = 32\n\nmax_train_epoch = 20\ndisplay_steps = 500\neval_steps = 2000\n\nmax_decoding_length = 256\n\nfilename_prefix = ""processed.""\ninput_dir = \'temp/run_en_vi_spm/data\'\nvocab_file = input_dir + \'/processed.vocab.text\'\nencoding = ""spm""\n'"
examples/transformer/config_model.py,0,"b'""""""Configurations of Transformer model\n""""""\nimport copy\n\nimport texar.torch as tx\n\nrandom_seed = 1234\n\nbeam_width = 5\n\nlength_penalty = 0.6\nhidden_dim = 512\n\nemb = {\n    ""name"": ""lookup_table"",\n    ""dim"": hidden_dim,\n    ""initializer"": {\n        ""type"": ""normal_"",\n        ""kwargs"": {""mean"": 0.0, ""std"": hidden_dim ** -0.5},\n    },\n}\n\nposition_embedder_hparams = {""dim"": hidden_dim}\n\nencoder = {\n    ""dim"": hidden_dim,\n    ""num_blocks"": 6,\n    ""multihead_attention"": {\n        ""num_heads"": 8,\n        ""output_dim"": hidden_dim\n        # See documentation for more optional hyperparameters\n    },\n    ""initializer"": {\n        ""type"": ""variance_scaling_initializer"",\n        ""kwargs"": {""factor"": 1.0, ""mode"": ""FAN_AVG"", ""uniform"": True},\n    },\n    ""poswise_feedforward"": tx.modules.default_transformer_poswise_net_hparams(\n        input_dim=hidden_dim,\n        output_dim=hidden_dim\n    ),\n}\n\ndecoder = copy.deepcopy(encoder)\n\nloss_label_confidence = 0.9\n\nopt = {\n    ""optimizer"": {\n        ""type"": ""Adam"",\n        ""kwargs"": {""beta1"": 0.9, ""beta2"": 0.997, ""epsilon"": 1e-9},\n    }\n}\n\nlr_config = {\n    ""learning_rate_schedule"": ""constant.linear_warmup.rsqrt_decay.rsqrt_depth"",\n    ""lr_constant"": 2 * (hidden_dim ** -0.5),\n    ""static_lr"": 1e-3,\n    ""warmup_steps"": 16000,\n}\n'"
examples/transformer/config_wmt14.py,0,"b'max_batch_tokens = 3072\ntest_batch_size = 32\n\nmax_train_epoch = 10\ndisplay_steps = 1000\neval_steps = 10000\n\nmax_decoding_length = 256\n\nfilename_prefix = ""processed.""\ninput_dir = \'temp/run_en_de_bpe/data\'\nvocab_file = input_dir + \'/processed.vocab.text\'\nencoding = ""bpe""\n'"
examples/transformer/model.py,13,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\nimport texar.torch as tx\n\n\nclass Transformer(nn.Module):\n    r""""""A standalone sequence-to-sequence Transformer model, from ""Attention\n    Is All You Need"". The Transformer model consists of the word embedding\n    layer, position embedding layer, an encoder and a decoder. Both encoder\n    and decoder are stacks of self-attention layers followed by feed-forward\n    layers. See ""Attention Is All You Need"" (https://arxiv.org/abs/1706.03762)\n    for the full description of the model.\n    """"""\n\n    def __init__(self, model_config, data_config, vocab: tx.data.Vocab):\n        super().__init__()\n\n        self.config_model = model_config\n        self.config_data = data_config\n        self.vocab = vocab\n        self.vocab_size = vocab.size\n\n        self.word_embedder = tx.modules.WordEmbedder(\n            vocab_size=self.vocab_size,\n            hparams=self.config_model.emb)\n        self.pos_embedder = tx.modules.SinusoidsPositionEmbedder(\n            position_size=self.config_data.max_decoding_length,\n            hparams=self.config_model.position_embedder_hparams)\n\n        self.encoder = tx.modules.TransformerEncoder(\n            hparams=self.config_model.encoder)\n        self.decoder = tx.modules.TransformerDecoder(\n            token_pos_embedder=self._embedding_fn,\n            vocab_size=self.vocab_size,\n            output_layer=self.word_embedder.embedding,\n            hparams=self.config_model.decoder)\n\n        self.smoothed_loss_func = LabelSmoothingLoss(\n            label_confidence=self.config_model.loss_label_confidence,\n            tgt_vocab_size=self.vocab_size,\n            ignore_index=0)\n\n    def _embedding_fn(self, tokens: torch.LongTensor,\n                      positions: torch.LongTensor) -> torch.Tensor:\n        word_embed = self.word_embedder(tokens)\n        scale = self.config_model.hidden_dim ** 0.5\n        pos_embed = self.pos_embedder(positions)\n        return word_embed * scale + pos_embed\n\n    def forward(self,  # type: ignore\n                encoder_input: torch.Tensor,\n                decoder_input: Optional[torch.LongTensor] = None,\n                labels: Optional[torch.LongTensor] = None,\n                beam_width: Optional[int] = None):\n        r""""""Compute the maximum likelihood loss or perform decoding, depending\n        on arguments.\n\n        Args:\n            encoder_input: the source sentence embedding, with the shape of\n                `[batch_size, source_seq_length, input_dim]`.\n            decoder_input: the target sentence embedding, with the shape of\n                `[batch_size, target_seq_length, input_dim]`.\n            labels: the target sentence labels, with the shape of\n                `[batch_size, target_seq_length]`.\n            beam_width: Used in beam search.\n\n        :returns:\n            - If both :attr:`decoder_input` and :attr:`labels` are both\n              provided, the function enters training logic and returns the\n              maximum likelihood loss.\n            - Otherwise the function enters inference logic and returns the\n              decoded sequence.\n            - If `beam_width` > 1, beam search decoding is performed. Please\n              refer to :meth:`texar.modules.TransformerDecoder.forward` for\n              details on return types.\n        """"""\n\n        batch_size = encoder_input.size(0)\n        # Text sequence length excluding padding\n        encoder_input_length = (encoder_input != 0).int().sum(dim=1)\n        positions = torch.arange(\n            encoder_input_length.max(), dtype=torch.long,\n            device=encoder_input.device).unsqueeze(0).expand(batch_size, -1)\n\n        # Source word embedding\n        src_input_embedding = self._embedding_fn(encoder_input, positions)\n\n        encoder_output = self.encoder(\n            inputs=src_input_embedding, sequence_length=encoder_input_length)\n\n        if decoder_input is not None and labels is not None:\n            # enter the training logic\n\n            # For training\n            outputs = self.decoder(\n                memory=encoder_output,\n                memory_sequence_length=encoder_input_length,\n                inputs=decoder_input,\n                decoding_strategy=""train_greedy"",\n            )\n            label_lengths = (labels != 0).long().sum(dim=1)\n            is_target = (labels != 0).float()\n            mle_loss = self.smoothed_loss_func(\n                outputs.logits, labels, label_lengths)\n            mle_loss = (mle_loss * is_target).sum() / is_target.sum()\n            return mle_loss\n\n        else:\n            start_tokens = encoder_input.new_full(\n                (batch_size,), self.vocab.bos_token_id)\n\n            predictions = self.decoder(\n                memory=encoder_output,\n                memory_sequence_length=encoder_input_length,\n                beam_width=beam_width,\n                length_penalty=self.config_model.length_penalty,\n                start_tokens=start_tokens,\n                end_token=self.vocab.eos_token_id,\n                max_decoding_length=self.config_data.max_decoding_length,\n                decoding_strategy=""infer_greedy"",\n            )\n            # Uses the best sample by beam search\n            return predictions\n\n\nclass LabelSmoothingLoss(nn.Module):\n    r""""""With label smoothing,\n    KL-divergence between q_{smoothed ground truth prob.}(w)\n    and p_{prob. computed by model}(w) is minimized.\n\n    Args:\n        label_confidence: the confidence weight on the ground truth label.\n        tgt_vocab_size: the size of the final classification.\n        ignore_index: The index in the vocabulary to ignore weight.\n    """"""\n    one_hot: torch.Tensor\n\n    def __init__(self, label_confidence, tgt_vocab_size, ignore_index=0):\n        super().__init__()\n        self.ignore_index = ignore_index\n        self.tgt_vocab_size = tgt_vocab_size\n\n        label_smoothing = 1 - label_confidence\n        assert 0.0 < label_smoothing <= 1.0\n        smoothing_value = label_smoothing / (tgt_vocab_size - 2)\n        one_hot = torch.full((tgt_vocab_size,), smoothing_value)\n        one_hot[self.ignore_index] = 0\n        self.register_buffer(""one_hot"", one_hot.unsqueeze(0))\n        self.confidence = label_confidence\n\n    def forward(self,  # type: ignore\n                output: torch.Tensor,\n                target: torch.Tensor,\n                label_lengths: torch.LongTensor) -> torch.Tensor:\n        r""""""Compute the label smoothing loss.\n\n        Args:\n            output (FloatTensor): batch_size x seq_length * n_classes\n            target (LongTensor): batch_size * seq_length, specify the label\n                target\n            label_lengths(torch.LongTensor): specify the length of the labels\n        """"""\n        orig_shapes = (output.size(), target.size())\n        output = output.view(-1, self.tgt_vocab_size)\n        target = target.view(-1)\n        model_prob = self.one_hot.repeat(target.size(0), 1)\n        model_prob = model_prob.to(device=target.device)\n        model_prob.scatter_(1, target.unsqueeze(1), self.confidence)\n        model_prob.masked_fill_((target == self.ignore_index).unsqueeze(1), 0)\n\n        output = output.view(orig_shapes[0])\n        model_prob = model_prob.view(orig_shapes[0])\n\n        return tx.losses.sequence_softmax_cross_entropy(\n            labels=model_prob,\n            logits=output,\n            sequence_length=label_lengths,\n            average_across_batch=False,\n            sum_over_timesteps=False,\n        )\n'"
examples/transformer/transformer_main.py,5,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Transformer model.\n""""""\n\nimport argparse\nimport functools\nimport importlib\nimport os\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\nimport torch\nfrom torch import nn\nimport texar.torch as tx\nfrom texar.torch.run import *\n\nfrom model import Transformer\nimport utils.data_utils as data_utils\nimport utils.utils as utils\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    ""--config-model"", type=str, default=""config_model"",\n    help=""The model config."")\nparser.add_argument(\n    ""--config-data"", type=str, default=""config_iwslt15"",\n    help=""The dataset config."")\nparser.add_argument(\n    ""--run-mode"", type=str, default=""train_and_evaluate"",\n    help=""Either train_and_evaluate or evaluate or test."")\nparser.add_argument(\n    ""--output-dir"", type=str, default=""./outputs/"",\n    help=""Path to save the trained model and logs."")\nparser.add_argument(\n    ""--load-checkpoint"", action=""store_true"", default=False,\n    help=""If specified, will load the pre-trained checkpoint from output_dir."")\n\nargs = parser.parse_args()\n\nconfig_model: Any = importlib.import_module(args.config_model)\nconfig_data: Any = importlib.import_module(args.config_data)\n\nmake_deterministic(config_model.random_seed)\n\n\nclass ModelWrapper(nn.Module):\n    def __init__(self, model: Transformer, beam_width: int):\n        super().__init__()\n        self.model = model\n        self.beam_width = beam_width\n\n    def forward(self,  # type: ignore\n                batch: tx.data.Batch) -> Dict[str, torch.Tensor]:\n        loss = self.model(encoder_input=batch.source,\n                          decoder_input=batch.target_input,\n                          labels=batch.target_output)\n        return {""loss"": loss}\n\n    def predict(self, batch: tx.data.Batch) -> Dict[str, torch.Tensor]:\n        predictions = self.model(encoder_input=batch.source,\n                                 beam_width=self.beam_width)\n        if self.beam_width == 1:\n            decoded_ids = predictions[0].sample_id\n        else:\n            decoded_ids = predictions[""sample_id""][:, :, 0]\n        return {""preds"": decoded_ids}\n\n\nclass DecodeMixin:\n    vocab: tx.data.Vocab\n    perform_decode: bool\n    encoding: Optional[str]\n\n    valid_encodings = [""bpe"", ""spm""]\n    spm_bos_token = ""\xe2\x96\x81""\n    bpe_cont_str = ""@@""\n\n    @staticmethod\n    def spm_decode(tokens: List[str]) -> List[str]:\n        words = []\n        pieces: List[str] = []\n        for t in tokens:\n            if t[0] == DecodeMixin.spm_bos_token:\n                if len(pieces) > 0:\n                    words.append(\'\'.join(pieces))\n                pieces = [t[1:]]\n            else:\n                pieces.append(t)\n        if len(pieces) > 0:\n            words.append(\'\'.join(pieces))\n        return words\n\n    @staticmethod\n    def bpe_decode(tokens: List[str]) -> List[str]:\n        words = []\n        pieces: List[str] = []\n        for t in tokens:\n            if t.endswith(DecodeMixin.bpe_cont_str):\n                pieces.append(t[:-2])\n            else:\n                words.append(\'\'.join(pieces + [t]))\n                pieces = []\n        if len(pieces) > 0:\n            words.append(\'\'.join(pieces))\n        return words\n\n    def _to_str(self, tokens: List[int]) -> str:\n        pos = next((idx for idx, x in enumerate(tokens)\n                    if x == self.vocab.eos_token_id), -1)\n        if pos != -1:\n            tokens = tokens[:pos]\n        vocab_map = self.vocab.id_to_token_map_py\n\n        words = [vocab_map[t] for t in tokens]\n        if self.encoding is not None and self.perform_decode:\n            if self.encoding == ""bpe"":\n                words = self.bpe_decode(words)\n            elif self.encoding == ""spm"":\n                words = self.spm_decode(words)\n        sentence = \' \'.join(words)\n        return sentence\n\n\nclass FileBLEU(metric.SimpleMetric[List[int], float], DecodeMixin):\n    def __init__(self, vocab: tx.data.Vocab,\n                 file_path: Optional[Union[str, Path]] = None,\n                 encoding: Optional[str] = None):\n        super().__init__(pred_name=""preds"", label_name=""target_output"")\n        self.vocab = vocab\n        self.file_path = file_path\n        self.perform_decode = True\n        if encoding is not None and encoding not in self.valid_encodings:\n            raise ValueError(f""Invalid encoding scheme {self.encoding}"")\n        self.encoding = encoding\n\n    @property\n    def metric_name(self) -> str:\n        return ""BLEU""\n\n    def _value(self) -> float:\n        if len(self.predicted) == 0:\n            return 0.0\n        path = self.file_path or tempfile.mktemp()\n        hypotheses, references = [], []\n        for hyp, ref in zip(self.predicted, self.labels):\n            hypotheses.append(self._to_str(hyp))\n            references.append(self._to_str(ref))\n        hyp_file, ref_file = tx.utils.write_paired_text(\n            hypotheses, references,\n            path, mode=""s"", src_fname_suffix=""hyp"", tgt_fname_suffix=""ref"")\n        bleu = tx.evals.file_bleu(ref_file, hyp_file, case_sensitive=True)\n        return bleu\n\n\nclass BLEUWrapper(metric.BLEU, DecodeMixin):\n    def __init__(self, vocab: tx.data.Vocab, decode: bool = False,\n                 encoding: Optional[str] = None):\n        super().__init__(pred_name=""preds"", label_name=""target_output"")\n        self.vocab = vocab\n        self.perform_decode = decode\n        if encoding is not None and encoding not in self.valid_encodings:\n            raise ValueError(f""Invalid encoding scheme {self.encoding}"")\n        self.encoding = encoding\n\n    @property\n    def metric_name(self) -> str:\n        return ""BLEU""\n\n    def add(self, predicted, labels) -> None:\n        predicted = [self._to_str(s) for s in predicted]\n        labels = [self._to_str(s) for s in labels]\n        super().add(predicted, labels)\n\n\ndef main() -> None:\n    """"""Entry point.\n    """"""\n    # Load data\n    vocab = tx.data.Vocab(config_data.vocab_file)\n    data_hparams = {\n        # ""batch_size"" is ignored for train since we use dynamic batching.\n        ""batch_size"": config_data.test_batch_size,\n        ""pad_id"": vocab.pad_token_id,\n        ""bos_id"": vocab.bos_token_id,\n        ""eos_id"": vocab.eos_token_id,\n    }\n    datasets = {\n        split: data_utils.Seq2SeqData(\n            os.path.join(config_data.input_dir,\n                         f""{config_data.filename_prefix}{split}.npy""),\n            # Only shuffle during training.\n            hparams={**data_hparams, ""shuffle"": split == ""train""},\n        ) for split in [""train"", ""valid"", ""test""]\n    }\n    print(f""Training data size: {len(datasets[\'train\'])}"")\n    batching_strategy = data_utils.CustomBatchingStrategy(\n        config_data.max_batch_tokens)\n\n    # Create model and optimizer\n    model = Transformer(config_model, config_data, vocab)\n    model = ModelWrapper(model, config_model.beam_width)\n\n    lr_config = config_model.lr_config\n    if lr_config[""learning_rate_schedule""] == ""static"":\n        init_lr = lr_config[""static_lr""]\n        scheduler_lambda = lambda x: 1.0\n    else:\n        init_lr = lr_config[""lr_constant""]\n        scheduler_lambda = functools.partial(\n            utils.get_lr_multiplier, warmup_steps=lr_config[""warmup_steps""])\n    optim = torch.optim.Adam(\n        model.parameters(), lr=init_lr, betas=(0.9, 0.997), eps=1e-9)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optim, scheduler_lambda)\n\n    output_dir = Path(args.output_dir)\n    encoding = getattr(config_data, \'encoding\', None)\n    executor = Executor(\n        model=model,\n        train_data=datasets[""train""],\n        valid_data=datasets[""valid""],\n        test_data=datasets[""test""],\n        batching_strategy=batching_strategy,\n        optimizer=optim,\n        lr_scheduler=scheduler,\n        log_destination=[sys.stdout, output_dir / ""log.txt""],\n        log_every=cond.iteration(config_data.display_steps),\n        validate_every=[cond.iteration(config_data.eval_steps), cond.epoch(1)],\n        stop_training_on=cond.epoch(config_data.max_train_epoch),\n        train_metrics=[\n            (""loss"", metric.RunningAverage(1)),  # only show current loss\n            (""lr"", metric.LR(optim))],\n        log_format=""{time} : Epoch {epoch:2d} @ {iteration:6d}it ""\n                   ""({progress}%, {speed}), lr = {lr:.3e}, loss = {loss:.3f}"",\n        valid_metrics=BLEUWrapper(vocab, encoding=encoding),\n        test_metrics=[\n            FileBLEU(vocab, output_dir / ""test.output"", encoding=encoding),\n            (""unofficial_bleu"", BLEUWrapper(\n                vocab, decode=True, encoding=encoding))],\n        valid_log_format=""{time} : Epoch {epoch}, ""\n                         ""{split} BLEU = {BLEU:.3f}"",\n        test_progress_log_format=(\n            ""{time} : Evaluating on test ({progress}%, {speed}), ""\n            ""unofficial BLEU = {unofficial_bleu:.2f}""),\n        validate_mode=\'predict\',\n        checkpoint_dir=args.output_dir,\n        save_every=cond.validation(better=True),\n        max_to_keep=1,\n        show_live_progress=True,\n    )\n    if args.run_mode == ""train_and_evaluate"":\n        executor.write_log(""Begin running with train_and_evaluate mode"")\n        if args.load_checkpoint:\n            load_path = executor.load(allow_failure=True)\n            if load_path is not None:\n                executor.test({""valid"": datasets[""valid""]})\n\n        executor.train()\n\n    elif args.run_mode in [""evaluate"", ""test""]:\n        executor.write_log(f""Begin running with {args.run_mode} mode"")\n        executor.load(load_training_state=False)\n        split = ""test"" if args.run_mode == ""test"" else ""valid""\n        executor.test({split: datasets[split]})\n\n    else:\n        raise ValueError(f""Unknown mode: {args.run_mode}"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/vae_text/config_lstm_ptb.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""VAE config.\n""""""\n\ndataset = ""ptb""\nnum_epochs = 100\nhidden_size = 256\ndec_dropout_in = 0.5\ndec_dropout_out = 0.5\nenc_dropout_in = 0.\nenc_dropout_out = 0.\nword_keep_prob = 0.5\nbatch_size = 32\nembed_dim = 256\n\nlatent_dims = 32\n\nlr_decay_hparams = {\n    ""init_lr"": 0.001,\n    ""threshold"": 2,\n    ""decay_factor"": 0.5,\n    ""max_decay"": 5\n}\n\n\ndecoder_type = \'lstm\'\n\nenc_cell_hparams = {\n    ""type"": ""LSTMCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": 1. - enc_dropout_out},\n    ""num_layers"": 1\n}\n\ndec_cell_hparams = {\n    ""type"": ""LSTMCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""bias"": 0.,\n    },\n    ""dropout"": {""output_keep_prob"": 1. - dec_dropout_out},\n    ""num_layers"": 1,\n}\n\nenc_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": enc_dropout_in,\n    \'initializer\': {\n        \'type\': \'normal_\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'std\': embed_dim**-0.5,\n        },\n    }\n}\n\ndec_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": dec_dropout_in,\n    \'initializer\': {\n        \'type\': \'normal_\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'std\': embed_dim**-0.5,\n        },\n    }\n}\n\n# KL annealing\nkl_anneal_hparams = {\n    ""warm_up"": 10,\n    ""start"": 0.1\n}\n\ntrain_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./simple-examples/data/ptb.train.txt\',\n        ""vocab_file"": \'./simple-examples/data/vocab.txt\'\n    }\n}\n\nval_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./simple-examples/data/ptb.valid.txt\',\n        ""vocab_file"": \'./simple-examples/data/vocab.txt\'\n    }\n}\n\ntest_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""dataset"": {\n        ""files"": \'./simple-examples/data/ptb.test.txt\',\n        ""vocab_file"": \'./simple-examples/data/vocab.txt\'\n    }\n}\n\nopt_hparams = {\n    \'optimizer\': {\n        \'type\': \'Adam\',\n        \'kwargs\': {\n            \'lr\': 0.001\n        }\n    },\n    \'gradient_clip\': {\n        ""type"": ""clip_grad_norm_"",\n        ""kwargs"": {\n            ""max_norm"": 5,\n            ""norm_type"": 2\n        }\n    }\n}\n'"
examples/vae_text/config_lstm_yahoo.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""VAE config.\n""""""\n\ndataset = ""yahoo""\nnum_epochs = 100\nhidden_size = 550\ndec_dropout_in = 0.5\ndec_dropout_out = 0.5\nenc_dropout_in = 0.\nenc_dropout_out = 0.\nbatch_size = 32\nembed_dim = 512\n\nlatent_dims = 32\n\nlr_decay_hparams = {\n    ""init_lr"": 0.001,\n    ""threshold"": 2,\n    ""decay_factor"": 0.5,\n    ""max_decay"": 5\n}\n\n\nrelu_dropout = 0.2\nembedding_dropout = 0.2\nattention_dropout = 0.2\nresidual_dropout = 0.2\nnum_blocks = 3\n\ndecoder_type = \'lstm\'\n\nenc_cell_hparams = {\n    ""type"": ""LSTMCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": 1. - enc_dropout_out},\n    ""num_layers"": 1\n}\n\ndec_cell_hparams = {\n    ""type"": ""LSTMCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": 1. - dec_dropout_out},\n    ""num_layers"": 1\n}\n\nenc_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": enc_dropout_in,\n    \'initializer\': {\n        \'type\': \'normal_\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'std\': embed_dim**-0.5,\n        },\n    }\n}\n\ndec_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": dec_dropout_in,\n    \'initializer\': {\n        \'type\': \'normal_\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'std\': embed_dim**-0.5,\n        },\n    }\n}\n\n\n# KL annealing\n# kl_weight = 1.0 / (1 + np.exp(-k*(step-x0)))\nkl_anneal_hparams = {\n    ""warm_up"": 10,\n    ""start"": 0.1\n}\n\ntrain_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.train.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\nval_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.valid.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\ntest_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.test.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\nopt_hparams = {\n    ""optimizer"": {\n        ""type"": ""Adam"",\n        ""kwargs"": {\n            ""lr"": 0.001\n        }\n    },\n    \'gradient_clip\': {\n        ""type"": ""clip_grad_norm_"",\n        ""kwargs"": {\n            ""max_norm"": 5,\n            ""norm_type"": 2\n        }\n    }\n}\n'"
examples/vae_text/config_trans_ptb.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Config file of VAE with Transformer decoder, on PTB data.\n""""""\n\ndataset = \'ptb\'\nnum_epochs = 100\nhidden_size = 256\ndec_dropout_in = 0.\nenc_dropout_in = 0.\nenc_dropout_out = 0.\nbatch_size = 32\nembed_dim = 256\n\nlatent_dims = 32\n\nlr_decay_hparams = {\n    \'init_lr\': 0.001,\n    \'threshold\': 2,\n    \'decay_factor\': 0.5,\n    \'max_decay\': 5\n}\n\n\nrelu_dropout = 0.2\nembedding_dropout = 0.2\nattention_dropout = 0.2\nresidual_dropout = 0.2\nnum_blocks = 3\n\ndecoder_type = \'transformer\'\n\nenc_cell_hparams = {\n    \'type\': \'LSTMCell\',\n    \'kwargs\': {\n        \'num_units\': hidden_size,\n        \'bias\': 0.\n    },\n    \'dropout\': {\'output_keep_prob\': 1. - enc_dropout_out},\n    \'num_layers\': 1\n}\n\nenc_emb_hparams = {\n    \'name\': \'lookup_table\',\n    \'dim\': embed_dim,\n    \'dropout_rate\': enc_dropout_in,\n    \'initializer\': {\n        \'type\': \'normal_\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'std\': embed_dim**-0.5,\n        },\n    }\n}\n\ndec_emb_hparams = {\n    \'name\': \'lookup_table\',\n    \'dim\': embed_dim,\n    \'dropout_rate\': dec_dropout_in,\n    \'initializer\': {\n        \'type\': \'normal_\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'std\': embed_dim**-0.5,\n        },\n    }\n}\n\nmax_pos = 200    # max sequence length in training data\ndec_pos_emb_hparams = {\n    \'dim\': hidden_size,\n}\n\n# due to the residual connection, the embed_dim should be equal to hidden_size\ntrans_hparams = {\n    \'output_layer_bias\': False,\n    \'embedding_dropout\': embedding_dropout,\n    \'residual_dropout\': residual_dropout,\n    \'num_blocks\': num_blocks,\n    \'dim\': hidden_size,\n    \'initializer\': {\n        \'type\': \'variance_scaling_initializer\',\n        \'kwargs\': {\n            \'factor\': 1.0,\n            \'mode\': \'FAN_AVG\',\n            \'uniform\': True,\n        },\n    },\n    \'multihead_attention\': {\n        \'dropout_rate\': attention_dropout,\n        \'num_heads\': 8,\n        \'num_units\': hidden_size,\n        \'output_dim\': hidden_size\n    },\n    \'poswise_feedforward\': {\n        \'name\': \'fnn\',\n        \'layers\': [\n            {\n                \'type\': \'Linear\',\n                \'kwargs\': {\n                    ""in_features"": hidden_size,\n                    ""out_features"": hidden_size * 4,\n                    ""bias"": True,\n                },\n            },\n            {\n                \'type\': \'ReLU\',\n            },\n            {\n                \'type\': \'Dropout\',\n                \'kwargs\': {\n                    \'p\': relu_dropout,\n                }\n            },\n            {\n                \'type\': \'Linear\',\n                \'kwargs\': {\n                    ""in_features"": hidden_size * 4,\n                    ""out_features"": hidden_size,\n                    ""bias"": True,\n                }\n            }\n        ],\n    }\n}\n\n# KL annealing\nkl_anneal_hparams = {\n    \'warm_up\': 10,\n    \'start\': 0.1\n}\n\ntrain_data_hparams = {\n    \'num_epochs\': 1,\n    \'batch_size\': batch_size,\n    \'seed\': 123,\n    \'dataset\': {\n        \'files\': \'./simple-examples/data/ptb.train.txt\',\n        \'vocab_file\': \'./simple-examples/data/vocab.txt\'\n    }\n}\n\nval_data_hparams = {\n    \'num_epochs\': 1,\n    \'batch_size\': batch_size,\n    \'seed\': 123,\n    \'dataset\': {\n        \'files\': \'./simple-examples/data/ptb.valid.txt\',\n        \'vocab_file\': \'./simple-examples/data/vocab.txt\'\n    }\n}\n\ntest_data_hparams = {\n    \'num_epochs\': 1,\n    \'batch_size\': batch_size,\n    \'dataset\': {\n        \'files\': \'./simple-examples/data/ptb.test.txt\',\n        \'vocab_file\': \'./simple-examples/data/vocab.txt\'\n    }\n}\n\nopt_hparams = {\n    \'optimizer\': {\n        \'type\': \'Adam\',\n        \'kwargs\': {\n            \'lr\': 0.001\n        }\n    },\n    \'gradient_clip\': {\n        ""type"": ""clip_grad_norm_"",\n        ""kwargs"": {\n            ""max_norm"": 5,\n            ""norm_type"": 2\n        }\n    }\n}\n'"
examples/vae_text/config_trans_yahoo.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""VAE config.\n""""""\n\ndataset = ""yahoo""\nnum_epochs = 100\nhidden_size = 512\ndec_dropout_in = 0.\nenc_dropout_in = 0.\nenc_dropout_out = 0.\nbatch_size = 32\nembed_dim = 512\n\nlatent_dims = 32\n\nlr_decay_hparams = {\n    ""init_lr"": 0.001,\n    ""threshold"": 2,\n    ""decay_factor"": 0.5,\n    ""max_decay"": 5\n}\n\n\nrelu_dropout = 0.2\nembedding_dropout = 0.2\nattention_dropout = 0.2\nresidual_dropout = 0.2\nnum_blocks = 3\n\ndecoder_type = \'transformer\'\n\nenc_cell_hparams = {\n    ""type"": ""LSTMCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": 1. - enc_dropout_out},\n    ""num_layers"": 1\n}\n\nenc_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": enc_dropout_in,\n    \'initializer\': {\n        \'type\': \'normal_\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'std\': embed_dim**-0.5,\n        },\n    }\n}\n\ndec_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": dec_dropout_in,\n    \'initializer\': {\n        \'type\': \'normal_\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'std\': embed_dim**-0.5,\n        },\n    }\n}\n\n\nmax_pos = 300    # max sequence length in training data\ndec_pos_emb_hparams = {\n    \'dim\': hidden_size,\n}\n\n# due to the residual connection, the embed_dim should be equal to hidden_size\ntrans_hparams = {\n    \'output_layer_bias\': False,\n    \'embedding_dropout\': embedding_dropout,\n    \'residual_dropout\': residual_dropout,\n    \'num_blocks\': num_blocks,\n    \'dim\': hidden_size,\n    \'initializer\': {\n        \'type\': \'variance_scaling_initializer\',\n        \'kwargs\': {\n            \'factor\': 1.0,\n            \'mode\': \'FAN_AVG\',\n            \'uniform\': True,\n        },\n    },\n    \'multihead_attention\': {\n        \'dropout_rate\': attention_dropout,\n        \'num_heads\': 8,\n        \'num_units\': hidden_size,\n        \'output_dim\': hidden_size\n    },\n    \'poswise_feedforward\': {\n        \'name\': \'fnn\',\n       \'layers\': [\n            {\n                \'type\': \'Linear\',\n                \'kwargs\': {\n                    ""in_features"": hidden_size,\n                    ""out_features"": hidden_size * 4,\n                    ""bias"": True,\n                },\n            },\n            {\n                \'type\': \'ReLU\',\n            },\n            {\n                \'type\': \'Dropout\',\n                \'kwargs\': {\n                    \'p\': relu_dropout,\n                }\n            },\n            {\n                \'type\': \'Linear\',\n                \'kwargs\': {\n                    ""in_features"": hidden_size * 4,\n                    ""out_features"": hidden_size,\n                    ""bias"": True,\n                }\n            }\n        ],\n    }\n}\n\n# KL annealing\nkl_anneal_hparams = {\n    ""warm_up"": 10,\n    ""start"": 0.1\n}\n\ntrain_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.train.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\nval_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.valid.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\ntest_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.test.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\nopt_hparams = {\n    \'optimizer\': {\n        \'type\': \'Adam\',\n        \'kwargs\': {\n            \'lr\': 0.001\n        }\n    },\n    \'gradient_clip\': {\n        ""type"": ""clip_grad_norm_"",\n        ""kwargs"": {\n            ""max_norm"": 5,\n            ""norm_type"": 2\n        }\n    }\n}\n'"
examples/vae_text/prepare_data.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for downloading and preprocessing the PTB and Yahoo data.\n""""""\nimport argparse\nimport os\n\nimport texar.torch as tx\n\n\ndef prepare_data(data_name):\n    """"""Prepare datasets.\n    Args:\n        data_path: the path to save the data\n        data_name: the name of dataset, ""ptb"" and ""yahoo""\n            are currently supported\n    """"""\n    if data_name == ""ptb"":\n        data_path = ""./simple-examples/data""\n        train_path = os.path.join(data_path, ""ptb.train.txt"")\n        if not os.path.exists(train_path):\n            url = \'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\'\n            tx.data.maybe_download(url, \'./\', extract=True)\n\n        train_path = os.path.join(data_path, ""ptb.train.txt"")\n        vocab_path = os.path.join(data_path, ""vocab.txt"")\n        word_to_id = tx.data.make_vocab(\n            train_path, return_type=""dict"")\n\n        with open(vocab_path, \'w\') as fvocab:\n            for word in word_to_id:\n                fvocab.write(""%s\\n"" % word)\n\n    elif data_name == ""yahoo"":\n        data_path = ""./data/yahoo""\n        train_path = os.path.join(data_path, ""yahoo.train.txt"")\n        if not os.path.exists(train_path):\n            url = \'https://drive.google.com/file/d/\' \\\n                  \'13IsiffVjcQ-wrrbBGMwiG3sYf-DFxtXH/view?usp=sharing\'\n            tx.data.maybe_download(url, path=\'./\', filenames=\'yahoo.zip\',\n                                   extract=True)\n    else:\n        raise ValueError(\'Unknown data: {}\'.format(data_name))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'prepare data\')\n    parser.add_argument(\'--data\', type=str, help=\'dataset to prepare\')\n    args = parser.parse_args()\n    prepare_data(args.data)\n'"
examples/vae_text/vae_train.py,27,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example for building the Variational Auto-Encoder.\n\nThis is an implementation of Variational Auto-Encoder for text generation\n\nTo run:\n\n$ python vae_train.py\n\nHyperparameters and data path may be specified in config_trans.py\n\n""""""\n\nimport argparse\nimport importlib\nimport math\nimport os\nimport sys\nimport time\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nfrom torch.optim.lr_scheduler import ExponentialLR\n\nimport texar.torch as tx\nfrom texar.torch.custom import MultivariateNormalDiag\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \'--config\', type=str, default=None,\n    help=""The config to use."")\nparser.add_argument(\n    \'--mode\', type=str, default=\'train\',\n    help=""Train or predict."")\nparser.add_argument(\n    \'--model\', type=str, default=None,\n    help=""Model path for generating sentences."")\nparser.add_argument(\n    \'--out\', type=str, default=None,\n    help=""Generation output path."")\n\nargs = parser.parse_args()\n\n\ndef kl_divergence(means: Tensor, logvars: Tensor) -> Tensor:\n    """"""Compute the KL divergence between Gaussian distribution\n    """"""\n    kl_cost = -0.5 * (logvars - means ** 2 -\n                      torch.exp(logvars) + 1.0)\n    kl_cost = torch.mean(kl_cost, 0)\n    return torch.sum(kl_cost)\n\n\nclass VAE(nn.Module):\n    _latent_z: Tensor\n\n    def __init__(self, vocab_size: int, config_model):\n        super().__init__()\n        # Model architecture\n        self._config = config_model\n        self.encoder_w_embedder = tx.modules.WordEmbedder(\n            vocab_size=vocab_size, hparams=config_model.enc_emb_hparams)\n\n        self.encoder = tx.modules.UnidirectionalRNNEncoder[tx.core.LSTMState](\n            input_size=self.encoder_w_embedder.dim,\n            hparams={\n                ""rnn_cell"": config_model.enc_cell_hparams,\n            })\n\n        self.decoder_w_embedder = tx.modules.WordEmbedder(\n            vocab_size=vocab_size, hparams=config_model.dec_emb_hparams)\n\n        if config_model.decoder_type == ""lstm"":\n            self.lstm_decoder = tx.modules.BasicRNNDecoder(\n                input_size=(self.decoder_w_embedder.dim +\n                            config_model.latent_dims),\n                vocab_size=vocab_size,\n                token_embedder=self._embed_fn_rnn,\n                hparams={""rnn_cell"": config_model.dec_cell_hparams})\n            sum_state_size = self.lstm_decoder.cell.hidden_size * 2\n\n        elif config_model.decoder_type == \'transformer\':\n            # position embedding\n            self.decoder_p_embedder = tx.modules.SinusoidsPositionEmbedder(\n                position_size=config_model.max_pos,\n                hparams=config_model.dec_pos_emb_hparams)\n            # decoder\n            self.transformer_decoder = tx.modules.TransformerDecoder(\n                # tie word embedding with output layer\n                output_layer=self.decoder_w_embedder.embedding,\n                token_pos_embedder=self._embed_fn_transformer,\n                hparams=config_model.trans_hparams)\n            sum_state_size = self._config.dec_emb_hparams[""dim""]\n\n        else:\n            raise ValueError(""Decoder type must be \'lstm\' or \'transformer\'"")\n\n        self.connector_mlp = tx.modules.MLPTransformConnector(\n            config_model.latent_dims * 2,\n            linear_layer_dim=self.encoder.cell.hidden_size * 2)\n\n        self.mlp_linear_layer = nn.Linear(\n            config_model.latent_dims, sum_state_size)\n\n    def forward(self,  # type: ignore\n                data_batch: tx.data.Batch,\n                kl_weight: float, start_tokens: torch.LongTensor,\n                end_token: int) -> Dict[str, Tensor]:\n        # encoder -> connector -> decoder\n        text_ids = data_batch[""text_ids""]\n        input_embed = self.encoder_w_embedder(text_ids)\n        _, encoder_states = self.encoder(\n            input_embed,\n            sequence_length=data_batch[""length""])\n\n        mean_logvar = self.connector_mlp(encoder_states)\n        mean, logvar = torch.chunk(mean_logvar, 2, 1)\n        kl_loss = kl_divergence(mean, logvar)\n        dst = MultivariateNormalDiag(\n            loc=mean, scale_diag=torch.exp(0.5 * logvar))\n\n        latent_z = dst.rsample()\n        helper = None\n        if self._config.decoder_type == ""lstm"":\n            helper = self.lstm_decoder.create_helper(\n                decoding_strategy=""train_greedy"",\n                start_tokens=start_tokens,\n                end_token=end_token)\n\n        # decode\n        seq_lengths = data_batch[""length""] - 1\n        outputs = self.decode(\n            helper=helper, latent_z=latent_z,\n            text_ids=text_ids[:, :-1], seq_lengths=seq_lengths)\n\n        logits = outputs.logits\n\n        # Losses & train ops\n        rc_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n            labels=text_ids[:, 1:], logits=logits,\n            sequence_length=seq_lengths)\n\n        nll = rc_loss + kl_weight * kl_loss\n\n        ret = {\n            ""nll"": nll,\n            ""kl_loss"": kl_loss,\n            ""rc_loss"": rc_loss,\n            ""lengths"": seq_lengths,\n        }\n\n        return ret\n\n    def _embed_fn_rnn(self, tokens: torch.LongTensor) -> Tensor:\n        r""""""Generates word embeddings\n        """"""\n        embedding = self.decoder_w_embedder(tokens)\n        latent_z = self._latent_z\n        if len(embedding.size()) > 2:\n            latent_z = latent_z.unsqueeze(0).repeat(tokens.size(0), 1, 1)\n        return torch.cat([embedding, latent_z], dim=-1)\n\n    def _embed_fn_transformer(self,\n                              tokens: torch.LongTensor,\n                              positions: torch.LongTensor) -> Tensor:\n        r""""""Generates word embeddings combined with positional embeddings\n        """"""\n        output_p_embed = self.decoder_p_embedder(positions)\n        output_w_embed = self.decoder_w_embedder(tokens)\n        output_w_embed = output_w_embed * self._config.hidden_size ** 0.5\n        output_embed = output_w_embed + output_p_embed\n        return output_embed\n\n    @property\n    def decoder(self) -> tx.modules.DecoderBase:\n        if self._config.decoder_type == ""lstm"":\n            return self.lstm_decoder\n        else:\n            return self.transformer_decoder\n\n    def decode(self,\n               helper: Optional[tx.modules.Helper],\n               latent_z: Tensor,\n               text_ids: Optional[torch.LongTensor] = None,\n               seq_lengths: Optional[Tensor] = None,\n               max_decoding_length: Optional[int] = None) \\\n            -> Union[tx.modules.BasicRNNDecoderOutput,\n                     tx.modules.TransformerDecoderOutput]:\n        self._latent_z = latent_z\n        fc_output = self.mlp_linear_layer(latent_z)\n\n        if self._config.decoder_type == ""lstm"":\n            lstm_states = torch.chunk(fc_output, 2, dim=1)\n            outputs, _, _ = self.lstm_decoder(\n                initial_state=lstm_states,\n                inputs=text_ids,\n                helper=helper,\n                sequence_length=seq_lengths,\n                max_decoding_length=max_decoding_length)\n        else:\n            transformer_states = fc_output.unsqueeze(1)\n            outputs = self.transformer_decoder(\n                inputs=text_ids,\n                memory=transformer_states,\n                memory_sequence_length=torch.ones(transformer_states.size(0)),\n                helper=helper,\n                max_decoding_length=max_decoding_length)\n        return outputs\n\n\ndef main() -> None:\n    """"""Entrypoint.\n    """"""\n    config: Any = importlib.import_module(args.config)\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    train_data = tx.data.MonoTextData(config.train_data_hparams, device=device)\n    val_data = tx.data.MonoTextData(config.val_data_hparams, device=device)\n    test_data = tx.data.MonoTextData(config.test_data_hparams, device=device)\n\n    iterator = tx.data.DataIterator(\n        {""train"": train_data, ""valid"": val_data, ""test"": test_data})\n\n    opt_vars = {\n        \'learning_rate\': config.lr_decay_hparams[""init_lr""],\n        \'best_valid_nll\': 1e100,\n        \'steps_not_improved\': 0,\n        \'kl_weight\': config.kl_anneal_hparams[""start""]\n    }\n\n    decay_cnt = 0\n    max_decay = config.lr_decay_hparams[""max_decay""]\n    decay_factor = config.lr_decay_hparams[""decay_factor""]\n    decay_ts = config.lr_decay_hparams[""threshold""]\n\n    save_dir = f""./models/{config.dataset}""\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    suffix = f""{config.dataset}_{config.decoder_type}Decoder.ckpt""\n\n    save_path = os.path.join(save_dir, suffix)\n\n    # KL term annealing rate\n    anneal_r = 1.0 / (config.kl_anneal_hparams[""warm_up""] *\n                      (len(train_data) / config.batch_size))\n\n    vocab = train_data.vocab\n    model = VAE(train_data.vocab.size, config)\n    model.to(device)\n\n    start_tokens = torch.full(\n        (config.batch_size,),\n        vocab.bos_token_id,\n        dtype=torch.long).to(device)\n    end_token = vocab.eos_token_id\n    optimizer = tx.core.get_optimizer(\n        params=model.parameters(),\n        hparams=config.opt_hparams)\n    scheduler = ExponentialLR(optimizer, decay_factor)\n\n    def _run_epoch(epoch: int, mode: str, display: int = 10) \\\n            -> Tuple[Tensor, float]:\n        iterator.switch_to_dataset(mode)\n\n        if mode == \'train\':\n            model.train()\n            opt_vars[""kl_weight""] = min(\n                1.0, opt_vars[""kl_weight""] + anneal_r)\n\n            kl_weight = opt_vars[""kl_weight""]\n        else:\n            model.eval()\n            kl_weight = 1.0\n        step = 0\n        start_time = time.time()\n        num_words = 0\n        nll_total = 0.\n\n        avg_rec = tx.utils.AverageRecorder()\n        for batch in iterator:\n            ret = model(batch, kl_weight, start_tokens, end_token)\n            if mode == ""train"":\n                opt_vars[""kl_weight""] = min(\n                    1.0, opt_vars[""kl_weight""] + anneal_r)\n                kl_weight = opt_vars[""kl_weight""]\n                ret[""nll""].backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n            batch_size = len(ret[""lengths""])\n            num_words += torch.sum(ret[""lengths""]).item()\n            nll_total += ret[""nll""].item() * batch_size\n            avg_rec.add(\n                [ret[""nll""].item(),\n                 ret[""kl_loss""].item(),\n                 ret[""rc_loss""].item()],\n                batch_size)\n            if step % display == 0 and mode == \'train\':\n                nll = avg_rec.avg(0)\n                klw = opt_vars[""kl_weight""]\n                KL = avg_rec.avg(1)\n                rc = avg_rec.avg(2)\n                log_ppl = nll_total / num_words\n                ppl = math.exp(log_ppl)\n                time_cost = time.time() - start_time\n\n                print(f""{mode}: epoch {epoch}, step {step}, nll {nll:.4f}, ""\n                      f""klw {klw:.4f}, KL {KL:.4f}, rc {rc:.4f}, ""\n                      f""log_ppl {log_ppl:.4f}, ppl {ppl:.4f}, ""\n                      f""time_cost {time_cost:.1f}"", flush=True)\n\n            step += 1\n\n        nll = avg_rec.avg(0)\n        KL = avg_rec.avg(1)\n        rc = avg_rec.avg(2)\n        log_ppl = nll_total / num_words\n        ppl = math.exp(log_ppl)\n        print(f""\\n{mode}: epoch {epoch}, nll {nll:.4f}, KL {KL:.4f}, ""\n              f""rc {rc:.4f}, log_ppl {log_ppl:.4f}, ppl {ppl:.4f}"")\n        return nll, ppl  # type: ignore\n\n    @torch.no_grad()\n    def _generate(start_tokens: torch.LongTensor,\n                  end_token: int,\n                  filename: Optional[str] = None):\n        ckpt = torch.load(args.model)\n        model.load_state_dict(ckpt[\'model\'])\n        model.eval()\n\n        batch_size = train_data.batch_size\n\n        dst = MultivariateNormalDiag(\n            loc=torch.zeros(batch_size, config.latent_dims),\n            scale_diag=torch.ones(batch_size, config.latent_dims))\n\n        latent_z = dst.rsample().to(device)\n\n        helper = model.decoder.create_helper(\n            decoding_strategy=\'infer_sample\',\n            start_tokens=start_tokens,\n            end_token=end_token)\n        outputs = model.decode(\n            helper=helper,\n            latent_z=latent_z,\n            max_decoding_length=100)\n\n        if config.decoder_type == ""transformer"":\n            outputs = outputs[0]\n\n        sample_tokens = vocab.map_ids_to_tokens_py(outputs.sample_id.cpu())\n\n        if filename is None:\n            fh = sys.stdout\n        else:\n            fh = open(filename, \'w\', encoding=\'utf-8\')\n\n        for sent in sample_tokens:\n            sent = tx.utils.compat_as_text(list(sent))\n            end_id = len(sent)\n            if vocab.eos_token in sent:\n                end_id = sent.index(vocab.eos_token)\n            fh.write(\' \'.join(sent[:end_id + 1]) + \'\\n\')\n\n        print(\'Output done\')\n        fh.close()\n\n    if args.mode == ""predict"":\n        _generate(start_tokens, end_token, args.out)\n        return\n    # Counts trainable parameters\n    total_parameters = sum(param.numel() for param in model.parameters())\n    print(f""{total_parameters} total parameters"")\n\n    best_nll = best_ppl = 0.\n\n    for epoch in range(config.num_epochs):\n        _, _ = _run_epoch(epoch, \'train\', display=200)\n        val_nll, _ = _run_epoch(epoch, \'valid\')\n        test_nll, test_ppl = _run_epoch(epoch, \'test\')\n\n        if val_nll < opt_vars[\'best_valid_nll\']:\n            opt_vars[\'best_valid_nll\'] = val_nll\n            opt_vars[\'steps_not_improved\'] = 0\n            best_nll = test_nll\n            best_ppl = test_ppl\n\n            states = {\n                ""model"": model.state_dict(),\n                ""optimizer"": optimizer.state_dict(),\n                ""scheduler"": scheduler.state_dict()\n            }\n            torch.save(states, save_path)\n        else:\n            opt_vars[\'steps_not_improved\'] += 1\n            if opt_vars[\'steps_not_improved\'] == decay_ts:\n                old_lr = opt_vars[\'learning_rate\']\n                opt_vars[\'learning_rate\'] *= decay_factor\n                opt_vars[\'steps_not_improved\'] = 0\n                new_lr = opt_vars[\'learning_rate\']\n                ckpt = torch.load(save_path)\n                model.load_state_dict(ckpt[\'model\'])\n                optimizer.load_state_dict(ckpt[\'optimizer\'])\n                scheduler.load_state_dict(ckpt[\'scheduler\'])\n                scheduler.step()\n                print(f""-----\\nchange lr, old lr: {old_lr}, ""\n                      f""new lr: {new_lr}\\n-----"")\n\n                decay_cnt += 1\n                if decay_cnt == max_decay:\n                    break\n\n    print(f""\\nbest testing nll: {best_nll:.4f},""\n          f""best testing ppl {best_ppl:.4f}\\n"")\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/xlnet/xlnet_classification_main.py,6,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example of building XLNet language model for classification/regression.\n""""""\n\nimport argparse\nimport importlib\nimport logging\nfrom typing import Dict, Optional, Union\n\nimport torch\nimport torch.nn.functional as F\nimport texar.torch as tx\nfrom texar.torch.run import *  # pylint: disable=wildcard-import\n\nfrom utils import dataset, model_utils\nfrom utils.processor import get_processor_class\n\n\ndef load_config_into_args(config_path: str, args):\n    config_module_path = config_path.replace(\'/\', \'.\').replace(\'\\\\\', \'.\')\n    if config_module_path.endswith("".py""):\n        config_module_path = config_module_path[:-3]\n    config_data = importlib.import_module(config_module_path)\n    for key in dir(config_data):\n        if not key.startswith(\'__\'):\n            setattr(args, key, getattr(config_data, key))\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--config-data"", required=True,\n        help=""Path to the dataset configuration file."")\n\n    parser.add_argument(\n        ""--mode"", choices=[""train"", ""eval""], default=""train"")\n    parser.add_argument(\n        ""--checkpoint"", type=str, default=None,\n        help=""Path to a saved checkpoint file to load"")\n    parser.add_argument(\n        ""--save-dir"", type=str, default=None,\n        help=""Directory to save model checkpoints"")\n\n    parser.add_argument(\n        ""--pretrained-model-name"", type=str,\n        default=""xlnet-large-cased"",\n        help=""The pre-trained model name to load selected in the list of: ""\n             ""`xlnet-base-cased`, `xlnet-large-cased`."")\n    parser.add_argument(\n        ""--data-dir"", type=str, default=None,\n        help=""Path to the directory containing raw data. ""\n             ""Defaults to \'data/<task name>\'"")\n    parser.add_argument(\n        ""--cache-dir"", type=str, default=None,\n        help=""Path to the directory to cache processed data. ""\n             ""Defaults to \'processed_data/<task name>\'"")\n    parser.add_argument(\n        ""--uncased"", type=bool, default=False,\n        help=""Whether the pretrained model is an uncased model"")\n\n    args = parser.parse_args()\n    load_config_into_args(args.config_data, args)\n    return args\n\n\ndef construct_datasets(args) -> Dict[str, tx.data.RecordData]:\n    cache_prefix = f""length{args.max_seq_len}""\n\n    tokenizer = tx.data.XLNetTokenizer(\n        pretrained_model_name=args.pretrained_model_name)\n    tokenizer.do_lower_case = args.uncased\n\n    processor_class = get_processor_class(args.task)\n    data_dir = args.data_dir or f""data/{processor_class.task_name}""\n    cache_dir = args.cache_dir or f""processed_data/{processor_class.task_name}""\n    task_processor = processor_class(data_dir)\n    dataset.construct_dataset(\n        task_processor, cache_dir, args.max_seq_len,\n        tokenizer, file_prefix=cache_prefix)\n\n    datasets = dataset.load_datasets(\n        args.task, cache_dir, args.max_seq_len, args.batch_size,\n        file_prefix=cache_prefix, eval_batch_size=args.eval_batch_size,\n        shuffle_buffer=None)\n    return datasets\n\n\nclass RegressorWrapper(tx.modules.XLNetRegressor):\n    def forward(self,  # type: ignore\n                batch: tx.data.Batch) -> Dict[str, torch.Tensor]:\n        preds = super().forward(inputs=batch.input_ids,\n                                segment_ids=batch.segment_ids,\n                                input_mask=batch.input_mask)\n        loss = (preds - batch.label_ids) ** 2\n        loss = loss.sum() / len(batch)\n        return {""loss"": loss, ""preds"": preds}\n\n\nclass ClassifierWrapper(tx.modules.XLNetClassifier):\n    def forward(self,  # type: ignore\n                batch: tx.data.Batch) -> Dict[str, torch.Tensor]:\n        logits, preds = super().forward(inputs=batch.input_ids,\n                                        segment_ids=batch.segment_ids,\n                                        input_mask=batch.input_mask)\n        loss = F.cross_entropy(logits, batch.label_ids, reduction=\'none\')\n        loss = loss.sum() / len(batch)\n        return {""loss"": loss, ""preds"": preds}\n\n\ndef main(args) -> None:\n    if args.seed != -1:\n        make_deterministic(args.seed)\n        print(f""Random seed set to {args.seed}"")\n\n    datasets = construct_datasets(args)\n    print(""Dataset constructed"")\n\n    processor_class = get_processor_class(args.task)\n    is_regression = processor_class.is_regression\n    model: Union[RegressorWrapper, ClassifierWrapper]\n    if is_regression:\n        model = RegressorWrapper(\n            pretrained_model_name=args.pretrained_model_name)\n    else:\n        model = ClassifierWrapper(\n            pretrained_model_name=args.pretrained_model_name,\n            hparams={""num_classes"": len(processor_class.labels)})\n    print(""Model constructed"")\n\n    optim = torch.optim.Adam(\n        model.param_groups(args.lr, args.lr_layer_decay_rate), lr=args.lr,\n        eps=args.adam_eps, weight_decay=args.weight_decay)\n    lambda_lr = model_utils.warmup_lr_lambda(\n        args.train_steps, args.warmup_steps, args.min_lr_ratio)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lambda_lr)\n\n    bps = args.backwards_per_step\n\n    def get_condition(steps: int) -> Optional[cond.Condition]:\n        if steps == -1:\n            return None\n        return cond.iteration(steps * bps)\n\n    if is_regression:\n        valid_metric: metric.Metric = metric.PearsonR(\n            pred_name=""preds"", label_name=""label_ids"")\n    else:\n        valid_metric = metric.Accuracy(\n            pred_name=""preds"", label_name=""label_ids"")\n    executor = Executor(\n        model=model,\n        train_data=datasets[""train""],\n        valid_data=datasets[""dev""],\n        test_data=datasets.get(""test"", None),\n        checkpoint_dir=args.save_dir or f""saved_models/{args.task}"",\n        save_every=get_condition(args.save_steps),\n        max_to_keep=1,\n        train_metrics=[\n            (""loss"", metric.RunningAverage(args.display_steps * bps)),\n            metric.LR(optim)],\n        optimizer=optim,\n        lr_scheduler=scheduler,\n        grad_clip=args.grad_clip,\n        num_iters_per_update=args.backwards_per_step,\n        log_every=cond.iteration(args.display_steps * bps),\n        validate_every=get_condition(args.eval_steps),\n        valid_metrics=[valid_metric, (""loss"", metric.Average())],\n        stop_training_on=cond.iteration(args.train_steps * bps),\n        log_format=""{time} : Epoch {epoch} @ {iteration:5d}it ""\n                   ""({speed}), LR = {LR:.3e}, loss = {loss:.3f}"",\n        test_mode=\'eval\',\n        show_live_progress=True,\n    )\n\n    if args.checkpoint is not None:\n        executor.load(args.checkpoint)\n\n    if args.mode == \'train\':\n        executor.train()\n        executor.save()\n        executor.test(tx.utils.dict_fetch(datasets, [""dev"", ""test""]))\n    else:\n        if args.checkpoint is None:\n            executor.load(load_training_state=False)  # load previous best model\n        executor.test(tx.utils.dict_fetch(datasets, [""dev"", ""test""]))\n\n\nif __name__ == \'__main__\':\n    logging.root.setLevel(logging.INFO)\n    _args = parse_args()\n    main(_args)\n'"
examples/xlnet/xlnet_generation_ipython.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example of building XLNet language model for sample generation.\n""""""\n\nimport torch\n\nimport texar.torch as tx\n\n\ndef main() -> None:\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    model = tx.modules.XLNetDecoder(pretrained_model_name=\'xlnet-large-cased\')\n    model = model.to(device)\n\n    tokenizer = tx.data.XLNetTokenizer(\n        pretrained_model_name=\'xlnet-large-cased\')\n\n    # A lengthy padding text used to workaround lack of context for short\n    # prompts. Refer to https://github.com/rusiaaman/XLNet-gen for the rationale\n    # behind this.\n    pad_txt = """"""\n        Texar-PyTorch is an open-source toolkit based on PyTorch, aiming to\n        support a broad set of machine learning, especially text generation\n        tasks, such as machine translation, dialog, summarization, content\n        manipulation, language modeling, and so on. Texar is designed for both\n        researchers and practitioners for fast prototyping and\n        experimentation.\n        With the design goals of modularity, versatility, and extensibility in\n        mind, Texar extracts the common patterns underlying the diverse tasks\n        and methodologies, creates a library of highly reusable modules and\n        functionalities, and facilitates arbitrary model architectures and\n        algorithmic paradigms. """"""\n    pad_ids = tokenizer.map_text_to_id(pad_txt)\n    eod_id = tokenizer.map_token_to_id(""<eod>"")\n    pad_ids.append(eod_id)\n\n    def split_by(xs, y):\n        r""""""Splits list `xs` by value `y`.\n\n        Example:\n            list(split_by([1,2,4,5,6,4,7,4], 4))\n            # [[1, 2], [5, 6], [7]]\n        """"""\n        p = 0\n        for idx, x in enumerate(xs):\n            if x == y:\n                if idx - p > 0:\n                    yield xs[p:idx]\n                p = idx + 1\n        if len(xs) - p > 0:\n            yield xs[p:]\n\n    @torch.no_grad()\n    def sample(text: str, length: int = 100, n_samples=3, **kwargs):\n        print(""=== Prompt ==="")\n        print(text)\n        model.eval()\n        text = text.replace(""\\n"", ""<eop>"")\n        tokens = pad_ids + tokenizer.map_text_to_id(text)\n        tokens = torch.tensor(tokens, device=device).expand(n_samples, -1)\n        kwargs.setdefault(""print_steps"", True)\n        decode_output, _ = model(start_tokens=tokens,\n                                 end_token=eod_id,\n                                 max_decoding_length=length,\n                                 **kwargs)\n        decode_samples = decode_output.sample_id.tolist()\n        for idx, sample_tokens in enumerate(decode_samples):\n            print(f""=== Sample {idx} ==="")\n            output = ""\\n"".join(tokenizer.map_id_to_text(xs) for xs in split_by(\n                sample_tokens, tokenizer.map_token_to_id(""<eop>"")))\n            print(output)\n\n    try:\n        from IPython import embed\n        print(""Generate text by calling: sample(\\""<your prompt text>\\"", ...).\\n""\n              ""For options, refer to `forward` method of `XLNetDecoder`.\\n"")\n        embed()\n    except ImportError:\n        print(""To be able to specify sampling options, please install IPython."")\n        try:\n            while True:\n                prompt = input(""Input prompt: "")\n                print(sample(prompt))\n        except EOFError:\n            pass\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/xlnet/xlnet_generation_main.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example of building XLNet language model for sample generation.\n""""""\n\nimport argparse\nimport sys\n\nimport torch\n\nimport texar.torch as tx\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--checkpoint\', type=str, default=None,\n                    help=""Checkpoint to load model weights from."")\nparser.add_argument(""--pretrained-model-name"", type=str,\n                    default=""xlnet-large-cased"",\n                    help=""The pre-trained model to load selected in the list ""\n                         ""of: `xlnet-base-cased`, `xlnet-large-cased`."")\nparser.add_argument(\'--seed\', type=int, default=None, help=""Random seed."")\nparser.add_argument(\'--nsamples\', type=int, default=1,\n                    help=""Total number of samples to generate. Used in ""\n                         ""non-interactive mode."")\nparser.add_argument(\'--batch-size\', type=int, default=1,\n                    help=""The batch size of input."")\nparser.add_argument(\'--max-decoding-length\', type=int, default=100,\n                    help=""The maximun length of generated text."")\nparser.add_argument(\'--temperature\', type=float, default=0.7,\n                    help=""Softmax temperature for top-k sample decoding. Must ""\n                         ""be strictly greater than 0. Defaults to 0.7."")\nparser.add_argument(\'--top-k\', type=int, default=40,\n                    help=""The number of top most likely candidates to choose ""\n                         ""from at each step. This is use ""\n                         ""TopKSampleEmbeddingHelper for decoding. Ignored if ""\n                         ""\'p\' is given."")\nparser.add_argument(\'--top-p\', type=float, default=None,\n                    help=""Select tokens with cumulative probability of at most ""\n                         ""\'top-p\' when arranged in decreasing order. This ""\n                         ""will use TopPSampleEmbeddingHelper for decoding."")\nparser.add_argument(\'--interactive\', action=\'store_true\',\n                    help=""Interactive mode or not."")\n\nargs = parser.parse_args()\n\n\ndef main() -> None:\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    model = tx.modules.XLNetDecoder(\n        pretrained_model_name=args.pretrained_model_name)\n    if args.checkpoint is not None:\n        model.load_state_dict(torch.load(args.checkpoint, map_location=device))\n        print(f""Loaded checkpoint from {args.checkpoint}"")\n    model = model.to(device)\n\n    tokenizer = tx.data.XLNetTokenizer(\n        pretrained_model_name=args.pretrained_model_name)\n\n    # A lengthy padding text used to workaround lack of context for short\n    # prompts. Refer to https://github.com/rusiaaman/XLNet-gen for the rationale\n    # behind this.\n    pad_txt = """"""\n        Texar-PyTorch is an open-source toolkit based on PyTorch, aiming to\n        support a broad set of machine learning, especially text generation\n        tasks, such as machine translation, dialog, summarization, content\n        manipulation, language modeling, and so on. Texar is designed for both\n        researchers and practitioners for fast prototyping and\n        experimentation.\n        With the design goals of modularity, versatility, and extensibility in\n        mind, Texar extracts the common patterns underlying the diverse tasks\n        and methodologies, creates a library of highly reusable modules and\n        functionalities, and facilitates arbitrary model architectures and\n        algorithmic paradigms. """"""\n    pad_ids = tokenizer.map_text_to_id(pad_txt)\n    eod_id = tokenizer.map_token_to_id(""<eod>"")\n    pad_ids.append(eod_id)\n\n    def split_by(xs, y):\n        p = 0\n        for idx, x in enumerate(xs):\n            if x == y:\n                if idx - p > 0:\n                    yield xs[p:idx]\n                p = idx + 1\n        if len(xs) - p > 0:\n            yield xs[p:]\n\n    @torch.no_grad()\n    def sample(text: str, length: int = 100, n_samples=3, **kwargs):\n        model.eval()\n        text = text.replace(""\\n"", ""<eop>"")\n        tokens = pad_ids + tokenizer.map_text_to_id(text)\n        tokens = torch.tensor(tokens, device=device).expand(n_samples, -1)\n        if args.top_p:\n            kwargs[""p""] = args.top_p\n            decode_output, _ = model(\n                start_tokens=tokens,\n                end_token=eod_id,\n                max_decoding_length=length,\n                print_steps=True,\n                helper_type=tx.modules.TopPSampleEmbeddingHelper,\n                **kwargs)\n        else:\n            kwargs[""top_k""] = args.top_k\n            decode_output, _ = model(\n                start_tokens=tokens,\n                end_token=eod_id,\n                max_decoding_length=length,\n                print_steps=True,\n                helper_type=tx.modules.TopKSampleEmbeddingHelper,\n                **kwargs)\n        decode_samples = decode_output.sample_id.tolist()\n        for idx, sample_tokens in enumerate(decode_samples):\n            print(f""=== Sample {idx} ==="")\n            output = ""\\n"".join(tokenizer.map_id_to_text(xs) for xs in split_by(\n                sample_tokens, tokenizer.map_token_to_id(""<eop>"")))\n            print(output)\n\n    nsamples = args.nsamples\n    batch_size = args.batch_size\n    max_decoding_length = args.max_decoding_length\n    assert nsamples % batch_size == 0, (\n        ""nsamples must be dividable by batch_size"")\n\n    if args.interactive:\n        while True:\n            try:\n                raw_text = input(""Model input >>> "")\n                while not raw_text:\n                    print(\'Input should not be empty!\')\n                    raw_text = input(""Model input >>> "")\n                sample(text=raw_text, length=max_decoding_length,\n                       n_samples=batch_size)\n            except EOFError:\n                print(""EOF entered, quitting."")\n                sys.exit()\n    else:\n        # Generate samples from scratch\n        for _ in range(nsamples // batch_size):\n            for _ in range(args.batch_size):\n                sample(text=""<BOS>"", length=max_decoding_length,\n                       n_samples=batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tests/core/attention_mechanism_test.py,36,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for attention mechanism.\n""""""\n\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.core.attention_mechanism import *\n\n\nclass AttentionMechanismTest(unittest.TestCase):\n    r""""""Tests attention mechanism.\n    """"""\n\n    def setUp(self):\n        self._batch_size = 8\n        self._max_time = 16\n        self._encoder_output_size = 64\n        self._attention_dim = 256\n        self._memory = torch.rand(\n            self._batch_size, self._max_time, self._encoder_output_size\n        )\n        self._memory_sequence_length = torch.tensor(\n            np.random.randint(self._max_time, size=[self._batch_size]) + 1\n        )\n        self._attention_state = torch.rand(self._batch_size, self._max_time)\n\n    def test_LuongAttention(self):\n        r""""""Tests `LuongAttention`\n        """"""\n        # Case 1\n        attention_mechanism = LuongAttention(\n            num_units=self._attention_dim,\n            encoder_output_size=self._encoder_output_size)\n\n        cell_output = torch.rand(self._batch_size, self._attention_dim)\n\n        attention, alignments, next_attention_state = \\\n            compute_attention(\n                attention_mechanism=attention_mechanism,\n                cell_output=cell_output,\n                attention_state=self._attention_state,\n                memory=self._memory,\n                attention_layer=None,\n                memory_sequence_length=self._memory_sequence_length)\n\n        self.assertEqual(attention.shape, torch.Size(\n            [self._batch_size, self._encoder_output_size]))\n        self.assertEqual(alignments.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(next_attention_state.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(len(attention_mechanism.trainable_variables), 1)\n\n        # Case 2\n        attention_mechanism = LuongAttention(\n            num_units=self._attention_dim,\n            encoder_output_size=self._encoder_output_size,\n            scale=True)\n\n        cell_output = torch.rand(self._batch_size, self._attention_dim)\n\n        attention, alignments, next_attention_state = \\\n            compute_attention(\n                attention_mechanism=attention_mechanism,\n                cell_output=cell_output,\n                attention_state=self._attention_state,\n                memory=self._memory,\n                attention_layer=None,\n                memory_sequence_length=self._memory_sequence_length)\n\n        self.assertEqual(attention.shape, torch.Size(\n            [self._batch_size, self._encoder_output_size]))\n        self.assertEqual(alignments.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(next_attention_state.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(len(attention_mechanism.trainable_variables), 2)\n\n    def test_BahdanauAttention(self):\n        r""""""Tests BahdanauAttention\n        """"""\n        # Case 1\n        attention_mechanism = BahdanauAttention(\n            num_units=self._attention_dim,\n            decoder_output_size=128,\n            encoder_output_size=self._encoder_output_size)\n\n        cell_output = torch.rand(self._batch_size, 128)\n\n        attention, alignments, next_attention_state = \\\n            compute_attention(\n                attention_mechanism=attention_mechanism,\n                cell_output=cell_output,\n                attention_state=self._attention_state,\n                memory=self._memory,\n                attention_layer=None,\n                memory_sequence_length=self._memory_sequence_length)\n\n        self.assertEqual(attention.shape, torch.Size(\n            [self._batch_size, self._encoder_output_size]))\n        self.assertEqual(alignments.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(next_attention_state.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(len(attention_mechanism.trainable_variables), 3)\n\n        # Case 2\n        attention_mechanism = BahdanauAttention(\n            num_units=self._attention_dim,\n            decoder_output_size=128,\n            encoder_output_size=self._encoder_output_size,\n            normalize=True)\n\n        cell_output = torch.rand(self._batch_size, 128)\n\n        attention, alignments, next_attention_state = \\\n            compute_attention(\n                attention_mechanism=attention_mechanism,\n                cell_output=cell_output,\n                attention_state=self._attention_state,\n                memory=self._memory,\n                attention_layer=None,\n                memory_sequence_length=self._memory_sequence_length)\n\n        self.assertEqual(attention.shape, torch.Size(\n            [self._batch_size, self._encoder_output_size]))\n        self.assertEqual(alignments.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(next_attention_state.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(len(attention_mechanism.trainable_variables), 5)\n\n    def test_LuongMonotonicAttention(self):\n        r""""""Tests LuongMonotonicAttention\n        """"""\n        # Case 1\n        attention_mechanism = LuongMonotonicAttention(\n            num_units=self._attention_dim,\n            encoder_output_size=self._encoder_output_size)\n\n        cell_output = torch.rand(self._batch_size, self._attention_dim)\n\n        attention, alignments, next_attention_state = \\\n            compute_attention(\n                attention_mechanism=attention_mechanism,\n                cell_output=cell_output,\n                attention_state=self._attention_state,\n                memory=self._memory,\n                attention_layer=None,\n                memory_sequence_length=self._memory_sequence_length)\n\n        self.assertEqual(attention.shape, torch.Size(\n            [self._batch_size, self._encoder_output_size]))\n        self.assertEqual(alignments.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(next_attention_state.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(len(attention_mechanism.trainable_variables), 2)\n\n        # Case 2\n        attention_mechanism = LuongMonotonicAttention(\n            num_units=self._attention_dim,\n            encoder_output_size=self._encoder_output_size,\n            scale=True)\n\n        cell_output = torch.rand(self._batch_size, self._attention_dim)\n\n        attention, alignments, next_attention_state = \\\n            compute_attention(\n                attention_mechanism=attention_mechanism,\n                cell_output=cell_output,\n                attention_state=self._attention_state,\n                memory=self._memory,\n                attention_layer=None,\n                memory_sequence_length=self._memory_sequence_length)\n\n        self.assertEqual(attention.shape, torch.Size(\n            [self._batch_size, self._encoder_output_size]))\n        self.assertEqual(alignments.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(next_attention_state.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(len(attention_mechanism.trainable_variables), 3)\n\n    def test_BahdanauMonotonicAttention(self):\n        r""""""Tests BahdanauMonotonicAttention\n        """"""\n        # Case 1\n        attention_mechanism = BahdanauMonotonicAttention(\n            num_units=self._attention_dim,\n            decoder_output_size=128,\n            encoder_output_size=self._encoder_output_size)\n\n        cell_output = torch.rand(self._batch_size, 128)\n\n        attention, alignments, next_attention_state = \\\n            compute_attention(\n                attention_mechanism=attention_mechanism,\n                cell_output=cell_output,\n                attention_state=self._attention_state,\n                memory=self._memory,\n                attention_layer=None,\n                memory_sequence_length=self._memory_sequence_length)\n\n        self.assertEqual(attention.shape, torch.Size(\n            [self._batch_size, self._encoder_output_size]))\n        self.assertEqual(alignments.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(next_attention_state.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(len(attention_mechanism.trainable_variables), 4)\n\n        # Case 2\n        attention_mechanism = BahdanauMonotonicAttention(\n            num_units=self._attention_dim,\n            decoder_output_size=128,\n            encoder_output_size=self._encoder_output_size,\n            normalize=True)\n\n        cell_output = torch.rand(self._batch_size, 128)\n\n        attention, alignments, next_attention_state = \\\n            compute_attention(\n                attention_mechanism=attention_mechanism,\n                cell_output=cell_output,\n                attention_state=self._attention_state,\n                memory=self._memory,\n                attention_layer=None,\n                memory_sequence_length=self._memory_sequence_length)\n\n        self.assertEqual(attention.shape, torch.Size(\n            [self._batch_size, self._encoder_output_size]))\n        self.assertEqual(alignments.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(next_attention_state.shape, torch.Size(\n            [self._batch_size, self._max_time]))\n        self.assertEqual(len(attention_mechanism.trainable_variables), 6)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/core/attention_mechanism_utils_test.py,15,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for attention mechanism utils.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.core.attention_mechanism import (\n    maybe_mask_score, prepare_memory, safe_cumprod)\nfrom texar.torch.core.attention_mechanism_utils import hardmax, sparsemax\n\n\nclass AttentionMechanismUtilsTest(unittest.TestCase):\n    r""""""Tests attention mechanism utils.\n    """"""\n\n    def test_prepare_memory(self):\n        r""""""Tests `texar.torch.core.attention_mechanism_utils.prepare_memory`.\n        """"""\n        memory = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [9, 8, 7]],\n                               [[3, 2, 1], [6, 5, 4], [9, 8, 7], [4, 5, 6]]])\n        memory_sequence_length = torch.tensor([3, 2])\n        masked_memory = prepare_memory(memory, memory_sequence_length)\n\n        expected_memory = [[[1, 2, 3], [4, 5, 6], [7, 8, 9], [0, 0, 0]],\n                           [[3, 2, 1], [6, 5, 4], [0, 0, 0], [0, 0, 0]]]\n\n        self.assertEqual(masked_memory.tolist(), expected_memory)\n\n    def test_maybe_mask_score(self):\n        r""""""Tests `texar.torch.core.attention_mechanism_utils.maybe_mask_score`.\n        """"""\n        score = torch.tensor([[1, 2, 3],\n                              [4, 5, 6],\n                              [7, 8, 9]])\n        score_mask_value = torch.tensor(-1)\n        memory_sequence_length = torch.tensor([1, 2, 3])\n        masked_score = maybe_mask_score(\n            score, score_mask_value, memory_sequence_length)\n\n        expected_score = [[1, -1, -1], [4, 5, -1], [7, 8, 9]]\n\n        self.assertEqual(masked_score.tolist(), expected_score)\n\n    def test_hardmax(self):\n        r""""""Tests `texar.torch.core.attention_mechanism_utils.hardmax`.\n        """"""\n        logits = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [9, 8, 7]],\n                               [[3, 2, 1], [6, 5, 4], [9, 8, 7], [4, 5, 6]]])\n        outputs = hardmax(logits)\n\n        expected_outputs = [[[0, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0]],\n                            [[1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1]]]\n\n        self.assertEqual(outputs.tolist(), expected_outputs)\n\n    def test_safe_cumprod(self):\n        r""""""Tests `texar.torch.core.attention_mechanism_utils.safe_cumprod`.\n        """"""\n        x = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5])\n        outputs = safe_cumprod(x, dim=0)\n\n        expected_outputs = [0.1, 0.02, 0.006, 0.0024, 0.0012]\n\n        outputs = outputs.tolist()\n        for i in range(5):\n            self.assertAlmostEqual(outputs[i], expected_outputs[i])\n\n    def test_sparsemax(self):\n        r""""""Tests `texar.torch.core.attention_mechanism_utils.sparsemax`.\n        """"""\n        logits = torch.tensor([[1.2, 3.2, 0.1, 4.3, 5.0],\n                               [3.3, 4.6, 9.5, 5.4, 8.7]])\n        outputs = sparsemax(logits)\n\n        expected_outputs = [[0.0, 0.0, 0.0, 0.1500001, 0.8499999],\n                            [0.0, 0.0, 0.8999996, 0.0, 0.09999943]]\n\n        outputs = outputs.tolist()\n        for i in range(2):\n            for j in range(5):\n                self.assertAlmostEqual(outputs[i][j],\n                                       expected_outputs[i][j],\n                                       places=5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/core/cell_wrappers_test.py,8,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests of :mod:`texar.torch.core.cell_wrappers` and\n:func:`~texar.torch.core.layers.get_rnn_cell`.\n""""""\n\nimport unittest\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.core import cell_wrappers as wrappers\nfrom texar.torch.core.layers import default_rnn_cell_hparams, get_rnn_cell\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.utils import utils\n\n\nclass WrappersTest(unittest.TestCase):\n    r""""""Tests cell wrappers and :func:`~texar.torch.core.layers.get_rnn_cell`.\n    """"""\n\n    def test_get_rnn_cell(self):\n        r""""""Tests the HParams class.\n        """"""\n        input_size = 10\n        hparams = {\n            \'type\': \'LSTMCell\',\n            \'kwargs\': {\n                \'num_units\': 20,\n                \'forget_bias\': 1.0,\n            },\n            \'num_layers\': 3,\n            \'dropout\': {\n                \'input_keep_prob\': 0.5,\n                \'output_keep_prob\': 0.5,\n                \'state_keep_prob\': 0.5,\n                \'variational_recurrent\': True\n            },\n            \'residual\': True,\n            \'highway\': True,\n        }\n        hparams = HParams(hparams, default_rnn_cell_hparams())\n\n        rnn_cell = get_rnn_cell(input_size, hparams)\n        self.assertIsInstance(rnn_cell, wrappers.MultiRNNCell)\n        self.assertEqual(len(rnn_cell._cell), hparams.num_layers)\n        self.assertEqual(rnn_cell.input_size, input_size)\n        self.assertEqual(rnn_cell.hidden_size, hparams.kwargs.num_units)\n\n        for idx, cell in enumerate(rnn_cell._cell):\n            layer_input_size = (input_size if idx == 0\n                                else hparams.kwargs.num_units)\n            self.assertEqual(cell.input_size, layer_input_size)\n            self.assertEqual(cell.hidden_size, hparams.kwargs.num_units)\n\n            if idx > 0:\n                highway = cell\n                residual = highway._cell\n                dropout = residual._cell\n                self.assertIsInstance(highway, wrappers.HighwayWrapper)\n                self.assertIsInstance(residual, wrappers.ResidualWrapper)\n            else:\n                dropout = cell\n            lstm = dropout._cell\n            builtin_lstm = lstm._cell\n            self.assertIsInstance(dropout, wrappers.DropoutWrapper)\n            self.assertIsInstance(lstm, wrappers.LSTMCell)\n            self.assertIsInstance(builtin_lstm, nn.LSTMCell)\n            h = hparams.kwargs.num_units\n            forget_bias = builtin_lstm.bias_ih[h:(2 * h)]\n            self.assertTrue((forget_bias == hparams.kwargs.forget_bias).all())\n\n            for key in [\'input\', \'output\', \'state\']:\n                self.assertEqual(getattr(dropout, f\'_{key}_keep_prob\'),\n                                 hparams.dropout[f\'{key}_keep_prob\'])\n            self.assertTrue(dropout._variational_recurrent)\n\n        batch_size = 8\n        seq_len = 6\n        state = None\n        for step in range(seq_len):\n            input = torch.zeros(batch_size, input_size)\n            output, state = rnn_cell(input, state)\n            self.assertEqual(\n                output.shape, (batch_size, hparams.kwargs.num_units))\n            self.assertEqual(len(state), hparams.num_layers)\n            utils.map_structure(lambda s: self.assertEqual(\n                s.shape, (batch_size, hparams.kwargs.num_units)), state)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/core/layers_test.py,26,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for various layers.\n""""""\nimport unittest\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom texar.torch.core import layers\n\n\nclass GetActivationFnTest(unittest.TestCase):\n    r""""""Tests :func:`texar.torch.core.layers.get_activation_fn`.\n    """"""\n\n    def test_get_activation_fn(self):\n        r""""""Tests.\n        """"""\n        fn = layers.get_activation_fn()\n        self.assertEqual(fn, None)\n\n        fn = layers.get_activation_fn(\'relu\')\n        self.assertEqual(fn, torch.relu)\n\n        inputs = torch.randn(64, 100)\n\n        fn = layers.get_activation_fn(\'leaky_relu\')\n        fn_output = fn(inputs)\n        ref_output = F.leaky_relu(inputs)\n        self.assertEqual(torch.all(torch.eq(fn_output, ref_output)), 1)\n\n        fn = layers.get_activation_fn(\'leaky_relu\',\n                                      kwargs={\'negative_slope\': 0.1})\n        fn_output = fn(inputs)\n        ref_output = F.leaky_relu(inputs, negative_slope=0.1)\n        self.assertEqual(torch.all(torch.eq(fn_output, ref_output)), 1)\n\n\nclass GetLayerTest(unittest.TestCase):\n    r""""""Tests layer creator.\n    """"""\n\n    def test_get_layer(self):\n        r""""""Tests :func:`texar.torch.core.layers.get_layer`.\n        """"""\n        hparams = {""type"": ""Conv1d"",\n                   ""kwargs"": {""in_channels"": 16,\n                              ""out_channels"": 32,\n                              ""kernel_size"": 2}\n                   }\n\n        layer = layers.get_layer(hparams)\n        self.assertIsInstance(layer, nn.Conv1d)\n\n        hparams = {\n            ""type"": ""MergeLayer"",\n            ""kwargs"": {\n                ""layers"": [\n                    {""type"": ""Conv1d"",\n                     ""kwargs"": {""in_channels"": 16,\n                                ""out_channels"": 32,\n                                ""kernel_size"": 2}},\n                    {""type"": ""Conv1d"",\n                     ""kwargs"": {""in_channels"": 16,\n                                ""out_channels"": 32,\n                                ""kernel_size"": 2}}\n                ]\n            }\n        }\n        layer = layers.get_layer(hparams)\n        self.assertIsInstance(layer, layers.MergeLayer)\n\n        hparams = {""type"": ""Conv1d"",\n                   ""kwargs"": {""in_channels"": 16,\n                              ""out_channels"": 32,\n                              ""kernel_size"": 2}\n                   }\n        layer = layers.get_layer(hparams)\n        self.assertIsInstance(layer, nn.Conv1d)\n\n        hparams = {\n            ""type"": nn.Conv1d(in_channels=16, out_channels=32, kernel_size=2)\n        }\n        layer = layers.get_layer(hparams)\n        self.assertIsInstance(layer, nn.Conv1d)\n\n\nclass ReducePoolingLayerTest(unittest.TestCase):\n    r""""""Tests reduce pooling layer.\n    """"""\n\n    def setUp(self):\n        unittest.TestCase.setUp(self)\n\n        self._batch_size = 64\n        self._emb_dim = 100\n        self._seq_length = 16\n\n    def test_max_reduce_pooling_layer(self):\n        r""""""Tests :class:`texar.torch.core.MaxReducePool1d`.""""""\n\n        pool_layer = layers.MaxReducePool1d()\n        inputs = torch.randn(self._batch_size, self._emb_dim, self._seq_length)\n        output = pool_layer(inputs)\n        output_reduce, _ = torch.max(inputs, dim=2)\n        self.assertEqual(output.shape, torch.Size([self._batch_size,\n                                                   self._emb_dim]))\n        self.assertEqual(torch.all(torch.eq(output, output_reduce)), 1)\n\n    def test_average_reduce_pooling_layer(self):\n        r""""""Tests :class:`texar.torch.core.AvgReducePool1d`.""""""\n\n        pool_layer = layers.AvgReducePool1d()\n        inputs = torch.randn(self._batch_size, self._emb_dim, self._seq_length)\n        output = pool_layer(inputs)\n        output_reduce = torch.mean(inputs, dim=2)\n        self.assertEqual(output.shape, torch.Size([self._batch_size,\n                                                   self._emb_dim]))\n        self.assertEqual(torch.all(torch.eq(output, output_reduce)), 1)\n\n\nclass MergeLayerTest(unittest.TestCase):\n    r""""""Tests MergeLayer.\n    """"""\n\n    def test_layer_logic(self):\n        r""""""Test the logic of MergeLayer.\n        """"""\n        layers_ = list()\n        layers_.append(nn.Conv1d(in_channels=32, out_channels=32,\n                                 kernel_size=3))\n        layers_.append(nn.Conv1d(in_channels=32, out_channels=32,\n                                 kernel_size=3))\n        layers_.append(nn.Conv1d(in_channels=32, out_channels=32,\n                                 kernel_size=3))\n\n        modes = [""concat"", ""sum"", ""mean"", ""prod"", ""max"", ""min"", ""logsumexp"",\n                 ""elemwise_sum"", ""elemwise_mul""]\n\n        for mode in modes:\n            m_layer = layers.MergeLayer(layers_, mode=mode)\n            input = torch.randn(32, 32, 10)\n            output = m_layer(input)\n\n            if mode == ""concat"":\n                self.assertEqual(output.shape, torch.Size([32, 32, 24]))\n            elif mode == ""elemwise_sum"" or mode == ""elemwise_mul"":\n                self.assertEqual(output.shape, torch.Size([32, 32, 8]))\n            else:\n                self.assertEqual(output.shape, torch.Size([32, 32]))\n\n        for mode in [""and"", ""or""]:\n            m_layer = layers.MergeLayer(layers=None, mode=mode)\n            input = torch.ones(32, 32, 10, dtype=torch.uint8)\n            output = m_layer(input)\n\n            self.assertEqual(output.shape, torch.Size([32, 32]))\n\n    def test_empty_merge_layer(self):\n        r""""""Test the output of MergeLayer with empty layers.\n        """"""\n        m_layer = layers.MergeLayer(layers=None)\n        input = torch.randn(32, 32, 10)\n        output = m_layer(input)\n        self.assertEqual(torch.all(torch.eq(output, input)), 1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/core/optimization_test.py,23,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for various optimization related utilities.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.core.optimization import *\n\n\nclass OptimizationTest(unittest.TestCase):\n    r""""""Test optimization.\n    """"""\n\n    def setUp(self):\n        N, D_in, H, D_out = 64, 100, 10, 1\n\n        self.x = torch.randn(N, D_in)\n        self.y = torch.randn(N, D_out)\n\n        self.model = torch.nn.Sequential(\n            torch.nn.Linear(D_in, H),\n            torch.nn.ReLU(),\n            torch.nn.Linear(H, D_out),)\n\n        self.loss_fn = torch.nn.MSELoss(reduction=\'sum\')\n\n    def test_get_optimizer(self):\n        r""""""Tests get_optimizer.\n        """"""\n        default_optimizer = get_optimizer(params=[torch.tensor(1)],\n                                          hparams=None)\n        self.assertIsInstance(default_optimizer, torch.optim.Adam)\n\n        hparams = {\n            ""optimizer"": {\n                ""type"": ""RMSprop"",\n                ""kwargs"": {\n                    ""lr"": 0.001,\n                    ""alpha"": 0.99,\n                    ""eps"": 1e-8,\n                    ""weight_decay"": 0,\n                    ""momentum"": 0,\n                    ""centered"": False\n                }\n            },\n            ""learning_rate_decay"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""gradient_clip"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""gradient_noise_scale"": None,\n            ""name"": None\n        }\n\n        rmsprop_optimizer = get_optimizer(params=[torch.tensor(1)],\n                                          hparams=hparams)\n        self.assertIsInstance(rmsprop_optimizer, torch.optim.RMSprop)\n\n        hparams = {\n            ""optimizer"": {\n                ""type"": torch.optim.SGD,\n                ""kwargs"": {\n                    ""lr"": 0.001,\n                    ""weight_decay"": 0,\n                    ""momentum"": 0\n                }\n            },\n            ""learning_rate_decay"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""gradient_clip"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""gradient_noise_scale"": None,\n            ""name"": None\n        }\n\n        sgd_optimizer = get_optimizer(params=[torch.tensor(1)],\n                                      hparams=hparams)\n        self.assertIsInstance(sgd_optimizer, torch.optim.SGD)\n\n    def test_get_scheduler(self):\n        r""""""Tests get_scheduler.\n        """"""\n        optimizer = get_optimizer(params=[torch.tensor(1)], hparams=None)\n\n        default_scheduler = get_scheduler(optimizer=optimizer,\n                                          hparams=None)\n        self.assertEqual(default_scheduler, None)\n\n        hparams = {\n            ""optimizer"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""learning_rate_decay"": {\n                ""type"": ""ExponentialLR"",\n                ""kwargs"": {\n                    ""gamma"": 0.99\n                }\n            },\n            ""gradient_clip"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""gradient_noise_scale"": None,\n            ""name"": None\n        }\n\n        scheduler = get_scheduler(optimizer=optimizer,\n                                  hparams=hparams)\n        self.assertIsInstance(scheduler, torch.optim.lr_scheduler.ExponentialLR)\n\n        hparams = {\n            ""optimizer"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""learning_rate_decay"": {\n                ""type"": torch.optim.lr_scheduler.ExponentialLR,\n                ""kwargs"": {\n                    ""gamma"": 0.99\n                }\n            },\n            ""gradient_clip"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""gradient_noise_scale"": None,\n            ""name"": None\n        }\n\n        scheduler = get_scheduler(optimizer=optimizer,\n                                  hparams=hparams)\n        self.assertIsInstance(scheduler, torch.optim.lr_scheduler.ExponentialLR)\n\n    def test_get_grad_clip_fn(self):\n        r""""""Tests get_grad_clip_fn.\n        """"""\n        default_grad_clip_fn = get_grad_clip_fn(hparams=None)\n        self.assertEqual(default_grad_clip_fn, None)\n\n        hparams = {\n            ""optimizer"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""learning_rate_decay"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""gradient_clip"": {\n                ""type"": ""clip_grad_norm_"",\n                ""kwargs"": {\n                    ""max_norm"": 10,\n                    ""norm_type"": 2\n                }\n            },\n            ""gradient_noise_scale"": None,\n            ""name"": None\n        }\n\n        grad_clip_fn = get_grad_clip_fn(hparams=hparams)\n        if not callable(grad_clip_fn):\n            raise ValueError(""grad_clip_fn is not callable"")\n\n        hparams = {\n            ""optimizer"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""learning_rate_decay"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""gradient_clip"": {\n                ""type"": torch.nn.utils.clip_grad_norm_,\n                ""kwargs"": {\n                    ""max_norm"": 10,\n                    ""norm_type"": 2\n                }\n            },\n            ""gradient_noise_scale"": None,\n            ""name"": None\n        }\n\n        grad_clip_fn = get_grad_clip_fn(hparams=hparams)\n        if not callable(grad_clip_fn):\n            raise ValueError(""grad_clip_fn is not callable"")\n\n    def test_get_train_op(self):\n        r""""""Tests get_train_op.\n        """"""\n        hparams = {\n            ""optimizer"": {\n                ""type"": torch.optim.SGD,\n                ""kwargs"": {\n                    ""lr"": 0.001\n                }\n            },\n            ""learning_rate_decay"": {\n                ""type"": torch.optim.lr_scheduler.ExponentialLR,\n                ""kwargs"": {\n                    ""gamma"": 0.99\n                }\n            },\n            ""gradient_clip"": {\n                ""type"": torch.nn.utils.clip_grad_norm_,\n                ""kwargs"": {\n                    ""max_norm"": 10,\n                    ""norm_type"": 2\n                }\n            },\n            ""gradient_noise_scale"": None,\n            ""name"": None\n        }\n\n        # Case 1\n        optimizer = get_optimizer(self.model.parameters(), hparams)\n        train_op = get_train_op(optimizer=optimizer, hparams=hparams)\n\n        for t in range(50):\n            y_pred = self.model(self.x)\n            loss = self.loss_fn(y_pred, self.y)\n            loss.backward()\n            train_op()\n\n        # Case 2\n        train_op = get_train_op(params=self.model.parameters(), hparams=hparams)\n\n        for t in range(50):\n            y_pred = self.model(self.x)\n            loss = self.loss_fn(y_pred, self.y)\n            loss.backward()\n            train_op()\n\n        # Case 3\n        optimizer = get_optimizer(self.model.parameters(), hparams)\n        scheduler = get_scheduler(optimizer=optimizer,\n                                  hparams=hparams)\n        train_op = get_train_op(scheduler=scheduler, hparams=hparams)\n\n        for t in range(50):\n            y_pred = self.model(self.x)\n            loss = self.loss_fn(y_pred, self.y)\n            loss.backward()\n            train_op()\n\n    def test_BertAdam(self):\n        r""""""Tests BertAdam.\n        """"""\n        optimizer = BertAdam(self.model.parameters())\n\n        for t in range(50):\n            y_pred = self.model(self.x)\n            loss = self.loss_fn(y_pred, self.y)\n            loss.backward()\n            optimizer.step()\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/core/regularizers_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for regularizers.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.core.regularizers import *\n\n\nclass RegularizerTest(unittest.TestCase):\n    r""""""Test regularizers.\n    """"""\n\n    def setUp(self):\n        self.x = torch.tensor([-1, 2, -3, 4, -5])\n        self.l1 = 0.1\n        self.l2 = 0.2\n\n    def test_l1(self):\n        r""""""Tests l1.""""""\n        regularizer = l1(self.l1)\n        self.assertEqual(regularizer.l1, 0.1)\n        self.assertEqual(regularizer.l2, 0)\n\n        regularization = regularizer(self.x)\n        self.assertEqual(regularization.item(), self.l1 * (1 + 2 + 3 + 4 + 5))\n\n        self.assertEqual(regularizer.get_config().get(""l1""), 0.1)\n        self.assertEqual(regularizer.get_config().get(""l2""), 0)\n\n    def test_l2(self):\n        r""""""Tests l2.""""""\n        regularizer = l2(self.l2)\n        self.assertEqual(regularizer.l1, 0)\n        self.assertEqual(regularizer.l2, 0.2)\n\n        regularization = regularizer(self.x)\n        self.assertEqual(regularization.item(), self.l2 * (1 + 4 + 9 + 16 + 25))\n\n        self.assertEqual(regularizer.get_config().get(""l1""), 0)\n        self.assertEqual(regularizer.get_config().get(""l2""), 0.2)\n\n    def test_l1_l2(self):\n        r""""""Tests l1_l2.""""""\n        regularizer = l1_l2(self.l1, self.l2)\n        self.assertEqual(regularizer.l1, 0.1)\n        self.assertEqual(regularizer.l2, 0.2)\n\n        regularization = regularizer(self.x)\n        self.assertEqual(regularization.item(), self.l1 * (1 + 2 + 3 + 4 + 5) +\n                         self.l2 * (1 + 4 + 9 + 16 + 25))\n\n        self.assertEqual(regularizer.get_config().get(""l1""), 0.1)\n        self.assertEqual(regularizer.get_config().get(""l2""), 0.2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/embedding_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for embedding related operations.\n""""""\n\nimport sys\nimport tempfile\nimport unittest\n\nimport numpy as np\n\nfrom texar.torch.data import embedding\n\nPy3 = sys.version_info[0] == 3\n\n\nclass EmbeddingTest(unittest.TestCase):\n    """"""Tests embedding related operations.\n    """"""\n\n    def test_load_glove(self):\n        """"""Tests the load_glove function.\n        """"""\n        word_vec_lines = [""word 1.2 3.4 5.6"", ""\xe8\xaf\x8d 1. 3. 5.""]\n        glove_file = tempfile.NamedTemporaryFile(mode=""w+"")\n        if Py3:\n            glove_file.write(\'\\n\'.join(word_vec_lines))\n        else:\n            glove_file.write(\'\\n\'.join(word_vec_lines).encode(""utf-8""))\n        glove_file.flush()\n        vocab = {""word"": 0, ""\xe8\xaf\x8d"": 1}\n        word_vecs = np.zeros([2, 3])\n\n        word_vecs = embedding.load_glove(glove_file.name, vocab, word_vecs)\n\n        self.assertEqual(word_vecs.shape[0], 2)\n        self.assertEqual(word_vecs.shape[1], 3)\n        np.testing.assert_array_equal(word_vecs[0], [1.2, 3.4, 5.6])\n        np.testing.assert_array_equal(word_vecs[1], [1., 3., 5.])\n\n    def test_load_word2vec(self):\n        """"""Tests the load_word2vec function.\n        """"""\n        header = ""2 3""\n        words = [""word"", ""\xe8\xaf\x8d""]\n        vec = np.array([1.2, 3.4, 5.6], dtype=\'float32\')\n        w2v_file = tempfile.NamedTemporaryFile()\n        w2v_file.write((header + ""\\n"").encode(\'utf-8\'))\n        for word in words:\n            w2v_file.write((word + "" "").encode(\'utf-8\'))\n            w2v_file.write(vec.tostring() + b\'\\n\')\n        w2v_file.flush()\n        vocab = {""word"": 0, ""\xe8\xaf\x8d"": 1}\n        word_vecs = np.zeros([2, 3])\n\n        word_vecs = embedding.load_word2vec(w2v_file.name, vocab, word_vecs)\n\n        self.assertEqual(word_vecs.shape[0], 2)\n        self.assertEqual(word_vecs.shape[1], 3)\n        np.testing.assert_array_equal(word_vecs[0], vec)\n        np.testing.assert_array_equal(word_vecs[1], vec)\n\n    def test_embedding(self):\n        """"""Tests :class:`texar.torch.data.embedding.Embedding`.\n        """"""\n        vocab = {""word"": 0, ""\xe8\xaf\x8d"": 1}\n        emb = embedding.Embedding(vocab)\n        self.assertEqual(len(emb.word_vecs), len(vocab))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/vocabulary_test.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for vocabulary related operations.\n""""""\nimport tempfile\nimport unittest\n\nfrom texar.torch.data import vocabulary\n\n\nclass VocabularyTest(unittest.TestCase):\n    """"""Tests vocabulary related operations.\n    """"""\n\n    def test_make_defaultdict(self):\n        """"""Tests the _make_defaultdict function.\n        """"""\n        keys = [\'word\', \'\xe8\xaf\x8d\']\n        values = [0, 1]\n        default_value = -1\n\n        dict_ = vocabulary._make_defaultdict(keys, values, default_value)\n\n        self.assertEqual(len(dict_), 2)\n        self.assertEqual(dict_[\'word\'], 0)\n        self.assertEqual(dict_[\'\xe8\xaf\x8d\'], 1)\n        self.assertEqual(dict_[\'sth_else\'], -1)\n\n    def test_vocab_construction(self):\n        """"""Test vocabulary construction.\n        """"""\n        vocab_list = [\'word\', \'\xe8\xaf\x8d\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n\n        vocab = vocabulary.Vocab(vocab_file.name)\n\n        self.assertEqual(vocab.size, len(vocab_list) + 4)\n        self.assertEqual(\n            set(vocab.token_to_id_map_py.keys()),\n            set([\'word\', \'\xe8\xaf\x8d\'] + vocab.special_tokens))\n\n        # Tests UNK token\n        unk_token_id = vocab.map_tokens_to_ids_py([\'new\'])\n        unk_token_text = vocab.map_ids_to_tokens_py(unk_token_id)\n        self.assertEqual(unk_token_text[0], vocab.unk_token)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/evals/bleu_moses_test.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for bleu_moses.\n""""""\n\nimport unittest\n\nimport numpy as np\n\nfrom texar.torch.evals.bleu_moses import sentence_bleu_moses, corpus_bleu_moses\n\n\nclass BLEUMosesTest(unittest.TestCase):\n    r""""""Tests bleu moses.\n    """"""\n\n    def _test_sentence_bleu(self, references, hypothesis, lowercase,\n                            true_bleu):\n        bleu = sentence_bleu_moses(references, hypothesis, lowercase=lowercase)\n\n        self.assertAlmostEqual(bleu, true_bleu, places=2)\n\n    def test_sentence_strings(self):\n        r""""""Tests hypothesis as strings.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        references = [""this is a test sentence to evaluate the bleu score .""]\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=67.03)\n\n    def test_sentence_list(self):\n        r""""""Tests hypothesis as a list of tokens.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        hypothesis = hypothesis.split()\n        references = [""this is a test sentence to evaluate the bleu score .""]\n        references = [references[0].split()]\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=67.03)\n\n    def test_sentence_multi_references(self):\n        r""""""Tests multiple references.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        references = [""this is a test sentence to evaluate the bleu score ."",\n                      ""this is a test sentence to evaluate the good score .""]\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=76.12)\n\n    def test_sentence_numpy(self):\n        r""""""Tests with numpy format.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        hypothesis = np.array(hypothesis.split())\n        references = [""this is a test sentence to evaluate the bleu score ."",\n                      ""this is a test sentence to evaluate the good score .""]\n        references = np.array([np.array(r.split()) for r in references])\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=76.12)\n\n    def _test_corpus_bleu(self, list_of_references, hypotheses, lowercase,\n                          return_all, true_bleu):\n        bleu = corpus_bleu_moses(list_of_references, hypotheses,\n                                 lowercase=lowercase, return_all=return_all)\n        if not return_all:\n            self.assertAlmostEqual(bleu, true_bleu, places=2)\n        else:\n            for ret, true in zip(bleu, true_bleu):\n                self.assertAlmostEqual(ret, true, places=2)\n\n    def test_corpus_strings(self):\n        r""""""Tests corpus level BLEU.\n        """"""\n        hypotheses = [\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d"",\n            ""i believe that that the script is \xe8\xaf\x8d perfectly correct .""\n        ]\n        list_of_references = [\n            [""this is a test sentence to evaluate the bleu score ."",\n             ""this is a test sentence to evaluate the good score .""],\n            [""i believe that the script is perfectly correct ."".split()]\n        ]\n        self._test_corpus_bleu(list_of_references, hypotheses,\n                               False, False, 63.02)\n\n        self._test_corpus_bleu(list_of_references, hypotheses,\n                               False, True, [63.02, 87.5, 77.3, 60.0, 38.9])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/evals/bleu_test.py,1,"b'""""""\nUnit tests for bleu.\n""""""\n\nimport unittest\n\nimport numpy as np\n\nfrom texar.torch.evals.bleu import sentence_bleu, corpus_bleu\n\n\nclass BLEUTest(unittest.TestCase):\n    r""""""Tests bleu.\n    """"""\n\n    def _test_sentence_bleu(self, references, hypothesis, lowercase,\n                            true_bleu):\n        bleu = sentence_bleu(references, hypothesis, lowercase=lowercase)\n        self.assertAlmostEqual(bleu, true_bleu, places=0)\n\n    def test_sentence_strings(self):\n        r""""""Tests hypothesis as strings.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        references = [""this is a test sentence to evaluate the bleu score .""]\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=67.03)\n\n    def test_sentence_list(self):\n        r""""""Tests hypothesis as a list of tokens.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        hypothesis = hypothesis.split()\n        references = [""this is a test sentence to evaluate the bleu score .""]\n        references = [references[0].split()]\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=67.03)\n\n    def test_sentence_multi_references(self):\n        r""""""Tests multiple references.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        references = [""this is a test sentence to evaluate the bleu score ."",\n                      ""this is a test sentence to evaluate the good score .""]\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=76.12)\n\n    def test_sentence_numpy(self):\n        r""""""Tests with numpy format.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        hypothesis = np.array(hypothesis.split())\n        references = [""this is a test sentence to evaluate the bleu score ."",\n                      ""this is a test sentence to evaluate the good score .""]\n        references = np.array([np.array(r.split()) for r in references])\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=76.12)\n\n    def _test_corpus_bleu(self, list_of_references, hypotheses, lowercase,\n                          return_all, true_bleu):\n        bleu = corpus_bleu(list_of_references, hypotheses,\n                           lowercase=lowercase, return_all=return_all)\n        if not return_all:\n            self.assertAlmostEqual(bleu, true_bleu, places=0)\n        else:\n            for ret, true in zip(bleu, true_bleu):\n                self.assertAlmostEqual(ret, true, places=0)\n\n    def test_corpus_strings(self):\n        r""""""Tests corpus level BLEU.\n        """"""\n        hypotheses = [\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d"",\n            ""i believe that that the script is \xe8\xaf\x8d perfectly correct .""\n        ]\n        list_of_references = [\n            [""this is a test sentence to evaluate the bleu score ."",\n             ""this is a test sentence to evaluate the good score .""],\n            [""i believe that the script is perfectly correct ."".split()]\n        ]\n        self._test_corpus_bleu(list_of_references, hypotheses,\n                               False, False, 63.02)\n\n        self._test_corpus_bleu(list_of_references, hypotheses,\n                               False, True, [63.02, 87.5, 77.3, 60.0, 38.9])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/evals/bleu_transformer_test.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for bleu_tool.\n""""""\n\nimport unittest\n\nimport tempfile\n\nfrom texar.torch.evals.bleu_transformer import (\n    bleu_transformer_tokenize, file_bleu)\n\n\nclass BLEUToolTest(unittest.TestCase):\n    r""""""Test bleu_tool.\n    """"""\n\n    def _create_temp_file(self, text):\n        temp_file = tempfile.NamedTemporaryFile(delete=False)\n        with open(temp_file.name, ""w"") as f:\n            f.write(text)\n        return temp_file.name\n\n    def test_bleu_same(self):\n        ref = self._create_temp_file(""test 1 two 3\\nmore tests!"")\n        hyp = self._create_temp_file(""test 1 two 3\\nmore tests!"")\n\n        uncased_score = file_bleu(ref, hyp, case_sensitive=False)\n        cased_score = file_bleu(ref, hyp, case_sensitive=True)\n        self.assertEqual(100, uncased_score)\n        self.assertEqual(100, cased_score)\n\n    def test_bleu_same_different_case(self):\n        ref = self._create_temp_file(""Test 1 two 3\\nmore tests!"")\n        hyp = self._create_temp_file(""test 1 two 3\\nMore tests!"")\n        uncased_score = file_bleu(ref, hyp, case_sensitive=False)\n        cased_score = file_bleu(ref, hyp, case_sensitive=True)\n        self.assertEqual(100, uncased_score)\n        self.assertLess(cased_score, 100)\n\n    def test_bleu_different(self):\n        ref = self._create_temp_file(""Testing\\nmore tests!"")\n        hyp = self._create_temp_file(""Dog\\nCat"")\n        uncased_score = file_bleu(ref, hyp, case_sensitive=False)\n        cased_score = file_bleu(ref, hyp, case_sensitive=True)\n        self.assertLess(uncased_score, 100)\n        self.assertLess(cased_score, 100)\n\n    def test_bleu_tokenize(self):\n        s = ""Test0, 1 two, 3""\n        tokenized = bleu_transformer_tokenize(s)\n        self.assertEqual([""Test0"", "","", ""1"", ""two"", "","", ""3""], tokenized)\n\n    def test_bleu_version(self):\n        ref = self._create_temp_file(""Test 1 two 3\\nmore tests!"")\n        hyp = self._create_temp_file(""test 1 two 3\\nMore tests!"")\n        uncased_score = file_bleu(\n            ref, hyp, bleu_version=""corpus_bleu"", case_sensitive=False)\n        cased_score = file_bleu(\n            ref, hyp, bleu_version=""corpus_bleu"", case_sensitive=True)\n        self.assertEqual(100, uncased_score)\n        self.assertLess(cased_score, 100)\n\n        uncased_score = file_bleu(\n            ref, hyp, bleu_version=""corpus_bleu_moses"", case_sensitive=False)\n        cased_score = file_bleu(\n            ref, hyp, bleu_version=""corpus_bleu_moses"", case_sensitive=True)\n        self.assertEqual(100, uncased_score)\n        self.assertLess(cased_score, 100)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/evals/metrics_test.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for metrics.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.evals import metrics\n\n\nclass MetricsTest(unittest.TestCase):\n    r""""""Tests metrics.\n    """"""\n\n    def test_accuracy(self):\n        r""""""Tests :meth:`~texar.torch.evals.accuracy`.\n        """"""\n        labels = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\n        preds = torch.tensor([1.0, 2.1, 3.0, 4, 5.2, 6, 8, 8])\n        accuracy = metrics.accuracy(labels, preds)\n        self.assertEqual(accuracy, 0.625)\n\n    def test_binary_clas_accuracy(self):\n        r""""""Tests :meth:`~texar.torch.evals.binary_clas_accuracy\n        """"""\n        pos_preds = torch.tensor([1, 1, 0, 0, 0])\n        neg_preds = torch.tensor([1, 1, 0, 0, 0])\n\n        accuracy = metrics.binary_clas_accuracy(pos_preds, neg_preds)\n        self.assertEqual(accuracy, 0.5)\n\n        accuracy = metrics.binary_clas_accuracy(pos_preds, None)\n        self.assertEqual(accuracy, 0.4)\n\n        accuracy = metrics.binary_clas_accuracy(None, neg_preds)\n        self.assertEqual(accuracy, 0.6)\n\n        accuracy = metrics.binary_clas_accuracy(None, None)\n        self.assertEqual(accuracy, None)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/losses/adv_losses_test.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for adv_losses.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.losses.adv_losses import binary_adversarial_losses\n\n\nclass AdvLossesTest(unittest.TestCase):\n    """"""Tests adversarial losses.\n    """"""\n\n    def test_binary_adversarial_losses(self):\n        """"""Tests :meth:`~texar.torch.losses.adv_losses.binary_adversarial_losse`.\n        """"""\n        batch_size = 16\n        data_dim = 64\n        real_data = torch.zeros(size=(batch_size, data_dim),\n                                dtype=torch.float32)\n        fake_data = torch.ones(size=(batch_size, data_dim),\n                               dtype=torch.float32)\n        const_logits = torch.zeros(size=(batch_size,), dtype=torch.float32)\n        # Use a dumb discriminator that always outputs logits=0.\n        gen_loss, disc_loss = binary_adversarial_losses(\n            real_data, fake_data, lambda x: const_logits)\n        gen_loss_2, disc_loss_2 = binary_adversarial_losses(\n            real_data, fake_data, lambda x: const_logits, mode=""min_fake"")\n\n        self.assertAlmostEqual(gen_loss.item(), -gen_loss_2.item())\n        self.assertAlmostEqual(disc_loss.item(), disc_loss_2.item())\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/losses/entropy_test.py,11,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for entropy.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.losses import entropy\nfrom texar.torch.utils.shapes import get_rank\n\n\nclass EntropyTest(unittest.TestCase):\n    """"""Tests entropy.\n    """"""\n\n    def setUp(self):\n        self._batch_size = 64\n        self._max_time = 128\n        self._d = 16\n        self._distribution_dim = 32\n        self._logits = torch.rand(self._batch_size, self._d,\n                                  self._distribution_dim)\n        self._sequence_logits = torch.rand(\n            self._batch_size, self._max_time, self._d,\n            self._distribution_dim)\n        self._sequence_length = torch.randint(\n            high=self._max_time, size=(self._batch_size,))\n\n    def _test_entropy(self, entropy_fn, logits, sequence_length=None):\n        if sequence_length is None:\n            entropy = entropy_fn(logits)\n            rank = get_rank(entropy)\n            self.assertEqual(rank, 0)\n\n            entropy = entropy_fn(logits, average_across_batch=False)\n            rank = get_rank(entropy)\n            self.assertEqual(rank, 1)\n            self.assertEqual(entropy.shape, torch.Size([self._batch_size]))\n        else:\n            entropy = entropy_fn(logits, sequence_length=sequence_length)\n            rank = get_rank(entropy)\n            self.assertEqual(rank, 0)\n\n            entropy = entropy_fn(logits, sequence_length=sequence_length,\n                                 sum_over_timesteps=False)\n            rank = get_rank(entropy)\n            self.assertEqual(rank, 1)\n            self.assertEqual(entropy.shape, torch.Size([self._max_time]))\n\n            entropy = entropy_fn(logits, sequence_length=sequence_length,\n                                 sum_over_timesteps=False,\n                                 average_across_timesteps=True,\n                                 average_across_batch=False)\n            rank = get_rank(entropy)\n            self.assertEqual(rank, 1)\n            self.assertEqual(entropy.shape, torch.Size([self._batch_size]))\n\n            entropy = entropy_fn(logits, sequence_length=sequence_length,\n                                 sum_over_timesteps=False,\n                                 average_across_batch=False)\n            rank = get_rank(entropy)\n            self.assertEqual(rank, 2)\n            self.assertEqual(entropy.shape, torch.Size([self._batch_size,\n                                                        self._max_time]))\n\n            sequence_length_time = torch.randint(size=(self._max_time,),\n                                                 high=self._batch_size)\n            entropy = entropy_fn(logits,\n                                 sequence_length=sequence_length_time,\n                                 sum_over_timesteps=False,\n                                 average_across_batch=False,\n                                 time_major=True)\n            self.assertEqual(entropy.shape, torch.Size([self._batch_size,\n                                                        self._max_time]))\n\n    def test_entropy_with_logits(self):\n        """"""Tests `entropy_with_logits`\n        """"""\n        self._test_entropy(\n            entropy.entropy_with_logits, self._logits)\n\n    def test_sequence_entropy_with_logits(self):\n        """"""Tests `sequence_entropy_with_logits`\n        """"""\n        self._test_entropy(\n            entropy.sequence_entropy_with_logits, self._sequence_logits,\n            sequence_length=self._sequence_length)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/losses/mle_losses_test.py,17,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for mle losses.\n""""""\n\nimport unittest\n\nimport torch\nimport torch.nn.functional as F\n\nfrom texar.torch.losses import mle_losses\nfrom texar.torch.utils.shapes import get_rank\n\n\nclass MLELossesTest(unittest.TestCase):\n    """"""Tests mle losses.\n    """"""\n\n    def setUp(self):\n        self._batch_size = 64\n        self._max_time = 16\n        self._num_classes = 100\n        self._labels = torch.randint(\n            self._num_classes, (self._batch_size, self._max_time),\n            dtype=torch.int64)\n        one_hot = torch.eye(self._num_classes)\n        one_hot_labels = F.embedding(self._labels, one_hot)\n        self._one_hot_labels = torch.reshape(\n            one_hot_labels, [self._batch_size, self._max_time, -1])\n        self._logits = torch.rand(\n            self._batch_size, self._max_time, self._num_classes)\n        self._sequence_length = torch.randint(\n            high=self._max_time, size=(self._batch_size,), dtype=torch.int64)\n\n    def _test_sequence_loss(self, loss_fn, labels, logits, sequence_length):\n        loss = loss_fn(labels, logits, sequence_length)\n        rank = get_rank(loss)\n        self.assertEqual(rank, 0)\n\n        loss = loss_fn(labels, logits, sequence_length,\n                       sum_over_timesteps=False)\n        rank = get_rank(loss)\n        self.assertEqual(rank, 1)\n        self.assertEqual(loss.shape, torch.Size([self._max_time]))\n\n        loss = loss_fn(\n            labels, logits, sequence_length, sum_over_timesteps=False,\n            average_across_timesteps=True, average_across_batch=False)\n        rank = get_rank(loss)\n        self.assertEqual(rank, 1)\n        self.assertEqual(loss.shape, torch.Size([self._batch_size]))\n\n        loss = loss_fn(\n            labels, logits, sequence_length, sum_over_timesteps=False,\n            average_across_batch=False)\n        rank = get_rank(loss)\n        self.assertEqual(rank, 2)\n        self.assertEqual(loss.shape, torch.Size([self._batch_size,\n                                                 self._max_time]))\n\n        sequence_length_time = torch.randint(size=(self._max_time,),\n                                             high=self._batch_size)\n        loss = loss_fn(\n            labels, logits, sequence_length_time, sum_over_timesteps=False,\n            average_across_batch=False, time_major=True)\n        self.assertEqual(loss.shape, torch.Size([self._batch_size,\n                                                 self._max_time]))\n\n    def test_sequence_softmax_cross_entropy(self):\n        """"""Tests `sequence_softmax_cross_entropy`\n        """"""\n        self._test_sequence_loss(\n            mle_losses.sequence_softmax_cross_entropy,\n            self._one_hot_labels, self._logits, self._sequence_length)\n\n    def test_sequence_sparse_softmax_cross_entropy(self):\n        """"""Tests `sequence_sparse_softmax_cross_entropy`\n        """"""\n        self._test_sequence_loss(\n            mle_losses.sequence_sparse_softmax_cross_entropy,\n            self._labels, self._logits, self._sequence_length)\n\n    def test_sequence_sigmoid_cross_entropy(self):\n        """"""Tests `texar.torch.losses.sequence_sigmoid_cross_entropy`.\n        """"""\n        self._test_sequence_loss(\n            mle_losses.sequence_sigmoid_cross_entropy,\n            self._one_hot_labels, self._logits, self._sequence_length)\n\n        self._test_sequence_loss(\n            mle_losses.sequence_sigmoid_cross_entropy,\n            self._one_hot_labels[:, :, 0],\n            self._logits[:, :, 0],\n            self._sequence_length)\n\n        loss = mle_losses.sequence_sigmoid_cross_entropy(\n            logits=self._logits[:, :, 0],\n            labels=torch.ones([self._batch_size, self._max_time]),\n            sequence_length=self._sequence_length)\n        rank = get_rank(loss)\n        self.assertEqual(rank, 0)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/losses/pg_losses_test.py,19,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for pg losses.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.losses.pg_losses import pg_loss_with_logits\nfrom texar.torch.utils.shapes import get_rank\n\n\nclass PGLossesTest(unittest.TestCase):\n    """"""Tests pg losses\n    """"""\n\n    def setUp(self):\n        self._batch_size = 64\n        self._max_time = 16\n        self._d1 = 3  # use smaller values to speedup testing\n        self._d2 = 4\n        self._d3 = 5\n        self._num_classes = 10\n        self._actions_batch = torch.ones(\n            self._batch_size, self._max_time, self._d1, self._d2, self._d3,\n            dtype=torch.int64)\n        self._logits_batch = torch.rand(\n            self._batch_size, self._max_time, self._d1, self._d2, self._d3,\n            self._num_classes)\n        self._advantages_batch = torch.rand(\n            self._batch_size, self._max_time, self._d1, self._d2, self._d3)\n        self._actions_no_batch = torch.ones(\n            self._max_time, self._d1, self._d2, self._d3, dtype=torch.int64)\n        self._logits_no_batch = torch.rand(\n            self._max_time, self._d1, self._d2, self._d3, self._num_classes)\n        self._advantages_no_batch = torch.rand(\n            self._max_time, self._d1, self._d2, self._d3)\n        self._sequence_length = torch.randint(\n            high=self._max_time, size=(self._batch_size,))\n\n    def _test_sequence_loss(self, loss_fn, actions, logits, advantages, batched,\n                            sequence_length):\n        loss = loss_fn(actions, logits, advantages, batched=batched,\n                       sequence_length=sequence_length)\n        rank = get_rank(loss)\n        self.assertEqual(rank, 0)\n\n        loss = loss_fn(actions, logits, advantages, batched=batched,\n                       sequence_length=sequence_length,\n                       sum_over_timesteps=False)\n        rank = get_rank(loss)\n        self.assertEqual(rank, 1)\n        self.assertEqual(loss.shape, torch.Size([self._max_time]))\n\n        loss = loss_fn(actions, logits, advantages, batched=batched,\n                       sequence_length=sequence_length,\n                       sum_over_timesteps=False,\n                       average_across_timesteps=True,\n                       average_across_batch=False)\n        rank = get_rank(loss)\n        if batched:\n            self.assertEqual(rank, 1)\n            self.assertEqual(loss.shape, torch.Size([self._batch_size]))\n        else:\n            self.assertEqual(rank, 0)\n\n        loss = loss_fn(actions, logits, advantages, batched=batched,\n                       sequence_length=sequence_length,\n                       sum_over_timesteps=False,\n                       average_across_batch=False)\n        rank = get_rank(loss)\n        if batched:\n            self.assertEqual(rank, 2)\n            self.assertEqual(loss.shape,\n                             torch.Size([self._batch_size, self._max_time]))\n        else:\n            self.assertEqual(rank, 1)\n            self.assertEqual(loss.shape,\n                             torch.Size([self._max_time]))\n\n        sequence_length_time = torch.randint(\n            high=self._batch_size, size=(self._max_time,))\n        loss = loss_fn(actions, logits, advantages, batched=batched,\n                       sequence_length=sequence_length_time,\n                       sum_over_timesteps=False,\n                       average_across_batch=False,\n                       time_major=True)\n        if batched:\n            self.assertEqual(loss.shape, torch.Size([self._batch_size,\n                                                     self._max_time]))\n        else:\n            self.assertEqual(loss.shape, torch.Size([self._max_time]))\n\n    def test_pg_loss_with_logits(self):\n        """"""Tests `texar.torch.losses.pg_loss_with_logits`.\n        """"""\n        self._test_sequence_loss(\n            pg_loss_with_logits,\n            self._actions_batch, self._logits_batch,\n            self._advantages_batch, True, self._sequence_length)\n\n        self._test_sequence_loss(\n            pg_loss_with_logits,\n            self._actions_no_batch, self._logits_no_batch,\n            self._advantages_no_batch, False, self._sequence_length)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/losses/rewards_test.py,12,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for RL rewards.\n""""""\n\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.losses import rewards\n\n\nclass RewardTest(unittest.TestCase):\n    """"""Tests reward related functions.\n    """"""\n\n    def test_discount_reward(self):\n        """"""Tests :func:`texar.torch.losses.rewards.discount_reward`\n        """"""\n        # 1D\n        reward = torch.ones(2)\n        sequence_length = torch.tensor([3, 5])\n\n        discounted_reward = rewards.discount_reward(\n            reward, sequence_length, discount=1.)\n        discounted_reward_n = rewards.discount_reward(\n            reward, sequence_length, discount=.1, normalize=True)\n\n        discounted_reward_ = rewards.discount_reward(\n            reward, sequence_length, discount=1.)\n        discounted_reward_n_ = rewards.discount_reward(\n            reward, sequence_length, discount=.1, normalize=True)\n\n        np.testing.assert_array_almost_equal(\n            discounted_reward, discounted_reward_, decimal=6)\n\n        np.testing.assert_array_almost_equal(\n            discounted_reward_n, discounted_reward_n_, decimal=6)\n\n        # 2D\n        reward = torch.ones(2, 10)\n        sequence_length = torch.tensor([5, 10])\n\n        discounted_reward = rewards.discount_reward(\n            reward, sequence_length, discount=1.)\n        discounted_reward_n = rewards.discount_reward(\n            reward, sequence_length, discount=.1, normalize=True)\n\n        discounted_reward_ = rewards.discount_reward(\n            reward, sequence_length, discount=1.)\n        discounted_reward_n_ = rewards.discount_reward(\n            reward, sequence_length, discount=.1, normalize=True)\n\n        np.testing.assert_array_almost_equal(\n            discounted_reward, discounted_reward_, decimal=6)\n\n        np.testing.assert_array_almost_equal(\n            discounted_reward_n, discounted_reward_n_, decimal=6)\n\n    def test_discount_reward_tensor_1d(self):\n        """"""Tests :func:`texar.torch.losses.rewards._discount_reward_tensor_1d`\n        """"""\n        reward = torch.ones(2)\n        sequence_length = torch.tensor([3, 5])\n\n        discounted_reward_1 = rewards._discount_reward_tensor_1d(\n            reward, sequence_length, discount=1.)\n\n        discounted_reward_2 = rewards._discount_reward_tensor_1d(\n            reward, sequence_length, discount=.1)\n\n        for i in range(5):\n            if i < 3:\n                self.assertEqual(discounted_reward_1[0, i], 1)\n            else:\n                self.assertEqual(discounted_reward_1[0, i], 0)\n            self.assertEqual(discounted_reward_1[1, i], 1)\n\n        for i in range(5):\n            if i < 3:\n                self.assertAlmostEqual(discounted_reward_2[0, i].item(),\n                                       0.1 ** (2 - i))\n            else:\n                self.assertAlmostEqual(discounted_reward_2[0, i].item(), 0)\n            self.assertAlmostEqual(discounted_reward_2[1, i].item(),\n                                   0.1 ** (4 - i))\n\n    def test_discount_reward_tensor_2d(self):\n        """"""Tests :func:`texar.torch.losses.rewards._discount_reward_tensor_2d`\n        """"""\n        reward = torch.ones(2, 10)\n        sequence_length = torch.tensor([5, 10])\n\n        discounted_reward_1 = rewards._discount_reward_tensor_2d(\n            reward, sequence_length, discount=1.)\n\n        discounted_reward_2 = rewards._discount_reward_tensor_2d(\n            reward, sequence_length, discount=.1)\n\n        for i in range(10):\n            if i < 5:\n                self.assertEqual(discounted_reward_1[0, i].item(), 5 - i)\n            else:\n                self.assertEqual(discounted_reward_1[0, i].item(), 0)\n            self.assertEqual(discounted_reward_1[1, i].item(), 10 - i)\n\n        for i in range(10):\n            if i < 5:\n                self.assertAlmostEqual(discounted_reward_2[0, i].item(),\n                                       int(11111. / 10 ** i) / 10 ** (4 - i),\n                                       places=6)\n            else:\n                self.assertAlmostEqual(discounted_reward_2[0, i].item(), 0)\n            self.assertAlmostEqual(discounted_reward_2[1, i].item(),\n                                   int(1111111111. / 10 ** i) / 10 ** (9 - i),\n                                   places=6)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/run/condition_test.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for condition related operations.\n""""""\nimport unittest\n\nimport torch\n\nimport texar.torch as tx\nfrom texar.torch.run import condition as cond\nfrom tests.run.executor_test import DummyClassifier, DummyData\n\n\nclass ConditionTest(unittest.TestCase):\n    def _create_dataset(self, n_examples: int):\n        data = torch.randint(self.vocab_size, size=(n_examples, 20))\n        labels = torch.randint(self.n_classes, size=(n_examples,)).tolist()\n        source = tx.data.SequenceDataSource(list(zip(data, labels)))\n        dataset = DummyData(source, hparams={""batch_size"": 10})\n        return dataset\n\n    def setUp(self) -> None:\n        self.vocab_size = 100\n        self.n_classes = 5\n        self.model = DummyClassifier(self.vocab_size, self.n_classes)\n        self.datasets = {\n            split: self._create_dataset(n_examples)\n            for split, n_examples in [\n                (""train"", 200), (""valid"", 50), (""test1"", 50), (""test2"", 20)]}\n\n    def test_once(self):\n        num_iters = 2\n        max_triggers = 3\n\n        test = self\n\n        class Recorder:\n            def __init__(self):\n                self.count = 0\n                self.once_triggered = False\n\n            def every_time_fn(self, executor):\n                self.count += 1\n                test.assertEqual(\n                    executor.status[""iteration""], self.count * num_iters)\n                if self.count > max_triggers:\n                    test.fail(""Training should terminate but didn\'t"")\n                elif self.count == max_triggers:\n                    executor.terminate()\n\n            def once_fn(self, _):\n                test.assertFalse(self.once_triggered)\n                self.once_triggered = True\n\n        recorder = Recorder()\n\n        executor = tx.run.Executor(\n            model=self.model,\n            train_data=self.datasets[""train""],\n            valid_data=self.datasets[""valid""],\n            test_data=[self.datasets[""test1""], (""t2"", self.datasets[""test2""])],\n            optimizer={""type"": torch.optim.Adam}\n        )\n\n        executor.on(cond.iteration(num_iters), recorder.every_time_fn)\n        executor.on(cond.once(cond.iteration(num_iters)), recorder.once_fn)\n\n        executor.train()\n'"
tests/run/executor_test.py,15,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for executor related operations.\n""""""\nimport shutil\nimport tempfile\nimport unittest\nfrom pathlib import Path\nimport os\nfrom typing import List, Dict, Tuple\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport texar.torch as tx\nfrom texar.torch.run import *\n\n\nclass DummyClassifier(nn.Module):\n    def __init__(self, vocab_size: int, n_classes: int):\n        super().__init__()\n        self.embedder = tx.modules.WordEmbedder(\n            vocab_size=vocab_size, hparams={""dim"": 10})\n        self.encoder = tx.modules.BidirectionalRNNEncoder(\n            input_size=10, hparams={\n                ""rnn_cell_fw"": {""kwargs"": {""num_units"": 256}},\n            })\n        self.linear = nn.Linear(sum(self.encoder.output_size), n_classes)\n\n    def _compute_logits(self, tokens: torch.LongTensor) -> torch.Tensor:\n        embeds = self.embedder(tokens)\n        fw_state, bw_state = self.encoder(embeds)[1]\n        state = torch.cat([fw_state[0], bw_state[0]], dim=1)\n        logits = self.linear(state)\n        return logits\n\n    def forward(self,  # type: ignore\n                batch: tx.data.Batch) -> Dict[str, torch.Tensor]:\n        logits = self._compute_logits(batch.tokens)\n        loss = F.cross_entropy(logits, batch.label)\n        preds = torch.argmax(logits, dim=1)\n        return {""loss"": loss, ""preds"": preds}\n\n    def predict(self, batch: tx.data.Batch) -> Dict[str, torch.Tensor]:\n        logits = self._compute_logits(batch.tokens)\n        preds = torch.argmax(logits, dim=1)\n        return {""preds"": preds}\n\n\nExample = Tuple[torch.LongTensor, int]\n\n\nclass DummyData(tx.data.DatasetBase[Example, Example]):\n    def process(self, raw_example: Example) -> Example:\n        return raw_example\n\n    def collate(self, examples: List[Example]) -> tx.data.Batch:\n        tokens = torch.stack([x for x, _ in examples], dim=0).to(self.device)\n        labels = torch.tensor([x for _, x in examples], device=self.device)\n        return tx.data.Batch(len(examples), tokens=tokens, label=labels)\n\n\nclass ExecutorTest(unittest.TestCase):\n    def _create_dataset(self, n_examples: int):\n        data = torch.randint(self.vocab_size, size=(n_examples, 20))\n        labels = torch.randint(self.n_classes, size=(n_examples,)).tolist()\n        source = tx.data.SequenceDataSource(list(zip(data, labels)))\n        dataset = DummyData(source, hparams={""batch_size"": 10})\n        return dataset\n\n    def setUp(self) -> None:\n        make_deterministic()\n        self.vocab_size = 100\n        self.n_classes = 5\n        self.model = DummyClassifier(self.vocab_size, self.n_classes)\n        self.datasets = {\n            split: self._create_dataset(n_examples)\n            for split, n_examples in [\n                (""train"", 200), (""valid"", 50), (""test1"", 50), (""test2"", 20)]}\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.tbx_logging_dir = tempfile.mkdtemp()\n\n    def tearDown(self) -> None:\n        shutil.rmtree(self.checkpoint_dir)\n        shutil.rmtree(self.tbx_logging_dir)\n\n    def test_train_loop(self):\n        optimizer = torch.optim.Adam(self.model.parameters())\n        executor = Executor(\n            model=self.model,\n            train_data=self.datasets[""train""],\n            valid_data=self.datasets[""valid""],\n            test_data=[self.datasets[""test1""], (""t2"", self.datasets[""test2""])],\n            test_mode=\'eval\',\n            checkpoint_dir=self.checkpoint_dir,\n            max_to_keep=3,\n            save_every=[cond.time(seconds=10), cond.validation(better=True)],\n            train_metrics=[(""loss"", metric.RunningAverage(20)),\n                           metric.F1(pred_name=""preds"", mode=""macro""),\n                           metric.Accuracy(pred_name=""preds""),\n                           metric.LR(optimizer)],\n            optimizer=optimizer,\n            stop_training_on=cond.epoch(10),\n            valid_metrics=[metric.F1(pred_name=""preds"", mode=""micro""),\n                           (""loss"", metric.Average())],\n            validate_every=[cond.epoch()],\n            test_metrics=[metric.F1(pred_name=""preds"", mode=""weighted"")],\n            plateau_condition=[\n                cond.consecutive(cond.validation(better=False), 2)],\n            action_on_plateau=[action.early_stop(patience=2),\n                               action.reset_params(),\n                               action.scale_lr(0.8)],\n            log_every=cond.iteration(20),\n            show_live_progress=True,\n        )\n\n        executor.train()\n        executor.test()\n\n        executor.save()\n        executor.load()\n\n    def test_tbx_logging(self):\n        executor = Executor(\n            model=self.model,\n            train_data=self.datasets[""train""],\n            valid_data=self.datasets[""valid""],\n            test_data=[self.datasets[""test1""], (""t2"", self.datasets[""test2""])],\n            test_mode=\'eval\',\n            tbx_logging_dir=self.tbx_logging_dir,\n            tbx_log_every=cond.iteration(1),\n            checkpoint_dir=self.checkpoint_dir,\n            max_to_keep=3,\n            save_every=[cond.time(seconds=10), cond.validation(better=True)],\n            train_metrics=[(""loss"", metric.RunningAverage(20)),\n                           metric.F1(pred_name=""preds"", mode=""macro""),\n                           metric.Accuracy(pred_name=""preds"")],\n            optimizer={""type"": torch.optim.Adam, ""kwargs"": {}},\n            stop_training_on=cond.epoch(10),\n            valid_metrics=[metric.F1(pred_name=""preds"", mode=""micro""),\n                           (""loss"", metric.Average())],\n            validate_every=[cond.epoch()],\n            test_metrics=[metric.F1(pred_name=""preds"", mode=""weighted"")],\n            plateau_condition=[\n                cond.consecutive(cond.validation(better=False), 2)],\n            action_on_plateau=[action.early_stop(patience=2),\n                               action.reset_params(),\n                               action.scale_lr(0.8)],\n            log_every=cond.iteration(20),\n            show_live_progress=True,\n        )\n\n        executor.train()\n        path = Path(self.tbx_logging_dir)\n        self.assertTrue(path.exists())\n        self.assertEqual(len(list(os.walk(path))), 1)\n\n\nif __name__ == ""__main__"":\n    test = ExecutorTest()\n    try:\n        test.setUp()\n        test.test_train_loop()\n    finally:\n        test.tearDown()\n'"
tests/utils/average_recorder_test.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for average recoder.\n""""""\n\nimport unittest\n\nfrom texar.torch.utils.average_recorder import _SingleAverageRecorder, AverageRecorder\n\n\nclass AverageRecorderTest(unittest.TestCase):\n    r""""""Tests average recoder.\n    """"""\n\n    def test_single_average_recoder(self):\n        r""""""Tests :class:`~texar.torch.utils._SingleAverageRecorder`\n        """"""\n        recoder = _SingleAverageRecorder(5)\n        for i in range(100):\n            self.assertEqual(recoder.add(1), 1.)\n            self.assertEqual(recoder.avg(), 1.)\n\n        recoder = _SingleAverageRecorder()\n        for i in range(100):\n            self.assertEqual(recoder.add(1), 1.)\n            self.assertEqual(recoder.avg(), 1.)\n\n        def _cal_ground_truth(n):\n            r""""""Calculates ((n-4)^2 + ... + n^5) / (n-4 + ... + n)\n            """"""\n            lb = max(n - 4, 0)\n            _sum = 0\n            _w = 0\n            for i in range(lb, n + 1):\n                _sum += i * i\n                _w += i\n            if _w == 0:\n                return 0\n            return _sum / _w\n\n        recoder = _SingleAverageRecorder(5)\n        for i in range(100):\n            self.assertEqual(recoder.add(i, i), _cal_ground_truth(i))\n            self.assertEqual(recoder.avg(), _cal_ground_truth(i))\n\n    def test_average_recorder(self):\n        r""""""Tests :class:`~texar.torch.utils.AverageRecorder`\n        """"""\n        recorder = AverageRecorder(5)\n        for _ in range(100):\n            self.assertEqual(recorder.add([1., 2.]), [1., 2.])\n            self.assertEqual(recorder.add([1.]), [1., 2.])\n            self.assertEqual(recorder.avg(), [1., 2.])\n            self.assertEqual(recorder.avg(0), 1.)\n            self.assertEqual(recorder.avg(1), 2.)\n            self.assertEqual(recorder.avg([0, 1]), [1., 2.])\n\n        recorder = AverageRecorder()\n        for _ in range(100):\n            self.assertEqual(recorder.add({\'1\': 1, \'2\': 2}), {\'1\': 1., \'2\': 2.})\n            self.assertEqual(recorder.add({\'1\': 1}), {\'1\': 1., \'2\': 2.})\n            self.assertEqual(recorder.avg(), {\'1\': 1., \'2\': 2.})\n            self.assertEqual(recorder.avg(\'1\'), 1.)\n            self.assertEqual(recorder.avg(\'2\'), 2.)\n            self.assertEqual(recorder.avg([\'1\', \'2\']), {\'1\': 1., \'2\': 2.})\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/utils/beam_search_test.py,37,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for beam search.\n""""""\n\nimport unittest\n\nimport torch\nimport numpy as np\n\nfrom texar.torch.utils import beam_search\n\n\nclass BeamSearchTest(unittest.TestCase):\n    r""""""Tests beam_search.\n    """"""\n\n    def testShapes(self):\n        batch_size = 2\n        beam_size = 3\n        vocab_size = 4\n        decode_length = 10\n\n        initial_ids = torch.tensor([0, 0], dtype=torch.int64)\n\n        def symbols_to_logits(_):\n            # Just return random logits\n            return torch.rand(batch_size * beam_size, vocab_size)\n\n        final_ids, final_probs = beam_search.beam_search(\n            symbols_to_logits_fn=symbols_to_logits,\n            initial_ids=initial_ids,\n            beam_size=beam_size,\n            decode_length=decode_length,\n            vocab_size=vocab_size,\n            alpha=0.0,\n            eos_id=1)\n\n        self.assertEqual(final_ids.shape[1], beam_size)\n        self.assertEqual(final_probs.shape, torch.Size([batch_size, beam_size]))\n\n    def testComputeTopkScoresAndSeq(self):\n        batch_size = 2\n        beam_size = 3\n\n        sequences = torch.tensor([[[2, 3], [4, 5], [6, 7], [19, 20]],\n                                  [[8, 9], [10, 11], [12, 13], [80, 17]]],\n                                 dtype=torch.int64)\n\n        scores = torch.tensor([[-0.1, -2.5, 0., -1.5],\n                               [-100., -5., -0.00789, -1.34]])\n\n        flags = torch.tensor([[True, False, False, True],\n                              [False, False, False, True]])\n\n        topk_seq, topk_scores, topk_flags, _ = (\n            beam_search.compute_topk_scores_and_seq(sequences=sequences,\n                                                    scores=scores,\n                                                    scores_to_gather=scores,\n                                                    flags=flags,\n                                                    beam_size=beam_size,\n                                                    batch_size=batch_size))\n\n        exp_seq = [[[6, 7], [2, 3], [19, 20]], [[12, 13], [80, 17], [10, 11]]]\n        exp_scores = [[0., -0.1, -1.5], [-0.00789, -1.34, -5.]]\n        exp_flags = [[False, True, True], [False, True, False]]\n\n        self.assertEqual(topk_seq.tolist(), exp_seq)\n        topk_scores = topk_scores.tolist()\n        for i in range(2):\n            for j in range(3):\n                self.assertAlmostEqual(topk_scores[i][j], exp_scores[i][j])\n        self.assertEqual(topk_flags.tolist(), exp_flags)\n\n    def testGreedyBatchOne(self):\n        batch_size = 1\n        beam_size = 1\n        vocab_size = 2\n        decode_length = 3\n\n        initial_ids = torch.tensor([0] * batch_size, dtype=torch.int64)\n\n        # Test that beam search finds the most probable sequence.\n        # These probabilities represent the following search\n        #\n        #               G0 (0)\n        #                  / \\\n        #                /     \\\n        #              /         \\\n        #            /             \\\n        #         0(0.7)          1(0.3)\n        #           / \\\n        #          /   \\\n        #         /     \\\n        #     0(0.4) 1(0.6)\n        #        /\\\n        #       /  \\\n        #      /    \\\n        #    0(0.5) 1(0.5)\n        # and the following decoding probabilities\n        # 0000 - 0.7 * 0.4  * 0.1\n        # 0001 - 0.7 * 0.4  * 0.9\n        # 001 - 0.7 * 0.6 (Best)\n        # 01 = 0.3\n        #\n        # 001 is the most likely sequence under these probabilities.\n        probabilities = torch.tensor([[[0.7, 0.3]], [[0.4, 0.6]], [[0.5, 0.5]]])\n\n        def symbols_to_logits(ids):\n            pos = ids.shape[1]\n            logits = torch.log(probabilities[pos - 1, :]).type(torch.float)\n            return logits\n\n        final_ids, final_probs = beam_search.beam_search(\n            symbols_to_logits_fn=symbols_to_logits,\n            initial_ids=initial_ids,\n            beam_size=beam_size,\n            decode_length=decode_length,\n            vocab_size=vocab_size,\n            alpha=0.0,\n            eos_id=1)\n\n        exp_ids = [[[0, 0, 1]]]\n        exp_probs = [[0.7 * 0.6]]\n\n        self.assertEqual(final_ids.tolist(), exp_ids)\n        self.assertAlmostEqual(np.exp(final_probs).tolist()[0][0],\n                               exp_probs[0][0])\n\n    def testNotGreedyBeamTwoWithStopEarly(self):\n        batch_size = 1\n        beam_size = 2\n        vocab_size = 3\n        decode_length = 10\n\n        initial_ids = torch.tensor([0] * batch_size, dtype=torch.int64)\n        probabilities = torch.tensor([[[0.1, 0.1, 0.8], [0.1, 0.1, 0.8]],\n                                      [[0.4, 0.5, 0.1], [0.2, 0.4, 0.4]],\n                                      [[0.05, 0.9, 0.05], [0.4, 0.4, 0.2]]])\n\n        def symbols_to_logits(ids):\n            pos = ids.shape[1]\n            logits = torch.log(probabilities[pos - 1, :]).type(torch.float)\n            return logits\n\n        final_ids, final_probs = beam_search.beam_search(\n            symbols_to_logits_fn=symbols_to_logits,\n            initial_ids=initial_ids,\n            beam_size=beam_size,\n            decode_length=decode_length,\n            vocab_size=vocab_size,\n            alpha=0.0,\n            eos_id=1,\n            stop_early=True)  # default value, but just to make this explicit\n\n        # given stop_early = True, the only \'assurance\' is w.r.t. the first beam\n        # (i.e., other beams may not even be completed)\n        # so, we check only the first beam\n        first_beam = final_ids[:, 0]\n        first_probs = final_probs[:, 0]\n\n        exp_ids = [[0, 2, 1]]\n        exp_probs = [0.8 * 0.5]\n\n        self.assertEqual(first_beam.tolist(), exp_ids)\n        self.assertAlmostEqual(np.exp(first_probs).tolist()[0], exp_probs[0])\n\n    def testNotGreedyBeamTwoWithoutStopEarly(self):\n        batch_size = 1\n        beam_size = 2\n        vocab_size = 3\n        decode_length = 3\n\n        initial_ids = torch.tensor([0] * batch_size, dtype=torch.int64)\n        probabilities = torch.tensor([[[0.1, 0.1, 0.8], [0.1, 0.1, 0.8]],\n                                      [[0.4, 0.5, 0.1], [0.2, 0.4, 0.4]],\n                                      [[0.05, 0.9, 0.05], [0.4, 0.4, 0.2]]])\n\n        def symbols_to_logits(ids):\n            pos = ids.shape[1]\n            logits = torch.log(probabilities[pos - 1, :]).type(torch.float)\n            return logits\n\n        final_ids, final_probs = beam_search.beam_search(\n            symbols_to_logits_fn=symbols_to_logits,\n            initial_ids=initial_ids,\n            beam_size=beam_size,\n            decode_length=decode_length,\n            vocab_size=vocab_size,\n            alpha=0.0,\n            eos_id=1,\n            stop_early=False)\n\n        # given stop_early = False, the algorithm will return all the beams\n        # so we can test all of them here\n\n        exp_ids = [[[0, 2, 1, 0], [0, 2, 0, 1]]]\n        exp_probs = [[0.8 * 0.5, 0.8 * 0.4 * 0.9]]\n\n        self.assertEqual(final_ids.tolist(), exp_ids)\n        self.assertAlmostEqual(np.exp(final_probs).tolist()[0][0],\n                               exp_probs[0][0])\n        self.assertAlmostEqual(np.exp(final_probs).tolist()[0][1],\n                               exp_probs[0][1])\n\n    def testGreedyWithCornerCase(self):\n        batch_size = 1\n        beam_size = 1\n        vocab_size = 3\n        decode_length = 2\n\n        initial_ids = torch.tensor([0] * batch_size, dtype=torch.int64)\n        probabilities = torch.tensor([[0.2, 0.1, 0.7], [0.4, 0.1, 0.5]])\n\n        def symbols_to_logits(ids):\n            pos = ids.shape[1]\n            logits = torch.log(probabilities[pos - 1, :]).type(torch.float)\n            return logits\n\n        final_ids, final_probs = beam_search.beam_search(\n            symbols_to_logits_fn=symbols_to_logits,\n            initial_ids=initial_ids,\n            beam_size=beam_size,\n            decode_length=decode_length,\n            vocab_size=vocab_size,\n            alpha=0.0,\n            eos_id=1)\n\n        exp_ids = [[[0, 2, 2]]]\n        exp_probs = [[0.7 * 0.5]]\n\n        self.assertEqual(final_ids.tolist(), exp_ids)\n        self.assertAlmostEqual(np.exp(final_probs).tolist()[0][0],\n                               exp_probs[0][0])\n\n    def testNotGreedyBatchTwoBeamTwoWithAlpha(self):\n        batch_size = 2\n        beam_size = 2\n        vocab_size = 3\n        decode_length = 3\n\n        initial_ids = torch.tensor([0] * batch_size, dtype=torch.int64)\n        # Probabilities for position * batch * beam * vocab\n        # Probabilities have been set such that with alpha = 3.5, the less\n        # probable but longer sequence will have a better score than the\n        # shorter sequence with higher log prob in batch 1, and the order will\n        # be reverse in batch 2. That is, the shorter sequence will still have\n        # a higher score in spite of the length penalty\n        probabilities = torch.tensor([[[[0.1, 0.1, 0.8], [0.1, 0.1, 0.8]],\n                                       [[0.1, 0.1, 0.8], [0.1, 0.1, 0.8]]],\n                                      [[[0.4, 0.5, 0.1], [0.2, 0.4, 0.4]],\n                                       [[0.3, 0.6, 0.1], [0.2, 0.4, 0.4]]],\n                                      [[[0.05, 0.9, 0.05], [0.4, 0.4, 0.2]],\n                                       [[0.05, 0.9, 0.05], [0.4, 0.4, 0.2]]]])\n\n        def symbols_to_logits(ids):\n            pos = ids.shape[1]\n            logits = torch.log(probabilities[pos - 1, :]).type(torch.float)\n            return logits\n\n        final_ids, final_probs = beam_search.beam_search(\n            symbols_to_logits_fn=symbols_to_logits,\n            initial_ids=initial_ids,\n            beam_size=beam_size,\n            decode_length=decode_length,\n            vocab_size=vocab_size,\n            alpha=3.5,\n            eos_id=1)\n\n        exp_ids = [[[0, 2, 0, 1], [0, 2, 1, 0]], [[0, 2, 1, 0], [0, 2, 0, 1]]]\n        exp_probs = [[np.log(0.8 * 0.4 * 0.9) / (8. / 6.)**3.5,\n                      np.log(0.8 * 0.5) / (7. / 6.)**3.5],\n                     [np.log(0.8 * 0.6) / (7. / 6.)**3.5,\n                      np.log(0.8 * 0.3 * 0.9) / (8. / 6.)**3.5]]\n\n        self.assertEqual(final_ids.tolist(), exp_ids)\n        for i in range(2):\n            for j in range(2):\n                self.assertAlmostEqual(final_probs.tolist()[i][j],\n                                       exp_probs[i][j])\n\n    def testNotGreedyBeamTwoWithAlpha(self):\n        batch_size = 1\n        beam_size = 2\n        vocab_size = 3\n        decode_length = 3\n\n        initial_ids = torch.tensor([0] * batch_size, dtype=torch.int64)\n        # Probabilities for position * batch * beam * vocab\n        # Probabilities have been set such that with alpha = 3.5, the less\n        # probable but longer sequence will have a better score that the\n        # shorter sequence with higher log prob.\n        probabilities = torch.tensor([[[0.1, 0.1, 0.8], [0.1, 0.1, 0.8]],\n                                      [[0.4, 0.5, 0.1], [0.2, 0.4, 0.4]],\n                                      [[0.05, 0.9, 0.05], [0.4, 0.4, 0.2]]])\n\n        def symbols_to_logits(ids):\n            pos = ids.shape[1]\n            logits = torch.log(probabilities[pos - 1, :]).type(torch.float)\n            return logits\n\n        # Disable early stopping\n        final_ids, final_probs = beam_search.beam_search(\n            symbols_to_logits_fn=symbols_to_logits,\n            initial_ids=initial_ids,\n            beam_size=beam_size,\n            decode_length=decode_length,\n            vocab_size=vocab_size,\n            alpha=3.5,\n            eos_id=1)\n\n        exp_ids = [[[0, 2, 0, 1], [0, 2, 1, 0]]]\n        exp_probs = [[np.log(0.8 * 0.4 * 0.9) / (8. / 6.)**3.5,\n                      np.log(0.8 * 0.5) / (7. / 6.)**3.5]]\n\n        self.assertEqual(final_ids.tolist(), exp_ids)\n        self.assertAlmostEqual(final_probs.tolist()[0][0], exp_probs[0][0])\n        self.assertAlmostEqual(final_probs.tolist()[0][1], exp_probs[0][1])\n\n    def testStates(self):\n        batch_size = 1\n        beam_size = 1\n        vocab_size = 2\n        decode_length = 3\n\n        initial_ids = torch.tensor([0] * batch_size, dtype=torch.int64)\n        probabilities = torch.tensor([[[0.7, 0.3]], [[0.4, 0.6]], [[0.5, 0.5]]])\n\n        expected_states = torch.tensor([[[0.]], [[1.]]])\n\n        def symbols_to_logits(ids, states):\n            pos = ids.shape[1]\n            logits = torch.log(probabilities[pos - 1, :]).type(torch.float)\n            states[""state""] += 1\n            return logits, states\n\n        states = {\n            ""state"": torch.zeros(batch_size, 1),\n        }\n\n        final_ids, _ = beam_search.beam_search(\n            symbols_to_logits_fn=symbols_to_logits,\n            initial_ids=initial_ids,\n            beam_size=beam_size,\n            decode_length=decode_length,\n            vocab_size=vocab_size,\n            alpha=0.0,\n            eos_id=1,\n            states=states)\n\n    def testStateBeamTwo(self):\n        batch_size = 1\n        beam_size = 2\n        vocab_size = 3\n        decode_length = 3\n\n        initial_ids = torch.tensor([0] * batch_size, dtype=torch.int64)\n        probabilities = torch.tensor([[[0.1, 0.1, 0.8], [0.1, 0.1, 0.8]],\n                                      [[0.4, 0.5, 0.1], [0.2, 0.4, 0.4]],\n                                      [[0.05, 0.9, 0.05], [0.4, 0.4, 0.2]]])\n\n        # The top beam is always selected so we should see the top beam\'s state\n        # at each position, which is the one that getting 3 added to it each\n        # step.\n        expected_states = torch.tensor([[[0.], [0.]], [[3.], [3.]], [[6.],\n                                                                     [6.]]])\n\n        def symbols_to_logits(ids, states):\n            pos = ids.shape[1]\n            logits = torch.log(probabilities[pos - 1, :]).type(torch.float)\n            states[""state""] += torch.tensor([[3.], [7.]])\n            return logits, states\n\n        states = {\n            ""state"": torch.zeros(batch_size, 1)\n        }\n\n        final_ids, _ = beam_search.beam_search(\n            symbols_to_logits_fn=symbols_to_logits,\n            initial_ids=initial_ids,\n            beam_size=beam_size,\n            decode_length=decode_length,\n            vocab_size=vocab_size,\n            alpha=0.0,\n            eos_id=1,\n            states=states)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/utils/rnn_test.py,115,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for rnn helpers.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.core.cell_wrappers import RNNCell, GRUCell, LSTMCell\nfrom texar.torch.utils.rnn import (\n    dynamic_rnn, reverse_sequence, bidirectional_dynamic_rnn)\n\n\nclass ReverseSequenceTest(unittest.TestCase):\n    r""""""Tests reverse_sequence.\n    """"""\n\n    def setUp(self):\n        self.inputs = [[[10, 11], [12, 13], [14, 15], [16, 17], [18, 19]],\n                       [[20, 21], [22, 23], [24, 25], [26, 27], [28, 29]],\n                       [[30, 31], [32, 33], [34, 35], [36, 37], [38, 39]],\n                       [[40, 41], [42, 43], [44, 45], [46, 47], [48, 49]]]\n        self.inputs = torch.tensor(self.inputs)\n\n    def test_reverse_sequence(self):\n        r""""""Tests :meth:`~texar.torch.utils.rnn.reverse_sequence`.\n        """"""\n        seq_lengths_batch_first = torch.tensor([1, 2, 3, 4])\n        output = reverse_sequence(inputs=self.inputs,\n                                  seq_lengths=seq_lengths_batch_first,\n                                  time_major=False)\n\n        expect_out = [[[10, 11], [12, 13], [14, 15], [16, 17], [18, 19]],\n                      [[22, 23], [20, 21], [24, 25], [26, 27], [28, 29]],\n                      [[34, 35], [32, 33], [30, 31], [36, 37], [38, 39]],\n                      [[46, 47], [44, 45], [42, 43], [40, 41], [48, 49]]]\n\n        self.assertEqual(output.tolist(), expect_out)\n\n        seq_lengths_time_first = torch.tensor([0, 1, 2, 3, 4])\n        output = reverse_sequence(inputs=self.inputs,\n                                  seq_lengths=seq_lengths_time_first,\n                                  time_major=True)\n\n        expect_out = [[[10, 11], [12, 13], [24, 25], [36, 37], [48, 49]],\n                      [[20, 21], [22, 23], [14, 15], [26, 27], [38, 39]],\n                      [[30, 31], [32, 33], [34, 35], [16, 17], [28, 29]],\n                      [[40, 41], [42, 43], [44, 45], [46, 47], [18, 19]]]\n\n        self.assertEqual(output.tolist(), expect_out)\n\n\nclass DynamicRNNTest(unittest.TestCase):\n    r""""""Tests dynamic_rnn.\n    """"""\n\n    def setUp(self):\n        self._batch_size = 8\n        self._max_time = 64\n        self._input_size = 16\n        self._hidden_size = 32\n        self._rnn = RNNCell(self._input_size, self._hidden_size)\n        self._lstm = LSTMCell(self._input_size, self._hidden_size)\n        self._gru = GRUCell(self._input_size, self._hidden_size)\n\n    def test_dynamic_rnn_basic(self):\n        r""""""Tests :meth:`~texar.torch.utils.rnn.dynamic_rnn`.\n        """"""\n        inputs = torch.rand(self._batch_size, self._max_time, self._input_size)\n\n        # RNN\n        outputs, final_state = dynamic_rnn(self._rnn,\n                                           inputs,\n                                           sequence_length=None,\n                                           initial_state=None,\n                                           time_major=False)\n\n        self.assertEqual(outputs.shape, torch.Size([self._batch_size,\n                                                    self._max_time,\n                                                    self._hidden_size]))\n        self.assertEqual(final_state.shape, torch.Size([self._batch_size,\n                                                        self._hidden_size]))\n        # LSTM\n        outputs, final_state = dynamic_rnn(self._lstm,\n                                           inputs,\n                                           sequence_length=None,\n                                           initial_state=None,\n                                           time_major=False)\n\n        self.assertEqual(outputs.shape, torch.Size([self._batch_size,\n                                                    self._max_time,\n                                                    self._hidden_size]))\n        self.assertIsInstance(final_state, tuple)\n        self.assertEqual(final_state[0].shape, torch.Size([self._batch_size,\n                                                           self._hidden_size]))\n        self.assertEqual(final_state[1].shape, torch.Size([self._batch_size,\n                                                           self._hidden_size]))\n\n        # GRU\n        outputs, final_state = dynamic_rnn(self._gru,\n                                           inputs,\n                                           sequence_length=None,\n                                           initial_state=None,\n                                           time_major=False)\n\n        self.assertEqual(outputs.shape, torch.Size([self._batch_size,\n                                                    self._max_time,\n                                                    self._hidden_size]))\n        self.assertEqual(final_state.shape, torch.Size([self._batch_size,\n                                                        self._hidden_size]))\n\n    def test_dynamic_rnn_time_major(self):\n        r""""""Tests :meth:`~texar.torch.utils.rnn.dynamic_rnn`.\n        """"""\n        inputs = torch.rand(self._max_time, self._batch_size, self._input_size)\n\n        # RNN\n        outputs, final_state = dynamic_rnn(self._rnn,\n                                           inputs,\n                                           sequence_length=None,\n                                           initial_state=None,\n                                           time_major=True)\n\n        self.assertEqual(outputs.shape, torch.Size([self._max_time,\n                                                    self._batch_size,\n                                                    self._hidden_size]))\n        self.assertEqual(final_state.shape, torch.Size([self._batch_size,\n                                                        self._hidden_size]))\n        # LSTM\n        outputs, final_state = dynamic_rnn(self._lstm,\n                                           inputs,\n                                           sequence_length=None,\n                                           initial_state=None,\n                                           time_major=True)\n\n        self.assertEqual(outputs.shape, torch.Size([self._max_time,\n                                                    self._batch_size,\n                                                    self._hidden_size]))\n        self.assertIsInstance(final_state, tuple)\n        self.assertEqual(final_state[0].shape, torch.Size([self._batch_size,\n                                                           self._hidden_size]))\n        self.assertEqual(final_state[1].shape, torch.Size([self._batch_size,\n                                                           self._hidden_size]))\n\n        # GRU\n        outputs, final_state = dynamic_rnn(self._gru,\n                                           inputs,\n                                           sequence_length=None,\n                                           initial_state=None,\n                                           time_major=True)\n\n        self.assertEqual(outputs.shape, torch.Size([self._max_time,\n                                                    self._batch_size,\n                                                    self._hidden_size]))\n        self.assertEqual(final_state.shape, torch.Size([self._batch_size,\n                                                        self._hidden_size]))\n\n    def test_dynamic_rnn_sequence_length(self):\n        r""""""Tests :meth:`~texar.torch.utils.rnn.dynamic_rnn`.\n        """"""\n        inputs = torch.rand(self._batch_size, self._max_time, self._input_size)\n        sequence_length = [0, 43, 23, 63, 12, 54, 33, 8]\n\n        # RNN\n        outputs, final_state = dynamic_rnn(self._rnn,\n                                           inputs,\n                                           sequence_length=sequence_length,\n                                           initial_state=None,\n                                           time_major=False)\n\n        self.assertEqual(outputs.shape, torch.Size([self._batch_size,\n                                                    self._max_time,\n                                                    self._hidden_size]))\n        self.assertEqual(final_state.shape, torch.Size([self._batch_size,\n                                                        self._hidden_size]))\n\n        # LSTM\n        outputs, final_state = dynamic_rnn(self._lstm,\n                                           inputs,\n                                           sequence_length=sequence_length,\n                                           initial_state=None,\n                                           time_major=False)\n\n        self.assertEqual(outputs.shape, torch.Size([self._batch_size,\n                                                    self._max_time,\n                                                    self._hidden_size]))\n        self.assertIsInstance(final_state, tuple)\n        self.assertEqual(final_state[0].shape, torch.Size([self._batch_size,\n                                                           self._hidden_size]))\n        self.assertEqual(final_state[1].shape, torch.Size([self._batch_size,\n                                                           self._hidden_size]))\n\n        # GRU\n        outputs, final_state = dynamic_rnn(self._gru,\n                                           inputs,\n                                           sequence_length=sequence_length,\n                                           initial_state=None,\n                                           time_major=False)\n\n        self.assertEqual(outputs.shape, torch.Size([self._batch_size,\n                                                    self._max_time,\n                                                    self._hidden_size]))\n        self.assertEqual(final_state.shape, torch.Size([self._batch_size,\n                                                        self._hidden_size]))\n\n    def test_dynamic_rnn_initial_state(self):\n        r""""""Tests :meth:`~texar.torch.utils.rnn.dynamic_rnn`.\n        """"""\n        inputs = torch.rand(self._batch_size, self._max_time, self._input_size)\n\n        rnn_initial_state = torch.rand(self._batch_size, self._hidden_size)\n        lstm_initial_state = (torch.rand(self._batch_size, self._hidden_size),\n                              torch.rand(self._batch_size, self._hidden_size))\n\n        # RNN\n        outputs, final_state = dynamic_rnn(self._rnn,\n                                           inputs,\n                                           sequence_length=None,\n                                           initial_state=rnn_initial_state,\n                                           time_major=False)\n\n        self.assertEqual(outputs.shape, torch.Size([self._batch_size,\n                                                    self._max_time,\n                                                    self._hidden_size]))\n        self.assertEqual(final_state.shape, torch.Size([self._batch_size,\n                                                        self._hidden_size]))\n\n        # LSTM\n        outputs, final_state = dynamic_rnn(self._lstm,\n                                           inputs,\n                                           sequence_length=None,\n                                           initial_state=lstm_initial_state,\n                                           time_major=False)\n\n        self.assertEqual(outputs.shape, torch.Size([self._batch_size,\n                                                    self._max_time,\n                                                    self._hidden_size]))\n        self.assertIsInstance(final_state, tuple)\n        self.assertEqual(final_state[0].shape, torch.Size([self._batch_size,\n                                                           self._hidden_size]))\n        self.assertEqual(final_state[1].shape, torch.Size([self._batch_size,\n                                                           self._hidden_size]))\n\n        # GRU\n        outputs, final_state = dynamic_rnn(self._gru,\n                                           inputs,\n                                           sequence_length=None,\n                                           initial_state=rnn_initial_state,\n                                           time_major=False)\n\n        self.assertEqual(outputs.shape, torch.Size([self._batch_size,\n                                                    self._max_time,\n                                                    self._hidden_size]))\n        self.assertEqual(final_state.shape, torch.Size([self._batch_size,\n                                                        self._hidden_size]))\n\n\nclass BidirectionalDynamicRNNTest(unittest.TestCase):\n    r""""""Tests bidirectional_dynamic_rnn.\n    """"""\n\n    def setUp(self):\n        self._batch_size = 8\n        self._max_time = 64\n        self._input_size = 16\n        self._hidden_size = 32\n        self._rnn_fw = RNNCell(self._input_size, self._hidden_size)\n        self._rnn_bw = RNNCell(self._input_size, self._hidden_size)\n        self._lstm_fw = LSTMCell(self._input_size, self._hidden_size)\n        self._lstm_bw = LSTMCell(self._input_size, self._hidden_size)\n        self._gru_fw = GRUCell(self._input_size, self._hidden_size)\n        self._gru_bw = GRUCell(self._input_size, self._hidden_size)\n\n    def test_bidirectional_dynamic_rnn_basic(self):\n        r""""""Tests :meth:`~texar.torch.utils.rnn.bidirectional_dynamic_rnn`.\n        """"""\n        inputs = torch.rand(self._batch_size, self._max_time, self._input_size)\n\n        # RNN\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._rnn_fw,\n            cell_bw=self._rnn_bw,\n            inputs=inputs,\n            sequence_length=None,\n            initial_state_fw=None,\n            initial_state_bw=None,\n            time_major=False)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertEqual(output_state[0].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n        self.assertEqual(output_state[1].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n\n        # LSTM\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._lstm_fw,\n            cell_bw=self._lstm_bw,\n            inputs=inputs,\n            sequence_length=None,\n            initial_state_fw=None,\n            initial_state_bw=None,\n            time_major=False)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertIsInstance(output_state[0], tuple)\n        self.assertEqual(output_state[0][0].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertEqual(output_state[0][1].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertIsInstance(output_state[1], tuple)\n        self.assertEqual(output_state[1][0].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertEqual(output_state[1][1].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n\n        # GRU\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._gru_fw,\n            cell_bw=self._gru_bw,\n            inputs=inputs,\n            sequence_length=None,\n            initial_state_fw=None,\n            initial_state_bw=None,\n            time_major=False)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertEqual(output_state[0].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n        self.assertEqual(output_state[1].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n\n    def test_bidirectional_dynamic_rnn_time_major(self):\n        r""""""Tests :meth:`~texar.torch.utils.rnn.bidirectional_dynamic_rnn`.\n        """"""\n        inputs = torch.rand(self._max_time, self._batch_size, self._input_size)\n\n        # RNN\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._rnn_fw,\n            cell_bw=self._rnn_bw,\n            inputs=inputs,\n            sequence_length=None,\n            initial_state_fw=None,\n            initial_state_bw=None,\n            time_major=True)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._max_time,\n                                                       self._batch_size,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._max_time,\n                                                       self._batch_size,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertEqual(output_state[0].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n        self.assertEqual(output_state[1].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n\n        # LSTM\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._lstm_fw,\n            cell_bw=self._lstm_bw,\n            inputs=inputs,\n            sequence_length=None,\n            initial_state_fw=None,\n            initial_state_bw=None,\n            time_major=True)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._max_time,\n                                                       self._batch_size,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._max_time,\n                                                       self._batch_size,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertIsInstance(output_state[0], tuple)\n        self.assertEqual(output_state[0][0].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertEqual(output_state[0][1].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertIsInstance(output_state[1], tuple)\n        self.assertEqual(output_state[1][0].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertEqual(output_state[1][1].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n\n        # GRU\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._gru_fw,\n            cell_bw=self._gru_bw,\n            inputs=inputs,\n            sequence_length=None,\n            initial_state_fw=None,\n            initial_state_bw=None,\n            time_major=True)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._max_time,\n                                                       self._batch_size,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._max_time,\n                                                       self._batch_size,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertEqual(output_state[0].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n        self.assertEqual(output_state[1].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n\n    def test_bidirectional_dynamic_rnn_sequence_length(self):\n        r""""""Tests :meth:`~texar.torch.utils.rnn.bidirectional_dynamic_rnn`.\n        """"""\n        inputs = torch.rand(self._batch_size, self._max_time, self._input_size)\n        sequence_length = [0, 43, 23, 63, 12, 54, 33, 8]\n\n        # RNN\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._rnn_fw,\n            cell_bw=self._rnn_bw,\n            inputs=inputs,\n            sequence_length=sequence_length,\n            initial_state_fw=None,\n            initial_state_bw=None,\n            time_major=False)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertEqual(output_state[0].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n        self.assertEqual(output_state[1].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n\n        # LSTM\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._lstm_fw,\n            cell_bw=self._lstm_bw,\n            inputs=inputs,\n            sequence_length=sequence_length,\n            initial_state_fw=None,\n            initial_state_bw=None,\n            time_major=False)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertIsInstance(output_state[0], tuple)\n        self.assertEqual(output_state[0][0].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertEqual(output_state[0][1].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertIsInstance(output_state[1], tuple)\n        self.assertEqual(output_state[1][0].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertEqual(output_state[1][1].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n\n        # GRU\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._gru_fw,\n            cell_bw=self._gru_bw,\n            inputs=inputs,\n            sequence_length=sequence_length,\n            initial_state_fw=None,\n            initial_state_bw=None,\n            time_major=False)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertEqual(output_state[0].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n        self.assertEqual(output_state[1].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n\n    def test_bidirectional_dynamic_rnn_initial_state(self):\n        r""""""Tests :meth:`~texar.torch.utils.rnn.bidirectional_dynamic_rnn`.\n        """"""\n        inputs = torch.rand(self._batch_size, self._max_time, self._input_size)\n\n        rnn_initial_state_fw = torch.rand(self._batch_size, self._hidden_size)\n        rnn_initial_state_bw = torch.rand(self._batch_size, self._hidden_size)\n        lstm_initial_state_fw = (torch.rand(self._batch_size,\n                                            self._hidden_size),\n                                 torch.rand(self._batch_size,\n                                            self._hidden_size))\n        lstm_initial_state_bw = (torch.rand(self._batch_size,\n                                            self._hidden_size),\n                                 torch.rand(self._batch_size,\n                                            self._hidden_size))\n\n        # RNN\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._rnn_fw,\n            cell_bw=self._rnn_bw,\n            inputs=inputs,\n            sequence_length=None,\n            initial_state_fw=rnn_initial_state_fw,\n            initial_state_bw=rnn_initial_state_bw,\n            time_major=False)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertEqual(output_state[0].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n        self.assertEqual(output_state[1].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n\n        # LSTM\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._lstm_fw,\n            cell_bw=self._lstm_bw,\n            inputs=inputs,\n            sequence_length=None,\n            initial_state_fw=lstm_initial_state_fw,\n            initial_state_bw=lstm_initial_state_bw,\n            time_major=False)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertIsInstance(output_state[0], tuple)\n        self.assertEqual(output_state[0][0].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertEqual(output_state[0][1].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertIsInstance(output_state[1], tuple)\n        self.assertEqual(output_state[1][0].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n        self.assertEqual(output_state[1][1].shape,\n                         torch.Size([self._batch_size, self._hidden_size]))\n\n        # GRU\n        outputs, output_state = bidirectional_dynamic_rnn(\n            cell_fw=self._gru_fw,\n            cell_bw=self._gru_bw,\n            inputs=inputs,\n            sequence_length=None,\n            initial_state_fw=rnn_initial_state_fw,\n            initial_state_bw=rnn_initial_state_bw,\n            time_major=False)\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       self._hidden_size]))\n\n        self.assertIsInstance(output_state, tuple)\n        self.assertEqual(output_state[0].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n        self.assertEqual(output_state[1].shape, torch.Size([self._batch_size,\n                                                            self._hidden_size]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/utils/shapes_test.py,15,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for shape-related utility functions.\n""""""\n\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.utils import shapes\n\n\nclass ShapesTest(unittest.TestCase):\n    r""""""Tests shape-related utility functions.\n    """"""\n\n    def test_mask_sequences(self):\n        r""""""Tests :func:`texar.torch.utils.shapes.mask_sequences`.\n        """"""\n        seq = torch.ones(3, 4, 3, dtype=torch.int32)\n        seq_length = torch.tensor([3, 2, 1], dtype=torch.int32)\n\n        masked_seq = shapes.mask_sequences(seq, seq_length)\n        np.testing.assert_array_equal(masked_seq.shape, seq.shape)\n        seq_sum = torch.sum(masked_seq, dim=(1, 2))\n        np.testing.assert_array_equal(seq_sum, seq_length * 3)\n\n    def test_pad_and_concat_long(self):\n        r""""""Test :func:`texar.torch.utils.shapes.pad_and_concat` with\n        torch.LongTensor.\n        """"""\n        a = torch.ones(3, 10, 2, dtype=torch.long)\n        b = torch.ones(4, 20, 3, dtype=torch.long)\n        c = torch.ones(5, 1, 4, dtype=torch.long)\n\n        t = shapes.pad_and_concat([a, b, c], 0)\n        np.testing.assert_array_equal(t.shape, [3 + 4 + 5, 20, 4])\n        t = shapes.pad_and_concat([a, b, c], 1)\n        np.testing.assert_array_equal(t.shape, [5, 10 + 20 + 1, 4])\n        t = shapes.pad_and_concat([a, b, c], 2)\n        np.testing.assert_array_equal(t.shape, [5, 20, 2 + 3 + 4])\n\n    def test_pad_and_concat_float(self):\n        r""""""Test :func:`texar.torch.utils.shapes.pad_and_concat` with\n        torch.FloatTensor.\n        """"""\n        a = torch.ones(3, 10, 2)\n        b = torch.ones(4, 20, 3)\n        c = torch.ones(5, 1, 4)\n\n        t = shapes.pad_and_concat([a, b, c], 0)\n        np.testing.assert_array_equal(t.shape, [3 + 4 + 5, 20, 4])\n        t = shapes.pad_and_concat([a, b, c], 1)\n        np.testing.assert_array_equal(t.shape, [5, 10 + 20 + 1, 4])\n        t = shapes.pad_and_concat([a, b, c], 2)\n        np.testing.assert_array_equal(t.shape, [5, 20, 2 + 3 + 4])\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/utils/utils_test.py,17,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for utility functions.\n""""""\n\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.utils import utils\n\n\nclass UtilsTest(unittest.TestCase):\n    r""""""Tests utility functions.\n    """"""\n\n    def test_sequence_mask(self):\n        r""""""Tests :meth:`texar.torch.utils.sequence_mask`.\n        """"""\n        mask1 = utils.sequence_mask([1, 3, 2], 5).numpy()\n        expected1 = np.asarray(\n            [[True, False, False, False, False],\n             [True, True, True, False, False],\n             [True, True, False, False, False]]\n        )\n        np.testing.assert_array_equal(mask1, expected1)\n\n        mask2 = utils.sequence_mask(torch.tensor([[1, 3], [2, 0]]))\n        expected2 = np.asarray(\n            [[[True, False, False],\n              [True, True, True]],\n             [[True, True, False],\n              [False, False, False]]]\n        )\n        np.testing.assert_array_equal(mask2, expected2)\n\n    def test_dict_patch(self):\n        r""""""Tests :meth:`texar.torch.utils.dict_patch`.\n        """"""\n        src_dict = {\n            ""k1"": ""k1"",\n            ""k_dict_1"": {\n                ""kd1_k1"": ""kd1_k1"",\n                ""kd1_k2"": ""kd1_k2""\n            },\n            ""k_dict_2"": {\n                ""kd2_k1"": ""kd2_k1""\n            }\n        }\n        tgt_dict = {\n            ""k1"": ""k1_tgt"",\n            ""k_dict_1"": {\n                ""kd1_k1"": ""kd1_k1""\n            },\n            ""k_dict_2"": ""kd2_not_dict""\n        }\n\n        patched_dict = utils.dict_patch(tgt_dict, src_dict)\n        self.assertEqual(patched_dict[""k1""], tgt_dict[""k1""])\n        self.assertEqual(patched_dict[""k_dict_1""], src_dict[""k_dict_1""])\n        self.assertEqual(patched_dict[""k_dict_2""], tgt_dict[""k_dict_2""])\n\n    def test_strip_token(self):\n        r""""""Tests :func:`texar.torch.utils.strip_token`\n        """"""\n        str_ = "" <PAD>  <PAD>\\t  i am <PAD> \\t <PAD>  \\t""\n        self.assertEqual(utils.strip_token(str_, ""<PAD>""), ""i am"")\n        self.assertEqual(utils.strip_token(str_, """"),\n                         ""<PAD> <PAD> i am <PAD> <PAD>"")\n        self.assertEqual(utils.strip_token([str_], ""<PAD>""), [""i am""])\n        self.assertEqual(\n            utils.strip_token(np.asarray([str_]), ""<PAD>""),\n            [""i am""])\n        self.assertEqual(type(utils.strip_token(np.asarray([str_]), ""<PAD>"")),\n                         np.ndarray)\n        self.assertEqual(\n            utils.strip_token([[[str_]], [\'\']], ""<PAD>""),\n            [[[""i am""]], [\'\']])\n\n        str_ = str_.split()\n        self.assertEqual(utils.strip_token(str_, ""<PAD>"", is_token_list=True),\n                         [""i"", ""am""])\n        self.assertEqual(utils.strip_token([str_], ""<PAD>"", is_token_list=True),\n                         [[""i"", ""am""]])\n\n    def test_strip_bos(self):\n        r""""""Tests :func:`texar.torch.utils.strip_bos`\n        """"""\n        str_ = ""<BOS> i am""\n        self.assertEqual(utils.strip_bos(str_, ""<BOS>""), ""i am"")\n        self.assertEqual(utils.strip_bos(str_, """"), ""<BOS> i am"")\n        self.assertEqual(utils.strip_bos([str_], ""<BOS>""), [""i am""])\n\n        str_ = str_.split()\n        self.assertEqual(utils.strip_bos(str_, ""<BOS>"", is_token_list=True),\n                         [""i"", ""am""])\n        self.assertEqual(utils.strip_bos([str_], ""<BOS>"", is_token_list=True),\n                         [[""i"", ""am""]])\n\n    def test_strip_eos(self):\n        r""""""Tests :func:`texar.torch.utils.strip_eos`\n        """"""\n        str_ = ""i am <EOS>""\n        self.assertEqual(utils.strip_eos(str_, ""<EOS>""), ""i am"")\n        self.assertEqual(utils.strip_eos([str_], ""<EOS>""), [""i am""])\n\n        str_ = str_.split()\n        self.assertEqual(utils.strip_eos(str_, ""<EOS>"", is_token_list=True),\n                         [""i"", ""am""])\n        self.assertEqual(utils.strip_eos([str_], ""<EOS>"", is_token_list=True),\n                         [[""i"", ""am""]])\n\n    def test_strip_special_tokens(self):\n        r""""""Test :func:`texar.torch.utils.strip_special_tokens`\n        """"""\n        str_ = ""<BOS> i am <EOS> <PAD> <PAD>""\n        self.assertEqual(utils.strip_special_tokens(str_), ""i am"")\n        self.assertEqual(utils.strip_special_tokens([str_]), [""i am""])\n\n        str_ = str_.split()\n        self.assertEqual(utils.strip_special_tokens(str_, is_token_list=True),\n                         [""i"", ""am""])\n        self.assertEqual(utils.strip_special_tokens([str_], is_token_list=True),\n                         [[""i"", ""am""]])\n\n    def test_str_join(self):\n        r""""""Tests :func:`texar.torch.utils.str_join`\n        """"""\n        tokens = np.ones([2, 2, 3], dtype=\'str\')\n\n        str_ = utils.str_join(tokens)\n        np.testing.assert_array_equal(\n            str_, np.asarray([[\'1 1 1\', \'1 1 1\'], [\'1 1 1\', \'1 1 1\']]))\n        self.assertIsInstance(str_, np.ndarray)\n\n        str_ = utils.str_join(tokens.tolist())\n        np.testing.assert_array_equal(\n            str_, [[\'1 1 1\', \'1 1 1\'], [\'1 1 1\', \'1 1 1\']])\n        self.assertIsInstance(str_, list)\n\n        tokens = [[], [\'1\', \'1\']]\n        str_ = utils.str_join(tokens)\n        np.testing.assert_array_equal(str_, [\'\', \'1 1\'])\n\n    def test_uniquify_str(self):\n        r""""""Tests :func:`texar.torch.utils.uniquify_str`.\n        """"""\n        str_set = [\'str\']\n        unique_str = utils.uniquify_str(\'str\', str_set)\n        self.assertEqual(unique_str, \'str_1\')\n\n        str_set.append(\'str_1\')\n        str_set.append(\'str_2\')\n        unique_str = utils.uniquify_str(\'str\', str_set)\n        self.assertEqual(unique_str, \'str_3\')\n\n    def test_sum_tensors(self):\n\n        inputs = [torch.tensor(1), torch.tensor(2)]\n        self.assertEqual(utils.sum_tensors(inputs), torch.tensor(3))\n\n        inputs = [torch.tensor(1), None, torch.tensor(2)]\n        self.assertEqual(utils.sum_tensors(inputs), torch.tensor(3))\n\n        inputs = [torch.tensor(1), None, None]\n        self.assertEqual(utils.sum_tensors(inputs), torch.tensor(1))\n\n        inputs = [None, None, None]\n        self.assertEqual(utils.sum_tensors(inputs), None)\n\n    def test_truncate_seq_pair(self):\n\n        tokens_a = [1, 2, 3]\n        tokens_b = [4, 5, 6]\n        utils.truncate_seq_pair(tokens_a, tokens_b, 4)\n        self.assertListEqual(tokens_a, [1, 2])\n        self.assertListEqual(tokens_b, [4, 5])\n\n        tokens_a = [1]\n        tokens_b = [2, 3, 4, 5]\n        utils.truncate_seq_pair(tokens_a, tokens_b, 3)\n        self.assertListEqual(tokens_a, [1])\n        self.assertListEqual(tokens_b, [2, 3])\n\n    # def test_map_ids_to_strs(self):\n    #    """"""Tests :func:`texar.torch.utils.map_ids_to_strs`.\n    #    """"""\n    #    vocab_list = [\'word\', \'\xe8\xaf\x8d\']\n    #    vocab_file = tempfile.NamedTemporaryFile()\n    #    vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n    #    vocab_file.flush()\n    #    vocab = Vocab(vocab_file.name)\n\n    #    text = [[\'<BOS>\', \'word\', \'\xe8\xaf\x8d\', \'<EOS>\', \'<PAD>\'],\n    #            [\'word\', \'\xe8\xaf\x8d\', \'word\', \'\xe8\xaf\x8d\', \'<PAD>\']]\n    #    text = np.asarray(text)\n    #    ids = vocab.map_tokens_to_ids_py(text)\n\n    #    ids = ids.tolist()\n    #    text_ = utils.map_ids_to_strs(ids, vocab)\n\n    #    self.assertEqual(text_[0], \'word \xe8\xaf\x8d\')\n    #    self.assertEqual(text_[1], \'word \xe8\xaf\x8d word \xe8\xaf\x8d\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
texar/torch/__init__.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.torch.version import VERSION as __version__\n\nfrom texar.torch import core\nfrom texar.torch import data\nfrom texar.torch import evals\nfrom texar.torch import losses\nfrom texar.torch import modules\nfrom texar.torch import run\nfrom texar.torch import utils\nfrom texar.torch.hyperparams import *\nfrom texar.torch.module_base import *\n'"
texar/torch/hyperparams.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nHyperparameter manager\n""""""\n\nimport copy\nimport json\nfrom typing import (\n    Any, Dict, ItemsView, Iterator, KeysView, Optional, Tuple, Union)\n\n__all__ = [\n    \'HParams\',\n]\n\n\ndef _type_name(value):\n    return type(value).__name__\n\n\nclass HParams:\n    r""""""A class that maintains hyperparameters for configuring Texar modules.\n    The class has several useful features:\n\n    - **Auto-completion of missing values.** Users can specify only a subset of\n      hyperparameters they care about. Other hyperparameters will automatically\n      take the default values. The auto-completion performs **recursively** so\n      that hyperparameters taking `dict` values will also be auto-completed\n      **All Texar modules** provide a :meth:`default_hparams` containing\n      allowed hyperparameters and their default values. For example:\n\n        .. code-block:: python\n\n            ## Recursive auto-completion\n            default_hparams = {""a"": 1, ""b"": {""c"": 2, ""d"": 3}}\n            hparams = {""b"": {""c"": 22}}\n            hparams_ = HParams(hparams, default_hparams)\n            hparams_.todict() == {""a"": 1, ""b"": {""c"": 22, ""d"": 3}}\n                # ""a"" and ""d"" are auto-completed\n\n            ## All Texar modules have built-in `default_hparams`\n            hparams = {""dropout_rate"": 0.1}\n            emb = tx.modules.WordEmbedder(hparams=hparams, ...)\n            emb.hparams.todict() == {\n                ""dropout_rate"": 0.1,  # provided value\n                ""dim"": 100            # default value\n                ...\n            }\n\n    - **Automatic type-check.** For most hyperparameters, provided value must\n      have the same or compatible dtype with the default value. :class:`HParams`\n      does necessary type-check, and raises Error if improper dtype is provided.\n      Also, hyperparameters not listed in `default_hparams` are not allowed,\n      except for `""kwargs""` as detailed below.\n\n    - **Flexible dtype for specified hyperparameters.**  Some hyperparameters\n      may allow different dtypes of values.\n\n        - Hyperparameters named `""type""` are not type-checked.\n          For example, in :func:`~texar.torch.core.get_rnn_cell`, hyperparameter\n          `""type""` can take value of an RNNCell class, its string name of module\n          path, or an RNNCell class instance. (String name or module path is\n          allowed so that users can specify the value in YAML configuration\n          files.)\n\n        - For other hyperparameters, list them in the `""@no_typecheck""` field\n          in :meth:`default_hparams` to skip type-check. For example, in\n          :class:`~texar.torch.modules.Conv1DNetwork`, hyperparameter\n          `""kernel_size""` can be set to either a `list` of `int`\\ s or simply\n          an `int`.\n\n    - **Special flexibility of keyword argument hyperparameters.**\n      Hyperparameters named ``""kwargs""`` are used as keyword arguments for a\n      class constructor or a function call. Such hyperparameters take a `dict`,\n      and users can add arbitrary valid keyword arguments to the dict.\n      For example:\n\n        .. code-block:: python\n\n            default_rnn_cell_hparams = {\n                ""type"": ""LSTMCell"",\n                ""kwargs"": {""num_units"": 256}\n                # Other hyperparameters\n                ...\n            }\n            my_hparams = {\n                ""kwargs"" {\n                    ""num_units"": 123,\n                    # Other valid keyword arguments for LSTMCell constructor\n                    ""forget_bias"": 0.0\n                    ""activation"": ""torch.nn.functional.relu""\n                }\n            }\n            _ = HParams(my_hparams, default_rnn_cell_hparams)\n\n    - **Rich interfaces.** An :class:`HParams` instance provides rich interfaces\n      for accessing, updating, or adding hyperparameters.\n\n        .. code-block:: python\n\n            hparams = HParams(my_hparams, default_hparams)\n            # Access\n            hparams.type == hparams[""type""]\n            # Update\n            hparams.type = ""GRUCell""\n            hparams.kwargs = { ""num_units"": 100 }\n            hparams.kwargs.num_units == 100\n            # Add new\n            hparams.add_hparam(""index"", 1)\n            hparams.index == 1\n\n            # Convert to `dict` (recursively)\n            type(hparams.todic()) == dict\n\n            # I/O\n            pickle.dump(hparams, ""hparams.dump"")\n            with open(""hparams.dump"", \'rb\') as f:\n                hparams_loaded = pickle.load(f)\n\n\n    Args:\n        hparams: A `dict` or an :class:`HParams` instance containing\n            hyperparameters. If `None`, all hyperparameters are set to default\n            values.\n        default_hparams (dict): Hyperparameters with default values. If `None`,\n            Hyperparameters are fully defined by :attr:`hparams`.\n        allow_new_hparam (bool): If `False` (default), :attr:`hparams` cannot\n            contain hyperparameters that are not included in\n            :attr:`default_hparams`, except for the case of :attr:`""kwargs""` as\n            above.\n    """"""\n\n    # - The default hyperparameters in :attr:`""kwargs""` are used (for type-check\n    #   and complementing missing hyperparameters) only when :attr:`""type""`\n    #   takes default value (i.e., missing in :attr:`hparams` or set to\n    #   the same value with the default). In this case :attr:`kwargs` allows to\n    #   contain new keys not included in :attr:`default_hparams[""kwargs""]`.\n    #\n    # - If :attr:`""type""` is set to an other value and :attr:`""kwargs""` is\n    #   missing in :attr:`hparams`, :attr:`""kwargs""` is set to an empty\n    #   dictionary.\n\n    def __init__(self, hparams: Optional[Union[\'HParams\', Dict[str, Any]]],\n                 default_hparams: Optional[Dict[str, Any]],\n                 allow_new_hparam: bool = False):\n        if isinstance(hparams, HParams):\n            hparams = hparams.todict()\n        if default_hparams is not None:\n            parsed_hparams = self._parse(\n                hparams, default_hparams, allow_new_hparam)\n        else:\n            parsed_hparams = self._parse(hparams, hparams)\n        super().__setattr__(\'_hparams\', parsed_hparams)\n\n    @staticmethod\n    def _parse(hparams: Optional[Dict[str, Any]],\n               default_hparams: Optional[Dict[str, Any]],\n               allow_new_hparam: bool = False):\n        r""""""Parses hyperparameters.\n\n        Args:\n            hparams (dict): Hyperparameters. If `None`, all hyperparameters are\n                set to default values.\n            default_hparams (dict): Hyperparameters with default values.\n                If `None`,Hyperparameters are fully defined by :attr:`hparams`.\n            allow_new_hparam (bool): If `False` (default), :attr:`hparams`\n                cannot contain hyperparameters that are not included in\n                :attr:`default_hparams`, except the case of :attr:`""kwargs""`.\n\n        Return:\n            A dictionary of parsed hyperparameters. Returns `None` if both\n            :attr:`hparams` and :attr:`default_hparams` are `None`.\n\n        Raises:\n            ValueError: If :attr:`hparams` is not `None` and\n                :attr:`default_hparams` is `None`.\n            ValueError: If :attr:`default_hparams` contains ""kwargs"" not does\n                not contains ""type"".\n        """"""\n        if hparams is None and default_hparams is None:\n            return None\n\n        if hparams is None:\n            return HParams._parse(default_hparams, default_hparams)\n\n        if default_hparams is None:\n            raise ValueError(""`default_hparams` cannot be `None` if `hparams` ""\n                             ""is not `None`."")\n        no_typecheck_names = default_hparams.get(\'@no_typecheck\', [])\n\n        if ""kwargs"" in default_hparams and ""type"" not in default_hparams:\n            raise ValueError(""Ill-defined hyperparameter structure: \'kwargs\' ""\n                             ""must accompany with \'type\'."")\n\n        parsed_hparams = copy.deepcopy(default_hparams)\n\n        # Parse recursively for params of type dictionary that are missing\n        # in `hparams`.\n        for name, value in default_hparams.items():\n            if name not in hparams and isinstance(value, dict):\n                if (name == \'kwargs\' and \'type\' in hparams and\n                        hparams[\'type\'] != default_hparams[\'type\']):\n                    # Set params named ""kwargs"" to empty dictionary if ""type""\n                    # takes value other than default.\n                    parsed_hparams[name] = HParams({}, {})\n                else:\n                    parsed_hparams[name] = HParams(value, value)\n\n        # Parse hparams\n        for name, value in hparams.items():\n            if name not in default_hparams:\n                if allow_new_hparam:\n                    parsed_hparams[name] = HParams._parse_value(value, name)\n                    continue\n                raise ValueError(\n                    ""Unknown hyperparameter: %s. Only hyperparameters ""\n                    ""named \'kwargs\' hyperparameters can contain new ""\n                    ""entries undefined in default hyperparameters."" % name)\n\n            if value is None:\n                parsed_hparams[name] = HParams._parse_value(\n                    parsed_hparams[name])\n\n            default_value = default_hparams[name]\n            if default_value is None:\n                parsed_hparams[name] = HParams._parse_value(value)\n                continue\n\n            # Parse recursively for params of type dictionary.\n            if isinstance(value, dict):\n                if (name not in no_typecheck_names and\n                        not isinstance(default_value, dict)):\n                    raise ValueError(\n                        ""Hyperparameter \'%s\' must have type %s, got %s"" %\n                        (name, _type_name(default_value), _type_name(value)))\n                if name == ""kwargs"":\n                    if (""type"" in hparams and\n                            hparams[""type""] != default_hparams[\'type\']):\n                        # Leave ""kwargs"" as-is if ""type"" takes value\n                        # other than default.\n                        parsed_hparams[name] = HParams(value, value)\n                    else:\n                        # Allow new hyperparameters if ""type"" takes default\n                        # value\n                        parsed_hparams[name] = HParams(\n                            value, default_value, allow_new_hparam=True)\n                elif name in no_typecheck_names:\n                    parsed_hparams[name] = HParams(value, value)\n                else:\n                    parsed_hparams[name] = HParams(\n                        value, default_value, allow_new_hparam)\n                continue\n\n            # Do not type-check hyperparameter named ""type"" and accompanied\n            # with ""kwargs""\n            if name == \'type\' and \'kwargs\' in default_hparams:\n                parsed_hparams[name] = value\n                continue\n\n            if name in no_typecheck_names:\n                parsed_hparams[name] = value\n            elif isinstance(value, type(default_value)):\n                parsed_hparams[name] = value\n            elif callable(value) and callable(default_value):\n                parsed_hparams[name] = value\n            else:\n                try:\n                    parsed_hparams[name] = type(default_value)(value)\n                except TypeError:\n                    raise ValueError(\n                        ""Hyperparameter \'%s\' must have type %s, got %s"" %\n                        (name, _type_name(default_value), _type_name(value)))\n\n        return parsed_hparams\n\n    @staticmethod\n    def _parse_value(value: Any, name: Optional[str] = None) -> Any:\n        if isinstance(value, dict) and (name is None or name != ""kwargs""):\n            return HParams(value, None)\n        else:\n            return value\n\n    def __getattr__(self, name: str) -> Any:\n        r""""""Retrieves the value of the hyperparameter.\n        """"""\n        if name == \'_hparams\':\n            return super().__getattribute__(\'_hparams\')\n        if name not in self._hparams:\n            # Raise AttributeError to allow copy.deepcopy, etc\n            raise AttributeError(""Unknown hyperparameter: %s"" % name)\n        return self._hparams[name]\n\n    def __getitem__(self, name: str) -> Any:\n        r""""""Retrieves the value of the hyperparameter.\n        """"""\n        return self.__getattr__(name)\n\n    def __setattr__(self, name: str, value: Any):\n        r""""""Sets the value of the hyperparameter.\n        """"""\n        if name not in self._hparams:\n            raise ValueError(\n                ""Unknown hyperparameter: %s. Only the `kwargs` ""\n                ""hyperparameters can contain new entries undefined ""\n                ""in default hyperparameters."" % name)\n        self._hparams[name] = self._parse_value(value, name)\n\n    def items(self) -> ItemsView[str, Any]:\n        r""""""Returns the list of hyperparameter `(name, value)` pairs.\n        """"""\n        return self._hparams.items()\n\n    def keys(self) -> KeysView[str]:\n        r""""""Returns the list of hyperparameter names.\n        """"""\n        return self._hparams.keys()\n\n    def __iter__(self) -> Iterator[Tuple[str, Any]]:\n        for name, value in self._hparams.items():\n            yield name, value\n\n    def __len__(self) -> int:\n        return len(self._hparams)\n\n    def __contains__(self, name) -> bool:\n        return name in self._hparams\n\n    def __str__(self) -> str:\n        r""""""Return a string of the hyperparameters.\n        """"""\n        hparams_dict = self.todict()\n        return json.dumps(hparams_dict, sort_keys=True, indent=2)\n\n    def get(self, name: str, default: Optional[Any] = None) -> Any:\n        r""""""Returns the hyperparameter value for the given name. If name is not\n        available then returns :attr:`default`.\n\n        Args:\n            name (str): the name of hyperparameter.\n            default: the value to be returned in case name does not exist.\n        """"""\n        try:\n            return self.__getattr__(name)\n        except AttributeError:\n            return default\n\n    def add_hparam(self, name: str, value: Any):\n        r""""""Adds a new hyperparameter.\n        """"""\n        if (name in self._hparams) or hasattr(self, name):\n            raise ValueError(""Hyperparameter name already exists: %s"" % name)\n        self._hparams[name] = self._parse_value(value, name)\n\n    def todict(self) -> Dict[str, Any]:\n        r""""""Returns a copy of hyperparameters as a dictionary.\n        """"""\n        dict_ = copy.deepcopy(self._hparams)\n        for name, value in self._hparams.items():\n            if isinstance(value, HParams):\n                dict_[name] = value.todict()\n        return dict_\n'"
texar/torch/module_base.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for modules.\n""""""\nfrom abc import ABC\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom torch import nn\n\nfrom texar.torch.hyperparams import HParams\n\n__all__ = [\n    \'ModuleBase\',\n]\n\n\nclass ModuleBase(nn.Module, ABC):\n    r""""""Base class inherited by modules that are configurable through\n    hyperparameters.\n\n    This is a subclass of :torch_nn:`Module`.\n\n    A Texar module inheriting :class:`~texar.torch.ModuleBase` is\n    **configurable through hyperparameters**. That is, each module defines\n    allowed hyperparameters and default values. Hyperparameters not\n    specified by users will take default values.\n\n    Args:\n        hparams (dict, optional): Hyperparameters of the module. See\n            :meth:`default_hparams` for the structure and default values.\n    """"""\n\n    def __init__(self, hparams: Optional[Union[HParams,\n                                               Dict[str, Any]]] = None):\n        super().__init__()\n        if not hasattr(self, \'_hparams\'):\n            self._hparams = HParams(hparams, self.default_hparams())\n        else:\n            # Probably already parsed by subclasses. We rely on subclass\n            # implementations to get this right.\n            # As a sanity check, we require `hparams` to be `None` in this case.\n            if hparams is not None:\n                raise ValueError(\n                    ""`self._hparams` is already assigned, but `hparams` ""\n                    ""argument is not None."")\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a `dict` of hyperparameters of the module with default\n        values. Used to replace the missing values of input `hparams`\n        during module construction.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""module""\n            }\n        """"""\n        return {\n            \'name\': \'module\'\n        }\n\n    @property\n    def trainable_variables(self) -> List[nn.Parameter]:\n        r""""""The list of trainable variables (parameters) of the module.\n        Parameters of this module and all its submodules are included.\n\n        .. note::\n            The list returned may contain duplicate parameters (e.g. output\n            layer shares parameters with embeddings). For most usages, it\'s not\n            necessary to ensure uniqueness.\n        """"""\n        return [x for x in self.parameters() if x.requires_grad]\n\n    @property\n    def hparams(self) -> HParams:\n        r""""""An :class:`~texar.torch.HParams` instance. The hyperparameters\n        of the module.\n        """"""\n        return self._hparams\n\n    @property\n    def output_size(self):\n        r""""""The feature size of :meth:`forward` output tensor(s),\n        usually it is equal to the last dimension value of the output\n        tensor size.\n        """"""\n        raise NotImplementedError\n'"
texar/torch/version.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n_MAJOR = ""0""\n_MINOR = ""1""\n_REVISION = ""2-unreleased""\n\nVERSION_SHORT = ""{0}.{1}"".format(_MAJOR, _MINOR)\nVERSION = ""{0}.{1}.{2}"".format(_MAJOR, _MINOR, _REVISION)\n'"
examples/bert/data/download_glue_data.py,0,"b'""""""Script for downloading all GLUE data.\n\nAdapted from https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e\n\n""""""\nimport argparse\nimport os\nimport sys\nimport urllib.request\nimport zipfile\n\nTASKS = [""CoLA"", ""SST"", ""MRPC"", ""QQP"", ""STS"", ""MNLI"", ""SNLI"", ""QNLI"",\n         ""RTE"", ""WNLI"", ""diagnostic""]\n\n# pylint: disable=line-too-long\n\nTASK2PATH = {\n    ""CoLA"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\',\n    ""SST"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\',\n    ""MRPC"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc\',\n    ""QQP"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&token=700c6acf-160d-4d89-81d1-de4191d02cb5\',\n    ""STS"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5\',\n    ""MNLI"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce\',\n    ""SNLI"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df\',\n    ""QNLI"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601\',\n    ""RTE"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\',\n    ""WNLI"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf\',\n    ""diagnostic"": \'https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&Expires=2498860800&Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D\'}\n\n# pylint: enable=line-too-long\n\n\ndef download_and_extract(task, data_dir):\n    print(""Downloading and extracting %s..."" % task)\n    data_file = ""%s.zip"" % task\n    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n    with zipfile.ZipFile(data_file) as zip_ref:\n        zip_ref.extractall(data_dir)\n    os.remove(data_file)\n    print(""\\tCompleted!"")\n\n\ndef format_mrpc(data_dir, path_to_data):\n    print(""Processing MRPC..."")\n    mrpc_dir = os.path.join(data_dir, ""MRPC"")\n    if not os.path.isdir(mrpc_dir):\n        os.mkdir(mrpc_dir)\n    if path_to_data:\n        mrpc_train_file = os.path.join(path_to_data, ""msr_paraphrase_train.txt"")\n        mrpc_test_file = os.path.join(path_to_data, ""msr_paraphrase_test.txt"")\n    else:\n        mrpc_train_file = os.path.join(mrpc_dir, ""msr_paraphrase_train.txt"")\n        mrpc_test_file = os.path.join(mrpc_dir, ""msr_paraphrase_test.txt"")\n    assert os.path.isfile(mrpc_train_file), \\\n        f""Train data not found at {mrpc_train_file}""\n    assert os.path.isfile(mrpc_test_file), \\\n        f""Test data not found at {mrpc_test_file}""\n    urllib.request.urlretrieve(TASK2PATH[""MRPC""],\n                               os.path.join(mrpc_dir, ""dev_ids.tsv""))\n\n    dev_ids = []\n    with open(os.path.join(mrpc_dir, ""dev_ids.tsv"")) as ids_fh:\n        for row in ids_fh:\n            dev_ids.append(row.strip().split(\'\\t\'))\n\n    with open(mrpc_train_file) as data_fh, \\\n            open(os.path.join(mrpc_dir, ""train.tsv""), \'w\') as train_fh, \\\n            open(os.path.join(mrpc_dir, ""dev.tsv""), \'w\') as dev_fh:\n        header = data_fh.readline()\n        train_fh.write(header)\n        dev_fh.write(header)\n        for row in data_fh:\n            label, id1, id2, s1, s2 = row.strip().split(\'\\t\')\n            if [id1, id2] in dev_ids:\n                dev_fh.write(f""{label}\\t{id1}\\t{id2}\\t{s1}\\t{s2}\\n"")\n            else:\n                train_fh.write(f""{label}\\t{id1}\\t{id2}\\t{s1}\\t{s2}\\n"")\n    with open(mrpc_test_file) as data_fh, \\\n            open(os.path.join(mrpc_dir, ""test.tsv""), \'w\') as test_fh:\n        _ = data_fh.readline()\n        test_fh.write(""index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n"")\n        for idx, row in enumerate(data_fh):\n            label, id1, id2, s1, s2 = row.strip().split(\'\\t\')\n            test_fh.write(f""{idx:d}\\t{id1}\\t{id2}\\t{s1}\\t{s2}\\n"")\n    print(""\\tCompleted!"")\n\n\ndef download_diagnostic(data_dir):\n    print(""Downloading and extracting diagnostic..."")\n    if not os.path.isdir(os.path.join(data_dir, ""diagnostic"")):\n        os.mkdir(os.path.join(data_dir, ""diagnostic""))\n    data_file = os.path.join(data_dir, ""diagnostic"", ""diagnostic.tsv"")\n    urllib.request.urlretrieve(TASK2PATH[""diagnostic""], data_file)\n    print(""\\tCompleted!"")\n\n\ndef get_tasks(task_names):\n    task_names = task_names.split(\',\')\n    if ""all"" in task_names:\n        tasks = TASKS\n    else:\n        tasks = []\n        for task_name in task_names:\n            assert task_name in TASKS, ""Task %s not found!"" % task_name\n            tasks.append(task_name)\n    return tasks\n\n\ndef main(arguments) -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--data_dir\', help=\'directory to save data to\',\n        type=str, default=\'data\')\n    parser.add_argument(\n        \'--tasks\',\n        help=\'tasks to download data for as a comma separated string\',\n        type=str, default=\'all\')\n    parser.add_argument(\n        \'--path_to_mrpc\',\n        help=\'path to directory containing extracted MRPC data,\'\n             \'msr_paraphrase_train.txt and msr_paraphrase_text.txt\',\n        type=str, default=\'\')\n    args = parser.parse_args(arguments)\n\n    if not os.path.isdir(args.data_dir):\n        os.mkdir(args.data_dir)\n    tasks = get_tasks(args.tasks)\n\n    for task in tasks:\n        if task == \'MRPC\':\n            import subprocess\n            # pylint: disable=line-too-long\n            if not os.path.exists(""data/MRPC""):\n                subprocess.run(""mkdir data/MRPC"", shell=True, check=False)\n            subprocess.run(\n                \'wget -P data/MRPC/ https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\',\n                shell=True, check=False)\n            subprocess.run(\n                \'wget -P data/MRPC/ https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\',\n                shell=True, check=False)\n            # pylint: enable=line-too-long\n            format_mrpc(args.data_dir, args.path_to_mrpc)\n            subprocess.run(\'rm data/MRPC/msr_paraphrase_train.txt\',\n                           shell=True, check=False)\n            subprocess.run(\'rm data/MRPC/msr_paraphrase_test.txt\',\n                           shell=True, check=False)\n        elif task == \'diagnostic\':\n            download_diagnostic(args.data_dir)\n        else:\n            download_and_extract(task, args.data_dir)\n\n\nif __name__ == \'__main__\':\n    sys.exit(main(sys.argv[1:]))\n'"
examples/bert/utils/__init__.py,0,b''
examples/bert/utils/data_utils.py,0,"b'# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nThis is the Data Loading Pipeline for Sentence Classifier Task adapted from:\n    `https://github.com/google-research/bert/blob/master/run_classifier.py`\n""""""\n\nimport csv\nimport logging\nimport os\n\nimport texar.torch as tx\n\n\nclass InputExample:\n    r""""""A single training/test example for simple sequence classification.""""""\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        r""""""Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence.\n                For single sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second\n                sequence. Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n                specified for train and dev examples, but not for test examples.\n        """"""\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass InputFeatures:\n    r""""""A single set of features of data.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\nclass DataProcessor:\n    r""""""Base class for data converters for sequence classification data sets.""""""\n\n    def get_train_examples(self, data_dir):\n        r""""""Gets a collection of `InputExample`s for the train set.""""""\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        r""""""Gets a collection of `InputExample`s for the dev set.""""""\n        raise NotImplementedError()\n\n    def get_test_examples(self, data_dir):\n        r""""""Gets a collection of `InputExample`s for prediction.""""""\n        raise NotImplementedError()\n\n    def get_labels(self):\n        r""""""Gets the list of labels for this data set.""""""\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        r""""""Reads a tab separated value file.""""""\n        with open(input_file, ""r"") as f:\n            reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                lines.append(line)\n        return lines\n\n\nclass SSTProcessor(DataProcessor):\n    r""""""Processor for the MRPC data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        r""""""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        r""""""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        r""""""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        r""""""See base class.""""""\n        return [""0"", ""1""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        r""""""Creates examples for the training and dev sets.""""""\n        examples = []\n        if set_type in (\'train\', \'dev\'):\n            for (i, line) in enumerate(lines):\n                if i == 0:\n                    continue\n                guid = f""{set_type}-{i}""\n                text_a = tx.utils.compat_as_text(line[0])\n                # Single sentence classification, text_b doesn\'t exist\n                text_b = None\n                label = tx.utils.compat_as_text(line[1])\n                examples.append(InputExample(guid=guid, text_a=text_a,\n                                             text_b=text_b, label=label))\n        if set_type == \'test\':\n            for (i, line) in enumerate(lines):\n                if i == 0:\n                    continue\n                guid = f""{set_type}-{i}""\n                text_a = tx.utils.compat_as_text(line[1])\n                # Single sentence classification, text_b doesn\'t exist\n                text_b = None\n                label = \'0\'  # arbitrary set as 0\n                examples.append(InputExample(guid=guid, text_a=text_a,\n                                             text_b=text_b, label=label))\n        return examples\n\n\nclass XnliProcessor(DataProcessor):\n    r""""""Processor for the XNLI data set.""""""\n\n    def __init__(self):\n        self.language = ""zh""\n\n    def get_train_examples(self, data_dir):\n        r""""""See base class.""""""\n        lines = self._read_tsv(\n            os.path.join(data_dir, ""multinli"",\n                         f""multinli.train.{self.language}.tsv""))\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = f""train-{i}""\n            text_a = tx.utils.compat_as_text(line[0])\n            text_b = tx.utils.compat_as_text(line[1])\n            label = tx.utils.compat_as_text(line[2])\n            if label == tx.utils.compat_as_text(""contradictory""):\n                label = tx.utils.compat_as_text(""contradiction"")\n            examples.append(InputExample(guid=guid, text_a=text_a,\n                                         text_b=text_b, label=label))\n        return examples\n\n    def get_dev_examples(self, data_dir):\n        r""""""See base class.""""""\n        lines = self._read_tsv(os.path.join(data_dir, ""xnli.dev.tsv""))\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = f""dev-{i}""\n            language = tx.utils.compat_as_text(line[0])\n            if language != tx.utils.compat_as_text(self.language):\n                continue\n            text_a = tx.utils.compat_as_text(line[6])\n            text_b = tx.utils.compat_as_text(line[7])\n            label = tx.utils.compat_as_text(line[1])\n            examples.append(InputExample(guid=guid, text_a=text_a,\n                                         text_b=text_b, label=label))\n        return examples\n\n    def get_labels(self):\n        r""""""See base class.""""""\n        return [""contradiction"", ""entailment"", ""neutral""]\n\n\nclass MnliProcessor(DataProcessor):\n    r""""""Processor for the MultiNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        r""""""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        r""""""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev_matched.tsv"")),\n            ""dev_matched"")\n\n    def get_test_examples(self, data_dir):\n        r""""""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test_matched.tsv"")),\n            ""test"")\n\n    def get_labels(self):\n        r""""""See base class.""""""\n        return [""contradiction"", ""entailment"", ""neutral""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        r""""""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = f""{set_type}-{tx.utils.compat_as_text(line[0])}""\n            text_a = tx.utils.compat_as_text(line[8])\n            text_b = tx.utils.compat_as_text(line[9])\n            if set_type == ""test"":\n                label = ""contradiction""\n            else:\n                label = tx.utils.compat_as_text(line[-1])\n            examples.append(InputExample(guid=guid, text_a=text_a,\n                                         text_b=text_b, label=label))\n        return examples\n\n\nclass MrpcProcessor(DataProcessor):\n    r""""""Processor for the MRPC data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        r""""""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")),\n            ""train"")\n\n    def get_dev_examples(self, data_dir):\n        r""""""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")),\n            ""dev"")\n\n    def get_test_examples(self, data_dir):\n        r""""""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")),\n            ""test"")\n\n    def get_labels(self):\n        r""""""See base class.""""""\n        return [""0"", ""1""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        r""""""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = f""{set_type}-{i}""\n            text_a = tx.utils.compat_as_text(line[3])\n            text_b = tx.utils.compat_as_text(line[4])\n            if set_type == ""test"":\n                label = ""0""\n            else:\n                label = tx.utils.compat_as_text(line[0])\n            examples.append(InputExample(guid=guid, text_a=text_a,\n                                         text_b=text_b, label=label))\n        return examples\n\n\nclass ColaProcessor(DataProcessor):\n    r""""""Processor for the CoLA data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        r""""""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")),\n            ""train"")\n\n    def get_dev_examples(self, data_dir):\n        r""""""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")),\n            ""dev"")\n\n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")),\n            ""test"")\n\n    def get_labels(self):\n        r""""""See base class.""""""\n        return [""0"", ""1""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        r""""""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            # Only the test set has a header\n            if set_type == ""test"" and i == 0:\n                continue\n            guid = f""{set_type}-{i}""\n            if set_type == ""test"":\n                text_a = tx.utils.compat_as_text(line[1])\n                label = ""0""\n            else:\n                text_a = tx.utils.compat_as_text(line[3])\n                label = tx.utils.compat_as_text(line[1])\n            examples.append(InputExample(guid=guid, text_a=text_a,\n                                         text_b=None, label=label))\n        return examples\n\n\ndef convert_single_example(ex_index, example, label_list, max_seq_length,\n                           tokenizer):\n    r""""""Converts a single `InputExample` into a single `InputFeatures`.""""""\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n\n    input_ids, segment_ids, input_mask = \\\n        tokenizer.encode_text(text_a=example.text_a,\n                              text_b=example.text_b,\n                              max_seq_length=max_seq_length)\n\n    label_id = label_map[example.label]\n\n    # here we disable the verbose printing of the data\n    if ex_index < 0:\n        logging.info(""*** Example ***"")\n        logging.info(""guid: %s"", example.guid)\n        logging.info(""input_ids: %s"", "" "".join([str(x) for x in input_ids]))\n        logging.info(""input_ids length: %d"", len(input_ids))\n        logging.info(""input_mask: %s"", "" "".join([str(x) for x in input_mask]))\n        logging.info(""segment_ids: %s"", "" "".join([str(x) for x in segment_ids]))\n        logging.info(""label: %s (id = %d)"", example.label, label_id)\n\n    feature = InputFeatures(input_ids=input_ids,\n                            input_mask=input_mask,\n                            segment_ids=segment_ids,\n                            label_id=label_id)\n    return feature\n\n\ndef convert_examples_to_features_and_output_to_files(\n        examples, label_list, max_seq_length, tokenizer, output_file,\n        feature_types):\n    r""""""Convert a set of `InputExample`s to a pickled file.""""""\n\n    with tx.data.RecordData.writer(output_file, feature_types) as writer:\n        for (ex_index, example) in enumerate(examples):\n            feature = convert_single_example(ex_index, example, label_list,\n                                             max_seq_length, tokenizer)\n\n            features = {\n                ""input_ids"": feature.input_ids,\n                ""input_mask"": feature.input_mask,\n                ""segment_ids"": feature.segment_ids,\n                ""label_ids"": feature.label_id\n            }\n            writer.write(features)\n\n\ndef prepare_record_data(processor, tokenizer,\n                        data_dir, max_seq_length, output_dir,\n                        feature_types):\n    r""""""Prepare record data.\n    Args:\n        processor: Data Preprocessor, which must have get_labels,\n            get_train/dev/test/examples methods defined.\n        tokenizer: The Sentence Tokenizer. Generally should be\n            SentencePiece Model.\n        data_dir: The input data directory.\n        max_seq_length: Max sequence length.\n        output_dir: The directory to save the pickled file in.\n        feature_types: The original type of the feature.\n    """"""\n    label_list = processor.get_labels()\n\n    train_examples = processor.get_train_examples(data_dir)\n    train_file = os.path.join(output_dir, ""train.pkl"")\n    convert_examples_to_features_and_output_to_files(\n        train_examples, label_list, max_seq_length,\n        tokenizer, train_file, feature_types)\n\n    eval_examples = processor.get_dev_examples(data_dir)\n    eval_file = os.path.join(output_dir, ""eval.pkl"")\n    convert_examples_to_features_and_output_to_files(\n        eval_examples, label_list,\n        max_seq_length, tokenizer, eval_file, feature_types)\n\n    test_examples = processor.get_test_examples(data_dir)\n    test_file = os.path.join(output_dir, ""predict.pkl"")\n    convert_examples_to_features_and_output_to_files(\n        test_examples, label_list,\n        max_seq_length, tokenizer, test_file, feature_types)\n'"
examples/bert/utils/model_utils.py,0,"b'""""""\nModel utility functions\n""""""\n\n\ndef get_lr_multiplier(step: int, total_steps: int, warmup_steps: int) -> float:\n    r""""""Calculate the learning rate multiplier given current step and the number\n    of warm-up steps. The learning rate schedule follows a linear warm-up and\n    linear decay.\n    """"""\n    step = min(step, total_steps)\n\n    multiplier = (1 - (step - warmup_steps) / (total_steps - warmup_steps))\n\n    if warmup_steps > 0 and step < warmup_steps:\n        warmup_percent_done = step / warmup_steps\n        multiplier = warmup_percent_done\n\n    return multiplier\n'"
examples/gpt-2/utils/__init__.py,0,b''
examples/gpt-2/utils/data_utils.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils of data preprocessing for GPT2 training.\n""""""\n\nfrom typing import Any, Dict, List\n\nimport os\n\nimport texar.torch as tx\n\n\ndef read_raw_data(data_fn: str):\n    r""""""\n    Reads raw data from a file. Each line contains one example.\n    """"""\n    examples = []\n    with open(data_fn, ""r"") as fin:\n        for line in fin:\n            examples.append(line.strip())\n    return examples\n\n\ndef convert_examples_to_features_and_output_to_files(\n        examples: List[str],\n        max_seq_length: int,\n        tokenizer: tx.data.GPT2Tokenizer,\n        output_file: str,\n        feature_types: Dict[str, Any],\n        append_eos_token: bool = True):\n    r""""""Converts a set of examples to a `pickle` file.""""""\n\n    with tx.data.RecordData.writer(output_file, feature_types) as writer:\n\n        for (_, example) in enumerate(examples):\n\n            text_ids, length = tokenizer.encode_text(\n                text=example, max_seq_length=max_seq_length,\n                append_eos_token=append_eos_token)\n\n            features = {\n                ""text_ids"": text_ids,\n                ""length"": length\n            }\n            writer.write(features)  # type: ignore\n\n\ndef prepare_pickle_data(data_dir: str,\n                        max_seq_length: int,\n                        tokenizer: tx.data.GPT2Tokenizer,\n                        output_dir: str,\n                        feature_types: Dict[str, Any]):\n    r""""""Prepare the `pickle` dataset.\n    Args:\n        data_dir: The input data directory.\n        max_seq_length: Max sequence length.\n        tokenizer: The GPT-2 tokenizer.\n        output_dir: The directory to save the pickled files in.\n        feature_types: The original type of the feature.\n    """"""\n    train_fn = os.path.join(data_dir, ""train.txt"")\n    if os.path.isfile(train_fn):\n        print(""Processing %s"" % train_fn)\n        train_examples = read_raw_data(train_fn)\n        train_file = os.path.join(output_dir, ""train.pkl"")\n        convert_examples_to_features_and_output_to_files(\n            train_examples, max_seq_length, tokenizer, train_file,\n            feature_types)\n\n    dev_fn = os.path.join(data_dir, ""dev.txt"")\n    if os.path.isfile(dev_fn):\n        print(""Processing %s"" % dev_fn)\n        eval_examples = read_raw_data(dev_fn)\n        eval_file = os.path.join(output_dir, ""dev.pkl"")\n        convert_examples_to_features_and_output_to_files(\n            eval_examples, max_seq_length, tokenizer, eval_file,\n            feature_types)\n\n    test_fn = os.path.join(data_dir, ""test.txt"")\n    if os.path.isfile(test_fn):\n        print(""Processing %s"" % test_fn)\n        test_examples = read_raw_data(test_fn)\n        test_file = os.path.join(output_dir, ""test.pkl"")\n        convert_examples_to_features_and_output_to_files(\n            test_examples, max_seq_length, tokenizer, test_file,\n            feature_types, append_eos_token=False)\n'"
examples/transformer/utils/__init__.py,0,b''
examples/transformer/utils/data_utils.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Data read/write utilities for Transformer.\n""""""\n\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\nimport torch\n\nimport texar.torch as tx\n\nExample = Tuple[np.ndarray, np.ndarray]\n\n\nclass CustomBatchingStrategy(tx.data.BatchingStrategy[Example]):\n    r""""""Create dynamically-sized batches for paired text data so that the total\n    number of source and target tokens (including padding) inside each batch is\n    constrained.\n\n    Args:\n        max_tokens (int): The maximum number of source or target tokens inside\n            each batch.\n    """"""\n    max_src_len: int\n    max_tgt_len: int\n    cur_batch_size: int\n\n    def __init__(self, max_tokens: int):\n        self.max_tokens = max_tokens\n\n    def reset_batch(self) -> None:\n        self.max_src_len = 0\n        self.max_tgt_len = 0\n        self.cur_batch_size = 0\n\n    def add_example(self, ex: Example) -> bool:\n        max_src_len = max(self.max_src_len, len(ex[0]))\n        max_tgt_len = max(self.max_tgt_len, len(ex[1]))\n        if ((self.cur_batch_size + 1) *\n                max(max_src_len, max_tgt_len) > self.max_tokens):\n            return False\n        self.max_src_len = max_src_len\n        self.max_tgt_len = max_tgt_len\n        self.cur_batch_size += 1\n        return True\n\n\nclass Seq2SeqData(tx.data.DatasetBase[Example, Example]):\n    r""""""A dataset that reads processed paired text from dumped NumPy files.\n\n    Args:\n        filename (str): The path to the dumped NumPy file.\n        hparams: A `dict` or instance of :class:`~texar.HParams` containing\n            hyperparameters. See :meth:`default_hparams` for the defaults.\n        device: The device of the produces batches. For GPU training, set to\n            current CUDA device.\n    """"""\n\n    def __init__(self, filename: str, hparams=None,\n                 device: Optional[torch.device] = None):\n        data: List[Example] = np.load(\n            filename,\n            encoding=""latin1"",\n            allow_pickle=True).tolist()\n        source = tx.data.SequenceDataSource(data)\n        super().__init__(source, hparams, device)\n\n    @staticmethod\n    def default_hparams():\n        return {\n            **tx.data.DatasetBase.default_hparams(),\n            ""pad_id"": 0,\n            ""bos_id"": 1,\n            ""eos_id"": 2,\n        }\n\n    def collate(self, examples: List[Example]) -> tx.data.Batch:\n        # Add EOS tokens.\n        src_seqs = [ex[0].tolist() + [self._hparams.eos_id] for ex in examples]\n        tgt_seqs = [ex[1].tolist() + [self._hparams.eos_id] for ex in examples]\n        # Pad sentences to equal length.\n        source, _ = tx.data.padded_batch(\n            src_seqs, pad_value=self._hparams.pad_id)\n        target_output, _ = tx.data.padded_batch(\n            tgt_seqs, pad_value=self._hparams.pad_id)\n        # Add BOS token to the target inputs.\n        target_input = np.pad(\n            target_output[:, :-1], ((0, 0), (1, 0)),\n            mode=""constant"", constant_values=self._hparams.bos_id)\n        source, target_input, target_output = [\n            torch.from_numpy(x)\n            for x in [source, target_input, target_output]\n        ]\n        return tx.data.Batch(\n            len(examples),\n            source=source,\n            target_input=target_input,\n            target_output=target_output,\n        )\n'"
examples/transformer/utils/preprocess.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\npreprocessing text data. Generally it\'s to generate plain text vocab file,\ntruncate sequence by length, generate the preprocessed dataset.\n""""""\nimport argparse\nimport collections\nimport json\nimport os\nimport pickle\nimport re\n\nimport numpy as np\n\nsplit_pattern = re.compile(r\'([.,!?""\\\':;)(])\')\ndigit_pattern = re.compile(r""\\d"")\n\n# Refer to\n# https://texar.readthedocs.io/en/latest/_modules/texar/data/vocabulary.html\n# these tokens will by default have token ids 0, 1, 2, 3 respectively\npad_token_id, bos_token_id, eos_token_id, unk_token_id = 0, 1, 2, 3\n\n\ndef split_sentence(s, tok=False):\n    """"""split sentence with some segmentation rules.""""""\n    if tok:\n        s = s.lower()\n        s = s.replace(""\\u2019"", ""\'"")\n        s = digit_pattern.sub(""0"", s)\n    words = []\n    for word in s.split():\n        if tok:\n            words.extend(split_pattern.split(word))\n        else:\n            words.append(word)\n    words = [w for w in words if w]\n    return words\n\n\ndef open_file(path):\n    """"""more robust open function""""""\n    return open(path, encoding=""utf-8"")\n\n\ndef read_file(path, tok=False):\n    """"""a generator to generate each line of file.""""""\n    with open_file(path) as f:\n        for line in f.readlines():\n            words = split_sentence(line.strip(), tok)\n            yield words\n\n\ndef count_words(path, max_vocab_size=40000, tok=False):\n    """"""count all words in the corpus and output a counter""""""\n    counts = collections.Counter()\n    for words in read_file(path, tok):\n        for word in words:\n            counts[word] += 1\n\n    vocab = [word for (word, _) in counts.most_common(max_vocab_size)]\n    return vocab\n\n\ndef make_array(word_id, words):\n    """"""generate id numpy array from plain text words.""""""\n    ids = [word_id.get(word, unk_token_id) for word in words]\n    return np.array(ids, ""i"")\n\n\ndef make_dataset(path, w2id, tok=False):\n    """"""generate dataset.""""""\n    dataset, npy_dataset = [], []\n    token_count, unknown_count = 0, 0\n    for words in read_file(path, tok):\n        array = make_array(w2id, words)\n        npy_dataset.append(array)\n        dataset.append(words)\n        token_count += array.size\n        unknown_count += (array == unk_token_id).sum()\n    print(""# of tokens:{}"".format(token_count))\n    print(\n        ""# of unknown {} {:.2}"".format(\n            unknown_count, 100.0 * unknown_count / token_count\n        )\n    )\n    return dataset, npy_dataset\n\n\ndef get_preprocess_args():\n    """"""Data preprocessing options.""""""\n\n    class Config:\n        pass\n\n    config = Config()\n    parser = argparse.ArgumentParser(description=""Preprocessing Options"")\n    parser.add_argument(\n        ""--source_vocab"",\n        type=int,\n        default=40000,\n        help=""Vocabulary size of source language"",\n    )\n    parser.add_argument(\n        ""--target_vocab"",\n        type=int,\n        default=40000,\n        help=""Vocabulary size of target language"",\n    )\n    parser.add_argument(\n        ""--tok"", dest=""tok"", action=""store_true"",\n        help=""tokenized and lowercased""\n    )\n    parser.set_defaults(tok=False)\n    parser.add_argument(""--max_seq_length"", type=int, default=70)\n    parser.add_argument(""--pre_encoding"", type=str, default=""spm"")\n    parser.add_argument(""--src"", type=str, default=""en"")\n    parser.add_argument(""--tgt"", type=str, default=""vi"")\n    parser.add_argument(\n        ""--input_dir"",\n        ""-i"",\n        type=str,\n        default=""./data/en_vi/data/"",\n        help=""Input directory"",\n    )\n    parser.add_argument(\n        ""--save_data"",\n        type=str,\n        default=""preprocess"",\n        help=""Output file for the prepared data"",\n    )\n    parser.parse_args(namespace=config)\n\n    # keep consistent with original implementation\n    # pylint:disable=attribute-defined-outside-init\n    config.input = config.input_dir\n    config.source_train = ""train."" + config.src\n    config.target_train = ""train."" + config.tgt\n    config.source_valid = ""valid."" + config.src\n    config.target_valid = ""valid."" + config.tgt\n    config.source_test = ""test."" + config.src\n    config.target_test = ""test."" + config.tgt\n    return config\n\n\ndef main() -> None:\n    args = get_preprocess_args()\n\n    print(json.dumps(args.__dict__, indent=4))\n\n    # pylint:disable=no-member\n    # Vocab Construction\n    source_path = os.path.join(args.input_dir, args.source_train)\n    target_path = os.path.join(args.input_dir, args.target_train)\n\n    src_cntr = count_words(source_path, args.source_vocab, args.tok)\n    trg_cntr = count_words(target_path, args.target_vocab, args.tok)\n    all_words = sorted(list(set(src_cntr + trg_cntr)))\n\n    vocab = [""<pad>"", ""<bos>"", ""<eos>"", ""<unk>""] + all_words\n\n    w2id = {word: index for index, word in enumerate(vocab)}\n\n    # Train Dataset\n    source_data, source_npy = make_dataset(source_path, w2id, args.tok)\n    target_data, target_npy = make_dataset(target_path, w2id, args.tok)\n    assert len(source_data) == len(target_data)\n\n    train_data = [\n        (s, t)\n        for s, t in zip(source_data, target_data)\n        if (s and len(s) < args.max_seq_length and\n            t and len(t) < args.max_seq_length)\n    ]\n    train_npy = [\n        (s, t)\n        for s, t in zip(source_npy, target_npy)\n        if 0 < len(s) < args.max_seq_length and 0 < len(t) < args.max_seq_length\n    ]\n    assert len(train_data) == len(train_npy)\n\n    # Display corpus statistics\n    print(""Vocab: {} with special tokens"".format(len(vocab)))\n    print(""Original training data size: %d"" % len(source_data))\n    print(""Filtered training data size: %d"" % len(train_data))\n\n    # Valid Dataset\n    source_path = os.path.join(args.input_dir, args.source_valid)\n    source_data, source_npy = make_dataset(source_path, w2id, args.tok)\n    target_path = os.path.join(args.input_dir, args.target_valid)\n    target_data, target_npy = make_dataset(target_path, w2id, args.tok)\n    assert len(source_data) == len(target_data)\n\n    valid_data = [(s, t) for s, t in zip(source_data, target_data) if s and t]\n    valid_npy = [\n        (s, t) for s, t in zip(source_npy, target_npy)\n        if len(s) > 0 and len(t) > 0\n    ]\n    assert len(valid_data) == len(valid_npy)\n    print(""Original dev data size: %d"" % len(source_data))\n    print(""Filtered dev data size: %d"" % len(valid_data))\n\n    # Test Dataset\n    source_path = os.path.join(args.input_dir, args.source_test)\n    source_data, source_npy = make_dataset(source_path, w2id, args.tok)\n    target_path = os.path.realpath(\n        os.path.join(args.input_dir, args.target_test))\n    target_data, target_npy = make_dataset(target_path, w2id, args.tok)\n    assert len(source_data) == len(target_data)\n    test_data = [(s, t) for s, t in zip(source_data, target_data) if s and t]\n    test_npy = [\n        (s, t) for s, t in zip(source_npy, target_npy)\n        if len(s) > 0 and len(t) > 0\n    ]\n    print(""Original test data size: %d"" % len(source_data))\n    print(""Filtered test data size: %d"" % len(test_data))\n    id2w = {i: w for w, i in w2id.items()}\n    # Save the dataset to numpy files\n    train_src_output = os.path.join(\n        args.input_dir, args.save_data + ""train."" + args.src + "".txt""\n    )\n    train_tgt_output = os.path.join(\n        args.input_dir, args.save_data + ""train."" + args.tgt + "".txt""\n    )\n    dev_src_output = os.path.join(\n        args.input_dir, args.save_data + ""dev."" + args.src + "".txt""\n    )\n    dev_tgt_output = os.path.join(\n        args.input_dir, args.save_data + ""dev."" + args.tgt + "".txt""\n    )\n    test_src_output = os.path.join(\n        args.input_dir, args.save_data + ""test."" + args.src + "".txt""\n    )\n    test_tgt_output = os.path.join(\n        args.input_dir, args.save_data + ""test."" + args.tgt + "".txt""\n    )\n\n    np.save(os.path.join(args.input, args.save_data + ""train.npy""), train_npy)\n    np.save(os.path.join(args.input, args.save_data + ""valid.npy""), valid_npy)\n    np.save(os.path.join(args.input, args.save_data + ""test.npy""), test_npy)\n    with open(os.path.join(args.input,\n                           args.save_data + ""vocab.pickle""), ""wb"") as f:\n        pickle.dump(id2w, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n    with open(train_src_output, ""w+"", encoding=""utf-8"") as fsrc, \\\n            open(train_tgt_output, ""w+"", encoding=""utf-8"") as ftgt:\n        for words in train_data:\n            fsrc.write(""{}\\n"".format("" "".join(words[0])))\n            ftgt.write(""{}\\n"".format("" "".join(words[1])))\n    with open(dev_src_output, ""w+"", encoding=""utf-8"") as fsrc, \\\n            open(dev_tgt_output, ""w+"", encoding=""utf-8"") as ftgt:\n        for words in valid_data:\n            fsrc.write(""{}\\n"".format("" "".join(words[0])))\n            ftgt.write(""{}\\n"".format("" "".join(words[1])))\n    with open(test_src_output, ""w+"", encoding=""utf-8"") as fsrc, \\\n            open(test_tgt_output, ""w+"", encoding=""utf-8"") as ftgt:\n        for words in test_data:\n            fsrc.write(""{}\\n"".format("" "".join(words[0])))\n            ftgt.write(""{}\\n"".format("" "".join(words[1])))\n    vocab_path = args.save_data + ""vocab.text""\n    with open(os.path.join(args.input_dir, vocab_path), ""w+"") as f:\n        max_size = len(id2w)\n        for idx in range(4, max_size):\n            f.write(""{}\\n"".format(id2w[idx]))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/transformer/utils/utils.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nHelper functions for model training.\n""""""\n\nimport math\n\n\ndef get_lr_multiplier(step: int, warmup_steps: int) -> float:\n    r""""""Calculate the learning rate multiplier given current step and the number\n    of warm-up steps. The learning rate schedule follows a linear warm-up and\n    square-root decay.\n    """"""\n    multiplier = (min(1.0, step / warmup_steps) *\n                  (1 / math.sqrt(max(step, warmup_steps))))\n    return multiplier\n'"
examples/xlnet/configs/config_data_imdb.py,0,"b'# Random seed. Set to `None` to disable.\nseed = 19260817\n# Name of the task to train on.\ntask = ""IMDB""\n\n# Maximum sequence length.\nmax_seq_len = 512\n# Batch size during training.\nbatch_size = 4\n# Accumulate gradients across multiple batches before performing an\n# optimizer step.\nbackwards_per_step = 16\n# Batch size during evaluation.\neval_batch_size = 64\n\n# Number of steps to train.\ntrain_steps = 4000\n# Base (maximum) learning rate.\nlr = 2e-5\n# Number of warm-up steps. Learning rate linearly grows to base LR from zero in\n# this many steps.\nwarmup_steps = 500\n# Per-layer LR scaling coefficient.\n# - Topmost (highest ID) layer: LR[N] = lr.\n# - Lower layers: LR[x - 1] = LR[x] * lr_layer_decay_rate.\nlr_layer_decay_rate = 1.0\n# Ratio of minimum LR. By end of training, LR will become lr * min_lr_ratio.\nmin_lr_ratio = 0.0\n# Epsilon value for Adam optimizer.\nadam_eps = 1e-8\n# Weight decay rate. When value is greater than zero, BertAdam optimizer\n# is used.\nweight_decay = 0.0\n# Maximum norm for gradient clipping.\ngrad_clip = 1.0\n\n# Display training stats per this many steps.\ndisplay_steps = 100\n# Evaluate model per this many steps. Set to -1 to only evaluate\n# at end of training.\neval_steps = 500\n# Save model per this many steps. Set to -1 to only evaluate\n# at end of training.\nsave_steps = 500\n'"
examples/xlnet/configs/config_data_stsb.py,0,"b'# Random seed. Set to `None` to disable.\nseed = 19260817\n# Name of the task to train on.\ntask = ""STS-B""\n\n# Maximum sequence length.\nmax_seq_len = 128\n# Batch size during training.\nbatch_size = 4\n# Accumulate gradients across multiple batches before performing an\n# optimizer step.\nbackwards_per_step = 8\n# Batch size during evaluation.\neval_batch_size = 64\n\n# Number of steps to train.\ntrain_steps = 1200\n# Base (maximum) learning rate.\nlr = 5e-5\n# Number of warm-up steps. Learning rate linearly grows to base LR from zero in\n# this many steps.\nwarmup_steps = 120\n# Per-layer LR scaling coefficient.\n# - Topmost (highest ID) layer: LR[N] = lr.\n# - Lower layers: LR[x - 1] = LR[x] * lr_layer_decay_rate.\nlr_layer_decay_rate = 1.0\n# Ratio of minimum LR. By end of training, LR will become lr * min_lr_ratio.\nmin_lr_ratio = 0.0\n# Epsilon value for Adam optimizer.\nadam_eps = 1e-8\n# Weight decay rate. When value is greater than zero, BertAdam optimizer\n# is used.\nweight_decay = 0.0\n# Maximum norm for gradient clipping.\ngrad_clip = 1.0\n\n# Display training stats per this many steps.\ndisplay_steps = 100\n# Evaluate model per this many steps. Set to -1 to only evaluate\n# at end of training.\neval_steps = 500\n# Save model per this many steps. Set to -1 to only evaluate\n# at end of training.\nsave_steps = 500\n'"
examples/xlnet/utils/__init__.py,0,b''
examples/xlnet/utils/data_utils.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nData preprocessing utilities. Adapted from\nhttps://github.com/zihangdai/xlnet/blob/master/classifier_utils.py\nhttps://github.com/zihangdai/xlnet/blob/master/prepro_utils.py\n""""""\n\nfrom typing import List, TypeVar\n\nimport texar.torch as tx\n\n__all__ = [\n    ""convert_single_example"",\n]\n\nT = TypeVar(\'T\')\n\n\nclass PaddingInputExample:\n    """"""Fake example so the num input examples is a multiple of the batch size.\n    When running eval/predict on the TPU, we need to pad the number of examples\n    to be a multiple of the batch size, because the TPU requires a fixed batch\n    size. The alternative is to drop the last batch, which is bad because it\n    means the entire output data won\'t be generated.\n    We use this class instead of `None` because treating `None` as padding\n    batches could cause silent errors.\n    """"""\n\n\nclass InputFeatures:\n    """"""A single set of features of data.""""""\n\n    def __init__(self,\n                 input_ids,\n                 input_mask,\n                 segment_ids,\n                 label_id,\n                 is_real_example=True):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n        self.is_real_example = is_real_example\n\n\ndef convert_single_example(example, label_list: List[str], max_seq_length: int,\n                           tokenizer: tx.data.XLNetTokenizer) -> InputFeatures:\n    r""""""Converts a single `InputExample` into a single `InputFeatures`.""""""\n\n    if isinstance(example, PaddingInputExample):\n        return InputFeatures(\n            input_ids=[0] * max_seq_length,\n            input_mask=[1] * max_seq_length,\n            segment_ids=[0] * max_seq_length,\n            label_id=0,\n            is_real_example=False)\n\n    input_ids, segment_ids, input_mask = \\\n        tokenizer.encode_text(text_a=example.text_a,\n                              text_b=example.text_b,\n                              max_seq_length=max_seq_length)\n\n    if len(label_list) > 0:\n        label_id = label_list.index(example.label)\n    else:\n        label_id = example.label\n\n    feature = InputFeatures(\n        input_ids=input_ids,\n        input_mask=input_mask,\n        segment_ids=segment_ids,\n        label_id=label_id)\n    return feature\n'"
examples/xlnet/utils/dataset.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nDataset construction routines. Adapted from\nhttps://github.com/zihangdai/xlnet/blob/master/run_classifier.py#L395-L508\n""""""\n\nimport collections\nimport logging\nimport os\nfrom typing import Any, Dict, Optional\n\nimport torch\nimport tqdm\nimport texar.torch as tx\n\nfrom utils import data_utils\nfrom utils.processor import DataProcessor, get_processor_class\n\n__all__ = [\n    ""construct_dataset"",\n    ""load_datasets"",\n]\n\n\ndef get_record_feature_types(seq_length: int, is_regression: bool):\n    name_to_features = {\n        ""input_ids"": (torch.long, \'stacked_tensor\', seq_length),\n        ""input_mask"": (torch.float, \'stacked_tensor\', seq_length),\n        ""segment_ids"": (torch.long, \'stacked_tensor\', seq_length),\n        ""label_ids"": (torch.long, \'stacked_tensor\'),\n        ""is_real_example"": (torch.long, \'stacked_tensor\'),\n    }\n    if is_regression:\n        name_to_features[""label_ids""] = (torch.float, \'stacked_tensor\')\n    return name_to_features\n\n\ndef construct_dataset(processor: DataProcessor, output_dir: str,\n                      max_seq_length: int, tokenizer: tx.data.XLNetTokenizer,\n                      file_prefix: Optional[str] = None,\n                      overwrite_data: bool = False):\n    """"""Convert a set of `InputExample`s to a TFRecord file.""""""\n\n    file_prefix = \'\' if file_prefix is None else file_prefix + \'.\'\n    # do not create duplicated records\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    elif (os.path.exists(os.path.join(output_dir, f""{file_prefix}train.pkl""))\n          and not overwrite_data):\n        logging.info(""Processed dataset with prefix \\""%s\\"" exists in %s, will ""\n                     ""not overwrite."", file_prefix, output_dir)\n        return\n\n    logging.info(""Creating dataset in directory %s."", output_dir)\n\n    feature_types = get_record_feature_types(\n        max_seq_length, is_regression=processor.is_regression)\n\n    split_examples = {\n        \'train\': processor.get_train_examples(),\n        \'dev\': processor.get_dev_examples(),\n    }\n    try:\n        split_examples[\'test\'] = processor.get_test_examples()\n    except (TypeError, NotImplementedError):\n        pass\n\n    for split, examples in split_examples.items():\n        output_file = os.path.join(output_dir, f""{file_prefix}{split}.pkl"")\n        writer = tx.data.RecordData.writer(output_file, feature_types)\n        for example in tqdm.tqdm(examples, ncols=80):\n            feature = data_utils.convert_single_example(\n                example, processor.labels, max_seq_length, tokenizer)\n\n            features: Dict[str, Any] = collections.OrderedDict()\n            features[""input_ids""] = feature.input_ids\n            features[""input_mask""] = feature.input_mask\n            features[""segment_ids""] = feature.segment_ids\n            if not processor.is_regression:\n                features[""label_ids""] = feature.label_id\n            else:\n                features[""label_ids""] = float(feature.label_id)\n            features[""is_real_example""] = int(feature.is_real_example)\n\n            writer.write(features)\n        writer.close()\n\n\ndef load_datasets(task: str, input_dir: str, seq_length: int, batch_size: int,\n                  drop_remainder: bool = False,\n                  file_prefix: Optional[str] = None,\n                  eval_batch_size: Optional[int] = None,\n                  shuffle_buffer: Optional[int] = None,\n                  device: Optional[torch.device] = None) \\\n        -> Dict[str, tx.data.RecordData]:\n    r""""""Creates an `input_fn` closure to be passed to TPUEstimator.""""""\n    processor_class = get_processor_class(task)\n    file_prefix = \'\' if file_prefix is None else file_prefix + \'.\'\n    eval_batch_size = eval_batch_size or batch_size\n\n    feature_types = get_record_feature_types(\n        seq_length, processor_class.is_regression)\n\n    logging.info(""Loading records with prefix \\""%s\\"" from %s"",\n                 file_prefix, input_dir)\n\n    datasets = {}\n    for split in [\'train\', \'dev\', \'test\']:\n        is_training = (split == \'train\')\n        input_file = os.path.join(input_dir, f""{file_prefix}{split}.pkl"")\n        if not os.path.exists(input_file):\n            logging.warning(""%s set does not exist for task %s"",\n                            split.capitalize(), processor_class.task_name)\n            continue\n        datasets[split] = tx.data.RecordData(\n            hparams={\n                ""dataset"": {\n                    ""files"": [input_file],\n                    ""feature_types"": feature_types,\n                },\n                ""batch_size"": (batch_size if is_training else eval_batch_size),\n                ""allow_smaller_final_batch"": not drop_remainder,\n                # ""shuffle"": is_training,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": shuffle_buffer,\n            }).to(device)\n\n    return datasets\n'"
examples/xlnet/utils/model_utils.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModel utilities.\n""""""\n\nfrom typing import Callable\n\n\n__all__ = [\n    ""warmup_lr_lambda"",\n]\n\n\ndef warmup_lr_lambda(total_steps: int, warmup_steps: int = 0,\n                     min_lr_ratio: float = 0.0) -> Callable[[int], float]:\n    r""""""Create a learning rate schedule with a linear warm-up stage and linear\n    decay.\n\n    Args:\n        total_steps (int): The total number of training steps.\n        warmup_steps (int): The number of steps in the warm-up stage.\n        min_lr_ratio (float): The LR at the end of training, represented as the\n            percentage of the base LR.\n\n    :return: A lambda function compatible with\n        `torch.optim.lr_scheduler.LambdaLR`.\n    """"""\n\n    def polynomial_lr(decay_steps: int, step: int) -> float:\n        return (1.0 - min_lr_ratio) * (1 - step / decay_steps) + min_lr_ratio\n\n    if warmup_steps == 0:\n        return lambda step: polynomial_lr(total_steps, step + 1)\n\n    def lambda_lr(step: int) -> float:\n        step += 1\n        if step <= warmup_steps:\n            return step / warmup_steps\n        return polynomial_lr(total_steps - warmup_steps, step - warmup_steps)\n\n    return lambda_lr\n'"
examples/xlnet/utils/processor.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nData processors. Adapted from\nhttps://github.com/zihangdai/xlnet/blob/master/run_classifier.py\n""""""\n\nimport csv\nimport logging\nfrom abc import ABC\nfrom pathlib import Path\nfrom typing import NamedTuple, Optional, Union, List, Dict, Type\n\n\nclass InputExample(NamedTuple):\n    r""""""A single training/test example for simple sequence classification.""""""\n    guid: str\n    r""""""Unique id for the example.""""""\n    text_a: str\n    r""""""string. The untokenized text of the first sequence. For single sequence\n    tasks, only this sequence must be specified.""""""\n    text_b: Optional[str]\n    r""""""(Optional) string. The untokenized text of the second sequence. Only\n    needs to be specified for sequence pair tasks.""""""\n    label: Optional[Union[str, float]]\n    r""""""(Optional) string. The label of the example. This should be specified\n    for train and dev examples, but not for test examples.""""""\n\n\nclass DataProcessor:\n    r""""""Base class for data converters for sequence classification data sets.""""""\n    labels: List[str]\n    is_regression: bool = False\n\n    task_name: str\n    __task_dict__: Dict[str, Type[\'DataProcessor\']] = {}\n\n    def __init__(self, data_dir: str):\n        self.data_dir = Path(data_dir)\n\n    @classmethod\n    def register(cls, *names):\n        def decorator(klass):\n            for name in names:\n                prev_processor = DataProcessor.__task_dict__.get(\n                    name.lower(), None)\n                if prev_processor is not None:\n                    raise ValueError(\n                        f""Cannot register {klass} as {name}. ""\n                        f""The name is already taken by {prev_processor}"")\n                DataProcessor.__task_dict__[name.lower()] = klass\n            klass.task_name = names[0]\n            return klass\n\n        return decorator\n\n    def get_train_examples(self) -> List[InputExample]:\n        r""""""Gets a collection of `InputExample`s for the train set.""""""\n        raise NotImplementedError\n\n    def get_dev_examples(self) -> List[InputExample]:\n        r""""""Gets a collection of `InputExample`s for the dev set.""""""\n        raise NotImplementedError\n\n    def get_test_examples(self) -> List[InputExample]:\n        r""""""Gets a collection of `InputExample`s for prediction.""""""\n        raise NotImplementedError\n\n    @classmethod\n    def _read_tsv(cls, input_file: Path,\n                  quotechar: Optional[str] = None) -> List[List[str]]:\n        """"""Reads a tab separated value file.""""""\n        with input_file.open(\'r\') as f:\n            reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                if len(line) == 0:\n                    continue\n                lines.append(line)\n            return lines\n\n\ndef get_processor_class(task: str) -> Type[DataProcessor]:\n    task = task.lower()\n    klass = DataProcessor.__task_dict__.get(task, None)\n    if klass is None:\n        raise ValueError(f""Unsupported task {task}"")\n    return klass\n\n\nclass GLUEProcessor(DataProcessor, ABC):\n    train_file = ""train.tsv""\n    dev_file = ""dev.tsv""\n    test_file = ""test.tsv""\n    label_column: int\n    text_a_column: int\n    text_b_column: int\n    contains_header = True\n    test_text_a_column: int\n    test_text_b_column: int\n    test_contains_header = True\n\n    def __init__(self, data_dir: str):\n        super().__init__(data_dir)\n        if not hasattr(self, \'test_text_a_column\'):\n            self.test_text_a_column = self.text_a_column\n        if not hasattr(self, \'test_text_b_column\'):\n            self.test_text_b_column = self.text_b_column\n\n    def get_train_examples(self) -> List[InputExample]:\n        return self._create_examples(\n            self._read_tsv(self.data_dir / self.train_file), ""train"")\n\n    def get_dev_examples(self) -> List[InputExample]:\n        return self._create_examples(\n            self._read_tsv(self.data_dir / self.dev_file), ""dev"")\n\n    def get_test_examples(self) -> List[InputExample]:\n        return self._create_examples(\n            self._read_tsv(self.data_dir / self.test_file), ""test"")\n\n    def _create_examples(self, lines: List[List[str]],\n                         set_type: str) -> List[InputExample]:\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0 and self.contains_header and set_type != ""test"":\n                continue\n            if i == 0 and self.test_contains_header and set_type == ""test"":\n                continue\n            guid = f""{set_type}-{i}""\n\n            a_column = (self.text_a_column if set_type != ""test"" else\n                        self.test_text_a_column)\n            b_column = (self.text_b_column if set_type != ""test"" else\n                        self.test_text_b_column)\n\n            # there are some incomplete lines in QNLI\n            if len(line) <= a_column:\n                logging.warning(\'Incomplete line, ignored.\')\n                continue\n            text_a = line[a_column]\n\n            if b_column is not None:\n                if len(line) <= b_column:\n                    logging.warning(\'Incomplete line, ignored.\')\n                    continue\n                text_b = line[b_column]\n            else:\n                text_b = None\n\n            if set_type == ""test"":\n                label = self.labels[0]\n            else:\n                if len(line) <= self.label_column:\n                    logging.warning(\'Incomplete line, ignored.\')\n                    continue\n                label = line[self.label_column]\n            examples.append(InputExample(guid, text_a, text_b, label))\n        return examples\n\n\n@DataProcessor.register(""MNLI"", ""MNLI_matched"")\nclass MnliMatchedProcessor(GLUEProcessor):\n    labels = [""contradiction"", ""entailment"", ""neutral""]\n\n    dev_file = ""dev_matched.tsv""\n    test_file = ""test_matched.tsv""\n    label_column = -1\n    text_a_column = 8\n    text_b_column = 9\n\n\n@DataProcessor.register(""MNLI_mismatched"")\nclass MnliMismatchedProcessor(MnliMatchedProcessor):\n    dev_file = ""dev_mismatched.tsv""\n    test_file = ""test_mismatched.tsv""\n\n\n@DataProcessor.register(""STS-B"", ""stsb"")\nclass StsbProcessor(GLUEProcessor):\n    labels: List[str] = []\n    is_regression = True\n\n    label_column = 9\n    text_a_column = 7\n    text_b_column = 8\n\n    def _create_examples(self, lines: List[List[str]],\n                         set_type: str) -> List[InputExample]:\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0 and self.contains_header and set_type != ""test"":\n                continue\n            if i == 0 and self.test_contains_header and set_type == ""test"":\n                continue\n            guid = f""{set_type}-{i}""\n\n            a_column = (self.text_a_column if set_type != ""test"" else\n                        self.test_text_a_column)\n            b_column = (self.text_b_column if set_type != ""test"" else\n                        self.test_text_b_column)\n\n            # there are some incomplete lines in QNLI\n            if len(line) <= a_column:\n                logging.warning(\'Incomplete line, ignored.\')\n                continue\n            text_a = line[a_column]\n\n            if b_column is not None:\n                if len(line) <= b_column:\n                    logging.warning(\'Incomplete line, ignored.\')\n                    continue\n                text_b = line[b_column]\n            else:\n                text_b = None\n\n            if set_type == ""test"":\n                label = 0.0\n            else:\n                if len(line) <= self.label_column:\n                    logging.warning(\'Incomplete line, ignored.\')\n                    continue\n                label = float(line[self.label_column])\n            examples.append(InputExample(guid, text_a, text_b, label))\n\n        return examples\n\n\n@DataProcessor.register(""Yelp5"")\nclass Yelp5Processor(DataProcessor):\n    labels = [""1"", ""2"", ""3"", ""4"", ""5""]\n\n    def get_train_examples(self) -> List[InputExample]:\n        return self._create_examples(self.data_dir / ""train.csv"")\n\n    def get_dev_examples(self) -> List[InputExample]:\n        return self._create_examples(self.data_dir / ""test.csv"")\n\n    def get_test_examples(self):\n        raise TypeError(""The Yelp 5 dataset does not have a test set."")\n\n    @staticmethod\n    def _create_examples(input_file: Path) -> List[InputExample]:\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        with input_file.open() as f:\n            reader = csv.reader(f)\n            for i, line in enumerate(reader):\n                label = line[0]\n                text_a = line[1].replace(\'""""\', \'""\').replace(\'\\\\""\', \'""\')\n                examples.append(InputExample(\n                    guid=str(i), text_a=text_a, text_b=None, label=label))\n        return examples\n\n\n@DataProcessor.register(""IMDB"")\nclass ImdbProcessor(DataProcessor):\n    labels = [""neg"", ""pos""]\n\n    def get_train_examples(self) -> List[InputExample]:\n        return self._create_examples(self.data_dir / ""train"")\n\n    def get_dev_examples(self) -> List[InputExample]:\n        return self._create_examples(self.data_dir / ""test"")\n\n    def get_test_examples(self):\n        raise TypeError(""The IMDB dataset does not have a test set."")\n\n    @staticmethod\n    def _create_examples(data_dir: Path) -> List[InputExample]:\n        examples = []\n        for label in [""neg"", ""pos""]:\n            cur_dir = data_dir / label\n            for filename in cur_dir.iterdir():\n                if filename.suffix != "".txt"":\n                    continue\n                with filename.open() as f:\n                    text = f.read().strip().replace(""<br />"", "" "")\n                examples.append(InputExample(\n                    guid=str(filename), text_a=text, text_b=None, label=label))\n        return examples\n'"
tests/data/data/data_iterators_test.py,17,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for data iterator related operations.\n""""""\nimport copy\nimport tempfile\nimport unittest\nfrom unittest.mock import patch\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.data.data.data_base import (\n    DatasetBase, IterDataSource, SequenceDataSource, ZipDataSource)\nfrom texar.torch.data.data.data_iterators import (\n    DataIterator, TrainTestDataIterator)\nfrom texar.torch.data.data.dataset_utils import Batch\nfrom texar.torch.data.data.mono_text_data import MonoTextData\nfrom texar.torch.data.data.sampler import TokenCountBatchingStrategy\n\n\nclass DataIteratorTest(unittest.TestCase):\n    r""""""Tests data iterators.\n    """"""\n\n    def setUp(self):\n        # Create data\n        self.train_text = list(np.linspace(1, 1000, num=1000, dtype=np.int64))\n        self.train_text = [str(x) for x in self.train_text]\n        train_text_file = tempfile.NamedTemporaryFile()\n        train_text_file.write(\'\\n\'.join(self.train_text).encode(""utf-8""))\n        train_text_file.flush()\n        self._train_text_file = train_text_file\n\n        test_text = list(np.linspace(1001, 2000, num=1000, dtype=np.int64))\n        test_text = [str(x) for x in test_text]\n        test_text_file = tempfile.NamedTemporaryFile()\n        test_text_file.write(\'\\n\'.join(test_text).encode(""utf-8""))\n        test_text_file.flush()\n        self._test_text_file = test_text_file\n\n        vocab_list = self.train_text + test_text\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        self._vocab_file = vocab_file\n        self._vocab_size = len(vocab_list)\n\n        self._train_hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 1,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": self._train_text_file.name,\n                ""vocab_file"": self._vocab_file.name,\n                ""bos_token"": \'\',\n                ""eos_token"": \'\'\n            },\n            ""name"": ""train""\n        }\n\n        self._test_hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 2,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": self._test_text_file.name,\n                ""vocab_file"": self._vocab_file.name,\n                ""bos_token"": \'\',\n                ""eos_token"": \'\'\n            },\n            ""name"": ""test""\n        }\n\n    def test_iterator_single_dataset(self):\n        r""""""Tests iterating over a single dataset.\n        """"""\n        data = MonoTextData(self._test_hparams)\n        data_iterator = DataIterator(data)\n        data_iterator.switch_to_dataset(dataset_name=""data"")\n        iterator = data_iterator.get_iterator()\n        i = 1001\n        for idx, batch in enumerate(iterator):\n            self.assertEqual(batch.batch_size, self._test_hparams[\'batch_size\'])\n            np.testing.assert_array_equal(batch[\'length\'], [1, 1])\n            for example in batch[\'text\']:\n                self.assertEqual(example[0], str(i))\n                i += 1\n        self.assertEqual(i, 2001)\n\n    def test_iterator_single_dataset_parallel(self):\n        r""""""Tests iterating over a single dataset with multiple workers.\n        """"""\n        hparams = copy.deepcopy(self._test_hparams)\n        hparams[\'num_parallel_calls\'] = 2\n        data = MonoTextData(hparams)\n        data_iterator = DataIterator(data)\n        data_iterator.switch_to_dataset(dataset_name=""data"")\n        iterator = data_iterator.get_iterator()\n        i = 1001\n        for idx, batch in enumerate(iterator):\n            self.assertEqual(batch.batch_size, self._test_hparams[\'batch_size\'])\n            np.testing.assert_array_equal(batch[\'length\'], [1, 1])\n            for example in batch[\'text\']:\n                self.assertEqual(example[0], str(i))\n                i += 1\n        self.assertEqual(i, 2001)\n\n    def test_iterator_multi_datasets(self):\n        r""""""Tests iterating over multiple datasets.\n        """"""\n        train = MonoTextData(self._train_hparams)\n        test = MonoTextData(self._test_hparams)\n        train_batch_size = self._train_hparams[""batch_size""]\n        test_batch_size = self._test_hparams[""batch_size""]\n        data_iterator = DataIterator({""train"": train, ""test"": test})\n        data_iterator.switch_to_dataset(dataset_name=""train"")\n        iterator = data_iterator.get_iterator()\n        for idx, val in enumerate(iterator):\n            self.assertEqual(len(val), train_batch_size)\n            number = idx * train_batch_size + 1\n            self.assertEqual(val.text[0], [str(number)])\n            # numbers: 1 - 2000, first 4 vocab entries are special tokens\n            self.assertEqual(val.text_ids[0], torch.tensor(number + 3))\n\n        data_iterator.switch_to_dataset(dataset_name=""test"")\n        iterator = data_iterator.get_iterator()\n        for idx, val in enumerate(iterator):\n            self.assertEqual(len(val), test_batch_size)\n            number = idx * test_batch_size + 1001\n            self.assertEqual(val.text[0], [str(number)])\n            self.assertEqual(val.text_ids[0], torch.tensor(number + 3))\n\n        # test `get_iterator` interface\n        for idx, val in enumerate(data_iterator.get_iterator(\'train\')):\n            self.assertEqual(len(val), train_batch_size)\n            number = idx * train_batch_size + 1\n            self.assertEqual(val.text[0], [str(number)])\n            self.assertEqual(val.text_ids[0], torch.tensor(number + 3))\n\n        # test exception for invalid dataset name\n        with self.assertRaises(ValueError) as context:\n            data_iterator.switch_to_dataset(\'val\')\n        self.assertTrue(\'not found\' in str(context.exception))\n\n    def test_train_test_data_iterator(self):\n        r""""""Tests :class:`texar.torch.data.TrainTestDataIterator`\n        """"""\n        train = MonoTextData(self._train_hparams)\n        test = MonoTextData(self._test_hparams)\n        train_batch_size = self._train_hparams[""batch_size""]\n        test_batch_size = self._test_hparams[""batch_size""]\n\n        data_iterator = TrainTestDataIterator(train=train, test=test)\n        data_iterator.switch_to_train_data()\n        iterator = data_iterator.get_iterator()\n\n        for idx, val in enumerate(iterator):\n            self.assertEqual(len(val), train_batch_size)\n            number = idx * train_batch_size + 1\n            self.assertEqual(val.text[0], [str(number)])\n            # numbers: 1 - 2000, first 4 vocab entries are special tokens\n            self.assertEqual(val.text_ids[0], torch.tensor(number + 3))\n\n        data_iterator.switch_to_test_data()\n        iterator = data_iterator.get_iterator()\n        for idx, val in enumerate(iterator):\n            self.assertEqual(len(val), test_batch_size)\n            number = idx * test_batch_size + 1001\n            self.assertEqual(val.text[0], [str(number)])\n            self.assertEqual(val.text_ids[0], torch.tensor(number + 3))\n\n        # test `get_*_iterator` interface\n        for idx, val in enumerate(data_iterator.get_test_iterator()):\n            self.assertEqual(len(val), test_batch_size)\n            number = idx * test_batch_size + 1001\n            self.assertEqual(val.text[0], [str(number)])\n            self.assertEqual(val.text_ids[0], torch.tensor(number + 3))\n\n        # test exception for invalid dataset name\n        with self.assertRaises(ValueError) as context:\n            data_iterator.switch_to_val_data()\n\n    def test_dynamic_batching(self):\n        r""""""Tests dynamic batching using :class:`texar.torch.data.BatchingStrategy`.\n        """"""\n        sent_lengths = np.random.randint(10, 20, size=(100,))\n        sentences = [[\'a\'] * length for length in sent_lengths]\n        data_source = SequenceDataSource(sentences)\n\n        class CustomData(DatasetBase):\n            def __init__(self, source):\n                super().__init__(source)\n\n            def collate(self, examples):\n                return Batch(len(examples), text=examples)\n\n        train_data = CustomData(data_source)\n\n        batch_size = 5\n        max_tokens = 75\n        strategy = TokenCountBatchingStrategy(\n            max_tokens, batch_size, len)\n        iterator = DataIterator(train_data, strategy)\n\n        for batch in iterator:\n            self.assertLessEqual(len(batch), batch_size)\n            self.assertLessEqual(sum(len(s) for s in batch.text), max_tokens)\n\n    @patch(""torch.cuda.is_available"", lambda: True)\n    def test_auto_storage_moving(self):\n        cuda_tensors = set()\n\n        def move_tensor(tensor, device, non_blocking=False):\n            if isinstance(device, torch.device) and device.type == ""cuda"":\n                self.assertTrue(non_blocking)\n                cuda_tensors.add(id(tensor))\n            return tensor\n\n        device = torch.device(""cuda:0"")\n\n        with patch.object(torch.Tensor, ""to"", move_tensor):\n            train = MonoTextData(self._train_hparams, device=device)\n            iterator = DataIterator(train)\n            for batch in iterator:\n                self.assertTrue(id(batch.text_ids) in cuda_tensors)\n                self.assertTrue(id(batch.length) in cuda_tensors)\n\n\nRawExample = Tuple[List[int], str]\nExample = Tuple[List[int], List[str]]\n\n\nclass MockDataBase(DatasetBase[RawExample, Example]):\n    def process(self, raw_example: RawExample) -> Example:\n        numbers, string = raw_example\n        numbers = [x + 1 for x in numbers]\n        strings = string.split()\n        return numbers, strings\n\n    def collate(self, examples: List[Example]) -> Batch:\n        numbers = np.asarray([ex[0] for ex in examples])\n        strings = np.asarray([ex[1] for ex in examples])\n        return Batch(len(numbers), numbers=numbers, strings=strings)\n\n\nclass LazinessCachingTest(unittest.TestCase):\n\n    def setUp(self) -> None:\n        self.size = 102\n        self.seq_len = 10\n        self.batch_size = 5\n        self.num_workers = 3\n\n    def _test_modes_with_workers(self, lazy_mode: str, cache_mode: str,\n                                 num_workers: int,\n                                 parallelize_processing: bool = True,\n                                 support_random_access: bool = False,\n                                 shuffle: bool = False,\n                                 **kwargs):\n        hparams = {\n            \'batch_size\': self.batch_size,\n            \'lazy_strategy\': lazy_mode,\n            \'cache_strategy\': cache_mode,\n            \'num_parallel_calls\': num_workers,\n            \'shuffle\': shuffle,\n            \'shuffle_buffer_size\': self.size // 5,\n            \'parallelize_processing\': parallelize_processing,\n            \'allow_smaller_final_batch\': False,\n            **kwargs,\n        }\n        numbers_data = [[x] * self.seq_len for x in range(self.size)]\n        string_data = [\' \'.join(map(str, range(self.seq_len)))\n                       for _ in range(self.size)]\n        if not support_random_access:\n            source = ZipDataSource(  # type: ignore\n                IterDataSource(numbers_data),\n                SequenceDataSource(string_data))\n        else:\n            source = ZipDataSource(\n                SequenceDataSource(numbers_data),\n                SequenceDataSource(string_data))\n        data = MockDataBase(source, hparams)  # type: ignore\n        iterator = DataIterator(data)\n\n        if data._hparams.allow_smaller_final_batch:\n            total_examples = self.size\n            total_batches = (self.size + self.batch_size - 1) // self.batch_size\n        else:\n            total_examples = self.size // self.batch_size * self.batch_size\n            total_batches = self.size // self.batch_size\n\n        def check_batch(idx, batch):\n            if idx == total_batches - 1:\n                batch_size = (total_examples - 1) % self.batch_size + 1\n            else:\n                batch_size = self.batch_size\n            self.assertEqual(batch.numbers.shape,\n                             (batch_size, self.seq_len))\n            if not shuffle:\n                numbers = np.asarray([idx * self.batch_size + x + 1\n                                      for x in range(batch_size)])\n                self.assertTrue(np.all(batch.numbers == numbers[:, np.newaxis]))\n\n        # check laziness\n        if parallelize_processing:\n            if lazy_mode == \'none\':\n                self.assertEqual(len(data._processed_cache), self.size)\n            else:\n                self.assertEqual(len(data._processed_cache), 0)\n                if not support_random_access:\n                    if lazy_mode == \'process\':\n                        self.assertEqual(len(data._cached_source._cache),\n                                         self.size)\n                    else:\n                        self.assertEqual(len(data._cached_source._cache), 0)\n\n        # first epoch\n        cnt = 0\n        for idx, batch in enumerate(iterator):\n            check_batch(idx, batch)\n            cnt += 1\n        self.assertEqual(cnt, total_batches)\n\n        # check cache\n        if parallelize_processing:\n            if cache_mode == \'none\':\n                self.assertEqual(len(data._processed_cache), 0)\n            elif cache_mode == \'loaded\':\n                self.assertEqual(len(data._processed_cache), 0)\n            else:\n                self.assertEqual(len(data._processed_cache), self.size)\n            if lazy_mode != \'none\' and not support_random_access:\n                if cache_mode == \'none\':\n                    self.assertEqual(len(data._cached_source._cache), 0)\n                elif cache_mode == \'loaded\':\n                    self.assertEqual(len(data._cached_source._cache), self.size)\n                else:\n                    self.assertEqual(len(data._cached_source._cache), 0)\n\n        # second epoch\n        cnt = 0\n        for idx, batch in enumerate(iterator):\n            check_batch(idx, batch)\n            cnt += 1\n        self.assertEqual(cnt, total_batches)\n\n        # check again\n        if parallelize_processing:\n            if cache_mode == \'none\':\n                self.assertEqual(len(data._processed_cache), 0)\n            elif cache_mode == \'loaded\':\n                self.assertEqual(len(data._processed_cache), 0)\n            else:\n                self.assertEqual(len(data._processed_cache), self.size)\n            if lazy_mode != \'none\' and not support_random_access:\n                if cache_mode == \'none\':\n                    self.assertEqual(len(data._cached_source._cache), 0)\n                elif cache_mode == \'loaded\':\n                    self.assertEqual(len(data._cached_source._cache), self.size)\n                else:\n                    self.assertEqual(len(data._cached_source._cache), 0)\n\n    def _test_modes(self, lazy_mode: str, cache_mode: str):\n        self._test_modes_with_workers(lazy_mode, cache_mode, self.num_workers)\n        self._test_modes_with_workers(lazy_mode, cache_mode, self.num_workers,\n                                      parallelize_processing=False)\n        self._test_modes_with_workers(lazy_mode, cache_mode, 1)\n        self._test_modes_with_workers(lazy_mode, cache_mode, self.num_workers,\n                                      support_random_access=True)\n        self._test_modes_with_workers(lazy_mode, cache_mode, self.num_workers,\n                                      shuffle=True)\n\n    def test_none_processed(self):\n        self._test_modes(\'none\', \'processed\')\n\n    def test_process_loaded(self):\n        self._test_modes(\'process\', \'loaded\')\n\n    def test_process_processed(self):\n        self._test_modes(\'process\', \'processed\')\n\n    def test_all_none(self):\n        self._test_modes(\'all\', \'none\')\n\n    def test_all_loaded(self):\n        self._test_modes(\'all\', \'loaded\')\n\n    def test_all_processed(self):\n        self._test_modes(\'all\', \'processed\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/data/large_file_test.py,10,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit test for large file related operations.\n""""""\nimport contextlib\nimport resource\nimport time\nimport unittest\nfrom typing import List, Optional, Tuple\n\nimport gc\nimport numpy as np\nimport torch\n\nfrom texar.torch.data.data.data_base import DatasetBase, DataSource\nfrom texar.torch.data.data.data_iterators import DataIterator\nfrom texar.torch.data.data.dataset_utils import Batch\nfrom texar.torch.data.data.text_data_base import TextLineDataSource\nfrom texar.torch.data.vocabulary import Vocab\nfrom texar.torch.utils.test import data_test\nfrom texar.torch.utils.utils import AnyDict\n\n\n@contextlib.contextmanager\ndef work_in_progress(msg):\n    print(msg + ""... "", flush=True)\n    begin_time = time.time()\n    yield\n    time_consumed = time.time() - begin_time\n    print(f""done. ({time_consumed:.2f}s)"", flush=True)\n\n\nRawExample = List[str]\nExample = Tuple[np.ndarray, np.ndarray]\n\n\nclass ParallelData(DatasetBase[RawExample, Example]):\n    def __init__(self, source: DataSource[RawExample],\n                 src_vocab_path: str,\n                 tgt_vocab_path: str,\n                 hparams: AnyDict,\n                 device: Optional[torch.device] = None):\n        # hparams.update(parallelize_processing=False)\n        self.src_vocab = Vocab(src_vocab_path)\n        self.tgt_vocab = Vocab(tgt_vocab_path)\n        self.device = device\n        super().__init__(source, hparams=hparams)\n\n    def process(self, raw_example: RawExample) -> Example:\n        src, tgt = raw_example\n        src = self.src_vocab.map_tokens_to_ids_py(src.split())\n        tgt = self.tgt_vocab.map_tokens_to_ids_py(tgt.split())\n        return src, tgt\n\n    def collate(self, examples: List[Example]) -> Batch:\n        src_pad_length = max(len(src) for src, _ in examples)\n        tgt_pad_length = max(len(tgt) for _, tgt in examples)\n        batch_size = len(examples)\n        src_indices = np.zeros((batch_size, src_pad_length), dtype=np.int64)\n        tgt_indices = np.zeros((batch_size, tgt_pad_length), dtype=np.int64)\n        for b_idx, (src, tgt) in enumerate(examples):\n            src_indices[b_idx, :len(src)] = src\n            tgt_indices[b_idx, :len(tgt)] = tgt\n        src_indices = torch.from_numpy(src_indices)\n        tgt_indices = torch.from_numpy(tgt_indices)\n        return Batch(batch_size, src=src_indices, tgt=tgt_indices)\n\n\ndef wrap_progress(func):\n    from tqdm import tqdm\n    return lambda: tqdm(func(), leave=False)\n\n\ndef get_process_memory():\n    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024 / 1024\n\n\n@data_test\nclass LargeFileTest(unittest.TestCase):\n    def setUp(self) -> None:\n        self.source = TextLineDataSource(\n            \'../../Downloads/en-es.bicleaner07.txt.gz\',\n            compression_type=\'gzip\', delimiter=\'\\t\')\n        self.source.__iter__ = wrap_progress(  # type: ignore\n            self.source.__iter__)\n        self.num_workers = 3\n        self.batch_size = 64\n\n    def _test_modes_with_workers(self, lazy_mode: str, cache_mode: str,\n                                 num_workers: int):\n        from tqdm import tqdm\n        gc.collect()\n        mem = get_process_memory()\n        with work_in_progress(f""Data loading with lazy mode \'{lazy_mode}\' ""\n                              f""and cache mode \'{cache_mode}\' ""\n                              f""with {num_workers} workers""):\n            print(f""Memory before: {mem:.2f} MB"")\n            with work_in_progress(""Construction""):\n                data = ParallelData(self.source,\n                                    \'../../Downloads/src.vocab\',\n                                    \'../../Downloads/tgt.vocab\',\n                                    {\'batch_size\': self.batch_size,\n                                     \'lazy_strategy\': lazy_mode,\n                                     \'cache_strategy\': cache_mode,\n                                     \'num_parallel_calls\': num_workers,\n                                     \'shuffle\': False,\n                                     \'allow_smaller_final_batch\': False,\n                                     \'max_dataset_size\': 100000})\n            print(f""Memory after construction: {mem:.2f} MB"")\n            iterator = DataIterator(data)\n            with work_in_progress(""Iteration""):\n                for batch in tqdm(iterator, leave=False):\n                    self.assertEqual(batch.batch_size, self.batch_size)\n            gc.collect()\n            print(f""Memory after iteration: {mem:.2f} MB"")\n            with work_in_progress(""2nd iteration""):\n                for batch in tqdm(iterator, leave=False):\n                    self.assertEqual(batch.batch_size, self.batch_size)\n\n    def _test_modes(self, lazy_mode: str, cache_mode: str):\n        self._test_modes_with_workers(lazy_mode, cache_mode, self.num_workers)\n        self._test_modes_with_workers(lazy_mode, cache_mode, 1)\n\n    def test_none_processed(self):\n        self._test_modes(\'none\', \'processed\')\n\n    def test_process_loaded(self):\n        self._test_modes(\'process\', \'loaded\')\n\n    def test_process_processed(self):\n        self._test_modes(\'process\', \'processed\')\n\n    def test_all_none(self):\n        self._test_modes(\'all\', \'none\')\n\n    def test_all_loaded(self):\n        self._test_modes(\'all\', \'loaded\')\n\n    def test_all_processed(self):\n        self._test_modes(\'all\', \'processed\')\n\n    def _test_all_combinations(self):\n        self.test_none_processed()\n        self.test_process_loaded()\n        self.test_process_processed()\n        self.test_all_none()\n        self.test_all_loaded()\n        self.test_all_processed()\n'"
tests/data/data/mono_text_data_test.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for data related operations.\n""""""\nimport copy\nimport tempfile\nimport unittest\n\nimport numpy as np\n\nfrom texar.torch.data.data.data_iterators import DataIterator\nfrom texar.torch.data.data.mono_text_data import MonoTextData\nfrom texar.torch.data.vocabulary import SpecialTokens\n\n\nclass MonoTextDataTest(unittest.TestCase):\n    r""""""Tests text data class.\n    """"""\n\n    def setUp(self):\n        # Create test data\n        vocab_list = [\'word\', \'\xe8\xaf\x8d\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        self._vocab_file = vocab_file\n        self._vocab_size = len(vocab_list)\n\n        text = [\'This is a test sentence .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82\']\n        text_file = tempfile.NamedTemporaryFile()\n        text_file.write(\'\\n\'.join(text).encode(""utf-8""))\n        text_file.flush()\n        self._text_file = text_file\n\n        self._hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 3,\n            ""dataset"": {\n                ""files"": self._text_file.name,\n                ""vocab_file"": self._vocab_file.name,\n            }\n        }\n\n        self.upper_cased_text = []\n        for sent in text:\n            upper_cased_tokens = sent.upper().split("" "")\n            upper_cased_tokens.insert(0, SpecialTokens.BOS)\n            upper_cased_tokens.append(SpecialTokens.EOS)\n            self.upper_cased_text.append(upper_cased_tokens)\n\n        max_length = max([len(tokens) for tokens in self.upper_cased_text])\n        self.upper_cased_text = [sent + [\'\'] * (max_length - len(sent)) for sent\n                                 in self.upper_cased_text]\n\n    def _run_and_test(self,\n                      hparams,\n                      test_batch_size=False,\n                      test_transform=False):\n        # Construct database\n        text_data = MonoTextData(hparams)\n        self.assertEqual(text_data.vocab.size,\n                         self._vocab_size + len(text_data.vocab.special_tokens))\n\n        iterator = DataIterator(text_data)\n\n        for data_batch in iterator:\n            self.assertEqual(set(data_batch.keys()),\n                             set(text_data.list_items()))\n\n            if test_batch_size:\n                self.assertEqual(len(data_batch[\'text\']), hparams[\'batch_size\'])\n\n            if test_transform:\n                for i in range(len(data_batch[\'text\'])):\n                    text_ = data_batch[\'text\'][i]\n                    self.assertTrue(text_ in self.upper_cased_text)\n\n            max_seq_length = text_data.hparams.dataset.max_seq_length\n            mode = text_data.hparams.dataset.length_filter_mode\n\n            max_l = max_seq_length\n            if max_seq_length is not None:\n                if text_data.hparams.dataset.eos_token != \'\':\n                    max_l += 1\n                if text_data.hparams.dataset.bos_token != \'\':\n                    max_l += 1\n\n            if max_seq_length == 6:\n                for length in data_batch[\'length\']:\n                    self.assertLessEqual(length, max_l)\n                if mode == ""discard"":\n                    for length in data_batch[\'length\']:\n                        self.assertEqual(length, 5)\n                elif mode == ""truncate"":\n                    num_length_6 = 0\n                    for length in data_batch[\'length\']:\n                        num_length_6 += int(length == 6)\n                    self.assertGreater(num_length_6, 0)\n                else:\n                    raise ValueError(""Unknown mode: %s"" % mode)\n\n            if text_data.hparams.dataset.pad_to_max_seq_length:\n                for x in data_batch[\'text\']:\n                    self.assertEqual(len(x), max_l)\n                for x in data_batch[\'text_ids\']:\n                    self.assertEqual(len(x), max_l)\n\n    def test_default_setting(self):\n        r""""""Tests the logic of MonoTextData.\n        """"""\n        self._run_and_test(self._hparams)\n\n    def test_batching(self):\n        r""""""Tests different batching.\n        """"""\n        # disallow smaller final batch\n        hparams = copy.deepcopy(self._hparams)\n        hparams.update({""allow_smaller_final_batch"": False})\n        self._run_and_test(hparams, test_batch_size=True)\n\n    @unittest.skip(""bucketing is not yet implemented"")\n    def test_bucketing(self):\n        r""""""Tests bucketing.\n        """"""\n        hparams = copy.deepcopy(self._hparams)\n        hparams.update({\n            ""bucket_boundaries"": [7],\n            ""bucket_batch_sizes"": [6, 4]})\n\n        text_data = MonoTextData(hparams)\n        iterator = DataIterator(text_data)\n\n        hparams.update({\n            ""bucket_boundaries"": [7],\n            ""bucket_batch_sizes"": [7, 7],\n            ""allow_smaller_final_batch"": False})\n\n        text_data_1 = MonoTextData(hparams)\n        iterator_1 = DataIterator(text_data_1)\n\n        for data_batch, data_batch_1 in zip(iterator, iterator_1):\n            length = data_batch[\'length\'][0]\n            if length < 7:\n                last_batch_size = hparams[\'num_epochs\'] % 6\n                self.assertTrue(\n                    len(data_batch[\'text\']) == 6 or\n                    len(data_batch[\'text\']) == last_batch_size)\n            else:\n                last_batch_size = hparams[\'num_epochs\'] % 4\n                self.assertTrue(\n                    len(data_batch[\'text\']) == 4 or\n                    len(data_batch[\'text\']) == last_batch_size)\n\n            self.assertEqual(len(data_batch_1[\'text\']), 7)\n\n    def test_shuffle(self):\n        r""""""Tests different shuffling strategies.\n        """"""\n        hparams = copy.deepcopy(self._hparams)\n        hparams.update({\n            ""shard_and_shuffle"": True,\n            ""shuffle_buffer_size"": 1})\n        self._run_and_test(hparams)\n\n    def test_prefetch(self):\n        r""""""Tests prefetching.\n        """"""\n        hparams = copy.deepcopy(self._hparams)\n        hparams.update({""prefetch_buffer_size"": 2})\n        self._run_and_test(hparams)\n\n    def test_other_transformations(self):\n        r""""""Tests use of other transformations\n        """"""\n\n        upper_func = lambda x: [w.upper() for w in x]\n\n        hparams = copy.deepcopy(self._hparams)\n        hparams[""dataset""].update(\n            {""other_transformations"": [upper_func]})\n        self._run_and_test(hparams, test_transform=True)\n\n    def test_list_items(self):\n        r""""""Tests the item names of the output data.\n        """"""\n        text_data = MonoTextData(self._hparams)\n        self.assertSetEqual(set(text_data.list_items()),\n                            {""text"", ""text_ids"", ""length""})\n\n        hparams = copy.deepcopy(self._hparams)\n        hparams[""dataset""][""data_name""] = ""data""\n        text_data = MonoTextData(hparams)\n        self.assertSetEqual(set(text_data.list_items()),\n                            {""data_text"", ""data_text_ids"", ""data_length""})\n\n    def test_length_discard(self):\n        r""""""Tests discard length seq.\n        """"""\n        hparams = copy.deepcopy(self._hparams)\n        hparams[""dataset""].update({""max_seq_length"": 4,\n                                   ""length_filter_mode"": ""discard""})\n        self._run_and_test(hparams)\n\n    def test_length_truncate(self):\n        r""""""Tests truncation.\n        """"""\n        hparams = copy.deepcopy(self._hparams)\n        hparams[""dataset""].update({""max_seq_length"": 4,\n                                   ""length_filter_mode"": ""truncate""})\n        hparams[""shuffle""] = False\n        hparams[""allow_smaller_final_batch""] = False\n        self._run_and_test(hparams)\n\n    def test_pad_to_max_length(self):\n        r""""""Tests padding.\n        """"""\n        hparams = copy.deepcopy(self._hparams)\n        hparams[""dataset""].update({""max_seq_length"": 10,\n                                   ""length_filter_mode"": ""truncate"",\n                                   ""pad_to_max_seq_length"": True})\n        self._run_and_test(hparams)\n\n\n@unittest.skip(""Skipping until Variable Utterance is implemented"")\nclass VarUttMonoTextDataTest(unittest.TestCase):\n    r""""""Tests variable utterance text data class.\n    """"""\n\n    def setUp(self):\n        # Create test data\n        vocab_list = [\'word\', \'sentence\', \'\xe8\xaf\x8d\', \'response\', \'dialog\', \'1\', \'2\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        self._vocab_file = vocab_file\n        self._vocab_size = len(vocab_list)\n\n        text = [\n            \'This is a dialog 1 sentence . ||| This is a dialog 1 sentence . \'\n            \'||| This is yet another dialog 1 sentence .\',  # //\n            \'This is a dialog 2 sentence . ||| \'\n            \'This is also a dialog 2 sentence . \',  # //\n            \'\xe8\xaf\x8d \xe8\xaf\x8d \xe8\xaf\x8d ||| word\',  # //\n            \'This This\',  # //\n            \'1 1 1 ||| 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ||| 1 1 1 ||| 2\'\n        ]\n        text_file = tempfile.NamedTemporaryFile()\n        text_file.write(\'\\n\'.join(text).encode(""utf-8""))\n        text_file.flush()\n        self._text_file = text_file\n\n        self._hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 3,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": self._text_file.name,\n                ""vocab_file"": self._vocab_file.name,\n                ""variable_utterance"": True,\n                ""max_utterance_cnt"": 3,\n                ""max_seq_length"": 10\n            }\n        }\n\n    def _run_and_test(self, hparams):\n        # Construct database\n        text_data = MonoTextData(hparams)\n        self.assertEqual(text_data.vocab.size,\n                         self._vocab_size + len(text_data.vocab.special_tokens))\n\n        iterator = DataIterator(text_data)\n\n        for data_batch in iterator:\n            # Run the logics\n            self.assertEqual(set(data_batch.keys()),\n                             set(text_data.list_items()))\n\n            # Test utterance count\n            utt_ind = np.sum(data_batch[""text_ids""], 2) != 0\n            utt_cnt = np.sum(utt_ind, 1)\n            self.assertListEqual(\n                data_batch[text_data.utterance_cnt_name].tolist(),\n                utt_cnt.tolist())\n\n            if text_data.hparams.dataset.pad_to_max_seq_length:\n                max_l = text_data.hparams.dataset.max_seq_length\n                max_l += text_data._decoder.added_length\n                for x in data_batch[\'text\']:\n                    for xx in x:\n                        self.assertEqual(len(xx), max_l)\n                for x in data_batch[\'text_ids\']:\n                    for xx in x:\n                        self.assertEqual(len(xx), max_l)\n\n    def test_default_setting(self):\n        r""""""Tests the logics of the text data.\n        """"""\n        self._run_and_test(self._hparams)\n\n    def test_pad_to_max_length(self):\n        r""""""Tests padding.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""dataset""].update({""max_seq_length"": 20,\n                                   ""length_filter_mode"": ""truncate"",\n                                   ""pad_to_max_seq_length"": True})\n        self._run_and_test(hparams)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/data/multi_aligned_data_test.py,5,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for data related operations.\n""""""\nimport copy\nimport os\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.data.data.data_iterators import DataIterator\nfrom texar.torch.data.data.multi_aligned_data import MultiAlignedData\nfrom texar.torch.data.data.record_data import RecordData\n\n\nclass MultiAlignedDataTest(unittest.TestCase):\n    """"""Tests multi aligned text data class.\n    """"""\n\n    def setUp(self):\n        # Create test data\n        vocab_list = [\'This\', \'is\', \'a\', \'word\', \'\xe8\xaf\x8d\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        self._vocab_file = vocab_file\n        self._vocab_size = len(vocab_list)\n\n        text_0 = [\'This is a sentence from source .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 source\']\n        text_0_file = tempfile.NamedTemporaryFile()\n        text_0_file.write(\'\\n\'.join(text_0).encode(""utf-8""))\n        text_0_file.flush()\n        self._text_0_file = text_0_file\n\n        text_1 = [\'This is a sentence from target .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 target\']\n        text_1_file = tempfile.NamedTemporaryFile()\n        text_1_file.write(\'\\n\'.join(text_1).encode(""utf-8""))\n        text_1_file.flush()\n        self._text_1_file = text_1_file\n\n        text_2 = [\n            \'This is a sentence from dialog . ||| dialog \',\n            \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 ||| \xe8\xaf\x8d dialog\']\n        text_2_file = tempfile.NamedTemporaryFile()\n        text_2_file.write(\'\\n\'.join(text_2).encode(""utf-8""))\n        text_2_file.flush()\n        self._text_2_file = text_2_file\n\n        int_3 = [0, 1]\n        int_3_file = tempfile.NamedTemporaryFile()\n        int_3_file.write((\'\\n\'.join([str(_) for _ in int_3])).encode(""utf-8""))\n        int_3_file.flush()\n        self._int_3_file = int_3_file\n\n        self._record_filepath = os.path.join(\n            tempfile.mkdtemp(), \'test.pkl\')\n        self._feature_types = {\n            \'number1\': (\'tf.int64\', \'stacked_tensor\'),\n            \'number2\': (\'tf.int64\', \'stacked_tensor\'),\n            \'text\': (\'tf.string\', \'stacked_tensor\')\n        }\n\n        features = [{\n            ""number1"": 128,\n            ""number2"": 512,\n            ""text"": ""This is a sentence for TFRecord \xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82""\n        },\n            {\n                ""number1"": 128,\n                ""number2"": 512,\n                ""text"": ""This is a another sentence for TFRecord \xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82""\n            }]\n        # Prepare Validation data\n        with RecordData.writer(self._record_filepath,\n                               self._feature_types) as writer:\n            for feature in features:\n                writer.write(feature)\n\n        # Construct database\n        self._hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 1,\n            ""datasets"": [\n                {  # dataset 0\n                    ""files"": [self._text_0_file.name],\n                    ""vocab_file"": self._vocab_file.name,\n                    ""bos_token"": """",\n                    ""data_name"": ""0""\n                },\n                {  # dataset 1\n                    ""files"": [self._text_1_file.name],\n                    ""vocab_share_with"": 0,\n                    ""eos_token"": ""<TARGET_EOS>"",\n                    ""data_name"": ""1""\n                },\n                {  # dataset 2\n                    ""files"": [self._text_2_file.name],\n                    ""vocab_file"": self._vocab_file.name,\n                    ""processing_share_with"": 0,\n                    # TODO(avinash) - Add it back once feature is added\n                    ""variable_utterance"": False,\n                    ""data_name"": ""2""\n                },\n                {  # dataset 3\n                    ""files"": self._int_3_file.name,\n                    ""data_type"": ""int"",\n                    ""data_name"": ""label""\n                },\n                {  # dataset 4\n                    ""files"": self._record_filepath,\n                    ""feature_types"": self._feature_types,\n                    ""feature_convert_types"": {\n                        \'number2\': \'tf.float32\',\n                    },\n                    ""num_shards"": 2,\n                    ""shard_id"": 1,\n                    ""data_type"": ""record"",\n                    ""data_name"": ""4""\n                }\n            ]\n        }\n\n    def _run_and_test(self, hparams, discard_index=None):\n        # Construct database\n        text_data = MultiAlignedData(hparams)\n        self.assertEqual(\n            text_data.vocab(0).size,\n            self._vocab_size + len(text_data.vocab(0).special_tokens))\n\n        iterator = DataIterator(text_data)\n        for batch in iterator:\n            self.assertEqual(set(batch.keys()),\n                             set(text_data.list_items()))\n            text_0 = batch[\'0_text\']\n            text_1 = batch[\'1_text\']\n            text_2 = batch[\'2_text\']\n            int_3 = batch[\'label\']\n            number_1 = batch[\'4_number1\']\n            number_2 = batch[\'4_number2\']\n            text_4 = batch[\'4_text\']\n\n            for t0, t1, t2, i3, n1, n2, t4 in zip(\n                    text_0, text_1, text_2, int_3,\n                    number_1, number_2, text_4):\n\n                np.testing.assert_array_equal(t0[:2], t1[1:3])\n                np.testing.assert_array_equal(t0[:3], t2[1:4])\n                if t0[0].startswith(\'This\'):\n                    self.assertEqual(i3, 0)\n                else:\n                    self.assertEqual(i3, 1)\n                self.assertEqual(n1, 128)\n                self.assertEqual(n2, 512)\n                self.assertIsInstance(n1, torch.Tensor)\n                self.assertIsInstance(n2, torch.Tensor)\n                self.assertIsInstance(t4, str)\n\n            if discard_index is not None:\n                hpms = text_data._hparams.datasets[discard_index]\n                max_l = hpms.max_seq_length\n                max_l += sum(int(x is not None and x != \'\')\n                             for x in [text_data.vocab(discard_index).bos_token,\n                                       text_data.vocab(discard_index).eos_token]\n                             )\n                for i in range(2):\n                    for length in batch[text_data.length_name(i)]:\n                        self.assertLessEqual(length, max_l)\n\n                # TODO(avinash): Add this back once variable utterance is added\n                # for lengths in batch[text_data.length_name(2)]:\n                #    for length in lengths:\n                #        self.assertLessEqual(length, max_l)\n\n            for i, hpms in enumerate(text_data._hparams.datasets):\n                if hpms.data_type != ""text"":\n                    continue\n                max_l = hpms.max_seq_length\n                mode = hpms.length_filter_mode\n                if max_l is not None and mode == ""truncate"":\n                    max_l += sum(int(x is not None and x != \'\')\n                                 for x in [text_data.vocab(i).bos_token,\n                                           text_data.vocab(i).eos_token])\n                    for length in batch[text_data.length_name(i)]:\n                        self.assertLessEqual(length, max_l)\n\n    def test_default_setting(self):\n        """"""Tests the logics of the text data.\n        """"""\n        self._run_and_test(self._hparams)\n\n    def test_length_filter(self):\n        """"""Tests filtering by length.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""datasets""][0].update(\n            {""max_seq_length"": 4,\n             ""length_filter_mode"": ""discard""})\n        hparams[""datasets""][1].update(\n            {""max_seq_length"": 2,\n             ""length_filter_mode"": ""truncate""})\n        self._run_and_test(hparams, discard_index=0)\n\n    def test_supported_scalar_types(self):\n        """"""Tests scalar types supported in MultiAlignedData.""""""\n        # int64 type\n        hparams = copy.copy(self._hparams)\n        hparams[""datasets""][3].update({\n            ""data_type"": ""int64""\n        })\n        self._run_and_test(hparams)\n\n        # float type\n        hparams = copy.copy(self._hparams)\n        hparams[""datasets""][3].update({\n            ""data_type"": ""float""\n        })\n        self._run_and_test(hparams)\n\n        # float64 type\n        hparams = copy.copy(self._hparams)\n        hparams[""datasets""][3].update({\n            ""data_type"": ""float64""\n        })\n        self._run_and_test(hparams)\n\n        # bool type\n        hparams = copy.copy(self._hparams)\n        hparams[""datasets""][3].update({\n            ""data_type"": ""bool""\n        })\n        self._run_and_test(hparams)\n\n    def test_unsupported_scalar_types(self):\n        """"""Tests if exception is thrown for unsupported types.""""""\n        hparams = copy.copy(self._hparams)\n        hparams[""datasets""][3].update({\n            ""data_type"": ""XYZ""\n        })\n\n        with self.assertRaises(ValueError):\n            self._run_and_test(hparams)\n\n        hparams = copy.copy(self._hparams)\n        hparams[""datasets""][3].update({\n            ""data_type"": ""str""\n        })\n\n        with self.assertRaises(ValueError):\n            self._run_and_test(hparams)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/data/paired_text_data_test.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for data related operations.\n""""""\nimport tempfile\nimport copy\nimport numpy as np\n\nimport unittest\n\nfrom texar.torch.data.data.data_iterators import DataIterator\nfrom texar.torch.data.data.paired_text_data import PairedTextData\nfrom texar.torch.data.vocabulary import SpecialTokens\n\n\nclass PairedTextDataTest(unittest.TestCase):\n    """"""Tests paired text data class.\n    """"""\n\n    def setUp(self):\n\n        # Create test data\n        vocab_list = [\'This\', \'is\', \'a\', \'word\', \'\xe8\xaf\x8d\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        self._vocab_file = vocab_file\n        self._vocab_size = len(vocab_list)\n\n        src_text = [\'This is a sentence from source .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 source\']\n        src_text_file = tempfile.NamedTemporaryFile()\n        src_text_file.write(\'\\n\'.join(src_text).encode(""utf-8""))\n        src_text_file.flush()\n        self._src_text_file = src_text_file\n\n        tgt_text = [\'This is a sentence from target .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 target\']\n        tgt_text_file = tempfile.NamedTemporaryFile()\n        tgt_text_file.write(\'\\n\'.join(tgt_text).encode(""utf-8""))\n        tgt_text_file.flush()\n        self._tgt_text_file = tgt_text_file\n\n        self._hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 3,\n            ""source_dataset"": {\n                ""files"": [self._src_text_file.name],\n                ""vocab_file"": self._vocab_file.name,\n            },\n            ""target_dataset"": {\n                ""files"": self._tgt_text_file.name,\n                ""vocab_share"": True,\n                ""eos_token"": ""<TARGET_EOS>""\n            }\n        }\n\n        self.src_upper_cased_text = []\n        for sent in src_text:\n            upper_cased_tokens = sent.upper().split("" "")\n            upper_cased_tokens.append(SpecialTokens.EOS)\n            self.src_upper_cased_text.append(upper_cased_tokens)\n\n        max_length = max([len(tokens) for tokens in self.src_upper_cased_text])\n        self.src_upper_cased_text = [sent + [\'\'] * (max_length - len(sent)) for\n                                     sent in self.src_upper_cased_text]\n\n        self.tgt_upper_cased_text = []\n        for sent in tgt_text:\n            upper_cased_tokens = sent.upper().split("" "")\n            upper_cased_tokens.insert(0, ""<BOS>"")\n            upper_cased_tokens.append(""<TARGET_EOS>"")\n            self.tgt_upper_cased_text.append(upper_cased_tokens)\n\n        max_length = max([len(tokens) for tokens in self.tgt_upper_cased_text])\n        self.tgt_upper_cased_text = [sent + [\'\'] * (max_length - len(sent)) for\n                                     sent in self.tgt_upper_cased_text]\n\n    def _run_and_test(self, hparams, proc_shr=False, test_transform=None,\n                      discard_src=False):\n        # Construct database\n        text_data = PairedTextData(hparams)\n        self.assertEqual(text_data.source_vocab.size,\n                         self._vocab_size +\n                         len(text_data.source_vocab.special_tokens))\n\n        iterator = DataIterator(text_data)\n        for data_batch in iterator:\n            self.assertEqual(set(data_batch.keys()),\n                             set(text_data.list_items()))\n\n            if proc_shr:\n                tgt_eos = \'<EOS>\'\n            else:\n                tgt_eos = \'<TARGET_EOS>\'\n\n            # Test matching\n            src_text = data_batch[\'source_text\']\n            tgt_text = data_batch[\'target_text\']\n            if proc_shr:\n                for src, tgt in zip(src_text, tgt_text):\n                    np.testing.assert_array_equal(src[:3], tgt[:3])\n            else:\n                for src, tgt in zip(src_text, tgt_text):\n                    np.testing.assert_array_equal(src[:3], tgt[1:4])\n            self.assertTrue(\n                tgt_eos in data_batch[\'target_text\'][0])\n\n            if test_transform:\n                for i in range(len(data_batch[\'source_text\'])):\n                    text_ = data_batch[\'source_text\'][i]\n                    self.assertTrue(text_ in self.src_upper_cased_text)\n                for i in range(len(data_batch[\'target_text\'])):\n                    text_ = data_batch[\'target_text\'][i]\n                    self.assertTrue(text_ in self.tgt_upper_cased_text)\n\n            if discard_src:\n                src_hparams = text_data.hparams.source_dataset\n                max_l = src_hparams.max_seq_length\n                max_l += sum(int(x is not None and x != \'\')\n                             for x in [text_data._src_bos_token,\n                                       text_data._tgt_bos_token])\n                for l in data_batch[""source_length""]:\n                    self.assertLessEqual(l, max_l)\n\n    def test_default_setting(self):\n        """"""Tests the logics of the text data.\n        """"""\n        self._run_and_test(self._hparams)\n\n    def test_shuffle(self):\n        """"""Tests toggling shuffle.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""shuffle""] = False\n        self._run_and_test(hparams)\n\n    def test_processing_share(self):\n        """"""Tests sharing processing.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""target_dataset""][""processing_share""] = True\n        self._run_and_test(hparams, proc_shr=True)\n\n    def test_other_transformations(self):\n        """"""Tests use of other transformations\n        """"""\n        _upper_func = lambda raw_example: [x.upper() for x in raw_example]\n\n        hparams = copy.copy(self._hparams)\n        hparams[""source_dataset""].update(\n            {""other_transformations"": [_upper_func]})\n        hparams[""target_dataset""].update(\n            {""other_transformations"": [_upper_func]})\n        self._run_and_test(hparams, test_transform=True)\n\n    def test_length_filter(self):\n        """"""Tests filtering by length.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""source_dataset""].update(\n            {""max_seq_length"": 4,\n             ""length_filter_mode"": ""discard""})\n        self._run_and_test(hparams, discard_src=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/data/record_data_test.py,5,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for data related operations.\n""""""\n\nimport copy\nimport os\nimport shutil\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.data.data.record_data import RecordData\nfrom texar.torch.data.data.data_iterators import DataIterator\nfrom texar.torch.data.data_utils import maybe_download\nfrom texar.torch.utils import get_numpy_dtype\n\n\nclass RecordDataTest(unittest.TestCase):\n    """"""Tests RecordData class.\n    """"""\n\n    def setUp(self):\n        # Create test data\n        self._test_dir = tempfile.mkdtemp()\n\n        cat_in_snow = maybe_download(\n            \'https://storage.googleapis.com/download.tensorflow.org/\'\n            \'example_images/320px-Felis_catus-cat_on_snow.jpg\',\n            self._test_dir, \'cat_0.jpg\')\n        williamsburg_bridge = maybe_download(\n            \'https://storage.googleapis.com/download.tensorflow.org/\'\n            \'example_images/194px-New_East_River_Bridge_from_Brooklyn_\'\n            \'det.4a09796u.jpg\',\n            self._test_dir, \'bridge_0.jpg\')\n\n        _feature_types = {\n            \'height\': (\'tf.int64\', \'FixedLenFeature\', 1),\n            \'width\': (\'tf.int64\', \'FixedLenFeature\', 1),\n            \'label\': (\'tf.int64\', \'stacked_tensor\', 1),\n            \'shape\': (np.int64, \'VarLenFeature\'),\n            \'image_raw\': (bytes, \'stacked_tensor\'),\n            \'variable1\': (np.str, \'FixedLenFeature\'),\n            \'variable2\': (\'tf.int64\', \'FixedLenFeature\'),\n        }\n        self._feature_convert_types = {\n            \'variable1\': \'tf.float32\',\n            \'variable2\': \'tf.string\',\n        }\n        _image_options = {}\n        self._unconvert_features = [\'height\', \'width\', \'label\']\n\n        self._dataset_valid = {\n            \'height\': [],\n            \'width\': [],\n            \'shape\': [],\n            \'label\': [],\n            \'image_raw\': [],\n            \'variable1\': [],\n            \'variable2\': [],\n        }\n        _toy_image_labels_valid = {\n            cat_in_snow: 0,\n            williamsburg_bridge: 1,\n        }\n        _toy_image_shapes = {\n            cat_in_snow: (213, 320, 3),\n            williamsburg_bridge: (239, 194),\n        }\n        _record_filepath = os.path.join(self._test_dir, \'test.pkl\')\n\n        # Prepare Validation data\n        with RecordData.writer(_record_filepath, _feature_types) as writer:\n            for image_path, label in _toy_image_labels_valid.items():\n                with open(image_path, \'rb\') as fid:\n                    image_data = fid.read()\n                image_shape = _toy_image_shapes[image_path]\n\n                # _construct_dataset_valid("""", shape, label)\n                single_data = {\n                    \'height\': image_shape[0],\n                    \'width\': image_shape[1],\n                    \'shape\': image_shape,\n                    \'label\': label,\n                    \'image_raw\': image_data,\n                    \'variable1\': ""1234567890"",\n                    \'variable2\': int(9876543210),\n                }\n                for key, value in single_data.items():\n                    self._dataset_valid[key].append(value)\n                writer.write(single_data)\n\n        self._hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 1,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": _record_filepath,\n                ""feature_original_types"": _feature_types,\n                ""feature_convert_types"": self._feature_convert_types,\n                ""image_options"": [_image_options],\n            }\n        }\n\n    def tearDown(self):\n        """"""Remove the downloaded files after the test\n        """"""\n        shutil.rmtree(self._test_dir)\n\n    def _run_and_test(self, hparams):\n        # Construct database\n        record_data = RecordData(hparams)\n        iterator = DataIterator(record_data)\n\n        def _prod(lst):\n            res = 1\n            for i in lst:\n                res *= i\n            return res\n\n        for idx, data_batch in enumerate(iterator):\n            self.assertEqual(\n                set(data_batch.keys()),\n                set(record_data.list_items()))\n\n            # Check data consistency\n            for key in self._unconvert_features:\n                value = data_batch[key][0]\n                self.assertEqual(value, self._dataset_valid[key][idx])\n            self.assertEqual(\n                list(data_batch[\'shape\'][0]),\n                list(self._dataset_valid[\'shape\'][idx]))\n\n            # Check data type conversion\n            for key, item in self._feature_convert_types.items():\n                dtype = get_numpy_dtype(item)\n                value = data_batch[key][0]\n                if dtype is np.str_:\n                    self.assertIsInstance(value, str)\n                elif dtype is np.bytes_:\n                    self.assertIsInstance(value, bytes)\n                else:\n                    if isinstance(value, torch.Tensor):\n                        value_dtype = get_numpy_dtype(value.dtype)\n                    else:\n                        value_dtype = value.dtype\n                    dtype_matched = np.issubdtype(value_dtype, dtype)\n                    self.assertTrue(dtype_matched)\n\n            # Check image decoding and resize\n            if hparams[""dataset""].get(""image_options""):\n                image_options = hparams[""dataset""].get(""image_options"")\n                if isinstance(image_options, dict):\n                    image_options = [image_options]\n                for image_option_feature in image_options:\n                    image_key = image_option_feature.get(\n                        ""image_feature_name"")\n                    if image_key is None:\n                        continue\n                    image_gen = data_batch[image_key][0]\n                    image_valid_shape = self._dataset_valid[""shape""][idx]\n                    resize_height = image_option_feature.get(\n                        ""resize_height"")\n                    resize_width = image_option_feature.get(\n                        ""resize_width"")\n                    if resize_height and resize_width:\n                        self.assertEqual(\n                            image_gen.shape[0] * image_gen.shape[1],\n                            resize_height * resize_width)\n                    else:\n                        self.assertEqual(\n                            _prod(image_gen.shape),\n                            _prod(image_valid_shape))\n\n    def test_default_setting(self):\n        """"""Tests the logics of TFRecordData.\n        """"""\n        self._run_and_test(self._hparams)\n\n    def test_image_resize(self):\n        """"""Tests the image resize function\n        """"""\n        hparams = copy.deepcopy(self._hparams)\n        _image_options = {\n            \'image_feature_name\': \'image_raw\',\n            \'resize_height\': 512,\n            \'resize_width\': 512,\n        }\n        hparams[""dataset""].update({""image_options"": _image_options})\n        self._run_and_test(hparams)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/data/sampler_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for sampler related operations.\n""""""\nimport unittest\nfrom typing import no_type_check\n\nimport numpy as np\n\nfrom texar.torch.data.data.data_base import (\n    DatasetBase, DataSource, IterDataSource, SequenceDataSource)\nfrom texar.torch.data.data.sampler import BufferShuffleSampler\n\n\nclass SamplerTest(unittest.TestCase):\n    r""""""Tests samplers.\n    """"""\n\n    class MockDataBase(DatasetBase):\n        def __init__(self, size: int, lazy_strategy: str,\n                     cache_strategy: str, unknown_size: bool = False):\n            data = list(range(size))\n            source: DataSource[int]\n            if unknown_size:\n                source = IterDataSource(data)\n            else:\n                source = SequenceDataSource(data)\n            hparams = {\n                \'lazy_strategy\': lazy_strategy,\n                \'cache_strategy\': cache_strategy,\n            }\n            super().__init__(source, hparams=hparams)\n\n    def setUp(self) -> None:\n        self.size = 10\n        self.buffer_size = 5\n\n    @no_type_check\n    def _test_data(self, data: DatasetBase,\n                   returns_data: bool = False,\n                   always_returns_data: bool = False):\n        sampler = BufferShuffleSampler(data, self.buffer_size)\n        for epoch in range(2):\n            indices = list(iter(sampler))\n            if always_returns_data or (returns_data and epoch == 0):\n                examples = [ex[1] for ex in indices]\n                indices = [ex[0] for ex in indices]\n                np.testing.assert_array_equal(indices, examples)\n            self.assertEqual(len(set(indices)), self.size)\n            self.assertEqual(min(indices), 0)\n            self.assertEqual(max(indices), self.size - 1)\n            data._fully_cached = True\n\n    def test_known_size(self):\n        data = self.MockDataBase(self.size, \'none\', \'processed\')\n        self._test_data(data)\n        data = self.MockDataBase(self.size, \'all\', \'none\', unknown_size=True)\n        self._test_data(data, always_returns_data=True)\n\n    def test_non_lazy_loading(self):\n        strategies = [\n            (\'none\', \'processed\'),\n            (\'process\', \'loaded\'),\n            (\'process\', \'processed\'),\n        ]\n        for lazy, cache in strategies:\n            data = self.MockDataBase(self.size, lazy, cache)\n            self._test_data(data)\n\n    def test_lazy_loading(self):\n        data = self.MockDataBase(self.size, \'all\', \'loaded\', unknown_size=True)\n        self._test_data(data, returns_data=True)\n        data = self.MockDataBase(self.size, \'all\', \'processed\',\n                                 unknown_size=True)\n        self._test_data(data, returns_data=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/data/scalar_data_test.py,5,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for data related operations.\n""""""\nimport torch\n\nimport copy\nimport tempfile\nimport numpy as np\n\nimport unittest\n\nfrom texar.torch.data import DataIterator, ScalarData\nfrom texar.torch.utils.dtypes import torch_bool\n\n\nclass ScalarDataTest(unittest.TestCase):\n    """"""Tests scalar data class.\n    """"""\n\n    def setUp(self):\n        # Create test data\n        int_data = np.linspace(0, 100, num=101, dtype=np.int32).tolist()\n        int_data = [str(i) for i in int_data]\n        int_file = tempfile.NamedTemporaryFile()\n        int_file.write(\'\\n\'.join(int_data).encode(""utf-8""))\n        int_file.flush()\n        self._int_file = int_file\n\n        self._int_hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 1,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": self._int_file.name,\n                ""data_type"": ""int"",\n                ""data_name"": ""label""\n            }\n        }\n\n        self._float_hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 1,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": self._int_file.name,\n                ""data_type"": ""float"",\n                ""data_name"": ""feat""\n            }\n        }\n\n        bool_data = [0, 1]\n        bool_data = [str(i) for i in bool_data]\n        bool_file = tempfile.NamedTemporaryFile()\n        bool_file.write(\'\\n\'.join(bool_data).encode(""utf-8""))\n        bool_file.flush()\n        self._bool_file = bool_file\n\n        self._bool_hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 1,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": self._bool_file.name,\n                ""data_type"": ""bool"",\n                ""data_name"": ""feat""\n            }\n        }\n\n    def _run_and_test(self, hparams, test_transform=False):\n        # Construct database\n        scalar_data = ScalarData(hparams)\n\n        self.assertEqual(scalar_data.list_items()[0],\n                         hparams[""dataset""][""data_name""])\n\n        iterator = DataIterator(scalar_data)\n\n        i = 0\n        for batch in iterator:\n            self.assertEqual(set(batch.keys()),\n                             set(scalar_data.list_items()))\n            value = batch[scalar_data.data_name][0]\n            if test_transform:\n                self.assertEqual(2 * i, value)\n            else:\n                self.assertEqual(i, value)\n            i += 1\n            data_type = hparams[""dataset""][""data_type""]\n            if data_type == ""int"":\n                self.assertEqual(value.dtype, torch.int32)\n            elif data_type == ""float"":\n                self.assertEqual(value.dtype, torch.float32)\n            elif data_type == ""bool"":\n                self.assertTrue(value.dtype, torch_bool)\n            self.assertIsInstance(value, torch.Tensor)\n\n    def test_default_setting(self):\n        """"""Tests the logic of ScalarData.\n        """"""\n        self._run_and_test(self._int_hparams)\n        self._run_and_test(self._float_hparams)\n        self._run_and_test(self._bool_hparams)\n\n    def test_shuffle(self):\n        """"""Tests results of toggling shuffle.\n        """"""\n        hparams = copy.copy(self._int_hparams)\n        hparams[""batch_size""] = 10\n        scalar_data = ScalarData(hparams)\n        iterator = DataIterator(scalar_data)\n\n        hparams_sfl = copy.copy(hparams)\n        hparams_sfl[""shuffle""] = True\n        scalar_data_sfl = ScalarData(hparams_sfl)\n        iterator_sfl = DataIterator(scalar_data_sfl)\n\n        vals = []\n        vals_sfl = []\n\n        for batch, batch_sfl in zip(iterator, iterator_sfl):\n            vals += batch[""label""].tolist()\n            vals_sfl += batch_sfl[""label""].tolist()\n\n        self.assertEqual(len(vals), len(vals_sfl))\n        self.assertSetEqual(set(vals), set(vals_sfl))\n\n    def test_transform(self):\n        """"""Tests transform logic.\n        """"""\n        hparams = copy.deepcopy(self._int_hparams)\n        hparams[""dataset""].update(\n            {""other_transformations"": [lambda x: x * 2]})\n        self._run_and_test(hparams, test_transform=True)\n\n        hparams = copy.deepcopy(self._float_hparams)\n        hparams[""dataset""].update(\n            {""other_transformations"": [lambda x: x * 2]})\n        self._run_and_test(hparams, test_transform=True)\n\n    def test_unsupported_scalar_types(self):\n        """"""Tests exception for unsupported scalar types.\n        """"""\n        hparams = copy.copy(self._int_hparams)\n        hparams[""dataset""].update({\n            ""data_type"": ""XYZ""\n        })\n\n        with self.assertRaises(ValueError):\n            self._run_and_test(hparams)\n\n        hparams = copy.copy(self._int_hparams)\n        hparams[""dataset""].update({\n            ""data_type"": ""str""\n        })\n\n        with self.assertRaises(ValueError):\n            self._run_and_test(hparams)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/tokenizers/bert_tokenizer_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for pre-trained BERT tokenizer.\n""""""\n\nimport unittest\n\nimport os\nimport pickle\nimport tempfile\n\nfrom texar.torch.data.tokenizers.bert_tokenizer import \\\n    BERTTokenizer\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass BERTTokenizerTest(unittest.TestCase):\n\n    def setUp(self):\n        vocab_tokens = [\n            ""[UNK]"", ""[CLS]"", ""[SEP]"", ""want"", ""##want"", ""##ed"", ""wa"", ""un"",\n            ""runn"",\n            ""##ing"", "","", ""low"", ""lowest"",\n        ]\n\n        self.tmp_dir = tempfile.TemporaryDirectory()\n        self.vocab_file = os.path.join(self.tmp_dir.name, \'vocab.txt\')\n        with open(self.vocab_file, ""w"", encoding=\'utf-8\') as vocab_writer:\n            vocab_writer.write("""".join([x + ""\\n"" for x in vocab_tokens]))\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    @pretrained_test\n    def test_model_loading(self):\n        for pretrained_model_name in BERTTokenizer.available_checkpoints():\n            tokenizer = BERTTokenizer(\n                pretrained_model_name=pretrained_model_name)\n            _ = tokenizer.map_text_to_token(u""UNwant\\u00E9d,running"")\n\n    def test_tokenize(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n\n        tokens = tokenizer.map_text_to_token(u""UNwant\\u00E9d,running"")\n        self.assertListEqual(tokens,\n                             [""un"", ""##want"", ""##ed"", "","", ""runn"", ""##ing""])\n\n        ids = tokenizer.map_token_to_id(tokens)\n        self.assertListEqual(ids, [7, 4, 5, 10, 8, 9])\n\n    def test_pickle(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n        self.assertIsNotNone(tokenizer)\n\n        text = u""Munich and Berlin are nice cities""\n        subwords = tokenizer.map_text_to_token(text)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            filename = os.path.join(tmpdirname, u""tokenizer.bin"")\n            with open(filename, ""wb"") as f:\n                pickle.dump(tokenizer, f)\n            with open(filename, ""rb"") as f:\n                tokenizer_new = pickle.load(f)\n\n        subwords_loaded = tokenizer_new.map_text_to_token(text)\n\n        self.assertListEqual(subwords, subwords_loaded)\n\n    def test_save_load(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n\n        before_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            tokenizer.save(tmpdirname)\n            tokenizer = tokenizer.load(tmpdirname)\n\n        after_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n        self.assertListEqual(before_tokens, after_tokens)\n\n    def test_pretrained_model_list(self):\n        model_list_1 = list(BERTTokenizer._MODEL2URL.keys())\n        model_list_2 = list(BERTTokenizer._MAX_INPUT_SIZE.keys())\n\n        self.assertListEqual(model_list_1, model_list_2)\n\n    def test_encode_decode(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n\n        input_text = u""UNwant\\u00E9d,running""\n        output_text = u""unwanted, running""\n\n        tokens = tokenizer.map_text_to_token(input_text)\n        ids = tokenizer.map_token_to_id(tokens)\n        ids_2 = tokenizer.map_text_to_id(input_text)\n        self.assertListEqual(ids, ids_2)\n\n        tokens_2 = tokenizer.map_id_to_token(ids)\n        text_2 = tokenizer.map_id_to_text(ids)\n\n        self.assertEqual(text_2, output_text)\n\n        self.assertNotEqual(len(tokens_2), 0)\n        self.assertIsInstance(text_2, str)\n\n    def test_add_tokens(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n\n        vocab_size = tokenizer.vocab_size\n        all_size = len(tokenizer)\n\n        self.assertNotEqual(vocab_size, 0)\n        self.assertEqual(vocab_size, all_size)\n\n        new_toks = [""aaaaabbbbbb"", ""cccccccccdddddddd""]\n        added_toks = tokenizer.add_tokens(new_toks)\n        vocab_size_2 = tokenizer.vocab_size\n        all_size_2 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_2, 0)\n        self.assertEqual(vocab_size, vocab_size_2)\n        self.assertEqual(added_toks, len(new_toks))\n        self.assertEqual(all_size_2, all_size + len(new_toks))\n\n        tokens = tokenizer.map_text_to_id(""aaaaabbbbbb low cccccccccdddddddd l"")\n        self.assertGreaterEqual(len(tokens), 4)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n\n        new_toks_2 = {\'eos_token\': "">>>>|||<||<<|<<"",\n                      \'pad_token\': ""<<<<<|||>|>>>>|>""}\n        added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n        vocab_size_3 = tokenizer.vocab_size\n        all_size_3 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_3, 0)\n        self.assertEqual(vocab_size, vocab_size_3)\n        self.assertEqual(added_toks_2, len(new_toks_2))\n        self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n\n        tokens = tokenizer.map_text_to_id(\n            "">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd ""\n            ""<<<<<|||>|>>>>|> l"")\n\n        self.assertGreaterEqual(len(tokens), 6)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[0], tokens[1])\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokens[-3])\n        self.assertEqual(tokens[0],\n                         tokenizer.map_token_to_id(tokenizer.eos_token))\n        self.assertEqual(tokens[-2],\n                         tokenizer.map_token_to_id(tokenizer.pad_token))\n\n    def test_encode_text(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n\n        text_1 = u""He is very happy""\n        text_2 = u""unwanted, running""\n\n        text_1_ids = tokenizer.map_text_to_id(text_1)\n        text_2_ids = tokenizer.map_text_to_id(text_2)\n\n        cls_token_id = tokenizer.map_token_to_id(tokenizer.cls_token)\n        sep_token_id = tokenizer.map_token_to_id(tokenizer.sep_token)\n\n        input_ids, segment_ids, input_mask = \\\n            tokenizer.encode_text(text_1, None, 4)\n\n        self.assertListEqual(input_ids,\n                             [cls_token_id] + text_1_ids[:2] + [sep_token_id])\n        self.assertListEqual(segment_ids, [0, 0, 0, 0])\n        self.assertListEqual(input_mask, [1, 1, 1, 1])\n\n        input_ids, segment_ids, input_mask = \\\n            tokenizer.encode_text(text_1, text_2, 7)\n\n        self.assertListEqual(input_ids, [cls_token_id] + text_1_ids[:2] +\n                             [sep_token_id] + text_2_ids[:2] + [sep_token_id])\n        self.assertListEqual(segment_ids, [0, 0, 0, 0, 1, 1, 1])\n        self.assertListEqual(input_mask, [1, 1, 1, 1, 1, 1, 1])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/tokenizers/bert_tokenizer_utils_test.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for pre-trained BERT tokenizer utils.\n""""""\n\nimport unittest\n\nfrom texar.torch.data.tokenizers.bert_tokenizer_utils import (\n    BasicTokenizer, WordpieceTokenizer, _is_control,\n    _is_punctuation, _is_whitespace)\n\n\nclass BERTTokenizerUtilsTest(unittest.TestCase):\n\n    def test_chinese(self):\n\n        tokenizer = BasicTokenizer()\n\n        self.assertListEqual(\n            tokenizer.tokenize(u""ah\\u535A\\u63A8zz""),\n            [u""ah"", u""\\u535A"", u""\\u63A8"", u""zz""])\n\n    def test_basic_tokenizer_lower(self):\n\n        tokenizer = BasicTokenizer(do_lower_case=True)\n\n        self.assertListEqual(\n            tokenizer.tokenize(u"" \\tHeLLo!how  \\n Are yoU?  ""),\n            [""hello"", ""!"", ""how"", ""are"", ""you"", ""?""])\n        self.assertListEqual(tokenizer.tokenize(u""H\\u00E9llo""), [""hello""])\n\n    def test_basic_tokenizer_no_lower(self):\n\n        tokenizer = BasicTokenizer(do_lower_case=False)\n\n        self.assertListEqual(\n            tokenizer.tokenize(u"" \\tHeLLo!how  \\n Are yoU?  ""),\n            [""HeLLo"", ""!"", ""how"", ""Are"", ""yoU"", ""?""])\n\n    def test_wordpiece_tokenizer(self):\n\n        vocab_tokens = [\n            ""[UNK]"", ""[CLS]"", ""[SEP]"", ""want"", ""##want"", ""##ed"", ""wa"", ""un"", ""runn"",\n            ""##ing""\n        ]\n\n        vocab = {}\n        for (i, token) in enumerate(vocab_tokens):\n            vocab[token] = i\n        tokenizer = WordpieceTokenizer(vocab=vocab, unk_token=""[UNK]"")\n\n        self.assertListEqual(tokenizer.tokenize(""""), [])\n\n        self.assertListEqual(\n            tokenizer.tokenize(""unwanted running""),\n            [""un"", ""##want"", ""##ed"", ""runn"", ""##ing""])\n\n        self.assertListEqual(\n            tokenizer.tokenize(""unwantedX running""), [""[UNK]"", ""runn"", ""##ing""])\n\n    def test_is_whitespace(self):\n\n        self.assertTrue(_is_whitespace(u"" ""))\n        self.assertTrue(_is_whitespace(u""\\t""))\n        self.assertTrue(_is_whitespace(u""\\r""))\n        self.assertTrue(_is_whitespace(u""\\n""))\n        self.assertTrue(_is_whitespace(u""\\u00A0""))\n\n        self.assertFalse(_is_whitespace(u""A""))\n        self.assertFalse(_is_whitespace(u""-""))\n\n    def test_is_control(self):\n\n        self.assertTrue(_is_control(u""\\u0005""))\n\n        self.assertFalse(_is_control(u""A""))\n        self.assertFalse(_is_control(u"" ""))\n        self.assertFalse(_is_control(u""\\t""))\n        self.assertFalse(_is_control(u""\\r""))\n\n    def test_is_punctuation(self):\n\n        self.assertTrue(_is_punctuation(u""-""))\n        self.assertTrue(_is_punctuation(u""$""))\n        self.assertTrue(_is_punctuation(u""`""))\n        self.assertTrue(_is_punctuation(u"".""))\n\n        self.assertFalse(_is_punctuation(u""A""))\n        self.assertFalse(_is_punctuation(u"" ""))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/tokenizers/gpt2_tokenizer_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for pre-trained GPT2 tokenizer.\n""""""\n\nimport unittest\n\nimport json\nimport os\nimport pickle\nimport tempfile\n\nfrom texar.torch.data.tokenizers.gpt2_tokenizer import \\\n    GPT2Tokenizer\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass GPT2TokenizerTest(unittest.TestCase):\n\n    def setUp(self):\n        vocab = [""l"", ""o"", ""w"", ""e"", ""r"", ""s"", ""t"", ""i"", ""d"", ""n"",\n                 ""lo"", ""low"", ""er"",\n                 ""low"", ""lowest"", ""newer"", ""wider"", ""<unk>""]\n        vocab_tokens = dict(zip(vocab, range(len(vocab))))\n        merges = [""#version: 0.2"", ""l o"", ""lo w"", ""e r"", """"]\n        self.special_tokens_map = {""unk_token"": ""<unk>""}\n\n        self.tmp_dir = tempfile.TemporaryDirectory()\n        self.vocab_file = os.path.join(self.tmp_dir.name, \'encoder.json\')\n        self.merges_file = os.path.join(self.tmp_dir.name, \'vocab.bpe\')\n\n        with open(self.vocab_file, ""w"") as fp:\n            fp.write(json.dumps(vocab_tokens))\n        with open(self.merges_file, ""w"") as fp:\n            fp.write(""\\n"".join(merges))\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    @pretrained_test\n    def test_model_loading(self):\n        for pretrained_model_name in \\\n                GPT2Tokenizer.available_checkpoints():\n            tokenizer = GPT2Tokenizer(\n                pretrained_model_name=pretrained_model_name)\n            _ = tokenizer.map_text_to_token(\n                u""Munich and Berlin are nice cities"")\n\n    def test_tokenize(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n\n        text = ""lower""\n        bpe_tokens = [""low"", ""er""]\n        tokens = tokenizer.map_text_to_token(text)\n        self.assertListEqual(tokens, bpe_tokens)\n\n        input_tokens = tokens + [tokenizer.unk_token]\n        input_bpe_tokens = [13, 12, 17]\n        self.assertListEqual(\n            tokenizer.map_token_to_id(input_tokens),\n            input_bpe_tokens)\n\n    def test_pickle(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n        self.assertIsNotNone(tokenizer)\n\n        text = u""Munich and Berlin are nice cities""\n        subwords = tokenizer.map_text_to_token(text)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            filename = os.path.join(tmpdirname, u""tokenizer.bin"")\n            with open(filename, ""wb"") as f:\n                pickle.dump(tokenizer, f)\n            with open(filename, ""rb"") as f:\n                tokenizer_new = pickle.load(f)\n\n        subwords_loaded = tokenizer_new.map_text_to_token(text)\n\n        self.assertListEqual(subwords, subwords_loaded)\n\n    def test_save_load(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n\n        before_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            tokenizer.save(tmpdirname)\n            tokenizer = tokenizer.load(tmpdirname)\n\n        after_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n        self.assertListEqual(before_tokens, after_tokens)\n\n    def test_pretrained_model_list(self):\n        model_list_1 = list(GPT2Tokenizer._MODEL2URL.keys())\n        model_list_2 = list(GPT2Tokenizer._MAX_INPUT_SIZE.keys())\n\n        self.assertListEqual(model_list_1, model_list_2)\n\n    def test_encode_decode(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n\n        input_text = u""lower newer""\n        output_text = u""lower<unk>newer""\n\n        tokens = tokenizer.map_text_to_token(input_text)\n        ids = tokenizer.map_token_to_id(tokens)\n        ids_2 = tokenizer.map_text_to_id(input_text)\n        self.assertListEqual(ids, ids_2)\n\n        tokens_2 = tokenizer.map_id_to_token(ids)\n        text_2 = tokenizer.map_id_to_text(ids)\n\n        self.assertEqual(text_2, output_text)\n\n        self.assertNotEqual(len(tokens_2), 0)\n        self.assertIsInstance(text_2, str)\n\n    def test_add_tokens(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n\n        vocab_size = tokenizer.vocab_size\n        all_size = len(tokenizer)\n\n        self.assertNotEqual(vocab_size, 0)\n        self.assertEqual(vocab_size, all_size)\n\n        new_toks = [""aaaaabbbbbb"", ""cccccccccdddddddd""]\n        added_toks = tokenizer.add_tokens(new_toks)\n        vocab_size_2 = tokenizer.vocab_size\n        all_size_2 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_2, 0)\n        self.assertEqual(vocab_size, vocab_size_2)\n        self.assertEqual(added_toks, len(new_toks))\n        self.assertEqual(all_size_2, all_size + len(new_toks))\n\n        tokens = tokenizer.map_text_to_id(""aaaaabbbbbb low cccccccccdddddddd l"")\n        self.assertGreaterEqual(len(tokens), 4)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n\n        new_toks_2 = {\'eos_token\': "">>>>|||<||<<|<<"",\n                      \'pad_token\': ""<<<<<|||>|>>>>|>""}\n        added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n        vocab_size_3 = tokenizer.vocab_size\n        all_size_3 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_3, 0)\n        self.assertEqual(vocab_size, vocab_size_3)\n        self.assertEqual(added_toks_2, len(new_toks_2))\n        self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n\n        tokens = tokenizer.map_text_to_id(\n            "">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd ""\n            ""<<<<<|||>|>>>>|> l"")\n\n        self.assertGreaterEqual(len(tokens), 6)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[0], tokens[1])\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokens[-3])\n        self.assertEqual(tokens[0],\n                         tokenizer.map_token_to_id(tokenizer.eos_token))\n        self.assertEqual(tokens[-2],\n                         tokenizer.map_token_to_id(tokenizer.pad_token))\n\n    def test_encode_text(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n\n        text_1 = u""lower newer""\n\n        text_1_ids = tokenizer.map_text_to_id(text_1)\n\n        input_ids, seq_len = \\\n            tokenizer.encode_text(text=text_1, max_seq_length=10)\n\n        bos_token_id = tokenizer.map_token_to_id(tokenizer.bos_token)\n        eos_token_id = tokenizer.map_token_to_id(tokenizer.eos_token)\n        pad_token_id = tokenizer.map_token_to_id(tokenizer.pad_token)\n\n        self.assertListEqual(input_ids,\n                             [bos_token_id] + text_1_ids + [eos_token_id] +\n                             [pad_token_id])\n        self.assertEqual(seq_len, 9)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/tokenizers/roberta_tokenizer_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for pre-trained RoBERTa tokenizer.\n""""""\n\nimport unittest\n\nimport json\nimport os\nimport pickle\nimport tempfile\n\nfrom texar.torch.data.tokenizers.roberta_tokenizer import \\\n    RoBERTaTokenizer\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass RoBERTaTokenizerTest(unittest.TestCase):\n\n    def setUp(self):\n        vocab = [""l"", ""o"", ""w"", ""e"", ""r"", ""s"", ""t"", ""i"", ""d"", ""n"",\n                 ""lo"", ""low"", ""er"",\n                 ""low"", ""lowest"", ""newer"", ""wider"", ""<unk>""]\n        vocab_tokens = dict(zip(vocab, range(len(vocab))))\n        merges = [""#version: 0.2"", ""l o"", ""lo w"", ""e r"", """"]\n        self.special_tokens_map = {""unk_token"": ""<unk>""}\n\n        self.tmp_dir = tempfile.TemporaryDirectory()\n        self.vocab_file = os.path.join(self.tmp_dir.name, \'encoder.json\')\n        self.merges_file = os.path.join(self.tmp_dir.name, \'vocab.bpe\')\n\n        with open(self.vocab_file, ""w"") as fp:\n            fp.write(json.dumps(vocab_tokens))\n        with open(self.merges_file, ""w"") as fp:\n            fp.write(""\\n"".join(merges))\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    @pretrained_test\n    def test_model_loading(self):\n        for pretrained_model_name in \\\n                RoBERTaTokenizer.available_checkpoints():\n            tokenizer = RoBERTaTokenizer(\n                pretrained_model_name=pretrained_model_name)\n            _ = tokenizer.map_text_to_token(\n                u""Munich and Berlin are nice cities"")\n\n    def test_tokenize(self):\n        tokenizer = RoBERTaTokenizer.load(self.tmp_dir.name,\n                                          self.special_tokens_map)\n\n        text = ""lower""\n        bpe_tokens = [""low"", ""er""]\n        tokens = tokenizer.map_text_to_token(text)\n        self.assertListEqual(tokens, bpe_tokens)\n\n        input_tokens = tokens + [tokenizer.unk_token]\n        input_bpe_tokens = [13, 12, 17]\n        self.assertListEqual(\n            tokenizer.map_token_to_id(input_tokens),\n            input_bpe_tokens)\n\n    def test_pickle(self):\n        tokenizer = RoBERTaTokenizer.load(self.tmp_dir.name,\n                                          self.special_tokens_map)\n        self.assertIsNotNone(tokenizer)\n\n        text = u""Munich and Berlin are nice cities""\n        subwords = tokenizer.map_text_to_token(text)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            filename = os.path.join(tmpdirname, u""tokenizer.bin"")\n            with open(filename, ""wb"") as f:\n                pickle.dump(tokenizer, f)\n            with open(filename, ""rb"") as f:\n                tokenizer_new = pickle.load(f)\n\n        subwords_loaded = tokenizer_new.map_text_to_token(text)\n\n        self.assertListEqual(subwords, subwords_loaded)\n\n    def test_save_load(self):\n        tokenizer = RoBERTaTokenizer.load(self.tmp_dir.name,\n                                          self.special_tokens_map)\n\n        before_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            tokenizer.save(tmpdirname)\n            tokenizer = tokenizer.load(tmpdirname)\n\n        after_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n        self.assertListEqual(before_tokens, after_tokens)\n\n    def test_pretrained_model_list(self):\n        model_list_1 = list(RoBERTaTokenizer._MODEL2URL.keys())\n        model_list_2 = list(RoBERTaTokenizer._MAX_INPUT_SIZE.keys())\n\n        self.assertListEqual(model_list_1, model_list_2)\n\n    def test_encode_decode(self):\n        tokenizer = RoBERTaTokenizer.load(self.tmp_dir.name,\n                                          self.special_tokens_map)\n\n        input_text = u""lower newer""\n        output_text = u""lower<unk>newer""\n\n        tokens = tokenizer.map_text_to_token(input_text)\n        ids = tokenizer.map_token_to_id(tokens)\n        ids_2 = tokenizer.map_text_to_id(input_text)\n        self.assertListEqual(ids, ids_2)\n\n        tokens_2 = tokenizer.map_id_to_token(ids)\n        text_2 = tokenizer.map_id_to_text(ids)\n\n        self.assertEqual(text_2, output_text)\n\n        self.assertNotEqual(len(tokens_2), 0)\n        self.assertIsInstance(text_2, str)\n\n    def test_add_tokens(self):\n        tokenizer = RoBERTaTokenizer.load(self.tmp_dir.name,\n                                          self.special_tokens_map)\n\n        vocab_size = tokenizer.vocab_size\n        all_size = len(tokenizer)\n\n        self.assertNotEqual(vocab_size, 0)\n        self.assertEqual(vocab_size, all_size)\n\n        new_toks = [""aaaaabbbbbb"", ""cccccccccdddddddd""]\n        added_toks = tokenizer.add_tokens(new_toks)\n        vocab_size_2 = tokenizer.vocab_size\n        all_size_2 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_2, 0)\n        self.assertEqual(vocab_size, vocab_size_2)\n        self.assertEqual(added_toks, len(new_toks))\n        self.assertEqual(all_size_2, all_size + len(new_toks))\n\n        tokens = tokenizer.map_text_to_id(""aaaaabbbbbb low cccccccccdddddddd l"")\n        self.assertGreaterEqual(len(tokens), 4)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n\n        new_toks_2 = {\'eos_token\': "">>>>|||<||<<|<<"",\n                      \'pad_token\': ""<<<<<|||>|>>>>|>""}\n        added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n        vocab_size_3 = tokenizer.vocab_size\n        all_size_3 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_3, 0)\n        self.assertEqual(vocab_size, vocab_size_3)\n        self.assertEqual(added_toks_2, len(new_toks_2))\n        self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n\n        tokens = tokenizer.map_text_to_id(\n            "">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd ""\n            ""<<<<<|||>|>>>>|> l"")\n\n        self.assertGreaterEqual(len(tokens), 6)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[0], tokens[1])\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokens[-3])\n        self.assertEqual(tokens[0],\n                         tokenizer.map_token_to_id(tokenizer.eos_token))\n        self.assertEqual(tokens[-2],\n                         tokenizer.map_token_to_id(tokenizer.pad_token))\n\n    def test_encode_text(self):\n        tokenizer = RoBERTaTokenizer.load(self.tmp_dir.name,\n                                          self.special_tokens_map)\n\n        text_1 = u""lower newer""\n        text_2 = u""He is very happy""\n\n        text_1_ids = tokenizer.map_text_to_id(text_1)\n        text_2_ids = tokenizer.map_text_to_id(text_2)\n\n        cls_token_id = tokenizer.map_token_to_id(tokenizer.cls_token)\n        sep_token_id = tokenizer.map_token_to_id(tokenizer.sep_token)\n\n        input_ids, input_mask = \\\n            tokenizer.encode_text(text_1, None, 4)\n\n        self.assertListEqual(input_ids,\n                             [cls_token_id] + text_1_ids[:2] + [sep_token_id])\n        self.assertListEqual(input_mask, [1, 1, 1, 1])\n\n        input_ids, input_mask = \\\n            tokenizer.encode_text(text_1, text_2, 7)\n\n        self.assertListEqual(input_ids, [cls_token_id] + text_1_ids[:2] +\n                             [sep_token_id] + [sep_token_id] + text_2_ids[:1]\n                             + [sep_token_id])\n        self.assertListEqual(input_mask, [1, 1, 1, 1, 1, 1, 1])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/tokenizers/sentencepiece_tokenizer_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for SentencePiece tokenizer.\n""""""\n\nimport unittest\n\nimport os\nimport pickle\nimport tempfile\n\nfrom texar.torch.data.data_utils import maybe_download\nfrom texar.torch.data.tokenizers.sentencepiece_tokenizer import \\\n    SentencePieceTokenizer\n\n\nclass SentencePieceTokenizerTest(unittest.TestCase):\n\n    def setUp(self):\n        self.tmp_dir = tempfile.TemporaryDirectory()\n        self.SAMPLE_VOCAB = maybe_download(\n            \'https://github.com/google/sentencepiece/blob/master/\'\n            \'python/test/test_model.model?raw=true\', self.tmp_dir.name)\n\n        self.tokenizer = SentencePieceTokenizer.load(self.SAMPLE_VOCAB)\n\n        self.tokenizer.save(self.tmp_dir.name)\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    def test_load(self):\n        tokenizer = SentencePieceTokenizer.load(self.tmp_dir.name)\n\n        self.assertEqual(1000, len(tokenizer))\n        self.assertEqual(0, tokenizer.map_token_to_id(\'<unk>\'))\n        self.assertEqual(1, tokenizer.map_token_to_id(\'<s>\'))\n        self.assertEqual(2, tokenizer.map_token_to_id(\'</s>\'))\n        self.assertEqual(\'<unk>\', tokenizer.map_id_to_token(0))\n        self.assertEqual(\'<s>\', tokenizer.map_id_to_token(1))\n        self.assertEqual(\'</s>\', tokenizer.map_id_to_token(2))\n        for i in range(len(tokenizer)):\n            token = tokenizer.map_id_to_token(i)\n            self.assertEqual(i, tokenizer.map_token_to_id(token))\n\n    def test_roundtrip(self):\n        tokenizer = SentencePieceTokenizer.load(self.tmp_dir.name)\n\n        text = \'I saw a girl with a telescope.\'\n        ids = tokenizer.map_text_to_id(text)\n        tokens = tokenizer.map_text_to_token(text)\n\n        self.assertEqual(text, tokenizer.map_id_to_text(ids))\n        self.assertEqual(text, tokenizer.map_token_to_text(tokens))\n\n    def test_train(self):\n        tmp_dir = tempfile.TemporaryDirectory()\n        TEXT_FILE = maybe_download(\n            \'https://github.com/google/sentencepiece/blob/master/\'\n            \'data/botchan.txt?raw=true\', tmp_dir.name)\n\n        hparams = {\n            ""vocab_file"": None,\n            ""text_file"": TEXT_FILE,\n            ""vocab_size"": 1000,\n        }\n        tokenizer = SentencePieceTokenizer(hparams=hparams)\n\n        with open(TEXT_FILE, \'r\', encoding=\'utf-8\') as file:\n            for line in file:\n                tokenizer.map_token_to_text(tokenizer.map_text_to_token(line))\n                tokenizer.map_id_to_text(tokenizer.map_text_to_id(line))\n\n    def test_pickle(self):\n        tokenizer = SentencePieceTokenizer.load(self.tmp_dir.name)\n        self.assertIsNotNone(tokenizer)\n\n        text = u""Munich and Berlin are nice cities""\n        subwords = tokenizer.map_text_to_token(text)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            filename = os.path.join(tmpdirname, u""tokenizer.bin"")\n            with open(filename, ""wb"") as f:\n                pickle.dump(tokenizer, f)\n            with open(filename, ""rb"") as f:\n                tokenizer_new = pickle.load(f)\n\n        subwords_loaded = tokenizer_new.map_text_to_token(text)\n\n        self.assertListEqual(subwords, subwords_loaded)\n\n    def test_save_load(self):\n        tokenizer = SentencePieceTokenizer.load(self.tmp_dir.name)\n\n        before_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            tokenizer.save(tmpdirname)\n            tokenizer = tokenizer.load(tmpdirname)\n\n        after_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n        self.assertListEqual(before_tokens, after_tokens)\n\n    def test_add_tokens(self):\n        tokenizer = SentencePieceTokenizer.load(self.tmp_dir.name)\n\n        vocab_size = tokenizer.vocab_size\n        all_size = len(tokenizer)\n\n        self.assertNotEqual(vocab_size, 0)\n        self.assertEqual(vocab_size, all_size)\n\n        new_toks = [""aaaaabbbbbb"", ""cccccccccdddddddd""]\n        added_toks = tokenizer.add_tokens(new_toks)\n        vocab_size_2 = tokenizer.vocab_size\n        all_size_2 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_2, 0)\n        self.assertEqual(vocab_size, vocab_size_2)\n        self.assertEqual(added_toks, len(new_toks))\n        self.assertEqual(all_size_2, all_size + len(new_toks))\n\n        tokens = tokenizer.map_text_to_id(""aaaaabbbbbb low cccccccccdddddddd l"")\n        self.assertGreaterEqual(len(tokens), 4)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n\n        new_toks_2 = {\'eos_token\': "">>>>|||<||<<|<<"",\n                      \'pad_token\': ""<<<<<|||>|>>>>|>""}\n        added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n        vocab_size_3 = tokenizer.vocab_size\n        all_size_3 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_3, 0)\n        self.assertEqual(vocab_size, vocab_size_3)\n        self.assertEqual(added_toks_2, len(new_toks_2))\n        self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n\n        tokens = tokenizer.map_text_to_id(\n            "">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd ""\n            ""<<<<<|||>|>>>>|> l"")\n\n        self.assertGreaterEqual(len(tokens), 6)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[0], tokens[1])\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokens[-3])\n        self.assertEqual(tokens[0],\n                         tokenizer.map_token_to_id(tokenizer.eos_token))\n        self.assertEqual(tokens[-2],\n                         tokenizer.map_token_to_id(tokenizer.pad_token))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/tokenizers/t5_tokenizer_test.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for T5 tokenizer.\n""""""\n\nimport unittest\n\nimport os\nimport tempfile\n\nfrom texar.torch.utils.test import pretrained_test\nfrom texar.torch.data.tokenizers.t5_tokenizer import T5Tokenizer\nfrom texar.torch.data.data_utils import maybe_download\n\n\nclass T5TokenizerTest(unittest.TestCase):\n\n    def setUp(self):\n        self.tmp_dir = tempfile.TemporaryDirectory()\n        self.SAMPLE_VOCAB = maybe_download(\n            \'https://github.com/google/sentencepiece/blob/master/\'\n            \'python/test/test_model.model?raw=true\', self.tmp_dir.name)\n\n        self.tokenizer = T5Tokenizer.load(self.SAMPLE_VOCAB)\n\n        self.tokenizer.save(self.tmp_dir.name)\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    @pretrained_test\n    def test_model_loading(self):\n        for pretrained_model_name in T5Tokenizer.available_checkpoints():\n            tokenizer = T5Tokenizer(\n                pretrained_model_name=pretrained_model_name)\n\n            info = list(os.walk(tokenizer.pretrained_model_dir))\n            _, _, files = info[0]\n\n            self.assertIn(\'sentencepiece.model\', files)\n\n            _ = tokenizer.map_text_to_token(u""This is a test"")\n\n    def test_roundtrip(self):\n        tokenizer = T5Tokenizer.load(self.tmp_dir.name)\n\n        text = \'I saw a girl with a telescope.\'\n        ids = tokenizer.map_text_to_id(text)\n        tokens = tokenizer.map_text_to_token(text)\n\n        self.assertEqual(text, tokenizer.map_id_to_text(ids))\n        self.assertEqual(text, tokenizer.map_token_to_text(tokens))\n\n        text = \'<extra_id_32> I saw a girl with a telescope.<extra_id_74>\'\n        ids = tokenizer.map_text_to_id(text)\n        tokens = tokenizer.map_text_to_token(text)\n\n        self.assertEqual(text, tokenizer.map_id_to_text(ids))\n        self.assertEqual(text, tokenizer.map_token_to_text(tokens))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/data/tokenizers/xlnet_tokenizer_test.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for pre-trained XLNet tokenizer.\n""""""\n\nimport unittest\n\nimport os\nimport pickle\nimport tempfile\n\nfrom texar.torch.data.data_utils import maybe_download\nfrom texar.torch.data.tokenizers.xlnet_tokenizer import \\\n    XLNetTokenizer, SPIECE_UNDERLINE\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass XLNetTokenizerTest(unittest.TestCase):\n\n    def setUp(self):\n        self.tmp_dir = tempfile.TemporaryDirectory()\n        self.SAMPLE_VOCAB = maybe_download(\n            \'https://github.com/huggingface/transformers/blob/master/\'\n            \'tests/fixtures/test_sentencepiece.model?raw=true\',\n            self.tmp_dir.name)\n\n        self.tokenizer = XLNetTokenizer.load(\n            self.SAMPLE_VOCAB, configs={\'keep_accents\': True})\n        self.tokenizer.save(self.tmp_dir.name)\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    @pretrained_test\n    def test_model_loading(self):\n        for pretrained_model_name in \\\n                XLNetTokenizer.available_checkpoints():\n            tokenizer = XLNetTokenizer(\n                pretrained_model_name=pretrained_model_name)\n            _ = tokenizer.map_text_to_token(u""This is a test"")\n\n    def test_tokenize(self):\n        tokens = self.tokenizer.map_text_to_token(u\'This is a test\')\n        self.assertListEqual(tokens, [u\'\xe2\x96\x81This\', u\'\xe2\x96\x81is\', u\'\xe2\x96\x81a\', u\'\xe2\x96\x81t\', u\'est\'])\n\n        self.assertListEqual(\n            self.tokenizer.map_token_to_id(tokens),\n            [285, 46, 10, 170, 382])\n\n        tokens = self.tokenizer.map_text_to_token(\n            u""I was born in 92000, and this is fals\xc3\xa9."")\n        self.assertListEqual(tokens, [SPIECE_UNDERLINE + u\'I\',\n                                      SPIECE_UNDERLINE + u\'was\',\n                                      SPIECE_UNDERLINE + u\'b\',\n                                      u\'or\', u\'n\', SPIECE_UNDERLINE + u\'in\',\n                                      SPIECE_UNDERLINE + u\'\',\n                                      u\'9\', u\'2\', u\'0\', u\'0\', u\'0\', u\',\',\n                                      SPIECE_UNDERLINE + u\'and\',\n                                      SPIECE_UNDERLINE + u\'this\',\n                                      SPIECE_UNDERLINE + u\'is\',\n                                      SPIECE_UNDERLINE + u\'f\', u\'al\', u\'s\',\n                                      u\'\xc3\xa9\', u\'.\'])\n        ids = self.tokenizer.map_token_to_id(tokens)\n        self.assertListEqual(\n            ids, [8, 21, 84, 55, 24, 19, 7, 0,\n                  602, 347, 347, 347, 3, 12, 66,\n                  46, 72, 80, 6, 0, 4])\n\n        back_tokens = self.tokenizer.map_id_to_token(ids)\n        self.assertListEqual(back_tokens, [SPIECE_UNDERLINE + u\'I\',\n                                           SPIECE_UNDERLINE + u\'was\',\n                                           SPIECE_UNDERLINE + u\'b\',\n                                           u\'or\', u\'n\',\n                                           SPIECE_UNDERLINE + u\'in\',\n                                           SPIECE_UNDERLINE + u\'\', u\'<unk>\',\n                                           u\'2\', u\'0\', u\'0\', u\'0\', u\',\',\n                                           SPIECE_UNDERLINE + u\'and\',\n                                           SPIECE_UNDERLINE + u\'this\',\n                                           SPIECE_UNDERLINE + u\'is\',\n                                           SPIECE_UNDERLINE + u\'f\', u\'al\', u\'s\',\n                                           u\'<unk>\', u\'.\'])\n\n    def test_pickle(self):\n        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)\n        self.assertIsNotNone(tokenizer)\n\n        text = u""Munich and Berlin are nice cities""\n        subwords = tokenizer.map_text_to_token(text)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            filename = os.path.join(tmpdirname, u""tokenizer.bin"")\n            with open(filename, ""wb"") as f:\n                pickle.dump(tokenizer, f)\n            with open(filename, ""rb"") as f:\n                tokenizer_new = pickle.load(f)\n\n        subwords_loaded = tokenizer_new.map_text_to_token(text)\n\n        self.assertListEqual(subwords, subwords_loaded)\n\n    def test_save_load(self):\n        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)\n\n        before_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            tokenizer.save(tmpdirname)\n            tokenizer = tokenizer.load(tmpdirname)\n\n        after_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n        self.assertListEqual(before_tokens, after_tokens)\n\n    def test_pretrained_model_list(self):\n        model_list_1 = list(XLNetTokenizer._MODEL2URL.keys())\n        model_list_2 = list(XLNetTokenizer._MAX_INPUT_SIZE.keys())\n\n        self.assertListEqual(model_list_1, model_list_2)\n\n    def test_encode_decode(self):\n        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)\n\n        input_text = u""This is a test""\n        output_text = u""This is a test""\n\n        tokens = tokenizer.map_text_to_token(input_text)\n        ids = tokenizer.map_token_to_id(tokens)\n        ids_2 = tokenizer.map_text_to_id(input_text)\n        self.assertListEqual(ids, ids_2)\n\n        tokens_2 = tokenizer.map_id_to_token(ids)\n        text_2 = tokenizer.map_id_to_text(ids)\n\n        self.assertEqual(text_2, output_text)\n\n        self.assertNotEqual(len(tokens_2), 0)\n        self.assertIsInstance(text_2, str)\n\n    def test_add_tokens(self):\n        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)\n\n        vocab_size = tokenizer.vocab_size\n        all_size = len(tokenizer)\n\n        self.assertNotEqual(vocab_size, 0)\n        self.assertEqual(vocab_size, all_size)\n\n        new_toks = [""aaaaabbbbbb"", ""cccccccccdddddddd""]\n        added_toks = tokenizer.add_tokens(new_toks)\n        vocab_size_2 = tokenizer.vocab_size\n        all_size_2 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_2, 0)\n        self.assertEqual(vocab_size, vocab_size_2)\n        self.assertEqual(added_toks, len(new_toks))\n        self.assertEqual(all_size_2, all_size + len(new_toks))\n\n        tokens = tokenizer.map_text_to_id(""aaaaabbbbbb low cccccccccdddddddd l"")\n        self.assertGreaterEqual(len(tokens), 4)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n\n        new_toks_2 = {\'eos_token\': "">>>>|||<||<<|<<"",\n                      \'pad_token\': ""<<<<<|||>|>>>>|>""}\n        added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n        vocab_size_3 = tokenizer.vocab_size\n        all_size_3 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_3, 0)\n        self.assertEqual(vocab_size, vocab_size_3)\n        self.assertEqual(added_toks_2, len(new_toks_2))\n        self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n\n        tokens = tokenizer.map_text_to_id(\n            "">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd ""\n            ""<<<<<|||>|>>>>|> l"")\n\n        self.assertGreaterEqual(len(tokens), 6)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[0], tokens[1])\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokens[-3])\n        self.assertEqual(tokens[0],\n                         tokenizer.map_token_to_id(tokenizer.eos_token))\n        self.assertEqual(tokens[-2],\n                         tokenizer.map_token_to_id(tokenizer.pad_token))\n\n    def test_tokenizer_lower(self):\n        tokenizer = XLNetTokenizer.load(\n            self.SAMPLE_VOCAB, configs={\'do_lower_case\': True,\n                                        \'keep_accents\': False})\n        tokens = tokenizer.map_text_to_token(\n            u""I was born in 92000, and this is fals\xc3\xa9."")\n        self.assertListEqual(tokens, [SPIECE_UNDERLINE + u\'\', u\'i\',\n                                      SPIECE_UNDERLINE + u\'was\',\n                                      SPIECE_UNDERLINE + u\'b\',\n                                      u\'or\', u\'n\', SPIECE_UNDERLINE + u\'in\',\n                                      SPIECE_UNDERLINE + u\'\',\n                                      u\'9\', u\'2\', u\'0\', u\'0\', u\'0\', u\',\',\n                                      SPIECE_UNDERLINE + u\'and\',\n                                      SPIECE_UNDERLINE + u\'this\',\n                                      SPIECE_UNDERLINE + u\'is\',\n                                      SPIECE_UNDERLINE + u\'f\', u\'al\', u\'se\',\n                                      u\'.\'])\n        self.assertListEqual(tokenizer.map_text_to_token(u""H\\u00E9llo""),\n                             [u""\xe2\x96\x81he"", u""ll"", u""o""])\n\n    def test_tokenizer_no_lower(self):\n        tokenizer = XLNetTokenizer.load(\n            self.SAMPLE_VOCAB, configs={\'do_lower_case\': False,\n                                        \'keep_accents\': False})\n        tokens = tokenizer.map_text_to_token(\n            u""I was born in 92000, and this is fals\xc3\xa9."")\n        self.assertListEqual(tokens, [SPIECE_UNDERLINE + u\'I\',\n                                      SPIECE_UNDERLINE + u\'was\',\n                                      SPIECE_UNDERLINE + u\'b\', u\'or\',\n                                      u\'n\', SPIECE_UNDERLINE + u\'in\',\n                                      SPIECE_UNDERLINE + u\'\',\n                                      u\'9\', u\'2\', u\'0\', u\'0\', u\'0\', u\',\',\n                                      SPIECE_UNDERLINE + u\'and\',\n                                      SPIECE_UNDERLINE + u\'this\',\n                                      SPIECE_UNDERLINE + u\'is\',\n                                      SPIECE_UNDERLINE + u\'f\', u\'al\', u\'se\',\n                                      u\'.\'])\n\n    def test_encode_text(self):\n        text_1 = u""He is very happy""\n        text_2 = u""unwanted, running""\n\n        text_1_ids = self.tokenizer.map_text_to_id(text_1)\n        text_2_ids = self.tokenizer.map_text_to_id(text_2)\n\n        cls_token_id = self.tokenizer.map_token_to_id(self.tokenizer.cls_token)\n        sep_token_id = self.tokenizer.map_token_to_id(self.tokenizer.sep_token)\n\n        input_ids, segment_ids, input_mask = \\\n            self.tokenizer.encode_text(text_1, None, 4)\n\n        self.assertListEqual(input_ids,\n                             text_1_ids[:2] + [sep_token_id] + [cls_token_id])\n        self.assertListEqual(segment_ids, [0, 0, 0, 2])\n        self.assertListEqual(input_mask, [0, 0, 0, 0])\n\n        input_ids, segment_ids, input_mask = \\\n            self.tokenizer.encode_text(text_1, text_2, 7)\n\n        self.assertListEqual(input_ids, text_1_ids[:2] +\n                             [sep_token_id] + text_2_ids[:2] + [sep_token_id] +\n                             [cls_token_id])\n        self.assertListEqual(segment_ids, [0, 0, 0, 1, 1, 1, 2])\n        self.assertListEqual(input_mask, [0, 0, 0, 0, 0, 0, 0])\n\n    def test_encode_text_for_generation(self):\n        text_1 = u""lower newer""\n\n        text_1_ids = self.tokenizer.map_text_to_id(text_1)\n\n        input_ids, seq_len = \\\n            self.tokenizer.encode_text_for_generation(text=text_1,\n                                                      max_seq_length=10)\n\n        bos_token_id = self.tokenizer.map_token_to_id(self.tokenizer.bos_token)\n        eos_token_id = self.tokenizer.map_token_to_id(self.tokenizer.eos_token)\n        pad_token_id = self.tokenizer.map_token_to_id(self.tokenizer.pad_token)\n\n        self.assertListEqual(input_ids,\n                             [bos_token_id] + text_1_ids + [eos_token_id] +\n                             [pad_token_id, pad_token_id, pad_token_id])\n        self.assertEqual(seq_len, 7)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/classifiers/bert_classifier_test.py,24,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for BERT classifiers.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.classifiers.bert_classifier import *\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass BERTClassifierTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.BERTClassifier` class.\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.inputs = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n        for pretrained_model_name in BERTClassifier.available_checkpoints():\n            classifier = BERTClassifier(\n                pretrained_model_name=pretrained_model_name)\n            _, _ = classifier(self.inputs)\n\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        classifier = BERTClassifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 199 + 2)\n        _, _ = classifier(self.inputs)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": 8,\n        }\n        classifier = BERTClassifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 199 + 2)\n        _, _ = classifier(self.inputs)\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = BERTClassifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 199 + 2)\n        _, _ = classifier(self.inputs)\n\n    def test_classification(self):\n        r""""""Tests classification.\n        """"""\n        inputs = torch.randint(30521, (self.batch_size, self.max_length))\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        classifier = BERTClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = BERTClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 0,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = BERTClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 4\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = BERTClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n    def test_binary(self):\n        r""""""Tests binary classification.\n        """"""\n        inputs = torch.randint(30521, (self.batch_size, self.max_length))\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = BERTClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = BERTClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size([self.batch_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = BERTClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size([self.batch_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n    def test_soft_ids(self):\n        r""""""Tests soft ids.\n        """"""\n        inputs = torch.rand(self.batch_size, self.max_length, 30522)\n\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = BERTClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/classifiers/conv_classifiers_test.py,11,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for conv encoders.\n""""""\nimport unittest\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.modules.classifiers.conv_classifiers import Conv1DClassifier\n\n\nclass Conv1DClassifierTest(unittest.TestCase):\n    """"""Tests :class:`~texar.torch.modules.Conv1DClassifier` class.\n    """"""\n\n    def test_classifier(self):\n        """"""Tests classification.\n        """"""\n        # case 1: default hparams\n        inputs = torch.randn(128, 32, 300)\n        classifier = Conv1DClassifier(in_channels=inputs.shape[1],\n                                      in_features=inputs.shape[2])\n\n        self.assertEqual(len(classifier.layers), 5)\n        self.assertIsInstance(classifier.layers[-1], nn.Linear)\n        logits, pred = classifier(inputs)\n        self.assertEqual(logits.shape, torch.Size([128, 2]))\n        self.assertEqual(pred.shape, torch.Size([128]))\n\n        # case 2\n        inputs = torch.randn(128, 32, 300)\n        hparams = {\n            ""num_classes"": 10,\n            ""logit_layer_kwargs"": {""bias"": False}\n        }\n        classifier = Conv1DClassifier(in_channels=inputs.shape[1],\n                                      in_features=inputs.shape[2],\n                                      hparams=hparams)\n        logits, pred = classifier(inputs)\n        self.assertEqual(logits.shape, torch.Size([128, 10]))\n        self.assertEqual(pred.shape, torch.Size([128]))\n\n    def test_classifier_with_no_dense_layer(self):\n        """"""Tests classifier with no dense layer.\n        """"""\n        inputs = torch.randn(128, 32, 300)\n        hparams = {\n            ""num_dense_layers"": 0,\n            ""num_classes"": 10,\n            ""logit_layer_kwargs"": {""bias"": False}\n        }\n        classifier = Conv1DClassifier(in_channels=inputs.shape[1],\n                                      in_features=inputs.shape[2],\n                                      hparams=hparams)\n        logits, pred = classifier(inputs)\n        self.assertEqual(logits.shape, torch.Size([128, 10]))\n        self.assertEqual(pred.shape, torch.Size([128]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/classifiers/gpt2_classifier_test.py,24,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for GPT2 classifier.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.classifiers.gpt2_classifier import *\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass GPT2ClassifierTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.GPT2Classifier` class.\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.inputs = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n        for pretrained_model_name in GPT2Classifier.available_checkpoints():\n            classifier = GPT2Classifier(\n                pretrained_model_name=pretrained_model_name)\n            _, _ = classifier(self.inputs)\n\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 198)\n        _, _ = classifier(self.inputs)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": 8,\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 198)\n        _, _ = classifier(self.inputs)\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 198)\n        _, _ = classifier(self.inputs)\n\n    def test_classification(self):\n        r""""""Tests classificaiton.\n        """"""\n        inputs = torch.randint(30521, (self.batch_size, self.max_length))\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 0,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 4\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n    def test_binary(self):\n        r""""""Tests binary classification.\n        """"""\n        inputs = torch.randint(30521, (self.batch_size, self.max_length))\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size([self.batch_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size([self.batch_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n    def test_soft_ids(self):\n        r""""""Tests soft ids.\n        """"""\n        inputs = torch.rand(self.batch_size, self.max_length, 50257)\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/classifiers/rnn_classifiers_test.py,17,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for RNN classifiers.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.classifiers.rnn_classifiers import *\n\n\nclass UnidirectionalRNNClassifierTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.UnidirectionalRNNClassifier` class.\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.emb_dim = 4\n        self.inputs = torch.rand(\n            self.batch_size, self.max_length, self.emb_dim)\n\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        # case 1\n        classifier = UnidirectionalRNNClassifier(input_size=self.emb_dim)\n        output, _ = classifier(self.inputs)\n        self.assertEqual(len(classifier.trainable_variables), 4 + 2)\n        self.assertEqual(output.size()[-1], classifier.output_size)\n\n        # case 2\n        hparams = {\n            ""output_layer"": {""num_layers"": 2},\n            ""logit_layer_kwargs"": {""bias"": False}\n        }\n        classifier = UnidirectionalRNNClassifier(input_size=self.emb_dim,\n                                                 hparams=hparams)\n        output, _ = classifier(self.inputs)\n        self.assertEqual(len(classifier.trainable_variables), 4 + 2 + 2 + 1)\n\n    def test_encode(self):\n        r""""""Tests encoding.\n        """"""\n        # case 1\n        classifier = UnidirectionalRNNClassifier(input_size=self.emb_dim)\n        logits, pred = classifier(self.inputs)\n        self.assertEqual(logits.shape,\n                         torch.Size([self.batch_size,\n                                     classifier.hparams.num_classes]))\n        self.assertEqual(pred.shape, torch.Size([self.batch_size]))\n\n        # case 2\n        hparams = {\n            ""num_classes"": 10,\n            ""clas_strategy"": ""time_wise""\n        }\n        classifier = UnidirectionalRNNClassifier(input_size=self.emb_dim,\n                                                 hparams=hparams)\n        logits, pred = classifier(self.inputs)\n        self.assertEqual(logits.shape,\n                         torch.Size([self.batch_size, self.max_length,\n                                     classifier.hparams.num_classes]))\n        self.assertEqual(pred.shape,\n                         torch.Size([self.batch_size, self.max_length]))\n\n        # case 3\n        hparams = {\n            ""output_layer"": {\n                ""num_layers"": 1,\n                ""layer_size"": 10\n            },\n            ""num_classes"": 0,\n            ""clas_strategy"": ""time_wise""\n        }\n        classifier = UnidirectionalRNNClassifier(input_size=self.emb_dim,\n                                                 hparams=hparams)\n        logits, pred = classifier(self.inputs)\n        self.assertEqual(logits.shape,\n                         torch.Size([self.batch_size, self.max_length, 10]))\n        self.assertEqual(pred.shape,\n                         torch.Size([self.batch_size, self.max_length]))\n\n        # case 4\n        hparams = {\n            ""num_classes"": 10,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": 5\n        }\n        classifier = UnidirectionalRNNClassifier(input_size=self.emb_dim,\n                                                 hparams=hparams)\n        logits, pred = classifier(self.inputs)\n        self.assertEqual(logits.shape,\n                         torch.Size([self.batch_size,\n                                     classifier.hparams.num_classes]))\n        self.assertEqual(pred.shape, torch.Size([self.batch_size]))\n\n    def test_binary(self):\n        r""""""Tests binary classification.\n        """"""\n        # case 1\n        hparams = {\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise""\n        }\n        classifier = UnidirectionalRNNClassifier(input_size=self.emb_dim,\n                                                 hparams=hparams)\n        logits, pred = classifier(self.inputs)\n        self.assertEqual(logits.shape,\n                         torch.Size([self.batch_size, self.max_length]))\n        self.assertEqual(pred.shape,\n                         torch.Size([self.batch_size, self.max_length]))\n\n        # case 2\n        hparams = {\n            ""output_layer"": {\n                ""num_layers"": 1,\n                ""layer_size"": 10\n            },\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise""\n        }\n        classifier = UnidirectionalRNNClassifier(input_size=self.emb_dim,\n                                                 hparams=hparams)\n        logits, pred = classifier(self.inputs)\n        self.assertEqual(logits.shape,\n                         torch.Size([self.batch_size, self.max_length]))\n        self.assertEqual(pred.shape,\n                         torch.Size([self.batch_size, self.max_length]))\n\n        # case 3\n        hparams = {\n            ""num_classes"": 1,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": 5\n        }\n        classifier = UnidirectionalRNNClassifier(input_size=self.emb_dim,\n                                                 hparams=hparams)\n        logits, pred = classifier(self.inputs)\n        self.assertEqual(logits.shape,\n                         torch.Size([self.batch_size]))\n        self.assertEqual(pred.shape,\n                         torch.Size([self.batch_size]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/classifiers/roberta_classifier_test.py,24,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for RoBERTa classifiers.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.classifiers.roberta_classifier import *\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass RoBERTaClassifierTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.RoBERTaClassifier` class.\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.inputs = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n        for pretrained_model_name in RoBERTaClassifier.available_checkpoints():\n            classifier = RoBERTaClassifier(\n                pretrained_model_name=pretrained_model_name)\n            _, _ = classifier(self.inputs)\n\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        classifier = RoBERTaClassifier(hparams=hparams)\n\n        self.assertEqual(len(classifier.trainable_variables), 200)\n        _, _ = classifier(self.inputs)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": 8,\n        }\n        classifier = RoBERTaClassifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 200)\n        _, _ = classifier(self.inputs)\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = RoBERTaClassifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 200)\n        _, _ = classifier(self.inputs)\n\n    def test_classification(self):\n        r""""""Tests classification.\n        """"""\n        inputs = torch.randint(30521, (self.batch_size, self.max_length))\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        classifier = RoBERTaClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = RoBERTaClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 0,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = RoBERTaClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 4\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = RoBERTaClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n    def test_binary(self):\n        r""""""Tests binary classification.\n        """"""\n        inputs = torch.randint(30521, (self.batch_size, self.max_length))\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = RoBERTaClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = RoBERTaClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size([self.batch_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = RoBERTaClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size([self.batch_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n    def test_soft_ids(self):\n        r""""""Tests soft ids.\n        """"""\n        inputs = torch.rand(self.batch_size, self.max_length, 50265)\n\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = RoBERTaClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/classifiers/xlnet_classifier_test.py,24,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for XLNet classifiers.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.classifiers.xlnet_classifier import *\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass XLNetClassifierTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.XLNetClassifier` class.\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.inputs = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n        for pretrained_model_name in XLNetClassifier.available_checkpoints():\n            classifier = XLNetClassifier(\n                pretrained_model_name=pretrained_model_name)\n            _, _ = classifier(self.inputs)\n\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 182 + 4)\n        _, _ = classifier(self.inputs)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""use_projection"": False,\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 182 + 2)\n        _, _ = classifier(self.inputs)\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": 8,\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 182 + 4)\n        _, _ = classifier(self.inputs)\n\n        # case 4\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        self.assertEqual(len(classifier.trainable_variables), 182 + 4)\n        _, _ = classifier(self.inputs)\n\n    def test_classification(self):\n        r""""""Tests classification.\n        """"""\n        inputs = torch.randint(32000, (self.batch_size, self.max_length))\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""time_wise""\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 0,\n            ""clas_strategy"": ""time_wise""\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 4\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, classifier.output_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n    def test_binary(self):\n        r""""""Tests binary classification.\n        """"""\n        inputs = torch.randint(30521, (self.batch_size, self.max_length))\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size([self.batch_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size([self.batch_size]))\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n    def test_soft_ids(self):\n        r""""""Tests soft ids.\n        """"""\n        inputs = torch.rand(self.batch_size, self.max_length, 32000)\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = XLNetClassifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        self.assertEqual(logits.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/connectors/connectors_test.py,40,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for connectors.\n""""""\n\nfrom __future__ import unicode_literals\n\nfrom collections import namedtuple\nimport unittest\nimport torch\nimport torch.distributions as tds\n\nfrom texar.torch.core import layers\nfrom texar.torch.modules.connectors.connectors import ConstantConnector\nfrom texar.torch.modules.connectors.connectors import MLPTransformConnector\nfrom texar.torch.modules.connectors.connectors import (\n    ReparameterizedStochasticConnector)\nfrom texar.torch.modules.connectors.connectors import _assert_same_size\n\nfrom texar.torch.utils import nest\n\n\nclass TestConnectors(unittest.TestCase):\n    r""""""Tests various connectors.\n    """"""\n\n    def setUp(self) -> None:\n        self._batch_size = 100\n\n        self._decoder_cell = layers.get_rnn_cell(\n            256, layers.default_rnn_cell_hparams())\n\n    def test_constant_connector(self):\n        r""""""Tests the logic of\n        :class:`~texar.torch.modules.connectors.ConstantConnector`.\n        """"""\n\n        state_size = namedtuple(\'LSTMStateTuple\', [\'h\', \'c\'])(256, 256)\n        connector_0 = ConstantConnector(state_size)\n        decoder_initial_state_0 = connector_0(self._batch_size)\n        connector_1 = ConstantConnector(\n            state_size, hparams={""value"": 1.})\n        decoder_initial_state_1 = connector_1(self._batch_size)\n\n        s_0 = decoder_initial_state_0\n        s_1 = decoder_initial_state_1\n        self.assertEqual(nest.flatten(s_0)[0][0, 0], 0.)\n        self.assertEqual(nest.flatten(s_1)[0][0, 0], 1.)\n\n        size = torch.Size([1, 2, 3])\n        connector_size_0 = ConstantConnector(\n            size, hparams={""value"": 2.})\n        size_tensor = connector_size_0(self._batch_size)\n        self.assertEqual(\n            torch.Size([self._batch_size]) + size, size_tensor.size())\n        self.assertEqual(size_tensor[0][0, 0, 0], 2.)\n\n        tuple_size_1 = (torch.Size([1, 2, 3]), torch.Size([4, 5, 6]))\n        connector_size_1 = ConstantConnector(\n            tuple_size_1, hparams={""value"": 3.})\n        tuple_size_tensor = connector_size_1(self._batch_size)\n        tuple_size_tensor_0 = tuple_size_tensor[0]\n        tuple_size_tensor_1 = tuple_size_tensor[1]\n        self.assertEqual(\n            torch.Size([self._batch_size]) + torch.Size([1, 2, 3]),\n            tuple_size_tensor_0.size())\n        self.assertEqual(tuple_size_tensor_0[0][0, 0, 0], 3.)\n        self.assertEqual(\n            torch.Size([self._batch_size]) + torch.Size([4, 5, 6]),\n            tuple_size_tensor_1.size())\n        self.assertEqual(tuple_size_tensor_1[0][0, 0, 0], 3.)\n\n        tuple_size_2 = (5, 10)\n        connector_size_2 = ConstantConnector(\n            tuple_size_2, hparams={""value"": 4.})\n        tuple_size_tensor = connector_size_2(self._batch_size)\n        tuple_size_tensor_0 = tuple_size_tensor[0]\n        tuple_size_tensor_1 = tuple_size_tensor[1]\n        self.assertEqual(\n            torch.Size([self._batch_size]) + torch.Size([5]),\n            tuple_size_tensor_0.size())\n        self.assertEqual(tuple_size_tensor_0[0][0], 4.)\n        self.assertEqual(\n            torch.Size([self._batch_size]) + torch.Size([10]),\n            tuple_size_tensor_1.size())\n        self.assertEqual(tuple_size_tensor_1[0][0], 4.)\n\n        tuple_size_3 = (torch.Size([1, 2, 3]), 10)\n        connector_size_3 = ConstantConnector(\n            tuple_size_3, hparams={""value"": 4.})\n        tuple_size_tensor = connector_size_3(self._batch_size)\n        tuple_size_tensor_0 = tuple_size_tensor[0]\n        tuple_size_tensor_1 = tuple_size_tensor[1]\n        self.assertEqual(\n            torch.Size([self._batch_size]) + torch.Size([1, 2, 3]),\n            tuple_size_tensor_0.size())\n        self.assertEqual(tuple_size_tensor_0[0][0, 0, 0], 4.)\n        self.assertEqual(\n            torch.Size([self._batch_size]) + torch.Size([10]),\n            tuple_size_tensor_1.size())\n        self.assertEqual(tuple_size_tensor_1[0][0], 4.)\n\n    def test_forward_connector(self):\n        r""""""Tests the logic of\n        :class:`~texar.torch.modules.connectors.ForwardConnector`.\n        """"""\n        # TODO(zhiting)\n        pass\n\n    def test_mlp_transform_connector(self):\n        r""""""Tests the logic of\n        :class:`~texar.torch.modules.connectors.MLPTransformConnector`.\n        """"""\n        state_size = namedtuple(\'LSTMStateTuple\', [\'h\', \'c\'])(256, 256)\n        connector = MLPTransformConnector(state_size, linear_layer_dim=10)\n        output = connector(torch.zeros(5, 10))\n        output_1 = output[0]\n        output_2 = output[1]\n        self.assertEqual(\n            torch.Size([5, 256]),\n            output_1.size())\n        self.assertEqual(\n            torch.Size([5, 256]),\n            output_2.size())\n\n        state_size = (16, 32)\n        connector = MLPTransformConnector(state_size, linear_layer_dim=10)\n        output = connector(torch.zeros(5, 10))\n        output_1 = output[0]\n        output_2 = output[1]\n        self.assertEqual(\n            torch.Size([5, 16]),\n            output_1.size())\n        self.assertEqual(\n            torch.Size([5, 32]),\n            output_2.size())\n\n        state_size = (torch.Size([8, 32]), torch.Size([16, 64]))\n        connector = MLPTransformConnector(state_size, linear_layer_dim=10)\n        output = connector(torch.zeros(5, 10))\n        output_1 = output[0]\n        output_2 = output[1]\n        self.assertEqual(\n            torch.Size([5, 8, 32]),\n            output_1.size())\n        self.assertEqual(\n            torch.Size([5, 16, 64]),\n            output_2.size())\n\n    def test_reparameterized_stochastic_connector(self):\n        r""""""Tests the logic of\n        :class:`~texar.torch.modules.ReparameterizedStochasticConnector`.\n        """"""\n\n        self._batch_size = 16\n        variable_size = 100\n        sample_num = 10\n\n        mu = torch.zeros([self._batch_size, variable_size])\n        var = torch.ones([variable_size])\n\n        state_size = (10, 11)\n        gauss_connector = ReparameterizedStochasticConnector(\n            state_size,\n            mlp_input_size=mu.size()[-1:],\n            distribution=""MultivariateNormal"",\n            distribution_kwargs={""loc"": mu, ""scale_tril"": torch.diag(var)})\n        output, samples = gauss_connector()\n\n        self.assertEqual(output[0].size(),\n                         torch.Size([self._batch_size, state_size[0]]))\n        self.assertEqual(output[1].size(),\n                         torch.Size([self._batch_size, state_size[1]]))\n        self.assertEqual(samples.size(),\n                         torch.Size([self._batch_size, variable_size]))\n\n        output, _ = gauss_connector(num_samples=12, transform=False)\n        self.assertEqual(output.size(),\n                         torch.Size([12, self._batch_size, variable_size]))\n\n        state_size_ts = (torch.Size([10, 10]), torch.Size([2, 3, 4]))\n        gauss_connector_ts = ReparameterizedStochasticConnector(\n            state_size_ts,\n            mlp_input_size=mu.size()[-1:],\n            distribution=""MultivariateNormal"",\n            distribution_kwargs={""loc"": mu, ""scale_tril"": torch.diag(var)})\n        output, samples = gauss_connector_ts()\n\n        _assert_same_size(output, state_size_ts)\n\n        # sample_mu = np.mean(sample_outputs, axis=0)\n        # # pylint: disable=no-member\n        # sample_var = np.var(sample_outputs, axis=0)\n\n        # # check if the value is approximated N(0, 1)\n        # for i in range(variable_size):\n        #     self.assertAlmostEqual(0, sample_mu[i], delta=0.2)\n        #     self.assertAlmostEqual(1, sample_var[i], delta=0.2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/decoders/decoder_helpers_test.py,5,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for Transformer decoder.\n""""""\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.decoders.decoder_helpers import (\n    GreedyEmbeddingHelper, TopKSampleEmbeddingHelper, TopPSampleEmbeddingHelper)\n\n\nclass SamplerTest(unittest.TestCase):\n    r""""""Tests decoder helper utilities.\n    """"""\n\n    def setUp(self):\n        self.logits = torch.Tensor([[0.2, 0.2, 0.55, 0.05]])\n        # softmax values for above tensor is\n        # tensor([[0.2337, 0.2337, 0.3316, 0.2011]])\n        self.start_token = torch.LongTensor([1])\n        self.end_token = 2\n\n    def test_greedy_sampler(self):\n        """"""Tests Greedy Sampler.""""""\n        sampler = GreedyEmbeddingHelper(start_tokens=self.start_token,\n                                        end_token=self.end_token)\n        index = sampler.sample(time=0, outputs=self.logits)\n        assert torch.equal(index, torch.argmax(self.logits, dim=1))\n\n    def test_top_k_sampler(self):\n        """"""Tests Top-K Sampler.""""""\n        sampler = TopKSampleEmbeddingHelper(start_tokens=self.start_token,\n                                            end_token=self.end_token, top_k=1)\n        index = sampler.sample(time=0, outputs=self.logits)\n        assert torch.equal(index, torch.argmax(self.logits, dim=1))\n\n    def test_top_p_sampler(self):\n        """"""Tests Top-P Sampler also known as Nucleus Sampler.""""""\n        sampler = TopPSampleEmbeddingHelper(start_tokens=self.start_token,\n                                            end_token=self.end_token, p=0.6)\n        index = sampler.sample(time=0, outputs=self.logits)\n        assert index.item() in [0, 1, 2]\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/decoders/gpt2_decoder_test.py,16,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for GPT2 decoder.\n""""""\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.decoders import decoder_helpers\nfrom texar.torch.modules.decoders.gpt2_decoder import GPT2Decoder\nfrom texar.torch.modules.decoders.transformer_decoders import \\\n    TransformerDecoderOutput\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass GPT2DecoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.GPT2Decoder`\n    """"""\n\n    def setUp(self) -> None:\n        # Use small prime numbers to speedup tests.\n        self.batch_size = 2\n        self.max_length = 3\n        self.beam_width = 5\n        self.inputs = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_hparams(self):\n        r""""""Tests the priority of the decoder arch parameters.\n        """"""\n        # case 1: set ""pretrained_mode_name"" by constructor argument\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-medium"",\n        }\n        decoder = GPT2Decoder(pretrained_model_name=""gpt2-small"",\n                              hparams=hparams)\n        self.assertEqual(decoder.hparams.decoder.num_blocks, 12)\n        _ = decoder(self.inputs)\n\n        # case 2: set ""pretrained_mode_name"" by hparams\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-small"",\n            ""decoder"": {\n                ""num_blocks"": 6,\n            },\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        self.assertEqual(decoder.hparams.decoder.num_blocks, 12)\n        _ = decoder(self.inputs)\n\n        # case 3: set to None in both hparams and constructor argument\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""decoder"": {\n                ""num_blocks"": 6,\n            },\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        self.assertEqual(decoder.hparams.decoder.num_blocks, 6)\n        _ = decoder(self.inputs)\n\n        # case 4: using default hparams\n        decoder = GPT2Decoder()\n        self.assertEqual(decoder.hparams.decoder.num_blocks, 12)\n        _ = decoder(self.inputs)\n\n    @pretrained_test\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        def get_variable_num(n_layers: int) -> int:\n            return 1 + 1 + n_layers * 26 + 2\n\n        # case 1: GPT2 small\n        decoder = GPT2Decoder()\n        self.assertEqual(len(decoder.trainable_variables), get_variable_num(12))\n        _ = decoder(self.inputs)\n\n        # case 2: GPT2 medium\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-medium"",\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        self.assertEqual(len(decoder.trainable_variables), get_variable_num(24))\n        _ = decoder(self.inputs)\n\n        # case 2: GPT2 large\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-large"",\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        self.assertEqual(len(decoder.trainable_variables), get_variable_num(36))\n        _ = decoder(self.inputs)\n\n        # case 3: self-designed GPT2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""decoder"": {\n                ""num_blocks"": 6,\n            },\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        self.assertEqual(len(decoder.trainable_variables), get_variable_num(6))\n        _ = decoder(self.inputs)\n\n    def test_decode_train(self):\n        r""""""Tests train_greedy.\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        decoder.train()\n\n        inputs = torch.randint(50257, (self.batch_size, self.max_length))\n        outputs = decoder(inputs)\n\n        self.assertEqual(outputs.logits.shape,\n                         torch.Size([self.batch_size, self.max_length, 50257]))\n        self.assertEqual(outputs.sample_id.shape,\n                         torch.Size([self.batch_size, self.max_length]))\n\n    def test_decode_infer_greedy(self):\n        r""""""Tests train_greedy\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        decoder.eval()\n\n        start_tokens = torch.full((self.batch_size,), 1, dtype=torch.int64)\n        end_token = 2\n\n        helper = decoder_helpers.GreedyEmbeddingHelper(start_tokens, end_token)\n\n        outputs, length = decoder(\n            helper=helper, max_decoding_length=self.max_length)\n\n        self.assertIsInstance(outputs, TransformerDecoderOutput)\n\n    def test_decode_infer_sample(self):\n        r""""""Tests infer_sample\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        decoder.eval()\n\n        start_tokens = torch.full((self.batch_size,), 1, dtype=torch.int64)\n        end_token = 2\n\n        helper = decoder_helpers.SampleEmbeddingHelper(start_tokens, end_token)\n\n        outputs, length = decoder(\n            helper=helper, max_decoding_length=self.max_length)\n\n        self.assertIsInstance(outputs, TransformerDecoderOutput)\n\n    def test_beam_search(self):\n        r""""""Tests beam_search\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        decoder.eval()\n\n        start_tokens = torch.full((self.batch_size,), 1, dtype=torch.int64)\n        end_token = 2\n\n        outputs = decoder(\n            start_tokens=start_tokens, beam_width=self.beam_width,\n            end_token=end_token, max_decoding_length=self.max_length)\n\n        self.assertEqual(\n            outputs[\'log_prob\'].shape,\n            torch.Size([self.batch_size, self.beam_width]))\n        self.assertEqual(outputs[\'sample_id\'].shape[0], self.batch_size)\n        self.assertLessEqual(outputs[\'sample_id\'].shape[1], self.max_length)\n        self.assertEqual(outputs[\'sample_id\'].shape[2], self.beam_width)\n\n    def test_greedy_embedding_helper(self):\n        r""""""Tests with tf.contrib.seq2seq.GreedyEmbeddingHelper\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        decoder.eval()\n\n        start_tokens = torch.full((self.batch_size,), 1, dtype=torch.int64)\n        end_token = 2\n\n        helper = decoder_helpers.GreedyEmbeddingHelper(start_tokens, end_token)\n\n        outputs, length = decoder(\n            helper=helper, max_decoding_length=self.max_length)\n\n        self.assertIsInstance(outputs, TransformerDecoderOutput)\n\n    def test_topk_embedding_helper(self):\n        r""""""Tests TopKSampleEmbeddingHelper\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        decoder.eval()\n\n        start_tokens = torch.full((self.batch_size,), 1, dtype=torch.int64)\n        end_token = 2\n\n        helper = decoder_helpers.TopKSampleEmbeddingHelper(\n            start_tokens=start_tokens, end_token=end_token,\n            top_k=40, softmax_temperature=0.7)\n\n        outputs, length = decoder(\n            helper=helper, max_decoding_length=self.max_length)\n\n        self.assertIsInstance(outputs, TransformerDecoderOutput)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/decoders/rnn_decoders_test.py,30,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for RNN decoders.\n""""""\n\nimport unittest\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.modules.decoders.decoder_helpers import get_helper\nfrom texar.torch.modules.decoders.rnn_decoders import (\n    AttentionRNNDecoder, AttentionRNNDecoderOutput, BasicRNNDecoder,\n    BasicRNNDecoderOutput)\nfrom texar.torch.modules.embedders.embedders import WordEmbedder\nfrom texar.torch.utils.utils import map_structure\n\n\nclass BasicRNNDecoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.decoders.rnn_decoders.BasicRNNDecoder`.\n    """"""\n\n    def setUp(self):\n        self._vocab_size = 4\n        self._max_time = 8\n        self._batch_size = 16\n        self._emb_dim = 20\n        self._inputs = torch.randint(\n            self._vocab_size, size=(self._batch_size, self._max_time))\n        embedding = torch.rand(\n            self._vocab_size, self._emb_dim, dtype=torch.float)\n        self._embedder = WordEmbedder(init_value=embedding)\n        self._hparams = HParams(None, BasicRNNDecoder.default_hparams())\n\n    def _test_outputs(self, decoder, outputs, final_state, sequence_lengths,\n                      test_mode=False):\n        hidden_size = decoder.hparams.rnn_cell.kwargs.num_units\n\n        self.assertIsInstance(outputs, BasicRNNDecoderOutput)\n        max_time = (self._max_time if not test_mode\n                    else max(sequence_lengths).item())\n        self.assertEqual(\n            outputs.logits.shape,\n            (self._batch_size, max_time, self._vocab_size))\n        if not test_mode:\n            np.testing.assert_array_equal(\n                sequence_lengths, [max_time] * self._batch_size)\n        self.assertEqual(final_state[0].shape, (self._batch_size, hidden_size))\n\n    def test_decode_train(self):\n        r""""""Tests decoding in training mode.\n        """"""\n        decoder = BasicRNNDecoder(\n            token_embedder=self._embedder, input_size=self._emb_dim,\n            vocab_size=self._vocab_size, hparams=self._hparams)\n        sequence_length = torch.tensor([self._max_time] * self._batch_size)\n\n        # Helper by default HParams\n        helper_train = decoder.create_helper()\n        outputs, final_state, sequence_lengths = decoder(\n            helper=helper_train, inputs=self._inputs,\n            sequence_length=sequence_length)\n        self._test_outputs(decoder, outputs, final_state, sequence_lengths)\n\n        # Helper by decoding strategy\n        helper_train = decoder.create_helper(decoding_strategy=\'train_greedy\')\n        outputs, final_state, sequence_lengths = decoder(\n            helper=helper_train, inputs=self._inputs,\n            sequence_length=sequence_length)\n        self._test_outputs(decoder, outputs, final_state, sequence_lengths)\n\n        # Implicit helper\n        outputs, final_state, sequence_lengths = decoder(\n            inputs=self._inputs, sequence_length=sequence_length)\n        self._test_outputs(decoder, outputs, final_state, sequence_lengths)\n\n        # Eval helper through forward args\n        outputs, final_state, sequence_lengths = decoder(\n            embedding=self._embedder,\n            start_tokens=torch.tensor([1] * self._batch_size),\n            end_token=2, infer_mode=True)\n        self._test_outputs(\n            decoder, outputs, final_state, sequence_lengths, test_mode=True)\n\n    @staticmethod\n    def _assert_tensor_equal(a: torch.Tensor, b: torch.Tensor) -> bool:\n        if torch.is_tensor(a):\n            a = a.detach().numpy()\n        if torch.is_tensor(b):\n            b = b.detach().numpy()\n        if any(np.issubdtype(array.dtype, np.floating) for array in [a, b]):\n            return np.testing.assert_allclose(a, b, rtol=1e-5, atol=1e-8)\n        return np.testing.assert_array_equal(a, b)\n\n    def test_decode_train_with_torch(self):\n        r""""""Compares decoding results with PyTorch built-in decoder.\n        """"""\n        decoder = BasicRNNDecoder(\n            token_embedder=self._embedder, input_size=self._emb_dim,\n            vocab_size=self._vocab_size, hparams=self._hparams)\n\n        input_size = self._emb_dim\n        hidden_size = decoder.hparams.rnn_cell.kwargs.num_units\n        num_layers = decoder.hparams.rnn_cell.num_layers\n        torch_lstm = nn.LSTM(input_size, hidden_size, num_layers,\n                             batch_first=True)\n\n        # match parameters\n        for name in [\'weight_ih\', \'weight_hh\', \'bias_ih\', \'bias_hh\']:\n            setattr(torch_lstm, f\'{name}_l0\',\n                    getattr(decoder._cell._cell, name))\n        torch_lstm.flatten_parameters()\n\n        output_layer = decoder._output_layer\n        input_lengths = torch.tensor([self._max_time] * self._batch_size)\n        inputs = torch.randint(\n            self._vocab_size, size=(self._batch_size, self._max_time))\n\n        # decoder outputs\n        helper_train = decoder.create_helper()\n        outputs, final_state, sequence_lengths = decoder(\n            inputs=inputs,\n            sequence_length=input_lengths,\n            helper=helper_train)\n\n        # torch LSTM outputs\n        lstm_inputs = F.embedding(inputs, self._embedder.embedding)\n        torch_outputs, torch_states = torch_lstm(lstm_inputs)\n        torch_outputs = output_layer(torch_outputs)\n        torch_sample_id = torch.argmax(torch_outputs, dim=-1)\n\n        self.assertEqual(final_state[0].shape,\n                         (self._batch_size, hidden_size))\n\n        self._assert_tensor_equal(outputs.logits, torch_outputs)\n        self._assert_tensor_equal(outputs.sample_id, torch_sample_id)\n        self._assert_tensor_equal(final_state[0], torch_states[0].squeeze(0))\n        self._assert_tensor_equal(final_state[1], torch_states[1].squeeze(0))\n        self._assert_tensor_equal(sequence_lengths, input_lengths)\n\n    def test_decode_infer(self):\n        r""""""Tests decoding in inference mode.""""""\n        decoder = BasicRNNDecoder(\n            token_embedder=self._embedder, input_size=self._emb_dim,\n            vocab_size=self._vocab_size, hparams=self._hparams)\n\n        decoder.eval()\n        start_tokens = torch.tensor([self._vocab_size - 2] * self._batch_size)\n\n        helpers = []\n        for strategy in [\'infer_greedy\', \'infer_sample\']:\n            helper = decoder.create_helper(\n                decoding_strategy=strategy,\n                start_tokens=start_tokens,\n                end_token=self._vocab_size - 1)\n            helpers.append(helper)\n        for klass in [\'TopKSampleEmbeddingHelper\', \'SoftmaxEmbeddingHelper\',\n                      \'GumbelSoftmaxEmbeddingHelper\']:\n            helper = get_helper(\n                klass, start_tokens=start_tokens,\n                end_token=self._vocab_size - 1,\n                top_k=self._vocab_size // 2, tau=2.0,\n                straight_through=True)\n            helpers.append(helper)\n\n        for helper in helpers:\n            max_length = 100\n            outputs, final_state, sequence_lengths = decoder(\n                helper=helper, max_decoding_length=max_length)\n            self.assertLessEqual(max(sequence_lengths), max_length)\n            self._test_outputs(decoder, outputs, final_state, sequence_lengths,\n                               test_mode=True)\n\n\nclass AttentionRNNDecoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.decoders.rnn_decoders.AttentionRNNDecoder`.\n    """"""\n\n    def setUp(self):\n        self._vocab_size = 10\n        self._max_time = 16\n        self._batch_size = 8\n        self._emb_dim = 20\n        self._attention_dim = 256\n        self._inputs = torch.randint(\n            self._vocab_size, size=(self._batch_size, self._max_time))\n        embedding = torch.rand(\n            self._vocab_size, self._emb_dim, dtype=torch.float)\n        self._embedder = WordEmbedder(init_value=embedding)\n        self._encoder_output = torch.rand(\n            self._batch_size, self._max_time, 64)\n\n        self._test_hparams = {}  # (cell_type, is_multi) -> hparams\n        for cell_type in [""RNNCell"", ""LSTMCell"", ""GRUCell""]:\n            hparams = {\n                ""rnn_cell"": {\n                    \'type\': cell_type,\n                    \'kwargs\': {\n                        \'num_units\': 256,\n                    },\n                },\n                ""attention"": {\n                    ""kwargs"": {\n                        ""num_units"": self._attention_dim\n                    },\n                }\n            }\n            self._test_hparams[(cell_type, False)] = HParams(\n                hparams, AttentionRNNDecoder.default_hparams())\n\n        hparams = {\n            ""rnn_cell"": {\n                \'type\': \'LSTMCell\',\n                \'kwargs\': {\n                    \'num_units\': 256,\n                },\n                \'num_layers\': 3,\n            },\n            ""attention"": {\n                ""kwargs"": {\n                    ""num_units"": self._attention_dim\n                },\n            }\n        }\n        self._test_hparams[(""LSTMCell"", True)] = HParams(\n            hparams, AttentionRNNDecoder.default_hparams())\n\n    def _test_outputs(self, decoder, outputs, final_state, sequence_lengths,\n                      test_mode=False):\n        hidden_size = decoder.hparams.rnn_cell.kwargs.num_units\n        cell_type = decoder.hparams.rnn_cell.type\n        is_multi = decoder.hparams.rnn_cell.num_layers > 1\n\n        self.assertIsInstance(outputs, AttentionRNNDecoderOutput)\n        max_time = (self._max_time if not test_mode\n                    else max(sequence_lengths).item())\n        self.assertEqual(\n            outputs.logits.shape,\n            (self._batch_size, max_time, self._vocab_size))\n        if not test_mode:\n            np.testing.assert_array_equal(\n                sequence_lengths, [max_time] * self._batch_size)\n\n        map_structure(\n            lambda t: self.assertEqual(\n                t.size(), (self._batch_size, hidden_size)),\n            final_state.cell_state)\n        state = final_state.cell_state\n        if is_multi:\n            self.assertIsInstance(state, list)\n            state = state[0]\n        if cell_type == ""LSTMCell"":\n            self.assertIsInstance(state, tuple)\n            state = state[0]\n        self.assertIsInstance(state, torch.Tensor)\n\n    def test_decode_infer(self):\n        r""""""Tests decoding in inference mode.\n        """"""\n        seq_length = np.random.randint(\n            self._max_time, size=[self._batch_size]) + 1\n        encoder_values_length = torch.tensor(seq_length)\n\n        for (cell_type, is_multi), hparams in self._test_hparams.items():\n            decoder = AttentionRNNDecoder(\n                encoder_output_size=64,\n                token_embedder=self._embedder,\n                vocab_size=self._vocab_size,\n                input_size=self._emb_dim,\n                hparams=hparams)\n\n            decoder.eval()\n\n            helper_infer = decoder.create_helper(\n                start_tokens=torch.tensor([1] * self._batch_size), end_token=2)\n\n            outputs, final_state, sequence_lengths = decoder(\n                memory=self._encoder_output,\n                memory_sequence_length=encoder_values_length,\n                helper=helper_infer)\n\n            self._test_outputs(decoder, outputs, final_state, sequence_lengths,\n                               test_mode=True)\n\n    def test_decode_train(self):\n        r""""""Tests decoding in training mode.\n        """"""\n        seq_length = np.random.randint(\n            self._max_time, size=[self._batch_size]) + 1\n        encoder_values_length = torch.tensor(seq_length)\n\n        for (cell_type, is_multi), hparams in self._test_hparams.items():\n            decoder = AttentionRNNDecoder(\n                encoder_output_size=64,\n                token_embedder=self._embedder,\n                vocab_size=self._vocab_size,\n                input_size=self._emb_dim,\n                hparams=hparams)\n\n            sequence_length = torch.tensor([self._max_time] * self._batch_size)\n\n            helper_train = decoder.create_helper()\n            outputs, final_state, sequence_lengths = decoder(\n                memory=self._encoder_output,\n                memory_sequence_length=encoder_values_length,\n                helper=helper_train,\n                inputs=self._inputs,\n                sequence_length=sequence_length)\n\n            self._test_outputs(decoder, outputs, final_state, sequence_lengths)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/decoders/transformer_decoders_test.py,22,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for Transformer decoder.\n""""""\nimport unittest\n\nimport torch\nimport torch.nn.functional as F\n\nfrom texar.torch.core.layers import identity\nfrom texar.torch.modules.decoders import decoder_helpers\nfrom texar.torch.modules.decoders.transformer_decoders import (\n    TransformerDecoder, TransformerDecoderOutput)\n\n\nclass TransformerDecoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.TransformerDecoder`\n    """"""\n\n    def setUp(self):\n        self._vocab_size = 101\n        self._batch_size = 3\n        self._max_time = 5\n        self._emb_dim = 512\n        self._max_decode_len = 7\n\n        self._inputs = torch.randint(\n            self._vocab_size, size=(self._batch_size, self._max_time))\n\n        self._memory = torch.rand(\n            self._batch_size, self._max_time, self._emb_dim, dtype=torch.float)\n        self._memory_sequence_length = torch.randint(\n            self._max_time, (self._batch_size,), dtype=torch.long)\n\n        self._embedding = torch.rand(\n            self._vocab_size, self._emb_dim, dtype=torch.float)\n        self._pos_embedding = torch.rand(\n            self._max_decode_len, self._emb_dim, dtype=torch.float)\n\n        def _embedding_fn(x, y):\n            x_emb = F.embedding(x, self._embedding)\n            y_emb = F.embedding(y, self._pos_embedding)\n            return x_emb * self._emb_dim ** 0.5 + y_emb\n\n        self._embedding_fn = _embedding_fn\n\n        self._output_layer = torch.rand(\n            self._vocab_size, self._emb_dim, dtype=torch.float)\n\n        self._start_tokens = torch.full(\n            (self._batch_size,), 1, dtype=torch.long)\n        self._end_token = 2\n        self.max_decoding_length = self._max_time\n\n        _context = [[3, 4, 5, 2, 0], [4, 3, 5, 7, 2]]\n        _context_length = [4, 5]\n        self._context = torch.tensor(_context)\n        self._context_length = torch.tensor(_context_length)\n\n    def test_output_layer(self):\n        decoder = TransformerDecoder(\n            token_pos_embedder=self._embedding_fn,\n            vocab_size=self._vocab_size, output_layer=None)\n        self.assertIsInstance(decoder, TransformerDecoder)\n\n        decoder = TransformerDecoder(\n            token_pos_embedder=self._embedding_fn,\n            output_layer=identity)\n        self.assertIsInstance(decoder, TransformerDecoder)\n\n        tensor = torch.rand(\n            self._vocab_size, self._emb_dim, dtype=torch.float)\n        decoder = TransformerDecoder(\n            token_pos_embedder=self._embedding_fn,\n            output_layer=tensor)\n        self.assertIsInstance(decoder, TransformerDecoder)\n        self.assertEqual(decoder.vocab_size, self._vocab_size)\n\n    def test_decode_train(self):\n        """"""Tests train_greedy\n        """"""\n        decoder = TransformerDecoder(\n            token_pos_embedder=self._embedding_fn,\n            vocab_size=self._vocab_size, output_layer=self._output_layer)\n        decoder.train()\n        # 6 blocks\n        # -self multihead_attention: 4 dense without bias + 2 layer norm vars\n        # -encdec multihead_attention: 4 dense without bias + 2 layer norm vars\n        # -poswise_network: Dense with bias, Dense with bias + 2 layer norm vars\n        # 2 layer norm vars\n        outputs = decoder(memory=self._memory,\n                          memory_sequence_length=self._memory_sequence_length,\n                          memory_attention_bias=None,\n                          inputs=self._inputs,\n                          decoding_strategy=\'train_greedy\')\n        # print(decoder)\n        # for name, _ in decoder.named_parameters():\n        #     print(name)\n        self.assertEqual(len(decoder.trainable_variables), 110)\n        self.assertIsInstance(outputs, TransformerDecoderOutput)\n\n    def test_decode_infer_greedy(self):\n        """"""Tests train_greedy\n        """"""\n        decoder = TransformerDecoder(\n            token_pos_embedder=self._embedding_fn,\n            vocab_size=self._vocab_size, output_layer=self._output_layer)\n        decoder.eval()\n        helper = decoder_helpers.GreedyEmbeddingHelper(\n            self._start_tokens, self._end_token)\n\n        outputs, length = decoder(\n            memory=self._memory,\n            memory_sequence_length=self._memory_sequence_length,\n            memory_attention_bias=None,\n            inputs=None,\n            helper=helper,\n            max_decoding_length=self._max_decode_len)\n\n        self.assertIsInstance(outputs, TransformerDecoderOutput)\n\n    def test_infer_greedy_with_context_without_memory(self):\n        """"""Tests train_greedy with context\n        """"""\n        decoder = TransformerDecoder(\n            token_pos_embedder=self._embedding_fn,\n            vocab_size=self._vocab_size,\n            output_layer=self._output_layer)\n        decoder.eval()\n        outputs, length = decoder(\n            memory=None,\n            memory_sequence_length=None,\n            memory_attention_bias=None,\n            inputs=None,\n            decoding_strategy=\'infer_greedy\',\n            context=self._context,\n            context_sequence_length=self._context_length,\n            end_token=self._end_token,\n            max_decoding_length=self._max_decode_len)\n\n        self.assertIsInstance(outputs, TransformerDecoderOutput)\n\n    def test_decode_infer_sample(self):\n        """"""Tests infer_sample\n        """"""\n        decoder = TransformerDecoder(\n            token_pos_embedder=self._embedding_fn,\n            vocab_size=self._vocab_size,\n            output_layer=self._output_layer)\n        decoder.eval()\n        helper = decoder_helpers.SampleEmbeddingHelper(\n            self._start_tokens, self._end_token)\n\n        outputs, length = decoder(\n            memory=self._memory,\n            memory_sequence_length=self._memory_sequence_length,\n            memory_attention_bias=None,\n            inputs=None,\n            helper=helper,\n            max_decoding_length=self._max_decode_len)\n\n        self.assertIsInstance(outputs, TransformerDecoderOutput)\n\n    def test_beam_search(self):\n        """"""Tests beam_search\n        """"""\n        decoder = TransformerDecoder(\n            token_pos_embedder=self._embedding_fn,\n            vocab_size=self._vocab_size,\n            output_layer=self._output_layer)\n        decoder.eval()\n        beam_width = 5\n        outputs = decoder(\n            memory=self._memory,\n            memory_sequence_length=self._memory_sequence_length,\n            memory_attention_bias=None,\n            inputs=None,\n            beam_width=beam_width,\n            start_tokens=self._start_tokens,\n            end_token=self._end_token,\n            max_decoding_length=self._max_decode_len)\n\n        self.assertEqual(outputs[\'log_prob\'].size(),\n                         (self._batch_size, beam_width))\n        self.assertEqual(outputs[\'sample_id\'].size(0), self._batch_size)\n        self.assertLessEqual(outputs[\'sample_id\'].size(2), self._max_decode_len)\n        self.assertEqual(outputs[\'sample_id\'].size(2), beam_width)\n\n    def test_greedy_embedding_helper(self):\n        """"""Tests with tf.contrib.seq2seq.GreedyEmbeddingHelper\n        """"""\n        decoder = TransformerDecoder(\n            token_pos_embedder=self._embedding_fn,\n            vocab_size=self._vocab_size,\n            output_layer=self._output_layer)\n        decoder.eval()\n        helper = decoder_helpers.GreedyEmbeddingHelper(\n            self._start_tokens, self._end_token)\n        outputs, length = decoder(\n            memory=self._memory,\n            memory_sequence_length=self._memory_sequence_length,\n            memory_attention_bias=None,\n            helper=helper,\n            max_decoding_length=self._max_decode_len)\n\n        self.assertIsInstance(outputs, TransformerDecoderOutput)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/decoders/xlnet_decoder_test.py,6,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for XLNet decoder.\n""""""\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.decoders.xlnet_decoder import *\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass XLNetDecoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.XLNetDecoder`\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.start_tokens = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_hparams(self):\n        r""""""Tests the priority of the decoder arch parameters.\n        """"""\n        # case 1: set ""pretrained_mode_name"" by constructor argument\n        hparams = {\n            ""pretrained_model_name"": ""xlnet-large-cased"",\n        }\n        decoder = XLNetDecoder(pretrained_model_name=""xlnet-base-cased"",\n                               hparams=hparams)\n        self.assertEqual(decoder.hparams.num_layers, 12)\n\n        _, _ = decoder(start_tokens=self.start_tokens,\n                       end_token=1, max_decoding_length=self.max_length)\n\n        # case 2: set ""pretrained_mode_name"" by hparams\n        hparams = {\n            ""pretrained_model_name"": ""xlnet-large-cased"",\n            ""num_layers"": 6,\n        }\n        decoder = XLNetDecoder(hparams=hparams)\n        self.assertEqual(decoder.hparams.num_layers, 24)\n\n        _, _ = decoder(start_tokens=self.start_tokens,\n                       end_token=1, max_decoding_length=self.max_length)\n\n        # case 3: set to None in both hparams and constructor argument\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_layers"": 6,\n        }\n        decoder = XLNetDecoder(hparams=hparams)\n        self.assertEqual(decoder.hparams.num_layers, 6)\n\n        _, _ = decoder(start_tokens=self.start_tokens,\n                       end_token=1, max_decoding_length=self.max_length)\n\n        # case 4: using default hparams\n        decoder = XLNetDecoder()\n        self.assertEqual(decoder.hparams.num_layers, 12)\n\n        _, _ = decoder(start_tokens=self.start_tokens,\n                       end_token=1, max_decoding_length=self.max_length)\n\n    @pretrained_test\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        # case 1\n        decoder = XLNetDecoder()\n        self.assertEqual(len(decoder.trainable_variables), 182 + 1)\n\n        _, _ = decoder(start_tokens=self.start_tokens,\n                       end_token=1, max_decoding_length=self.max_length)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": ""xlnet-large-cased"",\n        }\n        decoder = XLNetDecoder(hparams=hparams)\n        self.assertEqual(len(decoder.trainable_variables), 362 + 1)\n\n        _, _ = decoder(start_tokens=self.start_tokens,\n                       end_token=1, max_decoding_length=self.max_length)\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_layers"": 6\n        }\n        decoder = XLNetDecoder(hparams=hparams)\n        self.assertEqual(len(decoder.trainable_variables), 92 + 1)\n\n        _, _ = decoder(start_tokens=self.start_tokens,\n                       end_token=1, max_decoding_length=self.max_length)\n\n    @pretrained_test\n    def test_decode_infer_sample(self):\n        r""""""Tests train_greedy.""""""\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        decoder = XLNetDecoder(hparams=hparams)\n        decoder.train()\n\n        inputs = torch.randint(32000, (self.batch_size, self.max_length))\n        outputs, _ = decoder(\n            inputs, max_decoding_length=self.max_length, end_token=2)\n\n        self.assertIsInstance(outputs, XLNetDecoderOutput)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/embedders/embedder_utils_test.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for embedder utils.\n""""""\n\nimport unittest\n\nfrom texar.torch.modules.embedders import embedder_utils\n\n\nclass GetEmbeddingTest(unittest.TestCase):\n    """"""Tests embedding creator.\n    """"""\n\n    def test_get_embedding(self):\n        """"""Tests :func:`~texar.torch.modules.embedder.embedder_utils.get_embedding`.\n        """"""\n        vocab_size = 100\n        emb = embedder_utils.get_embedding(num_embeds=vocab_size)\n        self.assertEqual(emb.size(0), vocab_size)\n        self.assertEqual(emb.size(1),\n                         embedder_utils.default_embedding_hparams()[""dim""])\n\n        hparams = {\n            ""initializer"": {\n                ""type"": ""torch.nn.init.uniform_"",\n                ""kwargs"": {\'a\': -0.1, \'b\': 0.1}\n            }\n        }\n        emb = embedder_utils.get_embedding(\n            hparams=hparams, num_embeds=vocab_size, )\n        self.assertEqual(emb.size(0), vocab_size)\n        self.assertEqual(emb.size(1),\n                         embedder_utils.default_embedding_hparams()[""dim""])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/embedders/embedders_test.py,18,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for embedders.\n""""""\n\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.modules.embedders.embedders import WordEmbedder\nfrom texar.torch.modules.embedders.position_embedders import (\n    PositionEmbedder, SinusoidsPositionEmbedder)\n\n\nclass EmbedderTest(unittest.TestCase):\n    """"""Tests parameterized embedder.\n    """"""\n\n    def _test_word_embedder(self, hparams):\n        """"""Tests :class:`texar.torch.modules.WordEmbedder`.\n        """"""\n        embedder = WordEmbedder(\n            vocab_size=100, hparams=hparams)\n\n        inputs = torch.randint(embedder.vocab_size, (64, 16), dtype=torch.long)\n        outputs = embedder(inputs)\n\n        inputs_soft = torch.randn(\n            (64, 16, embedder.vocab_size), dtype=torch.float32)\n        outputs_soft = embedder(soft_ids=inputs_soft)\n\n        if isinstance(embedder.dim, (list, tuple)):\n            emb_dim = tuple(embedder.dim)\n        else:\n            emb_dim = (embedder.dim,)\n\n        if isinstance(hparams[""dim""], (list, tuple)):\n            hparams_dim = tuple(hparams[""dim""])\n        else:\n            hparams_dim = (hparams[""dim""],)\n\n        self.assertEqual(outputs.size(), (64, 16) + emb_dim)\n        self.assertEqual(outputs.size(-1), embedder.output_size)\n        self.assertEqual(outputs_soft.size(), (64, 16) + emb_dim)\n        self.assertEqual(outputs_soft.size(-1), embedder.output_size)\n        self.assertEqual(emb_dim, hparams_dim)\n        self.assertEqual(embedder.vocab_size, 100)\n        self.assertEqual(outputs.size(), (64, 16) + emb_dim)\n        self.assertEqual(outputs_soft.size(), (64, 16) + emb_dim)\n\n    def _test_position_embedder(self, hparams):\n        """"""Tests :class:`texar.torch.modules.PositionEmbedder`.\n        """"""\n        pos_size = 100\n        embedder = PositionEmbedder(\n            position_size=pos_size, hparams=hparams)\n        inputs = torch.randint(embedder.num_embeds, (64, 16), dtype=torch.long)\n        outputs = embedder(inputs)\n\n        if isinstance(embedder.dim, (list, tuple)):\n            emb_dim = tuple(embedder.dim)\n        else:\n            emb_dim = (embedder.dim,)\n\n        if isinstance(hparams[""dim""], (list, tuple)):\n            hparams_dim = tuple(hparams[""dim""])\n        else:\n            hparams_dim = (hparams[""dim""],)\n\n        self.assertEqual(outputs.size(), (64, 16) + emb_dim)\n        self.assertEqual(outputs.size(-1), embedder.output_size)\n        self.assertEqual(emb_dim, hparams_dim)\n        self.assertEqual(embedder.position_size, 100)\n        seq_length = torch.empty(64).uniform_(0, pos_size).long()\n        _ = embedder(sequence_length=seq_length)\n\n    def test_sinusoids_position_embedder(self):\n        """"""Tests :class:`texar.torch.modules.SinusoidsPositionEmbedder`.\n        """"""\n        position_size = 64\n        input_size = (23, 18)\n        hparams = {\'dim\': 513}  # use odd dimension to ensure padding correct\n        embedder = SinusoidsPositionEmbedder(position_size, hparams=hparams)\n        inputs = torch.randint(position_size - 1, input_size)\n        outputs = embedder(inputs)\n        self.assertEqual(outputs.size(), input_size + (hparams[\'dim\'],))\n        self.assertEqual(outputs.size(-1), embedder.output_size)\n        sequence_length = torch.randint(high=position_size + 1, size=(23,))\n        outputs_ = embedder(sequence_length=sequence_length)\n        self.assertEqual(outputs_.size(),\n                         (23, max(sequence_length).item()) + (hparams[\'dim\'],))\n        self.assertEqual(outputs_.size(-1), embedder.output_size)\n\n        embedder_no_cache = SinusoidsPositionEmbedder(\n            None, hparams={**hparams, \'cache_embeddings\': False})\n        wide_inputs = torch.randint(\n            -position_size, position_size * 2, input_size)\n        wide_outputs = embedder_no_cache(wide_inputs)\n        self.assertEqual(wide_outputs.size(), input_size + (hparams[\'dim\'],))\n        no_cache_outputs = embedder_no_cache(inputs)\n        np.testing.assert_array_equal(outputs, no_cache_outputs)\n\n    def test_embedder(self):\n        """"""Tests various embedders.\n        """"""\n        test_dims = [\n            14,\n            [14],\n            [14, 10],\n        ]\n        test_hparams = [\n            # no dropout\n            {""dropout_rate"": 0},\n            # dropout with default strategy\n            {""dropout_rate"": 0.3},\n            # dropout with different strategies\n            {""dropout_rate"": 0.3, ""dropout_strategy"": ""item""},\n            {""dropout_rate"": 0.3, ""dropout_strategy"": ""item_type""},\n        ]\n\n        for base_hparams in test_hparams:\n            for dim in test_dims:\n                hparams = base_hparams.copy()\n                hparams[\'dim\'] = dim\n                self._test_word_embedder(hparams)\n                self._test_position_embedder(hparams)\n\n    def test_embedder_multi_calls(self):\n        """"""Tests embedders called by multiple times.\n        """"""\n        hparams = {""dim"": 26, ""dropout_rate"": 0.3,\n                   ""dropout_strategy"": ""item""}\n        embedder = WordEmbedder(\n            vocab_size=100, hparams=hparams)\n        inputs = torch.randint(embedder.vocab_size, (64, 16), dtype=torch.long)\n        outputs = embedder(inputs)\n\n        if isinstance(embedder.dim, (list, tuple)):\n            emb_dim = tuple(embedder.dim)\n        else:\n            emb_dim = (embedder.dim,)\n        self.assertEqual(outputs.size(), (64, 16) + emb_dim)\n\n        # Call with inputs in a different shape\n        inputs = torch.randint(\n            embedder.vocab_size, (64, 10, 20), dtype=torch.long)\n        outputs = embedder(inputs)\n\n        self.assertEqual(outputs.size(), (64, 10, 20) + emb_dim)\n\n    def test_word_embedder_soft_ids(self):\n        """"""Tests the correctness of using soft ids.\n        """"""\n        init_value = np.expand_dims(np.arange(5), 1)\n        embedder = WordEmbedder(init_value=init_value)\n\n        ids = torch.tensor([3])\n        soft_ids = torch.tensor([0, 0, 0, 1, 0], dtype=torch.float)\n\n        outputs = embedder(ids=ids)\n        soft_outputs = embedder(soft_ids=soft_ids)\n        self.assertEqual(outputs, soft_outputs)\n\n    def test_word_embedder_trainable(self):\n        """"""Tests freezing the embedding parameters.\n        """"""\n        init_value = np.expand_dims(np.arange(5), 1)\n\n        embedder = WordEmbedder(init_value=init_value,\n                                hparams={""trainable"": False})\n        self.assertEqual(len(embedder.trainable_variables), 0)\n\n        embedder = WordEmbedder(init_value=init_value)\n        self.assertEqual(len(embedder.trainable_variables), 1)\n\n    def test_position_embedder_trainable(self):\n        """"""Tests freezing the embedding parameters.\n        """"""\n        pos_size = 100\n\n        embedder = PositionEmbedder(\n            position_size=pos_size, hparams={""trainable"": False})\n        self.assertEqual(len(embedder.trainable_variables), 0)\n\n        embedder = PositionEmbedder(position_size=pos_size)\n        self.assertEqual(len(embedder.trainable_variables), 1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/encoder_decoders/t5_encoder_decoder_test.py,11,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for T5 encoder-decoder\n""""""\n\nimport unittest\n\nimport numpy\nimport torch\n\nfrom texar.torch.modules.encoder_decoders import T5EncoderDecoder\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass T5EncoderDecoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.T5EncoderDecoder` class.\n\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.inputs = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n        for pretrained_model_name in T5EncoderDecoder.available_checkpoints():\n            if pretrained_model_name in [\'T5-11B\', \'T5-3B\']:\n                continue  # Too large to fit\n\n            model = T5EncoderDecoder(\n                pretrained_model_name=pretrained_model_name)\n            _, _ = model(self.inputs)\n\n    @pretrained_test\n    def test_hparams(self):\n        r""""""Tests the priority of the architecture.\n        """"""\n        # case 1: set ""pretrained_mode_name"" by constructor argument\n        hparams = {\n            ""pretrained_model_name"": ""T5-Small"",\n        }\n        t5 = T5EncoderDecoder(pretrained_model_name=""T5-Base"",\n                                 hparams=hparams)\n        self.assertEqual(t5.hparams.encoder.num_blocks, 12)\n        _, _ = t5(self.inputs)\n\n        # case 2: set ""pretrained_mode_name"" by hparams\n        hparams = {\n            ""pretrained_model_name"": ""T5-Small"",\n            ""encoder"": {\n                ""num_blocks"": 16,\n            }\n        }\n        t5 = T5EncoderDecoder(hparams=hparams)\n        self.assertEqual(t5.hparams.encoder.num_blocks, 6)\n        _, _ = t5(self.inputs)\n\n        # case 3: set to None in both hparams and constructor argument\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""encoder"": {\n                ""num_blocks"": 6,\n            },\n        }\n        t5 = T5EncoderDecoder(hparams=hparams)\n        self.assertEqual(t5.hparams.encoder.num_blocks, 6)\n        _, _ = t5(self.inputs)\n\n        # case 4: using default hparams\n        encoder = T5EncoderDecoder()\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 6)\n        _, _ = encoder(self.inputs)\n\n    @pretrained_test\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        # case 1: t5 small\n        encoder = T5EncoderDecoder(pretrained_model_name=""T5-Small"")\n        self.assertEqual(len(encoder.trainable_variables),\n                         13 * 6 + 3 + 8 * 6 + 3)\n        _, _ = encoder(self.inputs)\n\n        # case 2: bert large\n        hparams = {\n            ""pretrained_model_name"": ""T5-Base""\n        }\n        encoder = T5EncoderDecoder(hparams=hparams)\n        self.assertEqual(len(encoder.trainable_variables),\n                         13 * 12 + 3 + 8 * 12 + 3)\n        _, _ = encoder(self.inputs)\n\n    @pretrained_test\n    def test_t5_eval(self):\n        r""""""Tests pre-trained model and check it generates\n        same results everytime.\n        """"""\n        hparams = {\n            ""pretrained_model_name"": \'T5-Small\',\n        }\n        model = T5EncoderDecoder(hparams=hparams)\n        model.eval()\n\n        self.inputs = torch.from_numpy(\n            numpy.asarray([[8774, 6, 82, 1782, 19, 5295]]))\n        self.max_length = 6\n\n        encoder_output, decoder_output = model(self.inputs)\n\n        outputs_dim = model.hparams.encoder.dim\n        self.assertEqual(\n            decoder_output[0].shape,\n            torch.Size([self.inputs.size()[0], self.max_length, outputs_dim]))\n\n        self.assertEqual(\n            encoder_output.shape,\n            torch.Size([self.inputs.size()[0], self.max_length, outputs_dim]))\n\n        # Check if these value are same consistently. If not, there is something\n        # wrong with the pre-trained model.\n        self.assertEqual(\n            encoder_output.data[0][3][345].tolist(),\n            -0.16204041242599487\n        )\n        self.assertLess(  # leave some margin for minor stochastic differences\n            decoder_output[0].data[0][0][234].tolist() + 0.325570285320282,\n            0.000001\n        )\n\n    def test_t5(self):\n        r""""""t5 test.\n        """"""\n        hparams = {\n           ""pretrained_model_name"": None,\n        }\n\n        t5 = T5EncoderDecoder(hparams=hparams)\n\n        inputs = torch.randint(32128, (self.batch_size, self.max_length))\n\n        encoder_output, decoder_output = t5(inputs)\n\n        outputs_dim = t5.output_size\n\n        self.assertEqual(\n            decoder_output[0].shape,\n            torch.Size([self.batch_size, self.max_length, outputs_dim]))\n\n        self.assertEqual(\n            encoder_output.shape,\n            torch.Size([self.batch_size, self.max_length, outputs_dim]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/encoders/bert_encoder_test.py,13,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for BERT encoders.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.encoders.bert_encoder import BERTEncoder\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass BERTEncoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.BERTEncoder` class.\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.inputs = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n        for pretrained_model_name in BERTEncoder.available_checkpoints():\n            encoder = BERTEncoder(pretrained_model_name=pretrained_model_name)\n            _, _ = encoder(self.inputs)\n\n    @pretrained_test\n    def test_hparams(self):\n        r""""""Tests the priority of the encoder arch parameter.\n        """"""\n        # case 1: set ""pretrained_mode_name"" by constructor argument\n        hparams = {\n            ""pretrained_model_name"": ""bert-large-uncased"",\n        }\n        encoder = BERTEncoder(pretrained_model_name=""bert-base-uncased"",\n                              hparams=hparams)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n        _, _ = encoder(self.inputs)\n\n        # case 2: set ""pretrained_mode_name"" by hparams\n        hparams = {\n            ""pretrained_model_name"": ""bert-large-uncased"",\n            ""encoder"": {\n                ""num_blocks"": 6,\n            }\n        }\n        encoder = BERTEncoder(hparams=hparams)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 24)\n        _, _ = encoder(self.inputs)\n\n        # case 3: set to None in both hparams and constructor argument\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""encoder"": {\n                ""num_blocks"": 6,\n            },\n        }\n        encoder = BERTEncoder(hparams=hparams)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 6)\n        _, _ = encoder(self.inputs)\n\n        # case 4: using default hparams\n        encoder = BERTEncoder()\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n        _, _ = encoder(self.inputs)\n\n    @pretrained_test\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        # case 1: bert base\n        encoder = BERTEncoder()\n        self.assertEqual(len(encoder.trainable_variables), 3 + 2 + 12 * 16 + 2)\n        _, _ = encoder(self.inputs)\n\n        # case 2: bert large\n        hparams = {\n            ""pretrained_model_name"": ""bert-large-uncased""\n        }\n        encoder = BERTEncoder(hparams=hparams)\n        self.assertEqual(len(encoder.trainable_variables), 3 + 2 + 24 * 16 + 2)\n        _, _ = encoder(self.inputs)\n\n        # case 3: self-designed bert\n        hparams = {\n            ""encoder"": {\n                ""num_blocks"": 6,\n            },\n            ""pretrained_model_name"": None,\n        }\n        encoder = BERTEncoder(hparams=hparams)\n        self.assertEqual(len(encoder.trainable_variables), 3 + 2 + 6 * 16 + 2)\n        _, _ = encoder(self.inputs)\n\n    def test_encode(self):\n        r""""""Tests encoding.\n        """"""\n        # case 1: bert base\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        encoder = BERTEncoder(hparams=hparams)\n\n        inputs = torch.randint(30521, (self.batch_size, self.max_length))\n        outputs, pooled_output = encoder(inputs)\n\n        outputs_dim = encoder.hparams.encoder.dim\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, outputs_dim]))\n        self.assertEqual(\n            pooled_output.shape,\n            torch.Size([self.batch_size, encoder.output_size]))\n\n        # case 2: self-designed bert\n        hparams = {\n            \'pretrained_model_name\': None,\n            \'embed\': {\n                \'dim\': 96,\n            },\n            \'segment_embed\': {\n                \'dim\': 96,\n            },\n            \'position_embed\': {\n                \'dim\': 96,\n            },\n\n            \'encoder\': {\n                \'dim\': 96,\n                \'multihead_attention\': {\n                    \'num_units\': 96,\n                    \'output_dim\': 96,\n                },\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 96,\n                                \'out_features\': 96 * 4,\n                                \'bias\': True,\n                            },\n                            \'type\': \'Linear\',\n                        },\n                        {""type"": ""BertGELU""},\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 96 * 4,\n                                \'out_features\': 96,\n                                \'bias\': True,\n                            },\n                            \'type\': \'Linear\',\n                        }\n                    ]\n                },\n            },\n            \'hidden_size\': 96,\n        }\n        encoder = BERTEncoder(hparams=hparams)\n\n        outputs, pooled_output = encoder(inputs)\n\n        outputs_dim = encoder.hparams.encoder.dim\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, outputs_dim]))\n        self.assertEqual(\n            pooled_output.shape,\n            torch.Size([self.batch_size, encoder.output_size]))\n\n    def test_soft_ids(self):\n        r""""""Tests soft ids.\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        encoder = BERTEncoder(hparams=hparams)\n\n        inputs = torch.rand(self.batch_size, self.max_length, 30522)\n        outputs, pooled_output = encoder(inputs)\n\n        outputs_dim = encoder.hparams.encoder.dim\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, outputs_dim]))\n        self.assertEqual(\n            pooled_output.shape,\n            torch.Size([self.batch_size, encoder.output_size]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/encoders/conv_encoders_test.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for conv encoders.\n""""""\nimport unittest\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.core.layers import MergeLayer\nfrom texar.torch.modules.encoders.conv_encoders import Conv1DEncoder\n\n\nclass Conv1DEncoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.Conv1DEncoder` class.\n    """"""\n\n    def test_encode(self):\n        r""""""Tests encode.\n        """"""\n        inputs_1 = torch.ones([128, 32, 300])\n        encoder_1 = Conv1DEncoder(in_channels=inputs_1.size(1),\n                                  in_features=inputs_1.size(2))\n        self.assertEqual(len(encoder_1.layers), 4)\n        self.assertIsInstance(\n            encoder_1.layer_by_name(""MergeLayer""), MergeLayer)\n        for layer in encoder_1.layers[0].layers:\n            self.assertIsInstance(layer, nn.Sequential)\n\n        outputs_1 = encoder_1(inputs_1)\n        self.assertEqual(outputs_1.size(), torch.Size([128, 256]))\n        self.assertEqual(outputs_1.size(-1), encoder_1.output_size)\n\n        inputs_2 = torch.ones([128, 64, 300])\n        hparams = {\n            # Conv layers\n            ""num_conv_layers"": 2,\n            ""out_channels"": 128,\n            ""kernel_size"": [[3, 4, 5], 4],\n            ""other_conv_kwargs"": {""padding"": 0},\n            # Pooling layers\n            ""pooling"": ""AvgPool1d"",\n            ""pool_size"": 2,\n            ""pool_stride"": 1,\n            # Dense layers\n            ""num_dense_layers"": 3,\n            ""out_features"": [128, 128, 10],\n            ""dense_activation"": ""ReLU"",\n            ""other_dense_kwargs"": None,\n            # Dropout\n            ""dropout_conv"": [0, 1, 2],\n            ""dropout_dense"": 2\n        }\n        network_2 = Conv1DEncoder(in_channels=inputs_2.size(1),\n                                  in_features=inputs_2.size(2),\n                                  hparams=hparams)\n        # dropout-merge-dropout-conv-avgpool-dropout-flatten-\n        # (Sequential(Linear,ReLU))-(Sequential(Linear,ReLU))-dropout-linear\n        self.assertEqual(len(network_2.layers), 1 + 1 + 1 + 3 + 4 + 1)\n        self.assertIsInstance(\n            network_2.layer_by_name(""MergeLayer""), MergeLayer)\n        for layer in network_2.layers[1].layers:\n            self.assertIsInstance(layer, nn.Sequential)\n\n        outputs_2 = network_2(inputs_2)\n        self.assertEqual(outputs_2.size(), torch.Size([128, 10]))\n        self.assertEqual(outputs_2.size(-1), network_2.output_size)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/encoders/gpt2_encoder_test.py,10,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for GPT2 encoder.\n""""""\nimport unittest\n\nimport torch\nfrom texar.torch.modules.encoders.gpt2_encoder import GPT2Encoder\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass GPT2EncoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.GPT2Encoder` class.\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.inputs = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n        for pretrained_model_name in GPT2Encoder.available_checkpoints():\n            encoder = GPT2Encoder(pretrained_model_name=pretrained_model_name)\n            _ = encoder(self.inputs)\n\n    @pretrained_test\n    def test_hparams(self):\n        r""""""Tests the priority of the encoder arch parameter.\n        """"""\n        # case 1: set ""pretrained_mode_name"" by constructor argument\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-medium"",\n        }\n        encoder = GPT2Encoder(pretrained_model_name=""gpt2-small"",\n                              hparams=hparams)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n        _ = encoder(self.inputs)\n\n        # case 2: set ""pretrained_mode_name"" by hparams\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-small"",\n            ""encoder"": {\n                ""num_blocks"": 6,\n            }\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n        _ = encoder(self.inputs)\n\n        # case 3: set to None in both hparams and constructor argument\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""encoder"": {\n                ""num_blocks"": 6,\n            }\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 6)\n        _ = encoder(self.inputs)\n\n        # case 4: using default hparams\n        encoder = GPT2Encoder()\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n        _ = encoder(self.inputs)\n\n    @pretrained_test\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n\n        def get_variable_num(n_layers: int) -> int:\n            return 1 + 1 + n_layers * 16 + 2\n\n        # case 1: GPT2 small\n        encoder = GPT2Encoder()\n        self.assertEqual(len(encoder.trainable_variables), get_variable_num(12))\n        _ = encoder(self.inputs)\n\n        # case 2: GPT2 medium\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-medium"",\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n        self.assertEqual(len(encoder.trainable_variables), get_variable_num(24))\n        _ = encoder(self.inputs)\n\n        # case 2: GPT2 large\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-large"",\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n        self.assertEqual(len(encoder.trainable_variables), get_variable_num(36))\n        _ = encoder(self.inputs)\n\n        # case 3: self-designed GPT2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""encoder"": {\n                ""num_blocks"": 6,\n            },\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n        self.assertEqual(len(encoder.trainable_variables), get_variable_num(6))\n        _ = encoder(self.inputs)\n\n    def test_encode(self):\n        r""""""Tests encoding.\n        """"""\n        # case 1: GPT2 small\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n\n        inputs = torch.randint(30521, (self.batch_size, self.max_length))\n        outputs = encoder(inputs)\n\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, encoder.output_size]))\n\n        # case 2: self-designed GPT2\n        hparams = {\n            \'pretrained_model_name\': None,\n            \'embed\': {\n                \'dim\': 96,\n            },\n            \'position_embed\': {\n                \'dim\': 96,\n            },\n\n            \'encoder\': {\n                \'dim\': 96,\n                \'multihead_attention\': {\n                    \'num_units\': 96,\n                    \'output_dim\': 96,\n                },\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 96,\n                                \'out_features\': 96 * 4,\n                                \'bias\': True,\n                            },\n                            \'type\': \'Linear\',\n                        },\n                        {""type"": ""GPTGELU""},\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 96 * 4,\n                                \'out_features\': 96,\n                                \'bias\': True,\n                            },\n                            \'type\': \'Linear\',\n                        }\n                    ]\n                },\n            }\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n\n        outputs = encoder(inputs)\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, encoder.output_size]))\n\n    def test_soft_ids(self):\n        r""""""Tests soft ids.\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n\n        inputs = torch.rand(self.batch_size, self.max_length, 50257)\n        outputs = encoder(inputs)\n\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, encoder.output_size]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/encoders/rnn_encoders_test.py,17,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for RNN encoders.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.encoders.rnn_encoders import (\n    UnidirectionalRNNEncoder, BidirectionalRNNEncoder)\n\n\nclass UnidirectionalRNNEncoderTest(unittest.TestCase):\n    r""""""Tests unidirectional rnn encoder.\n    """"""\n\n    def setUp(self):\n        self._batch_size = 16\n        self._max_time = 8\n        self._input_size = 10\n\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        inputs = torch.rand(self._batch_size, self._max_time, self._input_size)\n\n        # case 1\n        encoder = UnidirectionalRNNEncoder(input_size=self._input_size)\n        output, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 4)\n        self.assertEqual(output.size()[-1], encoder.output_size)\n\n        # case 2\n        hparams = {\n            ""rnn_cell"": {\n                ""dropout"": {\n                    ""input_keep_prob"": 0.5\n                }\n            }\n        }\n        encoder = UnidirectionalRNNEncoder(input_size=self._input_size,\n                                           hparams=hparams)\n        output, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 4)\n        self.assertEqual(output.size()[-1], encoder.output_size)\n\n        # case 3\n        hparams = {""output_layer"": {\n            ""num_layers"": 2,\n            ""layer_size"": [100, 6],\n            ""activation"": ""ReLU"",\n            ""final_layer_activation"": ""Identity"",\n            ""dropout_layer_ids"": [0, 1, 2],\n            ""variational_dropout"": False}}\n\n        encoder = UnidirectionalRNNEncoder(input_size=self._input_size,\n                                           hparams=hparams)\n\n        output, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 8)\n        self.assertEqual(output.size()[-1], encoder.output_size)\n\n    def test_encode(self):\n        r""""""Tests encoding.\n        """"""\n        inputs = torch.rand(self._batch_size, self._max_time, self._input_size)\n\n        # case 1\n        encoder = UnidirectionalRNNEncoder(input_size=self._input_size)\n        outputs, state = encoder(inputs)\n\n        cell_dim = encoder.hparams.rnn_cell.kwargs.num_units\n\n        self.assertEqual(outputs.shape, torch.Size([self._batch_size,\n                                                    self._max_time,\n                                                    cell_dim]))\n        self.assertIsInstance(state, tuple)\n        self.assertEqual(state[0].shape, torch.Size([self._batch_size,\n                                                     cell_dim]))\n        self.assertEqual(state[1].shape, torch.Size([self._batch_size,\n                                                     cell_dim]))\n\n        # case 2: with output layers\n        hparams = {\n            ""output_layer"": {\n                ""num_layers"": 2,\n                ""layer_size"": [100, 6],\n                ""dropout_layer_ids"": [0, 1, 2],\n                ""variational_dropout"": False\n            }\n        }\n        encoder = UnidirectionalRNNEncoder(input_size=self._input_size,\n                                           hparams=hparams)\n        outputs, state, cell_outputs, output_size = encoder(\n            inputs, return_cell_output=True, return_output_size=True)\n\n        self.assertEqual(output_size, 6)\n        self.assertEqual(cell_outputs.shape[-1], encoder.cell.hidden_size)\n\n        out_dim = encoder.hparams.output_layer.layer_size[-1]\n        self.assertEqual(outputs.shape, torch.Size([self._batch_size,\n                                                    self._max_time,\n                                                    out_dim]))\n\n\nclass BidirectionalRNNEncoderTest(unittest.TestCase):\n    r""""""Tests bidirectional rnn encoder.\n    """"""\n\n    def setUp(self):\n        self._batch_size = 16\n        self._max_time = 8\n        self._input_size = 10\n\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        inputs = torch.rand(self._batch_size, self._max_time, self._input_size)\n\n        # case 1\n        encoder = BidirectionalRNNEncoder(input_size=self._input_size)\n        outputs, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 8)\n        self.assertEqual(\n            (outputs[0].size(-1), outputs[1].size(-1)), (encoder.output_size))\n\n        # case 2\n        hparams = {\n            ""rnn_cell_fw"": {\n                ""dropout"": {\n                    ""input_keep_prob"": 0.5\n                }\n            }\n        }\n        encoder = BidirectionalRNNEncoder(input_size=self._input_size,\n                                          hparams=hparams)\n        outputs, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 8)\n        self.assertEqual(\n            (outputs[0].size(-1), outputs[1].size(-1)), (encoder.output_size))\n\n        # case 3\n        hparams = {\n            ""output_layer_fw"": {\n                ""num_layers"": 2,\n                ""layer_size"": [100, 6],\n                ""activation"": ""ReLU"",\n                ""final_layer_activation"": ""Identity"",\n                ""dropout_layer_ids"": [0, 1, 2],\n                ""variational_dropout"": False\n            },\n            ""output_layer_bw"": {\n                ""num_layers"": 3,\n                ""other_dense_kwargs"": {""bias"": False}\n            },\n            ""output_layer_share_config"": False\n        }\n        encoder = BidirectionalRNNEncoder(input_size=self._input_size,\n                                          hparams=hparams)\n        outputs, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 8 + 3 + 4)\n        self.assertEqual(\n            (outputs[0].size(-1), outputs[1].size(-1)), (encoder.output_size))\n\n    def test_encode(self):\n        r""""""Tests encoding.\n        """"""\n        inputs = torch.rand(self._batch_size, self._max_time, self._input_size)\n\n        # case 1\n        encoder = BidirectionalRNNEncoder(input_size=self._input_size)\n        outputs, state = encoder(inputs)\n\n        cell_dim = encoder.hparams.rnn_cell_fw.kwargs.num_units\n\n        self.assertIsInstance(outputs, tuple)\n        self.assertEqual(outputs[0].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       cell_dim]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       cell_dim]))\n\n        self.assertIsInstance(state, tuple)\n        self.assertIsInstance(state[0], tuple)\n        self.assertEqual(state[0][0].shape, torch.Size([self._batch_size,\n                                                        cell_dim]))\n        self.assertEqual(state[0][0].shape, torch.Size([self._batch_size,\n                                                        cell_dim]))\n        self.assertIsInstance(state[1], tuple)\n        self.assertEqual(state[1][0].shape, torch.Size([self._batch_size,\n                                                        cell_dim]))\n        self.assertEqual(state[1][0].shape, torch.Size([self._batch_size,\n                                                        cell_dim]))\n\n        # case 2:\n        hparams = {\n            ""output_layer_fw"": {\n                ""num_layers"": 2,\n                ""layer_size"": [100, 6],\n                ""dropout_layer_ids"": [0, 1, 2],\n                ""variational_dropout"": False\n            }\n        }\n        encoder = BidirectionalRNNEncoder(input_size=self._input_size,\n                                          hparams=hparams)\n        outputs, state, cell_outputs, output_size = encoder(\n            inputs, return_cell_output=True, return_output_size=True)\n\n        self.assertEqual(output_size[0], 6)\n        self.assertEqual(output_size[1], 6)\n        self.assertEqual(cell_outputs[0].shape[-1], encoder.cell_fw.hidden_size)\n        self.assertEqual(cell_outputs[1].shape[-1], encoder.cell_bw.hidden_size)\n\n        out_dim = encoder.hparams.output_layer_fw.layer_size[-1]\n\n        self.assertEqual(outputs[0].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       out_dim]))\n        self.assertEqual(outputs[1].shape, torch.Size([self._batch_size,\n                                                       self._max_time,\n                                                       out_dim]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/encoders/roberta_encoder_test.py,13,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for RoBERTa encoders.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.encoders.roberta_encoder import RoBERTaEncoder\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass RoBERTaEncoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.RoBERTaEncoder` class.\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.inputs = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n        for pretrained_model_name in RoBERTaEncoder.available_checkpoints():\n            encoder = RoBERTaEncoder(\n                pretrained_model_name=pretrained_model_name)\n            _, _ = encoder(self.inputs)\n\n    @pretrained_test\n    def test_hparams(self):\n        r""""""Tests the priority of the encoder arch parameter.\n        """"""\n        # case 1: set ""pretrained_mode_name"" by constructor argument\n        hparams = {\n            ""pretrained_model_name"": ""roberta-large"",\n        }\n        encoder = RoBERTaEncoder(pretrained_model_name=""roberta-base"",\n                                 hparams=hparams)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n        _, _ = encoder(self.inputs)\n\n        # case 2: set ""pretrained_mode_name"" by hparams\n        hparams = {\n            ""pretrained_model_name"": ""roberta-large"",\n            ""encoder"": {\n                ""num_blocks"": 6,\n            }\n        }\n        encoder = RoBERTaEncoder(hparams=hparams)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 24)\n        _, _ = encoder(self.inputs)\n\n        # case 3: set to None in both hparams and constructor argument\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""encoder"": {\n                ""num_blocks"": 6,\n            },\n        }\n        encoder = RoBERTaEncoder(hparams=hparams)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 6)\n        _, _ = encoder(self.inputs)\n\n        # case 4: using default hparams\n        encoder = RoBERTaEncoder()\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n        _, _ = encoder(self.inputs)\n\n    @pretrained_test\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        # case 1: bert base\n        encoder = RoBERTaEncoder()\n        self.assertEqual(len(encoder.trainable_variables), 2 + 2 + 12 * 16 + 2)\n        _, _ = encoder(self.inputs)\n\n        # case 2: bert large\n        hparams = {\n            ""pretrained_model_name"": ""roberta-large""\n        }\n        encoder = RoBERTaEncoder(hparams=hparams)\n        self.assertEqual(len(encoder.trainable_variables), 2 + 2 + 24 * 16 + 2)\n        _, _ = encoder(self.inputs)\n\n        # case 3: self-designed bert\n        hparams = {\n            ""encoder"": {\n                ""num_blocks"": 6,\n            },\n            ""pretrained_model_name"": None,\n        }\n        encoder = RoBERTaEncoder(hparams=hparams)\n        self.assertEqual(len(encoder.trainable_variables), 2 + 2 + 6 * 16 + 2)\n        _, _ = encoder(self.inputs)\n\n    def test_encode(self):\n        r""""""Tests encoding.\n        """"""\n        # case 1: bert base\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        encoder = RoBERTaEncoder(hparams=hparams)\n\n        inputs = torch.randint(30521, (self.batch_size, self.max_length))\n        outputs, pooled_output = encoder(inputs)\n\n        outputs_dim = encoder.hparams.encoder.dim\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, outputs_dim]))\n        self.assertEqual(\n            pooled_output.shape,\n            torch.Size([self.batch_size, encoder.output_size]))\n\n        # case 2: self-designed bert\n        hparams = {\n            \'pretrained_model_name\': None,\n            \'embed\': {\n                \'dim\': 96,\n            },\n            \'position_embed\': {\n                \'dim\': 96,\n            },\n\n            \'encoder\': {\n                \'dim\': 96,\n                \'multihead_attention\': {\n                    \'num_units\': 96,\n                    \'output_dim\': 96,\n                },\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 96,\n                                \'out_features\': 96 * 4,\n                                \'bias\': True,\n                            },\n                            \'type\': \'Linear\',\n                        },\n                        {""type"": ""BertGELU""},\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 96 * 4,\n                                \'out_features\': 96,\n                                \'bias\': True,\n                            },\n                            \'type\': \'Linear\',\n                        }\n                    ]\n                },\n            },\n            \'hidden_size\': 96,\n        }\n        encoder = RoBERTaEncoder(hparams=hparams)\n\n        outputs, pooled_output = encoder(inputs)\n\n        outputs_dim = encoder.hparams.encoder.dim\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, outputs_dim]))\n        self.assertEqual(\n            pooled_output.shape,\n            torch.Size([self.batch_size, encoder.output_size]))\n\n    def test_soft_ids(self):\n        r""""""Tests soft ids.\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        encoder = RoBERTaEncoder(hparams=hparams)\n\n        inputs = torch.rand(self.batch_size, self.max_length, 50265)\n        outputs, pooled_output = encoder(inputs)\n\n        outputs_dim = encoder.hparams.encoder.dim\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, outputs_dim]))\n        self.assertEqual(\n            pooled_output.shape,\n            torch.Size([self.batch_size, encoder.output_size]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/encoders/transformer_encoder_test.py,12,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for Transformer encoder.\n""""""\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.encoders import TransformerEncoder\n\n\nclass TransformerEncoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.TransformerEncoder`\n    """"""\n\n    def setUp(self):\n        self._batch_size = 2\n        self._emb_dim = 512\n        self._max_time = 7\n\n    def test_trainable_variables(self):\n        r""""""Tests train_greedy\n        """"""\n        inputs = torch.rand(\n            self._batch_size, self._max_time, self._emb_dim, dtype=torch.float)\n\n        sequence_length = torch.randint(\n            self._max_time, (self._batch_size,), dtype=torch.long)\n\n        encoder = TransformerEncoder()\n\n        outputs = encoder(inputs=inputs, sequence_length=sequence_length)\n\n        # 6 blocks\n        # -self multihead_attention: 4 dense without bias + 2 layer norm vars\n        # -poswise_network: Dense with bias, Dense with bias + 2 layer norm vars\n        # 2 output layer norm vars\n        self.assertEqual(len(encoder.trainable_variables), 74)\n        self.assertEqual(outputs.size(-1), encoder.output_size)\n\n        hparams = {""use_bert_config"": True}\n        encoder = TransformerEncoder(hparams=hparams)\n\n        # 6 blocks\n        # -self multihead_attention: 4 dense without bias + 2 layer norm vars\n        # -poswise_network: Dense with bias, Dense with bias + 2 layer norm vars\n        # -output: 2 layer norm vars\n        # 2 input layer norm vars\n        outputs = encoder(inputs=inputs, sequence_length=sequence_length)\n        self.assertEqual(len(encoder.trainable_variables), 74)\n        self.assertEqual(outputs.size(-1), encoder.output_size)\n\n    def test_encode(self):\n        r""""""Tests encoding.\n        """"""\n        inputs = torch.rand(\n            self._batch_size, self._max_time, self._emb_dim, dtype=torch.float)\n\n        sequence_length = torch.randint(\n            self._max_time, (self._batch_size,), dtype=torch.long)\n\n        encoder = TransformerEncoder()\n        outputs = encoder(inputs=inputs, sequence_length=sequence_length)\n        self.assertEqual(outputs.size(),\n                         torch.Size((self._batch_size,\n                                     self._max_time,\n                                     self._emb_dim)))\n\n        hparams = {""use_bert_config"": True}\n        encoder = TransformerEncoder(hparams=hparams)\n        outputs = encoder(inputs=inputs, sequence_length=sequence_length)\n        self.assertEqual(outputs.size(),\n                         torch.Size((self._batch_size,\n                                     self._max_time,\n                                     self._emb_dim)))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/encoders/xlnet_encoder_test.py,10,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for XLNet encoder.\n""""""\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.encoders.xlnet_encoder import XLNetEncoder\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass XLNetEncoderTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.XLNetEncoder` class.\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.inputs = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n        for pretrained_model_name in XLNetEncoder.available_checkpoints():\n            encoder = XLNetEncoder(pretrained_model_name=pretrained_model_name)\n            _ = encoder(self.inputs)\n\n    @pretrained_test\n    def test_hparams(self):\n        r""""""Tests the priority of the encoder arch parameter.\n        """"""\n        # case 1: set ""pretrained_mode_name"" by constructor argument\n        hparams = {\n            ""pretrained_model_name"": ""xlnet-large-cased"",\n        }\n        encoder = XLNetEncoder(pretrained_model_name=""xlnet-base-cased"",\n                               hparams=hparams)\n        self.assertEqual(encoder.hparams.num_layers, 12)\n        _, _ = encoder(self.inputs)\n\n        # case 2: set ""pretrained_mode_name"" by hparams\n        hparams = {\n            ""pretrained_model_name"": ""xlnet-large-cased"",\n            ""num_layers"": 6,\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n        self.assertEqual(encoder.hparams.num_layers, 24)\n        _, _ = encoder(self.inputs)\n\n        # case 3: set to None in both hparams and constructor argument\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_layers"": 6,\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n        self.assertEqual(encoder.hparams.num_layers, 6)\n        _, _ = encoder(self.inputs)\n\n        # case 4: using default hparams\n        encoder = XLNetEncoder()\n        self.assertEqual(encoder.hparams.num_layers, 12)\n        _, _ = encoder(self.inputs)\n\n    @pretrained_test\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        # case 1: xlnet base\n        encoder = XLNetEncoder()\n        self.assertEqual(len(encoder.trainable_variables), 182)\n        _, _ = encoder(self.inputs)\n\n        # Case 2: xlnet large\n        hparams = {\n            ""pretrained_model_name"": ""xlnet-large-cased"",\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n        self.assertEqual(len(encoder.trainable_variables), 362)\n        _, _ = encoder(self.inputs)\n\n        # case 3: self-designed bert\n        hparams = {\n            ""num_layers"": 6,\n            ""pretrained_model_name"": None,\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n        self.assertEqual(len(encoder.trainable_variables), 92)\n        _, _ = encoder(self.inputs)\n\n    def test_encode(self):\n        r""""""Tests encoding.\n        """"""\n        # case 1: xlnet base\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n\n        inputs = torch.randint(32000, (self.batch_size, self.max_length))\n        outputs, new_memory = encoder(inputs)\n\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, encoder.output_size]))\n        self.assertEqual(new_memory, None)\n\n        # case 2: self-designed xlnet\n        hparams = {\n            \'pretrained_model_name\': None,\n            \'untie_r\': True,\n            \'num_layers\': 6,\n            \'mem_len\': 0,\n            \'reuse_len\': 0,\n            \'num_heads\': 8,\n            \'hidden_dim\': 32,\n            \'head_dim\': 64,\n            \'dropout\': 0.1,\n            \'attention_dropout\': 0.1,\n            \'use_segments\': True,\n            \'ffn_inner_dim\': 256,\n            \'activation\': \'gelu\',\n            \'vocab_size\': 32000,\n            \'max_seq_length\': 128,\n            \'initializer\': None,\n            \'name\': ""xlnet_encoder"",\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n        outputs, new_memory = encoder(inputs)\n\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, encoder.output_size]))\n        self.assertEqual(new_memory, None)\n\n    def test_soft_ids(self):\n        r""""""Tests soft ids.\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n\n        inputs = torch.rand(self.batch_size, self.max_length, 32000)\n        outputs, new_memory = encoder(inputs)\n\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([self.batch_size, self.max_length, encoder.output_size]))\n        self.assertEqual(new_memory, None)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/networks/conv_networks_test.py,19,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for conv networks.\n""""""\nimport unittest\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.core.layers import MergeLayer\nfrom texar.torch.modules.networks.conv_networks import Conv1DNetwork\n\n\nclass Conv1DNetworkTest(unittest.TestCase):\n    """"""Tests :class:`~texar.torch.modules.Conv1DNetwork` class.\n    """"""\n\n    def test_feedforward(self):\n        """"""Tests feed forward.\n        """"""\n        inputs_1 = torch.ones([128, 32, 300])\n        network_1 = Conv1DNetwork(in_channels=inputs_1.shape[1],\n                                  in_features=inputs_1.shape[2])\n        # dense layers are not constructed yet\n        self.assertEqual(len(network_1.layers), 4)\n        self.assertIsInstance(\n            network_1.layer_by_name(""MergeLayer""), MergeLayer)\n        for layer in network_1.layers[0].layers:\n            self.assertIsInstance(layer, nn.Sequential)\n\n        outputs_1 = network_1(inputs_1)\n        self.assertEqual(outputs_1.shape, torch.Size([128, 256]))\n        self.assertEqual(outputs_1.size(-1), network_1.output_size)\n\n        inputs_2 = torch.ones([128, 64, 300])\n        hparams = {\n            # Conv layers\n            ""num_conv_layers"": 2,\n            ""out_channels"": 128,\n            ""kernel_size"": [[3, 4, 5], 4],\n            ""other_conv_kwargs"": {""padding"": 0},\n            # Pooling layers\n            ""pooling"": ""AvgPool1d"",\n            ""pool_size"": 2,\n            ""pool_stride"": 1,\n            # Dense layers\n            ""num_dense_layers"": 3,\n            ""out_features"": [128, 128, 10],\n            ""dense_activation"": ""ReLU"",\n            ""other_dense_kwargs"": None,\n            # Dropout\n            ""dropout_conv"": [0, 1, 2],\n            ""dropout_dense"": 2\n        }\n\n        network_2 = Conv1DNetwork(in_channels=inputs_2.shape[1],\n                                  in_features=inputs_2.shape[2],\n                                  hparams=hparams)\n        # dropout-merge-dropout-(Sequential(Conv, ReLU))-avgpool-dropout-\n        # flatten-(Sequential(Linear,ReLU))-(Sequential(Linear,ReLU))-dropout\n        # -linear\n        self.assertEqual(len(network_2.layers), 1 + 1 + 1 + 3 + 4 + 1)\n        self.assertIsInstance(\n            network_2.layer_by_name(""MergeLayer""), MergeLayer)\n        for layer in network_2.layers[1].layers:\n            self.assertIsInstance(layer, nn.Sequential)\n\n        outputs_2 = network_2(inputs_2)\n        self.assertEqual(outputs_2.shape, torch.Size([128, 10]))\n        self.assertEqual(outputs_2.size(-1), network_2.output_size)\n\n        # test whether concatenation happens along channel dim when feature dim\n        # has been reduced\n        hparams = {\n            # Conv layers\n            ""num_conv_layers"": 1,\n            ""out_channels"": 128,\n            ""kernel_size"": [3, 4, 5],\n            ""other_conv_kwargs"": {""padding"": 0},\n            # Pooling layers\n            ""pooling"": ""AvgPool1d"",\n            ""pool_size"": None,\n            ""pool_stride"": 1,\n            # Dense layers\n            ""num_dense_layers"": 0,\n            ""out_features"": [],\n            ""dense_activation"": ""ReLU"",\n            ""other_dense_kwargs"": None,\n            # Dropout\n            ""dropout_conv"": [],\n            ""dropout_dense"": []\n        }\n\n        inputs_3 = torch.ones([128, 64, 300])\n        network_3 = Conv1DNetwork(in_channels=inputs_3.shape[1],\n                                  in_features=inputs_3.shape[2],\n                                  hparams=hparams)\n        outputs_3 = network_3(inputs_3)\n        num_of_kernels = len(hparams[""kernel_size""])\n        out_channels = hparams[""out_channels""]\n        self.assertEqual(outputs_3.shape,\n                         torch.Size([128, num_of_kernels * out_channels]))\n        self.assertEqual(outputs_3.size(-1), network_3.output_size)\n\n        # test for channels last tensors\n        inputs_3 = inputs_3.permute(0, 2, 1)\n        outputs_3 = network_3(inputs_3, data_format=""channels_last"")\n        self.assertEqual(outputs_3.shape,\n                         torch.Size([128, num_of_kernels * out_channels]))\n        self.assertEqual(outputs_3.size(-1), network_3.output_size)\n\n    def test_conv_and_pool_kwargs(self):\n\n        inputs = torch.ones([128, 64, 300])\n        hparams = {\n            # Conv layers\n            ""num_conv_layers"": 2,\n            ""out_channels"": 128,\n            ""kernel_size"": [[3, 4, 5], 4],\n            ""other_conv_kwargs"": [{""padding"": 0}, {""padding"": 0}],\n            # Pooling layers\n            ""pooling"": ""AvgPool1d"",\n            ""pool_size"": 2,\n            ""pool_stride"": 1,\n            ""other_pool_kwargs"": {},\n            # Dense layers\n            ""num_dense_layers"": 3,\n            ""out_features"": [128, 128, 10],\n            ""dense_activation"": ""ReLU"",\n            ""other_dense_kwargs"": None,\n            # Dropout\n            ""dropout_conv"": [0, 1, 2],\n            ""dropout_dense"": 2\n        }\n\n        network = Conv1DNetwork(in_channels=inputs.shape[1],\n                                  in_features=inputs.shape[2],\n                                  hparams=hparams)\n        # dropout-merge-dropout-(Sequential(Conv, ReLU))-avgpool-dropout-\n        # flatten-(Sequential(Linear,ReLU))-(Sequential(Linear,ReLU))-dropout\n        # -linear\n        self.assertEqual(len(network.layers), 1 + 1 + 1 + 3 + 4 + 1)\n        self.assertIsInstance(\n            network.layer_by_name(""MergeLayer""), MergeLayer)\n        for layer in network.layers[1].layers:\n            self.assertIsInstance(layer, nn.Sequential)\n\n    def test_channel_dimension(self):\n        inputs = torch.ones([128, 64, 300])\n        hparams = {\n            # Conv layers\n            ""num_conv_layers"": 2,\n            ""out_channels"": 128,\n            ""kernel_size"": [[3, 4, 5], 4],\n            ""other_conv_kwargs"": [{""padding"": 0}, {""padding"": 0}],\n            # Pooling layers\n            ""pooling"": ""AvgPool1d"",\n            ""pool_size"": 2,\n            ""pool_stride"": 1,\n            ""other_pool_kwargs"": {},\n            # Dense layers\n            ""num_dense_layers"": 3,\n            ""out_features"": [128, 128, 10],\n            ""dense_activation"": ""ReLU"",\n            ""other_dense_kwargs"": None,\n            # Dropout\n            ""dropout_conv"": [0, 1, 2],\n            ""dropout_dense"": 2\n        }\n\n        network = Conv1DNetwork(in_channels=inputs.shape[1],\n                                in_features=inputs.shape[2],\n                                hparams=hparams)\n        # dropout-merge-dropout-(Sequential(Conv, ReLU))-avgpool-dropout-\n        # flatten-(Sequential(Linear,ReLU))-(Sequential(Linear,ReLU))-dropout\n        # -linear\n        self.assertEqual(len(network.layers), 1 + 1 + 1 + 3 + 4 + 1)\n        self.assertIsInstance(\n            network.layer_by_name(""MergeLayer""), MergeLayer)\n        for layer in network.layers[1].layers:\n            self.assertIsInstance(layer, nn.Sequential)\n\n        inputs_t = torch.ones([128, 300, 64])\n        output = network(inputs_t, data_format=""channels_last"")\n        self.assertEqual(output.shape, torch.Size([128, 10]))\n        self.assertEqual(output.size(-1), network.output_size)\n\n    def test_sequence_length(self):\n        batch_size = 4\n        channels = 64\n        max_time = 300\n        sequence_lengths = torch.LongTensor(batch_size).random_(1, max_time)\n        inputs = torch.ones([batch_size, channels, max_time])\n        hparams = {\n            # Conv layers\n            ""num_conv_layers"": 2,\n            ""out_channels"": 128,\n            ""kernel_size"": [[3, 4, 5], 4],\n            ""other_conv_kwargs"": {""padding"": 0},\n            # Pooling layers\n            ""pooling"": ""AvgPool1d"",\n            ""pool_size"": 2,\n            ""pool_stride"": 1,\n            # Dense layers\n            ""num_dense_layers"": 0,\n            ""dense_activation"": ""ReLU"",\n            ""other_dense_kwargs"": None,\n            # Dropout\n            ""dropout_conv"": [0, 1, 2],\n            ""dropout_dense"": 2\n        }\n\n        network = Conv1DNetwork(in_channels=inputs.shape[1],\n                                in_features=inputs.shape[2],\n                                hparams=hparams)\n        outputs = network(input=inputs, sequence_length=sequence_lengths)\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([batch_size, network.hparams[""out_channels""], 884]))\n\n        # channels last\n        inputs = torch.ones([batch_size, max_time, channels])\n        network = Conv1DNetwork(in_channels=inputs.shape[2],\n                                in_features=inputs.shape[1],\n                                hparams=hparams)\n        outputs = network(input=inputs,\n                          sequence_length=sequence_lengths,\n                          data_format=""channels_last"")\n        self.assertEqual(\n            outputs.shape,\n            torch.Size([batch_size, 884, network.hparams[""out_channels""]]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/networks/networks_test.py,5,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for feed forward neural networks.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.networks.networks import FeedForwardNetwork\n\n\nclass FeedForwardNetworkTest(unittest.TestCase):\n    """"""Tests the class\n    :class:`~texar.torch.modules.networks.networks.FeedForwardNetwork`.\n    """"""\n\n    def test_feedforward(self):\n        """"""Tests feed-forward.\n        """"""\n        hparams = {\n            ""layers"": [\n                {\n                    ""type"": ""torch.nn.Linear"",\n                    ""kwargs"": {\n                        ""in_features"": 32,\n                        ""out_features"": 64\n                    }\n                },\n                {\n                    ""type"": ""torch.nn.Linear"",\n                    ""kwargs"": {\n                        ""in_features"": 64,\n                        ""out_features"": 128\n                    }\n                }\n            ]\n        }\n\n        nn = FeedForwardNetwork(hparams=hparams)\n\n        self.assertEqual(len(nn.layers), len(hparams[""layers""]))\n        outputs = nn(torch.ones(64, 16, 32))\n        self.assertEqual(len(nn.trainable_variables),\n                         len(hparams[""layers""]) * 2)\n        self.assertEqual(outputs.size(-1), nn.output_size)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/pretrained/bert_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for BERT utils.\n""""""\n\nimport os\nimport unittest\n\nfrom texar.torch.modules.pretrained.bert import *\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass BERTUtilsTest(unittest.TestCase):\n    r""""""Tests BERT utils.\n    """"""\n\n    @pretrained_test\n    def test_load_pretrained_bert_AND_transform_bert_to_texar_config(self):\n\n        pretrained_model_dir = PretrainedBERTMixin.download_checkpoint(\n            pretrained_model_name=""bert-base-uncased"")\n\n        info = list(os.walk(pretrained_model_dir))\n        _, _, files = info[0]\n        self.assertIn(\'bert_model.ckpt.meta\', files)\n        self.assertIn(\'bert_model.ckpt.data-00000-of-00001\', files)\n        self.assertIn(\'bert_model.ckpt.index\', files)\n        self.assertIn(\'bert_config.json\', files)\n\n        model_config = PretrainedBERTMixin._transform_config(\n            pretrained_model_name=""bert-base-uncased"",\n            cache_dir=pretrained_model_dir)\n\n        exp_config = {\n            \'hidden_size\': 768,\n            \'embed\': {\n                \'name\': \'word_embeddings\',\n                \'dim\': 768\n            },\n            \'vocab_size\': 30522,\n            \'segment_embed\': {\n                \'name\': \'token_type_embeddings\',\n                \'dim\': 768\n            },\n            \'type_vocab_size\': 2,\n            \'position_embed\': {\n                \'name\': \'position_embeddings\',\n                \'dim\': 768\n            },\n            \'position_size\': 512,\n            \'encoder\': {\n                \'name\': \'encoder\',\n                \'embedding_dropout\': 0.1,\n                \'num_blocks\': 12,\n                \'multihead_attention\': {\n                    \'use_bias\': True,\n                    \'num_units\': 768,\n                    \'num_heads\': 12,\n                    \'output_dim\': 768,\n                    \'dropout_rate\': 0.1,\n                    \'name\': \'self\'\n                },\n                \'residual_dropout\': 0.1,\n                \'dim\': 768,\n                \'use_bert_config\': True,\n                \'eps\': 1e-12,\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 768,\n                                \'out_features\': 3072,\n                                \'bias\': True\n                            }\n                        },\n                        {\'type\': \'BertGELU\'},\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 3072,\n                                \'out_features\': 768,\n                                \'bias\': True\n                            }\n                        }\n                    ]\n                }\n            }\n        }\n\n        self.assertDictEqual(model_config, exp_config)\n\n    @pretrained_test\n    def test_load_spanbert_AND_transform_spanbert_to_texar_config(\n            self):\n        pretrained_model_dir = PretrainedBERTMixin.download_checkpoint(\n            pretrained_model_name=""spanbert-base-cased"")\n\n        info = list(os.walk(pretrained_model_dir))\n        _, _, files = info[0]\n        self.assertIn(\'config.json\', files)\n        self.assertIn(\'pytorch_model.bin\', files)\n\n        model_config = PretrainedBERTMixin._transform_config(\n            pretrained_model_name=""spanbert-base-cased"",\n            cache_dir=pretrained_model_dir)\n\n        exp_config = {\n            \'hidden_size\': 768,\n            \'embed\': {\n                \'name\': \'word_embeddings\',\n                \'dim\': 768\n            },\n            \'vocab_size\': 28996,\n            \'position_embed\': {\n                \'name\': \'position_embeddings\',\n                \'dim\': 768\n            },\n            \'position_size\': 512,\n            \'encoder\': {\n                \'name\': \'encoder\',\n                \'embedding_dropout\': 0.1,\n                \'num_blocks\': 12,\n                \'multihead_attention\': {\n                    \'use_bias\': True,\n                    \'num_units\': 768,\n                    \'num_heads\': 12,\n                    \'output_dim\': 768,\n                    \'dropout_rate\': 0.1,\n                    \'name\': \'self\'\n                },\n                \'residual_dropout\': 0.1,\n                \'dim\': 768,\n                \'use_bert_config\': True,\n                \'eps\': 1e-12,\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 768,\n                                \'out_features\': 3072,\n                                \'bias\': True\n                            }\n                        },\n                        {\'type\': \'BertGELU\'},\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 3072,\n                                \'out_features\': 768,\n                                \'bias\': True\n                            }\n                        }\n                    ]\n                }\n            }\n        }\n\n        self.assertDictEqual(model_config, exp_config)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/pretrained/gpt2_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for GPT2 utils.\n""""""\n\nimport os\nimport unittest\n\nfrom texar.torch.modules.pretrained.gpt2 import *\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass GPT2UtilsTest(unittest.TestCase):\n    r""""""Tests GPT2 utils.\n    """"""\n\n    @pretrained_test\n    def test_load_pretrained_gpt2_AND_transform_gpt2_to_texar_config(self):\n        pretrained_model_dir = PretrainedGPT2Mixin.download_checkpoint(\n            pretrained_model_name=""gpt2-small"")\n\n        info = list(os.walk(pretrained_model_dir))\n        _, _, files = info[0]\n        self.assertIn(\'checkpoint\', files)\n        self.assertIn(\'encoder.json\', files)\n        self.assertIn(\'hparams.json\', files)\n        self.assertIn(\'model.ckpt.data-00000-of-00001\', files)\n        self.assertIn(\'model.ckpt.index\', files)\n        self.assertIn(\'model.ckpt.meta\', files)\n        self.assertIn(\'vocab.bpe\', files)\n\n        model_config = PretrainedGPT2Mixin._transform_config(\n            pretrained_model_name=""gpt2-small"",\n            cache_dir=pretrained_model_dir)\n\n        exp_config = {\n            \'vocab_size\': 50257,\n            \'context_size\': 1024,\n            \'embedding_size\': 768,\n            \'embed\': {\n                \'dim\': 768\n            },\n            \'position_size\': 1024,\n            \'position_embed\': {\n                \'dim\': 768\n            },\n\n            \'encoder\': {\n                \'dim\': 768,\n                \'num_blocks\': 12,\n                \'embedding_dropout\': 0,\n                \'residual_dropout\': 0,\n                \'multihead_attention\': {\n                    \'use_bias\': True,\n                    \'num_units\': 768,\n                    \'num_heads\': 12,\n                    \'output_dim\': 768\n                },\n                \'initializer\': {\n                    \'type\': \'variance_scaling_initializer\',\n                    \'kwargs\': {\n                        \'factor\': 1.0,\n                        \'mode\': \'FAN_AVG\',\n                        \'uniform\': True\n                    }\n                },\n                \'eps\': 1e-6,\n                \'poswise_feedforward\': {\n                    \'layers\':\n                        [\n                            {\n                                \'type\': \'Linear\',\n                                \'kwargs\': {\n                                    \'in_features\': 768,\n                                    \'out_features\': 3072,\n                                    \'bias\': True\n                                }\n                            },\n                            {\n                                \'type\': \'GPTGELU\',\n                                \'kwargs\': {}\n                            },\n                            {\n                                \'type\': \'Linear\',\n                                \'kwargs\': {\n                                    \'in_features\': 3072,\n                                    \'out_features\': 768,\n                                    \'bias\': True\n                                }\n                            }\n                        ],\n                    \'name\': \'ffn\'\n                },\n                \'use_bert_config\': False\n            }\n        }\n\n        self.assertDictEqual(model_config, exp_config)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/pretrained/roberta_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for RoBERTa utils.\n""""""\n\nimport os\nimport unittest\n\nfrom texar.torch.modules.pretrained.roberta import *\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass RoBERTaUtilsTest(unittest.TestCase):\n    r""""""Tests RoBERTa utils.\n    """"""\n\n    @pretrained_test\n    def test_load_pretrained_roberta_AND_transform_roberta_to_texar_config(\n            self):\n\n        pretrained_model_dir = PretrainedRoBERTaMixin.download_checkpoint(\n            pretrained_model_name=""roberta-base"")\n\n        info = list(os.walk(pretrained_model_dir))\n        _, _, files = info[0]\n        self.assertIn(\'dict.txt\', files)\n        self.assertIn(\'model.pt\', files)\n        self.assertIn(\'NOTE\', files)\n\n        model_config = PretrainedRoBERTaMixin._transform_config(\n            pretrained_model_name=""roberta-base"",\n            cache_dir=pretrained_model_dir)\n\n        exp_config = {\n            \'hidden_size\': 768,\n            \'embed\': {\n                \'name\': \'word_embeddings\',\n                \'dim\': 768\n            },\n            \'vocab_size\': 50265,\n            \'position_embed\': {\n                \'name\': \'position_embeddings\',\n                \'dim\': 768\n            },\n            \'position_size\': 514,\n            \'encoder\': {\n                \'name\': \'encoder\',\n                \'embedding_dropout\': 0.1,\n                \'num_blocks\': 12,\n                \'multihead_attention\': {\n                    \'use_bias\': True,\n                    \'num_units\': 768,\n                    \'num_heads\': 12,\n                    \'output_dim\': 768,\n                    \'dropout_rate\': 0.1,\n                    \'name\': \'self\'\n                },\n                \'residual_dropout\': 0.1,\n                \'dim\': 768,\n                \'eps\': 1e-12,\n                \'use_bert_config\': True,\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 768,\n                                \'out_features\': 3072,\n                                \'bias\': True\n                            }\n                        },\n                        {\'type\': \'BertGELU\'},\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 3072,\n                                \'out_features\': 768,\n                                \'bias\': True\n                            }\n                        }\n                    ]\n                }\n            }\n        }\n\n        self.assertDictEqual(model_config, exp_config)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/pretrained/t5_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for T5 utils.\n""""""\n\nimport os\nimport unittest\n\nfrom texar.torch.modules.pretrained.t5 import *\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass T5UtilsTest(unittest.TestCase):\n    r""""""Tests T5 utils.\n    """"""\n\n    @pretrained_test\n    def test_load_pretrained_t5_AND_transform_t5_to_texar_config(self):\n\n        pretrained_model_dir = PretrainedT5Mixin.download_checkpoint(\n            pretrained_model_name=""T5-Base"")\n\n        info = list(os.walk(pretrained_model_dir))\n        _, _, files = info[0]\n        self.assertIn(\'checkpoint\', files)\n        self.assertIn(\'operative_config.gin\', files)\n\n        # Check vocab file\n        self.assertIn(\'sentencepiece.model\', files)\n\n        model_config = PretrainedT5Mixin._transform_config(\n            pretrained_model_name=""T5-Base"",\n            cache_dir=pretrained_model_dir)\n\n        exp_config = {\n            \'hidden_size\': 768,\n            \'embed\': {\n                \'name\': \'word_embeddings\',\n                \'dim\': 768\n            },\n            \'vocab_size\': 32128,\n\n            \'encoder\': {\n                \'name\': \'encoder\',\n                \'embedding_dropout\': 0.1,\n                \'num_blocks\': 12,\n                \'multihead_attention\': {\n                    \'use_bias\': False,\n                    \'num_units\': 768,\n                    \'num_heads\': 12,\n                    \'output_dim\': 768,\n                    \'dropout_rate\': 0.1,\n                    \'name\': \'self\',\n                    \'is_decoder\': False,\n                    \'relative_attention_num_buckets\': 32\n                },\n                \'residual_dropout\': 0.1,\n                \'dim\': 768,\n                \'eps\': 1e-6,\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 768,\n                                \'out_features\': 3072,\n                                \'bias\': False\n                            }\n                        },\n                        {\'type\': \'ReLU\'},\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 3072,\n                                \'out_features\': 768,\n                                \'bias\': False\n                            }\n                        }\n                    ]\n                }\n            },\n            \'decoder\': {\n                \'name\': \'decoder\',\n                \'embedding_dropout\': 0.1,\n                \'num_blocks\': 12,\n                \'multihead_attention\': {\n                    \'use_bias\': False,\n                    \'num_units\': 768,\n                    \'num_heads\': 12,\n                    \'output_dim\': 768,\n                    \'dropout_rate\': 0.1,\n                    \'name\': \'self\',\n                    \'is_decoder\': True,\n                    \'relative_attention_num_buckets\': 32\n                },\n                \'residual_dropout\': 0.1,\n                \'dim\': 768,\n                \'eps\': 1e-6,\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 768,\n                                \'out_features\': 3072,\n                                \'bias\': False\n                            }\n                        },\n                        {\'type\': \'ReLU\'},\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 3072,\n                                \'out_features\': 768,\n                                \'bias\': False\n                            }\n                        }\n                    ]\n                }\n            }\n        }\n\n        self.assertDictEqual(model_config, exp_config)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/pretrained/t5_utils_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit test for gin utility functions\n""""""\n\nimport unittest\n\nimport os\nimport tempfile\n\nfrom texar.torch.modules.pretrained.t5_utils import read_t5_gin_config_file\n\n\nclass GinTest(unittest.TestCase):\n    r""""""Tests for T5 gin file.\n\n    """"""\n\n    def setUp(self):\n        test_gin = \'\'\'d_ff = 2048\\n\'\'\' \\\n                   \'\'\'d_kv = 64\\n\'\'\' \\\n                   \'\'\'d_model = 512\\n\'\'\'\\\n                   \'\'\'dropout_rate = 0.1\\n\'\'\' \\\n                   \'\'\'inputs_length = 512\\n\'\'\' \\\n                   \'\'\'mean_noise_span_length = 3.0\\n\'\'\' \\\n                   \'\'\'MIXTURE_NAME = \'all_mix\'\\n\'\'\' \\\n                   \'\'\'noise_density = 0.15\\n\'\'\' \\\n                   \'\'\'num_heads = 8\\n\'\'\' \\\n                   \'\'\'num_layers = 6\\n\'\'\' \\\n                   \'\'\'targets_length = 512\\n\'\'\' \\\n                   \'\'\'init_checkpoint = ""gs://t5-data/pretrained_models/\'\'\' \\\n                   \'\'\'small/model.ckpt-1000000""\\n\'\'\' \\\n                   \'\'\'tokens_per_batch = 1048576\\n\'\'\' \\\n                   \'\'\'\\n\'\'\' \\\n                   \'\'\'AdafactorOptimizer.beta1 = 0.0\\n\'\'\' \\\n                   \'\'\'AdafactorOptimizer.clipping_threshold = 1.0\\n\'\'\' \\\n                   \'\'\'AdafactorOptimizer.decay_rate = None\\n\'\'\' \\\n                   \'\'\'AdafactorOptimizer.epsilon1 = 1e-30\\n\'\'\' \\\n                   \'\'\'AdafactorOptimizer.epsilon2 = 0.001\\n\'\'\' \\\n                   \'\'\'AdafactorOptimizer.min_dim_size_to_factor = 128\\n\'\'\' \\\n                   \'\'\'AdafactorOptimizer.multiply_by_parameter_scale = \'\'\' \\\n                   \'\'\'True\\n\'\'\' \\\n                   \'\'\'Bitransformer.shared_embedding = True\\n\'\'\'\n        self.tmp_dir = tempfile.TemporaryDirectory()\n        self.gin_file = os.path.join(self.tmp_dir.name, \'config.gin\')\n        with open(self.gin_file, \'w\') as gin_writer:\n            gin_writer.write(test_gin)\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    def test_read_t5_gin_config_file(self):\n        r""""""Tests :meth:`~texar.torch.utils.gin.read_t5_gin_config_file`.\n        """"""\n        config = read_t5_gin_config_file(self.gin_file)\n\n        expect_config = {\'d_ff\': 2048,\n                         \'d_kv\': 64,\n                         \'d_model\': 512,\n                         \'dropout_rate\': 0.1,\n                         \'inputs_length\': 512,\n                         \'num_heads\': 8,\n                         \'num_layers\': 6\n                         }\n\n        self.assertEqual(config, expect_config)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/pretrained/xlnet_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for XLNet utils.\n""""""\n\nimport os\nimport unittest\n\nfrom texar.torch.modules.pretrained.xlnet import *\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass XLNetUtilsTest(unittest.TestCase):\n    r""""""Tests XLNet utils.\n    """"""\n\n    @pretrained_test\n    def test_load_pretrained_xlnet_AND_transform_xlnet_to_texar_config(self):\n\n        pretrained_model_dir = PretrainedXLNetMixin.download_checkpoint(\n            pretrained_model_name=""xlnet-base-cased"")\n\n        info = list(os.walk(pretrained_model_dir))\n        _, _, files = info[0]\n        self.assertIn(\'spiece.model\', files)\n        self.assertIn(\'xlnet_model.ckpt.meta\', files)\n        self.assertIn(\'xlnet_model.ckpt.data-00000-of-00001\', files)\n        self.assertIn(\'xlnet_model.ckpt.index\', files)\n        self.assertIn(\'xlnet_config.json\', files)\n\n        model_config = PretrainedXLNetMixin._transform_config(\n            pretrained_model_name=""xlnet-base-cased"",\n            cache_dir=pretrained_model_dir)\n\n        exp_config = {\'head_dim\': 64,\n                      \'ffn_inner_dim\': 3072,\n                      \'hidden_dim\': 768,\n                      \'activation\': \'gelu\',\n                      \'num_heads\': 12,\n                      \'num_layers\': 12,\n                      \'vocab_size\': 32000,\n                      \'untie_r\': True}\n\n        self.assertDictEqual(model_config, exp_config)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/pretrained/xlnet_utils_test.py,12,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for XLNet model utils.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.pretrained.xlnet_utils import *\n\n\nclass XLNetModelUtilsTest(unittest.TestCase):\n    r""""""Tests XLNet model utils.\n    """"""\n\n    def test_PositionWiseFF(self):\n\n        # Case 1\n        model = PositionWiseFF()\n        inputs = torch.rand(32, model._hparams.hidden_dim)\n        outputs = model(inputs)\n        self.assertEqual(outputs.shape, torch.Size([32,\n                                                    model._hparams.hidden_dim]))\n\n        # Case 2\n        hparams = {\n            ""hidden_dim"": 16,\n            ""ffn_inner_dim"": 32,\n            ""dropout"": 0.1,\n            ""activation"": \'relu\',\n        }\n        model = PositionWiseFF(hparams=hparams)\n        inputs = torch.rand(32, 16)\n        outputs = model(inputs)\n        self.assertEqual(outputs.shape, torch.Size([32, 16]))\n\n        # Case 3\n        hparams = {\n            ""hidden_dim"": 16,\n            ""ffn_inner_dim"": 32,\n            ""dropout"": 0.1,\n            ""activation"": \'gelu\',\n        }\n        model = PositionWiseFF(hparams=hparams)\n        inputs = torch.rand(32, 16)\n        outputs = model(inputs)\n        self.assertEqual(outputs.shape, torch.Size([32, 16]))\n\n    def test_RelativeMultiheadAttention(self):\n\n        model = RelativeMultiheadAttention()\n\n        states_h = torch.rand(16, 32, model._hparams.hidden_dim)\n        pos_embed = torch.rand(24, 32, model._hparams.hidden_dim)\n\n        output_h, output_g = model(states_h=states_h, pos_embed=pos_embed)\n\n        self.assertEqual(output_h.shape,\n                         torch.Size([16, 32, model._hparams.hidden_dim]))\n        self.assertEqual(output_g, None)\n\n    def test_RelativePositionalEncoding(self):\n\n        batch_size = 16\n        seq_len = 8\n        total_len = 32\n\n        # Case 1\n        model = RelativePositionalEncoding()\n        pos_embed = model(batch_size=batch_size,\n                          seq_len=seq_len,\n                          total_len=total_len)\n        self.assertEqual(pos_embed.shape,\n                         torch.Size([40, 16, model._hparams.dim]))\n\n        # Case 2\n        model = RelativePositionalEncoding()\n        pos_embed = model(batch_size=batch_size,\n                          seq_len=seq_len,\n                          total_len=total_len,\n                          attn_type=\'uni\')\n        self.assertEqual(pos_embed.shape,\n                         torch.Size([33, 16, model._hparams.dim]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/modules/regressors/xlnet_regressor_test.py,11,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for XLNet regressor.\n""""""\n\nimport unittest\n\nimport torch\n\nfrom texar.torch.modules.regressors.xlnet_regressor import *\nfrom texar.torch.utils.test import pretrained_test\n\n\nclass XLNetRegressorTest(unittest.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.XLNetRegressor` class.\n    """"""\n\n    def setUp(self) -> None:\n        self.batch_size = 2\n        self.max_length = 3\n        self.inputs = torch.zeros(\n            self.batch_size, self.max_length, dtype=torch.long)\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n        # case 1\n        regressor = XLNetRegressor(pretrained_model_name=""xlnet-base-cased"")\n        _ = regressor(self.inputs)\n\n        # case 2\n        regressor = XLNetRegressor(pretrained_model_name=""xlnet-large-cased"")\n        _ = regressor(self.inputs)\n\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        self.assertEqual(len(regressor.trainable_variables), 182 + 4)\n        _ = regressor(self.inputs)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""use_projection"": False,\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        self.assertEqual(len(regressor.trainable_variables), 182 + 2)\n        _ = regressor(self.inputs)\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""regr_strategy"": ""all_time"",\n            ""max_seq_length"": 8,\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        self.assertEqual(len(regressor.trainable_variables), 182 + 4)\n        _ = regressor(self.inputs)\n\n        # case 4\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""regr_strategy"": ""time_wise"",\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        self.assertEqual(len(regressor.trainable_variables), 182 + 4)\n        _ = regressor(self.inputs)\n\n    def test_regression(self):\n        r""""""Tests regression.\n        """"""\n        inputs = torch.randint(32000, (self.batch_size, self.max_length))\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        preds = regressor(inputs)\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""regr_strategy"": ""all_time"",\n            ""max_seq_length"": self.max_length,\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        preds = regressor(inputs)\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""regr_strategy"": ""time_wise"",\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        preds = regressor(inputs)\n        self.assertEqual(preds.shape, torch.Size(\n            [self.batch_size, self.max_length]))\n\n    def test_soft_ids(self):\n        r""""""Tests soft ids.\n        """"""\n        inputs = torch.rand(self.batch_size, self.max_length, 32000)\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        preds = regressor(inputs)\n        self.assertEqual(preds.shape, torch.Size([self.batch_size]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/run/metric/classification_test.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for classification related operations.\n""""""\nimport functools\nimport unittest\n\nimport numpy as np\n\nfrom texar.torch.run.executor import make_deterministic\nfrom texar.torch.run.metric.classification import *\nfrom texar.torch.utils.test import external_library_test\n\n\n@external_library_test(""sklearn"")\nclass ClassificationMetricTest(unittest.TestCase):\n    def setUp(self) -> None:\n        make_deterministic(0)\n        self.n_classes = 10\n        self.n_examples = 300\n        self.batch_size = 2\n        self.labels = np.random.randint(self.n_classes, size=self.n_examples)\n        self.guesses = np.random.randint(self.n_classes, size=self.n_examples)\n        self.binary_labels = np.random.randint(2, size=self.n_examples)\n        self.binary_guesses = np.random.randint(2, size=self.n_examples)\n\n    def _test_metric(self, metric, reference_fn, binary=False):\n        guesses = self.binary_guesses if binary else self.guesses\n        labels = self.binary_labels if binary else self.labels\n        for idx in range(0, self.n_examples, self.batch_size):\n            end_idx = idx + self.batch_size\n            metric.add(guesses[idx:end_idx], labels[idx:end_idx])\n            value = metric.value()\n            answer = reference_fn(labels[:end_idx], guesses[:end_idx])\n            self.assertAlmostEqual(value, answer)\n\n    def test_accuracy(self):\n        from sklearn.metrics import accuracy_score\n        metric = Accuracy(pred_name="""")\n        self._test_metric(metric, accuracy_score)\n\n    def test_precision(self):\n        from sklearn.metrics import precision_score\n        for mode in Precision._valid_modes:\n            metric = Precision(mode=mode, pos_label=1, pred_name="""")\n            self._test_metric(\n                metric, functools.partial(precision_score, average=mode),\n                binary=(mode == \'binary\'))\n\n    def test_recall(self):\n        from sklearn.metrics import recall_score\n        for mode in Recall._valid_modes:\n            metric = Recall(mode=mode, pos_label=1, pred_name="""")\n            self._test_metric(\n                metric, functools.partial(recall_score, average=mode),\n                binary=(mode == \'binary\'))\n\n    def test_f1(self):\n        from sklearn.metrics import f1_score\n        for mode in F1._valid_modes:\n            metric = F1(mode=mode, pos_label=1, pred_name="""")\n            self._test_metric(\n                metric, functools.partial(f1_score, average=mode),\n                binary=(mode == \'binary\'))\n'"
tests/run/metric/generation_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for generation related operations.\n""""""\nimport unittest\n\nfrom texar.torch.run.metric.generation import *\nfrom texar.torch.evals.bleu import corpus_bleu\n\n\nclass GenerationMetricTest(unittest.TestCase):\n    def setUp(self) -> None:\n        self.hypotheses = [\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d"",\n            ""i believe that that the script is \xe8\xaf\x8d perfectly correct ."",\n            ""this score should be pretty bad"".split(),\n        ]\n        self.references = [\n            ""this is a test sentence to evaluate the good score ."",\n            ""i believe that the script is perfectly correct ."".split(),\n            ""yeah , this is a totally different sentence ."",\n        ]\n\n    def test_bleu(self):\n        metric = BLEU(pred_name="""", label_name="""")\n        for idx, (hyp, ref) in enumerate(zip(self.hypotheses, self.references)):\n            metric.add([hyp], [ref])\n            value = metric.value()\n            answer = corpus_bleu([[r] for r in self.references[:(idx + 1)]],\n                                 self.hypotheses[:(idx + 1)])\n            self.assertAlmostEqual(value, answer)\n'"
tests/run/metric/regression_test.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for regression related operations.\n""""""\nimport unittest\n\nimport numpy as np\n\nfrom texar.torch.run.metric.regression import *\nfrom texar.torch.utils.test import external_library_test\n\n\nclass RegressionMetricTest(unittest.TestCase):\n    def setUp(self) -> None:\n        self.n_examples = 100\n        self.batch_size = 2\n        self.labels = np.random.randn(self.n_examples)\n        self.guesses = np.random.randn(self.n_examples)\n\n    def _test_metric(self, metric, reference_fn):\n        for idx in range(0, self.n_examples, self.batch_size):\n            end_idx = idx + self.batch_size\n            metric.add(self.guesses[idx:end_idx], self.labels[idx:end_idx])\n            value = metric.value()\n            answer = reference_fn(self.labels[:end_idx], self.guesses[:end_idx])\n            self.assertAlmostEqual(value, answer)\n\n    @external_library_test(""scipy"")\n    def test_pearsonr(self):\n        from scipy.stats import pearsonr\n        metric = PearsonR(pred_name="""")\n        self._test_metric(metric, lambda *args: pearsonr(*args)[0])\n\n    @external_library_test(""sklearn"")\n    def test_rmse(self):\n        from sklearn.metrics import mean_squared_error\n        metric = RMSE(pred_name="""")\n        self._test_metric(\n            metric, lambda *args: np.sqrt(mean_squared_error(*args)))\n'"
tests/run/metric/summary_test.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for summary related operations.\n""""""\nimport unittest\n\nimport numpy as np\n\nfrom texar.torch.run.metric.summary import *\n\n\nclass RegressionMetricTest(unittest.TestCase):\n    def setUp(self) -> None:\n        self.n_examples = 100\n        self.batch_size = 2\n        self.values = np.random.randn(self.n_examples)\n\n    def test_running_average(self):\n        queue_size = 10\n        metric = RunningAverage(queue_size)\n        for idx in range(0, self.n_examples, self.batch_size):\n            end_idx = idx + self.batch_size\n            metric.add(self.values[idx:end_idx], None)\n            value = metric.value()\n            answer = self.values[max(0, end_idx - queue_size):end_idx].mean()\n            self.assertAlmostEqual(value, answer)\n'"
texar/torch/core/__init__.py,6,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar core.\n""""""\n\nfrom texar.torch.core.attention_mechanism import *\nfrom texar.torch.core.attention_mechanism_utils import *\nfrom texar.torch.core.cell_wrappers import *\nfrom texar.torch.core.layers import *\nfrom texar.torch.core.optimization import *\nfrom texar.torch.core.regularizers import *\n'"
texar/torch/core/attention_mechanism.py,143,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious helper classes and utilities for attention cell wrappers.\n\nThe code structure adapted from:\n    `https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/\n    seq2seq/python/ops/attention_wrapper.py`\n""""""\n\nimport functools\nfrom abc import ABC\nfrom typing import Callable, List, NamedTuple, Optional, Tuple, TypeVar\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom texar.torch.core.attention_mechanism_utils import (\n    maybe_mask_score, prepare_memory, safe_cumprod)\nfrom texar.torch.module_base import ModuleBase\nfrom texar.torch.utils.types import MaybeList, MaybeTuple\n\n__all__ = [\n    ""AttentionMechanism"",\n    ""AttentionWrapperState"",\n    ""LuongAttention"",\n    ""BahdanauAttention"",\n    ""compute_attention"",\n    ""monotonic_attention"",\n    ""BahdanauMonotonicAttention"",\n    ""LuongMonotonicAttention"",\n]\n\nState = TypeVar(\'State\')\n\n\nclass AttentionMechanism(ModuleBase, ABC):\n    r""""""A base AttentionMechanism class providing common functionality.\n\n    Common functionality includes:\n\n    1. Storing the query and memory layers.\n    2. Preparing the score mask value.\n\n    Args:\n        encoder_output_size: The output size of the encoder cell.\n        memory_layer: Instance of `torch.nn.Linear`. The layer\'s depth must\n            match the depth of ``query_layer``.\n        query_layer (optional): Instance of `torch.nn.Linear`. The layer\'s\n            depth must  match the depth of ``memory_layer``.  If\n            ``query_layer`` is not provided, the shape of ``query`` must\n            match that of ``memory_layer``.\n        score_mask_value (optional): The mask value for score before\n            passing into `probability_fn`. The default is -inf. Only used\n            if `memory_sequence_length` is not None.\n    """"""\n\n    # Cached variables that are initialized by transforming the `memory` at\n    # the first forward pass of each batch. `clear_cache` should be called when\n    # the batch is finished to prevent holding references to variables in the\n    # computation graph.\n    _values: torch.Tensor\n    _keys: torch.Tensor\n\n    def __init__(self,\n                 encoder_output_size: int,\n                 memory_layer: nn.Module,\n                 query_layer: Optional[nn.Module] = None,\n                 score_mask_value: Optional[torch.Tensor] = None):\n        super().__init__()\n\n        if (query_layer is not None and\n                not isinstance(query_layer, nn.Linear)):\n            raise TypeError(""query_layer is not a Linear Layer: %s""\n                            % type(query_layer).__name__)\n        if (memory_layer is not None and\n                not isinstance(memory_layer, nn.Linear)):\n            raise TypeError(""memory_layer is not a Linear Layer: %s""\n                            % type(memory_layer).__name__)\n        self._query_layer = query_layer\n        self._memory_layer = memory_layer\n\n        if score_mask_value is None:\n            score_mask_value = torch.tensor(-np.inf)\n        self.score_mask_value = score_mask_value\n\n        self._encoder_output_size = encoder_output_size\n\n        self._values = None  # type: ignore\n        self._keys = None  # type: ignore\n\n    def _process_query_and_memory(self, query: torch.Tensor,\n                                  memory: torch.Tensor,\n                                  memory_sequence_length: Optional[\n                                      torch.Tensor] = None) -> torch.Tensor:\n        r""""""Preprocess the memory and query.\n\n        Args:\n            query: tensor, shaped ``[batch_size, query_depth]``.\n            memory: the memory to query; usually the output of an RNN encoder.\n                This tensor should be shaped ``[batch_size, max_time, ...]``.\n            memory_sequence_length (optional): sequence lengths for the batch\n                entries in memory.  If provided, the memory tensor rows are\n                masked with zeros for values past the respective sequence\n                lengths.\n        """"""\n        query = self._query_layer(query) if self._query_layer else query\n\n        if self._values is None and self._keys is None:\n            self._values = prepare_memory(memory, memory_sequence_length)\n\n            if self._memory_layer is not None:\n                self._keys = self._memory_layer(self._values)\n            else:\n                self._keys = self._values\n        return query\n\n    def forward(self,  # type: ignore\n                query: torch.Tensor,\n                state: torch.Tensor,\n                memory: torch.Tensor,\n                memory_sequence_length: Optional[torch.LongTensor] = None) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n        r""""""Score the query based on the keys and values.\n\n        Args:\n            query: tensor, shaped ``[batch_size, query_depth]``.\n            state: tensor, shaped ``[batch_size, alignments_size]``\n                (``alignments_size`` is memory\'s ``max_time``).\n            memory: the memory to query; usually the output of an RNN encoder.\n                This tensor should be shaped ``[batch_size, max_time, ...]``.\n            memory_sequence_length (optional): sequence lengths for the batch\n                entries in memory.  If provided, the memory tensor rows are\n                masked with zeros for values past the respective sequence\n                lengths.\n\n        Returns:\n            Tensor of dtype matching ``memory`` and shape\n            ``[batch_size, alignments_size]`` (``alignments_size`` is memory\'s\n            ``max_time``).\n        """"""\n        raise NotImplementedError\n\n    @property\n    def memory_layer(self) -> nn.Module:\n        r""""""The layer used to transform the attention memory.""""""\n        return self._memory_layer\n\n    @property\n    def query_layer(self) -> Optional[nn.Module]:\n        r""""""The layer used to transform the attention query.""""""\n        return self._query_layer\n\n    @property\n    def values(self) -> torch.Tensor:\n        r""""""Cached tensor of the attention values.""""""\n        return self._values\n\n    @property\n    def encoder_output_size(self) -> int:\n        r""""""Dimension of the encoder output.""""""\n        return self._encoder_output_size\n\n    def clear_cache(self):\n        r""""""Clear the cached preprocessed ``memory`` in the attention mechanism.\n        This function should be called at the end of `forward()` in\n        `AttentionRNNDecoder`.\n        """"""\n        self._values = None\n        self._keys = None\n\n    def initial_alignments(self,\n                           batch_size: int,\n                           max_time: int,\n                           dtype: torch.dtype,\n                           device: torch.device) -> torch.Tensor:\n        r""""""Creates the initial alignment values for the ``AttentionWrapper``\n        class.\n\n        This is important for ``AttentionMechanisms`` that use the previous\n        alignment to calculate the alignment at the next time step\n        (e.g. monotonic attention).\n\n        The default behavior is to return a tensor of all zeros.\n\n        Args:\n            batch_size: integer scalar, the batch_size.\n            max_time: integer scalar, the max_time (length of the source\n                sequence).\n            dtype: The `torch.dtype`.\n            device: The `torch.device`.\n\n        Returns:\n            A ``dtype`` tensor shaped ``[batch_size, alignments_size]``\n            (``alignments_size`` is the value of ``max_time``).\n        """"""\n        return torch.zeros(batch_size, max_time, dtype=dtype, device=device)\n\n    def initial_state(self,\n                      batch_size: int,\n                      max_time: int,\n                      dtype: torch.dtype,\n                      device: torch.device) -> torch.Tensor:\n        r""""""Creates the initial state values for the ``AttentionWrapper`` class.\n\n        This is important for ``AttentionMechanisms`` that use the previous\n        alignment to calculate the alignment at the next time step\n        (e.g. monotonic attention).\n\n        The default behavior is to return the same output as\n        ``initial_alignments``.\n\n        Args:\n            batch_size: integer scalar, the batch_size.\n            max_time: integer scalar, the max_time (length of the source\n                sequence).\n            dtype: The `torch.dtype`.\n            device: The `torch.device`.\n\n        Returns:\n            A ``dtype`` tensor shaped ``[batch_size, alignments_size]``\n            (``alignments_size`` is the value of ``max_time``).\n        """"""\n        return self.initial_alignments(batch_size, max_time, dtype, device)\n\n\ndef _luong_score(query: torch.Tensor,\n                 keys: torch.Tensor,\n                 scale: Optional[torch.Tensor]) -> torch.Tensor:\n    r""""""Implements Luong-style (multiplicative) scoring function.\n    This attention has two forms.\n\n    The first is standard Luong attention, as described in:\n    `Minh-Thang Luong, Hieu Pham, Christopher D. Manning.\n    ""Effective Approaches to Attention-based Neural Machine Translation.""\n    EMNLP 2015.  https://arxiv.org/abs/1508.04025`_\n\n    The second is the scaled form inspired partly by the normalized form of\n    Bahdanau attention.\n\n    To enable the second form, call this function with `scale=True`.\n\n    Args:\n        query: tensor, shape ``[batch_size, num_units]`` to compare to keys.\n        keys: processed memory, shape ``[batch_size, max_time, num_units]``.\n        scale (optional): tensor to scale the attention score.\n\n    Returns:\n        A ``[batch_size, max_time]`` tensor of unnormalized score values.\n\n    Raises:\n        ValueError: If ``key`` and ``query`` depths do not match.\n    """"""\n    depth = query.shape[-1]\n    key_units = keys.shape[-1]\n    if depth != key_units:\n        raise ValueError(\n            ""Incompatible or unknown inner dimensions between query and keys. ""\n            ""Query (%s) has units: %s.  Keys (%s) have units: %s. ""\n            ""Perhaps you need to set num_units to the keys\' dimension (%s)?"" %\n            (query, depth, keys, key_units, key_units))\n\n    # Reshape from [batch_size, depth] to [batch_size, 1, depth] for matmul.\n    query = torch.unsqueeze(query, 1)\n\n    # Inner product along the query units dimension.\n    # matmul shapes: query is [batch_size, 1, depth] and\n    #                keys is [batch_size, max_time, depth].\n    # the inner product is asked to transpose keys\' inner shape to get a batched\n    #  matmul on: [batch_size, 1, depth] . [batch_size, depth, max_time]\n    # resulting in an output shape of: [batch_size, 1, max_time].\n    # we then squeeze out the center singleton dimension.\n    score = torch.matmul(query, keys.permute(0, 2, 1))\n    score = torch.squeeze(score, 1)\n\n    if scale is not None:\n        # Scalar used in weight scaling\n        score = scale * score\n    return score\n\n\nclass LuongAttention(AttentionMechanism):\n    r""""""Implements Luong-style (multiplicative) attention scoring. This\n    attention has two forms.\n\n    The first is standard Luong attention, as described in:\n    `Minh-Thang Luong, Hieu Pham, Christopher D. Manning.\n    [Effective Approaches to Attention-based Neural Machine Translation.\n    EMNLP 2015.] <https://arxiv.org/abs/1508.04025>`_\n\n    The second is the scaled form inspired partly by the normalized form of\n    Bahdanau attention. To enable the second form, construct the object with\n    parameter `scale=True`.\n\n    Args:\n        num_units: The depth of the attention mechanism.\n        encoder_output_size: The output size of the encoder cell.\n        scale: Python boolean.  Whether to scale the energy term.\n        probability_fn (optional) A `callable`.  Converts the score to\n            probabilities.  The default is `torch.nn.softmax`. Other options\n            include :func:`~texar.torch.core.hardmax` and\n            :func:`~texar.torch.core.sparsemax`. Its signature should be:\n            :python:`probabilities = probability_fn(score)`.\n        score_mask_value (optional) The mask value for score before passing\n            into `probability_fn`. The default is `-inf`. Only used if\n            :attr:`memory_sequence_length` is not None.\n    """"""\n\n    def __init__(self,\n                 num_units: int,\n                 encoder_output_size: int,\n                 scale: bool = False,\n                 probability_fn: Optional[Callable[[torch.Tensor],\n                                                   torch.Tensor]] = None,\n                 score_mask_value: Optional[torch.Tensor] = None):\n        # For LuongAttention, we only transform the memory layer; thus\n        # num_units must match expected the query depth.\n        if probability_fn is None:\n            probability_fn = lambda x: F.softmax(x, dim=-1)\n        self._probability_fn = probability_fn\n\n        super().__init__(\n            encoder_output_size=encoder_output_size,\n            memory_layer=nn.Linear(encoder_output_size, num_units, False),\n            query_layer=None,\n            score_mask_value=score_mask_value)\n\n        self.attention_g: Optional[torch.Tensor] = None\n        if scale:\n            self.attention_g = nn.Parameter(torch.tensor(1.0),\n                                            requires_grad=True)\n\n    def forward(self,  # type: ignore\n                query: torch.Tensor,\n                state: torch.Tensor,\n                memory: torch.Tensor,\n                memory_sequence_length: Optional[torch.LongTensor] = None) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n        query = self._process_query_and_memory(\n            query, memory, memory_sequence_length)\n\n        score = _luong_score(query, self._keys, self.attention_g)\n\n        alignments = self._probability_fn(\n            maybe_mask_score(score, self.score_mask_value,\n                             memory_sequence_length))\n\n        next_state = alignments\n        return alignments, next_state\n\n\ndef _bahdanau_score(processed_query: torch.Tensor,\n                    keys: torch.Tensor,\n                    attention_v: torch.Tensor,\n                    attention_g: Optional[torch.Tensor] = None,\n                    attention_b: Optional[torch.Tensor] = None):\n    r""""""Implements Bahdanau-style (additive) scoring function.\n    This attention has two forms.\n\n    The first is Bhandanau attention, as described in:\n    `Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.\n    ""Neural Machine Translation by Jointly Learning to Align and Translate.""\n    ICLR 2015. <https://arxiv.org/abs/1409.0473>`_\n\n    The second is the normalized form.  This form is inspired by the\n    weight normalization article:\n    `Tim Salimans, Diederik P. Kingma.\n    ""Weight Normalization: A Simple Reparameterization to Accelerate\n     Training of Deep Neural Networks."" <https://arxiv.org/abs/1602.07868>`_\n    To enable the second form, set please pass in attention_g and attention_b.\n\n    Args:\n        processed_query: Tensor, shape ``[batch_size, num_units]`` to compare to\n            keys.\n        keys: Processed memory, shape ``[batch_size, max_time, num_units]``.\n        attention_v: Tensor, shape ``[num_units]``.\n        attention_g: Optional scalar tensor for normalization.\n        attention_b: Optional tensor with shape ``[num_units]`` for\n            normalization.\n\n    Returns:\n        A ``[batch_size, max_time]`` tensor of unnormalized score values.\n    """"""\n    processed_query = torch.unsqueeze(processed_query, 1)\n    if attention_g is not None and attention_b is not None:\n        normed_v = attention_g * attention_v * torch.rsqrt(\n            torch.sum(attention_v ** 2))\n        return torch.sum(normed_v * torch.tanh(keys + processed_query\n                                               + attention_b), 2)\n    else:\n        return torch.sum(attention_v * torch.tanh(keys + processed_query), 2)\n\n\nclass BahdanauAttention(AttentionMechanism):\n    r""""""Implements Bahdanau-style (additive) attention.\n    This attention has two forms.\n\n    The first is Bahdanau attention, as described in:\n    `Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.\n    ""Neural Machine Translation by Jointly Learning to Align and Translate.""\n    ICLR 2015. <https://arxiv.org/abs/1409.0473>`_\n\n    The second is the normalized form. This form is inspired by the\n    weight normalization article:\n    `Tim Salimans, Diederik P. Kingma. ""Weight Normalization: A Simple\n    Reparameterization to Accelerate Training of Deep Neural Networks.""\n    <https://arxiv.org/abs/1602.07868>`_\n    To enable the second form, construct the object with parameter\n    `normalize=True`.\n\n    Args:\n        num_units: The depth of the query mechanism.\n        decoder_output_size: The output size of the decoder cell.\n        encoder_output_size: The output size of the encoder cell.\n        normalize: bool.  Whether to normalize the energy term.\n        probability_fn (optional) A `callable`.  Converts the score to\n            probabilities.  The default is `torch.nn.softmax`. Other options\n            include :func:`~texar.torch.core.hardmax` and\n            :func:`~texar.torch.core.sparsemax`. Its signature should be:\n            :python:`probabilities = probability_fn(score)`:.\n        score_mask_value (optional): The mask value for score before passing\n            into ``probability_fn``. The default is `-inf`. Only used\n            if :attr:`memory_sequence_length` is not None.\n    """"""\n\n    def __init__(self,\n                 num_units: int,\n                 decoder_output_size: int,\n                 encoder_output_size: int,\n                 normalize: bool = False,\n                 probability_fn: Optional[Callable[[torch.Tensor],\n                                                   torch.Tensor]] = None,\n                 score_mask_value: Optional[torch.Tensor] = None):\n        if probability_fn is None:\n            probability_fn = lambda x: F.softmax(x, dim=-1)\n        self._probability_fn = probability_fn\n\n        super().__init__(\n            encoder_output_size=encoder_output_size,\n            query_layer=nn.Linear(decoder_output_size, num_units, False),\n            memory_layer=nn.Linear(encoder_output_size, num_units, False),\n            score_mask_value=score_mask_value)\n\n        limit = np.sqrt(3. / num_units)\n        self.attention_v = 2 * limit * torch.rand(num_units) - limit\n        self.attention_v = nn.Parameter(self.attention_v,\n                                        requires_grad=True)\n\n        self.attention_g: Optional[torch.Tensor]\n        self.attention_b: Optional[torch.Tensor]\n        if normalize:\n            self.attention_g = torch.sqrt(torch.tensor(1. / num_units))\n            self.attention_g = nn.Parameter(self.attention_g,\n                                            requires_grad=True)\n            self.attention_b = torch.zeros(num_units)\n            self.attention_b = nn.Parameter(self.attention_b,\n                                            requires_grad=True)\n        else:\n            self.attention_g = None\n            self.attention_b = None\n\n    def forward(self,  # type: ignore\n                query: torch.Tensor,\n                state: torch.Tensor,\n                memory: torch.Tensor,\n                memory_sequence_length: Optional[torch.Tensor] = None) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n        query = self._process_query_and_memory(\n            query, memory, memory_sequence_length)\n\n        score = _bahdanau_score(query,\n                                self._keys,\n                                self.attention_v,\n                                self.attention_g,\n                                self.attention_b)\n\n        alignments = self._probability_fn(\n            maybe_mask_score(score, self.score_mask_value,\n                             memory_sequence_length))\n\n        next_state = alignments\n        return alignments, next_state\n\n\ndef monotonic_attention(p_choose_i: torch.Tensor,\n                        previous_attention: torch.Tensor,\n                        mode: str) -> torch.Tensor:\n    r""""""Compute monotonic attention distribution from choosing probabilities.\n    Monotonic attention implies that the input sequence is processed in an\n    explicitly left-to-right manner when generating the output sequence.  In\n    addition, once an input sequence element is attended to at a given output\n    time step, elements occurring before it cannot be attended to at subsequent\n    output time steps.  This function generates attention distributions\n    according to these assumptions. For more information, see `Online and\n    Linear-Time Attention by Enforcing Monotonic Alignments`.\n\n    Args:\n        p_choose_i: Probability of choosing input sequence/memory element i.\n            Should be of shape (batch_size, input_sequence_length), and should\n            all be in the range [0, 1].\n        previous_attention: The attention distribution from the previous output\n            time step.  Should be of shape (batch_size, input_sequence_length).\n            For the first output time step, `previous_attention[n]` should be\n            `[1, 0, 0, ..., 0] for all n in [0, ... batch_size - 1]`.\n        mode: How to compute the attention distribution.\n            Must be one of ``""recursive""``, ``""parallel""``, or ``""hard""``:\n\n            - ``""recursive""`` recursively computes the distribution.\n              This is slowest but is exact, general, and does not suffer\n              from numerical instabilities.\n            - ``""parallel""`` uses parallelized cumulative-sum and\n              cumulative-product operations to compute a closed-form\n              solution to the recurrence relation defining the attention\n              distribution. This makes it more efficient than\n              ``""recursive""``, but it requires numerical checks which make\n              the distribution non-exact. This can be a problem in\n              particular when input sequence is long and/or\n              :attr:`p_choose_i` has entries very close to 0 or 1.\n            - ``""hard""`` requires that the probabilities in\n              :attr:`p_choose_i` are all either 0 or 1, and subsequently\n              uses a more efficient and exact solution.\n\n    Returns:\n        A tensor of shape (batch_size, input_sequence_length) representing the\n        attention distributions for each sequence in the batch.\n\n    Raises:\n        ValueError: mode is not one of ``""recursive""``, ``""parallel""``,\n            ``""hard""``.\n    """"""\n    # Force things to be tensors\n    if not isinstance(p_choose_i, torch.Tensor):\n        p_choose_i = torch.tensor(p_choose_i)\n\n    if not isinstance(previous_attention, torch.Tensor):\n        previous_attention = torch.tensor(previous_attention)\n\n    if mode == ""recursive"":\n        # Use .shape[0] when it\'s not None, or fall back on symbolic shape\n        batch_size = p_choose_i.shape[0]\n        # Compute [1, 1 - p_choose_i[0], 1 - p_choose_i[1], ...,\n        # 1 - p_choose_i[-2]]\n        shifted_1mp_choose_i = torch.cat((p_choose_i.new_ones(batch_size, 1),\n                                          1 - p_choose_i[:, :-1]), 1)\n\n        # Compute attention distribution recursively as\n        # q[i] = (1 - p_choose_i[i - 1])*q[i - 1] + previous_attention[i]\n        # attention[i] = p_choose_i[i]*q[i]\n\n        def f(x, yz):\n            return torch.reshape(yz[0] * x + yz[1], (batch_size,))\n\n        x_tmp = f(torch.zeros((batch_size,)), torch.transpose(\n            shifted_1mp_choose_i, 0, 1))\n        x_tmp = f(x_tmp, torch.transpose(previous_attention, 0, 1))\n        attention = p_choose_i * torch.transpose(x_tmp, 0, 1)\n    elif mode == ""parallel"":\n        batch_size = p_choose_i.shape[0]\n        shifted_1mp_choose_i = torch.cat((p_choose_i.new_ones(batch_size, 1),\n                                          1 - p_choose_i[:, :-1]), 1)\n        # safe_cumprod computes cumprod in logspace with numeric checks\n        cumprod_1mp_choose_i = safe_cumprod(shifted_1mp_choose_i, dim=1)\n        # Compute recurrence relation solution\n        attention = p_choose_i * cumprod_1mp_choose_i * torch.cumsum(\n            previous_attention / cumprod_1mp_choose_i.clamp(min=1e-10, max=1.),\n            dim=1)\n    elif mode == ""hard"":\n        # Remove any probabilities before the index chosen last time step\n        p_choose_i *= torch.cumsum(previous_attention, dim=1)\n        # Now, use exclusive cumprod to remove probabilities after the first\n        # chosen index, like so:\n        # p_choose_i = [0, 0, 0, 1, 1, 0, 1, 1]\n        # cumprod(1 - p_choose_i, exclusive=True) = [1, 1, 1, 1, 0, 0, 0, 0]\n        # Product of above: [0, 0, 0, 1, 0, 0, 0, 0]\n        batch_size = p_choose_i.shape[0]\n        shifted_1mp_choose_i = torch.cat((p_choose_i.new_ones(batch_size, 1),\n                                          1 - p_choose_i[:, :-1]), 1)\n        attention = p_choose_i * torch.cumprod(shifted_1mp_choose_i, dim=1)\n    else:\n        raise ValueError(""mode must be \'recursive\', \'parallel\', or \'hard\'."")\n    return attention\n\n\ndef _monotonic_probability_fn(score: torch.Tensor,\n                              previous_alignments: torch.Tensor,\n                              sigmoid_noise: float,\n                              mode: str) -> torch.Tensor:\n    r""""""Attention probability function for monotonic attention.\n    Takes in unnormalized attention scores, adds pre-sigmoid noise to\n    encourage the model to make discrete attention decisions, passes them\n    through a sigmoid to obtain ""choosing"" probabilities, and then calls\n    monotonic_attention to obtain the attention distribution.  For more\n    information, see\n    `Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,\n    ""Online and Linear-Time Attention by Enforcing Monotonic Alignments.""\n    ICML 2017.  https://arxiv.org/abs/1704.00784`_\n\n    Args:\n        score: Unnormalized attention scores, shape\n            ``[batch_size, alignments_size]``\n        previous_alignments: Previous attention distribution, shape\n            ``[batch_size, alignments_size]``\n        sigmoid_noise: Standard deviation of pre-sigmoid noise.  Setting this\n            larger than 0 will encourage the model to produce large attention\n            scores, effectively making the choosing probabilities discrete and\n            the resulting attention distribution one-hot.  It should be set to 0\n            at test-time, and when hard attention is not desired.\n        mode: How to compute the attention distribution.  Must be one of\n            ``""recursive""``, ``""parallel""``, or ``""hard""``. Refer to\n            :func:`~texar.torch.core.monotonic_attention` for more information.\n\n    Returns:\n        A ``[batch_size, alignments_size]`` shaped tensor corresponding to the\n        resulting attention distribution.\n    """"""\n    # Optionally add pre-sigmoid noise to the scores\n    if sigmoid_noise > 0:\n        noise = torch.randn(score.shape, dtype=score.dtype, device=score.device)\n        score += sigmoid_noise * noise\n    # Compute ""choosing"" probabilities from the attention scores\n    if mode == ""hard"":\n        # When mode is hard, use a hard sigmoid\n        p_choose_i = (score > 0).type(score.dtype)\n    else:\n        p_choose_i = torch.sigmoid(score)\n    # Convert from choosing probabilities to attention distribution\n    return monotonic_attention(p_choose_i, previous_alignments, mode)\n\n\nclass MonotonicAttentionMechanism(AttentionMechanism, ABC):\n    r""""""Base attention mechanism for monotonic attention.\n\n    Simply overrides the initial_alignments function to provide a dirac\n    distribution, which is needed in order for the monotonic attention\n    distributions to have the correct behavior.\n    """"""\n\n    def initial_alignments(self,\n                           batch_size: int,\n                           max_time: int,\n                           dtype: torch.dtype,\n                           device: torch.device) -> torch.Tensor:\n        r""""""Creates the initial alignment values for the monotonic attentions.\n\n        Initializes to dirac distributions, i.e. [1, 0, 0, ...memory length\n        ..., 0] for all entries in the batch.\n\n        Args:\n            batch_size: integer scalar, the batch_size.\n            max_time: integer scalar, the max_time (length of the source\n                sequence).\n            dtype: The `torch.dtype`.\n            device: The `torch.device`.\n\n        Returns:\n            A ``dtype`` tensor shaped ``[batch_size, alignments_size]``\n            (``alignments_size`` is the value of ``max_time``).\n        """"""\n        labels = torch.zeros((batch_size,), dtype=torch.int64,\n                             device=device)\n        one_hot = torch.eye(max_time, dtype=torch.int64)\n        return F.embedding(labels, one_hot)\n\n\nclass BahdanauMonotonicAttention(MonotonicAttentionMechanism):\n    r""""""Monotonic attention mechanism with Bahdanau-style energy function.\n    This type of attention enforces a monotonic constraint on the attention\n    distributions; that is once the model attends to a given point in the\n    memory it can\'t attend to any prior points at subsequence output\n    time steps.  It achieves this by using the :func:`_monotonic_probability_fn`\n    instead of softmax to construct its attention distributions.  Since the\n    attention scores are passed through a sigmoid, a learnable scalar bias\n    parameter is applied after the score function and before the sigmoid.\n    Otherwise, it is equivalent to BahdanauAttention.  This approach is\n    proposed in:\n    `Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,\n    ""Online and Linear-Time Attention by Enforcing Monotonic Alignments.""\n    ICML 2017.  <https://arxiv.org/abs/1704.00784>`_\n\n    Args:\n        num_units: The depth of the query mechanism.\n        decoder_output_size: The output size of the decoder cell.\n        encoder_output_size: The output size of the encoder cell.\n        normalize: Python boolean.  Whether to normalize the energy term.\n        score_mask_value: (optional): The mask value for score before\n            passing into ``probability_fn``. The default is -inf. Only used\n            if :attr:`memory_sequence_length` is not None.\n        sigmoid_noise: Standard deviation of pre-sigmoid noise.  Refer to\n            :func:`_monotonic_probability_fn` for more\n            information.\n        score_bias_init: Initial value for score bias scalar.  It\'s\n            recommended to initialize this to a negative value when the\n            length of the memory is large.\n        mode: How to compute the attention distribution.  Must be one of\n            ``""recursive""``, ``""parallel""``, or ``""hard""``.  Refer to\n            :func:`~texar.torch.core.monotonic_attention` for more information.\n    """"""\n\n    def __init__(self,\n                 num_units: int,\n                 decoder_output_size: int,\n                 encoder_output_size: int,\n                 normalize: bool = False,\n                 score_mask_value: Optional[torch.Tensor] = None,\n                 sigmoid_noise: float = 0.,\n                 score_bias_init: float = 0.,\n                 mode: str = ""parallel""):\n        # Set up the monotonic probability fn with supplied parameters\n        self.wrapped_probability_fn = functools.partial(\n            _monotonic_probability_fn,\n            sigmoid_noise=sigmoid_noise,\n            mode=mode)\n\n        super().__init__(\n            encoder_output_size=encoder_output_size,\n            query_layer=nn.Linear(decoder_output_size, num_units, False),\n            memory_layer=nn.Linear(encoder_output_size, num_units, False),\n            score_mask_value=score_mask_value)\n\n        limit = np.sqrt(3. / num_units)\n        self.attention_v = 2 * limit * torch.rand(num_units) - limit\n        self.attention_v = nn.Parameter(self.attention_v,\n                                        requires_grad=True)\n\n        self.attention_g: Optional[torch.Tensor]\n        self.attention_b: Optional[torch.Tensor]\n        if normalize:\n            self.attention_g = torch.sqrt(torch.tensor(1. / num_units))\n            self.attention_g = nn.Parameter(self.attention_g,\n                                            requires_grad=True)\n            self.attention_b = torch.zeros(num_units)\n            self.attention_b = nn.Parameter(self.attention_b,\n                                            requires_grad=True)\n        else:\n            self.attention_g = None\n            self.attention_b = None\n\n        if not isinstance(score_bias_init, torch.Tensor):\n            self.attention_score_bias = torch.tensor(score_bias_init)\n        self.attention_score_bias = nn.Parameter(self.attention_score_bias)\n\n    def forward(self,  # type: ignore\n                query: torch.Tensor,\n                state: torch.Tensor,\n                memory: torch.Tensor,\n                memory_sequence_length: Optional[torch.Tensor] = None) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n        query = self._process_query_and_memory(\n            query, memory, memory_sequence_length)\n\n        score = _bahdanau_score(query,\n                                self._keys,\n                                self.attention_v,\n                                self.attention_g,\n                                self.attention_b)\n        score += self.attention_score_bias\n\n        alignments = self.wrapped_probability_fn(\n            maybe_mask_score(score, self.score_mask_value,\n                             memory_sequence_length), state)\n\n        next_state = alignments\n        return alignments, next_state\n\n\nclass LuongMonotonicAttention(MonotonicAttentionMechanism):\n    r""""""Monotonic attention mechanism with Luong-style energy function.\n    This type of attention enforces a monotonic constraint on the attention\n    distributions; that is once the model attends to a given point in the\n    memory it can\'t attend to any prior points at subsequence output\n    time steps.  It achieves this by using :func:`_monotonic_probability_fn`\n    instead of softmax to construct its attention distributions.  Otherwise,\n    it is equivalent to LuongAttention.  This approach is proposed in:\n    `Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,\n    ""Online and Linear-Time Attention by Enforcing Monotonic Alignments.""\n    ICML 2017.  <https://arxiv.org/abs/1704.00784>`_\n\n    Args:\n        num_units: The depth of the query mechanism.\n        encoder_output_size: The output size of the encoder cell.\n        scale: Python boolean.  Whether to scale the energy term.\n        score_mask_value: (optional): The mask value for score before\n            passing into ``probability_fn``. The default is -inf. Only used\n            if :attr:`memory_sequence_length` is not None.\n        sigmoid_noise: Standard deviation of pre-sigmoid noise.  Refer to\n            :func:`_monotonic_probability_fn` for more\n            information.\n        score_bias_init: Initial value for score bias scalar.  It\'s\n            recommended to initialize this to a negative value when the\n            length of the memory is large.\n        mode: How to compute the attention distribution.  Must be one of\n            ``""recursive""``, ``""parallel""``, or ``""hard""``.  Refer to\n            :func:`~texar.torch.core.monotonic_attention` for more information.\n    """"""\n\n    def __init__(self,\n                 num_units: int,\n                 encoder_output_size: int,\n                 scale: bool = False,\n                 score_mask_value: Optional[torch.Tensor] = None,\n                 sigmoid_noise: float = 0.,\n                 score_bias_init: float = 0.,\n                 mode: str = ""parallel""):\n        # Set up the monotonic probability fn with supplied parameters\n        self.wrapped_probability_fn = functools.partial(\n            _monotonic_probability_fn,\n            sigmoid_noise=sigmoid_noise,\n            mode=mode)\n\n        super().__init__(\n            encoder_output_size=encoder_output_size,\n            query_layer=None,\n            memory_layer=nn.Linear(encoder_output_size, num_units, False),\n            score_mask_value=score_mask_value)\n\n        self.attention_g: Optional[torch.Tensor]\n        if scale:\n            self.attention_g = nn.Parameter(\n                torch.tensor(1.0, requires_grad=True))\n        else:\n            self.attention_g = None\n\n        if not isinstance(score_bias_init, torch.Tensor):\n            self.attention_score_bias = torch.tensor(score_bias_init)\n        self.attention_score_bias = nn.Parameter(self.attention_score_bias)\n\n    def forward(self,  # type: ignore\n                query: torch.Tensor,\n                state: torch.Tensor,\n                memory: torch.Tensor,\n                memory_sequence_length: Optional[torch.Tensor] = None) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n        query = self._process_query_and_memory(\n            query, memory, memory_sequence_length)\n\n        score = _luong_score(query, self._keys, self.attention_g)\n        score += self.attention_score_bias\n\n        alignments = self.wrapped_probability_fn(\n            maybe_mask_score(score, self.score_mask_value,\n                             memory_sequence_length), state)\n        next_state = alignments\n        return alignments, next_state\n\n\ndef compute_attention(attention_mechanism: AttentionMechanism,\n                      cell_output: torch.Tensor,\n                      attention_state: torch.Tensor,\n                      memory: torch.Tensor,\n                      attention_layer: Optional[nn.Module],\n                      memory_sequence_length: Optional[torch.LongTensor] = None\n                      ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    r""""""Computes the attention and alignments for a given\n    :attr:`attention_mechanism`.\n\n    Args:\n        attention_mechanism: The :class:`~texar.torch.core.AttentionMechanism`\n            instance used to compute attention.\n        cell_output (tensor): The decoder output (query tensor), shaped\n            ``[batch_size, query_depth]``.\n        attention_state (tensor): tensor, shaped\n            ``[batch_size, alignments_size]`` (``alignments_size`` is memory\'s\n            ``max_time``).\n        memory (tensor): the memory to query; usually the output of an RNN\n            encoder. This tensor should be shaped\n            ``[batch_size, max_time, ...]``.\n        attention_layer (:torch_nn:`Module`, optional): If specified, the\n            attention context is concatenated with :attr:`cell_output`, and\n            fed through this layer.\n        memory_sequence_length (tensor, optional): sequence lengths for the\n            batch entries in memory.  If provided, the memory tensor rows are\n            masked with zeros for values past the respective sequence lengths.\n\n    Returns:\n        A tuple of `(attention, alignments, next_attention_state)`, where\n\n        - ``attention``: The attention context (or the output of\n          :attr:`attention_layer`, if specified).\n        - ``alignments``: The computed attention alignments.\n        - ``next_attention_state``: The attention state after the current time\n          step.\n    """"""\n    alignments, next_attention_state = attention_mechanism(\n        query=cell_output,\n        state=attention_state,\n        memory=memory,\n        memory_sequence_length=memory_sequence_length)\n\n    # Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]\n    expanded_alignments = torch.unsqueeze(alignments, dim=1)\n    # Context is the inner product of alignments and values along the\n    # memory time dimension.\n    # alignments shape is\n    #   [batch_size, 1, memory_time]\n    # attention_mechanism.values shape is\n    #   [batch_size, memory_time, memory_size]\n    # the batched matmul is over memory_time, so the output shape is\n    #   [batch_size, 1, memory_size].\n    # we then squeeze out the singleton dim.\n    context = torch.matmul(expanded_alignments, attention_mechanism.values)\n    context = torch.squeeze(context, dim=1)\n\n    if attention_layer is not None:\n        attention = attention_layer(torch.cat((cell_output, context), dim=1))\n    else:\n        attention = context\n\n    return attention, alignments, next_attention_state\n\n\nclass AttentionWrapperState(NamedTuple):\n    r""""""A `namedtuple` storing the state of an\n    :class:`~texar.torch.core.AttentionWrapper`.\n    """"""\n    cell_state: MaybeList[MaybeTuple[torch.Tensor]]\n    r""""""The state of the wrapped `RNNCell` at the previous time step.""""""\n    attention: torch.Tensor\n    r""""""The attention emitted at the previous time step.""""""\n    time: int\n    r""""""The current time step.""""""\n    alignments: MaybeTuple[torch.Tensor]\n    r""""""A single or tuple of tensor(s) containing the alignments emitted at\n    the previous time step for each attention mechanism.""""""\n    alignment_history: MaybeTuple[List[torch.Tensor]]\n    r""""""(If enabled) A single or tuple of list(s) containing alignment matrices\n    from all time steps for each attention mechanism. Call :torch:`stack` on\n    each list to convert to a :tensor:`Tensor`.""""""\n    attention_state: MaybeTuple[torch.Tensor]\n    r""""""A single or tuple of nested objects containing attention mechanism\n    states for each attention mechanism.""""""\n'"
texar/torch/core/attention_mechanism_utils.py,38,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Various helper utilities for attention mechanism.\n\nThe implementation of `sparsemax` adapted from:\n    `https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/\n    modules/sparse_activations.py`\n""""""\n\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Function\n\nfrom texar.torch.utils.utils import sequence_mask\n\n__all__ = [\n    \'hardmax\',\n    \'maybe_mask_score\',\n    \'prepare_memory\',\n    \'safe_cumprod\',\n    \'sparsemax\',\n]\n\n\ndef hardmax(logits: torch.Tensor) -> torch.Tensor:\n    r""""""Returns batched one-hot vectors. The depth index containing\n    the `1` is that of the maximum logit value.\n\n    Args:\n        logits: A batch tensor of logit values.\n\n    Returns:\n        A batched one-hot tensor.\n    """"""\n    if not isinstance(logits, torch.Tensor):\n        logits = torch.tensor(logits)\n    depth = logits.shape[-1]\n    one_hot = torch.eye(depth, dtype=torch.int64)\n    return F.embedding(torch.argmax(logits, -1), one_hot)\n\n\ndef maybe_mask_score(score: torch.Tensor,\n                     score_mask_value: torch.Tensor,\n                     memory_sequence_length: Optional[torch.LongTensor]) \\\n        -> torch.Tensor:\n    r""""""Mask the attention score based on the masks.""""""\n    if memory_sequence_length is None:\n        return score\n\n    for memory_sequence_length_value in memory_sequence_length:\n        if memory_sequence_length_value <= 0:\n            raise ValueError(\n                ""All values in memory_sequence_length must be greater ""\n                ""than zero."")\n\n    score_mask = sequence_mask(memory_sequence_length,\n                               max_len=score.shape[1])\n    score_mask_values = score_mask_value * torch.ones_like(score)\n    return torch.where(score_mask, score, score_mask_values)\n\n\ndef prepare_memory(memory: torch.Tensor,\n                   memory_sequence_length: Optional[torch.LongTensor]) \\\n        -> torch.Tensor:\n    r""""""Convert to tensor and possibly mask ``memory``.\n\n    Args:\n        memory: tensor, shaped ``[batch_size, max_time, ...]``.\n        memory_sequence_length: integer tensor, shaped ``[batch_size]``.\n\n    Returns:\n        A (possibly masked), new ``memory``.\n\n    Raises:\n        ValueError: if ``memory`` and ``memory_sequence_length`` do not have\n        the same ``batch_size``.\n    """"""\n    if (memory_sequence_length is not None and\n            not isinstance(memory_sequence_length, torch.Tensor)):\n        memory_sequence_length = torch.tensor(memory_sequence_length,\n                                              dtype=torch.long,\n                                              device=memory.device)\n\n    if memory_sequence_length is None:\n        seq_len_mask = None\n    else:\n        seq_len_mask = sequence_mask(memory_sequence_length,\n                                     max_len=memory.shape[1],\n                                     dtype=memory.dtype)\n        seq_len_batch_size = memory_sequence_length.shape[0]\n\n    # Mask the memory based on the memory mask.\n    rank = memory.dim()\n    m_batch_size = memory.shape[0]\n\n    if seq_len_mask is not None:\n        if seq_len_batch_size != m_batch_size:\n            raise ValueError(""memory_sequence_length and memory tensor ""\n                             ""batch sizes do not match."")\n        return memory * seq_len_mask.view(\n            seq_len_mask.size() + (1,) * (rank - 2))\n    else:\n        return memory\n\n\ndef safe_cumprod(x: torch.Tensor,\n                 *args,\n                 **kwargs) -> torch.Tensor:\n    r""""""Computes cumprod of x in logspace using cumsum to avoid underflow.\n    The cumprod function and its gradient can result in numerical\n    instabilities when its argument has very small and/or zero values.\n    As long as the argument is all positive, we can instead compute the\n    cumulative product as `exp(cumsum(log(x)))`.  This function can be called\n    identically to :torch:`cumprod`.\n\n    Args:\n        x: Tensor to take the cumulative product of.\n        *args: Passed on to cumsum; these are identical to those in cumprod.\n        **kwargs: Passed on to cumsum; these are identical to those in cumprod.\n\n    Returns:\n        Cumulative product of x.\n    """"""\n    if not isinstance(x, torch.Tensor):\n        x = torch.tensor(x)\n\n    tiny = torch.finfo(x.dtype).tiny\n\n    return torch.exp(torch.cumsum(torch.log(torch.clamp(x, tiny, 1)),\n                                  *args, **kwargs))\n\n\ndef _make_ix_like(input: torch.Tensor,\n                  dim: int = -1) -> torch.Tensor:\n    d = input.size(dim)\n    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n    view = (-1,) + (1,) * (input.dim() - 1)\n    return rho.view(view).transpose(0, dim)\n\n\ndef _threshold_and_support(input: torch.Tensor,\n                           dim: int = -1) -> Tuple[torch.Tensor, torch.Tensor]:\n    r""""""`Sparsemax` building block: compute the threshold.\n\n    Args:\n        input: any dimension\n        dim: dimension along which to apply `sparsemax`.\n\n    Returns:\n        the threshold value\n    """"""\n\n    input_srt, _ = torch.sort(input, descending=True, dim=dim)\n    input_cumsum = input_srt.cumsum(dim) - 1\n    rhos = _make_ix_like(input, dim)\n    support = rhos * input_srt > input_cumsum\n\n    support_size = support.sum(dim=dim).unsqueeze(dim)\n    tau = input_cumsum.gather(dim, support_size - 1)\n    tau /= support_size.to(input.dtype)\n    return tau, support_size\n\n\nclass SparsemaxFunction(Function):\n\n    @staticmethod\n    def forward(ctx,  # type: ignore\n                input: torch.Tensor,\n                dim: int = -1) -> torch.Tensor:\n        ctx.dim = dim\n        max_val, _ = input.max(dim=dim, keepdim=True)\n        input -= max_val  # same numerical stability trick as for softmax\n        tau, supp_size = _threshold_and_support(input, dim=dim)\n        output = torch.clamp(input - tau, min=0)\n        ctx.save_for_backward(supp_size, output)\n        return output\n\n    @staticmethod\n    def backward(ctx,  # type: ignore\n                 grad_output: torch.Tensor) -> Tuple[torch.Tensor, None]:\n        supp_size, output = ctx.saved_tensors\n        dim = ctx.dim\n        grad_input = grad_output.clone()\n        grad_input[output == 0] = 0\n\n        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()\n        v_hat = v_hat.unsqueeze(dim)\n        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n        return grad_input, None\n\n\ndef sparsemax(input: torch.Tensor, dim: int = -1) -> torch.Tensor:\n    r""""""`sparsemax`: normalizing sparse transform (a la softmax).\n\n    Args:\n        input (Tensor): A batch tensor of logit values.\n        dim: Dimension along which to apply `sparsemax`.\n\n    Returns:\n        Tensor: output with the same shape as input.\n    """"""\n    return SparsemaxFunction.apply(input, dim)  # type: ignore\n'"
texar/torch/core/cell_wrappers.py,47,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTensorFlow-style RNN cell wrappers.\n""""""\n\nfrom typing import Callable, Generic, List, Optional, Tuple, TypeVar, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom texar.torch.core.attention_mechanism import (\n    AttentionMechanism, AttentionWrapperState, compute_attention)\nfrom texar.torch.utils import utils\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    \'RNNState\',\n    \'LSTMState\',\n    \'HiddenState\',\n    \'wrap_builtin_cell\',\n    \'RNNCellBase\',\n    \'RNNCell\',\n    \'GRUCell\',\n    \'LSTMCell\',\n    \'DropoutWrapper\',\n    \'ResidualWrapper\',\n    \'HighwayWrapper\',\n    \'MultiRNNCell\',\n    \'AttentionWrapper\',\n]\n\nState = TypeVar(\'State\')\nRNNState = torch.Tensor\nLSTMState = Tuple[torch.Tensor, torch.Tensor]\n\nHiddenState = MaybeList[Union[RNNState, LSTMState]]\n\n\ndef wrap_builtin_cell(cell: nn.RNNCellBase):\n    r""""""Convert a built-in :torch_nn:`RNNCellBase` derived RNN cell to\n    our wrapped version.\n\n    Args:\n        cell: the RNN cell to wrap around.\n\n    Returns:\n        The wrapped cell derived from\n        :class:`texar.torch.core.cell_wrappers.RNNCellBase`.\n    """"""\n    # convert cls to corresponding derived wrapper class\n    if isinstance(cell, nn.RNNCell):\n        self = RNNCellBase.__new__(RNNCell)\n    elif isinstance(cell, nn.GRUCell):\n        self = RNNCellBase.__new__(GRUCell)\n    elif isinstance(cell, nn.LSTMCell):\n        self = RNNCellBase.__new__(LSTMCell)\n    else:\n        raise TypeError(f""Unrecognized class {type(cell)}."")\n    RNNCellBase.__init__(self, cell)\n    return self\n\n\nclass RNNCellBase(nn.Module, Generic[State]):\n    r""""""The base class for RNN cells in our framework. Major differences over\n    :torch_nn:`RNNCell` are two-fold:\n\n    1. Holds an :torch_nn:`Module` which could either be a built-in\n       RNN cell or a wrapped cell instance. This design allows\n       :class:`RNNCellBase` to serve as the base class for both vanilla\n       cells and wrapped cells.\n\n    2. Adds :meth:`zero_state` method for initialization of hidden states,\n       which can also be used to implement batch-specific initialization\n       routines.\n    """"""\n\n    def __init__(self, cell: Union[nn.RNNCellBase, \'RNNCellBase\']):\n        super().__init__()\n        if not isinstance(cell, nn.Module):\n            raise ValueError(""Type of parameter \'cell\' must be derived from""\n                             ""nn.Module, and has \'input_size\' and \'hidden_size\'""\n                             ""attributes."")\n        self._cell = cell\n\n    @property\n    def input_size(self) -> int:\n        r""""""The number of expected features in the input.""""""\n        return self._cell.input_size\n\n    @property\n    def hidden_size(self) -> int:\n        r""""""The number of features in the hidden state.""""""\n        return self._cell.hidden_size\n\n    @property\n    def _param(self) -> nn.Parameter:\n        r""""""Convenience method to access a parameter under the module. Useful\n        when creating tensors of the same attributes using `param.new_*`.\n        """"""\n        return next(self.parameters())\n\n    def init_batch(self):\n        r""""""Perform batch-specific initialization routines. For most cells this\n        is a no-op.\n        """"""\n        pass\n\n    def zero_state(self, batch_size: int) -> State:\n        r""""""Return zero-filled state tensor(s).\n\n        Args:\n            batch_size: int, the batch size.\n\n        Returns:\n            State tensor(s) initialized to zeros. Note that different subclasses\n            might return tensors of different shapes and structures.\n        """"""\n        self.init_batch()\n        if isinstance(self._cell, nn.RNNCellBase):\n            state = self._param.new_zeros(\n                batch_size, self.hidden_size, requires_grad=False)\n        else:\n            state = self._cell.zero_state(batch_size)\n        return state\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor, state: Optional[State] = None) \\\n            -> Tuple[torch.Tensor, State]:\n        r""""""\n        Returns:\n            A tuple of (output, state). For single layer RNNs, output is\n            the same as state.\n        """"""\n        if state is None:\n            batch_size = input.size(0)\n            state = self.zero_state(batch_size)\n        return self._cell(input, state)\n\n\nclass BuiltinCellWrapper(RNNCellBase[State]):\n    r""""""Base class for wrappers over built-in :torch_nn:`RNNCellBase`\n    RNN cells.\n    """"""\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor, state: Optional[State] = None) \\\n            -> Tuple[torch.Tensor, State]:\n        if state is None:\n            batch_size = input.size(0)\n            state = self.zero_state(batch_size)\n        new_state = self._cell(input, state)\n        return new_state, new_state\n\n\nclass RNNCell(BuiltinCellWrapper[RNNState]):\n    r""""""A wrapper over :torch_nn:`RNNCell`.""""""\n\n    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=""tanh""):\n        cell = nn.RNNCell(\n            input_size, hidden_size, bias=bias, nonlinearity=nonlinearity)\n        super().__init__(cell)\n\n\nclass GRUCell(BuiltinCellWrapper[RNNState]):\n    r""""""A wrapper over :torch_nn:`GRUCell`.""""""\n\n    def __init__(self, input_size, hidden_size, bias=True):\n        cell = nn.GRUCell(input_size, hidden_size, bias=bias)\n        super().__init__(cell)\n\n\nclass LSTMCell(BuiltinCellWrapper[LSTMState]):\n    r""""""A wrapper over :torch_nn:`LSTMCell`, additionally providing the\n    option to initialize the forget-gate bias to a constant value.\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True,\n                 forget_bias: Optional[float] = None):\n        if forget_bias is not None and not bias:\n            raise ValueError(""Parameter \'forget_bias\' must be set to None when""\n                             ""\'bias\' is set to False."")\n        cell = nn.LSTMCell(input_size, hidden_size, bias=bias)\n        if forget_bias is not None:\n            with torch.no_grad():\n                cell.bias_ih[hidden_size:(2 * hidden_size)].fill_(forget_bias)\n                cell.bias_hh[hidden_size:(2 * hidden_size)].fill_(forget_bias)\n        super().__init__(cell)\n\n    def zero_state(self, batch_size: int) -> LSTMState:\n        r""""""Returns the zero state for LSTMs as (h, c).""""""\n        state = self._param.new_zeros(\n            batch_size, self.hidden_size, requires_grad=False)\n        return state, state\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor, state: Optional[LSTMState] = None) \\\n            -> Tuple[torch.Tensor, LSTMState]:\n        if state is None:\n            batch_size = input.size(0)\n            state = self.zero_state(batch_size)\n        new_state = self._cell(input, state)\n        return new_state[0], new_state\n\n\nclass DropoutWrapper(RNNCellBase[State]):\n    r""""""Operator adding dropout to inputs and outputs of the given cell.""""""\n\n    def __init__(self, cell: RNNCellBase[State],\n                 input_keep_prob: float = 1.0,\n                 output_keep_prob: float = 1.0,\n                 state_keep_prob: float = 1.0,\n                 variational_recurrent=False):\n        r""""""Create a cell with added input, state, and/or output dropout.\n\n        If `variational_recurrent` is set to `True` (**NOT** the default\n        behavior), then the same dropout mask is applied at every step, as\n        described in:\n\n        Y. Gal, Z Ghahramani.  ""A Theoretically Grounded Application of Dropout\n        in Recurrent Neural Networks"".  https://arxiv.org/abs/1512.05287\n\n        Otherwise a different dropout mask is applied at every time step.\n\n        Note, by default (unless a custom `dropout_state_filter` is provided),\n        the memory state (`c` component of any `LSTMStateTuple`) passing through\n        a `DropoutWrapper` is never modified.  This behavior is described in the\n        above article.\n\n        Args:\n            cell: an RNNCell.\n            input_keep_prob: float between 0 and 1, input keep probability;\n                if it is constant and 1, no input dropout will be added.\n            output_keep_prob: float between 0 and 1, output keep probability;\n                if it is constant and 1, no output dropout will be added.\n            state_keep_prob: float between 0 and 1, output keep probability;\n                if it is constant and 1, no output dropout will be added.\n                State dropout is performed on the outgoing states of the cell.\n            variational_recurrent: bool.  If `True`, then the same dropout\n                pattern is applied across all time steps for one batch. This is\n                implemented by initializing dropout masks in :meth:`zero_state`.\n        """"""\n        super().__init__(cell)\n\n        for prob, attr in [(input_keep_prob, ""input_keep_prob""),\n                           (state_keep_prob, ""state_keep_prob""),\n                           (output_keep_prob, ""output_keep_prob"")]:\n            if prob < 0.0 or prob > 1.0:\n                raise ValueError(\n                    f""Parameter \'{attr}\' must be between 0 and 1: {prob:d}"")\n\n        self._input_keep_prob = input_keep_prob\n        self._output_keep_prob = output_keep_prob\n        self._state_keep_prob = state_keep_prob\n\n        self._variational_recurrent = variational_recurrent\n        self._recurrent_input_mask: Optional[torch.Tensor] = None\n        self._recurrent_output_mask: Optional[torch.Tensor] = None\n        self._recurrent_state_mask: Optional[torch.Tensor] = None\n\n    def _new_mask(self, batch_size: int, mask_size: int,\n                  prob: float) -> torch.Tensor:\n        return self._param.new_zeros(batch_size, mask_size).bernoulli_(prob)\n\n    def init_batch(self):\n        r""""""Initialize dropout masks for variational dropout.\n\n        Note that we do not create dropout mask here, because the batch size\n        may not be known until actual input is passed in.\n        """"""\n        self._recurrent_input_mask = None\n        self._recurrent_output_mask = None\n        self._recurrent_state_mask = None\n\n    def _dropout(self, tensor: torch.Tensor, keep_prob: float,\n                 mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        r""""""Decides whether to perform standard dropout or recurrent dropout.""""""\n        if keep_prob == 1.0 or not self.training:\n            return tensor\n        if mask is not None:\n            return tensor.mul(mask).mul(1.0 / keep_prob)\n        return F.dropout(tensor, 1.0 - keep_prob, self.training)\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor, state: Optional[State] = None) \\\n            -> Tuple[torch.Tensor, State]:\n        if self.training and self._variational_recurrent:\n            # Create or check recurrent masks.\n            batch_size = input.size(0)\n            for name, size in [(\'input\', self.input_size),\n                               (\'output\', self.hidden_size),\n                               (\'state\', self.hidden_size)]:\n                prob = getattr(self, f\'_{name}_keep_prob\')\n                if prob == 1.0:\n                    continue\n                mask = getattr(self, f\'_recurrent_{name}_mask\')\n                if mask is None:\n                    # Initialize the mask according to current batch size.\n                    mask = self._new_mask(batch_size, size, prob)\n                    setattr(self, f\'_recurrent_{name}_mask\', mask)\n                else:\n                    # Check that size matches.\n                    if mask.size(0) != batch_size:\n                        raise ValueError(\n                            ""Variational recurrent dropout mask does not ""\n                            ""support variable batch sizes across time steps"")\n\n        input = self._dropout(input, self._input_keep_prob,\n                              self._recurrent_input_mask)\n        output, new_state = super().forward(input, state)\n        output = self._dropout(output, self._output_keep_prob,\n                               self._recurrent_output_mask)\n        new_state = utils.map_structure(\n            lambda x: self._dropout(\n                x, self._state_keep_prob, self._recurrent_state_mask),\n            new_state)\n        return output, new_state\n\n\nclass ResidualWrapper(RNNCellBase[State]):\n    r""""""RNNCell wrapper that ensures cell inputs are added to the outputs.""""""\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor, state: Optional[State] = None) \\\n            -> Tuple[torch.Tensor, State]:\n        output, new_state = super().forward(input, state)\n        output = input + output\n        return output, new_state\n\n\nclass HighwayWrapper(RNNCellBase[State]):\n    r""""""RNNCell wrapper that adds highway connection on cell input and output.\n\n    Based on: `R. K. Srivastava, K. Greff, and J. Schmidhuber, ""Highway\n    networks"", arXiv preprint arXiv:1505.00387, 2015.`\n    https://arxiv.org/pdf/1505.00387.pdf\n    """"""\n\n    def __init__(self, cell: RNNCellBase[State],\n                 carry_bias_init: Optional[float] = None,\n                 couple_carry_transform_gates: bool = True):\n        r""""""Constructs a `HighwayWrapper` for `cell`.\n\n        Args:\n            cell: An instance of `RNNCell`.\n            carry_bias_init: float, carry gates bias initialization.\n            couple_carry_transform_gates: boolean, should the Carry and\n                Transform gate be coupled.\n        """"""\n        super().__init__(cell)\n\n        self.carry = nn.Linear(self.input_size, self.input_size)\n        if not couple_carry_transform_gates:\n            self.transform = nn.Linear(self.input_size, self.input_size)\n\n        self._coupled = couple_carry_transform_gates\n        if carry_bias_init is not None:\n            nn.init.constant_(self.carry.bias, carry_bias_init)\n            if not couple_carry_transform_gates:\n                nn.init.constant_(self.transform.bias, -carry_bias_init)\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor, state: Optional[State] = None) \\\n            -> Tuple[torch.Tensor, State]:\n        output, new_state = super().forward(input, state)\n        carry = torch.sigmoid(self.carry(input))\n        if self._coupled:\n            transform = 1 - carry\n        else:\n            transform = torch.sigmoid(self.transform(input))\n        output = input * carry + output * transform\n        return output, new_state\n\n\nclass MultiRNNCell(RNNCellBase[List[State]]):\n    r""""""RNN cell composed sequentially of multiple simple cells.\n\n    .. code-block:: python\n\n        sizes = [128, 128, 64]\n        cells = [BasicLSTMCell(input_size, hidden_size)\n                 for input_size, hidden_size in zip(sizes[:-1], sizes[1:])]\n        stacked_rnn_cell = MultiRNNCell(cells)\n    """"""\n\n    _cell: nn.ModuleList  # type: ignore\n\n    def __init__(self, cells: List[RNNCellBase[State]]):\n        r""""""Create a RNN cell composed sequentially of a number of RNNCells.\n\n        Args:\n          cells: list of RNNCells that will be composed in this order.\n\n        Raises:\n          ValueError: if cells is empty (not allowed).\n        """"""\n        if len(cells) == 0:\n            raise ValueError(""Parameter \'cells\' should not be empty."")\n        cell = nn.ModuleList(cells)\n        super().__init__(cell)  # type: ignore\n\n    @property\n    def input_size(self):\n        return self._cell[0].input_size\n\n    @property\n    def hidden_size(self):\n        return self._cell[-1].hidden_size\n\n    def init_batch(self):\n        for cell in self._cell:\n            cell.init_batch()\n\n    def zero_state(self, batch_size: int) -> List[State]:\n        states = [cell.zero_state(batch_size)  # type: ignore\n                  for cell in self._cell]\n        return states\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor,\n                state: Optional[List[State]] = None) \\\n            -> Tuple[torch.Tensor, List[State]]:\n        r""""""Run this multi-layer cell on inputs, starting from state.""""""\n        if state is None:\n            batch_size = input.size(0)\n            state = self.zero_state(batch_size)\n        new_states = []\n        output = input\n        for cell, hx in zip(self._cell, state):\n            output, new_state = cell(output, hx)\n            new_states.append(new_state)\n        return output, new_states\n\n\nclass AttentionWrapper(RNNCellBase[AttentionWrapperState]):\n    r""""""Wraps another `RNNCell` with attention.""""""\n\n    def __init__(self,\n                 cell: RNNCellBase,\n                 attention_mechanism: MaybeList[AttentionMechanism],\n                 attention_layer_size: Optional[MaybeList[int]] = None,\n                 alignment_history: bool = False,\n                 cell_input_fn: Optional[Callable[[torch.Tensor, torch.Tensor],\n                                                  torch.Tensor]] = None,\n                 output_attention: bool = True):\n        r""""""Wraps RNN cell with attention.\n        Construct the `AttentionWrapper`.\n\n        Args:\n            cell: An instance of RNN cell.\n            attention_mechanism: A list of\n                :class:`~texar.torch.core.AttentionMechanism` instances or a\n                single instance.\n            attention_layer_size: A list of Python integers or a single Python\n                integer, the depth of the attention (output) layer(s). If None\n                (default), use the context as attention at each time step.\n                Otherwise, feed the context and cell output into the attention\n                layer to generate attention at each time step. If\n                attention_mechanism is a list, attention_layer_size must be a\n                list of the same length.\n            alignment_history (bool): whether to store alignment\n                history from all time steps in the final output state.\n            cell_input_fn (optional): A `callable`.  The default is:\n                `lambda inputs, attention: array_ops.concat([inputs, attention],\n                -1)`.\n            output_attention (bool): If `True` (default), the output at\n                each time step is the attention value.  This is the behavior of\n                Luong-style attention mechanisms. If `False`, the output at\n                each time step is the output of `cell`.  This is the behavior\n                of Bahdanau-style attention mechanisms.  In both cases, the\n                `attention` tensor is propagated to the next time step via the\n                state and is used there. This flag only controls whether the\n                attention mechanism is propagated up to the next cell in an RNN\n                stack or to the top RNN output.\n\n        Raises:\n            TypeError: :attr:`attention_layer_size` is not None and\n                `attention_mechanism` is a list but\n                :attr:`attention_layer_size` is not; or vice versa.\n            ValueError: if `attention_layer_size` is not None,\n                :attr:`attention_mechanism` is a list, and its length does not\n                match that of :attr:`attention_layer_size`; if\n                :attr:`attention_layer_size` and `attention_layer` are set\n                simultaneously.\n        """"""\n        super().__init__(cell)\n\n        self._is_multi: bool\n        if isinstance(attention_mechanism, (list, tuple)):\n            self._is_multi = True\n            attention_mechanisms = attention_mechanism\n            for mechanism in attention_mechanisms:\n                if not isinstance(mechanism, AttentionMechanism):\n                    raise TypeError(\n                        ""attention_mechanism must contain only instances of ""\n                        ""AttentionMechanism, saw type: %s"" %\n                        type(mechanism).__name__)\n        else:\n            self._is_multi = False\n            if not isinstance(attention_mechanism, AttentionMechanism):\n                raise TypeError(\n                    ""attention_mechanism must be an AttentionMechanism or list ""\n                    ""of multiple AttentionMechanism instances, saw type: %s"" %\n                    type(attention_mechanism).__name__)\n            attention_mechanisms = [attention_mechanism]\n\n        if cell_input_fn is None:\n            cell_input_fn = (\n                lambda inputs, attention: torch.cat((inputs, attention),\n                                                    dim=-1))\n        else:\n            if not callable(cell_input_fn):\n                raise TypeError(\n                    ""cell_input_fn must be callable, saw type: %s"" %\n                    type(cell_input_fn).__name__)\n\n        self._attention_layers: Optional[nn.ModuleList]\n\n        if attention_layer_size is not None:\n            if isinstance(attention_layer_size, (list, tuple)):\n                attention_layer_sizes = tuple(attention_layer_size)\n            else:\n                attention_layer_sizes = (attention_layer_size,)\n\n            if len(attention_layer_sizes) != len(attention_mechanisms):\n                raise ValueError(\n                    ""If provided, attention_layer_size must contain exactly ""\n                    ""one integer per attention_mechanism, saw: %d vs %d""\n                    % (len(attention_layer_sizes), len(attention_mechanisms)))\n\n            self._attention_layers = nn.ModuleList(\n                nn.Linear(attention_mechanisms[i].encoder_output_size +\n                          cell.hidden_size,\n                          attention_layer_sizes[i],\n                          False) for i in range(len(attention_layer_sizes)))\n            self._attention_layer_size = sum(attention_layer_sizes)\n        else:\n            self._attention_layers = None\n            self._attention_layer_size = sum(\n                attention_mechanism.encoder_output_size\n                for attention_mechanism in attention_mechanisms)\n\n        self._cell = cell\n        self.attention_mechanisms = attention_mechanisms\n        self._cell_input_fn = cell_input_fn\n        self._output_attention = output_attention\n        self._alignment_history = alignment_history\n        self._initial_cell_state = None\n\n    def _item_or_tuple(self, seq):\n        r""""""Returns `seq` as tuple or the singular element.\n        Which is returned is determined by how the AttentionMechanism(s) were\n        passed to the constructor.\n\n        Args:\n            seq: A non-empty sequence of items or generator.\n\n        Returns:\n            Either the values in the sequence as a tuple if\n            AttentionMechanism(s) were passed to the constructor as a sequence\n            or the singular element.\n        """"""\n        t = tuple(seq)\n        if self._is_multi:\n            return t\n        else:\n            return t[0]\n\n    @property\n    def output_size(self) -> int:\n        r""""""The number of features in the output tensor.""""""\n        if self._output_attention:\n            return self._attention_layer_size\n        else:\n            return self._cell.hidden_size\n\n    def zero_state(self,\n                   batch_size: int) -> AttentionWrapperState:\n        r""""""Return an initial (zero) state tuple for this\n        :class:`AttentionWrapper`.\n\n        .. note::\n                Please see the initializer documentation for details of how\n                to call :meth:`zero_state` if using an\n                :class:`~texar.torch.core.AttentionWrapper` with a\n                :class:`~texar.torch.modules.BeamSearchDecoder`.\n\n        Args:\n            batch_size: `0D` integer: the batch size.\n\n        Returns:\n            An :class:`~texar.torch.core.AttentionWrapperState` tuple containing\n            zeroed out tensors and Python lists.\n        """"""\n        cell_state: torch.Tensor = super().zero_state(batch_size)  # type:ignore\n\n        initial_alignments = [None for _ in self.attention_mechanisms]\n\n        alignment_history: List[List[Optional[torch.Tensor]]]\n        alignment_history = [[] for _ in initial_alignments]\n\n        return AttentionWrapperState(\n            cell_state=cell_state,\n            time=0,\n            attention=self._param.new_zeros(batch_size,\n                                            self._attention_layer_size,\n                                            requires_grad=False),\n            alignments=self._item_or_tuple(initial_alignments),\n            attention_state=self._item_or_tuple(initial_alignments),\n            alignment_history=self._item_or_tuple(alignment_history))\n\n    def forward(self,  # type: ignore\n                inputs: torch.Tensor,\n                state: Optional[AttentionWrapperState],\n                memory: torch.Tensor,\n                memory_sequence_length: Optional[torch.LongTensor] = None) -> \\\n            Tuple[torch.Tensor, AttentionWrapperState]:\n        r""""""Perform a step of attention-wrapped RNN.\n\n        - Step 1: Mix the :attr:`inputs` and previous step\'s `attention` output\n          via `cell_input_fn`.\n        - Step 2: Call the wrapped `cell` with this input and its previous\n          state.\n        - Step 3: Score the cell\'s output with `attention_mechanism`.\n        - Step 4: Calculate the alignments by passing the score through the\n          `normalizer`.\n        - Step 5: Calculate the context vector as the inner product between the\n          alignments and the attention_mechanism\'s values (memory).\n        - Step 6: Calculate the attention output by concatenating the cell\n          output and context through the attention layer (a linear layer with\n          `attention_layer_size` outputs).\n\n        Args:\n            inputs: (Possibly nested tuple of) Tensor, the input at this time\n                step.\n            state: An instance of\n                :class:`~texar.torch.core.AttentionWrapperState` containing\n                tensors from the previous time step.\n            memory: The memory to query; usually the output of an RNN encoder.\n                This tensor should be shaped `[batch_size, max_time, ...]`.\n            memory_sequence_length: (optional) Sequence lengths for the batch\n                entries in memory.  If provided, the memory tensor rows are\n                masked with zeros for values past the respective sequence\n                lengths.\n\n        Returns:\n            A tuple `(attention_or_cell_output, next_state)`, where\n\n            - `attention_or_cell_output` depending on `output_attention`.\n            - `next_state` is an instance of\n              :class:`~texar.torch.core.AttentionWrapperState` containing the\n              state calculated at this time step.\n\n        Raises:\n            TypeError: If `state` is not an instance of\n                :class:`~texar.torch.core.AttentionWrapperState`.\n        """"""\n        if state is None:\n            state = self.zero_state(batch_size=memory.shape[0])\n        elif not isinstance(state, AttentionWrapperState):\n            raise TypeError(""Expected state to be instance of ""\n                            ""AttentionWrapperState. Received type %s instead.""\n                            % type(state))\n\n        # Step 1: Calculate the true inputs to the cell based on the\n        # previous attention value.\n        cell_inputs = self._cell_input_fn(inputs, state.attention)\n        cell_state = state.cell_state\n\n        cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\n\n        if self._is_multi:\n            previous_attention_state = state.attention_state\n            previous_alignment_history = state.alignment_history\n        else:\n            previous_attention_state = [state.attention_state]  # type: ignore\n            previous_alignment_history = \\\n                [state.alignment_history]  # type: ignore\n\n        all_alignments = []\n        all_attentions = []\n        all_attention_states = []\n        maybe_all_histories = []\n        for i, attention_mechanism in enumerate(self.attention_mechanisms):\n            if previous_attention_state[i] is not None:\n                attention_state = previous_attention_state[i]\n            else:\n                attention_state = attention_mechanism.initial_state(\n                    memory.shape[0], memory.shape[1], self._param.dtype,\n                    self._param.device)\n\n            attention, alignments, next_attention_state = compute_attention(\n                attention_mechanism=attention_mechanism,\n                cell_output=cell_output,\n                attention_state=attention_state,\n                attention_layer=(self._attention_layers[i]\n                                 if self._attention_layers else None),\n                memory=memory,\n                memory_sequence_length=memory_sequence_length)\n\n            if self._alignment_history:\n                alignment_history = previous_alignment_history[i] + [alignments]\n            else:\n                alignment_history = previous_alignment_history[i]\n\n            all_attention_states.append(next_attention_state)\n            all_alignments.append(alignments)\n            all_attentions.append(attention)\n            maybe_all_histories.append(alignment_history)\n\n        attention = torch.cat(all_attentions, 1)\n        next_state = AttentionWrapperState(\n            time=state.time + 1,\n            cell_state=next_cell_state,\n            attention=attention,\n            attention_state=self._item_or_tuple(all_attention_states),\n            alignments=self._item_or_tuple(all_alignments),\n            alignment_history=self._item_or_tuple(maybe_all_histories))\n\n        if self._output_attention:\n            return attention, next_state\n        else:\n            return cell_output, next_state\n'"
texar/torch/core/layers.py,67,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious neural network layers\n""""""\n\nimport copy\nimport functools\nfrom typing import Any, Callable, Dict, List, Optional, Type, Union\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.core import cell_wrappers as wrappers\nfrom texar.torch.core.regularizers import L1L2, Regularizer\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.utils import utils\nfrom texar.torch.utils.dtypes import is_str\n\n__all__ = [\n    \'default_rnn_cell_hparams\',\n    \'get_rnn_cell\',\n    \'identity\',\n    \'default_regularizer_hparams\',\n    \'get_initializer\',\n    \'get_regularizer\',\n    \'get_activation_fn\',\n    \'get_layer\',\n    \'MaxReducePool1d\',\n    \'AvgReducePool1d\',\n    \'get_pooling_layer_hparams\',\n    \'MergeLayer\',\n    \'Flatten\',\n    \'Identity\',\n]\n\n\ndef default_rnn_cell_hparams():\n    r""""""Returns a `dict` of RNN cell hyperparameters and their default values.\n\n    .. code-block:: python\n\n        {\n            ""type"": ""LSTMCell"",\n            ""input_size"": 256,\n            ""kwargs"": {\n                ""hidden_size"": 256\n            },\n            ""num_layers"": 1,\n            ""dropout"": {\n                ""input_keep_prob"": 1.0,\n                ""output_keep_prob"": 1.0,\n                ""state_keep_prob"": 1.0,\n                ""variational_recurrent"": False,\n            },\n            ""residual"": False,\n            ""highway"": False,\n        }\n\n    Here:\n\n    `""type""`: str or cell class or cell instance\n        The RNN cell type. This can be\n\n        - The string name or full module path of a cell class. If class name is\n          provided, the class must be in module :mod:`torch.nn.modules.rnn`,\n          :mod:`texar.torch.core.cell_wrappers`, or :mod:`texar.torch.custom`.\n        - A cell class.\n        - An instance of a cell class. This is not valid if `""num_layers""` > 1.\n\n        For example\n\n        .. code-block:: python\n\n            ""type"": ""LSTMCell""  # class name\n            ""type"": ""torch.nn.GRUCell""  # module path\n            ""type"": ""my_module.MyCell""  # module path\n            ""type"": torch.nn.GRUCell  # class\n            ""type"": LSTMCell(hidden_size=100)  # cell instance\n            ""type"": MyCell(...)  # cell instance\n\n    `""kwargs""`: dict\n        Keyword arguments for the constructor of the cell class.\n        A cell is created by :python:`cell_class(**kwargs)`, where\n        `cell_class` is specified in ""type"" above.\n\n        Ignored if ""type"" is a cell instance.\n\n        .. note::\n            It is unnecessary to specify `""input_size""` within `""kwargs""`.\n            This value will be automatically filled based on layer index.\n\n        .. note::\n            Although PyTorch uses `""hidden_size""` to denote the hidden layer\n            size, we follow TensorFlow conventions and use `""num_units""`.\n\n    `""num_layers""`: int\n        Number of cell layers. Each layer is a cell created as above, with\n        the same hyperparameters specified in `""kwargs""`.\n\n    `""dropout""`: dict\n        Dropout applied to the cell in **each** layer. See\n        :class:`~texar.torch.core.cell_wrappers.DropoutWrapper` for details of\n        the hyperparameters. If all `""\\*_keep_prob""` = 1, no dropout is applied.\n\n        Specifically, if `""variational_recurrent""` = `True`,\n        the same dropout mask is applied across all time steps per batch.\n\n    `""residual""`: bool\n        If `True`, apply residual connection on the inputs and\n        outputs of cell in **each** layer except the first layer. Ignored\n        if `""num_layers""` = 1.\n\n    `""highway""`: bool\n        If True, apply highway connection on the inputs and\n        outputs of cell in each layer except the first layer. Ignored if\n        `""num_layers""` = 1.\n    """"""\n    return {\n        \'type\': \'LSTMCell\',\n        \'kwargs\': {\n            \'num_units\': 256,\n        },\n        \'num_layers\': 1,\n        \'dropout\': {\n            \'input_keep_prob\': 1.0,\n            \'output_keep_prob\': 1.0,\n            \'state_keep_prob\': 1.0,\n            \'variational_recurrent\': False,\n        },\n        \'residual\': False,\n        \'highway\': False,\n        \'@no_typecheck\': [\'type\']\n    }\n\n\ndef default_regularizer_hparams():\n    r""""""Returns the hyperparameters and their default values of a variable\n    regularizer:\n\n    .. code-block:: python\n\n        {\n            ""type"": ""L1L2"",\n            ""kwargs"": {\n                ""l1"": 0.,\n                ""l2"": 0.\n            }\n        }\n\n    The default value corresponds to\n    :class:`~texar.torch.core.regularizers.L1L2` and, with ``(l1=0, l2=0)``,\n    disables regularization.\n    """"""\n    return {\n        ""type"": ""L1L2"",\n        ""kwargs"": {\n            ""l1"": 0.,\n            ""l2"": 0.\n        }\n    }\n\n\ndef get_rnn_cell(input_size, hparams=None):\n    r""""""Creates an RNN cell.\n\n    See :func:`~texar.torch.core.default_rnn_cell_hparams` for all\n    hyperparameters and default values.\n\n    Args:\n        input_size (int): Size of the input to the cell in the first layer.\n        hparams (dict or HParams, optional): Cell hyperparameters. Missing\n            hyperparameters are set to default values.\n\n    Returns:\n        A cell instance.\n\n    Raises:\n        ValueError: If ``hparams[""num_layers""]``>1 and ``hparams[""type""]`` is a\n            class instance.\n    """"""\n    if hparams is None or isinstance(hparams, dict):\n        hparams = HParams(hparams, default_rnn_cell_hparams())\n\n    d_hp = hparams[\'dropout\']\n    variational_recurrent = d_hp[\'variational_recurrent\']\n    input_keep_prob = d_hp[\'input_keep_prob\']\n    output_keep_prob = d_hp[\'output_keep_prob\']\n    state_keep_prob = d_hp[\'state_keep_prob\']\n\n    cells = []\n    num_layers = hparams[\'num_layers\']\n    cell_kwargs = hparams[\'kwargs\'].todict()\n    # rename \'num_units\' to \'hidden_size\' following PyTorch conventions\n    cell_kwargs[\'hidden_size\'] = cell_kwargs[\'num_units\']\n    del cell_kwargs[\'num_units\']\n\n    for layer_i in range(num_layers):\n        # Create the basic cell\n        cell_type = hparams[""type""]\n        if layer_i == 0:\n            cell_kwargs[\'input_size\'] = input_size\n        else:\n            cell_kwargs[\'input_size\'] = cell_kwargs[\'hidden_size\']\n        if not isinstance(cell_type, str) and not isinstance(cell_type, type):\n            if num_layers > 1:\n                raise ValueError(\n                    ""If \'num_layers\'>1, then \'type\' must be a cell class or ""\n                    ""its name/module path, rather than a cell instance."")\n        cell_modules = [\'texar.torch.core.cell_wrappers\',  # prefer our wrappers\n                        \'torch.nn.modules.rnn\', \'texar.torch.custom\']\n        cell = utils.check_or_get_instance(cell_type, cell_kwargs, cell_modules)\n        if isinstance(cell, nn.RNNCellBase):\n            cell = wrappers.wrap_builtin_cell(cell)\n\n        # Optionally add dropout\n        if (input_keep_prob < 1.0 or\n                output_keep_prob < 1.0 or\n                state_keep_prob < 1.0):\n            # TODO: Would this result in non-final layer outputs being\n            #       dropped twice?\n            cell = wrappers.DropoutWrapper(\n                cell=cell,\n                input_keep_prob=input_keep_prob,\n                output_keep_prob=output_keep_prob,\n                state_keep_prob=state_keep_prob,\n                variational_recurrent=variational_recurrent)\n\n        # Optionally add residual and highway connections\n        if layer_i > 0:\n            if hparams[\'residual\']:\n                cell = wrappers.ResidualWrapper(cell)\n            if hparams[\'highway\']:\n                cell = wrappers.HighwayWrapper(cell)\n\n        cells.append(cell)\n\n    if hparams[\'num_layers\'] > 1:\n        cell = wrappers.MultiRNNCell(cells)\n    else:\n        cell = cells[0]\n\n    return cell\n\n\ndef identity(inputs: torch.Tensor):\n    r""""""Returns a tensor with the same content as the input tensor.\n\n    Arguments:\n        inputs: The input tensor.\n\n    Returns:\n        A tensor of the same shape, type, and content.\n    """"""\n    return inputs\n\n\ndef get_regularizer(hparams=None):\n    r""""""Returns a variable regularizer instance.\n\n    See :func:`~texar.torch.core.default_regularizer_hparams` for all\n    hyperparameters and default values.\n\n    The ""type"" field can be a subclass\n    of :class:`~texar.torch.core.regularizers.Regularizer`, its string name\n    or module path, or a class instance.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters are set to default values.\n\n    Returns:\n        A :class:`~texar.torch.core.regularizers.Regularizer` instance.\n        `None` if :attr:`hparams` is `None` or taking the default\n        hyperparameter value.\n\n    Raises:\n        ValueError: The resulting regularizer is not an instance of\n            :class:`~texar.torch.core.regularizers.Regularizer`.\n    """"""\n\n    if hparams is None:\n        return None\n\n    if isinstance(hparams, dict):\n        hparams = HParams(hparams, default_regularizer_hparams())\n\n    rgl = utils.check_or_get_instance(\n        hparams.type, hparams.kwargs.todict(),\n        [""texar.torch.core.regularizers"", ""texar.torch.custom""])\n\n    if not isinstance(rgl, Regularizer):\n        raise ValueError(""The regularizer must be an instance of ""\n                         ""texar.torch.core.regularizers.Regularizer."")\n\n    if isinstance(rgl, L1L2) and rgl.l1 == 0. and rgl.l2 == 0.:\n        return None\n\n    return rgl\n\n\ndef get_initializer(hparams=None) \\\n        -> Optional[Callable[[torch.Tensor], torch.Tensor]]:\n    r""""""Returns an initializer instance.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters with the structure\n\n            .. code-block:: python\n\n                {\n                    ""type"": ""initializer_class_or_function"",\n                    ""kwargs"": {\n                        # ...\n                    }\n                }\n\n            The `""type""` field can be a function name or module path. If name is\n            provided, it be must be from one the following modules:\n            :torch_docs:`torch.nn.init <nn.html#torch-nn-init>` and\n            :mod:`texar.torch.custom`.\n\n            Besides, the `""type""` field can also be an initialization function\n            called with :python:`initialization_fn(**kwargs)`. In this case\n            `""type""` can be the function, or its name or module path. If no\n            keyword argument is required, `""kwargs""` can be omitted.\n\n    Returns:\n        An initializer instance. `None` if :attr:`hparams` is `None`.\n    """"""\n    if hparams is None:\n        return None\n\n    kwargs = hparams.get(\'kwargs\', {})\n    if isinstance(kwargs, HParams):\n        kwargs = kwargs.todict()\n    modules = [\'torch.nn.init\', \'torch\', \'texar.torch.custom\']\n    initializer_fn = utils.get_function(hparams[\'type\'], modules)\n    initializer = functools.partial(initializer_fn, **kwargs)\n\n    return initializer\n\n\ndef get_activation_fn(fn_name: Optional[Union[str,\n                                              Callable[[torch.Tensor],\n                                                       torch.Tensor]]] = None,\n                      kwargs: Union[HParams, Dict, None] = None) \\\n        -> Optional[Callable[[torch.Tensor], torch.Tensor]]:\n    r""""""Returns an activation function `fn` with the signature\n    `output = fn(input)`.\n\n    If the function specified by :attr:`fn_name` has more than one arguments\n    without default values, then all these arguments except the input feature\n    argument must be specified in :attr:`kwargs`. Arguments with default values\n    can also be specified in :attr:`kwargs` to take values other than the\n    defaults. In this case a partial function is returned with the above\n    signature.\n\n    Args:\n        fn_name (str or callable): An activation function, or its name or\n            module path. The function can be:\n\n            - Built-in function defined in\n              :torch_docs:`torch.nn.functional<nn.html#torch-nn-functional>`\n            - User-defined activation functions in module\n              :mod:`texar.torch.custom`.\n            - External activation functions. Must provide the full module path,\n              e.g., ``""my_module.my_activation_fn""``.\n\n        kwargs (optional): A `dict` or instance of :class:`~texar.torch.HParams`\n            containing the keyword arguments of the activation function.\n\n    Returns:\n        An activation function. `None` if :attr:`fn_name` is `None`.\n    """"""\n    if fn_name is None:\n        return None\n\n    fn_modules = [\'torch\', \'torch.nn.functional\',\n                  \'texar.torch.custom\', \'texar.torch.core.layers\']\n    activation_fn_ = utils.get_function(fn_name, fn_modules)\n    activation_fn = activation_fn_\n\n    # Make a partial function if necessary\n    if kwargs is not None:\n        if isinstance(kwargs, HParams):\n            kwargs = kwargs.todict()\n\n        def _partial_fn(features):\n            return activation_fn_(features, **kwargs)\n\n        activation_fn = _partial_fn\n\n    return activation_fn\n\n\ndef get_layer(hparams: Union[HParams, Dict[str, Any]]) -> nn.Module:\n    r""""""Makes a layer instance.\n\n    The layer must be an instance of :torch_nn:`Module`.\n\n    Args:\n        hparams (dict or HParams): Hyperparameters of the layer, with\n            structure:\n\n            .. code-block:: python\n\n                {\n                    ""type"": ""LayerClass"",\n                    ""kwargs"": {\n                        # Keyword arguments of the layer class\n                        # ...\n                    }\n                }\n\n            Here:\n\n            `""type""`: str or layer class or layer instance\n                The layer type. This can be\n\n                - The string name or full module path of a layer class. If\n                  the class name is provided, the class must be in module\n                  :torch_nn:`Module`, :mod:`texar.torch.core`, or\n                  :mod:`texar.torch.custom`.\n                - A layer class.\n                - An instance of a layer class.\n\n                For example\n\n                .. code-block:: python\n\n                    ""type"": ""Conv1D""                               # class name\n                    ""type"": ""texar.torch.core.MaxReducePooling1D""  # module path\n                    ""type"": ""my_module.MyLayer""                    # module path\n                    ""type"": torch.nn.Module.Linear                 # class\n                    ""type"": Conv1D(filters=10, kernel_size=2)  # cell instance\n                    ""type"": MyLayer(...)                       # cell instance\n\n            `""kwargs""`: dict\n                A dictionary of keyword arguments for constructor of the\n                layer class. Ignored if :attr:`""type""` is a layer instance.\n\n                - Arguments named ""activation"" can be a callable, or a `str` of\n                  the name or module path to the activation function.\n                - Arguments named ""\\*_regularizer"" and ""\\*_initializer"" can be a\n                  class instance, or a `dict` of hyperparameters of respective\n                  regularizers and initializers. See\n                - Arguments named ""\\*_constraint"" can be a callable, or a `str`\n                  of the name or full path to the constraint function.\n\n    Returns:\n        A layer instance. If ``hparams[""type""]`` is a layer instance, returns it\n        directly.\n\n    Raises:\n        ValueError: If :attr:`hparams` is `None`.\n        ValueError: If the resulting layer is not an instance of\n            :torch_nn:`Module`.\n    """"""\n    if hparams is None:\n        raise ValueError(""`hparams` must not be `None`."")\n\n    layer_type = hparams[""type""]\n    if not is_str(layer_type) and not isinstance(layer_type, type):\n        layer = layer_type\n    else:\n        layer_modules = [""torch.nn"", ""texar.torch.core"", ""texar.torch.custom""]\n        layer_class: Type[nn.Module] = utils.check_or_get_class(\n            layer_type, layer_modules)\n        if isinstance(hparams, dict):\n            if (layer_class.__name__ == ""Linear"" and\n                    ""in_features"" not in hparams[""kwargs""]):\n                raise ValueError(""\\""in_features\\"" should be specified for ""\n                                 ""\\""torch.nn.{}\\"""".format(layer_class.__name__))\n            elif (layer_class.__name__ in [""Conv1d"", ""Conv2d"", ""Conv3d""] and\n                  ""in_channels"" not in hparams[""kwargs""]):\n                raise ValueError(""\\""in_channels\\"" should be specified for ""\n                                 ""\\""torch.nn.{}\\"""".format(layer_class.__name__))\n            default_kwargs: Dict[str, Any] = {}\n            default_hparams = {""type"": layer_type, ""kwargs"": default_kwargs}\n            hparams = HParams(hparams, default_hparams)\n\n        # this case needs to be handled separately because nn.Sequential\n        # does not accept kwargs\n        if layer_type == ""Sequential"":\n            names: List[str] = []\n            layer = nn.Sequential()\n            sub_hparams = hparams.kwargs.layers\n            for hparam in sub_hparams:\n                sub_layer = get_layer(hparam)\n                name = utils.uniquify_str(sub_layer.__class__.__name__, names)\n                names.append(name)\n                layer.add_module(name=name, module=sub_layer)\n        else:\n            layer = utils.get_instance(layer_type, hparams.kwargs.todict(),\n                                       layer_modules)\n\n    if not isinstance(layer, nn.Module):\n        raise ValueError(""layer must be an instance of `torch.nn.Module`."")\n\n    return layer\n\n\nclass MaxReducePool1d(nn.Module):\n    r""""""A subclass of :torch_nn:`Module`.\n    Max Pool layer for 1D inputs. The same as :torch_nn:`MaxPool1d` except that\n    the pooling dimension is entirely reduced (i.e., `pool_size=input_length`).\n    """"""\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor) -> torch.Tensor:\n        output, _ = torch.max(input, dim=2)\n        return output\n\n\nclass AvgReducePool1d(nn.Module):\n    r""""""A subclass of :torch_nn:`Module`.\n    Avg Pool layer for 1D inputs. The same as :torch_nn:`AvgPool1d` except that\n    the pooling dimension is entirely reduced (i.e., `pool_size=input_length`).\n    """"""\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor) -> torch.Tensor:\n        return torch.mean(input, dim=2)\n\n\n_POOLING_TO_REDUCE = {\n    ""MaxPool1d"": ""MaxReducePool1d"",\n    ""AvgPool1d"": ""AvgReducePool1d"",\n    torch.nn.MaxPool1d: MaxReducePool1d,\n    torch.nn.AvgPool1d: AvgReducePool1d\n}\n\n\ndef get_pooling_layer_hparams(hparams: Union[HParams, Dict[str, Any]]) \\\n        -> Dict[str, Any]:\n    r""""""Creates pooling layer hyperparameters `dict` for :func:`get_layer`.\n\n    If the :attr:`hparams` sets `\'pool_size\'` to `None`, the layer will be\n    changed to the respective reduce-pooling layer. For example,\n    :torch_docs:`torch.conv.MaxPool1d <nn.html#torch.nn.Conv1d>` is replaced\n    with :class:`~texar.torch.core.MaxReducePool1d`.\n    """"""\n    if isinstance(hparams, HParams):\n        hparams = hparams.todict()\n\n    new_hparams = copy.copy(hparams)\n    kwargs = new_hparams.get(\'kwargs\', None)\n\n    if kwargs and kwargs.get(\'kernel_size\', None) is None:\n        pool_type = hparams[\'type\']\n        new_hparams[\'type\'] = _POOLING_TO_REDUCE.get(pool_type, pool_type)\n        kwargs.pop(\'kernel_size\', None)\n        kwargs.pop(\'stride\', None)\n        kwargs.pop(\'padding\', None)\n\n    return new_hparams\n\n\nclass MergeLayer(nn.Module):\n    r""""""A subclass of :torch_nn:`Module`.\n    A layer that consists of multiple layers in parallel. Input is fed to\n    each of the parallel layers, and the outputs are merged with a\n    specified mode.\n\n    Args:\n        layers (list, optional): A list of :torch_docs:`torch.nn.Module\n            <nn.html#module>` instances, or a list of hyperparameter\n            dictionaries each of which specifies `""type""` and `""kwargs""` of each\n            layer (see the `hparams` argument of :func:`get_layer`).\n\n            If `None`, this layer degenerates to a merging operator that merges\n            inputs directly.\n        mode (str): Mode of the merge op. This can be:\n\n            - :attr:`\'concat\'`: Concatenates layer outputs along one dim.\n              Tensors must have the same shape except for the dimension\n              specified in `dim`, which can have different sizes.\n            - :attr:`\'elemwise_sum\'`: Outputs element-wise sum.\n            - :attr:`\'elemwise_mul\'`: Outputs element-wise product.\n            - :attr:`\'sum\'`: Computes the sum of layer outputs along the\n              dimension given by `dim`. For example, given `dim=1`,\n              two tensors of shape `[a, b]` and `[a, c]` respectively\n              will result in a merged tensor of shape `[a]`.\n            - :attr:`\'mean\'`: Computes the mean of layer outputs along the\n              dimension given in `dim`.\n            - :attr:`\'prod\'`: Computes the product of layer outputs along the\n              dimension given in `dim`.\n            - :attr:`\'max\'`: Computes the maximum of layer outputs along the\n              dimension given in `dim`.\n            - :attr:`\'min\'`: Computes the minimum of layer outputs along the\n              dimension given in `dim`.\n            - :attr:`\'and\'`: Computes the `logical and` of layer outputs along\n              the dimension given in `dim`.\n            - :attr:`\'or\'`: Computes the `logical or` of layer outputs along\n              the dimension given in `dim`.\n            - :attr:`\'logsumexp\'`: Computes\n              log(sum(exp(elements across the dimension of layer outputs)))\n        dim (int): The dim to use in merging. Ignored in modes\n            :attr:`\'elemwise_sum\'` and :attr:`\'elemwise_mul\'`.\n    """"""\n\n    _functions: Dict[str, Callable[[torch.Tensor, int], torch.Tensor]] = {\n        ""sum"": torch.sum,\n        ""mean"": torch.mean,\n        ""prod"": torch.prod,\n        ""max"": lambda tensors, dim: torch.max(tensors, dim)[0],\n        ""min"": lambda tensors, dim: torch.min(tensors, dim)[0],\n        ""and"": torch.all,\n        ""or"": torch.any,\n        ""logsumexp"": torch.logsumexp\n    }\n\n    def __init__(self, layers: Optional[List[nn.Module]] = None,\n                 mode: str = \'concat\', dim: Optional[int] = None):\n        super().__init__()\n        self._mode = mode\n        self._dim = dim\n\n        self._layers: Optional[nn.ModuleList] = None\n        if layers is not None:\n            if len(layers) == 0:\n                raise ValueError(\n                    ""\'layers\' must be either None or a non-empty list."")\n            self._layers = nn.ModuleList()\n            for layer in layers:\n                if isinstance(layer, nn.Module):\n                    self._layers.append(layer)\n                else:\n                    self._layers.append(get_layer(hparams=layer))\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        r""""""Feed input to every containing layer and merge the outputs.\n\n        Args:\n            input: The input tensor.\n\n        Returns:\n            The merged tensor.\n        """"""\n        layer_outputs: List[torch.Tensor]\n        if self._layers is None:\n            layer_outputs = input\n            if not isinstance(layer_outputs, (list, tuple)):\n                layer_outputs = [layer_outputs]\n        else:\n            layer_outputs = []\n            for layer in self._layers:\n                layer_output = layer(input)\n                layer_outputs.append(layer_output)\n\n        # the merge dimension cannot be determined until we get the output from\n        # individual layers.\n        # In case of reduce pooling operations, feature dim is removed and\n        # channel dim is merged.\n        # In non-reduce pooling operations, feature dim is merged.\n        dim = self._dim if self._dim is not None else -1\n\n        if self._mode == \'concat\':\n            outputs = torch.cat(tensors=layer_outputs, dim=dim)\n        elif self._mode == \'elemwise_sum\':\n            outputs = layer_outputs[0]\n            for i in range(1, len(layer_outputs)):\n                outputs = torch.add(outputs, layer_outputs[i])\n        elif self._mode == \'elemwise_mul\':\n            outputs = layer_outputs[0]\n            for i in range(1, len(layer_outputs)):\n                outputs = torch.mul(outputs, layer_outputs[i])\n        elif self._mode in self._functions:\n            _concat = torch.cat(tensors=layer_outputs, dim=dim)\n            outputs = self._functions[self._mode](_concat, dim)\n        else:\n            raise ValueError(""Unknown merge mode: \'%s\'"" % self._mode)\n\n        return outputs\n\n    @property\n    def layers(self) -> Optional[nn.ModuleList]:\n        r""""""The list of parallel layers.\n        """"""\n        return self._layers\n\n\nclass Flatten(nn.Module):\n    r""""""Flatten layer to flatten a tensor after convolution.""""""\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor) -> torch.Tensor:\n        return input.view(input.size()[0], -1)\n\n\nclass Identity(nn.Module):\n    r""""""Identity activation layer.""""""\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor) -> torch.Tensor:\n        return input\n'"
texar/torch/core/optimization.py,45,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious optimization related utilities.\n""""""\n\nimport functools\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\nfrom mypy_extensions import TypedDict\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.optimizer import Optimizer\n\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.utils import utils\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""default_optimization_hparams"",\n    ""get_optimizer"",\n    ""get_scheduler"",\n    ""get_grad_clip_fn"",\n    ""get_train_op"",\n    ""BertAdam""\n]\n\n\ndef default_optimization_hparams() -> Dict[str, Any]:\n    r""""""Returns a `dict` of default hyperparameters of training op\n    and their default values\n\n    .. code-block:: python\n\n        {\n            ""optimizer"": {\n                ""type"": ""Adam"",\n                ""kwargs"": {\n                    ""lr"": 0.001\n                }\n            },\n            ""learning_rate_decay"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""gradient_clip"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""gradient_noise_scale"": None,\n            ""name"": None\n        }\n\n    Here:\n\n    `""optimizer""`: dict\n        Hyperparameters of a\n        :torch_docs:`torch.optim.Optimizer <optim.html#torch.optim.Optimizer>`.\n\n        - `""type""` specifies the optimizer class. This can be\n\n          - The string name or full module path of an optimizer class.\n            If the class name is provided, the class must be in module\n            :torch_docs:`torch.optim <optim.html>` or :mod:`texar.torch.custom`,\n            :mod:`texar.torch.core.optimization`\n          - An optimizer class.\n          - An instance of an optimizer class.\n\n          For example\n\n          .. code-block:: python\n\n              ""type"": ""Adam""                    # class name\n              ""type"": ""my_module.MyOptimizer""   # module path\n              ""type"": texar.torch.custom.BertAdam     # class\n              ""type"": my_module.MyOptimizer     # class\n\n        - `""kwargs""` is a `dict` specifying keyword arguments for creating\n          the optimizer class instance, with :python:`opt_class(**kwargs)`.\n          Ignored if `""type""` is a class instance.\n\n    `""learning_rate_decay""`: dict\n        Hyperparameters of learning rate decay function. The learning rate\n        starts decay from :attr:`""start_decay_step""` and keeps unchanged after\n        :attr:`""end_decay_step""` or reaching :attr:`""min_learning_rate""`.\n\n        The decay function is specified in `""type""` and `""kwargs""`.\n\n        - `""type""` can be a decay function or its name or module path. If\n          function name is provided, it must be from module\n          :torch_docs:`torch.optim <optim.html>` or :mod:`texar.torch.custom`,\n          :mod:`texar.torch.core.optimization`.\n\n        - `""kwargs""` is a `dict` of keyword arguments for the function\n          excluding arguments named `""global_step""` and `""learning_rate""`.\n\n        The function is called with\n        :python:`lr = decay_fn(learning_rate=lr, global_step=offset_step,\n        **kwargs)`, where `offset_step` is the global step offset as above.\n\n    `""gradient_clip""`: dict\n        Hyperparameters of gradient clipping. The gradient clipping function\n        takes a list of `(gradients, variables)` tuples and returns a list\n        of `(clipped_gradients, variables)` tuples. Typical examples include\n        :torch_nn:`utils.clip_grad_norm_` and\n        :torch_nn:`utils.clip_grad_value_`.\n\n        ""type"" specifies the gradient clip function, and can be a function,\n        or its name or module path. If function name is provided, the\n        function must be from module :mod:`torch.nn.utils`,\n        :mod:`texar.torch.custom`, or :mod:`texar.torch.core.optimization`.\n\n        `""kwargs""` specifies keyword arguments to the function, except arguments\n        named `""parameters""`.\n\n    `""gradient_noise_scale""`: float, optional\n        Adds 0-mean normal noise scaled by this value to gradient.\n    """"""\n    return {\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""kwargs"": {\n                ""lr"": 0.001\n            }\n        },\n        ""learning_rate_decay"": {\n            ""type"": """",\n            ""kwargs"": {}\n        },\n        ""gradient_clip"": {\n            ""type"": """",\n            ""kwargs"": {}\n        },\n        ""gradient_noise_scale"": None,\n        # TODO(zhiting): allow module-level control of gradient_multipliers\n        ""name"": None\n    }\n\n\ndef get_optimizer(\n        params: Iterable[Union[torch.Tensor, Dict[str, Any]]],\n        hparams: Optional[Union[HParams, Dict[str, Any]]] = None) -> \\\n        Optimizer:\n    r""""""Creates a optimizer instance.\n\n    Args:\n        params: an iterable of :class:`torch.Tensor` or\n            :class:`dict`. Specifies what Tensors should be optimized.\n        hparams (dict or HParams, optional): hyperparameters. Missing\n            hyperparameters are set to default values automatically. See\n            :func:`~texar.torch.core.default_optimization_hparams` for\n            all hyperparameters and default values.\n\n    :return:\n        The :torch_docs:`torch.optim.Optimizer\n        <optim.html#torch.optim.Optimizer>` instance specified in\n        :attr:`hparams`.\n    """"""\n    if hparams is None or isinstance(hparams, dict):\n        hparams = HParams(hparams, default_optimization_hparams())\n\n    hparams_opt = hparams[""optimizer""]\n\n    optimizer_type = hparams_opt[""type""]\n    if isinstance(optimizer_type, Optimizer):\n        optimizer_class = optimizer_type\n    else:\n        optimizer_modules = [\'torch.optim\',\n                             \'texar.torch.custom\']\n        try:\n            optimizer_class = utils.check_or_get_class(  # type: ignore\n                optimizer_type, optimizer_modules, Optimizer)\n        except TypeError:\n            raise ValueError(\n                ""Unrecognized optimizer. Must be string name of the ""\n                ""optimizer class, or the class which is a subclass of ""\n                ""torch.optim.Optimizer, or an instance of the subclass of ""\n                ""Optimizer."")\n\n    optimizer_kwargs = hparams_opt[""kwargs""].todict()\n    optimizer_kwargs.update({""params"": params})\n    optimizer = optimizer_class(**optimizer_kwargs)  # type: ignore\n\n    return optimizer\n\n\ndef get_scheduler(optimizer: Optimizer,\n                  hparams: Optional[Union[HParams, Dict[str, Any]]] = None) -> \\\n        Optional[_LRScheduler]:\n    r""""""Creates a scheduler instance.\n\n    Args:\n        optimizer: A :torch_docs:`torch.optim.Optimizer\n            <optim.html#torch.optim.Optimizer>` instance.\n        hparams (dict or HParams, optional): hyperparameters. Missing\n            hyperparameters are set to default values automatically. See\n            :func:`~texar.torch.core.default_optimization_hparams` for\n            all hyperparameters and default values.\n\n    :return:\n        A :torch_docs:`torch.optim.lr_scheduler._LRScheduler\n        <optim.html#how-to-adjust-learning-rate>` instance.\n    """"""\n    if hparams is None or isinstance(hparams, dict):\n        hparams = HParams(hparams, default_optimization_hparams())\n\n    hparams_scheduler = hparams[""learning_rate_decay""]\n\n    scheduler_type = hparams_scheduler[""type""]\n    if scheduler_type == """" or scheduler_type is None:\n        scheduler = None\n    else:\n        if isinstance(scheduler_type, _LRScheduler):\n            scheduler_class = scheduler_type\n        else:\n            scheduler_modules = [\'torch.optim.lr_scheduler\',\n                                 \'texar.torch.custom\']\n            try:\n                scheduler_class = utils.check_or_get_class(  # type: ignore\n                    scheduler_type, scheduler_modules, _LRScheduler)\n            except TypeError:\n                raise ValueError(\n                    ""Unrecognized lr_scheduler. Must be string name of the ""\n                    ""lr_scheduler class, or the class which is a subclass of ""\n                    ""torch.optim._LRScheduler."")\n\n        scheduler_kwargs = hparams_scheduler[""kwargs""].todict()\n        scheduler_kwargs.update({""optimizer"": optimizer})\n        scheduler = scheduler_class(**scheduler_kwargs)  # type: ignore\n\n    return scheduler\n\n\ndef get_grad_clip_fn(hparams: Optional[Union[HParams,\n                                             Dict[str, Any]]] = None) -> \\\n        Optional[Callable[[torch.Tensor], Optional[torch.Tensor]]]:\n    r""""""Create a gradient clipping function.\n\n    Args:\n        hparams (dict or HParams, optional): hyperparameters. Missing\n            hyperparameters are set to default values automatically. See\n            :func:`~texar.torch.core.default_optimization_hparams` for\n            all hyperparameters and default values.\n\n    Returns:\n        A gradient clipping function.\n    """"""\n    if hparams is None or isinstance(hparams, dict):\n        hparams = HParams(hparams, default_optimization_hparams())\n\n    hparams_grad_clip = hparams[""gradient_clip""]\n\n    grad_clip_type = hparams_grad_clip[""type""]\n    if grad_clip_type == """" or grad_clip_type is None:\n        grad_clip_fn = None\n    else:\n        grad_clip_modules = [\'torch.nn.utils\',\n                             \'texar.torch.custom\']\n        grad_clip_fn = utils.get_function(grad_clip_type, grad_clip_modules)\n        grad_clip_fn_kwargs = hparams_grad_clip[""kwargs""].todict()\n        grad_clip_fn = functools.partial(grad_clip_fn, **grad_clip_fn_kwargs)\n\n    return grad_clip_fn\n\n\ndef get_train_op(params: Optional[Iterable[Union[torch.Tensor,\n                                                 Dict[str, Any]]]] = None,\n                 optimizer: Optional[Optimizer] = None,\n                 scheduler: Optional[_LRScheduler] = None,\n                 hparams: Optional[Union[HParams, Dict[str, Any]]] = None) -> \\\n        Callable[[], None]:\n    r""""""Creates a training op.\n\n    Args:\n        params: an iterable of :class:`torch.Tensor` or\n            :class:`dict`. Specifies what Tensors should be optimized.\n        optimizer: A :torch_docs:`torch.optim.Optimizer\n            <optim.html#torch.optim.Optimizer>` instance.\n        scheduler: A :torch_docs:`torch.optim.lr_scheduler._LRScheduler\n            <optim.html#how-to-adjust-learning-rate>` instance.\n        hparams (dict or HParams, optional): hyperparameters. Missing\n            hyperparameters are set to default values automatically. See\n            :func:`~texar.torch.core.default_optimization_hparams` for\n            all hyperparameters and default values.\n\n    Returns:\n        The callable used for variable optimization.\n    """"""\n    hparams = HParams(hparams, default_optimization_hparams())\n\n    if params is None and optimizer is None and scheduler is None:\n        raise ValueError(""\'params\', \'optimizer\' and \'scheduler\' must not be ""\n                         ""None simultaneously."")\n\n    if scheduler is None:\n        if optimizer is None and params is not None:\n            optimizer = get_optimizer(params, hparams)\n        if optimizer is not None:\n            scheduler = get_scheduler(optimizer, hparams)\n    else:\n        optimizer = scheduler.optimizer  # type: ignore\n\n    grad_clip_fn = get_grad_clip_fn(hparams)\n\n    # TODO: Support per-parameter options in the future.\n    params_list: List[nn.Parameter] = []\n    for param_group in optimizer.param_groups:  # type: ignore\n        params = param_group[""params""]\n        if isinstance(params, torch.Tensor):\n            params_list.append(params)\n        elif isinstance(params, list):\n            params_list += params\n\n    def _train_op():\n        if grad_clip_fn is not None:\n            grad_clip_fn(parameters=params_list)\n        optimizer.step()\n        # TODO: Ideally, scheduler should be used in the epoch level.\n        if scheduler is not None:\n            scheduler.step()\n        optimizer.zero_grad()\n\n    return _train_op\n\n\nclass BertAdamParamDict(TypedDict):\n    r""""""The :attr:`param_groups` dictionary used in PyTorch optimizers.""""""\n    params: List[nn.Parameter]\n    lr: float\n    betas: Tuple[float, float]\n    eps: float\n    weight_decay: float\n    max_grad_norm: float\n\n\nclass BertAdamStateDict(TypedDict):\n    r""""""The :attr:`state` dictionary used in :class:`BertAdam` optimizer.""""""\n    next_m: torch.Tensor\n    next_v: torch.Tensor\n\n\nOptimParamType = Union[\n    MaybeList[Iterable[nn.Parameter]],  # model.parameters()\n    MaybeList[Dict[str, Any]],  # {""params"": ..., ""other_kwargs"": ...}\n]\n\n\nclass BertAdam(Optimizer):\n    r""""""Implements BERT version of Adam algorithm with weight decay fix.\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        max_grad_norm: Maximum norm for the gradients (-1 means no clipping).\n            Default: 1.0\n    """"""\n\n    param_groups: List[BertAdamParamDict]\n    state: Dict[nn.Parameter, BertAdamStateDict]\n\n    def __init__(self, params: OptimParamType,\n                 lr: float = 0.001, betas: Tuple[float, float] = (0.9, 0.999),\n                 eps: float = 1e-08, weight_decay: float = 0,\n                 max_grad_norm: float = 1.0):\n\n        if lr < 0.0:\n            raise ValueError(f""Invalid learning rate: {lr}"")\n        if eps < 0.0:\n            raise ValueError(f""Invalid epsilon value: {eps}"")\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f""Invalid beta parameter at index 0: {betas[0]}"")\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f""Invalid beta parameter at index 1: {betas[1]}"")\n\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, max_grad_norm=max_grad_norm)\n        super().__init__(params, defaults)  # type: ignore\n\n    def step(self, closure: Optional[Callable[[], float]] = None):\n        r""""""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        ""Adam does not support sparse gradients, please ""\n                        ""consider SparseAdam instead"")\n\n                state = self.state[p]\n                # State initialization\n                if len(state) == 0:\n                    # Exponential moving average of gradient values\n                    state[\'next_m\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'next_v\'] = torch.zeros_like(p.data)\n\n                next_m, next_v = state[\'next_m\'], state[\'next_v\']\n                beta1, beta2 = group[\'betas\']\n\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                next_m.mul_(beta1).add_(1 - beta1, grad)\n                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                update = next_m / (next_v.sqrt() + group[\'eps\'])\n\n                # Just adding the square of the weights to the loss function is\n                # *not* # the correct way of using L2 regularization or weight\n                # decay with Adam, since that will interact with the m and v\n                # parameters in strange ways.\n                #\n                # Instead we want to decay the weights in a manner that doesn\'t\n                # interact with the m/v parameters. This is equivalent to adding\n                # the square of the weights to the loss with plain\n                # (non-momentum) SGD.\n                if group[\'weight_decay\'] > 0.0:\n                    update += group[\'weight_decay\'] * p.data\n\n                lr = group[\'lr\']\n                update_with_lr = lr * update\n                p.data.add_(-update_with_lr)\n\n                # No bias correction\n                # bias_correction1 = 1 - beta1 ** state[\'step\']\n                # bias_correction2 = 1 - beta2 ** state[\'step\']\n\n        return loss\n'"
texar/torch/core/regularizers.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nRegularizers\n""""""\n\n# pylint: disable=redefined-outer-name\n\nfrom typing import Dict, Union\n\nimport torch\n\n__all__ = [\n    \'Regularizer\',\n    \'L1L2\',\n    \'l1\',\n    \'l2\',\n    \'l1_l2\',\n]\n\n\nclass Regularizer:\n    r""""""Regularizer base class.\n    """"""\n\n    def __call__(self, x):\n        return 0.\n\n    @classmethod\n    def from_config(cls, config):\n        r""""""Construct a :class:`Regularizer` instance given configurations.\n        """"""\n        return cls(**config)\n\n    def get_config(self) -> Dict[str, float]:\n        r""""""Return a Dict with configurations for the current regularizer\n        instance.\n        """"""\n        raise NotImplementedError\n\n\nclass L1L2(Regularizer):\n    r""""""Regularizer for L1 and L2 regularization.\n\n    Args:\n        l1: Float or Int; L1 regularization factor.\n        l2: Float or Int; L2 regularization factor.\n    """"""\n\n    def __init__(self, l1: Union[int, float] = 0., l2: Union[int, float] = 0.):\n        self.l1 = float(l1)\n        self.l2 = float(l2)\n\n    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n        regularization = torch.tensor(0.)\n        if self.l1:\n            regularization += torch.sum(self.l1 * torch.abs(x).float())\n        if self.l2:\n            regularization += torch.sum(self.l2 * (x ** 2).float())\n        return regularization\n\n    def get_config(self) -> Dict[str, float]:\n        return {\'l1\': float(self.l1), \'l2\': float(self.l2)}\n\n\ndef l1(l: Union[int, float] = 0.01) -> Regularizer:\n    r""""""\n    Construct an L1 regularizer.\n\n    Args:\n        l: Float or Int\n            L1 regularization factor.\n    Returns:\n        An L1L2 regularization instance with l1=l\n    """"""\n    return L1L2(l1=l)\n\n\ndef l2(l: Union[int, float] = 0.01) -> Regularizer:\n    r""""""\n    Construct an L2 regularizer.\n\n    Args:\n        l: Float or Int\n            L2 regularization factor.\n    Returns:\n        An L1L2 regularization instance with l2=l\n    """"""\n    return L1L2(l2=l)\n\n\ndef l1_l2(l1: Union[int, float] = 0.01,\n          l2: Union[int, float] = 0.01) -> Regularizer:\n    r""""""\n    Construct a regularizer with both L1 and L2 components.\n\n    Args:\n        l1: Float or Int\n            L1 regularization factor.\n        l2: Float or Int\n            L2 regularization factor.\n    Returns:\n        An L1L2 regularization instance with above regularization factors\n    """"""\n    return L1L2(l1=l1, l2=l2)\n'"
texar/torch/custom/__init__.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nCustom modules in Texar\n""""""\n\nfrom texar.torch.custom.activation import *\nfrom texar.torch.custom.initializers import *\nfrom texar.torch.custom.distributions import *\n'"
texar/torch/custom/activation.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nCustom activation functions used in various methods.\n""""""\n\nimport math\n\nimport torch\nfrom torch import nn\n\n\nclass BertGELU(nn.Module):\n    r""""""Bert uses GELU as the activation function for the position-wise network.\n    """"""\n\n    def forward(self, x):\n        return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\n\nclass GPTGELU(nn.Module):\n    r""""""For information: OpenAI GPT\'s GELU is slightly different (and gives\n    slightly different results).\n    """"""\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n'"
texar/torch/custom/distributions.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom torch.distributions import Normal, Independent\n\n\ndef MultivariateNormalDiag(loc, scale_diag):\n    if loc.dim() < 1:\n        raise ValueError(""loc must be at least one-dimensional."")\n    return Independent(Normal(loc, scale_diag), 1)\n'"
texar/torch/custom/initializers.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nCustom initializers used in various methods.\n""""""\n\nimport math\n\nimport torch\n\n\ndef variance_scaling_initializer(inputs: torch.Tensor,\n                                 factor: float = 2.0, mode: str = \'FAN_IN\',\n                                 uniform: bool = False):\n    r""""""Returns an initializer that generates tensors without scaling variance.\n    When initializing a deep network, it is in principle advantageous to keep\n    the scale of the input variance constant, so it does not explode or diminish\n    by reaching the final layer. This initializer use the following formula:\n    ```python\n        if mode=\'FAN_IN\': # Count only number of input connections.\n        n = fan_in\n        elif mode=\'FAN_OUT\': # Count only number of output connections.\n        n = fan_out\n        elif mode=\'FAN_AVG\': # Average number of inputs and output connections.\n        n = (fan_in + fan_out)/2.0\n        truncated_normal(shape, 0.0, stddev=sqrt(factor / n))\n    ```\n    * To get [Delving Deep into Rectifiers](\n        http://arxiv.org/pdf/1502.01852v1.pdf) (also know as the ""MSRA\n        initialization""), use (Default):<br/>\n        `factor=2.0 mode=\'FAN_IN\' uniform=False`\n    * To get [Convolutional Architecture for Fast Feature Embedding](\n        http://arxiv.org/abs/1408.5093), use:<br/>\n        `factor=1.0 mode=\'FAN_IN\' uniform=True`\n    * To get [Understanding the difficulty of training deep feed-forward\n        neural networks](\n        http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf),\n        use:<br/>\n        `factor=1.0 mode=\'FAN_AVG\' uniform=True.`\n    * To get `xavier_initializer` use either:<br/>\n        `factor=1.0 mode=\'FAN_AVG\' uniform=True`, or<br/>\n        `factor=1.0 mode=\'FAN_AVG\' uniform=False`.\n    Args:\n        factor: Float.  A multiplicative factor.\n        mode: String.  \'FAN_IN\', \'FAN_OUT\', \'FAN_AVG\'.\n        uniform: Whether to use uniform or normal distributed\n                 random initialization.\n    Returns:\n        An initializer that generates tensors with unit variance.\n    Raises:\n        ValueError: if `dtype` is not a floating point type.\n        TypeError: if `mode` is not in [\'FAN_IN\', \'FAN_OUT\', \'FAN_AVG\'].\n    """"""\n\n    # Estimating fan_in and fan_out is not possible to do perfectly, but we\n    # try. This is the right thing for matrix multiply and convolutions.\n    shape = inputs.size()\n    fan_in = float(shape[-2]) if len(shape) > 1 else float(shape[-1])\n    fan_out = float(shape[-1])\n    for dim in shape[:-2]:\n        fan_in *= float(dim)\n        fan_out *= float(dim)\n\n    if mode == \'FAN_IN\':\n        # Count only number of input connections.\n        n = fan_in\n    elif mode == \'FAN_OUT\':\n        # Count only number of output connections.\n        n = fan_out\n    elif mode == \'FAN_AVG\':\n        # Average number of inputs and output connections.\n        n = (fan_in + fan_out) / 2.0\n    else:\n        raise ValueError(f""Unknown mode {mode} [FAN_IN, FAN_OUT, FAN_AVG]"")\n\n    if uniform:\n        # To get stddev = math.sqrt(factor / n) need to\n        # adjust for uniform.\n        limit = math.sqrt(3.0 * factor / n)\n        inputs.data.uniform_(-limit, limit)\n    else:\n        # To get stddev = math.sqrt(factor / n) need to\n        # adjust for truncated normal.\n        trunc_stddev = math.sqrt(1.3 * factor / n)\n\n        u1 = torch.rand(shape) * (1 - math.exp(-2)) + math.exp(-2)\n        u2 = torch.rand(shape)\n        rnd = torch.sqrt(-2 * torch.log(u1)) * torch.cos(2 * math.pi * u2)\n\n        ret = rnd * trunc_stddev\n        inputs.data = ret\n'"
texar/torch/data/__init__.py,5,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library data.\n""""""\nfrom texar.torch.data.data import *\nfrom texar.torch.data.tokenizers import *\nfrom texar.torch.data.data_utils import *\nfrom texar.torch.data.embedding import *\nfrom texar.torch.data.vocabulary import *\n'"
texar/torch/data/data_utils.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious utilities specific to data processing.\n""""""\nimport collections\nimport logging\nimport os\nimport sys\nimport tarfile\nimport urllib.request\nimport zipfile\nfrom typing import List, Optional, overload, Union, Dict, Tuple\n\nimport numpy as np\n\nfrom texar.torch.utils import utils_io\nfrom texar.torch.utils.types import MaybeList, MaybeTuple, PathLike\n\n__all__ = [\n    ""maybe_download"",\n    ""read_words"",\n    ""make_vocab"",\n    ""count_file_lines"",\n    ""get_filename""\n]\n\nPy3 = sys.version_info[0] == 3\n\n\n# TODO: Remove these once pylint supports function stubs.\n# pylint: disable=unused-argument,function-redefined,missing-docstring\n\n@overload\ndef maybe_download(urls: List[str], path: PathLike,\n                   filenames: Optional[List[str]] = None,\n                   extract: bool = False) -> List[str]: ...\n\n\n@overload\ndef maybe_download(urls: str, path: PathLike, filenames: Optional[str] = None,\n                   extract: bool = False) -> str: ...\n\n\ndef maybe_download(urls, path, filenames=None, extract=False):\n    r""""""Downloads a set of files.\n\n    Args:\n        urls: A (list of) URLs to download files.\n        path (str): The destination path to save the files.\n        filenames: A (list of) strings of the file names. If given,\n            must have the same length with :attr:`urls`. If `None`,\n            filenames are extracted from :attr:`urls`.\n        extract (bool): Whether to extract compressed files.\n\n    Returns:\n        A list of paths to the downloaded files.\n    """"""\n    utils_io.maybe_create_dir(path)\n\n    if not isinstance(urls, (list, tuple)):\n        is_list = False\n        urls = [urls]\n    else:\n        is_list = True\n    if filenames is not None:\n        if not isinstance(filenames, (list, tuple)):\n            filenames = [filenames]\n        if len(urls) != len(filenames):\n            raise ValueError(\n                \'`filenames` must have the same number of elements as `urls`.\')\n\n    result = []\n    for i, url in enumerate(urls):\n        if filenames is not None:\n            filename = filenames[i]\n        elif \'drive.google.com\' in url:\n            filename = _extract_google_drive_file_id(url)\n        else:\n            filename = url.split(\'/\')[-1]\n            # If downloading from GitHub, remove suffix ?raw=True\n            # from local filename\n            if filename.endswith(""?raw=true""):\n                filename = filename[:-9]\n\n        filepath = os.path.join(path, filename)\n        result.append(filepath)\n\n        # if not tf.gfile.Exists(filepath):\n        if not os.path.exists(filepath):\n            if \'drive.google.com\' in url:\n                filepath = _download_from_google_drive(url, filename, path)\n            else:\n                filepath = _download(url, filename, path)\n\n            if extract:\n                logging.info(\'Extract %s\', filepath)\n                if tarfile.is_tarfile(filepath):\n                    tarfile.open(filepath, \'r\').extractall(path)\n                elif zipfile.is_zipfile(filepath):\n                    with zipfile.ZipFile(filepath) as zfile:\n                        zfile.extractall(path)\n                else:\n                    logging.info(""Unknown compression type. Only .tar.gz""\n                                 "".tar.bz2, .tar, and .zip are supported"")\n    if not is_list:\n        return result[0]\n    return result\n\n\n# pylint: enable=unused-argument,function-redefined,missing-docstring\n\n\ndef _download(url: str, filename: str, path: str) -> str:\n    def _progress_hook(count, block_size, total_size):\n        percent = float(count * block_size) / float(total_size) * 100.\n        sys.stdout.write(f\'\\r>> Downloading {filename} {percent:.1f}%\')\n        sys.stdout.flush()\n\n    filepath = os.path.join(path, filename)\n    filepath, _ = urllib.request.urlretrieve(url, filepath, _progress_hook)\n    print()\n    statinfo = os.stat(filepath)\n    print(f\'Successfully downloaded {filename} {statinfo.st_size} bytes\')\n\n    return filepath\n\n\ndef _extract_google_drive_file_id(url: str) -> str:\n    # id is between `/d/` and \'/\'\n    url_suffix = url[url.find(\'/d/\') + 3:]\n    if url_suffix.find(\'/\') == -1:\n        # if there\'s no trailing \'/\'\n        return url_suffix\n    file_id = url_suffix[:url_suffix.find(\'/\')]\n    return file_id\n\n\ndef _download_from_google_drive(url: str, filename: str, path: str) -> str:\n    r""""""Adapted from `https://github.com/saurabhshri/gdrive-downloader`\n    """"""\n\n    try:\n        import requests\n    except ImportError:\n        print(""The requests library must be installed to download files from ""\n              ""Google drive. Please see: https://github.com/psf/requests"")\n        raise\n\n    def _get_confirm_token(response):\n        for key, value in response.cookies.items():\n            if key.startswith(\'download_warning\'):\n                return value\n        return None\n\n    file_id = _extract_google_drive_file_id(url)\n\n    gurl = ""https://docs.google.com/uc?export=download""\n    sess = requests.Session()\n    response = sess.get(gurl, params={\'id\': file_id}, stream=True)\n    token = _get_confirm_token(response)\n\n    if token:\n        params = {\'id\': file_id, \'confirm\': token}\n        response = sess.get(gurl, params=params, stream=True)\n\n    filepath = os.path.join(path, filename)\n    CHUNK_SIZE = 32768\n    with open(filepath, ""wb"") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk:\n                f.write(chunk)\n\n    print(f\'Successfully downloaded {filename}\')\n\n    return filepath\n\n\ndef read_words(filename: str, newline_token: Optional[str] = None) -> List[str]:\n    r""""""Reads word from a file.\n\n    Args:\n        filename (str): Path to the file.\n        newline_token (str, optional): The token to replace the original newline\n            token ""\\\\n"". For example, :python:`tx.data.SpecialTokens.EOS`.\n            If `None`, no replacement is performed.\n\n    Returns:\n        A list of words.\n    """"""\n    with open(filename, ""r"") as f:\n        if Py3:\n            if newline_token is None:\n                return f.read().split()\n            else:\n                return f.read().replace(""\\n"", newline_token).split()\n        else:\n            if newline_token is None:\n                return f.read().split()\n            else:\n                return f.read().replace(""\\n"", newline_token).split()\n\n\n# TODO: Remove these once pylint supports function stubs.\n# pylint: disable=unused-argument,function-redefined,missing-docstring\n\n# A saner overloaded version with default arguments...\n@overload\ndef make_vocab(filenames: MaybeList[str], max_vocab_size: int = -1,\n               newline_token: Optional[str] = None) -> List[str]: ...\n\n\n# ... and an insane version.\n@overload\ndef make_vocab(filenames: MaybeList[str], max_vocab_size: int = -1,\n               newline_token: Optional[str] = None,\n               return_type: str = ""list"", return_count: bool = False) \\\n        -> Union[Union[List[str], Tuple[List[str], List[int]]],\n                 MaybeTuple[Dict[str, int]]]: ...\n\n\ndef make_vocab(filenames, max_vocab_size=-1, newline_token=None,\n               return_type=""list"", return_count=False):\n    r""""""Builds vocab of the files.\n\n    Args:\n        filenames (str): A (list of) files.\n        max_vocab_size (int): Maximum size of the vocabulary. Low frequency\n            words that exceeding the limit will be discarded.\n            Set to `-1` (default) if no truncation is wanted.\n        newline_token (str, optional): The token to replace the original newline\n            token ""\\\\n"". For example, :python:`tx.data.SpecialTokens.EOS`.\n            If `None`, no replacement is performed.\n        return_type (str): Either ``list`` or ``dict``. If ``list`` (default),\n            this function returns a list of words sorted by frequency. If\n            ``dict``, this function returns a dict mapping words to their index\n            sorted by frequency.\n        return_count (bool): Whether to return word counts. If `True` and\n            :attr:`return_type` is ``dict``, then a count dict is returned,\n            which is a mapping from words to their frequency.\n\n    Returns:\n        - If :attr:`return_count` is False, returns a list or dict containing\n          the vocabulary words.\n\n        - If :attr:`return_count` if True, returns a pair of list or dict\n          `(a, b)`, where `a` is a list or dict containing the vocabulary\n          words, `b` is a list or dict containing the word counts.\n    """"""\n\n    if not isinstance(filenames, (list, tuple)):\n        filenames = [filenames]\n\n    words: List[str] = []\n    for fn in filenames:\n        words += read_words(fn, newline_token=newline_token)\n\n    counter = collections.Counter(words)\n    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n    words, counts = list(zip(*count_pairs))\n    words: List[str]\n    counts: List[int]\n    if max_vocab_size >= 0:\n        words = words[:max_vocab_size]\n    counts = counts[:max_vocab_size]\n\n    if return_type == ""list"":\n        if not return_count:\n            return words\n        else:\n            return words, counts\n    elif return_type == ""dict"":\n        word_to_id = dict(zip(words, range(len(words))))\n        if not return_count:\n            return word_to_id\n        else:\n            word_to_count = dict(zip(words, counts))\n            return word_to_id, word_to_count\n    else:\n        raise ValueError(f""Unknown return_type: {return_type}"")\n\n\n# pylint: enable=unused-argument,function-redefined,missing-docstring\n\ndef count_file_lines(filenames: MaybeList[str]) -> int:\n    r""""""Counts the number of lines in the file(s).\n    """"""\n\n    def _count_lines(fn):\n        with open(fn, ""rb"") as f:\n            i = -1\n            for i, _ in enumerate(f):\n                pass\n            return i + 1\n\n    if not isinstance(filenames, (list, tuple)):\n        filenames = [filenames]\n    num_lines = np.sum([_count_lines(fn) for fn in filenames]).item()\n    return num_lines\n\n\ndef get_filename(url: str) -> str:\n    r""""""Extracts the filename of the downloaded checkpoint file from the URL.\n    """"""\n    if \'drive.google.com\' in url:\n        return _extract_google_drive_file_id(url)\n    url, filename = os.path.split(url)\n    return filename or os.path.basename(url)\n'"
texar/torch/data/embedding.py,9,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nHelper functions and classes for embedding processing.\n""""""\nfrom typing import Callable, Dict\n\nimport numpy as np\n\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.utils import utils\n\n__all__ = [\n    ""load_word2vec"",\n    ""load_glove"",\n    ""Embedding"",\n]\n\n\ndef load_word2vec(filename: str, vocab: Dict[str, int],\n                  word_vecs: np.ndarray) -> np.ndarray:\n    r""""""Loads embeddings in the word2vec binary format which has a header line\n    containing the number of vectors and their dimensionality (two integers),\n    followed with number-of-vectors lines each of which is formatted as\n    ``<word-string> <embedding-vector>``.\n\n    Args:\n        filename (str): Path to the embedding file.\n        vocab (dict): A dictionary that maps token strings to integer index.\n            Tokens not in :attr:`vocab` are not read.\n        word_vecs: A 2D numpy array of shape `[vocab_size, embed_dim]`\n            which is updated as reading from the file.\n\n    Returns:\n        The updated :attr:`word_vecs`.\n    """"""\n    with open(filename, ""rb"") as fin:\n        header = fin.readline()\n        vocab_size, vector_size = [int(s) for s in header.split()]\n        if vector_size != word_vecs.shape[1]:\n            raise ValueError(""Inconsistent word vector sizes: %d vs %d"" %\n                             (vector_size, word_vecs.shape[1]))\n        binary_len = np.dtype(\'float32\').itemsize * vector_size\n        for _ in np.arange(vocab_size):\n            chars = []\n            while True:\n                char = fin.read(1)\n                if char == b\' \':\n                    break\n                if char != b\'\\n\':\n                    chars.append(char)\n            word = b\'\'.join(chars).decode(\'utf-8\')\n            if word in vocab:\n                word_vecs[vocab[word]] = np.frombuffer(\n                    fin.read(binary_len), dtype=\'float32\')\n            else:\n                fin.read(binary_len)\n    return word_vecs\n\n\ndef load_glove(filename: str, vocab: Dict[str, int],\n               word_vecs: np.ndarray) -> np.ndarray:\n    r""""""Loads embeddings in the glove text format in which each line is\n    ``<word-string> <embedding-vector>``. Dimensions of the embedding vector\n    are separated with whitespace characters.\n\n    Args:\n        filename (str): Path to the embedding file.\n        vocab (dict): A dictionary that maps token strings to integer index.\n            Tokens not in :attr:`vocab` are not read.\n        word_vecs: A 2D numpy array of shape `[vocab_size, embed_dim]`\n            which is updated as reading from the file.\n\n    Returns:\n        The updated :attr:`word_vecs`.\n    """"""\n    with open(filename) as fin:\n        for line in fin:\n            vec = line.strip().split()\n            if len(vec) == 0:\n                continue\n            word, vec = vec[0], vec[1:]\n            if word not in vocab:\n                continue\n            if len(vec) != word_vecs.shape[1]:\n                raise ValueError(""Inconsistent word vector sizes: %d vs %d"" %\n                                 (len(vec), word_vecs.shape[1]))\n            word_vecs[vocab[word]] = np.array([float(v) for v in vec])\n    return word_vecs\n\n\nclass Embedding:\n    r""""""Embedding class that loads token embedding vectors from file. Token\n    embeddings not in the embedding file are initialized as specified in\n    :attr:`hparams`.\n\n    Args:\n        vocab (dict): A dictionary that maps token strings to integer index.\n        hparams (dict): Hyperparameters. See :meth:`default_hparams` for the\n            defaults.\n    """"""\n\n    def __init__(self, vocab: Dict[str, int],\n                 hparams=None):\n        self._hparams = HParams(hparams, self.default_hparams())\n\n        # Initialize embeddings\n        init_fn_kwargs = self._hparams.init_fn.kwargs.todict()\n        if ""shape"" in init_fn_kwargs or ""size"" in init_fn_kwargs:\n            raise ValueError(""Argument \'shape\' or \'size\' must not be ""\n                             ""specified. They are inferred automatically."")\n        init_fn: Callable[..., np.ndarray]\n        init_fn = utils.get_function(\n            self._hparams.init_fn.type,\n            [""numpy.random"", ""numpy"", ""texar.torch.custom""])\n\n        try:\n            self._word_vecs = init_fn(  # type: ignore\n                size=[len(vocab), self._hparams.dim], **init_fn_kwargs)\n        except TypeError:\n            self._word_vecs = init_fn(  # type: ignore\n                shape=[len(vocab), self._hparams.dim], **init_fn_kwargs)\n\n        # Optionally read embeddings from file\n        if self._hparams.file is not None and self._hparams.file != """":\n            read_fn: Callable[[str, Dict[str, int], np.ndarray], np.ndarray]\n            read_fn = utils.get_function(  # type: ignore\n                self._hparams.read_fn,\n                [""texar.torch.data.embedding"", ""texar.torch.data"",\n                 ""texar.torch.custom""])\n\n            self._word_vecs = read_fn(self._hparams.file,\n                                      vocab, self._word_vecs)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values:\n\n        .. code-block:: python\n\n            {\n                ""file"": """",\n                ""dim"": 50,\n                ""read_fn"": ""load_word2vec"",\n                ""init_fn"": {\n                    ""type"": ""numpy.random.uniform"",\n                    ""kwargs"": {\n                        ""low"": -0.1,\n                        ""high"": 0.1,\n                    }\n                },\n            }\n\n        Here:\n\n        `""file""`: str\n            Path to the embedding file. If not provided, all embeddings are\n            initialized with the initialization function.\n\n        `""dim""`: int\n            Dimension size of each embedding vector\n\n        `""read_fn""`: str or callable\n            Function to read the embedding file. This can be the function,\n            or its string name or full module path. For example,\n\n            .. code-block:: python\n\n                ""read_fn"": texar.torch.data.load_word2vec\n                ""read_fn"": ""load_word2vec""\n                ""read_fn"": ""texar.torch.data.load_word2vec""\n                ""read_fn"": ""my_module.my_read_fn""\n\n            If function string name is used, the function must be in\n            one of the modules: :mod:`texar.torch.data` or\n            :mod:`texar.torch.custom`.\n\n            The function must have the same signature as with\n            :func:`load_word2vec`.\n\n        `""init_fn""`: dict\n            Hyperparameters of the initialization function used to initialize\n            embedding of tokens missing in the embedding\n            file.\n\n            The function must accept argument named `size` or `shape` to\n            specify the output shape, and return a numpy array of the shape.\n\n            The `dict` has the following fields:\n\n            `""type""`: str or callable\n                The initialization function. Can be either the function,\n                or its string name or full module path.\n\n            `""kwargs""`: dict\n                Keyword arguments for calling the function. The function\n                is called with :python:`init_fn(size=[.., ..], **kwargs)`.\n        """"""\n        return {\n            ""file"": """",\n            ""dim"": 50,\n            ""read_fn"": ""load_word2vec"",\n            ""init_fn"": {\n                ""type"": ""numpy.random.uniform"",\n                ""kwargs"": {\n                    ""low"": -0.1,\n                    ""high"": 0.1,\n                },\n            },\n            ""@no_typecheck"": [""read_fn"", ""init_fn""]\n        }\n\n    @property\n    def word_vecs(self):\n        r""""""2D numpy array of shape `[vocab_size, embedding_dim]`.\n        """"""\n        return self._word_vecs\n\n    @property\n    def vector_size(self):\n        r""""""The embedding dimension size.\n        """"""\n        return self._hparams.dim\n'"
texar/torch/data/vocabulary.py,5,"b'# -*- coding: utf-8 -*-\n# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nHelper functions and classes for vocabulary processing.\n""""""\nimport warnings\nfrom collections import defaultdict\nfrom typing import DefaultDict, Dict, List, Optional, Sequence, Tuple, Union\n\nimport numpy as np\n\nfrom texar.torch.utils.utils import (\n    _recur_split, dict_lookup, str_join, strip_special_tokens)\n\n__all__ = [\n    ""SpecialTokens"",\n    ""Vocab"",\n    ""map_ids_to_strs"",\n]\n\n\nclass SpecialTokens:\n    r""""""Special tokens, including :attr:`PAD`, :attr:`BOS`, :attr:`EOS`,\n    :attr:`UNK`. These tokens will by default have token ids 0, 1, 2, 3,\n    respectively.\n    """"""\n    PAD = ""<PAD>""\n    BOS = ""<BOS>""\n    EOS = ""<EOS>""\n    UNK = ""<UNK>""\n\n\ndef _make_defaultdict(keys: Sequence[Union[int, str]],\n                      values: Sequence[Union[int, str]],\n                      default_value: Union[int, str]) \\\n        -> DefaultDict[Union[int, str], Union[int, str]]:\n    r""""""Creates a Python `defaultdict`.\n\n    Args:\n        keys (list): Keys of the dictionary.\n        values (list): Values correspond to keys. The two lists :attr:`keys` and\n            :attr:`values` must be of the same length.\n        default_value: default value returned when key is missing.\n\n    Returns:\n        defaultdict: A Python `defaultdict` instance that maps keys to values.\n    """"""\n    dict_: DefaultDict[Union[int, str], Union[int, str]]\n    dict_ = defaultdict(lambda: default_value)\n    for k, v in zip(keys, values):\n        dict_[k] = v\n    return dict_\n\n\nclass Vocab:\n    r""""""Vocabulary class that loads vocabulary from file, and maintains mapping\n    tables between token strings and indexes.\n\n    Each line of the vocab file should contains one vocabulary token, e.g.::\n\n        vocab_token_1\n        vocab token 2\n        vocab       token | 3 .\n        ...\n\n    Args:\n        filename (str): Path to the vocabulary file where each line contains\n            one token.\n        bos_token (str): A special token that will be added to the beginning of\n            sequences.\n        eos_token (str): A special token that will be added to the end of\n            sequences.\n        unk_token (str): A special token that will replace all unknown tokens\n            (tokens not included in the vocabulary).\n        pad_token (str): A special token that is used to do padding.\n    """"""\n\n    def __init__(self,\n                 filename: str,\n                 pad_token: str = SpecialTokens.PAD,\n                 bos_token: str = SpecialTokens.BOS,\n                 eos_token: str = SpecialTokens.EOS,\n                 unk_token: str = SpecialTokens.UNK):\n        self._filename = filename\n        self._pad_token = pad_token\n        self._bos_token = bos_token\n        self._eos_token = eos_token\n        self._unk_token = unk_token\n\n        self._id_to_token_map_py, self._token_to_id_map_py \\\n            = self.load(self._filename)\n\n    def load(self, filename: str) \\\n            -> Tuple[Dict[int, str], Dict[str, int]]:\n        r""""""Loads the vocabulary from the file.\n\n        Args:\n            filename (str): Path to the vocabulary file.\n\n        Returns:\n            A tuple of mapping tables between word string and\n            index, (:attr:`id_to_token_map_py`, :attr:`token_to_id_map_py`),\n            where and :attr:`token_to_id_map_py` are python `defaultdict`\n            instances.\n        """"""\n        with open(filename, ""r"") as vocab_file:\n            vocab = list(line.strip() for line in vocab_file)\n\n        warnings.simplefilter(""ignore"", UnicodeWarning)\n\n        if self._bos_token in vocab:\n            raise ValueError(""Special begin-of-seq token already exists in the ""\n                             ""vocabulary: \'%s\'"" % self._bos_token)\n        if self._eos_token in vocab:\n            raise ValueError(""Special end-of-seq token already exists in the ""\n                             ""vocabulary: \'%s\'"" % self._eos_token)\n        if self._unk_token in vocab:\n            raise ValueError(""Special UNK token already exists in the ""\n                             ""vocabulary: \'%s\'"" % self._unk_token)\n        if self._pad_token in vocab:\n            raise ValueError(""Special padding token already exists in the ""\n                             ""vocabulary: \'%s\'"" % self._pad_token)\n\n        warnings.simplefilter(""default"", UnicodeWarning)\n\n        # Places _pad_token at the beginning to make sure it take index 0.\n        vocab = [self._pad_token, self._bos_token, self._eos_token,\n                 self._unk_token] + vocab\n        # Must make sure this is consistent with the above line\n        vocab_size = len(vocab)\n\n        # Creates python maps to interface with python code\n        id_to_token_map_py = dict(zip(range(vocab_size), vocab))\n        token_to_id_map_py = dict(zip(vocab, range(vocab_size)))\n\n        return id_to_token_map_py, token_to_id_map_py\n\n    def map_ids_to_tokens_py(self, ids: Union[List[int], np.ndarray]) \\\n            -> np.ndarray:\n        r""""""Maps ids into text tokens.\n\n        The input :attr:`ids` and returned tokens are both python\n        arrays or list.\n\n        Args:\n            ids: An `int` numpy array or (possibly nested) list of token ids.\n\n        Returns:\n            A numpy array of text tokens of the same shape as :attr:`ids`.\n        """"""\n        return dict_lookup(self.id_to_token_map_py, ids, self.unk_token)\n\n    def map_tokens_to_ids_py(self, tokens: List[str]) -> np.ndarray:\n        r""""""Maps text tokens into ids.\n\n        The input :attr:`tokens` and returned ids are both python\n        arrays or list.\n\n        Args:\n            tokens: A numpy array or (possibly nested) list of text tokens.\n\n        Returns:\n            A numpy array of token ids of the same shape as :attr:`tokens`.\n        """"""\n        return dict_lookup(self.token_to_id_map_py, tokens, self.unk_token_id)\n\n    @property\n    def id_to_token_map_py(self) -> Dict[int, str]:\n        r""""""The dictionary instance that maps from token index to the string\n        form.\n        """"""\n        return self._id_to_token_map_py\n\n    @property\n    def token_to_id_map_py(self) -> Dict[str, int]:\n        r""""""The dictionary instance that maps from token string to the index.\n        """"""\n        return self._token_to_id_map_py\n\n    @property\n    def size(self) -> int:\n        r""""""The vocabulary size.\n        """"""\n        return len(self.token_to_id_map_py)\n\n    @property\n    def bos_token(self) -> str:\n        r""""""A string of the special token indicating the beginning of sequence.\n        """"""\n        return self._bos_token\n\n    @property\n    def bos_token_id(self) -> int:\n        r""""""The `int` index of the special token indicating the beginning\n        of sequence.\n        """"""\n        return self.token_to_id_map_py[self._bos_token]\n\n    @property\n    def eos_token(self) -> str:\n        r""""""A string of the special token indicating the end of sequence.\n        """"""\n        return self._eos_token\n\n    @property\n    def eos_token_id(self) -> int:\n        r""""""The `int` index of the special token indicating the end\n        of sequence.\n        """"""\n        return self.token_to_id_map_py[self._eos_token]\n\n    @property\n    def unk_token(self) -> str:\n        r""""""A string of the special token indicating unknown token.\n        """"""\n        return self._unk_token\n\n    @property\n    def unk_token_id(self) -> int:\n        r""""""The `int` index of the special token indicating unknown token.\n        """"""\n        return self.token_to_id_map_py[self._unk_token]\n\n    @property\n    def pad_token(self) -> str:\n        r""""""A string of the special token indicating padding token. The\n        default padding token is an empty string.\n        """"""\n        return self._pad_token\n\n    @property\n    def pad_token_id(self) -> int:\n        r""""""The `int` index of the special token indicating padding token.\n        """"""\n        return self.token_to_id_map_py[self._pad_token]\n\n    @property\n    def special_tokens(self) -> List[str]:\n        r""""""The list of special tokens\n        [:attr:`pad_token`, :attr:`bos_token`, :attr:`eos_token`,\n        :attr:`unk_token`].\n        """"""\n        return [self._pad_token, self._bos_token, self._eos_token,\n                self._unk_token]\n\n\ndef map_ids_to_strs(ids: Union[np.ndarray, Sequence[int]],\n                    vocab: Vocab,\n                    join: bool = True, strip_pad: Optional[str] = \'<PAD>\',\n                    strip_bos: Optional[str] = \'<BOS>\',\n                    strip_eos: Optional[str] = \'<EOS>\') \\\n        -> Union[np.ndarray, List[str]]:\n    r""""""Transforms ``int`` indexes to strings by mapping ids to tokens,\n    concatenating tokens into sentences, and stripping special tokens, etc.\n\n    Args:\n        ids: An n-D numpy array or (possibly nested) list of ``int`` indexes.\n        vocab: An instance of :class:`~texar.torch.data.Vocab`.\n        join (bool): Whether to concatenate along the last dimension of the\n            the tokens into a string separated with a space character.\n        strip_pad (str): The PAD token to strip from the strings (i.e., remove\n            the leading and trailing PAD tokens of the strings). Default\n            is ``""<PAD>""`` as defined in\n            :class:`~texar.torch.data.SpecialTokens`.PAD.\n            Set to `None` or `False` to disable the stripping.\n        strip_bos (str): The BOS token to strip from the strings (i.e., remove\n            the leading BOS tokens of the strings).\n            Default is ``""<BOS>""`` as defined in\n            :class:`~texar.torch.data.SpecialTokens`.BOS.\n            Set to `None` or `False` to disable the stripping.\n        strip_eos (str): The EOS token to strip from the strings (i.e., remove\n            the EOS tokens and all subsequent tokens of the strings).\n            Default is ``""<EOS>""`` as defined in\n            :class:`~texar.torch.data.SpecialTokens`.EOS.\n            Set to `None` or `False` to disable the stripping.\n\n    Returns:\n        If :attr:`join` is True, returns a `(n-1)`-D numpy array (or list) of\n        concatenated strings. If :attr:`join` is False, returns an `n`-D numpy\n        array (or list) of str tokens.\n\n    Example:\n\n        .. code-block:: python\n\n            text_ids = [[1, 9, 6, 2, 0, 0], [1, 28, 7, 8, 2, 0]]\n\n            text = map_ids_to_strs(text_ids, data.vocab)\n            # text == [\'a sentence\', \'parsed from ids\']\n\n            text = map_ids_to_strs(\n                text_ids, data.vocab, join=False,\n                strip_pad=None, strip_bos=None, strip_eos=None)\n            # text == [[\'<BOS>\', \'a\', \'sentence\', \'<EOS>\', \'<PAD>\', \'<PAD>\'],\n            #          [\'<BOS>\', \'parsed\', \'from\', \'ids\', \'<EOS>\', \'<PAD>\']]\n    """"""\n    tokens = vocab.map_ids_to_tokens_py(ids)\n    if isinstance(ids, (list, tuple)):\n        tokens = tokens.tolist()\n\n    str_ = str_join(tokens)\n\n    str_ = strip_special_tokens(\n        str_, strip_pad=strip_pad, strip_bos=strip_bos, strip_eos=strip_eos)\n\n    if join:\n        return str_\n    else:\n        return _recur_split(str_, ids)  # type: ignore\n'"
texar/torch/evals/__init__.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar evals.\n""""""\n\nfrom texar.torch.evals.bleu import *\nfrom texar.torch.evals.bleu_moses import *\nfrom texar.torch.evals.bleu_transformer import *\nfrom texar.torch.evals.metrics import *\n'"
texar/torch/evals/bleu.py,2,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Modifications copyright (C) 2019 Texar\n# ==============================================================================\n""""""\nPython implementation of BLEU and smoothed BLEU adapted from:\n    `https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py`\n\nThis module provides a Python implementation of BLEU and smoothed BLEU.\nSmooth BLEU is computed following the method outlined in the paper:\n\n    (Lin et al. 2004) ORANGE: a method for evaluating automatic evaluation\n    metrics for machine translation.\n    Chin-Yew Lin, Franz Josef Och. COLING 2004.\n""""""\n\nimport collections\nimport math\nfrom typing import Counter, List, Tuple, Union\n\nfrom texar.torch.utils.dtypes import compat_as_text\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""sentence_bleu"",\n    ""corpus_bleu"",\n]\n\n\ndef _get_ngrams(segment: MaybeList[str],\n                max_order: int) -> Counter[Tuple[str, ...]]:\n    r""""""Extracts all n-grams up to a given maximum order from an\n    input segment.\n\n    Args:\n        segment: text segment from which n-grams will be extracted.\n        max_order: maximum length in tokens of the n-grams returned\n            by this methods.\n\n    Returns:\n        The Counter containing all n-grams upto :attr:`max_order`\n        in segment with a count of how many times each n-gram occurred.\n    """"""\n    ngram_counts: collections.Counter = collections.Counter()\n    for order in range(1, max_order + 1):\n        for i in range(0, len(segment) - order + 1):\n            ngram = tuple(segment[i:i + order])\n            ngram_counts[ngram] += 1\n    return ngram_counts\n\n\ndef _maybe_str_to_list(list_or_str: Union[str, List[str]]) -> List[str]:\n    if isinstance(list_or_str, str):\n        return list_or_str.split()\n    return list_or_str\n\n\ndef _lowercase(str_list: List[str]) -> List[str]:\n    return [str_.lower() for str_ in str_list]\n\n\ndef sentence_bleu(references: List[MaybeList[str]],\n                  hypothesis: MaybeList[str],\n                  max_order: int = 4,\n                  lowercase: bool = False,\n                  smooth: bool = False,\n                  use_bp: bool = True,\n                  return_all: bool = False) -> MaybeList[float]:\n    r""""""Calculates BLEU score of a hypothesis sentence.\n\n    Args:\n        references: A list of reference for the hypothesis.\n            Each reference can be either a list of string tokens, or a string\n            containing tokenized tokens separated with whitespaces.\n            List can also be numpy array.\n        hypothesis: A hypothesis sentence.\n            Each hypothesis can be either a list of string tokens, or a\n            string containing tokenized tokens separated with whitespaces.\n            List can also be numpy array.\n        lowercase (bool): If `True`, lowercase reference and hypothesis\n            tokens.\n        max_order (int): Maximum n-gram order to use when computing\n            BLEU score.\n        smooth (bool): Whether or not to apply `(Lin et al. 2004)` smoothing.\n        use_bp (bool): Whether to apply brevity penalty.\n        return_all (bool): If `True`, returns BLEU and all\n            n-gram precisions.\n\n    Returns:\n        If :attr:`return_all` is `False` (default), returns a float32\n        BLEU score.\n\n        If :attr:`return_all` is `True`, returns a list of float32\n        ``[BLEU] + n-gram precisions``, which is of length :attr:`max_order`\n        +1.\n    """"""\n    return corpus_bleu([references],\n                       [hypothesis],\n                       max_order=max_order,\n                       lowercase=lowercase,\n                       smooth=smooth,\n                       use_bp=use_bp,\n                       return_all=return_all)\n\n\ndef corpus_bleu(list_of_references: List[List[MaybeList[str]]],\n                hypotheses: List[MaybeList[str]],\n                max_order: int = 4,\n                lowercase: bool = False,\n                smooth: bool = False,\n                use_bp: bool = True,\n                return_all: bool = False) -> MaybeList[float]:\n    r""""""Computes corpus-level BLEU score.\n\n    Args:\n        list_of_references: A list of lists of references for each hypothesis.\n            Each reference can be either a list of string tokens, or a string\n            containing tokenized tokens separated with whitespaces.\n            List can also be numpy array.\n        hypotheses: A list of hypothesis sentences.\n            Each hypothesis can be either a list of string tokens, or a\n            string containing tokenized tokens separated with whitespaces.\n            List can also be numpy array.\n        lowercase (bool): If `True`, lowercase reference and hypothesis\n            tokens.\n        max_order (int): Maximum n-gram order to use when computing\n            BLEU score.\n        smooth (bool): Whether or not to apply `(Lin et al. 2004)` smoothing.\n        use_bp (bool): Whether to apply brevity penalty.\n        return_all (bool): If `True`, returns BLEU and all\n            n-gram precisions.\n\n    Returns:\n        If :attr:`return_all` is `False` (default), returns a ``float32``\n        BLEU score.\n\n        If :attr:`return_all` is `True`, returns a list of\n        ``float32`` scores: ``[BLEU] + n-gram precisions``,\n        which is of length :attr:`max_order` +1.\n    """"""\n    list_of_references = compat_as_text(list_of_references)\n    hypotheses = compat_as_text(hypotheses)\n\n    matches_by_order = [0] * max_order\n    possible_matches_by_order = [0] * max_order\n    reference_length = 0\n    hypothesis_length = 0\n\n    for (references, hypothesis) in zip(list_of_references, hypotheses):\n        reference_length += min(len(r) for r in references)\n        hypothesis_length += len(hypothesis)\n\n        merged_ref_ngram_counts: Counter[Tuple[str, ...]] = \\\n            collections.Counter()\n        for reference in references:\n            reference = _maybe_str_to_list(reference)\n            if lowercase:\n                reference = _lowercase(reference)\n            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n\n        hypothesis = _maybe_str_to_list(hypothesis)\n        if lowercase:\n            hypothesis = _lowercase(hypothesis)\n        hypothesis_ngram_counts = _get_ngrams(hypothesis, max_order)\n\n        overlap = hypothesis_ngram_counts & merged_ref_ngram_counts\n        for ngram in overlap:\n            matches_by_order[len(ngram) - 1] += overlap[ngram]\n        for order in range(1, max_order + 1):\n            possible_matches = len(hypothesis) - order + 1\n            if possible_matches > 0:\n                possible_matches_by_order[order - 1] += possible_matches\n\n    precisions = [0.0] * max_order\n    for i in range(0, max_order):\n        if smooth:\n            precisions[i] = ((matches_by_order[i] + 1.) /\n                             (possible_matches_by_order[i] + 1.))\n        else:\n            if possible_matches_by_order[i] > 0:\n                precisions[i] = (float(matches_by_order[i]) /\n                                 possible_matches_by_order[i])\n            else:\n                precisions[i] = 0.0\n\n    if min(precisions) > 0:\n        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n        geo_mean = math.exp(p_log_sum)\n    else:\n        geo_mean = 0\n\n    if use_bp:\n        ratio = float(hypothesis_length) / reference_length\n        if ratio > 1.0:\n            bp = 1.\n        else:\n            if abs(ratio) < 1e-8:\n                bp = 0.\n            else:\n                bp = math.exp(1 - 1. / ratio)\n    else:\n        bp = 1.\n\n    bleu = geo_mean * bp\n\n    if return_all:\n        return [bleu * 100] + [p * 100 for p in precisions]\n    else:\n        return bleu * 100\n'"
texar/torch/evals/bleu_moses.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nThe BLEU metric.\n""""""\n\nimport logging\nimport os\nimport re\nimport shutil\nimport subprocess\nimport tempfile\nfrom typing import List\n\nimport numpy as np\n\nfrom texar.torch.utils.dtypes import compat_as_text\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""sentence_bleu_moses"",\n    ""corpus_bleu_moses"",\n]\n\n\ndef _maybe_list_to_str(list_or_str: MaybeList[str]) -> str:\n    if isinstance(list_or_str, (tuple, list, np.ndarray)):\n        return \' \'.join(list_or_str)\n    return list_or_str\n\n\ndef _parse_multi_bleu_ret(bleu_str: str,\n                          return_all: bool = False) -> MaybeList[float]:\n    bleu_score = re.search(r""BLEU = (.+?),"", bleu_str).group(1)  # type: ignore\n    bleu_score = np.float32(bleu_score)\n\n    if return_all:\n        bleus = re.search(r"", (.+?)/(.+?)/(.+?)/(.+?) "", bleu_str)\n        bleus = [bleus.group(group_idx)  # type: ignore\n                 for group_idx in range(1, 5)]\n        bleus = [np.float32(b) for b in bleus]\n        bleu_score = [bleu_score] + bleus\n\n    return bleu_score\n\n\ndef sentence_bleu_moses(references: List[MaybeList[str]],\n                        hypothesis: MaybeList[str],\n                        lowercase: bool = False,\n                        return_all: bool = False) -> MaybeList[float]:\n    r""""""Calculates BLEU score of a hypothesis sentence using the\n    **MOSES `multi-bleu.perl`** script.\n\n    Args:\n        references: A list of reference for the hypothesis.\n            Each reference can be either a string, or a list of string tokens.\n            List can also be numpy array.\n        hypothesis: A hypothesis sentence.\n            The hypothesis can be either a string, or a list of string tokens.\n            List can also be numpy array.\n        lowercase (bool): If `True`, pass the ``""-lc""`` flag to the\n            `multi-bleu` script.\n        return_all (bool): If `True`, returns BLEU and all n-gram\n            precisions.\n\n    Returns:\n        If :attr:`return_all` is `False` (default), returns a ``float32``\n        BLEU score.\n\n        If :attr:`return_all` is `True`, returns a list of 5 ``float32``\n        scores: ``[BLEU, 1-gram precision, ..., 4-gram precision]``.\n    """"""\n    return corpus_bleu_moses(\n        [references], [hypothesis], lowercase=lowercase, return_all=return_all)\n\n\ndef corpus_bleu_moses(list_of_references: List[List[MaybeList[str]]],\n                      hypotheses: List[MaybeList[str]],\n                      lowercase: bool = False,\n                      return_all: bool = False) -> MaybeList[float]:\n    r""""""Calculates corpus-level BLEU score using the\n    **MOSES `multi-bleu.perl`** script.\n\n    Args:\n        list_of_references: A list of lists of references for each hypothesis.\n            Each reference can be either a string, or a list of string tokens.\n            List can also be numpy array.\n        hypotheses: A list of hypothesis sentences.\n            Each hypothesis can be either a string, or a list of string tokens.\n            List can also be numpy array.\n        lowercase (bool): If `True`, pass the ``""-lc""`` flag to the\n            `multi-bleu` script.\n        return_all (bool): If `True`, returns BLEU and all\n            n-gram precisions.\n\n    Returns:\n        If :attr:`return_all` is `False` (default), returns a ``float32``\n        BLEU score.\n\n        If :attr:`return_all` is `True`, returns a list of 5 ``float32``\n        scores: ``[BLEU, 1-gram precision, ..., 4-gram precision]``.\n    """"""\n    list_of_references = compat_as_text(list_of_references)\n    hypotheses = compat_as_text(hypotheses)\n\n    if np.size(hypotheses) == 0:\n        return np.float32(0.)\n\n    # Get multi-bleu.perl\n    cur_dir = os.path.dirname(os.path.realpath(__file__))\n    multi_bleu_path = os.path.abspath(\n        os.path.join(cur_dir, "".."", "".."", "".."",\n                     ""bin"", ""utils"", ""multi-bleu.perl""))\n\n    # Create a temporary folder containing hypothesis and reference files\n    result_path = tempfile.mkdtemp()\n    # Create hypothesis file\n    hfile_path = os.path.join(result_path, \'hyp\')\n    hyps = [_maybe_list_to_str(h) for h in hypotheses]\n    with open(hfile_path, \'w\', encoding=\'utf-8\') as hfile:\n        text = ""\\n"".join(hyps)\n        hfile.write(text)\n        hfile.write(""\\n"")\n    # Create reference files\n    max_nrefs = max([len(refs) for refs in list_of_references])\n    rfile_path = os.path.join(result_path, \'ref\')\n    for rid in range(max_nrefs):\n        with open(rfile_path + str(rid), \'w\', encoding=\'utf-8\') as rfile:\n            for refs in list_of_references:\n                if rid < len(refs):\n                    ref = _maybe_list_to_str(refs[rid])\n                    rfile.write(ref + ""\\n"")\n                else:\n                    rfile.write(""\\n"")\n\n    # Calculate BLEU\n    multi_bleu_cmd = [multi_bleu_path]\n    if lowercase:\n        multi_bleu_cmd += [""-lc""]\n    multi_bleu_cmd += [rfile_path]\n    with open(hfile_path, ""r"") as hyp_input:\n        try:\n            multi_bleu_ret = subprocess.check_output(\n                multi_bleu_cmd, stdin=hyp_input, stderr=subprocess.STDOUT)\n            multi_bleu_ret_ = multi_bleu_ret.decode(""utf-8"")\n            bleu_score = _parse_multi_bleu_ret(multi_bleu_ret_, return_all)\n        except subprocess.CalledProcessError as error:\n            if error.output is not None:\n                logging.warning(""multi-bleu.perl returned non-zero exit code"")\n                logging.warning(error.output)\n            if return_all:\n                bleu_score = [np.float32(0.0)] * 5\n            else:\n                bleu_score = np.float32(0.0)\n\n    shutil.rmtree(result_path)\n\n    return np.float32(bleu_score)\n'"
texar/torch/evals/bleu_transformer.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPython implementation of BLEU adapted from:\n    `https://github.com/tensorflow/models/blob/master/official/transformer/compute_bleu.py`\n""""""\n\nfrom typing import Callable, Counter, List, Tuple\n\nimport re\nimport sys\nimport unicodedata\nimport collections\nimport math\nimport numpy as np\n\nfrom texar.torch.evals.bleu import corpus_bleu\nfrom texar.torch.evals.bleu_moses import corpus_bleu_moses\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""corpus_bleu_transformer"",\n    ""bleu_transformer_tokenize"",\n    ""file_bleu"",\n]\n\n\ndef _get_ngrams(segment: List[str],\n                max_order: int) -> Counter[Tuple[str, ...]]:\n    r""""""Extracts all n-grams up to a given maximum order from an\n    input segment.\n\n    Args:\n        segment: text segment from which n-grams will be extracted.\n        max_order: maximum length in tokens of the n-grams returned\n            by this methods.\n\n    Returns:\n        The Counter containing all n-grams upto :attr:`max_order`\n        in segment with a count of how many times each n-gram occurred.\n    """"""\n    ngram_counts: collections.Counter = collections.Counter()\n    for order in range(1, max_order + 1):\n        for i in range(0, len(segment) - order + 1):\n            ngram = tuple(segment[i:i + order])\n            ngram_counts[ngram] += 1\n    return ngram_counts\n\n\ndef corpus_bleu_transformer(reference_corpus: List[List[str]],\n                            translation_corpus: List[List[str]],\n                            max_order: int = 4,\n                            use_bp: bool = True) -> float:\n    r""""""Computes BLEU score of translated segments against references.\n\n    This BLEU has been used in evaluating Transformer (Vaswani et al.)\n    ""Attention is all you need"" for machine translation. The resulting BLEU\n    score are usually a bit higher than that in\n    `texar.torch.evals.corpus_bleu` and `texar.torch.evals.corpus_bleu_moses`.\n\n    Args:\n        reference_corpus: list of references for each translation. Each\n            reference should be tokenized into a list of tokens.\n        translation_corpus: list of translations to score. Each translation\n            should be tokenized into a list of tokens.\n        max_order: Maximum n-gram order to use when computing BLEU score.\n        use_bp: boolean, whether to apply brevity penalty.\n\n    Returns:\n        BLEU score.\n    """"""\n    reference_length = 0\n    translation_length = 0\n    bp = 1.0\n    geo_mean = 0.0\n\n    matches_by_order = [0] * max_order\n    possible_matches_by_order = [0] * max_order\n\n    for (references, translations) in zip(reference_corpus, translation_corpus):\n        reference_length += len(references)\n        translation_length += len(translations)\n        ref_ngram_counts = _get_ngrams(references, max_order)\n        translation_ngram_counts = _get_ngrams(translations, max_order)\n\n        overlap = dict(\n            (ngram, min(count, translation_ngram_counts[ngram]))\n            for ngram, count in ref_ngram_counts.items()\n        )\n\n        for ngram in overlap:\n            matches_by_order[len(ngram) - 1] += overlap[ngram]\n        for ngram in translation_ngram_counts:\n            possible_matches_by_order[len(ngram) - 1] += \\\n                translation_ngram_counts[ngram]\n\n    precisions = [0.0] * max_order\n    smooth = 1.0\n\n    for i in range(max_order):\n        if possible_matches_by_order[i] > 0:\n            precisions[i] = matches_by_order[i] / possible_matches_by_order[i]\n            if matches_by_order[i] > 0:\n                precisions[i] = (matches_by_order[i] /\n                                 possible_matches_by_order[i])\n            else:\n                smooth *= 2\n                precisions[i] = 1.0 / (smooth * possible_matches_by_order[i])\n        else:\n            precisions[i] = 0.0\n\n    if max(precisions) > 0:\n        p_log_sum = sum(math.log(p) for p in precisions if p)\n        geo_mean = math.exp(p_log_sum / max_order)\n\n    if use_bp:\n        ratio = translation_length / reference_length\n        if ratio <= 0:\n            bp = 0\n        elif ratio < 1.0:\n            bp = math.exp(1 - 1.0 / ratio)\n        else:\n            bp = 1.0\n\n    bleu = geo_mean * bp\n\n    return np.float32(bleu) * 100\n\n\nclass UnicodeRegex:\n    """"""Ad-hoc hack to recognize all punctuation and symbols.""""""\n\n    def __init__(self):\n        punctuation = self.property_chars(""P"")\n        self.nondigit_punct_re = re.compile(r""([^\\d])(["" + punctuation + r""])"")\n        self.punct_nondigit_re = re.compile(r""(["" + punctuation + r""])([^\\d])"")\n        self.symbol_re = re.compile(""(["" + self.property_chars(""S"") + ""])"")\n\n    @staticmethod\n    def property_chars(prefix):\n        return """".join(\n            chr(x)\n            for x in range(sys.maxunicode)\n            if unicodedata.category(chr(x)).startswith(prefix)\n        )\n\n\nuregex = UnicodeRegex()\n\n\ndef bleu_transformer_tokenize(string: str) -> List[str]:\n    r""""""Tokenize a string following the official BLEU implementation.\n\n    The BLEU scores from `multi-bleu.perl` depend on your `tokenizer`, which is\n    unlikely to be reproducible from your experiment or consistent across\n    different users. This function provides a standard tokenization following\n    `mteval-v14.pl`.\n\n    See\n    `https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v14.pl#L954-L983`.\n    In our case, the input string is expected to be just one line\n    and no HTML entities de-escaping is needed.\n    So we just tokenize on punctuation and symbols,\n    except when a punctuation is preceded and followed by a digit\n    (e.g. a comma/dot as a thousand/decimal separator).\n\n    Note that a number (e.g. a year) followed by a dot at the end of sentence\n    is NOT tokenized,\n    i.e. the dot stays with the number because `s/(\\p{P})(\\P{N})/ $1 $2/g`\n    does not match this case (unless we add a space after each sentence).\n    However, this error is already in the original `mteval-v14.pl`\n    and we want to be consistent with it.\n\n    Args:\n        string: the input string\n\n    Returns:\n        a list of tokens\n    """"""\n    string = uregex.nondigit_punct_re.sub(r""\\1 \\2 "", string)\n    string = uregex.punct_nondigit_re.sub(r"" \\1 \\2"", string)\n    string = uregex.symbol_re.sub(r"" \\1 "", string)\n    return string.split()\n\n\ndef file_bleu(ref_filename: str,\n              hyp_filename: str,\n              bleu_version: str = ""corpus_bleu_transformer"",\n              case_sensitive: bool = False) -> float:\n    r""""""Compute BLEU for two files (reference and hypothesis translation).\n\n    Args:\n        ref_filename: Reference file path.\n        hyp_filename: Hypothesis file path.\n        bleu_version: A str with the name of a BLEU computing method selected\n            in the list of: `corpus_bleu`, `corpus_bleu_moses`,\n            `corpus_bleu_transformer`.\n        case_sensitive: If `False`, lowercase reference and hypothesis\n            tokens.\n\n    Returns:\n        BLEU score.\n    """"""\n    with open(ref_filename, encoding=""utf-8"") as f:\n        ref_lines = f.read().splitlines()\n\n    with open(hyp_filename, encoding=""utf-8"") as f:\n        hyp_lines = f.read().splitlines()\n\n    if len(ref_lines) != len(hyp_lines):\n        raise ValueError(\n            ""Reference and translation files have different number of lines"")\n\n    if not case_sensitive:\n        ref_lines = [x.lower() for x in ref_lines]\n        hyp_lines = [x.lower() for x in hyp_lines]\n\n    ref_tokens: List[MaybeList[List[str]]]\n    if bleu_version == ""corpus_bleu_transformer"":\n        ref_tokens = [bleu_transformer_tokenize(x) for x in ref_lines]\n    else:\n        ref_tokens = [[bleu_transformer_tokenize(x)] for x in ref_lines]\n    hyp_tokens = [bleu_transformer_tokenize(x) for x in hyp_lines]\n\n    bleu_dict = {\n        ""corpus_bleu"": corpus_bleu,\n        ""corpus_bleu_moses"": corpus_bleu_moses,\n        ""corpus_bleu_transformer"": corpus_bleu_transformer\n    }\n    fn: Callable[[List[MaybeList[List[str]]], List[List[str]]], float]\n    fn = bleu_dict[bleu_version]  # type: ignore\n\n    return fn(ref_tokens, hyp_tokens)\n'"
texar/torch/evals/metrics.py,13,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious metrics.\n""""""\n\nfrom typing import Optional\n\nimport torch\n\n__all__ = [\n    ""accuracy"",\n    ""binary_clas_accuracy"",\n]\n\n\ndef accuracy(labels: torch.Tensor,\n             preds: torch.Tensor) -> torch.Tensor:\n    r""""""Calculates the accuracy of predictions.\n\n    Args:\n        labels: The ground truth values. A Tensor of the same shape of\n            :attr:`preds`.\n        preds: A Tensor of any shape containing the predicted values.\n\n    Returns:\n        A float scalar Tensor containing the accuracy.\n    """"""\n    labels = labels.type(preds.dtype).reshape(preds.shape)\n    return (labels == preds).float().mean()\n\n\ndef binary_clas_accuracy(pos_preds: Optional[torch.Tensor] = None,\n                         neg_preds: Optional[torch.Tensor] = None) -> \\\n        Optional[torch.Tensor]:\n    r""""""Calculates the accuracy of binary predictions.\n\n    Args:\n        pos_preds (optional): A Tensor of any shape containing the\n            predicted values on positive data (i.e., ground truth labels are 1).\n        neg_preds (optional): A Tensor of any shape containing the\n            predicted values on negative data (i.e., ground truth labels are 0).\n\n    Returns:\n        A float scalar Tensor containing the accuracy.\n    """"""\n    if pos_preds is None and neg_preds is None:\n        return None\n    if pos_preds is not None:\n        pos_accu = accuracy(torch.ones_like(pos_preds), pos_preds)\n        psize = float(torch.numel(pos_preds))\n    else:\n        pos_accu = torch.tensor(0.0)\n        psize = torch.tensor(0.0)\n    if neg_preds is not None:\n        neg_accu = accuracy(torch.zeros_like(neg_preds), neg_preds)\n        nsize = float(torch.numel(neg_preds))\n    else:\n        neg_accu = torch.tensor(0.0)\n        nsize = torch.tensor(0.0)\n    accu = (pos_accu * psize + neg_accu * nsize) / (psize + nsize)\n    return accu\n'"
texar/torch/losses/__init__.py,6,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar losses.\n""""""\n\nfrom texar.torch.losses.adv_losses import *\nfrom texar.torch.losses.entropy import *\nfrom texar.torch.losses.losses_utils import *\nfrom texar.torch.losses.mle_losses import *\nfrom texar.torch.losses.pg_losses import *\nfrom texar.torch.losses.rewards import *\n'"
texar/torch/losses/adv_losses.py,10,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nAdversarial losses.\n""""""\n\nfrom typing import Callable, Tuple\n\nimport torch\nimport torch.nn.functional as F\n\nfrom texar.torch.utils.types import MaybeTuple\n\n__all__ = [\n    \'binary_adversarial_losses\',\n]\n\n\ndef binary_adversarial_losses(\n        real_data: torch.Tensor,\n        fake_data: torch.Tensor,\n        discriminator_fn: Callable[[torch.Tensor], MaybeTuple[torch.Tensor]],\n        mode: str = ""max_real"") -> Tuple[torch.Tensor, torch.Tensor]:\n    r""""""Computes adversarial losses of real/fake binary discrimination game.\n\n    Example:\n\n    .. code-block:: python\n\n        # Using BERTClassifier as the discriminator, which can accept\n        # ""soft"" token ids for gradient backpropagation\n        discriminator = tx.modules.BERTClassifier(\'bert-base-uncased\')\n\n        G_loss, D_loss = tx.losses.binary_adversarial_losses(\n            real_data=real_token_ids,  # [batch_size, max_time]\n            fake_data=fake_soft_token_ids,  # [batch_size, max_time, vocab_size]\n            discriminator_fn=discriminator)\n\n    Args:\n        real_data (Tensor or array): Real data of shape\n            `[num_real_examples, ...]`.\n        fake_data (Tensor or array): Fake data of shape\n            `[num_fake_examples, ...]`. `num_real_examples` does not\n            necessarily equal `num_fake_examples`.\n        discriminator_fn: A callable takes data (e.g., :attr:`real_data` and\n            :attr:`fake_data`) and returns the logits of being real. The\n            signature of `discriminator_fn` must be:\n            :python:`logits, ... = discriminator_fn(data)`.\n            The return value of `discriminator_fn` can be the logits, or\n            a tuple where the logits are the first element.\n        mode (str): Mode of the generator loss. Either ""max_real"" or ""min_fake"".\n\n            - **""max_real""** (default): minimizing the generator loss is to\n              maximize the probability of fake data being classified as real.\n\n            - **""min_fake""**: minimizing the generator loss is to minimize the\n              probability of fake data being classified as fake.\n\n    Returns:\n        A tuple `(generator_loss, discriminator_loss)` each of which is\n        a scalar Tensor, loss to be minimized.\n    """"""\n    real_logits = discriminator_fn(real_data)\n    if isinstance(real_logits, (list, tuple)):\n        real_logits = real_logits[0]\n    real_loss = F.binary_cross_entropy_with_logits(\n        real_logits, torch.ones_like(real_logits))\n\n    fake_logits = discriminator_fn(fake_data)\n    if isinstance(fake_logits, (list, tuple)):\n        fake_logits = fake_logits[0]\n    fake_loss = F.binary_cross_entropy_with_logits(\n        fake_logits, torch.zeros_like(fake_logits))\n\n    d_loss = real_loss + fake_loss\n\n    if mode == ""min_fake"":\n        g_loss = -fake_loss\n    elif mode == ""max_real"":\n        bce_loss = torch.nn.BCEWithLogitsLoss(reduction=\'mean\')\n        g_loss = bce_loss(fake_logits, torch.ones_like(fake_logits))\n    else:\n        raise ValueError(""Unknown mode: %s. Only \'min_fake\' and \'max_real\' ""\n                         ""are allowed."")\n\n    return g_loss, d_loss\n'"
texar/torch/losses/entropy.py,11,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious entropies.\n""""""\n\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom texar.torch.losses.losses_utils import mask_and_reduce, reduce_dimensions\nfrom texar.torch.utils.shapes import get_rank\n\n__all__ = [\n    ""entropy_with_logits"",\n    ""sequence_entropy_with_logits"",\n]\n\n\ndef _get_entropy(logits: torch.Tensor) -> torch.Tensor:\n    r""""""Compute entropy according to the definition.\n\n    Args:\n        logits: Unscaled log probabilities.\n\n    Return:\n        A tensor containing the Shannon entropy in the last dimension.\n    """"""\n    probs = F.softmax(logits, -1) + 1e-8\n    entropy = - probs * torch.log(probs)\n    entropy = torch.sum(entropy, -1)\n    return entropy\n\n\ndef entropy_with_logits(logits: torch.Tensor,\n                        rank: Optional[int] = None,\n                        average_across_batch: bool = True,\n                        average_across_remaining: bool = False,\n                        sum_over_batch: bool = False,\n                        sum_over_remaining: bool = True) -> torch.Tensor:\n    r""""""Shannon entropy given logits.\n\n    Args:\n        logits: Unscaled log probabilities of shape\n            `[batch_size, d_2, ..., d_{rank-1}, distribution_dim]`\n            and of dtype `float32` or `float64`.\n\n            The rank of the tensor is optionally specified by the argument\n            :attr:`rank`.\n\n            The tensor is considered as having `[batch_size, .., d_{rank-1}]`\n            elements, each of which has a distribution of length `d_rank`\n            (i.e., `distribution_dim`). So the last dimension is always\n            summed out to compute the entropy.\n        rank (int, optional): The rank of :attr:`logits`.\n            If `None` (default), `rank` is inferred automatically from\n            `logits`. If the inference fails, `rank` is\n            set to 2, i.e., assuming :attr:`logits` is of shape\n            `[batch_size, distribution_dim]`\n        average_across_batch (bool): If set, average the entropy across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_remaining (bool): If set, average the entropy across the\n            remaining dimensions. Must not set `average_across_remaining`\'\n            and `sum_over_remaining` at the same time.\n            Used only when :attr:`logits` has rank >= 3.\n        sum_over_batch (bool): If set, sum the entropy across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_remaining (bool): If set, sum the entropy across the\n            remaining dimension. Must not set `average_across_remaining`\n            and `sum_over_remaining` at the same time.\n            Used only when :attr:`logits` has rank >= 3.\n\n    Returns:\n        A Tensor containing the Shannon entropy. The dimensionality of the\n        Tensor depends on the configuration of reduction arguments. For\n        example, if both batch and remaining dimensions are reduced (by\n        either sum or average), the returned Tensor is a scalar Tensor.\n    """"""\n    entropy = _get_entropy(logits)\n\n    if rank is None:\n        rank = get_rank(logits)\n    if rank is None:\n        rank = 2\n    rank -= 1\n\n    if average_across_batch and sum_over_batch:\n        raise ValueError(""Only one of `average_across_batch` and ""\n                         ""`sum_over_batch` can be set."")\n    if average_across_remaining and sum_over_remaining:\n        raise ValueError(""Only one of `average_across_remaining` and ""\n                         ""`sum_over_remaining` can be set."")\n    sum_axes, average_axes = [], []\n    if sum_over_batch:\n        sum_axes.append(0)\n    if average_across_batch:\n        average_axes.append(0)\n    if sum_over_remaining and rank >= 2:\n        sum_axes += list(range(1, rank))\n    if average_across_remaining and rank >= 2:\n        average_axes += list(range(1, rank))\n\n    entropy = reduce_dimensions(\n        entropy, average_axes=average_axes, sum_axes=sum_axes\n    )\n\n    return entropy\n\n\ndef sequence_entropy_with_logits(\n        logits: torch.Tensor,\n        rank: Optional[int] = None,\n        sequence_length: Optional[torch.LongTensor] = None,\n        average_across_batch: bool = True,\n        average_across_timesteps: bool = False,\n        average_across_remaining: bool = False,\n        sum_over_batch: bool = False,\n        sum_over_timesteps: bool = True,\n        sum_over_remaining: bool = True,\n        time_major: bool = False) -> torch.Tensor:\n    r""""""Shannon entropy given logits.\n\n    Args:\n        logits: Unscaled log probabilities of shape\n            `[batch_size, max_time, d_3, ..., d_{rank-1}, distribution_dim]`\n            and of dtype `float32` or `float64`.\n\n            The rank of the tensor is optionally specified by the argument\n            :attr:`rank`.\n\n            The tensor is considered as having `[batch_size, .., d_{rank-1}]`\n            elements, each of which has a distribution of length `d_rank`\n            (i.e., `distribution_dim`). So the last dimension is always\n            summed out to compute the entropy.\n\n            The batch and time dimensions are exchanged if :attr:`time_major`\n            is `True`.\n        rank (int, optional): The rank of :attr:`logits`.\n            If `None` (default), `rank` is inferred automatically from\n            `logits`. If the inference fails, `rank` is\n            set to 3, i.e., assuming `logits` is of shape\n            `[batch_size, max_time, distribution_dim]`\n        sequence_length (optional): A Tensor of shape `[batch_size]`.\n            Time steps beyond the respective sequence lengths are\n            counted into the entropy.\n        average_across_timesteps (bool): If set, average the entropy across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the entropy across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_remaining (bool): If set, average the entropy across the\n            remaining dimensions. Must not set `average_across_remaining`\'\n            and `sum_over_remaining` at the same time.\n            Used only when :attr:`logits` has rank >= 4.\n        sum_over_timesteps (bool): If set, sum the entropy across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the entropy across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_remaining (bool): If set, sum the entropy across the\n            remaining dimension. Must not set `average_across_remaining`\n            and `sum_over_remaining` at the same time.\n            Used only when :attr:`logits` has rank >= 4.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`logits` must have shape `[max_time, batch_size, ...]`.\n            If `False` (default), it must have shape\n            `[batch_size, max_time, ...]`.\n\n    Returns:\n        A Tensor containing the Shannon entropy. The dimensionality of the\n        Tensor depends on the configuration of reduction arguments. For\n        example, if batch, time, and remaining dimensions are all reduced (by\n        either sum or average), the returned Tensor is a scalar Tensor.\n    """"""\n    entropy = _get_entropy(logits)\n\n    if rank is None:\n        rank = get_rank(logits)\n    if rank is None:\n        rank = 3\n    rank -= 1\n\n    entropy = mask_and_reduce(\n        entropy,\n        sequence_length,\n        rank=rank,\n        average_across_batch=average_across_batch,\n        average_across_timesteps=average_across_timesteps,\n        average_across_remaining=average_across_remaining,\n        sum_over_batch=sum_over_batch,\n        sum_over_timesteps=sum_over_timesteps,\n        sum_over_remaining=sum_over_remaining,\n        time_major=time_major\n    )\n\n    return entropy\n'"
texar/torch/losses/losses_utils.py,24,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious utilities for losses.\n""""""\n\nfrom typing import Optional\n\nimport torch\n\nfrom texar.torch.utils.shapes import mask_sequences, transpose_batch_time\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""mask_and_reduce"",\n    ""reduce_batch_time"",\n    ""reduce_dimensions"",\n]\n\n\ndef mask_and_reduce(sequence: torch.Tensor,\n                    sequence_length: Optional[torch.LongTensor],\n                    rank: int = 2,\n                    average_across_batch: bool = True,\n                    average_across_timesteps: bool = False,\n                    average_across_remaining: bool = False,\n                    sum_over_batch: bool = False,\n                    sum_over_timesteps: bool = True,\n                    sum_over_remaining: bool = True,\n                    dtype: Optional[torch.dtype] = None,\n                    time_major: bool = False) -> torch.Tensor:\n    r""""""Masks out sequence entries that are beyond the respective sequence\n    lengths, and reduces (average or sum) away dimensions.\n\n    This is a combination of :func:`~texar.torch.utils.shapes.mask_sequences`\n    and :func:`~texar.torch.losses.losses_utils.reduce_batch_time`.\n\n    Args:\n        sequence: A tensor of sequence values.\n            If `time_major=False` (default), this must be a tensor of shape\n            `[batch_size, max_time, d_2, ..., d_rank]`, where the rank of\n            the tensor is specified with :attr:`rank`.\n            The batch and time dimensions are exchanged if `time_major` is True.\n        sequence_length: A tensor of shape `[batch_size]`. Time steps beyond\n            the respective sequence lengths will be made zero. If `None`,\n            no masking is performed.\n        rank (int): The rank of :attr:`sequence`. Must be >= 2. Default is 2,\n            i.e., `sequence` is a 2D Tensor consisting of batch and time\n            dimensions.\n        average_across_timesteps (bool): If set, average the sequence across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the sequence across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_remaining (bool): If set, average the sequence across the\n            remaining dimensions. Must not set `average_across_remaining`\'\n            and `sum_over_remaining` at the same time.\n        sum_over_timesteps (bool): If set, sum the sequence across the time\n            dimension. Must not set `average_across_timesteps` and\n            `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the sequence across the batch\n            dimension. Must not set `average_across_batch` and `sum_over_batch`\n            at the same time.\n        sum_over_remaining (bool): If set, sum the sequence across the remaining\n            dimension. Must not set `average_across_remaining` and\n            `sum_over_remaining` at the same time.\n        dtype (torch.dtype): The dtype of the returned mask.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`sequence` must have shape `[max_time, batch_size, ...]`.\n            If `False` (default), `sequence` must have\n            shape `[batch_size, max_time, ...]`.\n\n    Returns:\n        A tensor containing the masked and reduced sequence.\n    """"""\n    if rank < 2:\n        raise ValueError(\'`rank` must be >= 2.\')\n\n    if time_major:\n        sequence = transpose_batch_time(sequence)\n\n    if sequence_length is not None:\n        sequence = mask_sequences(sequence,\n                                  sequence_length,\n                                  dtype=dtype,\n                                  time_major=False)\n    if rank > 2:\n        if average_across_remaining and sum_over_remaining:\n            raise ValueError(""Only one of `average_across_remaining` and ""\n                             ""`sum_over_remaining` can be set."")\n        if average_across_remaining:\n            for axis in sorted(list(range(2, rank)), reverse=True):\n                sequence = torch.mean(sequence, dim=axis)\n        elif sum_over_remaining:\n            for axis in sorted(list(range(2, rank)), reverse=True):\n                sequence = torch.sum(sequence, dim=axis)\n\n    sequence = reduce_batch_time(sequence,\n                                 sequence_length,\n                                 average_across_batch,\n                                 average_across_timesteps,\n                                 sum_over_batch,\n                                 sum_over_timesteps)\n\n    reduce_time = average_across_timesteps or sum_over_timesteps\n    reduce_batch = average_across_batch or sum_over_batch\n    if not reduce_time and not reduce_batch and time_major:\n        sequence = transpose_batch_time(sequence)\n\n    return sequence\n\n\ndef reduce_batch_time(sequence: torch.Tensor,\n                      sequence_length: Optional[torch.LongTensor],\n                      average_across_batch: bool = True,\n                      average_across_timesteps: bool = False,\n                      sum_over_batch: bool = False,\n                      sum_over_timesteps: bool = True) -> torch.Tensor:\n    r""""""Average or sum over the respective dimensions of :attr:`sequence`, which\n    is of shape `[batch_size, max_time, ...]`.\n\n    Assumes :attr:`sequence` has been properly masked according to\n    :attr:`sequence_length`.\n\n    Args:\n        sequence: A tensor to reduce.\n        sequence_length: A tensor of shape `[batch_size]`. Time steps beyond\n            the respective sequence lengths will be made zero. If `None`,\n            no masking is performed.\n        average_across_batch (bool): If set, average the sequence across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_timesteps (bool): If set, average the sequence across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the sequence across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_timesteps (bool): If set, sum the sequence across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n\n    Returns:\n        A tensor with dimension reduction.\n    """"""\n    if average_across_timesteps and sum_over_timesteps:\n        raise ValueError(""Only one of `average_across_timesteps` and ""\n                         ""`sum_over_timesteps` can be set."")\n    if average_across_batch and sum_over_batch:\n        raise ValueError(""Only one of `average_across_batch` and ""\n                         ""`sum_over_batch` can be set."")\n\n    if sum_over_timesteps:\n        sequence = torch.sum(sequence, dim=1)\n    elif average_across_timesteps:\n        if sequence_length is None:\n            sequence = torch.mean(sequence, dim=1)\n        else:\n            sequence = (torch.sum(sequence, dim=1).float() /\n                        sequence_length.float())\n\n    if sum_over_batch:\n        sequence = torch.sum(sequence, dim=0)\n    elif average_across_batch:\n        sequence = torch.mean(sequence, dim=0)\n\n    return sequence\n\n\ndef reduce_dimensions(tensor: torch.Tensor,\n                      average_axes: Optional[MaybeList[int]] = None,\n                      sum_axes: Optional[MaybeList[int]] = None,\n                      keepdims: Optional[bool] = None) -> torch.Tensor:\n    r""""""Average or sum over dimensions of :attr:`tensor`.\n\n    :attr:`average_axes` and :attr:`sum_axes` must be mutually exclusive. That\n    is, elements in `average_axes` must not be contained in\n    `sum_axes`, and vice versa.\n\n    Args:\n        tensor: A tensor to reduce.\n        average_axes (optional): A (list of) `int` that indicates the\n            dimensions to reduce by taking average.\n        sum_axes (optional): A (list of) `int` that indicates the\n            dimensions to reduce by taking sum.\n        keepdims (optional): If `True`, retains reduced dimensions with\n            length 1.\n\n    Returns:\n        A tensor with dimension reduction.\n    """"""\n    reduced_axes = set()\n    if average_axes is not None:\n        if not isinstance(average_axes, (list, tuple)):\n            average_axes = [average_axes]\n        if len(average_axes) > 0:\n            for average_axis in average_axes:\n                tensor = torch.mean(tensor, dim=average_axis, keepdim=True)\n            reduced_axes.update(average_axes)\n\n    if sum_axes is not None:\n        if not isinstance(sum_axes, (list, tuple)):\n            sum_axes = [sum_axes]\n        if len(sum_axes) > 0:\n            for sum_axis in sum_axes:\n                tensor = torch.sum(tensor, dim=sum_axis, keepdim=True)\n            reduced_axes.update(sum_axes)\n\n            if average_axes is not None:\n                if len(reduced_axes) != len(average_axes) + len(sum_axes):\n                    raise ValueError(\'`average_axes` and `sum_axes` must not \'\n                                     \'have overlapped elements.\')\n    if not keepdims:\n        for axis in sorted(list(reduced_axes), reverse=True):\n            tensor = torch.squeeze(tensor, dim=axis)\n    return tensor\n'"
texar/torch/losses/mle_losses.py,25,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious losses\n""""""\n\nfrom typing import Callable, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\n\nfrom texar.torch.losses.losses_utils import mask_and_reduce, reduce_dimensions\nfrom texar.torch.utils import shapes\nfrom texar.torch.utils.types import MaybeTuple\n\n__all__ = [\n    ""sequence_softmax_cross_entropy"",\n    ""sequence_sparse_softmax_cross_entropy"",\n    ""sequence_sigmoid_cross_entropy"",\n    ""binary_sigmoid_cross_entropy"",\n    ""binary_sigmoid_cross_entropy_with_clas"",\n]\n\n\ndef sequence_softmax_cross_entropy(\n        labels: torch.Tensor,\n        logits: torch.Tensor,\n        sequence_length: Optional[torch.LongTensor],\n        average_across_batch: bool = True,\n        average_across_timesteps: bool = False,\n        sum_over_batch: bool = False,\n        sum_over_timesteps: bool = True,\n        time_major: bool = False,\n        stop_gradient_to_label: bool = False) -> torch.Tensor:\n    r""""""Computes softmax cross entropy for each time step of sequence\n    predictions.\n\n    Args:\n        labels: Target class distributions.\n\n            - If :attr:`time_major` is `False` (default), this must be a\n              Tensor of shape `[batch_size, max_time, num_classes]`.\n\n            - If `time_major` is `True`, this must be a Tensor of shape\n              `[max_time, batch_size, num_classes]`.\n\n            Each row of `labels` should be a valid probability\n            distribution, otherwise, the computation of the gradient will be\n            incorrect.\n        logits: Unscaled log probabilities. This must have the shape of\n            `[max_time, batch_size, num_classes]` or\n            `[batch_size, max_time, num_classes]` according to\n            the value of `time_major`.\n        sequence_length: A Tensor of shape `[batch_size]`. Time steps beyond\n            the respective sequence lengths will have zero losses.\n        average_across_timesteps (bool): If set, average the loss across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        sum_over_timesteps (bool): If set, sum the loss across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`labels` and :attr:`logits` must have shape\n            `[max_time, batch_size, ...]`. If `False`\n            (default), they must have shape `[batch_size, max_time, ...]`.\n        stop_gradient_to_label (bool): If set, gradient propagation to\n            :attr:`labels` will be disabled.\n\n    Returns:\n        A Tensor containing the loss, of rank 0, 1, or 2 depending on the\n        arguments :attr:`{average_across}/{sum_over}_{timesteps}/{batch}`.\n        For example:\n\n        - If :attr:`sum_over_timesteps` and :attr:`average_across_batch`\n          are `True` (default), the return Tensor is of rank 0.\n\n        - If :attr:`average_across_batch` is `True` and other arguments are\n          `False`, the return Tensor is of shape `[max_time]`.\n    """"""\n    if stop_gradient_to_label:\n        labels = labels.detach()\n\n    losses = (-labels.type(logits.dtype) *\n              F.log_softmax(logits, -1)).sum(dim=-1)\n\n    losses = mask_and_reduce(losses,\n                             sequence_length,\n                             rank=2,\n                             average_across_batch=average_across_batch,\n                             average_across_timesteps=average_across_timesteps,\n                             sum_over_batch=sum_over_batch,\n                             sum_over_timesteps=sum_over_timesteps,\n                             time_major=time_major)\n    return losses\n\n\ndef sequence_sparse_softmax_cross_entropy(\n        labels: torch.Tensor,\n        logits: torch.Tensor,\n        sequence_length: Optional[torch.LongTensor],\n        average_across_batch: bool = True,\n        average_across_timesteps: bool = False,\n        sum_over_batch: bool = False,\n        sum_over_timesteps: bool = True,\n        time_major: bool = False) -> torch.Tensor:\n    r""""""Computes sparse softmax cross entropy for each time step of sequence\n    predictions.\n\n    Args:\n        labels: Target class indexes. I.e., classes are mutually exclusive\n            (each entry is in exactly one class).\n\n            - If :attr:`time_major` is `False` (default), this must be\n              a Tensor of shape `[batch_size, max_time]`.\n\n            - If `time_major` is `True`, this must be a Tensor of shape\n              `[max_time, batch_size].`\n        logits: Unscaled log probabilities. This must have the shape of\n            `[max_time, batch_size, num_classes]` or\n            `[batch_size, max_time, num_classes]` according to\n            the value of `time_major`.\n        sequence_length: A Tensor of shape `[batch_size]`. Time steps beyond\n            the respective sequence lengths will have zero losses.\n        average_across_timesteps (bool): If set, average the loss across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        sum_over_timesteps (bool): If set, sum the loss across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`labels` and :attr:`logits` must have shape\n            `[max_time, batch_size, ...]`. If `False`\n            (default), they must have shape `[batch_size, max_time, ...]`.\n\n    Returns:\n        A Tensor containing the loss, of rank 0, 1, or 2 depending on the\n        arguments :attr:`{average_across}/{sum_over}_{timesteps}/{batch}`.\n        For example:\n\n        - If :attr:`sum_over_timesteps` and :attr:`average_across_batch`\n          are `True` (default), the return Tensor is of rank 0.\n\n        - If :attr:`average_across_batch` is `True` and other arguments are\n          `False`, the return Tensor is of shape `[max_time]`.\n\n    Example:\n\n        .. code-block:: python\n\n            embedder = WordEmbedder(vocab_size=data.vocab.size)\n            decoder = BasicRNNDecoder(vocab_size=data.vocab.size)\n            outputs, _, _ = decoder(\n                decoding_strategy=\'train_greedy\',\n                inputs=embedder(data_batch[\'text_ids\']),\n                sequence_length=data_batch[\'length\']-1)\n\n            loss = sequence_sparse_softmax_cross_entropy(\n                labels=data_batch[\'text_ids\'][:, 1:],\n                logits=outputs.logits,\n                sequence_length=data_batch[\'length\']-1)\n\n    """"""\n    logits = F.log_softmax(logits, dim=2)\n    logits = logits.permute(0, 2, 1)\n    losses = F.nll_loss(logits, labels, reduction=\'none\')\n\n    losses = mask_and_reduce(losses,\n                             sequence_length,\n                             rank=2,\n                             average_across_batch=average_across_batch,\n                             average_across_timesteps=average_across_timesteps,\n                             sum_over_batch=sum_over_batch,\n                             sum_over_timesteps=sum_over_timesteps,\n                             time_major=time_major)\n    return losses\n\n\ndef sequence_sigmoid_cross_entropy(\n        labels: torch.Tensor,\n        logits: torch.Tensor,\n        sequence_length: Optional[torch.LongTensor],\n        average_across_batch: bool = True,\n        average_across_timesteps: bool = False,\n        average_across_classes: bool = True,\n        sum_over_batch: bool = False,\n        sum_over_timesteps: bool = True,\n        sum_over_classes: bool = False,\n        time_major: bool = False,\n        stop_gradient_to_label: bool = False) -> torch.Tensor:\n    r""""""Computes sigmoid cross entropy for each time step of sequence\n    predictions.\n\n    Args:\n        labels: Target class distributions.\n\n            - If :attr:`time_major` is `False` (default), this must be a\n              Tensor of shape `[batch_size, max_time(, num_classes)]`.\n\n            - If `time_major` is `True`, this must be a Tensor of shape\n              `[max_time, batch_size(, num_classes)]`.\n\n            Each row of `labels` should be a valid probability\n            distribution, otherwise, the computation of the gradient will be\n            incorrect.\n        logits: Unscaled log probabilities having the same shape as with\n            :attr:`labels`.\n        sequence_length: A Tensor of shape `[batch_size]`. Time steps beyond\n            the respective sequence lengths will have zero losses.\n        average_across_timesteps (bool): If set, average the loss across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_classes (bool): If set, average the loss across the\n            class dimension (if exists). Must not set\n            `average_across_classes`\' and `sum_over_classes` at\n            the same time. Ignored if :attr:`logits` is a 2D Tensor.\n        sum_over_timesteps (bool): If set, sum the loss across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_classes (bool): If set, sum the loss across the\n            class dimension. Must not set `average_across_classes`\n            and `sum_over_classes` at the same time. Ignored if\n            :attr:`logits` is a 2D Tensor.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`labels` and :attr:`logits` must have shape\n            `[max_time, batch_size, ...]`. If `False`\n            (default), they must have shape `[batch_size, max_time, ...]`.\n        stop_gradient_to_label (bool): If set, gradient propagation to\n            :attr:`labels` will be disabled.\n\n    Returns:\n        A Tensor containing the loss, of rank 0, 1, or 2 depending on the\n        arguments\n        :attr:`{average_across}/{sum_over}_{timesteps}/{batch}/{classes}`.\n        For example, if the class dimension does not exist, and\n\n        - If :attr:`sum_over_timesteps` and :attr:`average_across_batch`\n          are `True` (default), the return Tensor is of rank 0.\n\n        - If :attr:`average_across_batch` is `True` and other arguments are\n          `False`, the return Tensor is of shape `[max_time]`.\n    """"""\n    if stop_gradient_to_label:\n        labels = labels.detach()\n    losses = F.binary_cross_entropy_with_logits(\n        logits, labels.type(logits.dtype), reduction=\'none\')\n\n    rank = shapes.get_rank(logits) or shapes.get_rank(labels)\n\n    losses = mask_and_reduce(losses,\n                             sequence_length,\n                             rank=rank,\n                             average_across_batch=average_across_batch,\n                             average_across_timesteps=average_across_timesteps,\n                             average_across_remaining=average_across_classes,\n                             sum_over_batch=sum_over_batch,\n                             sum_over_timesteps=sum_over_timesteps,\n                             sum_over_remaining=sum_over_classes,\n                             time_major=time_major)\n\n    return losses\n\n\ndef binary_sigmoid_cross_entropy(\n        pos_logits: Optional[torch.Tensor] = None,\n        neg_logits: Optional[torch.Tensor] = None,\n        average_across_batch: bool = True,\n        average_across_classes: bool = True,\n        sum_over_batch: bool = False,\n        sum_over_classes: bool = False,\n        return_pos_neg_losses: bool = False) \\\n        -> Union[Tuple[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]:\n    r""""""Computes sigmoid cross entropy of binary predictions.\n\n    Args:\n        pos_logits: The logits of predicting positive on positive data. A\n            tensor of shape `[batch_size(, num_classes)]`.\n        neg_logits: The logits of predicting positive on negative data. A\n            tensor of shape `[batch_size(, num_classes)]`.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_classes (bool): If set, average the loss across the\n            class dimension (if exists). Must not set\n            `average_across_classes`\' and `sum_over_classes` at\n            the same time. Ignored if :attr:`logits` is a 1D Tensor.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_classes (bool): If set, sum the loss across the\n            class dimension. Must not set `average_across_classes`\n            and `sum_over_classes` at the same time. Ignored if\n            :attr:`logits` is a 2D Tensor.\n        return_pos_neg_losses (bool): If set, additionally returns the losses\n            on :attr:`pos_logits` and :attr:`neg_logits`, respectively.\n\n    Returns:\n        By default, a Tensor containing the loss, of rank 0, 1, or 2 depending\n        on the arguments :attr:`{average_across}/{sum_over}_{batch}/{classes}`.\n        For example:\n\n            - If :attr:`sum_over_batch` and :attr:`average_across_classes`\n              are `True` (default), the return Tensor is of rank 0.\n\n            - If  arguments are `False`, the return Tensor is of shape\n              `[batch_size(, num_classes)]`.\n\n        If :attr:`return_pos_neg_losses` is `True`, returns a tuple\n        `(loss, pos_loss, neg_loss)`, where `loss` is the loss above;\n        `pos_loss` is the loss on `pos_logits` only; and\n        `neg_loss` is the loss on `neg_logits` only. They have\n        `loss = pos_loss + neg_loss`.\n    """"""\n    average_axes = [0] if average_across_batch else []\n    average_axes += [1] if average_across_classes else []\n    sum_axes = [0] if sum_over_batch else []\n    sum_axes += [1] if sum_over_classes else []\n\n    if pos_logits is not None:\n        pos_loss = F.binary_cross_entropy_with_logits(\n            pos_logits, torch.ones_like(pos_logits), reduction=\'none\')\n\n        pos_loss = reduce_dimensions(pos_loss, average_axes, sum_axes)\n    else:\n        pos_loss = 0  # type: ignore\n\n    if neg_logits is not None:\n        neg_loss = F.binary_cross_entropy_with_logits(\n            neg_logits, torch.zeros_like(neg_logits), reduction=\'none\')\n\n        neg_loss = reduce_dimensions(neg_loss, average_axes, sum_axes)\n    else:\n        neg_loss = 0  # type: ignore\n\n    loss = pos_loss + neg_loss\n\n    if return_pos_neg_losses:\n        return loss, pos_loss, neg_loss\n    else:\n        return loss\n\n\ndef binary_sigmoid_cross_entropy_with_clas(\n        clas_fn: Callable[[torch.Tensor], MaybeTuple[torch.Tensor]],\n        pos_inputs: Optional[torch.Tensor] = None,\n        neg_inputs: Optional[torch.Tensor] = None,\n        average_across_batch: bool = True,\n        average_across_classes: bool = True,\n        sum_over_batch: bool = False,\n        sum_over_classes: bool = False,\n        return_pos_neg_losses: bool = False) \\\n        -> Union[Tuple[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]:\n    r""""""Computes sigmoid cross entropy of binary classifier.\n\n    Args:\n        clas_fn: A callable takes data (e.g., :attr:`pos_inputs` and\n            :attr:`fake_inputs`) and returns the logits of being positive. The\n            signature of `clas_fn` must be:\n            :python:`logits (, ...) = clas_fn(inputs)`.\n            The return value of `clas_fn` can be the logits, or\n            a tuple where the logits are the first element.\n        pos_inputs: The positive data fed into `clas_fn`.\n        neg_inputs: The negative data fed into `clas_fn`.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_classes (bool): If set, average the loss across the\n            class dimension (if exists). Must not set\n            `average_across_classes`\' and `sum_over_classes` at\n            the same time. Ignored if :attr:`logits` is a 1D Tensor.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_classes (bool): If set, sum the loss across the\n            class dimension. Must not set `average_across_classes`\n            and `sum_over_classes` at the same time. Ignored if\n            :attr:`logits` is a 2D Tensor.\n        return_pos_neg_losses (bool): If set, additionally returns the losses\n            on :attr:`pos_logits` and :attr:`neg_logits`, respectively.\n\n    Returns:\n        By default, a Tensor containing the loss, of rank 0, 1, or 2 depending\n        on the arguments :attr:`{average_across}/{sum_over}_{batch}/{classes}`.\n        For example:\n\n            - If :attr:`sum_over_batch` and :attr:`average_across_classes`\n              are `True` (default), the return Tensor is of rank 0.\n\n            - If  arguments are `False`, the return Tensor is of shape\n              `[batch_size(, num_classes)]`.\n\n        If :attr:`return_pos_neg_losses`=`True`, returns a tuple\n        `(loss, pos_loss, neg_loss)`, where `loss` is the loss above;\n        `pos_loss` is the loss on `pos_logits` only; and\n        `neg_loss` is the loss on `neg_logits` only. They have\n        `loss = pos_loss + neg_loss`.\n    """"""\n    pos_logits = None\n    if pos_inputs is not None:\n        out = clas_fn(pos_inputs)\n        pos_logits = out[0] if isinstance(out, (list, tuple)) else out\n\n    neg_logits = None\n    if neg_inputs is not None:\n        out = clas_fn(neg_inputs)\n        neg_logits = out[0] if isinstance(out, (list, tuple)) else out\n\n    return binary_sigmoid_cross_entropy(\n        pos_logits=pos_logits,\n        neg_logits=neg_logits,\n        average_across_batch=average_across_batch,\n        average_across_classes=average_across_classes,\n        sum_over_batch=sum_over_batch,\n        sum_over_classes=sum_over_classes,\n        return_pos_neg_losses=return_pos_neg_losses)\n'"
texar/torch/losses/pg_losses.py,16,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious loss functions for policy gradients.\n""""""\n\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom texar.torch.losses.losses_utils import mask_and_reduce\nfrom texar.torch.utils.shapes import get_rank\n\n__all__ = [\n    ""pg_loss_with_logits"",\n    ""pg_loss_with_log_probs"",\n]\n\n\ndef pg_loss_with_logits(actions: torch.Tensor,\n                        logits: torch.Tensor,\n                        advantages: torch.Tensor,\n                        rank: Optional[int] = None,\n                        batched: bool = False,\n                        sequence_length: Optional[torch.LongTensor] = None,\n                        average_across_batch: bool = True,\n                        average_across_timesteps: bool = False,\n                        average_across_remaining: bool = False,\n                        sum_over_batch: bool = False,\n                        sum_over_timesteps: bool = True,\n                        sum_over_remaining: bool = True,\n                        time_major: bool = False) -> torch.Tensor:\n    r""""""Policy gradient loss with logits. Used for discrete actions.\n\n    `pg_loss = reduce( advantages * -log_prob( actions )  )`,\n    where `advantages` and `actions` do not back-propagate gradients.\n\n    All arguments except :attr:`logits` and :attr:`actions` are the same with\n    :func:`pg_loss_with_log_probs`.\n\n    Args:\n        actions: Tensor of shape\n            `[(batch_size,) max_time, d_3, ..., d_rank]` and of dtype\n            `int32` or `int64`.\n            The rank of the Tensor is specified with :attr:`rank`.\n\n            The batch dimension exists only if :attr:`batched` is `True`.\n\n            The batch and time dimensions\n            are exchanged, i.e., `[max_time, batch_size, ...]` if\n            :attr:`time_major` is `True`.\n        logits: Unscaled log probabilities of shape\n            `[(batch_size,) max_time, d_3, ..., d_{rank+1}]`\n            and dtype `float32` or `float64`.\n            The batch and time dimensions are exchanged if `time_major`\n            is `True`.\n        advantages: Tensor of shape\n            `[(batch_size,) max_time, d_3, ..., d_rank]` and\n            dtype `float32` or `float64`.\n            The batch and time dimensions are exchanged if `time_major`\n            is `True`.\n        rank (int, optional): The rank of :attr:`actions`.\n            If `None` (default), rank is automatically inferred from\n            `actions` or `advantages`. If the inference fails,\n            `rank` is set to 1 if :attr:`batched` is `False`,\n            and set to 2 if :attr:`batched` is `True`.\n        batched (bool): `True` if the inputs are batched.\n        sequence_length (optional): A Tensor of shape `[batch_size]`.\n            Time steps beyond the respective sequence lengths will have zero\n            losses. Used if :attr:`batched` is `True`.\n        average_across_timesteps (bool): If set, average the loss across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n            Ignored if `batched` is `False`.\n        average_across_remaining (bool): If set, average the sequence across the\n            remaining dimensions. Must not set `average_across_remaining`\'\n            and `sum_over_remaining` at the same time. Ignored if\n            no more dimensions other than the batch and time dimensions.\n        sum_over_timesteps (bool): If set, sum the loss across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n            Ignored if `batched` is `False`.\n        sum_over_remaining (bool): If set, sum the loss across the\n            remaining dimension. Must not set `average_across_remaining`\n            and `sum_over_remaining` at the same time. Ignored if\n            no more dimensions other than the batch and time dimensions.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`logits`, :attr:`actions` and :attr:`advantages` must\n            have shape `[max_time, batch_size, ...]`. If `False` (default),\n            they must have shape `[batch_size, max_time, ...]`.\n            Ignored if `batched` is `False`.\n\n    Returns:\n        A Tensor containing the loss to minimize, whose rank depends on the\n        reduce arguments. For example, the batch dimension is reduced if\n        either :attr:`average_across_batch` or :attr:`sum_over_batch` is\n        `True`, which decreases the rank of output tensor by 1.\n    """"""\n    actions = actions.detach()\n    logits = F.log_softmax(logits, dim=-1)\n    logits = logits.permute((0, -1) + tuple(range(1, logits.dim() - 1)))\n    neg_log_probs = F.nll_loss(logits, actions, reduction=\'none\')\n\n    return pg_loss_with_log_probs(\n        log_probs=-neg_log_probs,\n        advantages=advantages,\n        rank=rank,\n        batched=batched,\n        sequence_length=sequence_length,\n        average_across_batch=average_across_batch,\n        average_across_timesteps=average_across_timesteps,\n        average_across_remaining=average_across_remaining,\n        sum_over_batch=sum_over_batch,\n        sum_over_timesteps=sum_over_timesteps,\n        sum_over_remaining=sum_over_remaining,\n        time_major=time_major)\n\n\ndef pg_loss_with_log_probs(log_probs: torch.Tensor,\n                           advantages: torch.Tensor,\n                           rank: Optional[int] = None,\n                           batched: bool = False,\n                           sequence_length: Optional[torch.LongTensor] = None,\n                           average_across_batch: bool = True,\n                           average_across_timesteps: bool = False,\n                           average_across_remaining: bool = False,\n                           sum_over_batch: bool = False,\n                           sum_over_timesteps: bool = True,\n                           sum_over_remaining: bool = True,\n                           time_major: bool = False) -> torch.Tensor:\n    r""""""Policy gradient loss with log probabilities of actions.\n\n    `pg_loss = reduce(advantages * -log_probs)`,\n    where `advantages` does not back-propagate gradients.\n\n    All arguments except :attr:`log_probs` are the same as\n    :func:`pg_loss_with_logits`.\n\n    Args:\n        log_probs: Log probabilities of shape\n            `[(batch_size,) max_time, ..., d_rank]` and dtype `float32`\n            or `float64`. The rank of the Tensor is specified\n            with :attr:`rank`.\n\n            The batch dimension exists only if :attr:`batched` is `True`.\n\n            The batch and time dimensions are exchanged, i.e.,\n            `[max_time, batch_size, ...]` if :attr:`time_major` is `True`.\n        advantages: Tensor of shape\n            `[(batch_size,) max_time, d_3, ..., d_rank]` and\n            dtype `float32` or `float64`.\n            The batch dimension exists only if `batched` is `True`.\n            The batch and time dimensions\n            are exchanged if `time_major` is `True`.\n        rank (int, optional): The rank of :attr:`log_probs`.\n            If `None` (default), rank is automatically inferred from\n            `log_probs` or `advantages`. If the inference fails,\n            `rank` is set to 1 if `batched``==False`,\n            and set to 2 if `batched``==True`.\n        batched (bool): `True` if the inputs are batched.\n        sequence_length (optional): A Tensor of shape `[batch_size]`.\n            Time steps beyond the respective sequence lengths will have zero\n            losses. Used if :attr:`batched` is `True`.\n        average_across_timesteps (bool): If set, average the loss across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n            Ignored if `batched` is `False`.\n        average_across_remaining (bool): If set, average the sequence across the\n            remaining dimensions. Must not set `average_across_remaining`\'\n            and `sum_over_remaining` at the same time. Ignored if\n            no more dimensions other than the batch and time dimensions.\n        sum_over_timesteps (bool): If set, sum the loss across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n            Ignored if `batched` is `False`.\n        sum_over_remaining (bool): If set, sum the loss across the\n            remaining dimension. Must not set `average_across_remaining`\n            and `sum_over_remaining` at the same time. Ignored if\n            no more dimensions other than the batch and time dimensions.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`log_probs` and :attr:`advantages` must have shape\n            `[max_time, batch_size, ...]`. If `False` (default),\n            they must have shape `[batch_size, max_time, ...]`.\n            Ignored if :attr:`batched` is `False`.\n\n    Returns:\n        A Tensor containing the loss to minimize, whose rank depends on the\n        reduce arguments. For example, the batch dimension is reduced if\n        either :attr:`average_across_batch` or :attr:`sum_over_batch` is\n        `True`, which decreases the rank of output tensor by 1.\n    """"""\n    advantages = advantages.detach()\n\n    losses = -log_probs * advantages\n\n    if rank is None:\n        rank = get_rank(log_probs) or get_rank(advantages)\n    if rank is None:\n        rank = 2 if batched else 1\n\n    if batched:\n        losses = mask_and_reduce(\n            losses,\n            sequence_length,\n            rank=rank,\n            average_across_batch=average_across_batch,\n            average_across_timesteps=average_across_timesteps,\n            average_across_remaining=average_across_remaining,\n            sum_over_batch=sum_over_batch,\n            sum_over_timesteps=sum_over_timesteps,\n            sum_over_remaining=sum_over_remaining,\n            time_major=time_major)\n    elif rank > 1:\n        if average_across_remaining and sum_over_remaining:\n            raise ValueError(""Only one of `average_across_remaining` and ""\n                             ""`sum_over_remaining` can be set."")\n        if average_across_remaining:\n            for average_axis in sorted(list(range(1, rank)), reverse=True):\n                losses = torch.mean(losses, dim=average_axis)\n        elif sum_over_remaining:\n            for sum_axis in sorted(list(range(1, rank)), reverse=True):\n                losses = torch.sum(losses, dim=sum_axis)\n\n    if not batched:\n        if average_across_timesteps and sum_over_timesteps:\n            raise ValueError(""Only one of `average_across_timesteps` and ""\n                             ""`sum_over_timesteps` can be set."")\n        if average_across_timesteps:\n            losses = torch.mean(losses, dim=0)\n        elif sum_over_timesteps:\n            losses = torch.sum(losses, dim=0)\n\n    return losses\n'"
texar/torch/losses/rewards.py,36,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious reward related functions.\n""""""\n\nfrom typing import Optional\n\nimport torch\n\nfrom texar.torch.utils.shapes import mask_sequences\nfrom texar.torch.utils.utils import sequence_mask\n\n__all__ = [\n    ""discount_reward"",\n    ""_discount_reward_tensor_1d"",\n    ""_discount_reward_tensor_2d"",\n]\n\n\ndef discount_reward(reward: torch.Tensor,\n                    sequence_length: Optional[torch.LongTensor] = None,\n                    discount: float = 1.,\n                    normalize: bool = False) -> torch.Tensor:\n    r""""""Computes discounted reward.\n\n    Args:\n        reward: A Tensor. Can be 1D with shape `[batch_size]`,\n            or 2D with shape `[batch_size, max_time]`.\n        sequence_length (optional): A Tensor of shape `[batch_size]`.\n            Time steps beyond the respective sequence lengths will be masked.\n            Required if :attr:`reward` is 1D.\n        discount (float): A scalar. The discount factor.\n        normalize (bool): Whether to normalize the discounted reward, by\n            `(discounted_reward - mean) / std`. Here `mean` and `std` are\n            over all time steps and all samples in the batch.\n\n    Returns:\n        A 2D Tensor of the discounted reward.\n    """"""\n    if not isinstance(reward, torch.Tensor):\n        reward = torch.tensor(reward)\n    if (sequence_length is not None and\n            not isinstance(sequence_length, torch.Tensor)):\n        sequence_length = torch.tensor(\n            sequence_length, dtype=torch.int64, device=reward.device)\n\n    tensor_rank = reward.dim()\n    if tensor_rank == 1:\n        disc_reward = _discount_reward_tensor_1d(\n            reward, sequence_length, discount)\n    elif tensor_rank == 2:\n        disc_reward = _discount_reward_tensor_2d(\n            reward, sequence_length, discount)\n    else:\n        raise ValueError(""The dimension of reward can only be 1 or 2."")\n\n    if normalize:\n        mu = torch.mean(disc_reward)\n        var = torch.std(disc_reward)\n        disc_reward = (disc_reward - mu) / (torch.sqrt(var) + 1e-8)\n\n    return disc_reward\n\n\ndef _discount_reward_tensor_1d(reward: torch.Tensor,\n                               sequence_length: Optional[torch.LongTensor],\n                               discount: float = 1.) -> torch.Tensor:\n    r""""""Computes discounted reward.\n\n    Args:\n        reward: 1D Tensor with shape `[batch_size]`.\n        sequence_length: A Tensor of shape `[batch_size]`.\n        Time steps beyond the respective sequence lengths will be masked.\n        discount (float): A scalar. The discount factor.\n\n    Returns:\n        A 2D Tensor of the discounted reward.\n    """"""\n    if sequence_length is None:\n        raise ValueError(\'sequence_length must not be `None` for 1D reward.\')\n\n    if not isinstance(sequence_length, torch.Tensor):\n        sequence_length = torch.tensor(\n            sequence_length, dtype=torch.int64, device=reward.device)\n\n    batch_size = reward.shape[0]\n    max_seq_length = torch.max(sequence_length)\n    dtype: torch.dtype = reward.dtype\n\n    if discount == 1.:\n        disc_reward = reward.unsqueeze(-1).expand(batch_size, max_seq_length)\n    else:\n        mask = sequence_mask(sequence_length, dtype=dtype)\n        mask = torch.cat((mask[:, 1:], torch.zeros_like(mask[:, -1:])), dim=1)\n        # Make each row = [discount, ..., discount, 1, ..., 1]\n        dmat = mask * discount + (1 - mask)\n        dmat = torch.flip(dmat, (1,))\n        dmat = torch.cumprod(dmat, dim=1)\n        dmat = torch.flip(dmat, (1,))\n        disc_reward = dmat * reward.unsqueeze(-1)\n\n    disc_reward = mask_sequences(disc_reward, sequence_length, dtype=dtype)\n\n    return disc_reward\n\n\ndef _discount_reward_tensor_2d(\n        reward: torch.Tensor,\n        sequence_length: Optional[torch.LongTensor] = None,\n        discount: float = 1.) -> torch.Tensor:\n    r""""""Computes discounted reward.\n\n    Args:\n        reward: 2D Tensor with shape `[batch_size, max_time]`.\n        sequence_length (optional): A Tensor of shape `[batch_size]`.\n            Time steps beyond the respective sequence lengths will be masked.\n        discount (float): A scalar. The discount factor.\n\n    Returns:\n        A 2D Tensor of the discounted reward.\n    """"""\n    dtype: torch.dtype = reward.dtype\n    if sequence_length is not None:\n        reward = mask_sequences(reward, sequence_length, dtype=dtype)\n\n    if discount == 1.:\n        reward = torch.flip(reward, (1,))\n        disc_reward = torch.cumsum(reward, dim=1)\n        disc_reward = torch.flip(disc_reward, (1,))\n    else:\n        # [max_time, batch_size]\n        rev_reward_T = torch.flip(reward, (1,)).permute(1, 0)\n\n        res = []\n        acc = torch.zeros_like(reward[:, 1])\n        for i in range(rev_reward_T.shape[0]):\n            cur = rev_reward_T[i]\n            acc = cur + discount * acc\n            res.append(acc)\n\n        rev_reward_T_cum = torch.stack(res, dim=0)\n        disc_reward = torch.flip(rev_reward_T_cum.permute(1, 0), (1,))\n\n    return disc_reward\n'"
texar/torch/modules/__init__.py,9,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library module.\n""""""\n\nfrom texar.torch.modules.classifiers import *\nfrom texar.torch.modules.connectors import *\nfrom texar.torch.modules.decoders import *\nfrom texar.torch.modules.embedders import *\nfrom texar.torch.modules.encoders import *\nfrom texar.torch.modules.networks import *\nfrom texar.torch.modules.pretrained import *\nfrom texar.torch.modules.regressors import *\nfrom texar.torch.modules.encoder_decoders import *\n'"
texar/torch/run/__init__.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nThe Executor module of Texar library.\n""""""\n\nfrom texar.torch.run import action\nfrom texar.torch.run import metric\nfrom texar.torch.run import condition as cond\nfrom texar.torch.run.executor import *\n\n__all__ = [\n    ""action"",\n    ""cond"",\n    ""metric"",\n    ""make_deterministic"",\n    ""Executor"",\n]\n'"
texar/torch/run/action.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nActions for the Executor module.\n""""""\n\nfrom abc import ABC, abstractmethod\n\nfrom texar.torch.run.executor import Executor\n\n\n__all__ = [\n    ""reset_params"",\n    ""scale_lr"",\n    ""early_stop"",\n]\n\n\nclass Action(ABC):\n    @abstractmethod\n    def __call__(self, executor: Executor) -> None:\n        raise NotImplementedError\n\n\nclass reset_params(Action):\n    def __init__(self, training_state: bool = True):\n        self.load_training_state = training_state\n\n    def __call__(self, executor: Executor):\n        # TODO: Only optimizer?\n        executor.load(load_training_state=self.load_training_state)\n\n\nclass scale_lr(Action):\n    def __init__(self, scale: float):\n        # TODO: Change to accept LRScheduler.\n        self.scale = scale\n\n    def __call__(self, executor: Executor):\n        new_lr = []\n        assert executor.optimizer is not None\n        for group in executor.optimizer.param_groups:  # type: ignore\n            lr = group[\'lr\'] * self.scale\n            group[\'lr\'] = lr\n            new_lr.append(lr)\n\n        lr_str = "", "".join(f""{lr:.4e}"" for lr in new_lr)\n        if len(new_lr) > 1:\n            lr_str = f""[{lr_str}]""\n        executor.write_log(f""Learning rate scaled by {self.scale}. ""\n                           f""New LR: {lr_str}"")\n\n\nclass early_stop(Action):\n    def __init__(self, patience: int):\n        if not isinstance(patience, int) or patience <= 0:\n            raise ValueError(""`patience` must be a positive integer"")\n        self.patience = patience\n        self.count = 0\n\n    def __call__(self, executor: Executor):\n        self.count += 1\n        if self.count >= self.patience:\n            executor.terminate()\n            executor.write_log(f""Early stopping patience exhausted, ""\n                               f""terminating training..."")\n        else:\n            executor.write_log(f""Early stopping patience decrease to ""\n                               f""{self.patience - self.count}"")\n'"
texar/torch/run/condition.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nConditions for the Executor module.\n""""""\n\nimport functools\nimport types\nfrom abc import ABC, abstractmethod\nfrom enum import Enum, auto\nfrom time import time as time_now\nfrom typing import Any, Dict, Optional, Tuple\n\nfrom texar.torch.run.executor_utils import MetricList\nfrom texar.torch.utils.types import MaybeTuple\n\n# pylint: disable=unused-argument\n\n__all__ = [\n    ""Event"",\n    ""EventPoint"",\n    ""Condition"",\n    ""epoch"",\n    ""iteration"",\n    ""validation"",\n    ""consecutive"",\n    ""time"",\n]\n\n\nclass Event(Enum):\n    Iteration = auto()\n    Epoch = auto()\n    Training = auto()\n    Validation = auto()\n    ValidationIteration = auto()\n    Testing = auto()\n    TestingIteration = auto()\n    ParameterUpdate = auto()\n\n\nEventPoint = Tuple[Event, bool]\n\n\nclass Condition(ABC):\n    _hooks: Dict[EventPoint, Any]\n\n    @property\n    @abstractmethod\n    def _hash_attributes(self) -> MaybeTuple[Any]:\n        raise NotImplementedError\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, Condition):\n            return False\n        return self._hash_attributes == other._hash_attributes  # pylint: disable=protected-access\n\n    def __hash__(self):\n        return hash(self._hash_attributes)\n\n    @property\n    def hooks(self) -> Dict[EventPoint, Any]:\n        return self._hooks\n\n    def __init__(self):\n        self._hooks = {}\n        for hook_name in dir(self):\n            if not hook_name.startswith(""check_""):\n                continue\n            name = hook_name\n            if name.endswith(""_begin""):\n                name = name[6:-6]\n                point = False\n            elif name.endswith(""_end""):\n                name = name[6:-4]\n                point = True\n            else:\n                raise ValueError(\n                    ""Final part of hook name must be \'begin\' or \'end\'"")\n            if name not in Event.__members__:\n                name = \'\'.join(x.capitalize() for x in name.split(""_""))\n                if name not in Event.__members__:\n                    raise ValueError(\n                        f""Hook name \'{hook_name}\' is not a valid event"")\n            event = Event.__members__[name]\n            self._hooks[(event, point)] = getattr(self, hook_name)\n\n\nclass epoch(Condition):\n    r""""""Triggers when the specified number of epochs has ended.\n\n    Args:\n        num_epochs (int): The number of epochs to wait before triggering the\n            event. In other words, the event is triggered every\n            :attr:`num_epochs` epochs.\n    """"""\n\n    def __init__(self, num_epochs: int = 1):\n        if not isinstance(num_epochs, int) or num_epochs <= 0:\n            raise ValueError(""`num_epochs` must be a positive integer"")\n        super().__init__()\n        self.num_epochs = num_epochs\n        self.count = 0\n\n    @property\n    def _hash_attributes(self):\n        return self.num_epochs\n\n    def check_epoch_end(self, executor) -> bool:\n        self.count += 1\n        if self.count == self.num_epochs:\n            self.count = 0\n            return True\n        return False\n\n\nclass iteration(Condition):\n    r""""""Triggers when the specified number of iterations had ended.\n\n    Args:\n        num_iters (int): The number of iterations to wait before triggering the\n            event. In other words, the event is triggered every\n            :attr:`num_iters` iterations.\n        mode (str): The mode under which iterations are counted. Available\n            choices are ``""train""``, ``""valid""``, and ``""test""``. Defaults to\n            ``""train""``.\n    """"""\n\n    def __new__(cls, num_iters: int = 1, mode: str = ""train""):\n        obj = super().__new__(cls)\n        # pylint: disable=protected-access\n        if mode == ""train"":\n            obj.check_iteration_end = obj._check_iteration_end\n        elif mode == ""valid"":\n            obj.check_validation_iteration_end = obj._check_iteration_end\n        elif mode == ""test"":\n            obj.check_testing_iteration_end = obj._check_iteration_end\n        else:\n            raise ValueError(f""Invalid mode {mode}"")\n        # pylint: enable=protected-access\n        return obj\n\n    def __init__(self, num_iters: int = 1, mode: str = ""train""):\n        if not isinstance(num_iters, int) or num_iters <= 0:\n            raise ValueError(""`num_iters` must be a positive integer"")\n        super().__init__()\n        self.num_iters = num_iters\n        self.count = 0\n\n    @property\n    def _hash_attributes(self):\n        return self.num_iters\n\n    def _check_iteration_end(self, executor) -> bool:\n        self.count += 1\n        if self.count == self.num_iters:\n            self.count = 0\n            return True\n        return False\n\n\nclass validation(Condition):\n    r""""""Triggers when validation ends, and optionally checks if validation\n    results improve or worsen.\n\n    Args:\n        num_validations (int): The number of validations to wait before\n            triggering the event. In other words, the event is triggered every\n            :attr:`num_validations` validations.\n        better (bool, optional): If `True`, this event only triggers when\n            validation results improve; if `False`, only triggers when results\n            worsen. Defaults to `None`, in which case the event triggers\n            regardless of results.\n    """"""\n\n    def __init__(self, num_validations: int = 1, better: Optional[bool] = None):\n        if not isinstance(num_validations, int) or num_validations <= 0:\n            raise ValueError(""`num_validations` must be a positive integer"")\n        super().__init__()\n        self.num_valids = num_validations\n        self.count = 0\n        self.better = better\n        self.prev_result: Optional[MetricList] = None\n\n    @property\n    def _hash_attributes(self):\n        return self.num_valids, self.better\n\n    def check_validation_end(self, executor) -> bool:\n        self.count += 1\n        if self.count < self.num_valids:\n            return False\n        self.count = 0\n        if self.better is None:\n            return True\n        metrics = executor.status[""eval_metric""]\n        cur_result = MetricList(metrics)\n        if self.prev_result is not None:\n            better = cur_result > self.prev_result\n        else:\n            better = True\n        if better:\n            self.prev_result = cur_result\n        return better == self.better\n\n\nclass consecutive(Condition):\n    r""""""Triggers when the specified condition passes checks for several times\n    consecutively.\n\n    For example: :python:`consecutive(validation(better=False), times=3)` would\n    trigger if validation results do not improve for 3 times in a row.\n\n    .. warning::\n        This method works by calling the inner condition at each event point\n        that it registers. The consecutive counter is reset to zero if any check\n        returns `False`. Thus, the behavior of :class:`consecutive` might be\n        different to what you expect. For instance:\n\n        - :python:`cond.consecutive(cond.iteration(1), n_times)` is equivalent\n          to :python:`cond.iteration(n_times)`.\n        - :python:`cond.consecutive(cond.iteration(2), n_times)` will never\n          trigger.\n\n        It is recommended against using :class:`consecutive` for conditions\n        except :class:`validation`. You should also be careful when implementing\n        custom conditions for using with :class:`consecutive`.\n\n    .. warning::\n        Conditions are stateful objects. Using a registered condition as the\n        inner condition here could result in unexpected behaviors. For example:\n\n        .. code-block:: python\n\n            my_cond = cond.validation(better=True)\n            executor.on(my_cond, some_action)\n            executor.on(cond.consecutive(my_cond, 2), some_other_action)\n\n        In the code above, if no other conditions are registered,\n        :python:`some_other_action` will never be called. This is because both\n        conditions are checked at the end of each iteration, but the\n        :class:`consecutive` condition internally checks :python:`my_cond`,\n        which has already updated the previous best result that it stored. As a\n        result, the check will never succeed.\n\n    Args:\n        cond: The base condition to check.\n        times (int): The number of times the base condition should pass checks\n            consecutively.\n        clear_after_trigger (bool): Whether the counter should be cleared after\n            the event is triggered. If :attr:`clear_after_trigger` is set to\n            `False`, once this event is triggered, it will trigger every time\n            :attr:`cond` is triggered, until :attr:`cond` fails to trigger at\n            some point. Defaults to `True`.\n    """"""\n\n    def __init__(self, cond: Condition, times: int,\n                 clear_after_trigger: bool = True):\n        super().__init__()\n        self.cond = cond\n        self.times = times\n        self.count = 0\n        self.clear_after_trigger = clear_after_trigger\n\n        for hook_point, method in self.cond.hooks.items():\n            self._hooks[hook_point] = self._create_check_method(method)\n\n    @property\n    def _hash_attributes(self):\n        return self.cond, self.times, self.clear_after_trigger\n\n    def _create_check_method(self, method):\n        @functools.wraps(method)\n        def check_fn(self, executor) -> bool:\n            if method(executor):\n                self.count += 1\n                if self.count >= self.times:\n                    if self.clear_after_trigger:\n                        self.count = 0\n                    return True\n            else:\n                self.count = 0\n            return False\n\n        return types.MethodType(check_fn, self)\n\n\nclass once(Condition):\n    r""""""Triggers only when the specified condition triggers for the first time.\n\n    Internally, this condition calls the\n    :meth:`~texar.torch.run.Executor.remove_action` method to remove itself from\n    the registered actions.\n\n    For example: :python:`once(iteration(5))` would only trigger on the 5th\n    epoch of the entire training loop.\n\n    .. warning::\n        Conditions are stateful objects. Using a registered condition as the\n        inner condition here could result in unexpected behaviors. Please refer\n        to :class:`consecutive` for a concrete example.\n\n    Args:\n        cond: The base condition to check.\n    """"""\n\n    def __init__(self, cond: Condition):\n        super().__init__()\n        self.cond = cond\n\n        for hook_point, method in self.cond.hooks.items():\n            self._hooks[hook_point] = self._create_check_method(method)\n\n    @property\n    def _hash_attributes(self):\n        return self.cond\n\n    def _create_check_method(self, method):\n        @functools.wraps(method)\n        def check_fn(self, executor) -> bool:\n            if method(executor):\n                executor.remove_action()\n                return True\n            return False\n\n        return types.MethodType(check_fn, self)\n\n\nclass time(Condition):\n    def __init__(self, *, hours: Optional[float] = None,\n                 minutes: Optional[float] = None,\n                 seconds: Optional[float] = None,\n                 only_training: bool = True):\n        super().__init__()\n        self.seconds = 0.0\n        if hours is not None:\n            self.seconds += hours * 3600.0\n        if minutes is not None:\n            self.seconds += minutes * 60.0\n        if seconds is not None:\n            self.seconds += seconds\n        self.only_training = only_training\n        self.start_time: Optional[float] = None\n        self.accumulated_time = 0.0\n\n    @property\n    def _hash_attributes(self):\n        return self.seconds, self.only_training\n\n    def _should_trigger(self) -> bool:\n        total_time = self.accumulated_time\n        if self.start_time is None:\n            cur_time = None\n        else:\n            cur_time = time_now()\n            total_time += cur_time - self.start_time\n        self.start_time = cur_time\n        if total_time >= self.seconds:\n            self.accumulated_time = 0.0\n            return True\n        else:\n            self.accumulated_time = total_time\n            return False\n\n    def check_training_begin(self, executor) -> bool:\n        self.start_time = time_now()\n        return False\n\n    def check_training_end(self, executor) -> bool:\n        return self._should_trigger()\n\n    def check_validation_begin(self, executor) -> bool:\n        if self.only_training and self.start_time is not None:\n            self.accumulated_time += time_now() - self.start_time\n            self.start_time = None\n        return self._should_trigger()\n\n    def check_validation_end(self, executor) -> bool:\n        if self.only_training:\n            self.start_time = time_now()\n            return False\n        else:\n            return self._should_trigger()\n\n    def check_testing_begin(self, executor) -> bool:\n        if self.only_training and self.start_time is not None:\n            self.accumulated_time += time_now() - self.start_time\n            self.start_time = None\n        return self._should_trigger()\n\n    def check_testing_end(self, executor) -> bool:\n        if self.only_training:\n            self.start_time = time_now()\n            return False\n        else:\n            return self._should_trigger()\n\n    def check_iteration_end(self, executor) -> bool:\n        return self._should_trigger()\n'"
texar/torch/run/executor.py,65,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nThe Executor module.\n""""""\n\nimport pickle\nimport random\nimport re\nimport sys\nimport time\nfrom collections import OrderedDict, defaultdict  # pylint: disable=unused-import\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import (\n    Any, Callable, Dict, IO, List, Optional, Sequence, Set, Union,\n    no_type_check, overload)\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim.lr_scheduler import _LRScheduler as LRScheduler\nfrom torch.optim.optimizer import Optimizer\n\nfrom texar.torch.data.data.data_base import DatasetBase\nfrom texar.torch.data.data.data_iterators import BatchingStrategy, DataIterator\nfrom texar.torch.data.data.dataset_utils import Batch\nfrom texar.torch.run import executor_utils as utils\nfrom texar.torch.run.condition import Condition, Event, EventPoint\nfrom texar.torch.run.executor_utils import Instance, OptionalDict, OptionalList\nfrom texar.torch.run.metric import Metric, StreamingMetric\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""make_deterministic"",\n    ""Executor"",\n]\n\nActionFn = Callable[[\'Executor\'], None]\nLogDest = Union[str, Path, IO[str]]\n\n\ndef make_deterministic(seed: int = 19260817,\n                       cudnn_deterministic: bool = False):\n    r""""""Make experiment deterministic by using specific random seeds across\n    all frameworks and (optionally) use deterministic algorithms.\n\n    Args:\n        seed (int): The random seed to set.\n        cudnn_deterministic (bool): If `True`, set CuDNN to use\n            deterministic algorithms. Setting this to `True` can negatively\n            impact performance, and might not be necessary for most cases.\n            Defaults to `False`.\n    """"""\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    if cudnn_deterministic:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n\nclass Executor:\n    # pylint: disable=line-too-long\n    r"""""":class:`Executor` is a substitute for the general training/evaluation\n    loop. It is designed with the following goals in mind:\n\n    1. **Minimize the amount of boilerplate code** that is essentially the same\n       across all experiments.\n    2. Provide **best practices** and hide hideous details from the user.\n    3. Guarantee **reproducability** (runs with same configuration & seed always\n       produces the same result) and **portability** (same code runs whether\n       using GPU or not).\n    4. Meanwhile, allowing **flexible configurations** and support\n       user-overridden behaviors.\n\n    Example:\n        Here is a realistic training loop example using :class:`Executor`,\n        showcasing the features built in. :class:`Executor` takes care of common\n        training procedures including logging, checkpoint management, evaluation\n        metrics, validation, and patience-based early-stopping.\n\n        .. code-block:: python\n\n            from texar.torch.run import *\n\n            executor = Executor(\n                model=model,\n                train_data=datasets[""train""],\n                valid_data=datasets[""dev""],\n                test_data=datasets[""test""],\n                checkpoint_dir=args.save_dir,\n                save_every=cond.validation(better=True),\n                train_metrics=(""loss"", metric.RunningAverage(args.display_steps)),\n                optimizer={""type"": torch.optim.Adam},\n                grad_clip=args.grad_clip,\n                log_every=cond.iteration(args.display_steps),\n                validate_every=cond.epoch(1),\n                valid_metrics=[\n                    metric.PearsonR(pred_name=""preds""),\n                    (""loss"", metric.Average())],\n                plateau_condition=[\n                    cond.consecutive(cond.validation(better=False), 2)],\n                action_on_plateau=[\n                    action.early_stop(patience=2),\n                    action.reset_params(),\n                    action.scale_lr(0.8)],\n                stop_training_on=cond.iteration(args.max_train_steps),\n                test_mode=\'eval\',\n            )\n            executor.train()\n\n    .. |Metric| replace:: :class:`~texar.torch.run.metric.Metric`\n    .. |Condition| replace:: :class:`~texar.torch.run.condition.Condition`\n    .. |Optimizer| replace::\n        :torch_docs:`torch.optim.Optimizer <optim.html#torch.optim.Optimizer>`\n    .. |LRScheduler| replace::\n        :torch_docs:`LRScheduler <optim.html#how-to-adjust-learning-rate>`\n    .. |Action| replace:: :class:`~texar.torch.run.action.Action`\n    .. |Event| replace:: :class:`~texar.torch.run.condition.Event`\n\n    **Concepts**\n\n    To make full use of :class:`Executor`, you\'ll need to understand the\n    following concepts:\n\n    - **Event:** Events are a set of pre-defined time spans within the training\n      and evaluation loop. Common events include a training iteration\n      (``Event.Iteration``), an epoch (``Event.Epoch``), or an entire validation\n      (``Event.Validation``). Please refer to the enum class |Event| for the\n      full list of events. The beginning and end of events are called **event\n      points**.\n    - **Condition:** |Condition|\\ s are checks performed at the beginning or end of\n      events. If a check passes, we say that the condition ""triggers"". To give\n      a few examples:\n\n      - :class:`cond.iteration(num_iters) <texar.torch.run.condition.iteration>`\n        is checked only at the end of training iterations, and the check passes\n        when the number of iterations passed equals :attr:`num_iters`. Thus, the\n        condition triggers at the end of every :attr:`num_iters` iterations.\n      - :class:`cond.validation(better=True)\n        <texar.torch.run.condition.validation>` is checked only at the end of\n        validations, and the check passes if the current validation result is\n        better than the previous best. Thus, the condition triggers whenever\n        a finished validation yields an improved result.\n      - :class:`cond.time <texar.torch.run.condition.time>` is checked at the\n        end of iterations, and at the beginning and end of training, validation,\n        and testing. It checks whether the elapsed time has reached the\n        specified duration, and triggers when it does.\n\n      Custom conditions must subclass the |Condition| class.\n    - **Action:** Actions are special callback functions that are called\n      either at the beginning or end of an event (actions registered by\n      :meth:`on_event`), or when conditions trigger (actions registered by\n      :meth:`on`, and by specifying :attr:`save_every` and similar arguments).\n      This is similar to hook functions that exists in common frameworks.\n      Actions take a single argument -- the :class:`Executor` instance itself,\n      and can perform any operations within.\n\n      Custom actions can be simple functions, or subclass the |Action| class.\n    - **Metric:** Metrics are used to evaluate the output of models. They are\n      categorized into two classes:\n      :class:`~texar.torch.run.metric.SimpleMetric` and\n      :class:`~texar.torch.run.metric.StreamingMetric`. The only difference is\n      that streaming metrics support incremental computation of metric values,\n      so they can be used to aggregate results over the training set, or provide\n      intermediate results on-the-fly.\n\n      Custom metrics must subclass one of the above classes.\n\n    **Customization**\n\n    You can easily extend the :class:`Executor` class by subclassing it and\n    overriding methods. Methods of interest include:\n\n    - :meth:`_train_step`: Perform a single step of training (i.e. process a\n      single batch, call :meth:`backward`, and potentially call optimizer\n      updates). Takes the data batch as argument.\n    - :meth:`_validate_step`: Perform a single step of validation. Takes the\n      data batch as argument.\n    - :meth:`_test_step`: Perform a single step of testing. Takes the data batch\n      as argument.\n    - :meth:`_train_loop`: Runs the entire training loop. Takes the data\n      iterator as argument.\n    - :meth:`_train_loop`: Runs the entire validation loop. Takes the data\n      iterator as argument.\n    - :meth:`_test_loop`: Runs the entire testing loop. Takes the data iterator\n      as argument.\n\n    You can also define custom events by writing a new enum class and modifying\n    the :attr:`_EVENT_TYPES` attribute. Event points can be signaled by calling\n    :meth:`_fire_event`. For example:\n\n    .. code-block:: python\n\n        class GANEvent(Enum):\n            DiscriminatorUpdate = auto()\n            GeneratorUpdate = auto()\n\n        class GANExecutor(Executor):\n            _EVENT_TYPES = (Event, GANEvent)\n\n            def __init__(self, *args, optimizer_g, optimizer_d, **kwargs):\n                kwargs[""optimizer""] = {\n                    ""g"": optimizer_g,\n                    ""d"": optimizer_d,\n                }\n                super.__init__(*args, **kwargs)\n\n            def _train_step(self, batch):\n                self._fire_event(GANEvent.GeneratorUpdate, False)\n                z = torch.randn(len(batch), args.latent_dim)\n                fake_image = self.model.generator(z)\n                logits = self.model.discriminator(fake_image)\n                g_loss = F.binary_cross_entropy(logits, torch.ones(len(batch))\n                g_loss.backward()\n                self.optimizer[""g""].step()\n                self.optimizer[""g""].zero_grad()\n                self._fire_event(GANEvent.GeneratorUpdate, True)\n\n                self._fire_event(GANEvent.DiscriminatorUpdate, False)\n                real_logits = self.model.discriminator(batch.image)\n                fake_logits = self.model.discriminator(fake_image.detach())\n                real_loss = F.binary_cross_entropy(real_logits, torch.ones(len(batch)))\n                fake_loss = F.binary_cross_entropy(fake_logits, torch.zeros(len(batch)))\n                d_loss = (real_loss + fake_loss) / 2\n                d_loss.backward()\n                self.optimizer[""d""].step()\n                self.optimizer[""d""].zero_grad()\n                self._fire_event(GANEvent.DiscriminatorUpdate, True)\n\n                return {""g_loss"": g_loss, ""d_loss"": d_loss}\n\n    **Arguments**\n\n    The constructor of :class:`Executor` takes many arguments, almost all of\n    which are keyword-only and optional. Some arguments can take values of\n    multiple types, either a single instance of a specific type, or a list or\n    dictionary of values of that type.\n\n    Arguments grouped by functions:\n\n    - :ref:`General arguments <executor-general-args>`\n    - :ref:`Arguments for checkpoint management <executor-checkpoint-args>`\n    - :ref:`Arguments for tensorboard logging <executor-tbx-logging-args>`\n    - :ref:`Arguments for training <executor-train-args>`\n    - :ref:`Arguments for validation <executor-valid-args>`\n    - :ref:`Arguments for testing <executor-test-args>`\n    - :ref:`Arguments for logging <executor-log-args>`\n\n    .. _executor-general-args:\n\n    **General arguments:**\n\n    `model`: :torch_nn:`Module`\n        The model to train or evaluate. The model must be a subclass of\n        :torch_nn:`Module`, with its :meth:`forward` method taking a single\n        argument ``batch`` and returning a dictionary of :tensor:`Tensor`\\ s:\n\n        - The ``batch`` argument is of type :class:`~texar.torch.data.Batch`,\n          which is the batch object produced by your provided dataset.\n        - The returned dictionary must contain an entry named ``""loss""``, which\n          will be used as the loss to backpropagate during training. You can\n          also include other values and use them in metrics.\n\n        If the model performs different routines during training and evaluation\n        (for instance, a sequence-to-sequence model may train using\n        teacher-forcing but evaluate using beam search-based inference), you can\n        define another method :meth:`predict` following the same signature as\n        :meth:`forward`. To use :meth:`predict` instead of :meth:`forward` in\n        validation or testing, set :attr:`validate_mode` or :attr:`test_mode` to\n        ``""predict""`` instead of ``""eval""``.\n\n        If the model you use does not follow this convention, you will need to\n        wrap the model in a new class. The following example demonstrates how to\n        wrap :class:`~texar.torch.modules.XLNetRegressor`:\n\n        .. code-block:: python\n\n            class RegressorWrapper(tx.modules.XLNetRegressor):\n                def forward(self, batch):\n                    preds = super().forward(token_ids=batch.input_ids,\n                                            segment_ids=batch.segment_ids,\n                                            input_mask=batch.input_mask)\n                    loss = (preds - batch.label_ids) ** 2\n                    loss = loss.sum() / len(batch)\n                    return {""loss"": loss, ""preds"": preds}\n\n    `train_data`: :class:`~texar.torch.data.DatasetBase`\n        The dataset used during training. Must be specified for training.\n\n    `valid_data`: :class:`~texar.torch.data.DatasetBase`\n        The dataset used during validation. If not specified, you cannot perform\n        validation during training (e.g., by setting :attr:`validate_every`).\n\n    `test_data`: :class:`~texar.torch.data.DatasetBase`, or a list or dictionary\n        The dataset(s) used during testing. If not specified, you cannot perform\n        testing during training (e.g., by setting :attr:`test_every`).\n\n    `batching_strategy`: :class:`~texar.torch.data.BatchingStrategy`\n        The batching strategy to use for batching. This will be passed as the\n        :attr:`batching_strategy` argument for\n        :class:`~texar.torch.data.DataIterator` during training, and evaluation\n        if the corresponding mode is set to ``""eval""``.\n\n    `validate_mode`: str\n        The evaluation mode for validation. Available choices are ``""eval""``\n        and ``""predict""``. Defaults to ``""eval""``. When mode is set to\n        ``""eval""``, :meth:`forward` method of the model will be called; when\n        set to ``""predict""``, :meth:`predict` method of the model will be\n        called.\n\n    `test_mode`: str\n        The evaluation mode for testing. Available choices are ``""eval""``\n        and ``""predict""``. Defaults to ``""predict""``.\n\n    `device`: ``torch.device``\n        The device on which the model and data should be placed. Defaults to\n        `None`, in which case GPUs will be used if available.\n\n    .. _executor-tbx-logging-args:\n\n    **Arguments for tensorboard logging:**\n\n    `tbx_logging_dir`: str\n        Path to the directory for storing tensorboard logs.\n\n    `tbx_log_every`: |Condition|\n        Conditions that, when triggered, saves the tensorboard logs for train\n        metrics. If None, :ref:`log_every <executor-log-args>` will be used.\n\n    .. _executor-checkpoint-args:\n\n    **Arguments for checkpoint management:**\n\n    `checkpoint_dir`: str\n        Path to the directory for storing checkpoints. If not specified, you\n        cannot save/load the model during training (e.g., by setting\n        :attr:`save_every` or using\n        :class:`~texar.torch.run.action.reset_params`).\n\n    `max_to_keep`: int\n        The maximum number of checkpoints to keep in the checkpoint directory.\n        When the number of checkpoints exceed this limit, the oldest one will be\n        removed. If `None`, no such limit is imposed. Defaults to `None`.\n\n        .. note::\n            Be careful when saving periodic snapshots along with the best\n            performing checkpoint. Periodic snapshots might overwrite best\n            models if checkpoint limit is exceeded.\n\n            A better workaround is to only save periodic snapshots with the\n            built-in mechanism, and register a custom action for saving a single\n            best performing checkpoint:\n\n            .. code-block:: python\n\n                # Don\'t\n                executor = Executor(\n                    save_every=[cond.epoch(1), cond.validation(better=True)],\n                    max_to_keep=3, ...)\n\n                # Do\n                executor = Executor(\n                    save_every=cond.epoch(1),\n                    max_to_keep=3, ...)\n\n                @executor.on(cond.validation(better=True))\n                def save_best_model(executor):\n                    executor.save(path=another_directory, max_to_keep=1)\n\n    `save_every`: |Condition|, or a list\n        Conditions that, when triggered, saves the model.\n\n        In the following example, the model will be saved every 1000 iterations,\n        or whenever validation results improve.\n\n        .. code-block:: python\n\n            save_every=[cond.validation(better=True), cond.iteration(1000)]\n\n    `save_training_state`: bool\n        If `False`, only save model parameters in checkpoint. If `True`, also\n        save optimizer and scheduler states, along with random number generator\n        states from Python, NumPy, and PyTorch. Defaults to `True`.\n\n    .. _executor-train-args:\n\n    **Arguments for training:**\n\n    .. _train-metrics:\n\n    `train_metrics`: |Metric|, or a list or dictionary\n        The metrics computed over the training set. In case of multiple metrics,\n        two sets of metric values will be compared in the provided order.\n\n        For example, if two metrics `f1` (:class:`~texar.torch.run.metric.F1`)\n        and `loss` (:class:`~texar.torch.run.metric.Average`) are defined (in\n        this order), when comparing two sets of values, the one with a higher\n        `f1` is considered better. If the two sets have the same `f1` value, the\n        one with a lower `loss` is considered better.\n\n        Acceptable values include:\n\n        - A single |Metric|, or a list of |Metric|\\ s. These metrics will be\n          automatically named when they\'re logged.\n        - A tuple of (``str``, |Metric|), or a list of this. These metrics are\n          explicitly named according to the provided strings. Note that names\n          must be unique, and should be valid identifier names.\n        - An :class:`~collections.OrderedDict` mapping names to metrics. Note\n          that a plain (unordered) dictionary is not accepted.\n\n        .. note::\n            Metrics that are logged will be evaluated once every time logging\n            is performed. For efficiency considerations, such metrics should be\n            :class:`~texar.torch.run.metric.StreamingMetric`\\ s. Please take\n            extra care when implementing your own metrics.\n\n    `optimizer`: |Optimizer|, or a dictionary of hyperparameters\n        The optimizer used during training. This can be a |Optimizer| instance,\n        or a dictionary of hyperparameters that will be passed into\n        :meth:`texar.torch.utils.get_instance`. Must be specified for training.\n\n    `lr_scheduler`: |LRScheduler|, or a dictionary of hyperparameters, optional\n        The learning rate scheduler used during training. This can be an\n        |LRScheduler| instance, or a dictionary of hyperparameters that will be\n        passed into :meth:`texar.torch.utils.get_instance`.\n\n    `stop_training_on`: |Condition|, or a list\n        Conditions that, when triggered, will stop training.\n\n        In the following example, training will be terminated after 5 epochs or\n        20000 iterations, whichever comes first:\n\n        .. code-block:: python\n\n            stop_training_on=[cond.epoch(5), cond.iteration(20000)]\n\n    `num_iters_per_update`: int\n        Number of iterations to run before performing a parameter update. When\n        this value is greater than 1, the loss is scaled by its reciprocal.\n        Defaults to 1, in which case the parameters are updated after each\n        ``.backward()`` call.\n\n        This can be used to accumulate gradients across multiple batches, in\n        order to simulate the effect of using a large batch size on a machine\n        with limited memory.\n\n    `grad_clip`: float, optional\n        Maximum norm of the gradients. Please refer to\n        :torch_docs:`nn.utils.clip_grad_norm_ <nn.html#torch.nn.utils.clip_grad_norm_>`\n        for details. Defaults to `None`, i.e. no clipping.\n\n    .. _executor-valid-args:\n\n    **Arguments for validation:**\n\n    `valid_metrics`: |Metric|, or a list or dictionary\n        The metrics computed over the validation set. Please see\n        :ref:`train_metrics <train-metrics>` for details.\n\n    `validate_every`: |Condition|, or a list\n        Conditions that, when triggered, performs validation.\n\n        In the following example, the model will be validated once per epoch.\n\n        .. code-block:: python\n\n            validate_every=cond.epoch(1)\n\n    `plateau_condition`: |Condition|, or a list\n        Conditions that, when triggered, indicates that training has reached\n        a plateau, i.e., the model has stopped improving.\n\n        In the following example, we consider that training has reached a\n        plateau if validation metrics have not improved for 3 consecutive\n        validations.\n\n        .. code-block:: python\n\n            plateau_condition=cond.consecutive(cond.validation(better=False))\n\n    `action_on_plateau`: |Action|, or a list\n        Actions that will be called when training has reached a plateau.\n\n        In the following example, we perform patience-based early-stopping when\n        reaching plateaus. A patience of 2 means training will be terminated\n        after plateau is reached twice. We also scale the learning rate by 0.8,\n        and reset the model & optimizer parameters to the previous best\n        checkpoint.\n\n        .. code-block:: python\n\n            action_on_plateau=[\n                action.reset_params(), action.scale_lr(0.8),\n                action.early_stop(patience=2)]\n\n    .. _executor-test-args:\n\n    **Arguments for testing:**\n\n    `test_metrics`: |Metric|, or a list or dictionary\n        The metrics computed over the test set. Please see\n        :ref:`train_metrics <train-metrics>` for details. :meth:`test` can only\n        be called if :attr:`test_metrics` is not `None`.\n\n        .. note::\n             :attr:`valid_metrics` will be automatically shared with\n             :attr:`test_metrics` if:\n\n             1. :attr:`test_metrics` is `None`;\n             2. :attr:`validate_mode` is the same as :attr:`test_mode`.\n\n             In this case, calling validation while testing (or vice versa)\n             could cause incorrect results since the same |Metric| instances\n             are used.\n\n    `test_every`: |Condition|, or a list\n        Conditions that, when triggered, performs testing.\n\n        In the following example, the model will be tested whenever validation\n        results improve.\n\n        .. code-block:: python\n\n            test_every=cond.validation(better=True)\n\n    .. _executor-log-args:\n\n    **Arguments for logging:**\n\n    `log_every`: |Condition|, or a list\n        Conditions that, when triggered, performs logging.\n\n        In the following example, a log will be printed every 100 iterations,\n        and after every epoch.\n\n        .. code-block:: python\n\n            log_every=[cond.iteration(100), cond.epoch()]\n\n    `log_destination`: ``str``, IO object, or a list\n        Logging destinations. Acceptable values include:\n\n        - An IO object. This can be an opened file, ``sys.stdout``, or any other\n          file-like object.\n        - A string, denoting the path to a log file. :class:`Executor` will open\n          the file and close it when the program exits. The file will be opened\n          in ""append"" (``""a""``) mode to prevent accidentally overwriting\n          previous logs. To force overwrite, supply the file object instead.\n        - A list, with each element being one of the above.\n\n        When writing to a file, special syntax for terminals (e.g. color codes)\n        are emitted. Also, live progress is not written to files.\n\n        By default, the log is only written to ``sys.stdout``.\n\n    .. _log-format:\n\n    `log_format`: ``str``\n        The format string for logs during training.\n\n        The format string follows the syntax of Python format strings. The\n        status variables you can reference include:\n\n        - ``epoch`` (int): The current epoch.\n        - ``iteration`` (int): The current iteration.\n        - ``progress`` (float): The epoch progress represented in percentage,\n          i.e. a floating-point number between 0 and 100. It should be noted\n          that progress may not be accurate, and may not be available if the\n          data is loaded lazily.\n        - ``speed`` (float): Average number of data examples processed per\n          second. It should be noted that speed may not be accurate.\n        - ``time``: The current date and time. Time format can be set using the\n          ""format spec"" syntax of Python format strings, i.e.:\n          ``{time:%H:%M:%S}`` prints time in the format ``08:26:03``. If time\n          format is not specified, it is equivalent to\n          ``{time:%Y-%m-%d %H:%M:%S}``, which corresponds to the format\n          ``2018-07-06 08:26:03``. For more information on time formatting,\n          please refer to documentation for Python built-in function\n          :meth:`date.strftime`.\n        - ``metric``: An aggregated representation of all metrics, in the\n          format of ``<name1>: <value1>, <name2>: <value2>, ...``. The format\n          spec for ``metric`` will be applied to all metrics whose value\n          supports such format spec.\n          For more fine grained control over metric formatting, use the\n          following methods.\n        - ``metric.<name>``: Value of the metric under the specified name\n          ``<name>``.\n        - ``<name>``: Value of the metric under the specified name ``<name>``.\n\n          .. note::\n              The metric can only be looked-up if its name does not coincide\n              with built-in status variables. For instance, a metric named\n              ""`loss`"" can be looked-up by ``loss`` and ``metric.loss``, but a\n              metric named ""`time`"" can only be looked-up by ``metric.time``.\n\n        The default format string is:\n\n        .. code-block::\n\n            {time} : Epoch {epoch} @ {iteration}it ({progress}%, {speed}), {{{metric:.3f}}}\n\n        which produces logs similar to:\n\n        .. code-block::\n\n            2019-08-05 11:46:26 : Epoch 1 @ 800it (20.3%, 16.14ex/s), {loss: 0.358, Accuracy: 0.567}\n\n    `valid_log_format`: str\n        The format string for logs when validation completes. Please refer to\n        :ref:`log_format <log-format>` for details of the format string.\n\n        The default format string is:\n\n        .. code-block::\n\n            {time} : Epoch {epoch}, {split} result = {{{metric:.3f}}}\n\n        which produces logs similar to:\n\n        .. code-block::\n\n            2019-08-05 11:36:53 : Epoch 6, valid result = {PearsonR: 0.918, loss: 0.363}\n\n    `test_log_format`: str, optional\n        The format string for logs when testing completes. Please refer to\n        :ref:`log_format <log-format>` for details of the format string.\n\n        If `None`, the format string for validation (``valid_log_format``) is\n        used. Defaults to `None`.\n\n    `valid_progress_log_format`: str\n        The format string for logs during validation if live progress is\n        enabled. Please refer to :ref:`log_format <log-format>` for details of\n        the format string.\n\n        The default format string is:\n\n        .. code-block::\n\n            {time} : Evaluating on {split} ({progress}%, {speed}), {{{metric:.3f}}}\n\n        which produces logs similar to:\n\n        .. code-block::\n\n            2019-08-05 11:35:56 : Evaluating on test (65.4%, 1.12s/ex), {PearsonR: 0.911, loss: 0.384}\n\n    `test_progress_log_format`: str\n        The format string for logs during testing if live progress is\n        enabled. Please refer to :ref:`log_format <log-format>` for details of\n        the format string.\n\n        If `None`, the format string for validation\n        (``valid_progress_log_format``) is used. Defaults to `None`.\n\n    `print_model_arch`: bool\n        If `True`, model architecture and will logged in a readable format.\n        Defaults to `True`.\n\n    `show_live_progress`: bool or str\n        Controls whether live progress will be shown. Acceptable values include:\n\n        - `True`: Live progress is enabled for training, validation, and\n          testing.\n        - `False`: Live progress is disabled.\n        - ``""train""``, ``""valid""``, ``""test""``, or a list of these strings:\n          Live progress is enabled for the specified stages only.\n\n        If live progress is enabled for a certain stage, the specified format\n        string will be shown similar to a sticky progress bar at the bottom of\n        the terminal window, and updated after each iteration.\n\n        Note that live progress is only shown on terminals. It will not be\n        printed to log files.\n\n        .. warning::\n            This may incur extra overhead because an update requires\n            re-evaluating metrics. Make sure that all metrics logged are\n            :class:`~texar.torch.run.metric.StreamingMetric`\\ s. You can also\n            explicitly log only streaming metrics, or disable live progress for\n            certain stages.\n    """"""\n    # pylint: enable=line-too-long\n\n    _EVENT_TYPES = (Event,)\n    _CHECKPOINT_METAINFO_FILE = ""checkpoint.meta-info""\n    _CHECKPOINT_EXTENSION = "".pt""\n    _defaults: Dict[str, Any] = {\n        ""log_format"": ""{time} : Epoch {epoch} @ {iteration}it ""\n                      ""({progress}%, {speed}), {{{metric:.3f}}}"",\n        ""valid_progress_log_format"": ""{time} : Evaluating on {split} ""\n                                     ""({progress}%, {speed}), {{{metric:.3f}}}"",\n        ""valid_log_format"": ""{time} : Epoch {epoch}, ""\n                            ""{split} result = {{{metric:.3f}}}"",\n        ""log_destination"": [sys.stdout],\n    }\n\n    # TODO: Add loss as a special default metric? Otherwise would have to use\n    #   RunningAverage with display steps\n\n    # TODO: Prevent the same action triggering twice without any other steps.\n\n    _hooks: Dict[EventPoint, Dict[Optional[Condition], List[ActionFn]]]\n\n    status: utils.TrainingStatus\n\n    def __init__(self, model: nn.Module,\n                 *,\n                 train_data: Optional[DatasetBase] = None,\n                 valid_data: Optional[DatasetBase] = None,\n                 test_data: OptionalDict[DatasetBase] = None,\n                 batching_strategy: Optional[BatchingStrategy] = None,\n                 device: Optional[torch.device] = None,\n                 # tbX logging\n                 tbx_logging_dir: Optional[str] = None,\n                 tbx_log_every: Optional[Condition] = None,\n                 # Checkpoint\n                 checkpoint_dir: Optional[str] = None,\n                 max_to_keep: Optional[int] = None,\n                 save_every: OptionalList[Condition] = None,\n                 save_training_state: bool = True,\n                 # Training\n                 train_metrics: OptionalDict[Metric] = None,\n                 optimizer: Optional[Instance[Optimizer]] = None,\n                 lr_scheduler: Optional[Instance[LRScheduler]] = None,\n                 stop_training_on: OptionalList[Condition] = None,\n                 num_iters_per_update: int = 1,\n                 grad_clip: Optional[float] = None,\n                 # Validation\n                 valid_metrics: OptionalDict[Metric] = None,\n                 validate_every: OptionalList[Condition] = None,\n                 plateau_condition: OptionalList[Condition] = None,\n                 action_on_plateau: OptionalList[ActionFn] = None,\n                 validate_mode: str = \'eval\',\n                 # Testing\n                 test_metrics: OptionalDict[Metric] = None,\n                 test_every: OptionalList[Condition] = None,\n                 test_mode: str = \'predict\',\n                 # Logging\n                 log_every: OptionalList[Condition] = None,\n                 log_format: Optional[str] = None,\n                 log_destination: OptionalList[LogDest] = None,\n                 print_model_arch: bool = True,\n                 valid_log_format: Optional[str] = None,\n                 test_log_format: Optional[str] = None,\n                 valid_progress_log_format: Optional[str] = None,\n                 test_progress_log_format: Optional[str] = None,\n                 show_live_progress: Union[bool, MaybeList[str]] = False):\n\n        try:\n            from tqdm._utils import _environ_cols_wrapper, _term_move_up\n            self._tty_ncols: Optional[Callable] = _environ_cols_wrapper()\n            self._tty_move_up: str = _term_move_up()\n        except ImportError:\n            self._tty_ncols = None\n\n        # Meta variables\n        self._should_terminate = False  # .terminate()\n        self._should_remove_current_action = False  # .remove_action()\n        # Since an event action could be internally fire other events (e.g.\n        # validation), there could be nested events. Thus we keep track of\n        # the number of layers.\n        self._event_nested_layers = 0  # ._fire_event()\n        self._status_line_str: Optional[str] = None\n\n        self.model = model\n        self.train_data = train_data\n        self.valid_data = valid_data\n        self.test_data = utils.to_dict(test_data, default_name=""test"")\n        self.batching_strategy = batching_strategy\n\n        # Device placement\n        if device is None:\n            if torch.cuda.is_available():\n                device = torch.device(torch.cuda.current_device())\n            else:\n                device = torch.device(\'cpu\')\n        self.device = device\n        self.model.to(device)\n\n        # Logging\n        self._log_conditions = utils.to_list(log_every)\n        self._print_model_arch = print_model_arch\n\n        self.log_format: str = log_format or self._defaults[""log_format""]\n        self.valid_log_format: str = (\n                valid_log_format or self._defaults[""valid_log_format""])\n        self.valid_progress_log_format: str = (\n                valid_progress_log_format or\n                self._defaults[""valid_progress_log_format""])\n        self.test_log_format: str = test_log_format or self.valid_log_format\n        self.test_progress_log_format: str = (\n                test_progress_log_format or self.valid_progress_log_format)\n\n        # Checkpoint management\n        self.checkpoint_dir = (Path(checkpoint_dir)\n                               if checkpoint_dir is not None else None)\n        self.max_to_keep = max_to_keep\n        self._save_conditions = utils.to_list(save_every)\n        self._save_training_state = save_training_state\n\n        self._directory_exists = False\n        if self.checkpoint_dir is not None:\n            if not self.checkpoint_dir.exists():\n                self.checkpoint_dir.mkdir(parents=True)\n            else:\n                self._directory_exists = True\n\n        # Create logging files after checkpoint directory is created.\n        self._log_destination: List[IO[str]] = []\n        self._log_destination_is_tty: List[bool] = []\n        self._opened_files: List[IO[str]] = []\n        self.log_destination = log_destination or \\\n                               self._defaults[""log_destination""]\n\n        # Training loop\n        self.train_metrics = utils.to_metric_dict(train_metrics)\n        self.optimizer = utils.to_instance(\n            Optimizer, optimizer, [""torch.optim"", ""texar.core""],\n            extra_kwargs={""params"": self.model.parameters()})\n        self.lr_scheduler = utils.to_instance(\n            LRScheduler, lr_scheduler,\n            [""torch.optim.lr_scheduler"", ""texar.core""],\n            extra_kwargs={""optimizer"": self.optimizer})\n        self._stop_training_conditions = utils.to_list(stop_training_on)\n        self.num_iters_per_update = num_iters_per_update\n        self.grad_clip = grad_clip\n\n        # Validation\n        self.valid_metrics = utils.to_metric_dict(valid_metrics)\n        self._valid_conditions = utils.to_list(validate_every)\n        self.validate_mode = validate_mode\n        self._plateau_conditions = utils.to_list(plateau_condition)\n        self._actions_on_plateau = utils.to_list(action_on_plateau)\n\n        # Testing\n        if (test_metrics is None and valid_metrics is not None and\n                validate_mode == test_mode):\n            self.test_metrics = self.valid_metrics\n        else:\n            self.test_metrics = utils.to_metric_dict(test_metrics)\n        self._test_conditions = utils.to_list(test_every)\n        self.test_mode = test_mode\n\n        # Initialize hooks.\n        self._hooks = {\n            (event, point): defaultdict(list)\n            for event_class in self._EVENT_TYPES\n            for event in event_class for point in (False, True)}\n\n        self.status = {\n            ""epoch"": 0,\n            ""iteration"": 0,\n            ""split"": ""train"",\n            ""metric"": self.train_metrics,\n            ""eval_metric"": self.valid_metrics,\n        }\n\n        # TODO: Move trackers to another process, as in tqdm? Refer to\n        #   `tqdm.TMonitor`\n        self._train_tracker = utils.ProgressTracker()\n        self._valid_tracker = utils.ProgressTracker()\n        self._test_tracker = utils.ProgressTracker()\n\n        if isinstance(show_live_progress, bool):\n            if show_live_progress is True:\n                show_live_progress = [""train"", ""valid"", ""test""]\n            else:\n                show_live_progress = []\n        elif isinstance(show_live_progress, str):\n            show_live_progress = [show_live_progress]\n        if any(stage not in [""train"", ""valid"", ""test""]\n               for stage in show_live_progress):\n            raise ValueError(""Invalid value for `show_live_progress`"")\n        self._register_logging_actions(show_live_progress)\n\n        # tbx logging\n        if tbx_logging_dir is not None:\n            try:\n                from tensorboardX import SummaryWriter\n            except ImportError:\n                print(\n                    ""To use tensorboard support with Executor, please ""\n                    ""install tensorboardX. Please see ""\n                    ""https://tensorboardx.readthedocs.io for further ""\n                    ""details"")\n                raise\n            self.summary_writer = SummaryWriter(tbx_logging_dir)\n            self._tbx_logging_conditions = utils.to_list(\n                tbx_log_every if tbx_log_every is not None else log_every)\n            self._register_tbx_logging_actions()\n\n        # Register other events and actions.\n        for cond in self._valid_conditions:\n            self._register_action(cond, self._validate.__func__)  # type: ignore\n        for cond in self._test_conditions:\n            self._register_action(cond, self.test.__func__)  # type: ignore\n        for cond in self._save_conditions:\n            self._register_action(cond, self.save.__func__)  # type: ignore\n\n        for cond in self._plateau_conditions:\n            for action in self._actions_on_plateau:\n                self._register_action(cond, action)\n\n        for cond in self._stop_training_conditions:\n            self._register_action(cond, self.terminate.__func__)  # type: ignore\n\n    def write_log(self, log_str: str, *, mode: str = ""info"",\n                  skip_tty: bool = False, skip_non_tty: bool = False) -> None:\n        r""""""Write a string to log.\n\n        Args:\n            log_str (str): The string to log.\n            mode (str): The logging mode. Supported values are:\n\n                - ``""log""``: A plain log. Logs created by :attr:`log_every` and\n                  :attr:`eval_log_every` are in this format.\n                - ``""info""``: Indicates a result or notification. Includes a\n                  header with ``""INFO""`` and a timestamp in green color.\n                  This is the default mode.\n                - ``""warning""``: Indicates an unexpected or incorrect situation.\n                  Includes a header with ``""WARNING""`` and a timestamp. The\n                  entire warning is in red color.\n            skip_tty: If `True`, the log string will not be written to terminal\n                log destinations. Defaults to `False`.\n            skip_non_tty: If `True`, the log string will not be written to\n                non-terminal (e.g., files) destinations. Defaults to `False`.\n        """"""\n        if mode == ""log"":\n            pass\n        elif mode == ""info"":\n            time_str = time.strftime(""%Y-%m-%d %H:%M:%S"")\n            pass\n            log_str = utils.color(f""INFO {time_str} : "", ""green"") + log_str\n        elif mode == ""warning"":\n            time_str = time.strftime(""%Y-%m-%d %H:%M:%S"")\n            log_str = f""WARNING {time_str} : {log_str}""\n            log_str = utils.color(log_str, ""red"")\n        else:\n            raise ValueError(f""Invalid logging mode {mode}"")\n        if not skip_tty and self._status_line_str is not None:\n            self._write_log(log_str, skip_non_tty=True, clear_line=True)\n            self._write_log(\n                self._status_line_str, skip_non_tty=True, newline=False)\n            if not skip_non_tty:\n                self._write_log(log_str, skip_tty=True, skip_non_tty=False)\n        else:\n            self._write_log(log_str, skip_tty, skip_non_tty)\n\n    def set_status_line(self, status_str: Optional[str]):\n        if status_str is None:\n            status_str = """"  # just clear the line\n        self._write_log(status_str, skip_non_tty=True,\n                        newline=False, clear_line=True)\n        self._status_line_str = status_str\n\n    # pylint: disable=unused-argument,no-self-use,function-redefined\n\n    @overload\n    def on(self, cond: Condition) -> Callable[[ActionFn], ActionFn]:\n        ...\n\n    @overload\n    def on(self, cond: Condition, func: ActionFn) -> \'Executor\':\n        ...\n\n    def on(self, cond: Condition, func=None):\n        r""""""Register a function as an action triggered on a condition. For\n        example:\n\n        .. code-block:: python\n\n            executor = Executor(...)\n\n            @executor.on(cond.iteration(10, mode=""valid""))\n            def log_during_validation(executor):\n                logging.info(""Validation iter %d"", executor.status[""iteration""])\n\n        The action function takes exactly one argument: the executor instance\n        itself.\n\n        Args:\n            cond: The condition that will call the function when triggered. Must\n                be of type :class:`Condition`.\n            func (optional): The function to register. If not `None`, the\n                function will be registered; if `None`, a decorator will be\n                returned.\n\n        :returns:\n            - If :attr:`func` is `None`, this method will return a decorator to\n              wrap around the function to register as hook.\n            - If :attr:`func` is not `None`, this method will return the\n              :class:`Executor` itself, allowing chained calls.\n        """"""\n        if not isinstance(cond, Condition):\n            raise ValueError(f""Invalid condition {cond}"")\n\n        if func is not None:\n            self._register_action(cond, func)\n            return self\n\n        def wrapper(f):\n            self._register_action(cond, f)\n            return f\n\n        return wrapper\n\n    @overload\n    def on_event(self, event: Event, point: str = \'end\') \\\n            -> Callable[[ActionFn], ActionFn]:\n        ...\n\n    @overload\n    def on_event(self, event: Event, point: str, func: ActionFn) -> \'Executor\':\n        ...\n\n    def on_event(self, event, point=\'end\', func=None):\n        r""""""Register a function as an action triggered on an event point. For\n        example:\n\n        .. code-block:: python\n\n            executor = Executor(...)\n\n            @executor.on_event(Event.Epoch, \'end\')\n            def log_at_end_of_epoch(executor):\n                logging.info(""Epoch %d done"", executor.status[""epoch""])\n\n        The action function takes exactly one argument: the executor instance\n        itself.\n\n        Args:\n            event: The event to hook on. Must be an enum value from\n                :class:`~Event`.\n            point (str): The point of event to hook on. Supported values are\n                ``""begin""`` and ``""end""``. Defaults to ``""end""``.\n            func (optional): The function to register. If not `None`, the\n                function will be registered; if `None`, a decorator will be\n                returned.\n\n        :returns:\n            - If :attr:`func` is `None`, this method will return a decorator to\n              wrap around the function to register as hook.\n            - If :attr:`func` is not `None`, this method will return the\n              :class:`Executor` itself, allowing chained calls.\n        """"""\n        if (not isinstance(event, self._EVENT_TYPES) or\n                point not in (\'begin\', \'end\')):\n            raise ValueError(f""Invalid event point ({event}, {point})"")\n        end = (point == \'end\')\n\n        if func is not None:\n            self._register_hook((event, end), func)\n            return self\n\n        def wrapper(f):\n            self._register_hook((event, end), f)\n            return f\n\n        return wrapper\n\n    # pylint: enable=unused-argument,no-self-use,function-redefined\n\n    def save(self, path: Optional[str] = None,\n             save_training_state: Optional[bool] = None):\n        r""""""Save a snapshot of the current state to a checkpoint file.\n\n        Args:\n            path (str, optional): Path to the checkpoint directory. If `None`,\n                :attr:`checkpoint_dir` in the constructor arguments will be\n                used. Defaults to `None`.\n            save_training_state (bool): If `True`, will save entire training\n                state from checkpoint. If `False`, only save model weights.\n                If `None`, the value from the constructor arguments will be\n                used. Defaults to `None`.\n        """"""\n        if path is not None:\n            ckpt_dir = Path(path)\n        elif self.checkpoint_dir is not None:\n            ckpt_dir = self.checkpoint_dir\n        else:\n            raise ValueError(\n                ""`path` must be specified when `checkpoint_dir` is `None`"")\n\n        if save_training_state is None:\n            save_training_state = self._save_training_state\n\n        # Load the checkpoint meta-info file.\n        meta_path = ckpt_dir / self._CHECKPOINT_METAINFO_FILE\n        if meta_path.exists():\n            try:\n                with meta_path.open(""rb"") as f:\n                    meta_dict = pickle.load(f)\n            except (EOFError, IOError):\n                meta_dict = {}\n        else:\n            meta_path.touch()\n            meta_dict = {}\n\n        # Remove earliest checkpoints if exceeds `max_to_keep`.\n        if self.max_to_keep is not None:\n            while len(meta_dict) >= self.max_to_keep:\n                checkpoint_name = min(\n                    meta_dict, key=lambda name: meta_dict[name][""timestamp""])\n                (ckpt_dir / checkpoint_name).unlink()\n                del meta_dict[checkpoint_name]\n                self.write_log(f""Previous checkpoint {checkpoint_name} removed ""\n                               f""due to `max_to_keep`(={self.max_to_keep}) ""\n                               f""limit"", mode=""info"")\n\n        timestamp = time.time()\n        checkpoint_name = str(timestamp) + self._CHECKPOINT_EXTENSION\n        ckpt_path = ckpt_dir / checkpoint_name\n        if ckpt_path.exists():\n            idx = 0\n            while True:\n                checkpoint_name = (\n                        str(timestamp) + f"".{idx}"" + self._CHECKPOINT_EXTENSION)\n                ckpt_path = ckpt_dir / checkpoint_name\n                if not ckpt_path.exists():\n                    break\n        if save_training_state and self.optimizer is not None:\n            train_state = utils.SavedTrainingState(\n                model=self.model.state_dict(),\n                optimizer=self.optimizer.state_dict(),\n                scheduler=(self.lr_scheduler.state_dict()\n                           if self.lr_scheduler is not None else None),\n                system_rng=random.getstate(),\n                numpy_rng=np.random.get_state(),\n                torch_rng=torch.random.get_rng_state(),\n            )\n            torch.save(train_state, str(ckpt_path))\n        else:\n            torch.save(self.model.state_dict(), str(ckpt_path))\n        meta_dict[checkpoint_name] = {\n            ""status"": self.status,\n            ""timestamp"": timestamp,\n        }\n        with meta_path.open(""wb"") as f:\n            pickle.dump(meta_dict, f)\n\n        self.write_log(f""Current checkpoint saved to {ckpt_path}"", mode=""info"")\n\n    def load(self, path: Optional[str] = None,\n             load_training_state: bool = True,\n             allow_failure: bool = False) -> Optional[Path]:\n        r""""""Load a previous model checkpoint from file.\n\n        Args:\n            path (str, optional): Path to a specific checkpoint or a checkpoint\n                directory. If a directory is specified, the most recent\n                checkpoint in the directory is loaded. If `None`,\n                :attr:`checkpoint_dir` in the constructor arguments will be\n                used. Defaults to `None`.\n            load_training_state (bool): If `True`, will load entire training\n                state from checkpoint (if the checkpoint contains training\n                state). Otherwise, just load model weights. Defaults to `True`.\n            allow_failure (bool): If `True`, no exceptions will be raised if\n                no checkpoints were found. Defaults to `False`. Note that\n                exceptions are still raised if the provided :attr:`path` does\n                not exist, or the selected checkpoint is corrupted.\n\n        Returns:\n            Path of the loaded checkpoint, or `None` if load failed.\n        """"""\n        if path is not None:\n            ckpt_path = Path(path)\n        elif self.checkpoint_dir is not None:\n            ckpt_path = self.checkpoint_dir\n        else:\n            raise ValueError(\n                ""`path` must be specified when `checkpoint_dir` is `None`"")\n        if ckpt_path.is_dir():\n            try:\n                meta_path = ckpt_path / self._CHECKPOINT_METAINFO_FILE\n                if meta_path.exists():\n                    with meta_path.open(""rb"") as f:\n                        meta_dict = pickle.load(f)\n                    best_metric, best_m_time, best_ckpt_name = max(\n                        (utils.MetricList(info[""status""][""eval_metric""]),\n                         info[""timestamp""], name)\n                        for name, info in meta_dict.items())\n                    status = meta_dict[best_ckpt_name][""status""]\n                    ckpt_path = self.checkpoint_dir / best_ckpt_name\n                    metric_vals = [(name, best_metric.values[name])\n                                   for name in best_metric.metrics]\n                    metric_str = "", "".join(\n                        f""{name}: "" + (\n                            f""{value:.3f}"" if isinstance(value, float)\n                            else f""{value}"")\n                        for name, value in metric_vals)\n                    time_str = datetime.fromtimestamp(best_m_time).strftime(\n                        ""%Y-%m-%d %H:%M:%S"")\n                    load_info_str = (f""saved at: {time_str}, ""\n                                     f""meta-info: epoch={status[\'epoch\']}, ""\n                                     f""iteration={status[\'iteration\']}, ""\n                                     f""valid metrics={{{metric_str}}}"")\n                else:\n                    m_time, ckpt_path = max(\n                        (name.stat().st_mtime, name)\n                        for name in ckpt_path.iterdir()\n                        if name.suffix == self._CHECKPOINT_EXTENSION)\n                    time_str = datetime.fromtimestamp(m_time).strftime(\n                        ""%Y-%m-%d %H:%M:%S"")\n                    load_info_str = f""saved at: {time_str}""\n                    self.write_log(\n                        ""Checkpoint meta-info not found. Will load the most ""\n                        ""recent checkpoint in directory"", mode=""warning"")\n            except (EOFError, IOError, ValueError):\n                # EOFError, IOError: pickle.load\n                # ValueError: max() arg is an empty sequence\n                if allow_failure:\n                    return None\n                raise\n        else:\n            load_info_str = """"\n\n        checkpoint = torch.load(str(ckpt_path), map_location=self.device)\n        if isinstance(checkpoint, utils.SavedTrainingState):\n            self.model.load_state_dict(checkpoint.model)\n            if load_training_state:\n                # TODO: Also somehow save/load data iterator state?\n                if self.optimizer is not None:\n                    self.optimizer.load_state_dict(checkpoint.optimizer)\n                if self.lr_scheduler is not None:\n                    assert checkpoint.scheduler is not None\n                    self.lr_scheduler.load_state_dict(checkpoint.scheduler)\n                random.setstate(checkpoint.system_rng)\n                np.random.set_state(checkpoint.numpy_rng)\n                torch.random.set_rng_state(checkpoint.torch_rng.cpu())\n        else:\n            self.model.load_state_dict(checkpoint)\n\n        if load_info_str != """":\n            self.write_log(f""Checkpoint ({load_info_str}) ""\n                           f""loaded from {ckpt_path}."", mode=""info"")\n        else:\n            self.write_log(f""Checkpoint loaded from {ckpt_path}."", mode=""info"")\n        return ckpt_path\n\n    def terminate(self) -> None:\n        r""""""Terminate training. This method is intended to be called within\n        actions. An example use case would be to implement a custom\n        early-stopping mechanism.\n\n        It is guaranteed that no other event points will be fired once\n        :meth:`terminate` is called. However, conditions and actions under the\n        same event point is still called.\n        """"""\n        if not self._event_nested_layers:\n            raise ValueError(f""terminate() should only be called ""\n                             ""within event actions"")\n        self._should_terminate = True\n\n    def remove_action(self) -> None:\n        r""""""Remove the current action being run. This method is intended to be\n        called within actions. An example use case would be to implement an\n        action that is only run once at a certain event point.\n        """"""\n        if not self._event_nested_layers:\n            raise ValueError(f""remove_action() should only be called ""\n                             ""within event actions"")\n        self._should_remove_current_action = True\n\n    def train(self):\n        r""""""Start the training loop.\n        """"""\n        # Check whether files have been opened, to avoid re-opening and closing.\n        # This could happen when, e.g., `test` is called in a registered hook\n        # during training.\n        should_open_file = (len(self._opened_files) == 0)\n        if should_open_file:\n            self._open_files()\n\n        if self._directory_exists:\n            self.write_log(\n                f""Specified checkpoint directory \'{self.checkpoint_dir}\' ""\n                f""exists, previous checkpoints might be erased"", mode=""warning"")\n\n        if len(self._stop_training_conditions) == 0:\n            self.write_log(\n                ""`stop_training_on` is not configured. Unless an event action ""\n                ""calls `executor.terminate()`, training will run indefinitely."",\n                mode=\'warning\')\n\n        # Initialize optimizer.\n        if self.optimizer is None:\n            raise ValueError(""Optimizer is not specified"")\n        self.optimizer.zero_grad()\n\n        # Initialize dataset.\n        if self.train_data is None:\n            raise ValueError(""No training dataset is specified"")\n        self.train_data.to(self.device)\n        iterator = DataIterator(self.train_data, self.batching_strategy)\n        if len(self._valid_conditions) > 0:\n            if self.valid_data is None:\n                raise ValueError(""Validation will be performed, but no ""\n                                 ""validation dataset is specified."")\n            self.valid_data.to(self.device)\n\n        self.status[""split""] = ""train""\n\n        self._fire_event(Event.Training, False)\n        self.write_log(""Training started"", mode=""info"")\n\n        if self._print_model_arch:\n            # TODO: Also somehow gather training settings?\n            model_repr = utils.repr_module(self.model)\n            self.write_log(f""Model architecture:\\n{model_repr}"", mode=""info"")\n\n        self.model.train()\n\n        try:\n            data_size: Optional[int] = len(self.train_data)\n        except TypeError:\n            # Try again after one epoch. If still doesn\'t work, it\'s not going\n            # to work ever.\n            def _try_get_data_size(executor: \'Executor\'):\n                assert executor.train_data is not None\n                try:\n                    size = len(executor.train_data)\n                    executor._train_tracker.set_size(size)  # pylint: disable=protected-access\n                except TypeError:\n                    pass\n                executor.remove_action()\n\n            self._register_hook((Event.Epoch, True), _try_get_data_size)\n            data_size = None\n        self._train_tracker.set_size(data_size)\n\n        # Main training loop.\n        self._train_tracker.start()\n        try:\n            self._train_loop(iterator)\n        except utils.ExecutorTerminateSignal:\n            self.write_log(""Training terminated"", mode=\'info\')\n        finally:\n            self._train_tracker.stop()\n\n        self._fire_event(Event.Training, True)\n\n        # Close the log files if we opened them here.\n        if should_open_file:\n            self._close_files()\n\n    def test(self, dataset: OptionalDict[DatasetBase] = None):\n        r""""""Start the test loop.\n\n        Args:\n            dataset (optional): The dataset(s) to test on. Acceptable values\n                include:\n\n                - A single :attr:`~texar.torch.data.DatasetBase` instance.\n                - A list of :attr:`~texar.torch.data.DatasetBase` instances.\n                - A dictionary mapping names to\n                  :attr:`~texar.torch.data.DatasetBase` instances.\n\n                If `None`, :attr:`test_data` from the constructor arguments is\n                used. Defaults to `None`.\n        """"""\n        # Check whether files have been opened, to avoid re-opening and closing.\n        # This could happen when, e.g., `test` is called in a registered hook\n        # during training.\n        should_open_file = (len(self._opened_files) == 0)\n        if should_open_file:\n            self._open_files()\n\n        if dataset is None and self.test_data is None:\n            raise ValueError(""No testing dataset is specified"")\n        if len(self.test_metrics) == 0:\n            raise ValueError(\n                ""No testing metric is specified. Validation metrics are not ""\n                ""used due to different modes for validation and test"")\n        datasets = (self.test_data if dataset is None\n                    else utils.to_dict(dataset, default_name=""test""))\n\n        model_mode = self.model.training\n        self.model.train(self.test_mode == ""train"")\n        for name, data in datasets.items():\n            self.status[""split""] = name\n\n            # Initialize metrics.\n            for metric in self.test_metrics.values():\n                metric.reset()\n\n            self._fire_event(Event.Testing, False)\n            data.to(self.device)\n            if self.test_mode == ""eval"":\n                iterator = DataIterator(data, self.batching_strategy)\n            else:\n                iterator = DataIterator(data)\n            try:\n                data_size: Optional[int] = len(data)\n            except TypeError:\n                data_size = None\n            self._test_tracker.set_size(data_size)\n\n            self._test_tracker.start()\n            try:\n                with torch.no_grad():\n                    self._test_loop(iterator)\n            except utils.ExecutorTerminateSignal:\n                self.write_log(\n                    ""Testing terminated. This is likely unintended. Please ""\n                    ""check your custom actions."", mode=\'warning\')\n            finally:\n                self._test_tracker.stop()\n\n            self._fire_event(Event.Testing, True)\n\n        self.model.train(model_mode)\n\n        # Close the log files if we opened them here.\n        if should_open_file:\n            self._close_files()\n\n    def _register_logging_actions(self, show_live_progress: List[str]):\n        # Register logging actions.\n        Points = Sequence[Union[Condition, Event]]\n        LogFn = Callable[[\'Executor\'], str]\n\n        def _register(points: Points, fn: ActionFn):\n            for cond_or_event in points:\n                if isinstance(cond_or_event, Condition):\n                    self._register_action(cond_or_event, fn)\n                else:\n                    self._register_hook((cond_or_event, True), fn)\n\n        def _register_log_fn(points: Points, logging_fn: LogFn,\n                             **log_kwargs):\n            def log_fn(executor: \'Executor\'):\n                executor.write_log(\n                    logging_fn(executor), mode=""log"", **log_kwargs)\n\n            _register(points, log_fn)\n\n        def _flush_log_hook(executor: \'Executor\'):\n            executor._write_log("""", skip_non_tty=True)  # pylint: disable=protected-access\n\n        def _register_status_fn(update_event: Event, log_fn: LogFn):\n            def status_fn(executor: \'Executor\'):\n                executor.set_status_line(log_fn(executor))\n\n            self._register_hook((update_event, True), status_fn)\n\n        if ""train"" in show_live_progress:\n            # Training progress is displayed by writing the log string after\n            # every iteration, and clearing the line. In this case, when a\n            # normal log condition is triggered, a newline character is simply\n            # printed. However, this applies to terminal output only, so we\n            # revert to the original approach for files.\n            train_log_fn = self._create_logging_fn(\n                self.log_format, self.train_metrics, self._train_tracker,\n                warn_non_streaming_metric=True)\n            _register_status_fn(Event.Iteration, train_log_fn)\n            for cond in self._log_conditions:\n                self._register_action(cond, _flush_log_hook)\n            _register_log_fn(self._log_conditions, train_log_fn, skip_tty=True)\n        else:\n            train_log_fn = self._create_logging_fn(\n                self.log_format, self.train_metrics, self._train_tracker)\n            _register_log_fn(self._log_conditions, train_log_fn)\n\n        def _register_eval_log_fn(name: str,\n                                  metrics: \'OrderedDict[str, Metric]\',\n                                  tracker: utils.ProgressTracker,\n                                  log_format: str, progress_log_format: str,\n                                  update_event: Event, end_event: Event):\n            log_fn = self._create_logging_fn(log_format, metrics, tracker)\n            if name in show_live_progress:\n                progress_log_fn = self._create_logging_fn(\n                    progress_log_format, metrics, tracker,\n                    warn_non_streaming_metric=True)\n                _register_status_fn(update_event, progress_log_fn)\n\n                def erase_status_and_log_fn(executor: \'Executor\'):\n                    # Compute metrics before erasing status line, so there won\'t\n                    # be a blank period.\n                    log_str = log_fn(executor)\n                    executor.set_status_line(None)\n                    executor.write_log(log_str, mode=""log"")\n\n                self._register_hook((end_event, True), erase_status_and_log_fn)\n            else:\n                _register_log_fn([end_event], log_fn)\n\n        _register_eval_log_fn(\n            ""valid"",\n            self.valid_metrics, self._valid_tracker,\n            self.valid_log_format, self.valid_progress_log_format,\n            Event.ValidationIteration, Event.Validation)\n        _register_eval_log_fn(\n            ""test"",\n            self.test_metrics, self._test_tracker,\n            self.test_log_format, self.test_progress_log_format,\n            Event.TestingIteration, Event.Testing)\n\n    def _register_tbx_logging_actions(self):\n\n        # Register logging actions.\n        Points = Sequence[Union[Condition, Event]]\n\n        def _register(points: Points, fn: ActionFn):\n            for cond_or_event in points:\n                if isinstance(cond_or_event, Condition):\n                    self._register_action(cond_or_event, fn)\n                else:\n                    self._register_hook((cond_or_event, True), fn)\n\n        def tbx_train_log_fn(executor: \'Executor\'):\n            train_metrics = executor.train_metrics\n\n            # log the metrics here\n            for key, value in train_metrics.items():\n                self.summary_writer.add_scalar(\n                    f""train/{key}"", value.value(), executor.status[""iteration""])\n\n        def tbx_valid_log_fn(executor: \'Executor\'):\n            valid_metrics = executor.valid_metrics\n\n            for key, value in valid_metrics.items():\n                self.summary_writer.add_scalar(\n                    f""valid/{key}"", value.value(), executor.status[""iteration""])\n\n        _register(self._tbx_logging_conditions, tbx_train_log_fn)\n        _register(self._valid_conditions, tbx_valid_log_fn)\n\n    def _write_log(self, log_str: str,\n                   skip_tty: bool = False, skip_non_tty: bool = False,\n                   newline: bool = True, clear_line: bool = False):\n        r""""""Write a string to log.\n\n        Args:\n            log_str (str): The string to log.\n            skip_tty: If `True`, the log string will not be written to terminal\n                log destinations. Defaults to `False`.\n            skip_non_tty: If `True`, the log string will not be written to\n                non-terminal (e.g., files) destinations. Defaults to `False`.\n            newline: If `True`, print a newline character after printing the log\n                string. Defaults to `True`.\n            clear_line: If `True`, clear the line before printing. This only\n                works for terminals. Defaults to `False`.\n        """"""\n        if not skip_tty:\n            for dest, isatty in zip(\n                    self._log_destination, self._log_destination_is_tty):\n                if not isatty:\n                    continue\n                if clear_line:\n                    dest.write(utils.CLEAR_LINE)\n                    if (self._tty_ncols is not None and\n                            self._status_line_str is not None):\n                        n_cols = self._tty_ncols(dest.fileno())\n                        status_len = len(self._status_line_str)\n                        n_lines = (status_len - 1) // n_cols + 1\n                        if n_lines > 1:\n                            dest.write(self._tty_move_up * (n_lines - 1))\n                dest.write(log_str)\n                if newline:\n                    dest.write(""\\n"")\n                dest.flush()\n        if not skip_non_tty:\n            plain_str = re.sub(r""\\033\\[\\d{1,2}[Km]"", """", log_str)\n            for dest, isatty in zip(\n                    self._log_destination, self._log_destination_is_tty):\n                if isatty:\n                    continue\n                # Erase color codes if the destination is not a terminal.\n                dest.write(plain_str)\n                if newline:\n                    dest.write(""\\n"")\n                dest.flush()\n\n    def _create_logging_fn(self, format_str: str,\n                           metrics: \'OrderedDict[str, Metric]\',\n                           tracker: utils.ProgressTracker,\n                           warn_non_streaming_metric: bool = False) \\\n            -> Callable[[\'Executor\'], str]:\n        r""""""Given a logging format string, create a function that takes the\n        executor instance as argument and returns the logging string.\n\n        Args:\n            format_str (str): The logging format string.\n            metrics: The metrics dictionary that will be used in the logging\n                hook function.\n            tracker: The progress tracker that will be used in the logging hook\n                function.\n            warn_non_streaming_metric (bool): If `True`, will issue a warning\n                if any of the provided metric is not a\n                :class:`~texar.torch.run.metric.StreamingMetric`. Defaults to\n                `False`. This is set to `True` when the logging string is used\n                as the status line.\n\n        Returns:\n            A hook function to print logs given the format string. Note that\n            the hook function can accept additional arguments to pass to\n            :meth:`executor.write_log`, allowing it to be used in combination\n            with :meth:`functools.partial`.\n        """"""\n        format_var_regex = re.compile(r""{([a-zA-Z_0-9]+)(:([^{}]+))?}"")\n        format_vars: Dict[str, Optional[str]] = {\n            match.group(1): match.group(3)\n            for match in format_var_regex.finditer(format_str)\n        }\n\n        # Set default time format.\n        if ""time"" in format_vars and format_vars[""time""] is None:\n            format_vars[""time""] = ""%Y-%m-%d %H:%M:%S""\n            format_str = re.sub(r""{time(:([^{}]+))?}"", ""{time}"", format_str)\n\n        # Set metric print formats for aggregated format variable.\n        if ""metric"" in format_vars and format_vars[""metric""] is not None:\n            if warn_non_streaming_metric:\n                # Check if any metrics are non-streaming.\n                for name, metric in metrics.items():\n                    if not isinstance(metric, StreamingMetric):\n                        self.write_log(\n                            f""Metric \\""{name}\\"" (`{metric.__class__.__name__}`)""\n                            f"" is not a StreamingMetric, which may cause ""\n                            f""performance issues"", mode=""warning"")\n\n            # Check which metrics can be printed with specified format.\n            fmt_metrics: Set[str] = set()\n            metric_format = format_vars[""metric""]\n            for name, metric in metrics.items():\n                try:\n                    metric_format.format(metric.value())\n                    fmt_metrics.add(name)\n                except ValueError:\n                    pass\n            metric_format_parts = []\n            for name in metrics:\n                metric_format_parts.append(\n                    f""{name}: {{{name}""\n                    f""{\':\' + metric_format if name in fmt_metrics else \'\'}}}"")\n            metric_format_str = \', \'.join(metric_format_parts)\n            format_vars[""metric""] = metric_format_str\n            format_str = re.sub(r""{metric(:([^{}]+))?}"", ""{metric}"", format_str)\n\n        # Gather metrics represented as ""name"" or ""metric.name"".\n        metrics_to_print: Set[str] = set()\n        for name in format_vars:\n            if name in self.status or name in [""time"", ""progress"", ""speed""]:\n                # built-in name\n                pass\n            elif name in metrics:\n                metrics_to_print.add(name)\n            else:\n                raise ValueError(\n                    f""Invalid status variable name \'{name}\' in format string"")\n\n        format_str_wo_progress = re.sub(\n            r""{progress(:([^{}]+))?}"", ""{progress}"", format_str)\n\n        @no_type_check\n        def log_fn(executor: \'Executor\') -> str:\n            format_args = executor.status.copy()\n            cur_format_str = format_str\n            if ""time"" in format_vars:\n                format_args[""time""] = time.strftime(format_vars[""time""])\n            if ""metric"" in format_vars:\n                metric_vals = {\n                    name: metric.value() for name, metric in metrics.items()}\n                metric_str = format_vars[""metric""].format(**metric_vals)\n                format_args[""metric""] = metric_str\n            if ""speed"" in format_vars:\n                format_args[""speed""] = tracker.speed()\n            if ""progress"" in format_vars:\n                progress = tracker.progress()\n                if progress is not None:\n                    if format_vars[""progress""] is None:\n                        progress = round(progress, 1)\n                    format_args[""progress""] = progress\n                else:\n                    cur_format_str = format_str_wo_progress\n                    format_args[""progress""] = ""unknown""\n            format_args.update({\n                name: metrics[name].value() for name in metrics_to_print})\n            log_str = cur_format_str.format(**format_args)\n            return log_str\n\n        return log_fn\n\n    def _register_action(self, cond: Condition, action: ActionFn):\n        for event_point in cond.hooks:\n            self._register_hook(event_point, action, cond)\n\n    def _register_hook(self, event_point: EventPoint, action: ActionFn,\n                       cond: Optional[Condition] = None):\n        # TODO: Check if a nested condition contains something that is already\n        #   registered.\n        try:\n            self._hooks[event_point][cond].append(action)\n        except KeyError:\n            raise ValueError(\n                f""Specified hook point {event_point} is invalid"") from None\n\n    def _open_files(self):\n        self._opened_files = []\n        self._log_destination = []\n        self._log_destination_is_tty = []\n\n        for dest in utils.to_list(self.log_destination):\n            if isinstance(dest, (str, Path)):\n                # Append to the logs to prevent accidentally overwriting\n                # previous logs.\n                file = open(dest, ""a"")\n                self._opened_files.append(file)\n                self._log_destination_is_tty.append(False)\n            else:\n                if not hasattr(dest, ""write""):\n                    raise ValueError(f""Log destination {dest} is not a ""\n                                     f""file-like object"")\n                try:\n                    isatty = dest.isatty()\n                except AttributeError:\n                    isatty = False\n                file = dest\n                self._log_destination_is_tty.append(isatty)\n            self._log_destination.append(file)\n\n    def _close_files(self):\n        for file in self._opened_files:\n            file.close()\n        self._opened_files = []\n\n        if hasattr(self, \'summary_writer\'):\n            self.summary_writer.close()\n\n    def _fire_event(self, event: Event, end: bool):\n        r""""""Signal the beginning or end of an event. Internally, this is where\n        conditions are checked and actions are executed.\n\n        Args:\n            event: The |Event| to fire.\n            end: If `True`, the fired event point is the end of :attr:`event`.\n                If `False`, the fired event point is the beginning of\n                :attr:`event`.\n\n        :raises: If any triggered action calls :meth:`terminate`,\n            :exc:`~texar.torch.run.executor_utils.ExecutorTerminateSignal` is\n            thrown after all conditions are checked and actions executed.\n        """"""\n        self._event_nested_layers += 1\n        _remove_count = 0\n        _conds_to_remove: List[Optional[Condition]] = []\n        for cond, actions in self._hooks[(event, end)].items():\n            # If condition is `None` (raw function hooks), action always\n            # triggers.\n            should_trigger = True\n            if cond is not None:\n                should_trigger = cond.hooks[(event, end)](self)\n                if self._should_remove_current_action:\n                    self._should_remove_current_action = False\n                    _conds_to_remove.append(cond)\n            if should_trigger:\n                for idx, action in enumerate(actions):\n                    action(self)\n                    if self._should_remove_current_action:\n                        # Rebind `actions` variable and assign to _hooks. This\n                        # does not affect the current for-loop over `actions`.\n                        self._should_remove_current_action = False\n                        index = idx - _remove_count\n                        _remove_count += 1\n                        actions = actions[:index] + actions[(index + 1):]\n                        if len(actions) == 0:\n                            _conds_to_remove.append(cond)\n                        else:\n                            self._hooks[(event, end)][cond] = actions\n        for cond in _conds_to_remove:\n            del self._hooks[(event, end)][cond]\n\n        self._event_nested_layers -= 1\n        if self._should_terminate:\n            self._should_terminate = False\n            raise utils.ExecutorTerminateSignal\n\n    def _validate_step(self, batch: Batch):\n        r""""""Perform one step of validation, i.e., perform a forward pass (or\n        decoding, depending on :attr:`validate_mode`) for a single batch.\n\n        Args:\n            batch: The batch to validate on.\n\n        Returns:\n            The dictionary containing values returned by the model. This is used\n            to compute metrics.\n        """"""\n        if self.validate_mode == \'predict\':\n            return_dict = self.model.predict(batch)  # type: ignore\n        else:\n            return_dict = self.model(batch)\n        return return_dict\n\n    def _test_step(self, batch: Batch):\n        r""""""Perform one step of testing, i.e., perform a forward pass (or\n        decoding, depending on :attr:`test_mode`) for a single batch.\n\n        Args:\n            batch: The batch to test on.\n\n        Returns:\n            The dictionary containing values returned by the model. This is used\n            to compute metrics.\n        """"""\n        if self.test_mode == \'predict\':\n            return_dict = self.model.predict(batch)  # type: ignore\n        else:\n            return_dict = self.model(batch)\n        return return_dict\n\n    def _train_step(self, batch: Batch):\n        r""""""Perform one step of training, i.e., perform a forward and backward\n        pass for a single batch. Parameter updates should also be performed when\n        necessary.\n\n        Args:\n            batch: The batch to train on.\n\n        Returns:\n            The dictionary containing values returned by the model. This is used\n            to compute metrics.\n        """"""\n        return_dict = self.model(batch)\n        try:\n            loss = return_dict[\'loss\']\n        except KeyError:\n            raise ValueError(""Return dictionary from model does not ""\n                             ""contain \'loss\' entry"")\n        loss = loss / self.num_iters_per_update\n        loss.backward()\n        if (self.num_iters_per_update == 1 or\n                self.status[""iteration""] % self.num_iters_per_update == 0):\n            self._fire_event(Event.ParameterUpdate, False)\n            if self.grad_clip is not None:\n                torch.nn.utils.clip_grad_norm_(\n                    self.model.parameters(), self.grad_clip)\n            self.optimizer.step()  # type: ignore\n            if self.lr_scheduler is not None:\n                self.lr_scheduler.step()\n            self.optimizer.zero_grad()  # type: ignore\n            self._fire_event(Event.ParameterUpdate, True)\n        return return_dict\n\n    def _train_loop(self, iterator: DataIterator) -> None:\n        r""""""Run the entire training loop given the data iterator.\n\n        Args:\n            iterator: The iterator over the training data.\n        """"""\n        epoch = 0\n        iteration = 0\n\n        # Initialize metrics.\n        for metric in self.train_metrics.values():\n            metric.reset()\n\n        while True:\n            epoch += 1\n            self.status[""epoch""] = epoch\n\n            self._fire_event(Event.Epoch, False)\n\n            for batch in iterator:\n                self._fire_event(Event.Iteration, False)\n                iteration += 1\n                self.status[""iteration""] = iteration\n\n                return_dict = self._train_step(batch)\n\n                self._train_tracker.add(len(batch))\n                utils.update_metrics(return_dict, batch, self.train_metrics)\n\n                self._fire_event(Event.Iteration, True)\n            self._fire_event(Event.Epoch, True)\n            self._train_tracker.reset()\n\n    def _validate_loop(self, iterator: DataIterator) -> None:\n        r""""""Run the validation loop given the data iterator.\n\n        Args:\n            iterator: The iterator over the validation data.\n        """"""\n        for batch in iterator:\n            self._fire_event(Event.ValidationIteration, False)\n            return_dict = self._validate_step(batch)\n\n            self._valid_tracker.add(len(batch))\n            utils.update_metrics(return_dict, batch, self.valid_metrics)\n\n            self._fire_event(Event.ValidationIteration, True)\n\n    def _test_loop(self, iterator: DataIterator) -> None:\n        r""""""Run the entire testing loop given the data iterator.\n\n        Args:\n            iterator: The iterator over the test data.\n        """"""\n        for batch in iterator:\n            self._fire_event(Event.TestingIteration, False)\n            return_dict = self._test_step(batch)\n\n            self._test_tracker.add(len(batch))\n            utils.update_metrics(return_dict, batch, self.test_metrics)\n\n            self._fire_event(Event.TestingIteration, True)\n\n    def _validate(self) -> None:\n        if self.valid_data is None:\n            raise ValueError(""Validation data not specified."")\n\n        self._fire_event(Event.Validation, False)\n\n        # Initialize metrics.\n        for metric in self.valid_metrics.values():\n            metric.reset()\n\n        if self.validate_mode == ""eval"":\n            iterator = DataIterator(self.valid_data, self.batching_strategy)\n        else:\n            iterator = DataIterator(self.valid_data)\n\n        try:\n            data_size: Optional[int] = len(self.valid_data)\n        except TypeError:\n            data_size = None\n        self._valid_tracker.set_size(data_size)\n\n        model_mode = self.model.training\n        self.model.train(self.test_mode == ""train"")\n\n        prev_split = self.status[""split""]\n        self.status[""split""] = ""valid""\n\n        # Main validation loop.\n        self._valid_tracker.start()\n        try:\n            with torch.no_grad():\n                self._validate_loop(iterator)\n        except utils.ExecutorTerminateSignal:\n            self.write_log(\n                ""Validation terminated. This is likely unintended. Please ""\n                ""check your custom actions."", mode=\'warning\')\n        finally:\n            self._valid_tracker.stop()\n\n        self._fire_event(Event.Validation, True)\n\n        # Restore status values.\n        self.status[""split""] = prev_split\n        self.model.train(model_mode)\n'"
texar/torch/run/executor_utils.py,8,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtility functions for the Executor module.\n""""""\n\nimport functools\nimport time\nfrom collections import Counter, OrderedDict\nfrom typing import (\n    Any, Callable, Counter as CounterType, Dict, Iterator, List, NamedTuple,\n    Optional, Tuple, Type, TypeVar, Union, Mapping, Sequence)\n\nfrom mypy_extensions import TypedDict\nimport torch\nfrom torch import nn\n\nfrom texar.torch.data.data.dataset_utils import Batch\nfrom texar.torch.run.metric import Metric\nfrom texar.torch.utils.types import MaybeSeq\nfrom texar.torch.utils.utils import get_instance\n\n__all__ = [\n    ""OptionalList"",\n    ""OptionalDict"",\n    ""Instance"",\n    ""to_list"",\n    ""to_metric_dict"",\n    ""to_instance"",\n    ""SavedTrainingState"",\n    ""TrainingStatus"",\n    ""CheckpointMetaInfo"",\n    ""ProgressTracker"",\n    ""ExecutorTerminateSignal"",\n    ""MetricList"",\n    ""update_metrics"",\n    ""color"",\n    ""repr_module"",\n]\n\nT = TypeVar(\'T\')\nOptionalList = Optional[MaybeSeq[T]]\nOptionalDict = Optional[Union[T, Sequence[Union[T, Tuple[str, T]]],\n                              Mapping[str, T]]]\nInstance = Union[T, Dict[str, Any]]\n\n\ndef to_list(xs: OptionalList[T]) -> List[T]:\n    if isinstance(xs, Sequence):\n        return list(xs)\n    if xs is None:\n        return []\n    return [xs]\n\n\ndef _to_dict(ds: OptionalDict[T],\n             unambiguous_name_fn: Callable[[str, T, int], str],\n             default_name_fn: Callable[[int, T], str]) -> \'OrderedDict[str, T]\':\n    if ds is None:\n        return OrderedDict()\n    if isinstance(ds, Mapping):\n        return OrderedDict(ds)\n    if isinstance(ds, Sequence):\n        xs = ds\n    else:\n        xs = [ds]\n    ret_dict: \'OrderedDict[str, T]\' = OrderedDict()\n    counter: CounterType[str] = Counter()\n    for idx, item in enumerate(xs):\n        if isinstance(item, tuple):\n            name, item = item\n        else:\n            name = default_name_fn(idx, item)\n        if name not in counter:\n            ret_dict[name] = item\n        else:\n            cnt = counter[name]\n            if cnt == 1:\n                prev_item = ret_dict[name]\n                ret_dict[unambiguous_name_fn(name, prev_item, 1)] = prev_item\n                del ret_dict[name]\n            ret_dict[unambiguous_name_fn(name, item, cnt + 1)] = item\n        counter.update([name])\n    return ret_dict\n\n\ndef to_dict(xs: OptionalDict[T],\n            default_name: Optional[str] = None) -> Dict[str, T]:\n    def unambiguous_name_fn(name: str, _item: T, cnt: int) -> str:\n        return f""{name}.{cnt}""\n\n    def default_name_fn(idx: int, _item: T) -> str:\n        if default_name is not None:\n            return default_name\n        return str(idx)\n\n    return _to_dict(xs, unambiguous_name_fn, default_name_fn)\n\n\ndef to_metric_dict(metrics: OptionalDict[Metric]) -> \'OrderedDict[str, Metric]\':\n    def unambiguous_name_fn(name: str, metric: Metric, _cnt: int) -> str:\n        new_name = f""{name}_{metric.pred_name}""\n        if metric.label_name is not None:\n            new_name = f""{new_name}_{metric.label_name}""\n        return new_name\n\n    def default_name_fn(_idx: int, metric: Metric) -> str:\n        return metric.metric_name\n\n    if isinstance(metrics, dict) and not isinstance(metrics, OrderedDict):\n        raise ValueError(""Metrics dictionary must be of type OrderedDict"")\n    metric_dict = _to_dict(metrics, unambiguous_name_fn, default_name_fn)\n\n    for name, metric in metric_dict.items():\n        if not name.isidentifier():\n            raise ValueError(f""Name \\""{name}\\"" for metric {metric} is not a ""\n                             f""valid identifier name"")\n        if not isinstance(metric, Metric):\n            raise ValueError(f""All metrics must be of class Metric, but found ""\n                             f""{type(metric)}"")\n    return metric_dict\n\n\ndef to_instance(typ: Type[T], instance: Instance[T], modules: List[str],\n                extra_kwargs: Optional[Dict[str, Any]] = None) -> Optional[T]:\n    if instance is None:\n        return None\n    if isinstance(instance, dict):\n        kwargs = {**instance.get(\'kwargs\', {}), **(extra_kwargs or {})}\n        instance = get_instance(instance[\'type\'], kwargs, modules)\n    if not isinstance(instance, typ):\n        raise ValueError(f""The instance {instance} is not of type {typ}"")\n    return instance\n\n\n# TODO: Also save training progress?\nclass SavedTrainingState(NamedTuple):\n    r""""""The entire training state to save to or load from checkpoints.""""""\n    model: Dict[str, torch.Tensor]\n    optimizer: Dict[str, torch.Tensor]\n    scheduler: Optional[Dict[str, Any]]\n    system_rng: Any\n    numpy_rng: Any\n    torch_rng: Any\n\n\nclass TrainingStatus(TypedDict):\n    epoch: int\n    iteration: int\n    split: str\n    metric: \'OrderedDict[str, Metric]\'\n    eval_metric: \'OrderedDict[str, Metric]\'\n\n\nclass CheckpointMetaInfo(TypedDict):\n    status: TrainingStatus\n    timestamp: float\n\n\nclass ProgressTracker:\n    start_time: float\n    size: Optional[int]\n    n_examples: int\n    accumulated_time: float\n    paused: bool\n\n    _tracker_stack: List[\'ProgressTracker\'] = []\n\n    def __init__(self, size: Optional[int] = None):\n        self.size = size\n        self.started = False\n\n    def set_size(self, size: Optional[int]):\n        self.size = size\n\n    def start(self):\n        if self.started:\n            return\n        self.started = True\n        if len(self._tracker_stack) > 0:\n            self._tracker_stack[-1].pause()\n        self._tracker_stack.append(self)\n        self.reset()\n        self.paused = False\n\n    def stop(self):\n        if not self.started:\n            return\n        self.started = False\n        obj = self._tracker_stack.pop(-1)\n        assert obj is self\n        if len(self._tracker_stack) > 0:\n            self._tracker_stack[-1].resume()\n\n    def reset(self):\n        self.n_examples = 0\n        self.start_time = time.time()\n        self.accumulated_time = 0.0\n\n    def pause(self):\n        if self.paused:\n            return\n        self.paused = True\n        self.accumulated_time += time.time() - self.start_time\n\n    def resume(self):\n        if not self.paused:\n            return\n        self.paused = False\n        self.start_time = time.time()\n\n    def add(self, n_examples: int):\n        self.n_examples += n_examples\n\n    def progress(self) -> Optional[float]:\n        if self.size is None:\n            return None\n        return self.n_examples / self.size * 100\n\n    def time_elapsed(self) -> float:\n        return self.accumulated_time + (time.time() - self.start_time)\n\n    def speed(self) -> str:\n        speed = self.n_examples / self.time_elapsed()\n        if speed > 1.0:\n            return f""{speed:.2f}ex/s""\n        try:\n            return f""{1.0 / speed:.2f}s/ex""\n        except ZeroDivisionError:\n            return f""0.00ex/s""\n\n\nclass ExecutorTerminateSignal(Exception):\n    pass\n\n\n@functools.total_ordering\nclass MetricList:\n    r""""""A class representing list of metrics along with their values at a\n    certain point. Used for metric comparisons.\n\n    Args:\n        metrics: The dictionary of metric instances.\n        values (optional): The dictionary of metric values. If `None` (default),\n            the current values of the provided metrics are used.\n    """"""\n\n    # TODO: Ignore non-streaming metrics here? Or in\n\n    def __init__(self, metrics: \'OrderedDict[str, Metric]\',\n                 values: Optional[Dict[str, Any]] = None):\n        self.metrics = metrics\n        if values is None:\n            self.values = {name: metric.value()\n                           for name, metric in metrics.items()}\n        else:\n            self.values = values\n\n    def _compare_metrics(self, other: Any):\n        if not isinstance(other, MetricList):\n            raise ValueError(\n                ""Cannot compare to an object not of type MetricList"")\n        for (name, metric), (other_name, other_metric) in zip(\n                self.metrics.items(), other.metrics.items()):\n            if name != other_name or type(metric) is not type(other_metric):\n                raise ValueError(""Cannot compare two metric lists with ""\n                                 ""different base metrics"")\n\n    def __eq__(self, other: Any) -> bool:\n        self._compare_metrics(other)\n        return all(self.values[name] == other.values[name]\n                   for name in self.metrics)\n\n    def __gt__(self, other: \'MetricList\') -> bool:\n        r""""""Compare this metric list to another, and return whether the current\n        list is better.\n        """"""\n        self._compare_metrics(other)\n        for name, metric in self.metrics.items():\n            cmp = metric.better(self.values[name], other.values[name])\n            if cmp is not None:\n                return cmp\n        return False\n\n\ndef update_metrics(return_dict: Dict[str, Any], batch: Batch,\n                   metrics: \'OrderedDict[str, Metric]\') -> None:\n    for metric_name, metric in metrics.items():\n        if metric.pred_name is not None:\n            try:\n                pred_val = return_dict[metric.pred_name]\n            except KeyError:\n                raise ValueError(\n                    f""Return dictionary from model does not contain ""\n                    f""\'{metric.pred_name}\' entry, which was required for ""\n                    f""metric \'{metric_name}\'"")\n            if isinstance(pred_val, torch.Tensor):\n                pred_val = pred_val.tolist()\n            pred_val = to_list(pred_val)\n        else:\n            pred_val = None\n        if metric.label_name is not None:\n            try:\n                label_val = batch[metric.label_name]\n            except KeyError:\n                raise ValueError(\n                    f""Data batch does not contain \'{metric.label_name}\' ""\n                    f""entry, which was required for metric \'{metric_name}\'"")\n            if isinstance(label_val, torch.Tensor):\n                label_val = label_val.tolist()\n            label_val = to_list(label_val)\n        else:\n            label_val = None\n        metric.add(pred_val, label_val)\n\n\nCLEAR_LINE = \'\\033[2K\\r\'\nRESET_CODE = \'\\033[0m\'\nCOLOR_CODE = {\n    \'red\': \'\\033[31m\',\n    \'green\': \'\\033[32m\',\n    \'yellow\': \'\\033[33m\',\n    \'blue\': \'\\033[94m\',\n    \'magenta\': \'\\033[35m\',\n    \'cyan\': \'\\033[36m\',\n    \'gray\': \'\\033[37m\',\n    \'grey\': \'\\033[37m\'\n}\n\n\ndef color(s: str, col: str):\n    return COLOR_CODE[col.lower()] + s + RESET_CODE\n\n\ndef _add_indent(s_, n_spaces):\n    s = s_.split(\'\\n\')\n    # don\'t do anything for single-line stuff\n    if len(s) == 1:\n        return s_\n    first = s.pop(0)\n    s = [(n_spaces * \' \') + line for line in s]\n    s = \'\\n\'.join(s)\n    s = first + \'\\n\' + s\n    return s\n\n\ndef _convert_id(keys: List[str]) -> Iterator[str]:\n    start = end = None\n    for key in keys:\n        if key.isnumeric() and end == int(key) - 1:\n            end = int(key)\n        else:\n            if start is not None:\n                if start == end:\n                    yield f""id {start}""\n                else:\n                    yield f""ids {start}-{end}""\n            if key.isnumeric():\n                start = end = int(key)\n            else:\n                start = end = None\n                yield key\n    if start is not None:\n        if start == end:\n            yield f""id {start}""\n        else:\n            yield f""ids {start}-{end}""\n\n\ndef repr_module(module: nn.Module) -> str:\n    r""""""Create a compressed representation by combining identical modules in\n    `nn.ModuleList`s and `nn.ParameterList`s.\n    """"""\n\n    # We treat the extra repr like the sub-module, one item per line\n    extra_lines: List[str] = []\n    extra_repr = module.extra_repr()\n    # empty string will be split into list [\'\']\n    if extra_repr:\n        extra_lines = extra_repr.split(\'\\n\')\n    child_lines = []\n    prev_mod_str = None\n    keys: List[str] = []\n    for key, submodule in module.named_children():\n        mod_str = repr_module(submodule)\n        mod_str = _add_indent(mod_str, 2)\n        if prev_mod_str is None or prev_mod_str != mod_str:\n            if prev_mod_str is not None:\n                for name in _convert_id(keys):\n                    child_lines.append(f""({name}): {prev_mod_str}"")\n            prev_mod_str = mod_str\n            keys = [key]\n        else:\n            keys.append(key)\n    if len(keys) > 0:\n        for name in _convert_id(keys):\n            child_lines.append(f""({name}): {prev_mod_str}"")\n    lines = extra_lines + child_lines\n\n    main_str = module.__class__.__name__ + \'(\'\n    if lines:\n        # simple one-liner info, which most builtin Modules will use\n        if len(extra_lines) == 1 and not child_lines:\n            main_str += extra_lines[0]\n        else:\n            main_str += \'\\n  \' + \'\\n  \'.join(lines) + \'\\n\'\n\n    main_str += \')\'\n    return main_str\n'"
texar/torch/utils/__init__.py,6,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library utils.\n""""""\n\nfrom texar.torch.utils.average_recorder import *\nfrom texar.torch.utils.dtypes import *\nfrom texar.torch.utils.exceptions import *\nfrom texar.torch.utils.shapes import *\nfrom texar.torch.utils.utils import *\nfrom texar.torch.utils.utils_io import *\n'"
texar/torch/utils/average_recorder.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtilities for maintaining moving average.\n""""""\n\nfrom collections import deque\nfrom typing import Deque, Dict, List, Optional, Union, no_type_check\n\nfrom texar.torch.utils.types import MaybeList, MaybeSeq\n\n__all__ = [\n    \'_SingleAverageRecorder\',\n    \'AverageRecorder\',\n]\n\nScalar = Union[int, float]\nID = Union[int, str]\nRecord = Union[Dict[ID, Scalar], MaybeSeq[Scalar]]\n\n\nclass _SingleAverageRecorder:\n    r""""""Maintains the moving average (i.e., the average of the latest N\n    records) of a single metric.\n\n    Args:\n        size (int, optional): The window size of moving average. If `None`,\n            the average of all added records is maintained.\n        name (str, optional): name of the recorder. Used when printing.\n    """"""\n\n    def __init__(self, size: Optional[int] = None, name: Optional[str] = None):\n        if size is not None and size <= 0:\n            raise ValueError(""`size` must be > 0 or `None`."")\n        self._size = size\n        self._q: Deque[Scalar] = deque([])\n        self._w: Deque[Scalar] = deque([])\n        self._sum = 0.\n        self._w_sum: Scalar = 0\n        self._name = name\n\n    def add(self, record: Scalar, weight: Optional[Scalar] = None):\n        r""""""Appends a new record.\n\n        Args:\n            record: A scalar; the new record to append.\n            weight (optional): A scalar, weight of the new record for\n                calculating a weighted average. If `None`, weight is set to\n                ``1``.\n                For example, :attr:`weight` can be set to batch size and\n                :attr:`record` the average value of certain metric on the batch\n                in order to calculate the average metric value on a whole\n                dataset.\n\n        Returns:\n            The (moving) average after appending the record.\n        """"""\n        w = weight if weight is not None else 1\n        self._w_sum += w\n        self._sum += record * w\n\n        if self._size is not None:\n            if len(self._q) == self._size:\n                w_pop = self._w.popleft()\n                self._sum -= self._q.popleft() * w_pop\n                self._w_sum -= w_pop\n            self._q.append(record)\n            self._w.append(w)\n\n        return self.avg()\n\n    def avg(self) -> float:\n        r""""""Returns the (moving) average.\n        """"""\n        if self._w_sum == 0:\n            return 0.\n        return self._sum / self._w_sum\n\n    def reset(self) -> None:\n        r""""""Cleans all records.\n        """"""\n        self._q.clear()\n        self._w.clear()\n        self._sum = 0.\n        self._w_sum = 0\n\n    def to_str(self, precision: Optional[int] = None) -> str:\n        r""""""Returns a string of the average value.\n\n        Args:\n            precision (int, optional): The number of decimal places to keep in\n                the returned string. For example, for an average value of\n                ``0.1234``, :python:`precision = 2` leads to ``""0.12""``.\n\n        Returns:\n            A string of the average value. If :meth:`name` is given, the\n            string is of the format like ``""name: 0.1234""`, otherwise\n            the string is of the format like ``""0.1234""``.\n        """"""\n        prec_str = ""{}""\n        if precision is not None:\n            prec_str = ""{:.%df}"" % precision\n\n        avg_str = prec_str.format(self.avg())\n        if self._name is not None:\n            avg_str = ""{}: {}"".format(self._name, avg_str)\n\n        return avg_str\n\n    @property\n    def name(self) -> str:\n        r""""""The name of the recorder.\n        """"""\n        return self.name\n\n\nclass AverageRecorder:\n    r""""""Maintains the moving averages (i.e., the average of the latest N\n    records) of (possibly multiple) fields.\n\n    Fields are determined by the first call of :meth:`add`.\n\n    Args:\n        size (int, optional): The window size of moving average. If `None`,\n            the average of all added records is maintained.\n\n    Example:\n\n        .. code-block:: python\n\n            ## Use to maintain moving average of training loss\n            avg_rec = AverageRecorder(size=10) # average over latest 10 records\n            while training:\n                loss_0, loss_1  = ...\n                avg_rec.add([loss_0, loss_1])\n                # avg_rec.avg() == [0.12343452, 0.567800323]\n                # avg_rec.avg(0) == 0.12343452\n                # avg_rec.to_str(precision=2, ) == \'0.12 0.57\'\n\n            ## Use to maintain average of test metrics on the whole test set\n            avg_rec = AverageRecorder() # average over ALL records\n            while test:\n                metric_0, metric_1  = ...\n                avg_rec.add({\'m0\': metric_0, \'m1\': metric_1}) # dict is allowed\n            print(avg_rec.to_str(precision=4, delimiter=\' , \'))\n            # \'m0: 0.1234 , m1: 0.5678\'\n            #\n            # avg_rec.avg() == {\'m0\': 0.12343452, \'m1\': 0.567800323}\n            # avg_rec.avg(0) == 0.12343452\n\n    """"""\n    _recorders: Dict[ID, _SingleAverageRecorder]\n    _record_type: type\n\n    def __init__(self, size: Optional[int] = None):\n        if size is not None and size <= 0:\n            raise ValueError(""`size` must be > 0 or `None`."")\n        self._size = size\n        self._recorders = None  # type: ignore\n        self._default_metric_name = ""metric""\n        self._record_type = None  # type: ignore\n\n    @no_type_check\n    def _to_dict(self, record: Record) -> Dict[ID, Scalar]:\n        if isinstance(record, dict):\n            record_dict = record\n        elif isinstance(record, (list, tuple)):\n            record_dict = dict(enumerate(record))\n        else:\n            record_dict = {self._default_metric_name: record}\n        return record_dict\n\n    def add(self, record: Record, weight: Optional[Scalar] = None):\n        r""""""Appends a new record.\n\n        :attr:`record` can be a ``list``, ``dict``, or a single scalar. The\n        record type is determined at the first time :meth:`add` is called.\n        All subsequent calls to :meth:`add` must have the same type of\n        :attr:`record`.\n\n        :attr:`record` in subsequent calls to :meth:`add` can contain only\n        a subset of fields than the first call to :meth:`add`.\n\n        Example:\n\n            .. code-block:: python\n\n                recorder.add({\'1\': 0.2, \'2\': 0.2}) # 1st call to `add`\n                x = recorder.add({\'1\': 0.4}) # 2nd call to `add`\n                # x == {\'1\': 0.3, \'2\': 0.2}\n\n        Args:\n            record: A single scalar, a list of scalars, or a dict of scalars.\n            weight (optional): A scalar, weight of the new record for\n                calculating a weighted average. If `None`, weight is set to\n                ``1``.\n                For example, :attr:`weight` can be set to batch size and\n                :attr:`record` the average value of certain metrics on the batch\n                in order to calculate the average metric values on a whole\n                dataset.\n\n        Returns:\n            The (moving) average after appending the record, with the same\n            type as :attr:`record`.\n        """"""\n        if self._record_type is None:\n            self._record_type = type(record)\n        elif self._record_type != type(record):\n            raise ValueError(\'The type of `record` is not consistent. \'\n                             \'Expect type `{}`\'.format(self._record_type))\n\n        record_dict = self._to_dict(record)\n        if self._recorders is None:\n            self._recorders = {\n                name: _SingleAverageRecorder(\n                    self._size, name if self._record_type == dict else None)\n                for name in record_dict.keys()\n            }\n\n        for name, val in record_dict.items():\n            self._recorders[name].add(val, weight=weight)\n\n        return self.avg()\n\n    def avg(self, id_or_name: Optional[MaybeList[ID]] = None) -> Record:\n        r""""""Returns the (moving) average.\n\n        Args:\n            id_or_name (optional): A list of or a single element.\n                Each element is the index (if the record type is ``list``) or\n                name (if the record type is ``dict``) of the field for which\n                the average is calculated. If not given, the average of all\n                fields are returned.\n\n        Returns:\n            The average value(s). If :attr:`id_or_name` is a single element\n            (not a list), then returns the average value of the corresponding\n            field. Otherwise, if :attr:`id_or_name` is a list of element(s),\n            then returns average value(s) in the same type as :attr:`record`\n            of :meth:`add`.\n        """"""\n        if self._recorders is None:\n            return 0.\n\n        keys = id_or_name\n        if keys is None:\n            keys = list(self._recorders.keys())\n\n        if not isinstance(keys, (list, tuple)):\n            return self._recorders[keys].avg()\n\n        avg = {key: self._recorders[key].avg() for key in keys}\n        if self._record_type in {list, tuple}:\n            ret_avg = []\n            for k, v in avg.items():\n                if k in keys:\n                    ret_avg.append(v)\n            return self._record_type(ret_avg)\n        elif self._record_type == dict:\n            return avg\n        else:\n            return avg[self._default_metric_name]\n\n    def reset(self, id_or_name: Optional[MaybeList[ID]] = None):\n        r""""""Resets the record.\n\n        Args:\n            id_or_name (optional): A list or a single element. Each element is\n                the index (if the record type is ``list``) or name (if the\n                record type is ``dict``) of the field to reset.\n                If `None`, all fields are reset.\n        """"""\n        keys = id_or_name\n        if keys is None:\n            keys = list(self._recorders.keys())\n        elif not isinstance(keys, (list, tuple)):\n            keys = [keys]\n\n        for key in keys:\n            self._recorders[key].reset()\n\n    def to_str(self, precision: Optional[int] = None,\n               delimiter: str = \' \') -> str:\n        r""""""Returns a string of the average values of the records.\n\n        Args:\n            precision (int, optional): The number of decimal places to keep in\n                the returned string. For example, for an average value of\n                ``0.1234``, :python:`precision = 2` leads to ``""0.12""``.\n            delimiter (str): The delimiter string that separates between\n                fields.\n\n        Returns:\n            A string of the average values.\n\n            If record is of type ``dict``, the string is a concatenation of\n            ``""field_name: average_value""``, delimited with :attr:`delimiter`.\n            For example, ``""field_name_1: 0.1234 field_name_2: 0.5678 ...""``.\n\n            Otherwise, the string is of a concatenation of \'average_value\'.\n            For example, ``""0.1234 0.5678 ...""``\n        """"""\n        strs = {name: rec.to_str(precision=precision)\n                for name, rec in self._recorders.items()}\n        str_list: List[str] = []\n        if self._record_type in [list, tuple]:\n            # Enumerates the keys in order, which are the indexes\n            str_list.extend(strs[i] for i in range(len(strs)))\n        elif self._record_type == dict:\n            str_list = list(strs.values())\n        else:\n            str_list = [strs[self._default_metric_name]]\n\n        avg_str = delimiter.join(str_list)\n\n        return avg_str\n'"
texar/torch/utils/beam_search.py,67,"b'# Adapted from the Tensor2Tensor\'s implementation.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Modifications copyright (C) 2019 Texar\n# ==============================================================================\n""""""\nImplementation of beam search with penalties.\n\nAdapted from:\n    `https://github.com/tensorflow/tensor2tensor/blob/eb048f69c7ea860324122b87cb9caf59c52a27f3/tensor2tensor/utils/beam_search.py`\n""""""\nfrom typing import Any, Callable, Optional, Tuple, TypeVar, overload\n\nimport torch\n\nfrom texar.torch.utils import map_structure, torch_bool\n\n__all__ = [\n    \'beam_search\',\n]\n\nState = TypeVar(\'State\')\n\n# Default value for INF\nINF = 1.0 * 1e7\n\n\ndef gather_nd(params: Any, indices: torch.Tensor) -> Any:\n    if not isinstance(params, torch.Tensor):\n        return params\n    assert len(indices.size()) == 3\n    orig_size = params.size()\n    index = indices[:, :, 1].view(-1) + indices[:, :, 0].view(-1) * orig_size[1]\n    ret = torch.index_select(\n        params.view(-1, *params.size()[2:]), dim=0, index=index\n    )\n    ret = ret.view(orig_size[0], indices.size(1), *orig_size[2:])\n\n    return ret\n\n\ndef _merge_beam_dim(tensor: Any) -> Any:\n    r""""""Reshapes first two dimensions in to single dimension.\n\n    Args:\n        tensor: Tensor to reshape of shape `[A, B, ...]`.\n\n    Returns:\n        Reshaped tensor of shape `[A * B, ...]`.\n    """"""\n    if not isinstance(tensor, torch.Tensor):\n        return tensor\n    shape = list(tensor.size())\n    shape[0] *= shape[1]  # batch -> batch * beam_size\n    shape.pop(1)  # Remove beam dim\n    return tensor.view(tuple(shape))\n\n\ndef _unmerge_beam_dim(tensor: Any, batch_size: int,\n                      beam_size: int) -> Any:\n    r""""""Reshapes first dimension back to `[batch_size, beam_size]`.\n\n    Args:\n        tensor: Tensor to reshape of shape `[batch_size * beam_size, ...]`.\n        batch_size: int, original batch size.\n        beam_size: int, original beam size.\n\n    Returns:\n        Reshaped tensor of shape `[batch_size, beam_size, ...]`.\n    """"""\n    if not isinstance(tensor, torch.Tensor):\n        return tensor\n    shape = list(tensor.size())\n    new_shape = [batch_size] + [beam_size] + shape[1:]\n    return tensor.view(tuple(new_shape))\n\n\ndef _expand_to_beam_size(tensor: Any,\n                         beam_size: int) -> Any:\n    r""""""Tiles a given tensor by :attr:`beam_size`.\n\n    Args:\n        tensor: tensor to tile. Shape: `[batch_size, ...]`.\n        beam_size: How much to tile the tensor by.\n\n    Returns:\n        Tiled tensor of shape `[batch_size, beam_size, ...]`.\n    """"""\n    if not isinstance(tensor, torch.Tensor):\n        return tensor\n    tensor = torch.unsqueeze(tensor, dim=1)\n    tile_dims = [1] * len(tensor.size())\n    tile_dims[1] = beam_size\n\n    return tensor.repeat(tuple(tile_dims))\n\n\ndef log_prob_from_logits(logits: torch.Tensor) -> torch.Tensor:\n    return logits - torch.logsumexp(logits, dim=-1, keepdim=True)\n\n\ndef compute_batch_indices(batch_size: int, beam_size: int) -> torch.LongTensor:\n    r""""""Computes the i-th coordinate that contains the batch index for\n    gathers.\n\n    The batch index tensor is a tensor like `[[0,0,0,0,],[1,1,1,1],..]`.\n    It says which batch the beam item is in. This will create the first\n    dimension of the 2D coordinates needed for the gather.\n\n    Args:\n        batch_size: Batch size\n        beam_size: Size of the beam.\n\n    Returns:\n        `[batch_size, beam_size]` tensor of ids.\n    """"""\n    batch_pos = torch.arange(batch_size)\n    batch_pos = batch_pos.view(-1, 1).expand(batch_size, beam_size)\n    return batch_pos\n\n\ndef compute_topk_scores_and_seq(\n    sequences: torch.LongTensor,\n    scores: torch.Tensor,\n    scores_to_gather: torch.Tensor,\n    flags: torch.ByteTensor,\n    beam_size: int,\n    batch_size: int,\n    states_to_gather: Optional[State] = None,\n) -> Tuple[torch.LongTensor, torch.Tensor, torch.ByteTensor, Optional[State]]:\n    r""""""Given sequences and scores, will gather the top-k (`k = beam`) size\n    sequences.\n\n    This function is used to grow alive, and finished. It takes sequences,\n    scores, and flags, and returns the top k from sequence\n    :attr:`scores_to_gather`, and flags based on the values in scores.\n\n    Args:\n        sequences: Tensor of sequences that we need to gather from.\n            Shape: `[batch_size, beam_size, seq_length]`.\n        scores: Tensor of scores for each sequence in sequences. We will use\n            these to compute the top-k. Shape: `[batch_size, beam_size]`.\n        scores_to_gather: Tensor of scores for each sequence in sequences.\n            Shape: `[batch_size, beam_size]`.\n            We will return the gathered scores from here.\n            Scores to gather is different from scores because for\n            grow_alive, we will need to return log-probabilities, while for\n            grow_finished, we will need to return the length penalized\n            scores.\n        flags: Tensor of booleans for sequences that say whether a sequence\n            has reached `EOS`.\n        beam_size: int\n        batch_size: int\n        states_to_gather: (possibly nested structure of) decoding states.\n\n    :returns: Tuple of:\n\n        - `topk_seq`: `[batch_size, beam_size, decode_length]`.\n        - `topk_gathered_scores`: `[batch_size, beam_size]`.\n        - `topk_finished_flags`: `[batch_size, beam_size]`.\n    """"""\n    # by default top-k is for the last dimension\n    _, topk_indexes = torch.topk(scores, k=beam_size)\n    # The next three steps are to create coordinates for torch.gather_nd to\n    # pull out the top-k sequences from sequences based on scores.\n    # batch pos is a tensor like [[0,0,0,0,],[1,1,1,1],..]. It says which\n    # batch the beam item is in. This will create the i of the i,j\n    # coordinate needed for the gather\n    batch_pos = compute_batch_indices(batch_size, beam_size)\n    batch_pos = batch_pos.to(device=topk_indexes.device)\n    # top coordinates will give us the actual coordinates to do the gather.\n    # stacking will create a tensor of dimension batch * beam * 2, where\n    # the last dimension contains the i,j gathering coordinates.\n    top_coordinates = torch.stack([batch_pos, topk_indexes], dim=2)\n\n    # Gather up the highest scoring sequences.\n    topk_seq = gather_nd(sequences, top_coordinates)\n    topk_flags = gather_nd(flags, top_coordinates)\n    topk_gathered_scores = gather_nd(scores_to_gather, top_coordinates)\n    if states_to_gather is not None:\n        topk_gathered_states = map_structure(\n            lambda state: gather_nd(state, top_coordinates), states_to_gather\n        )\n    else:\n        topk_gathered_states = states_to_gather\n    return topk_seq, topk_gathered_scores, topk_flags, topk_gathered_states\n\n\n# TODO: Remove these once pylint supports function stubs.\n# pylint: disable=unused-argument,function-redefined\n\n@overload\ndef beam_search(\n    symbols_to_logits_fn: Callable[[torch.Tensor, State],\n                                   Tuple[torch.Tensor, State]],\n    initial_ids: torch.LongTensor,\n    beam_size: int,\n    decode_length: int,\n    vocab_size: int,\n    alpha: float,\n    eos_id: int,\n    states: State,\n    stop_early: bool = True) -> Tuple[torch.LongTensor, torch.Tensor]: ...\n\n\n@overload\ndef beam_search(\n    symbols_to_logits_fn: Callable[[torch.Tensor], torch.Tensor],\n    initial_ids: torch.LongTensor,\n    beam_size: int,\n    decode_length: int,\n    vocab_size: int,\n    alpha: float,\n    eos_id: int,\n    states: Optional[State] = None,\n    stop_early: bool = True) -> Tuple[torch.LongTensor, torch.Tensor]: ...\n\n# pylint: enable=unused-argument\n\n\ndef beam_search(\n    symbols_to_logits_fn,\n    initial_ids,\n    beam_size,\n    decode_length,\n    vocab_size,\n    alpha,\n    eos_id,\n    states=None,\n    stop_early=True,\n):\n    r""""""Beam search with length penalties.\n\n    Requires a function that can take the currently decoded symbols and\n    return the logits for the next symbol. The implementation is inspired\n    by https://arxiv.org/abs/1609.08144.\n\n    Variables used within this function follow the naming pattern:\n    `(alive|finished)_topk_(seq,scores)`.\n\n    Variables marked `alive` represent the new beam sequences that will be\n    processed in the next step.    Variables marked `finished` represent\n    the completed beam sequences, which may be padded with 0 if no beams\n    finished.\n\n    Variables marked `seq` store the full beam sequence for the time step.\n    Variables marked `scores` store the sequence\'s final log scores.\n\n    The beam search steps will be processed sequentially in order, so when\n    capturing observed from these operations, tensors, clients can make\n    assumptions about which step is being recorded.\n\n    Args:\n        symbols_to_logits_fn: Interface to the model, to provide logits.\n            Should take `[batch_size, decoded_ids]` and return\n            `[batch_size, vocab_size]`.\n        initial_ids: LongTensor of shape `[batch_size]`. IDs to start off the\n            decoding, this will be the first thing handed to\n            :attr:`symbols_to_logits_fn` (after expanding to beam size).\n        beam_size: Size of the beam.\n        decode_length: Number of steps to decode for.\n        vocab_size: Size of the vocab, must equal the size of the logits\n            returned by :attr:`symbols_to_logits_fn`.\n        alpha: alpha for length penalty.\n        eos_id: ID for end of sentence.\n        states: (possibly nested structure of) decoding states.\n        stop_early: a boolean - stop once best sequence is provably\n            determined.\n\n    Returns:\n        Tuple of\n\n        - decoded beams (shape: `[batch_size, beam_size, decode_length]`)\n        - decoding probabilities (shape: `[batch_size, beam_size]`)\n    """"""\n\n    batch_size = initial_ids.size()[0]\n\n    # Assume initial_ids are prob 1.0\n    initial_log_probs = torch.Tensor(\n        [[0.0] + [-float(""inf"")] * (beam_size - 1)]\n    )  # [1, beam_size]\n    initial_log_probs = initial_log_probs.to(device=initial_ids.device)\n    # Expand to beam_size (batch_size, beam_size)\n    alive_log_probs = initial_log_probs.repeat((batch_size, 1))\n\n    # Expand each batch and state to beam_size\n    alive_seq = _expand_to_beam_size(initial_ids, beam_size)\n    alive_seq = torch.unsqueeze(alive_seq, dim=2)\n    # (batch_size, beam_size, 1)\n\n    if states is not None:\n        states = map_structure(\n            lambda state: _expand_to_beam_size(state, beam_size), states\n        )\n\n    # Finished will keep track of all the sequences that have finished so\n    # far\n    # Finished log-probs will be negative infinity in the beginning\n    # finished_flags will keep track of booleans\n    finished_seq = torch.zeros(alive_seq.size(), dtype=torch.long)\n    # Setting the scores of the initial to negative infinity.\n    finished_scores = torch.full((batch_size, beam_size), -INF)\n    finished_flags = torch.zeros((batch_size, beam_size), dtype=torch_bool)\n\n    finished_seq = finished_seq.to(device=initial_ids.device)\n    finished_scores = finished_scores.to(device=initial_ids.device)\n    finished_flags = finished_flags.to(device=initial_ids.device)\n\n    def grow_finished(\n        finished_seq: torch.LongTensor,\n        finished_scores: torch.Tensor,\n        finished_flags: torch.ByteTensor,\n        curr_seq: torch.LongTensor,\n        curr_scores: torch.Tensor,\n        curr_finished: torch.ByteTensor,\n    ) -> Tuple[torch.LongTensor, torch.Tensor, torch.ByteTensor]:\n        r""""""Given sequences and scores, will gather the top-k (`k = beam`) size\n        sequences.\n\n        Args:\n            finished_seq: Finished sequences.\n                Shape: `[batch_size, beam_size, current_decoded_length]`.\n            finished_scores: Scores for each finished sequences.\n                Shape: `[batch_size, beam_size]`.\n            finished_flags: Finished flags for each of these sequences.\n                Shape: `[batch_size, beam_size]`\n            curr_seq: Top-k sequences that has been grown by one\n                position.\n                Shape: `[batch_size, beam_size, current_decoded_length]`.\n            curr_scores: Scores for each of the top-k sequences.\n                Shape: `[batch_size, beam_size]`.\n            curr_finished: Finished flags for each of the top-k sequences.\n                Shape: `[batch_size, beam_size]`.\n\n        Returns:\n            Tuple of\n\n            - Top-k sequences based on scores.\n            - Log-probabilities of these sequences.\n            - Finished flags of these sequences.\n        """"""\n        # First append a column of 0\'ids to finished to make the same\n        # length with finished scores\n        _appended = torch.zeros(batch_size, beam_size, 1, dtype=torch.long)\n        _appended = _appended.to(device=finished_seq.device)\n        finished_seq = torch.cat([finished_seq, _appended], dim=2)\n\n        # Set the scores of the unfinished seq in curr_seq to large\n        # negative values\n        curr_scores = curr_scores + (1.0 - curr_finished.float()) * -INF\n        # concatenating the sequences and scores along beam axis\n        curr_finished_seq = torch.cat([finished_seq, curr_seq], dim=1)\n        curr_finished_scores = torch.cat([finished_scores, curr_scores], dim=1)\n        curr_finished_flags = torch.cat([finished_flags, curr_finished], dim=1)\n        next_seq, next_scores, next_flags, _ = compute_topk_scores_and_seq(\n            curr_finished_seq,\n            curr_finished_scores,\n            curr_finished_scores,\n            curr_finished_flags,\n            beam_size,\n            batch_size,\n        )\n        return next_seq, next_scores, next_flags\n\n    def grow_alive(\n        curr_seq: torch.LongTensor, curr_scores: torch.Tensor,\n            curr_log_probs: torch.Tensor, curr_finished: torch.ByteTensor,\n            states: Optional[State]\n    ) -> Tuple[torch.LongTensor, torch.Tensor, torch.ByteTensor,\n               Optional[State]]:\n        r""""""Given sequences and scores, will gather the top k=beam size\n        sequences.\n\n        Args:\n            curr_seq: Current top-k sequences that has been grown by one\n                position.\n                Shape: `[batch_size, beam_size, i + 1]`.\n            curr_scores: Scores for each of these sequences.\n                Shape: `[batch_size, beam_size]`.\n            curr_log_probs: Log-probabilities for each of these sequences.\n                Shape: `[batch_size, beam_size]`.\n            curr_finished: Finished flags for each of these sequences.\n                Shape: `[batch_size, beam_size]`.\n            states: (possibly nested structure of) decoding states.\n\n        :returns: Tuple of:\n\n            - Top-k sequences based on scores.\n            - Log-probabilities of these sequences.\n            - Finished flags of these sequences.\n            - Decoding states for these sequences.\n        """"""\n        # Set the scores of the finished seq in curr_seq to large negative\n        # values\n        curr_scores = curr_scores + curr_finished.float() * -INF\n        return compute_topk_scores_and_seq(\n            curr_seq,\n            curr_scores,\n            curr_log_probs,\n            curr_finished,\n            beam_size,\n            batch_size,\n            states,\n        )\n\n    def grow_topk(\n        i: int, alive_seq: torch.LongTensor, alive_log_probs: torch.Tensor,\n        states: Optional[State]\n    ) -> Tuple[torch.LongTensor, torch.Tensor, torch.Tensor,\n               torch.ByteTensor, Optional[State]]:\n        r""""""Inner beam search loop.\n\n        This function takes the current alive sequences, and grows them to\n        top-k sequences where `k = 2 * beam`. We use `2 * beam` because we could\n        have `beam_size` number of sequences that might hit `<EOS>` and there\n        will be no alive sequences to continue. With `2 * beam_size`, this\n        will not happen. This relies on the assumption the vocab size is >\n        beam size. If this is true, we\'ll have at least `beam_size` non-`<EOS>`\n        extensions if we extract the next top `2 * beam` words.\n        Length penalty is given by :math:`(5+len(decode)/6) ^ -\\alpha`.\n\n        Please refer to https://arxiv.org/abs/1609.08144.\n\n        Args:\n            i: loop index\n            alive_seq: Top-k sequences decoded so far.\n                Shape: `[batch_size, beam_size, i + 1]`.\n            alive_log_probs: Log-probabilities of these sequences.\n                Shape: `[batch_size, beam_size]`\n            states: (possibly nested structure of) decoding states.\n\n        :returns: Tuple of:\n\n            - Top-k sequences extended by the next word.\n            - Log-probabilities of these sequences,\n            - The scores with length penalty of these sequences,\n            - Flags indicating which of these sequences have finished\n              decoding.\n            - Transformed decoding states with same structure as :attr:`state`.\n        """"""\n        # Get the logits for all the possible next symbols\n        flat_ids = alive_seq.view(batch_size * beam_size, -1)\n\n        # (batch_size * beam_size, decoded_length)\n        if states is not None:\n            flat_states = map_structure(_merge_beam_dim, states)\n            flat_logits, flat_states = symbols_to_logits_fn(\n                flat_ids, flat_states\n            )\n            states = map_structure(\n                lambda t: _unmerge_beam_dim(t, batch_size, beam_size),\n                flat_states,\n            )\n        else:\n            flat_logits = symbols_to_logits_fn(flat_ids)\n        logits = flat_logits.view(batch_size, beam_size, -1)\n\n        # Convert logits to normalized log-probs\n        candidate_log_probs = log_prob_from_logits(logits)\n\n        # Multiply the probabilities by the current probabilities of the\n        # beam.\n        # (batch_size, beam_size, vocab_size) + (batch_size, beam_size, 1)\n        log_probs = candidate_log_probs + alive_log_probs.unsqueeze(dim=2)\n\n        length_penalty = ((5.0 + float(i + 1)) / 6.0) ** alpha\n\n        curr_scores = log_probs / length_penalty\n        # Flatten out (beam_size, vocab_size) probs in to a list of\n        # possibilities\n        flat_curr_scores = curr_scores.view(-1, beam_size * vocab_size)\n\n        topk_scores, topk_ids = torch.topk(flat_curr_scores, k=beam_size * 2)\n        # Recovering the log-probs because we will need to send them back\n        topk_log_probs = topk_scores * length_penalty\n\n        # Work out what beam the top probabilities are in.\n        topk_beam_index = topk_ids / vocab_size\n        topk_ids %= vocab_size  # Un-flatten the ids\n\n        # The next three steps are to create coordinates for torch.gather_nd\n        # to pull out the correct sequences from id\'s that we need to grow.\n        # We will also use the coordinates to gather the booleans of the\n        # beam items that survived.\n        batch_pos = compute_batch_indices(batch_size, beam_size * 2)\n        batch_pos = batch_pos.to(device=topk_beam_index.device)\n        # top beams will give us the actual coordinates to do the gather.\n        # stacking will create a tensor of dimension batch * beam * 2,\n        # where the last dimension contains the i,j gathering coordinates.\n        topk_coordinates = torch.stack([batch_pos, topk_beam_index], dim=2)\n        # [batch_size, beam_size, 2]\n\n        topk_seq = gather_nd(alive_seq, topk_coordinates)\n\n        if states is not None:\n            states = map_structure(\n                lambda state: gather_nd(state, topk_coordinates), states\n            )\n\n        # Append the most probable alive\n        topk_seq = torch.cat([topk_seq, topk_ids.unsqueeze(dim=2)], dim=2)\n\n        topk_finished = topk_ids == eos_id\n\n        return topk_seq, topk_log_probs, topk_scores, topk_finished, states\n\n    def inner_loop(\n        i: int,\n        alive_seq: torch.LongTensor,\n        alive_log_probs: torch.Tensor,\n        finished_seq: torch.LongTensor,\n        finished_scores: torch.Tensor,\n        finished_flags: torch.ByteTensor,\n        states: Optional[State],\n    ) -> Tuple[int, torch.LongTensor, torch.Tensor, torch.LongTensor,\n               torch.Tensor, torch.ByteTensor, Optional[State]]:\n        r""""""Inner beam search loop.\n\n        There are three groups of tensors: `alive`, `finished`, and `top-k`.\n\n        - The `alive` group contains information about the current alive\n          sequences.\n        - The `top-k` group contains information about `alive + top_k`\n          current decoded words.\n        - The `finished` group contains information about finished sentences,\n          that is, the ones that have decoded to `<EOS>`. These are what we\n          return.\n\n        The general beam search algorithm is as follows:\n\n            While not terminated (please refer to termination condition):\n\n            1. Grow the current `alive` to get `beam * 2` top-k sequences.\n            2. Among the `top-k`, move the top `beam_size` ones that haven\'t\n               reached `EOS` into `alive`.\n            3. Among the `top-k`, move the top `beam_size` ones have reached\n               `EOS` into `finished`.\n\n            Repeat\n\n        To make things simple with using fixed size tensors, we will end\n        up inserting unfinished sequences into finished in the beginning.\n        To prevent that we add `-INF` to the score of the unfinished\n        sequence so that when a true finished sequence does appear, it\n        will have a higher score than all the unfinished ones.\n\n        Args:\n            i: Loop index\n            alive_seq: Topk sequences decoded so far\n                Shape: `[batch_size, beam_size, i + 1]`.\n            alive_log_probs: Log-probabilities of the beams.\n                Shape: `[batch_size, beam_size]`\n            finished_seq: Current finished sequences.\n                Shape: `[batch_size, beam_size, i+1]`.\n            finished_scores: Scores for each of these sequences.\n                Shape: `[batch_size, beam_size]`.\n            finished_flags: Finished flags for each of these sequences.\n                Shape: `[batch_size, beam_size]`\n            states: (possibly nested structure of) decoding states.\n\n        :returns: Tuple of:\n\n            - Incremented loop index.\n            - New `alive` sequences.\n            - Log-probabilities of the `alive` sequences.\n            - New `finished` sequences.\n            - Scores of the `finished` sequences.\n            - Flags indicating which sequences in `finished` has reached `EOS`.\n            - Final decoding states with same structure as :attr:`state`.\n        """"""\n\n        # Each inner loop, we carry out three steps:\n        # 1. Get the current top-k items.\n        # 2. Extract the ones that have finished and haven\'t finished\n        # 3. Recompute the contents of finished based on scores.\n        topk_seq, topk_log_probs, topk_scores, topk_finished, \\\n                states = grow_topk(i, alive_seq, alive_log_probs, states)\n\n        alive_seq, alive_log_probs, _, states = grow_alive(\n            topk_seq, topk_scores, topk_log_probs, topk_finished, states\n        )\n        finished_seq, finished_scores, finished_flags = grow_finished(\n            finished_seq,\n            finished_scores,\n            finished_flags,\n            topk_seq,\n            topk_scores,\n            topk_finished,\n        )\n\n        return (\n            i + 1,\n            alive_seq,\n            alive_log_probs,\n            finished_seq,\n            finished_scores,\n            finished_flags,\n            states,\n        )\n\n    def _is_finished(\n        i: int,\n        alive_log_probs: torch.Tensor,\n        finished_scores: torch.Tensor\n    ) -> bool:\n        r""""""Check termination condition.\n\n        We terminate when we decoded up to `decode_length` or the lowest\n        scoring item in finished has a greater score that the highest probable\n        item in alive divided by the max length penalty.\n\n        Args:\n            i: Loop index\n            alive_log_probs: Log-probabilities of the beams.\n                Shape: `[batch_size, beam_size]`.\n            finished_scores: Scores for each of these sequences.\n                Shape: `[batch_size, beam_size]`.\n\n        Returns:\n            Bool.\n        """"""\n        max_length_penalty = ((5.0 + float(decode_length)) / 6.0) ** alpha\n        # The best possible score of the most likely alive sequence\n        lower_bound_alive_scores = alive_log_probs[:, 0] / max_length_penalty\n\n        if not stop_early:\n            # by considering the min score (in the top N beams) we ensure that\n            # the decoder will keep decoding until there is at least one beam\n            # (in the top N) that can be improved (w.r.t. the alive beams).\n            # any unfinished beam will have score -INF - thus the min\n            # will always be -INF if there is at least one unfinished beam -\n            # which means the bound_is_met condition cannot be true in this\n            # case.\n            lowest_score_of_finished_in_finished = torch.min(finished_scores)\n        else:\n            # by taking the max score we only care about the first beam;\n            # as soon as this first beam cannot be beaten from the alive beams\n            # the beam decoder can stop.\n            # similarly to the above, if the top beam is not completed, its\n            # finished_score is -INF, thus it will not activate the\n            # bound_is_met condition. (i.e., decoder will keep going on).\n            # note we need to find the max for every sequence eparately - so,\n            # we need to keep the batch dimension (see axis=1)\n            lowest_score_of_finished_in_finished, _ = torch.max(finished_scores,\n                                                                dim=1)\n\n        bound_is_met = (\n            (lowest_score_of_finished_in_finished > lower_bound_alive_scores)\n            .all()\n            .item()\n        )\n\n        ret = (i < decode_length) & (~bound_is_met)\n\n        return ret\n\n    step = 0\n    while _is_finished(step, alive_log_probs, finished_scores):\n        step, alive_seq, alive_log_probs, finished_seq, finished_scores, \\\n                finished_flags, states = inner_loop(\n            step,\n            alive_seq,\n            alive_log_probs,\n            finished_seq,\n            finished_scores,\n            finished_flags,\n            states,\n        )\n\n    # Accounting for corner case: It\'s possible that no sequence in alive\n    # for a particular batch item ever reached EOS. In that case, we\n    # should just copy the contents of alive for that batch item. tf\n    # reduce_any(finished_flags, 1)\n    # if 0, means that no sequence for that batch index had reached EOS.\n    # We need to do the same for the scores as well.\n\n    ret_seq, ret_scores = [], []\n    for idx, flag_per_instance in enumerate(finished_flags.any(dim=1).tolist()):\n        if flag_per_instance:\n            ret_seq.append(finished_seq[idx])\n            ret_scores.append(finished_scores[idx])\n        else:\n            ret_seq.append(alive_seq[idx])\n            ret_scores.append(alive_log_probs[idx])\n\n    ret_seq = torch.stack(ret_seq, dim=0)\n    ret_scores = torch.stack(ret_scores, dim=0)\n\n    return ret_seq, ret_scores\n\n# pylint: enable=function-redefined\n'"
texar/torch/utils/dtypes.py,20,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtility functions related to data types.\n""""""\n\nfrom typing import Any, Dict, Optional, Union\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.hyperparams import HParams\n\n__all__ = [\n    \'torch_bool\',\n    \'get_numpy_dtype\',\n    \'is_str\',\n    \'is_callable\',\n    \'get_supported_scalar_types\',\n    \'maybe_hparams_to_dict\',\n    \'compat_as_text\',\n]\n\n# `torch.bool` exists in PyTorch 1.1, but the default type for comparisons\n# is still `torch.uint8`.\ntorch_bool = (torch.empty(()) < 0).dtype\n\nDTYPE_MAP = {\n    np.float32: [\'float32\', \'float\', \'tf.float32\', \'torch.float\',\n                 \'torch.float32\', float, np.float32, torch.float32],\n    np.float64: [\'float64\', \'tf.float64\', \'torch.float64\', np.float64,\n                 np.float_, torch.float64],\n    np.float16: [\'float16\', \'tf.float16\', \'torch.float16\', np.float16,\n                 torch.float16],\n    np.int32: [\'int\', \'int32\', \'tf.int32\', \'torch.int\', \'torch.int32\', int,\n               np.int32, torch.int32],\n    np.int64: [\'int64\', \'tf.int64\', \'torch.int64\', np.int64, np.int_,\n               torch.int64],\n    np.int16: [\'int16\', \'tf.int16\', \'torch.int16\', np.int16, torch.int16],\n    np.int8: [\'int8\', \'char\', \'tf.int8\', \'torch.int8\', np.int8, torch.int8],\n    np.uint8: [\'uint8\', \'tf.uint8\', \'torch.uint8\', np.uint8, torch.uint8],\n    np.bool_: [\'bool\', \'tf.bool\', \'torch.bool\', bool, np.bool, np.bool_,\n               torch_bool],\n    np.str_: [\'string\', \'str\', \'tf.string\', str, np.str, np.str_],\n    np.bytes_: [\'bytes\', \'np.bytes\', bytes, np.bytes_]\n}\n\n\ndef get_numpy_dtype(dtype: Union[str, type]):\n    r""""""Returns equivalent NumPy dtype.\n\n    Args:\n        dtype: A str, Python numeric or string type, NumPy data type, or\n            PyTorch dtype.\n\n    Returns:\n        The corresponding NumPy dtype.\n    """"""\n    for np_dtype, valid_values in DTYPE_MAP.items():\n        if dtype in valid_values:\n            return np_dtype\n\n    raise ValueError(\n        f""Unsupported conversion from type {dtype!s} to NumPy dtype"")\n\n\ndef is_callable(x):\n    r""""""Return `True` if :attr:`x` is callable.\n    """"""\n    return callable(x)\n\n\ndef is_str(x):\n    r""""""Returns `True` if :attr:`x` is either a str or unicode.\n    Returns `False` otherwise.\n    """"""\n    return isinstance(x, str)\n\n\ndef get_supported_scalar_types():\n    r""""""Returns a list of scalar types supported.\n    """"""\n    types = []\n    for key, value in DTYPE_MAP.items():\n        if key not in {np.str_, np.bytes_}:\n            types.extend(value)\n\n    return types\n\n\ndef maybe_hparams_to_dict(hparams: Optional[Union[HParams, Dict[str, Any]]]) \\\n        -> Optional[Dict[str, Any]]:\n    r""""""If :attr:`hparams` is an instance of :class:`~texar.torch.HParams`,\n    converts it to a ``dict`` and returns. If :attr:`hparams` is a ``dict``,\n    returns as is.\n\n    Args:\n        hparams: The :class:`~texar.torch.HParams` instance to convert.\n\n    Returns:\n        dict: The corresponding ``dict`` instance\n    """"""\n    if hparams is None:\n        return None\n    if isinstance(hparams, dict):\n        return hparams\n    return hparams.todict()\n\n\ndef _maybe_list_to_array(str_list, dtype_as):\n    if isinstance(dtype_as, (list, tuple)):\n        return type(dtype_as)(str_list)\n    elif isinstance(dtype_as, np.ndarray):\n        return np.array(str_list)\n    else:\n        return str_list\n\n\ndef _as_text(bytes_or_text, encoding=\'utf-8\'):\n    r""""""Returns the given argument as a unicode string.\n\n    Adapted from ``tensorflow.compat.as_text``.\n\n    Args:\n        bytes_or_text: A ``bytes``, ``str``, or ``unicode`` object.\n            encoding: A string indicating the charset for decoding unicode.\n\n    Returns:\n        A ``unicode`` (Python 2) or ``str`` (Python 3) object.\n\n    Raises:\n        TypeError: If ``bytes_or_text`` is not a binary or unicode string.\n    """"""\n    if isinstance(bytes_or_text, str):\n        return bytes_or_text\n    elif isinstance(bytes_or_text, bytes):\n        return bytes_or_text.decode(encoding)\n    else:\n        raise TypeError(\n            f""Expected binary or unicode string, got {bytes_or_text!r}"")\n\n\ndef compat_as_text(str_):\n    r""""""Converts strings into ``unicode`` (Python 2) or ``str`` (Python 3).\n\n    Args:\n        str\\_: A string or other data types convertible to string, or an\n            `n`-D numpy array or (possibly nested) list of such elements.\n\n    Returns:\n        The converted strings of the same structure/shape as :attr:`str_`.\n    """"""\n\n    def _recur_convert(s):\n        if isinstance(s, (list, tuple, np.ndarray)):\n            s_ = [_recur_convert(si) for si in s]\n            return _maybe_list_to_array(s_, s)\n        else:\n            try:\n                return _as_text(s)\n            except TypeError:\n                return _as_text(str(s))\n\n    text = _recur_convert(str_)\n\n    return text\n'"
texar/torch/utils/exceptions.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTexar defined exceptions.\n""""""\n\n__all__ = [\n    ""TexarError"",\n]\n\n\nclass TexarError(Exception):\n    r""""""\n    Texar error.\n    """"""\n'"
texar/torch/utils/nest.py,4,"b'# Apply from Tensorflow:\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/nest.py\n\n""""""This module can perform operations on nested structures. A nested\nstructure is a Python sequence, tuple (including `namedtuple`), or\ndict that can contain further sequences, tuples, and dicts.\n""""""\n\nimport collections\nfrom typing import TypeVar, Mapping, Union, Tuple, List, Any\nimport torch\n\n\nTypeArg = TypeVar(\'TypeArg\')  # type argument\nNestedStructure = Any\n\n\ndef is_sequence(seq: Any) -> bool:\n    r""""""If a instance is sequance(list, tuple, excluding torch.Size),\n    return True, else False.\n    Args:\n        seq: instance to be checked.\n    Returns:\n        bool, True if the input instance is sequence, otherwise False.\n    """"""\n    if isinstance(seq, torch.Size):\n        return False\n    return isinstance(seq, (list, tuple))\n\n\ndef flatten(structure: NestedStructure) -> List[Any]:\n    r""""""Returns a flat list from a given nested structure.\n    If nest is not a sequence, tuple, or dict, then returns a single-element\n    `list:[nest]`.\n    In the case of dict instances, the sequence consists of the values,\n    sorted by key to ensure deterministic behavior. This is true also for\n    `OrderedDict` instances: their sequence order is ignored, the sorting order\n    of keys is used instead. The same convention is followed in\n    :func:`~texar.torch.utils.nest.pack_sequence_as`. This correctly repacks\n    dictionaries and `OrderedDict`s after they have been flattened, and also\n    allows flattening an `OrderedDict` and then repacking it back using a\n    corresponding plain dict, or vice-versa. Dictionaries with non-sortable\n    keys cannot be flattened. Users must not modify any collections used in\n    nest while this function is running.\n\n    Args:\n        structure: an arbitrarily nested structure or a scalar object. Note,\n            numpy arrays are considered scalars.\n\n    Returns:\n        A Python list, the flattened version of the input.\n\n    Raises:\n        TypeError: The nest is or contains a dict with non-sortable keys.\n    """"""\n    res: List[Any] = []\n    if isinstance(structure, dict):\n        structure = list(structure.values())\n\n    if not is_sequence(structure):\n        return [structure]\n    else:\n        for item in _yield_value(structure):\n            res += flatten(item)\n\n    return res\n\n\ndef pack_sequence_as(structure: NestedStructure,\n                     flat_sequence: Union[List, Tuple]\n                     ) -> NestedStructure:\n    r""""""Returns a given flattened sequence packed into a given structure.\n    If ``structure`` is a scalar, ``flat_sequence`` must be a single-element\n    list; in this case the return value is ``flat_sequence[0]``.\n    If ``structure`` is or contains a dict instance, the keys will be sorted to\n    pack the flat sequence in deterministic order. This is true also for\n    `OrderedDict` instances: their sequence order is ignored, the sorting\n    order of keys is used instead. The same convention is followed in\n    :func:`~texar.torch.utils.nest.flatten`. This correctly repacks dictionaries\n    and `OrderedDicts` after they have been flattened, and also allows\n    flattening an `OrderedDict` and then repacking it back using a\n    corresponding plain dict, or vice-versa. Dictionaries with non-sortable\n    keys cannot be flattened.\n\n    Args:\n        structure: Nested structure, whose structure is given by nested lists,\n            tuples, and dictionaries. Note: numpy arrays and strings are\n            considered scalars.\n        flat_sequence: flat sequence to pack.\n\n    Returns:\n        packed: ``flat_sequence`` converted to have the same recursive\n        structure as ``structure``.\n\n    Raises:\n        ValueError: If ``flat_sequence`` and ``structure`` have different\n            element counts.\n        TypeError: ``structure`` is or contains a dict with non-sortable keys.\n    """"""\n    if not is_sequence(flat_sequence):\n        raise TypeError(""flat_sequence must be a sequence"")\n    if isinstance(structure, dict):\n        structure = list(structure.values())\n    if not is_sequence(structure):\n        if len(flat_sequence) != 1:\n            raise ValueError(""Structure is a scalar""\n                             ""but len(flat_sequence) == %d > 1""\n                             % len(flat_sequence))\n        return flat_sequence[0]\n    try:\n        final_index, packed = _packed_nest_with_indices(structure,\n                                                        flat_sequence, 0)\n        if final_index < len(flat_sequence):\n            raise IndexError\n    except IndexError:\n        flat_structure = flatten(structure)\n        if len(flat_structure) != len(flat_sequence):\n            raise ValueError(\n                ""Could not pack sequence. Structure had %d elements, but ""\n                ""flat_sequence had %d elements.  Structure: %s,""\n                ""flat_sequence: %s."" %\n                (len(flat_structure),\n                 len(flat_sequence),\n                 structure,\n                 flat_sequence))\n    return _sequence_like(structure, packed)\n\n\ndef _sorted(dict_: Mapping[Any, Any]) -> List[TypeArg]:\n    r""""""Returns a sorted list of the dict keys, with error if keys not\n    sortable.\n    """"""\n    try:\n        return sorted(dict_)\n    except TypeError:\n        raise TypeError(""nest only supports dicts with sortable keys."")\n\n\ndef _is_namedtuple(instance: object) -> bool:\n    r""""""Returns True if `instance` is a `namedtuple`.\n    Args:\n        instance: An instance of a Python object.\n    Returns:\n        True if `instance` is a `namedtuple`.\n    """"""\n    type_ = type(instance)\n    base_ = type_.__bases__\n    if len(base_) != 1 or base_[0] != tuple:\n        return False\n    field_ = getattr(type_, \'_fields\', None)\n    if not isinstance(field_, tuple):\n        return False\n    return all(isinstance(n, str) for n in field_)\n\n\ndef _yield_value(iterable):\n    r""""""Yield only sorted values from `iterable`.\n    Args:\n        iterable: an iterable.\n    Yields:\n        The iterable\'s values, in order of sorted keys or items.\n    """"""\n    for _, value in _yield_sorted_items(iterable):\n        yield value\n\n\ndef _yield_sorted_items(iterable):\n    r""""""Yield (key, value) pairs for `iterable` in a deterministic order.\n    For Sequences, the key will be an int, the array index of a value.\n    For Mappings, the key will be the dictionary key.\n    For objects (e.g. namedtuples), the key will be the attribute name.\n    In all cases, the keys will be iterated in sorted order.\n    Args:\n        iterable: an iterable.\n    Yields:\n        The iterable\'s (key, value) pairs, in order of sorted keys.\n    """"""\n    if isinstance(iterable, collections.abc.Mapping):\n        for key in _sorted(iterable):\n            yield key, iterable[key]\n\n    elif _is_namedtuple(iterable):\n        for field in iterable._fields:\n            yield field, getattr(iterable, field)\n\n    else:\n        for index, item in enumerate(iterable):\n            yield index, item\n\n\ndef _packed_nest_with_indices(structure: NestedStructure,\n                              flat: NestedStructure,\n                              index: int) -> Tuple[int, NestedStructure]:\n    r""""""Helper function for pack_sequence_as.\n    Args:\n        structure: Substructure (list / tuple / dict) to mimic.\n        flat: Flattened values to output substructure for.\n        index: Index at which to start reading from flat.\n        is_seq: Function used to test if a value should be treated as a\n            sequence.\n    Returns:\n        The tuple (new_index, child), where:\n        * new_index - the updated index into `flat` having processed\n                    `structure`.\n        * packed - the subset of `flat` corresponding to `structure`,\n                    having started at `index`, and packed into the same nested\n                    format.\n    Raises:\n        ValueError: if `structure` contains more elements than `flat`\n        (assuming indexing starts from `index`).\n    """"""\n    packed = []\n    for value in _yield_value(structure):\n        if is_sequence(value):\n            new_index, child = _packed_nest_with_indices(value, flat, index)\n            packed.append(_sequence_like(value, child))\n            index = new_index\n        else:\n            packed.append(flat[index])\n            index += 1\n    return index, packed\n\n\nInstanceType = Any\n\n\ndef _sequence_like(instance: InstanceType,\n                   args: Any) -> InstanceType:\n    r""""""Converts the sequence `args` to the same type as `instance`.\n    Args:\n        instance: an instance of `tuple`, `list`, `namedtuple`, `dict`,\n            `collections.OrderedDict`.\n        args: elements to be converted to the `instance` type.\n    Returns:\n        `args` with the type of `instance`.\n    """"""\n    if isinstance(instance, collections.abc.Mapping):\n        result: Mapping[Any, Any] = dict(zip(_sorted(instance), args))\n        generator = ((key, result[key]) for key in instance)\n        return type(instance)(generator)  # type: ignore\n    elif _is_namedtuple(instance):\n        return type(instance)(*args)\n    else:\n        # Not a namedtuple\n        return type(instance)(args)\n'"
texar/torch/utils/rnn.py,30,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""RNN helpers for PyTorch models.""""""\n\nfrom typing import List, Optional, Tuple, TypeVar, Union\n\nimport torch\n\nfrom texar.torch.core.cell_wrappers import RNNCellBase\nfrom texar.torch.utils.shapes import mask_sequences\nfrom texar.torch.utils.utils import map_structure, map_structure_zip, no_map\n\n__all__ = [\n    ""reverse_sequence"",\n    ""dynamic_rnn"",\n    ""bidirectional_dynamic_rnn"",\n]\n\nState = TypeVar(\'State\')\n\n\ndef reverse_sequence(inputs: torch.Tensor,\n                     seq_lengths: Union[torch.LongTensor, List[int]],\n                     time_major: bool) -> torch.Tensor:\n    r""""""Reverses variable length slices.\n\n    This op first slices input along the dimension batch_axis, and for each\n    slice i, reverses the first seq_lengths[i] elements along the dimension\n    seq_axis.\n\n    The elements of seq_lengths must obey seq_lengths[i] <=\n    input.dims[seq_dim], and seq_lengths must be a vector of length\n    input.dims[batch_dim].\n\n    The output slice i along dimension batch_axis is then given by input slice\n    i, with the first seq_lengths[i] slices along dimension seq_axis reversed.\n\n    Args:\n        inputs: A Tensor. The input to reverse.\n        seq_lengths: A Tensor. Must be one of the following types: int32,\n            int64. 1-D with length input.dims(batch_dim) and\n            max(seq_lengths) <= input.dims(seq_dim)\n        time_major: The shape format of the ``inputs`` and ``outputs`` Tensors.\n            If true, these ``Tensors`` must be shaped\n            ``[max_time, batch_size, depth]``. If false, these ``Tensors`` must\n            be shaped ``[batch_size, max_time, depth]``.\n            Using ``time_major = True`` is a bit more efficient because it\n            avoids transposes at the beginning and end of the RNN calculation.\n            However, most TensorFlow data is batch-major, so by\n            default this functionb accepts input and emits output\n            in batch-major form.\n\n    Returns:\n        A ``Tensor``. Has the same type as input.\n    """"""\n    if time_major:\n        inputs = inputs.permute(1, 0, 2)\n\n    batch_size = inputs.shape[0]\n\n    outputs = inputs.clone()\n    for i in range(batch_size):\n        outputs[i][0:seq_lengths[i]] = torch.flip(\n            inputs[i][0:seq_lengths[i]], dims=(0,))\n    if time_major:\n        outputs = outputs.permute(1, 0, 2)\n\n    return outputs\n\n\ndef bidirectional_dynamic_rnn(\n        cell_fw: RNNCellBase[State],\n        cell_bw: RNNCellBase[State],\n        inputs: torch.Tensor,\n        sequence_length: Optional[Union[torch.LongTensor, List[int]]] = None,\n        initial_state_fw: Optional[State] = None,\n        initial_state_bw: Optional[State] = None,\n        time_major: bool = False) -> Tuple[Tuple[torch.Tensor, torch.Tensor],\n                                           Tuple[State, State]]:\n    r""""""Creates a dynamic version of bidirectional recurrent neural network.\n\n    Takes input and builds independent forward and backward RNNs. The\n    input_size of forward and backward cell must match. The initial state for\n    both directions is zero by default (but can be set optionally) and no\n    intermediate states are ever returned -- the network is fully unrolled\n    for the given (passed in) length(s) of the sequence(s) or completely\n    unrolled if length(s) is not given.\n\n    Args:\n        cell_fw: An instance of RNNCell, to be used for forward direction.\n        cell_bw: An instance of RNNCell, to be used for backward direction.\n        inputs: The RNN inputs.\n            If time_major == False (default), this must be a tensor of shape:\n            ``[batch_size, max_time, ...]``, or a nested tuple of such elements.\n            If time_major == True, this must be a tensor of shape:\n            ``[max_time, batch_size, ...]``, or a nested tuple of such elements.\n        sequence_length: (optional) An int32/int64 tensor, size\n            ``[batch_size]``, containing the actual lengths for each of the\n            sequences in\n            the batch. If not provided, all batch entries are assumed\n            to be full sequences; and time reversal is applied from time\n            ``0`` to ``max_time`` for each sequence.\n        initial_state_fw: (optional) An initial state for the forward RNN.\n            This must be a tensor of appropriate type and shape\n            ``[batch_size, cell_fw.state_size]``.\n            If ``cell_fw.state_size`` is a tuple, this should be a tuple of\n            tensors having shapes ``[batch_size, s]``\n            for ``s`` in ``cell_fw.state_size``.\n        initial_state_bw: (optional) Same as for ``initial_state_fw``, but using\n            the corresponding properties of ``cell_bw``.\n        time_major: The shape format of the ``inputs`` and ``outputs`` Tensors.\n            If true, these ``Tensors`` must be shaped\n            ``[max_time, batch_size, depth]``.\n            If false, these ``Tensors`` must be shaped\n            ``[batch_size, max_time, depth]``.\n            Using ``time_major = True`` is a bit more efficient because it\n            avoids transposes at the beginning and end of the RNN calculation.\n            However, most TensorFlow data is batch-major, so by\n            default this function accepts input and emits output\n            in batch-major form.\n\n    Returns:\n        A tuple (outputs, output_states) where:\n\n        outputs: A tuple (output_fw, output_bw) containing the forward and\n            the backward rnn output ``Tensor``.\n            If time_major == False (default),\n                output_fw will be a ``Tensor`` shaped:\n                ``[batch_size, max_time, cell_fw.output_size]``\n                and output_bw will be a ``Tensor`` shaped:\n                ``[batch_size, max_time, cell_bw.output_size]``.\n            If time_major == True,\n                output_fw will be a ``Tensor`` shaped:\n                ``[max_time, batch_size, cell_fw.output_size]``\n                and output_bw will be a ``Tensor`` shaped:\n                ``[max_time, batch_size, cell_bw.output_size]``.\n            It returns a tuple instead of a single concatenated ``Tensor``,\n            unlike in the ``bidirectional_rnn``. If the concatenated\n            one is preferred, the forward and backward outputs can\n            be concatenated as ``tf.concat(outputs, 2)``.\n        output_states: A tuple (output_state_fw, output_state_bw) containing\n            the forward and the backward final states of bidirectional rnn.\n    """"""\n    output_fw, output_state_fw = dynamic_rnn(cell=cell_fw,\n                                             inputs=inputs,\n                                             sequence_length=sequence_length,\n                                             initial_state=initial_state_fw,\n                                             time_major=time_major)\n    if time_major:\n        time_steps = inputs.shape[0]\n        batch_size = inputs.shape[1]\n    else:\n        time_steps = inputs.shape[1]\n        batch_size = inputs.shape[0]\n\n    if sequence_length is None:\n        sequence_length = torch.tensor([time_steps] * batch_size,\n                                       dtype=torch.int32,\n                                       device=inputs.device)\n\n    # Backward direction\n    inputs_reverse = reverse_sequence(inputs=inputs,\n                                      seq_lengths=sequence_length,\n                                      time_major=time_major)\n\n    tmp, output_state_bw = dynamic_rnn(cell=cell_bw,\n                                       inputs=inputs_reverse,\n                                       sequence_length=sequence_length,\n                                       initial_state=initial_state_bw,\n                                       time_major=time_major)\n    output_bw = reverse_sequence(inputs=tmp,\n                                 seq_lengths=sequence_length,\n                                 time_major=time_major)\n\n    outputs = (output_fw, output_bw)\n    output_states = (output_state_fw, output_state_bw)\n\n    return outputs, output_states\n\n\ndef dynamic_rnn(\n        cell: RNNCellBase[State],\n        inputs: torch.Tensor,\n        sequence_length: Optional[Union[torch.LongTensor, List[int]]] = None,\n        initial_state: Optional[State] = None,\n        time_major: bool = False) -> Tuple[torch.Tensor, State]:\n    r""""""Creates a recurrent neural network specified by RNNCell ``cell``.\n\n    Performs fully dynamic unrolling of ``inputs``.\n\n    Args:\n        cell: An instance of RNNCell.\n        inputs: The RNN inputs.\n            If ``time_major == False`` (default), this must be a ``Tensor``\n            of shape: ``[batch_size, max_time, ...]``, or a nested\n            tuple of such elements.\n            If ``time_major == True``, this must be a ``Tensor`` of shape:\n            ``[max_time, batch_size, ...]``, or a nested tuple of such\n            elements.\n            This may also be a (possibly nested) tuple of Tensors satisfying\n            this property.  The first two dimensions must match across all the\n            inputs, but otherwise the ranks and other shape components\n            may differ. In this case, input to ``cell`` at each time-step\n            will replicate the structure of these tuples, except for the\n            time dimension (from which the time is taken).\n            The input to ``cell`` at each time step will be a\n            ``Tensor`` or (possibly nested) tuple of Tensors each with\n            dimensions ``[batch_size, ...]``.\n        sequence_length: (optional) An int32/int64 tensor sized\n            ``[batch_size]``. Used to copy-through state and\n            zero-out outputs when past a batch element\'s sequence length.\n            So it\'s more for performance than correctness.\n        initial_state: (optional) An initial state for the RNN.\n            If ``cell.state_size`` is an integer, this must be\n            a ``Tensor`` of appropriate type and shape\n            ``[batch_size, cell.state_size]``. If ``cell.state_size`` is\n            a tuple, this should be a tuple of tensors having shapes\n            ``[batch_size, s]`` for ``s`` in ``cell.state_size``.\n        time_major: The shape format of the ``inputs`` and ``outputs`` Tensors.\n            If true, these ``Tensors`` must be shaped\n            ``[max_time, batch_size, depth]``. If false, these ``Tensors``\n            must be shaped ``[batch_size, max_time, depth]``.\n            Using ``time_major = True`` is a bit more efficient because\n            it avoids transposes at the beginning and end of the\n            RNN calculation. However, most TensorFlow data is batch-major,\n            so by default this function accepts input and emits output in\n            batch-major form.\n\n    Returns:\n        A pair (outputs, state) where:\n\n        outputs: The RNN output ``Tensor``.\n\n            If time_major == False (default), this will be a ``Tensor`` shaped:\n            ``[batch_size, max_time, cell.output_size]``.\n\n            If time_major == True, this will be a ``Tensor`` shaped:\n            ``[max_time, batch_size, cell.output_size]``.\n\n            Note, if ``cell.output_size`` is a (possibly nested) tuple of\n            integers or ``torch.Size`` objects, then ``outputs``\n            will be a tuple having the same structure as ``cell.output_size``,\n            containing Tensors having shapes corresponding to the shape\n            data in ``cell.output_size``.\n\n        state: The final state.  If ``cell.state_size`` is an int, this\n            will be shaped ``[batch_size, cell.state_size]``.  If it is a\n            ``torch.Size``, this will be shaped\n            ``[batch_size] + cell.state_size``.\n            If it is a (possibly nested) tuple of ints or ``torch.Size``,\n            this will be a tuple having the corresponding shapes.\n            If cells are ``LSTMCells``, ``state`` will be a tuple containing\n            a ``LSTMStateTuple`` for each cell.\n\n    Raises:\n        TypeError: If ``cell`` is not an instance of RNNCell.\n        ValueError: If inputs is None or an empty list.\n    """"""\n    # By default, time_major==False and inputs are batch-major: shaped\n    #   [batch, time, depth]\n    # For internal calculations, we transpose to [time, batch, depth]\n    if not time_major:\n        # (B,T,D) => (T,B,D)\n        inputs = inputs.permute(1, 0, 2)\n\n    time_steps = inputs.shape[0]\n    batch_size = inputs.shape[1]\n\n    if sequence_length is not None:\n        if not isinstance(sequence_length, torch.Tensor):\n            sequence_length = torch.tensor(sequence_length,\n                                           dtype=torch.int32,\n                                           device=inputs.device)\n\n        if sequence_length.dim() != 1:\n            raise ValueError(\n                ""sequence_length must be a vector of length batch_size, ""\n                ""but saw shape: %s"" % sequence_length.shape)\n        if sequence_length.shape != torch.Size([batch_size]):\n            raise ValueError(""Expected shape for Tensor sequence_length is %s""\n                             % batch_size, "" but saw shape: %s""\n                             % sequence_length.shape)\n    else:\n        sequence_length = torch.tensor([time_steps] * batch_size,\n                                       dtype=torch.int32,\n                                       device=inputs.device)\n\n    if initial_state is not None:\n        state = initial_state\n    else:\n        state = cell.zero_state(batch_size=batch_size)\n\n    (outputs, final_state) = _dynamic_rnn_loop(\n        cell, inputs, state, sequence_length=sequence_length)\n\n    # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\n    # If we are performing batch-major calculations, transpose output back\n    # to shape [batch, time, depth]\n    if not time_major:\n        # (T,B,D) => (B,T,D)\n        outputs = outputs.permute(1, 0, 2)\n\n    return outputs, final_state\n\n\ndef _dynamic_rnn_loop(cell: RNNCellBase[State],\n                      inputs: torch.Tensor,\n                      initial_state: State,\n                      sequence_length: torch.LongTensor) \\\n        -> Tuple[torch.Tensor, State]:\n    r""""""Internal implementation of Dynamic RNN.\n\n    Args:\n        cell: An instance of RNNCell.\n        inputs: A ``Tensor`` of shape ``[time, batch_size, input_size]``,\n            or a nested tuple of such elements.\n        initial_state: A ``Tensor`` of shape ``[batch_size, state_size]``,\n            or if ``cell.state_size`` is a tuple, then this should be a tuple\n            of tensors having shapes ``[batch_size, s]`` for ``s`` in\n            ``cell.state_size``.\n        sequence_length: (optional) An ``int32`` ``Tensor``\n            of shape ``[batch_size]``.\n\n    Returns:\n        Tuple ``(final_outputs, final_state)``.\n        final_outputs:\n            A ``Tensor`` of shape ``[time, batch_size, cell.output_size]``. If\n            ``cell.output_size`` is a (possibly nested) tuple of ints or\n            ``torch.Size`` objects, then this returns a\n            (possibly nested) tuple of Tensors matching the corresponding\n            shapes.\n        final_state:\n            A ``Tensor``, or possibly nested tuple of Tensors, matching\n            in length and shapes to ``initial_state``.\n    """"""\n    state = initial_state\n    time_steps = inputs.shape[0]\n    all_outputs = []\n\n    all_state = map_structure(lambda _: no_map(list), state)\n\n    for i in range(time_steps):\n        output, state = cell(inputs[i], state)\n        all_outputs.append(output)\n        map_structure_zip(lambda xs, x: xs.append(x), (all_state, state))\n    # TODO: Do not compute everything regardless of sequence_length\n\n    final_outputs = torch.stack(all_outputs, dim=0)\n    final_outputs = mask_sequences(final_outputs,\n                                   sequence_length=sequence_length,\n                                   time_major=True)\n\n    final_state = map_structure(lambda _: no_map(list), state)\n    # pylint: disable=cell-var-from-loop\n    # Our use case is fine because the function is called immediately and\n    # exclusively in the current iteration of the loop.\n    for batch_idx, time_idx in enumerate(sequence_length.tolist()):\n        if time_idx > 0:\n            map_structure_zip(\n                lambda xs, x: xs.append(x[time_idx - 1][batch_idx]),\n                (final_state, all_state))\n        else:\n            map_structure_zip(\n                lambda xs, x: xs.append(x[batch_idx]),\n                (final_state, initial_state))\n    # pylint: enable=cell-var-from-loop\n\n    final_state = map_structure(\n            lambda x: torch.stack(x, dim=0), final_state)\n\n    return final_outputs, final_state\n'"
texar/torch/utils/shapes.py,22,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtility functions related to tensor shapes.\n""""""\n\nfrom typing import Any, List, Optional, Union\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.utils import utils\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""transpose_batch_time"",\n    ""get_batch_size"",\n    ""get_rank"",\n    ""mask_sequences"",\n    ""flatten"",\n    ""pad_and_concat"",\n]\n\n\ndef transpose_batch_time(inputs: torch.Tensor) -> torch.Tensor:\n    r""""""Transposes inputs between time-major and batch-major.\n\n    Args:\n        inputs: A Tensor of shape ``[batch_size, max_time, ...]`` (batch-major)\n            or ``[max_time, batch_size, ...]`` (time-major), or a (possibly\n            nested) tuple of such elements.\n\n    Returns:\n        A (possibly nested tuple of) Tensor with transposed batch and\n        time dimensions of inputs.\n    """"""\n    return inputs.transpose(0, 1)\n\n\ndef get_batch_size(tensor: torch.Tensor) -> int:\n    r""""""Returns an ``int`` representing the batch size, i.e.,\n    the size of the 1st dimension of :attr:`tensor`.\n    """"""\n    return tensor.size(0)\n\n\ndef get_rank(tensor: torch.Tensor) -> int:\n    r""""""Returns the tensor rank as a Python ``int``. The input tensor can also\n    be a Python array.\n\n    Args:\n        tensor: A Tensor or Python array.\n\n    Returns:\n        A Python ``int`` representing the rank of :attr:`tensor`. Returns\n        `None` if the rank cannot be determined.\n    """"""\n    if torch.is_tensor(tensor):\n        rank = tensor.dim()\n    else:\n        array = np.asarray(tensor)\n        rank = array.ndim\n    return rank\n\n\ndef mask_sequences(sequence: Union[torch.Tensor, List[int]],\n                   sequence_length: Union[torch.LongTensor, List[int]],\n                   dtype: Optional[torch.dtype] = None,\n                   time_major: bool = False) -> torch.Tensor:\n    r""""""Masks out sequence entries that are beyond the respective sequence\n    lengths. Masks along the time dimension.\n\n    :attr:`sequence` and :attr:`sequence_length` can either be python\n    arrays or Tensors, respectively. If both are Python arrays (or None), the\n    return will be a Python array as well.\n\n    Args:\n        sequence: A Tensor or Python array of sequence values.\n            If ``time_major==False`` (default), this must be a Tensor of shape\n            ``[batch_size, max_time, ...]``. The batch and time dimension is\n            exchanged if ``time_major==True``.\n        sequence_length: A Tensor or python array of shape ``[batch_size]``.\n            Time steps beyond the respective sequence lengths will be\n            made zero.\n        dtype (dtype): Type of :attr:`sequence`. If `None`, infer from\n            :attr:`sequence` automatically.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`sequence` must have shape\n            ``[max_time, batch_size, ...]``.\n            If `False` (default), :attr:`sequence` must have\n            shape ``[batch_size, max_time, ...]``.\n\n    Returns:\n        The masked sequence, i.e., a Tensor or python array of the same shape\n        as :attr:`sequence` but with masked-out entries (set to zero).\n\n        If both :attr:`sequence` and :attr:`sequence_length` are python\n        arrays, the returned value is a python array as well.\n    """"""\n    if not torch.is_tensor(sequence):\n        sequence = torch.tensor(sequence, dtype=dtype)\n    sequence: torch.Tensor\n\n    rank = sequence.dim()\n    if rank < 2:\n        raise ValueError(""`sequence` must be 2D or higher order."")\n\n    if time_major:\n        sequence = transpose_batch_time(sequence)\n    max_time = sequence.size(1)\n    if dtype is None:\n        dtype = sequence.dtype\n    mask = utils.sequence_mask(sequence_length, max_time, dtype=dtype)\n    mask = mask.view(*mask.size(), *([1] * (rank - 2)))\n    sequence = sequence * mask\n    if time_major:\n        sequence = transpose_batch_time(sequence)\n\n    return sequence\n\n\ndef flatten(tensor: torch.Tensor, preserve_dims: int,\n            flattened_dim: Optional[int] = None) -> torch.Tensor:\n    r""""""Flattens a tensor whiling keeping several leading dimensions.\n\n    :attr:`preserve_dims` must be less than or equal to tensor\'s rank.\n\n    Args:\n        tensor: A Tensor to flatten.\n        preserve_dims (int): The number of leading dimensions to preserve.\n        flattened_dim (int, optional): The size of the resulting flattened\n            dimension. If not given, infer automatically.\n\n    Returns:\n        A Tensor with rank :attr:`preserve_dims` +1.\n\n    Example:\n        .. code-block:: python\n\n            x = torch.ones(d_1, d_2, d_3, d_4)\n            y = flatten(x, 2) # y.shape == [d_1, d_2, d_3 * d_4]\n    """"""\n    if preserve_dims > tensor.dim():\n        raise ValueError(\n            ""`preserve_dims` must be less than or equal to tensor\'s rank"")\n    if flattened_dim is None:\n        flattened_dim = -1\n    shape = tensor.size()[:preserve_dims] + (flattened_dim,)\n    tensor_ = tensor.reshape(shape)\n    return tensor_\n\n\ndef pad_and_concat(values: List[torch.Tensor], axis: int,\n                   pad_axis: Optional[MaybeList[int]] = None,\n                   pad_constant_values: Any = 0) -> torch.Tensor:\n    r""""""Concatenates tensors along one dimension. Pads each of other dimensions\n    of the tensors to the corresponding maximum size if necessary.\n\n    Args:\n        values: A list of Tensors of the same rank.\n        axis (int): A Python int. Dimension along which to concatenate.\n        pad_axis (int or list, optional): A Python int or a list of int.\n            Dimensions to pad. Paddings are only added to the end of\n            corresponding dimensions. If `None`, all dimensions except the\n            :attr:`axis` dimension are padded.\n        pad_constant_values: The scalar pad value to use. Must be same type\n            as the tensors.\n\n    Returns:\n        A ``Tensor`` resulting from padding and concatenation of the input\n        tensors.\n\n    Raises:\n        ValueError: If ``rank`` of :attr:`values` are not consistent.\n\n    Example:\n\n        .. code-block:: python\n\n            a = torch.ones([1, 2])\n            b = torch.ones([2, 3])\n\n            c = pad_and_concat([a,b], 0)\n            # c.shape == [3, 3]\n            # c == [[1, 1, 0],\n            #       [1, 1, 1],\n            #       [1, 1, 1]]\n\n            d = pad_and_concat([a,b], 1)\n            # d.shape == [2, 5]\n            # d == [[1, 1, 1, 1, 1]\n            #       [0, 0, 1, 1, 1]]\n    """"""\n    rank = values[0].dim()\n    if any(value.dim() != rank for value in values):\n        raise ValueError(""All tensors in `values` must have the same rank."")\n\n    if pad_axis is None:\n        pad_axis = [r for r in range(rank) if r != axis]\n    elif isinstance(pad_axis, int):\n        pad_axis = [pad_axis]\n    for pad_dim in pad_axis:\n        max_dim_size = max(v.size(pad_dim) for v in values)\n        for i, v in enumerate(values):\n            pad_shape: List[int] = list(v.size())\n            if pad_shape[pad_dim] == max_dim_size:\n                continue\n            pad_shape[pad_dim] = max_dim_size - pad_shape[pad_dim]\n            padding = values[0].new_full(tuple(pad_shape), pad_constant_values)\n            values[i] = torch.cat((v, padding), dim=pad_dim)\n\n    return torch.cat(values, dim=axis)\n'"
texar/torch/utils/test.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils for unit tests.\n""""""\n\nimport os\nimport unittest\n\n__all__ = [\n    ""pretrained_test"",\n    ""data_test"",\n    ""external_library_test"",\n]\n\n\ndef define_skip_condition(flag: str, explanation: str):\n    return unittest.skipUnless(\n        os.environ.get(flag, 0) or os.environ.get(\'TEST_ALL\', 0),\n        explanation + f"" Set `{flag}=1` or `TEST_ALL=1` to run."")\n\n\npretrained_test = define_skip_condition(\n    \'TEST_PRETRAINED\', ""Test requires loading pre-trained checkpoints."")\ndata_test = define_skip_condition(\n    \'TEST_DATA\', ""Test requires loading large data files."")\n\n\ndef external_library_test(name: str):\n    import importlib\n    try:\n        importlib.import_module(name)\n        return lambda x: x  # no changes\n    except ImportError:\n        return unittest.skip(\n            f""Test requires external library {name} to be installed."")\n'"
texar/torch/utils/transformer_attentions.py,6,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nAttentions specific to Transformer.\n""""""\n\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport torch\n\n__all__ = [\n    \'attention_bias_lower_triangle\',\n    \'attention_bias_ignore_padding\',\n    \'attention_bias_local\',\n]\n\n\ndef attention_bias_lower_triangle(length: int,\n                                  bias_value: float = -1e18) -> torch.Tensor:\n    r""""""Create an bias tensor to be added to attention logits.\n    Allows a query to attend to all positions up to and including its own.\n\n    Args:\n        length: a scalar.\n        bias_value: value to fill the bias tensor with.\n\n    Returns:\n        a ``Tensor`` with shape [1, 1, length, length].\n    """"""\n    return attention_bias_local(length, -1, 0, bias_value)\n\n\ndef attention_bias_local(length: int, max_backward: int, max_forward: int,\n                         bias_value: float = -1e18) -> torch.Tensor:\n    r""""""Create an bias tensor to be added to attention logits.\n    A position may attend to positions at most max_distance from it,\n    forward and backwards.\n\n    This does not actually save any computation.\n\n    Args:\n        length: int\n        max_backward: int, maximum distance backward to attend. Negative\n            values indicate unlimited.\n        max_forward: int, maximum distance forward to attend. Negative\n            values indicate unlimited.\n        bias_value: value to fill the bias tensor with.\n\n    Returns:\n        a ``Tensor`` with shape [1, 1, length, length].\n        [batch_size, num_heads, query_len, query_len]\n    """"""\n    band = _ones_matrix_band_part(\n        length,\n        length,\n        max_backward,\n        max_forward,\n        out_shape=(1, 1, length, length))\n    return bias_value * (1.0 - band)\n\n\ndef attention_bias_ignore_padding(memory_padding: torch.Tensor,\n                                  bias_value: float = -1e18) -> torch.Tensor:\n    r""""""Create an bias tensor to be added to attention logits.\n\n    Args:\n        memory_padding: a float ``Tensor`` with shape [batch, memory_length].\n        bias_value: value to fill the bias tensor with.\n\n    Returns:\n        a ``Tensor`` with shape [batch, 1, 1, memory_length].\n        each dim corresponding to batch_size, num_heads, queries_len,\n        memory_length\n    """"""\n    ret = memory_padding * bias_value\n    return ret.view(ret.size(0), 1, 1, ret.size(-1))\n\n\ndef _ones_matrix_band_part(rows: int, cols: int, num_lower: int, num_upper: int,\n                           out_shape: Optional[Tuple[int, ...]] = None) \\\n        -> torch.Tensor:\n    r""""""Matrix band part of ones.\n    """"""\n    if num_lower < 0:\n        num_lower = rows - 1\n    if num_upper < 0:\n        num_upper = cols - 1\n    lower_mask = np.tri(cols, rows, num_lower).T\n    upper_mask = np.tri(rows, cols, num_upper)\n    band = np.ones((rows, cols)) * lower_mask * upper_mask\n    if out_shape:\n        band = band.reshape(out_shape)\n    band = torch.as_tensor(band, dtype=torch.float32)\n    return band\n'"
texar/torch/utils/types.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nType annotation helpers.\n""""""\nimport os\nfrom typing import Dict, List, Sequence, Tuple, TypeVar, Union\n\n__all__ = [\n    \'MaybeTuple\',\n    \'MaybeList\',\n    \'MaybeSeq\',\n    \'MaybeDict\',\n    \'PathLike\',\n]\n\nT = TypeVar(\'T\')\nMaybeTuple = Union[T, Tuple[T, ...]]\nMaybeList = Union[T, List[T]]\nMaybeSeq = Union[T, Sequence[T]]\nMaybeDict = Union[T, Dict[str, T]]\nPathLike = TypeVar(\'PathLike\', str, os.PathLike)\n'"
texar/torch/utils/utils.py,28,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nMiscellaneous Utility functions.\n""""""\n\nimport collections\nimport copy\nimport inspect\nfrom functools import lru_cache\nfrom pydoc import locate\nfrom typing import (\n    Any, Callable, Collection, Dict, List, MutableMapping, Optional, Sequence,\n    Tuple, Type, TypeVar, Union, cast, no_type_check, overload)\n\nimport funcsigs\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.modules.conv import _ConvNd\n\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.utils.dtypes import _maybe_list_to_array\nfrom texar.torch.utils.types import MaybeSeq, MaybeTuple\n\nMAX_SEQ_LENGTH = np.iinfo(np.int32).max\n\n__all__ = [\n    \'no_map\',\n    \'map_structure\',\n    \'map_structure_zip\',\n    \'get_first_in_structure\',\n    \'sequence_mask\',\n    \'get_args\',\n    \'get_default_arg_values\',\n    \'check_or_get_class\',\n    \'get_class\',\n    \'check_or_get_instance\',\n    \'get_instance\',\n    \'check_or_get_instance_with_redundant_kwargs\',\n    \'get_instance_with_redundant_kwargs\',\n    \'get_function\',\n    \'call_function_with_redundant_kwargs\',\n    \'get_instance_kwargs\',\n    \'dict_patch\',\n    \'dict_lookup\',\n    \'dict_fetch\',\n    \'dict_pop\',\n    \'flatten_dict\',\n    \'strip_token\',\n    \'strip_eos\',\n    \'strip_bos\',\n    \'strip_special_tokens\',\n    \'str_join\',\n    \'default_str\',\n    \'uniquify_str\',\n    \'ceildiv\',\n    \'sum_tensors\',\n]\n\nT = TypeVar(\'T\')  # type argument\nR = TypeVar(\'R\')  # return type\nK = TypeVar(\'K\')  # key type\nV = TypeVar(\'V\')  # value type\nKwargs = Dict[str, Any]\nAnyDict = MutableMapping[str, Any]\nParamDict = Union[HParams, AnyDict]\n\nType_size_lambda_map = {\n    nn.Linear: lambda x: x.out_features,\n    nn.Bilinear: lambda x: x.out_features,\n    _ConvNd: lambda x: x.out_channels * len(x.kernel_size),\n    nn.Embedding: lambda x: x.embedding_dim,\n    nn.EmbeddingBag: lambda x: x.embedding_dim,\n    nn.RNNCellBase: lambda x: x.hidden_size,\n}\n\nType_size_keeper = [\n    nn.ELU, nn.Hardshrink, nn.Hardtanh, nn.LeakyReLU, nn.LogSigmoid,\n    nn.PReLU, nn.ReLU, nn.RReLU, nn.SELU, nn.CELU, nn.Sigmoid, nn.Softplus,\n    nn.Softshrink, nn.Softsign, nn.Tanh, nn.Tanhshrink, nn.Threshold,\n    nn.Softmin, nn.Softmax, nn.LogSoftmax, nn.Dropout, nn.AlphaDropout,\n]\n\n# NestedCollection = Union[T, Collection[\'NestedCollection\']]\n\n\n@lru_cache(maxsize=None)\ndef _no_map_type(container_type: Type[T]) -> Type[T]:\n    # Create a subtype of the container type that sets an normally inaccessible\n    # special attribute on instances.\n    # This is necessary because `setattr` does not work on built-in types\n    # (e.g. `list`).\n    new_type = type(""_no_map"" + container_type.__name__,\n                    (container_type,), {\'--no-map--\': True})\n    return new_type\n\n\ndef no_map(container_type: Type[T], *args, **kwargs) -> T:\n    r""""""Create a ""`non-mappable`"" container type, i.e. it will be treated as a\n    singleton object in :meth:`map_structure` and :meth:`map_structure_zip`,\n    its contents will not be traversed.\n\n    This is implemented by dynamically creating a subclass of the required type,\n    and overriding the :attr:`__subclasscheck__` class method to always return\n    `False`.\n\n    Args:\n        container_type: The type of the container to create,\n            e.g. `list`, `dict`.\n        args: Arguments to the constructor.\n        kwargs: Keyword arguments to the constructor\n\n    Returns:\n        The `non-mappable` container type.\n    """"""\n    return _no_map_type(container_type)(*args, **kwargs)  # type: ignore\n\n\n@no_type_check\ndef map_structure(fn: Callable[[T], R], obj: Collection[T]) -> Collection[R]:\n    r""""""Map a function over all elements in a (possibly nested) collection.\n\n    Args:\n        fn (callable): The function to call on elements.\n        obj: The collection to map function over.\n\n    Returns:\n        The collection in the same structure, with elements mapped.\n    """"""\n    if hasattr(obj, ""--no-map--""):\n        return fn(obj)\n    if isinstance(obj, list):\n        return [map_structure(fn, x) for x in obj]\n    if isinstance(obj, tuple):\n        if isinstance(obj, torch.Size):\n            return fn(obj)\n        if hasattr(obj, \'_fields\'):  # namedtuple\n            return type(obj)(*[map_structure(fn, x) for x in obj])\n        else:\n            return tuple(map_structure(fn, x) for x in obj)\n    if isinstance(obj, dict):\n        return {k: map_structure(fn, v) for k, v in obj.items()}\n    if isinstance(obj, set):\n        return {map_structure(fn, x) for x in obj}\n    return fn(obj)\n\n\n@no_type_check\ndef map_structure_zip(fn: Callable[..., R],\n                      objs: Sequence[Collection[T]]) -> Collection[R]:\n    r""""""Map a function over tuples formed by taking one elements from each\n    (possibly nested) collection. Each collection must have identical\n    structures.\n\n    .. note::\n        Although identical structures are required, it is not enforced by\n        assertions. The structure of the first collection is assumed to be\n        the structure for all collections.\n\n        For rare cases where collections need to have different structures,\n        refer to :meth:`no_map`.\n\n    Args:\n        fn (callable): The function to call on elements.\n        objs: The list of collections to map function over.\n\n    Returns:\n        A collection with the same structure, with elements mapped.\n    """"""\n    obj = objs[0]\n    if hasattr(obj, ""--no-map--""):\n        return fn(*objs)\n    if isinstance(obj, list):\n        return [map_structure_zip(fn, xs) for xs in zip(*objs)]\n    if isinstance(obj, tuple):\n        if isinstance(obj, torch.Size):\n            return fn(obj)\n        if hasattr(obj, \'_fields\'):  # namedtuple\n            return type(obj)(*[map_structure_zip(fn, xs) for xs in zip(*objs)])\n        else:\n            return tuple(map_structure_zip(fn, xs) for xs in zip(*objs))\n    if isinstance(obj, dict):\n        return {k: map_structure_zip(fn, [o[k] for o in objs])\n                for k in obj.keys()}\n    if isinstance(obj, set):\n        return {map_structure_zip(fn, xs) for xs in zip(*objs)}\n    return fn(*objs)\n\n\ndef get_first_in_structure(obj: Collection[T]) -> Optional[T]:\n    r""""""Return the first not-`None` element within a (possibly nested)\n    collection.\n\n    Args:\n        obj: The collection to pick the element from.\n\n    Returns:\n        The first non-`None` element from the collection, or `None` if the\n        collection is empty or contains only `None` elements.\n    """"""\n    item = None\n\n    def _get_first(x):\n        nonlocal item\n        if item is None:\n            item = x\n\n    map_structure(_get_first, obj)\n    return item\n\n\ndef sequence_mask(lengths: Union[torch.LongTensor, List[int]],\n                  max_len: Optional[int] = None,\n                  dtype: Optional[torch.dtype] = None,\n                  device: Optional[torch.device] = None) -> torch.ByteTensor:\n    r""""""Return a mask tensor representing the first N positions of each cell.\n\n    If ``lengths`` has shape ``[d_1, d_2, ..., d_n]`` the resulting tensor\n    ``mask`` has dtype ``dtype`` and shape ``[d_1, d_2, ..., d_n, maxlen]``,\n    with\n\n    ```\n    mask[i_1, i_2, ..., i_n, j] = (j < lengths[i_1, i_2, ..., i_n])\n    ```\n\n    Examples:\n\n    ```python\n    sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],\n                                 #  [True,  True,  True, False, False],\n                                 #  [True,  True, False, False, False]]\n\n    sequence_mask([[1, 3],[2,0]])  # [[[ True, False, False],\n                                   #   [ True,  True,  True]],\n                                   #  [[ True,  True, False],\n                                   #   [False, False, False]]]\n    ```\n\n    Args:\n        lengths: integer tensor or list of int, all its values <= max_len.\n        max_len: scalar integer tensor, size of last dimension of returned\n            tensor. Default is the maximum value in ``lengths``.\n        dtype: the desired data type of returned tensor. Default: if None,\n            returns :torch:`ByteTensor`.\n        device: the desired device of returned tensor. Default: if None, uses\n            the current device for the default tensor type.\n    Returns:\n        A mask tensor of shape :python:`lengths.shape + (max_len,)`, cast to\n        specified dtype.\n    Raises:\n        ValueError: if ``max_len`` is not a scalar.\n    """"""\n    if not isinstance(lengths, torch.Tensor):\n        lengths = torch.tensor(lengths, device=device)\n    elif device is None:\n        device = lengths.device\n    lengths: torch.LongTensor\n    if max_len is None:\n        max_len = torch.max(lengths).item()\n\n    size = lengths.size()\n    row_vector = torch.arange(max_len, device=device, dtype=lengths.dtype).view(\n        *([1] * len(size)), -1).expand(*size, max_len)\n    mask = (row_vector < lengths.unsqueeze(-1)).to(device=device)\n    if dtype is not None:\n        mask = mask.to(dtype=dtype)\n\n    return mask\n\n\ndef get_args(fn: Callable) -> List[str]:\n    r""""""Gets the arguments of a function.\n\n    Args:\n        fn (callable): The function to inspect.\n\n    Returns:\n        list: A list of argument names (``str``) of the function.\n    """"""\n    argspec = inspect.getfullargspec(fn)\n    args = argspec.args\n\n    # Empty args can be because `fn` is decorated. Use `funcsigs.signature`\n    # to re-do the inspect\n    if len(args) == 0:\n        args = funcsigs.signature(fn).parameters.keys()\n        args = list(args)\n\n    return args\n\n\ndef get_default_arg_values(fn: Callable) -> Dict[str, Any]:\n    r""""""Gets the arguments and respective default values of a function.\n\n    Only arguments with default values are included in the output dictionary.\n\n    Args:\n        fn (callable): The function to inspect.\n\n    Returns:\n        dict: A dictionary that maps argument names (``str``) to their default\n        values. The dictionary is empty if no arguments have default values.\n    """"""\n    argspec = inspect.getfullargspec(fn)\n    if argspec.defaults is None:\n        return {}\n    num_defaults = len(argspec.defaults)\n    return dict(zip(argspec.args[-num_defaults:], argspec.defaults))\n\n\ndef check_or_get_class(class_or_name: Union[Type[T], str],\n                       module_paths: Optional[List[str]] = None,\n                       superclass: Optional[MaybeTuple[type]] = None) \\\n        -> Type[T]:\n    r""""""Returns the class and checks if the class inherits :attr:`superclass`.\n\n    Args:\n        class_or_name: Name or full path to the class, or the class itself.\n        module_paths (list, optional): Paths to candidate modules to search\n            for the class. This is used if :attr:`class_or_name` is a string and\n            the class cannot be located solely based on :attr:`class_or_name`.\n            The first module in the list that contains the class\n            is used.\n        superclass (optional): A (list of) classes that the target class\n            must inherit.\n\n    Returns:\n        The target class.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_or_name` and\n            :attr:`module_paths`.\n        TypeError: If class does not inherits :attr:`superclass`.\n    """"""\n    class_ = class_or_name\n    if isinstance(class_, str):\n        class_ = get_class(class_, module_paths)\n    if superclass is not None:\n        if not issubclass(class_, superclass):\n            raise TypeError(\n                ""A subclass of {} is expected. Got: {}"".format(\n                    superclass, class_))\n    return class_\n\n\ndef get_class(class_name: str,\n              module_paths: Optional[List[str]] = None) -> type:\n    r""""""Returns the class based on class name.\n\n    Args:\n        class_name (str): Name or full path to the class.\n        module_paths (list): Paths to candidate modules to search for the\n            class. This is used if the class cannot be located solely based on\n            ``class_name``. The first module in the list that contains the class\n            is used.\n\n    Returns:\n        The target class.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_name` and\n            :attr:`module_paths`.\n    """"""\n    class_ = locate(class_name)\n    if (class_ is None) and (module_paths is not None):\n        for module_path in module_paths:\n            class_ = locate(\'.\'.join([module_path, class_name]))\n            if class_ is not None:\n                break\n\n    if class_ is None:\n        raise ValueError(\n            ""Class not found in {}: {}"".format(module_paths, class_name))\n\n    return class_  # type: ignore\n\n\ndef check_or_get_instance(ins_or_class_or_name: Union[Type[T], T, str],\n                          kwargs: Kwargs,\n                          module_paths: Optional[List[str]] = None,\n                          classtype: Optional[MaybeTuple[type]] = None) -> T:\n    r""""""Returns a class instance and checks types.\n\n    Args:\n        ins_or_class_or_name: Can be of 3 types:\n\n            - A class to instantiate.\n            - A string of the name or full path to a class to instantiate.\n            - The class instance to check types.\n\n        kwargs (dict): Keyword arguments for the class constructor. Ignored\n            if ``ins_or_class_or_name`` is a class instance.\n        module_paths (list, optional): Paths to candidate modules to\n            search for the class. This is used if the class cannot be\n            located solely based on :attr:`class_name`. The first module\n            in the list that contains the class is used.\n        classtype (optional): A (list of) class of which the instance must\n            be an instantiation.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_name` and\n            :attr:`module_paths`.\n        ValueError: If :attr:`kwargs` contains arguments that are invalid\n            for the class construction.\n        TypeError: If the instance is not an instantiation of\n            :attr:`classtype`.\n    """"""\n    ret = ins_or_class_or_name\n    if isinstance(ret, (str, type)):\n        ret = get_instance(ret, kwargs, module_paths)\n    if classtype is not None:\n        if not isinstance(ret, classtype):\n            raise TypeError(\n                ""An instance of {} is expected. Got: {}"".format(classtype, ret))\n    return ret\n\n\ndef get_instance(class_or_name: Union[Type[T], str], kwargs: Optional[Kwargs],\n                 module_paths: Optional[List[str]] = None) -> T:\n    r""""""Creates a class instance.\n\n    Args:\n        class_or_name: A class, or its name or full path to a class to\n            instantiate.\n        kwargs (dict): Keyword arguments for the class constructor.\n        module_paths (list, optional): Paths to candidate modules to\n            search for the class. This is used if the class cannot be\n            located solely based on :attr:`class_name`. The first module\n            in the list that contains the class is used.\n\n    Returns:\n        A class instance.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_or_name` and\n            :attr:`module_paths`.\n        ValueError: If :attr:`kwargs` contains arguments that are invalid\n            for the class construction.\n    """"""\n    # Locate the class\n    class_ = class_or_name\n    if isinstance(class_, str):\n        class_ = get_class(class_, module_paths)\n\n    # Check validity of arguments\n    class_args = set(get_args(class_.__init__))\n\n    if kwargs is None:\n        kwargs = {}\n    for key in kwargs.keys():\n        if key not in class_args:\n            raise ValueError(\n                ""Invalid argument for class %s.%s: %s, valid args: %s"" %\n                (class_.__module__, class_.__name__, key, list(class_args)))\n\n    return class_(**kwargs)  # type: ignore\n\n\ndef check_or_get_instance_with_redundant_kwargs(\n        ins_or_class_or_name: Union[Type[T], T, str], kwargs: Kwargs,\n        module_paths: Optional[List[str]] = None,\n        classtype: Optional[Type[T]] = None) -> T:\n    r""""""Returns a class instance and checks types.\n\n    Only those keyword arguments in :attr:`kwargs` that are included in the\n    class construction method are used.\n\n    Args:\n        ins_or_class_or_name: Can be of 3 types:\n\n            - A class to instantiate.\n            - A string of the name or module path to a class to instantiate.\n            - The class instance to check types.\n\n        kwargs (dict): Keyword arguments for the class constructor.\n        module_paths (list, optional): Paths to candidate modules to\n            search for the class. This is used if the class cannot be\n            located solely based on :attr:`class_name`. The first module\n            in the list that contains the class is used.\n        classtype (optional): A (list of) classes of which the instance must\n            be an instantiation.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_name` and\n            :attr:`module_paths`.\n        ValueError: If :attr:`kwargs` contains arguments that are invalid\n            for the class construction.\n        TypeError: If the instance is not an instantiation of\n            :attr:`classtype`.\n    """"""\n    ret = ins_or_class_or_name\n    if isinstance(ret, (str, type)):\n        ret = get_instance_with_redundant_kwargs(ret, kwargs, module_paths)\n    if classtype is not None:\n        if not isinstance(ret, classtype):\n            raise TypeError(\n                ""An instance of {} is expected. Got: {}"".format(classtype, ret))\n    return ret\n\n\ndef get_instance_with_redundant_kwargs(\n        class_name: Union[Type[T], str], kwargs: Kwargs,\n        module_paths: Optional[List[str]] = None) -> T:\n    r""""""Creates a class instance.\n\n    Only those keyword arguments in :attr:`kwargs` that are included in the\n    class construction method are used.\n\n    Args:\n        class_name (str): A class or its name or module path.\n        kwargs (dict): A dictionary of arguments for the class constructor. It\n            may include invalid arguments which will be ignored.\n        module_paths (list of str): A list of paths to candidate modules to\n            search for the class. This is used if the class cannot be located\n            solely based on :attr:`class_name`. The first module in the list\n            that contains the class is used.\n\n    Returns:\n        A class instance.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_name` and\n            :attr:`module_paths`.\n    """"""\n    # Locate the class\n    if isinstance(class_name, str):\n        class_ = get_class(class_name, module_paths)\n    else:\n        class_ = class_name\n\n    # Select valid arguments\n    selected_kwargs = {}\n    class_args = set(get_args(class_.__init__))  # type: ignore\n    if kwargs is None:\n        kwargs = {}\n    for key, value in kwargs.items():\n        if key in class_args:\n            selected_kwargs[key] = value\n\n    return class_(**selected_kwargs)\n\n\ndef get_function(fn_or_name: Union[str, Callable[[torch.Tensor], torch.Tensor]],\n                 module_paths: Optional[List[str]] = None) \\\n        -> Callable[[torch.Tensor], torch.Tensor]:\n    r""""""Returns the function of specified name and module.\n\n    Args:\n        fn_or_name (str or callable): Name or full path to a function, or the\n            function itself.\n        module_paths (list, optional): A list of paths to candidate modules to\n            search for the function. This is used only when the function\n            cannot be located solely based on :attr:`fn_or_name`. The first\n            module in the list that contains the function is used.\n\n    Returns:\n        A function.\n\n    Raises:\n        ValueError: If method with name as :attr:`fn_or_name` is not found.\n    """"""\n    if callable(fn_or_name):\n        return fn_or_name\n\n    fn = locate(fn_or_name)\n    if (fn is None) and (module_paths is not None):\n        for module_path in module_paths:\n            fn = locate(\'.\'.join([module_path, fn_or_name]))\n            if fn is not None:\n                break\n\n    if fn is None:\n        raise ValueError(\n            ""Method not found in {}: {}"".format(module_paths, fn_or_name))\n\n    return fn  # type: ignore\n\n\ndef call_function_with_redundant_kwargs(fn: Callable[..., R],\n                                        kwargs: Dict[str, Any]) -> R:\n    r""""""Calls a function and returns the results.\n\n    Only those keyword arguments in :attr:`kwargs` that are included in the\n    function\'s argument list are used to call the function.\n\n    Args:\n        fn (function): A callable. If :attr:`fn` is not a python function,\n            :attr:`fn.__call__` is called.\n        kwargs (dict): A ``dict`` of arguments for the callable. It\n            may include invalid arguments which will be ignored.\n\n    Returns:\n        The returned results by calling :attr:`fn`.\n    """"""\n    try:\n        fn_args = set(get_args(fn))\n    except TypeError:\n        fn_args = set(get_args(fn.__call__))  # type: ignore\n\n    if kwargs is None:\n        kwargs = {}\n\n    # Select valid arguments\n    selected_kwargs = {}\n    for key, value in kwargs.items():\n        if key in fn_args:\n            selected_kwargs[key] = value\n\n    return fn(**selected_kwargs)\n\n\ndef get_instance_kwargs(kwargs: Kwargs, hparams: ParamDict) -> Kwargs:\n    r""""""Makes a dictionary of keyword arguments with the following structure:\n\n    ``kwargs_ = {\'hparams\': dict(hparams), **kwargs}``.\n\n    This is typically used for constructing a module which takes a set of\n    arguments as well as a argument named ``""hparams""``.\n\n    Args:\n        kwargs (dict): A ``dict`` of keyword arguments. Can be `None`.\n        hparams: A ``dict`` or an instance of :class:`~texar.torch.HParams`.\n            Can be `None`.\n\n    Returns:\n        A ``dict`` that contains the keyword arguments in :attr:`kwargs`, and\n        an additional keyword argument named ``""hparams""``.\n    """"""\n    if hparams is None or isinstance(hparams, dict):\n        kwargs_ = {\'hparams\': hparams}\n    elif isinstance(hparams, HParams):\n        kwargs_ = {\'hparams\': hparams.todict()}\n    else:\n        raise ValueError(\n            \'`hparams` must be a dict, an instance of HParams, or a `None`.\')\n    kwargs_.update(kwargs or {})\n    return kwargs_\n\n\ndef get_output_size(input_instance: nn.Module) -> Optional[int]:\n    r""""""Return the final dimension size of :attr:`input_instance` output.\n\n    If type of :attr:`input_instance` is among the common types, the final\n    dimension size will be computed.\n\n    Args:\n        input_instance: A :class:`~torch.nn.Module` instance from\n            which to compute the final dimension size.\n\n    Returns:\n        int (optional): The final dimension size of the output.\n            If output size is determined by input, returns ``-1``,\n            otherwise if output size is not computable, return `None`.\n    """"""\n\n    for t, l in Type_size_lambda_map.items():\n        if isinstance(input_instance, t):\n            return l(input_instance)\n    for t in Type_size_keeper:\n        if isinstance(input_instance, t):\n            return -1\n    return None\n\n\ndef dict_patch(tgt_dict: AnyDict, src_dict: AnyDict) -> AnyDict:\n    r""""""Recursively patch :attr:`tgt_dict` by adding items from :attr:`src_dict`\n    that do not exist in :attr:`tgt_dict`.\n\n    If respective items in :attr:`src_dict` and :attr:`tgt_dict` are both\n    ``dict``, the :attr:`tgt_dict` item is patched recursively.\n\n    Args:\n        tgt_dict (dict): Target dictionary to patch.\n        src_dict (dict): Source dictionary.\n\n    Returns:\n        dict: The new :attr:`tgt_dict` that is patched.\n    """"""\n    if src_dict is None:\n        return tgt_dict\n\n    for key, value in src_dict.items():\n        if key not in tgt_dict:\n            tgt_dict[key] = copy.deepcopy(value)\n        elif isinstance(value, dict) and isinstance(tgt_dict[key], dict):\n            tgt_dict[key] = dict_patch(tgt_dict[key], value)\n    return tgt_dict\n\n\ndef dict_lookup(dict_: MutableMapping[K, V], keys: Union[List[K], np.ndarray],\n                default: Optional[V] = None) -> np.ndarray:\n    r""""""Looks up :attr:`keys` in the dictionary, returns the corresponding\n    values.\n\n    The :attr:`default` is used for keys not present in the dictionary.\n\n    Args:\n        dict\\_ (dict): A dictionary for lookup.\n        keys: A numpy array or a (possibly nested) list of keys.\n        default (optional): Value to be returned when a key is not in\n            :attr:`dict_`. Error is raised if :attr:`default` is not given and\n            key is not in the dictionary.\n\n    Returns:\n        A numpy array of values with the same structure as :attr:`keys`.\n\n    Raises:\n        TypeError: If key is not in :attr:`dict_` and :attr:`default` is\n            `None`.\n    """"""\n    return np.vectorize(lambda x: dict_.get(x, default))(keys)  # type: ignore\n\n\n# TODO: Remove these once pylint supports function stubs.\n# pylint: disable=unused-argument,function-redefined,multiple-statements\n\n@overload\ndef dict_fetch(src_dict: ParamDict,\n               tgt_dict_or_keys: Union[ParamDict, List[str]]) -> AnyDict: ...\n\n\n@overload\ndef dict_fetch(src_dict: None, tgt_dict_or_keys: Any) -> None: ...\n\n\ndef dict_fetch(src_dict: Optional[ParamDict],\n               tgt_dict_or_keys: Union[ParamDict, List[str]]) \\\n        -> Optional[AnyDict]:\n    r""""""Fetches a sub-dictionary of :attr:`src_dict` with the keys in\n    :attr:`tgt_dict_or_keys`.\n\n    Args:\n        src_dict: A dictionary or instance of :class:`~texar.torch.HParams`.\n            The source dictionary to fetch values from.\n        tgt_dict_or_keys: A dictionary, instance of\n            :class:`~texar.torch.HParams`, or a list (or a\n            ``dict_keys``/``KeysView``) of keys to be included in the output\n            dictionary.\n\n    Returns:\n        A new dictionary that is a sub-dictionary of :attr:`src_dict`.\n    """"""\n    if src_dict is None:\n        return src_dict\n\n    if isinstance(tgt_dict_or_keys, HParams):\n        tgt_dict_or_keys = tgt_dict_or_keys.todict()\n    if isinstance(tgt_dict_or_keys, MutableMapping):\n        tgt_dict_or_keys = tgt_dict_or_keys.keys()  # type: ignore\n    keys = list(tgt_dict_or_keys)\n\n    if isinstance(src_dict, HParams):\n        src_dict = src_dict.todict()\n\n    return {k: src_dict[k] for k in keys if k in src_dict}\n\n\n# pylint: enable=unused-argument,function-redefined,multiple-statements\n\n\ndef dict_pop(dict_: MutableMapping[T, Any], pop_keys: MaybeSeq[T],\n             default: Optional[Any] = None) -> Dict[T, Any]:\n    r""""""Removes keys from a dictionary and returns their values.\n\n    Args:\n        dict\\_ (dict): A dictionary from which items are removed.\n        pop_keys: A key or a list of keys to remove and return respective\n            values or :attr:`default`.\n        default (optional): Value to be returned when a key is not in\n            :attr:`dict_`. The default value is `None`.\n\n    Returns:\n        A ``dict`` of the items removed from :attr:`dict_`.\n    """"""\n    if not isinstance(pop_keys, (list, tuple)):\n        pop_keys = cast(List[T], [pop_keys])\n    ret_dict = {key: dict_.pop(key, default) for key in pop_keys}\n    return ret_dict\n\n\ndef flatten_dict(dict_: AnyDict, parent_key: str = """", sep: str = "".""):\n    r""""""Flattens a nested dictionary. Namedtuples within the dictionary are\n    also converted to dictionaries.\n\n    Adapted from:\n    https://github.com/google/seq2seq/blob/master/seq2seq/models/model_base.py\n\n    Args:\n        dict\\_ (dict): The dictionary to flatten.\n        parent_key (str): A prefix to prepend to each key.\n        sep (str): Separator that intervenes between parent and child keys.\n            For example, if :attr:`sep` == ``"".""``, then ``{ ""a"": { ""b"": 3 } }``\n            is converted into ``{ ""a.b"": 3 }``.\n\n    Returns:\n        A new flattened ``dict``.\n    """"""\n    items: List[Tuple[str, Any]] = []\n    for key, value in dict_.items():\n        key_ = parent_key + sep + key if parent_key else key\n        if isinstance(value, MutableMapping):\n            items.extend(flatten_dict(value, key_, sep=sep).items())\n        elif isinstance(value, tuple) and hasattr(value, \'_asdict\'):\n            # namedtuple\n            dict_items = collections.OrderedDict(\n                zip(value._fields, value))  # type: ignore\n            items.extend(flatten_dict(dict_items, key_, sep=sep).items())\n        else:\n            items.append((key_, value))\n    return dict(items)\n\n\ndef default_str(str_: Optional[str], default: str) -> str:\n    r""""""Returns :attr:`str_` if it is not `None` or empty, otherwise returns\n    :attr:`default_str`.\n\n    Args:\n        str\\_: A string.\n        default: A string.\n\n    Returns:\n        Either :attr:`str_` or :attr:`default_str`.\n    """"""\n    if str_ is not None and str_ != """":\n        return str_\n    else:\n        return default\n\n\ndef uniquify_str(str_: str, str_set: Collection[str]) -> str:\n    r""""""Uniquifies :attr:`str_` if :attr:`str_` is included in :attr:`str_set`.\n\n    This is done by appending a number to :attr:`str_`. Returns\n    :attr:`str_` directly if it is not included in :attr:`str_set`.\n\n    Args:\n        str\\_ (string): A string to uniquify.\n        str_set (set, dict, or list): A collection of strings. The returned\n            string is guaranteed to be different from the elements in the\n            collection.\n\n    Returns:\n        The uniquified string. Returns :attr:`str_` directly if it is\n        already unique.\n\n    Example:\n\n        .. code-block:: python\n\n            print(uniquify_str(\'name\', [\'name\', \'name_1\']))\n            # \'name_2\'\n\n    """"""\n    if str_ not in str_set:\n        return str_\n    else:\n        for i in range(1, len(str_set) + 1):\n            unique_str = str_ + ""_%d"" % i\n            if unique_str not in str_set:\n                return unique_str\n    raise ValueError(""Failed to uniquify string: "" + str_)\n\n\ndef _recur_split(s: MaybeSeq[str],\n                 dtype_as: Collection[str]) -> MaybeSeq[str]:\n    r""""""Splits (possibly nested list of) strings recursively.\n    """"""\n    if isinstance(s, str):\n        return _maybe_list_to_array(s.split(), dtype_as)\n    else:\n        s_ = [_recur_split(si, dtype_as) for si in s]\n        return _maybe_list_to_array(s_, s)\n\n\ndef strip_token(str_: MaybeSeq[str], token: str,\n                is_token_list: bool = False) -> MaybeSeq[str]:\n    r""""""Returns a copy of strings with leading and trailing tokens removed.\n\n    Note that besides :attr:`token`, all leading and trailing whitespace\n    characters are also removed.\n\n    If :attr:`is_token_list` is False, then the function assumes tokens in\n    :attr:`str_` are separated with whitespace character.\n\n    Args:\n        str\\_: A ``str``, or an ``n``-D numpy array or (possibly nested)\n            list of ``str``.\n        token (str): The token to strip, e.g., the ``""<PAD>""`` token defined in\n            :class:`~texar.torch.data.SpecialTokens`.\n        is_token_list (bool): Whether each sentence in :attr:`str_` is a list\n            of tokens. If False, each sentence in :attr:`str_` is assumed to\n            contain tokens separated with space character.\n\n    Returns:\n        The stripped strings of the same structure/shape as :attr:`str_`.\n\n    Example:\n\n        .. code-block:: python\n\n            str_ = \'<PAD> a sentence <PAD> <PAD>  \'\n            str_stripped = strip_token(str_, \'<PAD>\')\n            # str_stripped == \'a sentence\'\n\n            str_ = [\'<PAD>\', \'a\', \'sentence\', \'<PAD>\', \'<PAD>\', \'\', \'\']\n            str_stripped = strip_token(str_, \'<PAD>\', is_token_list=True)\n            # str_stripped == \'a sentence\'\n    """"""\n\n    def _recur_strip(s):\n        if isinstance(s, str):\n            if token == """":\n                return \' \'.join(s.strip().split())\n            else:\n                return (\' \'.join(s.strip().split())\n                        .replace(\' \' + token, \'\').replace(token + \' \', \'\'))\n        else:\n            s_ = [_recur_strip(si) for si in s]\n            return _maybe_list_to_array(s_, s)\n\n    s = str_\n\n    if is_token_list:\n        s = str_join(s)  # type: ignore\n\n    strp_str = _recur_strip(s)\n\n    if is_token_list:\n        strp_str = _recur_split(strp_str, str_)\n\n    return strp_str\n\n\ndef strip_eos(str_: MaybeSeq[str], eos_token: str = \'<EOS>\',\n              is_token_list: bool = False) -> MaybeSeq[str]:\n    r""""""Remove the EOS token and all subsequent tokens.\n\n    If :attr:`is_token_list` is False, then the function assumes tokens in\n    :attr:`str_` are separated with whitespace character.\n\n    Args:\n        str\\_: A ``str``, or an ``n``-D numpy array or (possibly nested)\n            list of ``str``.\n        eos_token (str): The EOS token. Default is ``""<EOS>""`` as defined in\n            :class:`~texar.torch.data.SpecialTokens`.EOS\n        is_token_list (bool): Whether each sentence in :attr:`str_` is a list\n            of tokens. If False, each sentence in :attr:`str_` is assumed to\n            contain tokens separated with space character.\n\n    Returns:\n        Strings of the same structure/shape as :attr:`str_`.\n    """"""\n\n    def _recur_strip(s):\n        if isinstance(s, str):\n            s_tokens = s.split()\n            if eos_token in s_tokens:\n                return \' \'.join(s_tokens[:s_tokens.index(eos_token)])\n            else:\n                return s\n        else:\n            s_ = [_recur_strip(si) for si in s]\n            return _maybe_list_to_array(s_, s)\n\n    s = str_\n\n    if is_token_list:\n        s = str_join(s)  # type: ignore\n\n    strp_str = _recur_strip(s)\n\n    if is_token_list:\n        strp_str = _recur_split(strp_str, str_)\n\n    return strp_str\n\n\n_strip_eos_ = strip_eos\n\n\ndef strip_bos(str_: MaybeSeq[str], bos_token: str = \'<BOS>\',\n              is_token_list: bool = False) -> MaybeSeq[str]:\n    r""""""Remove all leading BOS tokens.\n\n    Note that besides :attr:`bos_token`, all leading and trailing whitespace\n    characters are also removed.\n\n    If :attr:`is_token_list` is False, then the function assumes tokens in\n    :attr:`str_` are separated with whitespace character.\n\n    Args:\n        str_: A ``str``, or an ``n``-D numpy array or (possibly nested)\n            list of ``str``.\n        bos_token (str): The BOS token. Default is ``""<BOS>""`` as defined in\n            :class:`~texar.torch.data.SpecialTokens`.BOS\n        is_token_list (bool): Whether each sentence in :attr:`str_` is a list\n            of tokens. If False, each sentence in :attr:`str_` is assumed to\n            contain tokens separated with space character.\n\n    Returns:\n        Strings of the same structure/shape as :attr:`str_`.\n    """"""\n\n    def _recur_strip(s):\n        if isinstance(s, str):\n            if bos_token == \'\':\n                return \' \'.join(s.strip().split())\n            else:\n                return \' \'.join(s.strip().split()).replace(bos_token + \' \', \'\')\n        else:\n            s_ = [_recur_strip(si) for si in s]\n            return _maybe_list_to_array(s_, s)\n\n    s = str_\n\n    if is_token_list:\n        s = str_join(s)  # type: ignore\n\n    strp_str = _recur_strip(s)\n\n    if is_token_list:\n        strp_str = _recur_split(strp_str, str_)\n\n    return strp_str\n\n\n_strip_bos_ = strip_bos\n\n\n# pylint: disable=redefined-outer-name\n\ndef strip_special_tokens(str_: MaybeSeq[str],\n                         strip_pad: Optional[str] = \'<PAD>\',\n                         strip_bos: Optional[str] = \'<BOS>\',\n                         strip_eos: Optional[str] = \'<EOS>\',\n                         is_token_list: bool = False) -> MaybeSeq[str]:\n    r""""""Removes special tokens in strings, including:\n\n        - Removes EOS and all subsequent tokens\n        - Removes leading and and trailing PAD tokens\n        - Removes leading BOS tokens\n\n    Note that besides the special tokens, all leading and trailing whitespace\n    characters are also removed.\n\n    This is a joint function of :func:`strip_eos`, :func:`strip_pad`, and\n    :func:`strip_bos`\n\n    Args:\n        str\\_: A ``str``, or an ``n``-D numpy array or (possibly nested)\n            list of ``str``.\n        strip_pad (str): The PAD token to strip from the strings (i.e., remove\n            the leading and trailing PAD tokens of the strings). Default\n            is ``""<PAD>""`` as defined in\n            :class:`~texar.torch.data.SpecialTokens`.PAD.\n            Set to `None` or `False` to disable the stripping.\n        strip_bos (str): The BOS token to strip from the strings (i.e., remove\n            the leading BOS tokens of the strings).\n            Default is ``""<BOS>""`` as defined in\n            :class:`~texar.torch.data.SpecialTokens`.BOS.\n            Set to `None` or `False` to disable the stripping.\n        strip_eos (str): The EOS token to strip from the strings (i.e., remove\n            the EOS tokens and all subsequent tokens of the strings).\n            Default is ``""<EOS>""`` as defined in\n            :class:`~texar.torch.data.SpecialTokens`.EOS.\n            Set to `None` or `False` to disable the stripping.\n        is_token_list (bool): Whether each sentence in :attr:`str_` is a list\n            of tokens. If `False`, each sentence in :attr:`str_` is assumed to\n            contain tokens separated with space character.\n\n    Returns:\n        Strings of the same shape of :attr:`str_` with special tokens stripped.\n    """"""\n    s = str_\n\n    if is_token_list:\n        s = str_join(s)  # type: ignore\n\n    if strip_eos is not None and strip_eos is not False:\n        s = _strip_eos_(s, strip_eos, is_token_list=False)\n\n    if strip_pad is not None and strip_pad is not False:\n        s = strip_token(s, strip_pad, is_token_list=False)\n\n    if strip_bos is not None and strip_bos is not False:\n        s = _strip_bos_(s, strip_bos, is_token_list=False)\n\n    if is_token_list:\n        s = _recur_split(s, str_)\n\n    return s\n\n\ndef str_join(tokens: Sequence[List], sep: str = \' \') -> Sequence[str]:\n    r""""""Concatenates :attr:`tokens` along the last dimension with intervening\n    occurrences of :attr:`sep`.\n\n    Args:\n        tokens: An ``n``-D numpy array or (possibly nested) list of ``str``.\n        sep (str): The string intervening between the tokens.\n\n    Returns:\n        An ``(n-1)``-D numpy array (or list) of ``str``.\n    """"""\n\n    def _recur_join(s):\n        if len(s) == 0:\n            return \'\'\n        elif isinstance(s[0], str):\n            return sep.join(s)\n        else:\n            s_ = [_recur_join(si) for si in s]\n            return _maybe_list_to_array(s_, s)\n\n    str_ = _recur_join(tokens)\n\n    return str_\n\n\n# pylint: enable=redefined-outer-name\n\ndef ceildiv(a: int, b: int) -> int:\n    r""""""Compute division with results rounding up.\n\n    For example, ``5 / 2 = 2.5``, ``ceildiv(5, 2) = 3``.\n\n    Args:\n        a (int): The dividend.\n        b (int): The divisor.\n\n    Returns:\n        int: The quotient, rounded up.\n    """"""\n    return -(-a // b)\n\n\ndef sum_tensors(xs: List[Optional[torch.Tensor]]) -> Optional[torch.Tensor]:\n    r""""""Sum a list of tensors with possible `None` values.\n\n    Args:\n        xs: A list of tensors.\n\n    Returns:\n        The summation of all the elements in the list.\n    """"""\n    idx = next((idx for idx, tensor in enumerate(xs) if tensor is not None), -1)\n    if idx == -1:\n        return None\n    ret = xs[idx]\n    for tensor in xs[(idx + 1):]:\n        if tensor is not None:\n            ret = ret + tensor\n    return ret\n\n\ndef truncate_seq_pair(tokens_a: Union[List[int], List[str]],\n                      tokens_b: Union[List[int], List[str]],\n                      max_length: int):\n    r""""""Truncates a sequence pair in place to the maximum length.\n\n    This is a simple heuristic which will always truncate the longer sequence\n    one token at a time. This makes more sense than truncating an equal\n    percent of tokens from each, since if one sequence is very short then\n    each token that\'s truncated likely contains more information than a\n    longer sequence.\n\n    Example:\n        tokens_a = [1, 2, 3, 4, 5]\n        tokens_b = [6, 7]\n        truncate_seq_pair(tokens_a, tokens_b, 5)\n        tokens_a  # [1, 2, 3]\n        tokens_b  # [6, 7]\n\n    Args:\n        tokens_a: A list of tokens or token ids.\n        tokens_b: A list of tokens or token ids.\n        max_length: maximum sequence length.\n    """"""\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n'"
texar/torch/utils/utils_io.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtility functions related to input/output.\n""""""\nimport os\n\n__all__ = [\n    ""write_paired_text"",\n    ""maybe_create_dir"",\n]\n\n\ndef write_paired_text(src, tgt, fname, append=False, mode=\'h\', sep=\'\\t\',\n                      src_fname_suffix=\'src\', tgt_fname_suffix=\'tgt\'):\n    r""""""Writes paired text to a file.\n\n    Args:\n        src: A list (or array) of ``str`` source text.\n        tgt: A list (or array) of ``str`` target text.\n        fname (str): The output filename.\n        append (bool): Whether append content to the end of the file if exists.\n        mode (str): The mode of writing, with the following options:\n\n            - **\'h\'**: The ""horizontal"" mode. Each source target pair is\n              written in one line, intervened with :attr:`sep`, e.g.::\n\n                  source_1 target_1\n                  source_2 target_2\n\n            - **\'v\'**: The ``""vertical""`` mode. Each source target pair is\n              written in two consecutive lines, e.g::\n\n                  source_1\n                  target_1\n                  source_2\n                  target_2\n\n            - **\'s\'**: The ""separate"" mode. Each source target pair is\n              written in corresponding lines of two files named\n              as ``""{fname}.{src_fname_suffix}""``\n              and ``""{fname}.{tgt_fname_suffix}""``, respectively.\n\n        sep (str): The string intervening between source and target. Used\n            when :attr:`mode` is set to ``""h""``.\n        src_fname_suffix (str): Used when :attr:`mode` is ``""s""``. The suffix\n            to the source output filename. For example, with\n            ``(fname=\'output\', src_fname_suffix=\'src\')``, the output source\n            file is named as ``output.src``.\n        tgt_fname_suffix (str): Used when :attr:`mode` is ``""s""``. The suffix\n            to the target output filename.\n\n    Returns:\n        The filename(s). If ``mode`` == ``""h""`` or ``""v""``, returns\n        :attr:`fname`. If ``mode`` == ``""s""``, returns a list of filenames\n        ``[""{fname}.src"", ""{fname}.tgt""]``.\n    """"""\n    fmode = \'a\' if append else \'w\'\n    if mode == \'s\':\n        fn_src = \'{}.{}\'.format(fname, src_fname_suffix)\n        fn_tgt = \'{}.{}\'.format(fname, tgt_fname_suffix)\n        with open(fn_src, fmode, encoding=\'utf-8\') as fs:\n            fs.write(\'\\n\'.join(src))\n            fs.write(\'\\n\')\n        with open(fn_tgt, fmode, encoding=\'utf-8\') as ft:\n            ft.write(\'\\n\'.join(tgt))\n            ft.write(\'\\n\')\n        return fn_src, fn_tgt\n    else:\n        with open(fname, fmode, encoding=\'utf-8\') as f:\n            for s, t in zip(src, tgt):\n                if mode == \'h\':\n                    text = \'{}{}{}\\n\'.format(s, sep, t)\n                    f.write(text)\n                elif mode == \'v\':\n                    text = \'{}\\n{}\\n\'.format(s, t)\n                    f.write(text)\n                else:\n                    raise ValueError(\'Unknown mode: {}\'.format(mode))\n        return fname\n\n\ndef maybe_create_dir(dirname: str) -> bool:\n    r""""""Creates directory if it does not exist.\n\n    Args:\n        dirname (str): Path to the directory.\n\n    Returns:\n        bool: Whether a new directory is created.\n    """"""\n    if not os.path.isdir(dirname):\n        os.makedirs(dirname)\n        return True\n    return False\n'"
texar/torch/data/data/__init__.py,10,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library data inputs.\n""""""\n\nfrom texar.torch.data.data.data_base import *\nfrom texar.torch.data.data.data_iterators import *\nfrom texar.torch.data.data.dataset_utils import *\nfrom texar.torch.data.data.mono_text_data import *\nfrom texar.torch.data.data.multi_aligned_data import *\nfrom texar.torch.data.data.paired_text_data import *\nfrom texar.torch.data.data.record_data import *\nfrom texar.torch.data.data.sampler import *\nfrom texar.torch.data.data.scalar_data import *\nfrom texar.torch.data.data.text_data_base import *\n'"
texar/torch/data/data/data_base.py,23,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase data class that is inherited by all data classes.\nA data defines data reading, parsing, batching, and other\npreprocessing operations.\n""""""\nimport warnings\nfrom abc import ABC\nfrom typing import (\n    Callable, Dict, Generic, Iterable, Iterator, List, Optional, Sequence,\n    Tuple, TypeVar, Union)\n\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom texar.torch.data.data.dataset_utils import Batch\nfrom texar.torch.data.data.dataset_utils import _CacheStrategy, _LazyStrategy\nfrom texar.torch.hyperparams import HParams\n\n__all__ = [\n    ""DataSource"",\n    ""SequenceDataSource"",\n    ""IterDataSource"",\n    ""ZipDataSource"",\n    ""FilterDataSource"",\n    ""RecordDataSource"",\n    ""DatasetBase"",\n]\n\nRawExample = TypeVar(\'RawExample\')  # type of a raw example loaded from source\nExample = TypeVar(\'Example\')  # type of a data example\n\n\nclass DataSource(Generic[RawExample], ABC):\n    r""""""Base class for all data sources. A data source represents the *source*\n    of the data, from which raw data examples are read and returned.\n\n    Different to PyTorch :class:`~torch.utils.data.Dataset`, subclasses of this\n    class are not required to implement :meth:`__getitem__` (default\n    implementation raises `TypeError`), which is beneficial for certain sources\n    that only supports iteration (reading from text files, reading Python\n    iterators, etc.)\n    """"""\n\n    def __getitem__(self, index: int) -> RawExample:\n        raise TypeError(""This DataSource does not support random access"")\n\n    def __iter__(self) -> Iterator[RawExample]:\n        raise NotImplementedError\n\n    def __len__(self) -> int:\n        raise TypeError(""This DataSource does not support random access"")\n\n\nclass SequenceDataSource(DataSource[RawExample]):\n    r""""""Data source for reading from Python sequences.\n\n    This data source supports indexing.\n\n    Args:\n        sequence: The Python sequence to read from. Note that a sequence should\n            be iterable and supports `len`.\n    """"""\n\n    def __init__(self, sequence: Sequence[RawExample]):\n        self._seq = sequence\n\n    def __getitem__(self, index: int) -> RawExample:\n        return self._seq[index]\n\n    def __iter__(self) -> Iterator[RawExample]:\n        return iter(self._seq)\n\n    def __len__(self) -> int:\n        return len(self._seq)\n\n\nclass IterDataSource(DataSource[RawExample]):\n    r""""""Data source for reading from Python iterables. Please note: if passed\n    an *iterator* and caching strategy is set to \'none\', then the data source\n    can only be iterated over once.\n\n    This data source does not support indexing.\n\n    Args:\n        iterable: The Python iterable to read from.\n    """"""\n\n    def __init__(self, iterable: Iterable[RawExample]):\n        self._iter = iterable\n\n    def __iter__(self) -> Iterator[RawExample]:\n        return iter(self._iter)\n\n\nclass ZipDataSource(DataSource[Tuple[RawExample, ...]]):\n    r""""""Data source by combining multiple sources. The raw examples returned\n    from this data source are tuples, with elements being raw examples from each\n    of the constituting data sources.\n\n    This data source supports indexing if all the constituting data sources\n    support indexing.\n\n    Args:\n        sources: The list of data sources to combine.\n    """"""\n\n    def __init__(self, *sources: DataSource[RawExample]):\n        self._sources = list(sources)\n\n    def __getitem__(self, index: int) -> Tuple[RawExample, ...]:\n        return tuple(source[index] for source in self._sources)\n\n    def __iter__(self) -> Iterator[Tuple[RawExample, ...]]:\n        return zip(*[iter(source) for source in self._sources])\n\n    def __len__(self) -> int:\n        return min(len(source) for source in self._sources)\n\n\nclass FilterDataSource(DataSource[RawExample]):\n    r""""""Data source for filtering raw examples with a user-specified filter\n    function. Only examples for which the filter functions returns `True` are\n    returned.\n\n    This data source supports indexing if the wrapped data source supports\n    indexing.\n\n    Args:\n        source: The data source to filter.\n        filter_fn: A callable taking a raw example as argument and returning a\n            boolean value, indicating whether the raw example should be\n            **kept**.\n    """"""\n\n    def __init__(self, source: DataSource[RawExample],\n                 filter_fn: Callable[[RawExample], bool]):\n        self._source = source\n        self._filter_fn = filter_fn\n\n    def __iter__(self) -> Iterator[RawExample]:\n        for sentence in self._source:\n            if self._filter_fn(sentence):\n                yield sentence\n\n\nclass RecordDataSource(DataSource[Dict[str, RawExample]]):\n    r""""""Data source by structuring multiple sources. The raw examples returned\n    from this data source are dictionaries, with values being raw examples from\n    each of the constituting data sources.\n\n    This data source supports indexing if all the constituting data sources\n    support indexing.\n\n    Args:\n        sources: A dictionary mapping names to data sources, containing the\n            data sources to combine.\n    """"""\n\n    def __init__(self, sources: Dict[str, DataSource[RawExample]]):\n        self._sources = sources\n\n    def __getitem__(self, index: int) -> Dict[str, RawExample]:\n        return {key: source[index] for key, source in self._sources.items()}\n\n    def __iter__(self) -> Iterator[Dict[str, RawExample]]:\n        keys = list(self._sources.keys())\n        iterator = zip(*[iter(source) for source in self._sources.values()])\n        for values in iterator:\n            yield dict(zip(keys, values))\n\n    def __len__(self) -> int:\n        return min(len(source) for source in self._sources.values())\n\n\nclass _TruncatedDataSource(DataSource[RawExample]):\n    def __init__(self, data_source: DataSource[RawExample], max_size: int):\n        self._source = data_source\n        self._max_size = max_size\n\n    def __getitem__(self, item) -> RawExample:\n        if item >= self._max_size:\n            raise IndexError(\n                f""Data index ({item}) out of range [0, {self._max_size})"")\n        return self._source[item]\n\n    def __iter__(self) -> Iterator[RawExample]:\n        count = 0\n        iterator = iter(self._source)\n        while count < self._max_size:\n            yield next(iterator)\n            count += 1\n\n    def __len__(self) -> int:\n        try:\n            length = min(len(self._source), self._max_size)\n        except TypeError:\n            length = self._max_size\n        return length\n\n\nclass _TransformedDataSource(DataSource[Example], Generic[RawExample, Example]):\n    r""""""Data source by performing transformations on another data source.\n    """"""\n\n    def __init__(self, data_source: DataSource[RawExample],\n                 process_fn: Callable[[RawExample], Example]):\n        self._source = data_source\n        self._process = process_fn\n\n    def __getitem__(self, item):\n        return self._process(self._source[item])\n\n    def __iter__(self):\n        return map(self._process, iter(self._source))\n\n    def __len__(self):\n        return len(self._source)\n\n    def __getattr__(self, item):\n        return getattr(self._source, item)\n\n\nclass _CachedDataSource(DataSource[RawExample]):\n    r""""""Wrapper for random access support over a data source that does not\n    implement `__getitem__`. This class is only used internally in\n    :class:`~texar.torch.data.data.DatasetBase`, while conforming to user\n    `cache_strategy` and `shuffle_buffer_size` settings.\n    """"""\n\n    _cache: Union[Dict[int, RawExample], List[RawExample]]\n\n    def __init__(self, data_source: DataSource[RawExample],\n                 erase_after_access: bool = True):\n        r""""""\n\n        Args:\n            data_source: The data source to wrap around.\n            erase_after_access: If `True`, cached examples are erased after\n                being accessed through `__getitem__`. Useful when\n                :class:`~texar.torch.data.data.DatasetBase` hyperparameter\n                `cache_strategy` is set to `none` or `processed`.\n        """"""\n        self._source = data_source\n        self._iter = iter(data_source)\n        self._max_index = -1\n        self._erase_after_access = erase_after_access\n        if erase_after_access:\n            self._cache: Dict[int, RawExample] = {}\n        else:\n            self._cache: List[RawExample] = []\n\n    def __getitem__(self, index: int) -> RawExample:\n        # If specified `index` is not yet prefetched (or has already been\n        # accessed), this method may throw `IndexError` or `KeyError`.\n        example = self._cache[index]\n        if self._erase_after_access:\n            del self._cache[index]\n        return example\n\n    def __iter__(self) -> Iterator[RawExample]:\n        return iter(self._source)\n\n    def prefetch(self, index: int):\n        while self._max_index < index:\n            example = next(self._iter)\n            self._max_index += 1\n            if self._erase_after_access:\n                self._cache[self._max_index] = example\n            else:\n                self._cache.append(example)  # type: ignore\n\n    @property\n    def max_index(self) -> int:\n        return self._max_index\n\n    def reset(self) -> None:\n        if self._erase_after_access:\n            self._iter = iter(self._source)\n            self._max_index = -1\n\n\nclass DatasetBase(Dataset, Generic[RawExample, Example], ABC):\n    r""""""Base class inherited by all data classes.\n\n    Args:\n        source: An instance of type :class:`~texar.torch.data.DataSource`,\n        hparams: A `dict` or instance of :class:`~texar.torch.HParams`\n            containing hyperparameters. See :meth:`default_hparams` for the\n            defaults.\n        device: The device of the produced batches. For GPU training, set to\n            current CUDA device.\n\n            .. note::\n                When :attr:`device` is set to a CUDA device, tensors in the\n                batch will be automatically moved to the specified device. This\n                may result in performance issues if your data examples contain\n                complex structures (e.g., nested lists with many elements). In\n                this case, it is recommended to set :attr:`device` to `None` and\n                manually move your data.\n\n                For more details, see :meth:`collate`.\n\n    Users can also directly inherit from this class to implement customized data\n    processing routines. Two methods should be implemented in the subclass:\n\n    - :meth:`process`: Process a single data example read from the data source\n      (*raw example*). Default implementation returns the raw example as is.\n    - :meth:`collate`: Combine a list of processed examples into a single batch,\n      and return an object of type :class:`~texar.torch.data.Batch`.\n\n    Example:\n\n        Here, we define a custom data class named ``MyDataset``, which is\n        equivalent to the most basic usage of\n        :class:`~texar.torch.data.MonoTextData`.\n\n        .. code-block:: python\n\n            class MyDataset(tx.data.DatasetBase):\n                def __init__(self, data_path, vocab, hparams=None, device=None):\n                    source = tx.data.TextLineDataSource(data_path)\n                    self.vocab = vocab\n                    super().__init__(source, hparams, device)\n\n                def process(self, raw_example):\n                    # `raw_example` is a data example read from `self.source`,\n                    # in this case, a line of tokenized text, represented as a\n                    # list of `str`.\n                    return {\n                        ""text"": raw_example,\n                        ""ids"": self.vocab.map_tokens_to_ids_py(raw_example),\n                    }\n\n                def collate(self, examples):\n                    # `examples` is a list of objects returned from the\n                    # `process` method. These data examples should be collated\n                    # into a batch.\n\n                    # `text` is a list of list of `str`, storing the tokenized\n                    # sentences for each example in the batch.\n                    text = [ex[""text""] for ex in examples]\n                    # `ids` is the NumPy tensor built from the token IDs of each\n                    # sentence, and `lengths` the lengths of each sentence.\n                    # The `tx.data.padded_batch` function pads IDs to the same\n                    # length and then stack them together. This function is\n                    # commonly used in `collate` methods.\n                    ids, lengths = tx.data.padded_batch(\n                        [ex[""ids""] for ex in examples])\n                    return tx.data.Batch(\n                        len(examples),\n                        text=text,\n                        text_ids=torch.from_numpy(ids),\n                        lengths=torch.tensor(lengths))\n\n            vocab = tx.data.Vocab(""vocab.txt"")\n            hparams = {\'batch_size\': 1}\n            data = MyDataset(""data.txt"", vocab, hparams)\n            iterator = DataIterator(data)\n            for batch in iterator:\n                # batch contains the following\n                # batch_ == {\n                #    \'text\': [[\'<BOS>\', \'example\', \'sequence\', \'<EOS>\']],\n                #    \'text_ids\': [[1, 5, 10, 2]],\n                #    \'length\': [4]\n                # }\n    """"""\n\n    # pylint: disable=line-too-long\n\n    # The `DatasetBase` is used in combination with Texar `DataIterator`, which internally uses the PyTorch `DataLoader`\n    # for multi-processing support.\n    #\n    # We divide the entire data pipeline into three stages, namely *load*, *process*, and *batch*:\n    # - **Load** refers to loading data from the data source (e.g., a file, a Python list or iterator). In Texar,\n    #   loading is handled by `DataSource` classes.\n    # - **Process** refers to preprocessing routines for each data example (e.g., vocabulary mapping, tokenization). In\n    #   Texar, this is the `process` function of each `DatasetBase` class.\n    # - **Batch** refers to combining multiple examples to form a batch, which typically includes padding and moving\n    #   data across devices. In Texar, this is the `collate` function of each `DatasetBase` class.\n    #\n    # PyTorch DataLoader only performs batching, and since multi-processing is used, the entire dataset is expected to\n    # be in memory before iteration, i.e. loading and processing cannot be lazy. The `DatasetBase` class is carefully\n    # crafted to provide laziness and caching options at all possible stages.\n    #\n    # To support laziness, we pass data examples (either raw or processed, depending on whether processing is lazy) to\n    # the worker processes. To prevent modifying the underlying `DataLoader` implementation, we hack the PyTorch\n    # `Sampler` classes (responsible for sampling the next data example from the dataset, and returning its index) to\n    # also return data examples. To support caching, the worker may also need to return the processed examples through\n    # pipes.\n    #\n    # The following table describes the intended behavior of each combination of lazy/caching modes, and the exact\n    # behaviors of the sampler and workers. `<X>` means the mode combination does not make sense (e.g. with `Lazy.None`,\n    # processed data examples are effectively cached, so `Cache.None` makes no sense). Parts in `*[blah]*` hold true\n    # only for the first epoch.\n    #\n    # +---------------+-------------------------------+-------------------------------+-------------------------------+\n    # |               | Cache.None                    | Cache.Loaded                  | Cache.Processed               |\n    # |               | no caching                    | only cache loaded examples    | only cache processed examples |\n    # +===============+===============================+===============================+===============================+\n    # | Lazy.None     | <X>                           | <X>                           | Sampler returns indices.      |\n    # | eager load,   |                               |                               | Worker only does batching.    |\n    # | eager process |                               |                               | Worker returns batch.         |\n    # +---------------+-------------------------------+-------------------------------+-------------------------------+\n    # | Lazy.Process  | <X>                           | Sampler returns indices.      | Sampler returns indices.      |\n    # | eager load,   |                               | Worker does batching and      | Worker does batching          |\n    # | lazy process  |                               |   processing.                 |   *[and processing]*.         |\n    # |               |                               | Worker returns batch.         | Worker returns batch          |\n    # |               |                               |                               |   *[and processed examples]*. |\n    # +---------------+-------------------------------+-------------------------------+-------------------------------+\n    # | Lazy.All      | Sampler returns indices and   | Sampler returns indices       | Sampler returns indices       |\n    # | lazy load,    |   data examples.              |   *[and data examples]*.      |   *[and data examples]*.      |\n    # | lazy process  | Worker does batching and      | Worker does batching and      | Worker does batching          |\n    # |               |   processing.                 |   processing.                 |   *[and processing]*.         |\n    # |               | Worker returns batch.         | Worker returns batch.         | Worker returns batch          |\n    # |               |                               |                               |   *[and processed examples]*. |\n    # +---------------+-------------------------------+-------------------------------+-------------------------------+\n    #\n    # Note that in the above table we assume `parallelize_processing` to be True. In rare cases this may not be desired,\n    # for instance, when `process` depends on some shared variable that must be modified during iteration, e.g. a\n    # vocabulary constructed on-the-fly. When `parallelize_processing` is False, behaviors are as the following (much\n    # simpler) table. Although, note that compared to the above cases, this often results in worse performance.\n    #\n    # +---------------+-------------------------------+-------------------------------+-------------------------------+\n    # |               | Cache.None                    | Cache.Loaded                  | Cache.Processed               |\n    # |               | no caching                    | only cache loaded examples    | only cache processed examples |\n    # +===============+===============================+===============================+===============================+\n    # | Lazy.None     | <X>                           | <X>                           | Sampler returns indices.      |\n    # | eager load,   |                               |                               | Worker only does batching.    |\n    # | eager process |                               |                               | Worker returns batch.         |\n    # +---------------+-------------------------------+-------------------------------+-------------------------------+\n    # | Lazy.Process  | <X>                           | Sampler returns indices and processed examples.               |\n    # | eager load,   |                               | Worker only does batching.                                    |\n    # | lazy process  |                               | Worker returns batch.                                         |\n    # +---------------+-------------------------------+---------------------------------------------------------------+\n    # | Lazy.All      | Sampler returns indices and processed examples.                                               |\n    # | lazy load,    | Worker only does batching.                                                                    |\n    # | lazy process  | Worker returns batch.                                                                         |\n    # +---------------+-----------------------------------------------------------------------------------------------+\n\n    # pylint: enable=line-too-long\n\n    _source: DataSource[RawExample]\n    _dataset_size: Optional[int]\n\n    def __init__(self, source: DataSource[RawExample], hparams=None,\n                 device: Optional[torch.device] = None):\n        self._source = source\n        self._hparams = HParams(hparams, self.default_hparams())\n        self.device = device\n\n        if self._hparams.num_epochs != 1:\n            warnings.warn(f""\'num_epochs\' is set to {self._hparams.num_epochs}, ""\n                          f""but will be treated as 1."")\n\n        # Check and convert strategy hyperparameters.\n        self._lazy_strategy = _LazyStrategy(self._hparams.lazy_strategy)\n        self._cache_strategy = _CacheStrategy(self._hparams.cache_strategy)\n        if self._lazy_strategy is _LazyStrategy.NONE:\n            if self._cache_strategy is not _CacheStrategy.PROCESSED:\n                warnings.warn(\n                    f""Using \'{self._cache_strategy}\' cache strategy with ""\n                    f""\'none\' lazy strategy. This will be equivalent to ""\n                    f""\'processed\' cache strategy."")\n            self._cache_strategy = _CacheStrategy.PROCESSED\n        elif self._lazy_strategy is _LazyStrategy.PROCESS:\n            if self._cache_strategy is _CacheStrategy.NONE:\n                warnings.warn(\n                    f""Using \'none\' cache strategy with \'process\' lazy ""\n                    f""strategy. This will be equivalent to \'loaded\' cache ""\n                    f""strategy."")\n                self._cache_strategy = _CacheStrategy.LOADED\n        self._uses_multi_processing = self._hparams.num_parallel_calls > 0\n        self._parallelize_processing = self._hparams.parallelize_processing\n\n        self._processed_cache: List[Example] = []\n        self._fully_cached = False\n\n        # If specified maximum dataset size, wrap the data source. This is done\n        # before caching to avoid caching excess elements.\n        if self._hparams.max_dataset_size != -1:\n            self._source = _TruncatedDataSource[RawExample](\n                self._source, self._hparams.max_dataset_size)\n\n        # If processing should not be parallelized, combine processing with\n        # loading by wrapping the data source. In this case, **processed** data\n        # will be cached.\n        if (not self._parallelize_processing and\n                self._lazy_strategy is _LazyStrategy.ALL and\n                self._cache_strategy is not _CacheStrategy.LOADED):\n            self._transformed_source = _TransformedDataSource[\n                RawExample, Example](self._source, self.process)\n            self._source = self._transformed_source  # type: ignore\n\n        # Check whether data source supports random access, and obtain dataset\n        # size if it does.\n        self._supports_random_access = True\n        if self._lazy_strategy is not _LazyStrategy.NONE:\n            try:\n                self._dataset_size = len(self._source)\n                _ = self._source[0]\n            except TypeError:\n                self._supports_random_access = False\n                erase_after_access = (\n                        self._cache_strategy is not _CacheStrategy.LOADED)\n                self._cached_source = _CachedDataSource[RawExample](\n                    self._source, erase_after_access)\n                self._source = self._cached_source\n                self._dataset_size = None\n\n        # If processing should not be parallelized, combine processing with\n        # loading by wrapping the data source. In this case, **loaded** data\n        # will be cached.\n        if (not self._parallelize_processing and\n                self._cache_strategy is _CacheStrategy.LOADED):\n            self._transformed_source = _TransformedDataSource[\n                RawExample, Example](self._source, self.process)\n            self._source = self._transformed_source  # type: ignore\n\n        # Simplify some logic-heavy checks.\n        self.__should_return_processed_examples = (\n                self._lazy_strategy is not _LazyStrategy.NONE and\n                self._cache_strategy is _CacheStrategy.PROCESSED and\n                self._parallelize_processing)\n        self.__should_call_prefetch_source = (\n                self._lazy_strategy is _LazyStrategy.ALL and\n                self._cache_strategy is _CacheStrategy.NONE)\n        self.__should_call_prefetch_processed = (\n                not self._parallelize_processing and\n                self._lazy_strategy is _LazyStrategy.PROCESS and\n                self._cache_strategy is _CacheStrategy.PROCESSED)\n        self.__should_delete_source_in_add_cache = (\n                not self._supports_random_access and\n                self._parallelize_processing and\n                self._uses_multi_processing and\n                self._lazy_strategy is _LazyStrategy.PROCESS and\n                self._cache_strategy is _CacheStrategy.PROCESSED)\n\n        # Perform eager loading/processing if required.\n        if self._lazy_strategy is _LazyStrategy.NONE:\n            # Process entire dataset and cache.\n            self._processed_cache = [self.process(raw_example)\n                                     for raw_example in self._source]\n            self._dataset_size = len(self._processed_cache)\n            self._fully_cached = True\n        else:\n            if self._lazy_strategy is _LazyStrategy.PROCESS:\n                # Load entire dataset. Note that if data source supports random\n                # access, we assume it is already loaded into memory.\n                if not self._supports_random_access:\n                    self._prefetch_all_source()\n\n            if self._cache_strategy is _CacheStrategy.PROCESSED:\n                # Data can be processed in arbitrary order, so they need to be\n                # reordered before storing in the cache list.\n                self._reorder_cache: Dict[int, Example] = {}\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of default hyperparameters.\n\n        .. code-block:: python\n\n            {\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""lazy_strategy"": \'none\',\n                ""cache_strategy"": \'processed\',\n                ""parallelize_processing"": True,\n                ""name"": ""data""\n            }\n\n        Here:\n\n        `""num_epochs""`: int\n            Number of times the dataset should be repeated.\n\n            .. note::\n                This option only exists for compatibility, and will be\n                ignored. A warning will be generated is any value other than\n                1 is used.\n\n        `""batch_size""`: int\n            Batch size, i.e., the number of consecutive elements of the\n            dataset to combine in a single batch.\n\n        `""allow_smaller_final_batch""`: bool\n           Whether to allow the final batch to be smaller if there are\n           insufficient elements left. If `False`, the final batch is\n           discarded if it is smaller than batch size. Note that,\n           if `True`, `output_shapes` of the resulting dataset\n           will have a a **static** batch_size dimension equal to\n           ""batch_size"".\n\n        `""shuffle""`: bool\n            Whether to randomly shuffle the elements of the dataset.\n\n        `""shuffle_buffer_size""`: int\n            The buffer size for data shuffling. The larger, the better\n            the resulting data is mixed.\n\n            If `None` (default), buffer size is set to the size of the\n            whole dataset (i.e., make the shuffling the maximally\n            effective).\n\n        `""shard_and_shuffle""`: bool\n            Whether to first shard the dataset and then shuffle each\n            block respectively. Useful when the whole data is too large to\n            be loaded efficiently into the memory.\n\n            If `True`, :attr:`shuffle_buffer_size` must be specified to\n            determine the size of each shard.\n\n            .. warning::\n                Sharding is not yet supported. This option will be ignored.\n\n        `""num_parallel_calls""`: int\n            Number of elements from the datasets to process in parallel.\n            When ``""num_parallel_calls""`` equals 0, no worker processes will\n            be created; when the value is greater than 0, the number of worker\n            processes will be equal to ``""num_parallel_calls""``.\n\n        `""prefetch_buffer_size""`: int\n            The maximum number of elements that will be buffered when\n            prefetching.\n\n            .. note::\n                This option exists only for compatibility. Currently data\n                is only prefetched when ``""num_parallel_calls""`` is greater\n                than 1, and the number of examples to prefetch is controlled\n                internally by PyTorch :torch_docs:`DataLoader\n                <data.html#torch.utils.data.DataLoader>`.\n\n        `""max_dataset_size""`: int\n            Maximum number of instances to include in\n            the dataset. If set to `-1` or greater than the size of\n            dataset, all instances will be included. This constraint is\n            imposed after data shuffling and filtering.\n\n        `""seed""`: int, optional\n            The random seed for shuffle.\n\n            Note that if a seed is set, the shuffle order will be exact\n            the same every time when going through the (repeated) dataset.\n\n            .. warning::\n                Manual seeding is not yet supported. This option will be\n                ignored.\n\n        `""lazy_strategy""`: str\n            Lazy strategy for data examples. Lazy loading/processing defers\n            data loading/processing until when it\'s being accessed.\n            Non-lazy (eager) loading/processing would load/process all data\n            upon construction of dataset. Available options are:\n\n            - `none`: Perform eager loading and processing.\n            - `process`: Perform eager loading and lazy processing.\n            - `all`: Perform lazy loading and processing.\n\n            Defaults to `all`. Note that currently, all eager operations\n            are performed on a single process only.\n\n        `""cache_strategy""`: str\n            Caching strategy for data examples. Available options are:\n\n            - `none`: No data is cached. Data is always loaded from\n              source (e.g. file) and processed upon access.\n            - `loaded`: Only cache raw data loaded from source,\n              processing routines are performed upon access.\n            - `processed`: Processed data is cached. **Note:** raw data\n              will not be cached in this case, because raw data is\n              only used to construct the processed data.\n\n            Default value is `loaded`. This option depends on the value of\n            `lazy_strategy`, specifically:\n\n            - When `lazy_strategy` is `none`, all choices of\n              `cache_strategy` are equivalent to `processed`.\n            - When `lazy_strategy` is `process`, `none` is equivalent\n              to `loaded`.\n\n        `""parallelize_processing""`: bool\n            Whether to perform parallelized processing of data. Since\n            multi-processing parallelism is utilized, this flag should be\n            `False` if your process routine involves modifying a shared\n            object across examples.\n\n            Note that this only affects cases where `lazy_strategy` is not\n            `none`. If `lazy_strategy` is `none`, processing will be\n            performed on a single process regardless of this value.\n\n        `""name""`: str\n            Name of the data.\n        """"""\n        # TODO: Sharding not yet supported.\n        # TODO: `seed` is not yet applied.\n        # TODO: `prefetch_buffer_size` will not be supported, but could remain\n        #   for compatibility.\n        return {\n            ""name"": ""data"",\n            ""num_epochs"": 1,\n            ""batch_size"": 64,\n            ""allow_smaller_final_batch"": True,\n            ""shuffle"": True,\n            ""shuffle_buffer_size"": None,\n            ""shard_and_shuffle"": False,\n            ""num_parallel_calls"": 0,\n            ""prefetch_buffer_size"": 0,\n            ""max_dataset_size"": -1,\n            ""seed"": None,\n            ""lazy_strategy"": \'none\',\n            ""cache_strategy"": \'processed\',\n            ""parallelize_processing"": True,\n        }\n\n    def to(self, device: Optional[torch.device]):\n        r""""""Move the dataset to the specific device. Note that we don\'t actually\n        move data or do anything here -- data will be moved to the appropriate\n        device after :class:`~texar.torch.data.DataIterator` fetches the batch.\n        """"""\n        if device is not None:\n            self.device = device\n        return self\n\n    def _prefetch_processed(self, index: int):\n        r""""""Performs processing on the main process. This is called in\n        :meth:`texar.torch.data.data.DatasetBase._prefetch_source` if\n        `parallelize_processing` is `False`.""""""\n        if len(self._processed_cache) <= index:\n            self._processed_cache.extend(\n                self.process(self._source[x])\n                for x in range(len(self._processed_cache), index + 1))\n            if len(self._processed_cache) == self._dataset_size:\n                self._fully_cached = True\n\n    def _prefetch_all_source(self) -> int:\n        r""""""Prefetches all examples from data source. This is only called if\n        `__len__` is called before dataset size can be determined, or when using\n        eager loading.\n        """"""\n\n        try:\n            max_index = 10 ** 8\n            self._cached_source.prefetch(max_index)\n            warnings.warn(\n                f""The data source contains more than {max_index:.2e} ""\n                f""examples. Please check whether it is infinite."")\n            while True:\n                max_index *= 2\n                self._cached_source.prefetch(max_index)\n        except StopIteration:\n            self._dataset_size = self._cached_source.max_index + 1\n            return self._dataset_size\n\n    def _prefetch_source(self, index: int) -> Optional[int]:\n        r""""""Prefetches data so `__getitem__` will be available. This method\n        should only be called in the main process, because data sources are not\n        guaranteed to be thread-safe.\n\n        Args:\n            index: Prefetch data up to this index.\n\n        Returns:\n            If `index` is greater than dataset size, returns the inferred\n            dataset size. Otherwise, returns `None`.\n        """"""\n        if not self._supports_random_access:\n            try:\n                self._cached_source.prefetch(index)\n            except StopIteration:\n                self._dataset_size = self._cached_source.max_index + 1\n                # self._cached_source.reset()\n                if self._should_call_prefetch_processed:\n                    self._prefetch_processed(self._dataset_size - 1)\n                return self._dataset_size\n            if self._should_call_prefetch_processed:\n                self._prefetch_processed(index)\n        else:\n            # Dataset size must be known.\n            if index >= self._dataset_size:  # type: ignore\n                return self._dataset_size\n        return None\n\n    def __len__(self) -> int:\n        if self._dataset_size is None:\n            raise TypeError(\n                ""__len__ not supported for datasets with undetermined size"")\n        return self._dataset_size\n\n    def process(self, raw_example: RawExample) -> Example:\n        r""""""The process routine. A default implementation of no-op is provided,\n        but subclasses are free to override this behavior.\n\n        The process routine would take raw examples loaded from the data source\n        as input, and return processed examples. If `parallelize_processing`\n        is `True`, this method **must not** access shared variables that are\n        modified during iterator (e.g., constructing vocabularies on-the-fly).\n\n        Args:\n            raw_example: The raw example loaded from data.\n\n        Returns:\n            The processed example.\n        """"""\n        return raw_example  # type: ignore\n\n    def __getitem__(self, index: Union[int, Tuple[int, RawExample]]) -> Example:\n        if isinstance(index, int):\n            if self._fully_cached:\n                return self._processed_cache[index]\n            elif not self._parallelize_processing:\n                return self._transformed_source[index]\n            else:\n                return self.process(self._source[index])\n        else:\n            # `index` is a tuple of (index, example).\n            if not self._parallelize_processing:\n                return index[1]  # type: ignore\n            else:\n                return self.process(index[1])\n\n    def _add_cached_examples(self, indices: List[int], examples: List[Example]):\n        r""""""Called by :class:`texar.torch.data.data._CacheDataLoaderIter` to\n        cache examples processed in worker processes.\n\n        Args:\n            indices: Indices for each example.\n            examples: The examples processed in worker processes.\n        """"""\n        if self._should_delete_source_in_add_cache:\n            # In this case, `_CachedDataSource.__getitem__` will be\n            # called on worker processes, so the cache cannot be\n            # deleted. Thus, we move deletion to\n            # `_add_cached_examples`.\n            for index in indices:\n                del self._cached_source._cache[index]  # pylint: disable=protected-access\n        for index, example in zip(indices, examples):\n            if index == len(self._processed_cache):\n                self._processed_cache.append(example)\n            else:\n                self._reorder_cache[index] = example\n\n        while len(self._processed_cache) in self._reorder_cache:\n            index = len(self._processed_cache)\n            self._processed_cache.append(self._reorder_cache[index])\n            del self._reorder_cache[index]\n        if len(self._processed_cache) == self._dataset_size:\n            self._fully_cached = True\n\n    def _start_iteration(self) -> None:\n        r""""""Called by :class:`texar.torch.data.data.SamplerBase` before a new\n        round of iteration starts. Note that this method will only be called if\n        an unknown-sized iterator is used.\n        """"""\n        if not self._supports_random_access:\n            self._cached_source.reset()\n\n    @property\n    def num_epochs(self):\n        r""""""Number of epochs.\n        """"""\n        return self._hparams.num_epochs\n\n    @property\n    def batch_size(self):\n        r""""""The batch size.\n        """"""\n        return self._hparams.batch_size\n\n    @property\n    def hparams(self):\n        r""""""A :class:`~texar.torch.HParams` instance of the\n        data hyperparameters.\n        """"""\n        return self._hparams\n\n    @property\n    def name(self):\n        r""""""Name of the module.\n        """"""\n        return self._hparams.name\n\n    @property\n    def dataset(self):  # TODO: maybe change this to `data_source`\n        r""""""The data source.\n        """"""\n        return self._source\n\n    def collate(self, examples: List[Example]) -> Batch:\n        r""""""The collate routine. Subclasses must implement this method.\n\n        The collate routine is called to collate (combine) examples into\n        batches. This function takes a list of processed examples, and returns\n        an instance of :class:`~texar.torch.data.Batch`.\n\n        .. note::\n            Implementation should make sure that the returned callable is safe\n            and efficient under multi-processing scenarios. Basically, do not\n            rely on variables that could be modified during iteration, and avoid\n            accessing unnecessary variables, as each access would result in a\n            cross-process memory copy.\n\n        .. warning::\n            The recommended pattern is not to move tensor storage within this\n            method, but you are free to do so.\n\n            However, if multiple workers are used\n            (:attr:`num_parallel_calls` > 0), moving tensors to CUDA devices\n            within this method would result in CUDA errors being thrown.\n\n        Args:\n            examples: A list of processed examples in a batch.\n\n        Returns:\n            The collated batch.\n        """"""\n        raise NotImplementedError\n\n    def _collate_and_maybe_return(self, examples: List[Example]) -> \\\n            Union[Batch, Tuple[List[Example], Batch]]:\n        r""""""Called by :class:`~texar.torch.data.DataIterator` to obtain the\n        collated batch (and processed examples under certain circumstances).\n\n        Args:\n            examples: A list of processed examples in a batch.\n\n        Returns:\n            The collated batch.\n        """"""\n        batch = self.collate(examples)\n        if self._should_return_processed_examples:\n            return examples, batch\n        return batch\n\n    @property\n    def _should_return_processed_examples(self):\n        r""""""Returns `True` if the worker threads should perform processing and\n        return the processed examples.\n        """"""\n        return (not self._fully_cached and\n                self.__should_return_processed_examples)\n\n    @property\n    def _should_yield_raw_example(self):\n        r""""""Returns `True` if the sampler should yield raw examples.\n        """"""\n        return (self._lazy_strategy is _LazyStrategy.ALL and\n                (self._cache_strategy is _CacheStrategy.NONE or\n                 not self._fully_cached))\n\n    @property\n    def _should_call_prefetch_source(self):\n        r""""""Returns `True` if the sampler should call `_prefetch_source`.\n        """"""\n        return (self._dataset_size is None or\n                self.__should_call_prefetch_source)\n\n    @property\n    def _should_call_prefetch_processed(self):\n        r""""""Returns `True` if `_prefetch_source` should call\n        `_prefetch_processed`.\n        """"""\n        return self.__should_call_prefetch_processed\n\n    @property\n    def _should_delete_source_in_add_cache(self):\n        r""""""Returns `True` if `_add_cached_examples` should delete cached raw\n        examples.\n        """"""\n        return self.__should_delete_source_in_add_cache\n'"
texar/torch/data/data/data_iterators.py,39,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious data iterator classes.\n""""""\n\n# pylint: disable=protected-access\n\nfrom typing import (\n    Dict, Iterable, Iterator, List, Optional, Sequence, Union, Mapping)\n\nimport pkg_resources\nimport torch\nfrom torch import __version__ as _torch_version  # type: ignore\nfrom torch.utils.data import DataLoader\n\nfrom texar.torch.data.data.data_base import DatasetBase\nfrom texar.torch.data.data.dataset_utils import Batch\nfrom texar.torch.data.data.sampler import (\n    SamplerBase, SequentialSampler, RandomSampler, BufferShuffleSampler,\n    BatchingStrategy, DynamicBatchSampler)\nfrom texar.torch.utils.types import MaybeSeq\nfrom texar.torch.utils.utils import ceildiv, map_structure\n\n_torch_version = pkg_resources.parse_version(_torch_version)\n\n__all__ = [\n    ""DataIterator"",\n    ""TrainTestDataIterator"",\n]\n\n# `Dict` is invariant, `Mapping` is not.\nDatasetsType = Union[Mapping[str, DatasetBase], MaybeSeq[DatasetBase]]\n\n\n# pylint: disable=ungrouped-imports\nif _torch_version >= pkg_resources.parse_version(""1.2.0""):  # PyTorch 1.2.0 +\n    from torch.utils.data._utils.pin_memory import (  # type: ignore\n        pin_memory as _pin_memory)\nelif _torch_version >= pkg_resources.parse_version(""1.1.0""):  # PyTorch 1.1.0 +\n    from torch.utils.data._utils.pin_memory import (  # type: ignore\n        pin_memory_batch as _pin_memory)\nelse:\n    from torch.utils.data.dataloader import (  # type: ignore\n        pin_memory_batch as _pin_memory)\n\n\ndef move_memory(data, device):\n    def _move_fn(x):\n        if isinstance(x, torch.Tensor):\n            return x.to(device=device, non_blocking=True)\n        return x\n\n    if isinstance(data, Batch):\n        return Batch(len(data), batch={\n            key: map_structure(_move_fn, value)\n            for key, value in data.items()\n        })\n    return map_structure(_move_fn, data)\n\n\nif _torch_version >= pkg_resources.parse_version(""1.2.0""):  # PyTorch 1.2.0 +\n    # PyTorch 1.2 split the `_DataLoaderIter` class into two:\n    # `_SingleProcessDataLoaderIter` for when `num_workers == 0`, i.e. when\n    # multi-processing is disabled; `_MultiProcessingDataLoaderIter` for\n    # otherwise. The implementation is also slightly different from previous\n    # releases.\n    #\n    # To keep compatibility, our iterator classes should be a subclass of both\n    # PyTorch `_Single...`/`_Multi...` (for single/multi-process), and our own\n    # `_Cache...`/`_Data...` (for caching/no caching). This results in four\n    # different concrete classes, as this regex shows:\n    # `_[SM]P(Cache)?DataLoaderIter`.\n    #\n    # We only expose `_DataLoaderIter` and `_CacheDataLoaderIter` to other\n    # classes, and construct concrete classes in their `__new__` methods\n    # depending on the value of `num_workers`. This is for compatibility with\n    # previous versions, so we don\'t need to change other parts of the code.\n\n    from texar.torch.data.data.data_iterators_utils import \\\n        TexarBaseDataLoaderIter as _BaseDataLoaderIter\n    from texar.torch.data.data.data_iterators_utils import \\\n        TexarSingleProcessDataLoaderIter as _SingleProcessDataLoaderIter\n    from texar.torch.data.data.data_iterators_utils import \\\n        TexarMultiProcessingDataLoaderIter as _MultiProcessingDataLoaderIter\n\n    class _DataLoaderIter(_BaseDataLoaderIter):\n        r""""""Iterates once over the DataLoader\'s dataset. This is almost\n        identical to PyTorch\n        :class:`torch.utils.data.dataloader._BaseDataLoaderIter`, except that we\n        check `allow_smaller_final_batch` here. This is because using\n        `drop_last` in :class:`~torch.utils.data.sampler.BatchSampler` would\n        cause the dataset to not load/process/cache certain elements from the\n        final batch, which complicates the already complex logic.\n        """"""\n\n        def __new__(cls, loader: \'SingleDatasetIterator\'):\n            if loader.num_workers > 0:\n                return super().__new__(_MPDataLoaderIter)\n            else:\n                return super().__new__(_SPDataLoaderIter)\n\n        def __init__(self, loader: \'SingleDatasetIterator\'):\n            self.device = loader.device\n            self._batch_size = loader.batch_size\n            super().__init__(loader)\n\n        def __next__(self):\n            batch = super().__next__()\n            # Drop smaller final batch according to settings. Note that\n            # `_batch_size` could be None if dynamic batching is used.\n            if (self._batch_size is not None and\n                    batch.batch_size < self._batch_size and\n                    not self.dataset.hparams.allow_smaller_final_batch):\n                raise StopIteration\n            if self.device is not None:\n                batch = move_memory(batch, self.device)\n            return batch\n\n    class _SPDataLoaderIter(_DataLoaderIter, _SingleProcessDataLoaderIter):\n        pass\n\n    class _MPDataLoaderIter(_DataLoaderIter, _MultiProcessingDataLoaderIter):\n        pass\n\n    class _CacheDataLoaderIter(_BaseDataLoaderIter):\n        r""""""Iterates once over the DataLoader\'s dataset. This class is used when\n        examples are processed and returned by worker processes. We need to\n        record the corresponding indices of each batch, call\n        :meth:`texar.torch.data.data.DatasetBase._add_cached_examples` to cache\n        the processed examples, and return only the\n        :class:`~texar.torch.data.data.Batch` instance to the user.\n        """"""\n\n        def __new__(cls, loader: \'SingleDatasetIterator\'):\n            if loader.num_workers > 0:\n                return super().__new__(_MPCacheDataLoaderIter)\n            else:\n                return super().__new__(_SPCacheDataLoaderIter)\n\n        def __init__(self, loader: \'SingleDatasetIterator\'):\n            self._indices_dict: Dict[int, List[int]] = {}\n            self._batch_size = loader.batch_size\n            self.device = loader.device\n            super().__init__(loader)\n\n    class _SPCacheDataLoaderIter(_CacheDataLoaderIter,\n                                 _SingleProcessDataLoaderIter):\n        def __next__(self):\n            index = self._next_index()  # may raise StopIteration\n            data = self.dataset_fetcher.fetch(index)  # may raise StopIteration\n            if self.dataset._should_yield_raw_example:\n                index = [idx[0] for idx in index]\n            examples, data = data\n            self.dataset._add_cached_examples(index, examples)\n            if self.pin_memory:\n                data = move_memory(_pin_memory(data), self.device)\n            return data\n\n    class _MPCacheDataLoaderIter(_CacheDataLoaderIter,\n                                 _MultiProcessingDataLoaderIter):\n        dataset: DatasetBase\n\n        worker_queue_idx: int  # so that Pylint gives no errors\n\n        def _try_put_index(self):\n            assert self.tasks_outstanding < 2 * self.num_workers\n            try:\n                index = self._next_index()\n            except StopIteration:\n                return\n            for _ in range(self.num_workers):  # find next active worker, if any\n                worker_queue_idx = next(self.worker_queue_idx_cycle)\n                if self.workers_status[worker_queue_idx]:\n                    break\n            else:\n                # not found (i.e., didn\'t break)\n                return\n\n            self.index_queues[worker_queue_idx].put((self.send_idx, index))\n            if self.dataset._should_yield_raw_example:\n                index = [idx[0] for idx in index]\n            self._indices_dict[self.send_idx] = index\n            self.task_info[self.send_idx] = (worker_queue_idx,)\n            self.tasks_outstanding += 1\n            self.send_idx += 1\n\n        def _process_data(self, batch):\n            batch = super()._process_data(batch)\n            indices = self._indices_dict[self.rcvd_idx - 1]\n            del self._indices_dict[self.rcvd_idx - 1]\n            examples, batch = batch\n            self.dataset._add_cached_examples(indices, examples)\n            return batch\n\n        def __next__(self):\n            batch = super().__next__()\n            if (self._batch_size is not None and\n                    batch.batch_size < self.dataset.batch_size and\n                    not self.dataset.hparams.allow_smaller_final_batch):\n                raise StopIteration\n            batch = move_memory(batch, self.device)\n            return batch\nelse:\n    # PyTorch 1.1 and lower defines only the class `_DataLoaderIter` for\n    # iterating over `DataLoader`.\n\n    from torch.utils.data.dataloader import (  # type: ignore\n        _DataLoaderIter as torch_DataLoaderIter)\n\n    class _DataLoaderIter(torch_DataLoaderIter):  # type: ignore\n        r""""""Iterates once over the DataLoader\'s dataset. This is almost\n        identical to PyTorch\n        :class:`torch.utils.data.dataloader._DataLoaderIter`, except that we\n        check `allow_smaller_final_batch` here. This is because using\n        `drop_last` in :class:`~torch.utils.data.sampler.BatchSampler` would\n        cause the dataset to not load/process/cache certain elements from the\n        final batch, which complicates the already complex logic.\n        """"""\n\n        def __init__(self, loader: \'SingleDatasetIterator\'):\n            self._batch_size = loader.batch_size\n            self.device = loader.device\n            super().__init__(loader)\n\n        def __next__(self):\n            batch = super().__next__()\n            # Drop smaller final batch according to settings. Note that\n            # `_batch_size` could be None if dynamic batching is used.\n            if (self._batch_size is not None and\n                    batch.batch_size < self._batch_size and\n                    not self.dataset.hparams.allow_smaller_final_batch):\n                raise StopIteration\n            batch = move_memory(batch, self.device)\n            return batch\n\n    class _CacheDataLoaderIter(torch_DataLoaderIter):  # type: ignore\n        r""""""Iterates once over the DataLoader\'s dataset. This class is used when\n        examples are processed and returned by worker processes. We need to\n        record the corresponding indices of each batch, call\n        :meth:`texar.torch.data.data.DatasetBase._add_cached_examples` to cache\n        the processed examples, and return only the\n        :class:`~texar.torch.data.data.Batch` instance to the user.\n        """"""\n        dataset: DatasetBase\n\n        worker_queue_idx: int  # so that Pylint gives no errors\n\n        def __init__(self, loader: \'SingleDatasetIterator\'):\n            self._indices_dict: Dict[int, List[int]] = {}\n            self._batch_size = loader.batch_size\n            self.device = loader.device\n            super().__init__(loader)\n\n        def _put_indices(self):\n            assert self.batches_outstanding < 2 * self.num_workers\n            indices = next(self.sample_iter, None)\n            if indices is None:\n                return\n            self.index_queues[self.worker_queue_idx].put(\n                (self.send_idx, indices))\n            if self.dataset._should_yield_raw_example:\n                indices = [index[0] for index in indices]\n            self._indices_dict[self.send_idx] = indices\n            self.worker_queue_idx = ((self.worker_queue_idx + 1) %\n                                     self.num_workers)\n            self.batches_outstanding += 1\n            self.send_idx += 1\n\n        def _process_next_batch(self, batch):\n            batch = super()._process_next_batch(batch)\n            indices = self._indices_dict[self.rcvd_idx - 1]\n            del self._indices_dict[self.rcvd_idx - 1]\n            examples, batch = batch\n            self.dataset._add_cached_examples(indices, examples)\n            return batch\n\n        def __next__(self):\n            if self.num_workers == 0:  # same-process loading\n                indices = next(self.sample_iter)  # may raise StopIteration\n                batch = self.collate_fn([self.dataset[i] for i in indices])\n                if self.dataset._should_yield_raw_example:\n                    indices = [index[0] for index in indices]\n                examples, batch = batch\n                self.dataset._add_cached_examples(indices, examples)\n                if self.pin_memory:\n                    batch = _pin_memory(batch)\n            else:\n                batch = super().__next__()\n            if (self._batch_size is not None and\n                    batch.batch_size < self.dataset.batch_size and\n                    not self.dataset.hparams.allow_smaller_final_batch):\n                raise StopIteration\n            batch = move_memory(batch, self.device)\n            return batch\n\n\nclass SingleDatasetIterator(DataLoader):\n    r""""""Iterator for a single dataset. This iterator is based on the PyTorch\n    :class:`~torch.utils.data.DataLoader` interface, with a custom shuffling\n    routine. This class is used internally.\n\n    Args:\n        dataset: The dataset to iterator through. The dataset must be an\n            instance of :class:`texar.torch.data.DatasetBase`, because\n            configurations are read from the dataset `HParams`.\n        batching_strategy: The batching strategy to use when performing dynamic\n            batching. If `None`, fixed-sized batching is used.\n        pin_memory: If `True`, tensors will be moved onto page-locked memory\n            before returning. This argument is passed into the constructor for\n            :torch_docs:`DataLoader <data.html#torch.utils.data.DataLoader>`.\n\n            Defaults to `None`, which will set the value to `True` if the\n            :class:`~texar.torch.data.DatasetBase` instance is set to use a CUDA\n            device. Set to `True` or `False` to override this behavior.\n    """"""\n    dataset: DatasetBase\n\n    def __init__(self, dataset: DatasetBase,\n                 batching_strategy: Optional[BatchingStrategy] = None,\n                 pin_memory: Optional[bool] = None):\n        shuffle = dataset.hparams.shuffle\n        shuffle_buffer_size = dataset.hparams.shuffle_buffer_size\n        sampler: SamplerBase\n        if shuffle and shuffle_buffer_size is not None:\n            sampler = BufferShuffleSampler(dataset, shuffle_buffer_size)\n        elif shuffle:\n            sampler = RandomSampler(dataset)\n        else:\n            sampler = SequentialSampler(dataset)\n\n        num_workers = dataset.hparams.num_parallel_calls\n        collate_fn = dataset._collate_and_maybe_return\n\n        is_cuda = dataset.device is not None and dataset.device.type == ""cuda""\n        if pin_memory is None:\n            pin_memory = is_cuda\n        self.device = None\n        if pin_memory and is_cuda:\n            self.device = dataset.device\n\n        if batching_strategy is not None:\n            batch_sampler = DynamicBatchSampler(\n                dataset, sampler, batching_strategy)\n            super().__init__(\n                dataset, batch_sampler=batch_sampler,\n                collate_fn=collate_fn, num_workers=num_workers,\n                pin_memory=pin_memory)\n        else:\n            super().__init__(\n                dataset, batch_size=dataset.batch_size, drop_last=False,\n                sampler=sampler, collate_fn=collate_fn, num_workers=num_workers,\n                pin_memory=pin_memory)\n\n    def __iter__(self):\n        if self.dataset._should_return_processed_examples:\n            # Accepts processed examples from workers and add to dataset cache.\n            return _CacheDataLoaderIter(self)\n        else:\n            return _DataLoaderIter(self)\n\n    def __len__(self):\n        if self.batch_size is None:\n            raise TypeError(""__len__ not supported for dynamic batching"")\n        data_length = len(self.dataset)  # may throw TypeError\n        if self.dataset.hparams.allow_smaller_final_batch:\n            return ceildiv(data_length, self.batch_size)\n        return data_length // self.batch_size\n\n\nclass DataIterator:\n    r""""""Data iterator that switches and iterates through multiple datasets.\n\n    This is a wrapper of :class:`~texar.torch.data.SingleDatasetIterator`.\n\n    Args:\n        datasets: Datasets to iterate through. This can be:\n\n            - A single instance of :class:`~texar.torch.data.DatasetBase`.\n            - A `dict` that maps dataset name to instances of\n              :class:`~texar.torch.data.DatasetBase`.\n            - A `list` of instances of :class:`texar.torch.data.DatasetBase`.\n              The name of instances (:attr:`texar.torch.data.DatasetBase.name`)\n              must be unique.\n\n        batching_strategy: The batching strategy to use when performing dynamic\n            batching. If `None`, fixed-sized batching is used.\n        pin_memory: If `True`, tensors will be moved onto page-locked memory\n            before returning. This argument is passed into the constructor for\n            :torch_docs:`DataLoader <data.html#torch.utils.data.DataLoader>`.\n\n            Defaults to `None`, which will set the value to `True` if the\n            :class:`~texar.torch.data.DatasetBase` instance is set to use a CUDA\n            device. Set to `True` or `False` to override this behavior.\n\n    Example:\n\n        Create an iterator over two datasets and generating fixed-sized batches:\n\n        .. code-block:: python\n\n            train_data = MonoTextData(hparams_train)\n            test_data = MonoTextData(hparams_test)\n            iterator = DataIterator({\'train\': train_data, \'test\': test_data})\n\n            for epoch in range(200): # Run 200 epochs of train/test\n                # Starts iterating through training data from the beginning.\n                iterator.switch_to_dataset(\'train\')\n                for batch in iterator:\n                    ... # Do training with the batch.\n\n                # Starts iterating through test data from the beginning\n                for batch in iterator.get_iterator(\'test\'):\n                    ... # Do testing with the batch.\n\n        Dynamic batching based on total number of tokens:\n\n        .. code-block:: python\n\n            iterator = DataIterator(\n                {\'train\': train_data, \'test\': test_data},\n                batching_strategy=TokenCountBatchingStrategy(max_tokens=1000))\n\n        Dynamic batching with custom strategy (e.g. total number of tokens in\n        examples from :class:`~texar.torch.data.PairedTextData`, including\n        padding):\n\n        .. code-block:: python\n\n            class CustomBatchingStrategy(BatchingStrategy):\n                def __init__(self, max_tokens: int):\n                    self.max_tokens = max_tokens\n                    self.reset_batch()\n\n                def reset_batch(self) -> None:\n                    self.max_src_len = 0\n                    self.max_tgt_len = 0\n                    self.cur_batch_size = 0\n\n                def add_example(self, ex: Tuple[List[str], List[str]]) -> bool:\n                    max_src_len = max(self.max_src_len, len(ex[0]))\n                    max_tgt_len = max(self.max_tgt_len, len(ex[0]))\n                    if (max(max_src_len + max_tgt_len) *\n                            (self.cur_batch_size + 1) > self.max_tokens):\n                        return False\n                    self.max_src_len = max_src_len\n                    self.max_tgt_len = max_tgt_len\n                    self.cur_batch_size += 1\n                    return True\n\n            iterator = DataIterator(\n                {\'train\': train_data, \'test\': test_data},\n                batching_strategy=CustomBatchingStrategy(max_tokens=1000))\n    """"""\n\n    # TODO: Think about whether we should support save/load.\n\n    def __init__(self, datasets: DatasetsType,\n                 batching_strategy: Optional[BatchingStrategy] = None,\n                 pin_memory: Optional[bool] = None):\n        self._default_dataset_name = \'data\'\n        if isinstance(datasets, DatasetBase):\n            datasets = {self._default_dataset_name: datasets}\n        elif isinstance(datasets, Sequence):\n            if any(not isinstance(d, DatasetBase) for d in datasets):\n                raise ValueError(""`datasets` must be an non-empty list of ""\n                                 ""`texar.torch.data.DatasetBase` instances."")\n            num_datasets = len(datasets)\n            datasets = {d.name: d for d in datasets}\n            if len(datasets) < num_datasets:\n                raise ValueError(""Names of datasets must be unique."")\n\n        _datasets = {\n            name: SingleDatasetIterator(dataset, batching_strategy, pin_memory)\n            for name, dataset in datasets.items()}\n        self._datasets = _datasets\n\n        if len(self._datasets) <= 0:\n            raise ValueError(""`datasets` must not be empty."")\n\n        self._current_dataset_name: Optional[str] = None\n\n    @property\n    def num_datasets(self) -> int:\n        r""""""Number of datasets.\n        """"""\n        return len(self._datasets)\n\n    @property\n    def dataset_names(self) -> List[str]:\n        r""""""A list of dataset names.\n        """"""\n        return list(self._datasets.keys())\n\n    def _validate_dataset_name(self, dataset_name: Optional[str]) -> str:\n        r""""""Validate the provided dataset name, and return the validated name.\n        """"""\n        if dataset_name is None:\n            if self.num_datasets > 1:\n                raise ValueError(""`dataset_name` is required if there are ""\n                                 ""more than one datasets."")\n            dataset_name = next(iter(self._datasets))\n        if dataset_name not in self._datasets:\n            raise ValueError(""Dataset not found: "", dataset_name)\n        return dataset_name\n\n    def switch_to_dataset(self, dataset_name: Optional[str] = None):\n        r""""""Re-initializes the iterator of a given dataset and starts iterating\n        over the dataset (from the beginning).\n\n        Args:\n            dataset_name (optional): Name of the dataset. If not provided,\n                there must be only one Dataset.\n        """"""\n        self._current_dataset_name = self._validate_dataset_name(dataset_name)\n\n    def get_iterator(self,\n                     dataset_name: Optional[str] = None) -> Iterator[Batch]:\n        r""""""Re-initializes the iterator of a given dataset and starts iterating\n        over the dataset (from the beginning).\n\n        Args:\n            dataset_name (optional): Name of the dataset. If not provided,\n                there must be only one Dataset.\n        """"""\n        if dataset_name is not None or self._current_dataset_name is None:\n            dataset_name = self._validate_dataset_name(dataset_name)\n        elif self._current_dataset_name is not None:\n            dataset_name = self._current_dataset_name\n        else:\n            raise ValueError(""No dataset is selected."")\n\n        return iter(self._datasets[dataset_name])\n\n    def __iter__(self) -> Iterator[Batch]:\n        r""""""Returns the iterator for the currently selected or default dataset.\n        """"""\n        return self.get_iterator()\n\n    def __len__(self):\n        return len(self._datasets[self._validate_dataset_name(None)])\n\n\nclass TrainTestDataIterator(DataIterator):\n    r""""""Data iterator that alternates between training, validation, and test\n    datasets.\n\n    :attr:`train`, :attr:`val`, and :attr:`test` are instances of\n    :class:`~texar.torch.data.DatasetBase`. At least one of them must be\n    provided.\n\n    This is a wrapper of :class:`~texar.torch.data.DataIterator`.\n\n    Args:\n        train (optional): Training data.\n        val (optional): Validation data.\n        test (optional): Test data.\n        batching_strategy: The batching strategy to use when performing dynamic\n            batching. If `None`, fixed-sized batching is used.\n        pin_memory: If `True`, tensors will be moved onto page-locked memory\n            before returning. This argument is passed into the constructor for\n            :torch_docs:`DataLoader <data.html#torch.utils.data.DataLoader>`.\n\n            Defaults to `None`, which will set the value to `True` if the\n            :class:`~texar.torch.data.DatasetBase` instance is set to use a CUDA\n            device. Set to `True` or `False` to override this behavior.\n\n    Example:\n\n        .. code-block:: python\n\n            train_data = MonoTextData(hparams_train)\n            val_data = MonoTextData(hparams_val)\n            iterator = TrainTestDataIterator(train=train_data, val=val_data)\n\n            for epoch in range(200): # Run 200 epochs of train/val\n                # Starts iterating through training data from the beginning.\n                iterator.switch_to_train_data(sess)\n                for batch in iterator:\n                    ... # Do training with the batch.\n\n                # Starts iterating through val data from the beginning.\n                for batch in iterator.get_val_iterator():\n                    ... # Do validation on the batch.\n    """"""\n\n    def __init__(self, train: Optional[DatasetBase] = None,\n                 val: Optional[DatasetBase] = None,\n                 test: Optional[DatasetBase] = None,\n                 batching_strategy: Optional[BatchingStrategy] = None,\n                 pin_memory: Optional[bool] = None):\n        dataset_dict = {}\n        self._train_name = \'train\'\n        self._val_name = \'val\'\n        self._test_name = \'test\'\n        if train is not None:\n            dataset_dict[self._train_name] = train\n        if val is not None:\n            dataset_dict[self._val_name] = val\n        if test is not None:\n            dataset_dict[self._test_name] = test\n        if len(dataset_dict) == 0:\n            raise ValueError(""At least one of `train`, `val`, and `test` ""\n                             ""must be provided."")\n\n        super().__init__(dataset_dict, batching_strategy, pin_memory)\n\n    def switch_to_train_data(self) -> None:\n        r""""""Switch to training data.""""""\n        if self._train_name not in self._datasets:\n            raise ValueError(""Training data not provided."")\n        self.switch_to_dataset(self._train_name)\n\n    def switch_to_val_data(self) -> None:\n        r""""""Switch to validation data.""""""\n        if self._val_name not in self._datasets:\n            raise ValueError(""Validation data not provided."")\n        self.switch_to_dataset(self._val_name)\n\n    def switch_to_test_data(self) -> None:\n        r""""""Switch to test data.""""""\n        if self._test_name not in self._datasets:\n            raise ValueError(""Test data not provided."")\n        self.switch_to_dataset(self._test_name)\n\n    def get_train_iterator(self) -> Iterable[Batch]:\n        r""""""Obtain an iterator over training data.""""""\n        if self._train_name not in self._datasets:\n            raise ValueError(""Training data not provided."")\n        return self.get_iterator(self._train_name)\n\n    def get_val_iterator(self) -> Iterable[Batch]:\n        r""""""Obtain an iterator over validation data.""""""\n        if self._val_name not in self._datasets:\n            raise ValueError(""Validation data not provided."")\n        return self.get_iterator(self._val_name)\n\n    def get_test_iterator(self) -> Iterable[Batch]:\n        r""""""Obtain an iterator over test data.""""""\n        if self._test_name not in self._datasets:\n            raise ValueError(""Test data not provided."")\n        return self.get_iterator(self._test_name)\n'"
texar/torch/data/data/data_iterators_utils.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious data iterator utility classes.\n""""""\n\nimport pkg_resources\n\nfrom torch import __version__ as _torch_version  # type: ignore\nfrom torch.utils.data.dataloader import (  # type: ignore\n    _BaseDataLoaderIter, _SingleProcessDataLoaderIter,\n    _MultiProcessingDataLoaderIter)\n\n\n_torch_version = pkg_resources.parse_version(_torch_version)\n\n\n# PyTorch 1.3 change some attribute names in `_BaseDataLoaderIter`,\n# `_SingleProcessDataLoaderIter`, and `_MultiProcessingDataLoaderIter`\n\nif _torch_version >= pkg_resources.parse_version(""1.3.0""):\n\n    class TexarBaseDataLoaderIter(_BaseDataLoaderIter):\n\n        @property\n        def dataset(self):\n            return self._dataset\n\n        @property\n        def dataset_kind(self):\n            return self._dataset_kind\n\n        @property\n        def auto_collation(self):\n            return self._auto_collation\n\n        @property\n        def drop_last(self):\n            return self._drop_last\n\n        @property\n        def index_sampler(self):\n            return self._index_sampler\n\n        @property\n        def num_workers(self):\n            return self._num_workers\n\n        @property\n        def pin_memory(self):\n            return self._pin_memory\n\n        @property\n        def timeout(self):\n            return self._timeout\n\n        @property\n        def collate_fn(self):\n            return self._collate_fn\n\n        @property\n        def sampler_iter(self):\n            return self._sampler_iter\n\n        @property\n        def base_seed(self):\n            return self._base_seed\n\n    class TexarSingleProcessDataLoaderIter(_SingleProcessDataLoaderIter,\n                                           TexarBaseDataLoaderIter):\n\n        @property\n        def dataset_fetcher(self):\n            return self._dataset_fetcher\n\n    class TexarMultiProcessingDataLoaderIter(_MultiProcessingDataLoaderIter,\n                                             TexarBaseDataLoaderIter):\n\n        @property\n        def worker_init_fn(self):\n            return self._worker_init_fn\n\n        @property\n        def worker_queue_idx_cycle(self):\n            return self._worker_queue_idx_cycle\n\n        @property\n        def worker_result_queue(self):\n            return self._worker_result_queue\n\n        @property\n        def worker_pids_set(self):\n            return self._worker_pids_set\n\n        @property\n        def shutdown(self):\n            return self._shutdown\n\n        @property\n        def send_idx(self):\n            return self._send_idx\n\n        @send_idx.setter\n        def send_idx(self, value):\n            self._send_idx = value\n\n        @property\n        def rcvd_idx(self):\n            return self._rcvd_idx\n\n        @property\n        def task_info(self):\n            return self._task_info\n\n        @property\n        def tasks_outstanding(self):\n            return self._tasks_outstanding\n\n        @tasks_outstanding.setter\n        def tasks_outstanding(self, value):\n            self._tasks_outstanding = value\n\n        @property\n        def workers_done_event(self):\n            return self._workers_done_event\n\n        @property\n        def index_queues(self):\n            return self._index_queues\n\n        @property\n        def workers(self):\n            return self._workers\n\n        @property\n        def workers_status(self):\n            return self._workers_status\n\n        @property\n        def data_queue(self):\n            return self._data_queue\n\n        @property\n        def pin_memory_thread_done_event(self):\n            if hasattr(self, \'_pin_memory_thread_done_event\'):\n                return self._pin_memory_thread_done_event\n\n        @property\n        def pin_memory_thread(self):\n            if hasattr(self, \'_pin_memory_thread\'):\n                return self._pin_memory_thread\n\nelse:\n    class TexarBaseDataLoaderIter(_BaseDataLoaderIter):  # type: ignore\n        pass\n\n    class TexarSingleProcessDataLoaderIter(  # type: ignore\n        _SingleProcessDataLoaderIter, TexarBaseDataLoaderIter):\n        pass\n\n    class TexarMultiProcessingDataLoaderIter(  # type: ignore\n        _MultiProcessingDataLoaderIter, TexarBaseDataLoaderIter):\n        pass\n'"
texar/torch/data/data/dataset_utils.py,1,"b'""""""\nVarious utilities for data module\n""""""\n\nfrom enum import Enum\nfrom typing import (\n    Any, Dict, ItemsView, KeysView, List, Optional, Tuple, Union, ValuesView)\n\nimport numpy as np\n\n__all__ = [\n    \'padded_batch\',\n    \'connect_name\',\n    \'Batch\',\n    \'_LazyStrategy\',\n    \'_CacheStrategy\',\n]\n\n\ndef padded_batch(examples: Union[List[np.ndarray], List[List[int]]],\n                 pad_length: Optional[int] = None, pad_value: int = 0) \\\n        -> Tuple[np.ndarray, List[int]]:\n    r""""""Pad a batch of integer lists (or numpy arrays) to the same length, and\n    stack them together.\n\n    Args:\n        examples (list of lists): The list of examples.\n        pad_length (int, optional): The desired length after padding. If\n            `None`, use the maximum length of lists in the batch. Defaults to\n            `None`. Note that if ``pad_length`` is not `None` and the\n            maximum length of lists is greater than ``pad_length``, then all\n            lists are padded to the maximum length instead.\n        pad_value (int, optional): The value to fill in the padded positions.\n            Defaults to 0.\n\n    Returns:\n        A tuple of two elements, with the first being the padded batch, and the\n        second being the original lengths of each list.\n    """"""\n    lengths = [len(sent) for sent in examples]\n    pad_length = pad_length or max(lengths)\n\n    padded = np.full((len(examples), pad_length), pad_value, dtype=np.int64)\n    for b_idx, sent in enumerate(examples):\n        length = lengths[b_idx]\n        padded[b_idx, :length] = sent[:length]\n    return padded, lengths\n\n\ndef connect_name(lhs_name, rhs_name):\n    if not lhs_name:\n        return rhs_name\n    if not rhs_name:\n        return lhs_name\n    return ""{}_{}"".format(lhs_name, rhs_name)\n\n\nclass Batch:\n    r""""""Wrapper over Python dictionaries representing a batch. It provides a\n    dictionary-like interface to access its fields. This class can be used in\n    the followed way\n\n    .. code-block:: python\n\n        hparams = {\n            \'dataset\': { \'files\': \'data.txt\', \'vocab_file\': \'vocab.txt\' },\n            \'batch_size\': 1\n        }\n\n        data = MonoTextData(hparams)\n        iterator = DataIterator(data)\n\n        for batch in iterator:\n            # batch is Batch object and contains the following fields\n            # batch == {\n            #    \'text\': [[\'<BOS>\', \'example\', \'sequence\', \'<EOS>\']],\n            #    \'text_ids\': [[1, 5, 10, 2]],\n            #    \'length\': [4]\n            # }\n\n            input_ids = torch.tensor(batch[\'text_ids\'])\n\n            # we can also access the elements using dot notation\n            input_text = batch.text\n    """"""\n\n    def __init__(self, batch_size: int, batch: Optional[Dict[str, Any]] = None,\n                 **kwargs):\n        self.batch_size = batch_size\n        self._batch = batch or {}\n        if isinstance(self._batch, dict):\n            self._batch.update(kwargs)\n\n    def __getattr__(self, item):\n        if item not in super().__getattribute__(\'_batch\'):\n            raise AttributeError\n        return self._batch[item]\n\n    def __getitem__(self, item):\n        return self._batch[item]\n\n    def __len__(self) -> int:\n        return self.batch_size\n\n    def __repr__(self):\n        return repr(self._batch)\n\n    def keys(self) -> KeysView[str]:\n        return self._batch.keys()\n\n    def values(self) -> ValuesView[Any]:\n        return self._batch.values()\n\n    def items(self) -> ItemsView[str, Any]:\n        return self._batch.items()\n\n\nclass _LazyStrategy(Enum):\n    NONE = ""none""\n    PROCESS = ""process""\n    ALL = ""all""\n\n\nclass _CacheStrategy(Enum):\n    NONE = ""none""\n    LOADED = ""loaded""\n    PROCESSED = ""processed""\n'"
texar/torch/data/data/mono_text_data.py,18,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nMono text data class that define data reading, parsing, batching, and other\npreprocessing operations.\n""""""\nfrom enum import Enum\nfrom typing import List, Optional\n\nimport torch\n\nfrom texar.torch.data.data.data_base import DataSource\nfrom texar.torch.data.data.dataset_utils import Batch, padded_batch\nfrom texar.torch.data.data.text_data_base import (\n    TextDataBase, TextLineDataSource)\nfrom texar.torch.data.embedding import Embedding\nfrom texar.torch.data.vocabulary import SpecialTokens, Vocab\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.utils import utils\n\n__all__ = [\n    ""_default_mono_text_dataset_hparams"",\n    ""MonoTextData"",\n]\n\n\nclass _LengthFilterMode(Enum):\n    r""""""Options of length filter mode.\n    """"""\n    TRUNC = ""truncate""\n    DISCARD = ""discard""\n\n\ndef _default_mono_text_dataset_hparams():\n    r""""""Returns hyperparameters of a mono text dataset with default values.\n\n    See :meth:`texar.torch.MonoTextData.default_hparams` for details.\n    """"""\n    return {\n        ""files"": [],\n        ""compression_type"": None,\n        ""vocab_file"": """",\n        ""embedding_init"": Embedding.default_hparams(),\n        ""delimiter"": None,\n        ""max_seq_length"": None,\n        ""length_filter_mode"": ""truncate"",\n        ""pad_to_max_seq_length"": False,\n        ""bos_token"": SpecialTokens.BOS,\n        ""eos_token"": SpecialTokens.EOS,\n        ""other_transformations"": [],\n        ""variable_utterance"": False,\n        ""utterance_delimiter"": ""|||"",\n        ""max_utterance_cnt"": 5,\n        ""data_name"": None,\n        ""@no_typecheck"": [""files""]\n    }\n\n\n# todo(avinash): Add variable utterance logic\nclass MonoTextData(TextDataBase[List[str], List[str]]):\n    r""""""Text data processor that reads single set of text files. This can be\n    used for, e.g., language models, auto-encoders, etc.\n\n    Args:\n        hparams: A `dict` or instance of :class:`~texar.torch.HParams`\n            containing hyperparameters. See :meth:`default_hparams` for the\n            defaults.\n        device: The device of the produced batches. For GPU training, set to\n            current CUDA device.\n\n    By default, the processor reads raw data files, performs tokenization,\n    batching and other pre-processing steps, and results in a Dataset\n    whose element is a python `dict` including three fields:\n\n    ""text"":\n        A list of ``[batch_size]`` elements each containing a list of\n        **raw** text tokens of the sequences. Short sequences in the batch\n        are padded with **empty string**. By default only ``EOS`` token is\n        appended to each sequence. Out-of-vocabulary tokens are **NOT**\n        replaced with ``UNK``.\n    ""text_ids"":\n        A list of ``[batch_size]`` elements each containing a list of token\n        indexes of source sequences in the batch.\n    ""length"":\n        A list of ``[batch_size]`` elements of integers containing the length\n        of each source sequence in the batch (including ``BOS`` and ``EOS``\n        if added).\n\n    The above field names can be accessed through :attr:`text_name`,\n    :attr:`text_id_name`, :attr:`length_name`.\n\n    Example:\n\n        .. code-block:: python\n\n            hparams={\n                \'dataset\': { \'files\': \'data.txt\', \'vocab_file\': \'vocab.txt\' },\n                \'batch_size\': 1\n            }\n            data = MonoTextData(hparams)\n            iterator = DataIterator(data)\n            for batch in iterator:\n                # batch contains the following\n                # batch_ == {\n                #    \'text\': [[\'<BOS>\', \'example\', \'sequence\', \'<EOS>\']],\n                #    \'text_ids\': [[1, 5, 10, 2]],\n                #    \'length\': [4]\n                # }\n    """"""\n\n    _delimiter: Optional[str]\n    _bos: Optional[str]\n    _eos: Optional[str]\n    _max_seq_length: Optional[int]\n    _should_pad: bool\n\n    def __init__(self, hparams, device: Optional[torch.device] = None,\n                 vocab: Optional[Vocab] = None,\n                 embedding: Optional[Embedding] = None,\n                 data_source: Optional[DataSource] = None):\n        self._hparams = HParams(hparams, self.default_hparams())\n        if self._hparams.dataset.variable_utterance:\n            raise NotImplementedError\n\n        # Create vocabulary\n        self._bos_token = self._hparams.dataset.bos_token\n        self._eos_token = self._hparams.dataset.eos_token\n        self._other_transforms = self._hparams.dataset.other_transformations\n        bos = utils.default_str(self._bos_token, SpecialTokens.BOS)\n        eos = utils.default_str(self._eos_token, SpecialTokens.EOS)\n        if vocab is None:\n            self._vocab = Vocab(self._hparams.dataset.vocab_file,\n                                bos_token=bos, eos_token=eos)\n        else:\n            self._vocab = vocab\n\n        # Create embedding\n        if embedding is not None:\n            self._embedding = self.make_embedding(\n                self._hparams.dataset.embedding_init,\n                self._vocab.token_to_id_map_py)\n        else:\n            self._embedding = embedding\n\n        self._delimiter = self._hparams.dataset.delimiter\n        self._max_seq_length = self._hparams.dataset.max_seq_length\n        self._length_filter_mode = _LengthFilterMode(\n            self._hparams.dataset.length_filter_mode)\n        self._pad_length = self._max_seq_length\n        if self._pad_length is not None:\n            self._pad_length += sum(int(x != \'\')\n                                    for x in [self._bos_token, self._eos_token])\n\n        if data_source is None:\n            if (self._length_filter_mode is _LengthFilterMode.DISCARD and\n                    self._max_seq_length is not None):\n                data_source = TextLineDataSource(\n                    self._hparams.dataset.files,\n                    compression_type=self._hparams.dataset.compression_type,\n                    delimiter=self._delimiter,\n                    max_length=self._max_seq_length)\n            else:\n                data_source = TextLineDataSource(\n                    self._hparams.dataset.files,\n                    compression_type=self._hparams.dataset.compression_type)\n\n        super().__init__(data_source, hparams, device=device)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of default hyperparameters:\n\n        .. code-block:: python\n\n            {\n                # (1) Hyperparameters specific to text dataset\n                ""dataset"": {\n                    ""files"": [],\n                    ""compression_type"": None,\n                    ""vocab_file"": """",\n                    ""embedding_init"": {},\n                    ""delimiter"": None,\n                    ""max_seq_length"": None,\n                    ""length_filter_mode"": ""truncate"",\n                    ""pad_to_max_seq_length"": False,\n                    ""bos_token"": ""<BOS>""\n                    ""eos_token"": ""<EOS>""\n                    ""other_transformations"": [],\n                    ""variable_utterance"": False,\n                    ""utterance_delimiter"": ""|||"",\n                    ""max_utterance_cnt"": 5,\n                    ""data_name"": None,\n                }\n                # (2) General hyperparameters\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""name"": ""mono_text_data"",\n                # (3) Bucketing\n                ""bucket_boundaries"": [],\n                ""bucket_batch_sizes"": None,\n                ""bucket_length_fn"": None,\n            }\n\n        Here:\n\n        1. For the hyperparameters in the :attr:`""dataset""` field:\n\n          `""files""`: str or list\n              A (list of) text file path(s).\n\n              Each line contains a single text sequence.\n\n          `""compression_type""`: str, optional\n              One of `None` (no compression), ``""ZLIB""``, or ``""GZIP""``.\n\n          `""vocab_file""`: str\n              Path to vocabulary file. Each line of the file should contain\n              one vocabulary token.\n\n              Used to create an instance of :class:`~texar.torch.data.Vocab`.\n\n          `""embedding_init""`: dict\n              The hyperparameters for pre-trained embedding loading and\n              initialization.\n\n              The structure and default values are defined in\n              :meth:`texar.torch.data.Embedding.default_hparams`.\n\n          `""delimiter""`: str, optional\n              The delimiter to split each line of the text files into tokens.\n              If `None` (default), behavior will be equivalent to `str.split()`,\n              i.e. split on any blank character.\n\n          `""max_seq_length""`: int, optional\n              Maximum length of output sequences. Data samples exceeding the\n              length will be truncated or discarded according to\n              :attr:`""length_filter_mode""`. The length does not include\n              any added :attr:`""bos_token""` or :attr:`""eos_token""`. If\n              `None` (default), no filtering is performed.\n\n          `""length_filter_mode""`: str\n              Either ``""truncate""`` or ``""discard""``. If ``""truncate""``\n              (default), tokens exceeding :attr:`""max_seq_length""` will be\n              truncated.\n              If ``""discard""``, data samples longer than\n              :attr:`""max_seq_length""` will be discarded.\n\n          `""pad_to_max_seq_length""`: bool\n              If `True`, pad all data instances to length\n              :attr:`""max_seq_length""`.\n              Raises error if :attr:`""max_seq_length""` is not provided.\n\n          `""bos_token""`: str\n              The Begin-Of-Sequence token prepended to each sequence.\n\n              Set to an empty string to avoid prepending.\n\n          `""eos_token""`: str\n              The End-Of-Sequence token appended to each sequence.\n\n              Set to an empty string to avoid appending.\n\n          `""other_transformations""`: list\n              A list of transformation functions or function names/paths to\n              further transform each single data instance.\n\n              (More documentations to be added.)\n\n          `""variable_utterance""`: bool\n              If `True`, each line of the text file is considered to contain\n              multiple sequences (utterances) separated by\n              :attr:`""utterance_delimiter""`.\n\n              For example, in dialog data, each line can contain a series of\n              dialog history utterances. See the example in\n              `examples/hierarchical_dialog` for a use case.\n\n              .. warning::\n                  Variable utterances is not yet supported. This option (and\n                  related ones below) will be ignored.\n\n          `""utterance_delimiter""`: str\n              The delimiter to split over utterance level. Should not be the\n              same with :attr:`""delimiter""`. Used only when\n              :attr:`""variable_utterance""` is `True`.\n\n          `""max_utterance_cnt""`: int\n              Maximally allowed number of utterances in a data instance.\n              Extra utterances are truncated out.\n\n          `""data_name""`: str\n              Name of the dataset.\n\n        2. For the **general** hyperparameters, see\n        :meth:`texar.torch.data.DatasetBase.default_hparams` for details.\n\n        3. **Bucketing** is to group elements of the dataset\n        together by length and then pad and batch. For bucketing\n        hyperparameters:\n\n          `""bucket_boundaries""`: list\n              An int list containing the upper length boundaries of the\n              buckets.\n\n              Set to an empty list (default) to disable bucketing.\n\n          `""bucket_batch_sizes""`: list\n              An int list containing batch size per bucket. Length should be\n              `len(bucket_boundaries) + 1`.\n\n              If `None`, every bucket will have the same batch size specified\n              in :attr:`batch_size`.\n\n          `""bucket_length_fn""`: str or callable\n              Function maps dataset element to ``int``, determines\n              the length of the element.\n\n              This can be a function, or the name or full module path to the\n              function. If function name is given, the function must be in the\n              :mod:`texar.torch.custom` module.\n\n              If `None` (default), length is determined by the number of\n              tokens (including BOS and EOS if added) of the element.\n\n          .. warning::\n              Bucketing is not yet supported. These options will be ignored.\n\n        """"""\n        hparams = TextDataBase.default_hparams()\n        hparams[""name""] = ""mono_text_data""\n        hparams.update({\n            ""dataset"": _default_mono_text_dataset_hparams()\n        })\n        return hparams\n\n    @staticmethod\n    def make_embedding(emb_hparams, token_to_id_map):\n        r""""""Optionally loads embedding from file (if provided), and returns\n        an instance of :class:`texar.torch.data.Embedding`.\n        """"""\n        embedding = None\n        if emb_hparams[""file""] is not None and len(emb_hparams[""file""]) > 0:\n            embedding = Embedding(token_to_id_map, emb_hparams)\n        return embedding\n\n    def process(self, raw_example: List[str]) -> List[str]:\n        # Truncates sentences and appends BOS/EOS tokens.\n        words = raw_example\n        if (self._max_seq_length is not None and\n                len(words) > self._max_seq_length):\n            if self._length_filter_mode is _LengthFilterMode.TRUNC:\n                words = words[:self._max_seq_length]\n\n        if self._hparams.dataset[""bos_token""] != \'\':\n            words.insert(0, self._hparams.dataset[""bos_token""])\n        if self._hparams.dataset[""eos_token""] != \'\':\n            words.append(self._hparams.dataset[""eos_token""])\n\n        # Apply the ""other transformations"".\n        for transform in self._other_transforms:\n            words = transform(words)\n\n        return words\n\n    def collate(self, examples: List[List[str]]) -> Batch:\n        # For `MonoTextData`, each example is represented as a list of strings.\n        # `_collate` takes care of padding and numericalization.\n\n        # If `pad_length` is `None`, pad to the longest sentence in the batch.\n        text_ids = [self._vocab.map_tokens_to_ids_py(sent) for sent in examples]\n        text_ids, lengths = padded_batch(text_ids, self._pad_length,\n                                         pad_value=self._vocab.pad_token_id)\n        # Also pad the examples\n        pad_length = self._pad_length or max(lengths)\n        examples = [\n            sent + [\'\'] * (pad_length - len(sent))\n            if len(sent) < pad_length else sent\n            for sent in examples\n        ]\n\n        text_ids = torch.from_numpy(text_ids)\n        lengths = torch.tensor(lengths, dtype=torch.long)\n        batch = {self.text_name: examples, self.text_id_name: text_ids,\n                 self.length_name: lengths}\n        return Batch(len(examples), batch=batch)\n\n    def list_items(self) -> List[str]:\n        r""""""Returns the list of item names that the data can produce.\n\n        Returns:\n            A list of strings.\n        """"""\n        items = [\'text\', \'text_ids\', \'length\']\n        data_name = self._hparams.dataset.data_name\n        if data_name is not None:\n            items = [data_name + \'_\' + item for item in items]\n        return items\n\n    @property\n    def vocab(self) -> Vocab:\n        r""""""The vocabulary, an instance of :class:`~texar.torch.data.Vocab`.\n        """"""\n        return self._vocab\n\n    @property\n    def text_name(self):\n        r""""""The name for the text field""""""\n        if self.hparams.dataset[""data_name""]:\n            name = ""{}_text"".format(self.hparams.dataset[""data_name""])\n        else:\n            name = ""text""\n        return name\n\n    @property\n    def text_id_name(self):\n        r""""""The name for text ids""""""\n        if self.hparams.dataset[""data_name""]:\n            name = ""{}_text_ids"".format(self.hparams.dataset[""data_name""])\n        else:\n            name = ""text_ids""\n        return name\n\n    @property\n    def length_name(self):\n        r""""""The name for text length""""""\n        if self.hparams.dataset[""data_name""]:\n            name = ""{}_length"".format(self.hparams.dataset[""data_name""])\n        else:\n            name = ""length""\n        return name\n\n    @property\n    def embedding_init_value(self):\n        r""""""The `Tensor` containing the embedding value loaded from file.\n        `None` if embedding is not specified.\n        """"""\n        if self._embedding is None:\n            return None\n        return self._embedding.word_vecs\n'"
texar/torch/data/data/multi_aligned_data.py,21,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nData consisting of multiple aligned parts.\n""""""\nfrom typing import (Any, Callable, Dict, List, Optional, Tuple, Union)\n\nimport torch\nfrom texar.torch.data.data.data_base import (\n    DatasetBase, DataSource,\n    FilterDataSource, ZipDataSource, SequenceDataSource)\nfrom texar.torch.data.data.dataset_utils import Batch, connect_name\nfrom texar.torch.data.data.mono_text_data import (\n    MonoTextData, _LengthFilterMode, _default_mono_text_dataset_hparams)\nfrom texar.torch.data.data.record_data import (\n    PickleDataSource, RecordData, _default_record_dataset_hparams)\nfrom texar.torch.data.data.scalar_data import (\n    ScalarData, _default_scalar_dataset_hparams)\nfrom texar.torch.data.data.text_data_base import (\n    TextDataBase, TextLineDataSource)\nfrom texar.torch.data.embedding import Embedding\nfrom texar.torch.data.vocabulary import SpecialTokens, Vocab\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.utils import utils, dict_fetch\nfrom texar.torch.utils.dtypes import is_str, get_supported_scalar_types\n\n__all__ = [\n    ""_default_dataset_hparams"",\n    ""MultiAlignedData""\n]\n\n\ndef _is_text_data(data_type):\n    return data_type == ""text""\n\n\ndef _is_scalar_data(data_type):\n    return data_type in get_supported_scalar_types()\n\n\ndef _is_record_data(data_type):\n    return data_type == ""record""\n\n\ndef _default_dataset_hparams(data_type=None):\n    r""""""Returns hyperparameters of a dataset with default values.\n\n    See :meth:`texar.torch.data.MultiAlignedData.default_hparams` for details.\n    """"""\n    if data_type is None:\n        data_type = ""text""\n\n    if _is_text_data(data_type):\n        hparams = _default_mono_text_dataset_hparams()\n        hparams.update({\n            ""data_type"": data_type,\n            ""vocab_share_with"": None,\n            ""embedding_init_share_with"": None,\n            ""processing_share_with"": None,\n        })\n    elif _is_scalar_data(data_type):\n        hparams = _default_scalar_dataset_hparams()\n    elif _is_record_data(data_type):\n        hparams = _default_record_dataset_hparams()\n        hparams.update({\n            ""data_type"": data_type,\n        })\n    else:\n        raise ValueError(f""Invalid data type \'{data_type}\'"")\n    return hparams\n\n\nclass MultiAlignedData(\n    TextDataBase[Tuple[Union[str, Dict[str, Any]], ...],\n                 Tuple[Union[List[str], int, float, Dict[str, Any]], ...]]):\n    r""""""Data consisting of multiple aligned parts.\n\n    Args:\n        hparams (dict): Hyperparameters. See :meth:`default_hparams` for the\n            defaults.\n        device: The device of the produced batches. For GPU training, set to\n            current CUDA device.\n\n    The processor can read any number of parallel fields as specified in\n    the ""datasets"" list of :attr:`hparams`, and result in a Dataset whose\n    element is a python `dict` containing data fields from each of the\n    specified datasets. Fields from a text dataset or Record dataset have\n    names prefixed by its :attr:`""data_name""`. Fields from a scalar dataset are\n    specified by its :attr:`""data_name""`.\n\n    Example:\n\n        .. code-block:: python\n\n            hparams={\n                \'datasets\': [\n                    {\'files\': \'a.txt\', \'vocab_file\': \'v.a\', \'data_name\': \'x\'},\n                    {\'files\': \'b.txt\', \'vocab_file\': \'v.b\', \'data_name\': \'y\'},\n                    {\'files\': \'c.txt\', \'data_type\': \'int\', \'data_name\': \'z\'}\n                ]\n                \'batch_size\': 1\n            }\n            data = MultiAlignedData(hparams)\n            iterator = DataIterator(data)\n\n            for batch in iterator:\n                # batch contains the following\n                # batch == {\n                #    \'x_text\': [[\'<BOS>\', \'x\', \'sequence\', \'<EOS>\']],\n                #    \'x_text_ids\': [[\'1\', \'5\', \'10\', \'2\']],\n                #    \'x_length\': [4]\n                #    \'y_text\': [[\'<BOS>\', \'y\', \'sequence\', \'1\', \'<EOS>\']],\n                #    \'y_text_ids\': [[\'1\', \'6\', \'10\', \'20\', \'2\']],\n                #    \'y_length\': [5],\n                #    \'z\': [1000],\n                # }\n\n            ...\n\n            hparams={\n                \'datasets\': [\n                    {\'files\': \'d.txt\', \'vocab_file\': \'v.d\', \'data_name\': \'m\'},\n                    {\n                        \'files\': \'d.tfrecord\',\n                        \'data_type\': \'tf_record\',\n                        ""feature_types"": {\n                            \'image\': [\'tf.string\', \'stacked_tensor\']\n                        },\n                        \'image_options\': {\n                            \'image_feature_name\': \'image\',\n                            \'resize_height\': 512,\n                            \'resize_width\': 512,\n                        },\n                        \'data_name\': \'t\',\n                    }\n                ]\n                \'batch_size\': 1\n            }\n            data = MultiAlignedData(hparams)\n            iterator = DataIterator(data)\n            for batch in iterator:\n                # batch contains the following\n                # batch_ == {\n                #    \'x_text\': [[\'<BOS>\', \'NewYork\', \'City\', \'Map\', \'<EOS>\']],\n                #    \'x_text_ids\': [[\'1\', \'100\', \'80\', \'65\', \'2\']],\n                #    \'x_length\': [5],\n                #\n                #    # ""t_image"" is a list of a ""numpy.ndarray"" image\n                #    # in this example. Its width is equal to 512 and\n                #    # its height is equal to 512.\n                #    \'t_image\': [...]\n                # }\n\n    """"""\n\n    def __init__(self, hparams, device: Optional[torch.device] = None):\n        self._hparams = HParams(hparams, self.default_hparams())\n        # Defaultizes hyperparameters of each dataset\n        datasets_hparams = self._hparams.datasets\n        defaultized_datasets_hparams = []\n        for hparams_i in datasets_hparams:\n            data_type = hparams_i.get(""data_type"", None)\n            defaultized_ds_hpms = HParams(hparams_i,\n                                          _default_dataset_hparams(data_type))\n            defaultized_datasets_hparams.append(defaultized_ds_hpms)\n        self._hparams.datasets = defaultized_datasets_hparams\n\n        self._vocab = self.make_vocab(self._hparams.datasets)\n        self._embedding = self.make_embedding(\n            self._hparams.datasets, self._vocab)\n\n        dummy_source = SequenceDataSource[Any]([])\n        name_prefix: List[str] = []\n        self._names: List[Dict[str, Any]] = []\n        sources: List[DataSource] = []\n        filters: List[Optional[Callable[[str], bool]]] = []\n        self._databases: List[DatasetBase] = []\n        for idx, hparams_i in enumerate(self._hparams.datasets):\n            data_type = hparams_i.data_type\n            source_i: DataSource\n\n            if _is_text_data(data_type):\n                source_i = TextLineDataSource(\n                    hparams_i.files,\n                    compression_type=hparams_i.compression_type,\n                    delimiter=hparams_i.delimiter)\n                sources.append(source_i)\n                if ((hparams_i.length_filter_mode ==\n                     _LengthFilterMode.DISCARD.value) and\n                        hparams_i.max_seq_length is not None):\n\n                    def _get_filter(max_seq_length):\n                        return lambda x: len(x) <= max_seq_length\n\n                    filters.append(_get_filter(hparams_i.max_seq_length))\n                else:\n                    filters.append(None)\n\n                self._names.append({\n                    field: connect_name(hparams_i.data_name, field)\n                    for field in [""text"", ""text_ids"", ""length""]\n                })\n\n                dataset_hparams = dict_fetch(\n                    hparams_i, MonoTextData.default_hparams()[""dataset""])\n                dataset_hparams[""data_name""] = None\n                self._databases.append(MonoTextData(\n                    hparams={""dataset"": dataset_hparams}, device=device,\n                    vocab=self._vocab[idx],\n                    embedding=self._embedding[idx],\n                    data_source=dummy_source))\n            elif _is_scalar_data(data_type):\n                source_i = TextLineDataSource(\n                    hparams_i.files,\n                    compression_type=hparams_i.compression_type)\n                sources.append(source_i)\n                filters.append(None)\n                self._names.append({""data"": hparams_i.data_name})\n\n                dataset_hparams = dict_fetch(\n                    hparams_i, ScalarData.default_hparams()[""dataset""])\n                dataset_hparams[""data_name""] = ""data""\n                self._databases.append(ScalarData(\n                    hparams={""dataset"": dataset_hparams}, device=device,\n                    data_source=dummy_source))\n            elif _is_record_data(data_type):\n                source_i = PickleDataSource(file_paths=hparams_i.files)\n                sources.append(source_i)\n                # TODO: Only check `feature_types` when we finally remove\n                #   `feature_original_types`.\n                feature_types = (hparams_i.feature_types or\n                                 hparams_i.feature_original_types)\n                self._names.append({\n                    name: connect_name(hparams_i.data_name, name)\n                    for name in feature_types.keys()})\n                filters.append(None)\n\n                dataset_hparams = dict_fetch(\n                    hparams_i, RecordData.default_hparams()[""dataset""])\n                self._databases.append(RecordData(\n                    hparams={""dataset"": dataset_hparams}, device=device,\n                    data_source=dummy_source))\n            else:\n                raise ValueError(f""Unknown data type: {hparams_i.data_type}"")\n\n            # check for duplicate names\n            for i in range(1, len(name_prefix)):\n                if name_prefix[i] in name_prefix[:i - 1]:\n                    raise ValueError(f""Duplicate data name: {name_prefix[i]}"")\n\n            name_prefix.append(hparams_i[""data_name""])\n\n        self._name_to_id = {v: k for k, v in enumerate(name_prefix)}\n\n        data_source: DataSource = ZipDataSource(*sources)\n\n        if any(filters):\n            def filter_fn(data):\n                return all(fn(data) for fn, data in zip(filters, data)\n                           if fn is not None)\n\n            data_source = FilterDataSource(data_source, filter_fn=filter_fn)\n        super().__init__(data_source, self._hparams, device)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of default hyperparameters:\n\n        .. code-block:: python\n\n            {\n                # (1) Hyperparams specific to text dataset\n                ""datasets"": []\n                # (2) General hyperparams\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""name"": ""multi_aligned_data"",\n            }\n\n        Here:\n\n        1. ""datasets"" is a list of `dict` each of which specifies a\n           dataset which can be text, scalar or Record. The :attr:`""data_name""`\n           field of each dataset is used as the name prefix of the data fields\n           from the respective dataset. The :attr:`""data_name""` field of each\n           dataset should not be the same.\n\n           i) For scalar dataset, the allowed hyperparameters and default\n              values are the same as the ""dataset"" field of\n              :meth:`texar.torch.data.ScalarData.default_hparams`. Note that\n              :attr:`""data_type""` must be explicitly specified\n              (either ""int"" or ""float"").\n\n           ii) For Record dataset, the allowed hyperparameters and default\n               values are the same as the ""dataset"" field of\n               :meth:`texar.torch.data.RecordData.default_hparams`. Note that\n               :attr:`""data_type""` must be explicitly specified (""record"").\n\n           iii) For text dataset, the allowed hyperparameters and default\n                values are the same as the ""dataset"" filed of\n                :meth:`texar.torch.data.MonoTextData.default_hparams`, with\n                several extra hyperparameters:\n\n                `""data_type""`: str\n                    The type of the dataset, one of {""text"", ""int"", ""float"",\n                    ""record""}. If set to ""int"" or ""float"", the dataset is\n                    considered to be a scalar dataset. If set to\n                    ""record"", the dataset is considered to be a Record\n                    dataset.\n\n                    If not specified or set to ""text"", the dataset is\n                    considered to be a text dataset.\n\n                `""vocab_share_with""`: int, optional\n                    Share the vocabulary of a preceding text dataset with\n                    the specified index in the list (starting from 0). The\n                    specified dataset must be a text dataset, and must have\n                    an index smaller than the current dataset.\n\n                    If specified, the vocab file of current dataset is\n                    ignored. Default is `None` which disables the vocab\n                    sharing.\n\n                `""embedding_init_share_with""`: int, optional\n                    Share the embedding initial value of a preceding text\n                    dataset with the specified index in the list (starting\n                    from 0). The specified dataset must be a text dataset,\n                    and must have an index smaller than the current dataset.\n\n                    If specified, the :attr:`""embedding_init""` field of the\n                    current dataset is ignored. Default is `None` which\n                    disables the initial value sharing.\n\n                `""processing_share_with""`: int, optional\n                    Share the processing configurations of a preceding text\n                    dataset with the specified index in the list (starting\n                    from 0). The specified dataset must be a text dataset,\n                    and must have an index smaller than the current dataset.\n\n                    If specified, relevant field of the current dataset are\n                    ignored, including `delimiter`, `bos_token`,\n                    `eos_token`, and ""other_transformations"". Default is\n                    `None` which disables the processing sharing.\n\n        2. For the **general** hyperparameters, see\n        :meth:`texar.torch.data.DatasetBase.default_hparams` for details.\n\n        """"""\n        hparams = TextDataBase.default_hparams()\n        hparams[""name""] = ""multi_aligned_data""\n        hparams[""datasets""] = []\n        return hparams\n\n    def to(self, device: Optional[torch.device]):\n        for dataset in self._databases:\n            dataset.to(device)\n        return super().to(device)\n\n    @staticmethod\n    def _raise_sharing_error(err_data, share_data, hparam_name):\n        raise ValueError(\n            f""Must only share specifications with a preceding dataset. ""\n            f""Dataset {err_data} has \'{hparam_name}={share_data:d}\'"")\n\n    @staticmethod\n    def make_vocab(hparams: List[HParams]) -> List[Optional[Vocab]]:\n        r""""""Makes a list of vocabs based on the hyperparameters.\n\n        Args:\n            hparams (list): A list of dataset hyperparameters.\n\n        Returns:\n            A list of :class:`texar.torch.data.Vocab` instances. Some instances\n            may be the same objects if they are set to be shared and have\n            the same other configurations.\n        """"""\n        vocabs: List[Optional[Vocab]] = []\n        for i, hparams_i in enumerate(hparams):\n            if not _is_text_data(hparams_i.data_type):\n                vocabs.append(None)\n                continue\n\n            proc_share = hparams_i.processing_share_with\n            if proc_share is not None:\n                bos_token = hparams[proc_share].bos_token\n                eos_token = hparams[proc_share].eos_token\n            else:\n                bos_token = hparams_i.bos_token\n                eos_token = hparams_i.eos_token\n            bos_token = utils.default_str(bos_token, SpecialTokens.BOS)\n            eos_token = utils.default_str(eos_token, SpecialTokens.EOS)\n\n            vocab_share = hparams_i.vocab_share_with\n            if vocab_share is not None:\n                if vocab_share >= i:\n                    MultiAlignedData._raise_sharing_error(\n                        i, vocab_share, ""vocab_share_with"")\n                if vocabs[vocab_share] is None:\n                    raise ValueError(\n                        f""Cannot share vocab with dataset {vocab_share} which ""\n                        ""does not have a vocab."")\n                if (bos_token == vocabs[vocab_share].bos_token and\n                        eos_token == vocabs[vocab_share].eos_token):\n                    vocab = vocabs[vocab_share]\n                else:\n                    vocab = Vocab(hparams[vocab_share].vocab_file,\n                                  bos_token=bos_token, eos_token=eos_token)\n            else:\n                vocab = Vocab(hparams_i.vocab_file,\n                              bos_token=bos_token, eos_token=eos_token)\n            vocabs.append(vocab)\n\n        return vocabs\n\n    @staticmethod\n    def make_embedding(hparams: List[HParams], vocabs: List[Optional[Vocab]]) \\\n            -> List[Optional[Embedding]]:\n        r""""""Optionally loads embeddings from files (if provided), and\n        returns respective :class:`texar.torch.data.Embedding` instances.\n        """"""\n        embeddings: List[Optional[Embedding]] = []\n        for i, hparams_i in enumerate(hparams):\n            if not _is_text_data(hparams_i.data_type):\n                embeddings.append(None)\n                continue\n\n            emb_share = hparams_i.embedding_init_share_with\n            if emb_share is not None:\n                if emb_share >= i:\n                    MultiAlignedData._raise_sharing_error(\n                        i, emb_share, ""embedding_init_share_with"")\n                if not embeddings[emb_share]:\n                    raise ValueError(\n                        f""Cannot share embedding with dataset {emb_share} ""\n                        ""which does not have an embedding."")\n                if emb_share != hparams_i.vocab_share_with:\n                    raise ValueError(\n                        ""\'embedding_init_share_with\' != \'vocab_share_with\'.""\n                        ""\'embedding_init\' can be shared only when vocab is""\n                        ""shared."")\n                emb = embeddings[emb_share]\n            else:\n                emb = None\n                emb_file = hparams_i.embedding_init.file\n                vocab = vocabs[i]\n                if emb_file and emb_file != """":\n                    assert vocab is not None\n                    emb = Embedding(vocab.token_to_id_map_py,\n                                    hparams_i.embedding_init)\n            embeddings.append(emb)\n\n        return embeddings\n\n    def process(self, raw_example: Tuple[Union[str, Dict[str, Any]], ...]) \\\n            -> Tuple[Union[List[str], int, float, Dict[str, Any]], ...]:\n        processed_examples = []\n        for i, raw_example_i in enumerate(raw_example):\n            example_i = self._databases[i].process(raw_example_i)\n            processed_examples.append(example_i)\n        return tuple(processed_examples)\n\n    def collate(self, examples) -> Batch:\n        transposed_examples = map(list, zip(*examples))\n        batch: Dict[str, Any] = {}\n        for i, transposed_example in enumerate(transposed_examples):\n            kth_batch = self._databases[i].collate(transposed_example)\n            for key, name in self._names[i].items():\n                batch.update({name: kth_batch[key]})\n        return Batch(len(examples), batch=batch)\n\n    def list_items(self):\n        r""""""Returns the list of item names that the data can produce.\n\n        Returns:\n            A list of strings.\n        """"""\n        return [value for name in self._names for value in name.values()]\n\n    def _maybe_name_to_id(self, name_or_id):\n        if is_str(name_or_id):\n            if name_or_id not in self._name_to_id:\n                raise ValueError(""Unknown data name: {}"".format(name_or_id))\n            return self._name_to_id[name_or_id]\n        return name_or_id\n\n    def vocab(self, name_or_id):\n        r""""""Returns the :class:`~texar.torch.data.Vocab` of text dataset by its\n        name or id. `None` if the dataset is not of text type.\n\n        Args:\n            name_or_id (str or int): Data name or the index of text dataset.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        return self._vocab[i]\n\n    def embedding_init_value(self, name_or_id):\n        r""""""Returns the `Tensor` of embedding initial value of the\n        dataset by its name or id. `None` if the dataset is not of text type.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        return self._embedding[i]\n\n    def text_name(self, name_or_id):\n        r""""""The name of text tensor of text dataset by its name or id. If the\n        dataset is not of text type, returns `None`.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        if not _is_text_data(self._hparams.datasets[i][""data_type""]):\n            return None\n        return self._names[i][""text""]\n\n    def length_name(self, name_or_id):\n        r""""""The name of length tensor of text dataset by its name or id. If the\n        dataset is not of text type, returns `None`.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        if not _is_text_data(self._hparams.datasets[i][""data_type""]):\n            return None\n        return self._names[i][""length""]\n\n    def text_id_name(self, name_or_id):\n        r""""""The name of length tensor of text dataset by its name or id. If the\n        dataset is not of text type, returns `None`.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        if not _is_text_data(self._hparams.datasets[i][""data_type""]):\n            return None\n        return self._names[i][""text_ids""]\n\n    def data_name(self, name_or_id):\n        r""""""The name of the data tensor of scalar dataset by its name or id..\n        If the dataset is not a scalar data, returns `None`.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        if not _is_scalar_data(self._hparams.datasets[i][""data_type""]):\n            return None\n        return self._names[i][""label""]\n'"
texar/torch/data/data/paired_text_data.py,21,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPaired text data that consists of source text and target text.\n""""""\nimport math\nfrom typing import List, Optional, Tuple\n\nimport torch\n\nfrom texar.torch.data.data.data_base import (\n    DataSource, FilterDataSource, ZipDataSource)\nfrom texar.torch.data.data.dataset_utils import Batch, padded_batch\nfrom texar.torch.data.data.mono_text_data import (\n    MonoTextData, _LengthFilterMode, _default_mono_text_dataset_hparams)\nfrom texar.torch.data.data.text_data_base import (\n    TextDataBase, TextLineDataSource)\nfrom texar.torch.data.embedding import Embedding\nfrom texar.torch.data.vocabulary import SpecialTokens, Vocab\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.utils import utils\n\n__all__ = [\n    ""_default_paired_text_dataset_hparams"",\n    ""PairedTextData"",\n]\n\n\ndef _default_paired_text_dataset_hparams():\n    r""""""Returns hyperparameters of a paired text dataset with default values.\n\n    See :meth:`texar.torch.data.PairedTextData.default_hparams` for details.\n    """"""\n    source_hparams = _default_mono_text_dataset_hparams()\n    source_hparams[""bos_token""] = None\n    source_hparams[""data_name""] = ""source""\n    target_hparams = _default_mono_text_dataset_hparams()\n    target_hparams.update(\n        {\n            ""vocab_share"": False,\n            ""embedding_init_share"": False,\n            ""processing_share"": False,\n            ""data_name"": ""target""\n        }\n    )\n    return {\n        ""source_dataset"": source_hparams,\n        ""target_dataset"": target_hparams\n    }\n\n\nclass PairedTextData(TextDataBase[Tuple[List[str], List[str]],\n                                  Tuple[List[str], List[str]]]):\n    r""""""Text data processor that reads parallel source and target text.\n    This can be used in, e.g., seq2seq models.\n\n    Args:\n        hparams (dict): Hyperparameters. See :meth:`default_hparams` for the\n            defaults.\n        device: The device of the produced batches. For GPU training, set to\n            current CUDA device.\n\n    By default, the processor reads raw data files, performs tokenization,\n    batching and other pre-processing steps, and results in a Dataset\n    whose element is a python `dict` including six fields:\n\n    ""source_text"":\n        A list of ``[batch_size]`` elements each containing a list of\n        **raw** text tokens of source sequences. Short sequences in the\n        batch are padded with **empty string**. By default only ``EOS``\n        token is appended to each sequence. Out-of-vocabulary tokens are\n        **NOT** replaced with ``UNK``.\n    ""source_text_ids"":\n        A list of ``[batch_size]`` elements each containing a list of token\n        indexes of source sequences in the batch.\n    ""source_length"":\n        A list of ``[batch_size]`` elements of integers containing the length\n        of each source sequence in the batch.\n    ""target_text"":\n        A list same as ""source_text"" but for target sequences. By default\n        both BOS and EOS are added.\n    ""target_text_ids"":\n        A list same as ""source_text_ids"" but for target sequences.\n    ""target_length"":\n        An list same as ""source_length"" but for target sequences.\n\n    The above field names can be accessed through :attr:`source_text_name`,\n    :attr:`source_text_id_name`, :attr:`source_length_name`, and those prefixed\n    with ``target_``, respectively.\n\n    Example:\n\n    .. code-block:: python\n\n        hparams={\n            \'source_dataset\': {\'files\': \'s\', \'vocab_file\': \'vs\'},\n            \'target_dataset\': {\'files\': [\'t1\', \'t2\'], \'vocab_file\': \'vt\'},\n            \'batch_size\': 1\n        }\n        data = PairedTextData(hparams)\n        iterator = DataIterator(data)\n\n        for batch in iterator:\n            # batch contains the following\n            # batch_ == {\n            #    \'source_text\': [[\'source\', \'sequence\', \'<EOS>\']],\n            #    \'source_text_ids\': [[5, 10, 2]],\n            #    \'source_length\': [3]\n            #    \'target_text\': [[\'<BOS>\', \'target\', \'sequence\', \'1\',\n                                \'<EOS>\']],\n            #    \'target_text_ids\': [[1, 6, 10, 20, 2]],\n            #    \'target_length\': [5]\n            # }\n\n    """"""\n\n    def __init__(self, hparams, device: Optional[torch.device] = None):\n        self._hparams = HParams(hparams, self.default_hparams())\n\n        src_hparams = self.hparams.source_dataset\n        tgt_hparams = self.hparams.target_dataset\n\n        # create vocabulary\n        self._src_bos_token = src_hparams[""bos_token""]\n        self._src_eos_token = src_hparams[""eos_token""]\n        self._src_transforms = src_hparams[""other_transformations""]\n        self._src_vocab = Vocab(src_hparams.vocab_file,\n                                bos_token=src_hparams.bos_token,\n                                eos_token=src_hparams.eos_token)\n\n        if tgt_hparams[""processing_share""]:\n            self._tgt_bos_token = src_hparams[""bos_token""]\n            self._tgt_eos_token = src_hparams[""eos_token""]\n        else:\n            self._tgt_bos_token = tgt_hparams[""bos_token""]\n            self._tgt_eos_token = tgt_hparams[""eos_token""]\n        tgt_bos_token = utils.default_str(self._tgt_bos_token,\n                                          SpecialTokens.BOS)\n        tgt_eos_token = utils.default_str(self._tgt_eos_token,\n                                          SpecialTokens.EOS)\n        if tgt_hparams[""vocab_share""]:\n            if tgt_bos_token == self._src_vocab.bos_token and \\\n                    tgt_eos_token == self._src_vocab.eos_token:\n                self._tgt_vocab = self._src_vocab\n            else:\n                self._tgt_vocab = Vocab(src_hparams[""vocab_file""],\n                                        bos_token=tgt_bos_token,\n                                        eos_token=tgt_eos_token)\n        else:\n            self._tgt_vocab = Vocab(tgt_hparams[""vocab_file""],\n                                    bos_token=tgt_bos_token,\n                                    eos_token=tgt_eos_token)\n\n        # create embeddings\n        self._src_embedding = MonoTextData.make_embedding(\n            src_hparams.embedding_init, self._src_vocab.token_to_id_map_py)\n\n        if self._hparams.target_dataset.embedding_init_share:\n            self._tgt_embedding = self._src_embedding\n        else:\n            tgt_emb_file = tgt_hparams.embedding_init[""file""]\n            self._tgt_embedding = None\n            if tgt_emb_file is not None and tgt_emb_file != """":\n                self._tgt_embedding = MonoTextData.make_embedding(\n                    self._tgt_vocab.token_to_id_map_py,\n                    tgt_hparams.embedding_init)\n\n        # create data source\n        self._src_delimiter = src_hparams.delimiter\n        self._src_max_seq_length = src_hparams.max_seq_length\n        self._src_length_filter_mode = _LengthFilterMode(\n            src_hparams.length_filter_mode)\n        self._src_pad_length = self._src_max_seq_length\n        if self._src_pad_length is not None:\n            self._src_pad_length += sum(int(x is not None and x != \'\')\n                                        for x in [src_hparams.bos_token,\n                                                  src_hparams.eos_token])\n\n        src_data_source = TextLineDataSource(\n            src_hparams.files, compression_type=src_hparams.compression_type)\n\n        self._tgt_transforms = tgt_hparams[""other_transformations""]\n        self._tgt_delimiter = tgt_hparams.delimiter\n        self._tgt_max_seq_length = tgt_hparams.max_seq_length\n        self._tgt_length_filter_mode = _LengthFilterMode(\n            tgt_hparams.length_filter_mode)\n        self._tgt_pad_length = self._tgt_max_seq_length\n        if self._tgt_pad_length is not None:\n            self._tgt_pad_length += sum(int(x is not None and x != \'\')\n                                        for x in [tgt_hparams.bos_token,\n                                                  tgt_hparams.eos_token])\n\n        tgt_data_source = TextLineDataSource(\n            tgt_hparams.files, compression_type=tgt_hparams.compression_type)\n\n        data_source: DataSource[Tuple[List[str], List[str]]]\n        data_source = ZipDataSource(  # type: ignore\n            src_data_source, tgt_data_source)\n        if ((self._src_length_filter_mode is _LengthFilterMode.DISCARD and\n             self._src_max_seq_length is not None) or\n                (self._tgt_length_filter_mode is _LengthFilterMode.DISCARD and\n                 self._tgt_length_filter_mode is not None)):\n            max_source_length = self._src_max_seq_length or math.inf\n            max_tgt_length = self._tgt_max_seq_length or math.inf\n\n            def filter_fn(raw_example):\n                return (len(raw_example[0]) <= max_source_length and\n                        len(raw_example[1]) <= max_tgt_length)\n\n            data_source = FilterDataSource(data_source, filter_fn)\n\n        super().__init__(data_source, hparams, device=device)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of default hyperparameters.\n\n        .. code-block:: python\n\n            {\n                # (1) Hyperparams specific to text dataset\n                ""source_dataset"": {\n                    ""files"": [],\n                    ""compression_type"": None,\n                    ""vocab_file"": """",\n                    ""embedding_init"": {},\n                    ""delimiter"": None,\n                    ""max_seq_length"": None,\n                    ""length_filter_mode"": ""truncate"",\n                    ""pad_to_max_seq_length"": False,\n                    ""bos_token"": None,\n                    ""eos_token"": ""<EOS>"",\n                    ""other_transformations"": [],\n                    ""variable_utterance"": False,\n                    ""utterance_delimiter"": ""|||"",\n                    ""max_utterance_cnt"": 5,\n                    ""data_name"": ""source"",\n                },\n                ""target_dataset"": {\n                    # ...\n                    # Same fields are allowed as in ""source_dataset"" with the\n                    # same default values, except the\n                    # following new fields/values:\n                    ""bos_token"": ""<BOS>""\n                    ""vocab_share"": False,\n                    ""embedding_init_share"": False,\n                    ""processing_share"": False,\n                    ""data_name"": ""target""\n                }\n                # (2) General hyperparams\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""name"": ""paired_text_data"",\n                # (3) Bucketing\n                ""bucket_boundaries"": [],\n                ""bucket_batch_sizes"": None,\n                ""bucket_length_fn"": None,\n            }\n\n        Here:\n\n        1. Hyperparameters in the :attr:`""source_dataset""` and\n           :attr:`""target_dataset""` fields have the same definition as those\n           in :meth:`texar.torch.data.MonoTextData.default_hparams`, for source\n           and target text, respectively.\n\n           For the new hyperparameters in :attr:`""target_dataset""`:\n\n           `""vocab_share""`: bool\n               Whether to share the vocabulary of source.\n               If `True`, the vocab file of target is ignored.\n\n           `""embedding_init_share""`: bool\n               Whether to share the embedding initial value of source. If\n               `True`, :attr:`""embedding_init""` of target is ignored.\n\n              :attr:`""vocab_share""` must be true to share the embedding\n              initial value.\n\n           `""processing_share""`: bool\n               Whether to share the processing configurations of source,\n               including `""delimiter""`, `""bos_token""`, `""eos_token""`, and\n               `""other_transformations""`.\n\n        2. For the **general** hyperparameters, see\n           :meth:`texar.torch.data.DatasetBase.default_hparams` for details.\n\n        3. For **bucketing** hyperparameters, see\n           :meth:`texar.torch.data.MonoTextData.default_hparams` for details,\n           except that the default `""bucket_length_fn""` is the maximum sequence\n           length of source and target sequences.\n\n           .. warning::\n               Bucketing is not yet supported. These options will be ignored.\n\n        """"""\n        hparams = TextDataBase.default_hparams()\n        hparams[""name""] = ""paired_text_data""\n        hparams.update(_default_paired_text_dataset_hparams())\n        return hparams\n\n    @staticmethod\n    def make_embedding(src_emb_hparams, src_token_to_id_map,\n                       tgt_emb_hparams=None, tgt_token_to_id_map=None,\n                       emb_init_share=False):\n        r""""""Optionally loads source and target embeddings from files (if\n        provided), and returns respective :class:`texar.torch.data.Embedding`\n        instances.\n        """"""\n        src_embedding = MonoTextData.make_embedding(\n            src_emb_hparams, src_token_to_id_map)\n\n        if emb_init_share:\n            tgt_embedding = src_embedding\n        else:\n            tgt_emb_file = tgt_emb_hparams[""file""]\n            tgt_embedding = None\n            if tgt_emb_file is not None and tgt_emb_file != """":\n                tgt_embedding = Embedding(tgt_token_to_id_map, tgt_emb_hparams)\n\n        return src_embedding, tgt_embedding\n\n    def process(self, raw_example: Tuple[List[str], List[str]]) -> \\\n            Tuple[List[str], List[str]]:\n        # `_process` truncates sentences and appends BOS/EOS tokens.\n        src_words = raw_example[0]\n        if (self._src_max_seq_length is not None and\n                len(src_words) > self._src_max_seq_length):\n            if self._src_length_filter_mode is _LengthFilterMode.TRUNC:\n                src_words = src_words[:self._src_max_seq_length]\n\n        if self._src_bos_token is not None and self._src_bos_token != \'\':\n            src_words.insert(0, self._src_bos_token)\n        if self._src_eos_token is not None and self._src_eos_token != \'\':\n            src_words.append(self._src_eos_token)\n\n        # apply the transformations to source\n        for transform in self._src_transforms:\n            src_words = transform(src_words)\n\n        tgt_words = raw_example[1]\n        if (self._tgt_max_seq_length is not None and\n                len(tgt_words) > self._tgt_max_seq_length):\n            if self._tgt_length_filter_mode is _LengthFilterMode.TRUNC:\n                tgt_words = tgt_words[:self._tgt_max_seq_length]\n\n        if self._tgt_bos_token is not None and self._tgt_bos_token != \'\':\n            tgt_words.insert(0, self._tgt_bos_token)\n        if self._tgt_eos_token is not None and self._tgt_eos_token != \'\':\n            tgt_words.append(self._tgt_eos_token)\n\n        # apply the transformations to target\n        for transform in self._tgt_transforms:\n            tgt_words = transform(tgt_words)\n\n        return src_words, tgt_words\n\n    @staticmethod\n    def _get_name_prefix(src_hparams, tgt_hparams):\n        name_prefix = [\n            src_hparams[""data_name""], tgt_hparams[""data_name""]]\n        if name_prefix[0] == name_prefix[1]:\n            raise ValueError(""\'data_name\' of source and target ""\n                             ""datasets cannot be the same."")\n        return name_prefix\n\n    def collate(self, examples: List[Tuple[List[str], List[str]]]) -> Batch:\n        # For `PairedTextData`, each example is represented as a tuple of list\n        # of strings.\n        # `_collate` takes care of padding and numericalization.\n\n        # If `pad_length` is `None`, pad to the longest sentence in the batch.\n        src_examples = [example[0] for example in examples]\n        source_ids = [self._src_vocab.map_tokens_to_ids_py(sent) for sent\n                      in src_examples]\n        source_ids, source_lengths = \\\n            padded_batch(source_ids,\n                         self._src_pad_length,\n                         pad_value=self._src_vocab.pad_token_id)\n        src_pad_length = self._src_pad_length or max(source_lengths)\n        src_examples = [\n            sent + [\'\'] * (src_pad_length - len(sent))\n            if len(sent) < src_pad_length else sent\n            for sent in src_examples\n        ]\n\n        source_ids = torch.from_numpy(source_ids)\n        source_lengths = torch.tensor(source_lengths, dtype=torch.long)\n\n        tgt_examples = [example[1] for example in examples]\n        target_ids = [self._tgt_vocab.map_tokens_to_ids_py(sent) for sent\n                      in tgt_examples]\n        target_ids, target_lengths = \\\n            padded_batch(target_ids,\n                         self._tgt_pad_length,\n                         pad_value=self._tgt_vocab.pad_token_id)\n        tgt_pad_length = self._tgt_pad_length or max(target_lengths)\n        tgt_examples = [\n            sent + [\'\'] * (tgt_pad_length - len(sent))\n            if len(sent) < tgt_pad_length else sent\n            for sent in tgt_examples\n        ]\n\n        target_ids = torch.from_numpy(target_ids)\n        target_lengths = torch.tensor(target_lengths, dtype=torch.long)\n\n        return Batch(len(examples), source_text=src_examples,\n                     source_text_ids=source_ids, source_length=source_lengths,\n                     target_text=tgt_examples, target_text_ids=target_ids,\n                     target_length=target_lengths)\n\n    def list_items(self) -> List[str]:\n        r""""""Returns the list of item names that the data can produce.\n\n        Returns:\n            A list of strings.\n        """"""\n        items = [\'text\', \'text_ids\', \'length\']\n        src_name = self._hparams.source_dataset[\'data_name\']\n        tgt_name = self._hparams.target_dataset[\'data_name\']\n\n        if src_name is not None:\n            src_items = [src_name + \'_\' + item for item in items]\n        else:\n            src_items = items\n\n        if tgt_name is not None:\n            tgt_items = [tgt_name + \'_\' + item for item in items]\n        else:\n            tgt_items = items\n\n        return src_items + tgt_items\n\n    @property\n    def vocab(self):\n        r""""""A pair instances of :class:`~texar.torch.data.Vocab` that are source\n        and target vocabs, respectively.\n        """"""\n        return self._src_vocab, self._tgt_vocab\n\n    @property\n    def source_vocab(self):\n        r""""""The source vocab, an instance of :class:`~texar.torch.data.Vocab`.\n        """"""\n        return self._src_vocab\n\n    @property\n    def target_vocab(self):\n        r""""""The target vocab, an instance of :class:`~texar.torch.data.Vocab`.\n        """"""\n        return self._tgt_vocab\n\n    @property\n    def source_text_name(self):\n        r""""""The name for source text""""""\n        name = ""{}_text"".format(self.hparams.source_dataset[""data_name""])\n        return name\n\n    @property\n    def source_text_id_name(self):\n        r""""""The name for source text id""""""\n        name = ""{}_text_ids"".format(self.hparams.source_dataset[""data_name""])\n        return name\n\n    @property\n    def source_length_name(self):\n        r""""""The name for source length""""""\n        name = ""{}_length"".format(self.hparams.source_dataset[""data_name""])\n        return name\n\n    @property\n    def target_text_name(self):\n        r""""""The name for target text""""""\n        name = ""{}_text"".format(self.hparams.target_dataset[""data_name""])\n        return name\n\n    @property\n    def target_text_id_name(self):\n        r""""""The name for target text id""""""\n        name = ""{}_text_ids"".format(self.hparams.target_dataset[""data_name""])\n        return name\n\n    @property\n    def target_length_name(self):\n        r""""""The name for target length""""""\n        name = ""{}_length"".format(self.hparams.target_dataset[""data_name""])\n        return name\n\n    def embedding_init_value(self):\n        r""""""A pair of `Tensor` containing the embedding values of source and\n        target data loaded from file.\n        """"""\n        src_emb = self.hparams.source_dataset[""embedding_init""]\n        tgt_emb = self.hparams.target_dataser[""embedding_init""]\n        return src_emb, tgt_emb\n'"
texar/torch/data/data/record_data.py,22,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nData class that supports reading pickled data as record structures.\n""""""\nimport copy\nimport io\nimport pickle\nimport warnings\nfrom enum import Enum\nfrom typing import (\n    Any, Callable, Dict, List, NamedTuple, Optional, Tuple, TypeVar, Union)\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.data.data.data_base import DatasetBase, DataSource\nfrom texar.torch.data.data.dataset_utils import Batch, padded_batch\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.utils.dtypes import get_numpy_dtype\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""_default_record_dataset_hparams"",\n    ""PickleDataSource"",\n    ""RecordData"",\n]\n\n\ndef _default_record_dataset_hparams():\n    r""""""Returns hyperparameters of a record dataset with default values.\n\n    See :meth:`texar.torch.data.RecordData.default_hparams` for details.\n    """"""\n    return {\n        ""files"": [],\n        ""feature_types"": None,\n        ""feature_original_types"": None,\n        ""feature_convert_types"": {},\n        ""image_options"": {},\n        ""compression_type"": None,\n        ""other_transformations"": [],\n        ""num_shards"": None,\n        ""shard_id"": None,\n        ""data_name"": None,\n        ""@no_typecheck"": [\n            ""files"",\n            ""feature_types"",\n            ""feature_original_types"",\n            ""feature_convert_types"",\n            ""image_options""\n        ],\n    }\n\n\nRawExample = TypeVar(\'RawExample\')\n\n\nclass PickleDataSource(DataSource[RawExample]):\n    r""""""Data source for reading from (multiple) pickled binary files. Each file\n    could contain multiple pickled objects, and each object is yielded as an\n    example.\n\n    This data source does not support indexing.\n\n    Args:\n        file_paths (str or list[str]): Paths to pickled binary files.\n        lists_are_examples (bool): If `True`, lists will be treated as\n            a single example; if `False`, each element in the list will be\n            treated as separate examples. Default is `True`. Set this to\n            `False` if the entire pickled binary file is a list.\n\n            .. note::\n                It is recommended against storing all examples as a list,\n                because in this case, all examples can only be accessed\n                after the whole list is parsed.\n\n        pickle_kwargs: Additional keyword arguments to pass to\n            :meth:`pickle.load`.\n    """"""\n\n    def __init__(self, file_paths: MaybeList[str],\n                 lists_are_examples: bool = True, **pickle_kwargs):\n        if isinstance(file_paths, str):\n            file_paths = [file_paths]\n        self._file_paths = file_paths\n        self._lists_are_examples = lists_are_examples\n        self._pickle_kwargs = pickle_kwargs\n\n    def __iter__(self):\n        for path in self._file_paths:\n            with open(path, \'rb\') as f:\n                if self._lists_are_examples:\n                    while True:\n                        try:\n                            ex = pickle.load(f, **self._pickle_kwargs)\n                            if isinstance(ex, list):\n                                yield from ex\n                            else:\n                                yield ex\n                        except EOFError:\n                            break\n                else:\n                    while True:\n                        try:\n                            yield pickle.load(f, **self._pickle_kwargs)\n                        except EOFError:\n                            break\n\n\nTransformFn = Callable[[bytes], torch.ByteTensor]\n\n\ndef _create_image_transform(height: Optional[int], width: Optional[int],\n                            resize_method: Union[str, int] = \'bilinear\') \\\n        -> TransformFn:\n    r""""""Create a function based on `Pillow image transforms\n    <https://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.resize>`\n    that performs resizing with desired resize method (interpolation).\n\n    Args:\n        height (int, optional): Height of the transformed image. Set to `None`\n            to not perform resizing.\n        width (int, optional): Width of the transformed image. Set to `None`\n            to not perform resizing.\n        resize_method (str or int, optional): Interpolation method to use.\n            Supported values are ``""nearest""`` (nearest neighbor),\n            ``""bilinear""``, ``""bicubic""``, and ``""lanczos""``. Enum values from\n            PIL (e.g., ``PIL.Image.BILINEAR``) are also supported. Defaults to\n            ``""bilinear""``.\n\n    Returns:\n        The created transformation function.\n    """"""\n    try:\n        import PIL.Image\n    except ImportError:\n        raise ImportError(\n            ""To use image resizing with RecordData, the Pillow library must be ""\n            ""installed. Please see ""\n            ""https://pillow.readthedocs.io/en/stable/installation.html."")\n\n    # We take the final part of a possibly dot-separated string for\n    # compatibility reasons, because in Texar-TF `resize_method` could take the\n    # form of ""tf.image.ResizeMethod.BILINEAR"".\n    if isinstance(resize_method, int):\n        interpolation = resize_method\n    else:\n        method = resize_method.lower().split(\'.\')[-1]\n        if method in [""nearest_neighbor"", ""nearest""]:\n            interpolation = PIL.Image.NEAREST\n        elif method == ""bilinear"":\n            interpolation = PIL.Image.BILINEAR\n        elif method == ""bicubic"":\n            interpolation = PIL.Image.BICUBIC\n        elif method == ""lanczos"":\n            interpolation = PIL.Image.LANCZOS\n        else:\n            raise ValueError(f""Unsupported resize method \'{resize_method}\'"")\n    if height is None or width is None:\n        size = None\n    else:\n        size = (height, width)\n\n    def transform(raw_bytes):\n        image = PIL.Image.open(io.BytesIO(raw_bytes))\n        if size is not None:\n            image = image.resize(size, interpolation)\n\n        # Convert to torch Tensor. Adapted from\n        # torchvision.transform.functional.to_tensor.\n        if image.mode == \'1\':\n            tensor = 255 * torch.from_numpy(\n                np.array(image, np.uint8, copy=False))\n        else:\n            tensor = torch.ByteTensor(\n                torch.ByteStorage.from_buffer(image.tobytes()))\n        # PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK.\n        if image.mode == \'YCbCr\':\n            n_channel = 3\n        elif image.mode == \'I;16\':\n            n_channel = 1\n        else:\n            n_channel = len(image.mode)\n        tensor = tensor.view(image.size[1], image.size[0], n_channel)\n        return tensor\n\n    return transform\n\n\nclass CollateMethod(Enum):\n    StackedTensor = ""stacked_tensor""\n    PaddedTensor = ""padded_tensor""\n    List = ""list""\n\n\nclass FeatureDescription(NamedTuple):\n    r""""""Description of a feature.""""""\n    collate_method: CollateMethod\n    dtype: Optional[np.dtype]\n    shape: Optional[Tuple[int, ...]]\n\n\n_DEPRECATED_NAME_MAPPING = {\n    ""FixedLenFeature"": ""stacked_tensor"",\n    ""FixedLenSequenceFeature"": ""padded_tensor"",\n    ""VarLenFeature"": ""list"",\n}\n\n\ndef _convert_feature_hparams(feature_types: Union[Dict[str, Any], HParams]) \\\n        -> Dict[str, FeatureDescription]:\n    if isinstance(feature_types, HParams):\n        feature_types = feature_types.todict()\n    else:\n        feature_types = copy.deepcopy(feature_types)\n    show_deprecation_warning = False\n    for key, value in feature_types.items():\n        if len(value) > 1 and value[1] in _DEPRECATED_NAME_MAPPING:\n            feature_types[key] = (\n                value[0], _DEPRECATED_NAME_MAPPING[value[1]], *value[2:])\n            show_deprecation_warning = True\n    if show_deprecation_warning:\n        warnings.warn(f""RecordData feature types ""\n                      f""{\', \'.join(repr(x) for x in _DEPRECATED_NAME_MAPPING)} ""\n                      f""are deprecated. Please see RecordData.default_hparams ""\n                      f""for update instructions."", UserWarning)\n\n    features = {}\n    for key, value in feature_types.items():\n        shape: Optional[Tuple[int, ...]] = None\n        if len(value) == 3:\n            if isinstance(value[-1], int):\n                shape = (value[-1],)\n            elif all(isinstance(x, int) for x in value[-1]):\n                shape = tuple(value[-1])\n                if len(shape) == 0:\n                    shape = (1,)  # scalar tensor\n            else:\n                raise ValueError(f""\'shape\' of feature \'{key}\' is not of type ""\n                                 f""int, tuple, or torch.Size"")\n        if len(value) < 2:\n            collate_method = CollateMethod.StackedTensor\n        else:\n            try:\n                collate_method = CollateMethod(value[1])\n            except ValueError:\n                values = [x.value for x in CollateMethod.__members__.values()]\n                raise ValueError(\n                    f""Unsupported feature collate method \'{value[1]}\' for ""\n                    f""feature \'{key}\', only ""\n                    f""{\', \'.join(repr(x) for x in values)} are ""\n                    f""supported as of now."")\n\n        dtype = None\n        if value[0] is not None:\n            dtype = get_numpy_dtype(value[0])\n        elif collate_method is not CollateMethod.List:\n            raise ValueError(f""\'dtype\' for feature \'{key}\' must not be None ""\n                             f""unless collate method is \'list\'"")\n\n        features[key] = FeatureDescription(collate_method, dtype, shape)\n\n    return features\n\n\ndef _check_shape(tensor: np.ndarray, key: str, descriptor: FeatureDescription):\n    if descriptor.shape is None:\n        return\n    # Check whether shape matches.\n    if descriptor.collate_method is CollateMethod.PaddedTensor:\n        shape = tensor.shape[1:]\n    else:\n        shape = tensor.shape\n    if len(shape) == 0:\n        shape = (1,)  # scalar tensor\n\n    if shape != descriptor.shape:\n        if descriptor.collate_method is CollateMethod.PaddedTensor:\n            raise ValueError(\n                f""Expected tensor of shape {(\'any\', *descriptor.shape)} for ""\n                f""feature {key}, but received tensor of shape {tensor.shape}"")\n        else:\n            raise ValueError(\n                f""Expected tensor of shape {descriptor.shape} for ""\n                f""feature {key}, but received tensor of shape {tensor.shape}"")\n\n\nclass RecordData(DatasetBase[Dict[str, Any], Dict[str, Any]]):\n    r""""""Record data which loads and processes pickled files.\n\n    This module can be used to process image data, features, etc.\n\n    Args:\n        hparams (dict): Hyperparameters. See :meth:`default_hparams`\n            for the defaults.\n        device: The device of the produced batches. For GPU training, set to\n            current CUDA device.\n\n    The module reads and restores data from pickled files and results in a\n    dataset whose element is a Python `dict` that maps feature names to feature\n    values. The features names and dtypes are specified in\n    :attr:`hparams.dataset.feature_types`.\n\n    The module also provides simple processing options for image data, such\n    as image resize.\n\n    Example:\n\n        .. code-block:: python\n\n            # Read data from pickled file\n            hparams={\n                \'dataset\': {\n                    \'files\': \'image1.pkl\',\n                    \'feature_types\': {\n                        \'height\': [\'int64\', \'list\'],  # or \'stacked_tensor\'\n                        \'width\': [\'int64\', \'list\'],   # or \'stacked_tensor\'\n                        \'label\': [\'int64\', \'stacked_tensor\'],\n                        \'image_raw\': [\'bytes\', \'stacked_tensor\'],\n                    }\n                },\n                \'batch_size\': 1\n            }\n            data = RecordData(hparams)\n            iterator = DataIterator(data)\n\n            batch = next(iter(iterator))  # get the first batch in dataset\n            # batch == {\n            #    \'data\': {\n            #        \'height\': [239],\n            #        \'width\': [149],\n            #        \'label\': tensor([1]),\n            #\n            #        # \'image_raw\' is a NumPy ndarray of raw image bytes in this\n            #        # example.\n            #        \'image_raw\': [...],\n            #    }\n            # }\n\n        .. code-block:: python\n\n            # Read image data from pickled file and do resizing\n            hparams={\n                \'dataset\': {\n                    \'files\': \'image2.pkl\',\n                    \'feature_types\': {\n                        \'label\': [\'int64\', \'stacked_tensor\'],\n                        \'image_raw\': [\'bytes\', \'stacked_tensor\'],\n                    },\n                    \'image_options\': {\n                        \'image_feature_name\': \'image_raw\',\n                        \'resize_height\': 512,\n                        \'resize_width\': 512,\n                    }\n                },\n                \'batch_size\': 1\n            }\n            data = RecordData(hparams)\n            iterator = DataIterator(data)\n\n            batch = next(iter(iterator))  # get the first batch in dataset\n            # batch == {\n            #    \'data\': {\n            #        \'label\': tensor([1]),\n            #\n            #        # ""image_raw"" is a tensor of image pixel data in this\n            #        # example. Each image has a width of 512 and height of 512.\n            #        \'image_raw\': tensor([...])\n            #    }\n            # }\n\n    """"""\n\n    def __init__(self, hparams=None, device: Optional[torch.device] = None,\n                 data_source: Optional[DataSource] = None):\n        self._hparams = HParams(hparams, self.default_hparams())\n\n        feature_types = self._hparams.dataset.feature_original_types\n        if feature_types is not None:\n            warnings.warn(\n                ""\'feature_original_types\' of RecordData is deprecated. Please ""\n                ""see default_hparams of RecordData for update instructions"")\n        if self._hparams.dataset.feature_types is not None:\n            feature_types = self._hparams.dataset.feature_types\n        elif feature_types is None:\n            raise ValueError(""\'feature_types\' must be specified"")\n        self._features = _convert_feature_hparams(feature_types)\n\n        convert_types = self._hparams.dataset.feature_convert_types\n        self._convert_types = {key: get_numpy_dtype(value)\n                               for key, value in convert_types.items()}\n        for key, dtype in self._convert_types.items():\n            self._features[key] = self._features[key]._replace(dtype=dtype)\n\n        image_options = self._hparams.dataset.image_options\n        if isinstance(image_options, HParams):\n            image_options = [image_options]\n        self._image_transforms: Dict[str, TransformFn] = {}\n        for options in image_options:\n            key = options.get(\'image_feature_name\')\n            if key is None or key not in self._features:\n                continue\n            self._image_transforms[key] = _create_image_transform(\n                options.get(\'resize_height\'), options.get(\'resize_width\'),\n                options.get(\'resize_method\') or \'bilinear\')\n\n        self._other_transforms = self._hparams.dataset.other_transformations\n\n        if data_source is None:\n            data_source = PickleDataSource[Dict[str, Any]](\n                self._hparams.dataset.files)\n\n        super().__init__(data_source, hparams, device)\n\n    class _RecordWriter(io.BytesIO):\n        def __init__(self, file_path: str,\n                     features: Dict[str, FeatureDescription]):\n            super().__init__()\n            self._file_path = file_path\n            self._features = features\n            self._file_handle = open(self._file_path, \'wb\')\n\n        def close(self) -> None:\n            self._file_handle.close()\n\n        def write(self, example: Dict[str, Any]):  # type: ignore\n            converted = {}\n            for key, descriptor in self._features.items():\n                value = example[key]\n                if descriptor.collate_method is CollateMethod.List:\n                    converted[key] = value\n                    continue\n                # Convert to NumPy array.\n                value = np.asarray(value, dtype=descriptor.dtype)\n                _check_shape(value, key, descriptor)\n                converted[key] = value\n            pickle.dump(converted, self._file_handle)\n\n    @classmethod\n    def writer(cls, file_path: str,\n               feature_types: Dict[str, Tuple[Any, ...]]) \\\n            -> \'_RecordWriter\':\n        r""""""Construct a file writer object that saves records in pickled format.\n\n        Example:\n\n        .. code-block:: python\n\n            file_path = ""data/train.pkl""\n            feature_types = {\n                ""input_ids"": [""int64"", ""stacked_tensor"", 128],\n                ""label_ids"": [""int64"", ""stacked_tensor""],\n            }\n            with tx.data.RecordData.writer(file_path, feature_types) as writer:\n                writer.write({\n                    ""input_ids"": np.random.randint(0, 100, size=128),\n                    ""label_ids"": np.random.randint(0, 100),\n                })\n\n        Args:\n            file_path (str): Path to save the dataset.\n            feature_types: Feature names and types. Please refer to\n                :meth:`default_hparams` for details.\n\n        Returns:\n            A file writer object.\n        """"""\n        feature_types = _convert_feature_hparams(feature_types)\n        return cls._RecordWriter(file_path, feature_types)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of default hyperparameters.\n\n        .. code-block:: python\n\n            {\n                # (1) Hyperparameters specific to the record data\n                \'dataset\': {\n                    \'files\': [],\n                    \'feature_types\': {},\n                    \'feature_convert_types\': {},\n                    \'image_options\': {},\n                    ""num_shards"": None,\n                    ""shard_id"": None,\n                    ""other_transformations"": [],\n                    ""data_name"": None,\n                }\n                # (2) General hyperparameters\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""name"": ""tfrecord_data"",\n            }\n\n        Here:\n\n        1. For the hyperparameters in the :attr:`""dataset""` field:\n\n           `""files""`: str or list\n               A (list of) pickled file path(s).\n\n           `""feature_types""`: dict\n               The feature names (`str`) with their descriptions in the form of\n               ``feature_name: [dtype, feature_collate_method, shape]``:\n\n               - ``dtype`` is a Python type (``int``, ``str``), dtype instance\n                 from PyTorch (``torch.float``), NumPy (``np.int64``),\n                 or TensorFlow (``tf.string``), or their stringified names such\n                 as ``""torch.float""`` and ``""np.int64""``. The feature will be\n                 read from the files and parsed into this dtype.\n\n               - ``feature_collate_method`` is of type ``str``, and describes\n                 how features are collated in the batch. Available values are:\n\n                 - ``""stacked_tensor""``: Features are assumed to be tensors of a\n                   fixed shape (or scalars). When collating, features are\n                   stacked, with the batch dimension being the first dimension.\n                   This is the default value if ``feature_collate_method`` is\n                   not specified. For example:\n\n                   - 5 scalar features -> a tensor of shape [5].\n                   - 4 tensor features, each of shape [6, 5] -> a tensor of\n                     shape [4, 6, 5].\n\n                 - ``""padded_tensor""``: Features are assumed to be tensors, with\n                   all dimensions except the first having the same size. When\n                   collating, features are padded with zero values along the\n                   end of the first dimension so that every tensor has the same\n                   size, and then stacked, with the batch dimension being the\n                   first dimension. For example:\n\n                   - 3 tensor features, with shapes [4, 7, 8], [5, 7, 8], and\n                     [4, 7, 8] -> a tensor of shape [3, 5, 7, 8].\n\n                 - ``""list""``: Features can be any objects. When collating, the\n                   features are stored in a Python list.\n\n               - ``shape`` is optional, and can be of type ``int``, `tuple``, or\n                 ``torch.Size``. If specified, shapes of tensor features will be\n                 checked, depending on the ``feature_collate_method``:\n\n                 - ``""stacked_tensor""``: The shape of every feature tensor must\n                   be ``shape``.\n                 - ``""padded_tensor""``: The shape (excluding first dimension)\n                   of every feature tensor must be ``shape``.\n                 - ``""list""``: ``shape`` is ignored.\n\n                 .. note::\n                    Shape check is performed before any transformations are\n                    applied.\n\n               Example:\n\n               .. code-block:: python\n\n                   feature_types = {\n                       ""input_ids"": [""int64"", ""stacked_tensor"", 128],\n                       ""label_ids"": [""int64"", ""stacked_tensor""],\n                       ""name_lists"": [""string"", ""list""],\n                   }\n\n               .. note::\n                   This field is named `""feature_original_types""` in Texar-TF.\n                   This name is still supported, but is deprecated in favor of\n                   `""feature_types""`.\n\n                   Texar-TF also uses different names for feature types:\n\n                   - ``""FixedLenFeature""`` corresponds to ``""stacked_tensor""``.\n                   - ``""FixedLenSequenceFeature""`` corresponds to\n                     ``""padded_tensor""``.\n                   - ``""VarLenFeature""`` corresponds to ``""list""``.\n\n                   These names are also accepted in Texar-PyTorch, but are\n                   deprecated in favor of the new names.\n\n           `""feature_convert_types""`: dict, optional\n               Specifies dtype converting after reading the data files. This\n               `dict` maps feature names to desired data dtypes. For example,\n               you can first read a feature into dtype ``torch.int32`` by\n               specifying in :attr:`""feature_types""` above, and convert\n               the feature to dtype ``""torch.long""`` by specifying here.\n               Features not specified here will not do dtype-convert.\n\n               - ``dtype`` is a Python type (`int`, `str`), dtype instance from\n                 PyTorch (``torch.float``), NumPy (``np.int64``),\n                 or TensorFlow (``tf.string``), or their stringified names such\n                 as ``""torch.float""`` and ``""np.int64""``.\n\n               Note that this converting process happens after all the data\n               are restored.\n\n               Example:\n\n               .. code-block:: python\n\n                   feature_convert_types = {\n                       ""input_ids"": ""int32"",\n                       ""label_ids"": ""int32"",\n                   }\n\n           `""image_options""`: dict, optional\n               Specifies the image feature name and performs image resizing,\n               includes three fields:\n\n               - `""image_feature_name""`: str\n                   The name of the feature which contains the image data. If\n                   set, the image data will be restored in a `numpy.ndarray`.\n               - `""resize_height""`: int\n                   The height of the image after resizing.\n               - `""resize_width""`: int\n                   The width of the image after resizing.\n\n               If any of :attr:`""resize_height""` or :attr:`""resize_width""` is\n               not set, image data will be restored with original shape.\n\n           `""num_shards""`: int, optional\n               The number of data shards in distributed mode. Usually set to\n               the number of processes in distributed computing.\n               Used in combination with :attr:`""shard_id""`.\n\n               .. warning::\n                   Sharding is not yet supported. This option (and\n                   related ones below) will be ignored.\n\n           `""shard_id""`: int, optional\n               Sets the unique id to identify a shard. The module will\n               processes only the corresponding shard of the whole data.\n               Used in combination with :attr:`""num_shards""`.\n\n               For example, in a case of distributed computing on 2 GPUs, the\n               hyperparameters of the data module for the two processes can be\n               configured as below, respectively.\n\n               For GPU 0:\n\n               .. code-block:: python\n\n                   dataset: {\n                       ...\n                       ""num_shards"": 2,\n                       ""shard_id"": 0\n                   }\n\n               For GPU 1:\n\n               .. code-block:: python\n\n                   dataset: {\n                       ...\n                       ""num_shards"": 2,\n                       ""shard_id"": 1\n                   }\n\n               Also refer to `examples/bert` for a use case.\n\n           `""other_transformations""`: list\n               A list of transformation functions or function names/paths to\n               further transform each single data instance.\n\n           `""data_name""`: str\n               Name of the dataset.\n\n        2. For the **general** hyperparameters, see\n           :meth:`texar.torch.data.DatasetBase.default_hparams` for details.\n        """"""\n        hparams = DatasetBase.default_hparams()\n        hparams[""name""] = ""record_data""\n        hparams.update({\n            ""dataset"": _default_record_dataset_hparams()\n        })\n        return hparams\n\n    def process(self, raw_example: Dict[str, Any]) -> Dict[str, Any]:\n        for key, descriptor in self._features.items():\n            _check_shape(raw_example[key], key, descriptor)\n        example = raw_example\n        for key, dtype in self._convert_types.items():\n            example[key] = np.asarray(example[key], dtype=dtype)\n        for key, transform in self._image_transforms.items():\n            example[key] = transform(example[key])\n        for transform in self._other_transforms:\n            example = transform(example)\n        return example\n\n    def collate(self, examples: List[Dict[str, Any]]) -> Batch:\n        batch = {}\n        for key, descriptor in self._features.items():\n            values = [ex[key] for ex in examples]\n            if descriptor.collate_method is not CollateMethod.List:\n                # NumPy functions work on PyTorch tensors too.\n                if descriptor.collate_method is CollateMethod.StackedTensor:\n                    values = np.stack(values, axis=0)\n                else:  # padded_tensor\n                    values, _ = padded_batch(values)\n                if (not isinstance(values, torch.Tensor) and\n                        descriptor.dtype not in [np.str_, np.bytes_]):\n                    values = torch.from_numpy(values)\n            else:\n                # Just put everything in a Python list.\n                pass\n            batch[key] = values\n        return Batch(len(examples), batch)\n\n    def list_items(self) -> List[str]:\n        r""""""Returns the list of item names that the data can produce.\n\n        Returns:\n            A list of strings.\n        """"""\n        return list(self._features.keys())\n\n    @property\n    def feature_names(self):\n        r""""""A list of feature names.\n        """"""\n        return self.list_items()\n'"
texar/torch/data/data/sampler.py,26,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious sampler classes.\n""""""\n\n# pylint: disable=protected-access\n\nfrom typing import (\n    Any, Callable, Generic, Iterator, List, Optional, Tuple, TypeVar, Union)\n\nimport torch\nfrom torch.utils.data import sampler as torch_sampler\n\nfrom texar.torch.data.data.data_base import DatasetBase\n\n\n__all__ = [\n    ""BatchingStrategy"",\n    ""TokenCountBatchingStrategy"",\n]\n\nExample = TypeVar(\'Example\')\n\n\n# pylint: disable=attribute-defined-outside-init\n# TODO: Remove this when Pylint fixes the bug. If the `disable` directive is not\n#  added, Pylint incorrectly reports this error for `self.size` in subclasses of\n#  `SamplerBase` in Python 3.6 due to use of the Generic class.\n#  See Pylint issue: https://github.com/PyCQA/pylint/issues/2981\n\nclass SamplerBase(torch_sampler.Sampler, Generic[Example]):\n    r""""""A subclass of :torch_docs:`~torch.utils.data.Sampler\n    <data.html#torch.utils.data.Sampler>` that supports:\n\n    - Returning raw examples when required.\n    - Creating iterators with unknown dataset size.\n\n    This class is used internally in\n    :class:`~texar.torch.data.data.DataIterator`. It calls the\n    :meth:`~texar.torch.data.data.DatasetBase._prefetch_source` method to ensure\n    the required number of raw examples are prefetched from source.\n\n    Args:\n        data: The :class:`~texar.torch.data.data.DatasetBase` instance.\n    """"""\n    size: Optional[int]\n\n    def __init__(self, data: DatasetBase[Any, Example]):\n        super().__init__(data)\n\n        self._data = data\n        self.size = None\n\n    def _iterator_given_size(self, size: int) -> Iterator[int]:\n        r""""""Return an iterator that generates samples when the dataset size\n        is given.\n\n        Args:\n            size: The dataset size.\n        """"""\n        raise NotImplementedError\n\n    def _iterator_unknown_size(self) -> Iterator[int]:\n        r""""""Return an iterator that generates samples when the dataset size\n        is unknown. This iterator must also call\n        :meth:`texar.torch.data.data.DatasetBase._prefetch_source` and check\n        whether the dataset size can be determined, before yielding the index.\n        See example implementations for details.\n        """"""\n        raise NotImplementedError\n\n    def __iter__(self) -> Union[Iterator[int], Iterator[Tuple[int, Example]]]:\n        r""""""Return an iterator based on the dataset settings.\n        """"""\n        self.size = self._data._dataset_size\n        if (not self._data._fully_cached or\n                self._data._should_call_prefetch_source):\n            self._data._start_iteration()\n            # First epoch of lazy loading, calling prefetch, and returning\n            # indices and examples.\n            iterator = self._iterator_unknown_size()\n        else:\n            # Non-lazy loading, or when dataset has been fully iterated.\n            assert self.size is not None\n            iterator = self._iterator_given_size(self.size)\n\n        if self._data._should_call_prefetch_processed:\n            # Processing routine is performed in main process. Yield\n            # processed examples instead.\n            map_fn = lambda idx: (idx, self._data._processed_cache[idx])\n        elif self._data._should_yield_raw_example:\n            # Return indices and examples for any epoch in this case.\n            map_fn = lambda idx: (idx, self._data._source[idx])\n        else:\n            map_fn = None  # type: ignore\n        if map_fn is not None:\n            return map(map_fn, iterator)\n\n        return iterator\n\n    def __len__(self):\n        if self.size is not None:\n            return self.size\n        raise AttributeError(""Dataset size cannot be determined at this point"")\n\n\nclass SequentialSampler(SamplerBase[Example]):\n    r""""""Samples elements sequentially, always in the same order. Same as\n    :torch_docs:`~torch.utils.data.SequentialSampler\n    <data.html#torch.utils.data.SequentialSampler>`\n    """"""\n\n    def _iterator_given_size(self, size: int) -> Iterator[int]:\n        return iter(range(size))\n\n    def _iterator_unknown_size(self) -> Iterator[int]:\n        index = 0\n        while True:\n            cur_size = self._data._prefetch_source(index)\n            if cur_size is not None:\n                self.size = cur_size\n                break\n            yield index\n            index += 1\n\n\nclass RandomSampler(SamplerBase[Example]):\n    r""""""Samples elements randomly. If without replacement, then sample from a\n    shuffled dataset. If with replacement, then user can specify ``num_samples``\n    to draw.\n\n    This class uses :torch_docs:`torch.utils.data.RandomSampler\n    <data.html#torch.utils.data.RandomSampler>` directly. Given the\n    nature of such shuffling, it cannot be used for iterators with unknown size.\n\n    Args:\n        data: The :class:`~texar.torch.data.data.DatasetBase` instance.\n        num_samples (int): number of samples to draw, default=len(dataset)\n        replacement (bool): samples are drawn with replacement if `True`,\n            default=False\n    """"""\n\n    def __init__(self, data: DatasetBase[Any, Example],\n                 replacement: bool = False, num_samples: Optional[int] = None):\n        super().__init__(data)\n        self._sampler = torch_sampler.RandomSampler(\n            data, replacement, num_samples)\n\n    def _iterator_given_size(self, size: int) -> Iterator[int]:\n        del size  # not used\n        return iter(self._sampler)\n\n    def _iterator_unknown_size(self) -> Iterator[int]:\n        raise TypeError(\n            ""RandomSampler does not support lazy data loading. To perform ""\n            ""shuffling with lazy loading, use BufferShuffleSampler."")\n\n\nclass BufferShuffleSampler(SamplerBase[Example]):\n    r""""""A :torch_docs:`~torch.utils.data.Sampler\n    <data.html#torch.utils.data.Sampler>` that uses a shuffle buffer, as\n    in TensorFlow. The buffer is first filled with data examples. Each time a\n    sample is drawn from the buffer, and the drawn sample is replaced with the\n    next data example.\n\n    This class is used internally in\n    :class:`~texar.torch.data.data.DataIterator`. It calls the\n    :meth:`~texar.torch.data.data.DatasetBase._prefetch_source` method to ensure\n    the required number of raw examples are prefetched from source.\n\n    Args:\n        data: The :class:`~texar.torch.data.data.DatasetBase` instance.\n        buffer_size: The size of the shuffle buffer. Use larger buffer sizes for\n            more uniformly-random shuffling.\n    """"""\n\n    def __init__(self, data: DatasetBase[Any, Example], buffer_size: int):\n        super().__init__(data)\n        self.buffer_size = buffer_size\n\n    def _iterator_given_size(self, size) -> Iterator[int]:\n        if self.buffer_size >= size:\n            yield from iter(torch.randperm(size).tolist())\n            return\n\n        buffer = list(range(self.buffer_size))\n        for x in range(self.buffer_size, size):\n            sample = torch.randint(self.buffer_size, (1,)).item()\n            index = buffer[sample]\n            yield index\n            buffer[sample] = x\n        yield from (buffer[x] for x in torch.randperm(self.buffer_size))\n\n    def _iterator_unknown_size(self) -> Iterator[int]:\n        buffer = list(range(self.buffer_size))\n        x = self.buffer_size\n        while True:\n            sample = torch.randint(self.buffer_size, (1,)).item()\n            index = buffer[sample]\n            cur_size = self._data._prefetch_source(index)\n            if cur_size is not None and index >= cur_size:\n                self.size = cur_size\n            if self.size is not None and index >= self.size:\n                break\n            yield index\n            buffer[sample] = x\n            x += 1\n        yield from (buffer[x] for x in torch.randperm(self.buffer_size)\n                    if buffer[x] < self.size)\n\n\n# pylint: enable=attribute-defined-outside-init\n\n\nclass BatchingStrategy(Generic[Example]):\n    r""""""Decides batch boundaries in dynamic batching. Please refer to\n    :class:`TokenCountBatchingStrategy` for a concrete example.\n    """"""\n\n    def reset_batch(self) -> None:\n        r""""""Reset the internal state of the batching strategy. This method is\n        called at the start of iteration, and after each batch is yielded.\n        """"""\n        raise NotImplementedError\n\n    def add_example(self, example: Example) -> bool:\n        r""""""Add an example into the current batch, and modify internal states\n        accordingly. If the example should not be added to the batch, this\n        method does not modify the internal state, and returns `False`.\n\n        Args:\n            example: The example to add to the batch.\n\n        Returns:\n            A boolean value indicating whether :attr:`example` should be added\n            to the batch.\n        """"""\n        raise NotImplementedError\n\n\nclass TokenCountBatchingStrategy(BatchingStrategy[Example]):\n    r""""""Create dynamically-sized batches so that the total number of tokens\n    inside each batch is constrained.\n\n    Args:\n        max_tokens (int): The maximum number of tokens inside each batch.\n        max_batch_size (int, optional): The maximum number of examples for each\n            batch. If `None`, batches can contain arbitrary number of examples\n            as long as the total number of tokens does not exceed\n            :attr:`max_tokens`.\n        length_fn (callable, optional): A function taking a data example as\n            argument, and returning the number of tokens in the example. By\n            default, :python:`len` is used, which is the desired behavior if the\n            dataset in question is a :class:`~texar.torch.data.MonoTextData`.\n    """"""\n    sum_tokens: int\n    cur_batch_size: int\n\n    def __init__(self, max_tokens: int, max_batch_size: Optional[int] = None,\n                 length_fn: Optional[Callable[[Example], int]] = None):\n        self.max_batch_size = max_batch_size\n        self.max_tokens = max_tokens\n        self.length_fn: Callable[[Example], int]\n        self.length_fn = length_fn or len  # type: ignore\n\n    def reset_batch(self) -> None:\n        self.sum_tokens = 0\n        self.cur_batch_size = 0\n\n    def add_example(self, example: Example) -> bool:\n        if self.cur_batch_size == self.max_batch_size:\n            return False\n        cur_tokens = self.length_fn(example)\n        if cur_tokens + self.sum_tokens > self.max_tokens:\n            return False\n\n        self.cur_batch_size += 1\n        self.sum_tokens += cur_tokens\n        return True\n\n\nclass DynamicBatchSampler(torch_sampler.BatchSampler, Generic[Example]):\n    r""""""A subclass of :torch_docs:`~torch.utils.data.BatchSampler\n    <data.html#torch.utils.data.BatchSampler>` that supports dynamic batching\n    through a user-provided :class:`BatchingStrategy`. This class is used\n    internally.\n\n    Args:\n        dataset: The dataset to create batches from.\n        sampler: An instance of :class:`SamplerBase` that returns indices of\n            each sampled example.\n        strategy: An instance of :class:`BatchingStrategy` that decides whether\n            a batch should be yielded.\n    """"""\n\n    def __init__(self, dataset: DatasetBase[Any, Example],  # pylint: disable=super-init-not-called\n                 sampler: SamplerBase, strategy: BatchingStrategy[Example]):\n        self.dataset = dataset\n        self.sampler = sampler\n        self.strategy = strategy\n\n    def __iter__(self) -> Union[Iterator[List[int]],  # type: ignore\n                                Iterator[List[Tuple[int, Example]]]]:\n        batch = []  # type: ignore\n        self.strategy.reset_batch()\n        for idx in self.sampler:\n            if isinstance(idx, tuple):\n                example = self.dataset[idx[0]]\n            else:\n                example = self.dataset[idx]\n            while not self.strategy.add_example(example):\n                if len(batch) == 0:\n                    raise ValueError(f""Batching strategy refused to add ""\n                                     f""example {idx} to empty batch."")\n                yield batch\n                batch = []\n                self.strategy.reset_batch()\n            batch.append(idx)\n        if len(batch) > 0:\n            yield batch\n            self.strategy.reset_batch()\n\n    def __len__(self):\n        raise TypeError(""DynamicBatchSampler does not support __len__"")\n'"
texar/torch/data/data/scalar_data.py,11,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious data classes that define data reading, parsing, batching, and other\npreprocessing operations.\n""""""\nfrom typing import (List, Optional, Union)\n\nfrom distutils.util import strtobool\nimport numpy as np\nimport torch\n\nfrom texar.torch.data.data.data_base import DatasetBase, DataSource\nfrom texar.torch.data.data.dataset_utils import Batch\nfrom texar.torch.data.data.text_data_base import TextLineDataSource\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.utils.dtypes import get_numpy_dtype, \\\n    get_supported_scalar_types, torch_bool\n\n\n__all__ = [\n    ""_default_scalar_dataset_hparams"",\n    ""ScalarData""\n]\n\n\ndef _default_scalar_dataset_hparams():\n    """"""Returns hyperparameters of a scalar dataset with default values.\n\n    See :meth:`texar.torch.data.ScalarData.default_hparams` for details.\n    """"""\n    return {\n        ""files"": [],\n        ""compression_type"": None,\n        ""data_type"": ""int"",\n        ""data_name"": ""data"",\n        ""other_transformations"": [],\n        ""@no_typecheck"": [""files""]\n    }\n\n\nclass ScalarData(DatasetBase[List[str], Union[int, float]]):\n    r""""""Scalar data where each line of the files is a scalar (int or float),\n    e.g., a data label.\n\n    Args:\n        hparams (dict): Hyperparameters. See :meth:`default_hparams` for the\n            defaults.\n        device: The device of the produced batches. For GPU training, set to\n            current CUDA device.\n\n    The processor reads and processes raw data and results in a dataset\n    whose element is a python `dict` including one field. The field name is\n    specified in :attr:`hparams[""dataset""][""data_name""]`. If not specified,\n    the default name is `""data""`. The field name can be accessed through\n    :attr:`data_name`.\n\n    This field is a Tensor of shape `[batch_size]` containing a batch of\n    scalars, of either int or float type as specified in :attr:`hparams`.\n\n    Example:\n\n        .. code-block:: python\n\n            hparams={\n                \'dataset\': { \'files\': \'data.txt\', \'data_name\': \'label\' },\n                \'batch_size\': 2\n            }\n            data = ScalarData(hparams)\n            iterator = DataIterator(data)\n            for batch in iterator:\n                # batch contains the following\n                # batch == {\n                #     \'label\': [2, 9]\n                # }\n    """"""\n\n    def __init__(self, hparams, device: Optional[torch.device] = None,\n                 data_source: Optional[DataSource] = None):\n        self._hparams = HParams(hparams, self.default_hparams())\n        self._other_transforms = self._hparams.dataset.other_transformations\n        data_type = self._hparams.dataset[""data_type""]\n        if data_type not in get_supported_scalar_types():\n            raise ValueError(f""Unsupported data type \'{data_type}\'"")\n\n        # In Pytorch versions < 1.1.0, ""torch.uint8"" is treated as ""bool"" type\n        # hence we set self.data_type = np.uint8 here\n        if data_type == ""bool"":\n            self._data_type = get_numpy_dtype(str(torch_bool))\n        else:\n            self._data_type = get_numpy_dtype(data_type)\n\n        if data_source is None:\n            data_source = TextLineDataSource(\n                self._hparams.dataset.files,\n                compression_type=self._hparams.dataset.compression_type)\n        super().__init__(data_source, hparams, device=device)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of default hyperparameters.\n\n        .. code-block:: python\n\n            {\n                # (1) Hyperparams specific to scalar dataset\n                ""dataset"": {\n                    ""files"": [],\n                    ""compression_type"": None,\n                    ""data_type"": ""int"",\n                    ""other_transformations"": [],\n                    ""data_name"": ""data"",\n                }\n                # (2) General hyperparams\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""name"": ""scalar_data"",\n            }\n\n        Here:\n\n        1. For the hyperparameters in the :attr:`""dataset""` field:\n\n            `""files""`: str or list\n                A (list of) file path(s).\n\n                Each line contains a single scalar number.\n\n            `""compression_type""`: str, optional\n                One of """" (no compression), ""ZLIB"", or ""GZIP"".\n\n            `""data_type""`: str\n                The scalar type. Types defined in\n                :meth:`~texar.torch.utils.dtypes.get_supported_scalar_types` are\n                supported.\n\n            `""other_transformations""`: list\n                A list of transformation functions or function names/paths to\n                further transform each single data instance.\n\n                (More documentations to be added.)\n\n            `""data_name""`: str\n                Name of the dataset.\n\n        2. For the **general** hyperparameters, see\n           :meth:`texar.torch.data.DatasetBase.default_hparams` for details.\n\n        """"""\n        hparams = DatasetBase.default_hparams()\n        hparams[""name""] = ""scalar_data""\n        hparams.update({\n            ""dataset"": _default_scalar_dataset_hparams()\n        })\n        return hparams\n\n    def process(self, raw_example: List[str]) -> Union[bool, int, float]:\n        assert len(raw_example) == 1\n\n        example_: Union[int, str]\n        if self._data_type == np.bool_:\n            example_ = strtobool(raw_example[0])\n\n        else:\n            example_ = raw_example[0]\n\n        example = self._data_type(example_)\n\n        for transform in self._other_transforms:\n            example = transform(example)\n        return example\n\n    def collate(self, examples: List[Union[bool, int, float]]) -> Batch:\n        # convert the list of strings into appropriate tensors here\n        examples_np = np.array(examples, dtype=self._data_type)\n        collated_examples = torch.from_numpy(examples_np)\n        return Batch(len(examples),\n                     batch={self.data_name: collated_examples})\n\n    def list_items(self):\n        r""""""Returns the list of item names that the data can produce.\n\n        Returns:\n            A list of strings.\n        """"""\n        return [self.hparams.dataset[""data_name""]]\n\n    @property\n    def data_name(self):\n        r""""""The name of the data tensor, ""data"" by default if not specified in\n        :attr:`hparams`.\n        """"""\n        return self.hparams.dataset[""data_name""]\n'"
texar/torch/data/data/text_data_base.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase text data class that is inherited by all text data classes.\n""""""\nimport io\nimport locale\nfrom abc import ABC\nfrom typing import IO, Iterator, List, Optional, TypeVar\n\nimport torch\nfrom texar.torch.data.data.data_base import DatasetBase, DataSource\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""TextLineDataSource"",\n    ""TextDataBase"",\n]\n\nRawExample = TypeVar(\'RawExample\')\nExample = TypeVar(\'Example\')\n\n\nclass TextLineDataSource(DataSource[List[str]]):\n    r""""""Data source for reading from (multiple) text files. Each line is\n    tokenized and yielded as an example.\n\n    This data source does not support indexing.\n\n    Args:\n        file_paths (str or list[str]): Paths to the text files.\n        compression_type (str, optional): The compression type for the text\n            files, ``""gzip""`` and ``""zlib""`` are supported. Default is\n            `None`, in which case files are treated as plain text files.\n        encoding (str, optional): Encoding for the files. By default uses\n            the default locale of the system (usually UTF-8).\n        delimiter (str, optional): Delimiter for tokenization purposes. This\n            is used in combination with ``max_length``. If `None`, text is split\n            on any blank character.\n        max_length (int, optional): Maximum length for data examples. Length\n            is measured as the number of tokens in a line after being\n            tokenized using the provided ``delimiter``. Lines with more than\n            ``max_length`` tokens will be dropped.\n    """"""\n\n    def __init__(self, file_paths: MaybeList[str],\n                 compression_type: Optional[str] = None,\n                 encoding: Optional[str] = None,\n                 delimiter: Optional[str] = None,\n                 max_length: Optional[int] = None):\n        if compression_type is not None:\n            compression_type = compression_type.lower()\n            if compression_type not in [\'gzip\', \'zlib\']:\n                raise ValueError(\n                    f""Unsupported compression type: {compression_type}"")\n        if isinstance(file_paths, str):\n            file_paths = [file_paths]\n        self._compression_type = compression_type\n        self._encoding = encoding or locale.getpreferredencoding()\n        self._file_paths = file_paths\n        self._max_length = max_length\n        self._delimiter = delimiter\n\n    class _ZlibWrapper(io.BufferedReader):\n        def __init__(self, raw: IO[bytes]):\n            super().__init__(raw)  # type: ignore\n            import zlib\n            self.file = raw\n            self.zlib = zlib.decompressobj()\n            self.buffer = b\'\'\n\n        @property\n        def closed(self) -> bool:\n            return self.file.closed\n\n        def readable(self) -> bool:\n            return True\n\n        def close(self) -> None:\n            self.file.close()\n\n        def read1(self, n: int = -1) -> bytes:\n            # Our implementation does not really satisfy the definition for\n            # `read1`, but whatever, it seems to work.\n            if n == -1:\n                raw = self.file.read(n)\n                b = self.buffer\n                self.buffer = b\'\'\n                if raw:\n                    cur = self.zlib.decompress(raw)\n                    if len(cur) > 0:\n                        if len(b) > 0:\n                            b = b + cur\n                        else:\n                            b = cur\n            else:\n                if len(self.buffer) > 0:\n                    b = self.buffer[:n]\n                    self.buffer = self.buffer[n:]\n                    return b\n                while True:\n                    raw = self.file.read(n)\n                    if not raw:\n                        return b\'\'\n                    b = self.zlib.decompress(raw)\n                    if len(b) > 0:\n                        break\n                if len(b) > n:\n                    self.buffer = b[n:]\n                    return b[:n]\n            return b\n\n        def read(self, n: Optional[int] = -1) -> bytes:\n            if n is None or n < 0:\n                n = -1\n            return self.read1(n)\n\n        def __getattr__(self, item):\n            return getattr(self.file, item)\n\n    def _open_file(self, path: str) -> IO[str]:\n        if self._compression_type == \'zlib\':\n            f: IO[str] = io.TextIOWrapper(\n                self._ZlibWrapper(open(path, \'rb\')),  # type: ignore\n                encoding=self._encoding)\n        elif self._compression_type == \'gzip\':\n            import gzip\n            f = gzip.open(path, \'rt\', encoding=self._encoding)\n        else:\n            f = open(path, \'r\', encoding=self._encoding)\n        return f\n\n    def __iter__(self) -> Iterator[List[str]]:\n        for path in self._file_paths:\n            with self._open_file(path) as f:\n                for line in f:\n                    tokens = line.split(self._delimiter)\n                    if (self._max_length is not None and\n                            len(tokens) > self._max_length):\n                        continue\n                    yield tokens\n\n\nclass TextDataBase(DatasetBase[RawExample, Example], ABC):\n    r""""""Base class inherited by all text data classes.\n    """"""\n\n    def __init__(self, source: DataSource[RawExample], hparams,\n                 device: Optional[torch.device] = None):\n        super().__init__(source, hparams, device=device)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of default hyperparameters.\n\n        See the specific subclasses for the details.\n        """"""\n        hparams = DatasetBase.default_hparams()\n        hparams.update({\n            ""bucket_boundaries"": [],\n            ""bucket_batch_sizes"": None,\n            ""bucket_length_fn"": None})\n        return hparams\n'"
texar/torch/data/tokenizers/__init__.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTokenizer modules of Texar library.\n""""""\n\nfrom texar.torch.data.tokenizers.bert_tokenizer import *\nfrom texar.torch.data.tokenizers.gpt2_tokenizer import *\nfrom texar.torch.data.tokenizers.roberta_tokenizer import *\nfrom texar.torch.data.tokenizers.tokenizer_base import *\nfrom texar.torch.data.tokenizers.xlnet_tokenizer import *\nfrom texar.torch.data.tokenizers.sentencepiece_tokenizer import *\nfrom texar.torch.data.tokenizers.t5_tokenizer import *\n'"
texar/torch/data/tokenizers/bert_tokenizer.py,6,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPre-trained BERT tokenizer.\n\nCode structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_bert.py`\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport os\n\nfrom texar.torch.modules.pretrained.bert import PretrainedBERTMixin\nfrom texar.torch.data.tokenizers.tokenizer_base import TokenizerBase\nfrom texar.torch.data.tokenizers.bert_tokenizer_utils import \\\n    load_vocab, BasicTokenizer, WordpieceTokenizer\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.utils.utils import truncate_seq_pair\n\n__all__ = [\n    \'BERTTokenizer\',\n]\n\n\nclass BERTTokenizer(PretrainedBERTMixin, TokenizerBase):\n    r""""""Pre-trained BERT Tokenizer.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name of\n            pre-trained model (e.g., `bert-base-uncased`). Please refer to\n            :class:`~texar.torch.modules.PretrainedBERTMixin` for\n            all supported models.\n            If None, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    _IS_PRETRAINED = True\n    _MAX_INPUT_SIZE = {\n        # Standard BERT\n        \'bert-base-uncased\': 512,\n        \'bert-large-uncased\': 512,\n        \'bert-base-cased\': 512,\n        \'bert-large-cased\': 512,\n        \'bert-base-multilingual-uncased\': 512,\n        \'bert-base-multilingual-cased\': 512,\n        \'bert-base-chinese\': 512,\n\n        # BioBERT\n        \'biobert-v1.0-pmc\': 512,\n        \'biobert-v1.0-pubmed-pmc\': 512,\n        \'biobert-v1.0-pubmed\': 512,\n        \'biobert-v1.1-pubmed\': 512,\n\n        # SciBERT\n        \'scibert-scivocab-uncased\': 512,\n        \'scibert-scivocab-cased\': 512,\n        \'scibert-basevocab-uncased\': 512,\n        \'scibert-basevocab-cased\': 512,\n\n        # SpanBERT\n        \'spanbert-base-cased\': 512,\n        \'spanbert-large-cased\': 512,\n    }\n    _VOCAB_FILE_NAMES = {\'vocab_file\': \'vocab.txt\'}\n    _VOCAB_FILE_MAP = {\n        \'vocab_file\': {\n            # Standard BERT\n            \'bert-base-uncased\': \'vocab.txt\',\n            \'bert-large-uncased\': \'vocab.txt\',\n            \'bert-base-cased\': \'vocab.txt\',\n            \'bert-large-cased\': \'vocab.txt\',\n            \'bert-base-multilingual-uncased\': \'vocab.txt\',\n            \'bert-base-multilingual-cased\': \'vocab.txt\',\n            \'bert-base-chinese\': \'vocab.txt\',\n\n            # BioBERT\n            \'biobert-v1.0-pmc\': \'vocab.txt\',\n            \'biobert-v1.0-pubmed-pmc\': \'vocab.txt\',\n            \'biobert-v1.0-pubmed\': \'vocab.txt\',\n            \'biobert-v1.1-pubmed\': \'vocab.txt\',\n\n            # SciBERT\n            \'scibert-scivocab-uncased\': \'vocab.txt\',\n            \'scibert-scivocab-cased\': \'vocab.txt\',\n            \'scibert-basevocab-uncased\': \'vocab.txt\',\n            \'scibert-basevocab-cased\': \'vocab.txt\',\n\n            # SpanBERT\n            \'spanbert-base-cased\': \'vocab.txt\',\n            \'spanbert-large-cased\': \'vocab.txt\',\n        }\n    }\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n\n        # SpanBERT checkpoint files do not include vocabulary file, use\n        # standard BERT directly when user use the pre-trained SpanBERT.\n        if pretrained_model_name is not None:\n            if pretrained_model_name.startswith(\'spanbert\'):\n                pretrained_model_name = pretrained_model_name.lstrip(\'span\')\n        elif hparams is not None:\n            hparams = HParams(hparams, None)\n            if hparams.pretrained_model_name is not None and \\\n                    hparams.pretrained_model_name.startswith(\'spanbert\'):\n                pretrained_model_name = \\\n                    hparams.pretrained_model_name.lstrip(\'span\')\n\n        self.load_pretrained_config(pretrained_model_name, cache_dir, hparams)\n\n        super().__init__(hparams=None)\n\n        self.config = {\n            \'tokenize_chinese_chars\': self.hparams[\'tokenize_chinese_chars\'],\n            \'do_lower_case\': self.hparams[\'do_lower_case\'],\n            \'do_basic_tokenize\': self.hparams[\'do_basic_tokenize\'],\n            \'non_split_tokens\': self.hparams[\'non_split_tokens\'],\n        }\n\n        if self.pretrained_model_dir is not None:\n            assert self.pretrained_model_name is not None\n            vocab_file = os.path.join(self.pretrained_model_dir,\n                                      self._VOCAB_FILE_MAP[\'vocab_file\']\n                                      [self.pretrained_model_name])\n\n            if self._MAX_INPUT_SIZE.get(self.pretrained_model_name):\n                self.max_len = self._MAX_INPUT_SIZE[self.pretrained_model_name]\n        else:\n            vocab_file = self.hparams[\'vocab_file\']\n            if self.hparams.get(\'max_len\'):\n                self.max_len = self.hparams[\'max_len\']\n\n        if not os.path.isfile(vocab_file):\n            raise ValueError(""Can\'t find a vocabulary file at path ""\n                             ""\'{}"".format(vocab_file))\n        self.vocab = load_vocab(vocab_file)\n        self.ids_to_tokens = dict((ids, tok) for tok, ids in self.vocab.items())\n\n        self.do_basic_tokenize = self.hparams[\'do_basic_tokenize\']\n        if self.do_basic_tokenize:\n            self.basic_tokenizer = BasicTokenizer(\n                do_lower_case=self.hparams[""do_lower_case""],\n                never_split=self.hparams[""non_split_tokens""],\n                tokenize_chinese_chars=self.hparams[""tokenize_chinese_chars""])\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab,\n                                                      unk_token=self.unk_token)\n\n    def _map_text_to_token(self, text: str) -> List[str]:  # type: ignore\n        split_tokens = []\n        if self.do_basic_tokenize:\n            for token in self.basic_tokenizer.tokenize(\n                    text, never_split=self.all_special_tokens):\n                assert token is not None\n                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                    split_tokens.append(sub_token)\n        else:\n            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n        return split_tokens\n\n    def save_vocab(self, save_dir: str) -> Tuple[str]:\n        r""""""Save the tokenizer vocabulary to a directory or file.""""""\n        index = 0\n        if os.path.isdir(save_dir):\n            save_dir = os.path.join(save_dir,\n                                    self._VOCAB_FILE_NAMES[\'vocab_file\'])\n        with open(save_dir, ""w"", encoding=""utf-8"") as writer:\n            for token, token_index in sorted(self.vocab.items(),\n                                             key=lambda kv: kv[1]):\n                if index != token_index:\n                    print(""Saving vocabulary to {}: vocabulary indices are not ""\n                          ""consecutive. Please check that the vocabulary is ""\n                          ""not corrupted!"".format(save_dir))\n                    index = token_index\n                writer.write(token + u\'\\n\')\n                index += 1\n\n        return (save_dir, )\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self.vocab)\n\n    def _map_token_to_id(self, token: str) -> int:\n        r""""""Maps a token to an id using the vocabulary.""""""\n        unk_id = self.vocab.get(self.unk_token)\n        assert unk_id is not None\n        return self.vocab.get(token, unk_id)\n\n    def _map_id_to_token(self, index: int) -> str:\n        r""""""Maps an id to a token using the vocabulary.\n        """"""\n        return self.ids_to_tokens.get(index, self.unk_token)\n\n    def map_token_to_text(self, tokens: List[str]) -> str:\n        r""""""Maps a sequence of tokens (string) to a single string.""""""\n        out_string = \' \'.join(tokens).replace(\' ##\', \'\').strip()\n        return out_string\n\n    def encode_text(self,\n                    text_a: str,\n                    text_b: Optional[str] = None,\n                    max_seq_length: Optional[int] = None) -> \\\n            Tuple[List[int], List[int], List[int]]:\n        r""""""Adds special tokens to a sequence or sequence pair and computes the\n        corresponding segment ids and input mask for BERT specific tasks.\n        The sequence will be truncated if its length is larger than\n        ``max_seq_length``.\n\n        A BERT sequence has the following format:\n        `[cls_token]` X `[sep_token]`\n\n        A BERT sequence pair has the following format:\n        `[cls_token]` A `[sep_token]` B `[sep_token]`\n\n        Args:\n            text_a: The first input text.\n            text_b: The second input text.\n            max_seq_length: Maximum sequence length.\n\n        Returns:\n            A tuple of `(input_ids, segment_ids, input_mask)`, where\n\n            - ``input_ids``: A list of input token ids with added\n              special token ids.\n            - ``segment_ids``: A list of segment ids.\n            - ``input_mask``: A list of mask ids. The mask has 1 for real\n              tokens and 0 for padding tokens. Only real tokens are\n              attended to.\n        """"""\n        if max_seq_length is None:\n            max_seq_length = self.max_len\n\n        cls_token_id = self._map_token_to_id(self.cls_token)\n        sep_token_id = self._map_token_to_id(self.sep_token)\n\n        token_ids_a = self.map_text_to_id(text_a)\n        assert isinstance(token_ids_a, list)\n\n        token_ids_b = None\n        if text_b:\n            token_ids_b = self.map_text_to_id(text_b)\n\n        if token_ids_b:\n            assert isinstance(token_ids_b, list)\n            # Modifies `token_ids_a` and `token_ids_b` in place so that the\n            # total length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with ""- 3""\n            truncate_seq_pair(token_ids_a, token_ids_b, max_seq_length - 3)\n\n            input_ids = ([cls_token_id] + token_ids_a + [sep_token_id] +\n                         token_ids_b + [sep_token_id])\n            segment_ids = [0] * (len(token_ids_a) + 2) + \\\n                          [1] * (len(token_ids_b) + 1)\n        else:\n            # Account for [CLS] and [SEP] with ""- 2""\n            token_ids_a = token_ids_a[:max_seq_length - 2]\n\n            input_ids = [cls_token_id] + token_ids_a + [sep_token_id]\n            segment_ids = [0] * len(input_ids)\n\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the maximum sequence length.\n        input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n        segment_ids = segment_ids + [0] * (max_seq_length - len(segment_ids))\n        input_mask = input_mask + [0] * (max_seq_length - len(input_mask))\n\n        assert len(input_ids) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n\n        return input_ids, segment_ids, input_mask\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The tokenizer is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the tokenizer is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the tokenizer is defined by the\n          configurations in `hparams`.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""bert-base-uncased"",\n                ""vocab_file"": None,\n                ""max_len"": 512,\n                ""unk_token"": ""[UNK]"",\n                ""sep_token"": ""[SEP]"",\n                ""pad_token"": ""[PAD]"",\n                ""cls_token"": ""[CLS]"",\n                ""mask_token"": ""[MASK]"",\n                ""tokenize_chinese_chars"": True,\n                ""do_lower_case"": True,\n                ""do_basic_tokenize"": True,\n                ""non_split_tokens"": None,\n                ""name"": ""bert_tokenizer"",\n            }\n\n        Here:\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained BERT model.\n\n        `""vocab_file""`: str or None\n            The path to a one-wordpiece-per-line vocabulary file.\n\n        `""max_len""`: int\n            The maximum sequence length that this model might ever be used with.\n\n        `""unk_token""`: str\n            Unknown token.\n\n        `""sep_token""`: str\n            Separation token.\n\n        `""pad_token""`: str\n            Padding token.\n\n        `""cls_token""`: str\n            Classification token.\n\n        `""mask_token""`: str\n            Masking token.\n\n        `""tokenize_chinese_chars""`: bool\n            Whether to tokenize Chinese characters.\n\n        `""do_lower_case""`: bool\n            Whether to lower case the input\n            Only has an effect when `do_basic_tokenize=True`\n\n        `""do_basic_tokenize""`: bool\n            Whether to do basic tokenization before wordpiece.\n\n        `""non_split_tokens""`: list\n            List of tokens which will never be split during tokenization.\n            Only has an effect when `do_basic_tokenize=True`\n\n        `""name""`: str\n            Name of the tokenizer.\n\n        """"""\n        return {\n            \'pretrained_model_name\': \'bert-base-uncased\',\n            \'vocab_file\': None,\n            \'max_len\': 512,\n            \'unk_token\': \'[UNK]\',\n            \'sep_token\': \'[SEP]\',\n            \'pad_token\': \'[PAD]\',\n            \'cls_token\': \'[CLS]\',\n            \'mask_token\': \'[MASK]\',\n            \'tokenize_chinese_chars\': True,\n            \'do_lower_case\': True,\n            \'do_basic_tokenize\': True,\n            \'non_split_tokens\': None,\n            \'name\': \'bert_tokenizer\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str):\n        r""""""Returns the configuration of the pre-trained BERT tokenizer.""""""\n        return {\n            \'vocab_file\': None,\n            \'max_len\': 512,\n            \'unk_token\': \'[UNK]\',\n            \'sep_token\': \'[SEP]\',\n            \'pad_token\': \'[PAD]\',\n            \'cls_token\': \'[CLS]\',\n            \'mask_token\': \'[MASK]\',\n            \'tokenize_chinese_chars\': True,\n            \'do_lower_case\': True,\n            \'do_basic_tokenize\': True,\n            \'non_split_tokens\': None,\n        }\n'"
texar/torch/data/tokenizers/bert_tokenizer_utils.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utils of pre-trained BERT tokenizer.\n\nCode structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_bert.py`\n""""""\n\nfrom typing import Dict, List, Optional\n\nimport collections\nimport unicodedata\n\n\n__all__ = [\n    ""load_vocab"",\n    ""BasicTokenizer"",\n    ""WordpieceTokenizer"",\n]\n\n\ndef load_vocab(vocab_file: str) -> Dict[str, int]:\n    r""""""Loads a vocabulary file into a dictionary.""""""\n    vocab: Dict[str, int] = collections.OrderedDict()\n    with open(vocab_file, ""r"", encoding=""utf-8"") as reader:\n        tokens = reader.readlines()\n    for index, token in enumerate(tokens):\n        token = token.rstrip(\'\\n\')\n        vocab[token] = index\n    return vocab\n\n\nclass BasicTokenizer:\n    r""""""Runs basic tokenization (punctuation splitting, lower casing, etc.).\n\n    Args:\n        do_lower_case: Whether to lower case the input.\n        never_split: A list of tokens not to split.\n        tokenize_chinese_chars: Whether to tokenize Chinese characters.\n            This should likely be deactivated for Japanese:\n            see:\n            `https://github.com/huggingface/pytorch-pretrained-BERT/issues/328`\n    """"""\n\n    def __init__(self, do_lower_case: bool = True,\n                 never_split: Optional[List[str]] = None,\n                 tokenize_chinese_chars: bool = True):\n        if never_split is None:\n            never_split = []\n        self.do_lower_case = do_lower_case\n        self.never_split = never_split\n        self.tokenize_chinese_chars = tokenize_chinese_chars\n\n    def tokenize(self, text: str,\n                 never_split: Optional[List[str]] = None) -> \\\n            List[str]:\n        r""""""Basic tokenization of a piece of text.\n\n        Split on white spaces only, for sub-word tokenization, see\n        WordPieceTokenizer.\n\n        Args:\n            text: An input string.\n            never_split: A list of tokens not to split.\n        """"""\n        never_split = self.never_split + (never_split\n                                          if never_split is not None else [])\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        if self.tokenize_chinese_chars:\n            text = self._tokenize_chinese_chars(text)\n        # see: https://github.com/google-research/bert/blob/master/\n        # tokenization.py#L201\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case and token not in never_split:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    @classmethod\n    def _run_strip_accents(cls, text: str) -> str:\n        r""""""Strips accents from a piece of text.\n\n        Example:\n            accented_string = \'M\xc3\xa1laga\'\n            _run_strip_accents(accented_string)  # \'Malaga\'\n        """"""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    @classmethod\n    def _run_split_on_punc(cls, text: str,\n                           never_split: Optional[List[str]] = None) -> \\\n            List[str]:\n        r""""""Splits punctuation on a piece of text.\n\n        Example:\n            text = \'Texar-PyTorch is an open-source toolkit based on PyTorch.\'\n            _run_split_on_punc(text)\n            # [\'Texar\', \'-\', \'PyTorch is an open\', \'-\',\n            # \'source toolkit based on PyTorch\', \'.\']\n        """"""\n        if never_split is not None and text in never_split:\n            return [text]\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text: str) -> str:\n        r""""""Adds whitespace around any CJK character.\n\n        Example:\n            text = \'\xe4\xbb\x8a\xe5\xa4\xa9\xe5\xa4\xa9\xe6\xb0\x94\xe4\xb8\x8d\xe9\x94\x99\'\n            _tokenize_chinese_chars(text)\n            # \' \xe4\xbb\x8a  \xe5\xa4\xa9  \xe5\xa4\xa9  \xe6\xb0\x94  \xe4\xb8\x8d  \xe9\x94\x99 \'\n        """"""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    @classmethod\n    def _is_chinese_char(cls, cp: int) -> bool:\n        r""""""Checks whether cp is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode\n        # block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean\n        # characters, despite its name. The modern Korean Hangul alphabet is a\n        # different block, as is Japanese Hiragana and Katakana. Those\n        # alphabets are used to write space-separated words, so they are not\n        # treated specially and handled like the all of the other languages.\n        if ((0x4E00 <= cp <= 0x9FFF) or\n                (0x3400 <= cp <= 0x4DBF) or\n                (0x20000 <= cp <= 0x2A6DF) or\n                (0x2A700 <= cp <= 0x2B73F) or\n                (0x2B740 <= cp <= 0x2B81F) or\n                (0x2B820 <= cp <= 0x2CEAF) or\n                (0xF900 <= cp <= 0xFAFF) or\n                (0x2F800 <= cp <= 0x2FA1F)):\n            return True\n\n        return False\n\n    @classmethod\n    def _clean_text(cls, text: str) -> str:\n        r""""""Performs invalid character removal and whitespace cleanup on text.\n\n        Example:\n            text = \'Texar-PyTorch\\tis an open-source\\ntoolkit based on PyTorch.\'\n            _clean_text(text)\n            # \'Texar-PyTorch is an open-source toolkit based on PyTorch.\'\n        """"""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer:\n    r""""""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab: Dict[str, int],\n                 unk_token: str,\n                 max_input_chars_per_word: int = 100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text: str) -> List[str]:\n        r""""""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n            input = ""unaffable""\n            output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n            text: A single token or whitespace separated tokens. This should\n                have already been passed through `BasicTokenizer`.\n\n        Returns:\n            A list of wordpiece tokens.\n        """"""\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            assert token is not None\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef whitespace_tokenize(text: str) -> List[str]:\n    r""""""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens: List[str] = text.split()\n    return tokens\n\n\ndef _is_whitespace(char: str) -> bool:\n    r""""""Checks whether `char` is a whitespace character.\n\n    Note: this function is not standard and should be considered for BERT\n    tokenization only. See the comments for more details.\n    """"""\n    # \\t, \\n, and \\r are technically control characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char in ("" "", ""\\t"", ""\\n"", ""\\r""):\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char: str) -> bool:\n    r""""""Checks whether `char` is a control character.\n\n    Note: this function is not standard and should be considered for BERT\n    tokenization only. See the comments for more details.\n    """"""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char in (""\\t"", ""\\n"", ""\\r""):\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char: str) -> bool:\n    r""""""Checks whether `char` is a punctuation character.\n\n    Note: this function is not standard and should be considered for BERT\n    tokenization only. See the comments for more details.\n    """"""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((33 <= cp <= 47) or (58 <= cp <= 64) or\n            (91 <= cp <= 96) or (123 <= cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
texar/torch/data/tokenizers/gpt2_tokenizer.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPre-trained GPT-2 tokenizer.\n\nCode structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_gpt2.py`\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport os\nimport json\nimport regex as re\n\nfrom texar.torch.modules.pretrained.gpt2 import PretrainedGPT2Mixin\nfrom texar.torch.data.tokenizers.tokenizer_base import TokenizerBase\nfrom texar.torch.data.tokenizers.gpt2_tokenizer_utils import \\\n    bytes_to_unicode, get_pairs\n\n__all__ = [\n    \'GPT2Tokenizer\',\n]\n\n\nclass GPT2Tokenizer(TokenizerBase, PretrainedGPT2Mixin):\n    r""""""Pre-trained GPT2 Tokenizer.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name of\n            pre-trained model (e.g., `117M`). Please refer to\n            :class:`~texar.torch.modules.PretrainedGPT2Mixin` for\n            all supported models.\n            If None, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    _IS_PRETRAINED = True\n    _MAX_INPUT_SIZE = {\n        \'gpt2-small\': 1024,\n        \'gpt2-medium\': 1024,\n        \'gpt2-large\': 1024,\n        \'gpt2-xl\': 1024,\n    }\n    _DEPRECATED_MAX_INPUT_SIZE = {\n        \'117M\': 1024,\n        \'345M\': 1024,\n    }\n    _MAX_INPUT_SIZE.update(_DEPRECATED_MAX_INPUT_SIZE)\n\n    _VOCAB_FILE_NAMES = {\n        \'vocab_file\': \'encoder.json\',\n        \'merges_file\': \'vocab.bpe\',\n    }\n    _VOCAB_FILE_MAP = {\n        \'vocab_file\': {\n            \'gpt2-small\': \'encoder.json\',\n            \'gpt2-medium\': \'encoder.json\',\n            \'gpt2-large\': \'encoder.json\',\n            \'gpt2-xl\': \'encoder.json\',\n            \'117M\': \'encoder.json\',\n            \'345M\': \'encoder.json\',\n        },\n        \'merges_file\': {\n            \'gpt2-small\': \'vocab.bpe\',\n            \'gpt2-medium\': \'vocab.bpe\',\n            \'gpt2-large\': \'vocab.bpe\',\n            \'gpt2-xl\': \'vocab.bpe\',\n            \'117M\': \'vocab.bpe\',\n            \'345M\': \'vocab.bpe\',\n        },\n    }\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        self.load_pretrained_config(pretrained_model_name, cache_dir, hparams)\n\n        super().__init__(hparams=None)\n\n        self.config = {\n            \'errors\': self.hparams[\'errors\']\n        }\n\n        if self.pretrained_model_dir is not None:\n            assert self.pretrained_model_name is not None\n            vocab_file = os.path.join(self.pretrained_model_dir,\n                                      self._VOCAB_FILE_MAP[\'vocab_file\']\n                                      [self.pretrained_model_name])\n            merges_file = os.path.join(self.pretrained_model_dir,\n                                       self._VOCAB_FILE_MAP[\'merges_file\']\n                                       [self.pretrained_model_name])\n            assert pretrained_model_name is not None\n            if self._MAX_INPUT_SIZE.get(pretrained_model_name):\n                self.max_len = self._MAX_INPUT_SIZE[pretrained_model_name]\n        else:\n            vocab_file = self.hparams[\'vocab_file\']\n            merges_file = self.hparams[\'merges_file\']\n            if self.hparams.get(\'max_len\'):\n                self.max_len = self.hparams[\'max_len\']\n\n        if not os.path.isfile(vocab_file):\n            raise ValueError(""Can\'t find a vocabulary file at path ""\n                             ""\'{}"".format(vocab_file))\n\n        if not os.path.isfile(merges_file):\n            raise ValueError(""Can\'t find a merges file at path ""\n                             ""\'{}"".format(merges_file))\n\n        with open(vocab_file) as fp:\n            self.encoder = json.load(fp)\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.errors = self.hparams[""errors""]  # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        with open(merges_file, encoding=\'utf-8\') as fp:\n            bpe_data = fp.read().split(\'\\n\')[1:-1]\n        bpe_merges = [tuple(merge.split()) for merge in bpe_data]\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache: Dict[str, str] = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for\n        # capitalized versions of contractions\n        self.pat = re.compile(r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| """""" +\n                              r""""""""?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n    def _map_text_to_token(self, text: str) -> List[str]:  # type: ignore\n        r""""""Tokenize a string. """"""\n        bpe_tokens: List[str] = []\n        for token in re.findall(self.pat, text):\n            token = \'\'.join(self.byte_encoder[b] for b in token.encode(\'utf-8\'))\n            bpe_tokens.extend(\n                bpe_token for bpe_token in self._bpe(token).split(\' \'))\n        return bpe_tokens\n\n    def save_vocab(self, save_dir: str) -> Tuple[str, str]:\n        r""""""Save the tokenizer vocabulary and merge files to a directory.""""""\n        if not os.path.isdir(save_dir):\n            raise ValueError(""Vocabulary path ({}) should be a ""\n                             ""directory"".format(save_dir))\n\n        vocab_file = os.path.join(save_dir,\n                                  self._VOCAB_FILE_NAMES[\'vocab_file\'])\n        merge_file = os.path.join(save_dir,\n                                  self._VOCAB_FILE_NAMES[\'merges_file\'])\n\n        with open(vocab_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.encoder, ensure_ascii=False))\n\n        index = 0\n        with open(merge_file, ""w"", encoding=""utf-8"") as writer:\n            writer.write(u\'#version: 0.2\\n\')\n            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(),\n                                                  key=lambda kv: kv[1]):\n                if index != token_index:\n                    print(""Saving vocabulary to {}: BPE merge indices are ""\n                          ""not consecutive. Please check that the tokenizer ""\n                          ""is not corrupted!"".format(merge_file))\n                    index = token_index\n                writer.write(\' \'.join(bpe_tokens) + u\'\\n\')\n                index += 1\n\n        return (vocab_file, merge_file)\n\n    def _bpe(self, token: str) -> str:\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(\n                pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word: List[str] = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except ValueError:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 \\\n                        and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            pairs = get_pairs(word)\n        word = \' \'.join(word)\n        self.cache[token] = word\n        return word\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self.encoder)\n\n    def _map_token_to_id(self, token: str) -> int:\n        r""""""Maps a token to an id using the vocabulary.""""""\n        return self.encoder.get(token, self.encoder.get(self.unk_token))\n\n    def _map_id_to_token(self, index: int) -> str:\n        r""""""Maps an id to a token using the vocabulary.""""""\n        token = self.decoder.get(index)\n        assert isinstance(token, str)\n        return token\n\n    def map_token_to_text(self, tokens: List[str]) -> str:\n        r""""""Maps a sequence of tokens (string) in a single string.""""""\n        text = \'\'.join(tokens)\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\n            \'utf-8\', errors=self.errors)\n        return text\n\n    def encode_text(  # type: ignore\n            self,\n            text: str,\n            max_seq_length: Optional[int] = None,\n            append_eos_token: bool = True) -> Tuple[List[int], int]:\n        r""""""Adds special tokens to a sequence and computes the corresponding\n        sequence length for GPT2 specific tasks. The sequence will be truncated\n        if its length is larger than ``max_seq_length``.\n\n        A GPT2 sequence has the following format:\n        `[bos_token]` X `[eos_token]` `[pad_token]`\n\n        Args:\n            text: Input text.\n            max_seq_length: Maximum sequence length.\n            append_eos_token: Whether to append ``eos_token`` after the\n                sequence.\n\n        Returns:\n            A tuple of `(input_ids, seq_len)`, where\n\n            - ``input_ids``: A list of input token ids with added\n              special tokens.\n            - ``seq_len``: The sequence length.\n        """"""\n        if max_seq_length is None:\n            max_seq_length = self.max_len\n\n        token_ids = self.map_text_to_id(text)\n        assert isinstance(token_ids, list)\n\n        bos_token_id = self._map_token_to_id(self.bos_token)\n        eos_token_id = self._map_token_to_id(self.eos_token)\n        pad_token_id = self._map_token_to_id(self.pad_token)\n\n        if append_eos_token:\n            input_ids = token_ids[:max_seq_length - 2]\n            input_ids = [bos_token_id] + input_ids + [eos_token_id]\n        else:\n            input_ids = token_ids[:max_seq_length - 1]\n            input_ids = [bos_token_id] + input_ids\n\n        seq_len = len(input_ids)\n\n        # Pad up to the maximum sequence length.\n        input_ids = input_ids + [pad_token_id] * (max_seq_length - seq_len)\n\n        assert len(input_ids) == max_seq_length\n\n        return input_ids, seq_len\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The tokenizer is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the tokenizer is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the tokenizer is defined by the\n          configurations in `hparams`.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""117M"",\n                ""vocab_file"": None,\n                ""merges_file"": None,\n                ""max_len"": 1024,\n                ""bos_token"": ""<|endoftext|>"",\n                ""eos_token"": ""<|endoftext|>"",\n                ""unk_token"": ""<|endoftext|>"",\n                ""pad_token"": ""<|endoftext|>"",\n                ""errors"": ""replace"",\n                ""name"": ""gpt2_tokenizer"",\n            }\n\n        Here:\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained GPT2 model.\n\n        `""vocab_file""`: str or None\n            The path to a vocabulary json file mapping tokens to ids.\n\n        `""merges_file""`: str or None\n            The path to a merges file.\n\n        `""max_len""`: int\n            The maximum sequence length that this model might ever be used with.\n\n        `""bos_token""`: str\n            Beginning of sentence token\n\n        `""eos_token""`: str\n            End of sentence token\n\n        `""unk_token""`: str\n            Unknown token\n\n        `""pad_token""`: str\n            Padding token\n\n        `""errors""`: str\n            Response when mapping tokens to text fails. The possible values are\n            `ignore`, `replace`, and `strict`.\n\n        `""name""`: str\n            Name of the tokenizer.\n        """"""\n        return {\n            \'pretrained_model_name\': \'117M\',\n            \'vocab_file\': None,\n            \'merges_file\': None,\n            \'max_len\': 1024,\n            \'bos_token\': \'<|endoftext|>\',\n            \'eos_token\': \'<|endoftext|>\',\n            \'unk_token\': \'<|endoftext|>\',\n            \'pad_token\': \'<|endoftext|>\',\n            \'errors\': \'replace\',\n            \'name\': \'gpt2_tokenizer\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str):\n        r""""""Returns the configuration of the pre-trained GPT2 tokenizer.""""""\n        return {\n            \'vocab_file\': None,\n            \'merges_file\': None,\n            \'max_len\': 1024,\n            \'bos_token\': \'<|endoftext|>\',\n            \'eos_token\': \'<|endoftext|>\',\n            \'unk_token\': \'<|endoftext|>\',\n            \'pad_token\': \'<|endoftext|>\',\n            \'errors\': \'replace\',\n        }\n'"
texar/torch/data/tokenizers/gpt2_tokenizer_utils.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utils of pre-trained GPT2 tokenizer.\n\nCode structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_gpt2.py`\n""""""\n\nfrom functools import lru_cache\n\n__all__ = [\n    ""bytes_to_unicode"",\n    ""get_pairs"",\n]\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    r""""""Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings. This means you need a\n    large number of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing\n    around 5K for decent coverage. This is a significant percentage of your\n    normal, say, 32K bpe vocab. To avoid that, we want lookup tables between\n    utf-8 bytes and unicode strings.\n\n    Note that this function avoids the mapping to whitespace and control\n    characters, which is designed specifically for GPT-2 BPE.\n    """"""\n    bs = list(range(ord(""!""), ord(""~"") + 1)) + list(\n        range(ord(""\xc2\xa1""), ord(""\xc2\xac"") + 1)) + list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"") + 1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    r""""""Return set of symbol pairs in a word. Word is represented as tuple of\n    symbols (symbols being variable-length strings).\n\n    Example:\n        word = ""texar""\n        get_pairs(word)\n        # {(\'t\', \'e\'), (\'e\', \'x\'), (\'x\', \'a\'), (\'a\', \'r\')}\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n'"
texar/torch/data/tokenizers/roberta_tokenizer.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPre-trained RoBERTa tokenizer.\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom texar.torch.data.tokenizers.gpt2_tokenizer import GPT2Tokenizer\nfrom texar.torch.utils.utils import truncate_seq_pair\n\n__all__ = [\n    \'RoBERTaTokenizer\',\n]\n\n_ROBERTA_PATH = ""https://s3.amazonaws.com/models.huggingface.co/bert/""\n\n\nclass RoBERTaTokenizer(GPT2Tokenizer):\n    r""""""Pre-trained RoBERTa Tokenizer.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name of\n            pre-trained model (e.g., `roberta-base`). Please refer to\n            :class:`~texar.torch.modules.PretrainedRoBERTaMixin` for\n            all supported models.\n            If None, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    _MODEL2URL = {\n        \'roberta-base\': [\n            _ROBERTA_PATH + \'roberta-base-vocab.json\',\n            _ROBERTA_PATH + \'roberta-base-merges.txt\',\n        ],\n        \'roberta-large\': [\n            _ROBERTA_PATH + \'roberta-large-vocab.json\',\n            _ROBERTA_PATH + \'roberta-large-merges.txt\',\n        ],\n    }\n    _MAX_INPUT_SIZE = {\n        \'roberta-base\': 512,\n        \'roberta-large\': 512,\n    }\n    _VOCAB_FILE_MAP = {\n        \'vocab_file\': {\n            \'roberta-base\': \'roberta-base-vocab.json\',\n            \'roberta-large\': \'roberta-large-vocab.json\',\n        },\n        \'merges_file\': {\n            \'roberta-base\': \'roberta-base-merges.txt\',\n            \'roberta-large\': \'roberta-large-merges.txt\',\n        },\n    }\n\n    def encode_text(self,  # type: ignore\n                    text_a: str,\n                    text_b: Optional[str] = None,\n                    max_seq_length: Optional[int] = None) -> \\\n            Tuple[List[int], List[int]]:\n        r""""""Adds special tokens to a sequence or sequence pair and computes the\n        corresponding input mask for RoBERTa specific tasks.\n        The sequence will be truncated if its length is larger than\n        ``max_seq_length``.\n\n        A RoBERTa sequence has the following format:\n        `[cls_token]` X `[sep_token]`\n\n        A RoBERTa sequence pair has the following format:\n        `[cls_token]` A `[spe_token]` `[sep_token]` B `[sep_token]`\n\n        Args:\n            text_a: The first input text.\n            text_b: The second input text.\n            max_seq_length: Maximum sequence length.\n\n        Returns:\n            A tuple of `(input_ids, segment_ids, input_mask)`, where\n\n            - ``input_ids``: A list of input token ids with added\n              special token ids.\n            - ``input_mask``: A list of mask ids. The mask has 1 for real\n              tokens and 0 for padding tokens. Only real tokens are\n              attended to.\n        """"""\n        if max_seq_length is None:\n            max_seq_length = self.max_len\n\n        cls_token_id = self._map_token_to_id(self.cls_token)\n        sep_token_id = self._map_token_to_id(self.sep_token)\n\n        token_ids_a = self.map_text_to_id(text_a)\n        assert isinstance(token_ids_a, list)\n\n        token_ids_b = None\n        if text_b:\n            token_ids_b = self.map_text_to_id(text_b)\n\n        if token_ids_b:\n            assert isinstance(token_ids_b, list)\n            # Modifies `token_ids_a` and `token_ids_b` in place so that the\n            # total length is less than the specified length.\n            # Account for <s>, </s>, </s>, </s> with ""- 4""\n            truncate_seq_pair(token_ids_a, token_ids_b, max_seq_length - 4)\n\n            input_ids = ([cls_token_id] + token_ids_a + [sep_token_id] +\n                         [sep_token_id] + token_ids_b + [sep_token_id])\n        else:\n            # Account for <s> and </s> with ""- 2""\n            token_ids_a = token_ids_a[:max_seq_length - 2]\n\n            input_ids = [cls_token_id] + token_ids_a + [sep_token_id]\n\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the maximum sequence length.\n        input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n        input_mask = input_mask + [0] * (max_seq_length - len(input_mask))\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n\n        return input_ids, input_mask\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The tokenizer is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the tokenizer is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the tokenizer is defined by the\n          configurations in `hparams`.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""roberta-base"",\n                ""vocab_file"": None,\n                ""merges_file"": None,\n                ""max_len"": 512,\n                ""bos_token"": ""<s>"",\n                ""eos_token"": ""</s>"",\n                ""sep_token"": ""</s>"",\n                ""cls_token"": ""</s>"",\n                ""unk_token"": ""<unk>"",\n                ""pad_token"": ""<pad>"",\n                ""mask_token"": ""<mask>"",\n                ""errors"": ""replace"",\n                ""name"": ""roberta_tokenizer"",\n            }\n\n        Here:\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained RoBERTa model.\n\n        `""vocab_file""`: str or None\n            The path to a vocabulary json file mapping tokens to ids.\n\n        `""merges_file""`: str or None\n            The path to a merges file.\n\n        `""max_len""`: int\n            The maximum sequence length that this model might ever be used with.\n\n        `""bos_token""`: str\n            Beginning of sentence token.\n\n        `""eos_token""`: str\n            End of sentence token.\n\n        `""sep_token""`: str\n            Separation token.\n\n        `""cls_token""`: str\n            Classification token.\n\n        `""unk_token""`: str\n            Unknown token.\n\n        `""pad_token""`: str\n            Padding token.\n\n        `""mask_token""`: str\n            Masking token.\n\n        `""errors""`: str\n            Response when decoding fails. The possible values are\n            `ignore`, `replace`, and `strict`.\n\n        `""name""`: str\n            Name of the tokenizer.\n        """"""\n        return {\n            \'pretrained_model_name\': \'roberta-base\',\n            \'vocab_file\': None,\n            \'merges_file\': None,\n            \'max_len\': 512,\n            \'bos_token\': \'<s>\',\n            \'eos_token\': \'</s>\',\n            \'sep_token\': \'</s>\',\n            \'cls_token\': \'</s>\',\n            \'unk_token\': \'<unk>\',\n            \'pad_token\': \'<pad>\',\n            \'mask_token\': \'<mask>\',\n            \'errors\': \'replace\',\n            \'name\': \'roberta_tokenizer\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str):\n        r""""""Returns the configuration of the pre-trained RoBERTa tokenizer.""""""\n        return {\n            \'vocab_file\': None,\n            \'merges_file\': None,\n            \'max_len\': 512,\n            \'bos_token\': \'<s>\',\n            \'eos_token\': \'</s>\',\n            \'sep_token\': \'</s>\',\n            \'cls_token\': \'<s>\',\n            \'unk_token\': \'<unk>\',\n            \'pad_token\': \'<pad>\',\n            \'mask_token\': \'<mask>\',\n            \'errors\': \'replace\',\n        }\n'"
texar/torch/data/tokenizers/sentencepiece_tokenizer.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nSentencePiece Tokenizer.\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport os\nfrom shutil import copyfile, move\n\nimport sentencepiece as spm\n\nfrom texar.torch.data.tokenizers.tokenizer_base import TokenizerBase\nfrom texar.torch.modules.pretrained.pretrained_base import default_download_dir\nfrom texar.torch.utils.utils_io import maybe_create_dir\n\n__all__ = [\n    ""SentencePieceTokenizer""\n]\n\n\nclass SentencePieceTokenizer(TokenizerBase):\n    r""""""SentencePiece Tokenizer. This class is a wrapper of Google\'s\n    `SentencePiece`_ with richer ready-to-use functionalities such as\n    adding tokens and saving/loading.\n\n    `SentencePiece` is an unsupervised text tokenizer mainly for Neural\n    Network-based text generation systems where the vocabulary size\n    is predetermined prior to the neural model training. `SentencePiece`\n    implements sub-word units (e.g., byte-pair-encoding (BPE) and unigram\n    language model) with the extension of direct training from raw sentences.\n\n    The supported algorithms in `SentencePiece` are: ``bpe``, ``word``,\n    ``char``, and ``unigram``, which is specified in :attr:`hparams`.\n\n    Args:\n        cache_dir (optional): the path to a folder in which the\n            trained `sentencepiece` model will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. _`SentencePiece`: https://github.com/google/sentencepiece\n    """"""\n\n    _IS_PRETRAINED = False\n    _VOCAB_FILE_NAMES = {\n        \'vocab_file\': \'spiece.model\',\n    }\n    _TRAIN_ARG_MAP = {\n        \'text_file\': \'input\',\n        \'model_type\': \'model_type\',\n        \'vocab_size\': \'vocab_size\',\n        \'bos_token\': \'bos_piece\',\n        \'eos_token\': \'eos_piece\',\n        \'unk_token\': \'unk_piece\',\n        \'pad_token\': \'pad_piece\',\n    }\n\n    def __init__(self,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        super().__init__(hparams=hparams)\n\n        self.__dict__: Dict\n\n        if self.hparams[\'vocab_file\'] is not None:\n            self.vocab_file = self.hparams[\'vocab_file\']\n            self.sp_model = spm.SentencePieceProcessor()\n            self.sp_model.Load(self.vocab_file)\n\n            bos_id = self.sp_model.bos_id()\n            eos_id = self.sp_model.eos_id()\n            unk_id = self.sp_model.unk_id()\n            pad_id = self.sp_model.pad_id()\n\n            self.bos_token = None\n            if bos_id != -1:\n                self.bos_token = self.sp_model.IdToPiece(bos_id)\n\n            self.eos_token = None\n            if eos_id != -1:\n                self.eos_token = self.sp_model.IdToPiece(eos_id)\n\n            self.unk_token = None\n            if unk_id != -1:\n                self.unk_token = self.sp_model.IdToPiece(unk_id)\n\n            self.pad_token = None\n            if pad_id != -1:\n                self.pad_token = self.sp_model.IdToPiece(pad_id)\n\n        elif self.hparams[\'text_file\'] is not None:\n            cmd = [\'--model_prefix=spiece\']\n            for arg, val in self.hparams.items():\n                if arg in self._TRAIN_ARG_MAP:\n                    if arg in self._SPECIAL_TOKENS_ATTRIBUTES and val is None:\n                        cmd.append(\'--\' + arg.replace(\'token\', \'id\') + \'=-1\')\n                    else:\n                        cmd.append(\'--\' + self._TRAIN_ARG_MAP[arg] + \'=\' +\n                                   str(val))\n\n            cache_path = self.train("" "".join(cmd), cache_dir)\n            self.vocab_file = os.path.join(\n                cache_path, self._VOCAB_FILE_NAMES[\'vocab_file\'])\n            self.sp_model = spm.SentencePieceProcessor()\n            self.sp_model.Load(self.vocab_file)\n        else:\n            raise ValueError(""\'vocab_file\' and \'text_file\' can not be None ""\n                             ""at the same time."")\n\n    @classmethod\n    def train(cls, cmd: str,  # type: ignore\n              cache_dir: Optional[str] = None) -> str:\n        r""""""Trains the tokenizer from the raw text file. This function is\n        a wrapper of `sentencepiece.SentencePieceTrainer.Train`_ function.\n\n        Example:\n\n        .. code-block:: python\n\n            SentencePieceTokenizer.train(\'--input=test/botchan.txt\n            --model_prefix=m --vocab_size=1000\')\n\n        Args:\n            cmd (str): the command for the tokenizer training procedure.\n                See ``sentencepiece.SentencePieceTrainer.Train`` for the\n                detailed usage.\n            cache_dir (optional): the path to a folder in which the trained\n                `sentencepiece` model will be cached. If `None` (default),\n                a default directory (`texar_pytorch` folder under user\'s home\n                directory) will be used.\n\n        Returns:\n            Path to the cache directory.\n\n        .. _`sentencepiece.SentencePieceTrainer.Train`:\n            https://github.com/google/sentencepiece/blob/master/python/sentencepiece.py\n        """"""\n        if cache_dir is None:\n            cache_path = str(default_download_dir(\'SentencePiece\'))\n        else:\n            if not os.path.isdir(cache_dir):\n                raise ValueError(f""Cache directory ({cache_dir}) should be a ""\n                                 f""directory."")\n            cache_path = os.path.abspath(cache_dir)\n\n        maybe_create_dir(cache_path)\n\n        spm.SentencePieceTrainer.Train(cmd)\n        cwd = os.getcwd()\n\n        vocab_file = os.path.join(cwd, cls._VOCAB_FILE_NAMES[\'vocab_file\'])\n        out_vocab_file = os.path.join(\n            cache_path, cls._VOCAB_FILE_NAMES[\'vocab_file\'])\n\n        if os.path.abspath(vocab_file) != os.path.abspath(out_vocab_file):\n            move(vocab_file, out_vocab_file)\n\n        # Delete spiece.vocab (We might want to keep it as well)\n        extra_file = vocab_file.rstrip(\'model\') + \'vocab\'\n        os.remove(extra_file)\n\n        return cache_path\n\n    # spm.SentencePieceProcessor() is a SwigPyObject object which cannot be\n    # pickled. We need to define __getstate__ here.\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[""sp_model""] = None\n        state[""vocab_file""] = None\n        return state, self.vocab_file\n\n    # spm.SentencePieceProcessor() is a SwigPyObject object which cannot be\n    # pickled. We need to define __setstate__ here.\n    def __setstate__(self, d):\n        self.__dict__, self.vocab_file = d\n        self.sp_model = spm.SentencePieceProcessor()\n        self.sp_model.Load(self.vocab_file)\n\n    def save_vocab(self, save_dir: str) -> Tuple[str]:\n        r""""""Save the sentencepiece vocabulary (copy original file) to\n        a directory.\n        """"""\n        if not os.path.isdir(save_dir):\n            raise ValueError(""Vocabulary path ({}) should be a ""\n                             ""directory"".format(save_dir))\n        out_vocab_file = os.path.join(\n            save_dir, self._VOCAB_FILE_NAMES[\'vocab_file\'])\n\n        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n            copyfile(self.vocab_file, out_vocab_file)\n\n        return (out_vocab_file,)\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self.sp_model)\n\n    def _map_text_to_token(self, text: str) -> List[str]:  # type: ignore\n        return self.sp_model.EncodeAsPieces(text)\n\n    def _map_token_to_id(self, token: str) -> int:\n        return self.sp_model.PieceToId(token)\n\n    def _map_id_to_token(self, index: int) -> str:\n        token = self.sp_model.IdToPiece(index)\n        return token\n\n    def map_token_to_text(self, tokens: List[str]) -> str:\n        r""""""Maps a sequence of tokens (string) in a single string.""""""\n        out_string = self.sp_model.DecodePieces(tokens)\n        return out_string\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * If `hparams[\'vocab_file\']` is specified, the tokenizer is directly\n          loaded from the vocabulary file. In this case, all other\n          configurations in `hparams` are ignored.\n        * Otherwise, the tokenizer is automatically trained based on\n          `hparams[\'text_file\']`. In this case, `hparams[\'vocab_size\']` must\n          be specified.\n        * `hparams[\'vocab_file\']` and `hparams[\'text_file\']` can not be None\n          at the same time.\n\n        .. code-block:: python\n\n            {\n                ""vocab_file"": None,\n                ""text_file"": None,\n                ""vocab_size"": None,\n                ""model_type"": ""unigram"",\n                ""bos_token"": ""<s>"",\n                ""eos_token"": ""</s>"",\n                ""unk_token"": ""<unk>"",\n                ""pad_token"": ""<pad>"",\n            }\n\n        Here:\n\n        `""vocab_file""`: str or None\n            The path to a sentencepiece vocabulary file.\n\n        `""text_file""`: str or None\n            Comma separated list of input sentences.\n\n        `""vocab_size""`: int or None\n            Vocabulary size. The user can specify the vocabulary size, and the\n            tokenizer training procedure will train and yield a vocabulary\n            of the specified size.\n\n        `""model_type""`: str\n            Model algorithm to train the tokenizer. Available algorithms are:\n            ``bpe``, ``word``, ``char``, and ``unigram``.\n\n        `""bos_token""`: str or None\n            Beginning of sentence token. Set None to disable ``bos_token``.\n\n        `""eos_token""`: str or None\n            End of sentence token. Set None to disable ``eos_token``.\n\n        `""unk_token""`: str or None\n            Unknown token. Set None to disable ``unk_token``.\n\n        `""pad_token""`: str or None\n            Padding token. Set None to disable ``pad_token``.\n        """"""\n        return {\n            \'vocab_file\': None,\n            \'text_file\': None,\n            \'vocab_size\': None,\n            \'model_type\': \'unigram\',\n            \'bos_token\': \'<s>\',\n            \'eos_token\': \'</s>\',\n            \'pad_token\': \'<pad>\',\n            \'unk_token\': \'<unk>\',\n        }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str) -> Dict[str, Any]:\n        pass\n\n    def _init_from_checkpoint(self, pretrained_model_name: str,\n                              cache_dir: str, **kwargs):\n        pass\n'"
texar/torch/data/tokenizers/t5_tokenizer.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPre-trained T5 tokenizer.\n""""""\n\nfrom typing import Any, Dict, Optional\n\nimport os\nimport re\n\nfrom texar.torch.data.tokenizers.sentencepiece_tokenizer \\\n    import SentencePieceTokenizer\nfrom texar.torch.modules.pretrained.t5 import PretrainedT5Mixin\n\n__all__ = [\n    \'T5Tokenizer\',\n]\n\n\nclass T5Tokenizer(SentencePieceTokenizer, PretrainedT5Mixin):\n    r""""""Pre-trained T5 Tokenizer.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name of\n            pre-trained model (e.g., `T5-Small`). Please refer to\n            :class:`~texar.torch.modules.PretrainedT5Mixin` for\n            all supported models.\n            If None, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    _IS_PRETRAINED = True\n\n    _VOCAB_FILE_NAMES = {\n        \'vocab_file\': \'sentencepiece.model\'\n    }\n\n    _MAX_INPUT_SIZE = {\n        \'T5-Small\': 512,\n        \'T5-Base\': 512,\n        \'T5-Large\': 512,\n        \'T5-3B\': 512,\n        \'T5-11B\': 512\n    }\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n\n        self.load_pretrained_config(pretrained_model_name, cache_dir, hparams)\n\n        if self.pretrained_model_dir is not None:\n            assert self.pretrained_model_name is not None\n            vocab_file = os.path.join(self.pretrained_model_dir,\n                                      self._VOCAB_FILE_NAMES[\'vocab_file\'])\n\n            if self._MAX_INPUT_SIZE.get(self.pretrained_model_name):\n                self.max_len = self._MAX_INPUT_SIZE[self.pretrained_model_name]\n            setattr(self.hparams, \'vocab_file\', vocab_file)\n        else:\n            if self.hparams.get(\'max_len\'):\n                self.max_len = self.hparams[\'max_len\']\n\n        # Add extra_ids to the special token list\n        additional_special_tokens = []\n        extra_ids = self.hparams[\'extra_ids\']\n        if extra_ids > 0:\n            additional_special_tokens.extend(\n                [""<extra_id_{}>"".format(i) for i in range(extra_ids)])\n\n        setattr(self.hparams, \'additional_special_tokens\',\n                additional_special_tokens)\n\n        super().__init__(hparams=None)\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The tokenizer is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the tokenizer is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the tokenizer is defined by the\n          configurations in `hparams`.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""T5-Small"",\n                ""vocab_file"": None,\n                ""max_len"": 512,\n                ""bos_token"": None,\n                ""eos_token"": ""</s>"",\n                ""unk_token"": ""<unk>"",\n                ""pad_token"": ""<pad>"",\n                ""extra_ids"": 100,\n                ""additional_special_tokens"": [],\n                ""name"": ""t5_tokenizer"",\n            }\n\n        Here:\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained T5 model.\n\n        `""vocab_file""`: str or None\n            The path to a sentencepiece vocabulary file.\n\n        `""max_len""`: int or None\n            The maximum sequence length that this model might ever be used with.\n\n        `""bos_token""`: str or None\n            Beginning of sentence token. Set None to disable ``bos_token``.\n\n        `""eos_token""`: str\n            End of sentence token. Set None to disable ``eos_token``.\n\n        `""unk_token""`: str\n            Unknown token. Set None to disable ``unk_token``.\n\n        `""pad_token""`: str\n            Padding token. Set None to disable ``pad_token``.\n\n        `""extra_ids""`: int\n            Add a number of extra ids added to the end of the vocabulary for\n            use as sentinels. These tokens are accessible as `<extra_id_{%d}>`\n            where `{%d}` is a number between 0 and extra_ids-1. Extra tokens\n            are indexed from the end of the vocabulary up to beginning\n            (<extra_id_0> is the last token in the vocabulary) (like in T5\n            preprocessing) see:\n            `https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117`\n\n        `""additional_special_tokens""`: list\n            A list of additional special tokens.\n\n        `""name""`: str\n            Name of the tokenizer.\n        """"""\n        return {\n            \'pretrained_model_name\': \'T5-Small\',\n            \'vocab_file\': None,\n            \'max_len\': 512,\n            \'bos_token\': None,\n            \'eos_token\': \'</s>\',\n            \'unk_token\': \'<unk>\',\n            \'pad_token\': \'<pad>\',\n            \'extra_ids\': 100,\n            \'additional_special_tokens\': [],\n            \'name\': \'t5_tokenizer\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self.sp_model) + self.hparams[\'extra_ids\']\n\n    def _map_token_to_id(self, token: str) -> int:\n        if token.startswith(""<extra_id_""):\n            match = re.match(r""<extra_id_(\\d+)>"", token)\n            num = int(match.group(1))  # type: ignore\n            return self.vocab_size - num - 1\n        return self.sp_model.PieceToId(token)\n\n    def _map_id_to_token(self, index: int) -> str:\n        if index < self.sp_model.get_piece_size():\n            token = self.sp_model.IdToPiece(index)\n        else:\n            token = ""<extra_id_{}>"".format(self.vocab_size - 1 - index)\n        return token\n'"
texar/torch/data/tokenizers/tokenizer_base.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for all tokenizers.\n\nThe code structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_utils.py`\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple, overload\n\nimport json\nimport os\nimport warnings\n\nfrom texar.torch.module_base import ModuleBase\n\n__all__ = [\n    ""TokenizerBase"",\n]\n\nSPECIAL_TOKENS_MAP_FILE = \'special_tokens_map.json\'\nADDED_TOKENS_FILE = \'added_tokens.json\'\nCONFIG_FILE = \'config.json\'\n\n\nclass TokenizerBase(ModuleBase):\n    r""""""Base class inherited by all tokenizer classes. This class\n    handles downloading and loading pre-trained tokenizer and adding tokens to\n    the vocabulary.\n\n    Derived class can set up a few special tokens to be used in common scripts\n    and internals: :attr:`bos_token`, :attr:`eos_token`, :attr:`unk_token`,\n    :attr:`sep_token`, :attr:`pad_token`, :attr:`cls_token`,\n    :attr:`mask_token`, and :attr:`additional_special_tokens`.\n\n    We defined an :attr:`added_tokens_encoder` to add new tokens to the\n    vocabulary without having to handle the specific vocabulary augmentation\n    methods of the various underlying dictionary structures (`BPE`,\n    `sentencepiece` ...).\n    """"""\n\n    _IS_PRETRAINED: bool\n    _MAX_INPUT_SIZE: Dict[str, Optional[int]]\n    _VOCAB_FILE_NAMES: Dict[str, str]\n    _VOCAB_FILE_MAP: Dict[str, Dict[str, str]]\n    _SPECIAL_TOKENS_ATTRIBUTES = [""bos_token"", ""eos_token"", ""unk_token"",\n                                  ""sep_token"", ""pad_token"", ""cls_token"",\n                                  ""mask_token"", ""additional_special_tokens""]\n\n    def __init__(self, hparams):\n        super().__init__(hparams=hparams)\n\n        self.config = None\n\n        self.bos_token = None\n        self.eos_token = None\n        self.unk_token = None\n        self.sep_token = None\n        self.pad_token = None\n        self.cls_token = None\n        self.mask_token = None\n        self.additional_special_tokens = []\n\n        self.max_len = int(1e12)\n        self.added_tokens_encoder = {}\n        self.added_tokens_decoder = {}\n\n        for key, value in self.hparams.items():\n            if key in self._SPECIAL_TOKENS_ATTRIBUTES:\n                if key == \'additional_special_tokens\':\n                    assert isinstance(value, (list, tuple)) and \\\n                           all(isinstance(v, str) for v in value)\n                else:\n                    if value is not None:\n                        assert isinstance(value, str)\n                    else:\n                        warnings.warn(f""Trying to set None as value special ""\n                                      f""token \'{key}\'. Proceed only if you""\n                                      f"" are sure!"", UserWarning)\n                setattr(self, key, value)\n\n    @classmethod\n    def load(cls, pretrained_model_path: str, configs: Optional[Dict] = None):\n        r""""""Instantiate a tokenizer from the vocabulary files or the saved\n        tokenizer files.\n\n        Args:\n            pretrained_model_path: The path to a vocabulary file or a folder\n                that contains the saved pre-trained tokenizer files.\n            configs: Tokenizer configurations. You can overwrite the original\n                tokenizer configurations saved in the configuration file\n                by this dictionary.\n\n        Returns:\n            A tokenizer instance.\n        """"""\n        vocab_files = {}\n        # Look for the tokenizer main vocabulary files\n        for file_id, file_name in cls._VOCAB_FILE_NAMES.items():\n            full_file_name: Optional[str]\n            if os.path.isdir(pretrained_model_path):\n                # If a directory is provided we look for the standard file name\n                full_file_name = os.path.join(pretrained_model_path, file_name)\n            else:\n                # If a path to a file is provided we use it (will only work\n                # for non-BPE tokenizer using a single vocabulary file)\n                full_file_name = pretrained_model_path\n            if not os.path.exists(full_file_name):\n                print(""Can\'t find file {}. We won\'t load it."".format(\n                    full_file_name))\n                full_file_name = None\n            vocab_files[file_id] = full_file_name\n\n        # Look for the additional tokens files\n        all_vocab_files_names = {\n            \'added_tokens_file\': ADDED_TOKENS_FILE,\n            \'special_tokens_map_file\': SPECIAL_TOKENS_MAP_FILE,\n            \'config_file\': CONFIG_FILE}\n\n        # If a path to a file was provided, get the parent directory\n        saved_directory = pretrained_model_path\n        if os.path.exists(saved_directory) and not os.path.isdir(\n                saved_directory):\n            saved_directory = os.path.dirname(saved_directory)\n\n        for file_id, file_name in all_vocab_files_names.items():\n            full_file_name = os.path.join(saved_directory, file_name)\n            if not os.path.exists(full_file_name):\n                print(""Can\'t find file {}. We won\'t load it."".format(\n                    full_file_name))\n                full_file_name = None\n            vocab_files[file_id] = full_file_name\n\n        if all(full_file_name is None for full_file_name in\n               vocab_files.values()):\n            raise ValueError(""Can\'t find tokenizer files in {}."".format(\n                saved_directory))\n\n        kwargs: Dict[str, Any]\n        if cls._IS_PRETRAINED:\n            kwargs = {\'pretrained_model_name\': None}\n        else:\n            kwargs = {}\n\n        added_tokens_file = vocab_files.pop(\'added_tokens_file\', None)\n        special_tokens_map_file = vocab_files.pop(\n            \'special_tokens_map_file\', None)\n        tokenizer_config_file = vocab_files.pop(\'config_file\', None)\n\n        for args_name, file_path in vocab_files.items():\n            if args_name not in kwargs:\n                kwargs[args_name] = file_path\n\n        if special_tokens_map_file is not None:\n            with open(special_tokens_map_file, encoding=""utf-8"") as f:\n                special_tokens_map = json.load(f)\n            for key, value in special_tokens_map.items():\n                if key not in kwargs:\n                    kwargs[key] = value\n\n        if tokenizer_config_file is not None:\n            with open(tokenizer_config_file, encoding=""utf-8"") as f:\n                tokenizer_config = json.load(f)\n            for key, value in tokenizer_config.items():\n                if key not in kwargs:\n                    kwargs[key] = value\n\n        if configs is not None:\n            for key, value in configs.items():\n                kwargs[key] = value\n\n        tokenizer = cls(hparams=kwargs)\n\n        # Add supplementary tokens.\n        if added_tokens_file is not None:\n            with open(added_tokens_file, encoding=""utf-8"") as f:\n                added_tok_encoder = json.load(f)\n            added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}\n            tokenizer.added_tokens_encoder.update(added_tok_encoder)\n            tokenizer.added_tokens_decoder.update(added_tok_decoder)\n\n        return tokenizer\n\n    def save(self, save_dir: str) -> Tuple[str]:\n        r""""""Save the tokenizer vocabulary files (with added tokens), tokenizer\n        configuration file and a dictionary mapping special token class\n        attributes (:attr:`cls_token`, :attr:`unk_token`, ...) to their values\n        (`<unk>`, `<cls>`, ...) to a directory, so that it can be re-loaded\n        using the :meth:`~load`.\n\n        Args:\n            save_dir: The path to a folder in which the tokenizer files\n                will be saved.\n\n        Return:\n            The paths to the vocabulary file, added token file, special token\n            mapping file, and the configuration file.\n        """"""\n        if not os.path.isdir(save_dir):\n            raise ValueError(""Saving directory ({}) should be a ""\n                             ""directory"".format(save_dir))\n\n        special_tokens_map_file = os.path.join(save_dir,\n                                               SPECIAL_TOKENS_MAP_FILE)\n        added_tokens_file = os.path.join(save_dir, ADDED_TOKENS_FILE)\n        config_file = os.path.join(save_dir, CONFIG_FILE)\n\n        with open(special_tokens_map_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.special_tokens_map, ensure_ascii=False))\n\n        with open(added_tokens_file, \'w\', encoding=\'utf-8\') as f:\n            if self.added_tokens_encoder:\n                out_str = json.dumps(self.added_tokens_encoder,\n                                     ensure_ascii=False)\n            else:\n                out_str = u""{}""\n            f.write(out_str)\n\n        with open(config_file, \'w\', encoding=\'utf-8\') as f:\n            if self.config:\n                out_str = json.dumps(self.config, ensure_ascii=False)\n            else:\n                out_str = u""{}""\n            f.write(out_str)\n\n        vocab_files = self.save_vocab(save_dir)\n        return vocab_files + (special_tokens_map_file, added_tokens_file,\n                              config_file)\n\n    def save_vocab(self, save_dir):\n        r""""""Save the tokenizer vocabulary to a directory. This method does not\n        save added tokens, special token mappings, and the configuration file.\n\n        Please use :meth:`~save` to save the full tokenizer state so\n        that it can be reloaded using :meth:`~load`.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def vocab_size(self) -> int:\n        raise NotImplementedError\n\n    def __len__(self) -> int:\n        return self.vocab_size + len(self.added_tokens_encoder)\n\n    def add_tokens(self, new_tokens: List[Optional[str]]) -> int:\n        r""""""Add a list of new tokens to the tokenizer class. If the new tokens\n        are not in the vocabulary, they are added to the\n        :attr:`added_tokens_encoder` with indices starting from the last index\n        of the current vocabulary.\n\n        Args:\n            new_tokens: A list of new tokens.\n\n        Returns:\n            Number of tokens added to the vocabulary which can be used to\n            correspondingly increase the size of the associated model embedding\n            matrices.\n        """"""\n        if not new_tokens:\n            return 0\n\n        to_add_tokens = []\n        for token in new_tokens:\n            assert isinstance(token, str)\n            if token != self.unk_token and \\\n                    (self.map_token_to_id(token) ==\n                     self.map_token_to_id(self.unk_token)):\n                to_add_tokens.append(token)\n\n        added_tok_encoder = dict((tok, len(self) + i) for i, tok in\n                                 enumerate(to_add_tokens))\n        added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}\n        self.added_tokens_encoder.update(added_tok_encoder)\n        self.added_tokens_decoder.update(added_tok_decoder)\n\n        return len(to_add_tokens)\n\n    def add_special_tokens(self, special_tokens_dict: Dict[str, str]) -> int:\n        r""""""Add a dictionary of special tokens to the encoder and link them to\n        class attributes. If the special tokens are not in the vocabulary, they\n        are added to it and indexed starting from the last index of the\n        current vocabulary.\n\n        Args:\n            special_tokens_dict: A dictionary mapping special token class\n                attributes (:attr:`cls_token`, :attr:`unk_token`, ...) to their\n                values (`<unk>`, `<cls>`, ...).\n\n        Returns:\n            Number of tokens added to the vocabulary which can be used to\n            correspondingly increase the size of the associated model embedding\n            matrices.\n        """"""\n        if not special_tokens_dict:\n            return 0\n\n        added_tokens = 0\n        for key, value in special_tokens_dict.items():\n            assert key in self._SPECIAL_TOKENS_ATTRIBUTES\n            if key == \'additional_special_tokens\':\n                assert isinstance(value, (list, tuple)) and all(\n                    isinstance(t, str) for t in value)\n                added_tokens += self.add_tokens(value)\n            else:\n                assert isinstance(value, str)\n                added_tokens += self.add_tokens([value])\n            setattr(self, key, value)\n\n        return added_tokens\n\n    def map_text_to_token(self, text: Optional[str],\n                          **kwargs) -> List[str]:\n        r""""""Maps a string to a sequence of tokens (string), using the\n        tokenizer. Split in words for word-based vocabulary or sub-words for\n        sub-word-based vocabularies (`BPE`/`SentencePiece`/`WordPiece`).\n        This function also takes care of the added tokens.\n\n        Args:\n            text: A input string.\n\n        Return:\n            A list of tokens.\n        """"""\n\n        def split_on_tokens(tok_list, string):\n            if not string:\n                return []\n            if not tok_list:\n                return self._map_text_to_token(string, **kwargs)\n            tok = tok_list[0]\n            split_text = string.split(tok)\n            return sum((split_on_tokens(tok_list[1:], sub_text.strip()) + [tok]\n                        for sub_text in split_text), [])[:-1]\n\n        added_tokens = list(\n            self.added_tokens_encoder.keys()) + self.all_special_tokens\n        tokenized_text = split_on_tokens(added_tokens, text)\n        return tokenized_text\n\n    def _map_text_to_token(self, text: str, **kwargs) -> List[str]:\n        r""""""Maps a string to a sequence of tokens (string), using the\n        tokenizer. Split in words for word-based vocabulary or sub-words for\n        sub-word-based vocabularies (`BPE`/`SentencePiece`/`WordPiece`).\n        This function does not take care of the added tokens.\n        """"""\n        raise NotImplementedError\n\n    # TODO: Remove these once pylint supports function stubs.\n    # pylint: disable=unused-argument,function-redefined\n\n    @overload\n    def map_token_to_id(self, tokens: str) -> int:\n        ...\n\n    @overload\n    def map_token_to_id(self, tokens: List[str]) -> List[int]:\n        ...\n\n    def map_token_to_id(self, tokens):\n        r""""""Maps a single token or a sequence of tokens to a integer id\n        (resp.) a sequence of ids, using the vocabulary.\n\n        Args:\n            tokens: A single token or a list of tokens.\n\n        Returns:\n            A single token id or a list of token ids.\n        """"""\n        if isinstance(tokens, str):\n            return self._map_token_to_id_with_added_voc(tokens)\n\n        ids = []\n        for token in tokens:\n            ids.append(self._map_token_to_id_with_added_voc(token))\n        if len(ids) > self.max_len:\n            warnings.warn(\n                ""Token indices sequence length is longer than the specified ""\n                ""maximum sequence length for this model ({} > {}). Running ""\n                ""this sequence through the model will result in indexing ""\n                ""errors"".format(len(ids), self.max_len), UserWarning)\n        return ids\n\n    # pylint: enable=unused-argument,function-redefined\n\n    def _map_token_to_id_with_added_voc(self, token: str) -> int:\n        if token in self.added_tokens_encoder:\n            return self.added_tokens_encoder[token]\n        return self._map_token_to_id(token)\n\n    def _map_token_to_id(self, token: str) -> int:\n        raise NotImplementedError\n\n    def map_text_to_id(self, text: str) -> List[int]:\n        r""""""Maps a string to a sequence of ids (integer), using the\n        tokenizer and vocabulary. Same as\n        `self.map_token_to_id(self.map_text_to_token(text))`.\n\n        Args:\n            text: A input string.\n\n        Returns:\n            A single token id or a list of token ids.\n        """"""\n        return self.map_token_to_id(self.map_text_to_token(text))\n\n    # TODO: Remove these once pylint supports function stubs.\n    # pylint: disable=unused-argument,function-redefined\n\n    @overload\n    def map_id_to_token(self, token_ids: int,\n                        skip_special_tokens: bool = False) -> str:\n        ...\n\n    @overload\n    def map_id_to_token(self, token_ids: List[int],\n                        skip_special_tokens: bool = False) -> List[str]:\n        ...\n\n    def map_id_to_token(self, token_ids, skip_special_tokens=False):\n        r""""""Maps a single id or a sequence of ids to a token (resp.) a\n        sequence of tokens, using the vocabulary and added tokens.\n\n        Args:\n            token_ids: A single token id or a list of token ids.\n            skip_special_tokens: Whether to skip the special tokens.\n\n        Returns:\n            A single token or a list of tokens.\n        """"""\n        if isinstance(token_ids, int):\n            if token_ids in self.added_tokens_decoder:\n                return self.added_tokens_decoder[token_ids]\n            else:\n                return self._map_id_to_token(token_ids)\n        tokens = []\n        for index in token_ids:\n            if index in self.all_special_ids and skip_special_tokens:\n                continue\n            if index in self.added_tokens_decoder:\n                tokens.append(self.added_tokens_decoder[index])\n            else:\n                tokens.append(self._map_id_to_token(index))\n        return tokens\n\n    # pylint: enable=unused-argument,function-redefined\n\n    def _map_id_to_token(self, token_id: int) -> str:\n        raise NotImplementedError\n\n    def map_token_to_text(self, tokens: List[str]) -> str:\n        r""""""Maps a sequence of tokens (string) in a single string.\n        The most simple way to do it is :python:`\' \'.join(tokens)`, but we\n        often want to remove sub-word tokenization artifacts at the same time.\n        """"""\n        raise NotImplementedError\n\n    def map_id_to_text(self, token_ids: List[int],\n                       skip_special_tokens: bool = False,\n                       clean_up_tokenization_spaces: bool = True) -> str:\n        r""""""Maps a sequence of ids (integer) to a string, using the\n        tokenizer and vocabulary with options to remove special tokens and\n        clean up tokenization spaces.\n\n        Args:\n            token_ids: A list of token ids.\n            skip_special_tokens: Whether to skip the special tokens.\n            clean_up_tokenization_spaces: Whether to clean up a list of simple\n                English tokenization artifacts like spaces before punctuations\n                and abbreviated forms.\n        """"""\n        filtered_tokens = self.map_id_to_token(\n            token_ids, skip_special_tokens=skip_special_tokens)\n        text = self.map_token_to_text(filtered_tokens)\n        if clean_up_tokenization_spaces:\n            text = self.clean_up_tokenization(text)\n        return text\n\n    def encode_text(self,\n                    text_a: str,\n                    text_b: Optional[str] = None,\n                    max_seq_length: Optional[int] = None):\n        r""""""Adds special tokens to a sequence or sequence pair and computes\n        other information such as segment ids, input mask, and sequence length\n        for specific tasks.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def special_tokens_map(self) -> Dict[str, str]:\n        r""""""A dictionary mapping special token class attributes\n        (:attr:`cls_token`, :attr:`unk_token`, ...) to their values\n        (`<unk>`, `<cls>`, ...)\n        """"""\n        set_attr = {}\n        for attr in self._SPECIAL_TOKENS_ATTRIBUTES:\n            attr_value = getattr(self, attr)\n            if attr_value:\n                set_attr[attr] = attr_value\n        return set_attr\n\n    @property\n    def all_special_tokens(self) -> List[str]:\n        r""""""List all the special tokens (`<unk>`, `<cls>`, ...) mapped to class\n        attributes (:attr:`cls_token`, :attr:`unk_token`, ...).\n        """"""\n        all_toks: List[str] = []\n        set_attr = self.special_tokens_map\n        for attr_value in set_attr.values():\n            all_toks = all_toks + (\n                attr_value if isinstance(attr_value, (list, tuple)) else [\n                    attr_value])\n        all_toks = list(set(all_toks))\n        return all_toks\n\n    @property\n    def all_special_ids(self) -> List[int]:\n        r""""""List the vocabulary indices of the special tokens\n        (`<unk>`, `<cls>`, ...) mapped to class attributes\n        (:attr:`cls_token`, :attr:`unk_token`, ...).\n        """"""\n        all_toks = self.all_special_tokens\n        all_ids: List[int] = [self.map_token_to_id(t) for t in all_toks]\n        return all_ids\n\n    @staticmethod\n    def clean_up_tokenization(out_string: str) -> str:\n        r""""""Clean up a list of simple English tokenization artifacts like\n        spaces before punctuations and abbreviated forms.\n        """"""\n        out_string = out_string.replace(\' .\', \'.\').replace(\' ?\', \'?\'). \\\n            replace(\' !\', \'!\').replace(\' ,\', \',\').replace("" \' "", ""\'""). \\\n            replace("" n\'t"", ""n\'t"").replace("" \'m"", ""\'m""). \\\n            replace("" do not"", "" don\'t"").replace("" \'s"", ""\'s""). \\\n            replace("" \'ve"", ""\'ve"").replace("" \'re"", ""\'re"")\n        return out_string\n'"
texar/torch/data/tokenizers/xlnet_tokenizer.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPre-trained XLNet Tokenizer.\n\nCode structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_xlnet.py`\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport os\nimport unicodedata\nfrom shutil import copyfile\nimport sentencepiece as spm\n\nfrom texar.torch.modules.pretrained.xlnet import PretrainedXLNetMixin\nfrom texar.torch.data.tokenizers.tokenizer_base import TokenizerBase\nfrom texar.torch.utils.utils import truncate_seq_pair\n\n__all__ = [\n    ""XLNetTokenizer"",\n]\n\nSPIECE_UNDERLINE = u\'\xe2\x96\x81\'\n\nSEG_ID_A = 0\nSEG_ID_B = 1\nSEG_ID_CLS = 2\nSEG_ID_SEP = 3\nSEG_ID_PAD = 4\n\n\nclass XLNetTokenizer(PretrainedXLNetMixin, TokenizerBase):\n    r""""""Pre-trained XLNet Tokenizer.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name of\n            pre-trained model (e.g., `xlnet-base-uncased`). Please refer to\n            :class:`~texar.torch.modules.PretrainedXLNetMixin` for\n            all supported models.\n            If None, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    _IS_PRETRAINED = True\n    _MAX_INPUT_SIZE = {\n        \'xlnet-base-cased\': None,\n        \'xlnet-large-cased\': None,\n    }\n    _VOCAB_FILE_NAMES = {\'vocab_file\': \'spiece.model\'}\n    _VOCAB_FILE_MAP = {\n        \'vocab_file\': {\n            \'xlnet-base-cased\': \'spiece.model\',\n            \'xlnet-large-cased\': \'spiece.model\',\n        }\n    }\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        self.load_pretrained_config(pretrained_model_name, cache_dir, hparams)\n\n        super().__init__(hparams=None)\n\n        self.__dict__: Dict\n\n        self.config = {\n            \'do_lower_case\': self.hparams[\'do_lower_case\'],\n            \'remove_space\': self.hparams[\'remove_space\'],\n            \'keep_accents\': self.hparams[\'keep_accents\'],\n        }\n\n        if self.pretrained_model_dir is not None:\n            assert self.pretrained_model_name is not None\n            vocab_file = os.path.join(self.pretrained_model_dir,\n                                      self._VOCAB_FILE_MAP[\'vocab_file\']\n                                      [self.pretrained_model_name])\n            assert self.pretrained_model_name is not None\n            if self._MAX_INPUT_SIZE.get(self.pretrained_model_name):\n                self.max_len = self._MAX_INPUT_SIZE[self.pretrained_model_name]\n        else:\n            vocab_file = self.hparams[\'vocab_file\']\n            if self.hparams.get(\'max_len\'):\n                self.max_len = self.hparams[\'max_len\']\n\n        if not os.path.isfile(vocab_file):\n            raise ValueError(""Can\'t find a vocabulary file at path ""\n                             ""\'{}"".format(vocab_file))\n\n        self.do_lower_case = self.hparams[""do_lower_case""]\n        self.remove_space = self.hparams[""remove_space""]\n        self.keep_accents = self.hparams[""keep_accents""]\n        self.vocab_file = vocab_file\n\n        self.sp_model = spm.SentencePieceProcessor()\n        self.sp_model.Load(self.vocab_file)\n\n    # spm.SentencePieceProcessor() is a SwigPyObject object which cannot be\n    # pickled. We need to define __getstate__ here.\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[""sp_model""] = None\n        state[""vocab_file""] = None\n        return state, self.vocab_file\n\n    # spm.SentencePieceProcessor() is a SwigPyObject object which cannot be\n    # pickled. We need to define __setstate__ here.\n    def __setstate__(self, d):\n        self.__dict__, self.vocab_file = d\n        self.sp_model = spm.SentencePieceProcessor()\n        self.sp_model.Load(self.vocab_file)\n\n    def _preprocess_text(self, inputs: str) -> str:\n        r""""""Pre-process the text, including removing space,\n        stripping accents, and lower-casing the text.\n        """"""\n        if self.remove_space:\n            outputs = \' \'.join(inputs.strip().split())\n        else:\n            outputs = inputs\n        outputs = outputs.replace(""``"", \'""\').replace(""\'\'"", \'""\')\n\n        if not self.keep_accents:\n            outputs = unicodedata.normalize(\'NFKD\', outputs)\n            outputs = \'\'.join([c for c in outputs if not\n            unicodedata.combining(c)])\n        if self.do_lower_case:\n            outputs = outputs.lower()\n\n        return outputs\n\n    def _map_text_to_token(self, text: str,  # type: ignore\n                           sample: bool = False) -> List[str]:\n        text = self._preprocess_text(text)\n        if not sample:\n            pieces = self.sp_model.EncodeAsPieces(text)\n        else:\n            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n\n        new_pieces: List[str] = []\n        for piece in pieces:\n            if len(piece) > 1 and piece[-1] == \',\' and piece[-2].isdigit():\n                cur_pieces = self.sp_model.EncodeAsPieces(\n                    piece[:-1].replace(SPIECE_UNDERLINE, \'\'))\n                if piece[0] != SPIECE_UNDERLINE and \\\n                        cur_pieces[0][0] == SPIECE_UNDERLINE:\n                    if len(cur_pieces[0]) == 1:\n                        cur_pieces = cur_pieces[1:]\n                    else:\n                        cur_pieces[0] = cur_pieces[0][1:]\n                cur_pieces.append(piece[-1])\n                new_pieces.extend(cur_pieces)\n            else:\n                new_pieces.append(piece)\n\n        return new_pieces\n\n    def save_vocab(self, save_dir: str) -> Tuple[str]:\n        r""""""Save the sentencepiece vocabulary (copy original file) to\n        a directory.\n        """"""\n        if not os.path.isdir(save_dir):\n            raise ValueError(""Vocabulary path ({}) should be a ""\n                             ""directory"".format(save_dir))\n        out_vocab_file = os.path.join(save_dir,\n                                      self._VOCAB_FILE_NAMES[\'vocab_file\'])\n\n        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n            copyfile(self.vocab_file, out_vocab_file)\n\n        return (out_vocab_file,)\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self.sp_model)\n\n    def _map_token_to_id(self, token: str) -> int:\n        return self.sp_model.PieceToId(token)\n\n    def _map_id_to_token(self, index: int) -> str:\n        token = self.sp_model.IdToPiece(index)\n        return token\n\n    def map_token_to_text(self, tokens: List[str]) -> str:\n        r""""""Maps a sequence of tokens (string) in a single string.""""""\n        out_string = \'\'.join(tokens).replace(SPIECE_UNDERLINE, \' \').strip()\n        return out_string\n\n    def encode_text(self,\n                    text_a: str,\n                    text_b: Optional[str] = None,\n                    max_seq_length: Optional[int] = None) -> \\\n            Tuple[List[int], List[int], List[int]]:\n        r""""""Adds special tokens to a sequence or sequence pair and computes the\n        corresponding segment ids and input mask for XLNet specific tasks.\n        The sequence will be truncated if its length is larger than\n        ``max_seq_length``.\n\n        A XLNet sequence has the following format:\n        X `[sep_token]` `[cls_token]`\n\n        A XLNet sequence pair has the following format:\n        `[cls_token]` A `[sep_token]` B `[sep_token]`\n\n        Args:\n            text_a: The first input text.\n            text_b: The second input text.\n            max_seq_length: Maximum sequence length.\n\n        Returns:\n            A tuple of `(input_ids, segment_ids, input_mask)`, where\n\n            - ``input_ids``: A list of input token ids with added\n              special token ids.\n            - ``segment_ids``: A list of segment ids.\n            - ``input_mask``: A list of mask ids. The mask has 1 for real\n              tokens and 0 for padding tokens. Only real tokens are\n              attended to.\n        """"""\n        if max_seq_length is None:\n            max_seq_length = self.max_len\n\n        cls_token_id = self._map_token_to_id(self.cls_token)\n        sep_token_id = self._map_token_to_id(self.sep_token)\n\n        token_ids_a = self.map_text_to_id(text_a)\n        assert isinstance(token_ids_a, list)\n\n        token_ids_b = None\n        if text_b:\n            token_ids_b = self.map_text_to_id(text_b)\n\n        if token_ids_b:\n            assert isinstance(token_ids_b, list)\n            # Modifies `token_ids_a` and `token_ids_b` in place so that the\n            # total length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with ""- 3""\n            truncate_seq_pair(token_ids_a, token_ids_b, max_seq_length - 3)\n\n            input_ids = (token_ids_a + [sep_token_id] + token_ids_b +\n                         [sep_token_id] + [cls_token_id])\n            segment_ids = [SEG_ID_A] * (len(token_ids_a) + 1) + \\\n                          [SEG_ID_B] * (len(token_ids_b) + 1) + [SEG_ID_CLS]\n        else:\n            # Account for [CLS] and [SEP] with ""- 2""\n            token_ids = token_ids_a[:max_seq_length - 2]\n\n            input_ids = token_ids + [sep_token_id] + [cls_token_id]\n            segment_ids = [SEG_ID_A] * (len(input_ids) - 1) + [SEG_ID_CLS]\n\n        input_mask = [0] * len(input_ids)\n\n        # Zero-pad up to the maximum sequence length.\n        input_ids = [0] * (max_seq_length - len(input_ids)) + input_ids\n        input_mask = [1] * (max_seq_length - len(input_mask)) + input_mask\n        segment_ids = ([SEG_ID_PAD] * (max_seq_length - len(segment_ids)) +\n                       segment_ids)\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        return input_ids, segment_ids, input_mask\n\n    def encode_text_for_generation(\n            self,\n            text: str,\n            max_seq_length: Optional[int] = None,\n            append_eos_token: bool = True) -> Tuple[List[int], int]:\n        r""""""Adds special tokens to a sequence and computes the corresponding\n        sequence length for XLNet specific tasks. The sequence will be truncated\n        if its length is larger than ``max_seq_length``.\n\n        A XLNet sequence has the following format:\n        `[bos_token]` X `[eos_token]` `[pad_token]`\n\n        Args:\n            text: Input text.\n            max_seq_length: Maximum sequence length.\n            append_eos_token: Whether to append ``eos_token`` after the\n                sequence.\n\n        Returns:\n            A tuple of `(input_ids, seq_len)`, where\n\n            - ``input_ids``: A list of input token ids with added\n              special tokens.\n            - ``seq_len``: The sequence length.\n        """"""\n        if max_seq_length is None:\n            max_seq_length = self.max_len\n\n        token_ids = self.map_text_to_id(text)\n        assert isinstance(token_ids, list)\n\n        bos_token_id = self._map_token_to_id(self.bos_token)\n        eos_token_id = self._map_token_to_id(self.eos_token)\n        pad_token_id = self._map_token_to_id(self.pad_token)\n\n        if append_eos_token:\n            input_ids = token_ids[:max_seq_length - 2]\n            input_ids = [bos_token_id] + input_ids + [eos_token_id]\n        else:\n            input_ids = token_ids[:max_seq_length - 1]\n            input_ids = [bos_token_id] + input_ids\n\n        seq_len = len(input_ids)\n\n        # Pad up to the maximum sequence length.\n        input_ids = input_ids + [pad_token_id] * (max_seq_length - seq_len)\n\n        assert len(input_ids) == max_seq_length\n\n        return input_ids, seq_len\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The tokenizer is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the tokenizer is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the tokenizer is defined by the\n          configurations in `hparams`.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""xlnet-base-cased"",\n                ""vocab_file"": None,\n                ""max_len"": None,\n                ""bos_token"": ""<s>"",\n                ""eos_token"": ""</s>"",\n                ""unk_token"": ""<unk>"",\n                ""sep_token"": ""<sep>"",\n                ""pad_token"": ""<pad>"",\n                ""cls_token"": ""<cls>"",\n                ""mask_token"": ""<mask>"",\n                ""additional_special_tokens"": [""<eop>"", ""<eod>""],\n                ""do_lower_case"": False,\n                ""remove_space"": True,\n                ""keep_accents"": False,\n                ""name"": ""xlnet_tokenizer"",\n            }\n\n        Here:\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained XLNet model.\n\n        `""vocab_file""`: str or None\n            The path to a sentencepiece vocabulary file.\n\n        `""max_len""`: int or None\n            The maximum sequence length that this model might ever be used with.\n\n        `""bos_token""`: str\n            Beginning of sentence token.\n\n        `""eos_token""`: str\n            End of sentence token.\n\n        `""unk_token""`: str\n            Unknown token.\n\n        `""sep_token""`: str\n            Separation token.\n\n        `""pad_token""`: str\n            Padding token.\n\n        `""cls_token""`: str\n            Classification token.\n\n        `""mask_token""`: str\n            Masking token.\n\n        `""additional_special_tokens""`: list\n            A list of additional special tokens.\n\n        `""do_lower_case""`: bool\n            Whether to lower-case the text.\n\n        `""remove_space""`: bool\n            Whether to remove the space in the text.\n\n        `""keep_accents""`: bool\n            Whether to keep the accents in the text.\n\n        `""name""`: str\n            Name of the tokenizer.\n        """"""\n        return {\n            \'pretrained_model_name\': \'xlnet-base-cased\',\n            \'vocab_file\': None,\n            \'max_len\': None,\n            \'bos_token\': \'<s>\',\n            \'eos_token\': \'</s>\',\n            \'unk_token\': \'<unk>\',\n            \'sep_token\': \'<sep>\',\n            \'pad_token\': \'<pad>\',\n            \'cls_token\': \'<cls>\',\n            \'mask_token\': \'<mask>\',\n            \'additional_special_tokens\': [\'<eop>\', \'<eod>\'],\n            \'do_lower_case\': False,\n            \'remove_space\': True,\n            \'keep_accents\': False,\n            \'name\': \'xlnet_tokenizer\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str):\n        r""""""Returns the configuration of the pre-trained XLNet tokenizer.""""""\n        return {\n            \'vocab_file\': None,\n            \'max_len\': None,\n            \'bos_token\': \'<s>\',\n            \'eos_token\': \'</s>\',\n            \'unk_token\': \'<unk>\',\n            \'sep_token\': \'<sep>\',\n            \'pad_token\': \'<pad>\',\n            \'cls_token\': \'<cls>\',\n            \'mask_token\': \'<mask>\',\n            \'additional_special_tokens\': [\'<eop>\', \'<eod>\'],\n            \'do_lower_case\': False,\n            \'remove_space\': True,\n            \'keep_accents\': False,\n        }\n'"
texar/torch/modules/classifiers/__init__.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library classifiers.\n""""""\n\nfrom texar.torch.modules.classifiers.bert_classifier import *\nfrom texar.torch.modules.classifiers.classifier_base import *\nfrom texar.torch.modules.classifiers.conv_classifiers import *\nfrom texar.torch.modules.classifiers.gpt2_classifier import *\nfrom texar.torch.modules.classifiers.rnn_classifiers import *\nfrom texar.torch.modules.classifiers.roberta_classifier import *\nfrom texar.torch.modules.classifiers.xlnet_classifier import *\n'"
texar/torch/modules/classifiers/bert_classifier.py,23,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBERT classifier.\n""""""\nfrom typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom texar.torch.core.layers import get_initializer\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.modules.classifiers.classifier_base import ClassifierBase\nfrom texar.torch.modules.encoders.bert_encoder import BERTEncoder\nfrom texar.torch.modules.pretrained.bert import PretrainedBERTMixin\nfrom texar.torch.utils.utils import dict_fetch\n\n__all__ = [\n    ""BERTClassifier""\n]\n\n\nclass BERTClassifier(ClassifierBase, PretrainedBERTMixin):\n    r""""""Classifier based on BERT modules. Please see\n    :class:`~texar.torch.modules.PretrainedBERTMixin` for a brief description\n    of BERT.\n\n    This is a combination of the\n    :class:`~texar.torch.modules.BERTEncoder` with a classification\n    layer. Both step-wise classification and sequence-level classification\n    are supported, specified in :attr:`hparams`.\n\n    Arguments are the same as in\n    :class:`~texar.torch.modules.BERTEncoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``bert-base-uncased``). Please refer to\n            :class:`~texar.torch.modules.PretrainedBERTMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. document private functions\n    """"""\n    _ENCODER_CLASS = BERTEncoder\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n\n        super().__init__(hparams=hparams)\n\n        # Create the underlying encoder\n        encoder_hparams = dict_fetch(hparams,\n                                     self._ENCODER_CLASS.default_hparams())\n\n        self._encoder = self._ENCODER_CLASS(\n            pretrained_model_name=pretrained_model_name,\n            cache_dir=cache_dir,\n            hparams=encoder_hparams)\n\n        # Create a dropout layer\n        self._dropout_layer = nn.Dropout(self._hparams.dropout)\n\n        # Create an additional classification layer if needed\n        self.num_classes = self._hparams.num_classes\n        if self.num_classes <= 0:\n            self._logits_layer = None\n        else:\n            logit_kwargs = self._hparams.logit_layer_kwargs\n            if logit_kwargs is None:\n                logit_kwargs = {}\n            elif not isinstance(logit_kwargs, HParams):\n                raise ValueError(""hparams[\'logit_layer_kwargs\'] ""\n                                 ""must be a dict."")\n            else:\n                logit_kwargs = logit_kwargs.todict()\n\n            if self._hparams.clas_strategy == \'all_time\':\n                self._logits_layer = nn.Linear(\n                    self._encoder.output_size *\n                    self._hparams.max_seq_length,\n                    self.num_classes,\n                    **logit_kwargs)\n            else:\n                self._logits_layer = nn.Linear(\n                    self._encoder.output_size, self.num_classes,\n                    **logit_kwargs)\n\n        if self._hparams.initializer:\n            initialize = get_initializer(self._hparams.initializer)\n            assert initialize is not None\n            if self._logits_layer:\n                initialize(self._logits_layer.weight)\n                if self._logits_layer.bias:\n                    initialize(self._logits_layer.bias)\n\n        self.is_binary = (self.num_classes == 1) or \\\n                         (self.num_classes <= 0 and\n                          self._hparams.encoder.dim == 1)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in BertEncoder\n                ...\n                # (2) Additional hyperparameters\n                ""num_classes"": 2,\n                ""logit_layer_kwargs"": None,\n                ""clas_strategy"": ""cls_time"",\n                ""max_seq_length"": None,\n                ""dropout"": 0.1,\n                ""name"": ""bert_classifier""\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n           :class:`~texar.torch.modules.BERTEncoder`.\n           See the :meth:`~texar.torch.modules.BERTEncoder.default_hparams`.\n           An instance of BERTEncoder is created for feature extraction.\n\n        2. Additional hyperparameters:\n\n            `""num_classes""`: int\n                Number of classes:\n\n                - If **> 0**, an additional `Linear`\n                  layer is appended to the encoder to compute the logits over\n                  classes.\n                - If **<= 0**, no dense layer is appended. The number of\n                  classes is assumed to be the final dense layer size of the\n                  encoder.\n\n            `""logit_layer_kwargs""`: dict\n                Keyword arguments for the logit Dense layer constructor,\n                except for argument ""units"" which is set to `num_classes`.\n                Ignored if no extra logit layer is appended.\n\n            `""clas_strategy""`: str\n                The classification strategy, one of:\n\n                - **cls_time**: Sequence-level classification based on the\n                  output of the first time step (which is the `CLS` token).\n                  Each sequence has a class.\n                - **all_time**: Sequence-level classification based on\n                  the output of all time steps. Each sequence has a class.\n                - **time_wise**: Step-wise classification, i.e., make\n                  classification for each time step based on its output.\n\n            `""max_seq_length""`: int, optional\n                Maximum possible length of input sequences. Required if\n                `clas_strategy` is `all_time`.\n\n            `""dropout""`: float\n                The dropout rate of the BERT encoder output.\n\n            `""name""`: str\n                Name of the classifier.\n        """"""\n\n        hparams = BERTEncoder.default_hparams()\n        hparams.update({\n            ""num_classes"": 2,\n            ""logit_layer_kwargs"": None,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_length"": None,\n            ""dropout"": 0.1,\n            ""name"": ""bert_classifier""\n        })\n        return hparams\n\n    def forward(self,  # type: ignore\n                inputs: Union[torch.Tensor, torch.LongTensor],\n                sequence_length: Optional[torch.LongTensor] = None,\n                segment_ids: Optional[torch.LongTensor] = None) \\\n            -> Tuple[torch.Tensor, torch.LongTensor]:\n        r""""""Feeds the inputs through the network and makes classification.\n\n        The arguments are the same as in\n        :class:`~texar.torch.modules.BERTEncoder`.\n\n        Args:\n            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,\n                containing the ids of tokens in input sequences, or\n                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,\n                containing soft token ids (i.e., weights or probabilities)\n                used to mix the embedding vectors.\n            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.\n                Input tokens beyond respective sequence lengths are masked\n                out automatically.\n            segment_ids (optional): A 2D Tensor of shape\n                `[batch_size, max_time]`, containing the segment ids\n                of tokens in input sequences. If `None` (default), a tensor\n                with all elements set to zero is used.\n\n        Returns:\n            A tuple `(logits, preds)`, containing the logits over classes and\n            the predictions, respectively.\n\n            - If ``clas_strategy`` is ``cls_time`` or ``all_time``:\n\n                - If ``num_classes`` == 1, ``logits`` and ``pred`` are both of\n                  shape ``[batch_size]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, num_classes]`` and ``pred`` is of shape\n                  ``[batch_size]``.\n\n            - If ``clas_strategy`` is ``time_wise``:\n\n                - ``num_classes`` == 1, ``logits`` and ``pred`` are both of\n                  shape ``[batch_size, max_time]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, max_time, num_classes]`` and ``pred`` is of\n                  shape ``[batch_size, max_time]``.\n        """"""\n        enc_outputs, pooled_output = self._encoder(inputs,\n                                                   sequence_length,\n                                                   segment_ids)\n        # Compute logits\n        strategy = self._hparams.clas_strategy\n        if strategy == \'time_wise\':\n            logits = enc_outputs\n        elif strategy == \'cls_time\':\n            logits = pooled_output\n        elif strategy == \'all_time\':\n            # Pad `enc_outputs` to have max_seq_length before flatten\n            length_diff = self._hparams.max_seq_length - inputs.shape[1]\n            logit_input = F.pad(enc_outputs, [0, 0, 0, length_diff, 0, 0])\n            logit_input_dim = (self._encoder.output_size *\n                               self._hparams.max_seq_length)\n            logits = logit_input.view(-1, logit_input_dim)\n        else:\n            raise ValueError(\'Unknown classification strategy: {}\'.format(\n                strategy))\n\n        if self._logits_layer is not None:\n            logits = self._dropout_layer(logits)\n            logits = self._logits_layer(logits)\n\n        # Compute predictions\n        if strategy == ""time_wise"":\n            if self.is_binary:\n                logits = torch.squeeze(logits, -1)\n                preds = (logits > 0).long()\n            else:\n                preds = torch.argmax(logits, dim=-1)\n        else:\n            if self.is_binary:\n                preds = (logits > 0).long()\n                logits = torch.flatten(logits)\n            else:\n                preds = torch.argmax(logits, dim=-1)\n            preds = torch.flatten(preds)\n\n        return logits, preds\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of :meth:`forward` output :attr:`logits`.\n        If :attr:`logits` size is only determined by input\n        (i.e. if ``num_classes`` == 1), the feature size is equal to ``-1``.\n        Otherwise it is equal to last dimension value of :attr:`logits` size.\n        """"""\n        if self._hparams.num_classes == 1:\n            logit_dim = -1\n        elif self._hparams.num_classes > 1:\n            logit_dim = self._hparams.num_classes\n        elif self._hparams.clas_strategy == \'all_time\':\n            logit_dim = (self._encoder.output_size *\n                         self._hparams.max_seq_length)\n        elif self._hparams.clas_strategy == \'cls_time\':\n            logit_dim = self._encoder.output_size\n        elif self._hparams.clas_strategy == \'time_wise\':\n            logit_dim = self._hparams.encoder.dim\n\n        return logit_dim\n'"
texar/torch/modules/classifiers/classifier_base.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for classifiers.\n""""""\nfrom abc import ABC\nfrom typing import Any, Dict\n\nfrom texar.torch.module_base import ModuleBase\n\n__all__ = [\n    ""ClassifierBase"",\n]\n\n\nclass ClassifierBase(ModuleBase, ABC):\n    r""""""Base class inherited by all classifier classes.\n    """"""\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n        """"""\n        return {\n            ""name"": ""classifier""\n        }\n'"
texar/torch/modules/classifiers/conv_classifiers.py,17,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious classifier classes.\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.modules.classifiers.classifier_base import ClassifierBase\nfrom texar.torch.modules.encoders.conv_encoders import Conv1DEncoder\nfrom texar.torch.utils import utils\n\n__all__ = [\n    ""Conv1DClassifier"",\n]\n\n\nclass Conv1DClassifier(ClassifierBase):\n    r""""""Simple `Conv-1D` classifier.\n    This is a combination of the :class:`~texar.torch.modules.Conv1DEncoder`\n    with a classification layer.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        in_features (int): Size of the feature dimension in the input tensor.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    See :meth:`forward` for the inputs and outputs. If :attr:`""data_format""` is\n    set to ``""channels_first""`` (this is the default), inputs must be a tensor\n    of shape `[batch_size, channels, length]`. If :attr:`""data_format""` is set\n    to ``""channels_last""``, inputs must be a tensor of shape\n    `[batch_size, length, channels]`. For example, for sequence classification,\n    `length` corresponds to time steps, and `channels` corresponds to embedding\n    dim.\n\n    Example:\n\n    .. code-block:: python\n\n        inputs = torch.randn([64, 20, 256])\n\n        clas = Conv1DClassifier(in_channels=20, in_features=256,\n                                hparams={\'num_classes\': 10})\n\n        logits, pred = clas(inputs)\n        # logits == Tensor of shape [64, 10]\n        # pred   == Tensor of shape [64]\n\n    .. document private functions\n    """"""\n\n    def __init__(self, in_channels: int, in_features: Optional[int] = None,\n                 hparams: Optional[Union[HParams, Dict[str, Any]]] = None):\n        super().__init__(hparams=hparams)\n\n        encoder_hparams = utils.dict_fetch(hparams,\n                                           Conv1DEncoder.default_hparams())\n        self._encoder = Conv1DEncoder(in_channels=in_channels,\n                                      in_features=in_features,\n                                      hparams=encoder_hparams)\n\n        # Add an additional dense layer if needed\n        self._num_classes = self._hparams.num_classes\n        if self._num_classes > 0:\n            if self._hparams.num_dense_layers <= 0:\n                if in_features is None:\n                    raise ValueError(""\'in_features\' is required for logits ""\n                                     ""layer when \'num_dense_layers\' <= 0"")\n                self._encoder.append_layer({""type"": ""Flatten""})\n                ones = torch.ones(1, in_channels, in_features)\n                input_size = self._encoder._infer_dense_layer_input_size(ones)  # pylint: disable=protected-access\n                self.hparams.logit_layer_kwargs.in_features = input_size[1]\n\n            logit_kwargs = self._hparams.logit_layer_kwargs\n            if logit_kwargs is None:\n                logit_kwargs = {}\n            elif not isinstance(logit_kwargs, HParams):\n                raise ValueError(\n                    ""hparams[\'logit_layer_kwargs\'] must be a dict."")\n            else:\n                logit_kwargs = logit_kwargs.todict()\n            logit_kwargs.update({""out_features"": self._num_classes})\n\n            self._encoder.append_layer({""type"": ""Linear"",\n                                        ""kwargs"": logit_kwargs})\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in Conv1DEncoder\n                ...\n\n                # (2) Additional hyperparameters\n                ""num_classes"": 2,\n                ""logit_layer_kwargs"": {\n                    ""use_bias"": False\n                },\n                ""name"": ""conv1d_classifier""\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n           :class:`~texar.torch.modules.Conv1DEncoder`.\n           See the :meth:`~texar.torch.modules.Conv1DEncoder.default_hparams`.\n           An instance of :class:`~texar.torch.modules.Conv1DEncoder` is created\n           for feature extraction.\n\n        2. Additional hyperparameters:\n\n           `""num_classes""`: int\n               Number of classes:\n\n               - If `> 0`, an additional :torch_nn:`Linear`\n                 layer is appended to the encoder to compute the logits over\n                 classes.\n\n               - If `<= 0`, no dense layer is appended. The number of\n                 classes is assumed to be equal to ``out_features`` of the\n                 final dense layer size of the encoder.\n\n           `""logit_layer_kwargs""`: dict\n               Keyword arguments for the logit :torch_nn:`Linear` layer\n               constructor, except for argument ``out_features`` which is set\n               to ``""num_classes""``. Ignored if no extra logit layer is\n               appended.\n\n           `""name""`: str\n               Name of the classifier.\n        """"""\n        hparams = Conv1DEncoder.default_hparams()\n        hparams.update({\n            ""name"": ""conv1d_classifier"",\n            ""num_classes"": 2,  # set to <=0 to avoid appending output layer\n            ""logit_layer_kwargs"": {\n                ""in_features"": hparams[""out_features""],\n                ""bias"": True\n            }\n        })\n        return hparams\n\n    def forward(self,  # type:ignore\n                input: torch.Tensor,\n                sequence_length: Optional[Union[torch.LongTensor,\n                                                List[int]]] = None,\n                dtype: Optional[torch.dtype] = None,\n                data_format: Optional[str] = None) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n        r""""""Feeds the inputs through the network and makes classification.\n\n        The arguments are the same as in\n        :class:`~texar.torch.modules.Conv1DEncoder`.\n\n        The predictions of binary classification (``num_classes`` =1) and\n        multi-way classification (``num_classes`` >1) are different, as\n        explained below.\n\n        Args:\n            input: The inputs to the network, which is a 3D tensor. See\n                :class:`~texar.torch.modules.Conv1DEncoder` for more details.\n            sequence_length (optional): An int tensor of shape `[batch_size]` or\n                a python array containing the length of each element in\n                :attr:`inputs`. If given, time steps beyond the length will\n                first be masked out before feeding to the layers.\n            dtype (optional): Type of the inputs. If not provided, infers\n                from inputs automatically.\n            data_format (optional): Data type of the input tensor. If\n                ``channels_last``, the last dimension will be treated as channel\n                dimension so the size of the :attr:`input` should be\n                `[batch_size, X, channel]`. If ``channels_first``, first\n                dimension will be treated as channel dimension so the size\n                should be `[batch_size, channel, X]`. Defaults to None.\n                If None, the value will be picked from hyperparameters.\n\n        Returns:\n            A tuple ``(logits, pred)``, where\n\n            - ``logits`` is a :tensor:`Tensor` of shape\n              ``[batch_size, num_classes]`` for ``num_classes`` >1, and\n              ``[batch_size]`` for ``num_classes`` =1 (i.e., binary\n              classification).\n            - ``pred`` is the prediction, a :tensor:`LongTensor` of shape\n              ``[batch_size]``. For binary classification, the standard\n              sigmoid function is used for prediction, and the class labels are\n              ``{0, 1}``.\n        """"""\n        logits = self._encoder(input, sequence_length=sequence_length,\n                               dtype=dtype, data_format=data_format)\n\n        num_classes = self._hparams.num_classes\n        is_binary = num_classes == 1\n        is_binary = is_binary or (num_classes <= 0 and logits.shape[1] == 1)\n\n        if is_binary:\n            pred = (logits > 0)\n            logits = logits.view(-1)\n        else:\n            pred = torch.argmax(logits, dim=1)\n\n        pred = pred.view(-1).long()\n\n        return logits, pred\n\n    @property\n    def num_classes(self) -> int:\n        r""""""The number of classes.\n        """"""\n        return self._num_classes\n\n    @property\n    def encoder(self) -> nn.Module:\n        r""""""The classifier neural network.\n        """"""\n        return self._encoder\n\n    def has_layer(self, layer_name) -> bool:\n        r""""""Returns `True` if the network with the name exists. Returns\n        `False` otherwise.\n\n        Args:\n            layer_name (str): Name of the layer.\n        """"""\n        return self._encoder.has_layer(layer_name)\n\n    def layer_by_name(self, layer_name) -> Optional[nn.Module]:\n        r""""""Returns the layer with the name. Returns `None` if the layer name\n        does not exist.\n\n        Args:\n            layer_name (str): Name of the layer.\n        """"""\n        return self._encoder.layer_by_name(layer_name)\n\n    @property\n    def layers_by_name(self) -> Dict[str, nn.Module]:\n        r""""""A dictionary mapping layer names to the layers.\n        """"""\n        return self._encoder.layers_by_name\n\n    @property\n    def layers(self) -> nn.ModuleList:\n        r""""""A list of the layers.\n        """"""\n        return self._encoder.layers\n\n    @property\n    def layer_names(self) -> List[str]:\n        r""""""A list of uniquified layer names.\n        """"""\n        return self._encoder.layer_names\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of :meth:`forward` output :attr:`logits`.\n        If :attr:`logits` size is only determined by input\n        (i.e. if ``num_classes`` == 1), the feature size is equal\n        to ``-1``. Otherwise, if ``num_classes`` > 1, it is equal\n        to ``num_classes``.\n        """"""\n        if self._hparams.num_classes > 1:\n            logit_dim = self._hparams.num_classes\n        elif self._hparams.num_classes == 1:\n            logit_dim = -1\n        else:\n            raise AttributeError(""\'Conv1DClassifier\' object has""\n                                 ""no attribute \'output_size\'""\n                                 ""if \'self._hparams.num_classes\' < 1."")\n        return logit_dim\n'"
texar/torch/modules/classifiers/gpt2_classifier.py,24,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nGPT2 classifiers.\n""""""\nfrom typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom texar.torch.core.layers import get_initializer\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.modules.classifiers.classifier_base import ClassifierBase\nfrom texar.torch.modules.encoders.gpt2_encoder import GPT2Encoder\nfrom texar.torch.modules.pretrained.gpt2 import PretrainedGPT2Mixin\nfrom texar.torch.utils.utils import dict_fetch\n\n__all__ = [\n    ""GPT2Classifier""\n]\n\n\nclass GPT2Classifier(ClassifierBase, PretrainedGPT2Mixin):\n    r""""""Classifier based on GPT2 modules. Please see\n    :class:`~texar.torch.modules.PretrainedGPT2Mixin` for a brief description\n    of GPT2.\n\n    This is a combination of the\n    :class:`~texar.torch.modules.GPT2Encoder` with a classification\n    layer. Both step-wise classification and sequence-level classification\n    are supported, specified in :attr:`hparams`.\n\n    Arguments are the same as in\n    :class:`~texar.torch.modules.GPT2Encoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``gpt2-small``). Please refer to\n            :class:`~texar.torch.modules.PretrainedGPT2Mixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. document private functions\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n\n        super().__init__(hparams=hparams)\n\n        # Create the underlying encoder\n        encoder_hparams = dict_fetch(hparams, GPT2Encoder.default_hparams())\n\n        self._encoder = GPT2Encoder(pretrained_model_name=pretrained_model_name,\n                                    cache_dir=cache_dir,\n                                    hparams=encoder_hparams)\n\n        # Create a dropout layer\n        self._dropout_layer = nn.Dropout(self._hparams.dropout)\n\n        # Create an additional classification layer if needed\n        self.num_classes = self._hparams.num_classes\n        if self.num_classes <= 0:\n            self._logits_layer = None\n        else:\n            logit_kwargs = self._hparams.logit_layer_kwargs\n            if logit_kwargs is None:\n                logit_kwargs = {}\n            elif not isinstance(logit_kwargs, HParams):\n                raise ValueError(""hparams[\'logit_layer_kwargs\'] ""\n                                 ""must be a dict."")\n            else:\n                logit_kwargs = logit_kwargs.todict()\n\n            if self._hparams.clas_strategy == \'all_time\':\n                self._logits_layer = nn.Linear(\n                    self._encoder.output_size *\n                    self._hparams.max_seq_length,\n                    self.num_classes,\n                    **logit_kwargs)\n            else:\n                self._logits_layer = nn.Linear(\n                    self._encoder.output_size, self.num_classes,\n                    **logit_kwargs)\n\n        if self._hparams.initializer:\n            initialize = get_initializer(self._hparams.initializer)\n            assert initialize is not None\n            if self._logits_layer is not None:\n                initialize(self._logits_layer.weight)\n                if self._logits_layer.bias is not None:\n                    initialize(self._logits_layer.bias)\n\n        self.is_binary = (self.num_classes == 1) or \\\n                         (self.num_classes <= 0 and\n                          self._hparams.encoder.dim == 1)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in GPT2Encoder\n                ...\n                # (2) Additional hyperparameters\n                ""num_classes"": 2,\n                ""logit_layer_kwargs"": None,\n                ""clas_strategy"": `cls_time`,\n                ""max_seq_length"": None,\n                ""dropout"": 0.1,\n                ""name"": `gpt2_classifier`\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n           :class:`~texar.torch.modules.GPT2Encoder`.\n           See the :meth:`~texar.torch.modules.GPT2Encoder.default_hparams`.\n           An instance of GPT2Encoder is created for feature extraction.\n\n        2. Additional hyperparameters:\n\n            `""num_classes""`: int\n                Number of classes:\n\n                - If **> 0**, an additional `Linear`\n                  layer is appended to the encoder to compute the logits over\n                  classes.\n                - If **<= 0**, no dense layer is appended. The number of\n                  classes is assumed to be the final dense layer size of the\n                  encoder.\n\n            `""logit_layer_kwargs""`: dict\n                Keyword arguments for the logit Dense layer constructor,\n                except for argument ""units"" which is set to `num_classes`.\n                Ignored if no extra logit layer is appended.\n\n            `""clas_strategy""`: str\n                The classification strategy, one of:\n\n                - **cls_time**: Sequence-level classification based on the\n                  output of the last time step. Each sequence has a class.\n                - **all_time**: Sequence-level classification based on\n                  the output of all time steps. Each sequence has a class.\n                - **time_wise**: Step-wise classification, i.e., make\n                  classification for each time step based on its output.\n\n            `""max_seq_length""`: int, optional\n                Maximum possible length of input sequences. Required if\n                `clas_strategy` is `all_time`.\n\n            `""dropout""`: float\n                The dropout rate of the GPT2 encoder output.\n\n            `""name""`: str\n                Name of the classifier.\n        """"""\n\n        hparams = GPT2Encoder.default_hparams()\n        hparams.update({\n            ""num_classes"": 2,\n            ""logit_layer_kwargs"": None,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_length"": None,\n            ""dropout"": 0.1,\n            ""name"": ""gpt2_classifier""\n        })\n        return hparams\n\n    def forward(self,  # type: ignore\n                inputs: Union[torch.Tensor, torch.LongTensor],\n                sequence_length: Optional[torch.LongTensor] = None) \\\n            -> Tuple[torch.Tensor, torch.LongTensor]:\n        r""""""Feeds the inputs through the network and makes classification.\n\n        The arguments are the same as in\n        :class:`~texar.torch.modules.GPT2Encoder`.\n\n        Args:\n            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,\n                containing the ids of tokens in input sequences, or\n                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,\n                containing soft token ids (i.e., weights or probabilities)\n                used to mix the embedding vectors.\n            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.\n                Input tokens beyond respective sequence lengths are masked\n                out automatically.\n\n        Returns:\n            A tuple `(logits, preds)`, containing the logits over classes and\n            the predictions, respectively.\n\n            - If ``clas_strategy`` is ``cls_time`` or ``all_time``:\n\n                - If ``num_classes`` == 1, ``logits`` and ``pred`` are of both\n                  shape ``[batch_size]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, num_classes]`` and ``pred`` is of shape\n                  ``[batch_size]``.\n\n            - If ``clas_strategy`` is ``time_wise``:\n\n                - If ``num_classes`` == 1, ``logits`` and ``pred`` are of both\n                  shape ``[batch_size, max_time]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, max_time, num_classes]`` and ``pred`` is of\n                  shape ``[batch_size, max_time]``.\n        """"""\n        enc_outputs = self._encoder(inputs, sequence_length)\n\n        # Compute logits\n        strategy = self._hparams.clas_strategy\n        if strategy == \'time_wise\':\n            logits = enc_outputs\n        elif strategy == \'cls_time\':\n            if sequence_length is None:\n                logits = torch.squeeze(enc_outputs[:, -1, :], dim=1)\n            else:\n                logits = torch.stack([enc_outputs[batch_idx, time_idx - 1, :]\n                                      for batch_idx, time_idx in\n                                      enumerate(sequence_length)], dim=0)\n        elif strategy == \'all_time\':\n            # Pad `enc_outputs` to have max_seq_length before flatten\n            length_diff = self._hparams.max_seq_length - inputs.shape[1]\n            logit_input = F.pad(enc_outputs, [0, 0, 0, length_diff, 0, 0])\n            logit_input_dim = (self._encoder.output_size *\n                               self._hparams.max_seq_length)\n            logits = logit_input.view(-1, logit_input_dim)\n        else:\n            raise ValueError(\'Unknown classification strategy: {}\'.format(\n                strategy))\n\n        if self._logits_layer is not None:\n            logits = self._dropout_layer(logits)\n            logits = self._logits_layer(logits)\n\n        # Compute predictions\n        if strategy == ""time_wise"":\n            if self.is_binary:\n                logits = torch.squeeze(logits, -1)\n                preds = (logits > 0).long()\n            else:\n                preds = torch.argmax(logits, dim=-1)\n        else:\n            if self.is_binary:\n                preds = (logits > 0).long()\n                logits = torch.flatten(logits)\n            else:\n                preds = torch.argmax(logits, dim=-1)\n            preds = torch.flatten(preds)\n\n        return logits, preds\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of :meth:`forward` output :attr:`logits`.\n        If :attr:`logits` size is only determined by input\n        (i.e. if ``num_classes`` == 1), the feature size is equal to ``-1``.\n        Otherwise it is equal to last dimension value of :attr:`logits` size.\n        """"""\n        if self._hparams.num_classes == 1:\n            logit_dim = -1\n        elif self._hparams.num_classes > 1:\n            logit_dim = self._hparams.num_classes\n        elif self._hparams.clas_strategy == \'all_time\':\n            logit_dim = (self._encoder.output_size *\n                         self._hparams.max_seq_length)\n        else:\n            logit_dim = self._encoder.output_size\n\n        return logit_dim\n'"
texar/torch/modules/classifiers/rnn_classifiers.py,20,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious RNN classifiers.\n""""""\n\nfrom typing import Optional, Tuple, TypeVar\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom texar.torch.core.cell_wrappers import RNNCellBase\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.modules.classifiers.classifier_base import ClassifierBase\nfrom texar.torch.modules.encoders.rnn_encoders import \\\n        UnidirectionalRNNEncoder\nfrom texar.torch.utils.utils import dict_fetch\n\n\n__all__ = [\n    ""UnidirectionalRNNClassifier"",\n]\n\nState = TypeVar(\'State\')\n\n\nclass UnidirectionalRNNClassifier(ClassifierBase):\n    r""""""One directional RNN classifier. This is a combination of the\n    :class:`~texar.torch.modules.UnidirectionalRNNEncoder` with a classification\n    layer. Both step-wise classification and sequence-level classification\n    are supported, specified in :attr:`hparams`.\n\n    Arguments are the same as in\n    :class:`~texar.torch.modules.UnidirectionalRNNEncoder`.\n\n    Args:\n        input_size (int): The number of expected features in the input for the\n            cell.\n        cell: (RNNCell, optional) If not specified,\n            a cell is created as specified in :attr:`hparams[""rnn_cell""]`.\n        output_layer (optional): An instance of\n            :torch_nn:`Module`. Applies to the RNN cell\n            output of each step. If `None` (default), the output layer is\n            created as specified in :attr:`hparams[""output_layer""]`.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n    """"""\n\n    def __init__(self,\n                 input_size: int,\n                 cell: Optional[RNNCellBase[State]] = None,\n                 output_layer: Optional[nn.Module] = None,\n                 hparams=None):\n\n        super().__init__(hparams=hparams)\n\n        # Create the underlying encoder\n        encoder_hparams = dict_fetch(\n            hparams, UnidirectionalRNNEncoder.default_hparams())\n\n        self._encoder = UnidirectionalRNNEncoder(\n            input_size=input_size,\n            cell=cell,\n            output_layer=output_layer,\n            hparams=encoder_hparams)\n\n        # Create an additional classification layer if needed\n        self.num_classes = self._hparams.num_classes\n        if self.num_classes <= 0:\n            self._logits_layer = None\n        else:\n            logit_kwargs = self._hparams.logit_layer_kwargs\n            if logit_kwargs is None:\n                logit_kwargs = {}\n            elif not isinstance(logit_kwargs, HParams):\n                raise ValueError(""hparams[\'logit_layer_kwargs\'] ""\n                                 ""must be a dict."")\n            else:\n                logit_kwargs = logit_kwargs.todict()\n\n            if self._hparams.clas_strategy == \'all_time\':\n                self._logits_layer = nn.Linear(\n                    self._encoder.output_size *\n                    self._hparams.max_seq_length,\n                    self.num_classes,\n                    **logit_kwargs)\n            else:\n                self._logits_layer = nn.Linear(\n                    self._encoder.output_size, self.num_classes,\n                    **logit_kwargs)\n\n        self.is_binary = (self.num_classes == 1) or \\\n                         (self.num_classes <= 0 and\n                          self._encoder.output_size == 1)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in UnidirectionalRNNEncoder\n                ...\n\n                # (2) Additional hyperparameters\n                ""num_classes"": 2,\n                ""logit_layer_kwargs"": None,\n                ""clas_strategy"": ""final_time"",\n                ""max_seq_length"": None,\n                ""name"": ""unidirectional_rnn_classifier""\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n           :class:`~texar.torch.modules.UnidirectionalRNNEncoder`.\n           See the\n           :meth:`~texar.torch.modules.UnidirectionalRNNEncoder.default_hparams`\n           . An instance of UnidirectionalRNNEncoder is created for feature\n           extraction.\n\n        2. Additional hyperparameters:\n\n            `""num_classes""`: int\n                Number of classes:\n\n                - If **> 0**, an additional `Linear`\n                  layer is appended to the encoder to compute the logits over\n                  classes.\n                - If **<= 0**, no dense layer is appended. The number of\n                  classes is assumed to be the final dense layer size of the\n                  encoder.\n\n            `""logit_layer_kwargs""`: dict\n                Keyword arguments for the logit Dense layer constructor,\n                except for argument ""units"" which is set to `num_classes`.\n                Ignored if no extra logit layer is appended.\n\n            `""clas_strategy""`: str\n                The classification strategy, one of:\n\n                - **final_time**: Sequence-level classification based on the\n                  output of the final time step. Each sequence has a class.\n                - **all_time**: Sequence-level classification based on\n                  the output of all time steps. Each sequence has a class.\n                - **time_wise**: Step-wise classification, i.e., make\n                  classification for each time step based on its output.\n\n            `""max_seq_length""`: int, optional\n                Maximum possible length of input sequences. Required if\n                `clas_strategy` is `all_time`.\n\n            `""name""`: str\n                Name of the classifier.\n        """"""\n        hparams = UnidirectionalRNNEncoder.default_hparams()\n        hparams.update({\n            ""num_classes"": 2,\n            ""logit_layer_kwargs"": None,\n            ""clas_strategy"": ""final_time"",\n            ""max_seq_length"": None,\n            ""name"": ""bert_classifier""\n        })\n        return hparams\n\n    def forward(self,  # type: ignore\n                inputs: torch.Tensor,\n                sequence_length: Optional[torch.LongTensor] = None,\n                initial_state: Optional[State] = None,\n                time_major: bool = False) \\\n            -> Tuple[torch.Tensor, torch.LongTensor]:\n        r""""""Feeds the inputs through the network and makes classification.\n\n        The arguments are the same as in\n        :class:`~texar.torch.modules.UnidirectionalRNNEncoder`.\n\n        Args:\n            inputs: A 3D Tensor of shape ``[batch_size, max_time, dim]``.\n                The first two dimensions\n                :attr:`batch_size` and :attr:`max_time` are exchanged if\n                :attr:`time_major` is `True`.\n            sequence_length (optional): A 1D :tensor:`LongTensor` of shape\n                ``[batch_size]``.\n                Sequence lengths of the batch inputs. Used to copy-through\n                state and zero-out outputs when past a batch element\'s sequence\n                length.\n            initial_state (optional): Initial state of the RNN.\n            time_major (bool): The shape format of the :attr:`inputs` and\n                :attr:`outputs` Tensors. If `True`, these tensors are of shape\n                ``[max_time, batch_size, depth]``. If `False` (default),\n                these tensors are of shape ``[batch_size, max_time, depth]``.\n\n        Returns:\n            A tuple `(logits, preds)`, containing the logits over classes and\n            the predictions, respectively.\n\n            - If ``clas_strategy`` is ``final_time`` or ``all_time``:\n\n                - If ``num_classes`` == 1, ``logits`` and ``pred`` are both of\n                  shape ``[batch_size]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, num_classes]`` and ``pred`` is of shape\n                  ``[batch_size]``.\n\n            - If ``clas_strategy`` is ``time_wise``:\n\n                - ``num_classes`` == 1, ``logits`` and ``pred`` are both of\n                  shape ``[batch_size, max_time]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, max_time, num_classes]`` and ``pred`` is of\n                  shape ``[batch_size, max_time]``.\n                - If ``time_major`` is `True`, the batch and time dimensions\n                  are exchanged.\n        """"""\n        enc_outputs, _ = self._encoder(inputs=inputs,\n                                       sequence_length=sequence_length,\n                                       initial_state=initial_state,\n                                       time_major=time_major)\n        # Compute logits\n        strategy = self._hparams.clas_strategy\n        if strategy == \'time_wise\':\n            logits = enc_outputs\n        elif strategy == \'final_time\':\n            if time_major:\n                logits = enc_outputs[-1, :, :]\n            else:\n                logits = enc_outputs[:, -1, :]\n        elif strategy == \'all_time\':\n            if time_major:\n                length_diff = self._hparams.max_seq_length - inputs.shape[0]\n                logit_input = F.pad(enc_outputs, [0, length_diff, 0, 0, 0, 0])\n                logit_input_dim = (self._encoder.output_size *\n                                   self._hparams.max_seq_length)\n                logits = logit_input.view(-1, logit_input_dim)\n            else:\n                length_diff = self._hparams.max_seq_length - inputs.shape[1]\n                logit_input = F.pad(enc_outputs, [0, 0, 0, length_diff, 0, 0])\n                logit_input_dim = (self._encoder.output_size *\n                                   self._hparams.max_seq_length)\n                logits = logit_input.view(-1, logit_input_dim)\n        else:\n            raise ValueError(\'Unknown classification strategy: {}\'.format(\n                strategy))\n\n        if self._logits_layer is not None:\n            logits = self._logits_layer(logits)\n\n        # Compute predictions\n        if strategy == ""time_wise"":\n            if self.is_binary:\n                logits = torch.squeeze(logits, -1)\n                preds = (logits > 0).long()\n            else:\n                preds = torch.argmax(logits, dim=-1)\n        else:\n            if self.is_binary:\n                preds = (logits > 0).long()\n                logits = torch.flatten(logits)\n            else:\n                preds = torch.argmax(logits, dim=-1)\n            preds = torch.flatten(preds)\n\n        return logits, preds\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of :meth:`forward` output :attr:`logits`.\n        If :attr:`logits` size is only determined by input\n        (i.e. if ``num_classes`` == 1), the feature size is equal to ``-1``.\n        Otherwise it is equal to last dimension value of :attr:`logits` size.\n        """"""\n        if self._hparams.num_classes == 1:\n            logit_dim = -1\n        elif self._hparams.num_classes > 1:\n            logit_dim = self._hparams.num_classes\n        elif self._hparams.clas_strategy == \'all_time\':\n            logit_dim = (self._encoder.output_size *\n                         self._hparams.max_seq_length)\n        elif self._hparams.clas_strategy == \'final_time\':\n            logit_dim = self._encoder.output_size\n        elif self._hparams.clas_strategy == \'time_wise\':\n            logit_dim = self._hparams.encoder.dim\n\n        return logit_dim\n'"
texar/torch/modules/classifiers/roberta_classifier.py,13,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nRoBERTa classifier.\n""""""\nfrom typing import Optional, Tuple, Union\n\nimport torch\n\nfrom texar.torch.modules.encoders.roberta_encoder import RoBERTaEncoder\nfrom texar.torch.modules.classifiers.bert_classifier import BERTClassifier\nfrom texar.torch.modules.pretrained.roberta import \\\n    PretrainedRoBERTaMixin\n\n__all__ = [\n    ""RoBERTaClassifier""\n]\n\n\nclass RoBERTaClassifier(PretrainedRoBERTaMixin, BERTClassifier):\n    r""""""Classifier based on RoBERTa modules. Please see\n    :class:`~texar.torch.modules.PretrainedRoBERTaMixin` for a brief description\n    of RoBERTa.\n\n    This is a combination of the\n    :class:`~texar.torch.modules.RoBERTaEncoder` with a classification\n    layer. Both step-wise classification and sequence-level classification\n    are supported, specified in :attr:`hparams`.\n\n    Arguments are the same as in\n    :class:`~texar.torch.modules.RoBERTaEncoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``roberta-base``). Please refer to\n            :class:`~texar.torch.modules.PretrainedRoBERTaMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. document private functions\n    """"""\n    _ENCODER_CLASS = RoBERTaEncoder\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in RoBertaEncoder\n                ...\n                # (2) Additional hyperparameters\n                ""num_classes"": 2,\n                ""logit_layer_kwargs"": None,\n                ""clas_strategy"": ""cls_time"",\n                ""max_seq_length"": None,\n                ""dropout"": 0.1,\n                ""name"": ""roberta_classifier""\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n           :class:`~texar.torch.modules.RoBERTaEncoder`.\n           See the :meth:`~texar.torch.modules.RoBERTaEncoder.default_hparams`.\n           An instance of RoBERTaEncoder is created for feature extraction.\n\n        2. Additional hyperparameters:\n\n            `""num_classes""`: int\n                Number of classes:\n\n                - If **> 0**, an additional `Linear`\n                  layer is appended to the encoder to compute the logits over\n                  classes.\n                - If **<= 0**, no dense layer is appended. The number of\n                  classes is assumed to be the final dense layer size of the\n                  encoder.\n\n            `""logit_layer_kwargs""`: dict\n                Keyword arguments for the logit Dense layer constructor,\n                except for argument ""units"" which is set to `num_classes`.\n                Ignored if no extra logit layer is appended.\n\n            `""clas_strategy""`: str\n                The classification strategy, one of:\n\n                - **cls_time**: Sequence-level classification based on the\n                  output of the first time step (which is the `CLS` token).\n                  Each sequence has a class.\n                - **all_time**: Sequence-level classification based on\n                  the output of all time steps. Each sequence has a class.\n                - **time_wise**: Step-wise classification, i.e., make\n                  classification for each time step based on its output.\n\n            `""max_seq_length""`: int, optional\n                Maximum possible length of input sequences. Required if\n                `clas_strategy` is `all_time`.\n\n            `""dropout""`: float\n                The dropout rate of the RoBERTa encoder output.\n\n            `""name""`: str\n                Name of the classifier.\n        """"""\n\n        hparams = RoBERTaEncoder.default_hparams()\n        hparams.update({\n            ""num_classes"": 2,\n            ""logit_layer_kwargs"": None,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_length"": None,\n            ""dropout"": 0.1,\n            ""name"": ""roberta_classifier""\n        })\n        return hparams\n\n    def forward(self,  # type: ignore\n                inputs: Union[torch.Tensor, torch.LongTensor],\n                sequence_length: Optional[torch.LongTensor] = None) \\\n            -> Tuple[torch.Tensor, torch.LongTensor]:\n        r""""""Feeds the inputs through the network and makes classification.\n\n        The arguments are the same as in\n        :class:`~texar.torch.modules.RoBERTaEncoder`.\n\n        Args:\n            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,\n                containing the ids of tokens in input sequences, or\n                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,\n                containing soft token ids (i.e., weights or probabilities)\n                used to mix the embedding vectors.\n            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.\n                Input tokens beyond respective sequence lengths are masked\n                out automatically.\n\n        Returns:\n            A tuple `(logits, preds)`, containing the logits over classes and\n            the predictions, respectively.\n\n            - If ``clas_strategy`` is ``cls_time`` or ``all_time``:\n\n                - If ``num_classes`` == 1, ``logits`` and ``pred`` are both of\n                  shape ``[batch_size]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, num_classes]`` and ``pred`` is of shape\n                  ``[batch_size]``.\n\n            - If ``clas_strategy`` is ``time_wise``:\n\n                - ``num_classes`` == 1, ``logits`` and ``pred`` are both of\n                  shape ``[batch_size, max_time]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, max_time, num_classes]`` and ``pred`` is of\n                  shape ``[batch_size, max_time]``.\n        """"""\n        logits, preds = super().forward(inputs=inputs,\n                                        sequence_length=sequence_length,\n                                        segment_ids=None)\n        return logits, preds\n'"
texar/torch/modules/classifiers/xlnet_classifier.py,24,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nXLNet Classifier.\n""""""\n\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom texar.torch.core.layers import get_initializer\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.modules.classifiers.classifier_base import ClassifierBase\nfrom texar.torch.modules.encoders.xlnet_encoder import XLNetEncoder\nfrom texar.torch.modules.pretrained.xlnet import PretrainedXLNetMixin\nfrom texar.torch.modules.pretrained.xlnet_utils import (\n    init_weights, params_except_in)\nfrom texar.torch.utils.utils import dict_fetch\n\n__all__ = [\n    ""XLNetClassifier"",\n]\n\n\nclass XLNetClassifier(ClassifierBase, PretrainedXLNetMixin):\n    r""""""Classifier based on XLNet modules. Please see\n    :class:`~texar.torch.modules.PretrainedXLNetMixin` for a brief description\n    of XLNet.\n\n    Arguments are the same as in\n    :class:`~texar.torch.modules.XLNetEncoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``xlnet-based-cased``). Please refer to\n            :class:`~texar.torch.modules.PretrainedXLNetMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n\n        super().__init__(hparams=hparams)\n\n        # Create the underlying encoder\n        encoder_hparams = dict_fetch(hparams, XLNetEncoder.default_hparams())\n\n        self._encoder = XLNetEncoder(\n            pretrained_model_name=pretrained_model_name,\n            cache_dir=cache_dir,\n            hparams=encoder_hparams)\n\n        # TODO: The logic here is very similar to that in XLNetRegressor.\n        #  We need to reduce the code redundancy.\n        if self._hparams.use_projection:\n            if self._hparams.clas_strategy == \'all_time\':\n                self.projection = nn.Linear(\n                    self._encoder.output_size * self._hparams.max_seq_length,\n                    self._encoder.output_size * self._hparams.max_seq_length)\n            else:\n                self.projection = nn.Linear(self._encoder.output_size,\n                                            self._encoder.output_size)\n        self.dropout = nn.Dropout(self._hparams.dropout)\n\n        # Create an additional classification layer if needed\n        self.num_classes = self._hparams.num_classes\n        if self.num_classes <= 0:\n            self.hidden_to_logits = None\n        else:\n            logit_kwargs = self._hparams.logit_layer_kwargs\n            if logit_kwargs is None:\n                logit_kwargs = {}\n            elif not isinstance(logit_kwargs, HParams):\n                raise ValueError(""hparams[\'logit_layer_kwargs\'] ""\n                                 ""must be a dict."")\n            else:\n                logit_kwargs = logit_kwargs.todict()\n\n            if self._hparams.clas_strategy == \'all_time\':\n                self.hidden_to_logits = nn.Linear(\n                    self._encoder.output_size * self._hparams.max_seq_length,\n                    self.num_classes,\n                    **logit_kwargs)\n            else:\n                self.hidden_to_logits = nn.Linear(\n                    self._encoder.output_size, self.num_classes,\n                    **logit_kwargs)\n\n        if self._hparams.initializer:\n            initialize = get_initializer(self._hparams.initializer)\n            assert initialize is not None\n            if self._hparams.use_projection:\n                initialize(self.projection.weight)\n                initialize(self.projection.bias)\n            if self.hidden_to_logits:\n                initialize(self.hidden_to_logits.weight)\n                if self.hidden_to_logits.bias:\n                    initialize(self.hidden_to_logits.bias)\n        else:\n            if self._hparams.use_projection:\n                self.projection.apply(init_weights)\n            if self.hidden_to_logits:\n                self.hidden_to_logits.apply(init_weights)\n\n        self.is_binary = ((self.num_classes == 1) or\n                          (self.num_classes <= 0 and\n                           self._hparams.hidden_dim == 1))\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in XLNetEncoder\n                ...\n                # (2) Additional hyperparameters\n                ""clas_strategy"": ""cls_time"",\n                ""use_projection"": True,\n                ""num_classes"": 2,\n                ""name"": ""xlnet_classifier"",\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n            :class:`~texar.torch.modules.XLNetEncoder`.\n            See the :meth:`~texar.torch.modules.XLNetEncoder.default_hparams`.\n            An instance of XLNetEncoder is created for feature extraction.\n\n        2. Additional hyperparameters:\n\n            `""clas_strategy""`: str\n                The classification strategy, one of:\n\n                - **cls_time**: Sequence-level classification based on the\n                  output of the last time step (which is the `CLS` token).\n                  Each sequence has a class.\n                - **all_time**: Sequence-level classification based on\n                  the output of all time steps. Each sequence has a class.\n                - **time_wise**: Step-wise classification, i.e., make\n                  classification for each time step based on its output.\n\n            `""use_projection""`: bool\n                If `True`, an additional `Linear` layer is added after the\n                summary step.\n\n            `""num_classes""`: int\n                Number of classes:\n\n                - If **> 0**, an additional :torch_nn:`Linear`\n                  layer is appended to the encoder to compute the logits over\n                  classes.\n                - If **<= 0**, no dense layer is appended. The number of\n                  classes is assumed to be the final dense layer size of the\n                  encoder.\n\n            `""name""`: str\n                Name of the classifier.\n        """"""\n\n        hparams = XLNetEncoder.default_hparams()\n        hparams.update({\n            ""clas_strategy"": ""cls_time"",\n            ""use_projection"": True,\n            ""num_classes"": 2,\n            ""logit_layer_kwargs"": None,\n            ""name"": ""xlnet_classifier"",\n        })\n        return hparams\n\n    def param_groups(self,\n                     lr: Optional[float] = None,\n                     lr_layer_scale: float = 1.0,\n                     decay_base_params: bool = False):\n        r""""""Create parameter groups for optimizers. When\n        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form\n        separate groups with different base learning rates.\n\n        The return value of this method can be used in the constructor of\n        optimizers, for example:\n\n        .. code-block:: python\n\n            model = XLNetClassifier(...)\n            param_groups = model.param_groups(lr=2e-5, lr_layer_scale=0.8)\n            optim = torch.optim.Adam(param_groups)\n\n        Args:\n            lr (float): The learning rate. Can be omitted if\n                :attr:`lr_layer_decay_rate` is 1.0.\n            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer\n                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.\n            decay_base_params (bool): If `True`, treat non-layer parameters\n                (e.g. embeddings) as if they\'re in layer 0. If `False`, these\n                parameters are not scaled.\n\n        Returns:\n            The parameter groups, used as the first argument for optimizers.\n        """"""\n\n        # TODO: Same logic in XLNetRegressor. Reduce code redundancy.\n\n        if lr_layer_scale != 1.0:\n            if lr is None:\n                raise ValueError(\n                    ""lr must be specified when lr_layer_decay_rate is not 1.0"")\n\n            fine_tune_group = {\n                ""params"": params_except_in(self, [""_encoder""]),\n                ""lr"": lr\n            }\n            param_groups = [fine_tune_group]\n            param_group = self._encoder.param_groups(lr, lr_layer_scale,\n                                                     decay_base_params)\n            param_groups.extend(param_group)\n            return param_groups\n        return self.parameters()\n\n    def forward(self,  # type: ignore\n                inputs: Union[torch.Tensor, torch.LongTensor],\n                segment_ids: Optional[torch.LongTensor] = None,\n                input_mask: Optional[torch.Tensor] = None) \\\n            -> Tuple[torch.Tensor, torch.LongTensor]:\n        r""""""Feeds the inputs through the network and makes classification.\n\n        Args:\n            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,\n                containing the ids of tokens in input sequences, or\n                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,\n                containing soft token ids (i.e., weights or probabilities)\n                used to mix the embedding vectors.\n            segment_ids: Shape `[batch_size, max_time]`.\n            input_mask: Float tensor of shape `[batch_size, max_time]`. Note\n                that positions with value 1 are masked out.\n\n        Returns:\n            A tuple `(logits, preds)`, containing the logits over classes and\n            the predictions, respectively.\n\n            - If ``clas_strategy`` is ``cls_time`` or ``all_time``:\n\n                - If ``num_classes`` == 1, ``logits`` and ``pred`` are both of\n                  shape ``[batch_size]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, num_classes]`` and ``pred`` is of shape\n                  ``[batch_size]``.\n\n            - If ``clas_strategy`` is ``time_wise``:\n\n                - ``num_classes`` == 1, ``logits`` and ``pred`` are both of\n                  shape ``[batch_size, max_time]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, max_time, num_classes]`` and ``pred`` is of\n                  shape ``[batch_size, max_time]``.\n        """"""\n        # output: [batch_size, seq_len, hidden_dim]\n        output, _ = self._encoder(inputs=inputs,\n                                  segment_ids=segment_ids,\n                                  input_mask=input_mask)\n\n        strategy = self._hparams.clas_strategy\n        if strategy == \'time_wise\':\n            summary = output\n        elif strategy == \'cls_time\':\n            summary = output[:, -1]\n        elif strategy == \'all_time\':\n            length_diff = self._hparams.max_seq_length - inputs.shape[1]\n            summary_input = F.pad(output, [0, 0, 0, length_diff, 0, 0])\n            summary_input_dim = (self._encoder.output_size *\n                                 self._hparams.max_seq_length)\n\n            summary = summary_input.contiguous().view(-1, summary_input_dim)\n        else:\n            raise ValueError(f""Unknown classification strategy: {strategy}."")\n\n        if self._hparams.use_projection:\n            summary = torch.tanh(self.projection(summary))\n\n        if self.hidden_to_logits is not None:\n            summary = self.dropout(summary)\n            logits = self.hidden_to_logits(summary)\n        else:\n            logits = summary\n\n        # Compute predictions\n        if strategy == ""time_wise"":\n            if self.is_binary:\n                logits = torch.squeeze(logits, -1)\n                preds = (logits > 0).long()\n            else:\n                preds = torch.argmax(logits, dim=-1)\n        else:\n            if self.is_binary:\n                preds = (logits > 0).long()\n                logits = torch.flatten(logits)\n            else:\n                preds = torch.argmax(logits, dim=-1)\n            preds = torch.flatten(preds)\n\n        return logits, preds\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of :meth:`forward` output :attr:`logits`.\n        If :attr:`logits` size is only determined by input\n        (i.e. if ``num_classes`` == 1), the feature size is equal to ``-1``.\n        Otherwise it is equal to last dimension value of :attr:`logits` size.\n        """"""\n        if self._hparams.num_classes > 1:\n            logit_dim = self._hparams.num_classes\n        elif self._hparams.num_classes == 1:\n            logit_dim = -1\n        else:\n            logit_dim = self._hparams.hidden_dim\n        return logit_dim\n'"
texar/torch/modules/connectors/__init__.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library connectors.\n""""""\n\nfrom texar.torch.modules.connectors.connector_base import *\nfrom texar.torch.modules.connectors.connectors import *\n'"
texar/torch/modules/connectors/connector_base.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for connectors that transform inputs into specified output shape.\n""""""\n\nfrom abc import ABC\nfrom typing import Generic, Optional, TypeVar\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.module_base import ModuleBase\n\n__all__ = [\n    ""ConnectorBase""\n]\n\nOutputSize = TypeVar(\'OutputSize\')\nHParamsType = Optional[HParams]\n\n\nclass ConnectorBase(ModuleBase, Generic[OutputSize], ABC):\n    r""""""Base class inherited by all connector classes. A connector is to\n    transform inputs into outputs with any specified structure and shape.\n    For example, transforming the final state of an encoder to the initial\n    state of a decoder, and performing stochastic sampling in between as\n    in Variational Autoencoders (VAEs).\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set ``output_size`` to ``dim`` to generate output of\n            shape ``[batch_size, dim]``.\n            Can be an `int`, a tuple of `int`, a torch.Size, or a tuple of\n            torch.Sizes.\n            For example, to transform inputs to have decoder state size, set\n            :python:`output_size=decoder.state_size`.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n    """"""\n\n    def __init__(self, output_size: OutputSize, hparams: HParamsType = None):\n        super().__init__(hparams=hparams)\n        self._output_size = output_size\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n        """"""\n        return {\n            ""name"": ""connector""\n        }\n\n    @property\n    def output_size(self):\n        return self._output_size\n'"
texar/torch/modules/connectors/connectors.py,80,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious connectors.\n""""""\n\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.distributions.distribution import Distribution\n\nfrom texar.torch.core import get_activation_fn\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.modules.connectors.connector_base import ConnectorBase\nfrom texar.torch.utils import nest\nfrom texar.torch.utils import utils\nfrom texar.torch.utils.types import MaybeTuple\n\n__all__ = [\n    ""ConstantConnector"",\n    ""ForwardConnector"",\n    ""MLPTransformConnector"",\n    ""ReparameterizedStochasticConnector"",\n    ""StochasticConnector"",\n    # ""ConcatConnector""\n]\n\nTensorStruct = Union[List[torch.Tensor],\n                     Dict[Any, torch.Tensor],\n                     MaybeTuple[torch.Tensor]]\nOutputSize = MaybeTuple[Union[int, torch.Size]]\nActivationFn = Callable[[torch.Tensor], torch.Tensor]\nLinearLayer = Callable[[torch.Tensor], torch.Tensor]\n\n\ndef _assert_same_size(outputs: TensorStruct,\n                      output_size: OutputSize):\n    r""""""Check if outputs match output_size\n\n    Args:\n        outputs: A tensor or a (nested) tuple of tensors\n        output_size: Can be an ``int``, a ``torch.Size``, or a (nested)\n            tuple of ``int`` or ``torch.Size``.\n    """"""\n    flat_output_size = nest.flatten(output_size)\n    flat_output = nest.flatten(outputs)\n\n    for (output, size) in zip(flat_output, flat_output_size):\n\n        if isinstance(size, torch.Size):\n            if output[0].size() != size:\n                raise ValueError(""The output size does not match""\n                                 ""the required output_size"")\n        elif output[0].size()[-1] != size:\n            raise ValueError(\n                ""The output size does not match the required output_size"")\n\n\ndef _get_sizes(sizes: List[Any]) -> List[int]:\n    r""""""\n\n    Args:\n        sizes: A list of ``int`` or ``torch.Size``. If each element is of type\n            ``torch.Size``, the size is computed by taking the product of the\n            shape.\n\n    Returns:\n        A list of sizes with ``torch.Size`` replaced by product of its\n        individual dimensions\n    """"""\n    if isinstance(sizes[0], torch.Size):\n        size_list = [np.prod(shape) for shape in sizes]\n    else:\n        size_list = sizes\n\n    return size_list\n\n\ndef _sum_output_size(output_size: OutputSize) -> int:\n    r""""""Return sum of all dim values in :attr:`output_size`\n\n    Args:\n        output_size: Can be an ``int``, a ``torch.Size``, or a (nested)\n            tuple of ``int`` or ``torch.Size``.\n    """"""\n    flat_output_size = nest.flatten(output_size)\n    size_list = _get_sizes(flat_output_size)\n    ret = sum(size_list)\n    return ret\n\n\ndef _mlp_transform(inputs: TensorStruct,\n                   output_size: OutputSize,\n                   linear_layer: Optional[LinearLayer] = None,\n                   activation_fn: Optional[ActivationFn] = None) -> Any:\n    r""""""Transforms inputs through a fully-connected layer that creates\n    the output with specified size.\n\n    Args:\n        inputs: A Tensor of shape `[batch_size, d1, ..., dn]`, or a (nested)\n            tuple of such elements. The dimensions `d1, ..., dn` will be flatten\n            and transformed by a dense layer.\n        output_size: Can be an ``int``, a ``torch.Size``, or a (nested)\n            tuple of ``int`` or ``torch.Size``.\n        activation_fn: Activation function applied to the output.\n\n    :returns:\n        If :attr:`output_size` is an ``int`` or a ``torch.Size``,\n        returns a tensor of shape ``[batch_size, *, output_size]``.\n        If :attr:`output_size` is a tuple of ``int`` or ``torch.Size``,\n        returns a tuple having the same structure as :attr:`output_size`,\n        where each element has the same size as defined in :attr:`output_size`.\n    """"""\n    # Flatten inputs\n    flat_input = nest.flatten(inputs)\n    flat_input = [x.view(-1, x.size(-1)) for x in flat_input]\n    concat_input = torch.cat(flat_input, 1)\n\n    # Get output dimension\n    flat_output_size = nest.flatten(output_size)\n\n    size_list = _get_sizes(flat_output_size)\n\n    fc_output = concat_input\n    if linear_layer is not None:\n        fc_output = linear_layer(fc_output)\n    if activation_fn is not None:\n        fc_output = activation_fn(fc_output)\n\n    flat_output = torch.split(fc_output, size_list, dim=1)\n    flat_output = list(flat_output)\n\n    if isinstance(flat_output_size[0], torch.Size):\n        flat_output = [torch.reshape(output, (-1,) + shape) for output, shape\n                       in zip(flat_output, flat_output_size)]\n\n    output = nest.pack_sequence_as(structure=output_size,\n                                   flat_sequence=flat_output)\n    return output\n\n\nclass ConstantConnector(ConnectorBase):\n    r""""""Creates a constant tensor or (nested) tuple of Tensors that\n    contains a constant value.\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set :attr:`output_size` to ``dim`` to generate output of\n            shape ``[batch_size, dim]``.\n            Can be an ``int``, a tuple of ``int``, a ``torch.Size``,\n            or a tuple of ``torch.Size``.\n            For example, to transform inputs to have decoder state size, set\n            :python:`output_size=decoder.state_size`.\n            If :attr:`output_size` is a tuple ``(1, 2, 3)``, then the\n            output structure will be\n            ``([batch_size * 1], [batch_size * 2], [batch_size * 3])``.\n            If :attr:`output_size` is ``torch.Size([1, 2, 3])``, then the\n            output structure will be ``[batch_size, 1, 2, 3]``.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n    This connector does not have trainable parameters.\n\n    Example:\n\n        .. code-block:: python\n\n            state_size = (1, 2, 3)\n            connector = ConstantConnector(state_size, hparams={""value"": 1.})\n            one_state = connector(batch_size=64)\n            # `one_state` structure: (Tensor_1, Tensor_2, Tensor_3),\n            # Tensor_1.size() == torch.Size([64, 1])\n            # Tensor_2.size() == torch.Size([64, 2])\n            # Tensor_3.size() == torch.Size([64, 3])\n            # Tensors are filled with 1.0.\n            size = torch.Size([1, 2, 3])\n            connector_size = ConstantConnector(size, hparams={""value"": 2.})\n            size_state = connector_size(batch_size=64)\n            # `size_state` structure: Tensor with size [64, 1, 2, 3].\n            # Tensor is filled with 2.0.\n\n    """"""\n\n    def __init__(self,\n                 output_size: OutputSize,\n                 hparams: Optional[HParams] = None):\n\n        super().__init__(output_size, hparams=hparams)\n\n        self.value = self.hparams.value\n\n    @staticmethod\n    def default_hparams() -> dict:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""value"": 0.,\n                ""name"": ""constant_connector""\n            }\n\n        Here:\n\n        `""value""`: float\n            The constant scalar that the output tensor(s) has.\n        `""name""`: str\n            Name of the connector.\n        """"""\n        return {\n            ""value"": 0.,\n            ""name"": ""constant_connector""\n        }\n\n    def forward(self,  # type: ignore\n                batch_size: Union[int, torch.Tensor]) -> Any:\n        r""""""Creates output tensor(s) that has the given value.\n\n        Args:\n            batch_size: An ``int`` or ``int`` scalar tensor, the\n                batch size.\n\n        :returns:\n            A (structure of) tensor whose structure is the same as\n            :attr:`output_size`, with value specified by\n            ``value`` or :attr:`hparams`.\n        """"""\n\n        def full_tensor(x):\n            if isinstance(x, torch.Size):\n                return torch.full((batch_size,) + x, self.value)\n            else:\n                return torch.full((batch_size, x), self.value)\n\n        output = utils.map_structure(\n            full_tensor,\n            self._output_size)\n\n        return output\n\n\nclass ForwardConnector(ConnectorBase):\n    r""""""Transforms inputs to have specified structure.\n\n    Example:\n\n    .. code-block:: python\n\n        state_size = namedtuple(\'LSTMStateTuple\', [\'h\', \'c\'])(256, 256)\n        # state_size == LSTMStateTuple(c=256, h=256)\n        connector = ForwardConnector(state_size)\n        output = connector([tensor_1, tensor_2])\n        # output == LSTMStateTuple(c=tensor_1, h=tensor_2)\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set :attr:`output_size` to ``dim`` to generate output of\n            shape ``[batch_size, dim]``.\n            Can be an ``int``, a tuple of ``int``, a ``torch.Size``, or a\n            tuple of ``torch.Size``.\n            For example, to transform inputs to have decoder state size, set\n            :python:`output_size=decoder.state_size`.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    This connector does not have trainable parameters.\n    See :meth:`forward` for the inputs and outputs of the connector.\n    The input to the connector must have the same structure with\n    :attr:`output_size`, or must have the same number of elements and be\n    re-packable into the structure of :attr:`output_size`. Note that if input\n    is or contains a ``dict`` instance, the keys will be sorted to pack in\n    deterministic order (See :func:`~texar.torch.utils.nest.pack_sequence_as`).\n    """"""\n\n    def __init__(self,\n                 output_size: OutputSize,\n                 hparams: Optional[HParams] = None):\n        super().__init__(output_size, hparams=hparams)\n\n    @staticmethod\n    def default_hparams() -> dict:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""forward_connector""\n            }\n\n        Here:\n\n        `""name""`: str\n            Name of the connector.\n        """"""\n        return {\n            ""name"": ""forward_connector""\n        }\n\n    def forward(self,  # type: ignore\n                inputs: TensorStruct) -> Any:\n        r""""""Transforms inputs to have the same structure as with\n        :attr:`output_size`. Values of the inputs are not changed.\n        :attr:`inputs` must either have the same structure, or have the same\n        number of elements with :attr:`output_size`.\n\n        Args:\n            inputs: The input (structure of) tensor to pass forward.\n\n        :returns:\n            A (structure of) tensors that re-packs :attr:`inputs` to have\n            the specified structure of :attr:`output_size`.\n        """"""\n        flat_input = nest.flatten(inputs)\n        output = nest.pack_sequence_as(\n            self._output_size, flat_input)\n\n        return output\n\n\nclass MLPTransformConnector(ConnectorBase):\n    r""""""Transforms inputs with an MLP layer and packs the results into the\n    specified structure and size.\n\n    Example:\n\n        .. code-block:: python\n\n            cell = LSTMCell(num_units=256)\n            # cell.state_size == LSTMStateTuple(c=256, h=256)\n            connector = MLPTransformConnector(cell.state_size)\n            inputs = torch.zeros([64, 10])\n            output = connector(inputs)\n            # output == LSTMStateTuple(c=tensor_of_shape_(64, 256),\n            #                          h=tensor_of_shape_(64, 256))\n\n        .. code-block:: python\n\n            ## Use to connect encoder and decoder with different state size\n            encoder = UnidirectionalRNNEncoder(...)\n            _, final_state = encoder(inputs=...)\n            decoder = BasicRNNDecoder(...)\n            connector = MLPTransformConnector(decoder.state_size)\n            _ = decoder(\n                initial_state=connector(final_state),\n                ...)\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set :attr:`output_size` to ``dim`` to generate output of\n            shape ``[batch_size, dim]``.\n            Can be an ``int``, a tuple of ``int``, a ``torch.Size``,\n            or a tuple of ``torch.Size``.\n            For example, to transform inputs to have decoder state size, set\n            :python:`output_size=decoder.state_size`.\n        linear_layer_dim (int): Value of final dim of the input tensors i.e. the\n            input dim of the mlp linear layer.\n        hparams (dict, optional): Hyperparameters. Missing hyperparameter will\n            be set to default values. See :meth:`default_hparams` for the\n            hyperparameter structure and default values.\n\n    The input to the connector can have arbitrary structure and size.\n    """"""\n\n    def __init__(self,\n                 output_size: OutputSize,\n                 linear_layer_dim: int,\n                 hparams: Optional[HParams] = None):\n        super().__init__(output_size, hparams=hparams)\n        self._linear_layer = nn.Linear(\n            linear_layer_dim, _sum_output_size(output_size))\n        self._activation_fn = get_activation_fn(\n            self.hparams.activation_fn)\n\n    @staticmethod\n    def default_hparams() -> dict:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""activation_fn"": ""texar.torch.core.layers.identity"",\n                ""name"": ""mlp_connector""\n            }\n\n        Here:\n\n        `""activation_fn""`: str or callable\n            The activation function applied to the outputs of the MLP\n            transformation layer. Can\n            be a function, or its name or module path.\n        `""name""`: str\n            Name of the connector.\n        """"""\n        return {\n            ""activation_fn"": ""texar.torch.core.layers.identity"",\n            ""name"": ""mlp_connector""\n        }\n\n    def forward(self,  # type: ignore\n                inputs: TensorStruct) -> Any:\n        r""""""Transforms inputs with an MLP layer and packs the results to have\n        the same structure as specified by :attr:`output_size`.\n\n        Args:\n            inputs: Input (structure of) tensors to be transformed. Must be a\n                tensor of shape ``[batch_size, ...]`` or a (nested)\n                tuple of such Tensors. That is, the first dimension of\n                (each) tensor must be the batch dimension.\n\n        :returns:\n            A tensor or a (nested) tuple of tensors of the same structure of\n            :attr:`output_size`.\n        """"""\n\n        output = _mlp_transform(\n            inputs, self._output_size,\n            self._linear_layer, self._activation_fn)\n\n        return output\n\n\nclass ReparameterizedStochasticConnector(ConnectorBase):\n    r""""""Samples from a distribution with reparameterization trick, and\n    transforms samples into specified size.\n    Reparameterization allows gradients to be back-propagated through the\n    stochastic samples. Used in, e.g., Variational Autoencoders (VAEs).\n\n    Example:\n\n        .. code-block:: python\n\n            # Initialized without num_samples\n            cell = LSTMCell(num_units=256)\n            # cell.state_size == LSTMStateTuple(c=256, h=256)\n            mu = torch.zeros([16, 100])\n            var = torch.ones([100])\n\n            connector = ReparameterizedStochasticConnector(\n                cell.state_size,\n                mlp_input_size=mu.size()[-1],\n                distribution=""MultivariateNormal"",\n                distribution_kwargs={\n                    ""loc"": mu,\n                    ""scale_tril"": torch.diag(var)})\n            output, sample = connector()\n            # output == LSTMStateTuple(c=tensor_of_shape_(16, 256),\n            #                          h=tensor_of_shape_(16, 256))\n            # sample == Tensor([16, 100])\n\n            output_, sample_ = connector(num_samples=4)\n            # output_ == LSTMStateTuple(c=tensor_of_shape_(4, 16, 256),\n            #                           h=tensor_of_shape_(4, 16, 256))\n            # sample == Tensor([4, 16, 100])\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set ``output_size`` to ``dim`` to generate output of\n            shape ``[batch_size, dim]``.\n            Can be an ``int``, a tuple of ``int``, a ``torch.Size``, or\n            a tuple of ``torch.Size``.\n            For example, to transform inputs to have decoder state size, set\n            :python:`output_size=decoder.state_size`.\n        mlp_input_size: Size of MLP transfer process input, which is equal to\n            the distribution result size **excluding** the batch dimension,\n            Can be ``int`` or ``torch.Size`` or a tuple of ``int``.\n        distribution: A instance or name ``str`` of subclass of\n            :torch:`distributions.distribution.Distribution`,\n            Can be a distribution class instance or ``str``.\n        distribution_kwargs (dict, optional): ``dict`` of keyword arguments\n            for the :attr:`distribution`. Its keys are `str`, which are names\n            of keyword arguments; Its values are corresponding values for each\n            argument.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n    """"""\n\n    def __init__(self,\n                 output_size: OutputSize,\n                 mlp_input_size: Union[torch.Size, MaybeTuple[int], int],\n                 distribution: Union[Distribution, str] = \'MultivariateNormal\',\n                 distribution_kwargs: Optional[Dict[str, Any]] = None,\n                 hparams: Optional[HParams] = None):\n        super().__init__(output_size, hparams=hparams)\n        if distribution_kwargs is None:\n            distribution_kwargs = {}\n        self._dstr_type = distribution\n        self._dstr_kwargs = distribution_kwargs\n\n        for dstr_attr, dstr_val in distribution_kwargs.items():\n            if isinstance(dstr_val, torch.Tensor):\n                dstr_param = nn.Parameter(dstr_val)\n                distribution_kwargs[dstr_attr] = dstr_param\n                self.register_parameter(dstr_attr, dstr_param)\n        if isinstance(mlp_input_size, int):\n            input_feature = mlp_input_size\n        else:\n            input_feature = np.prod(mlp_input_size)\n        self._linear_layer = nn.Linear(\n            input_feature, _sum_output_size(output_size))\n\n        self._activation_fn = get_activation_fn(\n            self.hparams.activation_fn)\n\n    @staticmethod\n    def default_hparams() -> dict:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""activation_fn"": ""texar.torch.core.layers.identity"",\n                ""name"": ""reparameterized_stochastic_connector""\n            }\n\n        Here:\n\n        `""activation_fn""`: str\n            The activation function applied to the outputs of the MLP\n            transformation layer. Can be a function, or its name or module path.\n        `""name""`: str\n            Name of the connector.\n        """"""\n        return {\n            ""activation_fn"": ""texar.torch.core.layers.identity"",\n            ""name"": ""reparameterized_stochastic_connector""\n        }\n\n    def forward(self,  # type: ignore\n                num_samples: Optional[Union[int, torch.Tensor]] = None,\n                transform: bool = True) -> Tuple[Any, Any]:\n        r""""""Samples from a distribution and optionally performs transformation\n        with an MLP layer.\n        The distribution must be reparameterizable, i.e.,\n        :python:`Distribution.has_rsample == True`.\n\n        Args:\n            num_samples (optional): An ``int`` or ``int`` tensor.\n                Number of samples to generate. If not given,\n                generate a single sample. Note that if batch size has\n                already been included in :attr:`distribution`\'s dimensionality,\n                :attr:`num_samples` should be left as ``None``.\n            transform (bool): Whether to perform MLP transformation of the\n                distribution samples. If ``False``, the structure/shape of a\n                sample must match :attr:`output_size`.\n\n\n        :returns:\n            A tuple (:attr:`output`, :attr:`sample`), where\n\n            - output: A tensor or a (nested) tuple of Tensors with\n              the same structure and size of :attr:`output_size`.\n              The batch dimension equals :attr:`num_samples` if specified,\n              or is determined by the distribution dimensionality.\n              If :attr:`transform` is `False`, it will be\n              equal to :attr:`sample`.\n            - sample: The sample from the distribution, prior to transformation.\n\n            Otherwise, returns a tensor :attr:`sample`, where\n            - sample: The sample from the distribution, prior to transformation.\n\n        Raises:\n            ValueError: If distribution is not reparameterizable.\n            ValueError: The output does not match :attr:`output_size`.\n        """"""\n        if isinstance(self._dstr_type, str):\n            dstr: Distribution = utils.check_or_get_instance(\n                self._dstr_type, self._dstr_kwargs,\n                [""torch.distributions"", ""texar.torch.custom""])\n        else:\n            dstr = self._dstr_type\n\n        if not dstr.has_rsample:\n            raise ValueError(""Distribution should be reparameterizable"")\n\n        if num_samples:\n            sample = dstr.rsample([num_samples])\n        else:\n            sample = dstr.rsample()\n\n        if transform:\n            output = _mlp_transform(\n                sample,\n                self._output_size,\n                self._linear_layer,\n                self._activation_fn)\n            _assert_same_size(output, self._output_size)\n\n        else:\n            output = sample\n\n        return output, sample\n\n\nclass StochasticConnector(ConnectorBase):\n    r""""""Samples from a distribution and transforms samples into specified size.\n    The connector is the same as\n    :class:`~texar.torch.modules.ReparameterizedStochasticConnector`, except\n    that here reparameterization is disabled, and thus the gradients cannot be\n    back-propagated through the stochastic samples.\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set ``output_size`` to ``dim`` to generate output of\n            shape ``[batch_size, dim]``.\n            Can be an ``int``, a tuple of ``int``, a torch.Size, or a tuple of\n            torch.Size.\n            For example, to transform inputs to have decoder state size, set\n            :python:`output_size=decoder.state_size`.\n        mlp_input_size: Size of MLP transfer process input, which is equal to\n            the distribution result size **excluding** the batch dimension,\n            Can be ``int`` or ``torch.Size`` or a tuple of ``int``.\n        distribution: A instance of subclass of\n            :torch:`distributions.distribution.Distribution`,\n            Can be a class, its name or module path, or a class instance.\n            The :attr:`distribution` should not be reparameterizable.\n        distribution_kwargs (dict, optional): ``dict`` of keyword arguments\n            for the :attr:`distribution`. Its keys are `str`, which are names\n            of keyword arguments; Its values are corresponding values for each\n            argument.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n    """"""\n\n    def __init__(self,\n                 output_size: OutputSize,\n                 mlp_input_size: Union[torch.Size, MaybeTuple[int], int],\n                 distribution: Union[Distribution, str] = \'MultivariateNormal\',\n                 distribution_kwargs: Optional[Dict[str, Any]] = None,\n                 hparams: Optional[HParams] = None):\n        super().__init__(output_size, hparams=hparams)\n        if distribution_kwargs is None:\n            distribution_kwargs = {}\n        self._dstr_kwargs = distribution_kwargs\n        if isinstance(distribution, str):\n            self._dstr: Distribution = utils.check_or_get_instance(\n                distribution, self._dstr_kwargs,\n                [""torch.distributions"", ""texar.torch.custom""])\n        else:\n            self._dstr = distribution\n\n        if self._dstr.has_rsample:\n            raise ValueError(""Distribution should not be reparameterizable"")\n\n        if isinstance(mlp_input_size, int):\n            input_feature = mlp_input_size\n        else:\n            input_feature = np.prod(mlp_input_size)\n        self._linear_layer = nn.Linear(\n            input_feature, _sum_output_size(output_size))\n\n        self._activation_fn = get_activation_fn(\n            self.hparams.activation_fn)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""activation_fn"": ""texar.torch.core.layers.identity"",\n                ""name"": ""stochastic_connector""\n            }\n\n        Here:\n\n        `""activation_fn""`: str\n            The activation function applied to the outputs of the MLP\n            transformation layer. Can\n            be a function, or its name or module path.\n        `""name""`: str\n            Name of the connector.\n        """"""\n        return {\n            ""activation_fn"": ""texar.torch.core.layers.identity"",\n            ""name"": ""stochastic_connector""\n        }\n\n    def forward(self,  # type: ignore\n                num_samples: Optional[Union[int, torch.Tensor]] = None,\n                transform: bool = False) -> Any:\n        r""""""Samples from a distribution and optionally performs transformation\n        with an MLP layer.\n        The inputs and outputs are the same as\n        :class:`~texar.torch.modules.ReparameterizedStochasticConnector` except\n        that the distribution does not need to be reparameterizable, and\n        gradient cannot be back-propagate through the samples.\n\n        Args:\n            num_samples (optional): An ``int`` or ``int`` tensor.\n                Number of samples to generate. If not given,\n                generate a single sample. Note that if batch size has\n                already been included in :attr:`distribution`\'s dimensionality,\n                :attr:`num_samples` should be left as ``None``.\n            transform (bool): Whether to perform MLP transformation of the\n                distribution samples. If ``False``, the structure/shape of a\n                sample must match :attr:`output_size`.\n\n        :returns:\n            A tuple (:attr:`output`, :attr:`sample`), where\n\n            - output: A tensor or a (nested) tuple of Tensors with\n              the same structure and size of :attr:`output_size`.\n              The batch dimension equals :attr:`num_samples` if specified,\n              or is determined by the distribution dimensionality.\n              If :attr:`transform` is `False`, it will be\n              equal to :attr:`sample`.\n            - sample: The sample from the distribution, prior to transformation.\n\n        Raises:\n\n            ValueError: If distribution can be reparameterizable.\n            ValueError: The output does not match :attr:`output_size`.\n        """"""\n\n        if num_samples:\n            sample = self._dstr.sample([num_samples])\n        else:\n            sample = self._dstr.sample()\n\n        if self._dstr.event_shape == []:\n            sample = torch.reshape(\n                input=sample, shape=sample.size() + torch.Size([1]))\n\n        # Disable gradients through samples\n        sample = sample.detach().float()\n        if transform:\n            output = _mlp_transform(\n                sample,\n                self._output_size,\n                self._linear_layer,\n                self._activation_fn)\n            _assert_same_size(output, self._output_size)\n\n        else:\n            output = sample\n\n        return output, sample\n\n# class ConcatConnector(ConnectorBase):\n#    r""""""Concatenates multiple connectors into one connector. Used in, e.g.,\n#    semi-supervised variational autoencoders, disentangled representation\n#    learning, and other models.\n#\n#    Args:\n#        output_size: Size of output excluding the batch dimension (eg.\n#            :attr:`output_size = p` if :attr:`output.shape` is :attr:`[N, p]`).\n#            Can be an int, a tuple of int, a torch.Size, or a tuple of\n#            torch.Size.\n#            For example, to transform to decoder state size, set\n#            `output_size=decoder.cell.state_size`.\n#        hparams (dict): Hyperparameters of the connector.\n#    """"""\n#\n#    def __init__(self, output_size, hparams=None):\n#        super().__init__(self, output_size, hparams)\n#\n#    @staticmethod\n#    def default_hparams():\n#        r""""""Returns a dictionary of hyperparameters with default values.\n#\n#        Returns:\n#\n#            .. code-block:: python\n#\n#                {\n#                    ""activation_fn"": ""texar.torch.core.layers.identity"",\n#                    ""name"": ""concat_connector""\n#                }\n#\n#            Here:\n#\n#            `""activation_fn""`: (str or callable)\n#                The name or full path to the activation function applied to\n#                the outputs of the MLP layer. The activation functions can be:\n#\n#                - Built-in activation functions defined in :\n#                - User-defined activation functions in `texar.torch.custom`.\n#                - External activation functions. Must provide the full path, \\\n#                  e.g., ""my_module.my_activation_fn"".\n#\n#                The default value is :attr:`""identity""`, i.e., the MLP\n#                transformation is linear.\n#\n#            `""name""`: str\n#                Name of the connector.\n#\n#                The default value is ""concat_connector"".\n#        """"""\n#        return {\n#            ""activation_fn"": ""texar.torch.core.layers.identity"",\n#            ""name"": ""concat_connector""\n#        }\n#\n#    def forward(self, connector_inputs, transform=True):\n#        r""""""Concatenate multiple input connectors\n#\n#        Args:\n#            connector_inputs: a list of connector states\n#            transform (bool): If `True`, then the output are automatically\n#                transformed to match :attr:`output_size`.\n#\n#        Returns:\n#            A Tensor or a (nested) tuple of Tensors of the same structure of\n#            the decoder state.\n#        """"""\n#        connector_inputs = [connector.float()\n#                            for connector in connector_inputs]\n#        output = torch.cat(connector_inputs, dim=1)\n#\n#        if transform:\n#            fn_modules = [\'texar.torch.custom\', \'torch\', \'torch.nn\']\n#            activation_fn = utils.get_function(self.hparams.activation_fn,\n#                                         fn_modules)\n#            output, linear_layer = _mlp_transform(\n#               output, self._output_size, activation_fn)\n#            self._linear_layers.append(linear_layer)\n#        _assert_same_size(output, self._output_size)\n#\n#        self._add_internal_trainable_variables()\n#        self._built = True\n#\n#        return output\n'"
texar/torch/modules/decoders/__init__.py,8,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library decoders.\n""""""\n\nfrom texar.torch.modules.decoders.decoder_base import DecoderBase\nfrom texar.torch.modules.decoders.decoder_helpers import *\nfrom texar.torch.modules.decoders.rnn_decoder_base import *\nfrom texar.torch.modules.decoders.gpt2_decoder import *\nfrom texar.torch.modules.decoders.rnn_decoders import *\nfrom texar.torch.modules.decoders.transformer_decoders import *\nfrom texar.torch.modules.decoders.xlnet_decoder import *\nfrom texar.torch.modules.decoders.t5_decoder import *\n'"
texar/torch/modules/decoders/decoder_base.py,46,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for decoders.\n""""""\n\nimport copy\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Generic, Optional, Tuple, TypeVar, Union, overload\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.core.layers import identity, Identity\nfrom texar.torch.module_base import ModuleBase\nfrom texar.torch.modules.decoders import decoder_helpers as helpers\nfrom texar.torch.modules.decoders.decoder_helpers import Helper\nfrom texar.torch.utils import utils\nfrom texar.torch.utils.dtypes import torch_bool\n\n__all__ = [\n    \'_make_output_layer\',\n    \'DecoderBase\',\n]\n\nState = TypeVar(\'State\')\nOutput = TypeVar(\'Output\')  # output type can be of any nested structure\n\n\ndef _make_output_layer(layer: Optional[Union[nn.Module, torch.Tensor]],\n                       vocab_size: Optional[int],\n                       output_size: int,\n                       bias: bool) -> Tuple[nn.Module, Optional[int]]:\n    r""""""Construct the output layer for decoders. Based on the input, multiple\n    types of output layers could be constructed:\n\n    - If ``layer`` is a :torch_nn:`Module`, then the layer is returned as is.\n    - If ``layer`` is `None`, then a :torch_nn:`Linear` layer is constructed\n      with ``output_size`` and ``vocab_size`` as input and output dimensions.\n    - If ``layer`` is a :tensor:`Tensor`, then a :torch_nn:`Linear` layer is\n      constructed with the provided tensor as parameters. Note that this tensor\n      should have transposed shape, i.e. shape of ``[vocab_size, output_size]``.\n      Also, if the provided tensor is not an instance of :torch_nn:`Parameter`,\n      it will **not** accumulate gradients.\n    - If ``layer`` is :method:`texar.torch.core.identity`, identity function is\n      used as the output layer.\n    """"""\n    if isinstance(layer, nn.Module):\n        output_layer = layer\n    elif layer is None:\n        if vocab_size is None:\n            raise ValueError(\n                ""Either `output_layer` or `vocab_size` must be provided. ""\n                ""Set `output_layer=tx.core.identity` if no output ""\n                ""layer is wanted."")\n        output_layer = nn.Linear(output_size, vocab_size, bias)\n    elif torch.is_tensor(layer):\n        vocab_size = layer.size(0)\n        output_layer = nn.Linear(layer.size(1), vocab_size, bias)\n        if not isinstance(layer, nn.Parameter):\n            layer = nn.Parameter(layer, requires_grad=False)\n        output_layer.weight = layer\n    elif layer is identity:\n        output_layer = Identity()\n    else:\n        raise ValueError(\n            f""output_layer should be an instance of `nn.Module`, a tensor,""\n            f""or None. Unsupported type: {type(layer)}"")\n\n    return output_layer, vocab_size\n\n\nTokenEmbedder = Union[nn.Module, Callable[[torch.LongTensor], torch.Tensor]]\nTokenPosEmbedder = Union[\n    nn.Module, Callable[[torch.LongTensor, torch.LongTensor], torch.Tensor]]\n\n\nclass DecoderBase(ModuleBase, Generic[State, Output], ABC):\n    r""""""Base class inherited by all RNN decoder classes.\n    See :class:`~texar.torch.modules.BasicRNNDecoder` for the arguments.\n\n    See :meth:`forward` for the inputs and outputs of RNN decoders in general.\n    """"""\n\n    def __init__(self,\n                 token_embedder: Optional[TokenEmbedder] = None,\n                 token_pos_embedder: Optional[TokenPosEmbedder] = None,\n                 input_time_major: bool = False,\n                 output_time_major: bool = False,\n                 hparams=None):\n        super().__init__(hparams=hparams)\n\n        self._train_helper: Optional[Helper] = None\n        self._infer_helper: Optional[Helper] = None\n        self._input_time_major = input_time_major\n        self._output_time_major = output_time_major\n\n        if (token_embedder is not None and\n                token_pos_embedder is not None):\n            raise ValueError(""At most one among `token_embedder` and ""\n                             ""`token_pos_embedder` should be specified"")\n        if token_embedder is None and token_pos_embedder is None:\n            embed_token_func = self.embed_tokens.__func__  # type: ignore\n            if embed_token_func is DecoderBase.embed_tokens:\n                raise ValueError(\n                    ""Either `token_embedder` or `token_pos_embedder` must not ""\n                    ""be `None` if `DecoderBase.embed_tokens` is not ""\n                    ""overridden."")\n\n        self._token_embedder = token_embedder\n        self._token_pos_embedder = token_pos_embedder\n\n    def embed_tokens(self, tokens: torch.LongTensor,\n                     positions: torch.LongTensor) -> torch.Tensor:\n        r""""""Convert tokens along with positions to embeddings.\n\n        Args:\n            tokens: A :tensor:`LongTensor` denoting the token indices to convert\n                to embeddings.\n            positions: A :tensor:`LongTensor` with the same size as\n                :attr:`tokens`, denoting the positions of the tokens. This is\n                useful if the decoder uses positional embeddings.\n\n        Returns:\n            A :tensor:`Tensor` of size ``tokens.size() + (embed_dim,)``,\n            denoting the converted embeddings.\n        """"""\n        if self._token_embedder is not None:\n            return self._token_embedder(tokens)\n        assert self._token_pos_embedder is not None\n        return self._token_pos_embedder(tokens, positions)\n\n    def create_helper(self, *,\n                      decoding_strategy: Optional[str] = None,\n                      start_tokens: Optional[torch.LongTensor] = None,\n                      end_token: Optional[int] = None,\n                      softmax_temperature: Optional[float] = None,\n                      infer_mode: Optional[bool] = None,\n                      **kwargs) -> Helper:\n        r""""""Create a helper instance for the decoder. This is a shared interface\n        for both :class:`~texar.torch.modules.BasicRNNDecoder` and\n        :class:`~texar.torch.modules.AttentionRNNDecoder`.\n\n        The function provides **3 ways** to specify the\n        decoding method, with varying flexibility:\n\n        1. The :attr:`decoding_strategy` argument: A string taking value of:\n\n            - **""train_greedy""**: decoding in teacher-forcing fashion (i.e.,\n              feeding `ground truth` to decode the next step), and each sample\n              is obtained by taking the `argmax` of the output logits.\n              Arguments :attr:`(inputs, sequence_length)`\n              are required for this strategy, and argument :attr:`embedding`\n              is optional.\n            - **""infer_greedy""**: decoding in inference fashion (i.e., feeding\n              the `generated` sample to decode the next step), and each sample\n              is obtained by taking the `argmax` of the output logits.\n              Arguments :attr:`(embedding, start_tokens, end_token)` are\n              required for this strategy, and argument\n              :attr:`max_decoding_length` is optional.\n            - **""infer_sample""**: decoding in inference fashion, and each\n              sample is obtained by `random sampling` from the RNN output\n              distribution. Arguments\n              :attr:`(embedding, start_tokens, end_token)` are\n              required for this strategy, and argument\n              :attr:`max_decoding_length` is optional.\n\n          This argument is used only when argument :attr:`helper` is `None`.\n\n          Example:\n\n            .. code-block:: python\n\n                embedder = WordEmbedder(vocab_size=data.vocab.size)\n                decoder = BasicRNNDecoder(vocab_size=data.vocab.size)\n\n                # Teacher-forcing decoding\n                outputs_1, _, _ = decoder(\n                    decoding_strategy=\'train_greedy\',\n                    inputs=embedder(data_batch[\'text_ids\']),\n                    sequence_length=data_batch[\'length\'] - 1)\n\n                # Random sample decoding. Gets 100 sequence samples\n                outputs_2, _, sequence_length = decoder(\n                    decoding_strategy=\'infer_sample\',\n                    start_tokens=[data.vocab.bos_token_id] * 100,\n                    end_token=data.vocab.eos.token_id,\n                    embedding=embedder,\n                    max_decoding_length=60)\n\n        2. The :attr:`helper` argument: An instance of subclass of\n           :class:`~texar.torch.modules.decoders.decoder_helpers.Helper`. This\n           provides a superset of decoding strategies than above, for example:\n\n            - :class:`~texar.torch.modules.TrainingHelper` corresponding to the\n              ""train_greedy"" strategy.\n            - :class:`~texar.torch.modules.ScheduledEmbeddingTrainingHelper` and\n              :class:`~texar.torch.modules.ScheduledOutputTrainingHelper` for\n              scheduled sampling.\n            - :class:`~texar.torch.modules.SoftmaxEmbeddingHelper` and\n              :class:`~texar.torch.modules.GumbelSoftmaxEmbeddingHelper` for\n              soft decoding and gradient backpropagation.\n\n          This means gives the maximal flexibility of configuring the decoding\n          strategy.\n\n          Example:\n\n            .. code-block:: python\n\n                embedder = WordEmbedder(vocab_size=data.vocab.size)\n                decoder = BasicRNNDecoder(vocab_size=data.vocab.size)\n\n                # Teacher-forcing decoding, same as above with\n                # `decoding_strategy=\'train_greedy\'`\n                helper_1 = TrainingHelper(\n                    inputs=embedders(data_batch[\'text_ids\']),\n                    sequence_length=data_batch[\'length\'] - 1)\n                outputs_1, _, _ = decoder(helper=helper_1)\n\n                # Gumbel-softmax decoding\n                helper_2 = GumbelSoftmaxEmbeddingHelper(\n                    embedding=embedder,\n                    start_tokens=[data.vocab.bos_token_id] * 100,\n                    end_token=data.vocab.eos_token_id,\n                    tau=0.1)\n                outputs_2, _, sequence_length = decoder(\n                    max_decoding_length=60, helper=helper_2)\n\n        3. ``hparams[""helper_train""]`` and ``hparams[""helper_infer""]``:\n           Specifying the helper through hyperparameters. Train and infer\n           strategy is toggled based on :attr:`mode`. Appropriate arguments\n           (e.g., :attr:`inputs`, :attr:`start_tokens`, etc) are selected to\n           construct the helper. Additional arguments for helper constructor\n           can be provided either through :attr:`**kwargs`, or through\n           ``hparams[""helper_train/infer""][""kwargs""]``.\n\n           This means is used only when both :attr:`decoding_strategy` and\n           :attr:`helper` are `None`.\n\n           Example:\n\n             .. code-block:: python\n\n                h = {\n                    ""helper_infer"": {\n                        ""type"": ""GumbelSoftmaxEmbeddingHelper"",\n                        ""kwargs"": { ""tau"": 0.1 }\n                    }\n                }\n                embedder = WordEmbedder(vocab_size=data.vocab.size)\n                decoder = BasicRNNDecoder(vocab_size=data.vocab.size, hparams=h)\n\n                # Gumbel-softmax decoding\n                decoder.eval()  # disable dropout\n                output, _, _ = decoder(\n                    decoding_strategy=None, # Sets to None explicit\n                    embedding=embedder,\n                    start_tokens=[data.vocab.bos_token_id] * 100,\n                    end_token=data.vocab.eos_token_id,\n                    max_decoding_length=60)\n\n        Args:\n            decoding_strategy (str): A string specifying the decoding\n                strategy. Different arguments are required based on the\n                strategy.\n                Ignored if :attr:`helper` is given.\n            start_tokens (optional): A :tensor:`LongTensor` of shape\n                ``[batch_size]``, the start tokens.\n                Used when :attr:`decoding_strategy` is ``""infer_greedy""`` or\n                ``""infer_sample""``, or when `hparams`-configured\n                helper is used.\n                When used with the Texar data module, to get ``batch_size``\n                samples where ``batch_size`` is changing according to the data\n                module, this can be set as\n                ``start_tokens=torch.full_like(batch[\'length\'], bos_token_id)``.\n            end_token (optional): A integer or 0D :tensor:`LongTensor`, the\n                token that marks the end of decoding.\n                Used when :attr:`decoding_strategy` is ``""infer_greedy""`` or\n                ``""infer_sample""``, or when `hparams`-configured helper is used.\n            softmax_temperature (float, optional): Value to divide the logits\n                by before computing the softmax. Larger values (above 1.0)\n                result in more random samples. Must be > 0. If `None`, 1.0 is\n                used. Used when ``decoding_strategy=""infer_sample""``.\n            infer_mode (optional): If not `None`, overrides mode given by\n                :attr:`self.training`.\n            **kwargs: Other keyword arguments for constructing helpers\n                defined by ``hparams[""helper_train""]`` or\n                ``hparams[""helper_infer""]``.\n\n        Returns:\n            The constructed helper instance.\n        """"""\n        if decoding_strategy is not None:\n            if decoding_strategy == \'train_greedy\':\n                helper: Helper = helpers.TrainingHelper(\n                    self._input_time_major)\n            elif decoding_strategy in [\'infer_greedy\', \'infer_sample\']:\n                if start_tokens is None or end_token is None:\n                    raise ValueError(\n                        f""When using \'{decoding_strategy}\' decoding strategy, ""\n                        f""\'embedding\', \'start_tokens\', and \'end_token\' must ""\n                        f""not be `None`."")\n                if decoding_strategy == \'infer_greedy\':\n                    helper = helpers.GreedyEmbeddingHelper(\n                        start_tokens, end_token)\n                else:\n                    helper = helpers.SampleEmbeddingHelper(\n                        start_tokens, end_token, softmax_temperature)\n            else:\n                raise ValueError(\n                    f""Unknown decoding strategy: {decoding_strategy}"")\n        else:\n            is_training = (not infer_mode if infer_mode is not None\n                           else self.training)\n            if is_training:\n                kwargs_ = copy.copy(self._hparams.helper_train.kwargs.todict())\n                helper_type = self._hparams.helper_train.type\n            else:\n                kwargs_ = copy.copy(self._hparams.helper_infer.kwargs.todict())\n                helper_type = self._hparams.helper_infer.type\n            kwargs_.update({\n                \'time_major\': self._input_time_major,\n                \'start_tokens\': start_tokens,\n                \'end_token\': end_token,\n                \'softmax_temperature\': softmax_temperature})\n            kwargs_.update(kwargs)\n            helper = helpers.get_helper(helper_type, **kwargs_)\n        return helper\n\n    def _create_or_get_helper(self, infer_mode: Optional[bool] = None,\n                              **kwargs) -> Helper:\n        # Prefer creating a new helper when at least one kwarg is specified.\n        prefer_new = (len(kwargs) > 0)\n        kwargs.update(infer_mode=infer_mode)\n        is_training = (not infer_mode if infer_mode is not None\n                       else self.training)\n        helper = self._train_helper if is_training else self._infer_helper\n        if prefer_new or helper is None:\n            helper = self.create_helper(**kwargs)\n            if is_training and self._train_helper is None:\n                self._train_helper = helper\n            elif not is_training and self._infer_helper is None:\n                self._infer_helper = helper\n        return helper\n\n    def set_default_train_helper(self, helper: Helper):\n        r""""""Set the default helper used in training mode.\n\n        Args:\n            helper: The helper to set as default training helper.\n        """"""\n        self._train_helper = helper\n\n    def set_default_infer_helper(self, helper: Helper):\n        r""""""Set the default helper used in eval (inference) mode.\n\n        Args:\n            helper: The helper to set as default inference helper.\n        """"""\n        self._infer_helper = helper\n\n    def dynamic_decode(self, helper: Helper, inputs: Optional[torch.Tensor],\n                       sequence_length: Optional[torch.LongTensor],\n                       initial_state: Optional[State],\n                       max_decoding_length: Optional[int] = None,\n                       impute_finished: bool = False,\n                       step_hook: Optional[Callable[[int], None]] = None) \\\n            -> Tuple[Output, Optional[State], torch.LongTensor]:\n        r""""""Generic routine for dynamic decoding. Please check the\n        `documentation\n        <https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode>`_\n        for the TensorFlow counterpart.\n\n        Returns:\n            A tuple of output, final state, and sequence lengths. Note that\n            final state could be `None`, when all sequences are of zero length\n            and :attr:`initial_state` is also `None`.\n        """"""\n\n        # Decode\n        finished, step_inputs, state = self.initialize(\n            helper, inputs, sequence_length, initial_state)\n\n        zero_outputs = step_inputs.new_zeros(\n            step_inputs.size(0), self.output_size)\n\n        if max_decoding_length is not None:\n            finished |= (max_decoding_length <= 0)\n        sequence_lengths = torch.zeros_like(\n            finished, dtype=torch.long, device=finished.device)\n        time = 0\n\n        outputs = []\n\n        while (not torch.all(finished).item() and\n               (max_decoding_length is None or time < max_decoding_length)):\n\n            next_outputs, decoder_state = \\\n                self.step(helper, time, step_inputs, state)\n\n            if max_decoding_length is not None and \\\n                    time + 1 == max_decoding_length:\n                # Maximum decoding length reached, mark all batches as finished.\n                # This requires special handling because performing lookup on\n                # position embeddings with `time + 1` may result in IndexError.\n                decoder_finished = torch.tensor(1, dtype=torch_bool,\n                                                device=finished.device)\n                # Since `next_inputs` will not be used, simply create a null\n                # tensor.\n                next_inputs = torch.empty(0)\n            else:\n                next_inputs, decoder_finished = self.next_inputs(\n                    helper, time, next_outputs)\n\n            if getattr(self, \'tracks_own_finished\', False):\n                next_finished = decoder_finished\n            else:\n                next_finished = decoder_finished | finished\n\n            # Zero out output values past finish\n            if impute_finished:\n                emit = utils.map_structure_zip(\n                    lambda new, cur: torch.where(finished, cur, new),\n                    (next_outputs, zero_outputs))\n                next_state = utils.map_structure_zip(\n                    lambda new, cur: torch.where(finished, cur, new),\n                    (decoder_state, state))\n            else:\n                emit = next_outputs\n                next_state = decoder_state\n\n            outputs.append(emit)\n            sequence_lengths.index_fill_(\n                dim=0, value=time + 1,\n                index=torch.nonzero((~finished).long()).flatten())\n            time += 1\n            finished = next_finished\n            step_inputs = next_inputs\n            state = next_state\n\n            if step_hook is not None:\n                step_hook(time)\n\n        final_outputs = utils.map_structure_zip(\n            lambda *tensors: torch.stack(tensors),\n            outputs)  # output at each time step may be a namedtuple\n        final_state = state\n        final_sequence_lengths = sequence_lengths\n\n        try:\n            final_outputs, final_state = self.finalize(\n                final_outputs, final_state, final_sequence_lengths)\n        except NotImplementedError:\n            pass\n\n        if not self._output_time_major:\n            final_outputs = utils.map_structure(\n                lambda x: x.transpose(0, 1) if x.dim() >= 2 else x,\n                final_outputs)\n\n        return final_outputs, final_state, final_sequence_lengths\n\n    @abstractmethod\n    def initialize(self, helper: Helper, inputs: Optional[torch.Tensor],\n                   sequence_length: Optional[torch.LongTensor],\n                   initial_state: Optional[State]) \\\n            -> Tuple[torch.ByteTensor, torch.Tensor, Optional[State]]:\n        r""""""Called before any decoding iterations.\n\n        This methods must compute initial input values and initial state.\n\n        Args:\n            helper: The :class:`~texar.torch.modules.Helper` instance to use.\n            inputs (optional): A (structure of) input tensors.\n            sequence_length (optional): A :tensor:`LongTensor` representing\n                lengths of each sequence.\n            initial_state: A possibly nested structure of tensors indicating the\n                initial decoder state.\n\n        Returns:\n            A tuple ``(finished, initial_inputs, initial_state)`` representing\n            initial values of ``finished`` flags, inputs, and state.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def step(self, helper: Helper, time: int,\n             inputs: torch.Tensor, state: Optional[State]) \\\n            -> Tuple[Output, State]:\n        r""""""Compute the output and the state at the current time step.\n        Called per step of decoding (but only once for dynamic decoding).\n\n        Args:\n            helper: The :class:`~texar.torch.modules.Helper` instance to use.\n            time (int): Current step number.\n            inputs: Inputs for this time step.\n            state: Decoder state from the previous time step.\n\n        Returns:\n            A tuple ``(outputs, next_state)``.\n\n            - ``outputs`` is an object containing the decoder output.\n            - ``next_state`` is the decoder state for the next time step.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def next_inputs(self, helper: Helper, time: int, outputs: Output) -> \\\n            Tuple[torch.Tensor, torch.ByteTensor]:\n        r""""""Compute the input for the next time step.\n        Called per step of decoding (but only once for dynamic decoding).\n\n        Args:\n            helper: The :class:`~texar.torch.modules.Helper` instance to use.\n            time (int): Current step number.\n            outputs: An object containing the decoder output.\n\n        Returns:\n            A tuple ``(next_inputs, finished)``.\n\n            - ``next_inputs`` is the tensor that should be used as input for the\n              next step.\n            - ``finished`` is a :torch:`ByteTensor` tensor telling whether the\n              sequence is complete, for each sequence in the batch.\n        """"""\n        raise NotImplementedError\n\n    # TODO: Remove these once pylint supports function stubs.\n    # pylint: disable=missing-docstring,unused-argument,no-self-use\n    # pylint: disable=function-redefined\n\n    @overload\n    def finalize(self, outputs: Output, final_state: State,\n                 sequence_lengths: torch.LongTensor) -> Tuple[Output, State]:\n        ...\n\n    @overload\n    def finalize(self, outputs: Output, final_state: Optional[State],\n                 sequence_lengths: torch.LongTensor) \\\n            -> Tuple[Output, Optional[State]]:\n        ...\n\n    def finalize(self, outputs, final_state, sequence_lengths):\n        r""""""Called after all decoding iterations have finished.\n\n        Args:\n            outputs: Outputs at each time step.\n            final_state: The RNNCell state after the last time step.\n            sequence_lengths: Sequence lengths for each sequence in batch.\n\n        Returns:\n            A tuple ``(outputs, final_state)``.\n\n            - ``outputs`` is an object containing the decoder output.\n            - ``final_state`` is the final decoder state.\n        """"""\n        return outputs, final_state\n\n    # pylint: enable=missing-docstring,unused-argument,no-self-use\n    # pylint: enable=function-redefined\n\n    @property\n    def vocab_size(self):\n        r""""""The vocabulary size.\n        """"""\n        return self._vocab_size\n\n    @property\n    def output_layer(self):\n        r""""""The output layer.\n        """"""\n        return self._output_layer\n'"
texar/torch/modules/decoders/decoder_helpers.py,98,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious helper classes and utilities for RNN decoders.\n""""""\nfrom abc import ABC\nfrom typing import Callable, Generic, Optional, Tuple, Type, TypeVar, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical, Gumbel\n\nfrom texar.torch.utils import utils\nfrom texar.torch.utils.dtypes import torch_bool\n\n__all__ = [\n    \'Helper\',\n    \'TrainingHelper\',\n    \'EmbeddingHelper\',\n    \'GreedyEmbeddingHelper\',\n    \'SampleEmbeddingHelper\',\n    \'TopKSampleEmbeddingHelper\',\n    \'TopPSampleEmbeddingHelper\',\n    \'SoftmaxEmbeddingHelper\',\n    \'GumbelSoftmaxEmbeddingHelper\',\n    \'default_helper_train_hparams\',\n    \'default_helper_infer_hparams\',\n    \'get_helper\',\n]\n# TODO: Implement `ScheduledEmbeddingTrainingHelper` and\n#     `ScheduledOutputTrainingHelper`\n\nHelperInitTuple = Tuple[torch.ByteTensor, torch.Tensor]\nNextInputTuple = Tuple[torch.ByteTensor, torch.Tensor]\n\n# indices, position -> embeddings\nEmbeddingFn = Callable[[torch.LongTensor, torch.LongTensor], torch.Tensor]\n\nIDType = TypeVar(\'IDType\', bound=torch.Tensor)\n\n\n# Helper instances are used by :class:`texar.torch.modules.DecoderBase`.\nclass Helper(Generic[IDType], ABC):\n    r""""""Interface for implementing sampling in seq2seq decoders.\n\n    Please refer to the documentation for the TensorFlow counterpart\n    `tf.contrib.seq2seq.Helper\n    <https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/Helper>`_.\n    """"""\n\n    def initialize(self, embedding_fn: EmbeddingFn,\n                   inputs: Optional[torch.Tensor],\n                   sequence_length: Optional[torch.LongTensor]) \\\n            -> HelperInitTuple:\n        r""""""Initialize the current batch.\n\n        Args:\n            embedding_fn: A function taking input tokens and timestamps,\n                returning embedding tensors.\n            inputs: Input tensors.\n            sequence_length: An int32 vector tensor.\n\n        Returns:\n            ``(initial_finished, initial_inputs)``.\n        """"""\n        raise NotImplementedError\n\n    def sample(self, time: int, outputs: torch.Tensor) -> IDType:\n        r""""""Returns ``sample_ids``.\n        """"""\n        raise NotImplementedError\n\n    def next_inputs(self, embedding_fn: EmbeddingFn,\n                    time: int, outputs: torch.Tensor,\n                    sample_ids: IDType) -> NextInputTuple:\n        r""""""Returns ``(finished, next_inputs, next_state)``.\n        """"""\n        raise NotImplementedError\n\n\nclass TrainingHelper(Helper[torch.LongTensor]):\n    r""""""A helper for use during training. Only reads inputs.\n\n    Returned ``sample_ids`` are the argmax of the RNN output logits.\n\n    Please refer to the documentation for the TensorFlow counterpart\n    `tf.contrib.seq2seq.TrainingHelper\n    <https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper>`_.\n\n    Args:\n        time_major (bool):  Whether the tensors in ``inputs`` are time major.\n            If `False` (default), they are assumed to be batch major.\n    """"""\n    # the following are set in `initialize`\n    _inputs: torch.Tensor\n    _zero_inputs: torch.Tensor\n    _sequence_length: torch.LongTensor\n\n    def __init__(self, time_major: bool = False):\n        self._time_major = time_major\n\n    def initialize(self, embedding_fn: EmbeddingFn,\n                   inputs: Optional[torch.Tensor],\n                   sequence_length: Optional[torch.LongTensor]) \\\n            -> HelperInitTuple:\n        if inputs is None:\n            raise ValueError(""`inputs` cannot be None for TrainingHelper"")\n        if sequence_length is None:\n            raise ValueError(\n                ""`sequence_length` cannot be None for TrainingHelper"")\n        inputs: torch.Tensor\n        sequence_length: torch.LongTensor\n\n        if sequence_length.dim() != 1:\n            raise ValueError(\n                f""Expected \'sequence_length\' to be a vector, ""\n                f""but received shape: {sequence_length.shape}"")\n\n        if not self._time_major:\n            inputs = inputs.transpose(0, 1)  # make inputs time major\n        times = torch.arange(\n            sequence_length.max(), dtype=torch.long, device=inputs.device)\n        times = times.unsqueeze(1).expand(-1, inputs.size(1))\n        inputs = embedding_fn(inputs, times)\n\n        self._inputs = inputs\n        self._sequence_length = sequence_length\n        self._zero_inputs = inputs.new_zeros(inputs[0].size())\n\n        finished: torch.ByteTensor = (sequence_length == 0)\n        all_finished = torch.all(finished).item()\n        next_inputs = inputs[0] if not all_finished else self._zero_inputs\n        return (finished, next_inputs)\n\n    def sample(self, time: int, outputs: torch.Tensor) -> torch.LongTensor:\n        del time\n        sample_ids = torch.argmax(outputs, dim=-1)\n        return sample_ids\n\n    def next_inputs(self, embedding_fn: EmbeddingFn,\n                    time: int, outputs: torch.Tensor,\n                    sample_ids: torch.LongTensor) -> NextInputTuple:\n        del embedding_fn, outputs, sample_ids\n        next_time = time + 1\n        finished = (next_time >= self._sequence_length)\n        all_finished = torch.all(finished).item()\n\n        next_inputs = (self._inputs[next_time] if not all_finished\n                       else self._zero_inputs)\n        return (finished, next_inputs)\n\n\nclass EmbeddingHelper(Helper[IDType], ABC):\n    r""""""A generic helper for use during inference.\n\n    Uses output logits for sampling, and passes the result through an embedding\n    layer to get the next input.\n\n    Args:\n        start_tokens: 1D :tensor:`LongTensor` shaped ``[batch_size]``,\n            representing the start tokens for each sequence in batch.\n        end_token: Python int or scalar :tensor:`LongTensor`, denoting the\n            token that marks end of decoding.\n\n    Raises:\n        ValueError: if :attr:`start_tokens` is not a 1D tensor or\n            :attr:`end_token` is not a scalar.\n    """"""\n\n    _start_inputs: torch.Tensor  # set in `initialize`\n\n    def __init__(self, start_tokens: torch.LongTensor,\n                 end_token: Union[int, torch.LongTensor]):\n        if start_tokens.dim() != 1:\n            raise ValueError(""start_tokens must be a vector"")\n        if not isinstance(end_token, int) and end_token.dim() != 0:\n            raise ValueError(""end_token must be a scalar"")\n\n        self._start_tokens = start_tokens\n        self._batch_size = start_tokens.size(0)\n        if isinstance(end_token, int):\n            self._end_token = start_tokens.new_tensor(end_token)\n        else:\n            self._end_token = end_token\n\n    @property\n    def batch_size(self) -> int:\n        return self._batch_size\n\n    def initialize(self, embedding_fn: EmbeddingFn,\n                   inputs: Optional[torch.Tensor],\n                   sequence_length: Optional[torch.LongTensor]) \\\n            -> HelperInitTuple:\n        del inputs, sequence_length\n        times = torch.zeros_like(self._start_tokens)\n        self._start_inputs = embedding_fn(self._start_tokens, times)\n        finished = torch.zeros_like(self._start_tokens, dtype=torch_bool)\n        return (finished, self._start_inputs)\n\n\nclass SingleEmbeddingHelper(EmbeddingHelper[torch.LongTensor], ABC):\n    r""""""A generic helper for use during inference.\n\n    Based on :class:`~texar.torch.modules.EmbeddingHelper`, this class returns\n    samples that are vocabulary indices, and use the corresponding embeddings as\n    the next input. This class also supports callables as embeddings.\n\n    Args:\n        start_tokens: 1D :tensor:`LongTensor` shaped ``[batch_size]``,\n            representing the start tokens for each sequence in batch.\n        end_token: Python int or scalar :tensor:`LongTensor`, denoting the\n            token that marks end of decoding.\n\n    Raises:\n        ValueError: if :attr:`start_tokens` is not a 1D tensor or\n            :attr:`end_token` is not a scalar.\n    """"""\n\n    def next_inputs(self, embedding_fn: EmbeddingFn,\n                    time: int, outputs: torch.Tensor,\n                    sample_ids: torch.LongTensor) -> NextInputTuple:\n        del outputs  # unused by next_inputs_fn\n        finished = (sample_ids == self._end_token)\n        all_finished = torch.all(finished).item()\n\n        times = torch.full_like(sample_ids, time + 1)\n        embeddings = embedding_fn(sample_ids, times)\n\n        next_inputs = (embeddings if not all_finished else self._start_inputs)\n        return (finished, next_inputs)\n\n\nclass GreedyEmbeddingHelper(SingleEmbeddingHelper):\n    r""""""A helper for use during inference.\n\n    Uses the argmax of the output (treated as logits) and passes the\n    result through an embedding layer to get the next input.\n\n    Note that for greedy decoding, Texar\'s decoders provide a simpler\n    interface by specifying ``decoding_strategy=\'infer_greedy\'`` when calling a\n    decoder (see, e.g.,,\n    :meth:`RNN decoder <texar.torch.modules.RNNDecoderBase.forward>`). In this\n    case, use of :class:`GreedyEmbeddingHelper` is not necessary.\n\n    Please refer to the documentation for the TensorFlow counterpart\n    `tf.contrib.seq2seq.GreedyEmbeddingHelper\n    <https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper>`_.\n\n    Args:\n        start_tokens: 1D :tensor:`LongTensor` shaped ``[batch_size]``,\n            representing the start tokens for each sequence in batch.\n        end_token: Python int or scalar :tensor:`LongTensor`, denoting the\n            token that marks end of decoding.\n\n    Raises:\n        ValueError: if :attr:`start_tokens` is not a 1D tensor or\n            :attr:`end_token` is not a scalar.\n    """"""\n\n    def sample(self, time: int, outputs: torch.Tensor) -> torch.LongTensor:\n        del time  # unused by sample_fn\n        # Outputs are logits, use argmax to get the most probable id\n        if not torch.is_tensor(outputs):\n            raise TypeError(\n                f""Expected outputs to be a single Tensor, got: {type(outputs)}"")\n        sample_ids = torch.argmax(outputs, dim=-1)\n        return sample_ids\n\n\nclass SampleEmbeddingHelper(SingleEmbeddingHelper):\n    r""""""A helper for use during inference.\n\n    Uses sampling (from a distribution) instead of argmax and passes the\n    result through an embedding layer to get the next input.\n\n    Please refer to the documentation for the TensorFlow counterpart\n    `tf.contrib.seq2seq.SampleEmbeddingHelper\n    <https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/SampleEmbeddingHelper>`_.\n\n    Args:\n        embedding: A callable or the ``params`` argument for\n            :torch_nn:`functional.embedding`.\n            If a callable, it can take a vector tensor of ``ids`` (argmax\n            ids), or take two arguments (``ids``, ``times``), where ``ids``\n            is a vector of argmax ids, and ``times`` is a vector of current\n            time steps (i.e., position ids). The latter case can be used\n            when :attr:`embedding` is a combination of word embedding and\n            position embedding.\n            The returned tensor will be passed to the decoder input.\n        start_tokens: 1D :tensor:`LongTensor` shaped ``[batch_size]``,\n            representing the start tokens for each sequence in batch.\n        end_token: Python int or scalar :tensor:`LongTensor`, denoting the\n            token that marks end of decoding.\n        softmax_temperature (float, optional): Value to divide the logits by\n            before computing the softmax. Larger values (above 1.0) result\n            in more random samples, while smaller values push the sampling\n            distribution towards the argmax. Must be strictly greater than\n            0. Defaults to 1.0.\n\n    Raises:\n        ValueError: if :attr:`start_tokens` is not a 1D tensor or\n            :attr:`end_token` is not a scalar.\n    """"""\n\n    def __init__(self, start_tokens: torch.LongTensor,\n                 end_token: Union[int, torch.LongTensor],\n                 softmax_temperature: Optional[float] = None):\n        super().__init__(start_tokens, end_token)\n        self._softmax_temperature = softmax_temperature\n\n    def sample(self, time: int, outputs: torch.Tensor) -> torch.LongTensor:\n        del time  # unused by sample_fn\n        # Outputs are logits, we sample instead of argmax (greedy).\n        if not torch.is_tensor(outputs):\n            raise TypeError(\n                f""Expected outputs to be a single Tensor, got: {type(outputs)}"")\n        if self._softmax_temperature is None:\n            logits = outputs\n        else:\n            logits = outputs / self._softmax_temperature\n\n        sample_id_sampler = Categorical(logits=logits)\n        sample_ids = sample_id_sampler.sample()\n\n        return sample_ids\n\n\ndef _top_k_logits(logits: torch.Tensor, k: int) -> torch.Tensor:\n    r""""""Adapted from\n    https://github.com/openai/gpt-2/blob/master/src/sample.py#L63-L77\n    """"""\n    if k == 0:\n        # no truncation\n        return logits\n\n    values, _ = torch.topk(logits, k=k)\n    min_values: torch.Tensor = values[:, -1].unsqueeze(-1)\n    return torch.where(\n        logits < min_values,\n        torch.full_like(logits, float(\'-inf\')), logits)\n\n\ndef _top_p_logits(logits: torch.Tensor, p: float) -> torch.Tensor:\n    r""""""Adapted from\n    https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317#file-top-k-top-p-py-L16-L27""""""\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n    # Remove tokens with cumulative probability above the threshold\n    sorted_indices_to_remove = cumulative_probs > p\n    # Shift the indices to the right to keep also the first token above the\n    # threshold\n    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n    sorted_indices_to_remove[:, 0] = 0\n\n    for idx in range(logits.size(0)):\n        batch_indices = sorted_indices[idx, sorted_indices_to_remove[idx]]\n        logits[idx, batch_indices] = float(""-inf"")\n    return logits\n\n\nclass TopKSampleEmbeddingHelper(SingleEmbeddingHelper):\n    r""""""A helper for use during inference.\n\n    Samples from ``top_k`` most likely candidates from a vocab distribution,\n    and passes the result through an embedding layer to get the next input.\n\n    Args:\n        start_tokens: 1D :tensor:`LongTensor` shaped ``[batch_size]``,\n            representing the start tokens for each sequence in batch.\n        end_token: Python int or scalar :tensor:`LongTensor`, denoting the\n            token that marks end of decoding.\n        top_k (int, optional): Number of top candidates to sample from. Must\n            be `>=0`. If set to 0, samples from all candidates (i.e.,\n            regular random sample decoding). Defaults to 10.\n        softmax_temperature (float, optional): Value to divide the logits by\n            before computing the softmax. Larger values (above 1.0) result\n            in more random samples, while smaller values push the sampling\n            distribution towards the argmax. Must be strictly greater than\n            0. Defaults to 1.0.\n\n    Raises:\n        ValueError: if :attr:`start_tokens` is not a 1D tensor or\n            :attr:`end_token` is not a scalar.\n    """"""\n\n    def __init__(self, start_tokens: torch.LongTensor,\n                 end_token: Union[int, torch.LongTensor], top_k: int = 10,\n                 softmax_temperature: Optional[float] = None):\n        super().__init__(start_tokens, end_token)\n        self._top_k = top_k\n        self._softmax_temperature = softmax_temperature\n\n    def sample(self, time: int, outputs: torch.Tensor) -> torch.LongTensor:\n        del time  # unused by sample_fn\n        # Outputs are logits, we sample from the top-k candidates\n        if not torch.is_tensor(outputs):\n            raise TypeError(\n                f""Expected outputs to be a single Tensor, got: {type(outputs)}"")\n        if self._softmax_temperature is None:\n            logits = outputs\n        else:\n            logits = outputs / self._softmax_temperature\n\n        logits = _top_k_logits(logits, k=self._top_k)\n\n        sample_id_sampler = Categorical(logits=logits)\n        sample_ids = sample_id_sampler.sample()\n\n        return sample_ids\n\n\nclass TopPSampleEmbeddingHelper(SingleEmbeddingHelper):\n    r""""""A helper for use during inference.\n\n    Samples from candidates that have a cumulative probability of at most `p`\n    when arranged in decreasing order, and passes the result through an\n    embedding layer to get the next input. This is also named as\n    ""*Nucleus Sampling*"" as proposed in the paper\n    ""*The Curious Case of Neural Text Degeneration(Holtzman et al.)*"".\n\n    Args:\n        start_tokens: 1D :tensor:`LongTensor` shaped ``[batch_size]``,\n            representing the start tokens for each sequence in batch.\n        end_token: Python int or scalar :tensor:`LongTensor`, denoting the\n            token that marks end of decoding.\n        p (float, optional): A value used to filter out tokens whose cumulative\n            probability is greater than `p` when arranged in decreasing order of\n            probabilities. Must be between [0, 1.0]. If set to 1, samples from\n            all candidates (i.e., regular random sample decoding). Defaults to\n            0.5.\n        softmax_temperature (float, optional): Value to divide the logits by\n            before computing the softmax. Larger values (above 1.0) result\n            in more random samples, while smaller values push the sampling\n            distribution towards the argmax. Must be strictly greater than\n            0. Defaults to 1.0.\n\n    Raises:\n        ValueError: if :attr:`start_tokens` is not a 1D tensor or\n            :attr:`end_token` is not a scalar.\n    """"""\n\n    def __init__(self, start_tokens: torch.LongTensor,\n                 end_token: Union[int, torch.LongTensor], p: float = 0.9,\n                 softmax_temperature: Optional[float] = None):\n        super().__init__(start_tokens, end_token)\n        self._p = p\n        self._softmax_temperature = softmax_temperature\n\n    def sample(self, time: int, outputs: torch.Tensor) -> torch.LongTensor:\n        del time  # unused by sample_fn\n        # Outputs are logits, we sample from tokens with cumulative\n        # probability at most p when arranged in decreasing order\n        if not torch.is_tensor(outputs):\n            raise TypeError(\n                f""Expected outputs to be a single Tensor, got: {type(outputs)}"")\n        if self._softmax_temperature is None:\n            logits = outputs\n        else:\n            logits = outputs / self._softmax_temperature\n\n        logits = _top_p_logits(logits, p=self._p)\n\n        sample_id_sampler = Categorical(logits=logits)\n        sample_ids = sample_id_sampler.sample()\n\n        return sample_ids\n\n\nclass SoftmaxEmbeddingHelper(EmbeddingHelper[torch.Tensor]):\n    r""""""A helper that feeds softmax probabilities over vocabulary\n    to the next step.\n\n    Uses the softmax probability vector to pass through word embeddings to\n    get the next input (i.e., a mixed word embedding).\n\n    A subclass of :class:`~texar.torch.modules.Helper`. Used as a helper to\n    :class:`~texar.torch.modules.RNNDecoderBase` in inference mode.\n\n    Args:\n        embedding: A callable or the ``params`` argument for\n            :torch_nn:`functional.embedding`.\n            If a callable, it can take a vector tensor of ``ids`` (argmax\n            ids), or take two arguments (``ids``, ``times``), where ``ids``\n            is a vector of argmax ids, and ``times`` is a vector of current\n            time steps (i.e., position ids). The latter case can be used\n            when :attr:`embedding` is a combination of word embedding and\n            position embedding.\n            The returned tensor will be passed to the decoder input.\n        start_tokens: 1D :tensor:`LongTensor` shaped ``[batch_size]``,\n            representing the start tokens for each sequence in batch.\n        end_token: Python int or scalar :tensor:`LongTensor`, denoting the\n            token that marks end of decoding.\n        tau: A float scalar tensor, the softmax temperature.\n        stop_gradient (bool): Whether to stop the gradient backpropagation\n            when feeding softmax vector to the next step.\n        use_finish (bool): Whether to stop decoding once :attr:`end_token`\n            is generated. If `False`, decoding will continue until\n            :attr:`max_decoding_length` of the decoder is reached.\n\n    Raises:\n        ValueError: if :attr:`start_tokens` is not a 1D tensor or\n            :attr:`end_token` is not a scalar.\n    """"""\n\n    def __init__(self, start_tokens: torch.LongTensor,\n                 end_token: Union[int, torch.LongTensor], tau: float,\n                 stop_gradient: bool = False, use_finish: bool = True):\n        super().__init__(start_tokens, end_token)\n\n        self._tau = tau\n        self._stop_gradient = stop_gradient\n        self._use_finish = use_finish\n\n    def sample(self, time: int, outputs: torch.Tensor) -> torch.Tensor:\n        r""""""Returns ``sample_id`` which is softmax distributions over vocabulary\n        with temperature :attr:`tau`. Shape = ``[batch_size, vocab_size]``.\n        """"""\n        del time\n        sample_ids = torch.softmax(outputs / self._tau, dim=-1)\n        return sample_ids\n\n    def next_inputs(self, embedding_fn: EmbeddingFn,\n                    time: int, outputs: torch.Tensor,\n                    sample_ids: torch.LongTensor) -> NextInputTuple:\n        del outputs  # unused by next_inputs_fn\n        if self._use_finish:\n            hard_ids = torch.argmax(sample_ids, dim=-1)\n            finished = (hard_ids == self._end_token)\n        else:\n            finished = torch.zeros_like(self._start_tokens, dtype=torch_bool)\n        if self._stop_gradient:\n            sample_ids = sample_ids.detach()\n\n        indices = torch.arange(sample_ids.size(-1), device=sample_ids.device)\n        times = torch.full_like(indices, time + 1)\n        embeddings = embedding_fn(indices, times)\n\n        next_inputs = torch.matmul(sample_ids, embeddings)\n        return (finished, next_inputs)\n\n\nclass GumbelSoftmaxEmbeddingHelper(SoftmaxEmbeddingHelper):\n    r""""""A helper that feeds Gumbel softmax sample to the next step.\n\n    Uses the Gumbel softmax vector to pass through word embeddings to\n    get the next input (i.e., a mixed word embedding).\n\n    A subclass of :class:`~texar.torch.modules.Helper`. Used as a helper to\n    :class:`~texar.torch.modules.RNNDecoderBase` in inference mode.\n\n    Same as :class:`~texar.torch.modules.SoftmaxEmbeddingHelper` except that\n    here Gumbel softmax (instead of softmax) is used.\n\n    Args:\n        embedding: A callable or the ``params`` argument for\n            :torch_nn:`functional.embedding`.\n            If a callable, it can take a vector tensor of ``ids`` (argmax\n            ids), or take two arguments (``ids``, ``times``), where ``ids``\n            is a vector of argmax ids, and ``times`` is a vector of current\n            time steps (i.e., position ids). The latter case can be used\n            when :attr:`embedding` is a combination of word embedding and\n            position embedding.\n            The returned tensor will be passed to the decoder input.\n        start_tokens: 1D :tensor:`LongTensor` shaped ``[batch_size]``,\n            representing the start tokens for each sequence in batch.\n        end_token: Python int or scalar :tensor:`LongTensor`, denoting the\n            token that marks end of decoding.\n        tau: A float scalar tensor, the softmax temperature.\n        straight_through (bool): Whether to use straight through gradient\n            between time steps. If `True`, a single token with highest\n            probability (i.e., greedy sample) is fed to the next step and\n            gradient is computed using straight through. If `False`\n            (default), the soft Gumbel-softmax distribution is fed to the\n            next step.\n        stop_gradient (bool): Whether to stop the gradient backpropagation\n            when feeding softmax vector to the next step.\n        use_finish (bool): Whether to stop decoding once :attr:`end_token`\n            is generated. If `False`, decoding will continue until\n            :attr:`max_decoding_length` of the decoder is reached.\n\n    Raises:\n        ValueError: if :attr:`start_tokens` is not a 1D tensor or\n            :attr:`end_token` is not a scalar.\n    """"""\n\n    def __init__(self, start_tokens: torch.LongTensor,\n                 end_token: Union[int, torch.LongTensor], tau: float,\n                 straight_through: bool = False,\n                 stop_gradient: bool = False, use_finish: bool = True):\n        super().__init__(start_tokens, end_token, tau,\n                         stop_gradient, use_finish)\n        self._straight_through = straight_through\n        # unit-scale, zero-location Gumbel distribution\n        self._gumbel = Gumbel(loc=torch.tensor(0.0), scale=torch.tensor(1.0))\n\n    def sample(self, time: int, outputs: torch.Tensor) -> torch.Tensor:\n        r""""""Returns ``sample_id`` of shape ``[batch_size, vocab_size]``. If\n        :attr:`straight_through` is `False`, this contains the Gumbel softmax\n        distributions over vocabulary with temperature :attr:`tau`. If\n        :attr:`straight_through` is `True`, this contains one-hot vectors of\n        the greedy samples.\n        """"""\n        gumbel_samples = self._gumbel.sample(outputs.size()).to(\n            device=outputs.device, dtype=outputs.dtype)\n        sample_ids = torch.softmax(\n            (outputs + gumbel_samples) / self._tau, dim=-1)\n        if self._straight_through:\n            argmax_ids = torch.argmax(sample_ids, dim=-1).unsqueeze(1)\n            sample_ids_hard = torch.zeros_like(sample_ids).scatter_(\n                dim=-1, index=argmax_ids, value=1.0)  # one-hot vectors\n            sample_ids = (sample_ids_hard - sample_ids).detach() + sample_ids\n        return sample_ids\n\n\ndef default_helper_train_hparams():\n    r""""""Returns default hyperparameters of an RNN decoder helper in the training\n    phase.\n\n    See also\n    :meth:`~texar.torch.modules.decoders.rnn_decoder_helpers.get_helper` for\n    information of the hyperparameters.\n\n    Returns:\n        dict: A dictionary with following structure and values:\n\n        .. code-block:: python\n\n            {\n                # The `helper_type` argument for `get_helper`, i.e., the name\n                # or full path to the helper class.\n                ""type"": ""TrainingHelper"",\n\n                # The `**kwargs` argument for `get_helper`, i.e., additional\n                # keyword arguments for constructing the helper.\n                ""kwargs"": {}\n            }\n    """"""\n    return {\n        \'type\': \'TrainingHelper\',\n        \'kwargs\': {}\n    }\n\n\ndef default_helper_infer_hparams():\n    r""""""Returns default hyperparameters of an RNN decoder helper in the\n    inference phase.\n\n    See also\n    :meth:`~texar.torch.modules.decoders.rnn_decoder_helpers.get_helper` for\n    information of the hyperparameters.\n\n    Returns:\n        dict: A dictionary with following structure and values:\n\n        .. code-block:: python\n\n            {\n                # The `helper_type` argument for `get_helper`, i.e., the name\n                # or full path to the helper class.\n                ""type"": ""SampleEmbeddingHelper"",\n\n                # The `**kwargs` argument for `get_helper`, i.e., additional\n                # keyword arguments for constructing the helper.\n                ""kwargs"": {}\n            }\n    """"""\n    return {\n        \'type\': \'SampleEmbeddingHelper\',\n        \'kwargs\': {}\n    }\n\n\nT = TypeVar(\'T\')  # type argument\n\n\ndef get_helper(helper_type: Union[Type[T], T, str],\n               start_tokens: Optional[torch.LongTensor] = None,\n               end_token: Optional[Union[int, torch.LongTensor]] = None,\n               **kwargs):\n    r""""""Creates a Helper instance.\n\n    Args:\n        helper_type: A :class:`~texar.torch.modules.Helper` class, its\n            name or module path, or a class instance. If a class instance\n            is given, it is returned directly.\n        start_tokens: 1D :tensor:`LongTensor` shaped ``[batch_size]``,\n            representing the start tokens for each sequence in batch.\n        end_token: Python int or scalar :tensor:`LongTensor`, denoting the\n            token that marks end of decoding.\n        **kwargs: Additional keyword arguments for constructing the helper.\n\n    Returns:\n        A helper instance.\n    """"""\n    module_paths = [\n        \'texar.torch.modules.decoders.decoder_helpers\',\n        \'texar.torch.custom\']\n    class_kwargs = {\'start_tokens\': start_tokens,\n                    \'end_token\': end_token}\n    class_kwargs.update(kwargs)\n    return utils.check_or_get_instance_with_redundant_kwargs(\n        helper_type, class_kwargs, module_paths)\n'"
texar/torch/modules/decoders/gpt2_decoder.py,21,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nGPT2 decoder.\n""""""\n\nfrom typing import Dict, Optional, Tuple, Union\n\nimport torch\n\nfrom texar.torch.modules.decoders.decoder_helpers import Helper\nfrom texar.torch.modules.decoders.transformer_decoders import \\\n    TransformerDecoder, TransformerDecoderOutput\nfrom texar.torch.modules.embedders import PositionEmbedder, WordEmbedder\nfrom texar.torch.modules.pretrained.gpt2 import PretrainedGPT2Mixin\n\n__all__ = [\n    ""GPT2Decoder"",\n]\n\n\nclass GPT2Decoder(PretrainedGPT2Mixin):\n    r""""""Raw GPT2 Transformer for decoding sequences. Please see\n    :class:`~texar.torch.modules.PretrainedGPT2Mixin` for a brief description\n    of GPT2.\n\n    This module basically stacks\n    :class:`~texar.torch.modules.WordEmbedder`,\n    :class:`~texar.torch.modules.PositionEmbedder`,\n    :class:`~texar.torch.modules.TransformerDecoder`.\n\n    This module supports the architecture first proposed\n    in `(Radford et al.)` GPT2.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``gpt2-small``). Please refer to\n            :class:`~texar.torch.modules.PretrainedGPT2Mixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n    _IS_DECODE = True\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        super().__init__(hparams=hparams)\n\n        self.load_pretrained_config(pretrained_model_name, cache_dir)\n\n        # Word embedding\n        self.word_embedder = WordEmbedder(\n            vocab_size=self._hparams.vocab_size,\n            hparams=self._hparams.embed)\n\n        # Position embedding\n        self.position_embedder = PositionEmbedder(\n            position_size=self._hparams.position_size,\n            hparams=self._hparams.position_embed)\n\n        # The GPT2 decoder (a TransformerDecoder)\n        def func(tokens, positions):\n            word_embeds = self.word_embedder(tokens)\n            pos_embeds = self.position_embedder(positions)\n            return word_embeds + pos_embeds\n\n        class GPT2TransformerDecoder(TransformerDecoder):\n            def embed_tokens(self, tokens: torch.LongTensor,\n                             positions: torch.LongTensor) -> torch.Tensor:\n                return func(tokens, positions)\n\n        self.decoder = GPT2TransformerDecoder(\n            vocab_size=self._hparams.vocab_size,\n            output_layer=self.word_embedder.embedding,\n            hparams=self._hparams.decoder)\n\n        self.init_pretrained_weights()\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The decoder arch is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the encoder arch is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the encoder arch is defined by the\n          configurations in `hparams` and weights are randomly initialized.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""gpt2_decoder"",\n                ""pretrained_model_name"": ""gpt2-small"",\n                ""vocab_size"": 50257,\n                ""context_size"": 1024,\n                ""embedding_size"": 768,\n                ""embed"": {\n                    ""dim"": 768,\n                    ""name"": ""word_embeddings""\n                },\n                ""position_size"": 1024,\n                ""position_embed"": {\n                    ""dim"": 768,\n                    ""name"": ""position_embeddings""\n                },\n\n                # hparams for TransformerDecoder\n                ""decoder"": {\n                    ""dim"": 768,\n                    ""num_blocks"": 12,\n                    ""embedding_dropout"": 0,\n                    ""residual_dropout"": 0,\n                    ""multihead_attention"": {\n                        ""use_bias"": True,\n                        ""num_units"": 768,\n                        ""num_heads"": 12,\n                        ""dropout_rate"": 0.0,\n                        ""output_dim"": 768\n                    },\n                    ""initializer"": {\n                        ""type"": ""variance_scaling_initializer"",\n                        ""kwargs"": {\n                            ""factor"": 1.0,\n                            ""mode"": ""FAN_AVG"",\n                            ""uniform"": True\n                        }\n                    },\n                    ""eps"": 1e-5,\n                    ""poswise_feedforward"": {\n                        ""layers"": [\n                            {\n                                ""type"": ""Linear"",\n                                ""kwargs"": {\n                                    ""in_features"": 768,\n                                    ""out_features"": 3072,\n                                    ""bias"": True\n                                }\n                            },\n                            {\n                                ""type"": ""GPTGELU"",\n                                ""kwargs"": {}\n                            },\n                            {\n                                ""type"": ""Linear"",\n                                ""kwargs"": {\n                                    ""in_features"": 3072,\n                                    ""out_features"": 768,\n                                    ""bias"": True\n                                }\n                            }\n                        ],\n                        ""name"": ""ffn""\n                    }\n                },\n            }\n\n        Here:\n\n        The default parameters are values for 124M GPT2 model.\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained GPT2 model. If None, the model\n            will be randomly initialized.\n\n        `""embed""`: dict\n            Hyperparameters for word embedding layer.\n\n        `""vocab_size""`: int\n            The vocabulary size of `inputs` in `GPT2Model`.\n\n        `""position_embed""`: dict\n            Hyperparameters for position embedding layer.\n\n        `""eps""`: float\n            Epsilon values for layer norm layers.\n\n        `""position_size""`:  int\n            The maximum sequence length that this model might ever be used with.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n        return {\n            \'decoder\': {\n                \'dim\': 768,\n                \'num_blocks\': 12,\n                \'embedding_dropout\': 0,\n                \'residual_dropout\': 0,\n                \'multihead_attention\': {\n                    \'use_bias\': True,\n                    \'num_units\': 768,\n                    \'num_heads\': 12,\n                    ""dropout_rate"": 0.0,\n                    \'output_dim\': 768\n                },\n                \'initializer\': {\n                    \'type\': \'variance_scaling_initializer\',\n                    \'kwargs\': {\n                        \'factor\': 1.0,\n                        \'mode\': \'FAN_AVG\',\n                        \'uniform\': True\n                    }\n                },\n                \'eps\': 1e-5,\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 768,\n                                \'out_features\': 3072,\n                                \'bias\': True\n                            }\n                        },\n                        {\n                            \'type\': \'GPTGELU\',\n                            \'kwargs\': {}\n                        },\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 3072,\n                                \'out_features\': 768,\n                                \'bias\': True\n                            }\n                        }\n                    ],\n                    \'name\': \'ffn\'\n                },\n            },\n            \'pretrained_model_name\': \'gpt2-small\',\n            \'vocab_size\': 50257,\n            \'context_size\': 1024,\n            \'embedding_size\': 768,\n            \'embed\': {\n                \'dim\': 768,\n                \'name\': \'word_embeddings\'\n            },\n            \'position_size\': 1024,\n            \'position_embed\': {\n                \'dim\': 768,\n                \'name\': \'position_embeddings\'\n            },\n\n            \'name\': \'gpt2_decoder\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    def forward(self,  # type: ignore\n                inputs: Optional[torch.Tensor] = None,\n                sequence_length: Optional[torch.LongTensor] = None,\n                memory: Optional[torch.Tensor] = None,\n                memory_sequence_length: Optional[torch.LongTensor] = None,\n                memory_attention_bias: Optional[torch.Tensor] = None,\n                context: Optional[torch.Tensor] = None,\n                context_sequence_length: Optional[torch.LongTensor] = None,\n                helper: Optional[Helper] = None,\n                decoding_strategy: str = \'train_greedy\',\n                max_decoding_length: Optional[int] = None,\n                impute_finished: bool = False,\n                infer_mode: Optional[bool] = None,\n                beam_width: Optional[int] = None,\n                length_penalty: float = 0.,\n                **kwargs) \\\n            -> Union[\n                TransformerDecoderOutput,\n                Tuple[TransformerDecoderOutput, torch.LongTensor],\n                Dict[str, torch.Tensor]]:\n        r""""""Performs decoding. Has exact the same interfaces with\n        :meth:`texar.torch.modules.TransformerDecoder.forward`. Please refer to\n        it for the detailed usage.\n        """"""\n        return self.decoder(inputs=inputs,\n                            sequence_length=sequence_length,\n                            memory=memory,\n                            memory_sequence_length=memory_sequence_length,\n                            memory_attention_bias=memory_attention_bias,\n                            context=context,\n                            context_sequence_length=context_sequence_length,\n                            helper=helper,\n                            decoding_strategy=decoding_strategy,\n                            max_decoding_length=max_decoding_length,\n                            impute_finished=impute_finished,\n                            infer_mode=infer_mode,\n                            beam_width=beam_width,\n                            length_penalty=length_penalty,\n                            **kwargs)\n'"
texar/torch/modules/decoders/rnn_decoder_base.py,21,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for RNN decoders.\n""""""\n\nfrom typing import Optional, Tuple, TypeVar\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.core import layers\nfrom texar.torch.core.cell_wrappers import RNNCellBase\nfrom texar.torch.modules.decoders import decoder_helpers\nfrom texar.torch.modules.decoders.decoder_base import (\n    DecoderBase, TokenEmbedder, TokenPosEmbedder, _make_output_layer)\nfrom texar.torch.modules.decoders.decoder_helpers import Helper\nfrom texar.torch.utils import utils\n\n__all__ = [\n    \'RNNDecoderBase\',\n]\n\nState = TypeVar(\'State\')\nOutput = TypeVar(\'Output\')  # output type can be of any nested structure\n\n\nclass RNNDecoderBase(DecoderBase[State, Output]):\n    r""""""Base class inherited by all RNN decoder classes.\n    See :class:`~texar.torch.modules.BasicRNNDecoder` for the arguments.\n\n    See :meth:`forward` for the inputs and outputs of RNN decoders in general.\n    """"""\n\n    def __init__(self,\n                 input_size: int,\n                 vocab_size: int,\n                 token_embedder: Optional[TokenEmbedder] = None,\n                 token_pos_embedder: Optional[TokenPosEmbedder] = None,\n                 cell: Optional[RNNCellBase] = None,\n                 output_layer: Optional[nn.Module] = None,\n                 input_time_major: bool = False,\n                 output_time_major: bool = False,\n                 hparams=None):\n        super().__init__(token_embedder, token_pos_embedder,\n                         input_time_major, output_time_major, hparams=hparams)\n\n        self._input_size = input_size\n        self._vocab_size = vocab_size\n\n        # Make RNN cell\n        self._cell = cell or layers.get_rnn_cell(\n            input_size, self._hparams.rnn_cell)\n        self._beam_search_cell = None\n\n        # Make the output layer\n        self._output_layer, _ = _make_output_layer(\n            output_layer, self._vocab_size, self._cell.hidden_size,\n            self._hparams.output_layer_bias)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        The hyperparameters are the same as in\n        :meth:`~texar.torch.modules.BasicRNNDecoder.default_hparams` of\n        :class:`~texar.torch.modules.BasicRNNDecoder`, except that the default\n        ``""name""`` here is ``""rnn_decoder""``.\n        """"""\n        return {\n            \'rnn_cell\': layers.default_rnn_cell_hparams(),\n            \'helper_train\': decoder_helpers.default_helper_train_hparams(),\n            \'helper_infer\': decoder_helpers.default_helper_infer_hparams(),\n            \'max_decoding_length_train\': None,\n            \'max_decoding_length_infer\': None,\n            \'name\': \'rnn_decoder\',\n            ""output_layer_bias"": True,\n        }\n\n    def forward(self,  # type: ignore\n                inputs: Optional[torch.Tensor] = None,\n                sequence_length: Optional[torch.LongTensor] = None,\n                initial_state: Optional[State] = None,\n                helper: Optional[Helper] = None,\n                max_decoding_length: Optional[int] = None,\n                impute_finished: bool = False,\n                infer_mode: Optional[bool] = None, **kwargs) \\\n            -> Tuple[Output, Optional[State], torch.LongTensor]:\n        r""""""Performs decoding. This is a shared interface for both\n        :class:`~texar.torch.modules.BasicRNNDecoder` and\n        :class:`~texar.torch.modules.AttentionRNNDecoder`.\n\n        Implementation calls :meth:`initialize` once and :meth:`step`\n        repeatedly on the decoder object. Please refer to\n        `tf.contrib.seq2seq.dynamic_decode\n        <https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode>`_.\n\n        See Also:\n            Arguments of :meth:`create_helper`, for arguments like\n            :attr:`decoding_strategy`.\n\n        Args:\n            inputs (optional): Input tensors for teacher forcing decoding.\n                Used when :attr:`decoding_strategy` is set to\n                ``""train_greedy""``, or when `hparams`-configured helper is used.\n\n                The :attr:`inputs` is a :tensor:`LongTensor` used as index to\n                look up embeddings and feed in the decoder. For example, if\n                :attr:`embedder` is an instance of\n                :class:`~texar.torch.modules.WordEmbedder`, then :attr:`inputs`\n                is usually a 2D int Tensor `[batch_size, max_time]` (or\n                `[max_time, batch_size]` if `input_time_major` == `True`)\n                containing the token indexes.\n            sequence_length (optional): A 1D int Tensor containing the\n                sequence length of :attr:`inputs`.\n                Used when `decoding_strategy=""train_greedy""` or\n                `hparams`-configured helper is used.\n            initial_state (optional): Initial state of decoding.\n                If `None` (default), zero state is used.\n            max_decoding_length: A int scalar Tensor indicating the maximum\n                allowed number of decoding steps. If `None` (default), either\n                `hparams[""max_decoding_length_train""]` or\n                `hparams[""max_decoding_length_infer""]` is used\n                according to :attr:`mode`.\n            impute_finished (bool): If `True`, then states for batch\n                entries which are marked as finished get copied through and\n                the corresponding outputs get zeroed out.  This causes some\n                slowdown at each time step, but ensures that the final state\n                and outputs have the correct values and that backprop ignores\n                time steps that were marked as finished.\n            helper (optional): An instance of\n                :class:`~texar.torch.modules.Helper`\n                that defines the decoding strategy. If given,\n                ``decoding_strategy`` and helper configurations in\n                :attr:`hparams` are ignored.\n\n                :meth:`create_helper` can be used to create some of the common\n                helpers for, e.g., teacher-forcing decoding, greedy decoding,\n                sample decoding, etc.\n            infer_mode (optional): If not `None`, overrides mode given by\n                `self.training`.\n            **kwargs: Other keyword arguments for constructing helpers\n                defined by ``hparams[""helper_train""]`` or\n                ``hparams[""helper_infer""]``.\n\n        Returns:\n            ``(outputs, final_state, sequence_lengths)``, where\n\n            - `outputs`: an object containing the decoder output on all\n              time steps.\n            - `final_state`: the cell state of the final time step.\n            - `sequence_lengths`: a :tensor:`LongTensor` of shape\n              ``[batch_size]`` containing the length of each sample.\n        """"""\n        # TODO: Add faster code path for teacher-forcing training.\n\n        # Helper\n        if helper is None:\n            helper = self._create_or_get_helper(infer_mode, **kwargs)\n\n        if (isinstance(helper, decoder_helpers.TrainingHelper) and\n                (inputs is None or sequence_length is None)):\n            raise ValueError(""\'input\' and \'sequence_length\' must not be None ""\n                             ""when using \'TrainingHelper\'."")\n\n        # Initial state\n        self._cell.init_batch()\n\n        # Maximum decoding length\n        if max_decoding_length is None:\n            if self.training:\n                max_decoding_length = self._hparams.max_decoding_length_train\n            else:\n                max_decoding_length = self._hparams.max_decoding_length_infer\n            if max_decoding_length is None:\n                max_decoding_length = utils.MAX_SEQ_LENGTH\n\n        return self.dynamic_decode(\n            helper, inputs, sequence_length, initial_state,\n            max_decoding_length, impute_finished)\n\n    def _get_beam_search_cell(self):\n        self._beam_search_cell = self._cell\n        return self._cell\n\n    @property\n    def output_size(self):\n        r""""""Output size of one step.\n        """"""\n        raise NotImplementedError\n\n    def initialize(self, helper: Helper, inputs: Optional[torch.Tensor],\n                   sequence_length: Optional[torch.LongTensor],\n                   initial_state: Optional[State]) \\\n            -> Tuple[torch.ByteTensor, torch.Tensor, Optional[State]]:\n        initial_finished, initial_inputs = helper.initialize(\n            self.embed_tokens, inputs, sequence_length)\n        if initial_state is None:\n            state = self._cell.init_batch()\n        else:\n            state = initial_state\n        return (initial_finished, initial_inputs, state)\n\n    def step(self, helper: Helper, time: int,\n             inputs: torch.Tensor, state: Optional[State]) \\\n            -> Tuple[Output, State]:\n        raise NotImplementedError\n\n    def next_inputs(self, helper: Helper, time: int, outputs: Output) -> \\\n            Tuple[torch.Tensor, torch.ByteTensor]:\n        raise NotImplementedError\n\n    @property\n    def cell(self):\n        r""""""The RNN cell.\n        """"""\n        return self._cell\n\n    def zero_state(self, batch_size):\n        r""""""Zero state of the RNN cell.\n        Equivalent to :attr:`decoder.cell.zero_state`.\n        """"""\n        return self._cell.zero_state(batch_size=batch_size)\n\n    @property\n    def state_size(self):\n        r""""""The state size of decoder cell.\n        Equivalent to :attr:`decoder.cell.state_size`.\n        """"""\n        return self._cell.hidden_size\n\n    @property\n    def output_layer(self):\n        r""""""The output layer.\n        """"""\n        return self._output_layer\n'"
texar/torch/modules/decoders/rnn_decoders.py,80,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious RNN decoders.\n""""""\n\nfrom typing import Callable, Dict, NamedTuple, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.core import layers\nfrom texar.torch.core.attention_mechanism import (\n    AttentionMechanism, AttentionWrapperState)\nfrom texar.torch.core.cell_wrappers import (\n    AttentionWrapper, HiddenState, RNNCellBase)\nfrom texar.torch.modules.decoders import decoder_helpers\nfrom texar.torch.modules.decoders.decoder_base import (\n    TokenEmbedder, TokenPosEmbedder)\nfrom texar.torch.modules.decoders.decoder_helpers import Helper\nfrom texar.torch.modules.decoders.rnn_decoder_base import RNNDecoderBase\nfrom texar.torch.utils import utils\nfrom texar.torch.utils.beam_search import beam_search\nfrom texar.torch.utils.types import MaybeList, MaybeTuple\nfrom texar.torch.utils.utils import check_or_get_instance, get_function\n\n__all__ = [\n    \'BasicRNNDecoderOutput\',\n    \'AttentionRNNDecoderOutput\',\n    \'BasicRNNDecoder\',\n    \'AttentionRNNDecoder\',\n]\n\n\nclass BasicRNNDecoderOutput(NamedTuple):\n    r""""""The outputs of :class:`~texar.torch.modules.BasicRNNDecoder` that\n    include both RNN outputs and sampled IDs at each step. This is also used to\n    store results of all the steps after decoding the whole sequence.\n    """"""\n    logits: torch.Tensor\n    r""""""The outputs of RNN (at each step/of all steps) by applying the\n    output layer on cell outputs. For example, in\n    :class:`~texar.torch.modules.BasicRNNDecoder` with default hyperparameters,\n    this is a :tensor:`Tensor` of shape ``[batch_size, max_time, vocab_size]``\n    after decoding the whole sequence.""""""\n    sample_id: torch.LongTensor\n    r""""""The sampled results (at each step/of all steps). For example, in\n    :class:`~texar.torch.modules.BasicRNNDecoder` with decoding strategy of\n    ``""train_greedy""``, this is a :tensor:`LongTensor` of shape\n    ``[batch_size, max_time]`` containing the sampled token indices of all\n    steps. Note that the shape of ``sample_id`` is different for different\n    decoding strategy or helper. Please refer to\n    :class:`~texar.torch.modules.Helper` for the detailed information.""""""\n    cell_output: torch.Tensor\n    r""""""The output of RNN cell (at each step/of all steps). This contains the\n    results prior to the output layer. For example, in\n    :class:`~texar.torch.modules.BasicRNNDecoder` with default hyperparameters,\n    this is a :tensor:`Tensor` of shape\n    ``[batch_size, max_time, cell_output_size]`` after decoding the whole\n    sequence.""""""\n\n\nclass AttentionRNNDecoderOutput(NamedTuple):\n    r""""""The outputs of :class:`~texar.torch.modules.AttentionRNNDecoder` that\n    additionally includes attention results.\n    """"""\n    logits: torch.Tensor\n    r""""""The outputs of RNN (at each step/of all steps) by applying the\n    output layer on cell outputs. For example, in\n    :class:`~texar.torch.modules.AttentionRNNDecoder` with default\n    hyperparameters, this is a :tensor:`Tensor` of shape\n    ``[batch_size, max_time, vocab_size]`` after decoding the whole sequence.""""""\n    sample_id: torch.LongTensor\n    r""""""The sampled results (at each step/of all steps). For example, in\n    :class:`~texar.torch.modules.AttentionRNNDecoder` with decoding strategy of\n    ``""train_greedy""``, this is a :tensor:`LongTensor` of shape\n    ``[batch_size, max_time]`` containing the sampled token indices of all\n    steps. Note that the shape of ``sample_id`` is different for different\n    decoding strategy or helper. Please refer to\n    :class:`~texar.torch.modules.Helper` for the detailed information.""""""\n    cell_output: torch.Tensor\n    r""""""The output of RNN cell (at each step/of all steps). This contains the\n    results prior to the output layer. For example, in\n    :class:`~texar.torch.modules.AttentionRNNDecoder` with default\n    hyperparameters, this is a :tensor:`Tensor` of shape\n    ``[batch_size, max_time, cell_output_size]`` after decoding the whole\n    sequence.""""""\n    attention_scores: MaybeTuple[torch.Tensor]\n    r""""""A single or tuple of `Tensor(s)` containing the alignments emitted (at\n    the previous time step/of all time steps) for each attention mechanism.""""""\n    attention_context: torch.Tensor\n    r""""""The attention emitted (at the previous time step/of all time steps).""""""\n\n\nclass BasicRNNDecoder(RNNDecoderBase[HiddenState, BasicRNNDecoderOutput]):\n    r""""""Basic RNN decoder.\n\n    Args:\n        input_size (int): Dimension of input embeddings.\n        vocab_size (int, optional): Vocabulary size. Required if\n            :attr:`output_layer` is `None`.\n        token_embedder: An instance of :torch_nn:`Module`, or a function taking\n            a :tensor:`LongTensor` ``tokens`` as argument. This is the embedder\n            called in :meth:`embed_tokens` to convert input tokens to\n            embeddings.\n        token_pos_embedder: An instance of :torch_nn:`Module`, or a function\n            taking two :tensor:`LongTensor`\\ s ``tokens`` and ``positions`` as\n            argument. This is the embedder called in :meth:`embed_tokens` to\n            convert input tokens with positions to embeddings.\n\n            .. note::\n                Only one among :attr:`token_embedder` and\n                :attr:`token_pos_embedder` should be specified. If neither is\n                specified, you must subclass :class:`BasicRNNDecoder` and\n                override :meth:`embed_tokens`.\n        cell (RNNCellBase, optional): An instance of\n            :class:`~texar.torch.core.cell_wrappers.RNNCellBase`. If `None`\n            (default), a cell is created as specified in :attr:`hparams`.\n        output_layer (optional): An instance of :torch_nn:`Module`. Apply to\n            the RNN cell output to get logits. If `None`, a :torch_nn:`Linear`\n            layer is used with output dimension set to :attr:`vocab_size`.\n            Set ``output_layer`` to :func:`~texar.torch.core.identity` if you do\n            not want to have an output layer after the RNN cell outputs.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    See :meth:`~texar.torch.modules.RNNDecoderBase.forward` for the inputs and\n    outputs of the decoder. The decoder returns\n    ``(outputs, final_state, sequence_lengths)``, where ``outputs`` is an\n    instance of :class:`~texar.torch.modules.BasicRNNDecoderOutput`.\n\n    Example:\n        .. code-block:: python\n\n            embedder = WordEmbedder(vocab_size=data.vocab.size)\n            decoder = BasicRNNDecoder(vocab_size=data.vocab.size)\n            # Training loss\n            outputs, _, _ = decoder(\n                decoding_strategy=\'train_greedy\',\n                inputs=embedder(data_batch[\'text_ids\']),\n                sequence_length=data_batch[\'length\']-1)\n            loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n                labels=data_batch[\'text_ids\'][:, 1:],\n                logits=outputs.logits,\n                sequence_length=data_batch[\'length\']-1)\n\n            # Create helper\n            helper = decoder.create_helper(\n                decoding_strategy=\'infer_sample\',\n                start_tokens=[data.vocab.bos_token_id]*100,\n                end_token=data.vocab.eos.token_id,\n                embedding=embedder)\n\n            # Inference sample\n            outputs, _, _ = decoder(\n                helper=helerp,\n                max_decoding_length=60)\n\n            sample_text = tx.utils.map_ids_to_strs(\n                outputs.sample_id, data.vocab)\n            print(sample_text)\n            # [\n            #   the first sequence sample .\n            #   the second sequence sample .\n            #   ...\n            # ]\n    """"""\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""rnn_cell"": default_rnn_cell_hparams(),\n                ""max_decoding_length_train"": None,\n                ""max_decoding_length_infer"": None,\n                ""helper_train"": {\n                    ""type"": ""TrainingHelper"",\n                    ""kwargs"": {}\n                }\n                ""helper_infer"": {\n                    ""type"": ""SampleEmbeddingHelper"",\n                    ""kwargs"": {}\n                }\n                ""name"": ""basic_rnn_decoder""\n            }\n\n        Here:\n\n        `""rnn_cell""`: dict\n            A dictionary of RNN cell hyperparameters. Ignored if\n            :attr:`cell` is given to the decoder constructor.\n            The default value is defined in\n            :func:`~texar.torch.core.default_rnn_cell_hparams`.\n\n        `""max_decoding_length_train""`: int or None\n            Maximum allowed number of decoding steps in training mode. If\n            `None` (default), decoding is performed until fully done, e.g.,\n            encountering the ``<EOS>`` token. Ignored if\n            ``""max_decoding_length""`` is not `None` given when calling the\n            decoder.\n\n        `""max_decoding_length_infer""`: int or None\n            Same as ``""max_decoding_length_train""`` but for inference mode.\n\n        `""helper_train""`: dict\n            The hyperparameters of the helper used in training.\n            ``""type""`` can be a helper class, its name or module path, or a\n            helper instance. If a class name is given, the class must be\n            from module :mod:`texar.torch.modules`, or\n            :mod:`texar.torch.custom`. This is used only when both\n            ``""decoding_strategy""`` and ``""helper""`` arguments are `None` when\n            calling the decoder. See\n            :meth:`~texar.torch.modules.RNNDecoderBase.forward` for more\n            details.\n\n        `""helper_infer""`: dict\n            Same as ``""helper_train""`` but during inference mode.\n\n        `""name""`: str\n            Name of the decoder.\n            The default value is ``""basic_rnn_decoder""``.\n        """"""\n        hparams = RNNDecoderBase.default_hparams()\n        hparams[\'name\'] = \'basic_rnn_decoder\'\n        return hparams\n\n    def step(self, helper: Helper, time: int, inputs: torch.Tensor,\n             state: Optional[HiddenState]) \\\n            -> Tuple[BasicRNNDecoderOutput, HiddenState]:\n        cell_outputs, cell_state = self._cell(inputs, state)\n        logits = self._output_layer(cell_outputs)\n        sample_ids = helper.sample(time=time, outputs=logits)\n        next_state = cell_state\n        outputs = BasicRNNDecoderOutput(logits, sample_ids, cell_outputs)\n        return outputs, next_state\n\n    def next_inputs(self, helper: Helper, time: int,\n                    outputs: BasicRNNDecoderOutput) -> \\\n            Tuple[torch.Tensor, torch.ByteTensor]:\n        finished, next_inputs = helper.next_inputs(\n            self.embed_tokens, time, outputs.logits, outputs.sample_id)\n        return next_inputs, finished\n\n    @property\n    def output_size(self):\n        r""""""Output size of one step.\n        """"""\n        return self._cell.hidden_size\n\n\nclass AttentionRNNDecoder(RNNDecoderBase[AttentionWrapperState,\n                                         AttentionRNNDecoderOutput]):\n    r""""""RNN decoder with attention mechanism.\n\n    Args:\n        input_size (int): Dimension of input embeddings.\n        encoder_output_size (int): The output size of the encoder cell.\n        vocab_size (int): Vocabulary size. Required if\n            :attr:`output_layer` is `None`.\n        token_embedder: An instance of :torch_nn:`Module`, or a function taking\n            a :tensor:`LongTensor` ``tokens`` as argument. This is the embedder\n            called in :meth:`embed_tokens` to convert input tokens to\n            embeddings.\n        token_pos_embedder: An instance of :torch_nn:`Module`, or a function\n            taking two :tensor:`LongTensor`\\ s ``tokens`` and ``positions`` as\n            argument. This is the embedder called in :meth:`embed_tokens` to\n            convert input tokens with positions to embeddings.\n\n            .. note::\n                Only one among :attr:`token_embedder` and\n                :attr:`token_pos_embedder` should be specified. If neither is\n                specified, you must subclass :class:`AttentionRNNDecoder` and\n                override :meth:`embed_tokens`.\n        cell (RNNCellBase, optional): An instance of\n            :class:`~texar.torch.core.cell_wrappers.RNNCellBase`. If `None`,\n            a cell is created as specified in :attr:`hparams`.\n        output_layer (optional): An output layer that transforms cell output\n            to logits. This can be:\n\n            - A callable layer, e.g., an instance of :torch_nn:`Module`.\n            - A tensor. A dense layer will be created using the tensor\n              as the kernel weights. The bias of the dense layer is determined\n              by `hparams.output_layer_bias`. This can be used to tie the\n              output layer with the input embedding matrix, as proposed in\n              https://arxiv.org/pdf/1608.05859.pdf\n            - `None`. A dense layer will be created based on :attr:`vocab_size`\n              and `hparams.output_layer_bias`.\n            - If no output layer after the cell output is needed, set\n              `(vocab_size=None, output_layer=texar.torch.core.identity)`.\n        cell_input_fn (callable, optional): A callable that produces RNN cell\n            inputs. If `None` (default), the default is used:\n            :python:`lambda inputs, attention:\n            torch.cat([inputs, attention], -1)`,\n            which concatenates regular RNN cell inputs with attentions.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    See :meth:`texar.torch.modules.RNNDecoderBase.forward` for the inputs and\n    outputs of the decoder. The decoder returns\n    `(outputs, final_state, sequence_lengths)`, where `outputs` is an instance\n    of :class:`~texar.torch.modules.AttentionRNNDecoderOutput`.\n\n    Example:\n        .. code-block:: python\n\n            # Encodes the source\n            enc_embedder = WordEmbedder(data.source_vocab.size, ...)\n            encoder = UnidirectionalRNNEncoder(...)\n            enc_outputs, _ = encoder(\n                inputs=enc_embedder(data_batch[\'source_text_ids\']),\n                sequence_length=data_batch[\'source_length\'])\n            # Decodes while attending to the source\n            dec_embedder = WordEmbedder(vocab_size=data.target_vocab.size, ...)\n            decoder = AttentionRNNDecoder(\n                encoder_output_size=(self.encoder.cell_fw.hidden_size +\n                                     self.encoder.cell_bw.hidden_size),\n                input_size=dec_embedder.dim,\n                vocab_size=data.target_vocab.size)\n            outputs, _, _ = decoder(\n                decoding_strategy=\'train_greedy\',\n                memory=enc_outputs,\n                memory_sequence_length=data_batch[\'source_length\'],\n                inputs=dec_embedder(data_batch[\'target_text_ids\']),\n                sequence_length=data_batch[\'target_length\']-1)\n    """"""\n\n    def __init__(self,\n                 input_size: int,\n                 encoder_output_size: int,\n                 vocab_size: int,\n                 token_embedder: Optional[TokenEmbedder] = None,\n                 token_pos_embedder: Optional[TokenPosEmbedder] = None,\n                 cell: Optional[RNNCellBase] = None,\n                 output_layer: Optional[Union[nn.Module, torch.Tensor]] = None,\n                 cell_input_fn: Optional[Callable[[torch.Tensor, torch.Tensor],\n                                                  torch.Tensor]] = None,\n                 hparams=None):\n\n        super().__init__(\n            input_size, vocab_size, token_embedder, token_pos_embedder,\n            cell=cell, output_layer=output_layer, hparams=hparams)\n\n        attn_hparams = self._hparams[\'attention\']\n        attn_kwargs = attn_hparams[\'kwargs\'].todict()\n\n        # Compute the correct input_size internally.\n        if cell is None:\n            if cell_input_fn is None:\n                if attn_hparams[""attention_layer_size""] is None:\n                    input_size += encoder_output_size\n                else:\n                    input_size += attn_hparams[""attention_layer_size""]\n            else:\n                if attn_hparams[""attention_layer_size""] is None:\n                    input_size = cell_input_fn(\n                        torch.empty(input_size),\n                        torch.empty(encoder_output_size)).shape[-1]\n                else:\n                    input_size = cell_input_fn(\n                        torch.empty(input_size),\n                        torch.empty(\n                            attn_hparams[""attention_layer_size""])).shape[-1]\n            self._cell = layers.get_rnn_cell(input_size, self._hparams.rnn_cell)\n\n        # Parse the `probability_fn` argument\n        if \'probability_fn\' in attn_kwargs:\n            prob_fn = attn_kwargs[\'probability_fn\']\n            if prob_fn is not None and not callable(prob_fn):\n                prob_fn = get_function(prob_fn, [\'torch.nn.functional\',\n                                                 \'texar.torch.core\'])\n            attn_kwargs[\'probability_fn\'] = prob_fn\n\n        # Parse `encoder_output_size` and `decoder_output_size` arguments\n        if attn_hparams[\'type\'] in [\'BahdanauAttention\',\n                                    \'BahdanauMonotonicAttention\']:\n            attn_kwargs.update({""decoder_output_size"": self._cell.hidden_size})\n        attn_kwargs.update({""encoder_output_size"": encoder_output_size})\n\n        attn_modules = [\'texar.torch.core\']\n        # TODO: Support multiple attention mechanisms.\n        self.attention_mechanism: AttentionMechanism\n        self.attention_mechanism = check_or_get_instance(\n            attn_hparams[""type""], attn_kwargs, attn_modules,\n            classtype=AttentionMechanism)\n\n        self._attn_cell_kwargs = {\n            ""attention_layer_size"": attn_hparams[""attention_layer_size""],\n            ""alignment_history"": attn_hparams[""alignment_history""],\n            ""output_attention"": attn_hparams[""output_attention""],\n        }\n        self._cell_input_fn = cell_input_fn\n\n        if attn_hparams[""output_attention""] and vocab_size is not None and \\\n                self.attention_mechanism is not None:\n            if attn_hparams[""attention_layer_size""] is None:\n                self._output_layer = nn.Linear(\n                    encoder_output_size,\n                    vocab_size)\n            else:\n                self._output_layer = nn.Linear(\n                    sum(attn_hparams[""attention_layer_size""])\n                    if isinstance(attn_hparams[""attention_layer_size""], list)\n                    else attn_hparams[""attention_layer_size""],\n                    vocab_size)\n\n        attn_cell = AttentionWrapper(\n            self._cell,\n            self.attention_mechanism,\n            cell_input_fn=self._cell_input_fn,\n            **self._attn_cell_kwargs)\n\n        self._cell: AttentionWrapper = attn_cell\n\n        self.memory: Optional[torch.Tensor] = None\n        self.memory_sequence_length: Optional[torch.LongTensor] = None\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n        Common hyperparameters are the same as in\n        :class:`~texar.torch.modules.BasicRNNDecoder`.\n        :meth:`~texar.torch.modules.BasicRNNDecoder.default_hparams`.\n        Additional hyperparameters are for attention mechanism\n        configuration.\n\n        .. code-block:: python\n\n            {\n                ""attention"": {\n                    ""type"": ""LuongAttention"",\n                    ""kwargs"": {\n                        ""num_units"": 256,\n                    },\n                    ""attention_layer_size"": None,\n                    ""alignment_history"": False,\n                    ""output_attention"": True,\n                },\n                # The following hyperparameters are the same as with\n                # `BasicRNNDecoder`\n                ""rnn_cell"": default_rnn_cell_hparams(),\n                ""max_decoding_length_train"": None,\n                ""max_decoding_length_infer"": None,\n                ""helper_train"": {\n                    ""type"": ""TrainingHelper"",\n                    ""kwargs"": {}\n                }\n                ""helper_infer"": {\n                    ""type"": ""SampleEmbeddingHelper"",\n                    ""kwargs"": {}\n                }\n                ""name"": ""attention_rnn_decoder""\n            }\n\n        Here:\n\n        `""attention""`: dict\n            Attention hyperparameters, including:\n\n            `""type""`: str or class or instance\n                The attention type. Can be an attention class, its name or\n                module path, or a class instance. The class must be a subclass\n                of ``AttentionMechanism``. See :ref:`attention-mechanism` for\n                all supported attention mechanisms. If class name is given,\n                the class must be from modules\n                :mod:`texar.torch.core` or :mod:`texar.torch.custom`.\n\n                Example:\n\n                .. code-block:: python\n\n                    # class name\n                    ""type"": ""LuongAttention""\n                    ""type"": ""BahdanauAttention""\n                    # module path\n                    ""type"": ""texar.torch.core.BahdanauMonotonicAttention""\n                    ""type"": ""my_module.MyAttentionMechanismClass""\n                    # class\n                    ""type"": texar.torch.core.LuongMonotonicAttention\n                    # instance\n                    ""type"": LuongAttention(...)\n\n            `""kwargs""`: dict\n                keyword arguments for the attention class constructor.\n                Arguments :attr:`memory` and\n                :attr:`memory_sequence_length` should **not** be\n                specified here because they are given to the decoder\n                constructor. Ignored if ""type"" is an attention class\n                instance. For example:\n\n                .. code-block:: python\n\n                    ""type"": ""LuongAttention"",\n                    ""kwargs"": {\n                        ""num_units"": 256,\n                        ""probability_fn"": torch.nn.functional.softmax,\n                    }\n\n                Here `""probability_fn""` can also be set to the string name\n                or module path to a probability function.\n\n                `""attention_layer_size""`: int or None\n                    The depth of the attention (output) layer. The context and\n                    cell output are fed into the attention layer to generate\n                    attention at each time step.\n                    If `None` (default), use the context as attention at each\n                    time step.\n\n                `""alignment_history""`: bool\n                    whether to store alignment history from all time steps\n                    in the final output state. (Stored as a time major\n                    `TensorArray` on which you must call `stack()`.)\n\n                `""output_attention""`: bool\n                    If `True` (default), the output at each time step is\n                    the attention value. This is the behavior of Luong-style\n                    attention mechanisms. If `False`, the output at each\n                    time step is the output of `cell`.  This is the\n                    behavior of Bahdanau-style attention mechanisms.\n                    In both cases, the `attention` tensor is propagated to\n                    the next time step via the state and is used there.\n                    This flag only controls whether the attention mechanism\n                    is propagated up to the next cell in an RNN stack or to\n                    the top RNN output.\n        """"""\n        hparams = RNNDecoderBase.default_hparams()\n        hparams[""name""] = ""attention_rnn_decoder""\n        hparams[""attention""] = {\n            ""type"": ""LuongAttention"",\n            ""kwargs"": {\n                ""num_units"": 256,\n            },\n            ""attention_layer_size"": None,\n            ""alignment_history"": False,\n            ""output_attention"": True,\n        }\n        return hparams\n\n    def initialize(  # type: ignore\n            self, helper: Helper,\n            inputs: Optional[torch.Tensor],\n            sequence_length: Optional[torch.LongTensor],\n            initial_state: Optional[MaybeList[MaybeTuple[torch.Tensor]]]) -> \\\n            Tuple[torch.ByteTensor, torch.Tensor,\n                  Optional[AttentionWrapperState]]:\n        initial_finished, initial_inputs = helper.initialize(\n            self.embed_tokens, inputs, sequence_length)\n        if initial_state is None:\n            state = None\n        else:\n            tensor = utils.get_first_in_structure(initial_state)\n            assert tensor is not None\n            tensor: torch.Tensor\n            state = self._cell.zero_state(batch_size=tensor.size(0))\n            state = state._replace(cell_state=initial_state)\n\n        return initial_finished, initial_inputs, state\n\n    def step(self, helper: Helper, time: int, inputs: torch.Tensor,\n             state: Optional[AttentionWrapperState]) -> \\\n            Tuple[AttentionRNNDecoderOutput, AttentionWrapperState]:\n        wrapper_outputs, wrapper_state = self._cell(\n            inputs, state, self.memory, self.memory_sequence_length)\n        # Essentially the same as in BasicRNNDecoder.step()\n\n        logits = self._output_layer(wrapper_outputs)\n        sample_ids = helper.sample(time=time, outputs=logits)\n\n        attention_scores = wrapper_state.alignments\n        attention_context = wrapper_state.attention\n        outputs = AttentionRNNDecoderOutput(\n            logits, sample_ids, wrapper_outputs,\n            attention_scores, attention_context)\n        next_state = wrapper_state\n\n        return outputs, next_state\n\n    def next_inputs(self, helper: Helper, time: int,\n                    outputs: AttentionRNNDecoderOutput) -> \\\n            Tuple[torch.Tensor, torch.ByteTensor]:\n        finished, next_inputs = helper.next_inputs(\n            self.embed_tokens, time, outputs.logits, outputs.sample_id)\n        return next_inputs, finished\n\n    def forward(  # type: ignore\n            self,\n            memory: torch.Tensor,\n            memory_sequence_length: Optional[torch.LongTensor] = None,\n            inputs: Optional[torch.Tensor] = None,\n            sequence_length: Optional[torch.LongTensor] = None,\n            initial_state: Optional[MaybeList[MaybeTuple[torch.Tensor]]] = None,\n            helper: Optional[Helper] = None,\n            max_decoding_length: Optional[int] = None,\n            impute_finished: bool = False,\n            infer_mode: Optional[bool] = None,\n            beam_width: Optional[int] = None,\n            length_penalty: float = 0., **kwargs) \\\n            -> Union[Tuple[AttentionRNNDecoderOutput,\n                           Optional[AttentionWrapperState], torch.LongTensor],\n                     Dict[str, torch.Tensor]]:\n        r""""""Performs decoding.\n\n        Implementation calls initialize() once and step() repeatedly on the\n        Decoder object. Please refer to `tf.contrib.seq2seq.dynamic_decode`.\n\n        See Also:\n            Arguments of :meth:`create_helper`.\n\n        Args:\n            memory: The memory to query; usually the output of an RNN encoder.\n                This tensor should be shaped `[batch_size, max_time, ...]`.\n            memory_sequence_length: (optional) Sequence lengths for the batch\n                entries in memory.  If provided, the memory tensor rows are\n                masked with zeros for values past the respective sequence\n                lengths.\n            inputs (optional): Input tensors for teacher forcing decoding.\n                Used when :attr:`decoding_strategy` is set to\n                ``""train_greedy""``, or when `hparams`-configured helper is used.\n\n                The attr:`inputs` is a :tensor:`LongTensor` used as index to\n                look up embeddings and feed in the decoder. For example, if\n                :attr:`embedder` is an instance of\n                :class:`~texar.torch.modules.WordEmbedder`, then :attr:`inputs`\n                is usually a 2D int Tensor `[batch_size, max_time]` (or\n                `[max_time, batch_size]` if `input_time_major` == `True`)\n                containing the token indexes.\n            sequence_length (optional): A 1D int Tensor containing the\n                sequence length of :attr:`inputs`.\n                Used when `decoding_strategy=""train_greedy""` or\n                `hparams`-configured helper is used.\n            initial_state (optional): Initial state of decoding.\n                If `None` (default), zero state is used.\n            helper (optional): An instance of\n                :class:`~texar.torch.modules.Helper`\n                that defines the decoding strategy. If given,\n                ``decoding_strategy`` and helper configurations in\n                :attr:`hparams` are ignored.\n            max_decoding_length: A int scalar Tensor indicating the maximum\n                allowed number of decoding steps. If `None` (default), either\n                `hparams[""max_decoding_length_train""]` or\n                `hparams[""max_decoding_length_infer""]` is used\n                according to :attr:`mode`.\n            impute_finished (bool): If `True`, then states for batch\n                entries which are marked as finished get copied through and\n                the corresponding outputs get zeroed out.  This causes some\n                slowdown at each time step, but ensures that the final state\n                and outputs have the correct values and that backprop ignores\n                time steps that were marked as finished.\n            infer_mode (optional): If not `None`, overrides mode given by\n                `self.training`.\n            beam_width (int): Set to use beam search. If given,\n                :attr:`decoding_strategy` is ignored.\n            length_penalty (float): Length penalty coefficient used in beam\n                search decoding. Refer to https://arxiv.org/abs/1609.08144\n                for more details.\n                It should be larger if longer sentences are desired.\n            **kwargs: Other keyword arguments for constructing helpers\n                defined by ``hparams[""helper_train""]`` or\n                ``hparams[""helper_infer""]``.\n\n        Returns:\n\n            - For **beam search** decoding, returns a ``dict`` containing keys\n              ``""sample_id""`` and ``""log_prob""``.\n\n                - ``""sample_id""`` is a :tensor:`LongTensor` of shape\n                  ``[batch_size, max_time, beam_width]`` containing generated\n                  token indexes. ``sample_id[:,:,0]`` is the highest-probable\n                  sample.\n                - ``""log_prob""`` is a :tensor:`Tensor` of shape\n                  ``[batch_size, beam_width]`` containing the log probability\n                  of each sequence sample.\n\n            - For **""infer_greedy""** and **""infer_sample""** decoding or\n              decoding with :attr:`helper`, returns\n              a tuple `(outputs, final_state, sequence_lengths)`, where\n\n                - **outputs**: an object containing the decoder output on all\n                  time steps.\n                - **final_state**: is the cell state of the final time step.\n                - **sequence_lengths**: is an int Tensor of shape `[batch_size]`\n                  containing the length of each sample.\n        """"""\n        # TODO: Add faster code path for teacher-forcing training.\n\n        # Save memory and memory_sequence_length\n        self.memory = memory\n        self.memory_sequence_length = memory_sequence_length\n\n        # Maximum decoding length\n        if max_decoding_length is None:\n            if self.training:\n                max_decoding_length = self._hparams.max_decoding_length_train\n            else:\n                max_decoding_length = self._hparams.max_decoding_length_infer\n            if max_decoding_length is None:\n                max_decoding_length = utils.MAX_SEQ_LENGTH\n\n        # Beam search decode\n        if beam_width is not None and beam_width > 1:\n            if helper is not None:\n                raise ValueError(""Must not set \'beam_width\' and \'helper\' ""\n                                 ""simultaneously."")\n\n            start_tokens = kwargs.get(\'start_tokens\')\n            if start_tokens is None:\n                raise ValueError(""\'start_tokens\' must be specified when using""\n                                 ""beam search decoding."")\n\n            batch_size = start_tokens.shape[0]\n            state = self._cell.zero_state(batch_size)\n            if initial_state is not None:\n                state = state._replace(cell_state=initial_state)\n\n            end_token = kwargs.get(\'end_token\')\n            assert isinstance(end_token, int)\n\n            sample_id, log_prob = self.beam_decode(\n                start_tokens=start_tokens,\n                end_token=end_token,\n                initial_state=state,\n                beam_width=beam_width,\n                length_penalty=length_penalty,\n                decode_length=max_decoding_length)\n\n            # Release memory and memory_sequence_length in AttentionRNNDecoder\n            self.memory = None\n            self.memory_sequence_length = None\n\n            # Release the cached memory in AttentionMechanism\n            for attention_mechanism in self._cell.attention_mechanisms:\n                attention_mechanism.clear_cache()\n\n            return {""sample_id"": sample_id,\n                    ""log_prob"": log_prob}\n\n        # Helper\n        if helper is None:\n            helper = self._create_or_get_helper(infer_mode, **kwargs)\n\n        if (isinstance(helper, decoder_helpers.TrainingHelper) and\n                (inputs is None or sequence_length is None)):\n            raise ValueError(""\'input\' and \'sequence_length\' must not be None ""\n                             ""when using \'TrainingHelper\'."")\n\n        # Initial state\n        self._cell.init_batch()\n\n        (outputs, final_state,\n         sequence_lengths) = self.dynamic_decode(\n            helper, inputs, sequence_length, initial_state,  # type: ignore\n            max_decoding_length, impute_finished)\n\n        # Release memory and memory_sequence_length in AttentionRNNDecoder\n        self.memory = None\n        self.memory_sequence_length = None\n\n        # Release the cached memory in AttentionMechanism\n        for attention_mechanism in self._cell.attention_mechanisms:\n            attention_mechanism.clear_cache()\n\n        return outputs, final_state, sequence_lengths\n\n    def beam_decode(self,\n                    start_tokens: torch.LongTensor,\n                    end_token: int,\n                    initial_state: AttentionWrapperState,\n                    decode_length: int = 256,\n                    beam_width: int = 5,\n                    length_penalty: float = 0.6) \\\n            -> Tuple[torch.LongTensor, torch.Tensor]:\n\n        def _prepare_beam_search(x):\n            x = x.unsqueeze(1).repeat(1, beam_width, *([1] * (x.dim() - 1)))\n            x = x.view(-1, *x.size()[2:])\n            return x\n\n        memory_beam_search = _prepare_beam_search(self.memory)\n        memory_sequence_length_beam_search = _prepare_beam_search(\n            self.memory_sequence_length)\n\n        def _symbols_to_logits_fn(ids, state):\n            batch_size = ids.size(0)\n            step = ids.size(-1) - 1\n            times = ids.new_full((batch_size,), step)\n            inputs = self.embed_tokens(ids[:, -1], times)\n            wrapper_outputs, wrapper_state = self._cell(\n                inputs, state, memory_beam_search,\n                memory_sequence_length_beam_search)\n            logits = self._output_layer(wrapper_outputs)\n            return logits, wrapper_state\n\n        assert self._vocab_size is not None\n        outputs, log_prob = beam_search(\n            symbols_to_logits_fn=_symbols_to_logits_fn,\n            initial_ids=start_tokens,\n            beam_size=beam_width,\n            decode_length=decode_length,\n            vocab_size=self._vocab_size,\n            alpha=length_penalty,\n            states=initial_state,\n            eos_id=end_token)\n\n        # Ignores <BOS>\n        outputs = outputs[:, :, 1:]\n        # shape = [batch_size, seq_length, beam_width]\n        outputs = outputs.permute((0, 2, 1))\n        return outputs, log_prob\n\n    @property\n    def output_size(self):\n        r""""""Output size of one step.\n        """"""\n        return self._cell.output_size\n'"
texar/torch/modules/decoders/t5_decoder.py,20,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Union\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.modules.encoders.multihead_attention import Cache\nfrom texar.torch.modules.pretrained.t5_utils import \\\n    T5LayerNorm, MultiheadRPRAttention\nfrom texar.torch.modules.decoders.transformer_decoders import \\\n    TokenEmbedder, TokenPosEmbedder\nfrom texar.torch.modules.encoders.transformer_encoder import \\\n    default_transformer_poswise_net_hparams\nfrom texar.torch.modules.decoders.transformer_decoders \\\n    import TransformerDecoder\nfrom texar.torch.modules.networks.networks import FeedForwardNetwork\n\n\nclass T5Decoder(TransformerDecoder):\n    r""""""T5 decoder that applies multi-head self-attention with relative\n    position representation for sequence decoding.\n\n    It is a stack of\n    :class:`~texar.torch.modules.pretrained.t5_utilsMultiheadRPRAttention`,\n    :class:`~texar.torch.modules.FeedForwardNetwork`, and residual connections.\n\n    Args:\n        token_embedder: An instance of :torch_nn:`Module`, or a function taking\n            a :tensor:`LongTensor` ``tokens`` as argument. This is the embedder\n            called in :meth:`embed_tokens` to convert input tokens to\n            embeddings.\n        token_pos_embedder: An instance of :torch_nn:`Module`, or a function\n            taking two :tensor:`LongTensor`\\ s ``tokens`` and ``positions`` as\n            argument. This is the embedder called in :meth:`embed_tokens` to\n            convert input tokens with positions to embeddings.\n\n            .. note::\n                Only one among :attr:`token_embedder` and\n                :attr:`token_pos_embedder` should be specified. If neither is\n                specified, you must subclass :class:`TransformerDecoder` and\n                override :meth:`embed_tokens`.\n        vocab_size (int, optional): Vocabulary size. Required if\n            :attr:`output_layer` is `None`.\n        output_layer (optional): An output layer that transforms cell output\n            to logits. This can be:\n\n            - A callable layer, e.g., an instance of :torch_nn:`Module`.\n            - A tensor. A :torch_nn:`Linear` layer will be created using the\n              tensor as weights. The bias of the dense layer is determined\n              by ``hparams.output_layer_bias``. This can be used to tie the\n              output layer with the input embedding matrix, as proposed in\n              https://arxiv.org/pdf/1608.05859.pdf.\n            - `None`. A :torch_nn:`Linear` layer will be created based on\n              :attr:`vocab_size` and ``hparams.output_layer_bias``.\n            - If no output layer is needed at the end, set\n              :attr:`vocab_size` to `None` and ``output_layer`` to\n              :func:`~texar.torch.core.identity`.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    .. document private functions\n    """"""\n\n    # State variables used during `dynamic_decode`. Assigned in `forward`.\n    _state_max_decoding_length: int\n    _state_context: Optional[torch.LongTensor]\n    _state_context_sequence_length: Optional[torch.LongTensor]\n    _state_cache: Cache\n\n    def __init__(self,\n                 token_embedder: Optional[TokenEmbedder] = None,\n                 token_pos_embedder: Optional[TokenPosEmbedder] = None,\n                 vocab_size: Optional[int] = None,\n                 output_layer: Optional[Union[nn.Module, torch.Tensor]] = None,\n                 hparams=None):\n        super().__init__(\n            token_embedder, token_pos_embedder,\n            vocab_size=vocab_size, output_layer=output_layer, hparams=hparams)\n\n        self.final_layer_norm = T5LayerNorm(self._input_size,  # type: ignore\n                                            eps=self._hparams.eps)\n\n    def initialize_blocks(self):\n        r""""""Helper function to initialize blocks.\n        """"""\n        for i in range(self._hparams.num_blocks):\n            attn_module = MultiheadRPRAttention(\n                self._input_size,\n                self._hparams.multihead_attention,\n                stores_relative_position=bool(i == 0))\n            if self._hparams.dim != attn_module.output_size:\n                raise ValueError(""The output dimension of ""\n                                 ""MultiheadRPRAttention should be equal ""\n                                 ""to the dim of T5Decoder"")\n            self.self_attns.append(attn_module)\n            self.self_attn_layer_norm.append(\n                T5LayerNorm(self._input_size, eps=self._hparams.eps))\n\n            attn_module = MultiheadRPRAttention(\n                self._input_size, self._hparams.multihead_attention,\n                stores_relative_position=bool(i == 0)\n            )\n            if self._hparams.dim != attn_module.output_size:\n                raise ValueError(""The output dimension of ""\n                                 ""MultiheadRPRAttention should be equal ""\n                                 ""to the dim of T5Decoder"")\n            self.enc_dec_attns.append(attn_module)\n            self.end_dec_attn_layer_norm.append(\n                T5LayerNorm(self._input_size, eps=self._hparams.eps))\n\n            poswise_network = FeedForwardNetwork(\n                hparams=self._hparams.poswise_feedforward)\n            if (poswise_network.hparams.layers[-1][\'kwargs\'][\'out_features\']\n                    != self._hparams.dim):\n                raise ValueError(""The output dimension of ""\n                                 ""FeedForwardNetwork should be equal ""\n                                 ""to the dim of T5Decoder"")\n            self.poswise_networks.append(poswise_network)\n            self.poswise_layer_norm.append(\n                T5LayerNorm(self._input_size, eps=self._hparams.eps))\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # Same as in T5Encoder\n                ""num_blocks"": 6,\n                ""dim"": 512,\n                ""embedding_dropout"": 0.1,\n                ""residual_dropout"": 0.1,\n                ""poswise_feedforward"": default_transformer_poswise_net_hparams,\n                ""multihead_attention"": {\n                    \'name\': \'multihead_rpr_attention\',\n                    \'num_units\': 512,\n                    \'output_dim\': 512,\n                    \'num_heads\': 8,\n                    \'dropout_rate\': 0.1,\n                    \'use_bias\': False,\n                    \'is_decoder\': True,\n                    \'relative_attention_num_buckets\': 32\n                },\n                ""initializer"": None,\n                ""eps"": 1e-6,\n                ""name"": ""t5_decoder""\n\n                # Additional for TransformerDecoder\n                ""embedding_tie"": True,\n                ""output_layer_bias"": False,\n                ""max_decoding_length"": int(1e10),\n            }\n\n        Here:\n\n        `""num_blocks""`: int\n            Number of stacked blocks.\n\n        `""dim""`: int\n            Hidden dimension of the encoder.\n\n        `""embedding_dropout""`: float\n            Dropout rate of the input embedding.\n\n        `""residual_dropout""`: float\n            Dropout rate of the residual connections.\n\n        `""poswise_feedforward""`: dict\n            Hyperparameters for a feed-forward network used in residual\n            connections.\n            Make sure the dimension of the output tensor is equal to ``dim``.\n\n            See\n            :func:`~texar.torch.modules.default_transformer_poswise_net_hparams`\n            for details.\n\n        `""multihead_attention""`: dict\n            Hyperparameters for the multi-head attention strategy.\n            Make sure the ``output_dim`` in this module is equal to ``dim``.\n\n            See :class:`~texar.torch.modules.MultiheadRPRAttention`\n            for details.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""embedding_tie""`: bool\n            Whether to use the word embedding matrix as the output layer\n            that computes logits. If `False`, a new dense layer is created.\n\n        `""eps""`: float\n            Epsilon values for layer norm layers.\n\n        `""output_layer_bias""`: bool\n            Whether to use bias to the output layer.\n\n        `""max_decoding_length""`: int\n            The maximum allowed number of decoding steps.\n            Set to a very large number of avoid the length constraint.\n            Ignored if provided in :meth:`forward` or ``""train_greedy""``\n            decoding is used.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n        dim = 512\n        return {\n            \'num_blocks\': 6,\n            \'dim\': dim,\n            \'embedding_tie\': True,\n            \'output_layer_bias\': False,\n            \'max_decoding_length\': int(1e10),\n            \'embedding_dropout\': 0.1,\n            \'residual_dropout\': 0.1,\n            \'poswise_feedforward\': default_transformer_poswise_net_hparams(dim),\n            \'multihead_attention\': {\n                \'name\': \'multihead_rpr_attention\',\n                \'num_units\': 512,\n                \'num_heads\': 8,\n                \'dropout_rate\': 0.1,\n                \'output_dim\': 512,\n                \'use_bias\': False,\n                \'is_decoder\': True,\n                \'relative_attention_num_buckets\': 32\n            },\n            \'eps\': 1e-6,\n            \'initializer\': None,\n            \'name\': ""t5_decoder"",\n        }\n\n    def _self_attention_stack(\n            self, inputs: torch.Tensor,\n            memory: Optional[torch.Tensor],\n            decoder_self_attention_bias: Optional[torch.Tensor] = None,\n            memory_attention_bias: Optional[torch.Tensor] = None,\n            cache: Optional[Cache] = None\n    ) -> torch.Tensor:\n        r""""""Forward through the stacked multi-head rpr attentions.\n        """"""\n        if cache is not None:\n            if memory is not None:\n                memory_attention_bias = cache[\'memory_attention_bias\']\n        else:\n            assert decoder_self_attention_bias is not None\n\n        x = self.embed_dropout(inputs)\n        position_bias = None\n        encdec_position_bias = None\n        for i in range(self._hparams.num_blocks):\n            layer_cache = cache[\'layers\'][i] if cache is not None else None\n\n            selfatt_output, position_bias = self.self_attns[i](\n                queries=self.self_attn_layer_norm[i](x),\n                memory=None,\n                memory_attention_bias=decoder_self_attention_bias,\n                cache=layer_cache,\n                position_bias=position_bias\n            )\n\n            x = x + self.residual_dropout(selfatt_output)\n\n            if memory is not None:\n                encdec_output, encdec_position_bias = self.enc_dec_attns[i](\n                    queries=self.end_dec_attn_layer_norm[i](x),\n                    memory=memory,\n                    memory_attention_bias=memory_attention_bias,\n                    position_bias=encdec_position_bias\n                )\n\n                x = x + self.residual_dropout(encdec_output)\n\n            sub_output = self.poswise_networks[i](self.poswise_layer_norm[i](x))\n            x = x + self.residual_dropout(sub_output)\n\n        return self.final_layer_norm(x)\n'"
texar/torch/modules/decoders/transformer_decoders.py,69,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTransformer decoder.\n""""""\nimport warnings\nfrom typing import Callable, Dict, NamedTuple, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.core import layers\nfrom texar.torch.modules.decoders.decoder_base import (\n    DecoderBase, TokenEmbedder, TokenPosEmbedder, _make_output_layer)\nfrom texar.torch.modules.decoders.decoder_helpers import (\n    EmbeddingHelper, Helper)\nfrom texar.torch.modules.encoders.multihead_attention import (\n    Cache, MultiheadAttentionEncoder)\nfrom texar.torch.modules.encoders.transformer_encoder import (\n    default_transformer_poswise_net_hparams)\nfrom texar.torch.modules.networks.networks import FeedForwardNetwork\nfrom texar.torch.utils import transformer_attentions as attn\nfrom texar.torch.utils.beam_search import beam_search\nfrom texar.torch.utils.shapes import mask_sequences\nfrom texar.torch.utils.utils import sequence_mask\n\n__all__ = [\n    \'TransformerDecoderOutput\',\n    \'TransformerDecoder\',\n]\n\nEmbeddingFn = Callable[[torch.LongTensor, torch.LongTensor], torch.Tensor]\n\n\nclass TransformerDecoderOutput(NamedTuple):\n    r""""""The output of :class:`TransformerDecoder`.\n    """"""\n    logits: torch.Tensor\n    r""""""A :tensor:`Tensor` of shape ``[batch_size, max_time, vocab_size]``\n    containing the logits.""""""\n    sample_id: torch.LongTensor\n    r""""""A :tensor:`LongTensor` of shape ``[batch_size, max_time]``\n    (or ``[batch_size, max_time, vocab_size]``) containing the sampled\n    token indices. Note that the shape of ``sample_id`` is different for\n    different decoding strategy or helper. Please refer to\n    :class:`~texar.torch.modules.Helper` for the detailed information.""""""\n\n\nclass TransformerDecoder(DecoderBase[Cache, TransformerDecoderOutput]):\n    r""""""Transformer decoder that applies multi-head self-attention for\n    sequence decoding.\n\n    It is a stack of\n    :class:`~texar.torch.modules.MultiheadAttentionEncoder`,\n    :class:`~texar.torch.modules.FeedForwardNetwork`, and residual connections.\n\n    Args:\n        token_embedder: An instance of :torch_nn:`Module`, or a function taking\n            a :tensor:`LongTensor` ``tokens`` as argument. This is the embedder\n            called in :meth:`embed_tokens` to convert input tokens to\n            embeddings.\n        token_pos_embedder: An instance of :torch_nn:`Module`, or a function\n            taking two :tensor:`LongTensor`\\ s ``tokens`` and ``positions`` as\n            argument. This is the embedder called in :meth:`embed_tokens` to\n            convert input tokens with positions to embeddings.\n\n            .. note::\n                Only one among :attr:`token_embedder` and\n                :attr:`token_pos_embedder` should be specified. If neither is\n                specified, you must subclass :class:`TransformerDecoder` and\n                override :meth:`embed_tokens`.\n        vocab_size (int, optional): Vocabulary size. Required if\n            :attr:`output_layer` is `None`.\n        output_layer (optional): An output layer that transforms cell output\n            to logits. This can be:\n\n            - A callable layer, e.g., an instance of :torch_nn:`Module`.\n            - A tensor. A :torch_nn:`Linear` layer will be created using the\n              tensor as weights. The bias of the dense layer is determined\n              by ``hparams.output_layer_bias``. This can be used to tie the\n              output layer with the input embedding matrix, as proposed in\n              https://arxiv.org/pdf/1608.05859.pdf.\n            - `None`. A :torch_nn:`Linear` layer will be created based on\n              :attr:`vocab_size` and ``hparams.output_layer_bias``.\n            - If no output layer is needed at the end, set\n              :attr:`vocab_size` to `None` and ``output_layer`` to\n              :func:`~texar.torch.core.identity`.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    .. document private functions\n    """"""\n\n    # State variables used during `dynamic_decode`. Assigned in `forward`.\n    _state_max_decoding_length: int\n    _state_context: Optional[torch.LongTensor]\n    _state_context_sequence_length: Optional[torch.LongTensor]\n    _state_cache: Cache\n\n    def __init__(self,\n                 token_embedder: Optional[TokenEmbedder] = None,\n                 token_pos_embedder: Optional[TokenPosEmbedder] = None,\n                 vocab_size: Optional[int] = None,\n                 output_layer: Optional[Union[nn.Module, torch.Tensor]] = None,\n                 hparams=None):\n        super().__init__(\n            token_embedder, token_pos_embedder,\n            input_time_major=False, output_time_major=False, hparams=hparams)\n\n        if token_pos_embedder is None and token_embedder is not None:\n            warnings.warn(\n                ""Transformer models cannot capture positional information if ""\n                ""no positional embedding is provided."")\n\n        self._input_size = self._hparams.dim\n        self._output_layer, self._vocab_size = _make_output_layer(\n            output_layer, vocab_size, self._input_size,\n            self._hparams.output_layer_bias)\n\n        self.self_attns = nn.ModuleList()\n        self.self_attn_layer_norm = nn.ModuleList()\n        self.enc_dec_attns = nn.ModuleList()\n        self.end_dec_attn_layer_norm = nn.ModuleList()\n        self.poswise_networks = nn.ModuleList()\n        self.poswise_layer_norm = nn.ModuleList()\n\n        self.initialize_blocks()\n\n        self.final_layer_norm = nn.LayerNorm(self._input_size,\n                                             eps=self._hparams.eps)\n\n        self.embed_dropout = nn.Dropout(self._hparams.embedding_dropout)\n        self.residual_dropout = nn.Dropout(self._hparams.residual_dropout)\n\n        if self._hparams.initializer:\n            # TODO: This might be different to what TensorFlow does\n            initialize = layers.get_initializer(self._hparams.initializer)\n            assert initialize is not None\n            # Do not re-initialize LayerNorm modules.\n            for name, param in self.named_parameters():\n                if name.split(""."")[-1] == ""weight"" and ""layer_norm"" not in name:\n                    initialize(param)\n\n    def initialize_blocks(self):\n        r""""""Helper function which initializes blocks for decoder.\n\n        Should be overridden by any classes where block initialization varies.\n        """"""\n        for _ in range(self._hparams.num_blocks):\n            attn_module = MultiheadAttentionEncoder(\n                self._input_size, self._hparams.multihead_attention)\n            if self._hparams.dim != attn_module.output_size:\n                raise ValueError(""The output dimension of ""\n                                 ""MultiheadEncoder should be equal ""\n                                 ""to the dim of TransformerDecoder"")\n            self.self_attns.append(attn_module)\n            self.self_attn_layer_norm.append(\n                nn.LayerNorm(self._input_size, eps=self._hparams.eps))\n\n            attn_module = MultiheadAttentionEncoder(\n                self._input_size, self._hparams.multihead_attention)\n            if self._hparams.dim != attn_module.output_size:\n                raise ValueError(""The output dimension of ""\n                                 ""MultiheadEncoder should be equal ""\n                                 ""to the dim of TransformerDecoder"")\n            self.enc_dec_attns.append(attn_module)\n            self.end_dec_attn_layer_norm.append(\n                nn.LayerNorm(self._input_size, eps=self._hparams.eps))\n\n            poswise_network = FeedForwardNetwork(\n                hparams=self._hparams.poswise_feedforward)\n            if (poswise_network.hparams.layers[-1][\'kwargs\'][\'out_features\']\n                    != self._hparams.dim):\n                raise ValueError(""The output dimension of ""\n                                 ""FeedForwardNetwork should be equal ""\n                                 ""to the dim of TransformerDecoder"")\n            self.poswise_networks.append(poswise_network)\n            self.poswise_layer_norm.append(\n                nn.LayerNorm(self._input_size, eps=self._hparams.eps))\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # Same as in TransformerEncoder\n                ""num_blocks"": 6,\n                ""dim"": 512,\n                ""embedding_dropout"": 0.1,\n                ""residual_dropout"": 0.1,\n                ""poswise_feedforward"": default_transformer_poswise_net_hparams,\n                ""multihead_attention"": {\n                    \'name\': \'multihead_attention\',\n                    \'num_units\': 512,\n                    \'output_dim\': 512,\n                    \'num_heads\': 8,\n                    \'dropout_rate\': 0.1,\n                    \'use_bias\': False,\n                },\n                ""eps"": 1e-12,\n                ""initializer"": None,\n                ""name"": ""transformer_decoder""\n\n                # Additional for TransformerDecoder\n                ""embedding_tie"": True,\n                ""output_layer_bias"": False,\n                ""max_decoding_length"": int(1e10),\n            }\n\n        Here:\n\n        `""num_blocks""`: int\n            Number of stacked blocks.\n\n        `""dim""`: int\n            Hidden dimension of the encoder.\n\n        `""embedding_dropout""`: float\n            Dropout rate of the input word and position embeddings.\n\n        `""residual_dropout""`: float\n            Dropout rate of the residual connections.\n\n        `""poswise_feedforward""`: dict\n            Hyperparameters for a feed-forward network used in residual\n            connections.\n            Make sure the dimension of the output tensor is equal to ``dim``.\n\n            See\n            :func:`~texar.torch.modules.default_transformer_poswise_net_hparams`\n            for details.\n\n        `""multihead_attention""`: dict\n            Hyperparameters for the multi-head attention strategy.\n            Make sure the ``output_dim`` in this module is equal to ``dim``.\n\n            See :class:`~texar.torch.modules.MultiheadAttentionEncoder`\n            for details.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""embedding_tie""`: bool\n            Whether to use the word embedding matrix as the output layer\n            that computes logits. If `False`, a new dense layer is created.\n\n        `""eps""`: float\n            Epsilon values for layer norm layers.\n\n        `""output_layer_bias""`: bool\n            Whether to use bias to the output layer.\n\n        `""max_decoding_length""`: int\n            The maximum allowed number of decoding steps.\n            Set to a very large number of avoid the length constraint.\n            Ignored if provided in :meth:`forward` or ``""train_greedy""``\n            decoding is used.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n        dim = 512\n        return {\n            \'num_blocks\': 6,\n            \'dim\': dim,\n            \'embedding_tie\': True,\n            \'output_layer_bias\': False,\n            \'max_decoding_length\': int(1e10),\n            \'embedding_dropout\': 0.1,\n            \'residual_dropout\': 0.1,\n            \'poswise_feedforward\': default_transformer_poswise_net_hparams(dim),\n            \'multihead_attention\': {\n                \'name\': \'multihead_attention\',\n                \'num_units\': 512,\n                \'num_heads\': 8,\n                \'dropout_rate\': 0.1,\n                \'output_dim\': 512,\n                \'use_bias\': False,\n            },\n            \'eps\': 1e-12,\n            \'initializer\': None,\n            \'name\': ""transformer_decoder"",\n        }\n\n    def _inputs_to_outputs(self, inputs: torch.Tensor,\n                           cache: Cache) -> Tuple[torch.Tensor, Cache]:\n        r""""""Returns the outputs of one decoding step (for example,\n        the predicted logits of the next token).\n\n        :attr:`inputs` should be of shape ``[batch_size, dim]``.\n\n        Returns:\n            A tuple of logits and updated cache. Logits are of shape\n            ``[batch_size, vocab_size]``.\n        """"""\n        outputs = self._self_attention_stack(\n            inputs.unsqueeze(1), memory=cache[\'memory\'], cache=cache)\n        outputs = self._output_layer(outputs)\n        outputs = outputs.squeeze(1)\n        return outputs, cache\n\n    def forward(self,  # type: ignore\n                inputs: Optional[torch.Tensor] = None,\n                sequence_length: Optional[torch.LongTensor] = None,\n                memory: Optional[torch.Tensor] = None,\n                memory_sequence_length: Optional[torch.LongTensor] = None,\n                memory_attention_bias: Optional[torch.Tensor] = None,\n                context: Optional[torch.Tensor] = None,\n                context_sequence_length: Optional[torch.LongTensor] = None,\n                helper: Optional[Helper] = None,\n                decoding_strategy: str = \'train_greedy\',\n                max_decoding_length: Optional[int] = None,\n                impute_finished: bool = False,\n                infer_mode: Optional[bool] = None,\n                beam_width: Optional[int] = None,\n                length_penalty: float = 0.,\n                **kwargs) \\\n            -> Union[\n                TransformerDecoderOutput,\n                Tuple[TransformerDecoderOutput, torch.LongTensor],\n                Dict[str, torch.Tensor]]:\n        r""""""Performs decoding.\n\n        The interface is very similar to that of RNN decoders\n        (:class:`~texar.torch.modules.RNNDecoderBase`). In particular,\n        the function provides **3 ways** to specify the decoding method, with\n        varying flexibility:\n\n        1. The :attr:`decoding_strategy` argument.\n\n           - **""train_greedy""**: decoding in teacher-forcing fashion (i.e.,\n             feeding ground truth to decode the next step), and for each step\n             sample is obtained by taking the `argmax` of logits.\n             Argument :attr:`inputs` is required for this strategy.\n             :attr:`sequence_length` is optional.\n           - **""infer_greedy""**: decoding in inference fashion (i.e., feeding\n             `generated` sample to decode the next step), and for each step\n             sample is obtained by taking the `argmax` of logits.\n             Arguments :attr:`(start_tokens, end_token)` are\n             required for this strategy, and argument\n             :attr:`max_decoding_length` is optional.\n           - **""infer_sample""**: decoding in inference fashion, and for each\n             step sample is obtained by `random sampling` from the logits.\n             Arguments :attr:`(start_tokens, end_token)` are required for this\n             strategy, and argument :attr:`max_decoding_length` is optional.\n\n          This argument is used only when arguments :attr:`helper` and\n          :attr:`beam_width` are both `None`.\n\n        2. The :attr:`helper` argument: An instance of subclass of\n           :class:`~texar.torch.modules.Helper`.\n           This provides a superset of decoding strategies than above.\n           The interface is the same as in RNN decoders.\n           Please refer to :meth:`texar.torch.modules.RNNDecoderBase.forward`\n           for detailed usage and examples.\n\n           Note that, here, though using a\n           :class:`~texar.torch.modules.TrainingHelper` corresponding to the\n           ``""train_greedy""`` strategy above, the implementation is *slower*\n           than directly setting ``decoding_strategy=""train_greedy""`` (though\n           output results are the same).\n\n           Argument :attr:`max_decoding_length` is optional.\n\n        3. **Beam search**: set :attr:`beam_width` to use beam search decoding.\n           Arguments :attr:`(start_tokens, end_token)` are required,\n           and argument :attr:`max_decoding_length` is optional.\n\n        Args:\n            memory (optional): The memory to attend, e.g., the output of an RNN\n                encoder. A :tensor:`Tensor` of shape\n                ``[batch_size, memory_max_time, dim]``.\n            memory_sequence_length (optional): A :tensor:`Tensor` of shape\n                ``[batch_size]`` containing the sequence lengths for the batch\n                entries in memory. Used to create attention bias of\n                :attr:`memory_attention_bias` is not given. Ignored if\n                :attr:`memory_attention_bias` is provided.\n            memory_attention_bias (optional): A :tensor:`Tensor` of shape\n                ``[batch_size, num_heads, memory_max_time, dim]``.\n                An attention bias typically sets the value of a padding\n                position to a large negative value for masking. If not given,\n                :attr:`memory_sequence_length` is used to automatically\n                create an attention bias.\n            inputs (optional): Input tensors for teacher forcing decoding.\n                Used when :attr:`decoding_strategy` is set to\n                ``""train_greedy""``, or when `hparams`-configured helper is used.\n\n                The attr:`inputs` is a :tensor:`LongTensor` used as index to\n                look up embeddings and feed in the decoder. For example, if\n                :attr:`embedder` is an instance of\n                :class:`~texar.torch.modules.WordEmbedder`, then :attr:`inputs`\n                is usually a 2D int Tensor `[batch_size, max_time]` (or\n                `[max_time, batch_size]` if `input_time_major` == `True`)\n                containing the token indexes.\n            sequence_length (optional): A :tensor:`LongTensor` of shape\n                ``[batch_size]``, containing the sequence length of\n                :attr:`inputs`. Tokens beyond the respective sequence length are\n                masked out.\n                Used when :attr:`decoding_strategy` is set to\n                ``""train_greedy""``.\n            decoding_strategy (str): A string specifying the decoding\n                strategy, including ``""train_greedy""``, ``""infer_greedy""``,\n                ``""infer_sample""``.\n                Different arguments are required based on the\n                strategy. See above for details. Ignored if\n                :attr:`beam_width` or :attr:`helper` is set.\n            beam_width (int): Set to use beam search. If given,\n                :attr:`decoding_strategy` is ignored.\n            length_penalty (float): Length penalty coefficient used in beam\n                search decoding. Refer to https://arxiv.org/abs/1609.08144\n                for more details.\n                It should be larger if longer sentences are desired.\n            context (optional): An :tensor:`LongTensor` of shape\n                ``[batch_size, length]``, containing the starting tokens for\n                decoding. If context is set, ``start_tokens`` of the\n                :class:`~texar.torch.modules.Helper` will be ignored.\n            context_sequence_length (optional): Specify the length of context.\n            max_decoding_length (int, optional): The maximum allowed number of\n                decoding steps.\n                If `None` (default), use ``""max_decoding_length""`` defined in\n                :attr:`hparams`. Ignored in ``""train_greedy""`` decoding.\n            impute_finished (bool): If `True`, then states for batch\n                entries which are marked as finished get copied through and\n                the corresponding outputs get zeroed out.  This causes some\n                slowdown at each time step, but ensures that the final state\n                and outputs have the correct values and that backprop ignores\n                time steps that were marked as finished. Ignored in\n                ``""train_greedy""`` decoding.\n            helper (optional): An instance of\n                :class:`~texar.torch.modules.Helper`\n                that defines the decoding strategy. If given,\n                ``decoding_strategy`` and helper configurations in\n                :attr:`hparams` are ignored.\n            infer_mode (optional): If not `None`, overrides mode given by\n                :attr:`self.training`.\n            **kwargs (optional, dict): Other keyword arguments. Typically ones\n                such as:\n\n                - **start_tokens**: A :tensor:`LongTensor` of shape\n                  ``[batch_size]``, the start tokens.\n                  Used when :attr:`decoding_strategy` is ``""infer_greedy""`` or\n                  ``""infer_sample""`` or when :attr:`beam_search` is set.\n                  Ignored when :attr:`context` is set.\n\n                  When used with the Texar data module, to get ``batch_size``\n                  samples where ``batch_size`` is changing according to the\n                  data module, this can be set as\n                  :python:`start_tokens=torch.full_like(batch[\'length\'],\n                  bos_token_id)`.\n\n                - **end_token**: An integer or 0D :tensor:`LongTensor`, the\n                  token that marks the end of decoding.\n                  Used when :attr:`decoding_strategy` is ``""infer_greedy""`` or\n                  ``""infer_sample""``, or when :attr:`beam_search` is set.\n\n        Returns:\n\n            - For **""train_greedy""** decoding, returns an instance of\n              :class:`~texar.torch.modules.TransformerDecoderOutput` which\n              contains `sample_id` and `logits`.\n\n            - For **""infer_greedy""** and **""infer_sample""** decoding or\n              decoding with :attr:`helper`, returns\n              a tuple ``(outputs, sequence_lengths)``, where ``outputs`` is an\n              instance of :class:`~texar.torch.modules.TransformerDecoderOutput`\n              as in `""train_greedy""`, and ``sequence_lengths`` is a\n              :tensor:`LongTensor` of shape ``[batch_size]`` containing the\n              length of each sample.\n\n            - For **beam search** decoding, returns a ``dict`` containing keys\n              ``""sample_id""`` and ``""log_prob""``.\n\n                - ``""sample_id""`` is a :tensor:`LongTensor` of shape\n                  ``[batch_size, max_time, beam_width]`` containing generated\n                  token indexes. ``sample_id[:,:,0]`` is the highest-probable\n                  sample.\n                - ``""log_prob""`` is a :tensor:`Tensor` of shape\n                  ``[batch_size, beam_width]`` containing the log probability\n                  of each sequence sample.\n        """"""\n\n        if memory is not None:\n            if memory_attention_bias is None:\n                if memory_sequence_length is None:\n                    raise ValueError(\n                        ""`memory_sequence_length` is required if ""\n                        ""`memory_attention_bias` is not given."")\n\n                enc_padding = 1 - sequence_mask(\n                    memory_sequence_length, memory.size(1),\n                    dtype=torch.float32)\n                memory_attention_bias = attn.attention_bias_ignore_padding(\n                    enc_padding)\n\n        # record the context, which will be used in step function\n        # for dynamic_decode\n        if context is not None:\n            if context_sequence_length is None:\n                raise ValueError(""\'context_sequence_length\' must not be None""\n                                 ""when \'context\' is specified."")\n            self._state_context = context[:, 1:]\n            self._state_context_sequence_length = context_sequence_length - 1\n        else:\n            self._state_context = None\n            self._state_context_sequence_length = None\n\n        # Faster code path for teacher-forcing training\n        if (helper is None and beam_width is None and\n                decoding_strategy == \'train_greedy\'):\n            if inputs is None:\n                raise ValueError(""\'input\' must not be none ""\n                                 ""when using \'train_greedy\' decoding strategy."")\n            times = torch.arange(\n                inputs.size(1), dtype=torch.long, device=inputs.device)\n            times = times.unsqueeze(0).expand(inputs.size(0), -1)\n            inputs = self.embed_tokens(inputs, times)\n            if sequence_length is not None:\n                inputs = mask_sequences(inputs, sequence_length)\n\n            decoder_self_attention_bias = (\n                attn.attention_bias_lower_triangle(inputs.size(1)))\n\n            decoder_output = self._self_attention_stack(\n                inputs, memory, decoder_self_attention_bias,\n                memory_attention_bias, cache=None)\n            logits = self._output_layer(decoder_output)\n            sample_id = torch.argmax(logits, dim=-1)\n\n            return TransformerDecoderOutput(logits, sample_id)\n\n        # Inference code path.\n        if max_decoding_length is None:\n            max_decoding_length = self._hparams.max_decoding_length\n\n        self._state_max_decoding_length = max_decoding_length\n\n        if beam_width is None or beam_width == 1:  # Inference-like decoding\n            # Prepare helper\n            if helper is None:\n                kwargs.update(decoding_strategy=decoding_strategy)\n                if context is not None:\n                    kwargs.update(start_tokens=context[:, 0])\n                helper = self._create_or_get_helper(infer_mode, **kwargs)\n            assert isinstance(helper, EmbeddingHelper)\n\n            self._state_cache = self._init_cache(\n                memory, memory_attention_bias,\n                beam_search_decoding=False, batch_size=helper.batch_size)\n            if context is not None:\n                assert self._state_context is not None\n                pad_length = max_decoding_length - self._state_context.size(1)\n                if pad_length > 0:\n                    self._state_context = torch.cat((\n                        self._state_context,\n                        self._state_context.new_zeros(\n                            self._state_context.size(0), pad_length)\n                    ), dim=1)\n\n            outputs, cache, sequence_lengths = self.dynamic_decode(\n                helper, inputs=None, sequence_length=None,\n                initial_state=None, max_decoding_length=max_decoding_length,\n                impute_finished=impute_finished)\n            del cache  # not used\n\n            if context is not None:\n                # Here the length of sample_id will be larger than that\n                # of logit by 1, because there will be a additional\n                # start_token in the returned sample_id.\n                # the start_id should be the first token of the\n                # given context\n                start_tokens = context[:, 0]\n                outputs = TransformerDecoderOutput(\n                    logits=outputs.logits,\n                    sample_id=torch.cat([\n                        start_tokens.unsqueeze(1),\n                        outputs.sample_id\n                    ], dim=1))\n                sequence_lengths = sequence_lengths + 1\n\n            return outputs, sequence_lengths\n\n        else:  # Beam-search decoding\n            # Ignore `decoding_strategy` and # assume `helper` is not set.\n            if helper is not None:\n                raise ValueError(""Must not set \'beam_width\' and \'helper\' ""\n                                 ""simultaneously."")\n            if context is not None:\n                start_tokens = context[:, 0]\n            else:\n                if \'start_tokens\' not in kwargs:\n                    raise ValueError(\n                        ""\'start_tokens\' must be specified when using""\n                        ""beam search decoding."")\n                start_tokens = kwargs[\'start_tokens\']\n            _batch_size = start_tokens.size(0)\n            self._state_cache = self._init_cache(\n                memory, memory_attention_bias,\n                beam_search_decoding=True,\n                batch_size=_batch_size)\n            end_token: int = kwargs.get(\'end_token\')  # type: ignore\n\n            # The output format is different when running beam search.\n            sample_id, log_prob = self.beam_decode(\n                start_tokens,\n                end_token,\n                embedding_fn=self.embed_tokens,\n                beam_width=beam_width,\n                length_penalty=length_penalty,\n                decode_length=max_decoding_length)\n\n            return {\n                \'sample_id\': sample_id,\n                \'log_prob\': log_prob\n            }\n\n    def _self_attention_stack(\n            self, inputs: torch.Tensor,\n            memory: Optional[torch.Tensor],\n            decoder_self_attention_bias: Optional[torch.Tensor] = None,\n            memory_attention_bias: Optional[torch.Tensor] = None,\n            cache: Optional[Cache] = None) -> torch.Tensor:\n        r""""""Forward through the stacked multi-head attentions.\n        """"""\n        inputs = self.embed_dropout(inputs)\n        if cache is not None:\n            if memory is not None:\n                memory_attention_bias = cache[\'memory_attention_bias\']\n        else:\n            assert decoder_self_attention_bias is not None\n\n        x = inputs\n        for i in range(self._hparams.num_blocks):\n            layer_cache = cache[\'layers\'][i] if cache is not None else None\n\n            selfatt_output = self.self_attns[i](\n                queries=self.self_attn_layer_norm[i](x),\n                memory=None,\n                memory_attention_bias=decoder_self_attention_bias,\n                cache=layer_cache)\n            x = x + self.residual_dropout(selfatt_output)\n\n            if memory is not None:\n                encdec_output = self.enc_dec_attns[i](\n                    queries=self.end_dec_attn_layer_norm[i](x),\n                    memory=memory,\n                    memory_attention_bias=memory_attention_bias)\n                x = x + self.residual_dropout(encdec_output)\n\n            sub_output = self.poswise_networks[i](self.poswise_layer_norm[i](x))\n            x = x + self.residual_dropout(sub_output)\n\n        return self.final_layer_norm(x)\n\n    def _init_cache(self, memory: Optional[torch.Tensor],\n                    memory_attention_bias: Optional[torch.Tensor],\n                    beam_search_decoding: bool,\n                    batch_size: int) -> Cache:\n        r""""""Returns an initialized cache.\n\n        In order to support both inference-like decoding and beam-search\n        decoding, the elements of each layer must be initialized and extended\n        as different structure respectively. Specifically, for inference-like\n        decoding, a simple list is used; for beam-search decoding, a\n        :tensor:`Tensor` of shape ``[batch_size, current_steps, num_units]``\n        is maintained, where ``current_steps`` is the number of steps currently\n        decoded.\n        """"""\n\n        device = next(self.parameters()).device\n\n        def _create_ta():\n            return []\n\n        def _create_empty_tensor():\n            ret = torch.zeros(\n                batch_size, 0, self._hparams.multihead_attention.num_units,\n                dtype=torch.float, device=device)\n            return ret\n\n        _create_fn = (_create_empty_tensor if beam_search_decoding\n                      else _create_ta)\n\n        cache: Cache = {\n            \'memory\': memory,\n            \'memory_attention_bias\': memory_attention_bias,\n            \'layers\': [{\n                \'keys\': _create_fn(),\n                \'values\': _create_fn(),\n            } for _ in range(self._hparams.num_blocks)],\n        }\n\n        return cache\n\n    def beam_decode(self, start_tokens: torch.LongTensor, end_token: int,\n                    embedding_fn: Callable[\n                        [torch.LongTensor, torch.LongTensor], torch.Tensor],\n                    decode_length: int = 256, beam_width: int = 5,\n                    length_penalty: float = 0.6) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n\n        def _symbols_to_logits_fn(ids, cache):\n            batch_size = ids.size(0)\n            step = ids.size(-1) - 1\n            times = ids.new_full((batch_size,), step)\n            inputs = embedding_fn(ids[:, -1], times)\n            return self._inputs_to_outputs(inputs, cache)\n\n        assert self._vocab_size is not None\n\n        outputs, log_prob = beam_search(\n            _symbols_to_logits_fn,\n            start_tokens,\n            beam_width,\n            decode_length,\n            self._vocab_size,\n            length_penalty,\n            states=self._state_cache,\n            eos_id=end_token)\n\n        # Ignores <BOS>\n        outputs = outputs[:, :, 1:]\n        # shape = [batch_size, seq_length, beam_width]\n        outputs = outputs.permute(0, 2, 1)\n        return outputs, log_prob\n\n    @property\n    def output_size(self) -> int:\n        r""""""Output size of one step.\n        """"""\n        return self._input_size\n\n    def initialize(self, helper: Helper, inputs: Optional[torch.Tensor],\n                   sequence_length: Optional[torch.LongTensor],\n                   initial_state: Optional[Cache]) \\\n            -> Tuple[torch.ByteTensor, torch.Tensor, Cache]:\n        initial_finished, initial_inputs = helper.initialize(\n            self.embed_tokens, inputs, sequence_length)\n        state = initial_state or self._state_cache\n        return initial_finished, initial_inputs, state\n\n    def step(self, helper: Helper, time: int, inputs: torch.Tensor,\n             state: Optional[Cache]) -> \\\n            Tuple[TransformerDecoderOutput, Cache]:\n        assert state is not None\n        outputs, state = self._inputs_to_outputs(inputs, state)\n        sample_ids = helper.sample(time=time, outputs=outputs)\n        if self._state_context is not None:\n            assert self._state_context_sequence_length is not None\n            sample_ids = torch.where(\n                self._state_context_sequence_length > time,\n                self._state_context[:, time],\n                sample_ids)\n\n        next_state = state\n        outputs = TransformerDecoderOutput(\n            logits=outputs,\n            sample_id=sample_ids)\n        return outputs, next_state\n\n    def next_inputs(self, helper: Helper, time: int,\n                    outputs: TransformerDecoderOutput) -> \\\n            Tuple[torch.Tensor, torch.ByteTensor]:\n        finished, next_inputs = helper.next_inputs(\n            self.embed_tokens, time, outputs.logits, outputs.sample_id)\n        return next_inputs, finished\n\n    def finalize(self,  # type: ignore\n                 outputs: TransformerDecoderOutput,\n                 final_state: Optional[Cache],\n                 sequence_lengths: torch.LongTensor) \\\n            -> Tuple[TransformerDecoderOutput, Optional[Cache]]:\n        # Clear state variables at end of decoding.\n        del self._state_max_decoding_length\n        del self._state_context\n        del self._state_context_sequence_length\n        del self._state_cache\n\n        return super().finalize(outputs, final_state, sequence_lengths)\n'"
texar/torch/modules/decoders/xlnet_decoder.py,34,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Any, Dict, List, NamedTuple, Optional, Tuple, Type, Union\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom texar.torch.modules.decoders.decoder_base import DecoderBase\nfrom texar.torch.modules.decoders.decoder_helpers import (\n    Helper, SampleEmbeddingHelper)\nfrom texar.torch.modules.encoders.xlnet_encoder import XLNetEncoder\nfrom texar.torch.utils import get_instance\n\n__all__ = [\n    \'XLNetDecoderOutput\',\n    \'XLNetDecoder\',\n]\n\n\nclass XLNetDecoderOutput(NamedTuple):\n    r""""""The output of :class:`XLNetDecoder`.\n    """"""\n    logits: torch.Tensor\n    r""""""A :tensor:`Tensor` of shape ``[batch_size, max_time, vocab_size]``\n    containing the logits.""""""\n    sample_id: torch.LongTensor\n    r""""""A :tensor:`LongTensor` of shape ``[batch_size, max_time]``\n    (or ``[batch_size, max_time, vocab_size]``) containing the sampled token\n    indices. Note that the shape of ``sample_id`` is different for different\n    decoding strategy or helper. Please refer to\n    :class:`~texar.torch.modules.Helper` for the detailed information.""""""\n\n\nOutput = XLNetDecoderOutput\nState = List[torch.Tensor]\n\n\nclass XLNetDecoder(XLNetEncoder, DecoderBase[Optional[State], Output]):\n    r""""""Raw XLNet module for decoding sequences. Please see\n    :class:`~texar.torch.modules.PretrainedXLNetMixin` for a brief description\n    of XLNet.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``xlnet-based-cased``). Please refer to\n            :class:`~texar.torch.modules.PretrainedXLNetMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n    _IS_DECODE = True\n    # Variables persistent during decoding.\n    _state_cache_len: int\n    _state_recompute_memory: bool\n    # required for recomputing memory\n    _state_previous_inputs: List[torch.Tensor]\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The decoder arch is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the decoder arch is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the decoder arch is defined by the\n          configurations in `hparams` and weights are randomly initialized.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""xlnet-base-cased"",\n                ""untie_r"": True,\n                ""num_layers"": 12,\n                ""mem_len"": 0,\n                ""reuse_len"": 0,\n                ""num_heads"": 12,\n                ""hidden_dim"": 768,\n                ""head_dim"": 64,\n                ""dropout"": 0.1,\n                ""attention_dropout"": 0.1,\n                ""use_segments"": True,\n                ""ffn_inner_dim"": 3072,\n                ""activation"": \'gelu\',\n                ""vocab_size"": 32000,\n                ""max_seq_length"": 512,\n                ""initializer"": None,\n                ""name"": ""xlnet_decoder"",\n            }\n\n        Here:\n\n        The default parameters are values for cased XLNet-Base model.\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained XLNet model. If None, the model\n            will be randomly initialized.\n\n        `""untie_r""`: bool\n            Whether to untie the biases in attention.\n\n        `""num_layers""`: int\n            The number of stacked layers.\n\n        `""mem_len""`: int\n            The number of tokens to cache.\n\n        `""reuse_len""`: int\n            The number of tokens in the current batch to be cached and reused\n            in the future.\n\n        `""num_heads""`: int\n            The number of attention heads.\n\n        `""hidden_dim""`: int\n            The hidden size.\n\n        `""head_dim""`: int\n            The dimension size of each attention head.\n\n        `""dropout""`: float\n            Dropout rate.\n\n        `""attention_dropout""`: float\n            Dropout rate on attention probabilities.\n\n        `""use_segments""`: bool\n            Whether to use segment embedding.\n\n        `""ffn_inner_dim""`: int\n            The hidden size in feed-forward layers.\n\n        `""activation""`: str\n            `relu` or `gelu`.\n\n        `""vocab_size""`: int\n            The vocabulary size.\n\n        `""max_seq_length""`: int\n            The maximum sequence length for `RelativePositionalEncoding`.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n        return {\n            \'pretrained_model_name\': \'xlnet-base-cased\',\n            \'untie_r\': True,\n            \'num_layers\': 12,\n            \'mem_len\': 0,\n            \'reuse_len\': 0,\n            # layer\n            \'num_heads\': 12,\n            \'hidden_dim\': 768,\n            \'head_dim\': 64,\n            \'dropout\': 0.1,\n            \'attention_dropout\': 0.1,\n            \'use_segments\': True,\n            # ffn\n            \'ffn_inner_dim\': 3072,\n            \'activation\': \'gelu\',\n            # embedding\n            \'vocab_size\': 32000,\n            \'max_seq_length\': 512,\n            \'initializer\': None,\n            \'name\': ""xlnet_decoder"",\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    @staticmethod\n    def _create_input(inputs: List[torch.Tensor],\n                      initial: bool = False) \\\n            -> Dict[str, torch.Tensor]:\n        r""""""Create input tensors given the list of prompt tokens.\n        """"""\n        word_embed = torch.stack(inputs, dim=0)\n        seq_len, batch_size, embed_dim = word_embed.size()\n        if not initial:\n            # Add a dummy token at the end that stands for the token\n            # to predict.\n            word_embed = torch.cat([\n                word_embed,\n                word_embed.new_zeros(1, batch_size, embed_dim)\n            ], dim=0)\n            seq_len += 1\n        segment_ids = word_embed.new_zeros(\n            seq_len, batch_size, dtype=torch.long)\n        return_dict = {\n            ""word_embed"": word_embed.permute(1, 0, 2),\n            ""segment_ids"": segment_ids.permute(1, 0),\n        }\n\n        if not initial:\n            # Only the dummy token is considered target.\n            target_mapping = torch.cat([\n                torch.zeros(1, seq_len - 1, batch_size),\n                torch.ones(1, 1, batch_size)\n            ], dim=1).to(device=word_embed.device)\n            # Dummy token attends to nothing; actual tokens attend to all.\n            permute_mask = torch.cat([\n                torch.zeros(seq_len, seq_len - 1, batch_size),\n                torch.ones(seq_len, 1, batch_size),\n            ], dim=1).to(device=word_embed.device)\n            return_dict.update({\n                ""target_mapping"": target_mapping.permute(2, 0, 1),\n                ""permute_mask"": permute_mask.permute(2, 0, 1),\n            })\n\n        return return_dict\n\n    def embed_tokens(self, tokens: torch.LongTensor,\n                     positions: torch.LongTensor) -> torch.Tensor:  # pylint: disable=unused-argument\n        return self.word_embed(tokens)\n\n    def initialize(self,\n                   helper: Helper,\n                   inputs: Optional[torch.Tensor],\n                   sequence_length: Optional[torch.LongTensor],\n                   initial_state: Optional[State]) \\\n            -> Tuple[torch.ByteTensor, torch.Tensor, Optional[State]]:\n        initial_finished, initial_inputs = helper.initialize(\n            self.embed_tokens, inputs, sequence_length)\n        return initial_finished, initial_inputs, initial_state\n\n    def step(self, helper: Helper, time: int, inputs: torch.Tensor,\n             state: Optional[State]) -> \\\n            Tuple[Output, Optional[State]]:\n        self._state_previous_inputs.append(inputs)\n        if self._state_recompute_memory:\n            net_output, memory = self._forward(\n                two_stream=True,\n                **self._create_input(\n                    self._state_previous_inputs[-self._state_cache_len:]))\n        else:\n            assert state is not None\n            net_output, memory = self._forward(\n                memory=state, cache_len=self._state_cache_len, two_stream=True,\n                **self._create_input(self._state_previous_inputs[-1:]))\n            assert memory is not None\n            # Omit memory for the dummy token.\n            memory = [mem[:, :-1] for mem in memory]\n\n        logits = F.linear(net_output, self.word_embed.weight, self.lm_bias)\n        logits = logits[:, -1]\n        sample_ids = helper.sample(time=time, outputs=logits)\n        outputs = XLNetDecoderOutput(logits=logits, sample_id=sample_ids)\n        return outputs, memory\n\n    def next_inputs(self, helper: Helper, time: int,\n                    outputs: Output) -> \\\n            Tuple[torch.Tensor, torch.ByteTensor]:\n        finished, next_inputs = helper.next_inputs(\n            self.embed_tokens, time, outputs.logits, outputs.sample_id)\n        return next_inputs, finished\n\n    def finalize(self, outputs, final_state, sequence_lengths):\n        del self._state_cache_len\n        del self._state_recompute_memory\n        del self._state_previous_inputs\n        return super().finalize(outputs, final_state, sequence_lengths)\n\n    def forward(self,  # type: ignore\n                start_tokens: torch.LongTensor,\n                memory: Optional[State] = None,\n                cache_len: int = 512,\n                max_decoding_length: Optional[int] = 500,\n                recompute_memory: bool = True,\n                print_steps: bool = False,\n                helper_type: Optional[Union[str, Type[Helper]]] = None,\n                **helper_kwargs) \\\n            -> Tuple[Output, Optional[State]]:\n        r""""""Perform autoregressive decoding using XLNet. The algorithm is\n        largely inspired by: https://github.com/rusiaaman/XLNet-gen.\n\n        Args:\n            start_tokens: A LongTensor of shape `[batch_size, prompt_len]`,\n                representing the tokenized initial prompt.\n            memory (optional): The initial memory.\n            cache_len: Length of memory (number of tokens) to cache.\n            max_decoding_length (int): Maximum number of tokens to decode.\n            recompute_memory (bool): If `True`, the entire memory is recomputed\n                for each token to generate. This leads to better performance\n                because it enables every generated token to attend to each\n                other, compared to reusing previous memory which is equivalent\n                to using a causal attention mask. However, it is computationally\n                more expensive. Defaults to `True`.\n            print_steps (bool): If `True`, will print decoding progress.\n            helper: Type (or name of the type) of any sub-class of\n                :class:`~texar.torch.modules.Helper`.\n            helper_kwargs: The keyword arguments to pass to constructor of\n                the specific helper type.\n\n        :returns: A tuple of `(output, new_memory)`:\n            - **`output`**: The sampled tokens as a list of integers.\n            - **`new_memory`**: The memory of the sampled tokens.\n        """"""\n\n        start_tokens = start_tokens.t()\n        self._state_recompute_memory = recompute_memory\n        self._state_cache_len = cache_len\n        self._state_previous_inputs = list(\n            self.word_embed(start_tokens).unbind(dim=0))[:-1]\n\n        if helper_type is None:\n            helper_type = SampleEmbeddingHelper\n\n        if not recompute_memory and start_tokens.size(0) > 1:\n            _, memory = self._forward(\n                memory=memory, cache_len=cache_len,\n                **self._create_input(\n                    self._state_previous_inputs, initial=True))\n        start_tokens = start_tokens[-1]\n\n        helper_kwargs.update(start_tokens=start_tokens)\n\n        if helper_kwargs.get(""end_token"") is None:\n            raise ValueError(""\'end_token\' must be specified."")\n\n        helper = get_instance(\n            helper_type, helper_kwargs,\n            module_paths=[\'texar.torch.modules.decoders.decoder_helpers\'])\n\n        step_hook = None\n        if print_steps:\n            step_hook = lambda step: print(\n                f""\\033[2K\\rDecoding step: {step}"", end=\'\')\n        output, new_memory, _ = self.dynamic_decode(\n            helper, inputs=None, sequence_length=None, initial_state=memory,\n            max_decoding_length=max_decoding_length, step_hook=step_hook)\n        if print_steps:\n            print(""\\033[2K\\r"", end=\'\')\n\n        return output, new_memory\n'"
texar/torch/modules/embedders/__init__.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library embedders.\n""""""\n\nfrom texar.torch.modules.embedders.embedder_base import *\nfrom texar.torch.modules.embedders.embedders import *\nfrom texar.torch.modules.embedders.position_embedders import *\n'"
texar/torch/modules/embedders/embedder_base.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nThe base embedder class.\n""""""\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.module_base import ModuleBase\nfrom texar.torch.modules.embedders import embedder_utils\n\n__all__ = [\n    ""EmbedderBase"",\n    ""EmbeddingDropout"",\n]\n\n\nclass EmbedderBase(ModuleBase):\n    r""""""The base embedder class that all embedder classes inherit.\n\n    Args:\n        num_embeds (int, optional): The number of embedding elements, e.g.,\n            the vocabulary size of a word embedder.\n        init_value (Tensor or numpy array, optional): Initial values of the\n            embedding variable. If not given, embedding is initialized as\n            specified in ``hparams[""initializer""]``.\n        hparams (dict or HParams, optional): Embedder hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n    """"""\n\n    def __init__(self, num_embeds: Optional[int] = None,\n                 init_value: Optional[torch.Tensor] = None, hparams=None):\n        super().__init__(hparams=hparams)\n\n        if num_embeds is not None or init_value is not None:\n            self._embedding = nn.Parameter(embedder_utils.get_embedding(\n                num_embeds, init_value, hparams),\n                requires_grad=self.hparams[""trainable""])\n\n            self._num_embeds = self._embedding.size(0)\n\n            self._dim_rank = self._embedding.dim() - 1\n            if self._dim_rank == 1:\n                self._dim = self._embedding.size(1)\n            else:\n                self._dim = self._embedding.size()[1:]  # type: ignore\n\n    def _get_noise_shape(self, dropout_strategy: str,\n                         ids_rank: Optional[int] = None,\n                         dropout_input: Optional[torch.Tensor] = None) \\\n            -> Optional[Tuple[int, ...]]:\n\n        if dropout_strategy == \'element\':\n            noise_shape = None\n        elif dropout_strategy == \'item\':\n            assert dropout_input is not None\n            assert ids_rank is not None\n            shape_a = dropout_input.size()[:ids_rank]\n            shape_b = (1,) * self._dim_rank\n            noise_shape = shape_a + shape_b\n        elif dropout_strategy == \'item_type\':\n            noise_shape = (self._num_embeds,) + (1,) * self._dim_rank\n        else:\n            raise ValueError(f""Unknown dropout strategy: {dropout_strategy}"")\n        return noise_shape\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""embedder""\n            }\n        """"""\n        return {\n            ""name"": ""embedder""\n        }\n\n    @property\n    def num_embeds(self) -> int:\n        r""""""The number of embedding elements.\n        """"""\n        return self._num_embeds\n\n\nclass EmbeddingDropout(ModuleBase):\n    r""""""The dropout layer that used for the embedding.\n\n    Args:\n        rate (float, required): The dropout rate applied to the embedding.\n            For example, if rate is 0.1, 10% of the embedding will be zeroed\n            out. Set to 0 to disable dropout.\n\n        hparams (dict or HParams, optional): Embedder hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n    """"""\n\n    def __init__(self, rate: float, hparams=None):\n        super().__init__(hparams=hparams)\n        self._rate = rate\n\n    def forward(self,  # type: ignore\n                input_tensor: torch.Tensor,\n                noise_shape: Optional[torch.Size] = None) -> torch.Tensor:\n        r""""""Apply dropout on the tensor.\n\n        Args:\n            input_tensor: The tensor to apply dropout on.\n            noise_shape (list, optional): The shape of the noise mask which\n                specifies the dropout dimensions for the embedding.\n\n        Returns:\n            The tensor after applying dropout.\n        """"""\n        if not self.training or self._rate == 0.0:\n            return input_tensor\n        if noise_shape is None:\n            noise_shape = input_tensor.size()\n        keep_rate = 1 - self._rate\n        mask = input_tensor.new_full(noise_shape, keep_rate)\n        mask += input_tensor.new_empty(noise_shape).uniform_(0, 1)\n        mask = torch.floor(mask).div_(keep_rate)\n        return input_tensor * mask\n\n    @property\n    def output_size(self):\n        raise ValueError(""\'output_size\' can not be calculated ""\n                         ""because it is equal to the input size."")\n'"
texar/torch/modules/embedders/embedder_utils.py,14,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utils of embedder.\n""""""\nfrom typing import Optional\n\nimport torch\n\nfrom texar.torch.core import layers\nfrom texar.torch.hyperparams import HParams\n\n__all__ = [\n    ""default_embedding_hparams"",\n    ""get_embedding"",\n    ""soft_embedding_lookup"",\n]\n\n\ndef default_embedding_hparams():\n    r""""""Returns a ``dict`` of hyperparameters and default values of a embedder.\n\n     See :meth:`~texar.torch.modules.WordEmbedder.default_hparams` for details.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""embedding"",\n                ""dim"": 100,\n                ""initializer"": None,\n                ""dropout_rate"": 0.,\n                ""dropout_strategy"": \'element\',\n                ""trainable"": True,\n            }\n\n        Here:\n\n        `""name""`: str\n            Name of the embedding variable.\n\n        `""dim""`: int or list\n            Embedding dimension. Can be a list of integers to yield embeddings\n            with dimensionality > 1.\n\n        `""initializer""`: dict or None\n            Hyperparameters of the initializer for the embedding values. An\n            example is:\n\n            .. code-block:: python\n\n                {\n                    ""type"": torch.nn.init.uniform_,\n                    ""kwargs"": {\'a\': -0.1, \'b\': 0.1}\n                }\n\n            which corresponds to :torch_nn:`init.uniform_`, and includes:\n\n            `""type""`: str or function\n                Name, full path, or instance of the initializer class or\n                initializing function;\n                The function can be\n\n                - Built-in initializing function defined in\n                  :mod:`torch.nn.init`, e.g., :torch_nn:`init.xavier_uniform`\n                  or in :mod:`torch`, e.g., :torch:`rand`.\n                - User-defined initializing function in\n                  :mod:`texar.torch.custom`.\n                - External initializing function or initializer instance.\n                  Must provide the full path, e.g.,\n                  :attr:`""my_module.MyInitializer""`, or the instance.\n\n            `""kwargs""`: dict\n                A dictionary of arguments for constructor of the\n                initializer function. An initializer is\n                created by ``initializer = initializer_class_or_fn(**kwargs)``\n                where :attr:`initializer_class_or_fn` is specified in\n                :attr:`""type""`.\n\n        `""dropout_rate""`: float\n            The dropout rate between 0 and 1. For example, ``dropout_rate=0.1``\n            would zero out 10% of the embedding.\n\n        `""dropout_strategy""`: str\n            The dropout strategy. Can be one of the following\n\n            - ``""element""``: The regular strategy that drops individual elements\n              in the embedding vectors.\n            - ``""item""``: Drops individual items (e.g., words) entirely. For\n              example, for the word sequence ""the simpler the better"", the\n              strategy can yield ""_ simpler the better"", where the first ""the""\n              is dropped.\n            - ``""item_type""``: Drops item types (e.g., word types). For example,\n              for the above sequence, the strategy can yield ""_ simpler _\n              better"", where the word type ""the"" is dropped. The dropout will\n              never yield ""_ simpler the better"" as in the ``""item""`` strategy.\n\n        `""trainable""`: bool\n            Whether the embedding parameters are trainable. If false, freeze the\n            embedding parameters.\n    """"""\n    return {\n        ""name"": ""embedding"",\n        ""dim"": 100,\n        ""initializer"": None,\n        ""dropout_rate"": 0.,\n        ""dropout_strategy"": \'element\',\n        ""trainable"": True,\n        ""@no_typecheck"": [""dim""]\n    }\n\n\ndef get_embedding(num_embeds: Optional[int] = None,\n                  init_value: Optional[torch.Tensor] = None,\n                  hparams=None):\n    r""""""Creates embedding variable if not exists.\n\n    Args:\n        hparams (dict or HParams, optional): Embedding hyperparameters. Missing\n            hyperparameters are set to default values. See\n            :func:`~texar.torch.modules.default_embedding_hparams`\n            for all hyperparameters and default values.\n\n            If :attr:`init_value` is given, :attr:`hparams[""initializer""]`,\n            and :attr:`hparams[""dim""]` are ignored.\n        init_value (Tensor or numpy array, optional): Initial values of the\n            embedding variable. If not given, embedding is initialized as\n            specified in :attr:`hparams[""initializer""]`.\n        num_embeds (int, optional): The number of embedding items\n            (e.g., vocabulary size). Required if :attr:`init_value` is\n            not provided.\n\n    Returns:\n        A 2D :tensor:`Tensor` of the same shape with :attr:`init_value` or of\n        the shape ``[num_embeds, hparams[""dim""]]``.\n    """"""\n    if hparams is None or isinstance(hparams, dict):\n        hparams = HParams(hparams, default_embedding_hparams())\n    if init_value is None:\n        initializer = layers.get_initializer(\n            getattr(hparams, ""initializer"", None))\n        # TODO Shibiao: add regularizer\n        dim = hparams[""dim""]\n        if not isinstance(hparams[""dim""], (list, tuple)):\n            dim = [dim]\n        embedding = torch.empty(size=[num_embeds] + dim)\n        # initializer should be set by layers.get_initializer\n        if initializer:\n            initializer(embedding)\n        else:\n            torch.nn.init.xavier_uniform_(embedding)\n    else:\n        if torch.is_tensor(init_value):\n            embedding = init_value  # Do not copy the tensor.\n        else:\n            embedding = torch.tensor(init_value, dtype=torch.float)\n\n    return embedding\n\n\ndef soft_embedding_lookup(embedding, soft_ids):\n    r""""""Transforms soft ids (e.g., probability distribution over ids) into\n    embeddings, by mixing the embedding vectors with the soft weights.\n\n    Args:\n        embedding: A Tensor of shape ``[num_classes] + embedding-dim``\n            containing the embedding vectors. Embedding can have dimensionality\n            > 1, i.e., :attr:`embedding` can be of shape\n            ``[num_classes, emb_dim_1, emb_dim_2, ...]``\n        soft_ids: A Tensor of weights (probabilities) used to mix the\n            embedding vectors.\n\n    Returns:\n        A Tensor of shape ``shape(soft_ids)[:-1] + shape(embedding)[1:]``. For\n        example, if ``shape(soft_ids) = [batch_size, max_time, vocab_size]``\n        and ``shape(embedding) = [vocab_size, emb_dim]``, then the returned\n        tensor has shape ``[batch_size, max_time, emb_dim]``.\n\n    Example::\n\n        softmax = torch.nn.Softmax()\n        decoder_outputs, ... = decoder(...)\n        soft_seq_emb = soft_embedding_lookup(\n            embedding, softmax(decoder_outputs.logits))\n    """"""\n    return torch.tensordot(soft_ids, embedding, dims=([-1], [0]))\n'"
texar/torch/modules/embedders/embedders.py,11,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious embedders.\n""""""\nfrom typing import Optional\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom texar.torch.modules.embedders import embedder_utils\nfrom texar.torch.modules.embedders.embedder_base import (\n    EmbedderBase, EmbeddingDropout)\n\n__all__ = [\n    ""WordEmbedder"",\n]\n\n\nclass WordEmbedder(EmbedderBase):\n    r""""""Simple word embedder that maps indexes into embeddings. The indexes\n    can be soft (e.g., distributions over vocabulary).\n\n    Either :attr:`init_value` or :attr:`vocab_size` is required. If both are\n    given, there must be ``init_value.shape[0]==vocab_size``.\n\n    Args:\n        init_value (optional): A Tensor or numpy array that contains the\n            initial value of embeddings. It is typically of shape\n            ``[vocab_size] + embedding-dim``. Embeddings can have dimensionality\n            > 1.\n\n            If `None`, embedding is initialized as specified in\n            ``hparams[""initializer""]``. Otherwise, the\n            ``""initializer""`` and ``""dim""`` hyperparameters in :attr:`hparams`\n            are ignored.\n        vocab_size (int, optional): The vocabulary size. Required if\n            :attr:`init_value` is not given.\n        hparams (dict, optional): Embedder hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    See :meth:`forward` for the inputs and outputs of the embedder.\n\n    Example:\n\n    .. code-block:: python\n\n        ids = torch.empty([32, 10]).uniform_(to=10).type(torch.int64).\n        soft_ids = torch.empty([32, 10, 100]).uniform_()\n\n        embedder = WordEmbedder(vocab_size=100, hparams={\'dim\': 256})\n        ids_emb = embedder(ids=ids) # shape: [32, 10, 256]\n        soft_ids_emb = embedder(soft_ids=soft_ids) # shape: [32, 10, 256]\n\n    .. code-block:: python\n\n        # Use with Texar data module\n        hparams={\n            \'dataset\': {\n                \'embedding_init\': {\'file\': \'word2vec.txt\'}\n                ...\n            },\n        }\n        data = MonoTextData(data_params)\n        iterator = DataIterator(data)\n        batch = next(iter(iterator))\n\n        # Use data vocab size\n        embedder_1 = WordEmbedder(vocab_size=data.vocab.size)\n        emb_1 = embedder_1(batch[\'text_ids\'])\n\n        # Use pre-trained embedding\n        embedder_2 = WordEmbedder(init_value=data.embedding_init_value)\n        emb_2 = embedder_2(batch[\'text_ids\'])\n\n\n    .. document private functions\n    """"""\n\n    def __init__(self, init_value: Optional[torch.Tensor] = None,\n                 vocab_size: Optional[int] = None, hparams=None):\n\n        if init_value is None and vocab_size is None:\n            raise ValueError(\n                ""Either `init_value` or `vocab_size` is required."")\n\n        super().__init__(init_value=init_value,\n                         num_embeds=vocab_size, hparams=hparams)\n\n        if vocab_size is None:\n            self._vocab_size = self._num_embeds\n        else:\n            self._vocab_size = vocab_size\n        if self._vocab_size != self._num_embeds:\n            raise ValueError(\n                f""vocab_size must equal to init_value.shape[0]. ""\n                f""Got {self._vocab_size} and {self._num_embeds}"")\n\n        self._dropout_layer = EmbeddingDropout(self._hparams.dropout_rate)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""dim"": 100,\n                ""dropout_rate"": 0,\n                ""dropout_strategy"": \'element\',\n                ""initializer"": {\n                    ""type"": ""random_uniform_initializer"",\n                    ""kwargs"": {\n                        ""minval"": -0.1,\n                        ""maxval"": 0.1,\n                        ""seed"": None\n                    }\n                },\n                ""trainable"": True,\n                ""name"": ""word_embedder"",\n            }\n\n        Here:\n\n        `""dim""`: int or list\n            Embedding dimension. Can be a list of integers to yield embeddings\n            with dimensionality > 1.\n\n            Ignored if :attr:`init_value` is given to the embedder constructor.\n\n        `""dropout_rate""`: float\n            The dropout rate between 0 and 1. For example, ``dropout_rate=0.1``\n            would zero out 10% of the embeddings. Set to 0 to disable dropout.\n\n        `""dropout_strategy""`: str\n            The dropout strategy. Can be one of the following\n\n            - ``""element""``: The regular strategy that drops individual elements\n              in the embedding vectors.\n            - ``""item""``: Drops individual items (e.g., words) entirely. For\n              example, for the word sequence ""the simpler the better"", the\n              strategy can yield ""_ simpler the better"", where the first ""the""\n              is dropped.\n            - ``""item_type""``: Drops item types (e.g., word types). For example,\n              for the above sequence, the strategy can yield ""_ simpler _\n              better"", where the word type ""the"" is dropped. The dropout will\n              never yield ""_ simpler the better"" as in the ``""item""`` strategy.\n\n        `""initializer""`: dict or None\n            Hyperparameters of the initializer for embedding values. See\n            :func:`~texar.torch.core.get_initializer` for the details. Ignored\n            if :attr:`init_value` is given to the embedder constructor.\n\n        `""trainable""`: bool\n            Whether the embedding parameters are trainable. If false, freeze the\n            embedding parameters.\n\n        `""name""`: str\n            Name of the embedding variable.\n        """"""\n        hparams = embedder_utils.default_embedding_hparams()\n        hparams[""name""] = ""word_embedder""\n        return hparams\n\n    def extra_repr(self) -> str:\n        return f""vocab_size={self.vocab_size}, embedding_dim={self.dim}""\n\n    def forward(self,  # type: ignore\n                ids: Optional[torch.LongTensor] = None,\n                soft_ids: Optional[torch.Tensor] = None,\n                **kwargs) -> torch.Tensor:\n        r""""""Embeds (soft) ids.\n\n        Either :attr:`ids` or :attr:`soft_ids` must be given, and they\n        must not be given at the same time.\n\n        Args:\n            ids (optional): An integer tensor containing the ids to embed.\n            soft_ids (optional): A tensor of weights (probabilities) used to\n                mix the embedding vectors.\n            kwargs: Additional keyword arguments for\n                :torch_nn:`functional.embedding` besides :attr:`params` and\n                :attr:`ids`.\n\n        Returns:\n            If :attr:`ids` is given, returns a Tensor of shape\n            ``list(ids.shape) + embedding-dim``. For example,\n            if ``list(ids.shape) == [batch_size, max_time]``\n            and ``list(embedding.shape) == [vocab_size, emb_dim]``, then the\n            return tensor has shape ``[batch_size, max_time, emb_dim]``.\n\n            If :attr:`soft_ids` is given, returns a Tensor of shape\n            ``list(soft_ids.shape)[:-1] + embedding-dim``. For example,\n            if ``list(soft_ids.shape) == [batch_size, max_time, vocab_size]``\n            and ``list(embedding.shape) == [vocab_size, emb_dim]``, then the\n            return tensor has shape ``[batch_size, max_time, emb_dim]``.\n        """"""\n        if ids is not None:\n            if soft_ids is not None:\n                raise ValueError(\n                    \'Must not specify `ids` and `soft_ids` at the same time.\')\n            ids_rank = ids.dim()\n        elif soft_ids is not None:\n            ids_rank = soft_ids.dim() - 1\n        else:\n            raise ValueError(\'Either `ids` or `soft_ids` must be given.\')\n\n        embedding = self._embedding\n\n        if self._hparams.dropout_strategy == \'item_type\':\n            noise_shape = self._get_noise_shape(self._hparams.dropout_strategy)\n            embedding = self._dropout_layer(embedding, noise_shape)\n\n        if ids is not None:\n            outputs = F.embedding(ids, embedding, **kwargs)\n        else:\n            outputs = embedder_utils.soft_embedding_lookup(embedding, soft_ids)\n\n        if self._hparams.dropout_strategy != \'item_type\':\n            noise_shape = self._get_noise_shape(\n                self._hparams.dropout_strategy,\n                ids_rank=ids_rank, dropout_input=outputs)\n            outputs = self._dropout_layer(outputs, noise_shape)\n\n        return outputs\n\n    @property\n    def embedding(self) -> torch.Tensor:\n        r""""""The embedding tensor, of shape ``[vocab_size] + dim``.\n        """"""\n        return self._embedding\n\n    @property\n    def dim(self) -> int:\n        r""""""The embedding dimension.\n        """"""\n        return self._dim\n\n    @property\n    def vocab_size(self) -> int:\n        r""""""The vocabulary size.\n        """"""\n        return self._vocab_size\n\n    @property\n    def num_embeddings(self) -> int:\n        r""""""The vocabulary size. This interface matches\n        :torch_nn:`Embedding`.\n        """"""\n        return self._vocab_size\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of :meth:`forward` output. If the :attr:`dim`\n        hyperparameter is a ``list`` or ``tuple``, the feature size\n        equals its final dimension; otherwise, if :attr:`dim` is an\n        ``int``, the feature size equals :attr:`dim`.\n        """"""\n        if isinstance(self._dim, (list, tuple)):\n            return self._dim[-1]\n        else:\n            return self._dim\n'"
texar/torch/modules/embedders/position_embedders.py,26,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious position embedders.\n""""""\n\nimport math\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom texar.torch.modules.embedders import embedder_utils\nfrom texar.torch.modules.embedders.embedder_base import (\n    EmbedderBase, EmbeddingDropout)\nfrom texar.torch.utils.shapes import mask_sequences\n\n__all__ = [\n    ""PositionEmbedder"",\n    ""SinusoidsPositionEmbedder"",\n]\n\n\nclass PositionEmbedder(EmbedderBase):\n    r""""""Simple position embedder that maps position indexes into embeddings\n    via lookup.\n\n    Either :attr:`init_value` or :attr:`position_size` is required. If both are\n    given, there must be ``init_value.shape[0]==position_size``.\n\n    Args:\n        init_value (optional): A Tensor or numpy array that contains the\n            initial value of embeddings. It is typically of shape\n            ``[position_size, embedding dim]``.\n\n            If `None`, embedding is initialized as specified in\n            ``hparams[""initializer""]``. Otherwise, the\n            ``""initializer""`` and ``""dim""``\n            hyperparameters in :attr:`hparams` are ignored.\n        position_size (int, optional): The number of possible positions, e.g.,\n            the maximum sequence length. Required if :attr:`init_value` is\n            not given.\n        hparams (dict, optional): Embedder hyperparameters. If it is not\n            specified, the default hyperparameter setting is used. See\n            :attr:`default_hparams` for the structure and default values.\n\n\n    .. document private functions\n    """"""\n\n    def __init__(self, position_size: Optional[int] = None,\n                 init_value: Optional[torch.Tensor] = None, hparams=None):\n\n        if init_value is None and position_size is None:\n            raise ValueError(\n                ""Either `init_value` or `position_size` is required."")\n\n        super().__init__(position_size, init_value, hparams=hparams)\n\n        self._position_size = position_size\n        if position_size is None:\n            self._position_size = self._num_embeds\n        if self._position_size != self._num_embeds:\n            raise ValueError(\n                f""position_size must be equal to init_value.shape[0]. ""\n                f""Got {self._position_size} and {self._num_embeds}"")\n\n        self._built = True\n        self._dropout_layer = EmbeddingDropout(self._hparams.dropout_rate)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""dim"": 100,\n                ""initializer"": {\n                    ""type"": ""random_uniform_initializer"",\n                    ""kwargs"": {\n                        ""minval"": -0.1,\n                        ""maxval"": 0.1,\n                        ""seed"": None\n                    }\n                },\n                ""dropout_rate"": 0,\n                ""dropout_strategy"": \'element\',\n                ""trainable"": True,\n                ""name"": ""position_embedder""\n            }\n\n        The hyperparameters have the same meaning as those in\n        :meth:`texar.torch.modules.WordEmbedder.default_hparams`.\n        """"""\n        hparams = embedder_utils.default_embedding_hparams()\n        hparams[""name""] = ""position_embedder""\n        return hparams\n\n    def extra_repr(self) -> str:\n        return f""position_size={self.position_size}, embedding_dim={self.dim}""\n\n    def forward(self,  # type: ignore\n                positions: Optional[torch.LongTensor] = None,\n                sequence_length: Optional[torch.LongTensor] = None, **kwargs):\n        r""""""Embeds the positions.\n\n        Either :attr:`positions` or :attr:`sequence_length` is required:\n\n            - If both are given, :attr:`sequence_length` is used to mask out\n              embeddings of those time steps beyond the respective sequence\n              lengths.\n            - If only :attr:`sequence_length` is given, then positions\n              from 0 to ``sequence_length - 1`` are embedded.\n\n        Args:\n            positions (optional): A :tensor:`LongTensor` containing the position\n                IDs to embed.\n            sequence_length (optional): An :tensor:`LongTensor` of shape\n                ``[batch_size]``. Time steps beyond the respective sequence\n                lengths will have zero-valued embeddings.\n            kwargs: Additional keyword arguments for\n                :torch_nn:`functional.embedding` besides\n                :attr:`params` and :attr:`ids`.\n\n        Returns:\n            A `Tensor` of shape `shape(inputs) + embedding dimension`.\n        """"""\n        # Gets embedder inputs\n        if positions is None:\n            if sequence_length is None:\n                raise ValueError(\n                    \'Either `positions` or `sequence_length` is required.\')\n            max_length = torch.max(sequence_length)\n            single_inputs = torch.arange(start=0, end=max_length)\n            # Expands `single_inputs` to have shape [batch_size, max_length]\n            inputs = single_inputs.unsqueeze(0)\n            inputs = inputs.expand(len(sequence_length), -1).contiguous()\n        else:\n            inputs = positions\n\n        ids_rank = inputs.dim()\n        embedding = self._embedding\n        inputs = inputs.to(device=embedding.device)\n        # Gets dropout strategy\n        st = self._hparams.dropout_strategy\n\n        # Dropouts as \'item_type\' before embedding\n        if st == \'item_type\':\n            noise_shape = self._get_noise_shape(\n                dropout_strategy=st, dropout_input=embedding)\n            embedding = self._dropout_layer(embedding, noise_shape)\n\n        # Embeds\n        outputs = torch.nn.functional.embedding(\n            inputs.type(torch.long), embedding, **kwargs)\n\n        # Dropouts as \'item\' or \'elements\' after embedding\n        if st != \'item_type\':\n            noise_shape = self._get_noise_shape(\n                dropout_strategy=st, dropout_input=outputs, ids_rank=ids_rank)\n            outputs = self._dropout_layer(outputs, noise_shape)\n\n        # Optionally masks\n        if sequence_length is not None:\n            outputs = mask_sequences(\n                outputs, sequence_length)\n\n        return outputs\n\n    @property\n    def embedding(self):\n        r""""""The embedding tensor.\n        """"""\n        return self._embedding\n\n    @property\n    def dim(self):\n        r""""""The embedding dimension.\n        """"""\n        return self._dim\n\n    @property\n    def position_size(self):\n        r""""""The position size, i.e., maximum number of positions.\n        """"""\n        return self._position_size\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of :meth:`forward` output. If the :attr:`dim`\n        hyperparameter is a ``list`` or ``tuple``, the feature size\n        equals its final dimension; otherwise, if :attr:`dim` is an\n        ``int``, the feature size equals :attr:`dim`.\n        """"""\n        if isinstance(self._dim, (list, tuple)):\n            return self._dim[-1]\n        else:\n            return self._dim\n\n\nclass SinusoidsPositionEmbedder(EmbedderBase):\n    r""""""Sinusoid position embedder that maps position indexes into embeddings\n    via sinusoid calculation. This module does not have trainable parameters.\n    Used in, e.g., Transformer models\n    `(Vaswani et al.) ""Attention Is All You Need""`.\n\n    Each channel of the input Tensor is incremented by a sinusoid of a\n    different frequency and phase.\n    This allows attention to learn to use absolute and relative positions.\n\n    Timing signals should be added to some precursors of both the query\n    and the memory inputs to attention.\n    The use of relative position is possible because `sin(x+y)` and\n    `cos(x+y)` can be expressed in terms of `y`, `sin(x)`, and `cos(x)`.\n    In particular, we use a geometric sequence of timescales starting with\n    min_timescale and ending with max_timescale.  The number of different\n    timescales is equal to ``dim / 2``. For each timescale, we\n    generate the two sinusoidal signals `sin(timestep/timescale)` and\n    `cos(timestep/timescale)`.  All of these sinusoids are concatenated in\n    the dim dimension.\n\n    Args:\n        position_size (int): The number of possible positions, e.g., the maximum\n            sequence length. Set ``position_size=None`` and\n            ``hparams[\'cache_embeddings\']=False`` to use arbitrarily large or\n            negative position indices.\n\n    .. document private functions\n    """"""\n    signal: torch.Tensor\n    inv_timescales: torch.Tensor\n\n    def __init__(self, position_size: Optional[int] = None, hparams=None):\n        super().__init__(hparams=hparams)\n        self._num_embeds = position_size  # type: ignore\n        self._dim = self._hparams.dim\n        self._cache_embeddings = self._hparams.cache_embeddings\n\n        num_timescales = self._dim // 2\n        min_timescale = self._hparams.min_timescale\n        max_timescale = self._hparams.max_timescale\n\n        log_timescale_increment = (math.log(max_timescale / min_timescale) /\n                                   (num_timescales - 1))\n        inv_timescales = min_timescale * torch.exp(\n            (torch.arange(num_timescales, dtype=torch.float) *\n             -log_timescale_increment))\n        if self._cache_embeddings:\n            if position_size is None:\n                raise ValueError(""\'position_size\' must not be None when ""\n                                 ""\'cache_embeddings\' is set to True"")\n            positions = torch.arange(position_size, dtype=torch.float)\n            signal = self._compute_embeddings(positions, inv_timescales)\n            self.register_buffer(\'signal\', signal)\n        else:\n            self.register_buffer(\'inv_timescales\', inv_timescales)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values\n        We use a geometric sequence of timescales starting with\n        min_timescale and ending with max_timescale. The number of different\n        timescales is equal to ``dim / 2``.\n\n        .. code-block:: python\n\n            {\n                \'min_timescale\': 1.0,\n                \'max_timescale\': 10000.0,\n                \'dim\': 512,\n                \'cache_embeddings\': True,\n                \'name\':\'sinusoid_position_embedder\',\n            }\n\n        Here:\n\n        `""cache_embeddings""`: bool\n            If `True`, precompute embeddings for positions in range\n            `[0, position_size - 1]`. This leads to faster lookup but requires\n            lookup indices to be within this range.\n\n            If `False`, embeddings are computed on-the-fly during lookup. Set to\n            `False` if your application needs to handle sequences of arbitrary\n            length, or requires embeddings at negative positions.\n        """"""\n        return {\n            \'min_timescale\': 1.0,\n            \'max_timescale\': 1.0e4,\n            \'dim\': 512,\n            \'cache_embeddings\': True,\n            \'name\': \'sinusoid_position_embedder\',\n        }\n\n    def extra_repr(self) -> str:\n        return f""embedding_dim={self.dim}""\n\n    def _compute_embeddings(self, positions: torch.Tensor,\n                            inv_timescales: torch.Tensor) -> torch.Tensor:\n        scaled_time = (positions.type_as(inv_timescales).view(-1, 1) *\n                       inv_timescales.unsqueeze(0))\n        signal = torch.cat(\n            [torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n        if self._dim % 2 == 1:\n            # An extra dimension must be added when dimension is odd.\n            signal = torch.cat(\n                [signal, signal.new_zeros(signal.size(0), 1)], dim=1)\n        signal = signal.view(*positions.size(), -1).contiguous()\n        return signal\n\n    def forward(self,  # type: ignore\n                positions: Optional[torch.LongTensor] = None,\n                sequence_length: Optional[torch.LongTensor] = None, **kwargs) \\\n            -> torch.Tensor:\n        r""""""Embeds.\n        Either :attr:`positions` or :attr:`sequence_length` is required:\n\n        - If both are given, :attr:`sequence_length` is used to mask out\n          embeddings of those time steps beyond the respective sequence\n          lengths.\n        - If only :attr:`sequence_length` is given, then positions\n          from `0` to `sequence_length - 1` are embedded.\n\n        Args:\n            positions (optional): An :tensor:`LongTensor` containing the\n                position IDs to embed.\n            sequence_length (optional): An :tensor:`LongTensor` of shape\n                ``[batch_size]``. Time steps beyond\n                the respective sequence lengths will have zero-valued\n                embeddings.\n\n        Returns:\n            A Tensor of shape ``[batch_size, position_size, dim]``.\n        """"""\n        if positions is None:\n            if sequence_length is None:\n                raise ValueError(\n                    \'Either `positions` or `sequence_length` is required.\')\n            max_length = sequence_length.max()\n            batch_size = sequence_length.size(0)\n            inputs = torch.arange(max_length).to(device=sequence_length.device)\n            inputs = inputs.expand(batch_size, max_length)\n        else:\n            inputs = positions\n\n        if self._cache_embeddings:\n            outputs = F.embedding(inputs, self.signal, **kwargs)\n        else:\n            outputs = self._compute_embeddings(inputs, self.inv_timescales)\n\n        if sequence_length is not None:\n            outputs = mask_sequences(outputs, sequence_length)\n\n        return outputs\n\n    @property\n    def dim(self):\n        r""""""The embedding dimension.\n        """"""\n        return self._dim\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of :meth:`forward` output. If the :attr:`dim`\n        hyperparameter is a ``list`` or ``tuple``, the feature size\n        equals its final dimension; otherwise, if :attr:`dim` is an\n        ``int``, the feature size equals :attr:`dim`.\n        """"""\n        if isinstance(self._dim, (list, tuple)):\n            dim = self._dim[-1]\n        else:\n            dim = self._dim\n        return dim\n'"
texar/torch/modules/encoder_decoders/__init__.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library encoders.\n""""""\n\nfrom texar.torch.modules.encoder_decoders.encoder_decoder_base import *\nfrom texar.torch.modules.encoder_decoders.t5_encoder_decoder import *\n'"
texar/torch/modules/encoder_decoders/encoder_decoder_base.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for encoder_decoders.\n""""""\nfrom abc import ABC\nfrom typing import Any, Dict\n\nfrom texar.torch.module_base import ModuleBase\n\n__all__ = [\n    \'EncoderDecoderBase\',\n]\n\n\nclass EncoderDecoderBase(ModuleBase, ABC):\n    r""""""Base class inherited by all encoderdecoder classes.\n    """"""\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n        """"""\n        return {\n            \'name\': \'encoderdecoder\'\n        }\n'"
texar/torch/modules/encoder_decoders/t5_encoder_decoder.py,20,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nT5 Model.\n""""""\n\nfrom typing import Optional, Union\n\nimport torch\n\nfrom texar.torch.core import layers, Identity\nfrom texar.torch.modules.embedders.embedders import WordEmbedder\nfrom texar.torch.modules.encoder_decoders.encoder_decoder_base\\\n    import EncoderDecoderBase\nfrom texar.torch.modules.encoders import T5Encoder\nfrom texar.torch.modules.decoders import T5Decoder\nfrom texar.torch.modules.pretrained.t5 import PretrainedT5Mixin\n\n__all__ = [\n    ""T5EncoderDecoder""\n]\n\n\nclass T5EncoderDecoder(EncoderDecoderBase, PretrainedT5Mixin):\n    r""""""The pre-trained T5 model. Please see\n    :class:`~texar.torch.modules.PretrainedT5Mixin` for a brief description\n    of T5.\n\n    This module basically stacks\n    :class:`~texar.torch.modules.WordEmbedder`,\n    :class:`~texar.torch.modules.T5Encoder`, and\n    :class:`~texar.torch.modules.T5Decoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``T5-Small``). Please refer to\n            :class:`~texar.torch.modules.PretrainedT5Mixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        super().__init__(hparams=hparams)\n\n        self.load_pretrained_config(pretrained_model_name, cache_dir)\n\n        # Word embedding\n        self.word_embedder = WordEmbedder(\n            vocab_size=self._hparams.vocab_size,\n            hparams=self._hparams.embed)\n\n        # The encoder (a TransformerEncoder)\n        self.encoder = T5Encoder(hparams=self._hparams.encoder)\n\n        # The decoder (a TransformerDecoder)\n        self.decoder = T5Decoder(\n            token_embedder=self._embedding_fn,\n            output_layer=Identity(),\n            hparams=self._hparams.decoder)\n\n        self.init_pretrained_weights()\n\n    def _embedding_fn(self, tokens: torch.LongTensor\n                      ) -> torch.Tensor:\n        word_embed = self.word_embedder(tokens)\n        return word_embed\n\n    def reset_parameters(self):\n        initialize = layers.get_initializer(self._hparams.initializer)\n        if initialize is not None:\n            # Do not re-initialize LayerNorm modules.\n            for name, param in self.named_parameters():\n                if name.split(\'.\')[-1] == \'weight\' and \'layer_norm\' not in name:\n                    initialize(param)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The model arch is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the model arch is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the encoder arch is defined by the\n          configurations in `hparams` and weights are randomly initialized.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""T5-Small"",\n                ""embed"": {\n                    ""dim"": 768,\n                    ""name"": ""word_embeddings""\n                },\n                ""vocab_size"": 32128,\n\n                ""encoder"": {\n                    ""dim"": 768,\n                    ""embedding_dropout"": 0.1,\n                    ""multihead_attention"": {\n                        ""dropout_rate"": 0.1,\n                        ""name"": ""self"",\n                        ""num_heads"": 12,\n                        ""num_units"": 768,\n                        ""output_dim"": 768,\n                        ""use_bias"": False,\n                        ""is_decoder"": False,\n                        ""relative_attention_num_buckets"": 32,\n                    },\n                    ""eps"": 1e-6,\n                    ""name"": ""encoder"",\n                    ""num_blocks"": 12,\n                    ""poswise_feedforward"": {\n                        ""layers"": [\n                            {\n                                ""kwargs"": {\n                                    ""in_features"": 768,\n                                    ""out_features"": 3072,\n                                    ""bias"": False\n                                },\n                                ""type"": ""Linear""\n                            },\n                            {""type"": ""ReLU""},\n                            {\n                                ""kwargs"": {\n                                    ""in_features"": 3072,\n                                    ""out_features"": 768,\n                                    ""bias"": False\n                                },\n                                ""type"": ""Linear""\n                            }\n                        ]\n                    },\n                    ""residual_dropout"": 0.1,\n                    },\n\n                ""decoder"": {\n                    ""eps"": 1e-6,\n                    ""dim"": 768,\n                    ""embedding_dropout"": 0.1,\n                    ""multihead_attention"": {\n                        ""dropout_rate"": 0.1,\n                        ""name"": ""self"",\n                        ""num_heads"": 12,\n                        ""num_units"": 768,\n                        ""output_dim"": 768,\n                        ""use_bias"": False,\n                        ""is_decoder"": True,\n                        ""relative_attention_num_buckets"": 32,\n                    },\n                    ""name"": ""decoder"",\n                    ""num_blocks"": 12,\n                    ""poswise_feedforward"": {\n                        ""layers"": [\n                            {\n                                ""kwargs"": {\n                                    ""in_features"": 768,\n                                    ""out_features"": 3072,\n                                    ""bias"": False\n                                },\n                                ""type"": ""Linear""\n                            },\n                            {""type"": ""ReLU""},\n                            {\n                                ""kwargs"": {\n                                    ""in_features"": 3072,\n                                    ""out_features"": 768,\n                                    ""bias"": False\n                                },\n                                ""type"": ""Linear""\n                            }\n                        ]\n                    },\n                    ""residual_dropout"": 0.1,\n                    },\n                ""hidden_size"": 768,\n                ""initializer"": None,\n                ""name"": ""t5_encoder_decoder"",\n            }\n\n        Here:\n\n        The default parameters are values for T5-Small model.\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained T5 model. If None, the model\n            will be randomly initialized.\n\n        `""embed""`: dict\n            Hyperparameters for word embedding layer.\n\n        `""vocab_size""`: int\n            The vocabulary size of `inputs` in T5 model.\n\n        `""encoder""`: dict\n            Hyperparameters for the `T5Encoder`.\n            See :func:`~texar.torch.modules.T5Encoder.default_hparams`\n            for details.\n\n        `""decoder""`: dict\n            Hyperparameters for the `T5Decoder`.\n            See :func:`~texar.torch.modules.T5Decoder.default_hparams`\n            for details.\n\n        `""hidden_size""`: int\n            Size of the hidden layer.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n\n        return {\n            \'pretrained_model_name\': \'T5-Small\',\n            \'embed\': {\n                \'dim\': 768,\n                \'name\': \'word_embeddings\'\n            },\n            \'vocab_size\': 32128,\n\n            \'encoder\': {\n                \'dim\': 768,\n                \'embedding_dropout\': 0.1,\n                \'multihead_attention\': {\n                    \'dropout_rate\': 0.1,\n                    \'name\': \'self\',\n                    \'num_heads\': 12,\n                    \'num_units\': 768,\n                    \'output_dim\': 768,\n                    \'use_bias\': False,\n                    \'is_decoder\': False,\n                    \'relative_attention_num_buckets\': 32\n                },\n                \'eps\': 1e-6,\n                \'name\': \'encoder\',\n                \'num_blocks\': 12,\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 768,\n                                \'out_features\': 3072,\n                                \'bias\': False\n                            },\n                            \'type\': \'Linear\'\n                        },\n                        {""type"": ""ReLU""},\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 3072,\n                                \'out_features\': 768,\n                                \'bias\': False\n                            },\n                            \'type\': \'Linear\'\n                        }\n                    ]\n                },\n                \'residual_dropout\': 0.1,\n            },\n            \'decoder\': {\n                \'eps\': 1e-6,\n                \'dim\': 768,\n                \'embedding_dropout\': 0.1,\n                \'multihead_attention\': {\n                    \'dropout_rate\': 0.1,\n                    \'name\': \'self\',\n                    \'num_heads\': 12,\n                    \'num_units\': 768,\n                    \'output_dim\': 768,\n                    \'use_bias\': False,\n                    \'is_decoder\': True,\n                    \'relative_attention_num_buckets\': 32\n                },\n                \'name\': \'decoder\',\n                \'num_blocks\': 12,\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 768,\n                                \'out_features\': 3072,\n                                \'bias\': False\n                            },\n                            \'type\': \'Linear\'\n                        },\n                        {""type"": ""ReLU""},\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 3072,\n                                \'out_features\': 768,\n                                \'bias\': False\n                            },\n                            \'type\': \'Linear\'\n                        }\n                    ]\n                },\n                \'residual_dropout\': 0.1,\n            },\n            \'hidden_size\': 768,\n            \'initializer\': None,\n            \'name\': \'t5_encoder_decoder\',\n            \'@no_typecheck\': [\'pretrained_model_name\']\n        }\n\n    def forward(self,  # type: ignore\n                inputs: Union[torch.Tensor, torch.LongTensor],\n                sequence_length: Optional[torch.LongTensor] = None):\n        r""""""Performs encoding and decoding.\n\n        Args:\n            inputs: Either a **2D Tensor** of shape ``[batch_size, max_time]``,\n                containing the ids of tokens in input sequences, or\n                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,\n                containing soft token ids (i.e., weights or probabilities)\n                used to mix the embedding vectors.\n            sequence_length: A 1D :tensor:`Tensor` of shape\n                ``[batch_size]``. Input tokens beyond respective sequence\n                lengths are masked out automatically.\n\n        Returns:\n            A pair :attr:`(encoder_output, decoder_output)`\n\n            - :attr:`encoder_output`: A Tensor of shape\n              `[batch_size, max_time, dim]` containing the encoded vectors.\n\n            - :attr:`decoder_output`: An instance of\n              :class:`~texar.torch.modules.TransformerDecoderOutput` which\n              contains `sample_id` and `logits`.\n        """"""\n        if inputs.dim() == 2:\n            word_embeds = self.word_embedder(ids=inputs)\n        elif inputs.dim() == 3:\n            word_embeds = self.word_embedder(soft_ids=inputs)\n        else:\n            raise ValueError(""\'inputs\' should be a 2D or 3D tensor."")\n\n        batch_size = inputs.size(0)\n\n        if sequence_length is None:\n            sequence_length = inputs.new_full(\n                (batch_size,), inputs.size(1), dtype=torch.long)\n\n        encoder_output = self.encoder(inputs=word_embeds,\n                                      sequence_length=sequence_length)\n\n        decoder_output = self.decoder(inputs=inputs,\n                                      memory=encoder_output,\n                                      memory_sequence_length=sequence_length)\n\n        return encoder_output, decoder_output\n\n    @property\n    def output_size(self):\n        r""""""The feature size of :meth:`forward` output of the encoder.\n        """"""\n        return self._hparams.hidden_size\n'"
texar/torch/modules/encoders/__init__.py,10,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library encoders.\n""""""\n\nfrom texar.torch.modules.encoders.bert_encoder import *\nfrom texar.torch.modules.encoders.conv_encoders import *\nfrom texar.torch.modules.encoders.encoder_base import *\nfrom texar.torch.modules.encoders.gpt2_encoder import *\nfrom texar.torch.modules.encoders.multihead_attention import *\nfrom texar.torch.modules.encoders.rnn_encoders import *\nfrom texar.torch.modules.encoders.roberta_encoder import *\nfrom texar.torch.modules.encoders.transformer_encoder import *\nfrom texar.torch.modules.encoders.xlnet_encoder import *\nfrom texar.torch.modules.encoders.t5_encoder import *\n'"
texar/torch/modules/encoders/bert_encoder.py,20,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBERT encoder.\n""""""\n\nfrom typing import Optional, Union\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.core import layers\nfrom texar.torch.modules.embedders.embedders import WordEmbedder\nfrom texar.torch.modules.embedders.position_embedders import PositionEmbedder\nfrom texar.torch.modules.encoders.encoder_base import EncoderBase\nfrom texar.torch.modules.encoders.transformer_encoder import TransformerEncoder\nfrom texar.torch.modules.pretrained.bert import PretrainedBERTMixin\n\n__all__ = [\n    ""BERTEncoder"",\n]\n\n\nclass BERTEncoder(EncoderBase, PretrainedBERTMixin):\n    r""""""Raw BERT Transformer for encoding sequences. Please see\n    :class:`~texar.torch.modules.PretrainedBERTMixin` for a brief description\n    of BERT.\n\n    This module basically stacks\n    :class:`~texar.torch.modules.WordEmbedder`,\n    :class:`~texar.torch.modules.PositionEmbedder`,\n    :class:`~texar.torch.modules.TransformerEncoder` and a dense\n    pooler.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``bert-base-uncased``). Please refer to\n            :class:`~texar.torch.modules.PretrainedBERTMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        super().__init__(hparams=hparams)\n\n        self.load_pretrained_config(pretrained_model_name, cache_dir)\n\n        # Word embedding\n        self.word_embedder = WordEmbedder(\n            vocab_size=self._hparams.vocab_size,\n            hparams=self._hparams.embed)\n\n        # Segment embedding for each type of tokens\n        self.segment_embedder = None\n        if self._hparams.get(\'type_vocab_size\', 0) > 0:\n            if self.pretrained_model_name is not None and \\\n                    self.pretrained_model_name.startswith(\'spanbert\'):\n                # Do not construct segment_embedder for SpanBERT\n                pass\n            else:\n                self.segment_embedder = WordEmbedder(\n                    vocab_size=self._hparams.type_vocab_size,\n                    hparams=self._hparams.segment_embed)\n\n        # Position embedding\n        self.position_embedder = PositionEmbedder(\n            position_size=self._hparams.position_size,\n            hparams=self._hparams.position_embed)\n\n        # The BERT encoder (a TransformerEncoder)\n        self.encoder = TransformerEncoder(hparams=self._hparams.encoder)\n\n        self.pooler = nn.Sequential(\n            nn.Linear(self._hparams.hidden_size, self._hparams.hidden_size),\n            nn.Tanh())\n\n        self.init_pretrained_weights()\n\n    def reset_parameters(self):\n        initialize = layers.get_initializer(self._hparams.initializer)\n        if initialize is not None:\n            # Do not re-initialize LayerNorm modules.\n            for name, param in self.named_parameters():\n                if name.split(\'.\')[-1] == \'weight\' and \'layer_norm\' not in name:\n                    initialize(param)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The encoder arch is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the encoder arch is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the encoder arch is defined by the\n          configurations in `hparams` and weights are randomly initialized.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""bert-base-uncased"",\n                ""embed"": {\n                    ""dim"": 768,\n                    ""name"": ""word_embeddings""\n                },\n                ""vocab_size"": 30522,\n                ""segment_embed"": {\n                    ""dim"": 768,\n                    ""name"": ""token_type_embeddings""\n                },\n                ""type_vocab_size"": 2,\n                ""position_embed"": {\n                    ""dim"": 768,\n                    ""name"": ""position_embeddings""\n                },\n                ""position_size"": 512,\n\n                ""encoder"": {\n                    ""dim"": 768,\n                    ""embedding_dropout"": 0.1,\n                    ""multihead_attention"": {\n                        ""dropout_rate"": 0.1,\n                        ""name"": ""self"",\n                        ""num_heads"": 12,\n                        ""num_units"": 768,\n                        ""output_dim"": 768,\n                        ""use_bias"": True\n                    },\n                    ""name"": ""encoder"",\n                    ""num_blocks"": 12,\n                    ""eps"": 1e-12,\n                    ""poswise_feedforward"": {\n                        ""layers"": [\n                            {\n                                ""kwargs"": {\n                                    ""in_features"": 768,\n                                    ""out_features"": 3072,\n                                    ""bias"": True\n                                },\n                                ""type"": ""Linear""\n                            },\n                            {""type"": ""BertGELU""},\n                            {\n                                ""kwargs"": {\n                                    ""in_features"": 3072,\n                                    ""out_features"": 768,\n                                    ""bias"": True\n                                },\n                                ""type"": ""Linear""\n                            }\n                        ]\n                    },\n                    ""residual_dropout"": 0.1,\n                    ""use_bert_config"": True\n                    },\n                ""hidden_size"": 768,\n                ""initializer"": None,\n                ""name"": ""bert_encoder"",\n            }\n\n        Here:\n\n        The default parameters are values for uncased BERT-Base model.\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained BERT model. If None, the model\n            will be randomly initialized.\n\n        `""embed""`: dict\n            Hyperparameters for word embedding layer.\n\n        `""vocab_size""`: int\n            The vocabulary size of `inputs` in BERT model.\n\n        `""segment_embed""`: dict\n            Hyperparameters for segment embedding layer.\n\n        `""type_vocab_size""`: int\n            The vocabulary size of the `segment_ids` passed into `BertModel`.\n\n        `""position_embed""`: dict\n            Hyperparameters for position embedding layer.\n\n        `""position_size""`: int\n            The maximum sequence length that this model might ever be used with.\n\n        `""encoder""`: dict\n            Hyperparameters for the TransformerEncoder.\n            See :func:`~texar.torch.modules.TransformerEncoder.default_hparams`\n            for details.\n\n        `""hidden_size""`: int\n            Size of the pooler dense layer.\n\n        `""eps""`: float\n            Epsilon values for layer norm layers.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n\n        return {\n            \'pretrained_model_name\': \'bert-base-uncased\',\n            \'embed\': {\n                \'dim\': 768,\n                \'name\': \'word_embeddings\'\n            },\n            \'vocab_size\': 30522,\n            \'segment_embed\': {\n                \'dim\': 768,\n                \'name\': \'token_type_embeddings\'\n            },\n            \'type_vocab_size\': 2,\n            \'position_embed\': {\n                \'dim\': 768,\n                \'name\': \'position_embeddings\'\n            },\n            \'position_size\': 512,\n\n            \'encoder\': {\n                \'dim\': 768,\n                \'embedding_dropout\': 0.1,\n                \'multihead_attention\': {\n                    \'dropout_rate\': 0.1,\n                    \'name\': \'self\',\n                    \'num_heads\': 12,\n                    \'num_units\': 768,\n                    \'output_dim\': 768,\n                    \'use_bias\': True\n                },\n                \'name\': \'encoder\',\n                \'num_blocks\': 12,\n                \'eps\': 1e-12,\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 768,\n                                \'out_features\': 3072,\n                                \'bias\': True\n                            },\n                            \'type\': \'Linear\'\n                        },\n                        {""type"": ""BertGELU""},\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 3072,\n                                \'out_features\': 768,\n                                \'bias\': True\n                            },\n                            \'type\': \'Linear\'\n                        }\n                    ]\n                },\n                \'residual_dropout\': 0.1,\n                \'use_bert_config\': True\n            },\n            \'hidden_size\': 768,\n            \'initializer\': None,\n            \'name\': \'bert_encoder\',\n            \'@no_typecheck\': [\'pretrained_model_name\']\n        }\n\n    def forward(self,  # type: ignore\n                inputs: Union[torch.Tensor, torch.LongTensor],\n                sequence_length: Optional[torch.LongTensor] = None,\n                segment_ids: Optional[torch.LongTensor] = None):\n        r""""""Encodes the inputs. Note that the SpanBERT model does not use\n        segmentation embedding. As a result, SpanBERT does not require\n        `segment_ids` as an input when you use pre-trained SpanBERT checkpoint\n        files.\n\n        Args:\n            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,\n                containing the ids of tokens in input sequences, or\n                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,\n                containing soft token ids (i.e., weights or probabilities)\n                used to mix the embedding vectors.\n            segment_ids (optional): A 2D Tensor of shape\n                `[batch_size, max_time]`, containing the segment ids\n                of tokens in input sequences. If `None` (default), a\n                tensor with all elements set to zero is used.\n            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.\n                Input tokens beyond respective sequence lengths are masked\n                out automatically.\n\n        Returns:\n            A pair :attr:`(outputs, pooled_output)`\n\n            - :attr:`outputs`:  A Tensor of shape\n              `[batch_size, max_time, dim]` containing the encoded vectors.\n\n            - :attr:`pooled_output`: A Tensor of size\n              `[batch_size, hidden_size]` which is the output of a pooler\n              pre-trained on top of the hidden state associated to the first\n              character of the input (`CLS`), see BERT\'s paper.\n        """"""\n        if inputs.dim() == 2:\n            word_embeds = self.word_embedder(ids=inputs)\n        elif inputs.dim() == 3:\n            word_embeds = self.word_embedder(soft_ids=inputs)\n        else:\n            raise ValueError(""\'inputs\' should be a 2D or 3D tensor."")\n\n        batch_size = inputs.size(0)\n        pos_length = inputs.new_full((batch_size,), inputs.size(1),\n                                     dtype=torch.int64)\n        pos_embeds = self.position_embedder(sequence_length=pos_length)\n\n        if self.segment_embedder is not None:\n            if segment_ids is None:\n                segment_ids = torch.zeros((inputs.size(0), inputs.size(1)),\n                                          dtype=torch.long,\n                                          device=inputs.device)\n            segment_embeds = self.segment_embedder(segment_ids)\n            inputs_embeds = word_embeds + segment_embeds + pos_embeds\n        else:\n            inputs_embeds = word_embeds + pos_embeds\n\n        if sequence_length is None:\n            sequence_length = inputs.new_full((batch_size,), inputs.size(1),\n                                              dtype=torch.int64)\n\n        output = self.encoder(inputs_embeds, sequence_length)\n\n        # taking the hidden state corresponding to the first token.\n        first_token_tensor = output[:, 0, :]\n\n        pooled_output = self.pooler(first_token_tensor)\n\n        return output, pooled_output\n\n    @property\n    def output_size(self):\n        r""""""The feature size of :meth:`forward` output\n        :attr:`pooled_output`.\n        """"""\n        return self._hparams.hidden_size\n'"
texar/torch/modules/encoders/conv_encoders.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious convolutional network encoders.\n""""""\n\nfrom typing import Dict, Optional, Any\n\nfrom texar.torch.modules.encoders.encoder_base import EncoderBase\nfrom texar.torch.modules.networks.conv_networks import Conv1DNetwork\n\n__all__ = [\n    ""Conv1DEncoder"",\n]\n\n\nclass Conv1DEncoder(Conv1DNetwork, EncoderBase):\n    r""""""Simple `Conv-1D` encoder which consists of a sequence of convolutional\n    layers followed with a sequence of dense layers.\n\n    Wraps :class:`~texar.torch.modules.Conv1DNetwork` to be a subclass of\n    :class:`~texar.torch.modules.EncoderBase`. Has exact the same functionality\n    with :class:`~texar.torch.modules.Conv1DNetwork`.\n    """"""\n\n    def __init__(self, in_channels: int, in_features: Optional[int] = None,\n                 hparams=None):\n        super().__init__(in_channels=in_channels,\n                         in_features=in_features,\n                         hparams=hparams)\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        The same as :meth:`~texar.torch.modules.Conv1DNetwork.default_hparams`\n        of :class:`~texar.torch.modules.Conv1DNetwork`, except that the default\n        name is ``""conv_encoder""``.\n        """"""\n        hparams = Conv1DNetwork.default_hparams()\n        hparams[\'name\'] = \'conv_encoder\'\n        return hparams\n'"
texar/torch/modules/encoders/encoder_base.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for encoders.\n""""""\nfrom abc import ABC\nfrom typing import Any, Dict\n\nfrom texar.torch.module_base import ModuleBase\n\n__all__ = [\n    \'EncoderBase\',\n]\n\n\nclass EncoderBase(ModuleBase, ABC):\n    r""""""Base class inherited by all encoder classes.\n    """"""\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n        """"""\n        return {\n            \'name\': \'encoder\'\n        }\n'"
texar/torch/modules/encoders/gpt2_encoder.py,16,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nGPT2 encoders.\n""""""\n\nfrom typing import Optional, Union\n\nimport torch\n\nfrom texar.torch.modules.embedders.embedders import WordEmbedder\nfrom texar.torch.modules.embedders.position_embedders import PositionEmbedder\nfrom texar.torch.modules.encoders.encoder_base import EncoderBase\nfrom texar.torch.modules.encoders.transformer_encoder import TransformerEncoder\nfrom texar.torch.modules.pretrained.gpt2 import PretrainedGPT2Mixin\n\n__all__ = [\n    ""GPT2Encoder"",\n]\n\n\nclass GPT2Encoder(EncoderBase, PretrainedGPT2Mixin):\n    r""""""Raw GPT2 Transformer for encoding sequences. Please see\n    :class:`~texar.torch.modules.PretrainedGPT2Mixin` for a brief description\n    of GPT2.\n\n    This module basically stacks\n    :class:`~texar.torch.modules.WordEmbedder`,\n    :class:`~texar.torch.modules.PositionEmbedder`,\n    :class:`~texar.torch.modules.TransformerEncoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``gpt2-small``). Please refer to\n            :class:`~texar.torch.modules.PretrainedGPT2Mixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        super().__init__(hparams=hparams)\n\n        self.load_pretrained_config(pretrained_model_name, cache_dir)\n\n        # Word embedding\n        self.word_embedder = WordEmbedder(\n            vocab_size=self._hparams.vocab_size,\n            hparams=self._hparams.embed)\n\n        # Position embedding\n        self.position_embedder = PositionEmbedder(\n            position_size=self._hparams.position_size,\n            hparams=self._hparams.position_embed)\n\n        # The GPT2 encoder (a TransformerEncoder)\n        self.encoder = TransformerEncoder(hparams=self._hparams.encoder)\n\n        self.init_pretrained_weights(load_output_layer=False)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The encoder arch is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the encoder arch is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the encoder arch is defined by the\n          configurations in `hparams` and weights are randomly initialized.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""gpt2-small"",\n                ""vocab_size"": 50257,\n                ""context_size"": 1024,\n                ""embedding_size"": 768,\n                ""embed"": {\n                    ""dim"": 768,\n                    ""name"": ""word_embeddings""\n                },\n                ""position_size"": 1024,\n                ""position_embed"": {\n                    ""dim"": 768,\n                    ""name"": ""position_embeddings""\n                },\n\n                ""encoder"": {\n                    ""dim"": 768,\n                    ""num_blocks"": 12,\n                    ""use_bert_config"": False,\n                    ""embedding_dropout"": 0,\n                    ""residual_dropout"": 0,\n                    ""multihead_attention"": {\n                        ""use_bias"": True,\n                        ""num_units"": 768,\n                        ""num_heads"": 12,\n                        ""output_dim"": 768\n                    },\n                    ""eps"": 1e-6,\n                    ""initializer"": {\n                        ""type"": ""variance_scaling_initializer"",\n                        ""kwargs"": {\n                            ""factor"": 1.0,\n                            ""mode"": ""FAN_AVG"",\n                            ""uniform"": True\n                        }\n                    },\n                    ""poswise_feedforward"": {\n                        ""layers"": [\n                            {\n                                ""type"": ""Linear"",\n                                ""kwargs"": {\n                                    ""in_features"": 768,\n                                    ""out_features"": 3072,\n                                    ""bias"": True\n                                }\n                            },\n                            {\n                                ""type"": ""GPTGELU"",\n                                ""kwargs"": {}\n                            },\n                            {\n                                ""type"": ""Linear"",\n                                ""kwargs"": {\n                                    ""in_features"": 3072,\n                                    ""out_features"": 768,\n                                    ""bias"": True\n                                }\n                            }\n                        ],\n                        ""name"": ""ffn""\n                    }\n                },\n                ""initializer"": None,\n                ""name"": ""gpt2_encoder"",\n            }\n\n        Here:\n\n        The default parameters are values for 124M GPT2 model.\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained GPT2 model. If None, the model\n            will be randomly initialized.\n\n        `""embed""`: dict\n            Hyperparameters for word embedding layer.\n\n        `""vocab_size""`: int\n            The vocabulary size of `inputs` in `GPT2Model`.\n\n        `""position_embed""`: dict\n            Hyperparameters for position embedding layer.\n\n        `""position_size""`:  int\n            The maximum sequence length that this model might ever be used with.\n\n        `""decoder""`: dict\n            Hyperparameters for the TransformerDecoder.\n            See :func:`~texar.torch.modules.TransformerDecoder.default_hparams`\n            for details.\n\n        `""eps""`: float\n            Epsilon values for layer norm layers.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n        return {\n            \'encoder\': {\n                \'dim\': 768,\n                \'num_blocks\': 12,\n                \'use_bert_config\': False,\n                \'embedding_dropout\': 0,\n                \'residual_dropout\': 0,\n                \'multihead_attention\': {\n                    \'use_bias\': True,\n                    \'num_units\': 768,\n                    \'num_heads\': 12,\n                    \'output_dim\': 768\n                },\n                \'eps\': 1e-6,\n                \'initializer\': {\n                    \'type\': \'variance_scaling_initializer\',\n                    \'kwargs\': {\n                        \'factor\': 1.0,\n                        \'mode\': \'FAN_AVG\',\n                        \'uniform\': True\n                    }\n                },\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 768,\n                                \'out_features\': 3072,\n                                \'bias\': True\n                            }\n                        },\n                        {\n                            \'type\': \'GPTGELU\',\n                            \'kwargs\': {}\n                        },\n                        {\n                            \'type\': \'Linear\',\n                            \'kwargs\': {\n                                \'in_features\': 3072,\n                                \'out_features\': 768,\n                                \'bias\': True\n                            }\n                        }\n                    ],\n                    \'name\': \'ffn\'\n                },\n            },\n            \'pretrained_model_name\': \'gpt2-small\',\n            \'vocab_size\': 50257,\n            \'context_size\': 1024,\n            \'embedding_size\': 768,\n            \'embed\': {\n                \'dim\': 768,\n                \'name\': \'word_embeddings\'\n            },\n            \'position_size\': 1024,\n            \'position_embed\': {\n                \'dim\': 768,\n                \'name\': \'position_embeddings\'\n            },\n            \'initializer\': None,\n            \'name\': \'gpt2_encoder\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    def forward(self,  # type: ignore\n                inputs: Union[torch.Tensor, torch.LongTensor],\n                sequence_length: Optional[torch.LongTensor] = None):\n        r""""""Encodes the inputs.\n\n        Args:\n            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,\n                containing the ids of tokens in input sequences, or\n                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,\n                containing soft token ids (i.e., weights or probabilities)\n                used to mix the embedding vectors.\n            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.\n                Input tokens beyond respective sequence lengths are masked\n                out automatically.\n\n        Returns:\n            outputs:  A Tensor of shape\n            `[batch_size, max_time, dim]` containing the encoded vectors.\n        """"""\n        if inputs.dim() == 2:\n            word_embeds = self.word_embedder(ids=inputs)\n        elif inputs.dim() == 3:\n            word_embeds = self.word_embedder(soft_ids=inputs)\n        else:\n            raise ValueError(""\'inputs\' should be a 2D or 3D tensor."")\n\n        batch_size = inputs.size(0)\n        pos_length = inputs.new_full(\n            (batch_size,), inputs.size(1), dtype=torch.long)\n        pos_embeds = self.position_embedder(sequence_length=pos_length)\n\n        inputs_embeds = word_embeds + pos_embeds\n\n        if sequence_length is None:\n            sequence_length = inputs.new_full(\n                (batch_size,), inputs.size(1), dtype=torch.long)\n\n        output = self.encoder(\n            inputs=inputs_embeds, sequence_length=sequence_length)\n\n        return output\n\n    @property\n    def output_size(self):\n        r""""""The feature size of :meth:`forward` output.\n        """"""\n        return self._hparams.encoder.dim\n'"
texar/torch/modules/encoders/multihead_attention.py,25,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTransformer encoders with multi-head self attention.\n""""""\n\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom mypy_extensions import TypedDict\n\nfrom texar.torch.core import layers\nfrom texar.torch.modules.encoders.encoder_base import EncoderBase\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    \'MultiheadAttentionEncoder\',\n    \'Cache\',\n]\n\n\nclass LayerCache(TypedDict):\n    r""""""Cache (state) for a single self-attention layer in\n    :class:`MultiheadAttentionEncoder`.\n    """"""\n    keys: MaybeList[torch.Tensor]\n    values: MaybeList[torch.Tensor]\n\n\nclass Cache(TypedDict):\n    r""""""Cache (state) for the entire :class:`MultiheadAttentionEncoder`.\n    """"""\n    memory: Optional[torch.Tensor]\n    memory_attention_bias: Optional[torch.Tensor]\n    layers: List[LayerCache]\n\n\nclass MultiheadAttentionEncoder(EncoderBase):\n    r""""""Multi-head Attention Encoder.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    .. document private functions\n    """"""\n\n    def __init__(self, input_size: int, hparams=None):\n        super().__init__(hparams=hparams)\n        use_bias = self._hparams.use_bias\n\n        self.Q_dense = nn.Linear(input_size, self._hparams.num_units,\n                                 bias=use_bias)\n        self.K_dense = nn.Linear(input_size, self._hparams.num_units,\n                                 bias=use_bias)\n        self.V_dense = nn.Linear(input_size, self._hparams.num_units,\n                                 bias=use_bias)\n        self.O_dense = nn.Linear(self._hparams.num_units,\n                                 self._hparams.output_dim, bias=use_bias)\n\n        if self._hparams.initializer:\n            # TODO(haoransh): we may define kernel_initializer and bias\n            #  initializer seperately\n            initialize = layers.get_initializer(self._hparams.initializer)\n            assert initialize is not None\n            for name, param in self.named_parameters():\n                if name.split(\'.\')[-1] == \'weight\':\n                    print(\'name:{}\'.format(name))\n                    initialize(param)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""initializer"": None,\n                \'num_heads\': 8,\n                \'output_dim\': 512,\n                \'num_units\': 512,\n                \'dropout_rate\': 0.1,\n                \'use_bias\': False,\n                ""name"": ""multihead_attention""\n            }\n\n        Here:\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""num_heads""`: int\n            Number of heads for attention calculation.\n\n        `""output_dim""`: int\n            Output dimension of the returned tensor.\n\n        `""num_units""`: int\n            Hidden dimension of the unsplit attention space.\n            Should be divisible by `""num_heads""`.\n\n        `""dropout_rate""`: float\n            Dropout rate in the attention.\n\n        `""use_bias""`: bool\n            Use bias when projecting the key, value and query.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n        return {\n            \'initializer\': None,\n            \'num_heads\': 8,\n            \'output_dim\': 512,\n            \'num_units\': 512,\n            \'dropout_rate\': 0.1,\n            \'use_bias\': False,\n            \'name\': \'multihead_attention\',\n        }\n\n    def forward(self,  # type: ignore\n                queries: torch.Tensor,\n                memory: torch.Tensor,\n                memory_attention_bias: torch.Tensor,\n                cache: Optional[LayerCache] = None) \\\n            -> torch.Tensor:\n        r""""""Encodes the inputs.\n\n        Args:\n            queries: A 3D tensor with shape of\n                ``[batch, length_query, depth_query]``.\n            memory: A 3D tensor with shape of\n                ``[batch, length_key, depth_key]``.\n            memory_attention_bias: A 3D tensor with shape of\n                ``[batch, length_key, num_units]``.\n            cache: Memory cache only when inferring the sentence from scratch.\n\n        Returns:\n            A tensor of shape ``[batch_size, max_time, dim]`` containing the\n            encoded vectors.\n        """"""\n\n        num_heads = self._hparams.num_heads\n        num_units = self._hparams.num_units\n        if num_units % num_heads != 0:\n            raise ValueError(\n                f""Value depth ({num_units}) must be divisible by ""\n                f""the number of attention heads ({num_heads})."")\n\n        def _update_and_return(layer: nn.Module, key: str):\n            if memory is None:\n                # Self Attention\n                out = layer(queries)\n\n                if cache is not None:\n                    # decoder self attention when dynamic decoding\n                    res: MaybeList[torch.Tensor] = cache[key]\n                    if isinstance(res, list):\n                        # inference-like decoding\n                        res.append(out.squeeze(1))\n                        out = torch.stack(res, dim=1)\n                    else:\n                        # normal decoding\n                        res = torch.cat([res, out], dim=1)\n                        out = res\n                    cache[key] = res\n\n            else:\n                # encoder decoder attention\n                if cache is not None:\n                    res: MaybeList[torch.Tensor] = cache[key]  # type: ignore\n                    if isinstance(res, list):\n                        # inference-like decoding\n                        if len(res) == 0:\n                            out = layer(memory)\n                        else:\n                            out = torch.stack(res, dim=1)\n                    else:\n                        # normal decoding\n                        if res.size(1) == 0:\n                            out = layer(memory)\n                        else:\n                            out = res\n                else:\n                    out = layer(memory)\n\n            return out\n\n        Q = self.Q_dense(queries)\n        K = _update_and_return(self.K_dense, \'keys\')\n        V = _update_and_return(self.V_dense, \'values\')\n\n        Q_ = self._split_heads(Q)\n        K_ = self._split_heads(K)\n        V_ = self._split_heads(V)\n        # [batch_size, num_heads, seq_length, memory_depth]\n        key_depth_per_head = num_units // num_heads\n        Q_ *= key_depth_per_head ** -0.5\n\n        logits = torch.matmul(Q_, K_.transpose(-2, -1))\n        if memory_attention_bias is not None:\n            memory_attention_bias = memory_attention_bias.to(\n                device=logits.device)\n            logits += memory_attention_bias\n        weights = torch.softmax(logits, dim=-1)\n        weights = F.dropout(weights, self._hparams.dropout_rate, self.training)\n        outputs = torch.matmul(weights, V_)\n\n        outputs = self._combine_heads(outputs)\n        outputs = self.O_dense(outputs)\n        # (batch_size, length_query, output_dim)\n\n        return outputs\n\n    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n        r""""""Split channels (dimension 2) into multiple heads,\n        becomes dimension 1). Must ensure ``x.shape[-1]`` can be\n        divided by num_heads.\n        """"""\n        depth = x.size(-1)\n        split_x = torch.reshape(x, (\n            x.size(0), x.size(1),\n            self._hparams.num_heads, depth // self._hparams.num_heads))\n        return split_x.permute((0, 2, 1, 3))\n\n    def _combine_heads(self, x: torch.Tensor) -> torch.Tensor:\n        r""""""\n\n        Args:\n            x: A Tensor of shape ``[batch, num_heads, seq_len, dim]``\n        Returns:\n            A Tensor of shape ``[batch, seq_len, num_heads * dim]``\n        """"""\n        t = x.permute((0, 2, 1, 3))  # [batch, seq_len, num_heads, dim]\n        num_heads, dim = t.size()[-2:]\n        assert num_heads == self._hparams.num_heads\n        return torch.reshape(t, (t.size(0), t.size(1), num_heads * dim))\n\n    @property\n    def output_size(self):\n        r""""""The feature size of :meth:`forward` output.\n        """"""\n        return self._hparams.output_dim\n'"
texar/torch/modules/encoders/rnn_encoders.py,23,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious RNN encoders.\n""""""\n\nfrom typing import Any, Dict, Generic, List, Optional, Tuple, TypeVar, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom texar.torch.core import layers\nfrom texar.torch.core.cell_wrappers import LSTMCell, RNNCellBase\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.modules.encoders.encoder_base import EncoderBase\nfrom texar.torch.modules.networks.conv_networks import _to_list\nfrom texar.torch.utils.rnn import bidirectional_dynamic_rnn, dynamic_rnn\nfrom texar.torch.utils.shapes import mask_sequences\n\n__all__ = [\n    ""_forward_output_layers"",\n    ""RNNEncoderBase"",\n    ""UnidirectionalRNNEncoder"",\n    ""BidirectionalRNNEncoder"",\n]\n\nState = TypeVar(\'State\')\n\n\ndef _default_output_layer_hparams() -> Dict[str, Any]:\n    return {\n        ""num_layers"": 0,\n        ""layer_size"": 128,\n        ""activation"": ""Identity"",\n        ""final_layer_activation"": None,\n        ""other_dense_kwargs"": None,\n        ""dropout_layer_ids"": [],\n        ""dropout_rate"": 0.5,\n        ""variational_dropout"": False,\n        ""@no_typecheck"": [""activation"", ""final_layer_activation"",\n                          ""layer_size"", ""dropout_layer_ids""]\n    }\n\n\ndef _build_dense_output_layer(cell_output_size: int,\n                              hparams: HParams) -> Optional[nn.Sequential]:\n    r""""""Build the output layers.\n\n    Args:\n        cell_output_size: The output size of the rnn cell.\n        hparams (dict or HParams): Hyperparameters. Missing hyperparameters\n            will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    Returns:\n        A :torch_nn:`Sequential` module containing the output layers.\n    """"""\n    nlayers = hparams.num_layers\n\n    if nlayers <= 0:\n        return None\n\n    layer_size = _to_list(\n        hparams.layer_size, \'output_layer.layer_size\', nlayers)\n\n    dropout_layer_ids = _to_list(hparams.dropout_layer_ids)\n\n    other_kwargs = hparams.other_dense_kwargs or {}\n    if isinstance(other_kwargs, HParams):\n        other_kwargs = other_kwargs.todict()\n    if not isinstance(other_kwargs, dict):\n        raise ValueError(\n            ""hparams \'output_layer.other_dense_kwargs\' must be a dict."")\n\n    output_layers: List[nn.Module] = []\n    for i in range(nlayers):\n        if i in dropout_layer_ids:\n            # TODO: Variational dropout is not implemented.\n            output_layers.append(nn.Dropout(p=hparams.dropout_rate))\n\n        dense_layer = nn.Linear(in_features=(cell_output_size if i == 0\n                                             else layer_size[i - 1]),\n                                out_features=layer_size[i], **other_kwargs)\n\n        output_layers.append(dense_layer)\n\n        if i == nlayers - 1:\n            activation = hparams.final_layer_activation\n        else:\n            activation = hparams.activation\n\n        if activation is not None:\n            layer_hparams = {""type"": activation, ""kwargs"": {}}\n            activation_layer = layers.get_layer(hparams=layer_hparams)\n            output_layers.append(activation_layer)\n\n    if nlayers in dropout_layer_ids:\n        output_layers.append(nn.Dropout(p=hparams.dropout_rate))\n\n    return nn.Sequential(*output_layers)\n\n\ndef _forward_output_layers(\n        inputs: torch.Tensor,\n        output_layer: Optional[nn.Module],\n        time_major: bool,\n        sequence_length: Optional[Union[torch.LongTensor, List[int]]] = None) \\\n        -> Tuple[torch.Tensor, int]:\n    r""""""Forwards inputs through the output layers.\n\n    Args:\n        inputs: A Tensor of shape ``[batch_size, max_time] + input_size`` if\n            :attr:`time_major` is `False`, or shape\n            ``[max_time, batch_size] + input_size`` if :attr:`time_major` is\n            `True`.\n        output_layer (optional): :torch_nn:`Sequential` or :torch_nn:`Module`\n            of output layers.\n        time_major (bool): The shape format of the :attr:`inputs` and\n            :attr:`outputs` Tensors. If `True`, these tensors are of shape\n            `[max_time, batch_size, input_size]`. If `False` (default),\n            these tensors are of shape `[batch_size, max_time, input_size]`.\n        sequence_length (optional): A 1D :tensor:`LongTensor` of shape\n            ``[batch_size]``. Sequence lengths of the batch inputs. Used to\n            copy-through state and zero-out outputs when past a batch element\'s\n            sequence length.\n\n    Returns:\n        A pair :attr:`(outputs, outputs_size), where\n\n        - :attr:`outputs`: A Tensor of shape\n        `[batch_size, max_time] + outputs_size`.\n\n        - :attr:`outputs_size`: An `int` representing the output size.\n    """"""\n    if output_layer is None:\n        return inputs, inputs.shape[-1]\n\n    output = output_layer(inputs)\n\n    if sequence_length is not None:\n        output = mask_sequences(output, sequence_length, time_major=time_major)\n\n    output_size = output.shape[-1]\n\n    return output, output_size\n\n\nclass RNNEncoderBase(EncoderBase, Generic[State]):\n    r""""""Base class for all RNN encoder classes to inherit.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n    """"""\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""rnn_encoder""\n            }\n        """"""\n        return {\n            \'name\': \'rnn_encoder\'\n        }\n\n\nclass UnidirectionalRNNEncoder(RNNEncoderBase[State]):\n    r""""""One directional RNN encoder.\n\n    Args:\n        input_size (int): The number of expected features in the input for the\n            cell.\n        cell: (RNNCell, optional) If not specified,\n            a cell is created as specified in :attr:`hparams[""rnn_cell""]`.\n        output_layer (optional): An instance of\n            :torch_nn:`Module`. Applies to the RNN cell\n            output of each step. If `None` (default), the output layer is\n            created as specified in :attr:`hparams[""output_layer""]`.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    See :meth:`forward` for the inputs and outputs of the encoder.\n\n    Example:\n\n    .. code-block:: python\n\n        # Use with embedder\n        embedder = WordEmbedder(vocab_size, hparams=emb_hparams)\n        encoder = UnidirectionalRNNEncoder(hparams=enc_hparams)\n\n        outputs, final_state = encoder(\n            inputs=embedder(data_batch[\'text_ids\']),\n            sequence_length=data_batch[\'length\'])\n\n    .. document private functions\n    """"""\n    _cell: RNNCellBase[State]\n\n    def __init__(self,\n                 input_size: int,\n                 cell: Optional[RNNCellBase[State]] = None,\n                 output_layer: Optional[nn.Module] = None,\n                 hparams=None):\n        super().__init__(hparams=hparams)\n\n        # Make RNN cell\n        if cell is not None:\n            self._cell = cell\n        else:\n            self._cell = layers.get_rnn_cell(input_size,\n                                             self._hparams.rnn_cell)\n\n        # Make output layer\n        self._output_layer: Optional[nn.Module]\n        if output_layer is not None:\n            self._output_layer = output_layer\n            self._output_layer_hparams = None\n        else:\n            self._output_layer = _build_dense_output_layer(\n                self._cell.hidden_size, self._hparams.output_layer)\n            self._output_layer_hparams = self._hparams.output_layer\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""rnn_cell"": default_rnn_cell_hparams(),\n                ""output_layer"": {\n                    ""num_layers"": 0,\n                    ""layer_size"": 128,\n                    ""activation"": ""identity"",\n                    ""final_layer_activation"": None,\n                    ""other_dense_kwargs"": None,\n                    ""dropout_layer_ids"": [],\n                    ""dropout_rate"": 0.5,\n                    ""variational_dropout"": False\n                },\n                ""name"": ""unidirectional_rnn_encoder""\n            }\n\n        Here:\n\n        `""rnn_cell""`: dict\n            A dictionary of RNN cell hyperparameters. Ignored if\n            :attr:`cell` is given to the encoder constructor.\n\n            The default value is defined in\n            :func:`~texar.torch.core.default_rnn_cell_hparams`.\n\n        `""output_layer""`: dict\n            Output layer hyperparameters. Ignored if :attr:`output_layer`\n            is given to the encoder constructor. Includes:\n\n            `""num_layers""`: int\n                The number of output (dense) layers. Set to 0 to avoid any\n                output layers applied to the cell outputs.\n\n            `""layer_size""`: int or list\n                The size of each of the output (dense) layers.\n\n                If an `int`, each output layer will have the same size. If\n                a list, the length must equal to :attr:`num_layers`.\n\n            `""activation""`: str or callable or None\n                Activation function for each of the output (dense)\n                layer except for the final layer. This can be\n                a function, or its string name or module path.\n                If function name is given, the function must be from\n                :mod:`torch.nn`.\n                For example:\n\n                .. code-block:: python\n\n                    ""activation"": ""relu"" # function name\n                    ""activation"": ""my_module.my_activation_fn"" # module path\n                    ""activation"": my_module.my_activation_fn # function\n\n                Default is `None` which results in an identity activation.\n\n            `""final_layer_activation""`: str or callable or None\n                The activation function for the final output layer.\n\n            `""other_dense_kwargs""`: dict or None\n                Other keyword arguments to construct each of the output\n                dense layers, e.g., ``bias``. See\n                :torch_nn:`Linear` for the keyword arguments.\n\n            `""dropout_layer_ids""`: int or list\n                The indexes of layers (starting from 0) whose inputs\n                are applied with dropout. The index = :attr:`num_layers`\n                means dropout applies to the final layer output. For example,\n\n                .. code-block:: python\n\n                    {\n                        ""num_layers"": 2,\n                        ""dropout_layer_ids"": [0, 2]\n                    }\n\n                will leads to a series of layers as\n                `-dropout-layer0-layer1-dropout-`.\n\n                The dropout mode (training or not) is controlled\n                by :attr:`self.training`.\n\n            `""dropout_rate""`: float\n                The dropout rate, between 0 and 1. For example,\n                ``""dropout_rate"": 0.1`` would zero out 10% of elements.\n\n            `""variational_dropout""`: bool\n                Whether the dropout mask is the same across all time steps.\n\n        `""name""`: str\n            Name of the encoder\n        """"""\n        hparams = RNNEncoderBase.default_hparams()\n        hparams.update({\n            ""rnn_cell"": layers.default_rnn_cell_hparams(),\n            ""output_layer"": _default_output_layer_hparams(),\n            ""name"": ""unidirectional_rnn_encoder""\n        })\n        return hparams\n\n    def forward(self,  # type: ignore\n                inputs: torch.Tensor,\n                sequence_length: Optional[Union[torch.LongTensor,\n                                                List[int]]] = None,\n                initial_state: Optional[State] = None,\n                time_major: bool = False,\n                return_cell_output: bool = False,\n                return_output_size: bool = False):\n        r""""""Encodes the inputs.\n\n        Args:\n            inputs: A 3D Tensor of shape ``[batch_size, max_time, dim]``.\n                The first two dimensions\n                :attr:`batch_size` and :attr:`max_time` are exchanged if\n                :attr:`time_major` is `True`.\n            sequence_length (optional): A 1D :tensor:`LongTensor` of shape\n                ``[batch_size]``.\n                Sequence lengths of the batch inputs. Used to copy-through\n                state and zero-out outputs when past a batch element\'s sequence\n                length.\n            initial_state (optional): Initial state of the RNN.\n            time_major (bool): The shape format of the :attr:`inputs` and\n                :attr:`outputs` Tensors. If `True`, these tensors are of shape\n                ``[max_time, batch_size, depth]``. If `False` (default),\n                these tensors are of shape ``[batch_size, max_time, depth]``.\n            return_cell_output (bool): Whether to return the output of the RNN\n                cell. This is the results prior to the output layer.\n            return_output_size (bool): Whether to return the size of the\n                output (i.e., the results after output layers).\n\n        Returns:\n            - By default (both ``return_cell_output`` and ``return_output_size``\n              are `False`), returns a pair :attr:`(outputs, final_state)`,\n              where\n\n              - :attr:`outputs`: The RNN output tensor by the output layer\n                (if exists) or the RNN cell (otherwise). The tensor is of\n                shape ``[batch_size, max_time, output_size]`` if\n                ``time_major`` is `False`, or\n                ``[max_time, batch_size, output_size]`` if\n                ``time_major`` is `True`.\n                If RNN cell output is a (nested) tuple of Tensors, then the\n                :attr:`outputs` will be a (nested) tuple having the same\n                nest structure as the cell output.\n\n              - :attr:`final_state`: The final state of the RNN, which is a\n                Tensor of shape ``[batch_size] + cell.state_size`` or\n                a (nested) tuple of Tensors if ``cell.state_size`` is a\n                (nested) tuple.\n\n            - If ``return_cell_output`` is True, returns a triple\n              :attr:`(outputs, final_state, cell_outputs)`\n\n              - :attr:`cell_outputs`: The outputs by the RNN cell prior to the\n                output layer, having the same structure with :attr:`outputs`\n                except for the ``output_dim``.\n\n            - If ``return_output_size`` is `True`, returns a tuple\n              :attr:`(outputs, final_state, output_size)`\n\n              - :attr:`output_size`: A (possibly nested tuple of) int\n                representing the size of :attr:`outputs`. If a single int or\n                an int array, then ``outputs`` has shape\n                ``[batch/time, time/batch] + output_size``. If\n                a (nested) tuple, then ``output_size`` has the same\n                structure as with ``outputs``.\n\n            - If both ``return_cell_output`` and ``return_output_size`` are\n              `True`, returns\n              :attr:`(outputs, final_state, cell_outputs, output_size)`.\n        """"""\n\n        cell_outputs, state = dynamic_rnn(\n            cell=self._cell,\n            inputs=inputs,\n            sequence_length=sequence_length,\n            initial_state=initial_state,\n            time_major=time_major)\n\n        outputs, output_size = _forward_output_layers(\n            inputs=cell_outputs,\n            output_layer=self._output_layer,\n            time_major=time_major,\n            sequence_length=sequence_length)\n\n        rets = (outputs, state)\n        if return_cell_output:\n            rets += (cell_outputs,)  # type: ignore\n        if return_output_size:\n            rets += (output_size,)  # type: ignore\n        return rets\n\n    @property\n    def cell(self) -> RNNCellBase[State]:\n        r""""""The RNN cell.\n        """"""\n        return self._cell\n\n    @property\n    def state_size(self) -> int:\n        r""""""The state size of encoder cell.\n        Same as :attr:`encoder.cell.state_size`.\n        """"""\n        if isinstance(self._cell, LSTMCell):\n            return 2 * self._cell.hidden_size  # type: ignore\n        else:\n            return self._cell.hidden_size\n\n    @property\n    def output_layer(self) -> Optional[nn.Module]:\n        r""""""The output layer.\n        """"""\n        return self._output_layer\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of :meth:`forward` output :attr:`outputs`.\n        If output layer does not exist, the feature size is equal to\n        :attr:`encoder.cell.hidden_size`, otherwise the feature size\n        is equal to last dimension value of output layer output size.\n        """"""\n        # TODO: We will change the implementation to\n        # something that does not require a forward pass.\n\n        dim = self._cell.hidden_size\n        if self._output_layer is not None:\n            dummy_tensor = torch.Tensor(dim)\n            dim = self._output_layer(dummy_tensor).size(-1)\n        return dim\n\n\nclass BidirectionalRNNEncoder(RNNEncoderBase):\n    r""""""Bidirectional forward-backward RNN encoder.\n\n    Args:\n        cell_fw (RNNCell, optional): The forward RNN cell. If not given,\n            a cell is created as specified in ``hparams[""rnn_cell_fw""]``.\n        cell_bw (RNNCell, optional): The backward RNN cell. If not given,\n            a cell is created as specified in ``hparams[""rnn_cell_bw""]``.\n        output_layer_fw (optional): An instance of\n            :torch_nn:`Module`. Apply to the forward\n            RNN cell output of each step. If `None` (default), the output\n            layer is created as specified in ``hparams[""output_layer_fw""]``.\n        output_layer_bw (optional): An instance of\n            :torch_nn:`Module`. Apply to the backward\n            RNN cell output of each step. If `None` (default), the output\n            layer is created as specified in ``hparams[""output_layer_bw""]``.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    See :meth:`forward` for the inputs and outputs of the encoder.\n\n    Example:\n\n        .. code-block:: python\n\n            # Use with embedder\n            embedder = WordEmbedder(vocab_size, hparams=emb_hparams)\n            encoder = BidirectionalRNNEncoder(hparams=enc_hparams)\n\n            outputs, final_state = encoder(\n                inputs=embedder(data_batch[\'text_ids\']),\n                sequence_length=data_batch[\'length\'])\n            # outputs == (outputs_fw, outputs_bw)\n            # final_state == (final_state_fw, final_state_bw)\n\n    .. document private functions\n    """"""\n\n    def __init__(self,\n                 input_size: int,\n                 cell_fw: Optional[RNNCellBase[State]] = None,\n                 cell_bw: Optional[RNNCellBase[State]] = None,\n                 output_layer_fw: Optional[nn.Module] = None,\n                 output_layer_bw: Optional[nn.Module] = None,\n                 hparams=None):\n        super().__init__(hparams=hparams)\n\n        # Make RNN cells\n        if cell_fw is not None:\n            self._cell_fw = cell_fw\n        else:\n            self._cell_fw = layers.get_rnn_cell(input_size,\n                                                self._hparams.rnn_cell_fw)\n\n        if cell_bw is not None:\n            self._cell_bw = cell_bw\n        elif self._hparams.rnn_cell_share_config:\n            self._cell_bw = layers.get_rnn_cell(input_size,\n                                                self._hparams.rnn_cell_fw)\n        else:\n            self._cell_bw = layers.get_rnn_cell(input_size,\n                                                self._hparams.rnn_cell_bw)\n\n        # Make output layers\n\n        self.__output_layer_fw: Optional[nn.Module]\n        if output_layer_fw is not None:\n            self._output_layer_fw = output_layer_fw\n            self._output_layer_hparams_fw = None\n        else:\n            self._output_layer_fw = _build_dense_output_layer(  # type: ignore\n                self._cell_fw.hidden_size, self._hparams.output_layer_fw)\n            self._output_layer_hparams_fw = self._hparams.output_layer_fw\n\n        self.__output_layer_bw: Optional[nn.Module]\n        if output_layer_bw is not None:\n            self._output_layer_bw = output_layer_bw\n            self._output_layer_hparams_bw = None\n        elif self._hparams.output_layer_share_config:\n            self._output_layer_bw = _build_dense_output_layer(  # type: ignore\n                self._cell_bw.hidden_size, self._hparams.output_layer_fw)\n            self._output_layer_hparams_bw = self._hparams.output_layer_fw\n        else:\n            self._output_layer_bw = _build_dense_output_layer(  # type: ignore\n                self._cell_bw.hidden_size, self._hparams.output_layer_bw)\n            self._output_layer_hparams_bw = self._hparams.output_layer_bw\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""rnn_cell_fw"": default_rnn_cell_hparams(),\n                ""rnn_cell_bw"": default_rnn_cell_hparams(),\n                ""rnn_cell_share_config"": True,\n                ""output_layer_fw"": {\n                    ""num_layers"": 0,\n                    ""layer_size"": 128,\n                    ""activation"": ""identity"",\n                    ""final_layer_activation"": None,\n                    ""other_dense_kwargs"": None,\n                    ""dropout_layer_ids"": [],\n                    ""dropout_rate"": 0.5,\n                    ""variational_dropout"": False\n                },\n                ""output_layer_bw"": {\n                    # Same hyperparams and default values as ""output_layer_fw""\n                    # ...\n                },\n                ""output_layer_share_config"": True,\n                ""name"": ""bidirectional_rnn_encoder""\n            }\n\n        Here:\n\n        `""rnn_cell_fw""`: dict\n            Hyperparameters of the forward RNN cell.\n            Ignored if :attr:`cell_fw` is given to the encoder constructor.\n\n            The default value is defined in\n            :func:`~texar.torch.core.default_rnn_cell_hparams`.\n\n        `""rnn_cell_bw""`: dict\n            Hyperparameters of the backward RNN cell.\n            Ignored if :attr:`cell_bw` is given to the encoder constructor,\n            or if `""rnn_cell_share_config""` is `True`.\n\n            The default value is defined in\n            :meth:`~texar.torch.core.default_rnn_cell_hparams`.\n\n        `""rnn_cell_share_config""`: bool\n            Whether share hyperparameters of the backward cell with the\n            forward cell. Note that the cell parameters (variables) are not\n            shared.\n\n        `""output_layer_fw""`: dict\n            Hyperparameters of the forward output layer. Ignored if\n            ``output_layer_fw`` is given to the constructor.\n            See the ``""output_layer""`` field of\n            :meth:`~texar.torch.modules.UnidirectionalRNNEncoder` for details.\n\n        `""output_layer_bw""`: dict\n            Hyperparameters of the backward output layer. Ignored if\n            :attr:`output_layer_bw` is given to the constructor. Have the\n            same structure and defaults with :attr:`""output_layer_fw""`.\n\n            Ignored if ``output_layer_share_config`` is `True`.\n\n        `""output_layer_share_config""`: bool\n            Whether share hyperparameters of the backward output layer\n            with the forward output layer. Note that the layer parameters\n            (variables) are not shared.\n\n        `""name""`: str\n            Name of the encoder\n        """"""\n        hparams = RNNEncoderBase.default_hparams()\n        hparams.update({\n            ""rnn_cell_fw"": layers.default_rnn_cell_hparams(),\n            ""rnn_cell_bw"": layers.default_rnn_cell_hparams(),\n            ""rnn_cell_share_config"": True,\n            ""output_layer_fw"": _default_output_layer_hparams(),\n            ""output_layer_bw"": _default_output_layer_hparams(),\n            ""output_layer_share_config"": True,\n            ""name"": ""bidirectional_rnn_encoder""\n        })\n        return hparams\n\n    def forward(self,  # type: ignore\n                inputs: torch.Tensor,\n                sequence_length: Optional[Union[torch.LongTensor,\n                                                List[int]]] = None,\n                initial_state_fw: Optional[State] = None,\n                initial_state_bw: Optional[State] = None,\n                time_major: bool = False,\n                return_cell_output: bool = False,\n                return_output_size: bool = False):\n        r""""""Encodes the inputs.\n\n        Args:\n            inputs: A 3D Tensor of shape ``[batch_size, max_time, dim]``.\n                The first two dimensions\n                ``batch_size`` and ``max_time`` may be exchanged if\n                ``time_major`` is `True`.\n            sequence_length (optional): A 1D :tensor:`LongTensor` of shape\n                ``[batch_size]``.\n                Sequence lengths of the batch inputs. Used to copy-through\n                state and zero-out outputs when past a batch element\'s sequence\n                length.\n            initial_state_fw: (optional): Initial state of the forward RNN.\n            initial_state_bw: (optional): Initial state of the backward RNN.\n            time_major (bool): The shape format of the :attr:`inputs` and\n                :attr:`outputs` Tensors. If `True`, these tensors are of shape\n                ``[max_time, batch_size, depth]``. If `False` (default),\n                these tensors are of shape ``[batch_size, max_time, depth]``.\n            return_cell_output (bool): Whether to return the output of the RNN\n                cell. This is the results prior to the output layer.\n            return_output_size (bool): Whether to return the output size of the\n                RNN cell. This is the results after the output layer.\n\n        Returns:\n            - By default (both ``return_cell_output`` and ``return_output_size``\n              are `False`), returns a pair :attr:`(outputs, final_state)`\n\n              - :attr:`outputs`: A tuple ``(outputs_fw, outputs_bw)``\n                containing the forward and the backward RNN outputs, each of\n                which is of shape ``[batch_size, max_time, output_dim]``\n                if ``time_major`` is `False`, or\n                ``[max_time, batch_size, output_dim]`` if ``time_major``\n                is `True`.\n                If RNN cell output is a (nested) tuple of Tensors, then\n                ``outputs_fw`` and ``outputs_bw`` will be a (nested) tuple\n                having the same structure as the cell output.\n\n              - :attr:`final_state`: A tuple\n                ``(final_state_fw, final_state_bw)`` containing the final\n                states of the forward and backward RNNs, each of which is a\n                Tensor of shape ``[batch_size] + cell.state_size``, or a\n                (nested) tuple of Tensors if ``cell.state_size`` is a (nested)\n                tuple.\n\n            - If ``return_cell_output`` is `True`, returns a triple\n              :attr:`(outputs, final_state, cell_outputs)` where\n\n              - :attr:`cell_outputs`: A tuple\n                ``(cell_outputs_fw, cell_outputs_bw)`` containing the outputs\n                by the forward and backward RNN cells prior to the output\n                layers, having the same structure with :attr:`outputs` except\n                for the ``output_dim``.\n\n            - If ``return_output_size`` is `True`, returns a tuple\n              :attr:`(outputs, final_state, output_size)` where\n\n              - :attr:`output_size`: A tuple\n                ``(output_size_fw, output_size_bw)`` containing the size of\n                ``outputs_fw`` and ``outputs_bw``, respectively.\n                Take ``*_fw`` for example, ``output_size_fw`` is a (possibly\n                nested tuple of) int. If a single int or an int array, then\n                ``outputs_fw`` has shape\n                ``[batch/time, time/batch] + output_size_fw``. If a (nested)\n                tuple, then ``output_size_fw`` has the same structure as\n                ``outputs_fw``. The same applies to ``output_size_bw``.\n\n            - If both ``return_cell_output`` and ``return_output_size`` are\n              `True`, returns\n              :attr:`(outputs, final_state, cell_outputs, output_size)`.\n        """"""\n\n        cell_outputs, states = bidirectional_dynamic_rnn(\n            cell_fw=self._cell_fw,\n            cell_bw=self._cell_bw,\n            inputs=inputs,\n            sequence_length=sequence_length,\n            initial_state_fw=initial_state_fw,\n            initial_state_bw=initial_state_bw,\n            time_major=time_major)\n\n        outputs_fw, output_size_fw = _forward_output_layers(\n            inputs=cell_outputs[0],\n            output_layer=self._output_layer_fw,\n            time_major=time_major,\n            sequence_length=sequence_length)\n\n        outputs_bw, output_size_bw = _forward_output_layers(\n            inputs=cell_outputs[1],\n            output_layer=self._output_layer_bw,\n            time_major=time_major,\n            sequence_length=sequence_length)\n\n        outputs = (outputs_fw, outputs_bw)\n        output_size = (output_size_fw, output_size_bw)\n\n        returns = (outputs, states)\n        if return_cell_output:\n            returns += (cell_outputs,)  # type: ignore\n        if return_output_size:\n            returns += (output_size,)  # type: ignore\n        return returns\n\n    @property\n    def cell_fw(self) -> RNNCellBase[State]:\n        r""""""The forward RNN cell.\n        """"""\n        return self._cell_fw\n\n    @property\n    def cell_bw(self) -> RNNCellBase[State]:\n        r""""""The backward RNN cell.\n        """"""\n        return self._cell_bw\n\n    @property\n    def state_size_fw(self) -> int:\n        r""""""The state size of the forward encoder cell.\n        Same as :attr:`encoder.cell_fw.state_size`.\n        """"""\n        if isinstance(self._cell_fw, LSTMCell):\n            return 2 * self._cell_fw.hidden_size  # type: ignore\n        else:\n            return self._cell_fw.hidden_size\n\n    @property\n    def state_size_bw(self) -> int:\n        r""""""The state size of the backward encoder cell.\n        Same as :attr:`encoder.cell_bw.state_size`.\n        """"""\n        if isinstance(self._cell_bw, LSTMCell):\n            return 2 * self._cell_bw.hidden_size  # type: ignore\n        else:\n            return self._cell_bw.hidden_size\n\n    @property\n    def output_layer_fw(self) -> Optional[nn.Module]:\n        r""""""The output layer of the forward RNN.\n        """"""\n        return self._output_layer_fw\n\n    @property\n    def output_layer_bw(self) -> Optional[nn.Module]:\n        r""""""The output layer of the backward RNN.\n        """"""\n        return self._output_layer_bw\n\n    @property\n    def output_size(self) -> Tuple[int, int]:\n        r""""""The feature sizes of :meth:`forward` outputs\n        :attr:`output_size_fw` and :attr:`output_size_bw`.\n        Each feature size is equal to last dimension\n        value of corresponding result size.\n        """"""\n        # TODO: We will change the implementation to\n        # something that does not require a forward pass.\n        dim_bw = self._cell_bw.hidden_size\n        dim_fw = self._cell_fw.hidden_size\n        if self._output_layer_bw is not None:\n            dummy_tensor_bw = torch.Tensor(dim_bw)\n            output_bw = self._output_layer_bw(dummy_tensor_bw).size()[-1]\n        else:\n            output_bw = dim_bw\n        if self._output_layer_fw is not None:\n            dummy_tensor_fw = torch.Tensor(dim_fw)\n            output_fw = self._output_layer_fw(dummy_tensor_fw).size()[-1]\n        else:\n            output_fw = dim_fw\n        return (output_fw, output_bw)\n'"
texar/torch/modules/encoders/roberta_encoder.py,12,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nRoBERTa encoder.\n""""""\n\nfrom typing import Optional, Union\n\nimport torch\n\nfrom texar.torch.modules.encoders.bert_encoder import BERTEncoder\nfrom texar.torch.modules.pretrained.roberta import \\\n    PretrainedRoBERTaMixin\n\n__all__ = [\n    ""RoBERTaEncoder"",\n]\n\n\nclass RoBERTaEncoder(PretrainedRoBERTaMixin, BERTEncoder):\n    r""""""RoBERTa Transformer for encoding sequences. Please see\n    :class:`~texar.torch.modules.PretrainedRoBERTaMixin` for a brief description\n    of RoBERTa.\n\n    This module basically stacks\n    :class:`~texar.torch.modules.WordEmbedder`,\n    :class:`~texar.torch.modules.PositionEmbedder`,\n    :class:`~texar.torch.modules.TransformerEncoder` and a dense\n    pooler.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``roberta-base``). Please refer to\n            :class:`~texar.torch.modules.PretrainedRoBERTaMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The encoder arch is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the encoder arch is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the encoder arch is defined by the\n          configurations in `hparams` and weights are randomly initialized.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""roberta-base"",\n                ""embed"": {\n                    ""dim"": 768,\n                    ""name"": ""word_embeddings""\n                },\n                ""vocab_size"": 50265,\n                ""position_embed"": {\n                    ""dim"": 768,\n                    ""name"": ""position_embeddings""\n                },\n                ""position_size"": 514,\n\n                ""encoder"": {\n                    ""dim"": 768,\n                    ""embedding_dropout"": 0.1,\n                    ""multihead_attention"": {\n                        ""dropout_rate"": 0.1,\n                        ""name"": ""self"",\n                        ""num_heads"": 12,\n                        ""num_units"": 768,\n                        ""output_dim"": 768,\n                        ""use_bias"": True\n                    },\n                    ""name"": ""encoder"",\n                    ""num_blocks"": 12,\n                    ""eps"": 1e-12,\n                    ""poswise_feedforward"": {\n                        ""layers"": [\n                            {\n                                ""kwargs"": {\n                                    ""in_features"": 768,\n                                    ""out_features"": 3072,\n                                    ""bias"": True\n                                },\n                                ""type"": ""Linear""\n                            },\n                            {""type"": ""BertGELU""},\n                            {\n                                ""kwargs"": {\n                                    ""in_features"": 3072,\n                                    ""out_features"": 768,\n                                    ""bias"": True\n                                },\n                                ""type"": ""Linear""\n                            }\n                        ]\n                    },\n                    ""residual_dropout"": 0.1,\n                    ""use_bert_config"": True\n                    },\n                ""hidden_size"": 768,\n                ""initializer"": None,\n                ""name"": ""roberta_encoder"",\n            }\n\n        Here:\n\n        The default parameters are values for RoBERTa-Base model.\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained RoBERTa model. If None, the model\n            will be randomly initialized.\n\n        `""embed""`: dict\n            Hyperparameters for word embedding layer.\n\n        `""vocab_size""`: int\n            The vocabulary size of `inputs` in RoBERTa model.\n\n        `""position_embed""`: dict\n            Hyperparameters for position embedding layer.\n\n        `""position_size""`: int\n            The maximum sequence length that this model might ever be used with.\n\n        `""encoder""`: dict\n            Hyperparameters for the TransformerEncoder.\n            See :func:`~texar.torch.modules.TransformerEncoder.default_hparams`\n            for details.\n\n        `""hidden_size""`: int\n            Size of the pooler dense layer.\n\n        `""eps""`: float\n            Epsilon values for layer norm layers.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n\n        return {\n            \'pretrained_model_name\': \'roberta-base\',\n            \'embed\': {\n                \'dim\': 768,\n                \'name\': \'word_embeddings\'\n            },\n            \'vocab_size\': 50265,\n            \'position_embed\': {\n                \'dim\': 768,\n                \'name\': \'position_embeddings\'\n            },\n            \'position_size\': 514,\n\n            \'encoder\': {\n                \'dim\': 768,\n                \'embedding_dropout\': 0.1,\n                \'multihead_attention\': {\n                    \'dropout_rate\': 0.1,\n                    \'name\': \'self\',\n                    \'num_heads\': 12,\n                    \'num_units\': 768,\n                    \'output_dim\': 768,\n                    \'use_bias\': True\n                },\n                \'name\': \'encoder\',\n                \'num_blocks\': 12,\n                \'eps\': 1e-12,\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 768,\n                                \'out_features\': 3072,\n                                \'bias\': True\n                            },\n                            \'type\': \'Linear\'\n                        },\n                        {""type"": ""BertGELU""},\n                        {\n                            \'kwargs\': {\n                                \'in_features\': 3072,\n                                \'out_features\': 768,\n                                \'bias\': True\n                            },\n                            \'type\': \'Linear\'\n                        }\n                    ]\n                },\n                \'residual_dropout\': 0.1,\n                \'use_bert_config\': True\n            },\n            \'hidden_size\': 768,\n            \'initializer\': None,\n            \'name\': \'roberta_encoder\',\n            \'@no_typecheck\': [\'pretrained_model_name\']\n        }\n\n    def forward(self,  # type: ignore\n                inputs: Union[torch.Tensor, torch.LongTensor],\n                sequence_length: Optional[torch.LongTensor] = None,\n                segment_ids: Optional[torch.LongTensor] = None):\n        r""""""Encodes the inputs. Differing from the standard BERT, the RoBERTa\n        model does not use segmentation embedding. As a result, RoBERTa does not\n        require `segment_ids` as an input.\n\n        Args:\n            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,\n                containing the ids of tokens in input sequences, or\n                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,\n                containing soft token ids (i.e., weights or probabilities)\n                used to mix the embedding vectors.\n            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.\n                Input tokens beyond respective sequence lengths are masked\n                out automatically.\n\n        Returns:\n            A pair :attr:`(outputs, pooled_output)`\n\n            - :attr:`outputs`:  A Tensor of shape\n              `[batch_size, max_time, dim]` containing the encoded vectors.\n\n            - :attr:`pooled_output`: A Tensor of size\n              `[batch_size, hidden_size]` which is the output of a pooler\n              pre-trained on top of the hidden state associated to the first\n              character of the input (`CLS`), see RoBERTa\'s paper.\n        """"""\n        if segment_ids is not None:\n            raise ValueError(""segment_ids should be None in RoBERTaEncoder."")\n\n        output, pooled_output = super().forward(inputs=inputs,\n                                                sequence_length=sequence_length,\n                                                segment_ids=None)\n\n        return output, pooled_output\n'"
texar/torch/modules/encoders/t5_encoder.py,12,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\nfrom texar.torch.modules.encoders.transformer_encoder import TransformerEncoder\nfrom texar.torch.modules.pretrained.t5_utils import \\\n    T5LayerNorm, MultiheadRPRAttention\nfrom texar.torch.modules.networks.networks import FeedForwardNetwork\nfrom texar.torch.modules.encoders.transformer_encoder import \\\n    default_transformer_poswise_net_hparams\nfrom texar.torch.utils import sequence_mask, transformer_attentions as attn\n\n\nclass T5Encoder(TransformerEncoder):\n    r""""""Transformer based encoder that applies multi-head self attention with\n    relative positional representations for encoding sequences for T5.\n\n    This module basically stacks\n    :class:`~texar.torch.modules.pretrained.t5_utils.MultiheadRPRAttention`,\n    :class:`~texar.torch.modules.FeedForwardNetwork` and residual connections.\n    This module supports the standard T5 architecture proposed in\n    `(Raffel et al.) ""Exploring the Limits of Transfer Learning with a Unified\n    Text-to-Text Transformer""`.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    .. document private functions\n    """"""\n    def __init__(self, hparams=None):\n        super().__init__(hparams=hparams)\n\n        self.final_layer_norm = T5LayerNorm(self._input_size,\n                                            eps=self._hparams.eps)\n\n    def initialize_blocks(self):\n        r""""""Helper function to initialize blocks.\n        """"""\n        for i in range(self._hparams.num_blocks):\n            mh_attn = MultiheadRPRAttention(\n                self._input_size,\n                self._hparams.multihead_attention,\n                stores_relative_position=bool(i == 0)\n            )\n            self.self_attns.append(mh_attn)\n\n            self.self_attn_layer_norm.append(\n                T5LayerNorm(self._input_size, eps=self._hparams.eps))\n            if self._hparams.dim != mh_attn.hparams.output_dim:\n                raise ValueError(\n                    \'The ""dim"" in the hparams of \'\n                    \'""multihead_attention"" should be equal to the \'\n                    \'""dim"" of T5Encoder\')\n\n            pw_net = FeedForwardNetwork(\n                hparams=self._hparams[\'poswise_feedforward\'])\n\n            final_dim = pw_net.hparams.layers[-1][\'kwargs\'][\'out_features\']\n            if self._hparams.dim != final_dim:\n                raise ValueError(\n                    \'The output dimenstion of \'\n                    \'""poswise_feedforward"" should be equal \'\n                    \'to the ""dim"" of T5Encoder.\')\n\n            self.poswise_networks.append(pw_net)\n            self.poswise_layer_norm.append(\n                T5LayerNorm(self._input_size, eps=self._hparams.eps))\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""num_blocks"": 6,\n                ""dim"": 512,\n                ""embedding_dropout"": 0.1,\n                ""residual_dropout"": 0.1,\n                ""use_bert_config: False,\n                ""poswise_feedforward"": default_transformer_poswise_net_hparams,\n                \'multihead_attention\': {\n                    \'name\': \'multihead_rpr_attention\',\n                    \'num_units\': 512,\n                    \'num_heads\': 8,\n                    \'dropout_rate\': 0.1,\n                    \'output_dim\': 512,\n                    \'use_bias\': False,\n                    \'is_decoder\': False,\n                    \'relative_attention_num_buckets\': 32\n                },\n                ""initializer"": None,\n                ""eps"": 1e-6,\n                ""name"": ""t5_encoder""\n            }\n\n        Here:\n\n        `""num_blocks""`: int\n            Number of stacked blocks.\n\n        `""dim""`: int\n            Hidden dimension of the encoders.\n\n        `""embedding_dropout""`: float\n            Dropout rate of the input embedding.\n\n        `""residual_dropout""`: float\n            Dropout rate of the residual connections.\n\n        ""eps""`: float\n            Epsilon values for layer norm layers.\n\n        `""poswise_feedforward""`: dict\n            Hyperparameters for a feed-forward network used in residual\n            connections.\n            Make sure the dimension of the output tensor is equal to ``""dim""``.\n            See\n            :func:`~texar.torch.modules.default_transformer_poswise_net_hparams`\n            for details.\n\n        `""multihead_attention""`: dict\n            Hyperparameters for the multi-head attention strategy.\n            Make sure the ``""output_dim""`` in this module is equal to ``""dim""``.\n            See :class:`~texar.torch.modules.MultiheadRPRAttention` for\n            details.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n        dim = 512\n        return {\n            \'num_blocks\': 6,\n            \'dim\': dim,\n            \'embedding_dropout\': 0.1,\n            \'residual_dropout\': 0.1,\n            \'use_bert_config\': False,\n            \'poswise_feedforward\': default_transformer_poswise_net_hparams(dim),\n            \'multihead_attention\': {\n                \'name\': \'multihead_rpr_attention\',\n                \'num_units\': 512,\n                \'num_heads\': 8,\n                \'dropout_rate\': 0.1,\n                \'output_dim\': 512,\n                \'use_bias\': False,\n                \'is_decoder\': False,\n                \'relative_attention_num_buckets\': 32\n            },\n            \'initializer\': None,\n            \'eps\': 1e-6,\n            \'name\': \'t5_encoder\',\n        }\n\n    def forward(self,  # type: ignore\n                inputs: torch.Tensor,\n                sequence_length: torch.LongTensor) -> torch.Tensor:\n        r""""""Encodes the inputs.\n\n        Args:\n            inputs: A 3D Tensor of shape ``[batch_size, max_time, dim]``,\n                containing the embedding of input sequences. Note that\n                the embedding dimension `dim` must equal `""dim""` in\n                :attr:`hparams`. The input embedding is typically an\n                aggregation of word embedding and position embedding.\n            sequence_length: A 1D :tensor:`LongTensor` of shape\n                ``[batch_size]``. Input tokens beyond respective sequence\n                lengths are masked out automatically.\n\n        Returns:\n            A Tensor of shape ``[batch_size, max_time, dim]`` containing the\n            encoded vectors.\n        """"""\n        # Multiply input embedding with the sqrt of its dimension for\n        # normalization\n\n        inputs_padding = 1 - sequence_mask(\n            sequence_length, inputs.size()[1]).float()\n        ignore_padding = attn.attention_bias_ignore_padding(inputs_padding)\n        encoder_self_attention_bias = ignore_padding\n\n        x = self.embed_dropout(inputs)\n\n        position_bias = None\n\n        for i in range(self._hparams.num_blocks):\n            _queries_input = self.self_attn_layer_norm[i](x)\n\n            attention_output, position_bias = self.self_attns[i](\n                queries=_queries_input,\n                memory=_queries_input,\n                memory_attention_bias=encoder_self_attention_bias,\n                position_bias=position_bias\n            )\n\n            attention_output = self.residual_dropout(attention_output)\n\n            x = x + attention_output\n\n            poswise_network = self.poswise_networks[i]\n            poswise_normalizer = self.poswise_layer_norm[i]\n\n            y = poswise_normalizer(x)\n\n            original_shape = y.size()\n\n            y = y.view(-1, self._hparams.dim)\n\n            layer_output = poswise_network(y)\n            sub_output = self.residual_dropout(layer_output)\n            sub_output = sub_output.view(original_shape)\n\n            x = x + sub_output\n\n        x = self.final_layer_norm(x)\n\n        return x\n'"
texar/torch/modules/encoders/transformer_encoder.py,16,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTransformer encoders with multi-head self attention.\n""""""\n\nfrom typing import Any, Dict\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.core import layers\nfrom texar.torch.modules.encoders.encoder_base import EncoderBase\nfrom texar.torch.modules.encoders.multihead_attention import (\n    MultiheadAttentionEncoder)\nfrom texar.torch.modules.networks.networks import FeedForwardNetwork\nfrom texar.torch.utils import transformer_attentions as attn\nfrom texar.torch.utils.utils import sequence_mask\n\n__all__ = [\n    ""default_transformer_poswise_net_hparams"",\n    ""TransformerEncoder"",\n]\n\n\ndef default_transformer_poswise_net_hparams(input_dim: int,\n                                            output_dim: int = 512) \\\n        -> Dict[str, Any]:\n    r""""""Returns default hyperparameters of a\n    :class:`~texar.torch.modules.FeedForwardNetwork` as a position-wise network\n    used in :class:`~texar.torch.modules.TransformerEncoder` and\n    :class:`~texar.torch.modules.TransformerDecoder`.\n    This is a 2-layer dense network with dropout in-between.\n\n    .. code-block:: python\n\n        {\n            ""layers"": [\n                {\n                    ""type"": ""Linear"",\n                    ""kwargs"": {\n                        ""in_features"": input_dim,\n                        ""out_features"": output_dim * 4,\n                        ""bias"": True,\n                    }\n                },\n                {\n                    ""type"": ""nn.ReLU"",\n                    ""kwargs"": {\n                        ""inplace"": True\n                    }\n                },\n                {\n                    ""type"": ""Dropout"",\n                    ""kwargs"": {\n                        ""p"": 0.1,\n                    }\n                },\n                {\n                    ""type"": ""Linear"",\n                    ""kwargs"": {\n                        ""in_features"": output_dim * 4,\n                        ""out_features"": output_dim,\n                        ""bias"": True,\n                    }\n                }\n            ],\n            ""name"": ""ffn""\n        }\n\n    Args:\n        input_dim (int): The size of dense layer input.\n        output_dim (int): The size of dense layer output.\n    """"""\n    return {\n        ""layers"": [\n            {\n                ""type"": ""Linear"",\n                ""kwargs"": {\n                    ""in_features"": input_dim,\n                    ""out_features"": output_dim * 4,\n                    ""bias"": True,\n                }\n            },\n            {\n                ""type"": ""ReLU"",\n                ""kwargs"": {\n                    ""inplace"": True\n                }\n            },\n            {\n                ""type"": ""Dropout"",\n                ""kwargs"": {\n                    ""p"": 0.1,\n                }\n            },\n            {\n                ""type"": ""Linear"",\n                ""kwargs"": {\n                    ""in_features"": output_dim * 4,\n                    ""out_features"": output_dim,\n                    ""bias"": True,\n                }\n            }\n        ],\n        ""name"": ""ffn""\n    }\n\n\nclass TransformerEncoder(EncoderBase):\n    r""""""Transformer encoder that applies multi-head self attention for encoding\n    sequences.\n\n    This module basically stacks\n    :class:`~texar.torch.modules.MultiheadAttentionEncoder`,\n    :class:`~texar.torch.modules.FeedForwardNetwork` and residual connections.\n    This module supports two types of architectures, namely, the standard\n    Transformer Encoder architecture first proposed in\n    `(Vaswani et al.) ""Attention is All You Need""`, and\n    the variant first used in `(Devlin et al.)` BERT. See\n    :meth:`default_hparams` for the nuance between the two types of\n    architectures.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    .. document private functions\n    """"""\n\n    def __init__(self, hparams=None):\n        super().__init__(hparams=hparams)\n        self._input_size = self._hparams.dim\n        self.self_attns = nn.ModuleList()\n        if not self._hparams.use_bert_config:\n            self.self_attn_layer_norm = nn.ModuleList()\n        else:\n            self.output_layer_norm = nn.ModuleList()\n        self.poswise_networks = nn.ModuleList()\n        self.poswise_layer_norm = nn.ModuleList()\n\n        # In PyTorch, eps for LayerNorm is 1e-6 by default.\n        eps = self._hparams.eps\n\n        self.initialize_blocks()\n\n        self.embed_dropout = nn.Dropout(p=self._hparams.embedding_dropout)\n        self.residual_dropout = nn.Dropout(p=self._hparams.residual_dropout)\n\n        if self._hparams.use_bert_config:\n            self.input_normalizer = nn.LayerNorm(self._input_size, eps=eps)\n        else:\n            self.final_layer_norm = nn.LayerNorm(self._input_size, eps=eps)\n\n        if self._hparams.initializer:\n            initialize = layers.get_initializer(self._hparams.initializer)\n            assert initialize is not None\n            # Do not re-initialize LayerNorm modules.\n            for name, param in self.named_parameters():\n                if name.split(\'.\')[-1] == \'weight\' and \'layer_norm\' not in name:\n                    initialize(param)\n\n    def initialize_blocks(self):\n        r""""""Helper function which initializes blocks for encoder.\n\n        Should be overridden by any classes where block initialization varies.\n        """"""\n        for _ in range(self._hparams.num_blocks):\n            mh_attn = MultiheadAttentionEncoder(\n                self._input_size, self._hparams.multihead_attention)\n            self.self_attns.append(mh_attn)\n            if not self._hparams.use_bert_config:\n                self.self_attn_layer_norm.append(\n                    nn.LayerNorm(self._input_size, eps=self._hparams.eps))\n            if self._hparams.dim != mh_attn.hparams.output_dim:\n                raise ValueError(\n                    \'The ""dim"" in the hparams of \'\n                    \'""multihead_attention"" should be equal to the \'\n                    \'""dim"" of TransformerEncoder\')\n\n            pw_net = FeedForwardNetwork(\n                hparams=self._hparams[\'poswise_feedforward\'])\n\n            final_dim = pw_net.hparams.layers[-1][\'kwargs\'][\'out_features\']\n            if self._hparams.dim != final_dim:\n                raise ValueError(\n                    \'The output dimenstion of \'\n                    \'""poswise_feedforward"" should be equal \'\n                    \'to the ""dim"" of TransformerEncoder.\')\n\n            self.poswise_networks.append(pw_net)\n            self.poswise_layer_norm.append(\n                nn.LayerNorm(self._input_size, eps=self._hparams.eps))\n            if self._hparams.use_bert_config:\n                self.output_layer_norm.append(\n                    nn.LayerNorm(self._input_size, eps=self._hparams.eps))\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""num_blocks"": 6,\n                ""dim"": 512,\n                \'use_bert_config\': False,\n                ""embedding_dropout"": 0.1,\n                ""residual_dropout"": 0.1,\n                ""poswise_feedforward"": default_transformer_poswise_net_hparams,\n                \'multihead_attention\': {\n                    \'name\': \'multihead_attention\',\n                    \'num_units\': 512,\n                    \'num_heads\': 8,\n                    \'dropout_rate\': 0.1,\n                    \'output_dim\': 512,\n                    \'use_bias\': False,\n                },\n                ""eps"": 1e-6,\n                ""initializer"": None,\n                ""name"": ""transformer_encoder""\n            }\n\n        Here:\n\n        `""num_blocks""`: int\n            Number of stacked blocks.\n\n        `""dim""`: int\n            Hidden dimension of the encoders.\n\n        `""use_bert_config""`: bool\n            If `False`, apply the standard Transformer Encoder architecture from\n            the original paper `(Vaswani et al.) ""Attention is All You Need""`.\n            If `True`, apply the Transformer Encoder architecture used in BERT\n            `(Devlin et al.)` and the default setting of TensorFlow.\n            The differences lie in:\n\n            1. The standard arch restricts the word embedding of PAD token to\n               all zero. The BERT arch does not.\n            2. The attention bias for padding tokens: Standard architectures use\n               ``-1e8`` for negative attention mask. BERT uses ``-1e4`` instead.\n            3. The residual connections between internal tensors:\n               In BERT, a residual layer connects the tensors *after* layer\n               normalization. In standard architectures, the tensors are\n               connected *before* layer normalization.\n\n        `""embedding_dropout""`: float\n            Dropout rate of the input embedding.\n\n        `""residual_dropout""`: float\n            Dropout rate of the residual connections.\n\n        `""eps""`: float\n            Epsilon values for layer norm layers.\n\n        `""poswise_feedforward""`: dict\n            Hyperparameters for a feed-forward network used in residual\n            connections.\n            Make sure the dimension of the output tensor is equal to ``""dim""``.\n            See\n            :func:`~texar.torch.modules.default_transformer_poswise_net_hparams`\n            for details.\n\n        `""multihead_attention""`: dict\n            Hyperparameters for the multi-head attention strategy.\n            Make sure the ``""output_dim""`` in this module is equal to ``""dim""``.\n            See :class:`~texar.torch.modules.MultiheadAttentionEncoder` for\n            details.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n        dim = 512\n        return {\n            \'num_blocks\': 6,\n            \'dim\': dim,\n            \'use_bert_config\': False,\n            \'embedding_dropout\': 0.1,\n            \'residual_dropout\': 0.1,\n            \'poswise_feedforward\': default_transformer_poswise_net_hparams(dim),\n            \'multihead_attention\': {\n                \'name\': \'multihead_attention\',\n                \'num_units\': 512,\n                \'num_heads\': 8,\n                \'dropout_rate\': 0.1,\n                \'output_dim\': 512,\n                \'use_bias\': False,\n            },\n            \'initializer\': None,\n            \'eps\': 1e-6,\n            \'name\': \'transformer_encoder\',\n        }\n\n    def forward(self,  # type: ignore\n                inputs: torch.Tensor,\n                sequence_length: torch.LongTensor) -> torch.Tensor:\n        r""""""Encodes the inputs.\n\n        Args:\n            inputs: A 3D Tensor of shape ``[batch_size, max_time, dim]``,\n                containing the embedding of input sequences. Note that\n                the embedding dimension `dim` must equal ""dim"" in\n                :attr:`hparams`. The input embedding is typically an\n                aggregation of word embedding and position embedding.\n            sequence_length: A 1D :tensor:`LongTensor` of shape\n                ``[batch_size]``. Input tokens beyond respective sequence\n                lengths are masked out automatically.\n\n        Returns:\n            A Tensor of shape ``[batch_size, max_time, dim]`` containing the\n            encoded vectors.\n        """"""\n        # Multiply input embedding with the sqrt of its dimension for\n        # normalization\n\n        inputs_padding = 1 - sequence_mask(\n            sequence_length, inputs.size()[1]).float()\n        if self._hparams.use_bert_config:\n            ignore_padding = attn.attention_bias_ignore_padding(\n                inputs_padding, bias_value=-1e4)\n        else:\n            ignore_padding = attn.attention_bias_ignore_padding(\n                inputs_padding)\n        encoder_self_attention_bias = ignore_padding\n\n        input_embedding = inputs\n        if self._hparams.use_bert_config:\n            x = self.input_normalizer(input_embedding)\n            x = self.embed_dropout(x)\n        else:\n            x = self.embed_dropout(input_embedding)\n\n        for i in range(self._hparams.num_blocks):\n            # trivial difference between BERT and original Transformer\n            if self._hparams.use_bert_config:\n                _queries_input = x\n            else:\n                _queries_input = self.self_attn_layer_norm[i](x)\n\n            attention_output = self.self_attns[i](\n                queries=_queries_input,\n                memory=_queries_input,\n                memory_attention_bias=encoder_self_attention_bias,\n            )\n\n            attention_output = self.residual_dropout(attention_output)\n\n            x = x + attention_output\n\n            poswise_network = self.poswise_networks[i]\n            poswise_normalizer = self.poswise_layer_norm[i]\n\n            if self._hparams.use_bert_config:\n                x = poswise_normalizer(x)\n                y = x\n            else:\n                y = poswise_normalizer(x)\n\n            original_shape = y.size()\n\n            y = y.view(-1, self._hparams.dim)\n\n            layer_output = poswise_network(y)\n            sub_output = self.residual_dropout(layer_output)\n            sub_output = sub_output.view(original_shape)\n\n            x = x + sub_output\n            if self._hparams.use_bert_config:\n                x = self.output_layer_norm[i](x)\n\n        if not self._hparams.use_bert_config:\n            x = self.final_layer_norm(x)\n        return x\n\n    @property\n    def output_size(self) -> int:\n        return self._hparams.dim\n'"
texar/torch/modules/encoders/xlnet_encoder.py,45,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nXLNet encoder.\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom texar.torch.modules.encoders.encoder_base import EncoderBase\nfrom texar.torch.modules.pretrained.xlnet import PretrainedXLNetMixin\nfrom texar.torch.modules.pretrained.xlnet_utils import (\n    PositionWiseFF, RelativeMultiheadAttention, RelativePositionalEncoding,\n    params_except_in)\nfrom texar.torch.utils.utils import dict_fetch, sum_tensors\n\n__all__ = [\n    ""XLNetEncoder"",\n]\n\n\nclass XLNetEncoder(EncoderBase, PretrainedXLNetMixin):\n    r""""""Raw XLNet module for encoding sequences. Please see\n    :class:`~texar.torch.modules.PretrainedXLNetMixin` for a brief description\n    of XLNet.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``xlnet-based-cased``). Please refer to\n            :class:`~texar.torch.modules.PretrainedXLNetMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n    _IS_DECODE = False\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        super().__init__(hparams=hparams)\n        self.load_pretrained_config(pretrained_model_name, cache_dir)\n\n        num_layers = self._hparams.num_layers\n        num_heads = self._hparams.num_heads\n        head_dim = self._hparams.head_dim\n\n        self.word_embed = nn.Embedding(self._hparams.vocab_size,\n                                       self._hparams.hidden_dim)\n        self.pos_embed = RelativePositionalEncoding(\n            hparams={\n                ""dim"": self._hparams.hidden_dim,\n                ""max_seq_len"": self._hparams.max_seq_length,\n            })\n        self.dropout = nn.Dropout(self._hparams.dropout)\n\n        self.r_r_bias = None\n        self.r_w_bias = None\n        self.r_s_bias = None\n\n        if not self._hparams.untie_r:\n            self.r_r_bias = nn.Parameter(torch.Tensor(num_heads, head_dim))\n            self.r_w_bias = nn.Parameter(torch.Tensor(num_heads, head_dim))\n            self.r_s_bias = (nn.Parameter(torch.Tensor(num_heads, head_dim))\n                             if self._hparams.use_segments else None)\n\n        self.attn_layers = nn.ModuleList()\n        self.ff_layers = nn.ModuleList()\n        rel_attn_hparams = dict_fetch(\n            self._hparams, RelativeMultiheadAttention.default_hparams())\n        ff_hparams = dict_fetch(\n            self._hparams, PositionWiseFF.default_hparams())\n        for _ in range(num_layers):\n            self.attn_layers.append(RelativeMultiheadAttention(\n                self.r_r_bias, self.r_w_bias, self.r_s_bias,\n                hparams=rel_attn_hparams))\n            self.ff_layers.append(PositionWiseFF(hparams=ff_hparams))\n\n        self.mask_emb = nn.Parameter(\n            torch.Tensor(1, 1, self._hparams.hidden_dim))\n\n        if self._IS_DECODE:\n            self.lm_bias = nn.Parameter(torch.zeros(self._hparams.vocab_size))\n\n        self.init_pretrained_weights()\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The encoder arch is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the encoder arch is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the encoder arch is defined by the\n          configurations in `hparams` and weights are randomly initialized.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""xlnet-base-cased"",\n                ""untie_r"": True,\n                ""num_layers"": 12,\n                ""mem_len"": 0,\n                ""reuse_len"": 0,\n                ""num_heads"": 12,\n                ""hidden_dim"": 768,\n                ""head_dim"": 64,\n                ""dropout"": 0.1,\n                ""attention_dropout"": 0.1,\n                ""use_segments"": True,\n                ""ffn_inner_dim"": 3072,\n                ""activation"": \'gelu\',\n                ""vocab_size"": 32000,\n                ""max_seq_length"": 512,\n                ""initializer"": None,\n                ""name"": ""xlnet_encoder"",\n            }\n\n        Here:\n\n        The default parameters are values for cased XLNet-Base model.\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained XLNet model. If None, the model\n            will be randomly initialized.\n\n        `""untie_r""`: bool\n            Whether to untie the biases in attention.\n\n        `""num_layers""`: int\n            The number of stacked layers.\n\n        `""mem_len""`: int\n            The number of tokens to cache.\n\n        `""reuse_len""`: int\n            The number of tokens in the current batch to be cached and reused\n            in the future.\n\n        `""num_heads""`: int\n            The number of attention heads.\n\n        `""hidden_dim""`: int\n            The hidden size.\n\n        `""head_dim""`: int\n            The dimension size of each attention head.\n\n        `""dropout""`: float\n            Dropout rate.\n\n        `""attention_dropout""`: float\n            Dropout rate on attention probabilities.\n\n        `""use_segments""`: bool\n            Whether to use segment embedding.\n\n        `""ffn_inner_dim""`: int\n            The hidden size in feed-forward layers.\n\n        `""activation""`: str\n            `relu` or `gelu`.\n\n        `""vocab_size""`: int\n            The vocabulary size.\n\n        `""max_seq_length""`: int\n            The maximum sequence length for `RelativePositionalEncoding`.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n\n        return {\n            \'pretrained_model_name\': \'xlnet-base-cased\',\n            \'untie_r\': True,\n            \'num_layers\': 12,\n            \'mem_len\': 0,\n            \'reuse_len\': 0,\n            # layer\n            \'num_heads\': 12,\n            \'hidden_dim\': 768,\n            \'head_dim\': 64,\n            \'dropout\': 0.1,\n            \'attention_dropout\': 0.1,\n            \'use_segments\': True,\n            # ffn\n            \'ffn_inner_dim\': 3072,\n            \'activation\': \'gelu\',\n            # embedding\n            \'vocab_size\': 32000,\n            \'max_seq_length\': 512,\n            \'initializer\': None,\n            \'name\': ""xlnet_encoder"",\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    def param_groups(self,\n                     lr: Optional[float] = None,\n                     lr_layer_scale: float = 1.0,\n                     decay_base_params: bool = False):\n        r""""""Create parameter groups for optimizers. When\n        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form\n        separate groups with different base learning rates.\n\n        The return value of this method can be used in the constructor of\n        optimizers, for example:\n\n        .. code-block:: python\n\n            model = XLNetEncoder(...)\n            param_groups = model.param_groups(lr=2e-5, lr_layer_scale=0.8)\n            optim = torch.optim.Adam(param_groups)\n\n        Args:\n            lr (float): The learning rate. Can be omitted if\n                :attr:`lr_layer_decay_rate` is 1.0.\n            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer\n                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.\n            decay_base_params (bool): If `True`, treat non-layer parameters\n                (e.g. embeddings) as if they\'re in layer 0. If `False`, these\n                parameters are not scaled.\n\n        Returns:\n            The parameter groups, used as the first argument for optimizers.\n        """"""\n\n        if lr_layer_scale != 1.0:\n            if lr is None:\n                raise ValueError(\n                    ""lr must be specified when lr_layer_decay_rate is not 1.0"")\n\n            num_layers = self._hparams.num_layers\n            base_group = {\n                ""params"": params_except_in(\n                    self, [\'attn_layers\', \'ff_layers\']),\n                ""lr"": lr * (lr_layer_scale ** num_layers\n                            if decay_base_params else 1.0)\n            }\n            param_groups = [base_group]\n            for idx in range(num_layers):\n                decay_rate = lr_layer_scale ** (num_layers - idx - 1)\n                param_group = {\n                    ""params"": [*self.attn_layers[idx].parameters(),\n                               *self.ff_layers[idx].parameters()],\n                    ""lr"": lr * decay_rate,\n                }\n                param_groups.append(param_group)\n            return param_groups\n        return self.parameters()\n\n    @property\n    def output_size(self):\n        r""""""The feature size of :meth:`forward` output.\n        """"""\n        return self._hparams.hidden_dim\n\n    @staticmethod\n    def _cache_mem(output: torch.Tensor,\n                   prev_mem: Optional[torch.Tensor],\n                   mem_len: int,\n                   reuse_len: int = 0) -> torch.Tensor:\n        r""""""Cache hidden states into memory.""""""\n        assert mem_len > 0\n\n        if reuse_len is not None and reuse_len > 0:\n            output = output[:reuse_len]\n        if prev_mem is None:\n            new_mem = output[-mem_len:]\n        else:\n            new_mem = torch.cat([prev_mem, output], dim=0)[-mem_len:]\n        return new_mem.detach()\n\n    def _create_causal_attn_mask(self,\n                                 seq_len: int,\n                                 mem_len: int,\n                                 same_length: bool = False) -> torch.Tensor:\n        r""""""Create causal attention mask of shape\n        `(seq_len, mem_len + seq_len)`.\n        """"""\n        assert self.r_w_bias is not None\n        device = self.r_w_bias.device\n        attn_mask = torch.ones(seq_len, seq_len, device=device)\n        mask_u = torch.triu(attn_mask, diagonal=1)\n        attn_mask_pad = torch.zeros(seq_len, mem_len, device=device)\n        ret = torch.cat([attn_mask_pad, mask_u], dim=1)\n        if same_length:\n            mask_l = torch.tril(attn_mask, diagonal=-1)\n            ret = torch.cat([ret[:, :seq_len] + mask_l, ret[:, seq_len:]], 1)\n        return ret\n\n    def forward(self,  # type: ignore\n                inputs: Union[torch.Tensor, torch.LongTensor],\n                segment_ids: Optional[torch.LongTensor] = None,\n                input_mask: Optional[torch.Tensor] = None,\n                memory: Optional[List[torch.Tensor]] = None,\n                permute_mask: Optional[torch.Tensor] = None,\n                target_mapping: Optional[torch.Tensor] = None,\n                bi_data: bool = False,\n                clamp_len: Optional[int] = None,\n                cache_len: int = 0,\n                same_length: bool = False,\n                attn_type: str = \'bi\',\n                two_stream: bool = False) \\\n            -> Tuple[torch.Tensor, Optional[List[torch.Tensor]]]:\n        r""""""Compute XLNet representations for the input.\n\n        Args:\n            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,\n                containing the ids of tokens in input sequences, or\n                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,\n                containing soft token ids (i.e., weights or probabilities)\n                used to mix the embedding vectors.\n            segment_ids: Shape `[batch_size, max_time]`.\n            input_mask: Float tensor of shape `[batch_size, max_time]`. Note\n                that positions with value 1 are masked out.\n            memory: Memory from previous batches. A list of length `num_layers`,\n                each tensor of shape `[batch_size, mem_len, hidden_dim]`.\n            permute_mask: The permutation mask. Float tensor of shape\n                `[batch_size, max_time, max_time]`.\n                A value of 0 for ``permute_mask[i, j, k]`` indicates that\n                position `i` attends to position `j` in batch `k`.\n            target_mapping: The target token mapping. Float tensor of shape\n                `[batch_size, num_targets, max_time]`.\n                A value of 1 for ``target_mapping[i, j, k]`` indicates that\n                the `i`-th target token (in order of permutation) in batch `k`\n                is the token at position `j`.\n                Each row ``target_mapping[i, :, k]`` can have no more than one\n                value of 1.\n            bi_data (bool): Whether to use bidirectional data input pipeline.\n            clamp_len (int): Clamp all relative distances larger than\n                :attr:`clamp_len`. A value of -1 means no clamping.\n            cache_len (int): Length of memory (number of tokens) to cache.\n            same_length (bool): Whether to use the same attention length for\n                each token.\n            attn_type (str): Attention type. Supported values are `""uni""`\n                and `""bi""`.\n            two_stream (bool): Whether to use two-stream attention. Only set to\n                `True` when pre-training or generating text. Defaults to\n                `False`.\n\n        :returns: A tuple of `(output, new_memory)`:\n\n            - **`output`**: The final layer output representations. Shape\n              `[batch_size, max_time, hidden_dim]`.\n            - **`new_memory`**: The memory of the current batch.\n              If `cache_len` is 0, then `new_memory` is `None`. Otherwise, it is\n              a list of length `num_layers`, each tensor of shape\n              `[batch_size, cache_len, hidden_dim]`.\n              This can be used as the :attr:`memory` argument in the next batch.\n        """"""\n        if inputs.dim() == 2:\n            word_embeds = self.word_embed(inputs)\n        elif inputs.dim() == 3:\n            word_embeds = torch.tensordot(inputs, self.word_embed.weight,\n                                          dims=([-1], [0]))\n        else:\n            raise ValueError(""\'inputs\' should be a 2D or 3D tensor."")\n\n        return self._forward(word_embed=word_embeds,\n                             segment_ids=segment_ids,\n                             input_mask=input_mask,\n                             memory=memory,\n                             permute_mask=permute_mask,\n                             target_mapping=target_mapping,\n                             bi_data=bi_data,\n                             clamp_len=clamp_len,\n                             cache_len=cache_len,\n                             same_length=same_length,\n                             attn_type=attn_type,\n                             two_stream=two_stream)\n\n    def _forward(self,\n                 word_embed: torch.Tensor,\n                 segment_ids: Optional[torch.LongTensor] = None,\n                 input_mask: Optional[torch.Tensor] = None,\n                 memory: Optional[List[torch.Tensor]] = None,\n                 permute_mask: Optional[torch.Tensor] = None,\n                 target_mapping: Optional[torch.Tensor] = None,\n                 bi_data: bool = False,\n                 clamp_len: Optional[int] = None,\n                 cache_len: int = 0,\n                 same_length: bool = False,\n                 attn_type: str = \'bi\',\n                 two_stream: bool = False) \\\n            -> Tuple[torch.Tensor, Optional[List[torch.Tensor]]]:\n        r""""""Compute XLNet representations for the input. This layer exists\n        because :class:`XLNetDecoder` compute embeddings in the decoder helper.\n        `word_embed` has shape `[batch_size, max_time, word_embed_dim]`.\n        Please refer to :meth:`forward` for the detailed information of other\n        arguments.\n        """"""\n        # seq_len == max_time\n        # word_embed: [seq_len, batch_size, word_embed_dim]\n        word_embed = word_embed.permute(1, 0, 2)\n        # segment_ids: [seq_len, batch_size]\n        if segment_ids is not None:\n            segment_ids = segment_ids.permute(1, 0)\n        # input_mask: [seq_len, batch_size]\n        if input_mask is not None:\n            input_mask = input_mask.permute(1, 0)\n        # memory: A list of length num_layers\n        # each tensor of shape [mem_len, batch_size, hidden_dim]\n        if memory is not None:\n            memory = [m.permute(1, 0, 2) for m in memory]\n        # permute_mask: [seq_len, seq_len, batch_size]\n        if permute_mask is not None:\n            permute_mask = permute_mask.permute(1, 2, 0)\n        # target_mapping: [num_targets, seq_len, batch_size]\n        if target_mapping is not None:\n            target_mapping = target_mapping.permute(1, 2, 0)\n\n        seq_len, batch_size = word_embed.size()[:2]\n        mem_len = memory[0].size(0) if memory is not None else 0\n        tot_len = seq_len + mem_len\n        reuse_len = self._hparams.reuse_len\n\n        # Construct masks.\n        masks: List[Optional[torch.Tensor]] = []\n\n        # Causal attention mask.\n        if attn_type == \'uni\':\n            causal_mask = self._create_causal_attn_mask(\n                seq_len, mem_len, same_length)\n            # attn_mask: (seq_len, tot_len, 1, 1)\n            causal_mask = causal_mask.unsqueeze(2).unsqueeze(3)\n            masks.append(causal_mask)\n        elif attn_type == \'bi\':\n            pass\n        else:\n            raise ValueError(f""Unsupported attention type: {attn_type}"")\n\n        # Data mask: input mask & permutation mask.\n        if input_mask is not None:\n            input_mask = input_mask.expand(seq_len, -1, -1)\n        data_mask = sum_tensors([input_mask, permute_mask])\n        if data_mask is not None:\n            # All positions in memory can be attended to.\n            memory_mask = data_mask.new_zeros(seq_len, mem_len, batch_size)\n            # data_mask: (seq_len, tot_len, batch_size, 1)\n            data_mask = torch.cat([memory_mask, data_mask], dim=1).unsqueeze(3)\n            masks.append(data_mask)\n\n        # Exclude the main diagonal (target tokens) from the mask.\n        attn_mask = sum_tensors(masks)\n        if attn_mask is None:\n            final_mask = None\n        else:\n            attn_mask = (attn_mask > 0)\n            final_mask = -torch.eye(seq_len, device=attn_mask.device)\n            final_mask = torch.cat([\n                final_mask.new_zeros(seq_len, mem_len), final_mask], dim=-1)\n            final_mask = final_mask.unsqueeze(2).unsqueeze(3)\n            # final_mask: (seq_len, tot_len, batch_size, 1)\n            final_mask = ((attn_mask.float() + final_mask) > 0)\n\n        # Construct segment embedding.\n        if segment_ids is not None:\n            concat_segment_ids = torch.cat([\n                segment_ids.new_zeros(mem_len, batch_size), segment_ids])\n            segment_matrix = (segment_ids.unsqueeze(1) !=\n                              concat_segment_ids.unsqueeze(0)).long()\n            segment_matrix = F.one_hot(segment_matrix, num_classes=2).float()\n        else:\n            segment_matrix = None\n\n        pos_embed = self.pos_embed(\n            batch_size, seq_len, tot_len, clamp_len, attn_type, bi_data)\n        pos_embed = self.dropout(pos_embed)\n\n        states_h = self.dropout(word_embed)\n        states_g = None\n        if two_stream:\n            if target_mapping is not None:\n                word_embed_q = self.mask_emb.expand(\n                    target_mapping.size(0), batch_size, -1)\n            else:\n                word_embed_q = word_embed\n            states_g = self.dropout(word_embed_q)\n        new_memory = []\n\n        for idx in range(self._hparams.num_layers):\n            cur_memory = memory[idx] if memory is not None else None\n            if cache_len > 0:\n                new_memory.append(self._cache_mem(\n                    states_h, cur_memory, cache_len, reuse_len))\n            attn_layer: RelativeMultiheadAttention\n            attn_layer = self.attn_layers[idx]  # type: ignore\n            states_h, states_g = attn_layer(\n                states_h=states_h, states_g=states_g,\n                pos_embed=pos_embed, segment_mat=segment_matrix,\n                attn_mask_h=final_mask, attn_mask_g=attn_mask,\n                target_mapping=target_mapping, memory=cur_memory)\n            states_h = self.ff_layers[idx](states_h)\n            if states_g is not None:\n                states_g = self.ff_layers[idx](states_g)\n\n        output = self.dropout(states_h if states_g is None else states_g)\n\n        # Now output: [seq_len, batch_size, hidden_dim]\n        # new_memory: None or A list of length num_layers,\n        # each tensor of shape [cache_len, batch_size, hidden_dim]\n        output = output.permute(1, 0, 2)\n        if new_memory is not None:\n            new_memory = [m.permute(1, 0, 2) for m in new_memory]\n\n        if cache_len == 0:\n            return output, None\n\n        return output, new_memory\n'"
texar/torch/modules/networks/__init__.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of networks.\n""""""\n\nfrom texar.torch.modules.networks.conv_networks import *\nfrom texar.torch.modules.networks.network_base import *\nfrom texar.torch.modules.networks.networks import *\n'"
texar/torch/modules/networks/conv_networks.py,16,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious convolutional networks.\n""""""\nfrom typing import List, Optional, Tuple, Union, Dict, Any\n\nimport torch\n\nfrom texar.torch.core.layers import get_pooling_layer_hparams\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.modules.networks.network_base import FeedForwardNetworkBase\nfrom texar.torch.utils.shapes import mask_sequences\nfrom texar.torch.utils.utils import uniquify_str\n\n__all__ = [\n    ""_to_list"",\n    ""Conv1DNetwork"",\n]\n\n\ndef _to_list(value: Union[Dict[str, Any], List, Tuple, int], name=None,\n             list_length=None):\n    r""""""Converts `hparams` value into a list.\n\n    If :attr:`list_length` is given, then the canonicalized :attr:`value`\n    must be of length :attr:`list_length`.\n    """"""\n    if not isinstance(value, (list, tuple)):\n        if list_length is not None:\n            value = [value] * list_length\n        else:\n            value = [value]\n    if list_length is not None and len(value) != list_length:\n        name = \'\' if name is None else name\n        raise ValueError(""hparams \'%s\' must be a list of length %d""\n                         % (name, list_length))\n    return value\n\n\nclass Conv1DNetwork(FeedForwardNetworkBase):\n    r""""""Simple `Conv-1D` network which consists of a sequence of convolutional\n    layers followed with a sequence of dense layers.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        in_features (int): Size of the feature dimension in the input tensor.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    See :meth:`forward` for the inputs and outputs. If :attr:`""data_format""` is\n    set to ``""channels_first""`` (this is the default), inputs must be a tensor\n    of shape `[batch_size, channels, length]`. If :attr:`""data_format""` is set\n    to ``""channels_last""``, inputs must be a tensor of shape\n    `[batch_size, length, channels]`. For example, for sequence classification,\n    `length` corresponds to time steps, and `channels` corresponds to embedding\n    dim.\n\n    Example:\n\n    .. code-block:: python\n\n        nn = Conv1DNetwork(in_channels=20, in_features=256) # Use the default\n\n        inputs = torch.randn([64, 20, 256])\n        outputs = nn(inputs)\n        # outputs == Tensor of shape [64, 256], because the final dense layer\n        # has size 256.\n\n    .. document private functions\n    """"""\n\n    def __init__(self, in_channels: int, in_features: Optional[int] = None,\n                 hparams=None):\n        super().__init__(hparams=hparams)\n        if self.hparams.num_dense_layers > 0 and in_features is None:\n            raise ValueError(""\\""in_features\\"" cannot be None ""\n                             ""if \\""num_dense_layers\\"" > 0"")\n\n        # construct only non-dense layers first\n        layer_hparams = self._build_non_dense_layer_hparams(\n            in_channels=in_channels)\n        self._build_layers(layers=None, layer_hparams=layer_hparams)\n        if self.hparams.num_dense_layers > 0:\n            if in_features is None:\n                raise ValueError(""\\""in_features\\"" cannot be None ""\n                                 ""if \\""num_dense_layers\\"" > 0"")\n            ones = torch.ones(1, in_channels, in_features)\n            input_size = self._infer_dense_layer_input_size(ones)\n            layer_hparams = self._build_dense_hparams(\n                in_features=input_size[1], layer_hparams=layer_hparams)\n            self._build_layers(layers=None, layer_hparams=layer_hparams)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Conv layers\n                ""num_conv_layers"": 1,\n                ""out_channels"": 128,\n                ""kernel_size"": [3, 4, 5],\n                ""conv_activation"": ""ReLU"",\n                ""conv_activation_kwargs"": None,\n                ""other_conv_kwargs"": {},\n                ""data_format"": ""channels_first"",\n                # (2) Pooling layers\n                ""pooling"": ""MaxPool1d"",\n                ""pool_size"": None,\n                ""pool_stride"": 1,\n                ""other_pool_kwargs"": {},\n                # (3) Dense layers\n                ""num_dense_layers"": 1,\n                ""out_features"": 256,\n                ""dense_activation"": None,\n                ""dense_activation_kwargs"": None,\n                ""final_dense_activation"": None,\n                ""final_dense_activation_kwargs"": None,\n                ""other_dense_kwargs"": None,\n                # (4) Dropout\n                ""dropout_conv"": [1],\n                ""dropout_dense"": [],\n                ""dropout_rate"": 0.75,\n                # (5) Others\n                ""name"": ""conv1d_network""\n            }\n\n        Here:\n\n        1. For **convolutional** layers:\n\n           `""num_conv_layers""`: int\n               Number of convolutional layers.\n\n           `""out_channels""`: int or list\n               The number of out_channels in the convolution, i.e., the\n               dimensionality of the output space.\n\n               - If ``""num_conv_layers""`` > 1 and ``""out_channels""`` is an int,\n                 all convolution layers will have the same number of output\n                 channels.\n\n               - If ``""num_conv_layers""`` > 1 and ``""out_channels""`` is a list,\n                 the length must equal ``""num_conv_layers""``. The number of\n                 output channels of each convolution layer will be the\n                 corresponding element from this list.\n\n           `""kernel_size""`: int or list\n               Lengths of 1D convolution windows.\n\n               - If `""num_conv_layers""` = 1, this can also be a ``int`` list of\n                 arbitrary length denoting differently sized convolution\n                 windows. The number of output channels of each size is\n                 specified by ``""out_channels""``.\n                 For example, the default values will create 3 convolution\n                 layers, each of which has kernel size of 3, 4, and 5,\n                 respectively, and has output channel 128.\n\n               - If `""num_conv_layers""` > 1, this must be a list of length\n                 ``""num_conv_layers""``. Each element can be an ``int`` or a\n                 ``int`` list of arbitrary length denoting the kernel size of\n                 each layer.\n\n           `""conv_activation""`: str or callable\n               Activation applied to the output of the convolutional\n               layers. Set to `None` to maintain a linear activation.\n               See :func:`~texar.torch.core.get_layer` for more details.\n\n           `""conv_activation_kwargs""`: dict, optional\n               Keyword arguments for the activation following the convolutional\n               layer. See :func:`~texar.torch.core.get_layer` for more details.\n\n           `""other_conv_kwargs""`: list or dict, optional\n               Other keyword arguments for :torch_nn:`Conv1d` constructor,\n               e.g., ``padding``.\n\n               - If a dict, the same dict is applied to all the convolution\n                 layers.\n\n               - If a list, the length must equal ``""num_conv_layers""``. This\n                 list can contain nested lists. If the convolution layer at\n                 index i has multiple kernel sizes, then the corresponding\n                 element of this list can also be a list of length equal to\n                 ``""kernel_size""`` at index i. If the element at index i is\n                 instead a dict, then the same dict gets applied to all the\n                 convolution layers at index i.\n\n           `""data_format""`: str, optional\n               Data format of the input tensor. Defaults to ``channels_first``\n               denoting the first dimension to be the channel dimension. Set it\n               to ``channels_last`` to treat last dimension as the channel\n               dimension. This argument can also be passed in ``forward``\n               function, in which case the value specified here will be\n               ignored.\n\n        2. For **pooling** layers:\n\n           `""pooling""`: str or class or instance\n               Pooling layer after each of the convolutional layer(s). Can be a\n               pooling layer class, its name or module path, or a class\n               instance.\n\n           `""pool_size""`: int or list, optional\n               Size of the pooling window. If an ``int``, all pooling layer\n               will have the same pool size. If a list, the list length must\n               equal ``""num_conv_layers""``. If `None` and the pooling type\n               is either :torch_docs:`MaxPool1d <nn.html#maxpool1d>` or\n               :torch_docs:`AvgPool1d <nn.html#avgpool1d>`, the pool size will\n               be set to input size. That is, the output of the pooling layer\n               is a single unit.\n\n           `""pool_stride""`: int or list, optional\n               Strides of the pooling operation. If an ``int``, all\n               layers will have the same stride. If a list, the list length\n               must equal ``""num_conv_layers""``.\n\n           `""other_pool_kwargs""`: list or dict, optional\n               Other keyword arguments for pooling layer class constructor.\n\n               - If a dict, the same dict is applied to all the pooling layers.\n\n               - If a list, the length must equal ``""num_conv_layers""``. The\n                 pooling arguments for layer i will be the element at index i\n                 from this list.\n\n        3. For **dense** layers (note that here dense layers always follow\n           convolutional and pooling layers):\n\n           `""num_dense_layers""`: int\n               Number of dense layers.\n\n           `""out_features""`: int or list\n               Dimension of features after the dense layers. If an\n               ``int``, all dense layers will have the same feature dimension.\n               If a list of ``int``, the list length must equal\n               ``""num_dense_layers""``.\n\n           `""dense_activation""`: str or callable\n               Activation function applied to the output of the dense\n               layers **except** the last dense layer output. Set to\n               `None` to maintain a linear activation.\n\n           `""dense_activation_kwargs""`: dict, optional\n               Keyword arguments for dense layer activation functions before\n               the last dense layer.\n\n           `""final_dense_activation""`: str or callable\n               Activation function applied to the output of the **last** dense\n               layer. Set to `None` to maintain a linear activation.\n\n           `""final_dense_activation_kwargs""`: dict, optional\n               Keyword arguments for the activation function of last\n               dense layer.\n\n           `""other_dense_kwargs""`: dict, optional\n               Other keyword arguments for dense layer class constructor.\n\n        4. For **dropouts**:\n\n           `""dropout_conv""`: int or list\n               The indices of convolutional layers (starting from 0) whose\n               **inputs** are applied with dropout.\n               The index = :attr:`num_conv_layers` means dropout applies to the\n               final convolutional layer output. For example,\n\n               .. code-block:: python\n\n                   {\n                       ""num_conv_layers"": 2,\n                       ""dropout_conv"": [0, 2]\n                   }\n\n               will leads to a series of layers as\n               `-dropout-conv0-conv1-dropout-`.\n\n               The dropout mode (training or not) is controlled\n               by :attr:`self.training`.\n\n           `""dropout_dense""`: int or list\n               Same as ``""dropout_conv""`` but applied to dense layers (index\n               starting from 0).\n\n           `""dropout_rate""`: float\n               The dropout rate, between 0 and 1. For example,\n               ``""dropout_rate"": 0.1`` would drop out 10% of elements.\n\n        5. Others:\n\n           `""name""`: str\n               Name of the network.\n        """"""\n        return {\n            # (1) Conv layers\n            ""num_conv_layers"": 1,\n            ""out_channels"": 128,\n            ""kernel_size"": [3, 4, 5],\n            ""conv_activation"": ""ReLU"",\n            ""conv_activation_kwargs"": None,\n            ""other_conv_kwargs"": {},\n            ""data_format"": ""channels_first"",\n            # (2) Pooling layers\n            ""pooling"": ""MaxPool1d"",\n            ""pool_size"": None,\n            ""pool_stride"": 1,\n            ""other_pool_kwargs"": {},\n            # (3) Dense layers\n            ""num_dense_layers"": 1,\n            ""out_features"": 256,\n            ""dense_activation"": None,\n            ""dense_activation_kwargs"": None,\n            ""final_dense_activation"": None,\n            ""final_dense_activation_kwargs"": None,\n            ""other_dense_kwargs"": None,\n            # (4) Dropout\n            ""dropout_conv"": [1],\n            ""dropout_dense"": [],\n            ""dropout_rate"": 0.75,\n            # (5) Others\n            ""name"": ""conv1d_network"",\n            ""@no_typecheck"": [""out_channels"", ""kernel_size"", ""conv_activation"",\n                              ""other_conv_kwargs"", ""pool_size"", ""pool_stride"",\n                              ""other_pool_kwargs"", ""out_features"",\n                              ""dense_activation"", ""dropout_conv"",\n                              ""dropout_dense""]\n        }\n\n    def _build_pool_hparams(self):\n        pool_type = self._hparams.pooling\n        if pool_type == ""MaxPool"":\n            pool_type = ""MaxPool1d""\n        elif pool_type == ""AvgPool"":\n            pool_type = ""AvgPool1d""\n\n        npool = self._hparams.num_conv_layers\n        kernel_size = _to_list(self._hparams.pool_size, ""pool_size"", npool)\n        stride = _to_list(self._hparams.pool_stride, ""pool_stride"", npool)\n\n        other_kwargs = self._hparams.other_pool_kwargs\n        if isinstance(other_kwargs, HParams):\n            other_kwargs = other_kwargs.todict()\n            other_kwargs = _to_list(other_kwargs, ""other_kwargs"", npool)\n        elif isinstance(other_kwargs, (list, tuple)):\n            if len(other_kwargs) != npool:\n                raise ValueError(""The length of hparams[\'other_pool_kwargs\'] ""\n                                 ""must equal \'num_conv_layers\'"")\n        else:\n            raise ValueError(""hparams[\'other_pool_kwargs\'] must be either a ""\n                             ""dict or list/tuple"")\n\n        pool_hparams = []\n        for i in range(npool):\n            kwargs_i = {""kernel_size"": kernel_size[i], ""stride"": stride[i]}\n            kwargs_i.update(other_kwargs[i])\n            pool_hparams_ = get_pooling_layer_hparams({""type"": pool_type,\n                                                       ""kwargs"": kwargs_i})\n            pool_hparams.append(pool_hparams_)\n\n        return pool_hparams\n\n    def _build_conv1d_hparams(self, in_channels, pool_hparams):\n        r""""""Creates the hparams for each of the convolutional layers usable for\n        :func:`texar.torch.core.layers.get_layer`.\n        """"""\n        nconv = self._hparams.num_conv_layers\n        if len(pool_hparams) != nconv:\n            raise ValueError(""`pool_hparams` must be of length %d"" % nconv)\n\n        in_channels = [in_channels]\n        out_channels = _to_list(self._hparams.out_channels, \'out_channels\',\n                                nconv)\n        # because in_channels(i) = out_channels(i-1)\n        in_channels.extend(out_channels[:-1])\n\n        if nconv == 1:\n            kernel_size = _to_list(self._hparams.kernel_size)\n            if not isinstance(kernel_size[0], (list, tuple)):\n                kernel_size = [kernel_size]\n        elif nconv > 1:\n            kernel_size = _to_list(self._hparams.kernel_size,\n                                   \'kernel_size\', nconv)\n            kernel_size = [_to_list(ks) for ks in kernel_size]\n\n        other_kwargs = self._hparams.other_conv_kwargs\n        if isinstance(other_kwargs, HParams):\n            other_kwargs = other_kwargs.todict()\n            other_kwargs = _to_list(other_kwargs, ""other_conv_kwargs"", nconv)\n        elif isinstance(other_kwargs, (list, tuple)):\n            if len(other_kwargs) != nconv:\n                raise ValueError(""The length of hparams[\'other_conv_kwargs\'] ""\n                                 ""must be equal to \'num_conv_layers\'"")\n        else:\n            raise ValueError(""hparams[\'other_conv_kwargs\'] must be a either ""\n                             ""a dict or a list."")\n\n        def _activation_hparams(name, kwargs=None):\n            if kwargs is not None:\n                return {""type"": name, ""kwargs"": kwargs}\n            else:\n                return {""type"": name, ""kwargs"": {}}\n\n        conv_pool_hparams = []\n        for i in range(nconv):\n            hparams_i = []\n            names = []\n            if isinstance(other_kwargs[i], dict):\n                other_kwargs[i] = _to_list(other_kwargs[i], ""other_kwargs[i]"",\n                                           len(kernel_size[i]))\n            elif (isinstance(other_kwargs[i], (list, tuple))\n                  and len(other_kwargs[i]) != len(kernel_size[i])):\n                raise ValueError(""The length of hparams[\'other_conv_kwargs\'][i]""\n                                 "" must be equal to the length of ""\n                                 ""hparams[\'kernel_size\'][i]"")\n            for idx, ks_ij in enumerate(kernel_size[i]):\n                name = uniquify_str(""conv_%d"" % (i + 1), names)\n                names.append(name)\n                conv_kwargs_ij = {\n                    ""in_channels"": in_channels[i],\n                    ""out_channels"": out_channels[i],\n                    ""kernel_size"": ks_ij\n                }\n                conv_kwargs_ij.update(other_kwargs[i][idx])\n                hparams_i.append(\n                    {""type"": ""Conv1d"", ""kwargs"": conv_kwargs_ij})\n            if len(hparams_i) == 1:\n                if self._hparams.conv_activation:\n                    layers = {\n                        ""layers"": [hparams_i[0],\n                                   _activation_hparams(\n                                       self._hparams.conv_activation,\n                                       self._hparams.conv_activation_kwargs)]}\n                    sequential_layer = {""type"": ""Sequential"", ""kwargs"": layers}\n                    conv_pool_hparams.append([sequential_layer,\n                                              pool_hparams[i]])\n                else:\n                    conv_pool_hparams.append([hparams_i[0], pool_hparams[i]])\n            else:  # creates MergeLayer\n                mrg_kwargs_layers = []\n                for hparams_ij in hparams_i:\n                    if self._hparams.conv_activation:\n                        seq_kwargs_j = {\n                            ""layers"": [\n                                hparams_ij,\n                                _activation_hparams(\n                                    self._hparams.conv_activation,\n                                    self._hparams.conv_activation_kwargs),\n                                pool_hparams[i]\n                            ]\n                        }\n                    else:\n                        seq_kwargs_j = {""layers"": [hparams_ij, pool_hparams[i]]}\n                    mrg_kwargs_layers.append(\n                        {""type"": ""Sequential"", ""kwargs"": seq_kwargs_j})\n                mrg_hparams = {""type"": ""MergeLayer"",\n                               ""kwargs"": {""layers"": mrg_kwargs_layers}}\n                conv_pool_hparams.append(mrg_hparams)\n\n        return conv_pool_hparams\n\n    def _build_dense_hparams(self, in_features: int, layer_hparams):\n        ndense = self._hparams.num_dense_layers\n        in_features = [in_features]\n        out_features = _to_list(self._hparams.out_features, \'out_features\',\n                                ndense)\n        # because in_features(i) = out_features(i-1)\n        in_features.extend(out_features[:-1])\n        other_kwargs = self._hparams.other_dense_kwargs or {}\n        if isinstance(other_kwargs, HParams):\n            other_kwargs = other_kwargs.todict()\n        if not isinstance(other_kwargs, dict):\n            raise ValueError(""hparams[\'other_dense_kwargs\'] must be a dict."")\n\n        def _activation_hparams(name, kwargs=None):\n            if kwargs is not None:\n                return {""type"": name, ""kwargs"": kwargs}\n            else:\n                return {""type"": name, ""kwargs"": {}}\n\n        dense_hparams = []\n        for i in range(ndense):\n            kwargs_i = {""in_features"": in_features[i],\n                        ""out_features"": out_features[i]}\n            kwargs_i.update(other_kwargs)\n\n            dense_hparams_i = {""type"": ""Linear"", ""kwargs"": kwargs_i}\n            if i < ndense - 1 and self._hparams.dense_activation is not None:\n                layers = {\n                    ""layers"": [dense_hparams_i,\n                               _activation_hparams(\n                                   self._hparams.dense_activation,\n                                   self._hparams.dense_activation_kwargs)\n                               ]}\n                sequential_layer = {""type"": ""Sequential"", ""kwargs"": layers}\n                dense_hparams.append(sequential_layer)\n\n            elif (i == ndense - 1 and\n                  self._hparams.final_dense_activation is not None):\n                layers = {\n                    ""layers"": [dense_hparams_i,\n                               _activation_hparams(\n                                   self._hparams.final_dense_activation,\n                                   self._hparams.final_dense_activation_kwargs)\n                               ]}\n                sequential_layer = {""type"": ""Sequential"", ""kwargs"": layers}\n                dense_hparams.append(sequential_layer)\n            else:\n                dense_hparams.append(dense_hparams_i)\n\n        def _dropout_hparams():\n            return {""type"": ""Dropout"",\n                    ""kwargs"": {""p"": self._hparams.dropout_rate}}\n\n        dropout_dense = _to_list(self._hparams.dropout_dense)\n        ndense = self._hparams.num_dense_layers\n        if ndense > 0:  # Add flatten layers before dense layers\n            layer_hparams.append({""type"": ""Flatten""})\n        for dense_i in range(ndense):\n            if dense_i in dropout_dense:\n                layer_hparams.append(_dropout_hparams())\n            layer_hparams.append(dense_hparams[dense_i])\n        if ndense in dropout_dense:\n            layer_hparams.append(_dropout_hparams())\n\n        return layer_hparams\n\n    def _build_non_dense_layer_hparams(self, in_channels):\n        pool_hparams = self._build_pool_hparams()\n        conv_pool_hparams = self._build_conv1d_hparams(in_channels,\n                                                       pool_hparams)\n\n        def _dropout_hparams():\n            return {""type"": ""Dropout"",\n                    ""kwargs"": {""p"": self._hparams.dropout_rate}}\n\n        dropout_conv = _to_list(self._hparams.dropout_conv)\n\n        layers_hparams = []\n        nconv = self._hparams.num_conv_layers\n        for conv_i in range(nconv):\n            if conv_i in dropout_conv:\n                layers_hparams.append(_dropout_hparams())\n            if isinstance(conv_pool_hparams[conv_i], (list, tuple)):\n                layers_hparams += conv_pool_hparams[conv_i]\n            else:\n                layers_hparams.append(conv_pool_hparams[conv_i])\n        if nconv in dropout_conv:\n            layers_hparams.append(_dropout_hparams())\n\n        return layers_hparams\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor,\n                sequence_length: Optional[Union[torch.LongTensor,\n                                                List[int]]] = None,\n                dtype: Optional[torch.dtype] = None,\n                data_format: Optional[str] = None) -> torch.Tensor:\n        r""""""Feeds forward inputs through the network layers and returns outputs.\n\n        Args:\n            input: The inputs to the network, which is a 3D tensor.\n            sequence_length (optional): An :tensor:`LongTensor` of shape\n                ``[batch_size]`` or a python array containing the length of\n                each element in :attr:`inputs`. If given, time steps beyond\n                the length will first be masked out before feeding to the\n                layers.\n            dtype (optional): Type of the inputs. If not provided,\n                infers from inputs automatically.\n            data_format (optional): Data type of the input tensor. If\n                ``channels_last``, the last dimension will be treated as channel\n                dimension so the size of the :attr:`input` should be\n                `[batch_size, X, channel]`. If ``channels_first``, first\n                dimension will be treated as channel dimension so the size\n                should be `[batch_size, channel, X]`. Defaults to None.\n                If None, the value will be picked from hyperparameters.\n        Returns:\n            The output of the final layer.\n        """"""\n        if input.dim() != 3:\n            raise ValueError(""\'input\' should be a 3D tensor."")\n\n        if data_format is None:\n            data_format = self.hparams[""data_format""]\n\n        if data_format == ""channels_first"":\n            # masking requires channels in last dimension\n            input = input.permute(0, 2, 1)\n\n            if sequence_length is not None:\n                input = mask_sequences(input, sequence_length,\n                                       dtype=dtype, time_major=False)\n\n            # network is constructed for channel first tensors\n            input = input.permute(0, 2, 1)\n\n            output = super().forward(input)\n\n        elif data_format == ""channels_last"":\n            if sequence_length is not None:\n                input = mask_sequences(input, sequence_length,\n                                       dtype=dtype, time_major=False)\n\n            input = input.permute(0, 2, 1)\n\n            output = super().forward(input)\n\n            # transpose only when tensors are 3D\n            if output.dim() == 3:\n                output = output.permute(0, 2, 1)\n        else:\n            raise ValueError(""Invalid \'data_format\'"")\n\n        return output\n\n    def _infer_dense_layer_input_size(self, input: torch.Tensor) -> torch.Size:\n        # feed forward the input on the conv part of the network to infer\n        # input shape for dense layers\n        with torch.no_grad():\n            output = super().forward(input)\n        return output.view(output.size()[0], -1).size()\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of :meth:`forward` output.\n        """"""\n        if self.hparams.num_dense_layers <= 0:\n            out_channels = self._hparams.out_channels\n            if not isinstance(out_channels, (list, tuple)):\n                out_channels = [out_channels]\n            nconv = self._hparams.num_conv_layers\n            if nconv == 1:\n                kernel_size = _to_list(self._hparams.kernel_size)\n                if not isinstance(kernel_size[0], (list, tuple)):\n                    kernel_size = [kernel_size]\n            elif nconv > 1:\n                kernel_size = _to_list(self._hparams.kernel_size,\n                                       \'kernel_size\', nconv)\n                kernel_size = [_to_list(ks) for ks in kernel_size]\n            return out_channels[-1] * len(kernel_size[-1])\n        else:\n            out_features = self._hparams.out_features\n            if isinstance(out_features, (list, tuple)):\n                return out_features[-1]\n            else:\n                return out_features\n'"
texar/torch/modules/networks/network_base.py,8,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for feed forward neural networks.\n""""""\n\nfrom typing import Any, Dict, List, Optional, Union\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.core.layers import get_layer\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.module_base import ModuleBase\nfrom texar.torch.utils.utils import uniquify_str\n\n__all__ = [\n    ""FeedForwardNetworkBase"",\n]\n\n\nclass FeedForwardNetworkBase(ModuleBase):\n    r""""""Base class inherited by all feed-forward network classes.\n\n    Args:\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    See :meth:`forward` for the inputs and outputs.\n    """"""\n\n    def __init__(self,\n                 hparams: Optional[Union[HParams, Dict[str, Any]]] = None):\n        super().__init__(hparams)\n\n        self._layers = nn.ModuleList()\n        self._layer_names: List[str] = []\n        self._layers_by_name: Dict[str, nn.Module] = {}\n        self._layer_outputs: List[torch.Tensor] = []\n        self._layer_outputs_by_name: Dict[str, torch.Tensor] = {}\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""NN""\n            }\n        """"""\n        return {\n            ""name"": ""NN""\n        }\n\n    def __repr__(self) -> str:\n        if len(list(self.modules())) == 1:  # only contains `_layers`\n            return ModuleBase.__repr__(self._layers)\n        return super().__repr__()\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor) -> torch.Tensor:\n        r""""""Feeds forward inputs through the network layers and returns outputs.\n\n        Args:\n            input: The inputs to the network. The requirements on inputs\n                depends on the first layer and subsequent layers in the\n                network.\n\n        Returns:\n            The output of the network.\n        """"""\n        outputs = input\n        for layer in self._layers:\n            outputs = layer(outputs)\n\n        return outputs\n\n    def append_layer(self, layer: Union[nn.Module, HParams, Dict[str, Any]]):\n        r""""""Appends a layer to the end of the network.\n\n        Args:\n            layer: A subclass of :torch_nn:`Module`, or a dict of layer\n                hyperparameters.\n        """"""\n        layer_ = layer\n        if not isinstance(layer_, nn.Module):\n            layer_ = get_layer(hparams=layer_)\n        self._layers.append(layer_)\n        layer_name = uniquify_str(layer_.__class__.__name__, self._layer_names)\n        self._layer_names.append(layer_name)\n        self._layers_by_name[layer_name] = layer_\n\n    def has_layer(self, layer_name: str) -> bool:\n        r""""""Returns `True` if the network with the name exists. Returns\n        `False` otherwise.\n\n        Args:\n            layer_name (str): Name of the layer.\n        """"""\n        return layer_name in self._layers_by_name\n\n    def layer_by_name(self, layer_name: str) -> Optional[nn.Module]:\n        r""""""Returns the layer with the name. Returns `None` if the layer name\n        does not exist.\n\n        Args:\n            layer_name (str): Name of the layer.\n        """"""\n        return self._layers_by_name.get(layer_name, None)\n\n    @property\n    def layers_by_name(self) -> Dict[str, nn.Module]:\n        r""""""A dictionary mapping layer names to the layers.\n        """"""\n        return self._layers_by_name\n\n    @property\n    def layers(self) -> nn.ModuleList:\n        r""""""A list of the layers.\n        """"""\n        return self._layers\n\n    @property\n    def layer_names(self) -> List[str]:\n        r""""""A list of uniquified layer names.\n        """"""\n        return self._layer_names\n\n    def _build_layers(self,\n                      layers: Optional[nn.ModuleList] = None,\n                      layer_hparams: Optional[List[\n                          Union[HParams, Dict[str, Any]]]] = None):\n        r""""""Builds layers.\n\n        Either :attr:`layer_hparams` or :attr:`layers` must be\n        provided. If both are given, :attr:`layers` will be used.\n\n        Args:\n            layers (optional): A list of layer instances supplied as an instance\n                of :torch_nn:`ModuleList`.\n            layer_hparams (optional): A list of layer hparams, each to which\n                is fed to :func:`~texar.torch.core.layers.get_layer` to create\n                the layer instance.\n        """"""\n        if layers is not None:\n            self._layers = layers\n        else:\n            if layer_hparams is None:\n                raise ValueError(\n                    \'Either `layer` or `layer_hparams` is required.\')\n            self._layers = nn.ModuleList()\n            for _, hparams in enumerate(layer_hparams):\n                self._layers.append(get_layer(hparams=hparams))\n\n        for layer in self._layers:\n            layer_name = uniquify_str(layer.__class__.__name__,\n                                      self._layer_names)\n            self._layer_names.append(layer_name)\n            self._layers_by_name[layer_name] = layer\n'"
texar/torch/modules/networks/networks.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious neural networks and related utilities.\n""""""\n\nfrom texar.torch.modules.networks.network_base import FeedForwardNetworkBase\nfrom texar.torch.utils.utils import get_output_size\n\n__all__ = [\n    ""FeedForwardNetwork"",\n]\n\n\nclass FeedForwardNetwork(FeedForwardNetworkBase):\n    r""""""Feed-forward neural network that consists of a sequence of layers.\n\n    Args:\n        layers (list, optional): A list of :torch_nn:`Linear`\n            instances composing the network. If not given, layers are created\n            according to :attr:`hparams`.\n        hparams (dict, optional): Embedder hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    See :meth:`forward` for the inputs and outputs.\n\n    Example:\n\n        .. code-block:: python\n\n            hparams = { # Builds a two-layer dense NN\n                ""layers"": [\n                    { ""type"": ""Dense"", ""kwargs"": { ""units"": 256 },\n                    { ""type"": ""Dense"", ""kwargs"": { ""units"": 10 }\n                ]\n            }\n            nn = FeedForwardNetwork(hparams=hparams)\n\n            inputs = torch.randn([64, 100])\n            outputs = nn(inputs)\n            # outputs == Tensor of shape [64, 10]\n    """"""\n\n    def __init__(self, layers=None, hparams=None):\n        super().__init__(hparams=hparams)\n\n        self._build_layers(layers=layers, layer_hparams=self._hparams.layers)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""layers"": [],\n                ""name"": ""NN""\n            }\n\n        Here:\n\n        `""layers""`: list\n            A list of layer hyperparameters. See\n            :func:`~texar.torch.core.get_layer` for details on layer\n            hyperparameters.\n\n        `""name""`: str\n            Name of the network.\n        """"""\n        return {\n            ""layers"": [],\n            ""name"": ""NN""\n        }\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of network layers output. If output size is\n        only determined by input, the feature size is equal to ``-1``.\n        """"""\n        for i, layer in enumerate(reversed(self._layers)):\n            size = get_output_size(layer)\n            size_ext = getattr(layer, \'output_size\', None)\n            if size_ext is not None:\n                size = size_ext\n            if size is None:\n                break\n            if size > 0:\n                return size\n            elif i == len(self._layers) - 1:\n                return -1\n\n        raise ValueError(""\'output_size\' can not be calculated because ""\n                         ""\'FeedForwardNetwork\' contains submodule ""\n                         ""whose output size cannot be determined."")\n'"
texar/torch/modules/pretrained/__init__.py,6,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPre-trained modules of Texar library.\n""""""\n\nfrom texar.torch.modules.pretrained.pretrained_base import *\nfrom texar.torch.modules.pretrained.bert import *\nfrom texar.torch.modules.pretrained.gpt2 import *\nfrom texar.torch.modules.pretrained.roberta import *\nfrom texar.torch.modules.pretrained.xlnet import *\nfrom texar.torch.modules.pretrained.t5 import *\n'"
texar/torch/modules/pretrained/bert.py,9,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils of BERT Modules.\n""""""\n\nfrom typing import Any, Dict\n\nimport json\nimport os\n\nfrom abc import ABC\n\nimport torch\n\nfrom texar.torch.modules.pretrained.pretrained_base import PretrainedMixin\n\n__all__ = [\n    ""PretrainedBERTMixin"",\n]\n\n_BERT_PATH = ""https://storage.googleapis.com/bert_models/""\n_BIOBERT_PATH = ""https://github.com/naver/biobert-pretrained/releases/download/""\n_SCIBERT_PATH = ""https://s3-us-west-2.amazonaws.com/ai2-s2-research/"" \\\n                ""scibert/tensorflow_models/""\n_SPANBERT_PATH = ""https://dl.fbaipublicfiles.com/fairseq/models/""\n\n\nclass PretrainedBERTMixin(PretrainedMixin, ABC):\n    r""""""A mixin class to support loading pre-trained checkpoints for modules\n    that implement the BERT model.\n\n    Both standard BERT models and many domain specific BERT-based models are\n    supported. You can specify the :attr:`pretrained_model_name` argument to\n    pick which pre-trained BERT model to use. All available categories of\n    pre-trained models (and names) include:\n\n    * **Standard BERT**: proposed in (`Devlin et al`. 2018)\n      `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_\n      . A bidirectional Transformer language model pre-trained on large text\n      corpora. Available model names include:\n\n        * ``bert-base-uncased``: 12-layer, 768-hidden, 12-heads,\n          110M parameters.\n        * ``bert-large-uncased``: 24-layer, 1024-hidden, 16-heads,\n          340M parameters.\n        * ``bert-base-cased``: 12-layer, 768-hidden, 12-heads , 110M parameters.\n        * ``bert-large-cased``: 24-layer, 1024-hidden, 16-heads,\n          340M parameters.\n        * ``bert-base-multilingual-uncased``: 102 languages, 12-layer,\n          768-hidden, 12-heads, 110M parameters.\n        * ``bert-base-multilingual-cased``: 104 languages, 12-layer, 768-hidden,\n          12-heads, 110M parameters.\n        * ``bert-base-chinese``: Chinese Simplified and Traditional, 12-layer,\n          768-hidden, 12-heads, 110M parameters.\n\n    * **BioBERT**: proposed in (`Lee et al`. 2019)\n      `BioBERT: a pre-trained biomedical language representation model for biomedical text mining`_\n      . A domain specific language representation model pre-trained on\n      large-scale biomedical corpora. Based on the BERT architecture, BioBERT\n      effectively transfers the knowledge from a large amount of biomedical\n      texts to biomedical text mining models with minimal task-specific\n      architecture modifications. Available model names include:\n\n        * ``biobert-v1.0-pmc``: BioBERT v1.0 (+ PMC 270K) - based on\n          BERT-base-Cased (same vocabulary).\n        * ``biobert-v1.0-pubmed-pmc``: BioBERT v1.0 (+ PubMed 200K + PMC 270K) -\n          based on BERT-base-Cased (same vocabulary).\n        * ``biobert-v1.0-pubmed``: BioBERT v1.0 (+ PubMed 200K) - based on\n          BERT-base-Cased (same vocabulary).\n        * ``biobert-v1.1-pubmed``: BioBERT v1.1 (+ PubMed 1M) - based on\n          BERT-base-Cased (same vocabulary).\n\n    * **SciBERT**: proposed in (`Beltagy et al`. 2019)\n      `SciBERT: A Pretrained Language Model for Scientific Text`_. A BERT model\n      trained on scientific text. SciBERT leverages unsupervised pre-training\n      on a large multi-domain corpus of scientific publications to improve\n      performance on downstream scientific NLP tasks. Available model\n      names include:\n\n        * ``scibert-scivocab-uncased``: Uncased version of the model trained\n          on its own vocabulary.\n        * ``scibert-scivocab-cased``: Cased version of the model trained on\n          its own vocabulary.\n        * ``scibert-basevocab-uncased``: Uncased version of the model trained\n          on the original BERT vocabulary.\n        * ``scibert-basevocab-cased``: Cased version of the model trained on\n          the original BERT vocabulary.\n\n    * **SpanBERT**: proposed in (`Joshi et al`. 2019)\n      `SpanBERT: Improving Pre-training by Representing and Predicting Spans`_.\n      As a variant of the standard BERT model, SpanBERT extends BERT by\n      (1) masking contiguous random spans, rather than random tokens, and\n      (2) training the span boundary representations to predict the entire\n      content of the masked span, without relying on the individual token\n      representations within it. Differing from the standard BERT, the\n      SpanBERT model does not use segmentation embedding. Available model names\n      include:\n\n        * ``spanbert-base-cased``: SpanBERT using the BERT-base architecture,\n          12-layer, 768-hidden, 12-heads , 110M parameters.\n        * ``spanbert-large-cased``: SpanBERT using the BERT-large architecture,\n          24-layer, 1024-hidden, 16-heads, 340M parameters.\n\n    We provide the following BERT classes:\n\n      * :class:`~texar.torch.modules.BERTEncoder` for text encoding.\n      * :class:`~texar.torch.modules.BERTClassifier` for text classification and\n        sequence tagging.\n\n    .. _`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`:\n        https://arxiv.org/abs/1810.04805\n\n    .. _`BioBERT: a pre-trained biomedical language representation model for biomedical text mining`:\n        https://arxiv.org/abs/1901.08746\n\n    .. _`SciBERT: A Pretrained Language Model for Scientific Text`:\n        https://arxiv.org/abs/1903.10676\n\n    .. _`SpanBERT: Improving Pre-training by Representing and Predicting Spans`:\n        https://arxiv.org/abs/1907.10529\n    """"""\n\n    _MODEL_NAME = ""BERT""\n    _MODEL2URL = {\n        # Standard BERT\n        \'bert-base-uncased\':\n            _BERT_PATH + ""2018_10_18/uncased_L-12_H-768_A-12.zip"",\n        \'bert-large-uncased\':\n            _BERT_PATH + ""2018_10_18/uncased_L-24_H-1024_A-16.zip"",\n        \'bert-base-cased\':\n            _BERT_PATH + ""2018_10_18/cased_L-12_H-768_A-12.zip"",\n        \'bert-large-cased\':\n            _BERT_PATH + ""2018_10_18/cased_L-24_H-1024_A-16.zip"",\n        \'bert-base-multilingual-uncased\':\n            _BERT_PATH + ""2018_11_23/multi_cased_L-12_H-768_A-12.zip"",\n        \'bert-base-multilingual-cased\':\n            _BERT_PATH + ""2018_11_03/multilingual_L-12_H-768_A-12.zip"",\n        \'bert-base-chinese\':\n            _BERT_PATH + ""2018_11_03/chinese_L-12_H-768_A-12.zip"",\n\n        # BioBERT\n        \'biobert-v1.0-pmc\':\n            _BIOBERT_PATH + \'v1.0-pmc/biobert_v1.0_pmc.tar.gz\',\n        \'biobert-v1.0-pubmed-pmc\':\n            _BIOBERT_PATH + \'v1.0-pubmed-pmc/biobert_v1.0_pubmed_pmc.tar.gz\',\n        \'biobert-v1.0-pubmed\':\n            _BIOBERT_PATH + \'v1.0-pubmed/biobert_v1.0_pubmed.tar.gz\',\n        \'biobert-v1.1-pubmed\':\n            _BIOBERT_PATH + \'v1.1-pubmed/biobert_v1.1_pubmed.tar.gz\',\n\n        # SciBERT\n        \'scibert-scivocab-uncased\':\n            _SCIBERT_PATH + \'scibert_scivocab_uncased.tar.gz\',\n        \'scibert-scivocab-cased\':\n            _SCIBERT_PATH + \'scibert_scivocab_cased.tar.gz\',\n        \'scibert-basevocab-uncased\':\n            _SCIBERT_PATH + \'scibert_basevocab_uncased.tar.gz\',\n        \'scibert-basevocab-cased\':\n            _SCIBERT_PATH + \'scibert_basevocab_cased.tar.gz\',\n\n        # SpanBERT\n        \'spanbert-base-cased\':\n            _SPANBERT_PATH + ""spanbert_hf_base.tar.gz"",\n        \'spanbert-large-cased\':\n            _SPANBERT_PATH + ""spanbert_hf.tar.gz"",\n    }\n    _MODEL2CKPT = {\n        # Standard BERT\n        \'bert-base-uncased\': \'bert_model.ckpt\',\n        \'bert-large-uncased\': \'bert_model.ckpt\',\n        \'bert-base-cased\': \'bert_model.ckpt\',\n        \'bert-large-cased\': \'bert_model.ckpt\',\n        \'bert-base-multilingual-uncased\': \'bert_model.ckpt\',\n        \'bert-base-multilingual-cased\': \'bert_model.ckpt\',\n        \'bert-base-chinese\': \'bert_model.ckpt\',\n\n        # BioBERT\n        \'biobert-v1.0-pmc\': \'biobert_model.ckpt\',\n        \'biobert-v1.0-pubmed-pmc\': \'biobert_model.ckpt\',\n        \'biobert-v1.0-pubmed\': \'biobert_model.ckpt\',\n        \'biobert-v1.1-pubmed\': \'model.ckpt-1000000\',\n\n        # SciBERT\n        \'scibert-scivocab-uncased\': \'bert_model.ckpt\',\n        \'scibert-scivocab-cased\': \'bert_model.ckpt\',\n        \'scibert-basevocab-uncased\': \'bert_model.ckpt\',\n        \'scibert-basevocab-cased\': \'bert_model.ckpt\',\n\n        # SpanBERT\n        \'spanbert-base-cased\': \'pytorch_model.bin\',\n        \'spanbert-large-cased\': \'pytorch_model.bin\',\n    }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str) -> Dict[str, Any]:\n        info = list(os.walk(cache_dir))\n        root, _, files = info[0]\n        config_path = None\n\n        for file in files:\n            if file in (\'bert_config.json\', \'config.json\'):\n                config_path = os.path.join(root, file)\n                with open(config_path) as f:\n                    config_ckpt = json.loads(f.read())\n                    hidden_dim = config_ckpt[\'hidden_size\']\n                    vocab_size = config_ckpt[\'vocab_size\']\n                    if not pretrained_model_name.startswith(\'spanbert\'):\n                        type_vocab_size = config_ckpt[\'type_vocab_size\']\n                    position_size = config_ckpt[\'max_position_embeddings\']\n                    embedding_dropout = config_ckpt[\'hidden_dropout_prob\']\n                    num_blocks = config_ckpt[\'num_hidden_layers\']\n                    num_heads = config_ckpt[\'num_attention_heads\']\n                    dropout_rate = config_ckpt[\'attention_probs_dropout_prob\']\n                    residual_dropout = config_ckpt[\'hidden_dropout_prob\']\n                    intermediate_size = config_ckpt[\'intermediate_size\']\n                    hidden_act = config_ckpt[\'hidden_act\']\n\n        if config_path is None:\n            raise ValueError(f""Cannot find the config file in {cache_dir}"")\n\n        configs = {\n            \'hidden_size\': hidden_dim,\n            \'embed\': {\n                \'name\': \'word_embeddings\',\n                \'dim\': hidden_dim\n            },\n            \'vocab_size\': vocab_size,\n            \'position_embed\': {\n                \'name\': \'position_embeddings\',\n                \'dim\': hidden_dim\n            },\n            \'position_size\': position_size,\n            \'encoder\': {\n                \'name\': \'encoder\',\n                \'embedding_dropout\': embedding_dropout,\n                \'num_blocks\': num_blocks,\n                \'multihead_attention\': {\n                    \'use_bias\': True,\n                    \'num_units\': hidden_dim,\n                    \'num_heads\': num_heads,\n                    \'output_dim\': hidden_dim,\n                    \'dropout_rate\': dropout_rate,\n                    \'name\': \'self\'\n                },\n                \'residual_dropout\': residual_dropout,\n                \'dim\': hidden_dim,\n                \'eps\': 1e-12,\n                \'use_bert_config\': True,\n                \'poswise_feedforward\': {\n                    ""layers"": [{\n                        \'type\': \'Linear\',\n                        \'kwargs\': {\n                            \'in_features\': hidden_dim,\n                            \'out_features\': intermediate_size,\n                            \'bias\': True,\n                        }\n                    }, {\n                        \'type\': \'Bert\' + hidden_act.upper()\n                    }, {\n                        \'type\': \'Linear\',\n                        \'kwargs\': {\n                            \'in_features\': intermediate_size,\n                            \'out_features\': hidden_dim,\n                            \'bias\': True,\n                        }\n                    }],\n                },\n            }\n        }\n\n        if not pretrained_model_name.startswith(\'spanbert\'):\n            configs.update({\n                \'segment_embed\': {\n                    \'name\': \'token_type_embeddings\',\n                    \'dim\': hidden_dim},\n                \'type_vocab_size\': type_vocab_size,\n            })\n\n        return configs\n\n    def _init_from_checkpoint(self, pretrained_model_name: str,\n                              cache_dir: str, **kwargs):\n        if pretrained_model_name.startswith(\'spanbert\'):\n            global_tensor_map = {\n                \'bert.embeddings.word_embeddings.weight\':\n                    \'word_embedder._embedding\',\n                \'bert.embeddings.position_embeddings.weight\':\n                    \'position_embedder._embedding\',\n                \'bert.embeddings.LayerNorm.weight\':\n                    \'encoder.input_normalizer.weight\',\n                \'bert.embeddings.LayerNorm.bias\':\n                    \'encoder.input_normalizer.bias\',\n            }\n\n            attention_tensor_map = {\n                ""attention.self.key.bias"": ""self_attns.{}.K_dense.bias"",\n                ""attention.self.query.bias"": ""self_attns.{}.Q_dense.bias"",\n                ""attention.self.value.bias"": ""self_attns.{}.V_dense.bias"",\n                ""attention.output.dense.bias"": ""self_attns.{}.O_dense.bias"",\n                ""attention.output.LayerNorm.weight"":\n                    ""poswise_layer_norm.{}.weight"",\n                ""attention.output.LayerNorm.bias"": ""poswise_layer_norm.{}.bias"",\n                ""intermediate.dense.bias"": ""poswise_networks.{}._layers.0.bias"",\n                ""output.dense.bias"": ""poswise_networks.{}._layers.2.bias"",\n                ""output.LayerNorm.weight"": ""output_layer_norm.{}.weight"",\n                ""output.LayerNorm.bias"": ""output_layer_norm.{}.bias"",\n                ""attention.self.key.weight"": ""self_attns.{}.K_dense.weight"",\n                ""attention.self.query.weight"": ""self_attns.{}.Q_dense.weight"",\n                ""attention.self.value.weight"": ""self_attns.{}.V_dense.weight"",\n                ""attention.output.dense.weight"": ""self_attns.{}.O_dense.weight"",\n                ""intermediate.dense.weight"":\n                    ""poswise_networks.{}._layers.0.weight"",\n                ""output.dense.weight"": ""poswise_networks.{}._layers.2.weight"",\n            }\n            checkpoint_path = os.path.abspath(os.path.join(\n                cache_dir, self._MODEL2CKPT[pretrained_model_name]))\n\n            device = next(self.parameters()).device\n            params = torch.load(checkpoint_path, map_location=device)\n\n            for name, tensor in params.items():\n                if name in global_tensor_map:\n                    v_name = global_tensor_map[name]\n                    pointer = self._name_to_variable(v_name)\n                    assert pointer.shape == tensor.shape\n                    pointer.data = tensor.data.type(pointer.dtype)\n                elif name.startswith(\'bert.encoder.layer.\'):\n                    name = name.lstrip(\'bert.encoder.layer.\')\n                    layer_num, layer_name = name.split(\'.\', 1)\n                    if layer_name in attention_tensor_map:\n                        v_name = attention_tensor_map[layer_name]\n                        pointer = self._name_to_variable(\n                            \'encoder.\' + v_name.format(layer_num))\n                        assert pointer.shape == tensor.shape\n                        pointer.data = tensor.data.type(pointer.dtype)\n\n            return\n\n        try:\n            import numpy as np\n            import tensorflow as tf\n        except ImportError:\n            print(""Loading TensorFlow models in PyTorch requires installing ""\n                  ""TensorFlow. Please see https://www.tensorflow.org/install/ ""\n                  ""for installation instructions."")\n            raise\n\n        global_tensor_map = {\n            \'bert/embeddings/word_embeddings\': \'word_embedder._embedding\',\n            \'bert/embeddings/token_type_embeddings\':\n                \'segment_embedder._embedding\',\n            \'bert/embeddings/position_embeddings\':\n                \'position_embedder._embedding\',\n            \'bert/embeddings/LayerNorm/beta\':\n                \'encoder.input_normalizer.bias\',\n            \'bert/embeddings/LayerNorm/gamma\':\n                \'encoder.input_normalizer.weight\',\n        }\n        layer_tensor_map = {\n            ""attention/self/key/bias"": ""self_attns.{}.K_dense.bias"",\n            ""attention/self/query/bias"": ""self_attns.{}.Q_dense.bias"",\n            ""attention/self/value/bias"": ""self_attns.{}.V_dense.bias"",\n            ""attention/output/dense/bias"": ""self_attns.{}.O_dense.bias"",\n            ""attention/output/LayerNorm/gamma"": ""poswise_layer_norm.{}.weight"",\n            ""attention/output/LayerNorm/beta"": ""poswise_layer_norm.{}.bias"",\n            ""intermediate/dense/bias"": ""poswise_networks.{}._layers.0.bias"",\n            ""output/dense/bias"": ""poswise_networks.{}._layers.2.bias"",\n            ""output/LayerNorm/gamma"": ""output_layer_norm.{}.weight"",\n            ""output/LayerNorm/beta"": ""output_layer_norm.{}.bias"",\n        }\n        layer_transpose_map = {\n            ""attention/self/key/kernel"": ""self_attns.{}.K_dense.weight"",\n            ""attention/self/query/kernel"": ""self_attns.{}.Q_dense.weight"",\n            ""attention/self/value/kernel"": ""self_attns.{}.V_dense.weight"",\n            ""attention/output/dense/kernel"": ""self_attns.{}.O_dense.weight"",\n            ""intermediate/dense/kernel"": ""poswise_networks.{}._layers.0.weight"",\n            ""output/dense/kernel"": ""poswise_networks.{}._layers.2.weight"",\n        }\n        pooler_map = {\n            \'bert/pooler/dense/bias\': \'pooler.0.bias\',\n            \'bert/pooler/dense/kernel\': \'pooler.0.weight\'\n        }\n        tf_path = os.path.abspath(os.path.join(\n            cache_dir, self._MODEL2CKPT[pretrained_model_name]))\n\n        # Load weights from TF model\n        init_vars = tf.train.list_variables(tf_path)\n        tfnames, arrays = [], []\n        for name, _ in init_vars:\n            array = tf.train.load_variable(tf_path, name)\n            tfnames.append(name)\n            arrays.append(array.squeeze())\n        py_prefix = ""encoder.""\n\n        idx = 0\n        for name, array in zip(tfnames, arrays):\n            if name.startswith(\'cls\') or name == \'global_step\' or \\\n                    name.endswith(\'adam_m\') or name.endswith(\'adam_v\'):\n                # ignore those variables begin with cls\n                # ignore \'global_step\' variable\n                # ignore optimizer state variable\n                continue\n\n            if name in global_tensor_map:\n                v_name = global_tensor_map[name]\n                pointer = self._name_to_variable(v_name)\n                assert pointer.shape == array.shape\n                pointer.data = torch.from_numpy(array)\n                idx += 1\n            elif name in pooler_map:\n                pointer = self._name_to_variable(pooler_map[name])\n                if name.endswith(\'bias\'):\n                    assert pointer.shape == array.shape\n                    pointer.data = torch.from_numpy(array)\n                    idx += 1\n                else:\n                    array_t = np.transpose(array)\n                    assert pointer.shape == array_t.shape\n                    pointer.data = torch.from_numpy(array_t)\n                    idx += 1\n            else:\n                # here name is the TensorFlow variable name\n                name_tmp = name.split(""/"")\n                # e.g. layer_\n                layer_no = name_tmp[2][6:]\n                name_tmp = ""/"".join(name_tmp[3:])\n                if name_tmp in layer_tensor_map:\n                    v_name = layer_tensor_map[name_tmp].format(layer_no)\n                    pointer = self._name_to_variable(py_prefix + v_name)\n                    assert pointer.shape == array.shape\n                    pointer.data = torch.from_numpy(array)\n                elif name_tmp in layer_transpose_map:\n                    v_name = layer_transpose_map[name_tmp].format(layer_no)\n                    pointer = self._name_to_variable(py_prefix + v_name)\n                    array_t = np.transpose(array)\n                    assert pointer.shape == array_t.shape\n                    pointer.data = torch.from_numpy(array_t)\n                else:\n                    raise NameError(f""Variable with name \'{name}\' not found"")\n                idx += 1\n'"
texar/torch/modules/pretrained/gpt2.py,16,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils of GPT2 Modules.\n""""""\n\nimport json\nimport os\nimport warnings\nfrom abc import ABC\nfrom typing import Any, Dict\n\nimport torch\n\nfrom texar.torch.modules.pretrained.pretrained_base import PretrainedMixin\n\n__all__ = [\n    ""PretrainedGPT2Mixin"",\n]\n\n_GPT2_PATH = ""https://storage.googleapis.com/gpt-2/models/""\n_CHECKPOINT_FILES = [\n    ""checkpoint"", ""encoder.json"", ""hparams.json"", ""vocab.bpe"",\n    ""model.ckpt.data-00000-of-00001"", ""model.ckpt.index"", ""model.ckpt.meta""]\n\n\nclass PretrainedGPT2Mixin(PretrainedMixin, ABC):\n    r""""""A mixin class to support loading pre-trained checkpoints for modules\n    that implement the GPT2 model.\n\n    The GPT2 model was proposed in\n    `Language Models are Unsupervised Multitask Learners`_\n    by `Radford et al.` from OpenAI. It is a unidirectional Transformer model\n    pre-trained using the vanilla language modeling objective on a large corpus.\n\n    The available GPT2 models are as follows:\n\n      * ``gpt2-small``: Small version of GPT-2, 124M parameters.\n      * ``gpt2-medium``: Medium version of GPT-2, 355M parameters.\n      * ``gpt2-large``: Large version of GPT-2, 774M parameters.\n      * ``gpt2-xl``: XL version of GPT-2, 1558M parameters.\n\n    We provide the following GPT2 classes:\n\n      * :class:`~texar.torch.modules.GPT2Encoder` for text encoding.\n      * :class:`~texar.torch.modules.GPT2Decoder` for text generation and\n        decoding.\n      * :class:`~texar.torch.modules.GPT2Classifier` for text classification and\n        sequence tagging.\n\n    .. _`Language Models are Unsupervised Multitask Learners`:\n        https://openai.com/blog/better-language-models/\n    """"""\n    _MODEL_NAME = ""GPT2""\n    _MODEL2URL = {\n        \'gpt2-small\': [_GPT2_PATH + f""124M/{file}""\n                       for file in _CHECKPOINT_FILES],\n        \'gpt2-medium\': [_GPT2_PATH + f""355M/{file}""\n                        for file in _CHECKPOINT_FILES],\n        \'gpt2-large\': [_GPT2_PATH + f""774M/{file}""\n                       for file in _CHECKPOINT_FILES],\n        \'gpt2-xl\': [_GPT2_PATH + f""1558M/{file}""\n                    for file in _CHECKPOINT_FILES],\n    }\n\n    _IS_DECODE = False\n\n    # Raise warning for the deprecated pre-trained model names\n    class MyDict(dict):\n        def __contains__(self, key):\n            if key == \'117M\':\n                warnings.warn(""Pre-trained model name \'117M\' is deprecated, ""\n                              ""use \'gpt2-small\' instead."", UserWarning)\n                return True\n            elif key == \'345M\':\n                warnings.warn(""Pre-trained model name \'345M\' is deprecated, ""\n                              ""use \'gpt2-medium\' instead."", UserWarning)\n                return True\n            else:\n                return super().__contains__(key)\n    _DEPRECATED_MODEL2URL = {\n        \'117M\': [_GPT2_PATH + f""124M/{file}"" for file in _CHECKPOINT_FILES],\n        \'345M\': [_GPT2_PATH + f""355M/{file}"" for file in _CHECKPOINT_FILES],\n    }\n    _MODEL2URL.update(_DEPRECATED_MODEL2URL)\n    _MODEL2URL = MyDict(_MODEL2URL)  # type: ignore\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str) -> Dict[str, Any]:\n        info = list(os.walk(cache_dir))\n        root, _, files = info[0]\n        config_path = None\n        for file in files:\n            if file.endswith(\'hparams.json\'):\n                config_path = os.path.join(root, file)\n        if config_path is None:\n            raise ValueError(f""Cannot find the config file in {cache_dir}"")\n\n        with open(config_path) as f:\n            config_gpt = json.loads(f.read())\n\n        hidden_dim = config_gpt[""n_embd""]\n        configs = {\n            ""vocab_size"": config_gpt[""n_vocab""],\n            ""context_size"": config_gpt[""n_ctx""],\n            ""embedding_size"": config_gpt[""n_embd""], ""embed"": {\n                ""dim"": hidden_dim,\n            },\n            ""position_size"": config_gpt[""n_ctx""],\n            ""position_embed"": {\n                ""dim"": hidden_dim\n            }\n        }\n\n        module_name = \'decoder\' if cls._IS_DECODE else \'encoder\'\n        eps = 1e-5 if cls._IS_DECODE else 1e-6\n\n        configs.update({module_name: {\n            ""dim"": hidden_dim,\n            ""num_blocks"": config_gpt[""n_layer""],\n            ""embedding_dropout"": 0,\n            ""residual_dropout"": 0,\n            ""multihead_attention"": {\n                ""use_bias"": True,\n                ""num_units"": hidden_dim,\n                ""num_heads"": config_gpt[""n_head""],\n                ""output_dim"": hidden_dim,\n            },\n            ""initializer"": {\n                ""type"": ""variance_scaling_initializer"",\n                ""kwargs"": {\n                    ""factor"": 1.0,\n                    ""mode"": ""FAN_AVG"",\n                    ""uniform"": True,\n                },\n            },\n            \'eps\': eps,\n            ""poswise_feedforward"": {\n                ""layers"": [\n                    {\n                        ""type"": ""Linear"",\n                        ""kwargs"": {\n                            ""in_features"": hidden_dim,\n                            ""out_features"": hidden_dim * 4,\n                            ""bias"": True,\n                        }\n                    },\n                    {\n                        ""type"": ""GPTGELU"",\n                        ""kwargs"": {}\n                    },\n                    {\n                        ""type"": ""Linear"",\n                        ""kwargs"": {\n                            ""in_features"": hidden_dim * 4,\n                            ""out_features"": hidden_dim,\n                            ""bias"": True,\n                        }\n                    }\n                ],\n                ""name"": ""ffn"",\n            },\n        }})\n        if not cls._IS_DECODE:\n            configs[module_name].update({\'use_bert_config\': False})\n        return configs\n\n    def _init_from_checkpoint(self, pretrained_model_name: str,\n                              cache_dir: str,\n                              load_output_layer: bool = True, **kwargs):\n        r""""""Initialize model parameters from weights stored in the pre-trained\n        checkpoint.\n\n        Args:\n            pretrained_model_name (str): Name of the pre-trained model.\n            cache_dir (str): Path to the cache directory.\n            load_output_layer (bool): If `False`, will not load weights of the\n                output layer. Set this argument to `False` when loading weights\n                into a GPT2 encoder. Defaults to `True`.\n        """"""\n        try:\n            import numpy as np\n            import tensorflow as tf\n        except ImportError:\n            print(""Loading TensorFlow models in PyTorch requires installing ""\n                  ""TensorFlow. Please see https://www.tensorflow.org/install/ ""\n                  ""for installation instructions."")\n            raise\n\n        module_name = \'decoder\' if self._IS_DECODE else \'encoder\'\n\n        global_tensor_map = {\n            ""model/wte"": ""word_embedder.embedding"",\n            ""model/wpe"": ""position_embedder.embedding"",\n            ""model/ln_f/b"": module_name + "".final_layer_norm.bias"",\n            ""model/ln_f/g"": module_name + "".final_layer_norm.weight"",\n        }\n        layer_tensor_map = {\n            ""ln_1/b"": module_name + "".self_attn_layer_norm.{}.bias"",\n            ""ln_1/g"": module_name + "".self_attn_layer_norm.{}.weight"",\n            ""ln_2/b"": module_name + "".poswise_layer_norm.{}.bias"",\n            ""ln_2/g"": module_name + "".poswise_layer_norm.{}.weight"",\n            ""mlp/c_fc/b"": module_name + "".poswise_networks.{}._layers.0.bias"",\n            ""mlp/c_proj/b"": module_name + "".poswise_networks.{}._layers.2.bias"",\n            ""attn/c_proj/b"": module_name + "".self_attns.{}.O_dense.bias"",\n        }\n        layer_transpose_map = {\n            ""mlp/c_fc/w"": module_name + "".poswise_networks.{}._layers.0.weight"",\n            ""mlp/c_proj/w"": module_name + "".poswise_networks.{}._layers.2.""\n                                          ""weight"",\n            ""attn/c_proj/w"": module_name + "".self_attns.{}.O_dense.weight"",\n        }\n\n        tf_path = os.path.abspath(os.path.join(cache_dir, \'model.ckpt\'))\n        # Load weights from TF model\n        init_vars = tf.train.list_variables(tf_path)\n        names = []\n        arrays = []\n        for name, _ in init_vars:\n            array = tf.train.load_variable(tf_path, name)\n            names.append(name)\n            arrays.append(array.squeeze())\n\n        tensor_names = []\n        for name, _ in self.named_parameters():\n            tensor_names.append(name)\n\n        for name, array in zip(names, arrays):\n            if name in global_tensor_map:\n                v_name = global_tensor_map[name]\n                if name == ""model/wte"":\n                    pointer = self._name_to_variable(v_name)\n                    assert pointer.shape == array.shape\n                    pointer.data = torch.from_numpy(array)\n\n                    if load_output_layer:\n                        output_pointer = self._name_to_variable(\n                            ""decoder._output_layer.weight"")\n                        assert output_pointer.shape == array.shape\n                        output_pointer.data = torch.from_numpy(array)\n                elif name == ""model/wpe"":\n                    pointer = self._name_to_variable(v_name)\n                    assert pointer.shape == array.shape\n                    pointer.data = torch.from_numpy(array)\n                else:\n                    pointer = self._name_to_variable(v_name)\n                    assert pointer.shape == array.shape\n                    pointer.data = torch.from_numpy(array)\n\n            else:\n                name_tmp = name.split(""/"")\n                layer_no = name_tmp[1][1:]\n                name = ""/"".join(name_tmp[2:])\n                if name in layer_tensor_map:\n                    v_name = layer_tensor_map[name].format(layer_no)\n                    pointer = self._name_to_variable(v_name)\n                    assert pointer.shape == array.shape\n                    pointer.data = torch.from_numpy(array)\n                elif name in layer_transpose_map:\n                    v_name = layer_transpose_map[name].format(layer_no)\n                    pointer = self._name_to_variable(v_name)\n                    array_t = np.transpose(array)\n                    assert pointer.shape == array_t.shape\n                    pointer.data = torch.from_numpy(array_t)\n                elif name == ""attn/c_attn/w"":\n                    index_d = array.shape[-1] // 3\n\n                    Q_w = np.transpose(array[:, :index_d])\n                    K_w = np.transpose(array[:, index_d: 2 * index_d])\n                    V_w = np.transpose(array[:, 2 * index_d:])\n\n                    q_weight = self._name_to_variable(\n                        f""{module_name}.self_attns.{layer_no}.Q_dense.weight"")\n                    k_weight = self._name_to_variable(\n                        f""{module_name}.self_attns.{layer_no}.K_dense.weight"")\n                    v_weight = self._name_to_variable(\n                        f""{module_name}.self_attns.{layer_no}.V_dense.weight"")\n\n                    assert q_weight.shape == Q_w.shape\n                    assert k_weight.shape == K_w.shape\n                    assert v_weight.shape == V_w.shape\n\n                    q_weight.data = torch.from_numpy(Q_w)\n                    k_weight.data = torch.from_numpy(K_w)\n                    v_weight.data = torch.from_numpy(V_w)\n\n                elif name == ""attn/c_attn/b"":\n                    d = array.shape[0]\n                    Q_b = array[: d // 3]\n                    K_b = array[d // 3: 2 * d // 3]\n                    V_b = array[2 * d // 3:]\n                    q_bias = self._name_to_variable(\n                        f""{module_name}.self_attns.{layer_no}.Q_dense.bias"")\n                    k_bias = self._name_to_variable(\n                        f""{module_name}.self_attns.{layer_no}.K_dense.bias"")\n                    v_bias = self._name_to_variable(\n                        f""{module_name}.self_attns.{layer_no}.V_dense.bias"")\n\n                    assert q_bias.shape == Q_b.shape\n                    assert k_bias.shape == K_b.shape\n                    assert v_bias.shape == V_b.shape\n\n                    q_bias.data = torch.from_numpy(Q_b)\n                    k_bias.data = torch.from_numpy(K_b)\n                    v_bias.data = torch.from_numpy(V_b)\n\n                else:\n                    print(""Name error"", name)\n                    raise Exception\n'"
texar/torch/modules/pretrained/pretrained_base.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for Pre-trained Modules.\n""""""\nimport os\nimport sys\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom torch import nn\n\nfrom texar.torch.data.data_utils import maybe_download, get_filename\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.module_base import ModuleBase\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""default_download_dir"",\n    ""set_default_download_dir"",\n    ""PretrainedMixin"",\n]\n\n_default_texar_download_dir: Optional[Path] = None\n\n\ndef default_download_dir(name: str) -> Path:\n    r""""""Return the directory to which packages will be downloaded by default.\n    """"""\n    global _default_texar_download_dir  # pylint: disable=global-statement\n    if _default_texar_download_dir is None:\n        if sys.platform == \'win32\' and \'APPDATA\' in os.environ:\n            # On Windows, use %APPDATA%\n            home_dir = Path(os.environ[\'APPDATA\'])\n        else:\n            # Otherwise, install in the user\'s home directory.\n            home_dir = Path.home()\n\n        if os.access(home_dir, os.W_OK):\n            _default_texar_download_dir = home_dir / \'texar_data\'\n        else:\n            raise ValueError(f""The path {home_dir} is not writable. Please ""\n                             f""manually specify the download directory"")\n\n    if not _default_texar_download_dir.exists():\n        _default_texar_download_dir.mkdir(parents=True)\n\n    return _default_texar_download_dir / name\n\n\ndef set_default_download_dir(path: Union[str, Path]) -> None:\n    if isinstance(path, str):\n        path = Path(path)\n    elif not isinstance(path, Path):\n        raise ValueError(""`path` must be a string or a pathlib.Path object"")\n\n    if not os.access(path, os.W_OK):\n        raise ValueError(\n            f""The specified download directory {path} is not writable"")\n\n    global _default_texar_download_dir  # pylint: disable=global-statement\n    _default_texar_download_dir = path\n\n\nclass PretrainedMixin(ModuleBase, ABC):\n    r""""""A mixin class for all pre-trained classes to inherit.\n    """"""\n\n    _MODEL_NAME: str\n    _MODEL2URL: Dict[str, MaybeList[str]]\n\n    pretrained_model_dir: Optional[str]\n\n    @classmethod\n    def available_checkpoints(cls) -> List[str]:\n        return list(cls._MODEL2URL.keys())\n\n    def _name_to_variable(self, name: str) -> nn.Parameter:\n        r""""""Find the corresponding variable given the specified name.\n        """"""\n        pointer = self\n        for m_name in name.split("".""):\n            if m_name.isdigit():\n                num = int(m_name)\n                pointer = pointer[num]  # type: ignore\n            else:\n                pointer = getattr(pointer, m_name)\n        return pointer  # type: ignore\n\n    def load_pretrained_config(self,\n                               pretrained_model_name: Optional[str] = None,\n                               cache_dir: Optional[str] = None,\n                               hparams=None):\n        r""""""Load paths and configurations of the pre-trained model.\n\n        Args:\n            pretrained_model_name (optional): A str with the name\n                of a pre-trained model to load. If `None`, will use the model\n                name in :attr:`hparams`.\n            cache_dir (optional): The path to a folder in which the\n                pre-trained models will be cached. If `None` (default),\n                a default directory will be used.\n            hparams (dict or HParams, optional): Hyperparameters. Missing\n                hyperparameter will be set to default values. See\n                :meth:`default_hparams` for the hyperparameter structure\n                and default values.\n        """"""\n        if not hasattr(self, ""_hparams""):\n            self._hparams = HParams(hparams, self.default_hparams())\n        else:\n            # Probably already parsed by subclasses. We rely on subclass\n            # implementations to get this right.\n            # As a sanity check, we require `hparams` to be `None` in this case.\n            if hparams is not None:\n                raise ValueError(\n                    ""`self._hparams` is already assigned, but `hparams` ""\n                    ""argument is not None."")\n\n        self.pretrained_model_dir = None\n        self.pretrained_model_name = pretrained_model_name\n\n        if self.pretrained_model_name is None:\n            self.pretrained_model_name = self._hparams.pretrained_model_name\n        if self.pretrained_model_name is not None:\n            self.pretrained_model_dir = self.download_checkpoint(\n                self.pretrained_model_name, cache_dir)\n            pretrained_model_hparams = self._transform_config(\n                self.pretrained_model_name, self.pretrained_model_dir)\n            self._hparams = HParams(\n                pretrained_model_hparams, self._hparams.todict())\n\n    def init_pretrained_weights(self, *args, **kwargs):\n        if self.pretrained_model_dir:\n            self._init_from_checkpoint(\n                self.pretrained_model_name,\n                self.pretrained_model_dir, *args, **kwargs)\n        else:\n            self.reset_parameters()\n\n    def reset_parameters(self):\n        r""""""Initialize parameters of the pre-trained model. This method is only\n        called if pre-trained checkpoints are not loaded.\n        """"""\n        pass\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": None,\n                ""name"": ""pretrained_base""\n            }\n        """"""\n        return {\n            \'pretrained_model_name\': None,\n            \'name\': ""pretrained_base"",\n            \'@no_typecheck\': [\'pretrained_model_name\']\n        }\n\n    @classmethod\n    def download_checkpoint(cls, pretrained_model_name: str,\n                            cache_dir: Optional[str] = None) -> str:\n        r""""""Download the specified pre-trained checkpoint, and return the\n        directory in which the checkpoint is cached.\n\n        Args:\n            pretrained_model_name (str): Name of the model checkpoint.\n            cache_dir (str, optional): Path to the cache directory. If `None`,\n                uses the default directory (user\'s home directory).\n\n        Returns:\n            Path to the cache directory.\n        """"""\n        if pretrained_model_name in cls._MODEL2URL:\n            download_path = cls._MODEL2URL[pretrained_model_name]\n        else:\n            raise ValueError(\n                f""Pre-trained model not found: {pretrained_model_name}"")\n\n        if cache_dir is None:\n            cache_path = default_download_dir(cls._MODEL_NAME)\n        else:\n            cache_path = Path(cache_dir)\n        cache_path = cache_path / pretrained_model_name\n\n        if not cache_path.exists():\n            if isinstance(download_path, str):\n                filename = get_filename(download_path)\n                maybe_download(download_path, cache_path, extract=True)\n\n                # removing the compressed file\n                (cache_path / filename).unlink()\n\n                folder = None\n                # if extracted into a new directory\n                for file in cache_path.iterdir():\n                    if file.is_dir():\n                        folder = file\n                if folder is not None:\n                    for file in folder.iterdir():\n                        file.rename(file.parents[1] / file.name)\n                    folder.rmdir()\n            else:\n                for path in download_path:\n                    maybe_download(path, cache_path)\n            print(f""Pre-trained {cls._MODEL_NAME} checkpoint ""\n                  f""{pretrained_model_name} cached to {cache_path}"")\n        else:\n            print(f""Using cached pre-trained {cls._MODEL_NAME} checkpoint ""\n                  f""from {cache_path}."")\n\n        return str(cache_path)\n\n    @classmethod\n    @abstractmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str) -> Dict[str, Any]:\n        r""""""Load the official configuration file and transform it into\n        Texar-style hyperparameters.\n\n        Args:\n            pretrained_model_name (str): Name of the pre-trained model.\n            cache_dir (str): Path to the cache directory.\n\n        Returns:\n            dict: Texar module hyperparameters.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def _init_from_checkpoint(self, pretrained_model_name: str,\n                              cache_dir: str, **kwargs):\n        r""""""Initialize model parameters from weights stored in the pre-trained\n        checkpoint.\n\n        Args:\n            pretrained_model_name (str): Name of the pre-trained model.\n            cache_dir (str): Path to the cache directory.\n            **kwargs: Additional arguments for specific models.\n        """"""\n        raise NotImplementedError\n'"
texar/torch/modules/pretrained/roberta.py,6,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils of RoBERTa Modules.\n""""""\n\nimport os\nfrom abc import ABC\nfrom typing import Any, Dict\n\nimport torch\n\nfrom texar.torch.modules.pretrained.pretrained_base import PretrainedMixin\n\n__all__ = [\n    ""PretrainedRoBERTaMixin"",\n]\n\n_ROBERTA_PATH = ""https://dl.fbaipublicfiles.com/fairseq/models/""\n\n\nclass PretrainedRoBERTaMixin(PretrainedMixin, ABC):\n    r""""""A mixin class to support loading pre-trained checkpoints for modules\n    that implement the RoBERTa model.\n\n    The RoBERTa model was proposed in (`Liu et al`. 2019)\n    `RoBERTa: A Robustly Optimized BERT Pretraining Approach`_.\n    As a variant of the standard BERT model, RoBERTa trains for more\n    iterations on more data with a larger batch size as well as other tweaks\n    in pre-training. Differing from the standard BERT, the RoBERTa model\n    does not use segmentation embedding. Available model names include:\n\n      * ``roberta-base``: RoBERTa using the BERT-base architecture,\n        125M parameters.\n      * ``roberta-large``: RoBERTa using the BERT-large architecture,\n        355M parameters.\n\n    We provide the following RoBERTa classes:\n\n      * :class:`~texar.torch.modules.RoBERTaEncoder` for text encoding.\n      * :class:`~texar.torch.modules.RoBERTaClassifier` for text\n        classification and sequence tagging.\n\n    .. _`RoBERTa: A Robustly Optimized BERT Pretraining Approach`:\n        https://arxiv.org/abs/1907.11692\n    """"""\n\n    _MODEL_NAME = ""RoBERTa""\n    _MODEL2URL = {\n        \'roberta-base\':\n            _ROBERTA_PATH + ""roberta.base.tar.gz"",\n        \'roberta-large\':\n            _ROBERTA_PATH + ""roberta.large.tar.gz"",\n    }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str) -> Dict[str, Any]:\n        info = list(os.walk(cache_dir))\n        root, _, files = info[0]\n        config_path = None\n\n        for file in files:\n            if file.endswith(\'model.pt\'):\n                config_path = os.path.join(root, file)\n                args = torch.load(config_path, map_location=""cpu"")[\'args\']\n                hidden_dim = args.encoder_embed_dim\n                vocab_size = 50265\n                position_size = args.max_positions + 2\n                embedding_dropout = args.dropout\n                num_blocks = args.encoder_layers\n                num_heads = args.encoder_attention_heads\n                dropout_rate = args.attention_dropout\n                residual_dropout = args.dropout\n                intermediate_size = args.encoder_ffn_embed_dim\n                hidden_act = args.activation_fn\n\n        if config_path is None:\n            raise ValueError(f""Cannot find the config file in {cache_dir}"")\n\n        configs = {\n            \'hidden_size\': hidden_dim,\n            \'embed\': {\n                \'name\': \'word_embeddings\',\n                \'dim\': hidden_dim\n            },\n            \'vocab_size\': vocab_size,\n            \'position_embed\': {\n                \'name\': \'position_embeddings\',\n                \'dim\': hidden_dim\n            },\n            \'position_size\': position_size,\n            \'encoder\': {\n                \'name\': \'encoder\',\n                \'embedding_dropout\': embedding_dropout,\n                \'num_blocks\': num_blocks,\n                \'multihead_attention\': {\n                    \'use_bias\': True,\n                    \'num_units\': hidden_dim,\n                    \'num_heads\': num_heads,\n                    \'output_dim\': hidden_dim,\n                    \'dropout_rate\': dropout_rate,\n                    \'name\': \'self\'\n                },\n                \'residual_dropout\': residual_dropout,\n                \'dim\': hidden_dim,\n                \'eps\': 1e-12,\n                \'use_bert_config\': True,\n                \'poswise_feedforward\': {\n                    ""layers"": [{\n                        \'type\': \'Linear\',\n                        \'kwargs\': {\n                            \'in_features\': hidden_dim,\n                            \'out_features\': intermediate_size,\n                            \'bias\': True,\n                        }\n                    }, {\n                        \'type\': \'Bert\' + hidden_act.upper()\n                    }, {\n                        \'type\': \'Linear\',\n                        \'kwargs\': {\n                            \'in_features\': intermediate_size,\n                            \'out_features\': hidden_dim,\n                            \'bias\': True,\n                        }\n                    }],\n                },\n            }\n        }\n\n        return configs\n\n    def _init_from_checkpoint(self, pretrained_model_name: str,\n                              cache_dir: str, **kwargs):\n        global_tensor_map = {\n            \'decoder.sentence_encoder.embed_tokens.weight\':\n                \'word_embedder._embedding\',\n            \'decoder.sentence_encoder.embed_positions.weight\':\n                \'position_embedder._embedding\',\n            \'decoder.sentence_encoder.emb_layer_norm.weight\':\n                \'encoder.input_normalizer.weight\',\n            \'decoder.sentence_encoder.emb_layer_norm.bias\':\n                \'encoder.input_normalizer.bias\',\n        }\n\n        attention_tensor_map = {\n            \'final_layer_norm.weight\':\n                \'encoder.output_layer_norm.{}.weight\',\n            \'final_layer_norm.bias\':\n                \'encoder.output_layer_norm.{}.bias\',\n            \'fc1.weight\':\n                \'encoder.poswise_networks.{}._layers.0.weight\',\n            \'fc1.bias\':\n                \'encoder.poswise_networks.{}._layers.0.bias\',\n            \'fc2.weight\':\n                \'encoder.poswise_networks.{}._layers.2.weight\',\n            \'fc2.bias\':\n                \'encoder.poswise_networks.{}._layers.2.bias\',\n            \'self_attn_layer_norm.weight\':\n                \'encoder.poswise_layer_norm.{}.weight\',\n            \'self_attn_layer_norm.bias\':\n                \'encoder.poswise_layer_norm.{}.bias\',\n            \'self_attn.out_proj.weight\':\n                \'encoder.self_attns.{}.O_dense.weight\',\n            \'self_attn.out_proj.bias\':\n                \'encoder.self_attns.{}.O_dense.bias\',\n            \'self_attn.in_proj_weight\': [\n                \'encoder.self_attns.{}.Q_dense.weight\',\n                \'encoder.self_attns.{}.K_dense.weight\',\n                \'encoder.self_attns.{}.V_dense.weight\',\n            ],\n            \'self_attn.in_proj_bias\': [\n                \'encoder.self_attns.{}.Q_dense.bias\',\n                \'encoder.self_attns.{}.K_dense.bias\',\n                \'encoder.self_attns.{}.V_dense.bias\'\n            ],\n        }\n\n        checkpoint_path = os.path.abspath(os.path.join(cache_dir, \'model.pt\'))\n        device = next(self.parameters()).device\n        params = torch.load(checkpoint_path, map_location=device)[\'model\']\n\n        for name, tensor in params.items():\n            if name in global_tensor_map:\n                v_name = global_tensor_map[name]\n                pointer = self._name_to_variable(v_name)\n                assert pointer.shape == tensor.shape\n                pointer.data = tensor.data.type(pointer.dtype)\n            elif name.startswith(\'decoder.sentence_encoder.layers.\'):\n                name = name.lstrip(\'decoder.sentence_encoder.layers.\')\n                layer_num, layer_name = name.split(\'.\', 1)\n                if layer_name in attention_tensor_map:\n                    v_names = attention_tensor_map[layer_name]\n                    if isinstance(v_names, str):\n                        pointer = self._name_to_variable(\n                            v_names.format(layer_num))\n                        assert pointer.shape == tensor.shape\n                        pointer.data = tensor.data.type(pointer.dtype)\n                    else:\n                        # Q, K, V in self-attention\n                        tensors = torch.chunk(tensor, chunks=3, dim=0)\n                        for i in range(3):\n                            pointer = self._name_to_variable(\n                                v_names[i].format(layer_num))\n                            assert pointer.shape == tensors[i].shape\n                            pointer.data = tensors[i].data.type(pointer.dtype)\n                else:\n                    raise NameError(f""Layer name \'{layer_name}\' not found"")\n'"
texar/torch/modules/pretrained/t5.py,6,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils for T5 Modules\n""""""\n\nimport copy\nimport os\nfrom abc import ABC\nfrom typing import Any, Dict, List, Set\n\nimport numpy as np\nimport torch\n\nfrom texar.torch.modules.pretrained.pretrained_base import PretrainedMixin\nfrom texar.torch.modules.pretrained.t5_utils import read_t5_gin_config_file\n\n__all__ = [\n    ""PretrainedT5Mixin""\n]\n\n_T5_PATH = ""https://storage.googleapis.com/t5-data/pretrained_models/""\n_T5_VOCAB_PATH = ""https://storage.googleapis.com/t5-data/vocabs/cc_all.32000/""\n_CHECKPOINT_FILES_GEN_MAP = {  # stores a tuple of model_id and number of\n                               # partitions\n    \'small\': (1000000, 16),\n    \'base\': (999900, 16),\n    \'large\': (1000700, 8),\n    \'B\': (1000000, 64)\n}\n\n\ndef _generate_t5_file_list(ckpt_tuple: tuple) -> List[str]:\n    """""" Helper function to generate file list given a tuple of model_id and\n    partition size.\n\n    Args:\n        ckpt_tuple: A tuple of model_id and number of partitions\n\n    """"""\n    ckpt_id = ckpt_tuple[0]\n    ckpt_parts = ckpt_tuple[1]\n    return [\n        \'checkpoint\',\n        *[f\'model.ckpt-{ckpt_id}.data-{idx:05d}-of-{ckpt_parts:05d}\'\n          for idx in range(ckpt_parts)],\n        f\'model.ckpt-{ckpt_id}.index\',\n        f\'model.ckpt-{ckpt_id}.meta\',\n        \'operative_config.gin\']\n\n\nclass PretrainedT5Mixin(PretrainedMixin, ABC):\n    r""""""A mixin class to support loading pre-trained checkpoints for modules\n    that implement the T5 model.\n\n    The T5 model was proposed in\n    `Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer`_\n    by `Raffel et al.` from Google. It treats multiple NLP tasks in a similar\n    manner by encoding the different tasks as text directives in the input\n    stream. This enables a single model to be trained supervised on a wide\n    variety of NLP tasks. The T5 model examines factors relevant for leveraging\n    transfer learning at scale from pure unsupervised pre-training to\n    supervised tasks.\n\n    The available T5 models are as follows:\n\n      * ``T5-Small``: Small version of T5, 60 million parameters.\n      * ``T5-Base``: Base-line version of T5, 220 million parameters.\n      * ``T5-Large``: Large Version of T5, 770 million parameters.\n      * ``T5-3B``: A version of T5 with 3 billion parameters.\n      * ``T5-11B``: A version of T5 with 11 billion parameters.\n\n    We provide the following classes:\n\n      * :class:`~texar.torch.modules.T5Encoder` for loading weights for the\n        encoder stack.\n      * :class:`~texar.torch.modules.T5Decoder` for loading weights for the\n        decoding stack.\n      * :class:`~texar.torch.modules.T5EncoderDecoder` as a raw pre-trained\n        model.\n\n    .. _`Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer`:\n        https://arxiv.org/abs/1910.10683\n    """"""\n    _MODEL_NAME = ""T5""\n\n    _MODEL2URL = {\n        \'T5-Small\': [_T5_PATH + f""small/{file}""\n                     for file in _generate_t5_file_list(\n                      _CHECKPOINT_FILES_GEN_MAP[\'small\'])] +\n                    [_T5_VOCAB_PATH + \'sentencepiece.model\'],\n        \'T5-Base\': [_T5_PATH + f""base/{file}""\n                    for file in _generate_t5_file_list(\n                     _CHECKPOINT_FILES_GEN_MAP[\'base\'])] +\n                   [_T5_VOCAB_PATH + \'sentencepiece.model\'],\n        \'T5-Large\': [_T5_PATH + f""large/{file}""\n                     for file in _generate_t5_file_list(\n                      _CHECKPOINT_FILES_GEN_MAP[\'large\'])] +\n                    [_T5_VOCAB_PATH + \'sentencepiece.model\'],\n        \'T5-3B\': [_T5_PATH + f""3B/{file}""\n                  for file in _generate_t5_file_list(\n                   _CHECKPOINT_FILES_GEN_MAP[\'B\'])] + [_T5_VOCAB_PATH +\n                                                       \'sentencepiece.model\'],\n        \'T5-11B\': [_T5_PATH + f""11B/{file}""\n                   for file in _generate_t5_file_list(\n                    _CHECKPOINT_FILES_GEN_MAP[\'B\'])] +\n                  [_T5_VOCAB_PATH + \'sentencepiece.model\']\n    }\n\n    _MODEL2CKPT = {\n        \'T5-Small\': \'model.ckpt-1000000\',\n        \'T5-Base\': \'model.ckpt-999900\',\n        \'T5-Large\': \'model.ckpt-1000700\',\n        \'T5-3B\': \'model.ckpt-1000000\',\n        \'T5-11B\': \'model.ckpt-1000000\'\n    }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str) -> Dict[str, Any]:\n        info = list(os.walk(cache_dir))\n        root, _, files = info[0]\n        config_path = None\n        for file in files:\n            if file.endswith(\'operative_config.gin\'):\n                config_path = os.path.join(root, file)\n        if config_path is None:\n            raise ValueError(f""Cannot find the config file in {cache_dir}"")\n\n        gin_config = read_t5_gin_config_file(config_path)\n\n        hidden_dim = gin_config[\'d_model\']\n        vocab_size = 32128\n        eps = 1e-6\n        embedding_dropout = gin_config[\'dropout_rate\']\n        num_blocks = gin_config[\'num_layers\']\n        num_heads = gin_config[\'num_heads\']\n        num_units = gin_config[\'d_kv\'] * num_heads\n        dropout_rate = gin_config[\'dropout_rate\']\n        residual_dropout = gin_config[\'dropout_rate\']\n        intermediate_size = gin_config[\'d_ff\']\n        rel_attn_num_buckets = 32\n        use_bias = False\n\n        configs = {\n            \'hidden_size\': hidden_dim,\n            \'embed\': {\n                \'name\': \'word_embeddings\',\n                \'dim\': hidden_dim\n            },\n            \'vocab_size\': vocab_size,\n\n            \'encoder\': {\n                \'name\': \'encoder\',\n                \'embedding_dropout\': embedding_dropout,\n                \'num_blocks\': num_blocks,\n                \'multihead_attention\': {\n                    \'use_bias\': use_bias,\n                    \'num_units\': num_units,\n                    \'num_heads\': num_heads,\n                    \'output_dim\': hidden_dim,\n                    \'dropout_rate\': dropout_rate,\n                    \'name\': \'self\',\n                    \'is_decoder\': False,\n                    \'relative_attention_num_buckets\': rel_attn_num_buckets\n                },\n                \'eps\': eps,\n                \'residual_dropout\': residual_dropout,\n                \'dim\': hidden_dim,\n                \'poswise_feedforward\': {\n                    ""layers"": [{\n                        \'type\': \'Linear\',\n                        \'kwargs\': {\n                            \'in_features\': hidden_dim,\n                            \'out_features\': intermediate_size,\n                            \'bias\': use_bias,\n                        }\n                    }, {\n                        \'type\': ""ReLU""\n                    }, {\n                        \'type\': \'Linear\',\n                        \'kwargs\': {\n                            \'in_features\': intermediate_size,\n                            \'out_features\': hidden_dim,\n                            \'bias\': use_bias,\n                        }\n                    }],\n                },\n            },\n            \'decoder\': {\n                \'name\': \'decoder\',\n                \'embedding_dropout\': embedding_dropout,\n                \'num_blocks\': num_blocks,\n                \'multihead_attention\': {\n                    \'use_bias\': use_bias,\n                    \'num_units\': num_units,\n                    \'num_heads\': num_heads,\n                    \'output_dim\': hidden_dim,\n                    \'dropout_rate\': dropout_rate,\n                    \'name\': \'self\',\n                    \'is_decoder\': True,\n                    \'relative_attention_num_buckets\': rel_attn_num_buckets\n                },\n                \'eps\': eps,\n                \'residual_dropout\': residual_dropout,\n                \'dim\': hidden_dim,\n                \'poswise_feedforward\': {\n                    ""layers"": [{\n                        \'type\': \'Linear\',\n                        \'kwargs\': {\n                            \'in_features\': hidden_dim,\n                            \'out_features\': intermediate_size,\n                            \'bias\': use_bias,\n                        }\n                    }, {\n                        \'type\': ""ReLU""\n                    }, {\n                        \'type\': \'Linear\',\n                        \'kwargs\': {\n                            \'in_features\': intermediate_size,\n                            \'out_features\': hidden_dim,\n                            \'bias\': use_bias,\n                        }\n                    }],\n                },\n            }\n\n        }\n\n        return configs\n\n    def assign(self, from_array, to_param, transpose=False):\n        pointer = self._name_to_variable(to_param)\n        if transpose:\n            from_array = np.transpose(from_array)\n        assert pointer.shape == from_array.shape\n        pointer.data = torch.from_numpy(from_array.astype(np.float32))\n\n    def _init_from_checkpoint(self, pretrained_model_name: str,\n                              cache_dir: str, **kwargs):\n        try:\n            import tensorflow as tf\n        except ImportError:\n            print(""Loading TensorFlow models in PyTorch requires installing ""\n                  ""TensorFlow. Please see https://www.tensorflow.org/install/ ""\n                  ""for installation instructions."")\n            raise\n\n        tf_path = os.path.abspath(os.path.join(\n            cache_dir,\n            self._MODEL2CKPT[pretrained_model_name]))\n\n        # Load weights from TF model\n        init_vars = tf.train.list_variables(tf_path)\n\n        to_params: Set[str] = {x[0] for x in self.named_parameters()}\n        to_params.remove(  # Not used as duplicate weights stored\n            \'decoder.enc_dec_attns.0.relative_attention_bias.weight\')\n\n        tfnames, arrays = [], []\n        for name, _ in init_vars:\n            array = tf.train.load_variable(tf_path, name)\n            tfnames.append(name)\n            arrays.append(array.squeeze())\n\n        from_params = set(copy.deepcopy(tfnames))\n        global_tensor_map = {\n            \'shared/embedding\': \'word_embedder._embedding\'\n        }\n        self_attention_map = {\n            \'SelfAttention/k\': \'{}.self_attns.{}.K_dense.weight\',\n            \'SelfAttention/o\': \'{}.self_attns.{}.O_dense.weight\',\n            \'SelfAttention/q\': \'{}.self_attns.{}.Q_dense.weight\',\n            \'SelfAttention/v\': \'{}.self_attns.{}.V_dense.weight\',\n            \'SelfAttention/relative_attention_bias\':\n                \'{}.self_attns.{}.relative_attention_bias.weight\',\n            \'layer_norm/scale\': \'{}.self_attn_layer_norm.{}.w\'\n        }\n\n        enc_dec_attention_map = {\n            \'EncDecAttention/k\': \'{}.enc_dec_attns.{}.K_dense.weight\',\n            \'EncDecAttention/o\': \'{}.enc_dec_attns.{}.O_dense.weight\',\n            \'EncDecAttention/q\': \'{}.enc_dec_attns.{}.Q_dense.weight\',\n            \'EncDecAttention/v\': \'{}.enc_dec_attns.{}.V_dense.weight\',\n            \'layer_norm/scale\': \'{}.end_dec_attn_layer_norm.{}.w\'\n        }\n\n        drd_map = {\n            \'DenseReluDense/wi/kernel\':\n                \'{}.poswise_networks.{}._layers.0.weight\',\n            \'DenseReluDense/wo/kernel\':\n                \'{}.poswise_networks.{}._layers.2.weight\',\n            \'layer_norm/scale\': \'{}.poswise_layer_norm.{}.w\'\n        }\n\n        component_map = {  # For encoder/decoder level\n            \'final_layer_norm/scale\': \'{}.final_layer_norm.w\'\n        }\n\n        block_map = {\n            \'encoder0\': self_attention_map,\n            \'encoder1\': drd_map,\n            \'decoder0\': self_attention_map,\n            \'decoder1\': enc_dec_attention_map,\n            \'decoder2\': drd_map,\n        }\n\n        idx = 0\n\n        # Initialize this param separately\n        special_param_name = \\\n            \'decoder.enc_dec_attns.0.relative_attention_bias.weight\'\n        rab_pointer = self._name_to_variable(special_param_name)\n        rab_pointer.data.normal_(mean=0.0,\n                                 std=(self._hparams.hidden_size) ** -0.5)\n\n        for name, array in zip(tfnames, arrays):\n            if name.startswith(\'cls\') or name == \'global_step\' or \\\n                    name.endswith(\'adam_m\') or name.endswith(\'adam_v\')\\\n                    or \'_slot_\' in name:\n                # ignore those variables begin with cls\n                # ignore \'global_step\' variable\n                # ignore optimizer state variable\n                # ignore slot\n                from_params.remove(name)\n                continue\n\n            if name in global_tensor_map:\n                v_name = global_tensor_map[name]\n                self.assign(array, v_name)\n                idx += 1\n                from_params.remove(name)\n                to_params.remove(v_name)\n            else:\n                # e.g. decoder/block_000/layer_000/SelfAttention/k\n                tmp_name = name.split(\'/\')\n                submodule = tmp_name[0]  # encoder/decoder\n                if len(tmp_name) > 3:  # has block level data\n                    block_num = int(tmp_name[1][6:])\n                    layer_num = str(int(tmp_name[2][6:]))\n                    sublayer_name = ""/"".join(tmp_name[3:])\n                    # Block-wise params\n                    map_ = block_map[submodule + layer_num]\n                    if sublayer_name in map_:\n                        v_name = map_[sublayer_name].format(submodule,\n                                                            block_num)\n                        self.assign(array, v_name, True)\n                        idx += 1\n                        from_params.remove(name)\n                        to_params.remove(v_name)\n                else:\n                    # e.g. decoder/final_layer_norm/scale\n                    sublayer_name = ""/"".join(tmp_name[1:])\n                    if sublayer_name in component_map:\n                        v_name = component_map[sublayer_name].format(submodule)\n                        self.assign(array, v_name)\n                        idx += 1\n                        from_params.remove(name)\n                        to_params.remove(v_name)\n\n        if len(from_params) > 0:\n            print(f""WARNING: Certain weights from checkpoint are not loaded: ""\n                  f""{list(from_params)}"")\n\n        if len(to_params) > 0:\n            print(f""WARNING: Certain parameters are not initialized: ""\n                  f""{list(to_params)}"")\n'"
texar/torch/modules/pretrained/t5_utils.py,36,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport ast\nimport math\nfrom typing import Dict, Optional, Tuple\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch import nn\n\nfrom texar.torch.core import layers\nfrom texar.torch.module_base import ModuleBase\nfrom texar.torch.modules.encoders.multihead_attention import LayerCache\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""T5LayerNorm"",\n    ""MultiheadRPRAttention"",\n    ""read_t5_gin_config_file""\n]\n\nIMPORTANT_PARAMS = (\'d_ff\',\n                    \'d_kv\',\n                    \'d_model\',\n                    \'dropout\',\n                    \'num_heads\',\n                    \'num_layers\',\n                    \'inputs_length\'\n                    )\n\n\ndef read_t5_gin_config_file(config_file_path: str) -> Dict:\n    r""""""Simple helper function to read a gin file\n    and get hyperparameters for T5.\n\n    Args:\n        config_file_path: path of config.gin file as a string.\n\n    Returns:\n        A dictionary with important parameters for loading T5.\n\n    """"""\n    config = {}\n\n    with open(config_file_path, \'r\') as gin_file:\n        for line in gin_file:\n            if line.startswith(IMPORTANT_PARAMS):\n                assignment = line.strip().split()\n                assert len(assignment) == 3\n                arg_name, _, value = assignment\n                config[arg_name] = ast.literal_eval(value)\n\n    return config\n\n\nclass T5LayerNorm(nn.Module):\n    r"""""" Custom LayerNorm for T5 with no mean subtraction and no bias.\n    """"""\n\n    def __init__(self, input_size: int, eps: float = 1e-5):\n        super().__init__()\n\n        self.w = nn.Parameter(torch.ones(input_size))\n        self.eps = eps\n\n    def forward(self,  # type: ignore\n                x: torch.Tensor):\n        x = x / torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n        return self.w * x\n\n\nclass MultiheadRPRAttention(ModuleBase):\n    r""""""\n    A variation of MultiheadAttention introduced by\n\n    Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. ""Self-attention with\n    relative position representations"".  https://arxiv.org/pdf/1803.02155.pdf\n\n    wherein it uses an alternative means of encoding positional information\n    of an input sequence by learning an embedding which is stored in the\n    first layer of Encoder/Decoder Stack but is shared amongst all attention\n    layers in the stack.\n\n    Args:\n        input_size (int): Number of hidden dimension\n\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n        stores_relative_position (bool): If the instance stores the learned\n        realtive position embeddings.\n\n    .. document private functions\n    """"""\n\n    def __init__(self, input_size: int, hparams=None,\n                 stores_relative_position: bool = False):\n        super().__init__(hparams=hparams)\n\n        use_bias = self._hparams.use_bias\n        self.is_decoder = self._hparams.is_decoder\n        self.stores_rpr = stores_relative_position\n        self.relative_attention_num_buckets = \\\n            self._hparams.relative_attention_num_buckets\n\n        self.Q_dense = nn.Linear(input_size, self._hparams.num_units,\n                                 bias=use_bias)\n        self.K_dense = nn.Linear(input_size, self._hparams.num_units,\n                                 bias=use_bias)\n        self.V_dense = nn.Linear(input_size, self._hparams.num_units,\n                                 bias=use_bias)\n        self.O_dense = nn.Linear(self._hparams.num_units,\n                                 self._hparams.output_dim, bias=use_bias)\n\n        if self.stores_rpr:\n            self.relative_attention_bias = nn.Embedding(\n                self.relative_attention_num_buckets, self._hparams.num_heads)\n\n        if self._hparams.initializer:\n            # TODO(haoransh): we may define kernel_initializer and bias\n            #  initializer seperately\n            initialize = layers.get_initializer(self._hparams.initializer)\n            assert initialize is not None\n            for name, param in self.named_parameters():\n                if name.split(\'.\')[-1] == \'weight\':\n                    print(\'name:{}\'.format(name))\n                    initialize(param)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""initializer"": None,\n                \'num_heads\': 8,\n                \'output_dim\': 512,\n                \'num_units\': 512,\n                \'dropout_rate\': 0.1,\n                \'use_bias\': False,\n                ""name"": ""multihead_attention"",\n                ""is_decoder"": False,\n                ""relative_attention_num_buckets"": 32\n            }\n\n        Here:\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""num_heads""`: int\n            Number of heads for attention calculation.\n\n        `""output_dim""`: int\n            Output dimension of the returned tensor.\n\n        `""num_units""`: int\n            Hidden dimension of the unsplit attention space.\n            Should be divisible by `""num_heads""`.\n\n        `""dropout_rate""`: float\n            Dropout rate in the attention.\n\n        `""use_bias""`: bool\n            Use bias when projecting the key, value and query.\n\n        `""name""`: str\n            Name of the module.\n\n        `""is_decoder""`: bool\n            To pass in if the attention is for a encoder or decoder block.\n\n        `""name""`: relative_attention_num_buckets\n            If the Attention mechnanism needs to use relative positional\n            attention bias, then this hparam stores the relative attention\n            num buckets.\n        """"""\n        return {\n            \'initializer\': None,\n            \'num_heads\': 8,\n            \'output_dim\': 512,\n            \'num_units\': 512,\n            \'dropout_rate\': 0.1,\n            \'use_bias\': False,\n            \'name\': \'multihead_attention_rpr\',\n            \'is_decoder\': False,\n            \'relative_attention_num_buckets\': 32\n        }\n\n    @staticmethod\n    def _relative_position_bucket(relative_position,\n                                  bidirectional=True,\n                                  num_buckets=32,\n                                  max_distance=128):\n        """"""\n        Adapted from Mesh Tensorflow:\n        https://github.com/tensorflow/mesh/blob/master/\n        mesh_tensorflow/transformer/transformer_layers.py#L595\n\n        Translate relative position to a bucket number for relative attention.\n\n        The relative position is defined as memory_position - query_position,\n        i.e. the distance in tokens from the attending position to the\n        attended-to position.  If bidirectional=False, then positive relative\n        positions are invalid.\n\n        We use smaller buckets for small absolute relative_position and\n        larger buckets for larger absolute relative_positions.\n\n        All relative positions >=max_distance map to the same bucket.\n        All relative positions <=-max_distance map to the same bucket.\n\n        This should allow for more graceful generalization to longer\n        sequences than the model has been trained on.\n\n        Args:\n            relative_position: an int32 Tensor\n            bidirectional: a boolean - whether the attention is bidirectional\n            num_buckets: an integer\n            max_distance: an integer\n        Returns:\n            a Tensor with the same shape as relative_position, containing int32\n            values in the range [0, num_buckets)\n        """"""\n        ret = 0\n        n = -relative_position\n        if bidirectional:\n            num_buckets //= 2\n            # mtf.to_int32(mtf.less(n, 0)) * num_buckets\n            ret += (n < 0).to(torch.long) * num_buckets\n            n = torch.abs(n)\n        else:\n            n = torch.max(n, torch.zeros_like(n))\n        # now n is in the range [0, inf)\n\n        # half of the buckets are for exact increments in positions\n        max_exact = num_buckets // 2\n        is_small = (n < max_exact)\n\n        # The other half of the buckets are for logarithmically bigger bins\n        # in positions up to max_distance\n        val_if_large = max_exact + (\n                torch.log(n.float() / max_exact)\n                / math.log(max_distance / max_exact) * (\n                            num_buckets - max_exact)).to(torch.long)\n        val_if_large = torch.min(val_if_large,\n                                 torch.full_like(val_if_large, num_buckets - 1))\n\n        ret += torch.where(is_small, n, val_if_large)\n        return ret\n\n    def compute_bias(self, qlen, klen):\n        """""" Compute binned relative position bias """"""\n        context_position = torch.arange(qlen, dtype=torch.long)[:, None]\n        memory_position = torch.arange(klen, dtype=torch.long)[None, :]\n\n        relative_position = memory_position - context_position\n        #  [length_query, length_key]\n\n        rp_bucket = self._relative_position_bucket(\n            relative_position,\n            bidirectional=not self.is_decoder,\n            num_buckets=self.relative_attention_num_buckets)\n        # [length_query, length_key]\n\n        values = self.relative_attention_bias(rp_bucket)\n        # [length_query, length_key, num_heads]\n\n        values = values.permute([2, 0, 1]).unsqueeze(0)\n        return values  # [1, num_heads, length_query, length_key]\n\n    def forward(self,  # type: ignore\n                queries: torch.Tensor,\n                memory: torch.Tensor,\n                memory_attention_bias: torch.Tensor,\n                cache: Optional[LayerCache] = None,\n                position_bias: Optional[torch.Tensor] = None\n                ) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n        r""""""Encodes the inputs.\n\n        Args:\n            queries: A 3D tensor with shape of\n                ``[batch, length_query, depth_query]``.\n            memory: A 4D tensor with shape of\n                ``[batch, length_key, depth_key]``.\n            memory_attention_bias: A 4D tensor with shape of\n                ``[batch, length_key, num_units]``.\n            cache: Memory cache only when inferring the sentence from scratch.\n            position_bias: A 4D Tensor with shape of\n                ``[batch, num_heads, length_query, length_query)]``.\n\n        Returns:\n            A tuple of 2 tensors, first, a tensor of shape\n            ``[batch_size, max_time, dim]`` containing the encoded vectors\n            and second a tensor of shape\n            [batch, num_heads, length_query, length_query)] for sharing the\n            position bias\n        """"""\n        length_query = queries.size(1)\n        if memory is None:\n            length_key = length_query  # Self-Attention\n        else:\n            length_key = memory.size(1)\n\n        num_heads = self._hparams.num_heads\n        num_units = self._hparams.num_units\n\n        if num_units % num_heads != 0:\n            raise ValueError(\n                f""Value depth ({num_units}) must be divisible by ""\n                f""the number of attention heads ({num_heads})."")\n\n        def _update_and_return(layer: nn.Module, key: str):\n            if memory is None:\n                # Self Attention\n                out = layer(queries)\n\n                if cache is not None:\n                    # decoder self attention when dynamic decoding\n                    res: MaybeList[torch.Tensor] = cache[key]\n                    if isinstance(res, list):\n                        # inference-like decoding\n                        res.append(out.squeeze(1))\n                        out = torch.stack(res, dim=1)\n                    else:\n                        # normal decoding\n                        res = torch.cat([res, out], dim=1)\n                        out = res\n                    cache[key] = res\n\n            else:\n                # encoder decoder attention\n                if cache is not None:\n                    res: MaybeList[torch.Tensor] = cache[  # type: ignore\n                        key]  # type: ignore\n                    if isinstance(res, list):\n                        # inference-like decoding\n                        if len(res) == 0:\n                            out = layer(memory)\n                        else:\n                            out = torch.stack(res, dim=1)\n                    else:\n                        # normal decoding\n                        if res.size(1) == 0:\n                            out = layer(memory)\n                        else:\n                            out = res\n                else:\n                    out = layer(memory)\n\n            return out\n\n        Q = self.Q_dense(queries)\n        K = _update_and_return(self.K_dense, \'keys\')\n        V = _update_and_return(self.V_dense, \'values\')\n\n        Q_ = self._split_heads(Q)\n        K_ = self._split_heads(K)\n        V_ = self._split_heads(V)\n\n        # All of the above [batch_size, num_heads, seq_length, memory_depth]\n        # Q_ *= key_depth_per_head ** -0.5  # T5 does not scale\n\n        logits = torch.einsum(\'bnqd,bnkd->bnqk\', Q_, K_)  # type: ignore\n\n        if position_bias is None:\n            # Must compute bias using embedding stored.\n            # Check if self.stores_rpr is True\n            if not self.stores_rpr:\n                raise ValueError(""Layer must store embedding weights since""\n                                 ""relative bias not provided"")\n            position_bias = self.compute_bias(length_query, length_key)\n\n            if memory_attention_bias is not None:\n                memory_attention_bias = memory_attention_bias.to(\n                    device=logits.device)\n                position_bias = position_bias + memory_attention_bias\n\n        logits += position_bias\n        weights = torch.softmax(logits, dim=-1)\n        weights = F.dropout(weights, self._hparams.dropout_rate,\n                            self.training)\n        outputs = torch.matmul(weights, V_)\n\n        outputs = self._combine_heads(outputs)\n        outputs = self.O_dense(outputs)\n        # (batch_size, length_query, output_dim)\n\n        return outputs, position_bias\n\n    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n        r""""""Split channels (dimension 2) into multiple heads,\n        becomes dimension 1). Must ensure ``x.shape[-1]`` can be\n        divided by num_heads.\n        """"""\n        depth = x.size(-1)\n        split_x = torch.reshape(x, (\n            x.size(0), x.size(1),\n            self._hparams.num_heads, depth // self._hparams.num_heads))\n        return split_x.permute((0, 2, 1, 3))\n\n    def _combine_heads(self, x: torch.Tensor) -> torch.Tensor:\n        r""""""\n\n        Args:\n            x: A Tensor of shape ``[batch, num_heads, seq_len, dim]``\n        Returns:\n            A Tensor of shape ``[batch, seq_len, num_heads * dim]``\n        """"""\n        t = x.permute((0, 2, 1, 3))  # [batch, seq_len, num_heads, dim]\n        num_heads, dim = t.size()[-2:]\n        assert num_heads == self._hparams.num_heads\n        return torch.reshape(t, (t.size(0), t.size(1), num_heads * dim))\n\n    @property\n    def output_size(self):\n        r""""""The feature size of :meth:`forward` output.\n        """"""\n        return self._hparams.output_dim\n'"
texar/torch/modules/pretrained/xlnet.py,12,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils of XLNet Modules.\n""""""\n\nimport json\nimport os\nfrom abc import ABC\nfrom typing import Any, Callable, Dict, Optional, Union\n\nimport torch\nfrom torch import nn\n\nfrom texar.torch.modules.pretrained.pretrained_base import PretrainedMixin\nfrom texar.torch.modules.pretrained.xlnet_utils import (\n    PositionWiseFF, RelativeMultiheadAttention, init_weights)\n\n__all__ = [\n    ""PretrainedXLNetMixin"",\n]\n\n_XLNET_PATH = ""https://storage.googleapis.com/xlnet/released_models/""\n\n\nclass PretrainedXLNetMixin(PretrainedMixin, ABC):\n    r""""""A mixin class to support loading pre-trained checkpoints for modules\n    that implement the XLNet model.\n\n    The XLNet model was proposed in\n    `XLNet: Generalized Autoregressive Pretraining for Language Understanding`_\n    by `Yang et al.` It is based on the Transformer-XL model, pre-trained on a\n    large corpus using a language modeling objective that considers all\n    permutations of the input sentence.\n\n    The available XLNet models are as follows:\n\n      * ``xlnet-based-cased``: 12-layer, 768-hidden, 12-heads. This model is\n        trained on full data (different from the one in the paper).\n      * ``xlnet-large-cased``: 24-layer, 1024-hidden, 16-heads.\n\n    We provide the following XLNet classes:\n\n      * :class:`~texar.torch.modules.XLNetEncoder` for text encoding.\n      * :class:`~texar.torch.modules.XLNetDecoder` for text generation and\n        decoding.\n      * :class:`~texar.torch.modules.XLNetClassifier` for text classification\n        and sequence tagging.\n      * :class:`~texar.torch.modules.XLNetRegressor` for text regression.\n\n    .. _`XLNet: Generalized Autoregressive Pretraining for Language Understanding`:\n        http://arxiv.org/abs/1906.08237\n    """"""\n    _MODEL_NAME = ""XLNet""\n    _MODEL2URL = {\n        \'xlnet-base-cased\':\n            _XLNET_PATH + ""cased_L-12_H-768_A-12.zip"",\n        \'xlnet-large-cased\':\n            _XLNET_PATH + ""cased_L-24_H-1024_A-16.zip"",\n    }\n\n    def reset_parameters(self):\n        self.apply(init_weights)\n        if not self._hparams.untie_r:\n            nn.init.normal_(self.r_w_bias, 0.0, 0.02)\n            nn.init.normal_(self.r_r_bias, 0.0, 0.02)\n            if self._hparams.use_segments:\n                nn.init.normal_(self.r_s_bias, 0.0, 0.02)\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str) -> Dict[str, Any]:\n        info = list(os.walk(cache_dir))\n        root, _, files = info[0]\n        config_path = None\n        for file in files:\n            if file.endswith(\'config.json\'):\n                config_path = os.path.join(root, file)\n        if config_path is None:\n            raise ValueError(f""Cannot find the config file in {cache_dir}"")\n\n        with open(config_path) as f:\n            config_ckpt = json.loads(f.read())\n\n        configs = {\n            ""head_dim"": config_ckpt[""d_head""],\n            ""ffn_inner_dim"": config_ckpt[""d_inner""],\n            ""hidden_dim"": config_ckpt[""d_model""],\n            ""activation"": config_ckpt[""ff_activation""],\n            ""num_heads"": config_ckpt[""n_head""],\n            ""num_layers"": config_ckpt[""n_layer""],\n            ""vocab_size"": config_ckpt[""n_token""],\n            ""untie_r"": config_ckpt[""untie_r""]\n        }\n\n        return configs\n\n    def _init_from_checkpoint(self, pretrained_model_name: str,\n                              cache_dir: str, **kwargs):\n        # remember to call .contiguous after trans_fn\n        try:\n            import numpy as np\n            import tensorflow as tf\n        except ImportError:\n            print(""Loading TensorFlow models in PyTorch requires installing ""\n                  ""TensorFlow. Please see https://www.tensorflow.org/install/ ""\n                  ""for installation instructions."")\n            raise\n\n        ckpt = tf.train.load_checkpoint(\n            os.path.join(cache_dir, \'xlnet_model.ckpt\'))\n        from_params: Dict[str, np.ndarray] = {\n            key: ckpt.get_tensor(key)\n            for key in ckpt.get_variable_to_shape_map().keys()}\n        del from_params[""global_step""]  # useless variable\n        to_params: Dict[str, nn.Parameter] = dict(self.named_parameters())\n\n        def get_weight(name: str) -> torch.Tensor:\n            weight = from_params[""model/"" + name]\n            del from_params[""model/"" + name]\n            return torch.from_numpy(weight)\n\n        TransFn = Callable[[torch.Tensor], torch.Tensor]\n\n        def assign(param: nn.Parameter, weight: Union[str, torch.Tensor],\n                   trans_fn: Optional[TransFn] = None,\n                   allow_fail: bool = False):\n            param_key = next(k for k, v in to_params.items() if v is param)\n            # Delete regardless of whether weight exists.\n            del to_params[param_key]\n            if isinstance(weight, str):\n                try:\n                    weight = get_weight(weight)\n                except KeyError:\n                    if allow_fail:\n                        print(f""Weight {weight} not found in checkpoint"")\n                        return\n                    else:\n                        raise\n            if trans_fn is not None:\n                weight = trans_fn(weight).contiguous()\n            if param.size() != weight.size():\n                raise ValueError(f""Expected size {param.size()}, ""\n                                 f""actual size {weight.size()}"")\n            param.data = weight\n\n        def assign_linear(linear: nn.Linear, prefix: str):\n            trans_fn = lambda p: p.view(p.size(0), -1).t()\n            assign(linear.weight, prefix + ""kernel"", trans_fn)\n            if linear.bias is not None:\n                assign(linear.bias, prefix + ""bias"")\n\n        def assign_layer_norm(layer_norm: nn.LayerNorm, prefix: str):\n            assign(layer_norm.weight, prefix + ""LayerNorm/gamma"")\n            assign(layer_norm.bias, prefix + ""LayerNorm/beta"")\n\n        def load_xlnet_model(xlnet):\n            n_layers = len(xlnet.attn_layers)\n            for bias_name in [\'r_r_bias\', \'r_w_bias\', \'r_s_bias\']:\n                weight = get_weight(""transformer/"" + bias_name)\n                if xlnet.hparams.untie_r:\n                    for idx in range(n_layers):\n                        layer: RelativeMultiheadAttention\n                        layer = xlnet.attn_layers[idx]\n                        assign(getattr(layer, bias_name), weight[idx])\n                else:\n                    assign(getattr(xlnet, bias_name), weight)\n            assign(xlnet.word_embed.weight,\n                   ""transformer/word_embedding/lookup_table"")\n\n            for idx in range(n_layers):\n                layer: RelativeMultiheadAttention = xlnet.attn_layers[idx]\n                prefix = f""transformer/layer_{idx}/rel_attn/""\n                qkv_weights = [get_weight(prefix + f""{part}/kernel"")\n                               for part in ""qkv""]\n                assign(layer.head_projection.weight,\n                       torch.cat([\n                           p.view(p.size(0), -1) for p in qkv_weights\n                       ], dim=1).t())\n                assign_linear(layer.pos_projection, prefix + ""r/"")\n                assign(layer.output_projection.weight,  # DO NOT TRANSPOSE!!!!\n                       prefix + ""o/kernel"", lambda p: p.view(p.size(0), -1))\n                assign_layer_norm(layer.layer_norm, prefix)\n\n            for idx in range(n_layers):\n                layer: PositionWiseFF = xlnet.ff_layers[idx]\n                prefix = f""transformer/layer_{idx}/ff/""\n                for linear_idx in range(1, 2 + 1):\n                    linear_prefix = f""{prefix}layer_{linear_idx}/""\n                    linear_layer: nn.Linear = getattr(\n                        layer, f""linear{linear_idx}"")\n                    assign_linear(linear_layer, linear_prefix)\n                assign_layer_norm(layer.layer_norm, prefix)\n\n            seg_embeds = [\n                p.squeeze(0)\n                for p in torch.chunk(\n                    get_weight(""transformer/seg_embed""), n_layers, dim=0)]\n            for idx in range(n_layers):\n                assign(xlnet.attn_layers[idx].segment_embed, seg_embeds[idx])\n\n            if hasattr(xlnet, \'mask_emb\') and hasattr(xlnet, \'lm_bias\'):\n                assign(xlnet.mask_emb, ""transformer/mask_emb/mask_emb"")\n                assign(xlnet.lm_bias, ""lm_loss/bias"")\n\n        load_xlnet_model(self)\n\n        if len(from_params) > 0:\n            print(f""WARNING: Certain weights from checkpoint are not loaded: ""\n                  f""{list(from_params.keys())}"")\n\n        filtered_to_params = [k for k in to_params if k.startswith(""xlnet"")]\n        if len(filtered_to_params) > 0:\n            print(f""WARNING: Certain parameters are not initialized: ""\n                  f""{list(filtered_to_params)}"")\n'"
texar/torch/modules/pretrained/xlnet_utils.py,45,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModel Utils of XLNet Modules.\n\nAdapted from\nhttps://github.com/zihangdai/xlnet/blob/master/modeling.py\n""""""\nimport itertools\nfrom typing import Any, Dict, Iterable, List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom texar.torch.core import get_layer\nfrom texar.torch.module_base import ModuleBase\n\n__all__ = [\n    ""PositionWiseFF"",\n    ""RelativeMultiheadAttention"",\n    ""RelativePositionalEncoding"",\n    ""params_except_in"",\n    ""init_weights"",\n]\n\n\nclass PositionWiseFF(ModuleBase):\n\n    def __init__(self, hparams=None):\n        super().__init__(hparams=hparams)\n\n        hidden_dim = self._hparams.hidden_dim\n        ffn_inner_dim = self._hparams.ffn_inner_dim\n        dropout = self._hparams.dropout\n        activation = self._hparams.activation.capitalize()\n        if activation == \'Relu\':\n            activation = \'ReLU\'\n        elif activation == \'Gelu\':\n            activation = \'GPTGELU\'\n\n        self.linear1 = nn.Linear(hidden_dim, ffn_inner_dim)\n        self.activation_fn = get_layer({""type"": activation})\n        self.dropout = nn.Dropout(dropout, inplace=True)\n        self.linear2 = nn.Linear(ffn_inner_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-12)\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        return {\n            ""hidden_dim"": 768,\n            ""ffn_inner_dim"": 3072,\n            ""dropout"": 0.1,\n            ""activation"": \'relu\',\n        }\n\n    @property\n    def output_size(self):\n        r""""""The feature size of :meth:`forward` output.\n        """"""\n        return self._hparams.hidden_dim\n\n    def forward(self,  # type: ignore\n                input: torch.Tensor) -> torch.Tensor:\n        # position-wise feed-forward\n        output = self.linear1(input)\n        output = self.activation_fn(output)\n        output = self.dropout(output)\n        output = self.linear2(output)\n        output = self.dropout(output)\n        # residual + layer norm\n        output = self.layer_norm(input + output)\n        return output\n\n\nclass PositionalEmbedding(nn.Module):\n    inv_freq: torch.Tensor\n\n    def __init__(self, embed_dim: int):\n        super().__init__()\n\n        freq_seq = torch.arange(0.0, embed_dim, 2.0)\n        inv_freq = 1 / (10000 ** (freq_seq / embed_dim))\n        self.register_buffer(\'inv_freq\', inv_freq)\n\n    def forward(self,  # type: ignore\n                pos_seq: torch.Tensor) -> torch.Tensor:\n        sinusoid = torch.ger(pos_seq, self.inv_freq)\n        pos_embed = torch.cat([sinusoid.sin(), sinusoid.cos()], dim=-1)\n        return pos_embed\n\n\nclass RelativePositionalEncoding(ModuleBase):\n\n    def __init__(self, hparams=None):\n        super().__init__(hparams=hparams)\n        # self.sinusoid_embed = tx.modules.SinusoidsPositionEmbedder(\n        #     None, hparams={\n        #         ""dim"": self._hparams.dim,\n        #         ""cache_embeddings"": False,\n        #     })\n        self.sinusoid_embed = PositionalEmbedding(self._hparams.dim)\n\n    @staticmethod\n    def default_hparams():\n        return {\n            ""dim"": 768,\n            ""max_seq_len"": 512,\n        }\n\n    @property\n    def output_size(self):\n        r""""""The feature size of :meth:`forward` output.\n        """"""\n        return self._hparams.dim\n\n    def _create_positional_embedding(self,\n                                     start: int,\n                                     end: int,\n                                     step: int,\n                                     batch_size: int,\n                                     clamp_len: Optional[int] = None) \\\n            -> torch.Tensor:\n        embed_buffer = next(self.sinusoid_embed.buffers())\n        pos_seq = torch.arange(start, end, step, device=embed_buffer.device,\n                               dtype=embed_buffer.dtype)\n\n        if clamp_len is not None:\n            pos_seq = torch.clamp(pos_seq, -clamp_len, clamp_len)\n\n        pos_embed = self.sinusoid_embed(pos_seq)\n        pos_embed = pos_embed.unsqueeze(1).expand(-1, batch_size, -1)\n        return pos_embed\n\n    def forward(self,  # type: ignore\n                batch_size: int,\n                seq_len: int,\n                total_len: int,\n                clamp_len: Optional[int] = None,\n                attn_type: str = \'bi\',\n                bi_data: bool = True) -> torch.Tensor:\n        if attn_type == \'bi\':\n            start, end = total_len, -seq_len\n        elif attn_type == \'uni\':\n            start, end = total_len, -1\n        else:\n            raise ValueError(f""Unknown `attn_type` {attn_type}"")\n\n        if bi_data:\n            if batch_size % 2 != 0:\n                raise ValueError(""`batch_size` must be an even number"")\n            fwd_pos_embed = self._create_positional_embedding(\n                start, end, -1, batch_size // 2, clamp_len)\n            bwd_pos_embed = self._create_positional_embedding(\n                -start, -end, 1, batch_size // 2, clamp_len)\n            pos_embed = torch.cat([fwd_pos_embed, bwd_pos_embed], dim=1)\n        else:\n            pos_embed = self._create_positional_embedding(\n                start, end, -1, batch_size, clamp_len)\n        return pos_embed\n\n\nclass RelativeMultiheadAttention(ModuleBase):\n    def __init__(self,\n                 r_r_bias: Optional[nn.Parameter] = None,\n                 r_w_bias: Optional[nn.Parameter] = None,\n                 r_s_bias: Optional[nn.Parameter] = None,\n                 hparams=None):\n        super().__init__(hparams=hparams)\n\n        self.num_heads = self._hparams.num_heads\n        self.head_dim = self._hparams.head_dim\n        hidden_dim = self._hparams.hidden_dim\n\n        self.head_projection = nn.Linear(\n            hidden_dim, 3 * self.num_heads * self.head_dim, bias=False)\n        self.pos_projection = nn.Linear(\n            hidden_dim, self.num_heads * self.head_dim, bias=False)\n\n        self.dropout = nn.Dropout(self._hparams.dropout)\n        self.dropout_attn = nn.Dropout(self._hparams.attention_dropout)\n        self.output_projection = nn.Linear(\n            self.num_heads * self.head_dim, hidden_dim, bias=False)\n\n        bias_shape = (self.num_heads, self.head_dim)\n        self.untie_r = r_r_bias is None\n        self.r_r_bias = (r_r_bias if r_r_bias is not None\n                         else nn.Parameter(torch.Tensor(*bias_shape)))\n        self.r_w_bias = (r_w_bias if r_w_bias is not None\n                         else nn.Parameter(torch.Tensor(*bias_shape)))\n\n        if self._hparams.use_segments:\n            self.segment_embed = nn.Parameter(torch.Tensor(\n                2, self.num_heads, self.head_dim))\n            self.r_s_bias = (r_s_bias if r_s_bias is not None\n                             else nn.Parameter(torch.Tensor(*bias_shape)))\n\n        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-12)\n\n        self.scale = 1 / (self.head_dim ** 0.5)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        if self.untie_r:\n            nn.init.normal_(self.r_w_bias, 0.0, 0.02)\n            nn.init.normal_(self.r_r_bias, 0.0, 0.02)\n        if self._hparams.use_segments:\n            nn.init.normal_(self.segment_embed, 0.0, 0.02)\n            if self.untie_r:\n                nn.init.normal_(self.r_s_bias, 0.0, 0.02)\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        return {\n            ""num_heads"": 12,\n            ""hidden_dim"": 768,\n            ""head_dim"": 64,\n            ""dropout"": 0.1,\n            ""attention_dropout"": 0.1,\n            ""use_segments"": True,\n        }\n\n    @property\n    def output_size(self):\n        r""""""The feature size of :meth:`forward` output\n        :attr:`output_h`.\n        """"""\n        return self._hparams.hidden_dim\n\n    @staticmethod\n    def _rel_shift(x: torch.Tensor, klen: int) -> torch.Tensor:\n        shape = x.size()\n        x = x.view(shape[1], shape[0], *shape[2:])[1:]\n        x = x.view(shape[0], shape[1] - 1, *shape[2:])[:, :klen]\n        return x\n\n    def _compute_attention_score(self,\n                                 q_head: torch.Tensor,\n                                 k_head_h: torch.Tensor,\n                                 v_head_h: torch.Tensor,\n                                 k_head_r: torch.Tensor,\n                                 segment_mat: Optional[torch.Tensor] = None,\n                                 attn_mask: Optional[torch.Tensor] = None) \\\n            -> torch.Tensor:\n        # Content based attention score.\n        q_head_rw = q_head + self.r_w_bias\n        # attn_ac: (seq_len, tot_len, batch_size, n_head)\n        attn_ac = torch.einsum(\'ibnd,jbnd->ijbn\', [q_head_rw, k_head_h])\n\n        # Position based attention score.\n        q_head_rr = q_head + self.r_r_bias\n        # attn_bd: (seq_len, tot_len, batch_size, n_head)\n        attn_bd = torch.einsum(\'ibnd,jbnd->ijbn\', [q_head_rr, k_head_r])\n        attn_bd = self._rel_shift(attn_bd, klen=attn_ac.size(1))\n\n        # Segment based attention score.\n        if segment_mat is None:\n            attn_ef = 0\n        else:\n            q_head_rs = q_head + self.r_s_bias\n            attn_ef = torch.einsum(\n                \'ibnd,snd->ibns\', [q_head_rs, self.segment_embed])\n            attn_ef = torch.einsum(\'ijbs,ibns->ijbn\', [segment_mat, attn_ef])\n\n        # Merge attention scores and perform masking.\n        # attn_score: (seq_len, tot_len, batch_size, n_head)\n        attn_score = attn_ac + attn_bd + attn_ef\n        attn_score.mul_(self.scale)\n        if attn_mask is not None:\n            if attn_mask.dim() == 2:\n                attn_mask = attn_mask[None, :, :, None]\n            elif attn_mask.dim() == 3:\n                attn_mask = attn_mask[:, :, :, None]\n            attn_score = attn_score.float().masked_fill(\n                attn_mask, -1e30).type_as(attn_score)\n\n        # Compute attention probability.\n        # attn_prob: (seq_len, tot_len, batch_size, n_head)\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropout_attn(attn_prob)\n\n        # Compute attention vector.\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', [attn_prob, v_head_h])\n        return attn_vec.contiguous()\n\n    def _post_attention(self, attn_vec: torch.Tensor) -> torch.Tensor:\n        attn_vec = attn_vec.view(*attn_vec.size()[:2], -1)\n        attn_out = self.output_projection(attn_vec)\n        attn_out = self.dropout(attn_out)\n        return attn_out\n\n    def forward(self,  # type: ignore\n                states_h: torch.Tensor,\n                pos_embed: torch.Tensor,\n                states_g: Optional[torch.Tensor] = None,\n                segment_mat: Optional[torch.Tensor] = None,\n                attn_mask_h: Optional[torch.Tensor] = None,\n                attn_mask_g: Optional[torch.Tensor] = None,\n                target_mapping: Optional[torch.Tensor] = None,\n                memory: Optional[torch.Tensor] = None) \\\n            -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        seq_len, batch_size = states_h.size()[:2]\n        pos_len = pos_embed.size(0)\n\n        if memory is not None and memory.dim() > 1:\n            concat_input = torch.cat([memory, states_h], dim=0)\n        else:\n            concat_input = states_h\n\n        # Content heads.\n        heads = self.head_projection(concat_input)\n        q_head_h, k_head_h, v_head_h = torch.chunk(heads, 3, dim=-1)\n        q_head_h = q_head_h[-seq_len:]\n        tot_len = k_head_h.size(0)\n\n        q_head_h = q_head_h.view(\n            seq_len, batch_size, self.num_heads, self.head_dim)\n        k_head_h = k_head_h.view(\n            tot_len, batch_size, self.num_heads, self.head_dim)\n        v_head_h = v_head_h.view(\n            tot_len, batch_size, self.num_heads, self.head_dim)\n\n        # Positional heads.\n        k_head_r = self.pos_projection(pos_embed)\n        k_head_r = k_head_r.view(\n            pos_len, batch_size, self.num_heads, self.head_dim)\n\n        # Core attention ops.\n        attn_vec_h = self._compute_attention_score(\n            q_head_h, k_head_h, v_head_h, k_head_r,\n            segment_mat, attn_mask_h)\n\n        # Post attention processing.\n        attn_out_h = self._post_attention(attn_vec_h)\n        # residual + layer norm\n        output_h = self.layer_norm(states_h + attn_out_h)\n\n        output_g = None\n        if states_g is not None:\n            proj_dim = self.num_heads * self.head_dim\n            proj_weight = self.head_projection.weight[:proj_dim]\n            q_head_g = F.linear(states_g, proj_weight)\n            q_head_g = q_head_g.view(\n                q_head_g.size(0), batch_size, self.num_heads, self.head_dim)\n            if target_mapping is not None:\n                q_head_g = torch.einsum(\n                    \'mbnd,mlb->lbnd\', [q_head_g, target_mapping])\n            attn_vec_g = self._compute_attention_score(\n                q_head_g, k_head_h, v_head_h, k_head_r,\n                segment_mat, attn_mask_g)\n            if target_mapping is not None:\n                attn_vec_g = torch.einsum(\n                    \'lbnd,mlb->mbnd\', [attn_vec_g, target_mapping])\n            attn_out_g = self._post_attention(attn_vec_g)\n            output_g = self.layer_norm(states_g + attn_out_g)\n\n        return output_h, output_g\n\n\ndef params_except_in(module: nn.Module,\n                     except_names: List[str]) \\\n        -> Iterable[nn.Parameter]:\n    return itertools.chain.from_iterable(\n        child.parameters() for name, child in\n        module.named_children()\n        if name not in except_names)\n\n\ndef init_weights(module: nn.Module):\n    if isinstance(module, nn.Linear):\n        nn.init.normal_(module.weight, 0.0, 0.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Embedding):\n        nn.init.normal_(module.weight, 0.0, 0.02)\n'"
texar/torch/modules/regressors/__init__.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library regressors.\n""""""\n\nfrom texar.torch.modules.regressors.regressor_base import *\nfrom texar.torch.modules.regressors.xlnet_regressor import *\n'"
texar/torch/modules/regressors/regressor_base.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for regressors.\n""""""\nfrom abc import ABC\nfrom typing import Any, Dict\n\nfrom texar.torch.module_base import ModuleBase\n\n__all__ = [\n    ""RegressorBase"",\n]\n\n\nclass RegressorBase(ModuleBase, ABC):\n    r""""""Base class inherited by all regressor classes.\n    """"""\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n        """"""\n        return {\n            ""name"": ""regressor""\n        }\n'"
texar/torch/modules/regressors/xlnet_regressor.py,18,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nXLNet Regressors.\n""""""\n\nfrom typing import Any, Dict, Optional, Union\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom texar.torch.core.layers import get_initializer\nfrom texar.torch.hyperparams import HParams\nfrom texar.torch.modules.encoders.xlnet_encoder import XLNetEncoder\nfrom texar.torch.modules.regressors.regressor_base import RegressorBase\nfrom texar.torch.modules.pretrained.xlnet import PretrainedXLNetMixin\nfrom texar.torch.modules.pretrained.xlnet_utils import (\n    init_weights, params_except_in)\nfrom texar.torch.utils.utils import dict_fetch\n\n\n__all__ = [\n    ""XLNetRegressor"",\n]\n\n\nclass XLNetRegressor(RegressorBase, PretrainedXLNetMixin):\n    r""""""Regressor based on XLNet modules. Please see\n    :class:`~texar.torch.modules.PretrainedXLNetMixin` for a brief description\n    of XLNet.\n\n    Arguments are the same as in\n    :class:`~texar.torch.modules.XLNetEncoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``xlnet-based-cased``). Please refer to\n            :class:`~texar.torch.modules.PretrainedXLNetMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n\n        super().__init__(hparams=hparams)\n\n        # Create the underlying encoder\n        encoder_hparams = dict_fetch(hparams, XLNetEncoder.default_hparams())\n\n        self._encoder = XLNetEncoder(\n            pretrained_model_name=pretrained_model_name,\n            cache_dir=cache_dir,\n            hparams=encoder_hparams)\n\n        # TODO: The logic here is very similar to that in XLNetClassifier.\n        #  We need to reduce the code redundancy.\n        if self._hparams.use_projection:\n            if self._hparams.regr_strategy == \'all_time\':\n                self.projection = nn.Linear(\n                    self._encoder.output_size * self._hparams.max_seq_length,\n                    self._encoder.output_size * self._hparams.max_seq_length)\n            else:\n                self.projection = nn.Linear(self._encoder.output_size,\n                                            self._encoder.output_size)\n        self.dropout = nn.Dropout(self._hparams.dropout)\n\n        logit_kwargs = self._hparams.logit_layer_kwargs\n        if logit_kwargs is None:\n            logit_kwargs = {}\n        elif not isinstance(logit_kwargs, HParams):\n            raise ValueError(""hparams[\'logit_layer_kwargs\'] ""\n                             ""must be a dict."")\n        else:\n            logit_kwargs = logit_kwargs.todict()\n\n        if self._hparams.regr_strategy == \'all_time\':\n            self.hidden_to_logits = nn.Linear(\n                self._encoder.output_size * self._hparams.max_seq_length,\n                1, **logit_kwargs)\n        else:\n            self.hidden_to_logits = nn.Linear(\n                self._encoder.output_size, 1, **logit_kwargs)\n\n        if self._hparams.initializer:\n            initialize = get_initializer(self._hparams.initializer)\n            assert initialize is not None\n            if self._hparams.use_projection:\n                initialize(self.projection.weight)\n                initialize(self.projection.bias)\n            initialize(self.hidden_to_logits.weight)\n            if self.hidden_to_logits.bias:\n                initialize(self.hidden_to_logits.bias)\n        else:\n            if self._hparams.use_projection:\n                self.projection.apply(init_weights)\n            self.hidden_to_logits.apply(init_weights)\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in XLNetEncoder\n                ...\n                # (2) Additional hyperparameters\n                ""regr_strategy"": ""cls_time"",\n                ""use_projection"": True,\n                ""logit_layer_kwargs"": None,\n                ""name"": ""xlnet_regressor"",\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n           :class:`~texar.torch.modules.XLNetEncoder`.\n           See the :meth:`~texar.torch.modules.XLNetEncoder.default_hparams`.\n           An instance of XLNetEncoder is created for feature extraction.\n\n        2. Additional hyperparameters:\n\n            `""regr_strategy""`: str\n                The regression strategy, one of:\n\n                - **cls_time**: Sequence-level regression based on the\n                  output of the first time step (which is the `CLS` token).\n                  Each sequence has a prediction.\n                - **all_time**: Sequence-level regression based on\n                  the output of all time steps. Each sequence has a prediction.\n                - **time_wise**: Step-wise regression, i.e., make\n                  regression for each time step based on its output.\n\n            `""logit_layer_kwargs""`: dict\n                Keyword arguments for the logit :torch_nn:`Linear` layer\n                constructor. Ignored if no extra logit layer is appended.\n\n            `""use_projection""`: bool\n                If `True`, an additional :torch_nn:`Linear` layer is added after\n                the summary step.\n\n            `""name""`: str\n                Name of the regressor.\n        """"""\n\n        hparams = XLNetEncoder.default_hparams()\n        hparams.update(({\n            ""regr_strategy"": ""cls_time"",\n            ""use_projection"": True,\n            ""logit_layer_kwargs"": None,\n            ""name"": ""xlnet_regressor"",\n        }))\n        return hparams\n\n    def param_groups(self,\n                     lr: Optional[float] = None,\n                     lr_layer_scale: float = 1.0,\n                     decay_base_params: bool = False):\n        r""""""Create parameter groups for optimizers. When\n        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form\n        separate groups with different base learning rates.\n\n        The return value of this method can be used in the constructor of\n        optimizers, for example:\n\n        .. code-block:: python\n\n            model = XLNetRegressor(...)\n            param_groups = model.param_groups(lr=2e-5, lr_layer_scale=0.8)\n            optim = torch.optim.Adam(param_groups)\n\n        Args:\n            lr (float): The learning rate. Can be omitted if\n                :attr:`lr_layer_decay_rate` is 1.0.\n            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer\n                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.\n            decay_base_params (bool): If `True`, treat non-layer parameters\n                (e.g. embeddings) as if they\'re in layer 0. If `False`, these\n                parameters are not scaled.\n\n        Returns:\n            The parameter groups, used as the first argument for optimizers.\n        """"""\n\n        # TODO: Same logic in XLNetClassifier. Reduce code redundancy.\n\n        if lr_layer_scale != 1.0:\n            if lr is None:\n                raise ValueError(\n                    ""lr must be specified when lr_layer_decay_rate is not 1.0"")\n\n            fine_tune_group = {\n                ""params"": params_except_in(self, [""_encoder""]),\n                ""lr"": lr\n            }\n            param_groups = [fine_tune_group]\n            param_group = self._encoder.param_groups(lr, lr_layer_scale,\n                                                     decay_base_params)\n            param_groups.extend(param_group)\n            return param_groups\n        return self.parameters()\n\n    def forward(self,  # type: ignore\n                inputs: Union[torch.Tensor, torch.LongTensor],\n                segment_ids: Optional[torch.LongTensor] = None,\n                input_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        r""""""Feeds the inputs through the network and makes regression.\n\n        Args:\n            inputs: Either a **2D Tensor** of shape `[batch_size, max_time]`,\n                containing the ids of tokens in input sequences, or\n                a **3D Tensor** of shape `[batch_size, max_time, vocab_size]`,\n                containing soft token ids (i.e., weights or probabilities)\n                used to mix the embedding vectors.\n            segment_ids: Shape `[batch_size, max_time]`.\n            input_mask: Float tensor of shape `[batch_size, max_time]`. Note\n                that positions with value 1 are masked out.\n\n        Returns:\n            Regression predictions.\n\n            - If ``regr_strategy`` is ``cls_time`` or ``all_time``, predictions\n              have shape `[batch_size]`.\n\n            - If ``clas_strategy`` is ``time_wise``, predictions have shape\n              `[batch_size, max_time]`.\n        """"""\n        # output: [batch_size, seq_len, hidden_dim]\n        output, _ = self._encoder(inputs=inputs,\n                                  segment_ids=segment_ids,\n                                  input_mask=input_mask)\n\n        strategy = self._hparams.regr_strategy\n        if strategy == \'time_wise\':\n            summary = output\n        elif strategy == \'cls_time\':\n            summary = output[:, -1]\n        elif strategy == \'all_time\':\n            length_diff = self._hparams.max_seq_length - inputs.shape[1]\n            summary_input = F.pad(output, [0, 0, 0, length_diff, 0, 0])\n            summary_input_dim = (self._encoder.output_size *\n                                 self._hparams.max_seq_length)\n\n            summary = summary_input.contiguous().view(-1, summary_input_dim)\n        else:\n            raise ValueError(\'Unknown regression strategy: {}\'.format(\n                strategy))\n\n        if self._hparams.use_projection:\n            summary = torch.tanh(self.projection(summary))\n\n        summary = self.dropout(summary)\n\n        preds = self.hidden_to_logits(summary).squeeze(-1)\n\n        return preds\n\n    @property\n    def output_size(self) -> int:\n        r""""""The feature size of :meth:`forward` output. Since output size is\n        only determined by input, the feature size is equal to ``-1``.\n        """"""\n        return -1\n'"
texar/torch/run/metric/__init__.py,5,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nMetrics for the Executor module.\n""""""\n\nfrom texar.torch.run.metric.base_metric import *\nfrom texar.torch.run.metric.classification import *\nfrom texar.torch.run.metric.generation import *\nfrom texar.torch.run.metric.regression import *\nfrom texar.torch.run.metric.summary import *\n'"
texar/torch/run/metric/base_metric.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase classes for Executor metrics.\n""""""\n\nfrom abc import ABC, abstractmethod\nfrom typing import Generic, List, Optional, Sequence, TypeVar\n\n__all__ = [\n    ""Metric"",\n    ""SimpleMetric"",\n    ""StreamingMetric"",\n]\n\nInput = TypeVar(\'Input\')\nValue = TypeVar(\'Value\')\n\n\nclass Metric(Generic[Input, Value], ABC):\n    r""""""Base class of all metrics. You should not directly inherit this class,\n    but inherit from :class:`SimpleMetric` or :class:`StreamingMetric` instead.\n\n    Subclasses can override the class attributes to indicate their behaviors:\n\n    - :attr:`higher_is_better`: If `True`, higher (comparison using the greater\n      than operator ``>`` returns `True`) values are considered better metric\n      values. If `False`, lower values are considered better. Defaults to\n      `True`.\n    - :attr:`required_pred`: If `True`, predicted values are required to compute\n      the metric value. Defaults to `True`.\n    - :attr:`requires_label`: If `True`, labels are required to compute the\n      metric value. Defaults to `True`.\n\n    Keyword Args:\n        pred_name (str, optional): Name of the predicted value. This will be\n            used as the key to the dictionary returned by the model.\n        label_name (str, optional): Name of the label. This will be used as the\n            key to the batch object returned by the dataset. Defaults to\n            ``""label""``.\n        higher_is_better (bool, optional): If specified, the\n            :attr:`higher_is_better` attribute for the instance is overwritten\n            by the specified value.\n    """"""\n    higher_is_better: bool = True\n    requires_pred: bool = True\n    requires_label: bool = True\n\n    def __init__(self, *, pred_name: Optional[str],\n                 label_name: Optional[str] = ""label"",\n                 higher_is_better: Optional[bool] = None):\n        self.reset()\n        if self.requires_pred and pred_name is None:\n            raise ValueError(f""Metric {self.metric_name} requires a ""\n                             f""prediction name, but None is provided"")\n        if self.requires_label and label_name is None:\n            raise ValueError(f""Metric {self.metric_name} requires a ""\n                             f""label name, but None is provided"")\n        if higher_is_better is not None:\n            self.higher_is_better = higher_is_better\n        if not self.requires_pred:\n            pred_name = None\n        if not self.requires_label:\n            label_name = None\n        self._pred_name = pred_name\n        self._label_name = label_name\n\n    @property\n    def metric_name(self) -> str:\n        r""""""Name of the metric. By default, the class name is used.""""""\n        return self.__class__.__name__\n\n    @property\n    def pred_name(self) -> Optional[str]:\n        r""""""Name of the predicted value. This will be used as the key to the\n        dictionary returned by the model.\n        """"""\n        return self._pred_name\n\n    @property\n    def label_name(self) -> Optional[str]:\n        r""""""Name of the label (ground truth / gold value). This will be used as\n        the key to the batch object returned by the dataset.\n        """"""\n        return self._label_name\n\n    @abstractmethod\n    def reset(self) -> None:\n        r""""""Reset the internal state of the metric, and erase all previously\n        added data points.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def add(self, predicted: Sequence[Input], labels: Sequence[Input]) -> None:\n        r""""""Record a data batch in the metric.\n\n        Args:\n            predicted: The list of predicted values.\n            labels: The list of labels.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def value(self) -> Value:\n        r""""""Compute the metric value.\n\n        Returns:\n            The metric value.\n        """"""\n        raise NotImplementedError\n\n    def better(self, cur: Value, prev: Value) -> Optional[bool]:\n        r""""""Compare two metric values and return which is better.\n\n        Args:\n            cur: The ""current"" metric value.\n            prev: The ""previous"" metric value.\n\n        Returns:\n            Return value is either a `bool` or `None`.\n\n            - If `True`, the current metric value is considered better.\n            - If `False`, the previous metric value is considered better.\n            - If `None`, the two values are considered to be the same, or\n              uncomparable.\n        """"""\n        result = (True if cur > prev else  # type: ignore\n                  False if cur < prev else None)  # type: ignore\n        if not self.higher_is_better and result is not None:\n            result = not result\n        return result\n\n\nclass SimpleMetric(Metric[Input, Value], ABC):\n    r""""""Base class of simple metrics. Simple metrics are metrics that do not\n    support incremental computation. The value of the metric is computed only\n    after all data points have been added.\n\n    The default implementation of :meth:`add` simply stores the predicted values\n    and labels into lists.\n    """"""\n    labels: List[Input]\n    predicted: List[Input]\n    _cached_value: Optional[Value]\n\n    def reset(self) -> None:\n        self.labels = []\n        self.predicted = []\n        self._cached_value = None\n\n    def add(self, predicted: Sequence[Input], labels: Sequence[Input]):\n        if len(predicted) != len(labels):\n            raise ValueError(\n                ""Lists `predicted` and `labels` should have the same length"")\n        self.predicted.extend(predicted)\n        self.labels.extend(labels)\n        self._cached_value = None\n\n    def value(self):\n        if self._cached_value is not None:\n            return self._cached_value\n        self._cached_value = self._value()\n        return self._cached_value\n\n    def _value(self) -> Value:\n        r""""""Compute the metric value. This function is called in\n        :meth:`texar.torch.run.metric.SimpleMetric.value` and the output is\n        cached. This prevents recalculation of metrics which may be time\n        consuming.\n\n        Returns:\n            The metric value.\n        """"""\n        raise NotImplementedError\n\n\nclass StreamingMetric(Metric[Input, Value], ABC):\n    r""""""Base class of streaming metrics. Streaming metrics are metrics that\n    support incremental computation. The value of the metric may be queried\n    before all data points have been added, and the computation should not be\n    expensive.\n\n    The default implementation of :meth:`add` only keeps track of the number of\n    data points added. You should override this method.\n    """"""\n    count: int\n\n    def reset(self) -> None:\n        self.count = 0\n\n    def add(self, predicted: Sequence[Input], labels: Sequence[Input]) -> None:\n        if len(predicted) != len(labels):\n            raise ValueError(\n                ""Lists `predicted` and `labels` should have the same length"")\n        self.count += len(predicted)\n'"
texar/torch/run/metric/classification.py,6,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nExecutor metrics for classification tasks.\n""""""\n\nfrom abc import ABC\nfrom typing import Dict, List, Optional, Sequence, Tuple, TypeVar\n\nimport numpy as np\n\nfrom texar.torch.run.metric.base_metric import StreamingMetric\n\n__all__ = [\n    ""Accuracy"",\n    ""ConfusionMatrix"",\n    ""Precision"",\n    ""Recall"",\n    ""F1"",\n]\n\nInput = TypeVar(\'Input\')\nValue = TypeVar(\'Value\')\n\n\nclass Accuracy(StreamingMetric[Input, float]):\n    r""""""The accuracy metric for evaluation classification tasks. Accuracy is\n    defined as the ratio of correct (exactly matching) predictions out of all\n    predictions.\n\n    Accuracy is a :class:`~texar.torch.run.metric.StreamingMetric`, requires\n    both predicted values and labels. Accuracy values are :class:`float`\n    numbers between 0 and 1, with higher values being better.\n\n    Keyword Args:\n        pred_name (str): Name of the predicted value. This will be used as the\n            key to the dictionary returned by the model.\n        label_name (str): Name of the label. This will be used as the key to the\n            batch object returned by the dataset. Defaults to ``""label""``.\n    """"""\n    correct: int\n\n    def reset(self) -> None:\n        super().reset()\n        self.correct = 0\n\n    def add(self, predicted: Sequence[Input], labels: Sequence[Input]) -> None:\n        super().add(predicted, labels)\n        self.correct += sum(int(a == b) for a, b in zip(predicted, labels))\n\n    def value(self) -> float:\n        if self.count == 0:\n            return 0.0\n        return self.correct / self.count\n\n\nclass _ConfusionMatrix(StreamingMetric[Input, Value], ABC):\n    count: int\n    matrix: Optional[np.ndarray]  # matrix[pred][label]\n    pred_count: List[int]\n    label_count: List[int]\n    _class_id: Dict[Input, int]\n\n    def reset(self) -> None:\n        super().reset()\n        self.matrix = None\n        self.pred_count = []\n        self.label_count = []\n        self._class_id = {}\n\n    def _convert_ids(self, classes: Sequence[Input]) -> List[int]:\n        ids = []\n        cnt = 0\n        for klass in classes:\n            if klass not in self._class_id:\n                self._class_id[klass] = len(self._class_id)\n                cnt += 1\n            ids.append(self._class_id[klass])\n        if self.matrix is None:\n            self.matrix = np.zeros((cnt, cnt), dtype=np.int)\n        else:\n            self.matrix = np.pad(self.matrix, [(0, cnt), (0, cnt)],\n                                 ""constant"", constant_values=0)\n        self.pred_count.extend([0] * cnt)\n        self.label_count.extend([0] * cnt)\n        return ids\n\n    def add(self, predicted: Sequence[Input], labels: Sequence[Input]) -> None:\n        super().add(predicted, labels)\n        predicted = self._convert_ids(predicted)\n        labels = self._convert_ids(labels)\n        assert self.matrix is not None\n        for pred, label in zip(predicted, labels):\n            self.matrix[pred, label] += 1\n            self.pred_count[pred] += 1\n            self.label_count[label] += 1\n\n\nclass ConfusionMatrix(_ConfusionMatrix[Input, Optional[np.ndarray]]):\n    r""""""The confusion matrix is an evaluation metric for classification tasks.\n\n    Confusion matrix is a :class:`~texar.torch.run.metric.StreamingMetric`,\n    requires both predicted values and labels. Confusion matrix values are NumPy\n    arrays, with no clear definition of ""better"". Comparison of two confusion\n    matrices are not meaningful.\n\n    The value indexed at ``(i, j)`` of the confusion matrix is the number of\n    data points whose predicted label is `i` and whose ground truth label is\n    `j`. Labels are internally mapped to indices.\n\n    Keyword Args:\n        pred_name (str): Name of the predicted value. This will be used as the\n            key to the dictionary returned by the model.\n        label_name (str): Name of the label. This will be used as the key to the\n            batch object returned by the dataset. Defaults to ``""label""``.\n    """"""\n\n    def value(self) -> Optional[np.ndarray]:\n        return self.matrix\n\n    @property\n    def class_id(self):\n        r""""""Mapping of predicted values and labels to indices within the matrix.\n        """"""\n        return self._class_id\n\n    def better(self, cur: Value, prev: Value) -> Optional[bool]:\n        # Always return `None` to indicate values are uncomparable.\n        return None\n\n\nclass _MicroMacro(_ConfusionMatrix[Input, float], ABC):\n    _valid_modes = [\'binary\', \'micro\', \'macro\', \'weighted\']\n\n    def __init__(self, mode: str = \'binary\', pos_label: Optional[Input] = None,\n                 *, pred_name: str, label_name: str = ""label""):\n        super().__init__(pred_name=pred_name, label_name=label_name)\n        self.mode = mode\n        if self.mode not in self._valid_modes:\n            raise ValueError(f""Invalid mode {mode}. ""\n                             f""Supported modes are: {self._valid_modes}"")\n        if self.mode == \'binary\' and pos_label is None:\n            raise ValueError(""`pos_label` must not be none when `mode` is ""\n                             ""set to \'binary\'"")\n        if pos_label is not None:\n            self.pos_label = pos_label\n\n    def _safe_divide(self, numerator: np.ndarray, denominator: np.ndarray) \\\n            -> np.ndarray:\n        # Credit: sklearn.metrics.classification._prf_divide\n        if numerator.size == 1:\n            if denominator == 0.0:\n                return np.array(0.0)\n            return numerator / denominator\n\n        mask = denominator == 0.0\n        denominator = denominator.copy()\n        denominator[mask] = 1.0\n        value = numerator / denominator\n        return value\n\n    def _convert_value(self, value: np.ndarray) -> np.ndarray:\n        if self.mode == \'binary\':\n            label = self._class_id.get(self.pos_label, None)\n            if label is None:\n                return np.array(0)\n            return value[label]\n        if self.mode == \'micro\':\n            return value.sum()\n        return value\n\n    def _true_positive(self) -> np.ndarray:\n        assert self.matrix is not None\n        value = self.matrix.diagonal()\n        return self._convert_value(value)\n\n    def _true_negative(self) -> np.ndarray:\n        assert self.matrix is not None\n        value = (self.count\n                 - np.asarray(self.pred_count)\n                 - np.asarray(self.label_count)\n                 + self.matrix.diagonal())\n        return self._convert_value(value)\n\n    def _false_positive(self) -> np.ndarray:\n        assert self.matrix is not None\n        value = np.asarray(self.pred_count) - self.matrix.diagonal()\n        return self._convert_value(value)\n\n    def _false_negative(self) -> np.ndarray:\n        assert self.matrix is not None\n        value = np.asarray(self.label_count) - self.matrix.diagonal()\n        return self._convert_value(value)\n\n    def _value(self) -> Tuple[np.ndarray, np.ndarray]:\n        r""""""Return the numerator and denominator of the metric value.\n        """"""\n        raise NotImplementedError\n\n    def value(self) -> float:\n        if self.count == 0:\n            return 0.0\n        numerator, denominator = self._value()\n        value = self._safe_divide(numerator, denominator)\n        if self.mode == \'macro\':\n            value = value.sum() / len(self._class_id)\n        elif self.mode == \'weighted\':\n            value = (value * np.asarray(self.label_count)).sum() / self.count\n        return value.item()\n\n\nclass Precision(_MicroMacro[Input]):\n    r""""""The precision metric for evaluation classification tasks. Precision is\n    defined as the ratio of ``tp / (tp + fp)``, where ``tp`` is the number of\n    true positives and ``fp`` is the number of false positives.\n\n    Precision is a :class:`~texar.torch.run.metric.StreamingMetric`, requires\n    both predicted values and labels. Precision values are :class:`float`\n    numbers between 0 and 1, with higher values being better.\n\n    Args:\n        mode (str): The mode for computing averages across multiple labels.\n            Defaults to ``""binary""``. Available options include:\n\n            - ``""binary""``: Only report results for the class specified by\n              :attr:`pos_label`. This is only meaningful for binary\n              classification tasks.\n            - ``""micro""``: Return the precision value computed using globally\n              counted true positives and false positives.\n            - ``""macro""``: Return the unweighted average of precision values for\n              each label.\n            - ``""weighted""``: Return the average of precision values for each\n              label, weighted by the number of true instances for each label.\n        pos_label (str, optional): The label for the positive class. Only used\n            if :attr:`mode` is set to ``""binary""``.\n\n    Keyword Args:\n        pred_name (str): Name of the predicted value. This will be used as the\n            key to the dictionary returned by the model.\n        label_name (str): Name of the label. This will be used as the key to the\n            batch object returned by the dataset. Defaults to ``""label""``.\n    """"""\n\n    def _value(self) -> Tuple[np.ndarray, np.ndarray]:\n        return (self._true_positive(),\n                (self._true_positive() + self._false_positive()))\n\n\nclass Recall(_MicroMacro[Input]):\n    r""""""The recall metric for evaluation classification tasks. Precision is\n    defined as the ratio of ``tp / (tp + fn)``, where ``tp`` is the number of\n    true positives and ``fn`` is the number of false negatives.\n\n    Recall is a :class:`~texar.torch.run.metric.StreamingMetric`, requires both\n    predicted values and labels. Recall values are :class:`float` numbers\n    between 0 and 1, with higher values being better.\n\n    Args:\n        mode (str): The mode for computing averages across multiple labels.\n            Defaults to ``""binary""``. Available options include:\n\n            - ``""binary""``: Only report results for the class specified by\n              :attr:`pos_label`. This is only meaningful for binary\n              classification tasks.\n            - ``""micro""``: Return the recall value computed using globally\n              counted true positives and false negatives.\n            - ``""macro""``: Return the unweighted average of recall values for\n              each label.\n            - ``""weighted""``: Return the average of recall values for each\n              label, weighted by the number of true instances for each label.\n        pos_label (str, optional): The label for the positive class. Only used\n            if :attr:`mode` is set to ``""binary""``.\n\n    Keyword Args:\n        pred_name (str): Name of the predicted value. This will be used as the\n            key to the dictionary returned by the model.\n        label_name (str): Name of the label. This will be used as the key to the\n            batch object returned by the dataset. Defaults to ``""label""``.\n    """"""\n\n    def _value(self) -> Tuple[np.ndarray, np.ndarray]:\n        return (self._true_positive(),\n                (self._true_positive() + self._false_negative()))\n\n\nclass F1(Precision[Input], Recall[Input]):\n    r""""""The F1 metric for evaluation classification tasks. F1 is defined as the\n    harmonic mean of precision and recall.\n\n    F1 is a :class:`~texar.torch.run.metric.StreamingMetric`, requires both\n    predicted values and labels. F1 values are :class:`float` numbers between 0\n    and 1, with higher values being better.\n\n    Args:\n        mode (str): The mode for computing averages across multiple labels.\n            Defaults to ``""binary""``. Available options include:\n\n            - ``""binary""``: Only report results for the class specified by\n              :attr:`pos_label`. This is only meaningful for binary\n              classification tasks.\n            - ``""micro""``: Return the F1 value computed using globally counted\n              true positives, false positives, and false negatives.\n            - ``""macro""``: Return the unweighted average of F1 values for each\n              label.\n            - ``""weighted""``: Return the average of F1 values for each label,\n              weighted by the number of true instances for each label.\n        pos_label (str, optional): The label for the positive class. Only used\n            if :attr:`mode` is set to ``""binary""``.\n\n    Keyword Args:\n        pred_name (str): Name of the predicted value. This will be used as the\n            key to the dictionary returned by the model.\n        label_name (str): Name of the label. This will be used as the key to the\n            batch object returned by the dataset. Defaults to ``""label""``.\n    """"""\n\n    def _value(self) -> Tuple[np.ndarray, np.ndarray]:\n        # pylint: disable=protected-access\n        precision = self._safe_divide(*Precision._value(self))\n        recall = self._safe_divide(*Recall._value(self))\n        # pylint: enable=protected-access\n        return (2 * precision * recall,\n                precision + recall)\n'"
texar/torch/run/metric/generation.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nExecutor metrics for generation tasks.\n""""""\n\nimport collections\nimport math\nfrom typing import Counter, List, Sequence, Tuple\n\nfrom texar.torch.run.metric.base_metric import StreamingMetric\nfrom texar.torch.utils.types import MaybeList\n\n__all__ = [\n    ""BLEU"",\n]\n\n\ndef _get_ngrams(segment: MaybeList[str],\n                max_order: int) -> Counter[Tuple[str, ...]]:\n    r""""""Extracts all n-grams up to a given maximum order from an\n    input segment.\n\n    Args:\n        segment: text segment from which n-grams will be extracted.\n        max_order: maximum length in tokens of the n-grams returned\n            by this methods.\n\n    Returns:\n        The Counter containing all n-grams upto :attr:`max_order`\n        in segment with a count of how many times each n-gram occurred.\n    """"""\n    ngram_counts: Counter[Tuple[str, ...]] = collections.Counter()\n    for order in range(1, max_order + 1):\n        for i in range(0, len(segment) - order + 1):\n            ngram = tuple(segment[i:i + order])\n            ngram_counts[ngram] += 1\n    return ngram_counts\n\n\nclass BLEU(StreamingMetric[MaybeList[str], float]):\n    r""""""The BLEU metric for evaluating translation tasks. BLEU stands for\n    bilingual evaluation understudy, and measure the percentage of overlapping\n    n-grams between the translation (hypothesis) and references.\n\n    This metric also\n    supports Smooth BLEU, computed following the method outlined in the paper:\n\n        (Lin et al. 2004) ORANGE: a method for evaluating automatic evaluation\n        metrics for machine translation.\n        Chin-Yew Lin, Franz Josef Och. COLING 2004.\n\n    BLEU is a :class:`~texar.torch.run.metric.StreamingMetric`, requires both\n    predicted values and labels. BLEU values are :class:`float` numbers between\n    0 and 100, with higher values being better.\n\n    Args:\n        max_order (int): Maximum n-gram order to use when computing BLEU score.\n            Defaults to 4.\n        lowercase (bool): Whether to lowercase all text before computing. If\n            enabled, the metric is also known as ""uncased BLEU"". Defaults to\n            `False`.\n        smooth (bool): Whether or not to apply `(Lin et al. 2004)` smoothing.\n            Defaults to `False`.\n        brevity_penalty (bool): Whether to apply brevity penalty. Defaults to\n            `True`.\n\n    Keyword Args:\n        pred_name (str): Name of the predicted value. This will be used as the\n            key to the dictionary returned by the model.\n        label_name (str): Name of the label. This will be used as the key to the\n            batch object returned by the dataset.\n    """"""\n    reference_length: int\n    hypothesis_length: int\n    matches_by_order: List[int]\n    possible_matches_by_order: List[int]\n\n    def __init__(self, max_order: int = 4, lowercase: bool = False,\n                 smooth: bool = False, brevity_penalty: bool = True,\n                 *, pred_name: str, label_name: str):\n        self.max_order = max_order\n        self.lowercase = lowercase\n        self.smooth = smooth\n        self.brevity_penalty = brevity_penalty\n        super().__init__(pred_name=pred_name, label_name=label_name)\n\n    def reset(self) -> None:\n        self.reference_length = 0\n        self.hypothesis_length = 0\n        self.matches_by_order = [0] * self.max_order\n        self.possible_matches_by_order = [0] * self.max_order\n\n    def add(self, predicted: Sequence[MaybeList[str]],\n            labels: Sequence[MaybeList[str]]) -> None:\n        for (reference, hypothesis) in zip(labels, predicted):\n            if isinstance(reference, str):\n                reference = reference.split()\n            if self.lowercase:\n                reference = [x.lower() for x in reference]\n            reference_ngram_counts = _get_ngrams(reference, self.max_order)\n\n            if isinstance(hypothesis, str):\n                hypothesis = hypothesis.split()\n            if self.lowercase:\n                hypothesis = [x.lower() for x in hypothesis]\n            hypothesis_ngram_counts = _get_ngrams(hypothesis, self.max_order)\n\n            self.reference_length += len(reference)\n            self.hypothesis_length += len(hypothesis)\n\n            overlap = hypothesis_ngram_counts & reference_ngram_counts\n            for ngram in overlap:\n                self.matches_by_order[len(ngram) - 1] += overlap[ngram]\n            for order in range(self.max_order):\n                possible_matches = len(hypothesis) - order\n                self.possible_matches_by_order[order] += possible_matches\n\n    def value(self) -> float:\n        if self.reference_length == 0:\n            return 0.0\n        precisions = []\n        for i in range(self.max_order):\n            if self.smooth:\n                precision = ((self.matches_by_order[i] + 1.0) /\n                             (self.possible_matches_by_order[i] + 1.0))\n            elif self.possible_matches_by_order[i] > 0:\n                precision = (self.matches_by_order[i] /\n                             self.possible_matches_by_order[i])\n            else:\n                precision = 0.0\n            precisions.append(precision)\n\n        geo_mean = 0.0\n        if min(precisions) > 0:\n            p_log_sum = sum(math.log(p) for p in precisions) / self.max_order\n            geo_mean = math.exp(p_log_sum)\n\n        bp = 1.0\n        if self.brevity_penalty:\n            ratio = self.hypothesis_length / self.reference_length\n            if ratio > 1.0:\n                bp = 1.0\n            elif abs(ratio) < 1e-8:\n                bp = 0.0\n            else:\n                bp = math.exp(1 - 1.0 / ratio)\n\n        bleu = geo_mean * bp\n        return bleu * 100.0\n'"
texar/torch/run/metric/regression.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nExecutor metrics for regression tasks.\n""""""\n\nimport math\nfrom typing import Sequence\n\nfrom texar.torch.run.metric.base_metric import StreamingMetric\n\n__all__ = [\n    ""PearsonR"",\n    ""RMSE"",\n]\n\n\nclass PearsonR(StreamingMetric[float, float]):\n    r""""""The Pearson correlation coefficient (Pearson\'s r) metric for evaluation\n    regression tasks. Pearson\'s r is a measure of linear correlation between two\n    sets of variables. Pearson\'s r ranges between -1 and 1, with 1 indicating\n    total positive linear correlation, -1 indicating total negative linear\n    correlation, and 0 indication no linear correlation.\n\n    Pearson\'s r is a :class:`~texar.torch.run.metric.StreamingMetric`, requires\n    both predicted values and labels. Pearson\'s r values are :class:`float`\n    numbers between -1 and 1, with higher values being better.\n\n    Keyword Args:\n        pred_name (str): Name of the predicted value. This will be used as the\n            key to the dictionary returned by the model.\n        label_name (str): Name of the label. This will be used as the key to the\n            batch object returned by the dataset. Defaults to ``""label""``.\n    """"""\n    x_sum: float\n    x2_sum: float\n    y_sum: float\n    y2_sum: float\n    xy_sum: float\n\n    def reset(self) -> None:\n        super().reset()\n        self.x_sum = self.y_sum = 0.0\n        self.x2_sum = self.y2_sum = 0.0\n        self.xy_sum = 0.0\n\n    def add(self, xs: Sequence[float], ys: Sequence[float]):\n        super().add(xs, ys)\n        self.x_sum += sum(xs)\n        self.x2_sum += sum(x * x for x in xs)\n        self.y_sum += sum(ys)\n        self.y2_sum += sum(y * y for y in ys)\n        self.xy_sum += sum(x * y for x, y in zip(xs, ys))\n\n    def value(self) -> float:\n        if self.count == 0:\n            return 0.0\n        numerator = self.xy_sum - self.x_sum * self.y_sum / self.count\n        denominator_x = self.x2_sum - self.x_sum ** 2 / self.count\n        denominator_y = self.y2_sum - self.y_sum ** 2 / self.count\n        if denominator_x == 0.0 or denominator_y == 0.0:\n            return math.nan\n        return numerator / math.sqrt(denominator_x * denominator_y)\n\n\nclass RMSE(StreamingMetric[float, float]):\n    r""""""The root mean squared error (RMSE) metric for evaluation regression\n    tasks. RMSE is defined as the standard deviation of the residuals\n    (difference between predicted values and ground truth values).\n\n    RMSE is a :class:`~texar.torch.run.metric.StreamingMetric`, requires both\n    predicted values and labels. RMSE values are :class:`float` numbers with a\n    lower bound of 0. Lower values are better.\n\n    Keyword Args:\n        pred_name (str): Name of the predicted value. This will be used as the\n            key to the dictionary returned by the model.\n        label_name (str): Name of the label. This will be used as the key to the\n            batch object returned by the dataset. Defaults to ``""label""``.\n    """"""\n    higher_is_better = False\n\n    squared_sum: float\n\n    def reset(self) -> None:\n        super().reset()\n        self.squared_sum = 0.0\n\n    def add(self, predicted: Sequence[float], labels: Sequence[float]) -> None:\n        super().add(predicted, labels)\n        self.squared_sum += sum((x - y) ** 2 for x, y in zip(predicted, labels))\n\n    def value(self) -> float:\n        if self.count == 0:\n            return 0.0\n        return math.sqrt(self.squared_sum / self.count)\n'"
texar/torch/run/metric/summary.py,5,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nExecutor metrics for summaries.\n""""""\n\nfrom collections import deque\nfrom typing import Any, Deque, Optional, Sequence\nimport weakref\n\nimport numpy as np\nfrom torch.optim.optimizer import Optimizer\n\nfrom texar.torch.run.metric.base_metric import StreamingMetric\n\n__all__ = [\n    ""Average"",\n    ""AveragePerplexity"",\n    ""RunningAverage"",\n    ""LR"",\n]\n\n\nclass Average(StreamingMetric[float, float]):\n    r""""""The average of a specific predicted value.\n\n    Average is a :class:`~texar.torch.run.metric.StreamingMetric`, requires only\n    predicted values. Average values are unbounded :class:`float` numbers. By\n    default, lower values are better, but the behavior can be configured.\n\n    Keyword Args:\n        pred_name (str): Name of the predicted value. This will be used as the\n            key to the dictionary returned by the model. Defaults to ``""loss""``.\n        higher_is_better (bool, optional): If specified, the\n            :attr:`higher_is_better` attribute for the instance is overwritten\n            by the specified value. Defaults to `False`.\n    """"""\n    higher_is_better = False\n    requires_label = False\n\n    sum: float\n\n    def __init__(self, *, pred_name: str = ""loss"",\n                 higher_is_better: bool = False):\n        super().__init__(pred_name=pred_name, higher_is_better=higher_is_better)\n\n    def reset(self) -> None:\n        super().reset()\n        self.sum = 0.0\n\n    def add(self, predicted: Sequence[float], _) -> None:\n        self.count += len(predicted)\n        self.sum += sum(predicted)\n\n    def value(self) -> float:\n        if self.count == 0:\n            return 0.0\n        return self.sum / self.count\n\n\nclass AveragePerplexity(Average):\n    # TODO: Create a `WeightedAverage` class that takes `(value, weight)`\n    #   and subclass that instead.\n    higher_is_better = False\n\n    def add(self, predicted: Sequence[float], _) -> None:\n        super().add(np.exp(predicted), _)\n\n\nclass RunningAverage(StreamingMetric[float, float]):\n    r""""""The running average of a specific predicted value, i.e., the average\n    computed over the most recent :attr:`queue_size` values.\n\n    Running average is a :class:`~texar.torch.run.metric.StreamingMetric`,\n    requires only predicted values. Running average values are unbounded\n    :class:`float` numbers. By default, lower values are better, but the\n    behavior can be configured.\n\n    Keyword Args:\n        queue_size (int): Size of the queue to keep history values. The running\n            average is computed over the most recent :attr:`queue_size` values.\n        pred_name (str): Name of the predicted value. This will be used as the\n            key to the dictionary returned by the model. Defaults to ``""loss""``.\n        higher_is_better (bool, optional): If specified, the\n            :attr:`higher_is_better` attribute for the instance is overwritten\n            by the specified value.\n    """"""\n    higher_is_better = False\n    requires_label = False\n\n    history: Deque[float]\n    sum: float\n\n    def __init__(self, queue_size: int, *, pred_name: str = ""loss"",\n                 higher_is_better: bool = False):\n        super().__init__(pred_name=pred_name, higher_is_better=higher_is_better)\n        if not isinstance(queue_size, int) or queue_size <= 0:\n            raise ValueError(""\'queue_size\' must be a position integer"")\n        self.queue_size = queue_size\n\n    def reset(self) -> None:\n        super().reset()\n        self.sum = 0.0\n        self.history = deque()\n\n    def add(self, predicted: Sequence[float], _) -> None:\n        if len(predicted) >= self.queue_size:\n            self.history = deque(predicted[-self.queue_size:])\n            self.sum = sum(self.history)\n        else:\n            for _ in range(len(predicted) -\n                           (self.queue_size - len(self.history))):\n                self.sum -= self.history.popleft()\n            self.sum += sum(predicted)\n            self.history.extend(predicted)\n\n    def value(self) -> float:\n        if len(self.history) == 0:\n            return 0.0\n        return self.sum / len(self.history)\n\n\nclass LR(StreamingMetric[Any, float]):\n    r""""""The learning rate (LR) of the given optimizer. This is not exactly a\n    metric, but rather a convenience object to print learning rates in log.\n\n    LR is a :class:`~texar.torch.run.metric.StreamingMetric`, requires neither\n    predicted values nor labels. LR values are unbounded :class:`float` numbers,\n    with no clear definition of ""better"". Comparison of two learning rates are\n    not meaningful.\n\n    Keyword Args:\n        optimizer: The optimizer instance.\n        param_group (int, optional): Index of the parameter group to obtain the\n            learning rate from. Defaults to 0. You don\'t need to specify this if\n            the optimizer contains only one parameter group (e.g., constructed\n            using :python:`optim_class(model.parameters())`.\n    """"""\n\n    requires_pred = False\n    requires_label = False\n\n    def __init__(self, optimizer: Optimizer, param_group: int = 0):\n        super().__init__(pred_name=None)\n        self.optimizer = weakref.ref(optimizer)\n        self.group = param_group\n\n    def add(self, _, __):\n        pass\n\n    def value(self) -> float:\n        return self.optimizer().param_groups[self.group][\'lr\']  # type: ignore\n\n    def better(self, cur: float, prev: float) -> Optional[bool]:\n        # Always return `None` to indicate values are uncomparable.\n        return None\n\n    def __getstate__(self):\n        # There\'s no point in pickling an `LR` metric; just ignore it.\n        return None\n\n    def __getnewargs__(self):\n        # But when unpickling, we need to make sure we can construct something.\n        # This requires passing a dummy `optimizer` to which a weakref can be\n        # constructed. In this case, we use an arbitrary built-in class.\n        return (int,)\n'"
