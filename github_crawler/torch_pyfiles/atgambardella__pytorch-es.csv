file_path,api_count,code
envs.py,0,"b""# Taken from https://github.com/ikostrikov/pytorch-a3c\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nimport gym\nfrom gym.spaces.box import Box\nfrom universe import vectorized\nfrom universe.wrappers import Unvectorize, Vectorize\n\nimport cv2\n\n\n# Taken from https://github.com/openai/universe-starter-agent\ndef create_atari_env(env_id):\n    env = gym.make(env_id)\n    if len(env.observation_space.shape) > 1:\n        print('Preprocessing env')\n        env = Vectorize(env)\n        env = AtariRescale42x42(env)\n        env = NormalizedEnv(env)\n        env = Unvectorize(env)\n    else:\n        print('No preprocessing because env is too small')\n    return env\n\n\ndef _process_frame42(frame):\n    frame = frame[34:34 + 160, :160]\n    # Resize by half, then down to 42x42 (essentially mipmapping). If\n    # we resize directly we lose pixels that, when mapped to 42x42,\n    # aren't close enough to the pixel boundary.\n    frame = cv2.resize(frame, (80, 80))\n    frame = cv2.resize(frame, (42, 42))\n    frame = frame.mean(2)\n    frame = frame.astype(np.float32)\n    frame *= (1.0 / 255.0)\n    frame = np.reshape(frame, [1, 42, 42])\n    return frame\n\n\nclass AtariRescale42x42(vectorized.ObservationWrapper):\n\n    def __init__(self, env=None):\n        super(AtariRescale42x42, self).__init__(env)\n        self.observation_space = Box(0.0, 1.0, [1, 42, 42])\n\n    def _observation(self, observation_n):\n        return [_process_frame42(observation) for observation in observation_n]\n\n\nclass NormalizedEnv(vectorized.ObservationWrapper):\n\n    def __init__(self, env=None):\n        super(NormalizedEnv, self).__init__(env)\n        self.state_mean = 0\n        self.state_std = 0\n        self.alpha = 0.9999\n        self.max_episode_length = 0\n\n    def _observation(self, observation_n):\n        for observation in observation_n:\n            self.max_episode_length += 1\n            self.state_mean = self.state_mean * self.alpha + \\\n                observation.mean() * (1 - self.alpha)\n            self.state_std = self.state_std * self.alpha + \\\n                observation.std() * (1 - self.alpha)\n\n        denom = (1 - pow(self.alpha, self.max_episode_length))\n        unbiased_mean = self.state_mean / denom\n        unbiased_std = self.state_std / denom\n\n        return [(observation - unbiased_mean) / (unbiased_std + 1e-8)\n                for observation in observation_n]\n"""
main.py,1,"b'from __future__ import absolute_import, division, print_function\n\nimport os\nimport argparse\n\nimport torch\n\nfrom envs import create_atari_env\nfrom model import ES\nfrom train import train_loop, render_env\n\nparser = argparse.ArgumentParser(description=\'ES\')\nparser.add_argument(\'--env-name\', default=\'PongDeterministic-v4\',\n                    metavar=\'ENV\', help=\'environment\')\nparser.add_argument(\'--lr\', type=float, default=0.1, metavar=\'LR\',\n                    help=\'learning rate\')\nparser.add_argument(\'--lr-decay\', type=float, default=1, metavar=\'LRD\',\n                    help=\'learning rate decay\')\nparser.add_argument(\'--sigma\', type=float, default=0.05, metavar=\'SD\',\n                    help=\'noise standard deviation\')\nparser.add_argument(\'--useAdam\', action=\'store_true\',\n                    help=\'bool to determine if to use adam optimizer\')\nparser.add_argument(\'--n\', type=int, default=40, metavar=\'N\',\n                    help=\'batch size, must be even\')\nparser.add_argument(\'--max-episode-length\', type=int, default=100000,\n                    metavar=\'MEL\', help=\'maximum length of an episode\')\nparser.add_argument(\'--max-gradient-updates\', type=int, default=100000,\n                    metavar=\'MGU\', help=\'maximum number of updates\')\nparser.add_argument(\'--restore\', default=\'\', metavar=\'RES\',\n                    help=\'checkpoint from which to restore\')\nparser.add_argument(\'--small-net\', action=\'store_true\',\n                    help=\'Use simple MLP on CartPole\')\nparser.add_argument(\'--variable-ep-len\', action=\'store_true\',\n                    help=""Change max episode length during training"")\nparser.add_argument(\'--silent\', action=\'store_true\',\n                    help=\'Silence print statements during training\')\nparser.add_argument(\'--test\', action=\'store_true\',\n                    help=\'Just render the env, no training\')\n\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    assert args.n % 2 == 0\n    if args.small_net and args.env_name not in [\'CartPole-v0\', \'CartPole-v1\',\n                                                \'MountainCar-v0\']:\n        args.env_name = \'CartPole-v1\'\n        print(\'Switching env to CartPole\')\n\n    env = create_atari_env(args.env_name)\n    chkpt_dir = \'checkpoints/%s/\' % args.env_name\n    if not os.path.exists(chkpt_dir):\n        os.makedirs(chkpt_dir)\n    synced_model = ES(env.observation_space.shape[0],\n                      env.action_space, args.small_net)\n    for param in synced_model.parameters():\n        param.requires_grad = False\n    if args.restore:\n        state_dict = torch.load(args.restore)\n        synced_model.load_state_dict(state_dict)\n\n    if args.test:\n        render_env(args, synced_model, env)\n    else:\n        train_loop(args, synced_model, env, chkpt_dir)\n'"
model.py,5,"b'# Taken from https://github.com/ikostrikov/pytorch-a3c\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef normalized_columns_initializer(weights, std=1.0):\n    out = torch.randn(weights.size())\n    out *= std / torch.sqrt(out.pow(2).sum(1).expand_as(out))\n    return out\n\n\ndef weights_init(m):\n    """"""\n    Not actually using this but let\'s keep it here in case that changes\n    """"""\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = np.prod(weight_shape[1:4])\n        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n    elif classname.find(\'Linear\') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = weight_shape[1]\n        fan_out = weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n\ndef selu(x):\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n    return scale * F.elu(x, alpha)\n\nclass ES(torch.nn.Module):\n\n    def __init__(self, num_inputs, action_space, small_net=False):\n        """"""\n        Really I should be using inheritance for the small_net here\n        """"""\n        super(ES, self).__init__()\n        num_outputs = action_space.n\n        self.small_net = small_net\n        if self.small_net:\n            self.linear1 = nn.Linear(num_inputs, 64)\n            self.linear2 = nn.Linear(64, 64)\n            self.actor_linear = nn.Linear(64, num_outputs)\n        else:\n            self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n            self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n            self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n            self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n            self.lstm = nn.LSTMCell(32*3*3, 256)\n            self.actor_linear = nn.Linear(256, num_outputs)\n        self.train()\n\n\n\n    def forward(self, inputs):\n        if self.small_net:\n            x = selu(self.linear1(inputs))\n            x = selu(self.linear2(x))\n            return self.actor_linear(x)\n        else:\n            inputs, (hx, cx) = inputs\n            x = selu(self.conv1(inputs))\n            x = selu(self.conv2(x))\n            x = selu(self.conv3(x))\n            x = selu(self.conv4(x))\n            x = x.view(-1, 32*3*3)\n            hx, cx = self.lstm(x, (hx, cx))\n            x = hx\n            return self.actor_linear(x), (hx, cx)\n\n    def count_parameters(self):\n        count = 0\n        for param in self.parameters():\n            count += param.data.numpy().flatten().shape[0]\n        return count\n\n    def es_params(self):\n        """"""\n        The params that should be trained by ES (all of them)\n        """"""\n        return [(k, v) for k, v in zip(self.state_dict().keys(),\n                                       self.state_dict().values())]\n'"
train.py,18,"b'from __future__ import absolute_import, division, print_function\n\nimport os\nimport math\nimport numpy as np\n\nimport torch\nimport torch.legacy.optim as legacyOptim\n\nimport torch.nn.functional as F\nimport torch.multiprocessing as mp\nfrom torch.autograd import Variable\n\nfrom envs import create_atari_env\nfrom model import ES\n\nimport matplotlib.pyplot as plt\n\n\ndef do_rollouts(args, models, random_seeds, return_queue, env, are_negative):\n    """"""\n    For each model, do a rollout. Supports multiple models per thread but\n    don\'t do it -- it\'s inefficient (it\'s mostly a relic of when I would run\n    both a perturbation and its antithesis on the same thread).\n    """"""\n    all_returns = []\n    all_num_frames = []\n    for model in models:\n        if not args.small_net:\n            cx = Variable(torch.zeros(1, 256))\n            hx = Variable(torch.zeros(1, 256))\n        state = env.reset()\n        state = torch.from_numpy(state)\n        this_model_return = 0\n        this_model_num_frames = 0\n        # Rollout\n        for step in range(args.max_episode_length):\n            if args.small_net:\n                state = state.float()\n                state = state.view(1, env.observation_space.shape[0])\n                logit = model(Variable(state, volatile=True))\n            else:\n                logit, (hx, cx) = model(\n                    (Variable(state.unsqueeze(0), volatile=True),\n                     (hx, cx)))\n\n            prob = F.softmax(logit)\n            action = prob.max(1)[1].data.numpy()\n            state, reward, done, _ = env.step(action[0])\n            this_model_return += reward\n            this_model_num_frames += 1\n            if done:\n                break\n            state = torch.from_numpy(state)\n        all_returns.append(this_model_return)\n        all_num_frames.append(this_model_num_frames)\n    return_queue.put((random_seeds, all_returns, all_num_frames, are_negative))\n\n\ndef perturb_model(args, model, random_seed, env):\n    """"""\n    Modifies the given model with a pertubation of its parameters,\n    as well as the negative perturbation, and returns both perturbed\n    models.\n    """"""\n    new_model = ES(env.observation_space.shape[0],\n                   env.action_space, args.small_net)\n    anti_model = ES(env.observation_space.shape[0],\n                    env.action_space, args.small_net)\n    new_model.load_state_dict(model.state_dict())\n    anti_model.load_state_dict(model.state_dict())\n    np.random.seed(random_seed)\n    for (k, v), (anti_k, anti_v) in zip(new_model.es_params(),\n                                        anti_model.es_params()):\n        eps = np.random.normal(0, 1, v.size())\n        v += torch.from_numpy(args.sigma*eps).float()\n        anti_v += torch.from_numpy(args.sigma*-eps).float()\n    return [new_model, anti_model]\n\noptimConfig = []\naverageReward = []\nmaxReward = []\nminReward = []\nepisodeCounter = []\n\ndef gradient_update(args, synced_model, returns, random_seeds, neg_list,\n                    num_eps, num_frames, chkpt_dir, unperturbed_results):\n    def fitness_shaping(returns):\n        """"""\n        A rank transformation on the rewards, which reduces the chances\n        of falling into local optima early in training.\n        """"""\n        sorted_returns_backwards = sorted(returns)[::-1]\n        lamb = len(returns)\n        shaped_returns = []\n        denom = sum([max(0, math.log(lamb/2 + 1, 2) -\n                         math.log(sorted_returns_backwards.index(r) + 1, 2))\n                     for r in returns])\n        for r in returns:\n            num = max(0, math.log(lamb/2 + 1, 2) -\n                      math.log(sorted_returns_backwards.index(r) + 1, 2))\n            shaped_returns.append(num/denom + 1/lamb)\n        return shaped_returns\n\n    def unperturbed_rank(returns, unperturbed_results):\n        nth_place = 1\n        for r in returns:\n            if r > unperturbed_results:\n                nth_place += 1\n        rank_diag = (\'%d out of %d (1 means gradient \'\n                     \'is uninformative)\' % (nth_place,\n                                             len(returns) + 1))\n        return rank_diag, nth_place\n\n    batch_size = len(returns)\n    assert batch_size == args.n\n    assert len(random_seeds) == batch_size\n    shaped_returns = fitness_shaping(returns)\n    rank_diag, rank = unperturbed_rank(returns, unperturbed_results)\n    if not args.silent:\n        print(\'Episode num: %d\\n\'\n              \'Average reward: %f\\n\'\n              \'Variance in rewards: %f\\n\'\n              \'Max reward: %f\\n\'\n              \'Min reward: %f\\n\'\n              \'Batch size: %d\\n\'\n              \'Max episode length: %d\\n\'\n              \'Sigma: %f\\n\'\n              \'Learning rate: %f\\n\'\n              \'Total num frames seen: %d\\n\'\n              \'Unperturbed reward: %f\\n\'\n              \'Unperturbed rank: %s\\n\' \n              \'Using Adam: %r\\n\\n\' %\n              (num_eps, np.mean(returns), np.var(returns), max(returns),\n               min(returns), batch_size,\n               args.max_episode_length, args.sigma, args.lr, num_frames,\n               unperturbed_results, rank_diag, args.useAdam))\n\n    averageReward.append(np.mean(returns))\n    episodeCounter.append(num_eps)\n    maxReward.append(max(returns))\n    minReward.append(min(returns))\n\n    pltAvg, = plt.plot(episodeCounter, averageReward, label=\'average\')\n    pltMax, = plt.plot(episodeCounter, maxReward,  label=\'max\')\n    pltMin, = plt.plot(episodeCounter, minReward,  label=\'min\')\n\n    plt.ylabel(\'rewards\')\n    plt.xlabel(\'episode num\')\n    plt.legend(handles=[pltAvg, pltMax,pltMin])\n\n    fig1 = plt.gcf()\n\n    plt.draw()\n    fig1.savefig(\'graph.png\', dpi=100)\n\n    # For each model, generate the same random numbers as we did\n    # before, and update parameters. We apply weight decay once.\n    if args.useAdam:\n        globalGrads = None\n        for i in range(args.n):\n            np.random.seed(random_seeds[i])\n            multiplier = -1 if neg_list[i] else 1\n            reward = shaped_returns[i]\n\n            localGrads = []\n            idx = 0\n            for k, v in synced_model.es_params():\n                eps = np.random.normal(0, 1, v.size())\n                grad = torch.from_numpy((args.n*args.sigma) * (reward*multiplier*eps)).float()\n\n                localGrads.append(grad)\n                \n                if len(optimConfig) == idx:\n                    optimConfig.append({ \'learningRate\' : args.lr  })\n                idx = idx + 1\n\n            if globalGrads == None:\n                globalGrads = localGrads\n            else:\n                for i in range(len(globalGrads)):\n                    globalGrads[i] = torch.add(globalGrads[i], localGrads[i])\n\n        idx = 0\n        for k, v in synced_model.es_params():\n            r, _ = legacyOptim.adam( lambda x:  (1, -globalGrads[idx]), v , optimConfig[idx])\n            v.copy_(r)\n            idx = idx + 1\n    else:\n        # For each model, generate the same random numbers as we did\n        # before, and update parameters. We apply weight decay once.\n        for i in range(args.n):\n            np.random.seed(random_seeds[i])\n            multiplier = -1 if neg_list[i] else 1\n            reward = shaped_returns[i]\n            for k, v in synced_model.es_params():\n                eps = np.random.normal(0, 1, v.size())\n                v += torch.from_numpy(args.lr/(args.n*args.sigma) *\n                                      (reward*multiplier*eps)).float()\n        args.lr *= args.lr_decay\n\n    torch.save(synced_model.state_dict(),\n               os.path.join(chkpt_dir, \'latest.pth\'))\n    return synced_model\n\n\ndef render_env(args, model, env):\n    while True:\n        state = env.reset()\n        state = torch.from_numpy(state)\n        this_model_return = 0\n        if not args.small_net:\n            cx = Variable(torch.zeros(1, 256))\n            hx = Variable(torch.zeros(1, 256))\n        done = False\n        while not done:\n            if args.small_net:\n                state = state.float()\n                state = state.view(1, env.observation_space.shape[0])\n                logit = model(Variable(state, volatile=True))\n            else:\n                logit, (hx, cx) = model(\n                    (Variable(state.unsqueeze(0), volatile=True),\n                     (hx, cx)))\n\n            prob = F.softmax(logit)\n            action = prob.max(1)[1].data.numpy()\n            state, reward, done, _ = env.step(action[0, 0])\n            env.render()\n            this_model_return += reward\n            state = torch.from_numpy(state)\n        print(\'Reward: %f\' % this_model_return)\n\n\ndef generate_seeds_and_models(args, synced_model, env):\n    """"""\n    Returns a seed and 2 perturbed models\n    """"""\n    np.random.seed()\n    random_seed = np.random.randint(2**30)\n    two_models = perturb_model(args, synced_model, random_seed, env)\n    return random_seed, two_models\n\n\ndef train_loop(args, synced_model, env, chkpt_dir):\n    def flatten(raw_results, index):\n        notflat_results = [result[index] for result in raw_results]\n        return [item for sublist in notflat_results for item in sublist]\n    print(""Num params in network %d"" % synced_model.count_parameters())\n    num_eps = 0\n    total_num_frames = 0\n    for _ in range(args.max_gradient_updates):\n        processes = []\n        return_queue = mp.Queue()\n        all_seeds, all_models = [], []\n        # Generate a perturbation and its antithesis\n        for j in range(int(args.n/2)):\n            random_seed, two_models = generate_seeds_and_models(args,\n                                                                synced_model,\n                                                                env)\n            # Add twice because we get two models with the same seed\n            all_seeds.append(random_seed)\n            all_seeds.append(random_seed)\n            all_models += two_models\n        assert len(all_seeds) == len(all_models)\n        # Keep track of which perturbations were positive and negative\n        # Start with negative true because pop() makes us go backwards\n        is_negative = True\n        # Add all peturbed models to the queue\n        while all_models:\n            perturbed_model = all_models.pop()\n            seed = all_seeds.pop()\n            p = mp.Process(target=do_rollouts, args=(args,\n                                                     [perturbed_model],\n                                                     [seed],\n                                                     return_queue,\n                                                     env,\n                                                     [is_negative]))\n            p.start()\n            processes.append(p)\n            is_negative = not is_negative\n        assert len(all_seeds) == 0\n        # Evaluate the unperturbed model as well\n        p = mp.Process(target=do_rollouts, args=(args, [synced_model],\n                                                 [\'dummy_seed\'],\n                                                 return_queue, env,\n                                                 [\'dummy_neg\']))\n        p.start()\n        processes.append(p)\n        for p in processes:\n            p.join()\n        raw_results = [return_queue.get() for p in processes]\n        seeds, results, num_frames, neg_list = [flatten(raw_results, index)\n                                                for index in [0, 1, 2, 3]]\n        # Separate the unperturbed results from the perturbed results\n        _ = unperturbed_index = seeds.index(\'dummy_seed\')\n        seeds.pop(unperturbed_index)\n        unperturbed_results = results.pop(unperturbed_index)\n        _ = num_frames.pop(unperturbed_index)\n        _ = neg_list.pop(unperturbed_index)\n\n        total_num_frames += sum(num_frames)\n        num_eps += len(results)\n        synced_model = gradient_update(args, synced_model, results, seeds,\n                                       neg_list, num_eps, total_num_frames,\n                                       chkpt_dir, unperturbed_results)\n        if args.variable_ep_len:\n            args.max_episode_length = int(2*sum(num_frames)/len(num_frames))\n'"
