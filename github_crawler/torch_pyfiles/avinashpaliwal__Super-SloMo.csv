file_path,api_count,code
dataloader.py,1,"b'import torch.utils.data as data\nfrom PIL import Image\nimport os\nimport os.path\nimport random\n\n\ndef _make_dataset(dir):\n    """"""\n    Creates a 2D list of all the frames in N clips containing\n    M frames each.\n\n    2D List Structure:\n    [[frame00, frame01,...frameM]  <-- clip0\n     [frame00, frame01,...frameM]  <-- clip0\n     :\n     [frame00, frame01,...frameM]] <-- clipN\n\n    Parameters\n    ----------\n        dir : string\n            root directory containing clips.\n\n    Returns\n    -------\n        list\n            2D list described above.\n    """"""\n\n\n    framesPath = []\n    # Find and loop over all the clips in root `dir`.\n    for index, folder in enumerate(os.listdir(dir)):\n        clipsFolderPath = os.path.join(dir, folder)\n        # Skip items which are not folders.\n        if not (os.path.isdir(clipsFolderPath)):\n            continue\n        framesPath.append([])\n        # Find and loop over all the frames inside the clip.\n        for image in sorted(os.listdir(clipsFolderPath)):\n            # Add path to list.\n            framesPath[index].append(os.path.join(clipsFolderPath, image))\n    return framesPath\n\ndef _make_video_dataset(dir):\n    """"""\n    Creates a 1D list of all the frames.\n\n    1D List Structure:\n    [frame0, frame1,...frameN]\n\n    Parameters\n    ----------\n        dir : string\n            root directory containing frames.\n\n    Returns\n    -------\n        list\n            1D list described above.\n    """"""\n\n\n    framesPath = []\n    # Find and loop over all the frames in root `dir`.\n    for image in sorted(os.listdir(dir)):\n        # Add path to list.\n        framesPath.append(os.path.join(dir, image))\n    return framesPath\n\ndef _pil_loader(path, cropArea=None, resizeDim=None, frameFlip=0):\n    """"""\n    Opens image at `path` using pil and applies data augmentation.\n\n    Parameters\n    ----------\n        path : string\n            path of the image.\n        cropArea : tuple, optional\n            coordinates for cropping image. Default: None\n        resizeDim : tuple, optional\n            dimensions for resizing image. Default: None\n        frameFlip : int, optional\n            Non zero to flip image horizontally. Default: 0\n\n    Returns\n    -------\n        list\n            2D list described above.\n    """"""\n\n\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        # Resize image if specified.\n        resized_img = img.resize(resizeDim, Image.ANTIALIAS) if (resizeDim != None) else img\n        # Crop image if crop area specified.\n        cropped_img = img.crop(cropArea) if (cropArea != None) else resized_img\n        # Flip image horizontally if specified.\n        flipped_img = cropped_img.transpose(Image.FLIP_LEFT_RIGHT) if frameFlip else cropped_img\n        return flipped_img.convert(\'RGB\')\n    \n    \nclass SuperSloMo(data.Dataset):\n    """"""\n    A dataloader for loading N samples arranged in this way:\n\n        |-- clip0\n            |-- frame00\n            |-- frame01\n            :\n            |-- frame11\n            |-- frame12\n        |-- clip1\n            |-- frame00\n            |-- frame01\n            :\n            |-- frame11\n            |-- frame12\n        :\n        :\n        |-- clipN\n            |-- frame00\n            |-- frame01\n            :\n            |-- frame11\n            |-- frame12\n\n    ...\n\n    Attributes\n    ----------\n    framesPath : list\n        List of frames\' path in the dataset.\n\n    Methods\n    -------\n    __getitem__(index)\n        Returns the sample corresponding to `index` from dataset.\n    __len__()\n        Returns the size of dataset. Invoked as len(datasetObj).\n    __repr__()\n        Returns printable representation of the dataset object.\n    """"""\n\n\n    def __init__(self, root, transform=None, dim=(640, 360), randomCropSize=(352, 352), train=True):\n        """"""\n        Parameters\n        ----------\n            root : string\n                Root directory path.\n            transform : callable, optional\n                A function/transform that takes in\n                a sample and returns a transformed version.\n                E.g, ``transforms.RandomCrop`` for images.\n            dim : tuple, optional\n                Dimensions of images in dataset. Default: (640, 360)\n            randomCropSize : tuple, optional\n                Dimensions of random crop to be applied. Default: (352, 352)\n            train : boolean, optional\n                Specifies if the dataset is for training or testing/validation.\n                `True` returns samples with data augmentation like random \n                flipping, random cropping, etc. while `False` returns the\n                samples without randomization. Default: True\n        """"""\n\n\n        # Populate the list with image paths for all the\n        # frame in `root`.\n        framesPath = _make_dataset(root)\n        # Raise error if no images found in root.\n        if len(framesPath) == 0:\n            raise(RuntimeError(""Found 0 files in subfolders of: "" + root + ""\\n""))\n                \n        self.randomCropSize = randomCropSize\n        self.cropX0         = dim[0] - randomCropSize[0]\n        self.cropY0         = dim[1] - randomCropSize[1]\n        self.root           = root\n        self.transform      = transform\n        self.train          = train\n\n        self.framesPath     = framesPath\n\n    def __getitem__(self, index):\n        """"""\n        Returns the sample corresponding to `index` from dataset.\n\n        The sample consists of two reference frames - I0 and I1 -\n        and a random frame chosen from the 7 intermediate frames\n        available between I0 and I1 along with it\'s relative index.\n\n        Parameters\n        ----------\n            index : int\n                Index\n\n        Returns\n        -------\n            tuple\n                (sample, returnIndex) where sample is \n                [I0, intermediate_frame, I1] and returnIndex is \n                the position of `random_intermediate_frame`. \n                e.g.- `returnIndex` of frame next to I0 would be 0 and\n                frame before I1 would be 6.\n        """"""\n\n\n        sample = []\n        \n        if (self.train):\n            ### Data Augmentation ###\n            # To select random 9 frames from 12 frames in a clip\n            firstFrame = random.randint(0, 3)\n            # Apply random crop on the 9 input frames\n            cropX = random.randint(0, self.cropX0)\n            cropY = random.randint(0, self.cropY0)\n            cropArea = (cropX, cropY, cropX + self.randomCropSize[0], cropY + self.randomCropSize[1])\n            # Random reverse frame\n            #frameRange = range(firstFrame, firstFrame + 9) if (random.randint(0, 1)) else range(firstFrame + 8, firstFrame - 1, -1)\n            IFrameIndex = random.randint(firstFrame + 1, firstFrame + 7)\n            if (random.randint(0, 1)):\n                frameRange = [firstFrame, IFrameIndex, firstFrame + 8]\n                returnIndex = IFrameIndex - firstFrame - 1\n            else:\n                frameRange = [firstFrame + 8, IFrameIndex, firstFrame]\n                returnIndex = firstFrame - IFrameIndex + 7\n            # Random flip frame\n            randomFrameFlip = random.randint(0, 1)\n        else:\n            # Fixed settings to return same samples every epoch.\n            # For validation/test sets.\n            firstFrame = 0\n            cropArea = (0, 0, self.randomCropSize[0], self.randomCropSize[1])\n            IFrameIndex = ((index) % 7  + 1)\n            returnIndex = IFrameIndex - 1\n            frameRange = [0, IFrameIndex, 8]\n            randomFrameFlip = 0\n        \n        # Loop over for all frames corresponding to the `index`.\n        for frameIndex in frameRange:\n            # Open image using pil and augment the image.\n            image = _pil_loader(self.framesPath[index][frameIndex], cropArea=cropArea, frameFlip=randomFrameFlip)\n            # Apply transformation if specified.\n            if self.transform is not None:\n                image = self.transform(image)\n            sample.append(image)\n            \n        return sample, returnIndex\n\n\n    def __len__(self):\n        """"""\n        Returns the size of dataset. Invoked as len(datasetObj).\n\n        Returns\n        -------\n            int\n                number of samples.\n        """"""\n\n\n        return len(self.framesPath)\n\n    def __repr__(self):\n        """"""\n        Returns printable representation of the dataset object.\n\n        Returns\n        -------\n            string\n                info.\n        """"""\n\n\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of datapoints: {}\\n\'.format(self.__len__())\n        fmt_str += \'    Root Location: {}\\n\'.format(self.root)\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str\n    \nclass UCI101Test(data.Dataset):\n    """"""\n    A dataloader for loading N samples arranged in this way:\n\n        |-- clip0\n            |-- frame00\n            |-- frame01\n            |-- frame02\n        |-- clip1\n            |-- frame00\n            |-- frame01\n            |-- frame02\n        :\n        :\n        |-- clipN\n            |-- frame00\n            |-- frame01\n            |-- frame02\n\n    ...\n\n    Attributes\n    ----------\n    framesPath : list\n        List of frames\' path in the dataset.\n\n    Methods\n    -------\n    __getitem__(index)\n        Returns the sample corresponding to `index` from dataset.\n    __len__()\n        Returns the size of dataset. Invoked as len(datasetObj).\n    __repr__()\n        Returns printable representation of the dataset object.\n    """"""\n\n\n    def __init__(self, root, transform=None):\n        """"""\n        Parameters\n        ----------\n            root : string\n                Root directory path.\n            transform : callable, optional\n                A function/transform that takes in\n                a sample and returns a transformed version.\n                E.g, ``transforms.RandomCrop`` for images.\n        """"""\n\n\n        # Populate the list with image paths for all the\n        # frame in `root`.\n        framesPath = _make_dataset(root)\n        # Raise error if no images found in root.\n        if len(framesPath) == 0:\n            raise(RuntimeError(""Found 0 files in subfolders of: "" + root + ""\\n""))\n\n        self.root           = root\n        self.framesPath     = framesPath\n        self.transform      = transform\n\n    def __getitem__(self, index):\n        """"""\n        Returns the sample corresponding to `index` from dataset.\n\n        The sample consists of two reference frames - I0 and I1 -\n        and a intermediate frame between I0 and I1.\n\n        Parameters\n        ----------\n            index : int\n                Index\n\n        Returns\n        -------\n            tuple\n                (sample, returnIndex) where sample is \n                [I0, intermediate_frame, I1] and returnIndex is \n                the position of `intermediate_frame`.\n                The returnIndex is always 3 and is being returned\n                to maintain compatibility with the `SuperSloMo`\n                dataloader where 3 corresponds to the middle frame.\n        """"""\n\n\n        sample = []\n        # Loop over for all frames corresponding to the `index`.\n        for framePath in self.framesPath[index]:\n            # Open image using pil.\n            image = _pil_loader(framePath)\n            # Apply transformation if specified.\n            if self.transform is not None:\n                image = self.transform(image)\n            sample.append(image)\n        return sample, 3\n\n\n    def __len__(self):\n        """"""\n        Returns the size of dataset. Invoked as len(datasetObj).\n\n        Returns\n        -------\n            int\n                number of samples.\n        """"""\n\n\n        return len(self.framesPath)\n\n    def __repr__(self):\n        """"""\n        Returns printable representation of the dataset object.\n\n        Returns\n        -------\n            string\n                info.\n        """"""\n\n\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of datapoints: {}\\n\'.format(self.__len__())\n        fmt_str += \'    Root Location: {}\\n\'.format(self.root)\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str\n\nclass Video(data.Dataset):\n    """"""\n    A dataloader for loading all video frames in a folder:\n\n        |-- frame0\n        |-- frame1\n        :\n        :\n        |-- frameN\n\n    ...\n\n    Attributes\n    ----------\n    framesPath : list\n        List of frames\' path in the dataset.\n    origDim : tuple\n        original dimensions of the video.\n    dim : tuple\n        resized dimensions of the video (for CNN).\n\n    Methods\n    -------\n    __getitem__(index)\n        Returns the sample corresponding to `index` from dataset.\n    __len__()\n        Returns the size of dataset. Invoked as len(datasetObj).\n    __repr__()\n        Returns printable representation of the dataset object.\n    """"""\n\n\n    def __init__(self, root, transform=None):\n        """"""\n        Parameters\n        ----------\n            root : string\n                Root directory path.\n            transform : callable, optional\n                A function/transform that takes in\n                a sample and returns a transformed version.\n                E.g, ``transforms.RandomCrop`` for images.\n        """"""\n\n\n        # Populate the list with image paths for all the\n        # frame in `root`.\n        framesPath = _make_video_dataset(root)\n\n        # Get dimensions of frames\n        frame        = _pil_loader(framesPath[0])\n        self.origDim = frame.size\n        self.dim     = int(self.origDim[0] / 32) * 32, int(self.origDim[1] / 32) * 32\n\n        # Raise error if no images found in root.\n        if len(framesPath) == 0:\n            raise(RuntimeError(""Found 0 files in: "" + root + ""\\n""))\n\n        self.root           = root\n        self.framesPath     = framesPath\n        self.transform      = transform\n\n    def __getitem__(self, index):\n        """"""\n        Returns the sample corresponding to `index` from dataset.\n\n        The sample consists of two reference frames - I0 and I1.\n\n        Parameters\n        ----------\n            index : int\n                Index\n\n        Returns\n        -------\n            list\n                sample is [I0, I1] where I0 is the frame with index\n                `index` and I1 is the next frame.\n        """"""\n\n\n        sample = []\n        # Loop over for all frames corresponding to the `index`.\n        for framePath in [self.framesPath[index], self.framesPath[index + 1]]:\n            # Open image using pil.\n            image = _pil_loader(framePath, resizeDim=self.dim)\n            # Apply transformation if specified.\n            if self.transform is not None:\n                image = self.transform(image)\n            sample.append(image)\n        return sample\n\n\n    def __len__(self):\n        """"""\n        Returns the size of dataset. Invoked as len(datasetObj).\n\n        Returns\n        -------\n            int\n                number of samples.\n        """"""\n\n\n        # Using `-1` so that dataloader accesses only upto\n        # frames [N-1, N] and not [N, N+1] which because frame\n        # N+1 doesn\'t exist.\n        return len(self.framesPath) - 1 \n\n    def __repr__(self):\n        """"""\n        Returns printable representation of the dataset object.\n\n        Returns\n        -------\n            string\n                info.\n        """"""\n\n\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of datapoints: {}\\n\'.format(self.__len__())\n        fmt_str += \'    Root Location: {}\\n\'.format(self.root)\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str'"
eval.py,9,"b'""""""\nConverts a Video to SuperSloMo version\n""""""\nfrom time import time\nimport click\nimport cv2\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport model\nfrom torchvision import transforms\nfrom torch.functional import F\n\n\ntorch.set_grad_enabled(False)\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\ntrans_forward = transforms.ToTensor()\ntrans_backward = transforms.ToPILImage()\nif device != ""cpu"":\n    mean = [0.429, 0.431, 0.397]\n    mea0 = [-m for m in mean]\n    std = [1] * 3\n    trans_forward = transforms.Compose([trans_forward, transforms.Normalize(mean=mean, std=std)])\n    trans_backward = transforms.Compose([transforms.Normalize(mean=mea0, std=std), trans_backward])\n\nflow = model.UNet(6, 4).to(device)\ninterp = model.UNet(20, 5).to(device)\nback_warp = None\n\n\ndef setup_back_warp(w, h):\n    global back_warp\n    with torch.set_grad_enabled(False):\n        back_warp = model.backWarp(w, h, device).to(device)\n\n\ndef load_models(checkpoint):\n    states = torch.load(checkpoint, map_location=\'cpu\')\n    interp.load_state_dict(states[\'state_dictAT\'])\n    flow.load_state_dict(states[\'state_dictFC\'])\n\n\ndef interpolate_batch(frames, factor):\n    frame0 = torch.stack(frames[:-1])\n    frame1 = torch.stack(frames[1:])\n\n    i0 = frame0.to(device)\n    i1 = frame1.to(device)\n    ix = torch.cat([i0, i1], dim=1)\n\n    flow_out = flow(ix)\n    f01 = flow_out[:, :2, :, :]\n    f10 = flow_out[:, 2:, :, :]\n\n    frame_buffer = []\n    for i in range(1, factor):\n        t = i / factor\n        temp = -t * (1 - t)\n        co_eff = [temp, t * t, (1 - t) * (1 - t), temp]\n\n        ft0 = co_eff[0] * f01 + co_eff[1] * f10\n        ft1 = co_eff[2] * f01 + co_eff[3] * f10\n\n        gi0ft0 = back_warp(i0, ft0)\n        gi1ft1 = back_warp(i1, ft1)\n\n        iy = torch.cat((i0, i1, f01, f10, ft1, ft0, gi1ft1, gi0ft0), dim=1)\n        io = interp(iy)\n\n        ft0f = io[:, :2, :, :] + ft0\n        ft1f = io[:, 2:4, :, :] + ft1\n        vt0 = F.sigmoid(io[:, 4:5, :, :])\n        vt1 = 1 - vt0\n\n        gi0ft0f = back_warp(i0, ft0f)\n        gi1ft1f = back_warp(i1, ft1f)\n\n        co_eff = [1 - t, t]\n\n        ft_p = (co_eff[0] * vt0 * gi0ft0f + co_eff[1] * vt1 * gi1ft1f) / \\\n               (co_eff[0] * vt0 + co_eff[1] * vt1)\n\n        frame_buffer.append(ft_p)\n\n    return frame_buffer\n\n\ndef load_batch(video_in, batch_size, batch, w, h):\n    if len(batch) > 0:\n        batch = [batch[-1]]\n\n    for i in range(batch_size):\n        ok, frame = video_in.read()\n        if not ok:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame = Image.fromarray(frame)\n        frame = frame.resize((w, h), Image.ANTIALIAS)\n        frame = frame.convert(\'RGB\')\n        frame = trans_forward(frame)\n        batch.append(frame)\n\n    return batch\n\n\ndef denorm_frame(frame, w0, h0):\n    frame = frame.cpu()\n    frame = trans_backward(frame)\n    frame = frame.resize((w0, h0), Image.BILINEAR)\n    frame = frame.convert(\'RGB\')\n    return np.array(frame)[:, :, ::-1].copy()\n\n\ndef convert_video(source, dest, factor, batch_size=10, output_format=\'mp4v\', output_fps=30):\n    vin = cv2.VideoCapture(source)\n    count = vin.get(cv2.CAP_PROP_FRAME_COUNT)\n    w0, h0 = int(vin.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vin.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    codec = cv2.VideoWriter_fourcc(*output_format)\n    vout = cv2.VideoWriter(dest, codec, float(output_fps), (w0, h0))\n\n    w, h = (w0 // 32) * 32, (h0 // 32) * 32\n    setup_back_warp(w, h)\n\n    done = 0\n    batch = []\n    while True:\n        batch = load_batch(vin, batch_size, batch, w, h)\n        if len(batch) == 1:\n            break\n        done += len(batch) - 1\n\n        intermediate_frames = interpolate_batch(batch, factor)\n        intermediate_frames = list(zip(*intermediate_frames))\n\n        for fid, iframe in enumerate(intermediate_frames):\n            vout.write(denorm_frame(batch[fid], w0, h0))\n            for frm in iframe:\n                vout.write(denorm_frame(frm, w0, h0))\n\n        try:\n            yield len(batch), done, count\n        except StopIteration:\n            break\n\n    vout.write(denorm_frame(batch[0], w0, h0))\n\n    vin.release()\n    vout.release()\n\n\n@click.command(\'Evaluate Model by converting a low-FPS video to high-fps\')\n@click.argument(\'input\')\n@click.option(\'--checkpoint\', help=\'Path to model checkpoint\')\n@click.option(\'--output\', help=\'Path to output file to save\')\n@click.option(\'--batch\', default=2, help=\'Number of frames to process in single forward pass\')\n@click.option(\'--scale\', default=4, help=\'Scale Factor of FPS\')\n@click.option(\'--fps\', default=30, help=\'FPS of output video\')\ndef main(input, checkpoint, output, batch, scale, fps):\n    avg = lambda x, n, x0: (x * n/(n+1) + x0 / (n+1), n+1)\n    load_models(checkpoint)\n    t0 = time()\n    n0 = 0\n    fpx = 0\n    for dl, fd, fc in convert_video(input, output, int(scale), int(batch), output_fps=int(fps)):\n        fpx, n0 = avg(fpx, n0, dl / (time() - t0))\n        prg = int(100*fd/fc)\n        eta = (fc - fd) / fpx\n        print(\'\\rDone: {:03d}% FPS: {:05.2f} ETA: {:.2f}s\'.format(prg, fpx, eta) + \' \'*5, end=\'\')\n        t0 = time()\n\n\nif __name__ == \'__main__\':\n    main()\n\n\n'"
model.py,10,"b'import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass down(nn.Module):\n    """"""\n    A class for creating neural network blocks containing layers:\n    \n    Average Pooling --> Convlution + Leaky ReLU --> Convolution + Leaky ReLU\n    \n    This is used in the UNet Class to create a UNet like NN architecture.\n\n    ...\n\n    Methods\n    -------\n    forward(x)\n        Returns output tensor after passing input `x` to the neural network\n        block.\n    """"""\n\n\n    def __init__(self, inChannels, outChannels, filterSize):\n        """"""\n        Parameters\n        ----------\n            inChannels : int\n                number of input channels for the first convolutional layer.\n            outChannels : int\n                number of output channels for the first convolutional layer.\n                This is also used as input and output channels for the\n                second convolutional layer.\n            filterSize : int\n                filter size for the convolution filter. input N would create\n                a N x N filter.\n        """"""\n\n\n        super(down, self).__init__()\n        # Initialize convolutional layers.\n        self.conv1 = nn.Conv2d(inChannels,  outChannels, filterSize, stride=1, padding=int((filterSize - 1) / 2))\n        self.conv2 = nn.Conv2d(outChannels, outChannels, filterSize, stride=1, padding=int((filterSize - 1) / 2))\n           \n    def forward(self, x):\n        """"""\n        Returns output tensor after passing input `x` to the neural network\n        block.\n\n        Parameters\n        ----------\n            x : tensor\n                input to the NN block.\n\n        Returns\n        -------\n            tensor\n                output of the NN block.\n        """"""\n\n\n        # Average pooling with kernel size 2 (2 x 2).\n        x = F.avg_pool2d(x, 2)\n        # Convolution + Leaky ReLU\n        x = F.leaky_relu(self.conv1(x), negative_slope = 0.1)\n        # Convolution + Leaky ReLU\n        x = F.leaky_relu(self.conv2(x), negative_slope = 0.1)\n        return x\n    \nclass up(nn.Module):\n    """"""\n    A class for creating neural network blocks containing layers:\n    \n    Bilinear interpolation --> Convlution + Leaky ReLU --> Convolution + Leaky ReLU\n    \n    This is used in the UNet Class to create a UNet like NN architecture.\n\n    ...\n\n    Methods\n    -------\n    forward(x, skpCn)\n        Returns output tensor after passing input `x` to the neural network\n        block.\n    """"""\n\n\n    def __init__(self, inChannels, outChannels):\n        """"""\n        Parameters\n        ----------\n            inChannels : int\n                number of input channels for the first convolutional layer.\n            outChannels : int\n                number of output channels for the first convolutional layer.\n                This is also used for setting input and output channels for\n                the second convolutional layer.\n        """"""\n\n        \n        super(up, self).__init__()\n        # Initialize convolutional layers.\n        self.conv1 = nn.Conv2d(inChannels,  outChannels, 3, stride=1, padding=1)\n        # (2 * outChannels) is used for accommodating skip connection.\n        self.conv2 = nn.Conv2d(2 * outChannels, outChannels, 3, stride=1, padding=1)\n           \n    def forward(self, x, skpCn):\n        """"""\n        Returns output tensor after passing input `x` to the neural network\n        block.\n\n        Parameters\n        ----------\n            x : tensor\n                input to the NN block.\n            skpCn : tensor\n                skip connection input to the NN block.\n\n        Returns\n        -------\n            tensor\n                output of the NN block.\n        """"""\n\n        # Bilinear interpolation with scaling 2.\n        x = F.interpolate(x, scale_factor=2, mode=\'bilinear\')\n        # Convolution + Leaky ReLU\n        x = F.leaky_relu(self.conv1(x), negative_slope = 0.1)\n        # Convolution + Leaky ReLU on (`x`, `skpCn`)\n        x = F.leaky_relu(self.conv2(torch.cat((x, skpCn), 1)), negative_slope = 0.1)\n        return x\n\n\n\nclass UNet(nn.Module):\n    """"""\n    A class for creating UNet like architecture as specified by the\n    Super SloMo paper.\n    \n    ...\n\n    Methods\n    -------\n    forward(x)\n        Returns output tensor after passing input `x` to the neural network\n        block.\n    """"""\n\n\n    def __init__(self, inChannels, outChannels):\n        """"""\n        Parameters\n        ----------\n            inChannels : int\n                number of input channels for the UNet.\n            outChannels : int\n                number of output channels for the UNet.\n        """"""\n\n        \n        super(UNet, self).__init__()\n        # Initialize neural network blocks.\n        self.conv1 = nn.Conv2d(inChannels, 32, 7, stride=1, padding=3)\n        self.conv2 = nn.Conv2d(32, 32, 7, stride=1, padding=3)\n        self.down1 = down(32, 64, 5)\n        self.down2 = down(64, 128, 3)\n        self.down3 = down(128, 256, 3)\n        self.down4 = down(256, 512, 3)\n        self.down5 = down(512, 512, 3)\n        self.up1   = up(512, 512)\n        self.up2   = up(512, 256)\n        self.up3   = up(256, 128)\n        self.up4   = up(128, 64)\n        self.up5   = up(64, 32)\n        self.conv3 = nn.Conv2d(32, outChannels, 3, stride=1, padding=1)\n        \n    def forward(self, x):\n        """"""\n        Returns output tensor after passing input `x` to the neural network.\n\n        Parameters\n        ----------\n            x : tensor\n                input to the UNet.\n\n        Returns\n        -------\n            tensor\n                output of the UNet.\n        """"""\n\n\n        x  = F.leaky_relu(self.conv1(x), negative_slope = 0.1)\n        s1 = F.leaky_relu(self.conv2(x), negative_slope = 0.1)\n        s2 = self.down1(s1)\n        s3 = self.down2(s2)\n        s4 = self.down3(s3)\n        s5 = self.down4(s4)\n        x  = self.down5(s5)\n        x  = self.up1(x, s5)\n        x  = self.up2(x, s4)\n        x  = self.up3(x, s3)\n        x  = self.up4(x, s2)\n        x  = self.up5(x, s1)\n        x  = F.leaky_relu(self.conv3(x), negative_slope = 0.1)\n        return x\n\n\nclass backWarp(nn.Module):\n    """"""\n    A class for creating a backwarping object.\n\n    This is used for backwarping to an image:\n\n    Given optical flow from frame I0 to I1 --> F_0_1 and frame I1, \n    it generates I0 <-- backwarp(F_0_1, I1).\n\n    ...\n\n    Methods\n    -------\n    forward(x)\n        Returns output tensor after passing input `img` and `flow` to the backwarping\n        block.\n    """"""\n\n\n    def __init__(self, W, H, device):\n        """"""\n        Parameters\n        ----------\n            W : int\n                width of the image.\n            H : int\n                height of the image.\n            device : device\n                computation device (cpu/cuda). \n        """"""\n\n\n        super(backWarp, self).__init__()\n        # create a grid\n        gridX, gridY = np.meshgrid(np.arange(W), np.arange(H))\n        self.W = W\n        self.H = H\n        self.gridX = torch.tensor(gridX, requires_grad=False, device=device)\n        self.gridY = torch.tensor(gridY, requires_grad=False, device=device)\n        \n    def forward(self, img, flow):\n        """"""\n        Returns output tensor after passing input `img` and `flow` to the backwarping\n        block.\n        I0  = backwarp(I1, F_0_1)\n\n        Parameters\n        ----------\n            img : tensor\n                frame I1.\n            flow : tensor\n                optical flow from I0 and I1: F_0_1.\n\n        Returns\n        -------\n            tensor\n                frame I0.\n        """"""\n\n\n        # Extract horizontal and vertical flows.\n        u = flow[:, 0, :, :]\n        v = flow[:, 1, :, :]\n        x = self.gridX.unsqueeze(0).expand_as(u).float() + u\n        y = self.gridY.unsqueeze(0).expand_as(v).float() + v\n        # range -1 to 1\n        x = 2*(x/self.W - 0.5)\n        y = 2*(y/self.H - 0.5)\n        # stacking X and Y\n        grid = torch.stack((x,y), dim=3)\n        # Sample pixels using bilinear interpolation.\n        imgOut = torch.nn.functional.grid_sample(img, grid)\n        return imgOut\n\n\n# Creating an array of `t` values for the 7 intermediate frames between\n# reference frames I0 and I1. \nt = np.linspace(0.125, 0.875, 7)\n\ndef getFlowCoeff (indices, device):\n    """"""\n    Gets flow coefficients used for calculating intermediate optical\n    flows from optical flows between I0 and I1: F_0_1 and F_1_0.\n\n    F_t_0 = C00 x F_0_1 + C01 x F_1_0\n    F_t_1 = C10 x F_0_1 + C11 x F_1_0\n\n    where,\n    C00 = -(1 - t) x t\n    C01 = t x t\n    C10 = (1 - t) x (1 - t)\n    C11 = -t x (1 - t)\n\n    Parameters\n    ----------\n        indices : tensor\n            indices corresponding to the intermediate frame positions\n            of all samples in the batch.\n        device : device\n                computation device (cpu/cuda). \n\n    Returns\n    -------\n        tensor\n            coefficients C00, C01, C10, C11.\n    """"""\n\n\n    # Convert indices tensor to numpy array\n    ind = indices.detach().numpy()\n    C11 = C00 = - (1 - (t[ind])) * (t[ind])\n    C01 = (t[ind]) * (t[ind])\n    C10 = (1 - (t[ind])) * (1 - (t[ind]))\n    return torch.Tensor(C00)[None, None, None, :].permute(3, 0, 1, 2).to(device), torch.Tensor(C01)[None, None, None, :].permute(3, 0, 1, 2).to(device), torch.Tensor(C10)[None, None, None, :].permute(3, 0, 1, 2).to(device), torch.Tensor(C11)[None, None, None, :].permute(3, 0, 1, 2).to(device)\n\ndef getWarpCoeff (indices, device):\n    """"""\n    Gets coefficients used for calculating final intermediate \n    frame `It_gen` from backwarped images using flows F_t_0 and F_t_1.\n\n    It_gen = (C0 x V_t_0 x g_I_0_F_t_0 + C1 x V_t_1 x g_I_1_F_t_1) / (C0 x V_t_0 + C1 x V_t_1)\n\n    where,\n    C0 = 1 - t\n    C1 = t\n\n    V_t_0, V_t_1 --> visibility maps\n    g_I_0_F_t_0, g_I_1_F_t_1 --> backwarped intermediate frames\n\n    Parameters\n    ----------\n        indices : tensor\n            indices corresponding to the intermediate frame positions\n            of all samples in the batch.\n        device : device\n                computation device (cpu/cuda). \n\n    Returns\n    -------\n        tensor\n            coefficients C0 and C1.\n    """"""\n\n\n    # Convert indices tensor to numpy array\n    ind = indices.detach().numpy()\n    C0 = 1 - t[ind]\n    C1 = t[ind]\n    return torch.Tensor(C0)[None, None, None, :].permute(3, 0, 1, 2).to(device), torch.Tensor(C1)[None, None, None, :].permute(3, 0, 1, 2).to(device)'"
train.py,17,"b'\n#[Super SloMo]\n##High Quality Estimation of Multiple Intermediate Frames for Video Interpolation\n\nimport argparse\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport model\nimport dataloader\nfrom math import log10\nimport datetime\nfrom tensorboardX import SummaryWriter\n\n\n# For parsing commandline arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(""--dataset_root"", type=str, required=True, help=\'path to dataset folder containing train-test-validation folders\')\nparser.add_argument(""--checkpoint_dir"", type=str, required=True, help=\'path to folder for saving checkpoints\')\nparser.add_argument(""--checkpoint"", type=str, help=\'path of checkpoint for pretrained model\')\nparser.add_argument(""--train_continue"", type=bool, default=False, help=\'If resuming from checkpoint, set to True and set `checkpoint` path. Default: False.\')\nparser.add_argument(""--epochs"", type=int, default=200, help=\'number of epochs to train. Default: 200.\')\nparser.add_argument(""--train_batch_size"", type=int, default=6, help=\'batch size for training. Default: 6.\')\nparser.add_argument(""--validation_batch_size"", type=int, default=10, help=\'batch size for validation. Default: 10.\')\nparser.add_argument(""--init_learning_rate"", type=float, default=0.0001, help=\'set initial learning rate. Default: 0.0001.\')\nparser.add_argument(""--milestones"", type=list, default=[100, 150], help=\'Set to epoch values where you want to decrease learning rate by a factor of 0.1. Default: [100, 150]\')\nparser.add_argument(""--progress_iter"", type=int, default=100, help=\'frequency of reporting progress and validation. N: after every N iterations. Default: 100.\')\nparser.add_argument(""--checkpoint_epoch"", type=int, default=5, help=\'checkpoint saving frequency. N: after every N epochs. Each checkpoint is roughly of size 151 MB.Default: 5.\')\nargs = parser.parse_args()\n\n##[TensorboardX](https://github.com/lanpa/tensorboardX)\n### For visualizing loss and interpolated frames\n\n\nwriter = SummaryWriter(\'log\')\n\n\n###Initialize flow computation and arbitrary-time flow interpolation CNNs.\n\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\nflowComp = model.UNet(6, 4)\nflowComp.to(device)\nArbTimeFlowIntrp = model.UNet(20, 5)\nArbTimeFlowIntrp.to(device)\n\n\n###Initialze backward warpers for train and validation datasets\n\n\ntrainFlowBackWarp      = model.backWarp(352, 352, device)\ntrainFlowBackWarp      = trainFlowBackWarp.to(device)\nvalidationFlowBackWarp = model.backWarp(640, 352, device)\nvalidationFlowBackWarp = validationFlowBackWarp.to(device)\n\n\n###Load Datasets\n\n\n# Channel wise mean calculated on adobe240-fps training dataset\nmean = [0.429, 0.431, 0.397]\nstd  = [1, 1, 1]\nnormalize = transforms.Normalize(mean=mean,\n                                 std=std)\ntransform = transforms.Compose([transforms.ToTensor(), normalize])\n\ntrainset = dataloader.SuperSloMo(root=args.dataset_root + \'/train\', transform=transform, train=True)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=args.train_batch_size, shuffle=True)\n\nvalidationset = dataloader.SuperSloMo(root=args.dataset_root + \'/validation\', transform=transform, randomCropSize=(640, 352), train=False)\nvalidationloader = torch.utils.data.DataLoader(validationset, batch_size=args.validation_batch_size, shuffle=False)\n\nprint(trainset, validationset)\n\n\n###Create transform to display image from tensor\n\n\nnegmean = [x * -1 for x in mean]\nrevNormalize = transforms.Normalize(mean=negmean, std=std)\nTP = transforms.Compose([revNormalize, transforms.ToPILImage()])\n\n\n###Utils\n    \ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\'lr\']\n\n\n###Loss and Optimizer\n\n\nL1_lossFn = nn.L1Loss()\nMSE_LossFn = nn.MSELoss()\n\nparams = list(ArbTimeFlowIntrp.parameters()) + list(flowComp.parameters())\n\noptimizer = optim.Adam(params, lr=args.init_learning_rate)\n# scheduler to decrease learning rate by a factor of 10 at milestones.\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.milestones, gamma=0.1)\n\n\n###Initializing VGG16 model for perceptual loss\n\n\nvgg16 = torchvision.models.vgg16(pretrained=True)\nvgg16_conv_4_3 = nn.Sequential(*list(vgg16.children())[0][:22])\nvgg16_conv_4_3.to(device)\nfor param in vgg16_conv_4_3.parameters():\n\t\tparam.requires_grad = False\n\n\n### Validation function\n# \n\n\ndef validate():\n    # For details see training.\n    psnr = 0\n    tloss = 0\n    flag = 1\n    with torch.no_grad():\n        for validationIndex, (validationData, validationFrameIndex) in enumerate(validationloader, 0):\n            frame0, frameT, frame1 = validationData\n\n            I0 = frame0.to(device)\n            I1 = frame1.to(device)\n            IFrame = frameT.to(device)\n                        \n            \n            flowOut = flowComp(torch.cat((I0, I1), dim=1))\n            F_0_1 = flowOut[:,:2,:,:]\n            F_1_0 = flowOut[:,2:,:,:]\n\n            fCoeff = model.getFlowCoeff(validationFrameIndex, device)\n\n            F_t_0 = fCoeff[0] * F_0_1 + fCoeff[1] * F_1_0\n            F_t_1 = fCoeff[2] * F_0_1 + fCoeff[3] * F_1_0\n\n            g_I0_F_t_0 = validationFlowBackWarp(I0, F_t_0)\n            g_I1_F_t_1 = validationFlowBackWarp(I1, F_t_1)\n            \n            intrpOut = ArbTimeFlowIntrp(torch.cat((I0, I1, F_0_1, F_1_0, F_t_1, F_t_0, g_I1_F_t_1, g_I0_F_t_0), dim=1))\n                \n            F_t_0_f = intrpOut[:, :2, :, :] + F_t_0\n            F_t_1_f = intrpOut[:, 2:4, :, :] + F_t_1\n            V_t_0   = F.sigmoid(intrpOut[:, 4:5, :, :])\n            V_t_1   = 1 - V_t_0\n                \n            g_I0_F_t_0_f = validationFlowBackWarp(I0, F_t_0_f)\n            g_I1_F_t_1_f = validationFlowBackWarp(I1, F_t_1_f)\n            \n            wCoeff = model.getWarpCoeff(validationFrameIndex, device)\n            \n            Ft_p = (wCoeff[0] * V_t_0 * g_I0_F_t_0_f + wCoeff[1] * V_t_1 * g_I1_F_t_1_f) / (wCoeff[0] * V_t_0 + wCoeff[1] * V_t_1)\n            \n            # For tensorboard\n            if (flag):\n                retImg = torchvision.utils.make_grid([revNormalize(frame0[0]), revNormalize(frameT[0]), revNormalize(Ft_p.cpu()[0]), revNormalize(frame1[0])], padding=10)\n                flag = 0\n            \n            \n            #loss\n            recnLoss = L1_lossFn(Ft_p, IFrame)\n            \n            prcpLoss = MSE_LossFn(vgg16_conv_4_3(Ft_p), vgg16_conv_4_3(IFrame))\n            \n            warpLoss = L1_lossFn(g_I0_F_t_0, IFrame) + L1_lossFn(g_I1_F_t_1, IFrame) + L1_lossFn(validationFlowBackWarp(I0, F_1_0), I1) + L1_lossFn(validationFlowBackWarp(I1, F_0_1), I0)\n        \n            loss_smooth_1_0 = torch.mean(torch.abs(F_1_0[:, :, :, :-1] - F_1_0[:, :, :, 1:])) + torch.mean(torch.abs(F_1_0[:, :, :-1, :] - F_1_0[:, :, 1:, :]))\n            loss_smooth_0_1 = torch.mean(torch.abs(F_0_1[:, :, :, :-1] - F_0_1[:, :, :, 1:])) + torch.mean(torch.abs(F_0_1[:, :, :-1, :] - F_0_1[:, :, 1:, :]))\n            loss_smooth = loss_smooth_1_0 + loss_smooth_0_1\n            \n            \n            loss = 204 * recnLoss + 102 * warpLoss + 0.005 * prcpLoss + loss_smooth\n            tloss += loss.item()\n            \n            #psnr\n            MSE_val = MSE_LossFn(Ft_p, IFrame)\n            psnr += (10 * log10(1 / MSE_val.item()))\n            \n    return (psnr / len(validationloader)), (tloss / len(validationloader)), retImg\n\n\n### Initialization\n\n\nif args.train_continue:\n    dict1 = torch.load(args.checkpoint)\n    ArbTimeFlowIntrp.load_state_dict(dict1[\'state_dictAT\'])\n    flowComp.load_state_dict(dict1[\'state_dictFC\'])\nelse:\n    dict1 = {\'loss\': [], \'valLoss\': [], \'valPSNR\': [], \'epoch\': -1}\n\n\n### Training\n\n\nimport time\n\nstart = time.time()\ncLoss   = dict1[\'loss\']\nvalLoss = dict1[\'valLoss\']\nvalPSNR = dict1[\'valPSNR\']\ncheckpoint_counter = 0\n\n### Main training loop\nfor epoch in range(dict1[\'epoch\'] + 1, args.epochs):\n    print(""Epoch: "", epoch)\n        \n    # Append and reset\n    cLoss.append([])\n    valLoss.append([])\n    valPSNR.append([])\n    iLoss = 0\n    \n    # Increment scheduler count    \n    scheduler.step()\n    \n    for trainIndex, (trainData, trainFrameIndex) in enumerate(trainloader, 0):\n        \n\t\t## Getting the input and the target from the training set\n        frame0, frameT, frame1 = trainData\n        \n        I0 = frame0.to(device)\n        I1 = frame1.to(device)\n        IFrame = frameT.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Calculate flow between reference frames I0 and I1\n        flowOut = flowComp(torch.cat((I0, I1), dim=1))\n        \n        # Extracting flows between I0 and I1 - F_0_1 and F_1_0\n        F_0_1 = flowOut[:,:2,:,:]\n        F_1_0 = flowOut[:,2:,:,:]\n        \n        fCoeff = model.getFlowCoeff(trainFrameIndex, device)\n        \n        # Calculate intermediate flows\n        F_t_0 = fCoeff[0] * F_0_1 + fCoeff[1] * F_1_0\n        F_t_1 = fCoeff[2] * F_0_1 + fCoeff[3] * F_1_0\n        \n        # Get intermediate frames from the intermediate flows\n        g_I0_F_t_0 = trainFlowBackWarp(I0, F_t_0)\n        g_I1_F_t_1 = trainFlowBackWarp(I1, F_t_1)\n        \n        # Calculate optical flow residuals and visibility maps\n        intrpOut = ArbTimeFlowIntrp(torch.cat((I0, I1, F_0_1, F_1_0, F_t_1, F_t_0, g_I1_F_t_1, g_I0_F_t_0), dim=1))\n        \n        # Extract optical flow residuals and visibility maps\n        F_t_0_f = intrpOut[:, :2, :, :] + F_t_0\n        F_t_1_f = intrpOut[:, 2:4, :, :] + F_t_1\n        V_t_0   = F.sigmoid(intrpOut[:, 4:5, :, :])\n        V_t_1   = 1 - V_t_0\n        \n        # Get intermediate frames from the intermediate flows\n        g_I0_F_t_0_f = trainFlowBackWarp(I0, F_t_0_f)\n        g_I1_F_t_1_f = trainFlowBackWarp(I1, F_t_1_f)\n        \n        wCoeff = model.getWarpCoeff(trainFrameIndex, device)\n        \n        # Calculate final intermediate frame \n        Ft_p = (wCoeff[0] * V_t_0 * g_I0_F_t_0_f + wCoeff[1] * V_t_1 * g_I1_F_t_1_f) / (wCoeff[0] * V_t_0 + wCoeff[1] * V_t_1)\n        \n        # Loss\n        recnLoss = L1_lossFn(Ft_p, IFrame)\n            \n        prcpLoss = MSE_LossFn(vgg16_conv_4_3(Ft_p), vgg16_conv_4_3(IFrame))\n        \n        warpLoss = L1_lossFn(g_I0_F_t_0, IFrame) + L1_lossFn(g_I1_F_t_1, IFrame) + L1_lossFn(trainFlowBackWarp(I0, F_1_0), I1) + L1_lossFn(trainFlowBackWarp(I1, F_0_1), I0)\n        \n        loss_smooth_1_0 = torch.mean(torch.abs(F_1_0[:, :, :, :-1] - F_1_0[:, :, :, 1:])) + torch.mean(torch.abs(F_1_0[:, :, :-1, :] - F_1_0[:, :, 1:, :]))\n        loss_smooth_0_1 = torch.mean(torch.abs(F_0_1[:, :, :, :-1] - F_0_1[:, :, :, 1:])) + torch.mean(torch.abs(F_0_1[:, :, :-1, :] - F_0_1[:, :, 1:, :]))\n        loss_smooth = loss_smooth_1_0 + loss_smooth_0_1\n          \n        # Total Loss - Coefficients 204 and 102 are used instead of 0.8 and 0.4\n        # since the loss in paper is calculated for input pixels in range 0-255\n        # and the input to our network is in range 0-1\n        loss = 204 * recnLoss + 102 * warpLoss + 0.005 * prcpLoss + loss_smooth\n        \n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n        iLoss += loss.item()\n               \n        # Validation and progress every `args.progress_iter` iterations\n        if ((trainIndex % args.progress_iter) == args.progress_iter - 1):\n            end = time.time()\n            \n            psnr, vLoss, valImg = validate()\n            \n            valPSNR[epoch].append(psnr)\n            valLoss[epoch].append(vLoss)\n            \n            #Tensorboard\n            itr = trainIndex + epoch * (len(trainloader))\n            \n            writer.add_scalars(\'Loss\', {\'trainLoss\': iLoss/args.progress_iter,\n                                        \'validationLoss\': vLoss}, itr)\n            writer.add_scalar(\'PSNR\', psnr, itr)\n            \n            writer.add_image(\'Validation\',valImg , itr)\n            #####\n            \n            endVal = time.time()\n            \n            print("" Loss: %0.6f  Iterations: %4d/%4d  TrainExecTime: %0.1f  ValLoss:%0.6f  ValPSNR: %0.4f  ValEvalTime: %0.2f LearningRate: %f"" % (iLoss / args.progress_iter, trainIndex, len(trainloader), end - start, vLoss, psnr, endVal - end, get_lr(optimizer)))\n            \n            \n            cLoss[epoch].append(iLoss/args.progress_iter)\n            iLoss = 0\n            start = time.time()\n    \n    # Create checkpoint after every `args.checkpoint_epoch` epochs\n    if ((epoch % args.checkpoint_epoch) == args.checkpoint_epoch - 1):\n        dict1 = {\n                \'Detail\':""End to end Super SloMo."",\n                \'epoch\':epoch,\n                \'timestamp\':datetime.datetime.now(),\n                \'trainBatchSz\':args.train_batch_size,\n                \'validationBatchSz\':args.validation_batch_size,\n                \'learningRate\':get_lr(optimizer),\n                \'loss\':cLoss,\n                \'valLoss\':valLoss,\n                \'valPSNR\':valPSNR,\n                \'state_dictFC\': flowComp.state_dict(),\n                \'state_dictAT\': ArbTimeFlowIntrp.state_dict(),\n                }\n        torch.save(dict1, args.checkpoint_dir + ""/SuperSloMo"" + str(checkpoint_counter) + "".ckpt"")\n        checkpoint_counter += 1\n'"
video_to_slomo.py,7,"b'#!/usr/bin/env python3\nimport argparse\nimport os\nimport os.path\nimport ctypes\nfrom shutil import rmtree, move\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\nimport model\nimport dataloader\nimport platform\nfrom tqdm import tqdm\n\n# For parsing commandline arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(""--ffmpeg_dir"", type=str, default="""", help=\'path to ffmpeg.exe\')\nparser.add_argument(""--video"", type=str, required=True, help=\'path of video to be converted\')\nparser.add_argument(""--checkpoint"", type=str, required=True, help=\'path of checkpoint for pretrained model\')\nparser.add_argument(""--fps"", type=float, default=30, help=\'specify fps of output video. Default: 30.\')\nparser.add_argument(""--sf"", type=int, required=True, help=\'specify the slomo factor N. This will increase the frames by Nx. Example sf=2 ==> 2x frames\')\nparser.add_argument(""--batch_size"", type=int, default=1, help=\'Specify batch size for faster conversion. This will depend on your cpu/gpu memory. Default: 1\')\nparser.add_argument(""--output"", type=str, default=""output.mkv"", help=\'Specify output file name. Default: output.mp4\')\nargs = parser.parse_args()\n\ndef check():\n    """"""\n    Checks the validity of commandline arguments.\n\n    Parameters\n    ----------\n        None\n\n    Returns\n    -------\n        error : string\n            Error message if error occurs otherwise blank string.\n    """"""\n\n\n    error = """"\n    if (args.sf < 2):\n        error = ""Error: --sf/slomo factor has to be atleast 2""\n    if (args.batch_size < 1):\n        error = ""Error: --batch_size has to be atleast 1""\n    if (args.fps < 1):\n        error = ""Error: --fps has to be atleast 1""\n    if "".mkv"" not in args.output:\n        error = ""output needs to have mkv container""\n    return error\n\ndef extract_frames(video, outDir):\n    """"""\n    Converts the `video` to images.\n\n    Parameters\n    ----------\n        video : string\n            full path to the video file.\n        outDir : string\n            path to directory to output the extracted images.\n\n    Returns\n    -------\n        error : string\n            Error message if error occurs otherwise blank string.\n    """"""\n\n\n    error = """"\n    print(\'{} -i {} -vsync 0 {}/%06d.png\'.format(os.path.join(args.ffmpeg_dir, ""ffmpeg""), video, outDir))\n    retn = os.system(\'{} -i ""{}"" -vsync 0 {}/%06d.png\'.format(os.path.join(args.ffmpeg_dir, ""ffmpeg""), video, outDir))\n    if retn:\n        error = ""Error converting file:{}. Exiting."".format(video)\n    return error\n\ndef create_video(dir):\n    error = """"\n    print(\'{} -r {} -i {}/%d.png -vcodec ffvhuff {}\'.format(os.path.join(args.ffmpeg_dir, ""ffmpeg""), args.fps, dir, args.output))\n    retn = os.system(\'{} -r {} -i {}/%d.png -vcodec ffvhuff ""{}""\'.format(os.path.join(args.ffmpeg_dir, ""ffmpeg""), args.fps, dir, args.output))\n    if retn:\n        error = ""Error creating output video. Exiting.""\n    return error\n\n\ndef main():\n    # Check if arguments are okay\n    error = check()\n    if error:\n        print(error)\n        exit(1)\n\n    # Create extraction folder and extract frames\n    IS_WINDOWS = \'Windows\' == platform.system()\n    extractionDir = ""tmpSuperSloMo""\n    if not IS_WINDOWS:\n        # Assuming UNIX-like system where ""."" indicates hidden directories\n        extractionDir = ""."" + extractionDir\n    if os.path.isdir(extractionDir):\n        rmtree(extractionDir)\n    os.mkdir(extractionDir)\n    if IS_WINDOWS:\n        FILE_ATTRIBUTE_HIDDEN = 0x02\n        # ctypes.windll only exists on Windows\n        ctypes.windll.kernel32.SetFileAttributesW(extractionDir, FILE_ATTRIBUTE_HIDDEN)\n\n    extractionPath = os.path.join(extractionDir, ""input"")\n    outputPath     = os.path.join(extractionDir, ""output"")\n    os.mkdir(extractionPath)\n    os.mkdir(outputPath)\n    error = extract_frames(args.video, extractionPath)\n    if error:\n        print(error)\n        exit(1)\n\n    # Initialize transforms\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n    mean = [0.429, 0.431, 0.397]\n    std  = [1, 1, 1]\n    normalize = transforms.Normalize(mean=mean,\n                                     std=std)\n\n    negmean = [x * -1 for x in mean]\n    revNormalize = transforms.Normalize(mean=negmean, std=std)\n\n    # Temporary fix for issue #7 https://github.com/avinashpaliwal/Super-SloMo/issues/7 -\n    # - Removed per channel mean subtraction for CPU.\n    if (device == ""cpu""):\n        transform = transforms.Compose([transforms.ToTensor()])\n        TP = transforms.Compose([transforms.ToPILImage()])\n    else:\n        transform = transforms.Compose([transforms.ToTensor(), normalize])\n        TP = transforms.Compose([revNormalize, transforms.ToPILImage()])\n\n    # Load data\n    videoFrames = dataloader.Video(root=extractionPath, transform=transform)\n    videoFramesloader = torch.utils.data.DataLoader(videoFrames, batch_size=args.batch_size, shuffle=False)\n\n    # Initialize model\n    flowComp = model.UNet(6, 4)\n    flowComp.to(device)\n    for param in flowComp.parameters():\n        param.requires_grad = False\n    ArbTimeFlowIntrp = model.UNet(20, 5)\n    ArbTimeFlowIntrp.to(device)\n    for param in ArbTimeFlowIntrp.parameters():\n        param.requires_grad = False\n\n    flowBackWarp = model.backWarp(videoFrames.dim[0], videoFrames.dim[1], device)\n    flowBackWarp = flowBackWarp.to(device)\n\n    dict1 = torch.load(args.checkpoint, map_location=\'cpu\')\n    ArbTimeFlowIntrp.load_state_dict(dict1[\'state_dictAT\'])\n    flowComp.load_state_dict(dict1[\'state_dictFC\'])\n\n    # Interpolate frames\n    frameCounter = 1\n\n    with torch.no_grad():\n        for _, (frame0, frame1) in enumerate(tqdm(videoFramesloader), 0):\n\n            I0 = frame0.to(device)\n            I1 = frame1.to(device)\n\n            flowOut = flowComp(torch.cat((I0, I1), dim=1))\n            F_0_1 = flowOut[:,:2,:,:]\n            F_1_0 = flowOut[:,2:,:,:]\n\n            # Save reference frames in output folder\n            for batchIndex in range(args.batch_size):\n                (TP(frame0[batchIndex].detach())).resize(videoFrames.origDim, Image.BILINEAR).save(os.path.join(outputPath, str(frameCounter + args.sf * batchIndex) + "".png""))\n            frameCounter += 1\n\n            # Generate intermediate frames\n            for intermediateIndex in range(1, args.sf):\n                t = float(intermediateIndex) / args.sf\n                temp = -t * (1 - t)\n                fCoeff = [temp, t * t, (1 - t) * (1 - t), temp]\n\n                F_t_0 = fCoeff[0] * F_0_1 + fCoeff[1] * F_1_0\n                F_t_1 = fCoeff[2] * F_0_1 + fCoeff[3] * F_1_0\n\n                g_I0_F_t_0 = flowBackWarp(I0, F_t_0)\n                g_I1_F_t_1 = flowBackWarp(I1, F_t_1)\n\n                intrpOut = ArbTimeFlowIntrp(torch.cat((I0, I1, F_0_1, F_1_0, F_t_1, F_t_0, g_I1_F_t_1, g_I0_F_t_0), dim=1))\n\n                F_t_0_f = intrpOut[:, :2, :, :] + F_t_0\n                F_t_1_f = intrpOut[:, 2:4, :, :] + F_t_1\n                V_t_0   = torch.sigmoid(intrpOut[:, 4:5, :, :])\n                V_t_1   = 1 - V_t_0\n\n                g_I0_F_t_0_f = flowBackWarp(I0, F_t_0_f)\n                g_I1_F_t_1_f = flowBackWarp(I1, F_t_1_f)\n\n                wCoeff = [1 - t, t]\n\n                Ft_p = (wCoeff[0] * V_t_0 * g_I0_F_t_0_f + wCoeff[1] * V_t_1 * g_I1_F_t_1_f) / (wCoeff[0] * V_t_0 + wCoeff[1] * V_t_1)\n\n                # Save intermediate frame\n                for batchIndex in range(args.batch_size):\n                    (TP(Ft_p[batchIndex].cpu().detach())).resize(videoFrames.origDim, Image.BILINEAR).save(os.path.join(outputPath, str(frameCounter + args.sf * batchIndex) + "".png""))\n                frameCounter += 1\n\n            # Set counter accounting for batching of frames\n            frameCounter += args.sf * (args.batch_size - 1)\n\n    # Generate video from interpolated frames\n    create_video(outputPath)\n\n    # Remove temporary files\n    rmtree(extractionDir)\n\n    exit(0)\n\nmain()\n'"
data/create_dataset.py,0,"b'import argparse\nimport os\nimport os.path\nfrom shutil import rmtree, move\nimport random\n\n# For parsing commandline arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(""--ffmpeg_dir"", type=str, required=True, help=\'path to ffmpeg.exe\')\nparser.add_argument(""--dataset"", type=str, default=""custom"", help=\'specify if using ""adobe240fps"" or custom video dataset\')\nparser.add_argument(""--videos_folder"", type=str, required=True, help=\'path to the folder containing videos\')\nparser.add_argument(""--dataset_folder"", type=str, required=True, help=\'path to the output dataset folder\')\nparser.add_argument(""--img_width"", type=int, default=640, help=""output image width"")\nparser.add_argument(""--img_height"", type=int, default=360, help=""output image height"")\nparser.add_argument(""--train_test_split"", type=tuple, default=(90, 10), help=""train test split for custom dataset"")\nargs = parser.parse_args()\n\n\ndef extract_frames(videos, inDir, outDir):\n    """"""\n    Converts all the videos passed in `videos` list to images.\n\n    Parameters\n    ----------\n        videos : list\n            name of all video files.\n        inDir : string\n            path to input directory containing videos in `videos` list.\n        outDir : string\n            path to directory to output the extracted images.\n\n    Returns\n    -------\n        None\n    """"""\n\n\n    for video in videos:\n        os.mkdir(os.path.join(outDir, os.path.splitext(video)[0]))\n        retn = os.system(\'{} -i {} -vf scale={}:{} -vsync 0 -qscale:v 2 {}/%04d.jpg\'.format(os.path.join(args.ffmpeg_dir, ""ffmpeg""), os.path.join(inDir, video), args.img_width, args.img_height, os.path.join(outDir, os.path.splitext(video)[0])))\n        if retn:\n            print(""Error converting file:{}. Exiting."".format(video))\n\n\ndef create_clips(root, destination):\n    """"""\n    Distributes the images extracted by `extract_frames()` in\n    clips containing 12 frames each.\n\n    Parameters\n    ----------\n        root : string\n            path containing extracted image folders.\n        destination : string\n            path to output clips.\n\n    Returns\n    -------\n        None\n    """"""\n\n\n    folderCounter = -1\n\n    files = os.listdir(root)\n\n    # Iterate over each folder containing extracted video frames.\n    for file in files:\n        images = sorted(os.listdir(os.path.join(root, file)))\n\n        for imageCounter, image in enumerate(images):\n            # Bunch images in groups of 12 frames\n            if (imageCounter % 12 == 0):\n                if (imageCounter + 11 >= len(images)):\n                    break\n                folderCounter += 1\n                os.mkdir(""{}/{}"".format(destination, folderCounter))\n            move(""{}/{}/{}"".format(root, file, image), ""{}/{}/{}"".format(destination, folderCounter, image))\n        rmtree(os.path.join(root, file))\n\ndef main():\n    # Create dataset folder if it doesn\'t exist already.\n    if not os.path.isdir(args.dataset_folder):\n        os.mkdir(args.dataset_folder)\n\n    extractPath      = os.path.join(args.dataset_folder, ""extracted"")\n    trainPath        = os.path.join(args.dataset_folder, ""train"")\n    testPath         = os.path.join(args.dataset_folder, ""test"")\n    validationPath   = os.path.join(args.dataset_folder, ""validation"")\n    os.mkdir(extractPath)\n    os.mkdir(trainPath)\n    os.mkdir(testPath)\n    os.mkdir(validationPath)\n\n    if(args.dataset == ""adobe240fps""):\n        f = open(""adobe240fps/test_list.txt"", ""r"")\n        videos = f.read().split(\'\\n\')\n        extract_frames(videos, args.videos_folder, extractPath)\n        create_clips(extractPath, testPath)\n\n        f = open(""adobe240fps/train_list.txt"", ""r"")\n        videos = f.read().split(\'\\n\')\n        extract_frames(videos, args.videos_folder, extractPath)\n        create_clips(extractPath, trainPath)\n\n        # Select 100 clips at random from test set for validation set.\n        testClips = os.listdir(testPath)\n        indices = random.sample(range(len(testClips)), 100)\n        for index in indices:\n            move(""{}/{}"".format(testPath, index), ""{}/{}"".format(validationPath, index))\n\n    else: # custom dataset\n        \n        # Extract video names\n        videos = os.listdir(args.videos_folder)\n\n        # Create random train-test split.\n        testIndices  = random.sample(range(len(videos)), int((args.train_test_split[1] * len(videos)) / 100))\n        trainIndices = [x for x in range((len(videos))) if x not in testIndices]\n\n        # Create list of video names\n        testVideoNames  = [videos[index] for index in testIndices]\n        trainVideoNames = [videos[index] for index in trainIndices]\n\n        # Create train-test dataset\n        extract_frames(testVideoNames, args.videos_folder, extractPath)\n        create_clips(extractPath, testPath)\n        extract_frames(trainVideoNames, args.videos_folder, extractPath)\n        create_clips(extractPath, trainPath)\n\n        # Select clips at random from test set for validation set.\n        testClips = os.listdir(testPath)\n        indices = random.sample(range(len(testClips)), min(100, int(len(testClips) / 5)))\n        for index in indices:\n            move(""{}/{}"".format(testPath, index), ""{}/{}"".format(validationPath, index))\n\n    rmtree(extractPath)\n\nmain()\n'"
