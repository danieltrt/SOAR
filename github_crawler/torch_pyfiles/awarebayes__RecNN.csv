file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nsetup(name=\'recnn\',\n      version=\'0.1\',\n      description=""A Python toolkit for Reinforced News Recommendation."",\n      long_description=""A Python toolkit for Reinforced News Recommendation."",\n      author=\'Mike Watts\',\n      author_email=\'awarebayes@gmail.com\',\n      license=\'Apache 2.0\',\n      packages=find_packages(),\n      install_requires=[\n          \'torch\',\n          \'numpy\',\n      ],\n      url = \'https://github.com/awarebayes/RecNN\',\n      zip_safe=False,\n      classifiers = [\n                  \'Development Status :: 4 - Beta\',\n                  \'Intended Audience :: Developers\',  # Define that your audience are developers\n                  \'Topic :: Software Development :: Build Tools\',\n                  \'License :: OSI Approved :: MIT License\',  # Again, pick a license\n                  \'Programming Language :: Python :: 3\',  # Specify which pyhton versions that you want to support\n                  \'Programming Language :: Python :: 3.4\',\n                  \'Programming Language :: Python :: 3.5\',\n                  \'Programming Language :: Python :: 3.6\',\n              ],\n      )\n\n'"
examples/streamlit_demo.py,8,"b'import streamlit as st\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pickle\nimport json\nimport copy\nimport pandas as pd\nimport random\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom scipy.spatial import distance\n\n# == recnn ==\nimport sys\nsys.path.append(""../"")\nimport recnn\n\n\ntqdm.pandas()\n\n# constants\nML20MPATH = \'../data/ml-20m/\'\nMODELSPATH = \'../models/\'\nDATAPATH = \'../data/streamlit/\'\nSHOW_TOPN_MOVIES = 200 # recommend me a movie. show only top ... movies, higher values lead to slow ux\n\n# disable it if you get an error\nfrom jupyterthemes import jtplot\njtplot.style(theme=\'grade3\')\n\n\ndef render_header():\n    st.write(""""""\n        <p align=""center""> \n            <img src=""https://raw.githubusercontent.com/awarebayes/RecNN/master/res/logo%20big.png"">\n        </p>\n\n\n        <p align=""center""> \n        <iframe src=""https://ghbtns.com/github-btn.html?user=awarebayes&repo=recnn&type=star&count=true&size=large"" frameborder=""0"" scrolling=""0"" width=""160px"" height=""30px""></iframe>\n        <iframe src=""https://ghbtns.com/github-btn.html?user=awarebayes&repo=recnn&type=fork&count=true&size=large"" frameborder=""0"" scrolling=""0"" width=""158px"" height=""30px""></iframe>\n        <iframe src=""https://ghbtns.com/github-btn.html?user=awarebayes&type=follow&count=true&size=large"" frameborder=""0"" scrolling=""0"" width=""220px"" height=""30px""></iframe>\n        </p>\n\n        <p align=""center""> \n\n        <a href=\'https://circleci.com/gh/awarebayes/RecNN\'>\n        <img src=\'https://circleci.com/gh/awarebayes/RecNN.svg?style=svg\' alt=\'Documentation Status\' />\n        </a>\n\n        <a href=""https://codeclimate.com/github/awarebayes/RecNN/maintainability"">\n        <img src=""https://api.codeclimate.com/v1/badges/d3a06ffe45906969239d/maintainability"" />            \n        </a>\n\n        <a href=""https://colab.research.google.com/github/awarebayes/RecNN/"">\n        <img src=""https://colab.research.google.com/assets/colab-badge.svg"" />\n        </a>\n\n        <a href=\'https://recnn.readthedocs.io/en/latest/?badge=latest\'>\n        <img src=\'https://readthedocs.org/projects/recnn/badge/?version=latest\' alt=\'Documentation Status\' />\n        </a>\n\n        </p>\n\n        <p align=""center""> \n            <b> Choose the page on the left sidebar to proceed </b>\n        </p>\n\n        <p align=""center""> \n            This is my school project. It focuses on Reinforcement Learning for personalized news recommendation.\n            The main distinction is that it tries to solve online off-policy learning with dynamically generated \n            item embeddings. I want to create a library with SOTA algorithms for reinforcement learning\n            recommendation, providing the level of abstraction you like.\n        </p>\n\n        <p align=""center"">\n            <a href=""https://recnn.readthedocs.io"">recnn.readthedocs.io</a>\n        </p>\n\n        ### \xf0\x9f\x93\x9a Read the articles on medium!\n\n        - Pretty much what you need to get started with this library if you know recommenders\n          but don\'t know much about reinforcement learning:\n        <p align=""center""> \n           <a href=""https://towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011"">\n                <img src=""https://raw.githubusercontent.com/awarebayes/RecNN/master/res/article_1.png""  width=""100%"">\n            </a>\n        </p>\n\n        - Top-K Off-Policy Correction for a REINFORCE Recommender System:\n        <p align=""center""> \n           <a href=""https://towardsdatascience.com/top-k-off-policy-correction-for-a-reinforce-recommender-system-e34381dceef8"">\n                <img src=""https://raw.githubusercontent.com/awarebayes/RecNN/master/res/article_2.png"" width=""100%"">\n            </a>\n        </p>\n\n    """""", unsafe_allow_html=True)\n\n    st.markdown(""""""\n\n        ### \xf0\x9f\xa4\x96 You can play with these (more will be implemented):\n\n        | Algorithm                             | Paper                            | Code                       |\n        |---------------------------------------|----------------------------------|----------------------------|\n        | Deep Deterministic Policy Gradients   | https://arxiv.org/abs/1509.02971 | examples/1.Vanilla RL/DDPG |\n        | Twin Delayed DDPG (TD3)               | https://arxiv.org/abs/1802.09477 | examples/1.Vanilla RL/TD3  |\n        | Soft Actor-Critic                     | https://arxiv.org/abs/1801.01290 | examples/1.Vanilla RL/SAC  |\n        | REINFORCE Top-K Off-Policy Correction | https://arxiv.org/abs/1812.02353 | examples/2. REINFORCE TopK |\n    """""")\n\n\n@st.cache\ndef load_mekd():\n    return pickle.load(open(DATAPATH + \'mekd.pkl\', \'rb\'))\n\n\ndef get_batch(device):\n    # gets a random batch using cached load\n    @st.cache\n    def load_batch():\n        return pickle.load(open(DATAPATH + \'batch.pkl\', \'rb\'))\n    # todo remove randomness\n    return [i.to(device) for i in random.choice(load_batch())]\n\n\ndef get_embeddings():\n    movie_embeddings_key_dict = load_mekd()\n    movies_embeddings_tensor, key_to_id, id_to_key = recnn.data.utils.make_items_tensor(movie_embeddings_key_dict)\n    return  movies_embeddings_tensor, key_to_id, id_to_key\n\n@st.cache\ndef load_omdb_meta():\n    return json.load(open(DATAPATH + \'omdb.json\'))\n\n\ndef load_models(device):\n    ddpg = recnn.nn.models.Actor(1290, 128, 256).to(device)\n    td3 = recnn.nn.models.Actor(1290, 128, 256).to(device)\n\n    ddpg.load_state_dict(torch.load(MODELSPATH + \'ddpg_policy.model\', map_location=device))\n    td3.load_state_dict(torch.load(MODELSPATH + \'td3_policy.model\', map_location=device))\n    return {\'ddpg\': ddpg, \'td3\': td3}\n\n@st.cache\ndef load_links():\n    return pd.read_csv(ML20MPATH + \'links.csv\', index_col=\'tmdbId\')\n\n@st.cache\ndef get_mov_base():\n    links = load_links()\n    movies_embeddings_tensor, key_to_id, id_to_key = get_embeddings()\n    meta = load_omdb_meta()\n\n    popular = pd.read_csv(DATAPATH + \'movie_counts.csv\')[:SHOW_TOPN_MOVIES]\n    st.write(popular[\'id\'])\n    mov_base = {}\n\n    for i, k in list(meta.items()):\n        tmdid = int(meta[i][\'tmdbId\'])\n        if tmdid > 0 and popular[\'id\'].isin([i]).any():\n            movieid = pd.to_numeric(links.loc[tmdid][\'movieId\'])\n            if isinstance(movieid, pd.Series):\n                continue\n            mov_base[int(movieid)] = meta[i][\'omdb\'][\'Title\']\n\n    return mov_base\n\n\ndef get_index():\n    import faiss\n    from sklearn.preprocessing import normalize\n    # test indexes\n    indexL2 = faiss.IndexFlatL2(128)\n    indexIP = faiss.IndexFlatIP(128)\n    indexCOS = faiss.IndexFlatIP(128)\n\n    mov_mat, _, _ = get_embeddings()\n    mov_mat = mov_mat.numpy().astype(\'float32\')\n    indexL2.add(mov_mat)\n    indexIP.add(mov_mat)\n    indexCOS.add(normalize(mov_mat, axis=1, norm=\'l2\'))\n    return {\'L2\': indexL2, \'IP\': indexIP, \'COS\': indexCOS}\n\ndef rank(gen_action, metric, k):\n    scores = []\n    movie_embeddings_key_dict = load_mekd()\n    meta = load_omdb_meta()\n\n    for i in movie_embeddings_key_dict.keys():\n        if i == 0 or i == \'0\':\n            continue\n        scores.append([i, metric(movie_embeddings_key_dict[i], gen_action)])\n    scores = list(sorted(scores, key = lambda x: x[1]))\n    scores = scores[:k]\n    ids = [i[0] for i in scores]\n    for i in range(k):\n        scores[i].extend([meta[str(scores[i][0])][\'omdb\'][key]  for key in [\'Title\',\n                                                                            \'Genre\', \'imdbRating\']])\n    indexes = [\'id\', \'score\', \'Title\', \'Genre\', \'imdbRating\']\n    table_dict = dict([(key, [i[idx] for i in scores]) for idx, key in enumerate(indexes)])\n    table = pd.DataFrame(table_dict)\n    return table\n\n@st.cache\ndef load_reinforce():\n    indexes = pickle.load(open(DATAPATH + \'reinforce_indexes.pkl\', \'rb\'))\n    state = pickle.load(open(DATAPATH + \'reinforce_state.pkl\', \'rb\'))\n    return state, indexes\n\ndef main():\n    st.sidebar.header(\'\xf0\x9f\x93\xb0 recnn by @awarebayes \xf0\x9f\x91\xa8\xe2\x80\x8d\xf0\x9f\x94\xa7\')\n\n    if st.sidebar.checkbox(\'Use cuda\', torch.cuda.is_available()):\n        device = torch.device(\'cuda\')\n    else:\n        device = torch.device(\'cpu\')\n\n    st.sidebar.subheader(\'Choose a page to proceed:\')\n    page = st.sidebar.selectbox("""", [""\xf0\x9f\x9a\x80 Get Started"", ""\xf0\x9f\x93\xbd \xef\xb8\x8fRecommend me a movie"", ""\xf0\x9f\x94\xa8 Test Recommendation"",\n                                     ""\xe2\x9b\x8f\xef\xb8\x8f Test Diversity"", ""\xf0\x9f\xa4\x96 Reinforce Top K""])\n\n    st.sidebar.markdown(""""""\n    ### I need your help!\n    Currently, I am at my final year of high school, doing all this to get into a university.\n    I live in Russia and believe that I have no future here.\n    \n    If you happened to know a prof/teacher/postdoc/anyone at your \n    university, please show them my CV: [link](https://drive.google.com/file/d/1jgM-SzEUbUjqgHzaajoUv4ENhC7-oaDT/view?usp=sharing).\n    \n    **I promise that I will make this library even better if I get a college degree!**\n    """""")\n\n    if page == ""\xf0\x9f\x9a\x80 Get Started"":\n        render_header()\n\n        st.subheader(""If you have cloned this repo, here is some stuff for you:"")\n\n        st.markdown(\n            """"""\n            \xf0\x9f\x93\x81 **Downloads** + change the **constants**, so they point to this unpacked folder:\n            \n            - [Models](https://drive.google.com/file/d/1goGa15XZmDAp2msZvRi2v_1h9xfmnhz7/view?usp=sharing)\n             **= MODELSPATH**\n            - [Data for Streamlit Demo](https://drive.google.com/file/d/1nuhHDdC4mCmiB7g0fmwUSOh1jEUQyWuz/view?usp=sharing)\n             **= DATAPATH**\n            - [ML20M Dataset](https://grouplens.org/datasets/movielens/20m/)\n             **= ML20MPATH**\n             \n            p.s. ml20m is only needed for links.csv, I couldn\'t include it in my streamlit data because of copyright.\n            This is all the data you need.\n            """"""\n        )\n\n    if page == ""\xf0\x9f\x94\xa8 Test Recommendation"":\n\n        st.header(""Test the Recommendations"")\n\n        st.info(""Upon the first opening the data will start loading.""\n                ""\\n Unfortunately there is no progress verbose in streamlit. Look in your console."")\n\n        st.success(\'Data is loaded!\')\n\n        models = load_models(device)\n        st.success(\'Models are loaded!\')\n\n        state, action, reward, next_state, done = get_batch(device)\n\n        st.subheader(\'Here is a random batch sampled from testing environment:\')\n        if st.checkbox(\'Print batch info\'):\n            st.subheader(\'State\')\n            st.write(state)\n            st.subheader(\'Action\')\n            st.write(action)\n            st.subheader(\'Reward\')\n            st.write(reward.squeeze())\n\n        st.subheader(\'(Optional) Select the state are getting the recommendations for\')\n\n        action_id = np.random.randint(0, state.size(0), 1)[0]\n        action_id_manual = st.checkbox(\'Manually set state index\')\n        if action_id_manual:\n            action_id = st.slider(""Choose state index:"", min_value=0, max_value=state.size(0))\n\n        st.write(\'state:\', state[action_id])\n\n        algorithm = st.selectbox(\'Choose an algorithm\', (\'ddpg\', \'td3\'))\n        metric = st.selectbox(\'Choose a metric\', (\'euclidean\', \'cosine\', \'correlation\',\n                                                  \'canberra\', \'minkowski\', \'chebyshev\',\n                                                  \'braycurtis\', \'cityblock\',))\n        topk = st.slider(""TOP K items to recommend:"", min_value=1, max_value=30, value=7)\n\n        dist = {\'euclidean\': distance.euclidean, \'cosine\': distance.cosine,\n                \'correlation\': distance.correlation, \'canberra\': distance.canberra,\n                \'minkowski\': distance.minkowski, \'chebyshev\': distance.chebyshev,\n                \'braycurtis\': distance.braycurtis, \'cityblock\': distance.cityblock}\n\n        action = models[algorithm].forward(state)\n\n        st.markdown(\'**Recommendations for state with index {}**\'.format(action_id))\n        st.write(rank(action[action_id].detach().cpu().numpy(), dist[metric], topk))\n\n        st.subheader(\'Pairwise distances for all actions in the batch:\')\n        st.pyplot(recnn.utils.pairwise_distances_fig(action))\n\n    if page == ""\xe2\x9b\x8f\xef\xb8\x8f Test Diversity"":\n        st.header(""Test the Distances (diversity and pinpoint accuracy)"")\n\n        models = load_models(device)\n        st.success(\'Models are loaded!\')\n        state, action, reward, next_state, done = get_batch(device)\n\n        indexes = get_index()\n\n        def query(index, action, k=20):\n            D, I = index.search(action, k)\n            return D, I\n\n        def get_err(action, dist, k=5, euc=False):\n            D, I = query(indexes[dist], action, k)\n            if euc:\n                D = D ** 0.5  # l2 -> euclidean\n            mean = D.mean(axis=1).mean()\n            std = D.std(axis=1).mean()\n            return I, mean, std\n\n        def get_action(model_name, action_id):\n            gen_action = models[model_name].forward(state)\n            gen_action = gen_action[action_id].detach().cpu().numpy()\n            return gen_action\n\n        st.subheader(\'(Optional) Select the state are getting the recommendations for\')\n\n        action_id = np.random.randint(0, state.size(0), 1)[0]\n        action_id_manual = st.checkbox(\'Manually set state index\')\n        if action_id_manual:\n            action_id = st.slider(""Choose state index:"", min_value=0, max_value=state.size(0))\n\n        st.header(\'Metric\')\n        dist = st.selectbox(\'Select distance\', [\'L2\', \'IP\', \'COS\'])\n\n        ddpg_action = get_action(\'ddpg\', action_id).reshape(1, -1)\n        td3_action  = get_action(\'td3\', action_id).reshape(1, -1)\n\n        topk = st.slider(""TOP K items to recommend:"", min_value=1, max_value=30, value=10)\n\n        ddpg_I, ddpg_mean, ddpg_std = get_err(ddpg_action, dist, topk, euc=True)\n        td3_I, td3_mean, td3_std = get_err(td3_action, dist, topk, euc=True)\n\n        # Mean Err\n        st.subheader(\'Mean error\')\n        st.markdown(""""""\n        How close are we to the actual movie embedding? \n        \n        The closer the better, although higher error may\n        produce more diverse recommendations.\n        """""")\n        labels = [\'DDPG\', \'TD3\']\n        x_pos = np.arange(len(labels))\n        CTEs = [ddpg_mean, td3_mean]\n        error = [ddpg_std, td3_std]\n\n        fig, ax = plt.subplots(figsize=(16, 9))\n        ax.bar(x_pos, CTEs, yerr=error, )\n        ax.set_xticks(x_pos)\n        ax.grid(False)\n        ax.set_xticklabels(labels)\n        ax.set_title(dist + \' error\')\n        ax.yaxis.grid(True)\n\n        st.pyplot(fig)\n\n        # Similarities\n        st.header(\'Similarities\')\n        emb, _, _ = get_embeddings()\n\n        st.markdown(\'Heatmap of correlation similarities (Grammarian Product of actions)\'\n                    \'\\n\\n\'\n                    \'Higher = mode diverse, lower = less diverse. You decide what is better...\')\n\n        st.subheader(\'ddpg\')\n        st.pyplot(recnn.utils.pairwise_distances_fig(torch.tensor(emb[ddpg_I])))\n        st.subheader(\'td3\')\n        st.pyplot(recnn.utils.pairwise_distances_fig(torch.tensor(emb[td3_I])))\n\n    if page == ""\xf0\x9f\x93\xbd \xef\xb8\x8fRecommend me a movie"":\n        st.header(""\xf0\x9f\x93\xbd \xef\xb8\x8fRecommend me a movie"")\n        st.markdown(""""""\n        **Now, this is probably why you came here. Let\'s get you some movies suggested**\n        \n        You need to choose 10 movies in the bar below by typing their titles.\n        Due to the client side limitations, I am only able to display top 200 movies.\n        P.S. you can type to search\n        """""")\n\n        mov_base = get_mov_base()\n        mov_base_by_title = {v: k for k, v in mov_base.items()}\n        movies_chosen = st.multiselect(\'Choose 10 movies\', list(mov_base.values()))\n        st.markdown(\'**{} chosen {} to go**\'.format(len(movies_chosen), 10 - len(movies_chosen)))\n\n        if len(movies_chosen) > 10:\n            st.error(\'Please select exactly 10 movies, you have selected {}\'.format(len(movies_chosen)))\n        if len(movies_chosen) == 10:\n            st.success(""You have selected 10 movies. Now let\'s rate them"")\n        else:\n            st.info(\'Please select 10 movies in the input above\')\n\n        if len(movies_chosen) == 10:\n            st.markdown(\'### Rate each movie from 1 to 10\')\n            ratings = dict([(i, st.number_input(i, min_value=1, max_value=10, value=5)) for i in movies_chosen])\n            # st.write(\'for debug your ratings are:\', ratings)\n\n\n            ids = [mov_base_by_title[i] for i in movies_chosen]\n            # st.write(\'Movie indexes\', list(ids))\n            embs = load_mekd()\n            state = torch.cat([torch.cat([embs[i] for i in ids]), torch.tensor(list(ratings.values())).float() - 5])\n            st.write(\'your state\', state)\n            state = state.to(device).squeeze(0)\n\n            models = load_models(device)\n            algorithm = st.selectbox(\'Choose an algorithm\', (\'ddpg\', \'td3\'))\n\n            metric = st.selectbox(\'Choose a metric\', (\'euclidean\', \'cosine\', \'correlation\',\n                                                      \'canberra\', \'minkowski\', \'chebyshev\',\n                                                      \'braycurtis\', \'cityblock\',))\n\n            dist = {\'euclidean\': distance.euclidean, \'cosine\': distance.cosine,\n                    \'correlation\': distance.correlation, \'canberra\': distance.canberra,\n                    \'minkowski\': distance.minkowski, \'chebyshev\': distance.chebyshev,\n                    \'braycurtis\': distance.braycurtis, \'cityblock\': distance.cityblock}\n\n            topk = st.slider(""TOP K items to recommend:"", min_value=1, max_value=30, value=7)\n            action = models[algorithm].forward(state)\n\n            st.subheader(\'The neural network thinks you should watch:\')\n            st.write(rank(action[0].detach().cpu().numpy(), dist[metric], topk))\n\n    if page == ""\xf0\x9f\xa4\x96 Reinforce Top K"":\n        st.title(""\xf0\x9f\xa4\x96 Reinforce Top K"")\n        st.markdown(""**Reinforce is a discrete state algorithm, meaning a lot of metrics (i.e. error, diversity test) ""\n                    ""won\'t be possible. **"")\n        st.subheader(\'This page is under construction\')\n\n\n\n\n\n\n\n\nif __name__ == ""__main__"":\n    main()'"
recnn/__init__.py,0,"b'from recnn import optim, data, utils, nn, rep\n'"
recnn/optim.py,7,"b'import math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\nimport itertools as it\n\n# https://github.com/LiyuanLucasLiu/RAdam\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'RAdam does not support sparse gradients\')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[\'step\'] += 1\n                buffered = self.buffer[int(state[\'step\'] % 10)]\n                if state[\'step\'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\'step\']\n                    beta2_t = beta2 ** state[\'step\']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it\'s an approximated value\n                    if N_sma >= 5:\n                        step_size = group[\'lr\'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[\'step\'])\n                    else:\n                        step_size = group[\'lr\'] / (1 - beta1 ** state[\'step\'])\n                    buffered[2] = step_size\n\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n\n                # more conservative since it\'s an approximated value\n                if N_sma >= 5:            \n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n    \n    \n#Ranger deep learning optimizer - RAdam + Lookahead combined.\n#https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n\n#Ranger has now been used to capture 12 records on the FastAI leaderboard.\n\n#This version = 9.3.19  \n\n#Credits:\n#RAdam -->  https://github.com/LiyuanLucasLiu/RAdam\n#Lookahead --> rewritten by lessw2020, but big thanks to Github @LonePatient and @RWightman for ideas from their code.\n#Lookahead paper --> MZhang,G Hinton  https://arxiv.org/abs/1907.08610\n\n#summary of changes: \n#full code integration with all updates at param level instead of group, moves slow weights into state dict (from generic weights), \n#supports group learning rates (thanks @SHolderbach), fixes sporadic load from saved model issues.\n#changes 8/31/19 - fix references to *self*.N_sma_threshold; \n                #changed eps to 1e-5 as better default than 1e-8.\n\n\n\n\nclass Ranger(Optimizer):\n\n    def __init__(self, params, lr=1e-3, alpha=0.5, k=6, N_sma_threshhold=5, betas=(.95,0.999), eps=1e-5, weight_decay=0):\n        #parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\'Invalid slow update rate: {alpha}\')\n        if not 1 <= k:\n            raise ValueError(f\'Invalid lookahead steps: {k}\')\n        if not lr > 0:\n            raise ValueError(f\'Invalid Learning Rate: {lr}\')\n        if not eps > 0:\n            raise ValueError(f\'Invalid eps: {eps}\')\n\n        #parameter comments:\n        # beta1 (momentum) of .95 seems to work better than .90...\n        #N_sma_threshold of 5 seems better in testing than 4.\n        #In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n\n        #prep defaults and init torch.optim base\n        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas, N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n        super().__init__(params,defaults)\n\n        #adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n        #look ahead params\n        self.alpha = alpha\n        self.k = k \n\n        #radam buffer for state\n        self.radam_buffer = [[None,None,None] for ind in range(10)]\n\n    def __setstate__(self, state):\n        print(""set state called"")\n        super(Ranger, self).__setstate__(state)\n\n\n    def step(self, closure=None):\n        loss = None\n        #note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.  \n        #Uncomment if you need to use the actual closure...\n\n        #if closure is not None:\n            #loss = closure()\n\n        #Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'Ranger optimizer does not support sparse gradients\')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]  #get state dict for this param\n\n                if len(state) == 0:   #if first time to run...init dictionary with our desired entries\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n\n                    #look ahead weight storage now in state dict \n                    state[\'slow_buffer\'] = torch.empty_like(p.data)\n                    state[\'slow_buffer\'].copy_(p.data)\n\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\n\n                #begin computations \n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                #compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                #compute mean moving avg\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[\'step\'] += 1\n\n\n                buffered = self.radam_buffer[int(state[\'step\'] % 10)]\n                if state[\'step\'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\'step\']\n                    beta2_t = beta2 ** state[\'step\']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[\'step\'])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state[\'step\'])\n                    buffered[2] = step_size\n\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n                    p_data_fp32.addcdiv_(-step_size * group[\'lr\'], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group[\'lr\'], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n                #integrated look ahead...\n                #we do it at the param level instead of group level\n                if state[\'step\'] % group[\'k\'] == 0:\n                    slow_p = state[\'slow_buffer\'] #get access to slow param tensor\n                    slow_p.add_(self.alpha, p.data - slow_p)  #(fast weights - slow weights) * alpha\n                    p.data.copy_(slow_p)  #copy interpolated weights to RAdam param tensor\n\n        return loss\n'"
.circleci/tests/learning.py,6,"b""import torch\n\n# == recnn ==\nimport recnn\n\nstate = torch.randn(10, 1290)\naction = torch.randn(10, 128)\nreward = torch.randn(10, 1)\nnext_state = torch.randn(10, 1290)\ndone = torch.randn(10, 1)\nbatch = {'state': state, 'action': action, 'reward': reward, 'next_state': next_state, 'done': done}\n\nvalue_net = recnn.nn.Critic(1290, 128, 256, 54e-2)\npolicy_net = recnn.nn.Actor(1290, 128, 256, 6e-1)\n\n\ndef test_recommendation():\n\n    recommendation = policy_net(state)\n    value = value_net(state, recommendation)\n\n    assert recommendation.std() > 0 and recommendation.mean != 0\n    assert value.std() > 0\n\n\ndef check_loss_and_networks(loss, nets):\n    assert loss['value'] > 0 and loss['policy'] != 0 and loss['step'] == 0\n    for name, netw in nets.items():\n        assert netw.training == ('target' not in name)\n\n\ndef test_update_function():\n    target_value_net = recnn.nn.Critic(1290, 128, 256)\n    target_policy_net = recnn.nn.Actor(1290, 128, 256)\n\n    target_policy_net.eval()\n    target_value_net.eval()\n\n\n    # soft update\n    recnn.utils.soft_update(value_net, target_value_net, soft_tau=1.0)\n    recnn.utils.soft_update(policy_net, target_policy_net, soft_tau=1.0)\n\n    # define optimizers\n    value_optimizer = recnn.optim.RAdam(value_net.parameters(),\n                                  lr=1e-5, weight_decay=1e-2)\n    policy_optimizer = recnn.optim.RAdam(policy_net.parameters(), lr=1e-5 , weight_decay=1e-2)\n\n    nets = {\n        'value_net': value_net,\n        'target_value_net': target_value_net,\n        'policy_net': policy_net,\n        'target_policy_net': target_policy_net,\n    }\n\n    optimizer = {\n        'policy_optimizer': policy_optimizer,\n        'value_optimizer':  value_optimizer\n    }\n\n    debug = {}\n    writer = recnn.utils.misc.DummyWriter()\n\n    step = 0\n    params = {\n        'gamma'      : 0.99,\n        'min_value'  : -10,\n        'max_value'  : 10,\n        'policy_step': 10,\n        'soft_tau'   : 0.001,\n    }\n\n    loss = recnn.nn.update.ddpg_update(batch, params, nets, optimizer,\n                           torch.device('cpu'), debug, writer, step=step)\n\n    check_loss_and_networks(loss, nets)\n\n\ndef test_algo():\n    value_net  = recnn.nn.Critic(1290, 128, 256, 54e-2)\n    policy_net = recnn.nn.Actor(1290, 128, 256, 6e-1)\n\n    ddpg = recnn.nn.DDPG(policy_net, value_net)\n    ddpg = ddpg\n    loss = ddpg.update(batch, learn=True)\n    check_loss_and_networks(loss, ddpg.nets)\n"""
.circleci/tests/main.py,0,"b'from learning import *\n\nif __name__ == ""__main__"":\n    test_recommendation()\n    test_update_function()\n    test_algo()\n    print(""All passed"")\n'"
docs/source/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport sys\nsys.path.append(""../../"")\nimport recnn\nimport sphinx_rtd_theme\n\nautodoc_mock_imports = [\'torch\', \'tqdm\']\n\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'recnn\'\ncopyright = \'2019, Mike Watts\'\nauthor = \'Mike Watts\'\n\n# The full version, including alpha/beta/rc tags\nrelease = \'0.1\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\', \'sphinx_rtd_theme\',]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nmaster_doc = \'index\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n'"
recnn/data/__init__.py,0,"b'from . import utils, env\nfrom .utils import *\nfrom .env import *\nfrom .dataset_functions import *\n'"
recnn/data/dataset_functions.py,0,"b'from recnn.data.utils import make_items_tensor\n\n""""""     \n    What?\n    +++++\n    \n    Chain of responsibility pattern.\n    https://refactoring.guru/design-patterns/chain-of-responsibility/python/example\n    \n    RecNN is designed to work with your data flow. \n    Function that contain \'dataset\' are needed to interact with environment.\n    The environment is provided via env.argument.\n    These functions can interact with env and set up some stuff how you like.\n    They are also designed to be argument agnostic\n    \n    Basically you can stack them how you want.\n    \n    To further illustrate this, let\'s take a look onto code sample from FrameEnv::\n    \n        class Env:\n            def __init__(self, ...,\n                 prepare_dataset=dataset_functions.prepare_dataset, # <- look at this function provided here\n                 .....):\n    \n                self.user_dict = None\n                self.users = None  # filtered keys of user_dict\n            \n                self.prepare_dataset(df=self.ratings, key_to_id=self.key_to_id,\n                                     min_seq_size=min_seq_size, frame_size=min_seq_size, env=self)\n                                         \n                # after this call user_dict and users should be set to their values!\n                \n    In reinforce example I further modify it to look like::\n    \n        def prepare_dataset(**kwargs):\n            recnn.data.build_data_pipeline([recnn.data.truncate_dataset,\n                                            recnn.data.prepare_dataset], reduce_items_to=5000, **kwargs)\n                                            \n    Notice: prepare_dataset doesn\'t take **reduce_items_to** argument, but it is required in truncate_dataset.\n    As I previously mentioned RecNN is designed to be argument agnostic, meaning you provide some kwarg in the  \n    build_data_pipeline function, and it is passed down the function chain. If needed, it will be used. Otherwise, ignored  \n""""""\n\n\ndef prepare_dataset(df, key_to_id, frame_size, env, sort_users=False, **kwargs):\n\n    """"""\n        Basic prepare dataset function. Automatically makes index linear, in ml20 movie indices look like:\n        [1, 34, 123, 2000], recnn makes it look like [0,1,2,3] for you.\n    """"""\n\n    df[\'rating\'] = df[\'rating\'].progress_apply(lambda i: 2 * (i - 2.5))\n    df[\'movieId\'] = df[\'movieId\'].progress_apply(lambda i: key_to_id.get(i))\n\n    users = df[[\'userId\', \'movieId\']].groupby([\'userId\']).size()\n    users = users[users > frame_size]\n    if sort_users:\n        users = users.sort_values(ascending=False)\n    users = users.index\n    ratings = df.sort_values(by=\'timestamp\').set_index(\'userId\').drop(\'timestamp\', axis=1).groupby(\'userId\')\n\n    # Groupby user\n    user_dict = {}\n\n    def app(x):\n        userid = x.index[0]\n        user_dict[int(userid)] = {}\n        user_dict[int(userid)][\'items\'] = x[\'movieId\'].values\n        user_dict[int(userid)][\'ratings\'] = x[\'rating\'].values\n\n    ratings.progress_apply(app)\n\n    env.user_dict = user_dict\n    env.users = users\n\n    return {\'df\': df, \'key_to_id\': key_to_id,\n            \'frame_size\': frame_size, \'env\': env, \'sort_users\': sort_users, **kwargs}\n\n\ndef truncate_dataset(df, key_to_id, frame_size, env, reduce_items_to, sort_users=False, **kwargs):\n    """"""\n        Truncate #items to num_items provided in the arguments\n    """"""\n\n    # here are adjusted n items to keep\n    num_items = reduce_items_to\n\n    to_remove = df[\'movieId\'].value_counts().sort_values()[:-num_items].index\n    to_keep = df[\'movieId\'].value_counts().sort_values()[-num_items:].index\n    to_remove_indices = df[df[\'movieId\'].isin(to_remove)].index\n    num_removed = len(to_remove)\n\n    df.drop(to_remove_indices, inplace=True)\n\n    for i in list(env.movie_embeddings_key_dict.keys()):\n        if i not in to_keep:\n            del env.movie_embeddings_key_dict[i]\n\n    env.embeddings, env.key_to_id, env.id_to_key = make_items_tensor(env.movie_embeddings_key_dict)\n\n    print(\'action space is reduced to {} - {} = {}\'.format(num_items + num_removed, num_removed,\n                                                           num_items))\n\n    return {\'df\': df, \'key_to_id\': env.key_to_id, \'env\': env,\n            \'frame_size\': frame_size, \'sort_users\': sort_users, **kwargs}\n\n\ndef build_data_pipeline(chain, **kwargs):\n    """"""\n        Chain of responsibility pattern\n\n        :param chain: array of callable\n        :param **kwargs: any kwargs you like\n    """"""\n\n    kwargdict = kwargs\n    for call in chain:\n        kwargdict = call(**kwargdict)\n    return kwargdict\n\n'"
recnn/data/env.py,12,"b'from . import utils, dataset_functions as dset_F\nimport pickle\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\n\n""""""\n.. module:: env\n   :synopsis: Main abstraction of the library for datasets is called environment, similar to how other reinforcement\n    learning libraries name it. This interface is created to provide SARSA like input for your RL Models. When you are \n    working with recommendation env, you have two choices: using static length inputs (say 10 items) or dynamic length \n    time series with sequential encoders (many to one rnn). Static length is provided via FrameEnv, and dynamic length\n    along with sequential state representation encoder is implemented in SeqEnv. Let\xe2\x80\x99s take a look at FrameEnv first:\n\n\n.. moduleauthor:: Mike Watts <awarebayes@gmail.com>\n\n\n""""""\n\n\nclass UserDataset(Dataset):\n\n    """"""\n    Low Level API: dataset class user: [items, ratings], Instance of torch.DataSet\n    """"""\n\n    def __init__(self, users, user_dict):\n        """"""\n\n        :param users: integer list of user_id. Useful for train/test splitting\n        :type users: list<int>.\n        :param user_dict: dictionary of users with user_id as key and [items, ratings] as value\n        :type user_dict: (dict{ user_id<int>: dict{\'items\': list<int>, \'ratings\': list<int>} }).\n\n        """"""\n\n        self.users = users\n        self.user_dict = user_dict\n\n    def __len__(self):\n        """"""\n        useful for tqdm, consists of a single line:\n        return len(self.users)\n        """"""\n        return len(self.users)\n\n    def __getitem__(self, idx):\n        """"""\n        getitem is a function where non linear user_id maps to a linear index. For instance in the ml20m dataset,\n        there are big gaps between neighbouring user_id. getitem removes these gaps, optimizing the speed.\n\n        :param idx: index drawn from range(0, len(self.users)). User id can be not linear, idx is.\n        :type idx: int\n\n        :returns:  dict{\'items\': list<int>, rates:list<int>, sizes: int}\n        """"""\n        idx = self.users[idx]\n        group = self.user_dict[idx]\n        items = group[\'items\'][:]\n        rates = group[\'ratings\'][:]\n        size = items.shape[0]\n        return {\'items\': items, \'rates\': rates, \'sizes\': size, \'users\': idx}\n\n\nclass Env:\n\n    """"""\n    Env abstract class\n    """"""\n\n    def __init__(self, embeddings, ratings, test_size=0.05, min_seq_size=10,\n                 prepare_dataset=dset_F.prepare_dataset,\n                 embed_batch=utils.batch_tensor_embeddings):\n\n        """"""\n        .. note::\n            embeddings need to be provided in {movie_id: torch.tensor} format!\n\n        :param embeddings: path to where item embeddings are stored.\n        :type embeddings: str\n        :param ratings: path to the dataset that is similar to the ml20m\n        :type ratings: str\n        :param test_size: ratio of users to use in testing. Rest will be used for training/validation\n        :type test_size: int\n        :param min_seq_size: filter users: len(user.items) > min seq size\n        :type min_seq_size: int\n        :param prepare_dataset: function you provide. should yield user_dict, users\n        :type prepare_dataset: function\n        :param embed_batch: function to apply embeddings to batch. can be set to yield continuous/discrete state/action\n        :type embed_batch: function\n        """"""\n\n        self.prepare_dataset = prepare_dataset\n        self.embed_batch = embed_batch\n        self.movie_embeddings_key_dict = pickle.load(open(embeddings, \'rb\'))\n        movies_embeddings_tensor, key_to_id, id_to_key = utils.make_items_tensor(self.movie_embeddings_key_dict)\n        self.embeddings = movies_embeddings_tensor\n        self.key_to_id = key_to_id\n        self.id_to_key = id_to_key\n        self.ratings = pd.read_csv(ratings)\n\n        self.user_dict = None\n        self.users = None  # filtered keys of user_dict\n\n        self.prepare_dataset(df=self.ratings, key_to_id=self.key_to_id,\n                             min_seq_size=min_seq_size, frame_size=min_seq_size, env=self)\n        # after this call user_dict and users should be set to their values!\n\n        self.train_users, self.test_users = train_test_split(self.users, test_size=test_size)\n        self.train_users = utils.sort_users_itemwise(self.user_dict, self.train_users)[2:]\n        self.test_users = utils.sort_users_itemwise(self.user_dict, self.test_users)\n        self.train_user_dataset = UserDataset(self.train_users, self.user_dict)\n        self.test_user_dataset = UserDataset(self.test_users, self.user_dict)\n\n\nclass FrameEnv(Env):\n    """"""\n    Static length user environment.\n    """"""\n    def __init__(self, embeddings, ratings, frame_size=10, batch_size=25, num_workers=1, *args, **kwargs):\n\n        """"""\n        :param embeddings: path to where item embeddings are stored.\n        :type embeddings: str\n        :param ratings: path to the dataset that is similar to the ml20m\n        :type ratings: str\n        :param frame_size: len of a static sequence, frame\n        :type frame_size: int\n\n        p.s. you can also provide **pandas_conf in the arguments.\n\n        It is useful if you dataset columns are different from ml20::\n\n            pandas_conf = {user_id=\'userId\', rating=\'rating\', item=\'movieId\', timestamp=\'timestamp\'}\n            env = FrameEnv(embed_dir, rating_dir, **pandas_conf)\n\n        """"""\n\n        super(FrameEnv, self).__init__(embeddings, ratings, min_seq_size=frame_size+1, *args, **kwargs)\n\n        def prepare_batch_wrapper(x):\n            batch = utils.prepare_batch_static_size(x, self.embeddings,\n                                                    embed_batch=self.embed_batch,\n                                                    frame_size=frame_size)\n            return batch\n\n        self.prepare_batch_wrapper = prepare_batch_wrapper\n        self.frame_size = frame_size\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n        self.train_dataloader = DataLoader(self.train_user_dataset, batch_size=batch_size,\n                                           shuffle=True, num_workers=num_workers, collate_fn=prepare_batch_wrapper)\n\n        self.test_dataloader = DataLoader(self.test_user_dataset, batch_size=batch_size,\n                                          shuffle=True, num_workers=num_workers, collate_fn=prepare_batch_wrapper)\n\n    def train_batch(self):\n        """""" Get batch for training """"""\n        return next(iter(self.train_dataloader))\n\n    def test_batch(self):\n        """""" Get batch for testing """"""\n        return next(iter(self.test_dataloader))\n\n\nclass SeqEnv(Env):\n\n    """"""\n    WARNING: THIS FEATURE IS IN ALPHA\n    Dynamic length user environment.\n    Due to some complications, this module is implemented quiet differently from FrameEnv.\n    First of all, it relies on the replay buffer. Train/Test batch is a generator.\n    In batch generator, I iterate through the batch, and choose target action with certain probability.\n    Hence, ~95% is state that is encoded with state encoder and ~5% are actions.\n    If you have a better solution, your contribution is welcome\n    """"""\n\n    def __init__(self, embeddings, ratings,  state_encoder, batch_size=25, device=torch.device(\'cuda\'),\n                 layout=None, max_buf_size=1000, num_workers=1, embed_batch=utils.batch_tensor_embeddings,\n                 *args, **kwargs):\n\n        """"""\n        :param embeddings: path to where item embeddings are stored.\n        :type embeddings: str\n        :param ratings: path to the dataset that is similar to the ml20m\n        :type ratings: str\n        :param state_encoder: state encoder of your choice\n        :type state_encoder: nn.Module\n        :param device: device of your choice\n        :type device: torch.device\n        :param max_buf_size: maximum size of a replay buffer\n        :type max_buf_size: int\n        :param layout: how sizes in batch should look like\n        :type layout: list<torch.Size>\n        """"""\n\n        super(SeqEnv, self).__init__(embeddings, ratings, min_seq_size=10, *args, **kwargs)\n        print(""Sequential support is super experimental and is not guaranteed to work"")\n        self.embed_batch = embed_batch\n\n        if layout is None:\n            # default ml20m layout for my (action=128) embeddings, (state=256)\n            layout = [torch.Size([max_buf_size, 256]),\n                      torch.Size([max_buf_size, 128]),\n                      torch.Size([max_buf_size, 1]),\n                      torch.Size([max_buf_size, 256])]\n\n        def prepare_batch_wrapper(batch):\n\n            batch = utils.padder(batch)\n            batch = utils.prepare_batch_dynamic_size(batch, self.embeddings)\n            return batch\n\n        self.prepare_batch_wrapper = prepare_batch_wrapper\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n        self.device = device\n        self.state_encoder = state_encoder\n        self.max_buf_size = max_buf_size\n        self.train_dataloader = DataLoader(self.train_user_dataset, batch_size=batch_size,\n                                           shuffle=False, num_workers=num_workers, collate_fn=prepare_batch_wrapper)\n        self.test_dataloader = DataLoader(self.test_user_dataset, batch_size=batch_size,\n                                          shuffle=False, num_workers=num_workers, collate_fn=prepare_batch_wrapper)\n\n        self.buffer_layout = layout\n\n        self.train_buffer = utils.ReplayBuffer(self.max_buf_size, layout=self.buffer_layout)\n        self.test_buffer = utils.ReplayBuffer(self.max_buf_size, layout=self.buffer_layout)\n\n    def train_batch(self):\n        while 1:\n            for batch in tqdm(self.train_dataloader):\n                items, ratings, sizes, users = utils.get_irsu(batch)\n                items, ratings, sizes = [i.to(self.device) for i in [items, ratings, sizes]]\n                hidden = None\n                state = None\n                self.train_buffer.meta.update({\'sizes\': sizes, \'users\': users})\n                for t in range(int(sizes.min().item()) - 1):\n                    action = items[:, t]\n                    reward = ratings[:, t].unsqueeze(-1)\n                    s = torch.cat([action, reward], 1).unsqueeze(0)\n                    next_state, hidden = self.state_encoder(s, hidden) if hidden else self.state_encoder(s)\n                    next_state = next_state.squeeze()\n\n                    if np.random.random() > 0.95 and state is not None:\n                        batch = {\'state\': state, \'action\': action, \'reward\': reward,\n                                 \'next_state\': next_state, \'step\': t}\n                        self.train_buffer.append(batch)\n\n                    if self.train_buffer.len() >= self.max_buf_size:\n                        g = self.train_buffer.get()\n                        self.train_buffer.flush()\n                        yield g\n\n                    state = next_state\n\n    def test_batch(self):\n        while 1:\n            for batch in tqdm(self.test_dataloader):\n                batch = [i.to(self.device) for i in batch]\n                items, ratings, sizes = batch\n                hidden = None\n                state = None\n                for t in range(int(sizes.min().item()) - 1):\n                    action = items[:, t]\n                    reward = ratings[:, t].unsqueeze(-1)\n                    s = torch.cat([action, reward], 1).unsqueeze(0)\n                    next_state, hidden = self.state_encoder(s, hidden) if hidden else self.state_encoder(s)\n                    next_state = next_state.squeeze()\n\n                    if np.random.random() > 0.95 and state is not None:\n                        batch = [state, action, reward, next_state]\n                        self.test_buffer.append(batch)\n\n                    if self.test_buffer.len() >= self.max_buf_size:\n                        g = self.test_buffer.get()\n                        self.test_buffer.flush()\n                        yield g\n                        del g\n\n                    state = next_state\n'"
recnn/data/utils.py,26,"b'\nimport numpy as np\nimport torch\nimport pandas as pd\n\n# helper function similar to pandas.Series.rolling\ndef rolling_window(a, window):\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\n\ndef get_irsu(batch):\n    items_t, ratings_t, sizes_t, users_t = batch[\'items\'], batch[\'ratings\'], batch[\'sizes\'], batch[\'users\']\n    return items_t, ratings_t, sizes_t, users_t\n\n\ndef batch_no_embeddings(batch, frame_size, *args, **kwargs):\n    """"""\n    Embed Batch: discrete state discrete action\n    """"""\n    items_t, ratings_t, sizes_t, users_t = get_irsu(batch)\n    b_size = ratings_t.size(0)\n    items = items_t[:, :-1]\n    next_items = items_t[:, 1:]\n    ratings = ratings_t[:, :-1]\n    next_ratings = ratings_t[:, 1:]\n    action = items_t[:, -1]\n    reward = ratings_t[:, -1]\n    done = torch.zeros(b_size)\n\n    done[torch.cumsum(sizes_t - frame_size, dim=0) - 1] = 1\n    batch = {\'items\': items, \'next_items\': next_items,\n             ratings: \'ratings\', \'next_ratings\': next_ratings,\n             \'action\': action, \'reward\': reward, \'done\': done,\n             \'meta\': {\'users\': users_t, \'sizes\': sizes_t}}\n    return batch\n\n\ndef batch_tensor_embeddings(batch, item_embeddings_tensor, frame_size, *args, **kwargs):\n    """"""\n    Embed Batch: continuous state continuous action\n    """"""\n\n    items_t, ratings_t, sizes_t, users_t = get_irsu(batch)\n    items_emb = item_embeddings_tensor[items_t.long()]\n    b_size = ratings_t.size(0)\n\n    items = items_emb[:, :-1, :].view(b_size, -1)\n    next_items = items_emb[:, 1:, :].view(b_size, -1)\n    ratings = ratings_t[:, :-1]\n    next_ratings = ratings_t[:, 1:]\n\n    state = torch.cat([items, ratings], 1)\n    next_state = torch.cat([next_items, next_ratings], 1)\n    action = items_emb[:, -1, :]\n    reward = ratings_t[:, -1]\n\n    done = torch.zeros(b_size)\n    done[torch.cumsum(sizes_t - frame_size, dim=0) - 1] = 1\n\n    batch = {\'state\': state, \'action\': action, \'reward\': reward, \'next_state\': next_state, \'done\': done,\n             \'meta\': {\'users\': users_t, \'sizes\': sizes_t}}\n    return batch\n\n\ndef batch_contstate_discaction(batch, item_embeddings_tensor, frame_size, num_items, *args, **kwargs):\n\n    """"""\n    Embed Batch: continuous state discrete action\n    """"""\n\n    items_t, ratings_t, sizes_t, users_t = get_irsu(batch)\n    items_emb = item_embeddings_tensor[items_t.long()]\n    b_size = ratings_t.size(0)\n\n    items = items_emb[:, :-1, :].view(b_size, -1)\n    next_items = items_emb[:, 1:, :].view(b_size, -1)\n    ratings = ratings_t[:, :-1]\n    next_ratings = ratings_t[:, 1:]\n\n    state = torch.cat([items, ratings], 1)\n    next_state = torch.cat([next_items, next_ratings], 1)\n    action = items_t[:, -1]\n    reward = ratings_t[:, -1]\n\n    done = torch.zeros(b_size)\n    done[torch.cumsum(sizes_t - frame_size, dim=0) - 1] = 1\n\n    one_hot_action = torch.zeros(b_size, num_items)\n    one_hot_action.scatter_(1, action.view(-1, 1), 1)\n\n    batch = {\'state\': state, \'action\': one_hot_action, \'reward\': reward, \'next_state\': next_state, \'done\': done,\n             \'meta\': {\'users\': users_t, \'sizes\': sizes_t}}\n    return batch\n\n# pads stuff to work with lstms\ndef padder(x):\n    items_t = []\n    ratings_t = []\n    sizes_t = []\n    users_t = []\n    for i in range(len(x)):\n        items_t.append(torch.tensor(x[i][\'items\']))\n        ratings_t.append(torch.tensor(x[i][\'rates\']))\n        sizes_t.append(x[i][\'sizes\'])\n        users_t.append(x[i][\'users\'])\n    items_t = torch.nn.utils.rnn.pad_sequence(items_t, batch_first=True).long()\n    ratings_t = torch.nn.utils.rnn.pad_sequence(ratings_t, batch_first=True).float()\n    sizes_t = torch.tensor(sizes_t).float()\n    return {\'items\': items_t, \'ratings\': ratings_t, \'sizes\': sizes_t, \'users\': users_t}\n\n\ndef sort_users_itemwise(user_dict, users):\n    return pd.Series(dict([(i, user_dict[i][\'items\'].shape[0]) for i in users])).sort_values(ascending=False).index\n\n\ndef prepare_batch_dynamic_size(batch, item_embeddings_tensor, embed_batch=None):\n    item_idx, ratings_t, sizes_t, users_t = get_irsu(batch)\n    item_t = item_embeddings_tensor[item_idx]\n    batch = {\'items\': item_t, \'users\': users_t, \'ratings\': ratings_t, \'sizes\': sizes_t}\n    return batch\n\n\n# Main function that is used as torch.DataLoader->collate_fn\n# CollateFn docs:\n# https://pytorch.org/docs/stable/data.html#working-with-collate-fn\n\ndef prepare_batch_static_size(batch, item_embeddings_tensor, frame_size=10,\n                              embed_batch=batch_tensor_embeddings):\n    item_t, ratings_t, sizes_t, users_t = [], [], [], []\n    for i in range(len(batch)):\n        item_t.append(batch[i][\'items\'])\n        ratings_t.append(batch[i][\'rates\'])\n        sizes_t.append(batch[i][\'sizes\'])\n        users_t.append(batch[i][\'users\'])\n\n    item_t = np.concatenate([rolling_window(i, frame_size + 1) for i in item_t], 0)\n    ratings_t = np.concatenate([rolling_window(i, frame_size + 1) for i in ratings_t], 0)\n\n    item_t = torch.tensor(item_t)\n    users_t = torch.tensor(users_t)\n    ratings_t = torch.tensor(ratings_t).float()\n    sizes_t = torch.tensor(sizes_t)\n\n    batch = {\'items\': item_t, \'users\': users_t, \'ratings\': ratings_t, \'sizes\': sizes_t}\n\n    return embed_batch(batch=batch,  item_embeddings_tensor=item_embeddings_tensor,\n                       frame_size=frame_size)\n\n\n# Usually in data sets there item index is inconsistent (if you plot it doesn\'t look like a line)\n# This function makes the index linear, allows for better compression of the data\n# And also makes use of tensor[tensor] semantics\n\n# items_embeddings_key_dict:arg - item embeddings by key\n# include_zero:arg - whether to include items_embeddings_id_dict[0] = [0, 0, 0, ..., 0] (128)\n# sometimes needed for rnn padding, by default True\n# returns:\n# items_embeddings_tensor - items_embeddings_dict compressed into tensor\n# key_to_id - dict key -> index\n# id_to_key - dict index -> key\n\n\ndef make_items_tensor(items_embeddings_key_dict):\n    keys = list(sorted(items_embeddings_key_dict.keys()))\n    key_to_id = dict(zip(keys, range(len(keys))))\n    id_to_key = dict(zip(range(len(keys)), keys))\n\n    items_embeddings_id_dict = {}\n    for k in items_embeddings_key_dict.keys():\n        items_embeddings_id_dict[key_to_id[k]] = items_embeddings_key_dict[k]\n    items_embeddings_tensor = torch.stack([items_embeddings_id_dict[i] for i in range(len(items_embeddings_id_dict))])\n    return items_embeddings_tensor, key_to_id, id_to_key\n\n\nclass ReplayBuffer:\n    def __init__(self, buffer_size, layout):\n        self.buffer = None\n        self.idx = 0\n        self.size = buffer_size\n        self.layout = layout\n        self.meta = {\'step\': []}\n        self.flush()\n\n    def flush(self):\n        # state, action, reward, next_state\n        del self.buffer\n        self.buffer = [torch.zeros(i) for i in self.layout]\n        self.idx = 0\n        self.meta[\'step\'] = []\n\n    def append(self, batch):\n        state, action, reward, next_state, step = batch[\'state\'], batch[\'action\'], \\\n                                                  batch[\'reward\'], batch[\'next_state\'], batch[\'step\']\n        self.meta[\'step\'].append(step)\n        lower = self.idx\n        upper = state.size(0) + lower\n        self.buffer[0][lower:upper] = state\n        self.buffer[1][lower:upper] = action\n        self.buffer[2][lower:upper] = reward\n        self.buffer[3][lower:upper] = next_state\n        self.idx = upper\n\n    def get(self):\n        state, action, reward, next_state = self.buffer\n        batch = {\'state\': state, \'action\': action, \'reward\': reward, \'next_state\': next_state,\n                 \'meta\': self.meta}\n        return batch\n\n    def len(self):\n        return self.idx\n\n\ndef get_base_batch(batch, device=torch.device(\'cuda\'), done=True):\n    b = [batch[\'state\'], batch[\'action\'], batch[\'reward\'].unsqueeze(1), batch[\'next_state\'], ]\n    if done:\n        b.append(batch[\'done\'].unsqueeze(1))\n    else:\n        batch.append(torch.zeros_like(batch[\'reward\']))\n    return [i.to(device) for i in b]\n\n\n'"
recnn/nn/__init__.py,0,"b'from . import algo, models, update\nfrom .models import *\nfrom .algo import *\nfrom .update import *\n'"
recnn/nn/algo.py,2,"b'from recnn import utils, optim\nfrom recnn.nn import update\nfrom recnn.nn.update import ChooseREINFORCE\n\nimport torch\nimport copy\n\n\n""""""\n Algorithms as is aren\'t much. Just classes with pre set up parameters, optimizers and stuff.\n""""""\n\n\nclass Algo:\n    def __init__(self):\n        self.nets = {\n            \'value_net\': None,\n            \'policy_net\': None,\n        }\n\n        self.optimizers = {\n            \'policy_optimizer\': None,\n            \'value_optimizer\': None\n        }\n\n        self.params = {\n            \'Some parameters here\': None\n        }\n\n        self._step = 0\n\n        self.debug = {}\n\n        # by default it will not output anything\n        # use torch.SummaryWriter instance if you want output\n        self.writer = utils.misc.DummyWriter()\n\n        self.device = torch.device(\'cpu\')\n\n        self.loss_layout = {\n            \'test\': {\'value\': [], \'policy\': [], \'step\': []},\n            \'train\': {\'value\': [], \'policy\': [], \'step\': []}\n        }\n\n        self.algorithm = None\n\n    def update(self, batch, learn=True):\n        return self.algorithm(batch, self.params, self.nets, self.optimizers,\n                              device=self.device, debug=self.debug, writer=self.writer,\n                              learn=learn, step=self._step)\n\n    def to(self, device):\n        self.nets = {k: v.to(device) for k, v in self.nets.items()}\n        self.device = device\n        return self\n\n    def step(self):\n        self._step += 1\n\n\nclass DDPG(Algo):\n\n    def __init__(self, policy_net, value_net):\n\n        super(DDPG, self).__init__()\n\n        self.algorithm = update.ddpg_update\n\n        # these are target networks that we need for ddpg algorigm to work\n        target_policy_net = copy.deepcopy(policy_net)\n        target_value_net = copy.deepcopy(value_net)\n\n        target_policy_net.eval()\n        target_value_net.eval()\n\n        # soft update\n        utils.soft_update(value_net, target_value_net, soft_tau=1.0)\n        utils.soft_update(policy_net, target_policy_net, soft_tau=1.0)\n\n        # define optimizers\n        value_optimizer = optim.Ranger(value_net.parameters(), lr=1e-5, weight_decay=1e-2)\n        policy_optimizer = optim.Ranger(policy_net.parameters(), lr=1e-5, weight_decay=1e-2)\n\n        self.nets = {\n            \'value_net\': value_net,\n            \'target_value_net\': target_value_net,\n            \'policy_net\': policy_net,\n            \'target_policy_net\': target_policy_net,\n        }\n\n        self.optimizers = {\n            \'policy_optimizer\': policy_optimizer,\n            \'value_optimizer\': value_optimizer\n        }\n\n        self.params = {\n            \'gamma\': 0.99,\n            \'min_value\': -10,\n            \'max_value\': 10,\n            \'policy_step\': 10,\n            \'soft_tau\': 0.001,\n        }\n\n        self.loss_layout = {\n            \'test\': {\'value\': [], \'policy\': [], \'step\': []},\n            \'train\': {\'value\': [], \'policy\': [], \'step\': []}\n        }\n\n\nclass TD3(Algo):\n\n    def __init__(self, policy_net, value_net1, value_net2):\n\n        super(TD3, self).__init__()\n\n        self.algorithm = update.td3_update\n\n        # these are target networks that we need for TD3 algorigm to work\n        target_policy_net = copy.deepcopy(policy_net)\n        target_value_net1 = copy.deepcopy(value_net1)\n        target_value_net2 = copy.deepcopy(value_net2)\n\n        target_policy_net.eval()\n        target_value_net1.eval()\n        target_value_net2.eval()\n\n        # soft update\n        utils.soft_update(value_net1, target_value_net1, soft_tau=1.0)\n        utils.soft_update(value_net2, target_value_net2, soft_tau=1.0)\n        utils.soft_update(policy_net, target_policy_net, soft_tau=1.0)\n\n        # define optimizers\n        value_optimizer1 = optim.Ranger(value_net1.parameters(), lr=1e-5, weight_decay=1e-2)\n        value_optimizer2 = optim.Ranger(value_net2.parameters(), lr=1e-5, weight_decay=1e-2)\n        policy_optimizer = optim.Ranger(policy_net.parameters(), lr=1e-5, weight_decay=1e-2)\n\n        self.nets = {\n            \'value_net1\': value_net1,\n            \'target_value_net1\': target_value_net1,\n            \'value_net2\': value_net2,\n            \'target_value_net2\': target_value_net2,\n            \'policy_net\': policy_net,\n            \'target_policy_net\': target_policy_net,\n        }\n\n        self.optimizers = {\n            \'policy_optimizer\': policy_optimizer,\n            \'value_optimizer1\': value_optimizer1,\n            \'value_optimizer2\': value_optimizer2,\n        }\n\n        self.params = {\n            \'gamma\': 0.99,\n            \'noise_std\': 0.5,\n            \'noise_clip\': 3,\n            \'soft_tau\': 0.001,\n            \'policy_update\': 10,\n\n            \'policy_lr\': 1e-5,\n            \'value_lr\': 1e-5,\n\n            \'actor_weight_init\': 25e-2,\n            \'critic_weight_init\': 6e-1,\n        }\n\n        self.loss_layout = {\n            \'test\': {\'value1\': [], \'value2\': [], \'policy\': [], \'step\': []},\n            \'train\': {\'value1\': [], \'value2\': [], \'policy\': [], \'step\': []}\n        }\n\n\nclass Reinforce(Algo):\n    def __init__(self, policy_net, value_net):\n\n        super(Reinforce, self).__init__()\n\n        self.algorithm = update.reinforce_update\n\n        # these are target networks that we need for ddpg algorigm to work\n        target_policy_net = copy.deepcopy(policy_net)\n        target_value_net = copy.deepcopy(value_net)\n\n        target_policy_net.eval()\n        target_value_net.eval()\n\n        # soft update\n        utils.soft_update(value_net, target_value_net, soft_tau=1.0)\n        utils.soft_update(policy_net, target_policy_net, soft_tau=1.0)\n\n        # define optimizers\n        value_optimizer = optim.Ranger(value_net.parameters(), lr=1e-5, weight_decay=1e-2)\n        policy_optimizer = optim.Ranger(policy_net.parameters(), lr=1e-5, weight_decay=1e-2)\n\n        self.nets = {\n            \'value_net\': value_net,\n            \'target_value_net\': target_value_net,\n            \'policy_net\': policy_net,\n            \'target_policy_net\': target_policy_net,\n        }\n\n        self.optimizers = {\n            \'policy_optimizer\': policy_optimizer,\n            \'value_optimizer\': value_optimizer\n        }\n\n        self.params = {\n            \'reinforce\': ChooseREINFORCE(ChooseREINFORCE.basic_reinforce),\n            \'K\': 10,\n            \'gamma\': 0.99,\n            \'min_value\': -10,\n            \'max_value\': 10,\n            \'policy_step\': 10,\n            \'soft_tau\': 0.001,\n        }\n\n        self.loss_layout = {\n            \'test\': {\'value\': [], \'policy\': [], \'step\': []},\n            \'train\': {\'value\': [], \'policy\': [], \'step\': []}\n        }\n\n'"
recnn/nn/models.py,13,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\n\n\nclass AnomalyDetector(nn.Module):\n\n    """"""\n    Anomaly detector used for debugging. Basically an auto encoder.\n    P.S. You need to use different weights for different embeddings.\n    """"""\n\n    def __init__(self):\n        super(AnomalyDetector, self).__init__()\n        self.ae = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.BatchNorm1d(32),\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        """"""""""""\n        return self.ae(x)\n\n    def rec_error(self, x):\n        error = torch.sum((x - self.ae(x)) ** 2, 1)\n        if x.size(1) != 1:\n            return error.detach()\n        return error.item()\n\n\nclass Actor(nn.Module):\n\n    """"""\n    Vanilla actor. Takes state as an argument, returns action.\n    """"""\n\n    def __init__(self, input_dim, action_dim, hidden_size, init_w=2e-1):\n        super(Actor, self).__init__()\n        \n        self.drop_layer = nn.Dropout(p=0.5)\n        \n        self.linear1 = nn.Linear(input_dim, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, hidden_size)\n        self.linear3 = nn.Linear(hidden_size, action_dim)\n        \n        self.linear3.weight.data.uniform_(-init_w, init_w)\n        self.linear3.bias.data.uniform_(-init_w, init_w)\n        \n    def forward(self, state, tanh=False):\n        """"""\n        :param action: nothing should be provided here.\n        :param state: state\n        :param tanh: whether to use tahn as action activation\n        :return: action\n        """"""\n        action = F.relu(self.linear1(state))\n        action = self.drop_layer(action)\n        action = F.relu(self.linear2(action))\n        action = self.drop_layer(action)\n        action = self.linear3(action)\n        if tanh:\n            action = F.tanh(action)\n        return action\n\n\nclass DiscreteActor(nn.Module):\n    def __init__(self, input_dim, action_dim, hidden_size, init_w=0):\n        super(DiscreteActor, self).__init__()\n\n        self.linear1 = nn.Linear(input_dim, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, action_dim)\n\n        self.saved_log_probs = []\n        self.rewards = []\n        self.correction = []\n        self.lambda_k = []\n\n        # What\'s action source? See this issue: https://github.com/awarebayes/RecNN/issues/7\n        # by default {pi: pi, beta: beta}\n        # you can change it to be like {pi: beta, beta: beta} as miracle24 suggested\n\n        self.action_source = {\'pi\': \'pi\', \'beta\': \'beta\'}\n        self.select_action = self._select_action\n\n    def forward(self, inputs):\n        x = inputs\n        x = F.relu(self.linear1(x))\n        action_scores = self.linear2(x)\n        return F.softmax(action_scores)\n\n    def gc(self):\n        del self.rewards[:]\n        del self.saved_log_probs[:]\n        del self.correction[:]\n        del self.lambda_k[:]\n\n    def _select_action(self, state, **kwargs):\n\n        # for reinforce without correction only pi_probs is available.\n        # the action source is ignored, since there is no beta\n\n        pi_probs = self.forward(state)\n        pi_categorical = Categorical(pi_probs)\n        pi_action = pi_categorical.sample()\n        self.saved_log_probs.append(pi_categorical.log_prob(pi_action))\n        return pi_probs\n\n    def pi_beta_sample(self, state, beta, action, **kwargs):\n        # 1. obtain probabilities\n        # note: detach is to block gradient\n        beta_probs = beta(state.detach(), action=action)\n        pi_probs = self.forward(state)\n\n        # 2. probabilities -> categorical distribution.\n        beta_categorical = Categorical(beta_probs)\n        pi_categorical = Categorical(pi_probs)\n\n        # 3. sample the actions\n        # See this issue: https://github.com/awarebayes/RecNN/issues/7\n        # usually it works like:\n        # pi_action = pi_categorical.sample(); beta_action = beta_categorical.sample();\n        # but changing the action_source to {pi: beta, beta: beta} can be configured to be:\n        # pi_action = beta_categorical.sample(); beta_action = beta_categorical.sample();\n        available_actions = {\'pi\': pi_categorical.sample(), \'beta\': beta_categorical.sample()}\n        pi_action = available_actions[self.action_source[\'pi\']]\n        beta_action = available_actions[self.action_source[\'beta\']]\n\n        # 4. calculate stuff we need\n        pi_log_prob = pi_categorical.log_prob(pi_action)\n        beta_log_prob = beta_categorical.log_prob(beta_action)\n\n        return pi_log_prob, beta_log_prob, pi_probs\n\n    def _select_action_with_correction(self, state, beta, action, writer, step, **kwargs):\n        pi_log_prob, beta_log_prob, pi_probs = self.pi_beta_sample(state, beta, action)\n\n        # calculate correction\n        corr = torch.exp(pi_log_prob) / torch.exp(beta_log_prob)\n\n        writer.add_histogram(\'correction\', corr, step)\n        writer.add_histogram(\'pi_log_prob\', pi_log_prob, step)\n        writer.add_histogram(\'beta_log_prob\', beta_log_prob, step)\n\n        self.correction.append(corr)\n        self.saved_log_probs.append(pi_log_prob)\n\n        return pi_probs\n\n    def _select_action_with_TopK_correction(self, state, beta, action, K, writer, step, **kwargs):\n        pi_log_prob, beta_log_prob, pi_probs = self.pi_beta_sample(state, beta, action)\n\n        # calculate correction\n        corr = torch.exp(pi_log_prob) / torch.exp(beta_log_prob)\n\n        # calculate top K correction\n        l_k = K * (1 - torch.exp(pi_log_prob)) ** (K-1)\n\n        writer.add_histogram(\'correction\', corr, step)\n        writer.add_histogram(\'l_k\', l_k, step)\n        writer.add_histogram(\'pi_log_prob\', pi_log_prob, step)\n        writer.add_histogram(\'beta_log_prob\', beta_log_prob, step)\n\n        self.correction.append(corr)\n        self.lambda_k.append(l_k)\n        self.saved_log_probs.append(pi_log_prob)\n\n        return pi_probs\n\n\nclass Critic(nn.Module):\n\n    """"""\n    Vanilla critic. Takes state and action as an argument, returns value.\n    """"""\n\n    def __init__(self, input_dim, action_dim, hidden_size, init_w=3e-5):\n        super(Critic, self).__init__()\n        \n        self.drop_layer = nn.Dropout(p=0.5)\n        \n        self.linear1 = nn.Linear(input_dim + action_dim, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, hidden_size)\n        self.linear3 = nn.Linear(hidden_size, 1)\n        \n        self.linear3.weight.data.uniform_(-init_w, init_w)\n        self.linear3.bias.data.uniform_(-init_w, init_w)\n        \n    def forward(self, state, action):\n        """"""""""""\n        value = torch.cat([state, action], 1)\n        value = F.relu(self.linear1(value))\n        value = self.drop_layer(value)\n        value = F.relu(self.linear2(value))\n        value = self.drop_layer(value)\n        value = self.linear3(value)\n        return value\n\n\nclass bcqPerturbator(nn.Module):\n\n    """"""\n    Batch constrained perturbative actor. Takes action as an argument, adjusts it.\n    """"""\n\n    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-1):\n        super(bcqPerturbator, self).__init__()\n        \n        self.drop_layer = nn.Dropout(p=0.5)\n        \n        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, hidden_size)\n        self.linear3 = nn.Linear(hidden_size, num_actions)\n        \n        self.linear3.weight.data.uniform_(-init_w, init_w)\n        self.linear3.bias.data.uniform_(-init_w, init_w)\n        \n    def forward(self, state, action):\n        """"""""""""\n        a = torch.cat([state, action], 1)\n        a = F.relu(self.linear1(a))\n        a = self.drop_layer(a)\n        a = F.relu(self.linear2(a))\n        a = self.drop_layer(a)\n        a = self.linear3(a) \n        return a + action\n\n\nclass bcqGenerator(nn.Module):\n\n    """"""\n    Batch constrained generator. Basically VAE\n    """"""\n\n    def __init__(self, state_dim, action_dim, latent_dim):\n        super(bcqGenerator, self).__init__()\n        # encoder\n        self.e1 = nn.Linear(state_dim + action_dim, 750)\n        self.e2 = nn.Linear(750, 750)\n\n        self.mean = nn.Linear(750, latent_dim)\n        self.log_std = nn.Linear(750, latent_dim)\n        \n        # decoder\n        self.d1 = nn.Linear(state_dim + latent_dim, 750)\n        self.d2 = nn.Linear(750, 750)\n        self.d3 = nn.Linear(750, action_dim)\n        \n        self.latent_dim = latent_dim\n        self.normal = torch.distributions.Normal(0, 1)\n\n    def forward(self, state, action):\n        """"""""""""\n        # z is encoded state + action\n        z = F.relu(self.e1(torch.cat([state, action], 1)))\n        z = F.relu(self.e2(z))\n\n        mean = self.mean(z)\n        # Clamped for numerical stability \n        log_std = self.log_std(z).clamp(-4, 15)\n        std = torch.exp(log_std)\n        z = mean + std * self.normal.sample(std.size()).to(next(self.parameters()).device)\n\n        # u is decoded action\n        u = self.decode(state, z)\n\n        return u, mean, std\n    \n    def decode(self, state, z=None):\n        # When sampling from the VAE, the latent vector is clipped to [-0.5, 0.5]\n        if z is None:\n            z = self.normal.sample([state.size(0), self.latent_dim])\n            z = z.clamp(-0.5, 0.5).to(next(self.parameters()).device)\n\n        a = F.relu(self.d1(torch.cat([state, z], 1)))\n        a = F.relu(self.d2(a))\n        return self.d3(a)\n'"
recnn/rep/TCN.py,2,"b'\n# SOURCE: https://github.com/locuslab/TCN/blob/master/TCN/tcn.py\n\nimport torch.nn as nn\nfrom torch.nn.utils import weight_norm\n\n\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n        super(TemporalConvNet, self).__init__()\n        layers = []\n        num_levels = len(num_channels)\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.network(x)\n'"
recnn/rep/__init__.py,0,b'from . import TCN\n'
recnn/utils/__init__.py,0,"b'from . import misc, plot\nfrom .misc import *\nfrom .plot import *\n'"
recnn/utils/misc.py,0,"b""def soft_update(net, target_net, soft_tau=1e-2):\n    for target_param, param in zip(target_net.parameters(), net.parameters()):\n            target_param.data.copy_(\n                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n            )\n\n\ndef write_losses(writer, loss_dict, kind='train'):\n\n    def write_loss(kind, key, item, step):\n        writer.add_scalar(kind + '/' + key, item, global_step=step)\n\n    step = loss_dict['step']\n    for k, v in loss_dict.items():\n        if k == 'step':\n            continue\n        write_loss(kind, k, v, step)\n\n    writer.close()\n\n\nclass DummyWriter:\n    def add_figure(self, *args, **kwargs):\n        pass\n\n    def add_histogram(self, *args, **kwargs):\n        pass\n\n    def add_scalar(self, *args, **kwargs):\n        pass\n\n    def add_scalars(self, *args, **kwargs):\n        pass\n\n    def close(self, *args, **kwargs):\n        pass\n"""
recnn/utils/plot.py,2,"b""from scipy.spatial import distance\nfrom scipy import ndimage\nimport matplotlib.pyplot as plt\nimport torch\nfrom scipy import stats\nimport numpy as np\n\n\ndef pairwise_distances_fig(embs):\n    embs = embs.detach().cpu().numpy()\n    similarity_matrix_cos = distance.cdist(embs, embs, 'cosine')\n    similarity_matrix_euc = distance.cdist(embs, embs, 'euclidean')\n\n    fig = plt.figure(figsize=(16,10))\n\n    ax = fig.add_subplot(121)\n    cax = ax.matshow(similarity_matrix_cos)\n    fig.colorbar(cax)\n    ax.set_title('Cosine')\n    ax.axis('off')\n\n    ax = fig.add_subplot(122)\n    cax = ax.matshow(similarity_matrix_euc)\n    fig.colorbar(cax)\n    ax.set_title('Euclidian')\n    ax.axis('off')\n\n    fig.suptitle('Action pairwise distances')\n    plt.close()\n    return fig\n\n\ndef pairwise_distances(embs):\n    fig = pairwise_distances_fig(embs)\n    fig.show()\n\n\ndef smooth(scalars, weight):  # Weight between 0 and 1\n    last = scalars[0]  # First value in the plot (first timestep)\n    smoothed = list()\n    for point in scalars:\n        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n        smoothed.append(smoothed_val)                        # Save it\n        last = smoothed_val                                  # Anchor the last smoothed value\n\n    return smoothed\n\n\ndef smooth_gauss(arr, var):\n    return ndimage.gaussian_filter1d(arr, var)\n\n\nclass Plotter:\n    def __init__(self, loss, style):\n        self.loss = loss\n        self.style = style\n        self.smoothing = lambda x: smooth_gauss(x, 4)\n\n    def set_smoothing_func(self, f):\n        self.smoothing = f\n\n    def plot_loss(self):\n        for row in self.style:\n            fig, axes = plt.subplots(1, len(row), figsize=(16, 6))\n            if len(row) == 1: axes = [axes]\n            for col in range(len(row)):\n                key = row[col]\n                axes[col].set_title(key)\n                axes[col].plot(self.loss['train']['step'],\n                               self.smoothing(self.loss['train'][key]), 'b-',\n                               label='train')\n                axes[col].plot(self.loss['test']['step'],\n                               self.loss['test'][key], 'r-.',\n                               label='test')\n            plt.legend()\n        plt.show()\n\n    def log_loss(self, key, item, test=False):\n        kind = 'train'\n        if test:\n            kind = 'test'\n        self.loss[kind][key].append(item)\n\n    def log_losses(self, losses, test=False):\n        for key, val in losses.items():\n            self.log_loss(key, val, test)\n\n    @staticmethod\n    def kde_reconstruction_error(ad, gen_actions, true_actions, device=torch.device('cpu')):\n\n        def rec_score(actions):\n            return ad.rec_error(torch.tensor(actions).to(device).float()).detach().cpu().numpy()\n\n        true_scores = rec_score(true_actions)\n        gen_scores = rec_score(gen_actions)\n\n        true_kernel = stats.gaussian_kde(true_scores)\n        gen_kernel = stats.gaussian_kde(gen_scores)\n\n        x = np.linspace(0, 1000, 100)\n        probs_true = true_kernel(x)\n        probs_gen = gen_kernel(x)\n        fig = plt.figure(figsize=(16, 10))\n        ax = fig.add_subplot(111)\n        ax.plot(x, probs_true, '-b', label='true dist')\n        ax.plot(x, probs_gen, '-r', label='generated dist')\n        ax.legend()\n        return fig\n\n    @staticmethod\n    def plot_kde_reconstruction_error(*args, **kwargs):\n        fig = Plotter.kde_reconstruction_error(*args, **kwargs)\n        fig.show()\n\n"""
recnn/nn/update/__init__.py,0,"b'from .misc import temporal_difference, value_update\nfrom .ddpg import ddpg_update\nfrom .td3 import td3_update\nfrom .bcq import bcq_update\nfrom .reinforce import ChooseREINFORCE, reinforce_update\n'"
recnn/nn/update/bcq.py,11,"b'import torch\nimport torch.functional as F\n\nfrom recnn import utils\nfrom recnn import data\nfrom recnn.utils import soft_update\nfrom recnn.nn.update import temporal_difference\n\n\n# batch, params, writer, debug, learn=True, step=-1\ndef bcq_update(batch, params, nets, optimizer,\n               device=torch.device(\'cpu\'),\n               debug=None, writer=utils.DummyWriter(),\n               learn=False, step=-1):\n\n    """"""\n    :param batch: batch [state, action, reward, next_state] returned by environment.\n    :param params: dict of algorithm parameters.\n    :param nets: dict of networks.\n    :param optimizer: dict of optimizers\n    :param device: torch.device\n    :param debug: dictionary where debug data about actions is saved\n    :param writer: torch.SummaryWriter\n    :param learn: whether to learn on this step (used for testing)\n    :param step: integer step for policy update\n    :return: loss dictionary\n\n    How parameters should look like::\n\n        params = {\n            # algorithm parameters\n            \'gamma\'              : 0.99,\n            \'soft_tau\'           : 0.001,\n            \'n_generator_samples\': 10,\n            \'perturbator_step\'   : 30,\n\n            # learning rates\n            \'perturbator_lr\' : 1e-5,\n            \'value_lr\'       : 1e-5,\n            \'generator_lr\'   : 1e-3,\n        }\n\n\n        nets = {\n            \'generator_net\': models.bcqGenerator,\n            \'perturbator_net\': models.bcqPerturbator,\n            \'target_perturbator_net\': models.bcqPerturbator,\n            \'value_net1\': models.Critic,\n            \'target_value_net1\': models.Critic,\n            \'value_net2\': models.Critic,\n            \'target_value_net2\': models.Critic,\n        }\n\n        optimizer = {\n            \'generator_optimizer\': some optimizer\n            \'policy_optimizer\': some optimizer\n            \'value_optimizer1\':  some optimizer\n            \'value_optimizer2\':  some optimizer\n        }\n\n\n    """"""\n\n    if debug is None:\n        debug = dict()\n    state, action, reward, next_state, done = data.get_base_batch(batch, device=device)\n    batch_size = done.size(0)\n\n    # --------------------------------------------------------#\n    # Variational Auto-Encoder Learning\n    recon, mean, std = nets[\'generator_net\'](state, action)\n    recon_loss = F.mse_loss(recon, action)\n    KL_loss = -0.5 * (1 + torch.log(std.pow(2)) - mean.pow(2) - std.pow(2)).mean()\n    generator_loss = recon_loss + 0.5 * KL_loss\n\n    if not learn:\n        writer.add_histogram(\'generator_mean\', mean, step)\n        writer.add_histogram(\'generator_std\', std, step)\n        debug[\'recon\'] = recon\n        writer.add_figure(\'reconstructed\',\n                          utils.pairwise_distances_fig(recon[:50]), step)\n\n    if learn:\n        optimizer[\'generator_optimizer\'].zero_grad()\n        generator_loss.backward()\n        optimizer[\'generator_optimizer\'].step()\n    # --------------------------------------------------------#\n    # Value Learning\n    with torch.no_grad():\n        # p.s. repeat_interleave was added in torch 1.1\n        # if an error pops up, run \'conda update pytorch\'\n        state_rep = torch.repeat_interleave(next_state, params[\'n_generator_samples\'], 0)\n        sampled_action = nets[\'generator_net\'].decode(state_rep)\n        perturbed_action = nets[\'target_perturbator_net\'](state_rep, sampled_action)\n        target_Q1 = nets[\'target_value_net1\'](state_rep, perturbed_action)\n        target_Q2 = nets[\'target_value_net1\'](state_rep, perturbed_action)\n        target_value = 0.75 * torch.min(target_Q1, target_Q2)  # value soft update\n        target_value += 0.25 * torch.max(target_Q1, target_Q2)  #\n        target_value = target_value.view(batch_size, -1).max(1)[0].view(-1, 1)\n\n        expected_value = temporal_difference(reward, done, params[\'gamma\'], target_value)\n\n    value = nets[\'value_net1\'](state, action)\n    value_loss = torch.pow(value - expected_value.detach(), 2).mean()\n\n    if learn:\n        optimizer[\'value_optimizer1\'].zero_grad()\n        optimizer[\'value_optimizer2\'].zero_grad()\n        value_loss.backward()\n        optimizer[\'value_optimizer1\'].step()\n        optimizer[\'value_optimizer2\'].step()\n    else:\n        writer.add_histogram(\'value\', value, step)\n        writer.add_histogram(\'target_value\', target_value, step)\n        writer.add_histogram(\'expected_value\', expected_value, step)\n        writer.close()\n\n    # --------------------------------------------------------#\n    # Perturbator learning\n    sampled_actions = nets[\'generator_net\'].decode(state)\n    perturbed_actions = nets[\'perturbator_net\'](state, sampled_actions)\n    perturbator_loss = -nets[\'value_net1\'](state, perturbed_actions)\n    if not learn:\n        writer.add_histogram(\'perturbator_loss\', perturbator_loss, step)\n    perturbator_loss = perturbator_loss.mean()\n\n    if learn:\n        if step % params[\'perturbator_step\'] == 0:\n            optimizer[\'perturbator_optimizer\'].zero_grad()\n            perturbator_loss.backward()\n            torch.nn.utils.clip_grad_norm_(nets[\'perturbator_net\'].parameters(), -1, 1)\n            optimizer[\'perturbator_optimizer\'].step()\n\n        soft_update(nets[\'value_net1\'], nets[\'target_value_net1\'], soft_tau=params[\'soft_tau\'])\n        soft_update(nets[\'value_net2\'], nets[\'target_value_net2\'], soft_tau=params[\'soft_tau\'])\n        soft_update(nets[\'perturbator_net\'], nets[\'target_perturbator_net\'], soft_tau=params[\'soft_tau\'])\n    else:\n        debug[\'sampled_actions\'] = sampled_actions\n        debug[\'perturbed_actions\'] = perturbed_actions\n        writer.add_figure(\'sampled_actions\',\n                          utils.pairwise_distances_fig(sampled_actions[:50]), step)\n        writer.add_figure(\'perturbed_actions\',\n                          utils.pairwise_distances_fig(perturbed_actions[:50]), step)\n\n    # --------------------------------------------------------#\n\n    losses = {\'value\': value_loss.item(),\n              \'perturbator\': perturbator_loss.item(),\n              \'generator\': generator_loss.item(),\n              \'step\': step}\n\n    utils.write_losses(writer, losses, kind=\'train\' if learn else \'test\')\n    return losses\n'"
recnn/nn/update/ddpg.py,4,"b'import torch\nfrom recnn import utils\nfrom recnn import data\nfrom recnn.utils import soft_update\nfrom recnn.nn.update import value_update\n\n\ndef ddpg_update(batch, params, nets, optimizer,\n                device=torch.device(\'cpu\'),\n                debug=None, writer=utils.DummyWriter(),\n                learn=False, step=-1):\n\n\n    """"""\n    :param batch: batch [state, action, reward, next_state] returned by environment.\n    :param params: dict of algorithm parameters.\n    :param nets: dict of networks.\n    :param optimizer: dict of optimizers\n    :param device: torch.device\n    :param debug: dictionary where debug data about actions is saved\n    :param writer: torch.SummaryWriter\n    :param learn: whether to learn on this step (used for testing)\n    :param step: integer step for policy update\n    :return: loss dictionary\n\n    How parameters should look like::\n\n        params = {\n            \'gamma\'      : 0.99,\n            \'min_value\'  : -10,\n            \'max_value\'  : 10,\n            \'policy_step\': 3,\n            \'soft_tau\'   : 0.001,\n            \'policy_lr\'  : 1e-5,\n            \'value_lr\'   : 1e-5,\n            \'actor_weight_init\': 3e-1,\n            \'critic_weight_init\': 6e-1,\n        }\n        nets = {\n            \'value_net\': models.Critic,\n            \'target_value_net\': models.Critic,\n            \'policy_net\': models.Actor,\n            \'target_policy_net\': models.Actor,\n        }\n        optimizer - {\n            \'policy_optimizer\': some optimizer\n            \'value_optimizer\':  some optimizer\n        }\n\n    """"""\n\n    state, action, reward, next_state, done = data.get_base_batch(batch, device=device)\n\n    # --------------------------------------------------------#\n    # Value Learning\n\n    value_loss = value_update(batch, params, nets, optimizer,\n                              writer=writer, device=device,\n                              debug=debug, learn=learn, step=step)\n\n    # --------------------------------------------------------#\n    # Policy learning\n\n    gen_action = nets[\'policy_net\'](state)\n    policy_loss = -nets[\'value_net\'](state, gen_action)\n\n    if not learn:\n        debug[\'gen_action\'] = gen_action\n        writer.add_histogram(\'policy_loss\', policy_loss, step)\n        writer.add_figure(\'next_action\',\n                          utils.pairwise_distances_fig(gen_action[:50]), step)\n    policy_loss = policy_loss.mean()\n\n    if learn and step % params[\'policy_step\'] == 0:\n        optimizer[\'policy_optimizer\'].zero_grad()\n        policy_loss.backward(retain_graph=True)\n        torch.nn.utils.clip_grad_norm_(nets[\'policy_net\'].parameters(), -1, 1)\n        optimizer[\'policy_optimizer\'].step()\n\n        soft_update(nets[\'value_net\'], nets[\'target_value_net\'], soft_tau=params[\'soft_tau\'])\n        soft_update(nets[\'policy_net\'], nets[\'target_policy_net\'], soft_tau=params[\'soft_tau\'])\n\n    losses = {\'value\': value_loss.item(), \'policy\': policy_loss.item(), \'step\': step}\n    utils.write_losses(writer, losses, kind=\'train\' if learn else \'test\')\n    return losses\n'"
recnn/nn/update/misc.py,4,"b'\nimport torch\nfrom recnn import utils\nfrom recnn import data\n\n\ndef temporal_difference(reward, done, gamma, target):\n    return reward + (1.0 - done) * gamma * target\n\n\ndef value_update(batch, params, nets, optimizer,\n                 device=torch.device(\'cpu\'),\n                 debug=None, writer=utils.DummyWriter(),\n                 learn=False, step=-1):\n    """"""\n        Everything is the same as in ddpg_update\n    """"""\n\n    state, action, reward, next_state, done = data.get_base_batch(batch, device=device)\n\n    with torch.no_grad():\n        next_action = nets[\'target_policy_net\'](next_state)\n        target_value = nets[\'target_value_net\'](next_state, next_action.detach())\n        expected_value = temporal_difference(reward, done, params[\'gamma\'], target_value)\n        expected_value = torch.clamp(expected_value,\n                                     params[\'min_value\'], params[\'max_value\'])\n\n    value = nets[\'value_net\'](state, action)\n\n    value_loss = torch.pow(value - expected_value.detach(), 2).mean()\n\n    if learn:\n        optimizer[\'value_optimizer\'].zero_grad()\n        value_loss.backward(retain_graph=True)\n        optimizer[\'value_optimizer\'].step()\n\n    elif not learn:\n        debug[\'next_action\'] = next_action\n        writer.add_figure(\'next_action\',\n                          utils.pairwise_distances_fig(next_action[:50]), step)\n        writer.add_histogram(\'value\', value, step)\n        writer.add_histogram(\'target_value\', target_value, step)\n        writer.add_histogram(\'expected_value\', expected_value, step)\n\n    return value_loss\n'"
recnn/nn/update/reinforce.py,5,"b""import torch\n\nfrom recnn import utils\nfrom recnn import data\nfrom recnn.utils import soft_update\nfrom recnn.nn.update import value_update\nimport gc\n\n\nclass ChooseREINFORCE:\n    def __init__(self, method=None):\n        if method is None:\n            method = ChooseREINFORCE.basic_reinforce\n        self.method = method\n\n    @staticmethod\n    def basic_reinforce(policy, returns, *args, **kwargs):\n        policy_loss = []\n        for log_prob, R in zip(policy.saved_log_probs, returns):\n            policy_loss.append(-log_prob * R)  # <- this line here\n        policy_loss = torch.cat(policy_loss).sum()\n        return policy_loss\n\n    @staticmethod\n    def reinforce_with_correction(policy, returns, *args, **kwargs):\n        policy_loss = []\n        for corr, log_prob, R in zip(policy.correction, policy.saved_log_probs, returns):\n            policy_loss.append(corr * -log_prob * R)  # <- this line here\n        policy_loss = torch.cat(policy_loss).sum()\n        return policy_loss\n\n    @staticmethod\n    def reinforce_with_TopK_correction(policy, returns, *args, **kwargs):\n        policy_loss = []\n        for l_k, corr, log_prob, R in zip(policy.lambda_k, policy.correction, policy.saved_log_probs, returns):\n            policy_loss.append(l_k * corr * -log_prob * R)  # <- this line here\n        policy_loss = torch.cat(policy_loss).sum()\n        return policy_loss\n\n    def __call__(self, policy, optimizer, learn=True):\n        R = 0\n\n        returns = []\n        for r in policy.rewards[::-1]:\n            R = r + 0.99 * R\n            returns.insert(0, R)\n\n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + 0.0001)\n\n        policy_loss = self.method(policy, returns)\n\n        if learn:\n            optimizer.zero_grad()\n            policy_loss.backward()\n            optimizer.step()\n\n        policy.gc()\n        gc.collect()\n\n        return policy_loss\n\n\ndef reinforce_update(batch, params, nets, optimizer,\n                     device=torch.device('cpu'),\n                     debug=None, writer=utils.DummyWriter(),\n                     learn=True, step=-1):\n\n    # Due to its mechanics, reinforce doesn't support testing!\n    learn = True\n\n    state, action, reward, next_state, done = data.get_base_batch(batch)\n\n    predicted_probs = nets['policy_net'].select_action(state=state, action=action, K=params['K'],\n                                                       learn=learn, writer=writer, step=step)\n    writer.add_histogram('predicted_probs_std', predicted_probs.std(), step)\n    writer.add_histogram('predicted_probs_mean', predicted_probs.mean(), step)\n    mx = predicted_probs.max(dim=1).values\n    writer.add_histogram('predicted_probs_max_mean', mx.mean(), step)\n    writer.add_histogram('predicted_probs_max_std', mx.std(), step)\n    reward = nets['value_net'](state, predicted_probs).detach()\n    nets['policy_net'].rewards.append(reward.mean())\n\n    value_loss = value_update(batch, params, nets, optimizer,\n                              writer=writer, device=device,\n                              debug=debug, learn=True, step=step)\n\n    if step % params['policy_step'] == 0 and step > 0:\n        policy_loss = params['reinforce'](nets['policy_net'], optimizer['policy_optimizer'],)\n\n        utils.soft_update(nets['value_net'], nets['target_value_net'], soft_tau=params['soft_tau'])\n        utils.soft_update(nets['policy_net'], nets['target_policy_net'], soft_tau=params['soft_tau'])\n\n        losses = {'value': value_loss.item(),\n                  'policy': policy_loss.item(),\n                  'step': step}\n\n        utils.write_losses(writer, losses, kind='train' if learn else 'test')\n\n        return losses"""
recnn/nn/update/td3.py,9,"b'import torch\nfrom recnn import utils\nfrom recnn import data\nfrom recnn.utils import soft_update\nfrom recnn.nn.update import temporal_difference\n\ndef td3_update(batch, params, nets, optimizer,\n               device=torch.device(\'cpu\'),\n               debug=None, writer=utils.DummyWriter(),\n               learn=False, step=-1):\n    """"""\n    :param batch: batch [state, action, reward, next_state] returned by environment.\n    :param params: dict of algorithm parameters.\n    :param nets: dict of networks.\n    :param optimizer: dict of optimizers\n    :param device: torch.device\n    :param debug: dictionary where debug data about actions is saved\n    :param writer: torch.SummaryWriter\n    :param learn: whether to learn on this step (used for testing)\n    :param step: integer step for policy update\n    :return: loss dictionary\n\n    How parameters should look like::\n\n        params = {\n            \'gamma\': 0.99,\n            \'noise_std\': 0.5,\n            \'noise_clip\': 3,\n            \'soft_tau\': 0.001,\n            \'policy_update\': 10,\n\n            \'policy_lr\': 1e-5,\n            \'value_lr\': 1e-5,\n\n            \'actor_weight_init\': 25e-2,\n            \'critic_weight_init\': 6e-1,\n        }\n\n\n        nets = {\n            \'value_net1\': models.Critic,\n            \'target_value_net1\': models.Critic,\n            \'value_net2\': models.Critic,\n            \'target_value_net2\': models.Critic,\n            \'policy_net\': models.Actor,\n            \'target_policy_net\': models.Actor,\n        }\n\n        optimizer = {\n            \'policy_optimizer\': some optimizer\n            \'value_optimizer1\':  some optimizer\n            \'value_optimizer2\':  some optimizer\n        }\n\n\n    """"""\n\n    if debug is None:\n        debug = dict()\n    state, action, reward, next_state, done = data.get_base_batch(batch, device=device)\n\n    # --------------------------------------------------------#\n    # Value Learning\n\n    next_action = nets[\'target_policy_net\'](next_state)\n    noise = torch.normal(torch.zeros(next_action.size()),\n                         params[\'noise_std\']).to(device)\n    noise = torch.clamp(noise, -params[\'noise_clip\'], params[\'noise_clip\'])\n    next_action += noise\n\n    with torch.no_grad():\n        target_q_value1 = nets[\'target_value_net1\'](next_state, next_action)\n        target_q_value2 = nets[\'target_value_net2\'](next_state, next_action)\n        target_q_value = torch.min(target_q_value1, target_q_value2)\n        expected_q_value = temporal_difference(reward, done, params[\'gamma\'], target_q_value)\n\n    q_value1 = nets[\'value_net1\'](state, action)\n    q_value2 = nets[\'value_net2\'](state, action)\n\n    value_criterion = torch.nn.MSELoss()\n    value_loss1 = value_criterion(q_value1, expected_q_value.detach())\n    value_loss2 = value_criterion(q_value2, expected_q_value.detach())\n\n    if learn:\n        optimizer[\'value_optimizer1\'].zero_grad()\n        value_loss1.backward()\n        optimizer[\'value_optimizer1\'].step()\n\n        optimizer[\'value_optimizer2\'].zero_grad()\n        value_loss2.backward()\n        optimizer[\'value_optimizer2\'].step()\n    else:\n        debug[\'next_action\'] = next_action\n        writer.add_figure(\'next_action\',\n                          utils.pairwise_distances_fig(next_action[:50]), step)\n        writer.add_histogram(\'value1\', q_value1, step)\n        writer.add_histogram(\'value2\', q_value2, step)\n        writer.add_histogram(\'target_value\', target_q_value, step)\n        writer.add_histogram(\'expected_value\', expected_q_value, step)\n\n    # --------------------------------------------------------#\n    # Policy learning\n\n    gen_action = nets[\'policy_net\'](state)\n    policy_loss = nets[\'value_net1\'](state, gen_action)\n    policy_loss = -policy_loss\n\n    if not learn:\n        debug[\'gen_action\'] = gen_action\n        writer.add_figure(\'gen_action\',\n                          utils.pairwise_distances_fig(gen_action[:50]), step)\n        writer.add_histogram(\'policy_loss\', policy_loss, step)\n\n    policy_loss = policy_loss.mean()\n\n    # delayed policy update\n    if step % params[\'policy_update\'] == 0 and learn:\n        optimizer[\'policy_optimizer\'].zero_grad()\n        policy_loss.backward()\n        torch.nn.utils.clip_grad_norm_(nets[\'policy_net\'].parameters(), -1, 1)\n        optimizer[\'policy_optimizer\'].step()\n\n        soft_update(nets[\'value_net1\'], nets[\'target_value_net1\'], soft_tau=params[\'soft_tau\'])\n        soft_update(nets[\'value_net2\'], nets[\'target_value_net2\'], soft_tau=params[\'soft_tau\'])\n\n    losses = {\'value1\': value_loss1.item(),\n              \'value2\': value_loss2.item(),\n              \'policy\': policy_loss.item(),\n              \'step\': step}\n    utils.write_losses(writer, losses, kind=\'train\' if learn else \'test\')\n    return losses\n\n'"
